+ RUN=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='0 1'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output69
+ for fn in f1
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0
+ date
Mon Oct 26 09:07:24 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb85bf510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb8690f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb86a29d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb86b4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb85efea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb860c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb8581400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb8575e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb853c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb854c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb8520d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb84fe7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb84ae2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb84c8d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb848c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb8499268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb84a1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb84626a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb841b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb8403b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb83cd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb83cc0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb8375ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb8331598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb832a048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb8348a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb82e8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb82eff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb82c09d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb8272488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb827cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb8259950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb8244400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb81f3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb81f98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3eb81c0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.0228864e-06
Iter: 2 loss: 2.7045262e-06
Iter: 3 loss: 1.74496495e-06
Iter: 4 loss: 1.50848109e-06
Iter: 5 loss: 1.30573039e-06
Iter: 6 loss: 1.24119799e-06
Iter: 7 loss: 1.25897395e-06
Iter: 8 loss: 1.16240847e-06
Iter: 9 loss: 1.13177441e-06
Iter: 10 loss: 1.04444234e-06
Iter: 11 loss: 1.44971784e-06
Iter: 12 loss: 1.01315698e-06
Iter: 13 loss: 1.04117805e-06
Iter: 14 loss: 1.00540888e-06
Iter: 15 loss: 1.00071895e-06
Iter: 16 loss: 1.01606179e-06
Iter: 17 loss: 9.99408485e-07
Iter: 18 loss: 9.96510607e-07
Iter: 19 loss: 9.95850314e-07
Iter: 20 loss: 9.9397289e-07
Iter: 21 loss: 9.90492254e-07
Iter: 22 loss: 9.97942379e-07
Iter: 23 loss: 9.89091e-07
Iter: 24 loss: 9.85123847e-07
Iter: 25 loss: 1.03181731e-06
Iter: 26 loss: 9.85068937e-07
Iter: 27 loss: 9.8281464e-07
Iter: 28 loss: 9.7556017e-07
Iter: 29 loss: 9.81089215e-07
Iter: 30 loss: 9.69369466e-07
Iter: 31 loss: 9.55610517e-07
Iter: 32 loss: 9.89252e-07
Iter: 33 loss: 9.50707715e-07
Iter: 34 loss: 9.4523449e-07
Iter: 35 loss: 9.45146667e-07
Iter: 36 loss: 9.45907061e-07
Iter: 37 loss: 9.442482e-07
Iter: 38 loss: 9.43483542e-07
Iter: 39 loss: 9.41852136e-07
Iter: 40 loss: 9.66838684e-07
Iter: 41 loss: 9.41787164e-07
Iter: 42 loss: 9.41491578e-07
Iter: 43 loss: 9.41003634e-07
Iter: 44 loss: 9.40400071e-07
Iter: 45 loss: 9.38834035e-07
Iter: 46 loss: 9.49427886e-07
Iter: 47 loss: 9.38448522e-07
Iter: 48 loss: 9.36722188e-07
Iter: 49 loss: 9.33611886e-07
Iter: 50 loss: 1.00765237e-06
Iter: 51 loss: 9.33612966e-07
Iter: 52 loss: 9.34521609e-07
Iter: 53 loss: 9.32275043e-07
Iter: 54 loss: 9.30883743e-07
Iter: 55 loss: 9.33416345e-07
Iter: 56 loss: 9.30297176e-07
Iter: 57 loss: 9.29392684e-07
Iter: 58 loss: 9.2749292e-07
Iter: 59 loss: 9.58457349e-07
Iter: 60 loss: 9.27440794e-07
Iter: 61 loss: 9.26174152e-07
Iter: 62 loss: 9.25984693e-07
Iter: 63 loss: 9.24596407e-07
Iter: 64 loss: 9.24284336e-07
Iter: 65 loss: 9.23388711e-07
Iter: 66 loss: 9.21642311e-07
Iter: 67 loss: 9.23386438e-07
Iter: 68 loss: 9.20651587e-07
Iter: 69 loss: 9.1956241e-07
Iter: 70 loss: 9.19272338e-07
Iter: 71 loss: 9.18580554e-07
Iter: 72 loss: 9.18440776e-07
Iter: 73 loss: 9.18128706e-07
Iter: 74 loss: 9.17673049e-07
Iter: 75 loss: 9.18843853e-07
Iter: 76 loss: 9.17537932e-07
Iter: 77 loss: 9.17180785e-07
Iter: 78 loss: 9.17672423e-07
Iter: 79 loss: 9.17022135e-07
Iter: 80 loss: 9.16306e-07
Iter: 81 loss: 9.14576617e-07
Iter: 82 loss: 9.33343e-07
Iter: 83 loss: 9.14394377e-07
Iter: 84 loss: 9.12868e-07
Iter: 85 loss: 9.10969675e-07
Iter: 86 loss: 9.10804374e-07
Iter: 87 loss: 9.11307154e-07
Iter: 88 loss: 9.09823825e-07
Iter: 89 loss: 9.08772e-07
Iter: 90 loss: 9.07813615e-07
Iter: 91 loss: 9.07537697e-07
Iter: 92 loss: 9.06694481e-07
Iter: 93 loss: 9.095873e-07
Iter: 94 loss: 9.06437e-07
Iter: 95 loss: 9.05933803e-07
Iter: 96 loss: 9.05914e-07
Iter: 97 loss: 9.05588365e-07
Iter: 98 loss: 9.04876856e-07
Iter: 99 loss: 9.14138127e-07
Iter: 100 loss: 9.04808644e-07
Iter: 101 loss: 9.0368269e-07
Iter: 102 loss: 9.04306034e-07
Iter: 103 loss: 9.02933834e-07
Iter: 104 loss: 9.02235e-07
Iter: 105 loss: 9.025091e-07
Iter: 106 loss: 9.01685041e-07
Iter: 107 loss: 9.01022418e-07
Iter: 108 loss: 9.00995e-07
Iter: 109 loss: 9.00232237e-07
Iter: 110 loss: 9.00695056e-07
Iter: 111 loss: 8.99726274e-07
Iter: 112 loss: 8.99293923e-07
Iter: 113 loss: 8.99284601e-07
Iter: 114 loss: 8.98847361e-07
Iter: 115 loss: 8.9739774e-07
Iter: 116 loss: 8.97839413e-07
Iter: 117 loss: 8.96013262e-07
Iter: 118 loss: 8.9383127e-07
Iter: 119 loss: 9.04135732e-07
Iter: 120 loss: 8.9343132e-07
Iter: 121 loss: 8.95214555e-07
Iter: 122 loss: 8.92933883e-07
Iter: 123 loss: 8.92673086e-07
Iter: 124 loss: 8.92035416e-07
Iter: 125 loss: 8.98241296e-07
Iter: 126 loss: 8.91956176e-07
Iter: 127 loss: 8.91474428e-07
Iter: 128 loss: 8.96416907e-07
Iter: 129 loss: 8.91464936e-07
Iter: 130 loss: 8.90913952e-07
Iter: 131 loss: 8.91952e-07
Iter: 132 loss: 8.9067828e-07
Iter: 133 loss: 8.90297088e-07
Iter: 134 loss: 8.90083697e-07
Iter: 135 loss: 8.89943408e-07
Iter: 136 loss: 8.89099397e-07
Iter: 137 loss: 8.88386296e-07
Iter: 138 loss: 8.88125783e-07
Iter: 139 loss: 8.86887619e-07
Iter: 140 loss: 8.90847332e-07
Iter: 141 loss: 8.86544512e-07
Iter: 142 loss: 8.8617287e-07
Iter: 143 loss: 8.85893883e-07
Iter: 144 loss: 8.85526958e-07
Iter: 145 loss: 8.84909582e-07
Iter: 146 loss: 8.84921519e-07
Iter: 147 loss: 8.83811254e-07
Iter: 148 loss: 8.87966166e-07
Iter: 149 loss: 8.83566486e-07
Iter: 150 loss: 8.82869472e-07
Iter: 151 loss: 8.81054405e-07
Iter: 152 loss: 8.9311726e-07
Iter: 153 loss: 8.80619723e-07
Iter: 154 loss: 8.79310392e-07
Iter: 155 loss: 8.7931312e-07
Iter: 156 loss: 8.78193646e-07
Iter: 157 loss: 8.90332558e-07
Iter: 158 loss: 8.78171591e-07
Iter: 159 loss: 8.77801938e-07
Iter: 160 loss: 8.77009086e-07
Iter: 161 loss: 8.8819661e-07
Iter: 162 loss: 8.76953436e-07
Iter: 163 loss: 8.77054504e-07
Iter: 164 loss: 8.76698778e-07
Iter: 165 loss: 8.76437e-07
Iter: 166 loss: 8.75871251e-07
Iter: 167 loss: 8.85823738e-07
Iter: 168 loss: 8.75860735e-07
Iter: 169 loss: 8.75086926e-07
Iter: 170 loss: 8.73174201e-07
Iter: 171 loss: 8.93658523e-07
Iter: 172 loss: 8.72989744e-07
Iter: 173 loss: 8.7178023e-07
Iter: 174 loss: 8.71633688e-07
Iter: 175 loss: 8.70781832e-07
Iter: 176 loss: 8.69451242e-07
Iter: 177 loss: 8.69447263e-07
Iter: 178 loss: 8.67971266e-07
Iter: 179 loss: 8.67945e-07
Iter: 180 loss: 8.67593201e-07
Iter: 181 loss: 8.68309598e-07
Iter: 182 loss: 8.67420169e-07
Iter: 183 loss: 8.66955872e-07
Iter: 184 loss: 8.6886314e-07
Iter: 185 loss: 8.66876e-07
Iter: 186 loss: 8.66653863e-07
Iter: 187 loss: 8.66118171e-07
Iter: 188 loss: 8.73826252e-07
Iter: 189 loss: 8.66091682e-07
Iter: 190 loss: 8.65557354e-07
Iter: 191 loss: 8.65549623e-07
Iter: 192 loss: 8.65209586e-07
Iter: 193 loss: 8.6447227e-07
Iter: 194 loss: 8.73784302e-07
Iter: 195 loss: 8.64407298e-07
Iter: 196 loss: 8.63557204e-07
Iter: 197 loss: 8.63571813e-07
Iter: 198 loss: 8.6312491e-07
Iter: 199 loss: 8.6247735e-07
Iter: 200 loss: 8.62464276e-07
Iter: 201 loss: 8.61467868e-07
Iter: 202 loss: 8.62282e-07
Iter: 203 loss: 8.60879084e-07
Iter: 204 loss: 8.60128125e-07
Iter: 205 loss: 8.68993254e-07
Iter: 206 loss: 8.60115108e-07
Iter: 207 loss: 8.59395186e-07
Iter: 208 loss: 8.60560135e-07
Iter: 209 loss: 8.59057423e-07
Iter: 210 loss: 8.58521e-07
Iter: 211 loss: 8.66254311e-07
Iter: 212 loss: 8.58514341e-07
Iter: 213 loss: 8.58090232e-07
Iter: 214 loss: 8.57818804e-07
Iter: 215 loss: 8.57632187e-07
Iter: 216 loss: 8.57180567e-07
Iter: 217 loss: 8.5718e-07
Iter: 218 loss: 8.5693523e-07
Iter: 219 loss: 8.56315751e-07
Iter: 220 loss: 8.61385729e-07
Iter: 221 loss: 8.56205077e-07
Iter: 222 loss: 8.56187171e-07
Iter: 223 loss: 8.55905284e-07
Iter: 224 loss: 8.55649148e-07
Iter: 225 loss: 8.54985046e-07
Iter: 226 loss: 8.58946464e-07
Iter: 227 loss: 8.54777568e-07
Iter: 228 loss: 8.54232724e-07
Iter: 229 loss: 8.54212885e-07
Iter: 230 loss: 8.53636607e-07
Iter: 231 loss: 8.53562767e-07
Iter: 232 loss: 8.53166739e-07
Iter: 233 loss: 8.52429366e-07
Iter: 234 loss: 8.51605932e-07
Iter: 235 loss: 8.51504296e-07
Iter: 236 loss: 8.50249592e-07
Iter: 237 loss: 8.5654591e-07
Iter: 238 loss: 8.50026481e-07
Iter: 239 loss: 8.49252388e-07
Iter: 240 loss: 8.49246476e-07
Iter: 241 loss: 8.48843513e-07
Iter: 242 loss: 8.50999641e-07
Iter: 243 loss: 8.48766831e-07
Iter: 244 loss: 8.4832152e-07
Iter: 245 loss: 8.48178e-07
Iter: 246 loss: 8.47931119e-07
Iter: 247 loss: 8.47541173e-07
Iter: 248 loss: 8.47537308e-07
Iter: 249 loss: 8.47285207e-07
Iter: 250 loss: 8.46606724e-07
Iter: 251 loss: 8.51003676e-07
Iter: 252 loss: 8.4641681e-07
Iter: 253 loss: 8.46101557e-07
Iter: 254 loss: 8.45980139e-07
Iter: 255 loss: 8.45532725e-07
Iter: 256 loss: 8.448514e-07
Iter: 257 loss: 8.44842702e-07
Iter: 258 loss: 8.44184399e-07
Iter: 259 loss: 8.45644195e-07
Iter: 260 loss: 8.43930934e-07
Iter: 261 loss: 8.43043267e-07
Iter: 262 loss: 8.49311505e-07
Iter: 263 loss: 8.42970394e-07
Iter: 264 loss: 8.42554414e-07
Iter: 265 loss: 8.41834549e-07
Iter: 266 loss: 8.41835913e-07
Iter: 267 loss: 8.41059659e-07
Iter: 268 loss: 8.43291161e-07
Iter: 269 loss: 8.40810912e-07
Iter: 270 loss: 8.40300572e-07
Iter: 271 loss: 8.4028477e-07
Iter: 272 loss: 8.39956e-07
Iter: 273 loss: 8.40415908e-07
Iter: 274 loss: 8.39778068e-07
Iter: 275 loss: 8.39246638e-07
Iter: 276 loss: 8.40471444e-07
Iter: 277 loss: 8.390424e-07
Iter: 278 loss: 8.38696337e-07
Iter: 279 loss: 8.42105578e-07
Iter: 280 loss: 8.38701112e-07
Iter: 281 loss: 8.38379776e-07
Iter: 282 loss: 8.37705e-07
Iter: 283 loss: 8.48885634e-07
Iter: 284 loss: 8.3768e-07
Iter: 285 loss: 8.37203743e-07
Iter: 286 loss: 8.41618771e-07
Iter: 287 loss: 8.37184302e-07
Iter: 288 loss: 8.366016e-07
Iter: 289 loss: 8.37693847e-07
Iter: 290 loss: 8.3638497e-07
Iter: 291 loss: 8.35996502e-07
Iter: 292 loss: 8.3544893e-07
Iter: 293 loss: 8.3544262e-07
Iter: 294 loss: 8.34848834e-07
Iter: 295 loss: 8.34831326e-07
Iter: 296 loss: 8.34496859e-07
Iter: 297 loss: 8.33633749e-07
Iter: 298 loss: 8.41283622e-07
Iter: 299 loss: 8.33476633e-07
Iter: 300 loss: 8.32604428e-07
Iter: 301 loss: 8.38624146e-07
Iter: 302 loss: 8.32537353e-07
Iter: 303 loss: 8.32085504e-07
Iter: 304 loss: 8.32077717e-07
Iter: 305 loss: 8.31713237e-07
Iter: 306 loss: 8.32087323e-07
Iter: 307 loss: 8.31516957e-07
Iter: 308 loss: 8.31146451e-07
Iter: 309 loss: 8.3467836e-07
Iter: 310 loss: 8.31121611e-07
Iter: 311 loss: 8.30921579e-07
Iter: 312 loss: 8.31440218e-07
Iter: 313 loss: 8.30840804e-07
Iter: 314 loss: 8.30543058e-07
Iter: 315 loss: 8.30156864e-07
Iter: 316 loss: 8.30114459e-07
Iter: 317 loss: 8.29718829e-07
Iter: 318 loss: 8.30202396e-07
Iter: 319 loss: 8.29510327e-07
Iter: 320 loss: 8.28964403e-07
Iter: 321 loss: 8.34262778e-07
Iter: 322 loss: 8.28923191e-07
Iter: 323 loss: 8.28630391e-07
Iter: 324 loss: 8.27762733e-07
Iter: 325 loss: 8.33433376e-07
Iter: 326 loss: 8.27557756e-07
Iter: 327 loss: 8.27807526e-07
Iter: 328 loss: 8.27128702e-07
Iter: 329 loss: 8.2685176e-07
Iter: 330 loss: 8.26103246e-07
Iter: 331 loss: 8.31603643e-07
Iter: 332 loss: 8.25972847e-07
Iter: 333 loss: 8.25240704e-07
Iter: 334 loss: 8.27488122e-07
Iter: 335 loss: 8.2500685e-07
Iter: 336 loss: 8.24557901e-07
Iter: 337 loss: 8.24528172e-07
Iter: 338 loss: 8.2416642e-07
Iter: 339 loss: 8.24707456e-07
Iter: 340 loss: 8.23974119e-07
Iter: 341 loss: 8.23589062e-07
Iter: 342 loss: 8.25748657e-07
Iter: 343 loss: 8.23529263e-07
Iter: 344 loss: 8.23234359e-07
Iter: 345 loss: 8.2351346e-07
Iter: 346 loss: 8.23063942e-07
Iter: 347 loss: 8.22587708e-07
Iter: 348 loss: 8.23292908e-07
Iter: 349 loss: 8.22336176e-07
Iter: 350 loss: 8.21994661e-07
Iter: 351 loss: 8.21579476e-07
Iter: 352 loss: 8.21515357e-07
Iter: 353 loss: 8.21285312e-07
Iter: 354 loss: 8.21150593e-07
Iter: 355 loss: 8.20944877e-07
Iter: 356 loss: 8.20323862e-07
Iter: 357 loss: 8.2217656e-07
Iter: 358 loss: 8.19961429e-07
Iter: 359 loss: 8.19908621e-07
Iter: 360 loss: 8.1958342e-07
Iter: 361 loss: 8.19198362e-07
Iter: 362 loss: 8.1842677e-07
Iter: 363 loss: 8.33851857e-07
Iter: 364 loss: 8.18439844e-07
Iter: 365 loss: 8.17684224e-07
Iter: 366 loss: 8.17467708e-07
Iter: 367 loss: 8.17009095e-07
Iter: 368 loss: 8.16389615e-07
Iter: 369 loss: 8.21330559e-07
Iter: 370 loss: 8.16354714e-07
Iter: 371 loss: 8.15984038e-07
Iter: 372 loss: 8.18753165e-07
Iter: 373 loss: 8.15954e-07
Iter: 374 loss: 8.15696e-07
Iter: 375 loss: 8.16936335e-07
Iter: 376 loss: 8.15675833e-07
Iter: 377 loss: 8.15392468e-07
Iter: 378 loss: 8.15775763e-07
Iter: 379 loss: 8.15250701e-07
Iter: 380 loss: 8.14941245e-07
Iter: 381 loss: 8.16640522e-07
Iter: 382 loss: 8.14893269e-07
Iter: 383 loss: 8.14688519e-07
Iter: 384 loss: 8.14239911e-07
Iter: 385 loss: 8.19705065e-07
Iter: 386 loss: 8.14194721e-07
Iter: 387 loss: 8.13912266e-07
Iter: 388 loss: 8.13870543e-07
Iter: 389 loss: 8.13526867e-07
Iter: 390 loss: 8.12626354e-07
Iter: 391 loss: 8.18771866e-07
Iter: 392 loss: 8.12400799e-07
Iter: 393 loss: 8.11834354e-07
Iter: 394 loss: 8.11833161e-07
Iter: 395 loss: 8.11188784e-07
Iter: 396 loss: 8.12056101e-07
Iter: 397 loss: 8.10843517e-07
Iter: 398 loss: 8.10457436e-07
Iter: 399 loss: 8.10965957e-07
Iter: 400 loss: 8.10279516e-07
Iter: 401 loss: 8.09991e-07
Iter: 402 loss: 8.12025405e-07
Iter: 403 loss: 8.09968242e-07
Iter: 404 loss: 8.09733365e-07
Iter: 405 loss: 8.11137738e-07
Iter: 406 loss: 8.09686867e-07
Iter: 407 loss: 8.09488427e-07
Iter: 408 loss: 8.0944767e-07
Iter: 409 loss: 8.09332732e-07
Iter: 410 loss: 8.08931304e-07
Iter: 411 loss: 8.10412928e-07
Iter: 412 loss: 8.08853429e-07
Iter: 413 loss: 8.0847758e-07
Iter: 414 loss: 8.1025695e-07
Iter: 415 loss: 8.08402831e-07
Iter: 416 loss: 8.08086611e-07
Iter: 417 loss: 8.07346623e-07
Iter: 418 loss: 8.16868294e-07
Iter: 419 loss: 8.07282e-07
Iter: 420 loss: 8.06939909e-07
Iter: 421 loss: 8.06882667e-07
Iter: 422 loss: 8.06363232e-07
Iter: 423 loss: 8.05583795e-07
Iter: 424 loss: 8.05574132e-07
Iter: 425 loss: 8.05013769e-07
Iter: 426 loss: 8.06510684e-07
Iter: 427 loss: 8.04819e-07
Iter: 428 loss: 8.04205968e-07
Iter: 429 loss: 8.116711e-07
Iter: 430 loss: 8.04199658e-07
Iter: 431 loss: 8.03930618e-07
Iter: 432 loss: 8.03377588e-07
Iter: 433 loss: 8.12520284e-07
Iter: 434 loss: 8.03355192e-07
Iter: 435 loss: 8.02786303e-07
Iter: 436 loss: 8.08579671e-07
Iter: 437 loss: 8.0278528e-07
Iter: 438 loss: 8.02422392e-07
Iter: 439 loss: 8.05709533e-07
Iter: 440 loss: 8.02379645e-07
Iter: 441 loss: 8.02143234e-07
Iter: 442 loss: 8.0186436e-07
Iter: 443 loss: 8.0182997e-07
Iter: 444 loss: 8.01443548e-07
Iter: 445 loss: 8.07321499e-07
Iter: 446 loss: 8.01448664e-07
Iter: 447 loss: 8.01198837e-07
Iter: 448 loss: 8.02105774e-07
Iter: 449 loss: 8.01125907e-07
Iter: 450 loss: 8.00868e-07
Iter: 451 loss: 8.00579414e-07
Iter: 452 loss: 8.00536554e-07
Iter: 453 loss: 8.00190151e-07
Iter: 454 loss: 8.00854082e-07
Iter: 455 loss: 8.00036162e-07
Iter: 456 loss: 7.99391614e-07
Iter: 457 loss: 8.00269845e-07
Iter: 458 loss: 7.99061752e-07
Iter: 459 loss: 7.98608e-07
Iter: 460 loss: 7.97775215e-07
Iter: 461 loss: 8.16444697e-07
Iter: 462 loss: 7.97763448e-07
Iter: 463 loss: 7.97619e-07
Iter: 464 loss: 7.97242137e-07
Iter: 465 loss: 7.9696872e-07
Iter: 466 loss: 7.96351e-07
Iter: 467 loss: 8.06357889e-07
Iter: 468 loss: 7.96351401e-07
Iter: 469 loss: 7.95947699e-07
Iter: 470 loss: 8.00783198e-07
Iter: 471 loss: 7.95949745e-07
Iter: 472 loss: 7.95741414e-07
Iter: 473 loss: 7.97094572e-07
Iter: 474 loss: 7.95702931e-07
Iter: 475 loss: 7.95507958e-07
Iter: 476 loss: 7.95345954e-07
Iter: 477 loss: 7.95291953e-07
Iter: 478 loss: 7.95045821e-07
Iter: 479 loss: 7.97664768e-07
Iter: 480 loss: 7.95031497e-07
Iter: 481 loss: 7.94784796e-07
Iter: 482 loss: 7.95062306e-07
Iter: 483 loss: 7.94622224e-07
Iter: 484 loss: 7.94259e-07
Iter: 485 loss: 7.94662e-07
Iter: 486 loss: 7.94074936e-07
Iter: 487 loss: 7.93706363e-07
Iter: 488 loss: 7.93091317e-07
Iter: 489 loss: 7.93095523e-07
Iter: 490 loss: 7.92553578e-07
Iter: 491 loss: 7.92521746e-07
Iter: 492 loss: 7.92214109e-07
Iter: 493 loss: 7.91458206e-07
Iter: 494 loss: 7.95764322e-07
Iter: 495 loss: 7.91209459e-07
Iter: 496 loss: 7.9160543e-07
Iter: 497 loss: 7.90878971e-07
Iter: 498 loss: 7.90670413e-07
Iter: 499 loss: 7.90228455e-07
Iter: 500 loss: 7.9850804e-07
Iter: 501 loss: 7.90227773e-07
Iter: 502 loss: 7.89787464e-07
Iter: 503 loss: 7.91379421e-07
Iter: 504 loss: 7.8970271e-07
Iter: 505 loss: 7.89367164e-07
Iter: 506 loss: 7.92420224e-07
Iter: 507 loss: 7.89361366e-07
Iter: 508 loss: 7.89115575e-07
Iter: 509 loss: 7.89179e-07
Iter: 510 loss: 7.88915827e-07
Iter: 511 loss: 7.88572606e-07
Iter: 512 loss: 7.89365743e-07
Iter: 513 loss: 7.88438967e-07
Iter: 514 loss: 7.8795631e-07
Iter: 515 loss: 7.9093013e-07
Iter: 516 loss: 7.87924819e-07
Iter: 517 loss: 7.87609792e-07
Iter: 518 loss: 7.88646787e-07
Iter: 519 loss: 7.87519639e-07
Iter: 520 loss: 7.87258728e-07
Iter: 521 loss: 7.867759e-07
Iter: 522 loss: 7.867676e-07
Iter: 523 loss: 7.86665339e-07
Iter: 524 loss: 7.86543865e-07
Iter: 525 loss: 7.8631615e-07
Iter: 526 loss: 7.85656255e-07
Iter: 527 loss: 7.88179875e-07
Iter: 528 loss: 7.85378234e-07
Iter: 529 loss: 7.84678e-07
Iter: 530 loss: 7.93648326e-07
Iter: 531 loss: 7.84693952e-07
Iter: 532 loss: 7.83885412e-07
Iter: 533 loss: 7.85271141e-07
Iter: 534 loss: 7.83542419e-07
Iter: 535 loss: 7.83063854e-07
Iter: 536 loss: 7.82996835e-07
Iter: 537 loss: 7.82657366e-07
Iter: 538 loss: 7.82237805e-07
Iter: 539 loss: 7.82226834e-07
Iter: 540 loss: 7.81950916e-07
Iter: 541 loss: 7.81984e-07
Iter: 542 loss: 7.81759468e-07
Iter: 543 loss: 7.8147616e-07
Iter: 544 loss: 7.81809831e-07
Iter: 545 loss: 7.81316317e-07
Iter: 546 loss: 7.81071208e-07
Iter: 547 loss: 7.81057565e-07
Iter: 548 loss: 7.80863672e-07
Iter: 549 loss: 7.80726225e-07
Iter: 550 loss: 7.80672622e-07
Iter: 551 loss: 7.80223218e-07
Iter: 552 loss: 7.79705715e-07
Iter: 553 loss: 7.79640231e-07
Iter: 554 loss: 7.7915729e-07
Iter: 555 loss: 7.85013185e-07
Iter: 556 loss: 7.79123127e-07
Iter: 557 loss: 7.78487447e-07
Iter: 558 loss: 7.7800604e-07
Iter: 559 loss: 7.77787704e-07
Iter: 560 loss: 7.77129287e-07
Iter: 561 loss: 7.77405035e-07
Iter: 562 loss: 7.76627815e-07
Iter: 563 loss: 7.7645592e-07
Iter: 564 loss: 7.76185857e-07
Iter: 565 loss: 7.76031925e-07
Iter: 566 loss: 7.75557396e-07
Iter: 567 loss: 7.79266031e-07
Iter: 568 loss: 7.75477474e-07
Iter: 569 loss: 7.75098897e-07
Iter: 570 loss: 7.75097135e-07
Iter: 571 loss: 7.74806495e-07
Iter: 572 loss: 7.75141245e-07
Iter: 573 loss: 7.74661e-07
Iter: 574 loss: 7.74356181e-07
Iter: 575 loss: 7.74104706e-07
Iter: 576 loss: 7.74014609e-07
Iter: 577 loss: 7.73744773e-07
Iter: 578 loss: 7.7373403e-07
Iter: 579 loss: 7.73400188e-07
Iter: 580 loss: 7.736611e-07
Iter: 581 loss: 7.73191346e-07
Iter: 582 loss: 7.72851706e-07
Iter: 583 loss: 7.7419844e-07
Iter: 584 loss: 7.7278537e-07
Iter: 585 loss: 7.72545377e-07
Iter: 586 loss: 7.72213809e-07
Iter: 587 loss: 7.72201417e-07
Iter: 588 loss: 7.71793566e-07
Iter: 589 loss: 7.71771681e-07
Iter: 590 loss: 7.71486498e-07
Iter: 591 loss: 7.7061668e-07
Iter: 592 loss: 7.73940542e-07
Iter: 593 loss: 7.70210136e-07
Iter: 594 loss: 7.69512098e-07
Iter: 595 loss: 7.69483449e-07
Iter: 596 loss: 7.68712539e-07
Iter: 597 loss: 7.71676127e-07
Iter: 598 loss: 7.68487894e-07
Iter: 599 loss: 7.68231075e-07
Iter: 600 loss: 7.68557811e-07
Iter: 601 loss: 7.6810278e-07
Iter: 602 loss: 7.67762799e-07
Iter: 603 loss: 7.71587e-07
Iter: 604 loss: 7.67757342e-07
Iter: 605 loss: 7.67573624e-07
Iter: 606 loss: 7.67614097e-07
Iter: 607 loss: 7.6746494e-07
Iter: 608 loss: 7.67216306e-07
Iter: 609 loss: 7.67066354e-07
Iter: 610 loss: 7.66971084e-07
Iter: 611 loss: 7.6660217e-07
Iter: 612 loss: 7.66605353e-07
Iter: 613 loss: 7.66323637e-07
Iter: 614 loss: 7.6586e-07
Iter: 615 loss: 7.65861842e-07
Iter: 616 loss: 7.65075811e-07
Iter: 617 loss: 7.65777031e-07
Iter: 618 loss: 7.64617e-07
Iter: 619 loss: 7.64064339e-07
Iter: 620 loss: 7.70649308e-07
Iter: 621 loss: 7.64083438e-07
Iter: 622 loss: 7.63470609e-07
Iter: 623 loss: 7.63210323e-07
Iter: 624 loss: 7.62914055e-07
Iter: 625 loss: 7.6243532e-07
Iter: 626 loss: 7.61905824e-07
Iter: 627 loss: 7.61845058e-07
Iter: 628 loss: 7.62035654e-07
Iter: 629 loss: 7.61560045e-07
Iter: 630 loss: 7.6134836e-07
Iter: 631 loss: 7.60849844e-07
Iter: 632 loss: 7.68654672e-07
Iter: 633 loss: 7.6083154e-07
Iter: 634 loss: 7.60499688e-07
Iter: 635 loss: 7.64524714e-07
Iter: 636 loss: 7.60493322e-07
Iter: 637 loss: 7.6019677e-07
Iter: 638 loss: 7.60534135e-07
Iter: 639 loss: 7.60064211e-07
Iter: 640 loss: 7.59806369e-07
Iter: 641 loss: 7.60060857e-07
Iter: 642 loss: 7.59695808e-07
Iter: 643 loss: 7.59375212e-07
Iter: 644 loss: 7.5931024e-07
Iter: 645 loss: 7.59096508e-07
Iter: 646 loss: 7.5856218e-07
Iter: 647 loss: 7.64671199e-07
Iter: 648 loss: 7.58542342e-07
Iter: 649 loss: 7.58301837e-07
Iter: 650 loss: 7.58054284e-07
Iter: 651 loss: 7.58024953e-07
Iter: 652 loss: 7.57374096e-07
Iter: 653 loss: 7.56573513e-07
Iter: 654 loss: 7.56508655e-07
Iter: 655 loss: 7.5611041e-07
Iter: 656 loss: 7.55961651e-07
Iter: 657 loss: 7.55491e-07
Iter: 658 loss: 7.54846e-07
Iter: 659 loss: 7.54811936e-07
Iter: 660 loss: 7.54406472e-07
Iter: 661 loss: 7.55891051e-07
Iter: 662 loss: 7.54304494e-07
Iter: 663 loss: 7.54118901e-07
Iter: 664 loss: 7.54072573e-07
Iter: 665 loss: 7.53970426e-07
Iter: 666 loss: 7.53672566e-07
Iter: 667 loss: 7.55264693e-07
Iter: 668 loss: 7.53574113e-07
Iter: 669 loss: 7.53405516e-07
Iter: 670 loss: 7.53331619e-07
Iter: 671 loss: 7.53132497e-07
Iter: 672 loss: 7.52791095e-07
Iter: 673 loss: 7.61162482e-07
Iter: 674 loss: 7.52795e-07
Iter: 675 loss: 7.52295705e-07
Iter: 676 loss: 7.51808045e-07
Iter: 677 loss: 7.51730113e-07
Iter: 678 loss: 7.51354207e-07
Iter: 679 loss: 7.51348068e-07
Iter: 680 loss: 7.50890592e-07
Iter: 681 loss: 7.50950448e-07
Iter: 682 loss: 7.50595291e-07
Iter: 683 loss: 7.5021444e-07
Iter: 684 loss: 7.49576657e-07
Iter: 685 loss: 7.49595074e-07
Iter: 686 loss: 7.48871059e-07
Iter: 687 loss: 7.58815077e-07
Iter: 688 loss: 7.48853438e-07
Iter: 689 loss: 7.48461787e-07
Iter: 690 loss: 7.49337062e-07
Iter: 691 loss: 7.48288073e-07
Iter: 692 loss: 7.47992431e-07
Iter: 693 loss: 7.4800289e-07
Iter: 694 loss: 7.47870502e-07
Iter: 695 loss: 7.47638467e-07
Iter: 696 loss: 7.50928166e-07
Iter: 697 loss: 7.47624142e-07
Iter: 698 loss: 7.47435536e-07
Iter: 699 loss: 7.4743491e-07
Iter: 700 loss: 7.47270406e-07
Iter: 701 loss: 7.48191553e-07
Iter: 702 loss: 7.47272054e-07
Iter: 703 loss: 7.47136767e-07
Iter: 704 loss: 7.46956232e-07
Iter: 705 loss: 7.46968453e-07
Iter: 706 loss: 7.46743069e-07
Iter: 707 loss: 7.46444755e-07
Iter: 708 loss: 7.46418891e-07
Iter: 709 loss: 7.46158264e-07
Iter: 710 loss: 7.46149624e-07
Iter: 711 loss: 7.45909915e-07
Iter: 712 loss: 7.46138e-07
Iter: 713 loss: 7.45771445e-07
Iter: 714 loss: 7.45539296e-07
Iter: 715 loss: 7.45148554e-07
Iter: 716 loss: 7.45144803e-07
Iter: 717 loss: 7.44801241e-07
Iter: 718 loss: 7.44815e-07
Iter: 719 loss: 7.44483543e-07
Iter: 720 loss: 7.44384749e-07
Iter: 721 loss: 7.44203874e-07
Iter: 722 loss: 7.44009412e-07
Iter: 723 loss: 7.43982469e-07
Iter: 724 loss: 7.43800399e-07
Iter: 725 loss: 7.43286762e-07
Iter: 726 loss: 7.46750231e-07
Iter: 727 loss: 7.43167334e-07
Iter: 728 loss: 7.42685188e-07
Iter: 729 loss: 7.45342049e-07
Iter: 730 loss: 7.42601401e-07
Iter: 731 loss: 7.4213068e-07
Iter: 732 loss: 7.4632726e-07
Iter: 733 loss: 7.42109421e-07
Iter: 734 loss: 7.41905581e-07
Iter: 735 loss: 7.4291836e-07
Iter: 736 loss: 7.41870736e-07
Iter: 737 loss: 7.4167167e-07
Iter: 738 loss: 7.41536155e-07
Iter: 739 loss: 7.41463339e-07
Iter: 740 loss: 7.41196e-07
Iter: 741 loss: 7.41251711e-07
Iter: 742 loss: 7.41000804e-07
Iter: 743 loss: 7.40594032e-07
Iter: 744 loss: 7.46805597e-07
Iter: 745 loss: 7.40581527e-07
Iter: 746 loss: 7.40415658e-07
Iter: 747 loss: 7.40151052e-07
Iter: 748 loss: 7.40142696e-07
Iter: 749 loss: 7.39959546e-07
Iter: 750 loss: 7.4123875e-07
Iter: 751 loss: 7.39920438e-07
Iter: 752 loss: 7.39665552e-07
Iter: 753 loss: 7.39788447e-07
Iter: 754 loss: 7.39513894e-07
Iter: 755 loss: 7.39366897e-07
Iter: 756 loss: 7.41441454e-07
Iter: 757 loss: 7.39377413e-07
Iter: 758 loss: 7.39184088e-07
Iter: 759 loss: 7.39317898e-07
Iter: 760 loss: 7.39073357e-07
Iter: 761 loss: 7.38947904e-07
Iter: 762 loss: 7.38683866e-07
Iter: 763 loss: 7.44027318e-07
Iter: 764 loss: 7.3869262e-07
Iter: 765 loss: 7.38419715e-07
Iter: 766 loss: 7.39557322e-07
Iter: 767 loss: 7.38375093e-07
Iter: 768 loss: 7.37966616e-07
Iter: 769 loss: 7.40077553e-07
Iter: 770 loss: 7.37899427e-07
Iter: 771 loss: 7.37664777e-07
Iter: 772 loss: 7.38035055e-07
Iter: 773 loss: 7.37573657e-07
Iter: 774 loss: 7.37137611e-07
Iter: 775 loss: 7.36394327e-07
Iter: 776 loss: 7.52794335e-07
Iter: 777 loss: 7.36399443e-07
Iter: 778 loss: 7.36093057e-07
Iter: 779 loss: 7.36018251e-07
Iter: 780 loss: 7.35640469e-07
Iter: 781 loss: 7.35496315e-07
Iter: 782 loss: 7.35300205e-07
Iter: 783 loss: 7.3505413e-07
Iter: 784 loss: 7.35801109e-07
Iter: 785 loss: 7.34980631e-07
Iter: 786 loss: 7.34722107e-07
Iter: 787 loss: 7.35701917e-07
Iter: 788 loss: 7.34691355e-07
Iter: 789 loss: 7.34488594e-07
Iter: 790 loss: 7.34537593e-07
Iter: 791 loss: 7.34359162e-07
Iter: 792 loss: 7.34247237e-07
Iter: 793 loss: 7.34222112e-07
Iter: 794 loss: 7.34116895e-07
Iter: 795 loss: 7.33816648e-07
Iter: 796 loss: 7.34706077e-07
Iter: 797 loss: 7.3366e-07
Iter: 798 loss: 7.33195179e-07
Iter: 799 loss: 7.34105697e-07
Iter: 800 loss: 7.32979913e-07
Iter: 801 loss: 7.32940634e-07
Iter: 802 loss: 7.32778744e-07
Iter: 803 loss: 7.32595254e-07
Iter: 804 loss: 7.32368903e-07
Iter: 805 loss: 7.32344688e-07
Iter: 806 loss: 7.32073659e-07
Iter: 807 loss: 7.35064077e-07
Iter: 808 loss: 7.32062e-07
Iter: 809 loss: 7.31869e-07
Iter: 810 loss: 7.31502155e-07
Iter: 811 loss: 7.40281621e-07
Iter: 812 loss: 7.31494538e-07
Iter: 813 loss: 7.3106969e-07
Iter: 814 loss: 7.37587357e-07
Iter: 815 loss: 7.31058094e-07
Iter: 816 loss: 7.30881936e-07
Iter: 817 loss: 7.30527063e-07
Iter: 818 loss: 7.37747712e-07
Iter: 819 loss: 7.30536613e-07
Iter: 820 loss: 7.30221814e-07
Iter: 821 loss: 7.34459832e-07
Iter: 822 loss: 7.30214651e-07
Iter: 823 loss: 7.29980911e-07
Iter: 824 loss: 7.29939302e-07
Iter: 825 loss: 7.29786052e-07
Iter: 826 loss: 7.29561407e-07
Iter: 827 loss: 7.2960421e-07
Iter: 828 loss: 7.29380758e-07
Iter: 829 loss: 7.29078408e-07
Iter: 830 loss: 7.29273438e-07
Iter: 831 loss: 7.28852513e-07
Iter: 832 loss: 7.28807947e-07
Iter: 833 loss: 7.28629402e-07
Iter: 834 loss: 7.28450175e-07
Iter: 835 loss: 7.2807353e-07
Iter: 836 loss: 7.36068046e-07
Iter: 837 loss: 7.28089731e-07
Iter: 838 loss: 7.27922725e-07
Iter: 839 loss: 7.27883844e-07
Iter: 840 loss: 7.27682391e-07
Iter: 841 loss: 7.27338374e-07
Iter: 842 loss: 7.27340307e-07
Iter: 843 loss: 7.26979749e-07
Iter: 844 loss: 7.27119755e-07
Iter: 845 loss: 7.26725204e-07
Iter: 846 loss: 7.26480948e-07
Iter: 847 loss: 7.26462304e-07
Iter: 848 loss: 7.26227313e-07
Iter: 849 loss: 7.2598624e-07
Iter: 850 loss: 7.25934228e-07
Iter: 851 loss: 7.25657515e-07
Iter: 852 loss: 7.25629093e-07
Iter: 853 loss: 7.25465668e-07
Iter: 854 loss: 7.25552525e-07
Iter: 855 loss: 7.25350674e-07
Iter: 856 loss: 7.25161442e-07
Iter: 857 loss: 7.25853113e-07
Iter: 858 loss: 7.25132622e-07
Iter: 859 loss: 7.25038035e-07
Iter: 860 loss: 7.2485841e-07
Iter: 861 loss: 7.28708073e-07
Iter: 862 loss: 7.24839538e-07
Iter: 863 loss: 7.24543725e-07
Iter: 864 loss: 7.24569645e-07
Iter: 865 loss: 7.24323058e-07
Iter: 866 loss: 7.24306574e-07
Iter: 867 loss: 7.24115353e-07
Iter: 868 loss: 7.23939365e-07
Iter: 869 loss: 7.23451421e-07
Iter: 870 loss: 7.25276493e-07
Iter: 871 loss: 7.2321626e-07
Iter: 872 loss: 7.23160781e-07
Iter: 873 loss: 7.22965524e-07
Iter: 874 loss: 7.22708e-07
Iter: 875 loss: 7.22382538e-07
Iter: 876 loss: 7.22364121e-07
Iter: 877 loss: 7.22078767e-07
Iter: 878 loss: 7.22208426e-07
Iter: 879 loss: 7.21889364e-07
Iter: 880 loss: 7.21649371e-07
Iter: 881 loss: 7.21652441e-07
Iter: 882 loss: 7.21388801e-07
Iter: 883 loss: 7.21292452e-07
Iter: 884 loss: 7.21139088e-07
Iter: 885 loss: 7.20966568e-07
Iter: 886 loss: 7.20943547e-07
Iter: 887 loss: 7.2083742e-07
Iter: 888 loss: 7.20698267e-07
Iter: 889 loss: 7.20685591e-07
Iter: 890 loss: 7.20520461e-07
Iter: 891 loss: 7.20997548e-07
Iter: 892 loss: 7.20447588e-07
Iter: 893 loss: 7.20266769e-07
Iter: 894 loss: 7.20055e-07
Iter: 895 loss: 7.20035928e-07
Iter: 896 loss: 7.19880177e-07
Iter: 897 loss: 7.19872162e-07
Iter: 898 loss: 7.19703564e-07
Iter: 899 loss: 7.19364266e-07
Iter: 900 loss: 7.25719246e-07
Iter: 901 loss: 7.19372338e-07
Iter: 902 loss: 7.19012462e-07
Iter: 903 loss: 7.19215336e-07
Iter: 904 loss: 7.18783213e-07
Iter: 905 loss: 7.18386e-07
Iter: 906 loss: 7.18382694e-07
Iter: 907 loss: 7.18202273e-07
Iter: 908 loss: 7.17912144e-07
Iter: 909 loss: 7.17905095e-07
Iter: 910 loss: 7.17791181e-07
Iter: 911 loss: 7.17777198e-07
Iter: 912 loss: 7.17690114e-07
Iter: 913 loss: 7.17780324e-07
Iter: 914 loss: 7.17630485e-07
Iter: 915 loss: 7.17539933e-07
Iter: 916 loss: 7.1767e-07
Iter: 917 loss: 7.1749173e-07
Iter: 918 loss: 7.17299656e-07
Iter: 919 loss: 7.17317334e-07
Iter: 920 loss: 7.17155444e-07
Iter: 921 loss: 7.1692773e-07
Iter: 922 loss: 7.16641921e-07
Iter: 923 loss: 7.16611112e-07
Iter: 924 loss: 7.16338036e-07
Iter: 925 loss: 7.16333943e-07
Iter: 926 loss: 7.16177965e-07
Iter: 927 loss: 7.15821898e-07
Iter: 928 loss: 7.23022e-07
Iter: 929 loss: 7.15823546e-07
Iter: 930 loss: 7.15696501e-07
Iter: 931 loss: 7.1565762e-07
Iter: 932 loss: 7.1551392e-07
Iter: 933 loss: 7.15478677e-07
Iter: 934 loss: 7.15380907e-07
Iter: 935 loss: 7.15237661e-07
Iter: 936 loss: 7.16977297e-07
Iter: 937 loss: 7.15244141e-07
Iter: 938 loss: 7.15149156e-07
Iter: 939 loss: 7.15041324e-07
Iter: 940 loss: 7.15027795e-07
Iter: 941 loss: 7.14839075e-07
Iter: 942 loss: 7.14394162e-07
Iter: 943 loss: 7.19160596e-07
Iter: 944 loss: 7.14357213e-07
Iter: 945 loss: 7.13871486e-07
Iter: 946 loss: 7.16611453e-07
Iter: 947 loss: 7.13790598e-07
Iter: 948 loss: 7.13773e-07
Iter: 949 loss: 7.1363155e-07
Iter: 950 loss: 7.13482e-07
Iter: 951 loss: 7.13968234e-07
Iter: 952 loss: 7.1341583e-07
Iter: 953 loss: 7.1331e-07
Iter: 954 loss: 7.13414693e-07
Iter: 955 loss: 7.13240581e-07
Iter: 956 loss: 7.13048507e-07
Iter: 957 loss: 7.13151451e-07
Iter: 958 loss: 7.12944029e-07
Iter: 959 loss: 7.12731321e-07
Iter: 960 loss: 7.13847896e-07
Iter: 961 loss: 7.12698522e-07
Iter: 962 loss: 7.12485075e-07
Iter: 963 loss: 7.12313522e-07
Iter: 964 loss: 7.12258213e-07
Iter: 965 loss: 7.12069209e-07
Iter: 966 loss: 7.12063752e-07
Iter: 967 loss: 7.11865596e-07
Iter: 968 loss: 7.11600933e-07
Iter: 969 loss: 7.1158064e-07
Iter: 970 loss: 7.11373559e-07
Iter: 971 loss: 7.11401128e-07
Iter: 972 loss: 7.11189102e-07
Iter: 973 loss: 7.11105145e-07
Iter: 974 loss: 7.11054781e-07
Iter: 975 loss: 7.10919494e-07
Iter: 976 loss: 7.10987706e-07
Iter: 977 loss: 7.10824338e-07
Iter: 978 loss: 7.10710424e-07
Iter: 979 loss: 7.10520453e-07
Iter: 980 loss: 7.15151145e-07
Iter: 981 loss: 7.10524318e-07
Iter: 982 loss: 7.1050556e-07
Iter: 983 loss: 7.10402219e-07
Iter: 984 loss: 7.10336906e-07
Iter: 985 loss: 7.10130507e-07
Iter: 986 loss: 7.12053065e-07
Iter: 987 loss: 7.10105724e-07
Iter: 988 loss: 7.09870164e-07
Iter: 989 loss: 7.10164613e-07
Iter: 990 loss: 7.09755341e-07
Iter: 991 loss: 7.09617098e-07
Iter: 992 loss: 7.09592712e-07
Iter: 993 loss: 7.09471294e-07
Iter: 994 loss: 7.0930912e-07
Iter: 995 loss: 7.09290418e-07
Iter: 996 loss: 7.0907555e-07
Iter: 997 loss: 7.09397682e-07
Iter: 998 loss: 7.08969765e-07
Iter: 999 loss: 7.08788207e-07
Iter: 1000 loss: 7.0878059e-07
Iter: 1001 loss: 7.08694529e-07
Iter: 1002 loss: 7.08423556e-07
Iter: 1003 loss: 7.10122038e-07
Iter: 1004 loss: 7.08353582e-07
Iter: 1005 loss: 7.08015534e-07
Iter: 1006 loss: 7.09441906e-07
Iter: 1007 loss: 7.07953348e-07
Iter: 1008 loss: 7.07845288e-07
Iter: 1009 loss: 7.0778367e-07
Iter: 1010 loss: 7.07678964e-07
Iter: 1011 loss: 7.07396509e-07
Iter: 1012 loss: 7.10117263e-07
Iter: 1013 loss: 7.07340064e-07
Iter: 1014 loss: 7.07275e-07
Iter: 1015 loss: 7.07209551e-07
Iter: 1016 loss: 7.07066761e-07
Iter: 1017 loss: 7.06845753e-07
Iter: 1018 loss: 7.06844105e-07
Iter: 1019 loss: 7.06595642e-07
Iter: 1020 loss: 7.07224103e-07
Iter: 1021 loss: 7.06527089e-07
Iter: 1022 loss: 7.0637094e-07
Iter: 1023 loss: 7.07976596e-07
Iter: 1024 loss: 7.06348828e-07
Iter: 1025 loss: 7.06192964e-07
Iter: 1026 loss: 7.06744629e-07
Iter: 1027 loss: 7.06154538e-07
Iter: 1028 loss: 7.06057904e-07
Iter: 1029 loss: 7.05944672e-07
Iter: 1030 loss: 7.0588851e-07
Iter: 1031 loss: 7.05750665e-07
Iter: 1032 loss: 7.0784921e-07
Iter: 1033 loss: 7.05762204e-07
Iter: 1034 loss: 7.05639479e-07
Iter: 1035 loss: 7.05473e-07
Iter: 1036 loss: 7.05470597e-07
Iter: 1037 loss: 7.0523015e-07
Iter: 1038 loss: 7.04913361e-07
Iter: 1039 loss: 7.04910349e-07
Iter: 1040 loss: 7.04554338e-07
Iter: 1041 loss: 7.07846e-07
Iter: 1042 loss: 7.04541094e-07
Iter: 1043 loss: 7.04363231e-07
Iter: 1044 loss: 7.04325316e-07
Iter: 1045 loss: 7.04248123e-07
Iter: 1046 loss: 7.03998808e-07
Iter: 1047 loss: 7.05721845e-07
Iter: 1048 loss: 7.03945602e-07
Iter: 1049 loss: 7.03724083e-07
Iter: 1050 loss: 7.0371118e-07
Iter: 1051 loss: 7.03626256e-07
Iter: 1052 loss: 7.03435148e-07
Iter: 1053 loss: 7.05416255e-07
Iter: 1054 loss: 7.03388423e-07
Iter: 1055 loss: 7.03234946e-07
Iter: 1056 loss: 7.05082925e-07
Iter: 1057 loss: 7.03211e-07
Iter: 1058 loss: 7.03075e-07
Iter: 1059 loss: 7.04067361e-07
Iter: 1060 loss: 7.03077831e-07
Iter: 1061 loss: 7.02939133e-07
Iter: 1062 loss: 7.02997625e-07
Iter: 1063 loss: 7.02858529e-07
Iter: 1064 loss: 7.0272165e-07
Iter: 1065 loss: 7.02906277e-07
Iter: 1066 loss: 7.02656621e-07
Iter: 1067 loss: 7.02531e-07
Iter: 1068 loss: 7.04121135e-07
Iter: 1069 loss: 7.02524801e-07
Iter: 1070 loss: 7.02444481e-07
Iter: 1071 loss: 7.02312434e-07
Iter: 1072 loss: 7.0569763e-07
Iter: 1073 loss: 7.02312548e-07
Iter: 1074 loss: 7.02163561e-07
Iter: 1075 loss: 7.02065222e-07
Iter: 1076 loss: 7.01988029e-07
Iter: 1077 loss: 7.01736951e-07
Iter: 1078 loss: 7.02687259e-07
Iter: 1079 loss: 7.01686531e-07
Iter: 1080 loss: 7.0154681e-07
Iter: 1081 loss: 7.01505314e-07
Iter: 1082 loss: 7.01406861e-07
Iter: 1083 loss: 7.01132592e-07
Iter: 1084 loss: 7.03267915e-07
Iter: 1085 loss: 7.01074214e-07
Iter: 1086 loss: 7.00846158e-07
Iter: 1087 loss: 7.00799319e-07
Iter: 1088 loss: 7.00743e-07
Iter: 1089 loss: 7.00556711e-07
Iter: 1090 loss: 7.01537317e-07
Iter: 1091 loss: 7.00497708e-07
Iter: 1092 loss: 7.00352246e-07
Iter: 1093 loss: 7.00338887e-07
Iter: 1094 loss: 7.00223154e-07
Iter: 1095 loss: 7.00149883e-07
Iter: 1096 loss: 7.00103953e-07
Iter: 1097 loss: 6.99931547e-07
Iter: 1098 loss: 7.002742e-07
Iter: 1099 loss: 6.99867655e-07
Iter: 1100 loss: 6.99786e-07
Iter: 1101 loss: 6.99765224e-07
Iter: 1102 loss: 6.99681209e-07
Iter: 1103 loss: 6.99423254e-07
Iter: 1104 loss: 7.01815907e-07
Iter: 1105 loss: 6.99422174e-07
Iter: 1106 loss: 6.99171835e-07
Iter: 1107 loss: 6.99342138e-07
Iter: 1108 loss: 6.99032967e-07
Iter: 1109 loss: 6.98762051e-07
Iter: 1110 loss: 6.99364705e-07
Iter: 1111 loss: 6.98663655e-07
Iter: 1112 loss: 6.98392682e-07
Iter: 1113 loss: 6.99466227e-07
Iter: 1114 loss: 6.98305485e-07
Iter: 1115 loss: 6.98198505e-07
Iter: 1116 loss: 6.9814962e-07
Iter: 1117 loss: 6.98076292e-07
Iter: 1118 loss: 6.97932194e-07
Iter: 1119 loss: 7.00247369e-07
Iter: 1120 loss: 6.97926055e-07
Iter: 1121 loss: 6.97861708e-07
Iter: 1122 loss: 6.97824476e-07
Iter: 1123 loss: 6.9776388e-07
Iter: 1124 loss: 6.97564587e-07
Iter: 1125 loss: 6.98507e-07
Iter: 1126 loss: 6.97483642e-07
Iter: 1127 loss: 6.97401902e-07
Iter: 1128 loss: 6.97374162e-07
Iter: 1129 loss: 6.97268092e-07
Iter: 1130 loss: 6.97238647e-07
Iter: 1131 loss: 6.97172482e-07
Iter: 1132 loss: 6.97053679e-07
Iter: 1133 loss: 6.97460564e-07
Iter: 1134 loss: 6.96997176e-07
Iter: 1135 loss: 6.96846143e-07
Iter: 1136 loss: 6.96931863e-07
Iter: 1137 loss: 6.96762868e-07
Iter: 1138 loss: 6.96587449e-07
Iter: 1139 loss: 6.96665325e-07
Iter: 1140 loss: 6.96477798e-07
Iter: 1141 loss: 6.96270604e-07
Iter: 1142 loss: 6.96016059e-07
Iter: 1143 loss: 6.9599389e-07
Iter: 1144 loss: 6.95629751e-07
Iter: 1145 loss: 6.96965401e-07
Iter: 1146 loss: 6.95576603e-07
Iter: 1147 loss: 6.95303811e-07
Iter: 1148 loss: 6.96943289e-07
Iter: 1149 loss: 6.95276754e-07
Iter: 1150 loss: 6.95228096e-07
Iter: 1151 loss: 6.95169717e-07
Iter: 1152 loss: 6.95108497e-07
Iter: 1153 loss: 6.94934499e-07
Iter: 1154 loss: 6.96203642e-07
Iter: 1155 loss: 6.94904031e-07
Iter: 1156 loss: 6.94980145e-07
Iter: 1157 loss: 6.94857476e-07
Iter: 1158 loss: 6.94824166e-07
Iter: 1159 loss: 6.94654318e-07
Iter: 1160 loss: 6.94852361e-07
Iter: 1161 loss: 6.94546372e-07
Iter: 1162 loss: 6.94391133e-07
Iter: 1163 loss: 6.94363e-07
Iter: 1164 loss: 6.94233e-07
Iter: 1165 loss: 6.94060475e-07
Iter: 1166 loss: 6.9405661e-07
Iter: 1167 loss: 6.93847596e-07
Iter: 1168 loss: 6.9420264e-07
Iter: 1169 loss: 6.93763241e-07
Iter: 1170 loss: 6.9358e-07
Iter: 1171 loss: 6.9549111e-07
Iter: 1172 loss: 6.93591801e-07
Iter: 1173 loss: 6.93398761e-07
Iter: 1174 loss: 6.93304571e-07
Iter: 1175 loss: 6.93222091e-07
Iter: 1176 loss: 6.93092431e-07
Iter: 1177 loss: 6.92864717e-07
Iter: 1178 loss: 6.92864774e-07
Iter: 1179 loss: 6.92542301e-07
Iter: 1180 loss: 6.92954586e-07
Iter: 1181 loss: 6.92408776e-07
Iter: 1182 loss: 6.92182084e-07
Iter: 1183 loss: 6.93502216e-07
Iter: 1184 loss: 6.92143544e-07
Iter: 1185 loss: 6.91978698e-07
Iter: 1186 loss: 6.93985271e-07
Iter: 1187 loss: 6.91974833e-07
Iter: 1188 loss: 6.91869445e-07
Iter: 1189 loss: 6.91963635e-07
Iter: 1190 loss: 6.91805667e-07
Iter: 1191 loss: 6.91633659e-07
Iter: 1192 loss: 6.92291337e-07
Iter: 1193 loss: 6.9158267e-07
Iter: 1194 loss: 6.91482683e-07
Iter: 1195 loss: 6.91312948e-07
Iter: 1196 loss: 6.91318405e-07
Iter: 1197 loss: 6.91147932e-07
Iter: 1198 loss: 6.91142361e-07
Iter: 1199 loss: 6.91084324e-07
Iter: 1200 loss: 6.90917e-07
Iter: 1201 loss: 6.92558956e-07
Iter: 1202 loss: 6.90883041e-07
Iter: 1203 loss: 6.90738489e-07
Iter: 1204 loss: 6.92796448e-07
Iter: 1205 loss: 6.90741103e-07
Iter: 1206 loss: 6.90603201e-07
Iter: 1207 loss: 6.91250079e-07
Iter: 1208 loss: 6.90589559e-07
Iter: 1209 loss: 6.90480874e-07
Iter: 1210 loss: 6.90485649e-07
Iter: 1211 loss: 6.90420165e-07
Iter: 1212 loss: 6.90279592e-07
Iter: 1213 loss: 6.89961325e-07
Iter: 1214 loss: 6.92381491e-07
Iter: 1215 loss: 6.89900048e-07
Iter: 1216 loss: 6.89512717e-07
Iter: 1217 loss: 6.92798835e-07
Iter: 1218 loss: 6.89496972e-07
Iter: 1219 loss: 6.89363787e-07
Iter: 1220 loss: 6.89304215e-07
Iter: 1221 loss: 6.89201499e-07
Iter: 1222 loss: 6.89150568e-07
Iter: 1223 loss: 6.89080593e-07
Iter: 1224 loss: 6.88949967e-07
Iter: 1225 loss: 6.90416e-07
Iter: 1226 loss: 6.88956163e-07
Iter: 1227 loss: 6.88833779e-07
Iter: 1228 loss: 6.88850093e-07
Iter: 1229 loss: 6.88713953e-07
Iter: 1230 loss: 6.88635282e-07
Iter: 1231 loss: 6.88996806e-07
Iter: 1232 loss: 6.88627779e-07
Iter: 1233 loss: 6.8852512e-07
Iter: 1234 loss: 6.88616581e-07
Iter: 1235 loss: 6.88466571e-07
Iter: 1236 loss: 6.88373e-07
Iter: 1237 loss: 6.88262105e-07
Iter: 1238 loss: 6.88268415e-07
Iter: 1239 loss: 6.88172236e-07
Iter: 1240 loss: 6.8815325e-07
Iter: 1241 loss: 6.88091291e-07
Iter: 1242 loss: 6.88163368e-07
Iter: 1243 loss: 6.88046612e-07
Iter: 1244 loss: 6.8793895e-07
Iter: 1245 loss: 6.87987836e-07
Iter: 1246 loss: 6.87868521e-07
Iter: 1247 loss: 6.87776776e-07
Iter: 1248 loss: 6.87542183e-07
Iter: 1249 loss: 6.90800846e-07
Iter: 1250 loss: 6.87491706e-07
Iter: 1251 loss: 6.87187821e-07
Iter: 1252 loss: 6.88357e-07
Iter: 1253 loss: 6.87101704e-07
Iter: 1254 loss: 6.86838e-07
Iter: 1255 loss: 6.88781313e-07
Iter: 1256 loss: 6.86821863e-07
Iter: 1257 loss: 6.86674639e-07
Iter: 1258 loss: 6.86671797e-07
Iter: 1259 loss: 6.86541796e-07
Iter: 1260 loss: 6.87152237e-07
Iter: 1261 loss: 6.86518206e-07
Iter: 1262 loss: 6.8643044e-07
Iter: 1263 loss: 6.86787871e-07
Iter: 1264 loss: 6.8642612e-07
Iter: 1265 loss: 6.86338467e-07
Iter: 1266 loss: 6.86178282e-07
Iter: 1267 loss: 6.88674675e-07
Iter: 1268 loss: 6.86160661e-07
Iter: 1269 loss: 6.8603174e-07
Iter: 1270 loss: 6.87511601e-07
Iter: 1271 loss: 6.86048907e-07
Iter: 1272 loss: 6.85909185e-07
Iter: 1273 loss: 6.86275143e-07
Iter: 1274 loss: 6.85874284e-07
Iter: 1275 loss: 6.85787427e-07
Iter: 1276 loss: 6.85627754e-07
Iter: 1277 loss: 6.89100432e-07
Iter: 1278 loss: 6.85627697e-07
Iter: 1279 loss: 6.85468365e-07
Iter: 1280 loss: 6.8547115e-07
Iter: 1281 loss: 6.85363489e-07
Iter: 1282 loss: 6.85612463e-07
Iter: 1283 loss: 6.8532438e-07
Iter: 1284 loss: 6.85192958e-07
Iter: 1285 loss: 6.85194152e-07
Iter: 1286 loss: 6.8507768e-07
Iter: 1287 loss: 6.8492875e-07
Iter: 1288 loss: 6.84654879e-07
Iter: 1289 loss: 6.91351886e-07
Iter: 1290 loss: 6.84661131e-07
Iter: 1291 loss: 6.84352e-07
Iter: 1292 loss: 6.84970928e-07
Iter: 1293 loss: 6.8425004e-07
Iter: 1294 loss: 6.84538918e-07
Iter: 1295 loss: 6.84153918e-07
Iter: 1296 loss: 6.84120096e-07
Iter: 1297 loss: 6.84081897e-07
Iter: 1298 loss: 6.8404978e-07
Iter: 1299 loss: 6.8398549e-07
Iter: 1300 loss: 6.84594625e-07
Iter: 1301 loss: 6.83982194e-07
Iter: 1302 loss: 6.83924554e-07
Iter: 1303 loss: 6.83823941e-07
Iter: 1304 loss: 6.84835186e-07
Iter: 1305 loss: 6.83807343e-07
Iter: 1306 loss: 6.83766189e-07
Iter: 1307 loss: 6.8374095e-07
Iter: 1308 loss: 6.8366046e-07
Iter: 1309 loss: 6.83540748e-07
Iter: 1310 loss: 6.83537905e-07
Iter: 1311 loss: 6.83408302e-07
Iter: 1312 loss: 6.83153871e-07
Iter: 1313 loss: 6.83152223e-07
Iter: 1314 loss: 6.83025235e-07
Iter: 1315 loss: 6.83005055e-07
Iter: 1316 loss: 6.82878863e-07
Iter: 1317 loss: 6.82826908e-07
Iter: 1318 loss: 6.82772907e-07
Iter: 1319 loss: 6.82622101e-07
Iter: 1320 loss: 6.82645918e-07
Iter: 1321 loss: 6.82502332e-07
Iter: 1322 loss: 6.82368579e-07
Iter: 1323 loss: 6.84264421e-07
Iter: 1324 loss: 6.82384098e-07
Iter: 1325 loss: 6.82283712e-07
Iter: 1326 loss: 6.82353516e-07
Iter: 1327 loss: 6.82192592e-07
Iter: 1328 loss: 6.82202e-07
Iter: 1329 loss: 6.8215661e-07
Iter: 1330 loss: 6.82119548e-07
Iter: 1331 loss: 6.82041104e-07
Iter: 1332 loss: 6.82732832e-07
Iter: 1333 loss: 6.82007112e-07
Iter: 1334 loss: 6.81930032e-07
Iter: 1335 loss: 6.82211748e-07
Iter: 1336 loss: 6.81914457e-07
Iter: 1337 loss: 6.81851589e-07
Iter: 1338 loss: 6.82619429e-07
Iter: 1339 loss: 6.81850963e-07
Iter: 1340 loss: 6.81781898e-07
Iter: 1341 loss: 6.81652239e-07
Iter: 1342 loss: 6.83680696e-07
Iter: 1343 loss: 6.81663e-07
Iter: 1344 loss: 6.81469771e-07
Iter: 1345 loss: 6.82283371e-07
Iter: 1346 loss: 6.81433448e-07
Iter: 1347 loss: 6.81314646e-07
Iter: 1348 loss: 6.81164693e-07
Iter: 1349 loss: 6.81134452e-07
Iter: 1350 loss: 6.80935386e-07
Iter: 1351 loss: 6.84076326e-07
Iter: 1352 loss: 6.80944765e-07
Iter: 1353 loss: 6.80809535e-07
Iter: 1354 loss: 6.80630592e-07
Iter: 1355 loss: 6.80630762e-07
Iter: 1356 loss: 6.80433232e-07
Iter: 1357 loss: 6.8094073e-07
Iter: 1358 loss: 6.80391054e-07
Iter: 1359 loss: 6.80223138e-07
Iter: 1360 loss: 6.8038662e-07
Iter: 1361 loss: 6.80150322e-07
Iter: 1362 loss: 6.80016171e-07
Iter: 1363 loss: 6.8112405e-07
Iter: 1364 loss: 6.79996333e-07
Iter: 1365 loss: 6.79852178e-07
Iter: 1366 loss: 6.80259518e-07
Iter: 1367 loss: 6.79804e-07
Iter: 1368 loss: 6.79708251e-07
Iter: 1369 loss: 6.79513107e-07
Iter: 1370 loss: 6.83235214e-07
Iter: 1371 loss: 6.79505433e-07
Iter: 1372 loss: 6.79486334e-07
Iter: 1373 loss: 6.79440859e-07
Iter: 1374 loss: 6.79363552e-07
Iter: 1375 loss: 6.79227355e-07
Iter: 1376 loss: 6.79230084e-07
Iter: 1377 loss: 6.79112532e-07
Iter: 1378 loss: 6.80864673e-07
Iter: 1379 loss: 6.7910355e-07
Iter: 1380 loss: 6.78994866e-07
Iter: 1381 loss: 6.7894041e-07
Iter: 1382 loss: 6.7888061e-07
Iter: 1383 loss: 6.78779941e-07
Iter: 1384 loss: 6.79305e-07
Iter: 1385 loss: 6.78759193e-07
Iter: 1386 loss: 6.78597075e-07
Iter: 1387 loss: 6.78419894e-07
Iter: 1388 loss: 6.78409606e-07
Iter: 1389 loss: 6.78266474e-07
Iter: 1390 loss: 6.80659241e-07
Iter: 1391 loss: 6.78263746e-07
Iter: 1392 loss: 6.78182801e-07
Iter: 1393 loss: 6.78194e-07
Iter: 1394 loss: 6.7810555e-07
Iter: 1395 loss: 6.78033871e-07
Iter: 1396 loss: 6.7810322e-07
Iter: 1397 loss: 6.77989419e-07
Iter: 1398 loss: 6.77983053e-07
Iter: 1399 loss: 6.77915864e-07
Iter: 1400 loss: 6.77882213e-07
Iter: 1401 loss: 6.77789e-07
Iter: 1402 loss: 6.7847e-07
Iter: 1403 loss: 6.77762841e-07
Iter: 1404 loss: 6.77657454e-07
Iter: 1405 loss: 6.78208266e-07
Iter: 1406 loss: 6.77627156e-07
Iter: 1407 loss: 6.77485104e-07
Iter: 1408 loss: 6.77578441e-07
Iter: 1409 loss: 6.77405751e-07
Iter: 1410 loss: 6.77226126e-07
Iter: 1411 loss: 6.77595267e-07
Iter: 1412 loss: 6.77166781e-07
Iter: 1413 loss: 6.76956915e-07
Iter: 1414 loss: 6.78234755e-07
Iter: 1415 loss: 6.76931563e-07
Iter: 1416 loss: 6.76811055e-07
Iter: 1417 loss: 6.76753189e-07
Iter: 1418 loss: 6.76697141e-07
Iter: 1419 loss: 6.76505806e-07
Iter: 1420 loss: 6.77733681e-07
Iter: 1421 loss: 6.76486252e-07
Iter: 1422 loss: 6.76344882e-07
Iter: 1423 loss: 6.76374839e-07
Iter: 1424 loss: 6.7624e-07
Iter: 1425 loss: 6.76045715e-07
Iter: 1426 loss: 6.76465e-07
Iter: 1427 loss: 6.75964316e-07
Iter: 1428 loss: 6.75845683e-07
Iter: 1429 loss: 6.75787646e-07
Iter: 1430 loss: 6.75666058e-07
Iter: 1431 loss: 6.75577496e-07
Iter: 1432 loss: 6.77272283e-07
Iter: 1433 loss: 6.75565786e-07
Iter: 1434 loss: 6.7542851e-07
Iter: 1435 loss: 6.76216473e-07
Iter: 1436 loss: 6.75409183e-07
Iter: 1437 loss: 6.75362799e-07
Iter: 1438 loss: 6.75243e-07
Iter: 1439 loss: 6.76002401e-07
Iter: 1440 loss: 6.75210629e-07
Iter: 1441 loss: 6.75097112e-07
Iter: 1442 loss: 6.75101546e-07
Iter: 1443 loss: 6.74937155e-07
Iter: 1444 loss: 6.74629e-07
Iter: 1445 loss: 6.8025679e-07
Iter: 1446 loss: 6.74639921e-07
Iter: 1447 loss: 6.74583816e-07
Iter: 1448 loss: 6.74521743e-07
Iter: 1449 loss: 6.74391742e-07
Iter: 1450 loss: 6.74201885e-07
Iter: 1451 loss: 6.74207399e-07
Iter: 1452 loss: 6.74071771e-07
Iter: 1453 loss: 6.74392e-07
Iter: 1454 loss: 6.74002592e-07
Iter: 1455 loss: 6.74063813e-07
Iter: 1456 loss: 6.73958425e-07
Iter: 1457 loss: 6.73922727e-07
Iter: 1458 loss: 6.73898739e-07
Iter: 1459 loss: 6.73890213e-07
Iter: 1460 loss: 6.7383337e-07
Iter: 1461 loss: 6.73917384e-07
Iter: 1462 loss: 6.7381427e-07
Iter: 1463 loss: 6.73737304e-07
Iter: 1464 loss: 6.73878105e-07
Iter: 1465 loss: 6.73712066e-07
Iter: 1466 loss: 6.73673867e-07
Iter: 1467 loss: 6.73662896e-07
Iter: 1468 loss: 6.73589966e-07
Iter: 1469 loss: 6.73588033e-07
Iter: 1470 loss: 6.73563591e-07
Iter: 1471 loss: 6.73523232e-07
Iter: 1472 loss: 6.73434101e-07
Iter: 1473 loss: 6.752324e-07
Iter: 1474 loss: 6.73417333e-07
Iter: 1475 loss: 6.73355544e-07
Iter: 1476 loss: 6.73353554e-07
Iter: 1477 loss: 6.73289549e-07
Iter: 1478 loss: 6.73067234e-07
Iter: 1479 loss: 6.73862473e-07
Iter: 1480 loss: 6.72998908e-07
Iter: 1481 loss: 6.72683541e-07
Iter: 1482 loss: 6.72980605e-07
Iter: 1483 loss: 6.72493854e-07
Iter: 1484 loss: 6.72490387e-07
Iter: 1485 loss: 6.723306e-07
Iter: 1486 loss: 6.72225269e-07
Iter: 1487 loss: 6.72041438e-07
Iter: 1488 loss: 6.75198862e-07
Iter: 1489 loss: 6.72026772e-07
Iter: 1490 loss: 6.72074634e-07
Iter: 1491 loss: 6.71980104e-07
Iter: 1492 loss: 6.71934e-07
Iter: 1493 loss: 6.71832e-07
Iter: 1494 loss: 6.72711565e-07
Iter: 1495 loss: 6.71818839e-07
Iter: 1496 loss: 6.71741077e-07
Iter: 1497 loss: 6.71759494e-07
Iter: 1498 loss: 6.71659848e-07
Iter: 1499 loss: 6.71512055e-07
Iter: 1500 loss: 6.72378349e-07
Iter: 1501 loss: 6.71491534e-07
Iter: 1502 loss: 6.71422868e-07
Iter: 1503 loss: 6.71509952e-07
Iter: 1504 loss: 6.71373698e-07
Iter: 1505 loss: 6.71257339e-07
Iter: 1506 loss: 6.72085e-07
Iter: 1507 loss: 6.71239093e-07
Iter: 1508 loss: 6.71207431e-07
Iter: 1509 loss: 6.71139674e-07
Iter: 1510 loss: 6.71129555e-07
Iter: 1511 loss: 6.71032e-07
Iter: 1512 loss: 6.7165081e-07
Iter: 1513 loss: 6.71012742e-07
Iter: 1514 loss: 6.70944416e-07
Iter: 1515 loss: 6.70876091e-07
Iter: 1516 loss: 6.70871145e-07
Iter: 1517 loss: 6.70787e-07
Iter: 1518 loss: 6.7079975e-07
Iter: 1519 loss: 6.70739212e-07
Iter: 1520 loss: 6.70599775e-07
Iter: 1521 loss: 6.71610508e-07
Iter: 1522 loss: 6.70564873e-07
Iter: 1523 loss: 6.7054259e-07
Iter: 1524 loss: 6.70489385e-07
Iter: 1525 loss: 6.70443e-07
Iter: 1526 loss: 6.70408781e-07
Iter: 1527 loss: 6.70383429e-07
Iter: 1528 loss: 6.70317775e-07
Iter: 1529 loss: 6.70256838e-07
Iter: 1530 loss: 6.70234613e-07
Iter: 1531 loss: 6.70144345e-07
Iter: 1532 loss: 6.70956751e-07
Iter: 1533 loss: 6.70140253e-07
Iter: 1534 loss: 6.70075565e-07
Iter: 1535 loss: 6.70233248e-07
Iter: 1536 loss: 6.70064878e-07
Iter: 1537 loss: 6.69958354e-07
Iter: 1538 loss: 6.6996347e-07
Iter: 1539 loss: 6.69871611e-07
Iter: 1540 loss: 6.69709607e-07
Iter: 1541 loss: 6.73534942e-07
Iter: 1542 loss: 6.697054e-07
Iter: 1543 loss: 6.69559938e-07
Iter: 1544 loss: 6.69547831e-07
Iter: 1545 loss: 6.69435394e-07
Iter: 1546 loss: 6.69242809e-07
Iter: 1547 loss: 6.69246219e-07
Iter: 1548 loss: 6.69180508e-07
Iter: 1549 loss: 6.69201711e-07
Iter: 1550 loss: 6.69129292e-07
Iter: 1551 loss: 6.69043743e-07
Iter: 1552 loss: 6.69948236e-07
Iter: 1553 loss: 6.69030953e-07
Iter: 1554 loss: 6.68995767e-07
Iter: 1555 loss: 6.68925168e-07
Iter: 1556 loss: 6.68925509e-07
Iter: 1557 loss: 6.68903908e-07
Iter: 1558 loss: 6.68870939e-07
Iter: 1559 loss: 6.68848941e-07
Iter: 1560 loss: 6.68800681e-07
Iter: 1561 loss: 6.69352971e-07
Iter: 1562 loss: 6.68783855e-07
Iter: 1563 loss: 6.68713596e-07
Iter: 1564 loss: 6.68839448e-07
Iter: 1565 loss: 6.68679149e-07
Iter: 1566 loss: 6.68626853e-07
Iter: 1567 loss: 6.68551138e-07
Iter: 1568 loss: 6.68550683e-07
Iter: 1569 loss: 6.68431198e-07
Iter: 1570 loss: 6.69623887e-07
Iter: 1571 loss: 6.68435689e-07
Iter: 1572 loss: 6.68402777e-07
Iter: 1573 loss: 6.68307166e-07
Iter: 1574 loss: 6.69488372e-07
Iter: 1575 loss: 6.68296479e-07
Iter: 1576 loss: 6.68138455e-07
Iter: 1577 loss: 6.67898746e-07
Iter: 1578 loss: 6.67883114e-07
Iter: 1579 loss: 6.67708491e-07
Iter: 1580 loss: 6.67736231e-07
Iter: 1581 loss: 6.67635959e-07
Iter: 1582 loss: 6.67509312e-07
Iter: 1583 loss: 6.67490326e-07
Iter: 1584 loss: 6.67407335e-07
Iter: 1585 loss: 6.67564e-07
Iter: 1586 loss: 6.67361178e-07
Iter: 1587 loss: 6.67290465e-07
Iter: 1588 loss: 6.67274378e-07
Iter: 1589 loss: 6.67257723e-07
Iter: 1590 loss: 6.67177e-07
Iter: 1591 loss: 6.67171435e-07
Iter: 1592 loss: 6.67148754e-07
Iter: 1593 loss: 6.6708958e-07
Iter: 1594 loss: 6.67091058e-07
Iter: 1595 loss: 6.66980213e-07
Iter: 1596 loss: 6.67000904e-07
Iter: 1597 loss: 6.66905578e-07
Iter: 1598 loss: 6.66752783e-07
Iter: 1599 loss: 6.66823951e-07
Iter: 1600 loss: 6.66677238e-07
Iter: 1601 loss: 6.66607036e-07
Iter: 1602 loss: 6.66570259e-07
Iter: 1603 loss: 6.66509e-07
Iter: 1604 loss: 6.66480787e-07
Iter: 1605 loss: 6.66441792e-07
Iter: 1606 loss: 6.66342544e-07
Iter: 1607 loss: 6.66579297e-07
Iter: 1608 loss: 6.66278709e-07
Iter: 1609 loss: 6.66229312e-07
Iter: 1610 loss: 6.66455549e-07
Iter: 1611 loss: 6.66214078e-07
Iter: 1612 loss: 6.66157291e-07
Iter: 1613 loss: 6.66058554e-07
Iter: 1614 loss: 6.66077199e-07
Iter: 1615 loss: 6.65960783e-07
Iter: 1616 loss: 6.66004837e-07
Iter: 1617 loss: 6.65906839e-07
Iter: 1618 loss: 6.6581174e-07
Iter: 1619 loss: 6.65808e-07
Iter: 1620 loss: 6.65729601e-07
Iter: 1621 loss: 6.66199867e-07
Iter: 1622 loss: 6.65736252e-07
Iter: 1623 loss: 6.65668381e-07
Iter: 1624 loss: 6.65759785e-07
Iter: 1625 loss: 6.6564553e-07
Iter: 1626 loss: 6.65575044e-07
Iter: 1627 loss: 6.65766663e-07
Iter: 1628 loss: 6.65565381e-07
Iter: 1629 loss: 6.65490461e-07
Iter: 1630 loss: 6.6536245e-07
Iter: 1631 loss: 6.67666768e-07
Iter: 1632 loss: 6.65372e-07
Iter: 1633 loss: 6.65339144e-07
Iter: 1634 loss: 6.6530032e-07
Iter: 1635 loss: 6.65236712e-07
Iter: 1636 loss: 6.65179073e-07
Iter: 1637 loss: 6.65126549e-07
Iter: 1638 loss: 6.65015193e-07
Iter: 1639 loss: 6.65648372e-07
Iter: 1640 loss: 6.65014511e-07
Iter: 1641 loss: 6.64910942e-07
Iter: 1642 loss: 6.64831418e-07
Iter: 1643 loss: 6.64809477e-07
Iter: 1644 loss: 6.64699655e-07
Iter: 1645 loss: 6.66356414e-07
Iter: 1646 loss: 6.64691e-07
Iter: 1647 loss: 6.64636104e-07
Iter: 1648 loss: 6.64507411e-07
Iter: 1649 loss: 6.66487495e-07
Iter: 1650 loss: 6.64496611e-07
Iter: 1651 loss: 6.64413847e-07
Iter: 1652 loss: 6.65758648e-07
Iter: 1653 loss: 6.64411459e-07
Iter: 1654 loss: 6.64351887e-07
Iter: 1655 loss: 6.64523554e-07
Iter: 1656 loss: 6.64330457e-07
Iter: 1657 loss: 6.64271511e-07
Iter: 1658 loss: 6.64714264e-07
Iter: 1659 loss: 6.64278161e-07
Iter: 1660 loss: 6.64237859e-07
Iter: 1661 loss: 6.64178174e-07
Iter: 1662 loss: 6.64174763e-07
Iter: 1663 loss: 6.64062327e-07
Iter: 1664 loss: 6.64246e-07
Iter: 1665 loss: 6.64002698e-07
Iter: 1666 loss: 6.6390885e-07
Iter: 1667 loss: 6.64087054e-07
Iter: 1668 loss: 6.63877074e-07
Iter: 1669 loss: 6.63813864e-07
Iter: 1670 loss: 6.63815626e-07
Iter: 1671 loss: 6.63771402e-07
Iter: 1672 loss: 6.63808123e-07
Iter: 1673 loss: 6.63759e-07
Iter: 1674 loss: 6.6370751e-07
Iter: 1675 loss: 6.63638161e-07
Iter: 1676 loss: 6.63639582e-07
Iter: 1677 loss: 6.6356813e-07
Iter: 1678 loss: 6.64076708e-07
Iter: 1679 loss: 6.63553919e-07
Iter: 1680 loss: 6.63492074e-07
Iter: 1681 loss: 6.63475589e-07
Iter: 1682 loss: 6.63421361e-07
Iter: 1683 loss: 6.63373498e-07
Iter: 1684 loss: 6.6340283e-07
Iter: 1685 loss: 6.63317792e-07
Iter: 1686 loss: 6.63206833e-07
Iter: 1687 loss: 6.63910782e-07
Iter: 1688 loss: 6.63216213e-07
Iter: 1689 loss: 6.63150729e-07
Iter: 1690 loss: 6.63742696e-07
Iter: 1691 loss: 6.63151e-07
Iter: 1692 loss: 6.63091669e-07
Iter: 1693 loss: 6.63070182e-07
Iter: 1694 loss: 6.63033234e-07
Iter: 1695 loss: 6.62971843e-07
Iter: 1696 loss: 6.63149422e-07
Iter: 1697 loss: 6.62930063e-07
Iter: 1698 loss: 6.62828938e-07
Iter: 1699 loss: 6.62776074e-07
Iter: 1700 loss: 6.62730656e-07
Iter: 1701 loss: 6.62699279e-07
Iter: 1702 loss: 6.6266557e-07
Iter: 1703 loss: 6.62623563e-07
Iter: 1704 loss: 6.62493676e-07
Iter: 1705 loss: 6.6414475e-07
Iter: 1706 loss: 6.62497712e-07
Iter: 1707 loss: 6.62374873e-07
Iter: 1708 loss: 6.62374873e-07
Iter: 1709 loss: 6.62343041e-07
Iter: 1710 loss: 6.62280399e-07
Iter: 1711 loss: 6.62290745e-07
Iter: 1712 loss: 6.6218837e-07
Iter: 1713 loss: 6.62542675e-07
Iter: 1714 loss: 6.62179275e-07
Iter: 1715 loss: 6.62122943e-07
Iter: 1716 loss: 6.62105e-07
Iter: 1717 loss: 6.62073035e-07
Iter: 1718 loss: 6.6200414e-07
Iter: 1719 loss: 6.6210589e-07
Iter: 1720 loss: 6.61962076e-07
Iter: 1721 loss: 6.61865158e-07
Iter: 1722 loss: 6.61860213e-07
Iter: 1723 loss: 6.61826107e-07
Iter: 1724 loss: 6.61936667e-07
Iter: 1725 loss: 6.61799959e-07
Iter: 1726 loss: 6.6173709e-07
Iter: 1727 loss: 6.61709464e-07
Iter: 1728 loss: 6.61672573e-07
Iter: 1729 loss: 6.61581112e-07
Iter: 1730 loss: 6.61643071e-07
Iter: 1731 loss: 6.61498689e-07
Iter: 1732 loss: 6.61412912e-07
Iter: 1733 loss: 6.62007892e-07
Iter: 1734 loss: 6.61421154e-07
Iter: 1735 loss: 6.61298259e-07
Iter: 1736 loss: 6.61455829e-07
Iter: 1737 loss: 6.61252898e-07
Iter: 1738 loss: 6.61165e-07
Iter: 1739 loss: 6.61375225e-07
Iter: 1740 loss: 6.61133072e-07
Iter: 1741 loss: 6.61028366e-07
Iter: 1742 loss: 6.61117269e-07
Iter: 1743 loss: 6.6094691e-07
Iter: 1744 loss: 6.60861133e-07
Iter: 1745 loss: 6.61223169e-07
Iter: 1746 loss: 6.60831176e-07
Iter: 1747 loss: 6.60740056e-07
Iter: 1748 loss: 6.6072306e-07
Iter: 1749 loss: 6.60653939e-07
Iter: 1750 loss: 6.60573903e-07
Iter: 1751 loss: 6.60643309e-07
Iter: 1752 loss: 6.60512285e-07
Iter: 1753 loss: 6.60450723e-07
Iter: 1754 loss: 6.60447142e-07
Iter: 1755 loss: 6.60388196e-07
Iter: 1756 loss: 6.60516093e-07
Iter: 1757 loss: 6.60358751e-07
Iter: 1758 loss: 6.60307137e-07
Iter: 1759 loss: 6.60588569e-07
Iter: 1760 loss: 6.60290766e-07
Iter: 1761 loss: 6.6024279e-07
Iter: 1762 loss: 6.60090905e-07
Iter: 1763 loss: 6.60096532e-07
Iter: 1764 loss: 6.59958289e-07
Iter: 1765 loss: 6.59997113e-07
Iter: 1766 loss: 6.59864895e-07
Iter: 1767 loss: 6.59713e-07
Iter: 1768 loss: 6.60895921e-07
Iter: 1769 loss: 6.5970778e-07
Iter: 1770 loss: 6.59637067e-07
Iter: 1771 loss: 6.59636555e-07
Iter: 1772 loss: 6.59594889e-07
Iter: 1773 loss: 6.59525597e-07
Iter: 1774 loss: 6.60162073e-07
Iter: 1775 loss: 6.59520879e-07
Iter: 1776 loss: 6.59447778e-07
Iter: 1777 loss: 6.5944414e-07
Iter: 1778 loss: 6.59409579e-07
Iter: 1779 loss: 6.59370812e-07
Iter: 1780 loss: 6.59352281e-07
Iter: 1781 loss: 6.59273496e-07
Iter: 1782 loss: 6.59249054e-07
Iter: 1783 loss: 6.59203351e-07
Iter: 1784 loss: 6.59078808e-07
Iter: 1785 loss: 6.58975409e-07
Iter: 1786 loss: 6.58926524e-07
Iter: 1787 loss: 6.5873536e-07
Iter: 1788 loss: 6.60140586e-07
Iter: 1789 loss: 6.58713589e-07
Iter: 1790 loss: 6.5858012e-07
Iter: 1791 loss: 6.58571366e-07
Iter: 1792 loss: 6.58527938e-07
Iter: 1793 loss: 6.58406861e-07
Iter: 1794 loss: 6.60763703e-07
Iter: 1795 loss: 6.58403337e-07
Iter: 1796 loss: 6.58297438e-07
Iter: 1797 loss: 6.59831358e-07
Iter: 1798 loss: 6.58289764e-07
Iter: 1799 loss: 6.58208e-07
Iter: 1800 loss: 6.58140266e-07
Iter: 1801 loss: 6.5812003e-07
Iter: 1802 loss: 6.58056194e-07
Iter: 1803 loss: 6.58121621e-07
Iter: 1804 loss: 6.58009071e-07
Iter: 1805 loss: 6.57984913e-07
Iter: 1806 loss: 6.57974851e-07
Iter: 1807 loss: 6.57919202e-07
Iter: 1808 loss: 6.57868725e-07
Iter: 1809 loss: 6.57875603e-07
Iter: 1810 loss: 6.57831038e-07
Iter: 1811 loss: 6.57832288e-07
Iter: 1812 loss: 6.57811711e-07
Iter: 1813 loss: 6.57706437e-07
Iter: 1814 loss: 6.5806671e-07
Iter: 1815 loss: 6.57678697e-07
Iter: 1816 loss: 6.57611736e-07
Iter: 1817 loss: 6.57530222e-07
Iter: 1818 loss: 6.57513851e-07
Iter: 1819 loss: 6.57357418e-07
Iter: 1820 loss: 6.57318083e-07
Iter: 1821 loss: 6.57200872e-07
Iter: 1822 loss: 6.57001e-07
Iter: 1823 loss: 6.57950523e-07
Iter: 1824 loss: 6.56968098e-07
Iter: 1825 loss: 6.56738621e-07
Iter: 1826 loss: 6.58525096e-07
Iter: 1827 loss: 6.56721795e-07
Iter: 1828 loss: 6.56642e-07
Iter: 1829 loss: 6.56674274e-07
Iter: 1830 loss: 6.56572354e-07
Iter: 1831 loss: 6.56457701e-07
Iter: 1832 loss: 6.57536e-07
Iter: 1833 loss: 6.56439056e-07
Iter: 1834 loss: 6.56399266e-07
Iter: 1835 loss: 6.56288535e-07
Iter: 1836 loss: 6.57262433e-07
Iter: 1837 loss: 6.56271709e-07
Iter: 1838 loss: 6.56127781e-07
Iter: 1839 loss: 6.57683074e-07
Iter: 1840 loss: 6.56117663e-07
Iter: 1841 loss: 6.56088673e-07
Iter: 1842 loss: 6.56056727e-07
Iter: 1843 loss: 6.56039049e-07
Iter: 1844 loss: 6.55935537e-07
Iter: 1845 loss: 6.56365842e-07
Iter: 1846 loss: 6.55892393e-07
Iter: 1847 loss: 6.55761198e-07
Iter: 1848 loss: 6.55948497e-07
Iter: 1849 loss: 6.55705492e-07
Iter: 1850 loss: 6.55713848e-07
Iter: 1851 loss: 6.55653764e-07
Iter: 1852 loss: 6.55618805e-07
Iter: 1853 loss: 6.55505914e-07
Iter: 1854 loss: 6.56712189e-07
Iter: 1855 loss: 6.55503072e-07
Iter: 1856 loss: 6.55348458e-07
Iter: 1857 loss: 6.55973736e-07
Iter: 1858 loss: 6.55332769e-07
Iter: 1859 loss: 6.55217718e-07
Iter: 1860 loss: 6.55684801e-07
Iter: 1861 loss: 6.55185488e-07
Iter: 1862 loss: 6.55081408e-07
Iter: 1863 loss: 6.55654901e-07
Iter: 1864 loss: 6.55070949e-07
Iter: 1865 loss: 6.55022347e-07
Iter: 1866 loss: 6.55274221e-07
Iter: 1867 loss: 6.54996711e-07
Iter: 1868 loss: 6.54959422e-07
Iter: 1869 loss: 6.55049121e-07
Iter: 1870 loss: 6.54913663e-07
Iter: 1871 loss: 6.54879898e-07
Iter: 1872 loss: 6.5482061e-07
Iter: 1873 loss: 6.5481413e-07
Iter: 1874 loss: 6.54813107e-07
Iter: 1875 loss: 6.54775931e-07
Iter: 1876 loss: 6.54777466e-07
Iter: 1877 loss: 6.54781047e-07
Iter: 1878 loss: 6.54766097e-07
Iter: 1879 loss: 6.54777693e-07
Iter: 1880 loss: 6.54773316e-07
Iter: 1881 loss: 6.54784e-07
Iter: 1882 loss: 6.5477991e-07
Iter: 1883 loss: 6.54786106e-07
Iter: 1884 loss: 6.54781047e-07
Iter: 1885 loss: 6.54773146e-07
Iter: 1886 loss: 6.54780365e-07
Iter: 1887 loss: 6.5477775e-07
Iter: 1888 loss: 6.5477218e-07
Iter: 1889 loss: 6.54775818e-07
Iter: 1890 loss: 6.54777239e-07
Iter: 1891 loss: 6.54776898e-07
Iter: 1892 loss: 6.54776386e-07
Iter: 1893 loss: 6.54776898e-07
Iter: 1894 loss: 6.54776045e-07
Iter: 1895 loss: 6.54776045e-07
Iter: 1896 loss: 6.54776954e-07
Iter: 1897 loss: 6.54776159e-07
Iter: 1898 loss: 6.54776272e-07
Iter: 1899 loss: 6.54776272e-07
Iter: 1900 loss: 6.54776272e-07
Iter: 1901 loss: 6.54776272e-07
Iter: 1902 loss: 6.54776954e-07
Iter: 1903 loss: 6.54563735e-07
Iter: 1904 loss: 6.56371753e-07
Iter: 1905 loss: 6.54535143e-07
Iter: 1906 loss: 6.54421456e-07
Iter: 1907 loss: 6.55060035e-07
Iter: 1908 loss: 6.54424696e-07
Iter: 1909 loss: 6.54257406e-07
Iter: 1910 loss: 6.54538781e-07
Iter: 1911 loss: 6.54200051e-07
Iter: 1912 loss: 6.54091195e-07
Iter: 1913 loss: 6.54539917e-07
Iter: 1914 loss: 6.54083124e-07
Iter: 1915 loss: 6.54011217e-07
Iter: 1916 loss: 6.54356199e-07
Iter: 1917 loss: 6.54003e-07
Iter: 1918 loss: 6.53933398e-07
Iter: 1919 loss: 6.54187886e-07
Iter: 1920 loss: 6.53921632e-07
Iter: 1921 loss: 6.53870416e-07
Iter: 1922 loss: 6.54051746e-07
Iter: 1923 loss: 6.5387195e-07
Iter: 1924 loss: 6.53808513e-07
Iter: 1925 loss: 6.54052e-07
Iter: 1926 loss: 6.53802431e-07
Iter: 1927 loss: 6.53773782e-07
Iter: 1928 loss: 6.53731263e-07
Iter: 1929 loss: 6.55004953e-07
Iter: 1930 loss: 6.53729785e-07
Iter: 1931 loss: 6.53683685e-07
Iter: 1932 loss: 6.53680786e-07
Iter: 1933 loss: 6.53628035e-07
Iter: 1934 loss: 6.53618599e-07
Iter: 1935 loss: 6.53598875e-07
Iter: 1936 loss: 6.53547886e-07
Iter: 1937 loss: 6.5347183e-07
Iter: 1938 loss: 6.54878932e-07
Iter: 1939 loss: 6.53476036e-07
Iter: 1940 loss: 6.53346319e-07
Iter: 1941 loss: 6.53283905e-07
Iter: 1942 loss: 6.5324474e-07
Iter: 1943 loss: 6.5317e-07
Iter: 1944 loss: 6.53124403e-07
Iter: 1945 loss: 6.53021573e-07
Iter: 1946 loss: 6.52888673e-07
Iter: 1947 loss: 6.52892822e-07
Iter: 1948 loss: 6.52750259e-07
Iter: 1949 loss: 6.53949314e-07
Iter: 1950 loss: 6.5276015e-07
Iter: 1951 loss: 6.52648168e-07
Iter: 1952 loss: 6.52984681e-07
Iter: 1953 loss: 6.52634867e-07
Iter: 1954 loss: 6.52532094e-07
Iter: 1955 loss: 6.52786866e-07
Iter: 1956 loss: 6.52517144e-07
Iter: 1957 loss: 6.52455526e-07
Iter: 1958 loss: 6.53142138e-07
Iter: 1959 loss: 6.52453878e-07
Iter: 1960 loss: 6.52427161e-07
Iter: 1961 loss: 6.52361109e-07
Iter: 1962 loss: 6.52339054e-07
Iter: 1963 loss: 6.52301367e-07
Iter: 1964 loss: 6.52493327e-07
Iter: 1965 loss: 6.52292158e-07
Iter: 1966 loss: 6.52206495e-07
Iter: 1967 loss: 6.52524079e-07
Iter: 1968 loss: 6.522e-07
Iter: 1969 loss: 6.52128733e-07
Iter: 1970 loss: 6.51945243e-07
Iter: 1971 loss: 6.52528456e-07
Iter: 1972 loss: 6.5185418e-07
Iter: 1973 loss: 6.5153688e-07
Iter: 1974 loss: 6.52437677e-07
Iter: 1975 loss: 6.51449113e-07
Iter: 1976 loss: 6.51282789e-07
Iter: 1977 loss: 6.52669883e-07
Iter: 1978 loss: 6.51267214e-07
Iter: 1979 loss: 6.51190589e-07
Iter: 1980 loss: 6.51207642e-07
Iter: 1981 loss: 6.51152277e-07
Iter: 1982 loss: 6.51065648e-07
Iter: 1983 loss: 6.52490883e-07
Iter: 1984 loss: 6.51079915e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4
+ date
Mon Oct 26 09:19:21 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.4_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.4_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae834de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae835c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae835c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae83ed598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae83ed158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae838d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae83079d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae82a4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae82a4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae82a4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae8278378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae8236a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae82458c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae82190d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae821d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae821d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae828e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae81d50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae80e6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae81578c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae8154950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae80aa158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae8154840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae8064ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae802b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae8044bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faae8044840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faad010d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faad010d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faad00c09d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faad007d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faad00c0bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faad00c0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faad0062840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa847760d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faa84753f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.87015143e-06
Iter: 2 loss: 3.52913139e-06
Iter: 3 loss: 2.43362206e-06
Iter: 4 loss: 2.09099721e-06
Iter: 5 loss: 2.00810382e-06
Iter: 6 loss: 1.79028211e-06
Iter: 7 loss: 1.73076455e-06
Iter: 8 loss: 1.69112e-06
Iter: 9 loss: 1.65391748e-06
Iter: 10 loss: 1.57983641e-06
Iter: 11 loss: 2.99910243e-06
Iter: 12 loss: 1.57895909e-06
Iter: 13 loss: 1.5466278e-06
Iter: 14 loss: 1.54253109e-06
Iter: 15 loss: 1.52276107e-06
Iter: 16 loss: 1.63706716e-06
Iter: 17 loss: 1.52011148e-06
Iter: 18 loss: 1.51070651e-06
Iter: 19 loss: 1.49750804e-06
Iter: 20 loss: 1.49700418e-06
Iter: 21 loss: 1.48636252e-06
Iter: 22 loss: 1.48612048e-06
Iter: 23 loss: 1.47362914e-06
Iter: 24 loss: 1.44378339e-06
Iter: 25 loss: 1.77859181e-06
Iter: 26 loss: 1.44086687e-06
Iter: 27 loss: 1.41380781e-06
Iter: 28 loss: 1.45810623e-06
Iter: 29 loss: 1.40144709e-06
Iter: 30 loss: 1.38428516e-06
Iter: 31 loss: 1.5228386e-06
Iter: 32 loss: 1.38317375e-06
Iter: 33 loss: 1.37529275e-06
Iter: 34 loss: 1.43259604e-06
Iter: 35 loss: 1.37464212e-06
Iter: 36 loss: 1.37104735e-06
Iter: 37 loss: 1.40032898e-06
Iter: 38 loss: 1.37081861e-06
Iter: 39 loss: 1.36701874e-06
Iter: 40 loss: 1.39707697e-06
Iter: 41 loss: 1.36677932e-06
Iter: 42 loss: 1.36482913e-06
Iter: 43 loss: 1.36156575e-06
Iter: 44 loss: 1.36153812e-06
Iter: 45 loss: 1.35654227e-06
Iter: 46 loss: 1.39756116e-06
Iter: 47 loss: 1.35621121e-06
Iter: 48 loss: 1.35422147e-06
Iter: 49 loss: 1.34954144e-06
Iter: 50 loss: 1.40558416e-06
Iter: 51 loss: 1.34913421e-06
Iter: 52 loss: 1.34503716e-06
Iter: 53 loss: 1.34473851e-06
Iter: 54 loss: 1.34244101e-06
Iter: 55 loss: 1.33883327e-06
Iter: 56 loss: 1.33876085e-06
Iter: 57 loss: 1.33560184e-06
Iter: 58 loss: 1.3697246e-06
Iter: 59 loss: 1.33552157e-06
Iter: 60 loss: 1.33310948e-06
Iter: 61 loss: 1.35060441e-06
Iter: 62 loss: 1.33287631e-06
Iter: 63 loss: 1.33126173e-06
Iter: 64 loss: 1.3288543e-06
Iter: 65 loss: 1.32880541e-06
Iter: 66 loss: 1.32587593e-06
Iter: 67 loss: 1.32703667e-06
Iter: 68 loss: 1.32388232e-06
Iter: 69 loss: 1.3208695e-06
Iter: 70 loss: 1.3249105e-06
Iter: 71 loss: 1.31935042e-06
Iter: 72 loss: 1.31689114e-06
Iter: 73 loss: 1.31688284e-06
Iter: 74 loss: 1.31411082e-06
Iter: 75 loss: 1.32342893e-06
Iter: 76 loss: 1.31336253e-06
Iter: 77 loss: 1.31186982e-06
Iter: 78 loss: 1.31116576e-06
Iter: 79 loss: 1.31043703e-06
Iter: 80 loss: 1.30711805e-06
Iter: 81 loss: 1.31712477e-06
Iter: 82 loss: 1.30610533e-06
Iter: 83 loss: 1.30391572e-06
Iter: 84 loss: 1.30061585e-06
Iter: 85 loss: 1.30055173e-06
Iter: 86 loss: 1.30041974e-06
Iter: 87 loss: 1.29876207e-06
Iter: 88 loss: 1.2978478e-06
Iter: 89 loss: 1.29532248e-06
Iter: 90 loss: 1.30741262e-06
Iter: 91 loss: 1.2944522e-06
Iter: 92 loss: 1.29234922e-06
Iter: 93 loss: 1.2923349e-06
Iter: 94 loss: 1.29033469e-06
Iter: 95 loss: 1.29982e-06
Iter: 96 loss: 1.28996157e-06
Iter: 97 loss: 1.28850058e-06
Iter: 98 loss: 1.28695638e-06
Iter: 99 loss: 1.28671581e-06
Iter: 100 loss: 1.28422676e-06
Iter: 101 loss: 1.28562124e-06
Iter: 102 loss: 1.28258546e-06
Iter: 103 loss: 1.27987e-06
Iter: 104 loss: 1.2840992e-06
Iter: 105 loss: 1.27863132e-06
Iter: 106 loss: 1.27676674e-06
Iter: 107 loss: 1.27675889e-06
Iter: 108 loss: 1.27504302e-06
Iter: 109 loss: 1.28922204e-06
Iter: 110 loss: 1.27493809e-06
Iter: 111 loss: 1.27404041e-06
Iter: 112 loss: 1.27218641e-06
Iter: 113 loss: 1.30513376e-06
Iter: 114 loss: 1.27215e-06
Iter: 115 loss: 1.27037993e-06
Iter: 116 loss: 1.27033616e-06
Iter: 117 loss: 1.26970326e-06
Iter: 118 loss: 1.26822033e-06
Iter: 119 loss: 1.28611941e-06
Iter: 120 loss: 1.26809323e-06
Iter: 121 loss: 1.26755958e-06
Iter: 122 loss: 1.26724615e-06
Iter: 123 loss: 1.26648115e-06
Iter: 124 loss: 1.26464113e-06
Iter: 125 loss: 1.28181068e-06
Iter: 126 loss: 1.26437897e-06
Iter: 127 loss: 1.26321572e-06
Iter: 128 loss: 1.28014165e-06
Iter: 129 loss: 1.26324596e-06
Iter: 130 loss: 1.262e-06
Iter: 131 loss: 1.26525276e-06
Iter: 132 loss: 1.26158852e-06
Iter: 133 loss: 1.26059263e-06
Iter: 134 loss: 1.25865176e-06
Iter: 135 loss: 1.29692603e-06
Iter: 136 loss: 1.25864426e-06
Iter: 137 loss: 1.25723318e-06
Iter: 138 loss: 1.27340309e-06
Iter: 139 loss: 1.25722192e-06
Iter: 140 loss: 1.2561693e-06
Iter: 141 loss: 1.25818872e-06
Iter: 142 loss: 1.25572797e-06
Iter: 143 loss: 1.25473684e-06
Iter: 144 loss: 1.25474435e-06
Iter: 145 loss: 1.25393228e-06
Iter: 146 loss: 1.25407598e-06
Iter: 147 loss: 1.25343968e-06
Iter: 148 loss: 1.25290762e-06
Iter: 149 loss: 1.25169493e-06
Iter: 150 loss: 1.26773887e-06
Iter: 151 loss: 1.25159829e-06
Iter: 152 loss: 1.25054089e-06
Iter: 153 loss: 1.25482438e-06
Iter: 154 loss: 1.25031454e-06
Iter: 155 loss: 1.24891858e-06
Iter: 156 loss: 1.25533052e-06
Iter: 157 loss: 1.24865676e-06
Iter: 158 loss: 1.24802455e-06
Iter: 159 loss: 1.24721305e-06
Iter: 160 loss: 1.24715768e-06
Iter: 161 loss: 1.24653e-06
Iter: 162 loss: 1.24644657e-06
Iter: 163 loss: 1.24585881e-06
Iter: 164 loss: 1.24414601e-06
Iter: 165 loss: 1.25315614e-06
Iter: 166 loss: 1.24362964e-06
Iter: 167 loss: 1.24372809e-06
Iter: 168 loss: 1.24299743e-06
Iter: 169 loss: 1.2423402e-06
Iter: 170 loss: 1.24123721e-06
Iter: 171 loss: 1.2412338e-06
Iter: 172 loss: 1.24022858e-06
Iter: 173 loss: 1.24071812e-06
Iter: 174 loss: 1.23957363e-06
Iter: 175 loss: 1.23875816e-06
Iter: 176 loss: 1.24617839e-06
Iter: 177 loss: 1.23872917e-06
Iter: 178 loss: 1.238099e-06
Iter: 179 loss: 1.24376788e-06
Iter: 180 loss: 1.23808195e-06
Iter: 181 loss: 1.23729819e-06
Iter: 182 loss: 1.23668678e-06
Iter: 183 loss: 1.23644099e-06
Iter: 184 loss: 1.23544544e-06
Iter: 185 loss: 1.23436121e-06
Iter: 186 loss: 1.23416385e-06
Iter: 187 loss: 1.23360223e-06
Iter: 188 loss: 1.23321206e-06
Iter: 189 loss: 1.23231939e-06
Iter: 190 loss: 1.23056884e-06
Iter: 191 loss: 1.2619164e-06
Iter: 192 loss: 1.2305336e-06
Iter: 193 loss: 1.2300668e-06
Iter: 194 loss: 1.22981783e-06
Iter: 195 loss: 1.22927315e-06
Iter: 196 loss: 1.22830386e-06
Iter: 197 loss: 1.2283017e-06
Iter: 198 loss: 1.22771007e-06
Iter: 199 loss: 1.22981089e-06
Iter: 200 loss: 1.22758e-06
Iter: 201 loss: 1.22694894e-06
Iter: 202 loss: 1.23279108e-06
Iter: 203 loss: 1.22695496e-06
Iter: 204 loss: 1.22644781e-06
Iter: 205 loss: 1.22523102e-06
Iter: 206 loss: 1.2346834e-06
Iter: 207 loss: 1.22496942e-06
Iter: 208 loss: 1.22369352e-06
Iter: 209 loss: 1.22783717e-06
Iter: 210 loss: 1.2233412e-06
Iter: 211 loss: 1.22217648e-06
Iter: 212 loss: 1.22714107e-06
Iter: 213 loss: 1.22193342e-06
Iter: 214 loss: 1.22101392e-06
Iter: 215 loss: 1.22099925e-06
Iter: 216 loss: 1.22052893e-06
Iter: 217 loss: 1.21966195e-06
Iter: 218 loss: 1.23954374e-06
Iter: 219 loss: 1.21963694e-06
Iter: 220 loss: 1.2192861e-06
Iter: 221 loss: 1.21925541e-06
Iter: 222 loss: 1.21882397e-06
Iter: 223 loss: 1.2184272e-06
Iter: 224 loss: 1.21832659e-06
Iter: 225 loss: 1.21782762e-06
Iter: 226 loss: 1.22213032e-06
Iter: 227 loss: 1.21781727e-06
Iter: 228 loss: 1.2173391e-06
Iter: 229 loss: 1.21724042e-06
Iter: 230 loss: 1.2169138e-06
Iter: 231 loss: 1.21640232e-06
Iter: 232 loss: 1.21574897e-06
Iter: 233 loss: 1.21569451e-06
Iter: 234 loss: 1.21535948e-06
Iter: 235 loss: 1.21519201e-06
Iter: 236 loss: 1.21462278e-06
Iter: 237 loss: 1.2136237e-06
Iter: 238 loss: 1.21362143e-06
Iter: 239 loss: 1.21265987e-06
Iter: 240 loss: 1.21299217e-06
Iter: 241 loss: 1.21196013e-06
Iter: 242 loss: 1.21045673e-06
Iter: 243 loss: 1.22008601e-06
Iter: 244 loss: 1.21028393e-06
Iter: 245 loss: 1.20911795e-06
Iter: 246 loss: 1.21451524e-06
Iter: 247 loss: 1.20890036e-06
Iter: 248 loss: 1.20762161e-06
Iter: 249 loss: 1.21691937e-06
Iter: 250 loss: 1.20749007e-06
Iter: 251 loss: 1.2070368e-06
Iter: 252 loss: 1.20627863e-06
Iter: 253 loss: 1.20628386e-06
Iter: 254 loss: 1.20592733e-06
Iter: 255 loss: 1.20585855e-06
Iter: 256 loss: 1.20544416e-06
Iter: 257 loss: 1.2052044e-06
Iter: 258 loss: 1.2050325e-06
Iter: 259 loss: 1.20465347e-06
Iter: 260 loss: 1.20486243e-06
Iter: 261 loss: 1.20444383e-06
Iter: 262 loss: 1.20392372e-06
Iter: 263 loss: 1.20949835e-06
Iter: 264 loss: 1.20390951e-06
Iter: 265 loss: 1.20359209e-06
Iter: 266 loss: 1.20274217e-06
Iter: 267 loss: 1.20999539e-06
Iter: 268 loss: 1.20258028e-06
Iter: 269 loss: 1.20203015e-06
Iter: 270 loss: 1.20191453e-06
Iter: 271 loss: 1.20142602e-06
Iter: 272 loss: 1.20037521e-06
Iter: 273 loss: 1.21783535e-06
Iter: 274 loss: 1.2003286e-06
Iter: 275 loss: 1.1994764e-06
Iter: 276 loss: 1.20014249e-06
Iter: 277 loss: 1.19894787e-06
Iter: 278 loss: 1.1978957e-06
Iter: 279 loss: 1.20039704e-06
Iter: 280 loss: 1.19750086e-06
Iter: 281 loss: 1.19674621e-06
Iter: 282 loss: 1.20760126e-06
Iter: 283 loss: 1.19673268e-06
Iter: 284 loss: 1.19635706e-06
Iter: 285 loss: 1.19634478e-06
Iter: 286 loss: 1.19602032e-06
Iter: 287 loss: 1.19540914e-06
Iter: 288 loss: 1.20851178e-06
Iter: 289 loss: 1.19538413e-06
Iter: 290 loss: 1.19478318e-06
Iter: 291 loss: 1.19581262e-06
Iter: 292 loss: 1.19450306e-06
Iter: 293 loss: 1.19401511e-06
Iter: 294 loss: 1.19400363e-06
Iter: 295 loss: 1.19363733e-06
Iter: 296 loss: 1.19276012e-06
Iter: 297 loss: 1.19904303e-06
Iter: 298 loss: 1.19257516e-06
Iter: 299 loss: 1.19181664e-06
Iter: 300 loss: 1.19182823e-06
Iter: 301 loss: 1.19100275e-06
Iter: 302 loss: 1.19221806e-06
Iter: 303 loss: 1.190597e-06
Iter: 304 loss: 1.19001527e-06
Iter: 305 loss: 1.18919775e-06
Iter: 306 loss: 1.18916762e-06
Iter: 307 loss: 1.18876108e-06
Iter: 308 loss: 1.1885212e-06
Iter: 309 loss: 1.18816013e-06
Iter: 310 loss: 1.18737671e-06
Iter: 311 loss: 1.20038203e-06
Iter: 312 loss: 1.18735807e-06
Iter: 313 loss: 1.18662342e-06
Iter: 314 loss: 1.18671e-06
Iter: 315 loss: 1.18607682e-06
Iter: 316 loss: 1.18542391e-06
Iter: 317 loss: 1.19194624e-06
Iter: 318 loss: 1.1853956e-06
Iter: 319 loss: 1.18460298e-06
Iter: 320 loss: 1.18639912e-06
Iter: 321 loss: 1.18430262e-06
Iter: 322 loss: 1.18389698e-06
Iter: 323 loss: 1.18333469e-06
Iter: 324 loss: 1.18331286e-06
Iter: 325 loss: 1.1828904e-06
Iter: 326 loss: 1.18285038e-06
Iter: 327 loss: 1.18243383e-06
Iter: 328 loss: 1.18253274e-06
Iter: 329 loss: 1.18211358e-06
Iter: 330 loss: 1.18165e-06
Iter: 331 loss: 1.18178525e-06
Iter: 332 loss: 1.18127787e-06
Iter: 333 loss: 1.18098137e-06
Iter: 334 loss: 1.18095977e-06
Iter: 335 loss: 1.18063781e-06
Iter: 336 loss: 1.1800613e-06
Iter: 337 loss: 1.19315769e-06
Iter: 338 loss: 1.18006415e-06
Iter: 339 loss: 1.17951663e-06
Iter: 340 loss: 1.18113462e-06
Iter: 341 loss: 1.1793486e-06
Iter: 342 loss: 1.1787256e-06
Iter: 343 loss: 1.18513117e-06
Iter: 344 loss: 1.17869922e-06
Iter: 345 loss: 1.17839477e-06
Iter: 346 loss: 1.17752347e-06
Iter: 347 loss: 1.18202342e-06
Iter: 348 loss: 1.17723675e-06
Iter: 349 loss: 1.17651177e-06
Iter: 350 loss: 1.17649188e-06
Iter: 351 loss: 1.17601121e-06
Iter: 352 loss: 1.17601041e-06
Iter: 353 loss: 1.17576224e-06
Iter: 354 loss: 1.17503191e-06
Iter: 355 loss: 1.17906359e-06
Iter: 356 loss: 1.17485411e-06
Iter: 357 loss: 1.17415152e-06
Iter: 358 loss: 1.18320315e-06
Iter: 359 loss: 1.17417233e-06
Iter: 360 loss: 1.17354489e-06
Iter: 361 loss: 1.17765285e-06
Iter: 362 loss: 1.17349646e-06
Iter: 363 loss: 1.17314664e-06
Iter: 364 loss: 1.17279092e-06
Iter: 365 loss: 1.17273976e-06
Iter: 366 loss: 1.17212358e-06
Iter: 367 loss: 1.17442869e-06
Iter: 368 loss: 1.17196498e-06
Iter: 369 loss: 1.17131572e-06
Iter: 370 loss: 1.17537e-06
Iter: 371 loss: 1.17123477e-06
Iter: 372 loss: 1.17082652e-06
Iter: 373 loss: 1.1699816e-06
Iter: 374 loss: 1.18578373e-06
Iter: 375 loss: 1.16997398e-06
Iter: 376 loss: 1.1696726e-06
Iter: 377 loss: 1.16951014e-06
Iter: 378 loss: 1.16906949e-06
Iter: 379 loss: 1.16813021e-06
Iter: 380 loss: 1.1829402e-06
Iter: 381 loss: 1.16806336e-06
Iter: 382 loss: 1.1673244e-06
Iter: 383 loss: 1.16812203e-06
Iter: 384 loss: 1.16691308e-06
Iter: 385 loss: 1.16714341e-06
Iter: 386 loss: 1.1666566e-06
Iter: 387 loss: 1.16638842e-06
Iter: 388 loss: 1.16579463e-06
Iter: 389 loss: 1.17550258e-06
Iter: 390 loss: 1.16575745e-06
Iter: 391 loss: 1.1652678e-06
Iter: 392 loss: 1.16555032e-06
Iter: 393 loss: 1.16490878e-06
Iter: 394 loss: 1.16445926e-06
Iter: 395 loss: 1.16445301e-06
Iter: 396 loss: 1.16414367e-06
Iter: 397 loss: 1.16367789e-06
Iter: 398 loss: 1.16367687e-06
Iter: 399 loss: 1.16315846e-06
Iter: 400 loss: 1.16580213e-06
Iter: 401 loss: 1.16304307e-06
Iter: 402 loss: 1.162731e-06
Iter: 403 loss: 1.16472324e-06
Iter: 404 loss: 1.16268825e-06
Iter: 405 loss: 1.16235583e-06
Iter: 406 loss: 1.16203159e-06
Iter: 407 loss: 1.16199431e-06
Iter: 408 loss: 1.16159663e-06
Iter: 409 loss: 1.16480601e-06
Iter: 410 loss: 1.16155752e-06
Iter: 411 loss: 1.1610374e-06
Iter: 412 loss: 1.16098272e-06
Iter: 413 loss: 1.16059869e-06
Iter: 414 loss: 1.16003173e-06
Iter: 415 loss: 1.15901162e-06
Iter: 416 loss: 1.18319372e-06
Iter: 417 loss: 1.15901503e-06
Iter: 418 loss: 1.15819137e-06
Iter: 419 loss: 1.15817716e-06
Iter: 420 loss: 1.15736884e-06
Iter: 421 loss: 1.16312776e-06
Iter: 422 loss: 1.15729461e-06
Iter: 423 loss: 1.15702028e-06
Iter: 424 loss: 1.15649141e-06
Iter: 425 loss: 1.16735248e-06
Iter: 426 loss: 1.15647083e-06
Iter: 427 loss: 1.15638909e-06
Iter: 428 loss: 1.15624471e-06
Iter: 429 loss: 1.15602779e-06
Iter: 430 loss: 1.15559396e-06
Iter: 431 loss: 1.16442425e-06
Iter: 432 loss: 1.15560886e-06
Iter: 433 loss: 1.1551507e-06
Iter: 434 loss: 1.15808257e-06
Iter: 435 loss: 1.15510556e-06
Iter: 436 loss: 1.15467947e-06
Iter: 437 loss: 1.15589103e-06
Iter: 438 loss: 1.15455509e-06
Iter: 439 loss: 1.15408545e-06
Iter: 440 loss: 1.15406158e-06
Iter: 441 loss: 1.15370676e-06
Iter: 442 loss: 1.15316072e-06
Iter: 443 loss: 1.15393891e-06
Iter: 444 loss: 1.15290368e-06
Iter: 445 loss: 1.15228863e-06
Iter: 446 loss: 1.16108072e-06
Iter: 447 loss: 1.15228602e-06
Iter: 448 loss: 1.15203443e-06
Iter: 449 loss: 1.15146372e-06
Iter: 450 loss: 1.1576019e-06
Iter: 451 loss: 1.15139596e-06
Iter: 452 loss: 1.15071134e-06
Iter: 453 loss: 1.15184116e-06
Iter: 454 loss: 1.15042849e-06
Iter: 455 loss: 1.15041269e-06
Iter: 456 loss: 1.15005446e-06
Iter: 457 loss: 1.1498164e-06
Iter: 458 loss: 1.14919817e-06
Iter: 459 loss: 1.15131547e-06
Iter: 460 loss: 1.14888348e-06
Iter: 461 loss: 1.14842032e-06
Iter: 462 loss: 1.14841441e-06
Iter: 463 loss: 1.1478852e-06
Iter: 464 loss: 1.1488703e-06
Iter: 465 loss: 1.14761758e-06
Iter: 466 loss: 1.14730472e-06
Iter: 467 loss: 1.14795898e-06
Iter: 468 loss: 1.14717068e-06
Iter: 469 loss: 1.14681461e-06
Iter: 470 loss: 1.14928e-06
Iter: 471 loss: 1.14679324e-06
Iter: 472 loss: 1.14645491e-06
Iter: 473 loss: 1.14647196e-06
Iter: 474 loss: 1.14622219e-06
Iter: 475 loss: 1.14576062e-06
Iter: 476 loss: 1.14547368e-06
Iter: 477 loss: 1.14530121e-06
Iter: 478 loss: 1.1449215e-06
Iter: 479 loss: 1.14489046e-06
Iter: 480 loss: 1.14456088e-06
Iter: 481 loss: 1.14377326e-06
Iter: 482 loss: 1.14963e-06
Iter: 483 loss: 1.14356158e-06
Iter: 484 loss: 1.14275872e-06
Iter: 485 loss: 1.14574254e-06
Iter: 486 loss: 1.14255295e-06
Iter: 487 loss: 1.14277213e-06
Iter: 488 loss: 1.14231534e-06
Iter: 489 loss: 1.14209274e-06
Iter: 490 loss: 1.14158433e-06
Iter: 491 loss: 1.14779277e-06
Iter: 492 loss: 1.14156205e-06
Iter: 493 loss: 1.14116961e-06
Iter: 494 loss: 1.14156182e-06
Iter: 495 loss: 1.14093029e-06
Iter: 496 loss: 1.14041814e-06
Iter: 497 loss: 1.1479608e-06
Iter: 498 loss: 1.14039699e-06
Iter: 499 loss: 1.14012994e-06
Iter: 500 loss: 1.13975352e-06
Iter: 501 loss: 1.13975534e-06
Iter: 502 loss: 1.13933265e-06
Iter: 503 loss: 1.14556519e-06
Iter: 504 loss: 1.13933265e-06
Iter: 505 loss: 1.13902206e-06
Iter: 506 loss: 1.13902843e-06
Iter: 507 loss: 1.1387931e-06
Iter: 508 loss: 1.13832357e-06
Iter: 509 loss: 1.13827934e-06
Iter: 510 loss: 1.13796705e-06
Iter: 511 loss: 1.13767214e-06
Iter: 512 loss: 1.13765464e-06
Iter: 513 loss: 1.13735064e-06
Iter: 514 loss: 1.13681199e-06
Iter: 515 loss: 1.14989598e-06
Iter: 516 loss: 1.13680892e-06
Iter: 517 loss: 1.13627266e-06
Iter: 518 loss: 1.13600152e-06
Iter: 519 loss: 1.13579267e-06
Iter: 520 loss: 1.13553961e-06
Iter: 521 loss: 1.13542364e-06
Iter: 522 loss: 1.13505325e-06
Iter: 523 loss: 1.13508565e-06
Iter: 524 loss: 1.13475107e-06
Iter: 525 loss: 1.13446299e-06
Iter: 526 loss: 1.1338733e-06
Iter: 527 loss: 1.14468162e-06
Iter: 528 loss: 1.13386363e-06
Iter: 529 loss: 1.13374745e-06
Iter: 530 loss: 1.13350927e-06
Iter: 531 loss: 1.13327383e-06
Iter: 532 loss: 1.13274598e-06
Iter: 533 loss: 1.14122281e-06
Iter: 534 loss: 1.13274859e-06
Iter: 535 loss: 1.13232329e-06
Iter: 536 loss: 1.13848762e-06
Iter: 537 loss: 1.13233614e-06
Iter: 538 loss: 1.13195927e-06
Iter: 539 loss: 1.13189458e-06
Iter: 540 loss: 1.1316838e-06
Iter: 541 loss: 1.13121155e-06
Iter: 542 loss: 1.13217197e-06
Iter: 543 loss: 1.1310201e-06
Iter: 544 loss: 1.13061378e-06
Iter: 545 loss: 1.13202486e-06
Iter: 546 loss: 1.13050055e-06
Iter: 547 loss: 1.12998794e-06
Iter: 548 loss: 1.1304553e-06
Iter: 549 loss: 1.12969656e-06
Iter: 550 loss: 1.12925431e-06
Iter: 551 loss: 1.12843406e-06
Iter: 552 loss: 1.14548106e-06
Iter: 553 loss: 1.12844259e-06
Iter: 554 loss: 1.12740838e-06
Iter: 555 loss: 1.12837847e-06
Iter: 556 loss: 1.1268063e-06
Iter: 557 loss: 1.12821374e-06
Iter: 558 loss: 1.12645978e-06
Iter: 559 loss: 1.12624411e-06
Iter: 560 loss: 1.1256692e-06
Iter: 561 loss: 1.12907264e-06
Iter: 562 loss: 1.12553448e-06
Iter: 563 loss: 1.12515272e-06
Iter: 564 loss: 1.1251384e-06
Iter: 565 loss: 1.1246716e-06
Iter: 566 loss: 1.12508428e-06
Iter: 567 loss: 1.12439193e-06
Iter: 568 loss: 1.12397686e-06
Iter: 569 loss: 1.12409907e-06
Iter: 570 loss: 1.1236998e-06
Iter: 571 loss: 1.12304497e-06
Iter: 572 loss: 1.12703356e-06
Iter: 573 loss: 1.12296425e-06
Iter: 574 loss: 1.12256066e-06
Iter: 575 loss: 1.12277689e-06
Iter: 576 loss: 1.1223085e-06
Iter: 577 loss: 1.12175007e-06
Iter: 578 loss: 1.12319469e-06
Iter: 579 loss: 1.1215601e-06
Iter: 580 loss: 1.1210974e-06
Iter: 581 loss: 1.12629778e-06
Iter: 582 loss: 1.12107239e-06
Iter: 583 loss: 1.12073099e-06
Iter: 584 loss: 1.12010059e-06
Iter: 585 loss: 1.13210399e-06
Iter: 586 loss: 1.12012219e-06
Iter: 587 loss: 1.11945974e-06
Iter: 588 loss: 1.11937538e-06
Iter: 589 loss: 1.11890176e-06
Iter: 590 loss: 1.1184286e-06
Iter: 591 loss: 1.11835243e-06
Iter: 592 loss: 1.11771067e-06
Iter: 593 loss: 1.11759823e-06
Iter: 594 loss: 1.11719828e-06
Iter: 595 loss: 1.11673205e-06
Iter: 596 loss: 1.11691941e-06
Iter: 597 loss: 1.11642385e-06
Iter: 598 loss: 1.11625695e-06
Iter: 599 loss: 1.11613645e-06
Iter: 600 loss: 1.11594977e-06
Iter: 601 loss: 1.11548957e-06
Iter: 602 loss: 1.11974623e-06
Iter: 603 loss: 1.11544898e-06
Iter: 604 loss: 1.11533586e-06
Iter: 605 loss: 1.11522047e-06
Iter: 606 loss: 1.11506631e-06
Iter: 607 loss: 1.11465715e-06
Iter: 608 loss: 1.11801023e-06
Iter: 609 loss: 1.11457621e-06
Iter: 610 loss: 1.11408667e-06
Iter: 611 loss: 1.11898612e-06
Iter: 612 loss: 1.11405484e-06
Iter: 613 loss: 1.11362249e-06
Iter: 614 loss: 1.11477993e-06
Iter: 615 loss: 1.11348402e-06
Iter: 616 loss: 1.11293775e-06
Iter: 617 loss: 1.11302984e-06
Iter: 618 loss: 1.11253894e-06
Iter: 619 loss: 1.11206589e-06
Iter: 620 loss: 1.11211284e-06
Iter: 621 loss: 1.11167151e-06
Iter: 622 loss: 1.11117743e-06
Iter: 623 loss: 1.1159409e-06
Iter: 624 loss: 1.11117902e-06
Iter: 625 loss: 1.11071336e-06
Iter: 626 loss: 1.11644681e-06
Iter: 627 loss: 1.11071586e-06
Iter: 628 loss: 1.11055374e-06
Iter: 629 loss: 1.11017471e-06
Iter: 630 loss: 1.11428835e-06
Iter: 631 loss: 1.11012162e-06
Iter: 632 loss: 1.10997496e-06
Iter: 633 loss: 1.10992119e-06
Iter: 634 loss: 1.1096763e-06
Iter: 635 loss: 1.1090865e-06
Iter: 636 loss: 1.11683323e-06
Iter: 637 loss: 1.10904557e-06
Iter: 638 loss: 1.10879603e-06
Iter: 639 loss: 1.10872168e-06
Iter: 640 loss: 1.10849805e-06
Iter: 641 loss: 1.10795838e-06
Iter: 642 loss: 1.11579789e-06
Iter: 643 loss: 1.10794622e-06
Iter: 644 loss: 1.10744793e-06
Iter: 645 loss: 1.11333293e-06
Iter: 646 loss: 1.10744656e-06
Iter: 647 loss: 1.10698102e-06
Iter: 648 loss: 1.10813698e-06
Iter: 649 loss: 1.10683072e-06
Iter: 650 loss: 1.10631731e-06
Iter: 651 loss: 1.10771896e-06
Iter: 652 loss: 1.10617202e-06
Iter: 653 loss: 1.10571978e-06
Iter: 654 loss: 1.10531323e-06
Iter: 655 loss: 1.1052233e-06
Iter: 656 loss: 1.10456619e-06
Iter: 657 loss: 1.10623137e-06
Iter: 658 loss: 1.10438782e-06
Iter: 659 loss: 1.10380734e-06
Iter: 660 loss: 1.10869e-06
Iter: 661 loss: 1.10379e-06
Iter: 662 loss: 1.10347025e-06
Iter: 663 loss: 1.10290739e-06
Iter: 664 loss: 1.11591203e-06
Iter: 665 loss: 1.10292547e-06
Iter: 666 loss: 1.10233623e-06
Iter: 667 loss: 1.10230076e-06
Iter: 668 loss: 1.10182771e-06
Iter: 669 loss: 1.10211295e-06
Iter: 670 loss: 1.101527e-06
Iter: 671 loss: 1.10127712e-06
Iter: 672 loss: 1.1008251e-06
Iter: 673 loss: 1.10084045e-06
Iter: 674 loss: 1.10042447e-06
Iter: 675 loss: 1.10198528e-06
Iter: 676 loss: 1.10031203e-06
Iter: 677 loss: 1.09972495e-06
Iter: 678 loss: 1.10015947e-06
Iter: 679 loss: 1.09934285e-06
Iter: 680 loss: 1.09903e-06
Iter: 681 loss: 1.10176734e-06
Iter: 682 loss: 1.0990077e-06
Iter: 683 loss: 1.09870552e-06
Iter: 684 loss: 1.09900748e-06
Iter: 685 loss: 1.09852976e-06
Iter: 686 loss: 1.09821974e-06
Iter: 687 loss: 1.09831092e-06
Iter: 688 loss: 1.09798452e-06
Iter: 689 loss: 1.09753501e-06
Iter: 690 loss: 1.09925622e-06
Iter: 691 loss: 1.09741586e-06
Iter: 692 loss: 1.097123e-06
Iter: 693 loss: 1.10081078e-06
Iter: 694 loss: 1.09712778e-06
Iter: 695 loss: 1.09687403e-06
Iter: 696 loss: 1.09619e-06
Iter: 697 loss: 1.1014663e-06
Iter: 698 loss: 1.09605401e-06
Iter: 699 loss: 1.09526718e-06
Iter: 700 loss: 1.09623943e-06
Iter: 701 loss: 1.09486e-06
Iter: 702 loss: 1.09427231e-06
Iter: 703 loss: 1.0942897e-06
Iter: 704 loss: 1.0937872e-06
Iter: 705 loss: 1.09767643e-06
Iter: 706 loss: 1.0937415e-06
Iter: 707 loss: 1.09347911e-06
Iter: 708 loss: 1.09294183e-06
Iter: 709 loss: 1.10279575e-06
Iter: 710 loss: 1.09293228e-06
Iter: 711 loss: 1.09260372e-06
Iter: 712 loss: 1.09253324e-06
Iter: 713 loss: 1.09230791e-06
Iter: 714 loss: 1.09182815e-06
Iter: 715 loss: 1.09868472e-06
Iter: 716 loss: 1.0917795e-06
Iter: 717 loss: 1.09141547e-06
Iter: 718 loss: 1.09137852e-06
Iter: 719 loss: 1.09117036e-06
Iter: 720 loss: 1.0907238e-06
Iter: 721 loss: 1.09769178e-06
Iter: 722 loss: 1.09071982e-06
Iter: 723 loss: 1.09029747e-06
Iter: 724 loss: 1.09527991e-06
Iter: 725 loss: 1.09029952e-06
Iter: 726 loss: 1.0900585e-06
Iter: 727 loss: 1.09025086e-06
Iter: 728 loss: 1.08995334e-06
Iter: 729 loss: 1.08956544e-06
Iter: 730 loss: 1.09016037e-06
Iter: 731 loss: 1.08935274e-06
Iter: 732 loss: 1.08902691e-06
Iter: 733 loss: 1.08863844e-06
Iter: 734 loss: 1.08860104e-06
Iter: 735 loss: 1.08807944e-06
Iter: 736 loss: 1.09352072e-06
Iter: 737 loss: 1.08804602e-06
Iter: 738 loss: 1.08772201e-06
Iter: 739 loss: 1.0905037e-06
Iter: 740 loss: 1.08772349e-06
Iter: 741 loss: 1.08745655e-06
Iter: 742 loss: 1.08696156e-06
Iter: 743 loss: 1.09788846e-06
Iter: 744 loss: 1.08697225e-06
Iter: 745 loss: 1.0867692e-06
Iter: 746 loss: 1.08669042e-06
Iter: 747 loss: 1.086461e-06
Iter: 748 loss: 1.08595168e-06
Iter: 749 loss: 1.09518055e-06
Iter: 750 loss: 1.08595304e-06
Iter: 751 loss: 1.08566621e-06
Iter: 752 loss: 1.08565655e-06
Iter: 753 loss: 1.08535505e-06
Iter: 754 loss: 1.0847898e-06
Iter: 755 loss: 1.09655787e-06
Iter: 756 loss: 1.08476854e-06
Iter: 757 loss: 1.08423978e-06
Iter: 758 loss: 1.09065172e-06
Iter: 759 loss: 1.08424626e-06
Iter: 760 loss: 1.08372967e-06
Iter: 761 loss: 1.08528445e-06
Iter: 762 loss: 1.08357426e-06
Iter: 763 loss: 1.08303561e-06
Iter: 764 loss: 1.08387599e-06
Iter: 765 loss: 1.082783e-06
Iter: 766 loss: 1.08227732e-06
Iter: 767 loss: 1.08219729e-06
Iter: 768 loss: 1.08186384e-06
Iter: 769 loss: 1.08134657e-06
Iter: 770 loss: 1.08834502e-06
Iter: 771 loss: 1.08135578e-06
Iter: 772 loss: 1.08105473e-06
Iter: 773 loss: 1.08287611e-06
Iter: 774 loss: 1.08102483e-06
Iter: 775 loss: 1.08073095e-06
Iter: 776 loss: 1.08044969e-06
Iter: 777 loss: 1.08038785e-06
Iter: 778 loss: 1.08016684e-06
Iter: 779 loss: 1.08015979e-06
Iter: 780 loss: 1.07990024e-06
Iter: 781 loss: 1.07960068e-06
Iter: 782 loss: 1.07957464e-06
Iter: 783 loss: 1.07930225e-06
Iter: 784 loss: 1.08225515e-06
Iter: 785 loss: 1.07926985e-06
Iter: 786 loss: 1.07897517e-06
Iter: 787 loss: 1.07868527e-06
Iter: 788 loss: 1.07861797e-06
Iter: 789 loss: 1.07820097e-06
Iter: 790 loss: 1.0786315e-06
Iter: 791 loss: 1.07796484e-06
Iter: 792 loss: 1.07744086e-06
Iter: 793 loss: 1.08314759e-06
Iter: 794 loss: 1.07742108e-06
Iter: 795 loss: 1.07705682e-06
Iter: 796 loss: 1.07736719e-06
Iter: 797 loss: 1.07687174e-06
Iter: 798 loss: 1.07646974e-06
Iter: 799 loss: 1.07638755e-06
Iter: 800 loss: 1.07612425e-06
Iter: 801 loss: 1.0757326e-06
Iter: 802 loss: 1.08053769e-06
Iter: 803 loss: 1.07573419e-06
Iter: 804 loss: 1.07547862e-06
Iter: 805 loss: 1.07685401e-06
Iter: 806 loss: 1.07547271e-06
Iter: 807 loss: 1.07519054e-06
Iter: 808 loss: 1.07511084e-06
Iter: 809 loss: 1.07494736e-06
Iter: 810 loss: 1.07471453e-06
Iter: 811 loss: 1.07616052e-06
Iter: 812 loss: 1.07469236e-06
Iter: 813 loss: 1.07433948e-06
Iter: 814 loss: 1.07473443e-06
Iter: 815 loss: 1.07416031e-06
Iter: 816 loss: 1.07393839e-06
Iter: 817 loss: 1.07456844e-06
Iter: 818 loss: 1.0738438e-06
Iter: 819 loss: 1.07348467e-06
Iter: 820 loss: 1.07405185e-06
Iter: 821 loss: 1.07331016e-06
Iter: 822 loss: 1.0730281e-06
Iter: 823 loss: 1.07276492e-06
Iter: 824 loss: 1.07270171e-06
Iter: 825 loss: 1.07233586e-06
Iter: 826 loss: 1.07233654e-06
Iter: 827 loss: 1.07208871e-06
Iter: 828 loss: 1.07217693e-06
Iter: 829 loss: 1.07190795e-06
Iter: 830 loss: 1.07160486e-06
Iter: 831 loss: 1.07172923e-06
Iter: 832 loss: 1.07139545e-06
Iter: 833 loss: 1.07111077e-06
Iter: 834 loss: 1.07309813e-06
Iter: 835 loss: 1.07106666e-06
Iter: 836 loss: 1.07084588e-06
Iter: 837 loss: 1.07281107e-06
Iter: 838 loss: 1.07081644e-06
Iter: 839 loss: 1.07062681e-06
Iter: 840 loss: 1.07055598e-06
Iter: 841 loss: 1.07044389e-06
Iter: 842 loss: 1.07018377e-06
Iter: 843 loss: 1.0706492e-06
Iter: 844 loss: 1.07011408e-06
Iter: 845 loss: 1.06975733e-06
Iter: 846 loss: 1.07115943e-06
Iter: 847 loss: 1.06966354e-06
Iter: 848 loss: 1.06942139e-06
Iter: 849 loss: 1.06921902e-06
Iter: 850 loss: 1.06913672e-06
Iter: 851 loss: 1.06863297e-06
Iter: 852 loss: 1.07241453e-06
Iter: 853 loss: 1.06859193e-06
Iter: 854 loss: 1.0683425e-06
Iter: 855 loss: 1.06788571e-06
Iter: 856 loss: 1.06788e-06
Iter: 857 loss: 1.0676531e-06
Iter: 858 loss: 1.06757693e-06
Iter: 859 loss: 1.06735411e-06
Iter: 860 loss: 1.06730613e-06
Iter: 861 loss: 1.0671838e-06
Iter: 862 loss: 1.06692698e-06
Iter: 863 loss: 1.06741209e-06
Iter: 864 loss: 1.06676907e-06
Iter: 865 loss: 1.06647985e-06
Iter: 866 loss: 1.06714469e-06
Iter: 867 loss: 1.06639914e-06
Iter: 868 loss: 1.06616085e-06
Iter: 869 loss: 1.06920618e-06
Iter: 870 loss: 1.0661654e-06
Iter: 871 loss: 1.06594644e-06
Iter: 872 loss: 1.06580671e-06
Iter: 873 loss: 1.06571247e-06
Iter: 874 loss: 1.06544303e-06
Iter: 875 loss: 1.06555342e-06
Iter: 876 loss: 1.06523362e-06
Iter: 877 loss: 1.06481116e-06
Iter: 878 loss: 1.07018764e-06
Iter: 879 loss: 1.06482707e-06
Iter: 880 loss: 1.06460118e-06
Iter: 881 loss: 1.06429377e-06
Iter: 882 loss: 1.06428388e-06
Iter: 883 loss: 1.06392952e-06
Iter: 884 loss: 1.06962125e-06
Iter: 885 loss: 1.06392156e-06
Iter: 886 loss: 1.06374307e-06
Iter: 887 loss: 1.06334369e-06
Iter: 888 loss: 1.07068786e-06
Iter: 889 loss: 1.06334687e-06
Iter: 890 loss: 1.06305129e-06
Iter: 891 loss: 1.0630414e-06
Iter: 892 loss: 1.06284642e-06
Iter: 893 loss: 1.06260836e-06
Iter: 894 loss: 1.06258585e-06
Iter: 895 loss: 1.06224638e-06
Iter: 896 loss: 1.06357345e-06
Iter: 897 loss: 1.06219898e-06
Iter: 898 loss: 1.06186565e-06
Iter: 899 loss: 1.06217612e-06
Iter: 900 loss: 1.0616593e-06
Iter: 901 loss: 1.06131029e-06
Iter: 902 loss: 1.06485379e-06
Iter: 903 loss: 1.06127891e-06
Iter: 904 loss: 1.06098491e-06
Iter: 905 loss: 1.06086986e-06
Iter: 906 loss: 1.06072287e-06
Iter: 907 loss: 1.06025277e-06
Iter: 908 loss: 1.06013783e-06
Iter: 909 loss: 1.05987874e-06
Iter: 910 loss: 1.05944321e-06
Iter: 911 loss: 1.05941831e-06
Iter: 912 loss: 1.05914535e-06
Iter: 913 loss: 1.05876927e-06
Iter: 914 loss: 1.05876893e-06
Iter: 915 loss: 1.05852973e-06
Iter: 916 loss: 1.05848517e-06
Iter: 917 loss: 1.05831236e-06
Iter: 918 loss: 1.05792969e-06
Iter: 919 loss: 1.06215361e-06
Iter: 920 loss: 1.05789638e-06
Iter: 921 loss: 1.05761387e-06
Iter: 922 loss: 1.05758431e-06
Iter: 923 loss: 1.05741174e-06
Iter: 924 loss: 1.05712695e-06
Iter: 925 loss: 1.0571398e-06
Iter: 926 loss: 1.05674349e-06
Iter: 927 loss: 1.05784807e-06
Iter: 928 loss: 1.05663935e-06
Iter: 929 loss: 1.0562344e-06
Iter: 930 loss: 1.0560384e-06
Iter: 931 loss: 1.05583263e-06
Iter: 932 loss: 1.0553606e-06
Iter: 933 loss: 1.05534127e-06
Iter: 934 loss: 1.05506331e-06
Iter: 935 loss: 1.05478512e-06
Iter: 936 loss: 1.05472645e-06
Iter: 937 loss: 1.05421191e-06
Iter: 938 loss: 1.05513595e-06
Iter: 939 loss: 1.05398976e-06
Iter: 940 loss: 1.05388335e-06
Iter: 941 loss: 1.05374056e-06
Iter: 942 loss: 1.05350443e-06
Iter: 943 loss: 1.05300796e-06
Iter: 944 loss: 1.05905281e-06
Iter: 945 loss: 1.05297499e-06
Iter: 946 loss: 1.05272898e-06
Iter: 947 loss: 1.05269692e-06
Iter: 948 loss: 1.05241611e-06
Iter: 949 loss: 1.05186746e-06
Iter: 950 loss: 1.06177777e-06
Iter: 951 loss: 1.05184427e-06
Iter: 952 loss: 1.05163895e-06
Iter: 953 loss: 1.05155266e-06
Iter: 954 loss: 1.05131937e-06
Iter: 955 loss: 1.05104687e-06
Iter: 956 loss: 1.0510023e-06
Iter: 957 loss: 1.05068455e-06
Iter: 958 loss: 1.05208483e-06
Iter: 959 loss: 1.05062304e-06
Iter: 960 loss: 1.05021093e-06
Iter: 961 loss: 1.04978039e-06
Iter: 962 loss: 1.04973344e-06
Iter: 963 loss: 1.04954233e-06
Iter: 964 loss: 1.04943604e-06
Iter: 965 loss: 1.04918831e-06
Iter: 966 loss: 1.04919832e-06
Iter: 967 loss: 1.04899482e-06
Iter: 968 loss: 1.04871492e-06
Iter: 969 loss: 1.04845515e-06
Iter: 970 loss: 1.04837284e-06
Iter: 971 loss: 1.04797084e-06
Iter: 972 loss: 1.04817548e-06
Iter: 973 loss: 1.0476831e-06
Iter: 974 loss: 1.04697574e-06
Iter: 975 loss: 1.05118659e-06
Iter: 976 loss: 1.04688252e-06
Iter: 977 loss: 1.04650417e-06
Iter: 978 loss: 1.04690321e-06
Iter: 979 loss: 1.04631044e-06
Iter: 980 loss: 1.04613946e-06
Iter: 981 loss: 1.04611877e-06
Iter: 982 loss: 1.0459205e-06
Iter: 983 loss: 1.04560218e-06
Iter: 984 loss: 1.04559024e-06
Iter: 985 loss: 1.04542141e-06
Iter: 986 loss: 1.04539595e-06
Iter: 987 loss: 1.0452278e-06
Iter: 988 loss: 1.04485321e-06
Iter: 989 loss: 1.05051117e-06
Iter: 990 loss: 1.04483934e-06
Iter: 991 loss: 1.04431615e-06
Iter: 992 loss: 1.04441483e-06
Iter: 993 loss: 1.04390506e-06
Iter: 994 loss: 1.04303672e-06
Iter: 995 loss: 1.04514902e-06
Iter: 996 loss: 1.04274159e-06
Iter: 997 loss: 1.04232458e-06
Iter: 998 loss: 1.04494598e-06
Iter: 999 loss: 1.04226444e-06
Iter: 1000 loss: 1.04189598e-06
Iter: 1001 loss: 1.04707533e-06
Iter: 1002 loss: 1.04191042e-06
Iter: 1003 loss: 1.04174478e-06
Iter: 1004 loss: 1.04125479e-06
Iter: 1005 loss: 1.04456876e-06
Iter: 1006 loss: 1.04118396e-06
Iter: 1007 loss: 1.04095261e-06
Iter: 1008 loss: 1.04094249e-06
Iter: 1009 loss: 1.04065725e-06
Iter: 1010 loss: 1.04136802e-06
Iter: 1011 loss: 1.04058813e-06
Iter: 1012 loss: 1.04033779e-06
Iter: 1013 loss: 1.04017954e-06
Iter: 1014 loss: 1.04010235e-06
Iter: 1015 loss: 1.03991135e-06
Iter: 1016 loss: 1.03989703e-06
Iter: 1017 loss: 1.03976652e-06
Iter: 1018 loss: 1.03974662e-06
Iter: 1019 loss: 1.03967955e-06
Iter: 1020 loss: 1.03942546e-06
Iter: 1021 loss: 1.03935281e-06
Iter: 1022 loss: 1.03919785e-06
Iter: 1023 loss: 1.03882621e-06
Iter: 1024 loss: 1.03879324e-06
Iter: 1025 loss: 1.038547e-06
Iter: 1026 loss: 1.03813841e-06
Iter: 1027 loss: 1.04144749e-06
Iter: 1028 loss: 1.0380902e-06
Iter: 1029 loss: 1.03778257e-06
Iter: 1030 loss: 1.03757964e-06
Iter: 1031 loss: 1.03745697e-06
Iter: 1032 loss: 1.03742241e-06
Iter: 1033 loss: 1.03724631e-06
Iter: 1034 loss: 1.03711e-06
Iter: 1035 loss: 1.03691673e-06
Iter: 1036 loss: 1.03690445e-06
Iter: 1037 loss: 1.03674438e-06
Iter: 1038 loss: 1.03676825e-06
Iter: 1039 loss: 1.03664195e-06
Iter: 1040 loss: 1.03634193e-06
Iter: 1041 loss: 1.03731509e-06
Iter: 1042 loss: 1.03624654e-06
Iter: 1043 loss: 1.0360036e-06
Iter: 1044 loss: 1.03598654e-06
Iter: 1045 loss: 1.03580146e-06
Iter: 1046 loss: 1.03549405e-06
Iter: 1047 loss: 1.0385204e-06
Iter: 1048 loss: 1.03547359e-06
Iter: 1049 loss: 1.03520961e-06
Iter: 1050 loss: 1.03532409e-06
Iter: 1051 loss: 1.03504055e-06
Iter: 1052 loss: 1.03469392e-06
Iter: 1053 loss: 1.03711534e-06
Iter: 1054 loss: 1.03465982e-06
Iter: 1055 loss: 1.0344429e-06
Iter: 1056 loss: 1.03402567e-06
Iter: 1057 loss: 1.04297089e-06
Iter: 1058 loss: 1.03402056e-06
Iter: 1059 loss: 1.03366517e-06
Iter: 1060 loss: 1.03703201e-06
Iter: 1061 loss: 1.03365483e-06
Iter: 1062 loss: 1.03338459e-06
Iter: 1063 loss: 1.03335367e-06
Iter: 1064 loss: 1.03317439e-06
Iter: 1065 loss: 1.03292223e-06
Iter: 1066 loss: 1.03686614e-06
Iter: 1067 loss: 1.0329137e-06
Iter: 1068 loss: 1.03262937e-06
Iter: 1069 loss: 1.03274124e-06
Iter: 1070 loss: 1.03240131e-06
Iter: 1071 loss: 1.03220123e-06
Iter: 1072 loss: 1.03237676e-06
Iter: 1073 loss: 1.03206548e-06
Iter: 1074 loss: 1.03176933e-06
Iter: 1075 loss: 1.03468301e-06
Iter: 1076 loss: 1.03175421e-06
Iter: 1077 loss: 1.03154571e-06
Iter: 1078 loss: 1.03116031e-06
Iter: 1079 loss: 1.03118543e-06
Iter: 1080 loss: 1.03087984e-06
Iter: 1081 loss: 1.03087109e-06
Iter: 1082 loss: 1.03065179e-06
Iter: 1083 loss: 1.03095806e-06
Iter: 1084 loss: 1.03054322e-06
Iter: 1085 loss: 1.03034972e-06
Iter: 1086 loss: 1.03149569e-06
Iter: 1087 loss: 1.03031891e-06
Iter: 1088 loss: 1.03013167e-06
Iter: 1089 loss: 1.02977799e-06
Iter: 1090 loss: 1.03602349e-06
Iter: 1091 loss: 1.02978845e-06
Iter: 1092 loss: 1.02951503e-06
Iter: 1093 loss: 1.03155287e-06
Iter: 1094 loss: 1.02949571e-06
Iter: 1095 loss: 1.02925196e-06
Iter: 1096 loss: 1.03000389e-06
Iter: 1097 loss: 1.02918534e-06
Iter: 1098 loss: 1.02895808e-06
Iter: 1099 loss: 1.02901959e-06
Iter: 1100 loss: 1.02880176e-06
Iter: 1101 loss: 1.02840522e-06
Iter: 1102 loss: 1.03170305e-06
Iter: 1103 loss: 1.02838737e-06
Iter: 1104 loss: 1.02820127e-06
Iter: 1105 loss: 1.02784838e-06
Iter: 1106 loss: 1.03432376e-06
Iter: 1107 loss: 1.02783e-06
Iter: 1108 loss: 1.02759736e-06
Iter: 1109 loss: 1.0275503e-06
Iter: 1110 loss: 1.02736044e-06
Iter: 1111 loss: 1.02699573e-06
Iter: 1112 loss: 1.02700074e-06
Iter: 1113 loss: 1.02671834e-06
Iter: 1114 loss: 1.02861236e-06
Iter: 1115 loss: 1.02667593e-06
Iter: 1116 loss: 1.02639706e-06
Iter: 1117 loss: 1.02776335e-06
Iter: 1118 loss: 1.02636409e-06
Iter: 1119 loss: 1.02614467e-06
Iter: 1120 loss: 1.02667741e-06
Iter: 1121 loss: 1.02605441e-06
Iter: 1122 loss: 1.02592094e-06
Iter: 1123 loss: 1.02560648e-06
Iter: 1124 loss: 1.03248726e-06
Iter: 1125 loss: 1.02560921e-06
Iter: 1126 loss: 1.02518425e-06
Iter: 1127 loss: 1.02498109e-06
Iter: 1128 loss: 1.02478793e-06
Iter: 1129 loss: 1.02417516e-06
Iter: 1130 loss: 1.03007346e-06
Iter: 1131 loss: 1.02415527e-06
Iter: 1132 loss: 1.02375964e-06
Iter: 1133 loss: 1.02380818e-06
Iter: 1134 loss: 1.02348463e-06
Iter: 1135 loss: 1.02335957e-06
Iter: 1136 loss: 1.02323543e-06
Iter: 1137 loss: 1.02305125e-06
Iter: 1138 loss: 1.02261959e-06
Iter: 1139 loss: 1.03033574e-06
Iter: 1140 loss: 1.02259332e-06
Iter: 1141 loss: 1.02236572e-06
Iter: 1142 loss: 1.02237163e-06
Iter: 1143 loss: 1.02213914e-06
Iter: 1144 loss: 1.02208833e-06
Iter: 1145 loss: 1.02192621e-06
Iter: 1146 loss: 1.02169338e-06
Iter: 1147 loss: 1.02269587e-06
Iter: 1148 loss: 1.0216437e-06
Iter: 1149 loss: 1.0213879e-06
Iter: 1150 loss: 1.02231388e-06
Iter: 1151 loss: 1.02133868e-06
Iter: 1152 loss: 1.02114814e-06
Iter: 1153 loss: 1.02283059e-06
Iter: 1154 loss: 1.02114564e-06
Iter: 1155 loss: 1.02103058e-06
Iter: 1156 loss: 1.02089712e-06
Iter: 1157 loss: 1.02087517e-06
Iter: 1158 loss: 1.02067543e-06
Iter: 1159 loss: 1.02060642e-06
Iter: 1160 loss: 1.02051342e-06
Iter: 1161 loss: 1.02017361e-06
Iter: 1162 loss: 1.02054014e-06
Iter: 1163 loss: 1.01999694e-06
Iter: 1164 loss: 1.01955527e-06
Iter: 1165 loss: 1.02116019e-06
Iter: 1166 loss: 1.0194226e-06
Iter: 1167 loss: 1.01908711e-06
Iter: 1168 loss: 1.02072022e-06
Iter: 1169 loss: 1.01901969e-06
Iter: 1170 loss: 1.01865e-06
Iter: 1171 loss: 1.02082595e-06
Iter: 1172 loss: 1.01858086e-06
Iter: 1173 loss: 1.01839407e-06
Iter: 1174 loss: 1.01813544e-06
Iter: 1175 loss: 1.01813794e-06
Iter: 1176 loss: 1.01784553e-06
Iter: 1177 loss: 1.01783939e-06
Iter: 1178 loss: 1.01769069e-06
Iter: 1179 loss: 1.0174748e-06
Iter: 1180 loss: 1.0174623e-06
Iter: 1181 loss: 1.0171226e-06
Iter: 1182 loss: 1.01913656e-06
Iter: 1183 loss: 1.01708963e-06
Iter: 1184 loss: 1.01685532e-06
Iter: 1185 loss: 1.01825094e-06
Iter: 1186 loss: 1.01684282e-06
Iter: 1187 loss: 1.01662033e-06
Iter: 1188 loss: 1.01692342e-06
Iter: 1189 loss: 1.0165212e-06
Iter: 1190 loss: 1.01632145e-06
Iter: 1191 loss: 1.01598403e-06
Iter: 1192 loss: 1.0159763e-06
Iter: 1193 loss: 1.01553212e-06
Iter: 1194 loss: 1.01642922e-06
Iter: 1195 loss: 1.01537739e-06
Iter: 1196 loss: 1.01492242e-06
Iter: 1197 loss: 1.01753176e-06
Iter: 1198 loss: 1.01484545e-06
Iter: 1199 loss: 1.0145252e-06
Iter: 1200 loss: 1.01539786e-06
Iter: 1201 loss: 1.01443447e-06
Iter: 1202 loss: 1.0142802e-06
Iter: 1203 loss: 1.01425155e-06
Iter: 1204 loss: 1.01413195e-06
Iter: 1205 loss: 1.01384251e-06
Iter: 1206 loss: 1.01700675e-06
Iter: 1207 loss: 1.01380078e-06
Iter: 1208 loss: 1.01372541e-06
Iter: 1209 loss: 1.01364833e-06
Iter: 1210 loss: 1.01353487e-06
Iter: 1211 loss: 1.01338105e-06
Iter: 1212 loss: 1.01337037e-06
Iter: 1213 loss: 1.01321143e-06
Iter: 1214 loss: 1.01501109e-06
Iter: 1215 loss: 1.01320927e-06
Iter: 1216 loss: 1.01307421e-06
Iter: 1217 loss: 1.01305989e-06
Iter: 1218 loss: 1.01294313e-06
Iter: 1219 loss: 1.01275339e-06
Iter: 1220 loss: 1.01473825e-06
Iter: 1221 loss: 1.0127427e-06
Iter: 1222 loss: 1.0126198e-06
Iter: 1223 loss: 1.0125093e-06
Iter: 1224 loss: 1.01247804e-06
Iter: 1225 loss: 1.01226908e-06
Iter: 1226 loss: 1.01205728e-06
Iter: 1227 loss: 1.01200931e-06
Iter: 1228 loss: 1.01163664e-06
Iter: 1229 loss: 1.01512796e-06
Iter: 1230 loss: 1.01164062e-06
Iter: 1231 loss: 1.01138824e-06
Iter: 1232 loss: 1.01209253e-06
Iter: 1233 loss: 1.01132287e-06
Iter: 1234 loss: 1.01114767e-06
Iter: 1235 loss: 1.01335058e-06
Iter: 1236 loss: 1.01111698e-06
Iter: 1237 loss: 1.01095861e-06
Iter: 1238 loss: 1.01082014e-06
Iter: 1239 loss: 1.01077046e-06
Iter: 1240 loss: 1.01057867e-06
Iter: 1241 loss: 1.01191256e-06
Iter: 1242 loss: 1.0105781e-06
Iter: 1243 loss: 1.0103887e-06
Iter: 1244 loss: 1.01054991e-06
Iter: 1245 loss: 1.01026762e-06
Iter: 1246 loss: 1.01012517e-06
Iter: 1247 loss: 1.01039018e-06
Iter: 1248 loss: 1.01006708e-06
Iter: 1249 loss: 1.00982982e-06
Iter: 1250 loss: 1.01029377e-06
Iter: 1251 loss: 1.00974239e-06
Iter: 1252 loss: 1.00955867e-06
Iter: 1253 loss: 1.01149908e-06
Iter: 1254 loss: 1.00955504e-06
Iter: 1255 loss: 1.00937348e-06
Iter: 1256 loss: 1.00934528e-06
Iter: 1257 loss: 1.00924956e-06
Iter: 1258 loss: 1.00905675e-06
Iter: 1259 loss: 1.00889577e-06
Iter: 1260 loss: 1.00885359e-06
Iter: 1261 loss: 1.00858279e-06
Iter: 1262 loss: 1.00969589e-06
Iter: 1263 loss: 1.00852822e-06
Iter: 1264 loss: 1.00829243e-06
Iter: 1265 loss: 1.00946045e-06
Iter: 1266 loss: 1.0082465e-06
Iter: 1267 loss: 1.00808597e-06
Iter: 1268 loss: 1.0091876e-06
Iter: 1269 loss: 1.00808234e-06
Iter: 1270 loss: 1.00792795e-06
Iter: 1271 loss: 1.00870284e-06
Iter: 1272 loss: 1.00790385e-06
Iter: 1273 loss: 1.00780562e-06
Iter: 1274 loss: 1.00771763e-06
Iter: 1275 loss: 1.00771695e-06
Iter: 1276 loss: 1.0075446e-06
Iter: 1277 loss: 1.00889906e-06
Iter: 1278 loss: 1.00752447e-06
Iter: 1279 loss: 1.00742329e-06
Iter: 1280 loss: 1.00726481e-06
Iter: 1281 loss: 1.00726118e-06
Iter: 1282 loss: 1.00708326e-06
Iter: 1283 loss: 1.00976513e-06
Iter: 1284 loss: 1.00708894e-06
Iter: 1285 loss: 1.00693785e-06
Iter: 1286 loss: 1.00695183e-06
Iter: 1287 loss: 1.0068577e-06
Iter: 1288 loss: 1.00663465e-06
Iter: 1289 loss: 1.00736679e-06
Iter: 1290 loss: 1.00655302e-06
Iter: 1291 loss: 1.00640909e-06
Iter: 1292 loss: 1.00631746e-06
Iter: 1293 loss: 1.00625266e-06
Iter: 1294 loss: 1.0060412e-06
Iter: 1295 loss: 1.00608611e-06
Iter: 1296 loss: 1.00587556e-06
Iter: 1297 loss: 1.00561329e-06
Iter: 1298 loss: 1.00785712e-06
Iter: 1299 loss: 1.00559976e-06
Iter: 1300 loss: 1.00538955e-06
Iter: 1301 loss: 1.006405e-06
Iter: 1302 loss: 1.00536977e-06
Iter: 1303 loss: 1.00521152e-06
Iter: 1304 loss: 1.00669445e-06
Iter: 1305 loss: 1.00522925e-06
Iter: 1306 loss: 1.00508555e-06
Iter: 1307 loss: 1.00496379e-06
Iter: 1308 loss: 1.00494242e-06
Iter: 1309 loss: 1.0047994e-06
Iter: 1310 loss: 1.00479349e-06
Iter: 1311 loss: 1.00466707e-06
Iter: 1312 loss: 1.00442708e-06
Iter: 1313 loss: 1.00771672e-06
Iter: 1314 loss: 1.00442662e-06
Iter: 1315 loss: 1.00424973e-06
Iter: 1316 loss: 1.00424381e-06
Iter: 1317 loss: 1.00409261e-06
Iter: 1318 loss: 1.00385796e-06
Iter: 1319 loss: 1.00383352e-06
Iter: 1320 loss: 1.00357283e-06
Iter: 1321 loss: 1.00357715e-06
Iter: 1322 loss: 1.00342e-06
Iter: 1323 loss: 1.00300349e-06
Iter: 1324 loss: 1.00831176e-06
Iter: 1325 loss: 1.00298325e-06
Iter: 1326 loss: 1.00257557e-06
Iter: 1327 loss: 1.00444e-06
Iter: 1328 loss: 1.0025019e-06
Iter: 1329 loss: 1.00227498e-06
Iter: 1330 loss: 1.00354475e-06
Iter: 1331 loss: 1.00222235e-06
Iter: 1332 loss: 1.00208399e-06
Iter: 1333 loss: 1.00259285e-06
Iter: 1334 loss: 1.00204966e-06
Iter: 1335 loss: 1.00192051e-06
Iter: 1336 loss: 1.00198906e-06
Iter: 1337 loss: 1.00183206e-06
Iter: 1338 loss: 1.00168825e-06
Iter: 1339 loss: 1.0021871e-06
Iter: 1340 loss: 1.00163402e-06
Iter: 1341 loss: 1.00149146e-06
Iter: 1342 loss: 1.00203965e-06
Iter: 1343 loss: 1.00144712e-06
Iter: 1344 loss: 1.0012468e-06
Iter: 1345 loss: 1.00114835e-06
Iter: 1346 loss: 1.00106649e-06
Iter: 1347 loss: 1.00084605e-06
Iter: 1348 loss: 1.00133718e-06
Iter: 1349 loss: 1.00078876e-06
Iter: 1350 loss: 1.00053444e-06
Iter: 1351 loss: 1.00229147e-06
Iter: 1352 loss: 1.000515e-06
Iter: 1353 loss: 1.00039949e-06
Iter: 1354 loss: 1.00117461e-06
Iter: 1355 loss: 1.00038801e-06
Iter: 1356 loss: 1.00024511e-06
Iter: 1357 loss: 1.0000274e-06
Iter: 1358 loss: 1.00567195e-06
Iter: 1359 loss: 1.0000274e-06
Iter: 1360 loss: 9.99802069e-07
Iter: 1361 loss: 1.00020884e-06
Iter: 1362 loss: 9.99731e-07
Iter: 1363 loss: 9.99504891e-07
Iter: 1364 loss: 9.99861072e-07
Iter: 1365 loss: 9.99391432e-07
Iter: 1366 loss: 9.99257168e-07
Iter: 1367 loss: 9.99245e-07
Iter: 1368 loss: 9.99124e-07
Iter: 1369 loss: 9.99110057e-07
Iter: 1370 loss: 9.99007625e-07
Iter: 1371 loss: 9.98851533e-07
Iter: 1372 loss: 9.99580152e-07
Iter: 1373 loss: 9.98815381e-07
Iter: 1374 loss: 9.98690211e-07
Iter: 1375 loss: 9.99142344e-07
Iter: 1376 loss: 9.9864485e-07
Iter: 1377 loss: 9.98523774e-07
Iter: 1378 loss: 9.98797e-07
Iter: 1379 loss: 9.98457267e-07
Iter: 1380 loss: 9.98282303e-07
Iter: 1381 loss: 9.97994675e-07
Iter: 1382 loss: 9.98015139e-07
Iter: 1383 loss: 9.978055e-07
Iter: 1384 loss: 9.97798679e-07
Iter: 1385 loss: 9.97630877e-07
Iter: 1386 loss: 9.97472853e-07
Iter: 1387 loss: 9.97449206e-07
Iter: 1388 loss: 9.97182156e-07
Iter: 1389 loss: 1.00016541e-06
Iter: 1390 loss: 9.97174538e-07
Iter: 1391 loss: 9.97065172e-07
Iter: 1392 loss: 9.96876e-07
Iter: 1393 loss: 9.96884182e-07
Iter: 1394 loss: 9.9670774e-07
Iter: 1395 loss: 9.972523e-07
Iter: 1396 loss: 9.96649305e-07
Iter: 1397 loss: 9.96516519e-07
Iter: 1398 loss: 9.96835638e-07
Iter: 1399 loss: 9.96469794e-07
Iter: 1400 loss: 9.96296e-07
Iter: 1401 loss: 9.97587449e-07
Iter: 1402 loss: 9.96281869e-07
Iter: 1403 loss: 9.96152153e-07
Iter: 1404 loss: 9.95862138e-07
Iter: 1405 loss: 1.00117938e-06
Iter: 1406 loss: 9.95870096e-07
Iter: 1407 loss: 9.95535743e-07
Iter: 1408 loss: 9.98178e-07
Iter: 1409 loss: 9.95506753e-07
Iter: 1410 loss: 9.9529916e-07
Iter: 1411 loss: 9.96346216e-07
Iter: 1412 loss: 9.95269829e-07
Iter: 1413 loss: 9.9506542e-07
Iter: 1414 loss: 9.9495287e-07
Iter: 1415 loss: 9.94874426e-07
Iter: 1416 loss: 9.94660468e-07
Iter: 1417 loss: 9.96597237e-07
Iter: 1418 loss: 9.94662628e-07
Iter: 1419 loss: 9.94466518e-07
Iter: 1420 loss: 9.94860784e-07
Iter: 1421 loss: 9.9441877e-07
Iter: 1422 loss: 9.94326e-07
Iter: 1423 loss: 9.94327593e-07
Iter: 1424 loss: 9.94240509e-07
Iter: 1425 loss: 9.94061224e-07
Iter: 1426 loss: 9.95110895e-07
Iter: 1427 loss: 9.94011316e-07
Iter: 1428 loss: 9.93750177e-07
Iter: 1429 loss: 9.9435772e-07
Iter: 1430 loss: 9.9364e-07
Iter: 1431 loss: 9.93327831e-07
Iter: 1432 loss: 9.93495519e-07
Iter: 1433 loss: 9.93123535e-07
Iter: 1434 loss: 9.92874675e-07
Iter: 1435 loss: 9.96163749e-07
Iter: 1436 loss: 9.9286251e-07
Iter: 1437 loss: 9.92605692e-07
Iter: 1438 loss: 9.93610456e-07
Iter: 1439 loss: 9.92539753e-07
Iter: 1440 loss: 9.92398782e-07
Iter: 1441 loss: 9.92707783e-07
Iter: 1442 loss: 9.92332616e-07
Iter: 1443 loss: 9.92171408e-07
Iter: 1444 loss: 9.92664241e-07
Iter: 1445 loss: 9.92122523e-07
Iter: 1446 loss: 9.91946649e-07
Iter: 1447 loss: 9.92288e-07
Iter: 1448 loss: 9.91876504e-07
Iter: 1449 loss: 9.91695288e-07
Iter: 1450 loss: 9.91742809e-07
Iter: 1451 loss: 9.9155227e-07
Iter: 1452 loss: 9.91347292e-07
Iter: 1453 loss: 9.93530648e-07
Iter: 1454 loss: 9.9132194e-07
Iter: 1455 loss: 9.91185289e-07
Iter: 1456 loss: 9.91459501e-07
Iter: 1457 loss: 9.91126853e-07
Iter: 1458 loss: 9.90986e-07
Iter: 1459 loss: 9.91758e-07
Iter: 1460 loss: 9.90948593e-07
Iter: 1461 loss: 9.90860144e-07
Iter: 1462 loss: 9.90587523e-07
Iter: 1463 loss: 9.93059189e-07
Iter: 1464 loss: 9.90563876e-07
Iter: 1465 loss: 9.90207354e-07
Iter: 1466 loss: 9.91975867e-07
Iter: 1467 loss: 9.90167e-07
Iter: 1468 loss: 9.89934506e-07
Iter: 1469 loss: 9.89651539e-07
Iter: 1470 loss: 9.89638579e-07
Iter: 1471 loss: 9.89488285e-07
Iter: 1472 loss: 9.89378577e-07
Iter: 1473 loss: 9.89160753e-07
Iter: 1474 loss: 9.88993065e-07
Iter: 1475 loss: 9.88914167e-07
Iter: 1476 loss: 9.88737497e-07
Iter: 1477 loss: 9.88743295e-07
Iter: 1478 loss: 9.8864632e-07
Iter: 1479 loss: 9.88531269e-07
Iter: 1480 loss: 9.88523198e-07
Iter: 1481 loss: 9.8835244e-07
Iter: 1482 loss: 9.88662e-07
Iter: 1483 loss: 9.88270358e-07
Iter: 1484 loss: 9.88143256e-07
Iter: 1485 loss: 9.88148827e-07
Iter: 1486 loss: 9.88049123e-07
Iter: 1487 loss: 9.87904059e-07
Iter: 1488 loss: 9.87890871e-07
Iter: 1489 loss: 9.87763769e-07
Iter: 1490 loss: 9.89677574e-07
Iter: 1491 loss: 9.87760359e-07
Iter: 1492 loss: 9.87578119e-07
Iter: 1493 loss: 9.87290832e-07
Iter: 1494 loss: 9.9459e-07
Iter: 1495 loss: 9.87295607e-07
Iter: 1496 loss: 9.86960458e-07
Iter: 1497 loss: 9.87046178e-07
Iter: 1498 loss: 9.86688519e-07
Iter: 1499 loss: 9.86281066e-07
Iter: 1500 loss: 9.87948852e-07
Iter: 1501 loss: 9.86175564e-07
Iter: 1502 loss: 9.85894758e-07
Iter: 1503 loss: 9.86410214e-07
Iter: 1504 loss: 9.85773113e-07
Iter: 1505 loss: 9.85794372e-07
Iter: 1506 loss: 9.85683187e-07
Iter: 1507 loss: 9.85595534e-07
Iter: 1508 loss: 9.85502766e-07
Iter: 1509 loss: 9.85481392e-07
Iter: 1510 loss: 9.85383281e-07
Iter: 1511 loss: 9.86581199e-07
Iter: 1512 loss: 9.85386123e-07
Iter: 1513 loss: 9.85270162e-07
Iter: 1514 loss: 9.85127599e-07
Iter: 1515 loss: 9.85115662e-07
Iter: 1516 loss: 9.84898634e-07
Iter: 1517 loss: 9.85370434e-07
Iter: 1518 loss: 9.84802455e-07
Iter: 1519 loss: 9.84651251e-07
Iter: 1520 loss: 9.85011866e-07
Iter: 1521 loss: 9.84575e-07
Iter: 1522 loss: 9.84375674e-07
Iter: 1523 loss: 9.85879069e-07
Iter: 1524 loss: 9.84388e-07
Iter: 1525 loss: 9.84265853e-07
Iter: 1526 loss: 9.84160579e-07
Iter: 1527 loss: 9.84127837e-07
Iter: 1528 loss: 9.83835889e-07
Iter: 1529 loss: 9.85227643e-07
Iter: 1530 loss: 9.83766e-07
Iter: 1531 loss: 9.83640348e-07
Iter: 1532 loss: 9.83293603e-07
Iter: 1533 loss: 9.87776e-07
Iter: 1534 loss: 9.83270638e-07
Iter: 1535 loss: 9.82967435e-07
Iter: 1536 loss: 9.84284e-07
Iter: 1537 loss: 9.82924348e-07
Iter: 1538 loss: 9.82759e-07
Iter: 1539 loss: 9.84853841e-07
Iter: 1540 loss: 9.82759502e-07
Iter: 1541 loss: 9.82619554e-07
Iter: 1542 loss: 9.83982e-07
Iter: 1543 loss: 9.82626716e-07
Iter: 1544 loss: 9.82572374e-07
Iter: 1545 loss: 9.82570896e-07
Iter: 1546 loss: 9.82522351e-07
Iter: 1547 loss: 9.82437655e-07
Iter: 1548 loss: 9.82951065e-07
Iter: 1549 loss: 9.82442089e-07
Iter: 1550 loss: 9.82346819e-07
Iter: 1551 loss: 9.82129336e-07
Iter: 1552 loss: 9.83649443e-07
Iter: 1553 loss: 9.82096e-07
Iter: 1554 loss: 9.81829317e-07
Iter: 1555 loss: 9.83706741e-07
Iter: 1556 loss: 9.81810445e-07
Iter: 1557 loss: 9.81609e-07
Iter: 1558 loss: 9.8422629e-07
Iter: 1559 loss: 9.81595235e-07
Iter: 1560 loss: 9.81457447e-07
Iter: 1561 loss: 9.8132648e-07
Iter: 1562 loss: 9.81288167e-07
Iter: 1563 loss: 9.81049e-07
Iter: 1564 loss: 9.82962433e-07
Iter: 1565 loss: 9.8103169e-07
Iter: 1566 loss: 9.80788286e-07
Iter: 1567 loss: 9.80563527e-07
Iter: 1568 loss: 9.80481332e-07
Iter: 1569 loss: 9.80310233e-07
Iter: 1570 loss: 9.80341611e-07
Iter: 1571 loss: 9.80124582e-07
Iter: 1572 loss: 9.79928586e-07
Iter: 1573 loss: 9.80203822e-07
Iter: 1574 loss: 9.79805463e-07
Iter: 1575 loss: 9.79808874e-07
Iter: 1576 loss: 9.79690412e-07
Iter: 1577 loss: 9.79622655e-07
Iter: 1578 loss: 9.79465653e-07
Iter: 1579 loss: 9.79461674e-07
Iter: 1580 loss: 9.79402444e-07
Iter: 1581 loss: 9.79368224e-07
Iter: 1582 loss: 9.79314336e-07
Iter: 1583 loss: 9.79108108e-07
Iter: 1584 loss: 9.81182e-07
Iter: 1585 loss: 9.7909151e-07
Iter: 1586 loss: 9.78871071e-07
Iter: 1587 loss: 9.79142214e-07
Iter: 1588 loss: 9.78771823e-07
Iter: 1589 loss: 9.78669277e-07
Iter: 1590 loss: 9.78646881e-07
Iter: 1591 loss: 9.78514436e-07
Iter: 1592 loss: 9.78290814e-07
Iter: 1593 loss: 9.78315938e-07
Iter: 1594 loss: 9.78072421e-07
Iter: 1595 loss: 9.80159e-07
Iter: 1596 loss: 9.78059234e-07
Iter: 1597 loss: 9.77911554e-07
Iter: 1598 loss: 9.78596e-07
Iter: 1599 loss: 9.77862e-07
Iter: 1600 loss: 9.7775e-07
Iter: 1601 loss: 9.77592e-07
Iter: 1602 loss: 9.7757993e-07
Iter: 1603 loss: 9.77394279e-07
Iter: 1604 loss: 9.77317768e-07
Iter: 1605 loss: 9.77201353e-07
Iter: 1606 loss: 9.77006493e-07
Iter: 1607 loss: 9.79932111e-07
Iter: 1608 loss: 9.77011155e-07
Iter: 1609 loss: 9.76848355e-07
Iter: 1610 loss: 9.7866689e-07
Iter: 1611 loss: 9.76834372e-07
Iter: 1612 loss: 9.76751835e-07
Iter: 1613 loss: 9.76614501e-07
Iter: 1614 loss: 9.76610409e-07
Iter: 1615 loss: 9.76385422e-07
Iter: 1616 loss: 9.77469085e-07
Iter: 1617 loss: 9.76335514e-07
Iter: 1618 loss: 9.76203523e-07
Iter: 1619 loss: 9.75975354e-07
Iter: 1620 loss: 9.75988883e-07
Iter: 1621 loss: 9.758229e-07
Iter: 1622 loss: 9.75812327e-07
Iter: 1623 loss: 9.75677267e-07
Iter: 1624 loss: 9.76128831e-07
Iter: 1625 loss: 9.75608373e-07
Iter: 1626 loss: 9.75504918e-07
Iter: 1627 loss: 9.75638386e-07
Iter: 1628 loss: 9.75445e-07
Iter: 1629 loss: 9.75331886e-07
Iter: 1630 loss: 9.76389288e-07
Iter: 1631 loss: 9.75346438e-07
Iter: 1632 loss: 9.75240141e-07
Iter: 1633 loss: 9.75150328e-07
Iter: 1634 loss: 9.75132707e-07
Iter: 1635 loss: 9.7501561e-07
Iter: 1636 loss: 9.74942168e-07
Iter: 1637 loss: 9.74891e-07
Iter: 1638 loss: 9.74720933e-07
Iter: 1639 loss: 9.75233888e-07
Iter: 1640 loss: 9.74649538e-07
Iter: 1641 loss: 9.74591217e-07
Iter: 1642 loss: 9.74556428e-07
Iter: 1643 loss: 9.74471618e-07
Iter: 1644 loss: 9.74274371e-07
Iter: 1645 loss: 9.75440457e-07
Iter: 1646 loss: 9.74188e-07
Iter: 1647 loss: 9.74081445e-07
Iter: 1648 loss: 9.74052e-07
Iter: 1649 loss: 9.73906936e-07
Iter: 1650 loss: 9.7367e-07
Iter: 1651 loss: 9.77341415e-07
Iter: 1652 loss: 9.73650913e-07
Iter: 1653 loss: 9.73488113e-07
Iter: 1654 loss: 9.75741386e-07
Iter: 1655 loss: 9.73488909e-07
Iter: 1656 loss: 9.73361239e-07
Iter: 1657 loss: 9.74345539e-07
Iter: 1658 loss: 9.73355668e-07
Iter: 1659 loss: 9.73262104e-07
Iter: 1660 loss: 9.73203896e-07
Iter: 1661 loss: 9.73148303e-07
Iter: 1662 loss: 9.73063266e-07
Iter: 1663 loss: 9.7439829e-07
Iter: 1664 loss: 9.73049509e-07
Iter: 1665 loss: 9.72960493e-07
Iter: 1666 loss: 9.72915359e-07
Iter: 1667 loss: 9.72873295e-07
Iter: 1668 loss: 9.72773933e-07
Iter: 1669 loss: 9.72620455e-07
Iter: 1670 loss: 9.72621137e-07
Iter: 1671 loss: 9.72401722e-07
Iter: 1672 loss: 9.72820771e-07
Iter: 1673 loss: 9.72300768e-07
Iter: 1674 loss: 9.72230737e-07
Iter: 1675 loss: 9.72230509e-07
Iter: 1676 loss: 9.72123075e-07
Iter: 1677 loss: 9.72137e-07
Iter: 1678 loss: 9.72020416e-07
Iter: 1679 loss: 9.71925374e-07
Iter: 1680 loss: 9.72573162e-07
Iter: 1681 loss: 9.7191139e-07
Iter: 1682 loss: 9.71833174e-07
Iter: 1683 loss: 9.71745635e-07
Iter: 1684 loss: 9.71704367e-07
Iter: 1685 loss: 9.7160364e-07
Iter: 1686 loss: 9.71669806e-07
Iter: 1687 loss: 9.71535e-07
Iter: 1688 loss: 9.71449481e-07
Iter: 1689 loss: 9.71446184e-07
Iter: 1690 loss: 9.71378654e-07
Iter: 1691 loss: 9.71315785e-07
Iter: 1692 loss: 9.71302825e-07
Iter: 1693 loss: 9.71195504e-07
Iter: 1694 loss: 9.71996087e-07
Iter: 1695 loss: 9.71197323e-07
Iter: 1696 loss: 9.71100576e-07
Iter: 1697 loss: 9.71202326e-07
Iter: 1698 loss: 9.71052373e-07
Iter: 1699 loss: 9.7096472e-07
Iter: 1700 loss: 9.70926749e-07
Iter: 1701 loss: 9.70896735e-07
Iter: 1702 loss: 9.70760766e-07
Iter: 1703 loss: 9.70972e-07
Iter: 1704 loss: 9.70686415e-07
Iter: 1705 loss: 9.7057773e-07
Iter: 1706 loss: 9.71821e-07
Iter: 1707 loss: 9.70569545e-07
Iter: 1708 loss: 9.70478936e-07
Iter: 1709 loss: 9.71029067e-07
Iter: 1710 loss: 9.70450174e-07
Iter: 1711 loss: 9.70413453e-07
Iter: 1712 loss: 9.70488941e-07
Iter: 1713 loss: 9.70392534e-07
Iter: 1714 loss: 9.70288283e-07
Iter: 1715 loss: 9.70479277e-07
Iter: 1716 loss: 9.70257133e-07
Iter: 1717 loss: 9.70183919e-07
Iter: 1718 loss: 9.70078872e-07
Iter: 1719 loss: 9.70090127e-07
Iter: 1720 loss: 9.70064775e-07
Iter: 1721 loss: 9.70030328e-07
Iter: 1722 loss: 9.69960297e-07
Iter: 1723 loss: 9.69882876e-07
Iter: 1724 loss: 9.69889e-07
Iter: 1725 loss: 9.69808866e-07
Iter: 1726 loss: 9.70164479e-07
Iter: 1727 loss: 9.69781468e-07
Iter: 1728 loss: 9.69677785e-07
Iter: 1729 loss: 9.69938583e-07
Iter: 1730 loss: 9.69629355e-07
Iter: 1731 loss: 9.69528742e-07
Iter: 1732 loss: 9.69392e-07
Iter: 1733 loss: 9.69383791e-07
Iter: 1734 loss: 9.69175062e-07
Iter: 1735 loss: 9.69226335e-07
Iter: 1736 loss: 9.69018856e-07
Iter: 1737 loss: 9.68788299e-07
Iter: 1738 loss: 9.69659368e-07
Iter: 1739 loss: 9.68758172e-07
Iter: 1740 loss: 9.68666e-07
Iter: 1741 loss: 9.6863e-07
Iter: 1742 loss: 9.68588893e-07
Iter: 1743 loss: 9.68473614e-07
Iter: 1744 loss: 9.68469863e-07
Iter: 1745 loss: 9.68389941e-07
Iter: 1746 loss: 9.68386303e-07
Iter: 1747 loss: 9.68331733e-07
Iter: 1748 loss: 9.6818053e-07
Iter: 1749 loss: 9.6953363e-07
Iter: 1750 loss: 9.68151653e-07
Iter: 1751 loss: 9.67987489e-07
Iter: 1752 loss: 9.68169e-07
Iter: 1753 loss: 9.67906317e-07
Iter: 1754 loss: 9.67792516e-07
Iter: 1755 loss: 9.67774e-07
Iter: 1756 loss: 9.67678716e-07
Iter: 1757 loss: 9.6753331e-07
Iter: 1758 loss: 9.70390715e-07
Iter: 1759 loss: 9.67519441e-07
Iter: 1760 loss: 9.67480673e-07
Iter: 1761 loss: 9.67442929e-07
Iter: 1762 loss: 9.67395067e-07
Iter: 1763 loss: 9.67291726e-07
Iter: 1764 loss: 9.67285e-07
Iter: 1765 loss: 9.67144274e-07
Iter: 1766 loss: 9.67798883e-07
Iter: 1767 loss: 9.67119149e-07
Iter: 1768 loss: 9.67034111e-07
Iter: 1769 loss: 9.6758265e-07
Iter: 1770 loss: 9.67028882e-07
Iter: 1771 loss: 9.66964649e-07
Iter: 1772 loss: 9.67022288e-07
Iter: 1773 loss: 9.66930884e-07
Iter: 1774 loss: 9.66848575e-07
Iter: 1775 loss: 9.66722382e-07
Iter: 1776 loss: 9.69539542e-07
Iter: 1777 loss: 9.66720791e-07
Iter: 1778 loss: 9.66576181e-07
Iter: 1779 loss: 9.668845e-07
Iter: 1780 loss: 9.66527e-07
Iter: 1781 loss: 9.66492735e-07
Iter: 1782 loss: 9.66433e-07
Iter: 1783 loss: 9.6637109e-07
Iter: 1784 loss: 9.66202606e-07
Iter: 1785 loss: 9.68250788e-07
Iter: 1786 loss: 9.66203629e-07
Iter: 1787 loss: 9.66156904e-07
Iter: 1788 loss: 9.66142e-07
Iter: 1789 loss: 9.66067319e-07
Iter: 1790 loss: 9.65981712e-07
Iter: 1791 loss: 9.65967274e-07
Iter: 1792 loss: 9.65907475e-07
Iter: 1793 loss: 9.664343e-07
Iter: 1794 loss: 9.65906906e-07
Iter: 1795 loss: 9.65807885e-07
Iter: 1796 loss: 9.65859385e-07
Iter: 1797 loss: 9.65753884e-07
Iter: 1798 loss: 9.65642812e-07
Iter: 1799 loss: 9.65564141e-07
Iter: 1800 loss: 9.65576191e-07
Iter: 1801 loss: 9.65433287e-07
Iter: 1802 loss: 9.65746608e-07
Iter: 1803 loss: 9.65370191e-07
Iter: 1804 loss: 9.65253548e-07
Iter: 1805 loss: 9.65367121e-07
Iter: 1806 loss: 9.65179083e-07
Iter: 1807 loss: 9.65168852e-07
Iter: 1808 loss: 9.65136337e-07
Iter: 1809 loss: 9.65081426e-07
Iter: 1810 loss: 9.64999572e-07
Iter: 1811 loss: 9.65902359e-07
Iter: 1812 loss: 9.64946139e-07
Iter: 1813 loss: 9.64855644e-07
Iter: 1814 loss: 9.65224217e-07
Iter: 1815 loss: 9.64835e-07
Iter: 1816 loss: 9.64706715e-07
Iter: 1817 loss: 9.64673177e-07
Iter: 1818 loss: 9.64585752e-07
Iter: 1819 loss: 9.64533456e-07
Iter: 1820 loss: 9.64518222e-07
Iter: 1821 loss: 9.64458422e-07
Iter: 1822 loss: 9.64436367e-07
Iter: 1823 loss: 9.64409082e-07
Iter: 1824 loss: 9.6434735e-07
Iter: 1825 loss: 9.64592459e-07
Iter: 1826 loss: 9.64336209e-07
Iter: 1827 loss: 9.64270612e-07
Iter: 1828 loss: 9.64615765e-07
Iter: 1829 loss: 9.64257652e-07
Iter: 1830 loss: 9.64226e-07
Iter: 1831 loss: 9.64150672e-07
Iter: 1832 loss: 9.64147375e-07
Iter: 1833 loss: 9.64076207e-07
Iter: 1834 loss: 9.64061087e-07
Iter: 1835 loss: 9.6401061e-07
Iter: 1836 loss: 9.63910338e-07
Iter: 1837 loss: 9.63936827e-07
Iter: 1838 loss: 9.63829734e-07
Iter: 1839 loss: 9.63706725e-07
Iter: 1840 loss: 9.64050287e-07
Iter: 1841 loss: 9.63655793e-07
Iter: 1842 loss: 9.63600087e-07
Iter: 1843 loss: 9.63605316e-07
Iter: 1844 loss: 9.63515163e-07
Iter: 1845 loss: 9.63487309e-07
Iter: 1846 loss: 9.63443199e-07
Iter: 1847 loss: 9.63349407e-07
Iter: 1848 loss: 9.63515e-07
Iter: 1849 loss: 9.63330194e-07
Iter: 1850 loss: 9.6327426e-07
Iter: 1851 loss: 9.63264824e-07
Iter: 1852 loss: 9.63223101e-07
Iter: 1853 loss: 9.63136358e-07
Iter: 1854 loss: 9.64220931e-07
Iter: 1855 loss: 9.6314136e-07
Iter: 1856 loss: 9.63047e-07
Iter: 1857 loss: 9.63605316e-07
Iter: 1858 loss: 9.6302108e-07
Iter: 1859 loss: 9.62943886e-07
Iter: 1860 loss: 9.63958882e-07
Iter: 1861 loss: 9.62943e-07
Iter: 1862 loss: 9.6286783e-07
Iter: 1863 loss: 9.62838271e-07
Iter: 1864 loss: 9.62808144e-07
Iter: 1865 loss: 9.62704689e-07
Iter: 1866 loss: 9.63129e-07
Iter: 1867 loss: 9.62677632e-07
Iter: 1868 loss: 9.62598278e-07
Iter: 1869 loss: 9.63001639e-07
Iter: 1870 loss: 9.62587478e-07
Iter: 1871 loss: 9.62514378e-07
Iter: 1872 loss: 9.62553941e-07
Iter: 1873 loss: 9.62454237e-07
Iter: 1874 loss: 9.62374543e-07
Iter: 1875 loss: 9.62352942e-07
Iter: 1876 loss: 9.62316108e-07
Iter: 1877 loss: 9.62247e-07
Iter: 1878 loss: 9.62241302e-07
Iter: 1879 loss: 9.62178433e-07
Iter: 1880 loss: 9.62102717e-07
Iter: 1881 loss: 9.62084187e-07
Iter: 1882 loss: 9.61986e-07
Iter: 1883 loss: 9.62439117e-07
Iter: 1884 loss: 9.61954129e-07
Iter: 1885 loss: 9.618268e-07
Iter: 1886 loss: 9.62581e-07
Iter: 1887 loss: 9.61792693e-07
Iter: 1888 loss: 9.61751084e-07
Iter: 1889 loss: 9.61609658e-07
Iter: 1890 loss: 9.63238108e-07
Iter: 1891 loss: 9.61575779e-07
Iter: 1892 loss: 9.61526553e-07
Iter: 1893 loss: 9.61493697e-07
Iter: 1894 loss: 9.61391152e-07
Iter: 1895 loss: 9.61334536e-07
Iter: 1896 loss: 9.61287469e-07
Iter: 1897 loss: 9.61202431e-07
Iter: 1898 loss: 9.61876253e-07
Iter: 1899 loss: 9.61191517e-07
Iter: 1900 loss: 9.61057e-07
Iter: 1901 loss: 9.61182423e-07
Iter: 1902 loss: 9.61022238e-07
Iter: 1903 loss: 9.6087615e-07
Iter: 1904 loss: 9.61329761e-07
Iter: 1905 loss: 9.60822717e-07
Iter: 1906 loss: 9.60741545e-07
Iter: 1907 loss: 9.60797252e-07
Iter: 1908 loss: 9.60695502e-07
Iter: 1909 loss: 9.60589659e-07
Iter: 1910 loss: 9.60761668e-07
Iter: 1911 loss: 9.60552825e-07
Iter: 1912 loss: 9.60415491e-07
Iter: 1913 loss: 9.61474143e-07
Iter: 1914 loss: 9.60403327e-07
Iter: 1915 loss: 9.60370244e-07
Iter: 1916 loss: 9.60291686e-07
Iter: 1917 loss: 9.6027361e-07
Iter: 1918 loss: 9.60193802e-07
Iter: 1919 loss: 9.60215e-07
Iter: 1920 loss: 9.60150601e-07
Iter: 1921 loss: 9.60030661e-07
Iter: 1922 loss: 9.60912757e-07
Iter: 1923 loss: 9.59999625e-07
Iter: 1924 loss: 9.59860699e-07
Iter: 1925 loss: 9.5976e-07
Iter: 1926 loss: 9.5968187e-07
Iter: 1927 loss: 9.59491217e-07
Iter: 1928 loss: 9.60848752e-07
Iter: 1929 loss: 9.59485646e-07
Iter: 1930 loss: 9.59452336e-07
Iter: 1931 loss: 9.59397312e-07
Iter: 1932 loss: 9.59339218e-07
Iter: 1933 loss: 9.59237923e-07
Iter: 1934 loss: 9.61551905e-07
Iter: 1935 loss: 9.59249633e-07
Iter: 1936 loss: 9.59197678e-07
Iter: 1937 loss: 9.59181762e-07
Iter: 1938 loss: 9.5915459e-07
Iter: 1939 loss: 9.59159365e-07
Iter: 1940 loss: 9.59122872e-07
Iter: 1941 loss: 9.59091e-07
Iter: 1942 loss: 9.59369345e-07
Iter: 1943 loss: 9.59070121e-07
Iter: 1944 loss: 9.59041699e-07
Iter: 1945 loss: 9.59252702e-07
Iter: 1946 loss: 9.59035106e-07
Iter: 1947 loss: 9.59014642e-07
Iter: 1948 loss: 9.58927558e-07
Iter: 1949 loss: 9.59217573e-07
Iter: 1950 loss: 9.58896408e-07
Iter: 1951 loss: 9.58806709e-07
Iter: 1952 loss: 9.58819101e-07
Iter: 1953 loss: 9.58713713e-07
Iter: 1954 loss: 9.58631745e-07
Iter: 1955 loss: 9.58607643e-07
Iter: 1956 loss: 9.58494e-07
Iter: 1957 loss: 9.58452e-07
Iter: 1958 loss: 9.58403461e-07
Iter: 1959 loss: 9.582551e-07
Iter: 1960 loss: 9.58637656e-07
Iter: 1961 loss: 9.5821315e-07
Iter: 1962 loss: 9.58151645e-07
Iter: 1963 loss: 9.59306e-07
Iter: 1964 loss: 9.58143687e-07
Iter: 1965 loss: 9.58087185e-07
Iter: 1966 loss: 9.587053e-07
Iter: 1967 loss: 9.58079681e-07
Iter: 1968 loss: 9.58045121e-07
Iter: 1969 loss: 9.58004e-07
Iter: 1970 loss: 9.5800192e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8
+ date
Mon Oct 26 09:30:30 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.8_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.8_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b11fda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b11569d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b10b19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b11827b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b1149bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b10b10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b1048730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b10271e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b1027268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b1027620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b0fda2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b0f8d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b0f8d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b0f3e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b0f3f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b0f3e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b0f297b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b0eff268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b0eff048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b0f29ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b0e50378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43929b50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43b0e50f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4392973e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f439292c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4392950b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4392950730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43928fd378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43928fd048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f43928b9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f436c165268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f436c18cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f436c146598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f436c18ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f436c0f60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f436c09df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.98834948e-06
Iter: 2 loss: 2.47702246e-06
Iter: 3 loss: 2.42463921e-06
Iter: 4 loss: 2.13180783e-06
Iter: 5 loss: 2.48245146e-06
Iter: 6 loss: 1.97734016e-06
Iter: 7 loss: 1.89065804e-06
Iter: 8 loss: 2.96630878e-06
Iter: 9 loss: 1.88981767e-06
Iter: 10 loss: 1.81669907e-06
Iter: 11 loss: 1.83727138e-06
Iter: 12 loss: 1.76393974e-06
Iter: 13 loss: 1.7279234e-06
Iter: 14 loss: 2.04265302e-06
Iter: 15 loss: 1.72605758e-06
Iter: 16 loss: 1.70745182e-06
Iter: 17 loss: 1.9068641e-06
Iter: 18 loss: 1.706999e-06
Iter: 19 loss: 1.69621262e-06
Iter: 20 loss: 1.69042357e-06
Iter: 21 loss: 1.68554186e-06
Iter: 22 loss: 1.6720129e-06
Iter: 23 loss: 1.75294872e-06
Iter: 24 loss: 1.67031703e-06
Iter: 25 loss: 1.65328561e-06
Iter: 26 loss: 1.65546e-06
Iter: 27 loss: 1.64027915e-06
Iter: 28 loss: 1.62692186e-06
Iter: 29 loss: 1.6177421e-06
Iter: 30 loss: 1.61286584e-06
Iter: 31 loss: 1.5953932e-06
Iter: 32 loss: 1.65492975e-06
Iter: 33 loss: 1.59073466e-06
Iter: 34 loss: 1.58007549e-06
Iter: 35 loss: 1.67385247e-06
Iter: 36 loss: 1.57957766e-06
Iter: 37 loss: 1.57290947e-06
Iter: 38 loss: 1.569814e-06
Iter: 39 loss: 1.56648048e-06
Iter: 40 loss: 1.55935254e-06
Iter: 41 loss: 1.5592268e-06
Iter: 42 loss: 1.5528459e-06
Iter: 43 loss: 1.60956336e-06
Iter: 44 loss: 1.55252656e-06
Iter: 45 loss: 1.54901409e-06
Iter: 46 loss: 1.54166696e-06
Iter: 47 loss: 1.66212942e-06
Iter: 48 loss: 1.54146096e-06
Iter: 49 loss: 1.53367114e-06
Iter: 50 loss: 1.53362646e-06
Iter: 51 loss: 1.5312072e-06
Iter: 52 loss: 1.52690677e-06
Iter: 53 loss: 1.52691155e-06
Iter: 54 loss: 1.52582061e-06
Iter: 55 loss: 1.52465486e-06
Iter: 56 loss: 1.52339067e-06
Iter: 57 loss: 1.52113603e-06
Iter: 58 loss: 1.52113591e-06
Iter: 59 loss: 1.51942027e-06
Iter: 60 loss: 1.51939139e-06
Iter: 61 loss: 1.51759355e-06
Iter: 62 loss: 1.51451241e-06
Iter: 63 loss: 1.51451684e-06
Iter: 64 loss: 1.51082725e-06
Iter: 65 loss: 1.51498193e-06
Iter: 66 loss: 1.50877077e-06
Iter: 67 loss: 1.50419578e-06
Iter: 68 loss: 1.51056906e-06
Iter: 69 loss: 1.50195194e-06
Iter: 70 loss: 1.49738821e-06
Iter: 71 loss: 1.50536312e-06
Iter: 72 loss: 1.49535595e-06
Iter: 73 loss: 1.49495577e-06
Iter: 74 loss: 1.49349466e-06
Iter: 75 loss: 1.49182301e-06
Iter: 76 loss: 1.48852473e-06
Iter: 77 loss: 1.55585019e-06
Iter: 78 loss: 1.4884946e-06
Iter: 79 loss: 1.48664481e-06
Iter: 80 loss: 1.48660638e-06
Iter: 81 loss: 1.48465756e-06
Iter: 82 loss: 1.48209563e-06
Iter: 83 loss: 1.48194795e-06
Iter: 84 loss: 1.47920889e-06
Iter: 85 loss: 1.48386255e-06
Iter: 86 loss: 1.47796493e-06
Iter: 87 loss: 1.47551964e-06
Iter: 88 loss: 1.47549395e-06
Iter: 89 loss: 1.4744727e-06
Iter: 90 loss: 1.47244828e-06
Iter: 91 loss: 1.51313452e-06
Iter: 92 loss: 1.47241485e-06
Iter: 93 loss: 1.47062451e-06
Iter: 94 loss: 1.47058608e-06
Iter: 95 loss: 1.46966886e-06
Iter: 96 loss: 1.46817581e-06
Iter: 97 loss: 1.46817979e-06
Iter: 98 loss: 1.46623199e-06
Iter: 99 loss: 1.46917887e-06
Iter: 100 loss: 1.46530749e-06
Iter: 101 loss: 1.46295065e-06
Iter: 102 loss: 1.46586035e-06
Iter: 103 loss: 1.46172476e-06
Iter: 104 loss: 1.4592149e-06
Iter: 105 loss: 1.46644811e-06
Iter: 106 loss: 1.45843933e-06
Iter: 107 loss: 1.45663103e-06
Iter: 108 loss: 1.45647277e-06
Iter: 109 loss: 1.45550723e-06
Iter: 110 loss: 1.45353795e-06
Iter: 111 loss: 1.48665924e-06
Iter: 112 loss: 1.45350521e-06
Iter: 113 loss: 1.45231684e-06
Iter: 114 loss: 1.45210856e-06
Iter: 115 loss: 1.45125455e-06
Iter: 116 loss: 1.4494417e-06
Iter: 117 loss: 1.47982882e-06
Iter: 118 loss: 1.44938986e-06
Iter: 119 loss: 1.44779892e-06
Iter: 120 loss: 1.45696549e-06
Iter: 121 loss: 1.44760861e-06
Iter: 122 loss: 1.44535545e-06
Iter: 123 loss: 1.44793967e-06
Iter: 124 loss: 1.44416686e-06
Iter: 125 loss: 1.44264379e-06
Iter: 126 loss: 1.4481484e-06
Iter: 127 loss: 1.44228579e-06
Iter: 128 loss: 1.44018111e-06
Iter: 129 loss: 1.44228625e-06
Iter: 130 loss: 1.43902685e-06
Iter: 131 loss: 1.43773559e-06
Iter: 132 loss: 1.43660191e-06
Iter: 133 loss: 1.43626085e-06
Iter: 134 loss: 1.43430884e-06
Iter: 135 loss: 1.44867363e-06
Iter: 136 loss: 1.43415377e-06
Iter: 137 loss: 1.43284137e-06
Iter: 138 loss: 1.43374e-06
Iter: 139 loss: 1.43201919e-06
Iter: 140 loss: 1.4310956e-06
Iter: 141 loss: 1.43106502e-06
Iter: 142 loss: 1.42996282e-06
Iter: 143 loss: 1.42933595e-06
Iter: 144 loss: 1.42885415e-06
Iter: 145 loss: 1.42759075e-06
Iter: 146 loss: 1.42951853e-06
Iter: 147 loss: 1.42697331e-06
Iter: 148 loss: 1.42541262e-06
Iter: 149 loss: 1.44063938e-06
Iter: 150 loss: 1.42535089e-06
Iter: 151 loss: 1.42460647e-06
Iter: 152 loss: 1.42272e-06
Iter: 153 loss: 1.44045384e-06
Iter: 154 loss: 1.42245449e-06
Iter: 155 loss: 1.42186377e-06
Iter: 156 loss: 1.42140152e-06
Iter: 157 loss: 1.42035287e-06
Iter: 158 loss: 1.41899704e-06
Iter: 159 loss: 1.41893793e-06
Iter: 160 loss: 1.41758676e-06
Iter: 161 loss: 1.43381988e-06
Iter: 162 loss: 1.41757539e-06
Iter: 163 loss: 1.4161493e-06
Iter: 164 loss: 1.41484134e-06
Iter: 165 loss: 1.41454484e-06
Iter: 166 loss: 1.41302314e-06
Iter: 167 loss: 1.41328474e-06
Iter: 168 loss: 1.41190822e-06
Iter: 169 loss: 1.41046735e-06
Iter: 170 loss: 1.42267709e-06
Iter: 171 loss: 1.41036912e-06
Iter: 172 loss: 1.40944724e-06
Iter: 173 loss: 1.41390069e-06
Iter: 174 loss: 1.40928887e-06
Iter: 175 loss: 1.40845873e-06
Iter: 176 loss: 1.41751934e-06
Iter: 177 loss: 1.40845555e-06
Iter: 178 loss: 1.40785e-06
Iter: 179 loss: 1.40679595e-06
Iter: 180 loss: 1.43191585e-06
Iter: 181 loss: 1.40681823e-06
Iter: 182 loss: 1.40634393e-06
Iter: 183 loss: 1.40621751e-06
Iter: 184 loss: 1.40565601e-06
Iter: 185 loss: 1.40435259e-06
Iter: 186 loss: 1.42068404e-06
Iter: 187 loss: 1.40424936e-06
Iter: 188 loss: 1.40277155e-06
Iter: 189 loss: 1.40713314e-06
Iter: 190 loss: 1.402306e-06
Iter: 191 loss: 1.40066641e-06
Iter: 192 loss: 1.41787223e-06
Iter: 193 loss: 1.40063844e-06
Iter: 194 loss: 1.39990493e-06
Iter: 195 loss: 1.39977806e-06
Iter: 196 loss: 1.39927101e-06
Iter: 197 loss: 1.39821407e-06
Iter: 198 loss: 1.40826819e-06
Iter: 199 loss: 1.39817359e-06
Iter: 200 loss: 1.39758868e-06
Iter: 201 loss: 1.39635154e-06
Iter: 202 loss: 1.41525368e-06
Iter: 203 loss: 1.39630924e-06
Iter: 204 loss: 1.39489953e-06
Iter: 205 loss: 1.39961116e-06
Iter: 206 loss: 1.39457927e-06
Iter: 207 loss: 1.39316239e-06
Iter: 208 loss: 1.39532153e-06
Iter: 209 loss: 1.39251938e-06
Iter: 210 loss: 1.3914605e-06
Iter: 211 loss: 1.39145038e-06
Iter: 212 loss: 1.39039503e-06
Iter: 213 loss: 1.38945938e-06
Iter: 214 loss: 1.38918745e-06
Iter: 215 loss: 1.3882451e-06
Iter: 216 loss: 1.39432655e-06
Iter: 217 loss: 1.38814335e-06
Iter: 218 loss: 1.38722635e-06
Iter: 219 loss: 1.39216252e-06
Iter: 220 loss: 1.38710129e-06
Iter: 221 loss: 1.38653638e-06
Iter: 222 loss: 1.38552014e-06
Iter: 223 loss: 1.40964289e-06
Iter: 224 loss: 1.38551206e-06
Iter: 225 loss: 1.38525411e-06
Iter: 226 loss: 1.38500241e-06
Iter: 227 loss: 1.38458597e-06
Iter: 228 loss: 1.38351606e-06
Iter: 229 loss: 1.39112103e-06
Iter: 230 loss: 1.383273e-06
Iter: 231 loss: 1.3824399e-06
Iter: 232 loss: 1.38233702e-06
Iter: 233 loss: 1.38163193e-06
Iter: 234 loss: 1.38049904e-06
Iter: 235 loss: 1.38048267e-06
Iter: 236 loss: 1.37931897e-06
Iter: 237 loss: 1.38020448e-06
Iter: 238 loss: 1.37863731e-06
Iter: 239 loss: 1.3771903e-06
Iter: 240 loss: 1.3830952e-06
Iter: 241 loss: 1.37689631e-06
Iter: 242 loss: 1.37547238e-06
Iter: 243 loss: 1.37608367e-06
Iter: 244 loss: 1.37451411e-06
Iter: 245 loss: 1.37353527e-06
Iter: 246 loss: 1.37348036e-06
Iter: 247 loss: 1.37241955e-06
Iter: 248 loss: 1.37462825e-06
Iter: 249 loss: 1.37199061e-06
Iter: 250 loss: 1.37135805e-06
Iter: 251 loss: 1.37166762e-06
Iter: 252 loss: 1.37087704e-06
Iter: 253 loss: 1.36992753e-06
Iter: 254 loss: 1.37692518e-06
Iter: 255 loss: 1.36985682e-06
Iter: 256 loss: 1.36928929e-06
Iter: 257 loss: 1.36868516e-06
Iter: 258 loss: 1.36862e-06
Iter: 259 loss: 1.36817334e-06
Iter: 260 loss: 1.36814742e-06
Iter: 261 loss: 1.36770348e-06
Iter: 262 loss: 1.36698145e-06
Iter: 263 loss: 1.36696713e-06
Iter: 264 loss: 1.36646827e-06
Iter: 265 loss: 1.36645167e-06
Iter: 266 loss: 1.36591939e-06
Iter: 267 loss: 1.36489632e-06
Iter: 268 loss: 1.38707253e-06
Iter: 269 loss: 1.36489552e-06
Iter: 270 loss: 1.36363292e-06
Iter: 271 loss: 1.36277924e-06
Iter: 272 loss: 1.36229801e-06
Iter: 273 loss: 1.36028189e-06
Iter: 274 loss: 1.37139159e-06
Iter: 275 loss: 1.36001324e-06
Iter: 276 loss: 1.35848154e-06
Iter: 277 loss: 1.36739527e-06
Iter: 278 loss: 1.35828952e-06
Iter: 279 loss: 1.35805976e-06
Iter: 280 loss: 1.35787616e-06
Iter: 281 loss: 1.35746427e-06
Iter: 282 loss: 1.35660412e-06
Iter: 283 loss: 1.37207905e-06
Iter: 284 loss: 1.35660457e-06
Iter: 285 loss: 1.35611162e-06
Iter: 286 loss: 1.3624699e-06
Iter: 287 loss: 1.35609025e-06
Iter: 288 loss: 1.35549385e-06
Iter: 289 loss: 1.35572736e-06
Iter: 290 loss: 1.35508276e-06
Iter: 291 loss: 1.35441667e-06
Iter: 292 loss: 1.35328469e-06
Iter: 293 loss: 1.35326377e-06
Iter: 294 loss: 1.35217738e-06
Iter: 295 loss: 1.35211485e-06
Iter: 296 loss: 1.35142045e-06
Iter: 297 loss: 1.35027585e-06
Iter: 298 loss: 1.35026244e-06
Iter: 299 loss: 1.34964239e-06
Iter: 300 loss: 1.34957395e-06
Iter: 301 loss: 1.34903394e-06
Iter: 302 loss: 1.34900279e-06
Iter: 303 loss: 1.34857964e-06
Iter: 304 loss: 1.3479771e-06
Iter: 305 loss: 1.34698053e-06
Iter: 306 loss: 1.34699258e-06
Iter: 307 loss: 1.34580512e-06
Iter: 308 loss: 1.34824768e-06
Iter: 309 loss: 1.34532661e-06
Iter: 310 loss: 1.34398203e-06
Iter: 311 loss: 1.3452393e-06
Iter: 312 loss: 1.34317497e-06
Iter: 313 loss: 1.34179231e-06
Iter: 314 loss: 1.35946277e-06
Iter: 315 loss: 1.34178515e-06
Iter: 316 loss: 1.34036316e-06
Iter: 317 loss: 1.34417951e-06
Iter: 318 loss: 1.33988442e-06
Iter: 319 loss: 1.33903859e-06
Iter: 320 loss: 1.33814501e-06
Iter: 321 loss: 1.33797994e-06
Iter: 322 loss: 1.33756669e-06
Iter: 323 loss: 1.33737672e-06
Iter: 324 loss: 1.33679305e-06
Iter: 325 loss: 1.33588605e-06
Iter: 326 loss: 1.3358449e-06
Iter: 327 loss: 1.3350284e-06
Iter: 328 loss: 1.33551771e-06
Iter: 329 loss: 1.33449453e-06
Iter: 330 loss: 1.33393746e-06
Iter: 331 loss: 1.33383548e-06
Iter: 332 loss: 1.33333333e-06
Iter: 333 loss: 1.3322267e-06
Iter: 334 loss: 1.34813092e-06
Iter: 335 loss: 1.33215769e-06
Iter: 336 loss: 1.33104481e-06
Iter: 337 loss: 1.33991921e-06
Iter: 338 loss: 1.33097274e-06
Iter: 339 loss: 1.32992091e-06
Iter: 340 loss: 1.3349586e-06
Iter: 341 loss: 1.32973855e-06
Iter: 342 loss: 1.32901914e-06
Iter: 343 loss: 1.32860544e-06
Iter: 344 loss: 1.3283186e-06
Iter: 345 loss: 1.32744572e-06
Iter: 346 loss: 1.33166122e-06
Iter: 347 loss: 1.3272911e-06
Iter: 348 loss: 1.32670425e-06
Iter: 349 loss: 1.32670925e-06
Iter: 350 loss: 1.3263e-06
Iter: 351 loss: 1.3251738e-06
Iter: 352 loss: 1.33327831e-06
Iter: 353 loss: 1.32500395e-06
Iter: 354 loss: 1.3241397e-06
Iter: 355 loss: 1.32414334e-06
Iter: 356 loss: 1.32324055e-06
Iter: 357 loss: 1.32491368e-06
Iter: 358 loss: 1.32284822e-06
Iter: 359 loss: 1.32218952e-06
Iter: 360 loss: 1.32115542e-06
Iter: 361 loss: 1.32112518e-06
Iter: 362 loss: 1.32093282e-06
Iter: 363 loss: 1.32060177e-06
Iter: 364 loss: 1.32015703e-06
Iter: 365 loss: 1.3193719e-06
Iter: 366 loss: 1.31937e-06
Iter: 367 loss: 1.31861793e-06
Iter: 368 loss: 1.32216587e-06
Iter: 369 loss: 1.31849083e-06
Iter: 370 loss: 1.3177513e-06
Iter: 371 loss: 1.32223204e-06
Iter: 372 loss: 1.31768741e-06
Iter: 373 loss: 1.31703416e-06
Iter: 374 loss: 1.31688262e-06
Iter: 375 loss: 1.3164597e-06
Iter: 376 loss: 1.31567492e-06
Iter: 377 loss: 1.31692491e-06
Iter: 378 loss: 1.31530635e-06
Iter: 379 loss: 1.31479692e-06
Iter: 380 loss: 1.31476622e-06
Iter: 381 loss: 1.31433717e-06
Iter: 382 loss: 1.31345598e-06
Iter: 383 loss: 1.33266747e-06
Iter: 384 loss: 1.31346542e-06
Iter: 385 loss: 1.31274487e-06
Iter: 386 loss: 1.31499712e-06
Iter: 387 loss: 1.31254228e-06
Iter: 388 loss: 1.31185311e-06
Iter: 389 loss: 1.3225673e-06
Iter: 390 loss: 1.31184015e-06
Iter: 391 loss: 1.31148136e-06
Iter: 392 loss: 1.31059255e-06
Iter: 393 loss: 1.31813363e-06
Iter: 394 loss: 1.31044328e-06
Iter: 395 loss: 1.30989326e-06
Iter: 396 loss: 1.30982926e-06
Iter: 397 loss: 1.30916192e-06
Iter: 398 loss: 1.30899025e-06
Iter: 399 loss: 1.30857541e-06
Iter: 400 loss: 1.30780791e-06
Iter: 401 loss: 1.30797923e-06
Iter: 402 loss: 1.30728063e-06
Iter: 403 loss: 1.30649903e-06
Iter: 404 loss: 1.30650528e-06
Iter: 405 loss: 1.30596709e-06
Iter: 406 loss: 1.30646561e-06
Iter: 407 loss: 1.30564547e-06
Iter: 408 loss: 1.30513467e-06
Iter: 409 loss: 1.30546221e-06
Iter: 410 loss: 1.3048184e-06
Iter: 411 loss: 1.30439832e-06
Iter: 412 loss: 1.30442368e-06
Iter: 413 loss: 1.30402532e-06
Iter: 414 loss: 1.30351702e-06
Iter: 415 loss: 1.30352976e-06
Iter: 416 loss: 1.30292051e-06
Iter: 417 loss: 1.30327476e-06
Iter: 418 loss: 1.30253113e-06
Iter: 419 loss: 1.3020915e-06
Iter: 420 loss: 1.30203171e-06
Iter: 421 loss: 1.30166518e-06
Iter: 422 loss: 1.30072362e-06
Iter: 423 loss: 1.30585022e-06
Iter: 424 loss: 1.30040553e-06
Iter: 425 loss: 1.29942919e-06
Iter: 426 loss: 1.31247e-06
Iter: 427 loss: 1.29943942e-06
Iter: 428 loss: 1.2984251e-06
Iter: 429 loss: 1.30303908e-06
Iter: 430 loss: 1.29824059e-06
Iter: 431 loss: 1.29775935e-06
Iter: 432 loss: 1.29703744e-06
Iter: 433 loss: 1.29701641e-06
Iter: 434 loss: 1.29620878e-06
Iter: 435 loss: 1.29622106e-06
Iter: 436 loss: 1.29556315e-06
Iter: 437 loss: 1.29635873e-06
Iter: 438 loss: 1.29523221e-06
Iter: 439 loss: 1.29457317e-06
Iter: 440 loss: 1.29495788e-06
Iter: 441 loss: 1.29413081e-06
Iter: 442 loss: 1.29338923e-06
Iter: 443 loss: 1.29821865e-06
Iter: 444 loss: 1.29332784e-06
Iter: 445 loss: 1.29247769e-06
Iter: 446 loss: 1.29349201e-06
Iter: 447 loss: 1.29203067e-06
Iter: 448 loss: 1.29144189e-06
Iter: 449 loss: 1.29107502e-06
Iter: 450 loss: 1.2908547e-06
Iter: 451 loss: 1.29049101e-06
Iter: 452 loss: 1.29037267e-06
Iter: 453 loss: 1.28995e-06
Iter: 454 loss: 1.28910733e-06
Iter: 455 loss: 1.30377248e-06
Iter: 456 loss: 1.28911427e-06
Iter: 457 loss: 1.28836746e-06
Iter: 458 loss: 1.29052478e-06
Iter: 459 loss: 1.28817123e-06
Iter: 460 loss: 1.28748388e-06
Iter: 461 loss: 1.28747376e-06
Iter: 462 loss: 1.28711213e-06
Iter: 463 loss: 1.28626107e-06
Iter: 464 loss: 1.29723696e-06
Iter: 465 loss: 1.28620604e-06
Iter: 466 loss: 1.28555348e-06
Iter: 467 loss: 1.28554348e-06
Iter: 468 loss: 1.28492877e-06
Iter: 469 loss: 1.2858261e-06
Iter: 470 loss: 1.28461602e-06
Iter: 471 loss: 1.28397176e-06
Iter: 472 loss: 1.28464046e-06
Iter: 473 loss: 1.28360762e-06
Iter: 474 loss: 1.2830028e-06
Iter: 475 loss: 1.28564989e-06
Iter: 476 loss: 1.28287058e-06
Iter: 477 loss: 1.28220358e-06
Iter: 478 loss: 1.28507077e-06
Iter: 479 loss: 1.28208274e-06
Iter: 480 loss: 1.28165038e-06
Iter: 481 loss: 1.28106103e-06
Iter: 482 loss: 1.28104125e-06
Iter: 483 loss: 1.28071281e-06
Iter: 484 loss: 1.28064426e-06
Iter: 485 loss: 1.28022361e-06
Iter: 486 loss: 1.27960561e-06
Iter: 487 loss: 1.279618e-06
Iter: 488 loss: 1.27902695e-06
Iter: 489 loss: 1.27954991e-06
Iter: 490 loss: 1.27867884e-06
Iter: 491 loss: 1.2782175e-06
Iter: 492 loss: 1.27819294e-06
Iter: 493 loss: 1.27782914e-06
Iter: 494 loss: 1.27702947e-06
Iter: 495 loss: 1.28596162e-06
Iter: 496 loss: 1.27695535e-06
Iter: 497 loss: 1.27613362e-06
Iter: 498 loss: 1.28132888e-06
Iter: 499 loss: 1.27602561e-06
Iter: 500 loss: 1.27515727e-06
Iter: 501 loss: 1.279e-06
Iter: 502 loss: 1.27497333e-06
Iter: 503 loss: 1.27432736e-06
Iter: 504 loss: 1.27548765e-06
Iter: 505 loss: 1.27400335e-06
Iter: 506 loss: 1.27335898e-06
Iter: 507 loss: 1.27516569e-06
Iter: 508 loss: 1.2731216e-06
Iter: 509 loss: 1.27245767e-06
Iter: 510 loss: 1.27768499e-06
Iter: 511 loss: 1.27240742e-06
Iter: 512 loss: 1.27190492e-06
Iter: 513 loss: 1.27096246e-06
Iter: 514 loss: 1.28997351e-06
Iter: 515 loss: 1.27097337e-06
Iter: 516 loss: 1.27032831e-06
Iter: 517 loss: 1.27031922e-06
Iter: 518 loss: 1.26969235e-06
Iter: 519 loss: 1.26993461e-06
Iter: 520 loss: 1.26923806e-06
Iter: 521 loss: 1.26873033e-06
Iter: 522 loss: 1.26826308e-06
Iter: 523 loss: 1.2681312e-06
Iter: 524 loss: 1.26783334e-06
Iter: 525 loss: 1.26767736e-06
Iter: 526 loss: 1.26732698e-06
Iter: 527 loss: 1.26661803e-06
Iter: 528 loss: 1.27828378e-06
Iter: 529 loss: 1.26656607e-06
Iter: 530 loss: 1.2659558e-06
Iter: 531 loss: 1.26898294e-06
Iter: 532 loss: 1.26581949e-06
Iter: 533 loss: 1.26522764e-06
Iter: 534 loss: 1.26881912e-06
Iter: 535 loss: 1.26515397e-06
Iter: 536 loss: 1.26472048e-06
Iter: 537 loss: 1.26479824e-06
Iter: 538 loss: 1.26436703e-06
Iter: 539 loss: 1.26379791e-06
Iter: 540 loss: 1.26557802e-06
Iter: 541 loss: 1.2636217e-06
Iter: 542 loss: 1.26314944e-06
Iter: 543 loss: 1.26663713e-06
Iter: 544 loss: 1.26311033e-06
Iter: 545 loss: 1.26268355e-06
Iter: 546 loss: 1.26199518e-06
Iter: 547 loss: 1.26198097e-06
Iter: 548 loss: 1.26136172e-06
Iter: 549 loss: 1.26787836e-06
Iter: 550 loss: 1.26130692e-06
Iter: 551 loss: 1.26064765e-06
Iter: 552 loss: 1.26219743e-06
Iter: 553 loss: 1.26043028e-06
Iter: 554 loss: 1.2599528e-06
Iter: 555 loss: 1.25927431e-06
Iter: 556 loss: 1.25924157e-06
Iter: 557 loss: 1.25887732e-06
Iter: 558 loss: 1.25879944e-06
Iter: 559 loss: 1.25833549e-06
Iter: 560 loss: 1.25757072e-06
Iter: 561 loss: 1.25757163e-06
Iter: 562 loss: 1.25693214e-06
Iter: 563 loss: 1.26051145e-06
Iter: 564 loss: 1.25684869e-06
Iter: 565 loss: 1.25629276e-06
Iter: 566 loss: 1.2609753e-06
Iter: 567 loss: 1.25626548e-06
Iter: 568 loss: 1.25590282e-06
Iter: 569 loss: 1.2561693e-06
Iter: 570 loss: 1.25568272e-06
Iter: 571 loss: 1.25524048e-06
Iter: 572 loss: 1.25660483e-06
Iter: 573 loss: 1.25512088e-06
Iter: 574 loss: 1.25477459e-06
Iter: 575 loss: 1.25669555e-06
Iter: 576 loss: 1.25475208e-06
Iter: 577 loss: 1.25432e-06
Iter: 578 loss: 1.25400686e-06
Iter: 579 loss: 1.25390602e-06
Iter: 580 loss: 1.25340534e-06
Iter: 581 loss: 1.25532529e-06
Iter: 582 loss: 1.2533128e-06
Iter: 583 loss: 1.2527189e-06
Iter: 584 loss: 1.25591623e-06
Iter: 585 loss: 1.25260976e-06
Iter: 586 loss: 1.25226597e-06
Iter: 587 loss: 1.25153338e-06
Iter: 588 loss: 1.26474299e-06
Iter: 589 loss: 1.25148779e-06
Iter: 590 loss: 1.25112706e-06
Iter: 591 loss: 1.25103293e-06
Iter: 592 loss: 1.25054873e-06
Iter: 593 loss: 1.25015026e-06
Iter: 594 loss: 1.25000543e-06
Iter: 595 loss: 1.24940084e-06
Iter: 596 loss: 1.25030965e-06
Iter: 597 loss: 1.24909241e-06
Iter: 598 loss: 1.248427e-06
Iter: 599 loss: 1.25680253e-06
Iter: 600 loss: 1.24842973e-06
Iter: 601 loss: 1.24794906e-06
Iter: 602 loss: 1.24816233e-06
Iter: 603 loss: 1.24764233e-06
Iter: 604 loss: 1.2470199e-06
Iter: 605 loss: 1.24927396e-06
Iter: 606 loss: 1.24689495e-06
Iter: 607 loss: 1.24640769e-06
Iter: 608 loss: 1.24803807e-06
Iter: 609 loss: 1.24628514e-06
Iter: 610 loss: 1.24567168e-06
Iter: 611 loss: 1.24580254e-06
Iter: 612 loss: 1.2452457e-06
Iter: 613 loss: 1.24462872e-06
Iter: 614 loss: 1.24573307e-06
Iter: 615 loss: 1.24437202e-06
Iter: 616 loss: 1.24375458e-06
Iter: 617 loss: 1.25164377e-06
Iter: 618 loss: 1.24373923e-06
Iter: 619 loss: 1.24344376e-06
Iter: 620 loss: 1.2426251e-06
Iter: 621 loss: 1.25229769e-06
Iter: 622 loss: 1.2425827e-06
Iter: 623 loss: 1.24220583e-06
Iter: 624 loss: 1.24215751e-06
Iter: 625 loss: 1.24170742e-06
Iter: 626 loss: 1.24154815e-06
Iter: 627 loss: 1.24128042e-06
Iter: 628 loss: 1.24076132e-06
Iter: 629 loss: 1.24091866e-06
Iter: 630 loss: 1.24040571e-06
Iter: 631 loss: 1.23988298e-06
Iter: 632 loss: 1.23987456e-06
Iter: 633 loss: 1.2394612e-06
Iter: 634 loss: 1.23925372e-06
Iter: 635 loss: 1.23905215e-06
Iter: 636 loss: 1.23847099e-06
Iter: 637 loss: 1.24151256e-06
Iter: 638 loss: 1.23836867e-06
Iter: 639 loss: 1.23790255e-06
Iter: 640 loss: 1.2388399e-06
Iter: 641 loss: 1.23771224e-06
Iter: 642 loss: 1.23710447e-06
Iter: 643 loss: 1.23770383e-06
Iter: 644 loss: 1.23676671e-06
Iter: 645 loss: 1.23617656e-06
Iter: 646 loss: 1.23660868e-06
Iter: 647 loss: 1.23581265e-06
Iter: 648 loss: 1.23520499e-06
Iter: 649 loss: 1.24540111e-06
Iter: 650 loss: 1.23519953e-06
Iter: 651 loss: 1.23478412e-06
Iter: 652 loss: 1.23388088e-06
Iter: 653 loss: 1.24469216e-06
Iter: 654 loss: 1.23378754e-06
Iter: 655 loss: 1.23325503e-06
Iter: 656 loss: 1.23323366e-06
Iter: 657 loss: 1.2326218e-06
Iter: 658 loss: 1.23317989e-06
Iter: 659 loss: 1.23228028e-06
Iter: 660 loss: 1.23174e-06
Iter: 661 loss: 1.23147561e-06
Iter: 662 loss: 1.23123812e-06
Iter: 663 loss: 1.23068821e-06
Iter: 664 loss: 1.23067048e-06
Iter: 665 loss: 1.23023869e-06
Iter: 666 loss: 1.23013763e-06
Iter: 667 loss: 1.22984613e-06
Iter: 668 loss: 1.22925906e-06
Iter: 669 loss: 1.23285872e-06
Iter: 670 loss: 1.22921244e-06
Iter: 671 loss: 1.22872564e-06
Iter: 672 loss: 1.22963661e-06
Iter: 673 loss: 1.22852225e-06
Iter: 674 loss: 1.22797223e-06
Iter: 675 loss: 1.2292553e-06
Iter: 676 loss: 1.22776942e-06
Iter: 677 loss: 1.22729318e-06
Iter: 678 loss: 1.22695735e-06
Iter: 679 loss: 1.2267401e-06
Iter: 680 loss: 1.22614688e-06
Iter: 681 loss: 1.22611596e-06
Iter: 682 loss: 1.22570657e-06
Iter: 683 loss: 1.224794e-06
Iter: 684 loss: 1.23897053e-06
Iter: 685 loss: 1.22476263e-06
Iter: 686 loss: 1.22408051e-06
Iter: 687 loss: 1.23023096e-06
Iter: 688 loss: 1.22407891e-06
Iter: 689 loss: 1.22327367e-06
Iter: 690 loss: 1.22542951e-06
Iter: 691 loss: 1.22305119e-06
Iter: 692 loss: 1.22249708e-06
Iter: 693 loss: 1.22201152e-06
Iter: 694 loss: 1.22189613e-06
Iter: 695 loss: 1.22133474e-06
Iter: 696 loss: 1.22133417e-06
Iter: 697 loss: 1.22090705e-06
Iter: 698 loss: 1.22085544e-06
Iter: 699 loss: 1.22056053e-06
Iter: 700 loss: 1.22002507e-06
Iter: 701 loss: 1.22226186e-06
Iter: 702 loss: 1.21988887e-06
Iter: 703 loss: 1.21939786e-06
Iter: 704 loss: 1.22049084e-06
Iter: 705 loss: 1.21919754e-06
Iter: 706 loss: 1.21872881e-06
Iter: 707 loss: 1.22094866e-06
Iter: 708 loss: 1.21865082e-06
Iter: 709 loss: 1.21821063e-06
Iter: 710 loss: 1.21751714e-06
Iter: 711 loss: 1.2175135e-06
Iter: 712 loss: 1.21711128e-06
Iter: 713 loss: 1.21699395e-06
Iter: 714 loss: 1.21656126e-06
Iter: 715 loss: 1.21571691e-06
Iter: 716 loss: 1.23083748e-06
Iter: 717 loss: 1.2157227e-06
Iter: 718 loss: 1.21497885e-06
Iter: 719 loss: 1.21687867e-06
Iter: 720 loss: 1.21470339e-06
Iter: 721 loss: 1.21376377e-06
Iter: 722 loss: 1.22345841e-06
Iter: 723 loss: 1.21374046e-06
Iter: 724 loss: 1.21329811e-06
Iter: 725 loss: 1.21254902e-06
Iter: 726 loss: 1.21254061e-06
Iter: 727 loss: 1.21190692e-06
Iter: 728 loss: 1.21189396e-06
Iter: 729 loss: 1.21135281e-06
Iter: 730 loss: 1.2111193e-06
Iter: 731 loss: 1.21081621e-06
Iter: 732 loss: 1.2100819e-06
Iter: 733 loss: 1.21452399e-06
Iter: 734 loss: 1.20996674e-06
Iter: 735 loss: 1.20935385e-06
Iter: 736 loss: 1.21021401e-06
Iter: 737 loss: 1.20902132e-06
Iter: 738 loss: 1.20842753e-06
Iter: 739 loss: 1.21112987e-06
Iter: 740 loss: 1.20833329e-06
Iter: 741 loss: 1.20768073e-06
Iter: 742 loss: 1.20699951e-06
Iter: 743 loss: 1.20687969e-06
Iter: 744 loss: 1.20654818e-06
Iter: 745 loss: 1.20637901e-06
Iter: 746 loss: 1.20596189e-06
Iter: 747 loss: 1.20521327e-06
Iter: 748 loss: 1.20521497e-06
Iter: 749 loss: 1.20452944e-06
Iter: 750 loss: 1.20433072e-06
Iter: 751 loss: 1.20395089e-06
Iter: 752 loss: 1.20333107e-06
Iter: 753 loss: 1.20320192e-06
Iter: 754 loss: 1.2028504e-06
Iter: 755 loss: 1.20208847e-06
Iter: 756 loss: 1.21539097e-06
Iter: 757 loss: 1.20206732e-06
Iter: 758 loss: 1.20149502e-06
Iter: 759 loss: 1.20808158e-06
Iter: 760 loss: 1.20146524e-06
Iter: 761 loss: 1.20077902e-06
Iter: 762 loss: 1.20112077e-06
Iter: 763 loss: 1.20031143e-06
Iter: 764 loss: 1.19959782e-06
Iter: 765 loss: 1.20421919e-06
Iter: 766 loss: 1.19949846e-06
Iter: 767 loss: 1.19897163e-06
Iter: 768 loss: 1.19952256e-06
Iter: 769 loss: 1.1986923e-06
Iter: 770 loss: 1.19812796e-06
Iter: 771 loss: 1.19847766e-06
Iter: 772 loss: 1.19775586e-06
Iter: 773 loss: 1.19665e-06
Iter: 774 loss: 1.19814115e-06
Iter: 775 loss: 1.19612696e-06
Iter: 776 loss: 1.19543142e-06
Iter: 777 loss: 1.19854053e-06
Iter: 778 loss: 1.19527363e-06
Iter: 779 loss: 1.19429637e-06
Iter: 780 loss: 1.19604454e-06
Iter: 781 loss: 1.19385629e-06
Iter: 782 loss: 1.19329343e-06
Iter: 783 loss: 1.19252502e-06
Iter: 784 loss: 1.19249319e-06
Iter: 785 loss: 1.19268054e-06
Iter: 786 loss: 1.19210381e-06
Iter: 787 loss: 1.19185665e-06
Iter: 788 loss: 1.19128936e-06
Iter: 789 loss: 1.19788956e-06
Iter: 790 loss: 1.1912615e-06
Iter: 791 loss: 1.19068591e-06
Iter: 792 loss: 1.19448146e-06
Iter: 793 loss: 1.19064032e-06
Iter: 794 loss: 1.19006484e-06
Iter: 795 loss: 1.19145852e-06
Iter: 796 loss: 1.18982837e-06
Iter: 797 loss: 1.18935623e-06
Iter: 798 loss: 1.19220124e-06
Iter: 799 loss: 1.18930052e-06
Iter: 800 loss: 1.18887613e-06
Iter: 801 loss: 1.18883895e-06
Iter: 802 loss: 1.18853654e-06
Iter: 803 loss: 1.18791024e-06
Iter: 804 loss: 1.18781372e-06
Iter: 805 loss: 1.18737489e-06
Iter: 806 loss: 1.1866548e-06
Iter: 807 loss: 1.19750985e-06
Iter: 808 loss: 1.18663684e-06
Iter: 809 loss: 1.18614275e-06
Iter: 810 loss: 1.18642379e-06
Iter: 811 loss: 1.18583853e-06
Iter: 812 loss: 1.1853914e-06
Iter: 813 loss: 1.18540152e-06
Iter: 814 loss: 1.18514208e-06
Iter: 815 loss: 1.18452499e-06
Iter: 816 loss: 1.18907747e-06
Iter: 817 loss: 1.18434787e-06
Iter: 818 loss: 1.1840807e-06
Iter: 819 loss: 1.18396e-06
Iter: 820 loss: 1.18354444e-06
Iter: 821 loss: 1.18311709e-06
Iter: 822 loss: 1.18305752e-06
Iter: 823 loss: 1.1825548e-06
Iter: 824 loss: 1.18299977e-06
Iter: 825 loss: 1.18225853e-06
Iter: 826 loss: 1.18164405e-06
Iter: 827 loss: 1.18888306e-06
Iter: 828 loss: 1.18164064e-06
Iter: 829 loss: 1.18125672e-06
Iter: 830 loss: 1.18230219e-06
Iter: 831 loss: 1.18115759e-06
Iter: 832 loss: 1.18076036e-06
Iter: 833 loss: 1.18115838e-06
Iter: 834 loss: 1.18057676e-06
Iter: 835 loss: 1.18015703e-06
Iter: 836 loss: 1.17981278e-06
Iter: 837 loss: 1.1797257e-06
Iter: 838 loss: 1.17915147e-06
Iter: 839 loss: 1.18533671e-06
Iter: 840 loss: 1.17913282e-06
Iter: 841 loss: 1.17857599e-06
Iter: 842 loss: 1.17942182e-06
Iter: 843 loss: 1.17835589e-06
Iter: 844 loss: 1.17797504e-06
Iter: 845 loss: 1.18359333e-06
Iter: 846 loss: 1.17797924e-06
Iter: 847 loss: 1.17763807e-06
Iter: 848 loss: 1.17693321e-06
Iter: 849 loss: 1.18752735e-06
Iter: 850 loss: 1.1768866e-06
Iter: 851 loss: 1.17626132e-06
Iter: 852 loss: 1.18150763e-06
Iter: 853 loss: 1.17623267e-06
Iter: 854 loss: 1.17566287e-06
Iter: 855 loss: 1.17915442e-06
Iter: 856 loss: 1.17557624e-06
Iter: 857 loss: 1.175281e-06
Iter: 858 loss: 1.17469517e-06
Iter: 859 loss: 1.18601588e-06
Iter: 860 loss: 1.17469722e-06
Iter: 861 loss: 1.17429863e-06
Iter: 862 loss: 1.1742784e-06
Iter: 863 loss: 1.17393665e-06
Iter: 864 loss: 1.17434024e-06
Iter: 865 loss: 1.1737535e-06
Iter: 866 loss: 1.1733357e-06
Iter: 867 loss: 1.17458762e-06
Iter: 868 loss: 1.17322861e-06
Iter: 869 loss: 1.17290961e-06
Iter: 870 loss: 1.17255797e-06
Iter: 871 loss: 1.17250647e-06
Iter: 872 loss: 1.17204786e-06
Iter: 873 loss: 1.17374e-06
Iter: 874 loss: 1.17193281e-06
Iter: 875 loss: 1.17146658e-06
Iter: 876 loss: 1.17530453e-06
Iter: 877 loss: 1.17146624e-06
Iter: 878 loss: 1.17115655e-06
Iter: 879 loss: 1.17218e-06
Iter: 880 loss: 1.17104537e-06
Iter: 881 loss: 1.17067646e-06
Iter: 882 loss: 1.17087927e-06
Iter: 883 loss: 1.17042941e-06
Iter: 884 loss: 1.17005607e-06
Iter: 885 loss: 1.16991623e-06
Iter: 886 loss: 1.16973865e-06
Iter: 887 loss: 1.16942306e-06
Iter: 888 loss: 1.16937792e-06
Iter: 889 loss: 1.16912975e-06
Iter: 890 loss: 1.1684167e-06
Iter: 891 loss: 1.1731546e-06
Iter: 892 loss: 1.16825231e-06
Iter: 893 loss: 1.16789784e-06
Iter: 894 loss: 1.16786885e-06
Iter: 895 loss: 1.16744013e-06
Iter: 896 loss: 1.16758019e-06
Iter: 897 loss: 1.16715557e-06
Iter: 898 loss: 1.16679166e-06
Iter: 899 loss: 1.16679064e-06
Iter: 900 loss: 1.16658725e-06
Iter: 901 loss: 1.16633623e-06
Iter: 902 loss: 1.16631986e-06
Iter: 903 loss: 1.16597084e-06
Iter: 904 loss: 1.16571505e-06
Iter: 905 loss: 1.16559613e-06
Iter: 906 loss: 1.16513377e-06
Iter: 907 loss: 1.17174977e-06
Iter: 908 loss: 1.16512297e-06
Iter: 909 loss: 1.16472086e-06
Iter: 910 loss: 1.16523665e-06
Iter: 911 loss: 1.16450781e-06
Iter: 912 loss: 1.16416936e-06
Iter: 913 loss: 1.16929e-06
Iter: 914 loss: 1.16415708e-06
Iter: 915 loss: 1.1638823e-06
Iter: 916 loss: 1.1633847e-06
Iter: 917 loss: 1.17393597e-06
Iter: 918 loss: 1.16339061e-06
Iter: 919 loss: 1.16284093e-06
Iter: 920 loss: 1.16489878e-06
Iter: 921 loss: 1.16267302e-06
Iter: 922 loss: 1.1622767e-06
Iter: 923 loss: 1.16226761e-06
Iter: 924 loss: 1.1620823e-06
Iter: 925 loss: 1.1615432e-06
Iter: 926 loss: 1.16326169e-06
Iter: 927 loss: 1.16131082e-06
Iter: 928 loss: 1.16068622e-06
Iter: 929 loss: 1.16480544e-06
Iter: 930 loss: 1.16062074e-06
Iter: 931 loss: 1.160328e-06
Iter: 932 loss: 1.16025956e-06
Iter: 933 loss: 1.16004162e-06
Iter: 934 loss: 1.15976309e-06
Iter: 935 loss: 1.15974922e-06
Iter: 936 loss: 1.15953617e-06
Iter: 937 loss: 1.15950706e-06
Iter: 938 loss: 1.15931698e-06
Iter: 939 loss: 1.15903697e-06
Iter: 940 loss: 1.16650085e-06
Iter: 941 loss: 1.15903526e-06
Iter: 942 loss: 1.15870739e-06
Iter: 943 loss: 1.16151955e-06
Iter: 944 loss: 1.15869807e-06
Iter: 945 loss: 1.15842158e-06
Iter: 946 loss: 1.15865976e-06
Iter: 947 loss: 1.15821433e-06
Iter: 948 loss: 1.15782882e-06
Iter: 949 loss: 1.15902526e-06
Iter: 950 loss: 1.15769365e-06
Iter: 951 loss: 1.1573494e-06
Iter: 952 loss: 1.1568743e-06
Iter: 953 loss: 1.15684088e-06
Iter: 954 loss: 1.15638591e-06
Iter: 955 loss: 1.15637408e-06
Iter: 956 loss: 1.15594264e-06
Iter: 957 loss: 1.15544231e-06
Iter: 958 loss: 1.15537557e-06
Iter: 959 loss: 1.15485432e-06
Iter: 960 loss: 1.15458624e-06
Iter: 961 loss: 1.15435887e-06
Iter: 962 loss: 1.1537702e-06
Iter: 963 loss: 1.15908722e-06
Iter: 964 loss: 1.15374428e-06
Iter: 965 loss: 1.15336741e-06
Iter: 966 loss: 1.15333933e-06
Iter: 967 loss: 1.15319426e-06
Iter: 968 loss: 1.15299019e-06
Iter: 969 loss: 1.1529628e-06
Iter: 970 loss: 1.1525367e-06
Iter: 971 loss: 1.154492e-06
Iter: 972 loss: 1.15244234e-06
Iter: 973 loss: 1.15219393e-06
Iter: 974 loss: 1.15199339e-06
Iter: 975 loss: 1.15191517e-06
Iter: 976 loss: 1.1515092e-06
Iter: 977 loss: 1.15578086e-06
Iter: 978 loss: 1.1515009e-06
Iter: 979 loss: 1.15123385e-06
Iter: 980 loss: 1.15208468e-06
Iter: 981 loss: 1.15117223e-06
Iter: 982 loss: 1.15088244e-06
Iter: 983 loss: 1.15050295e-06
Iter: 984 loss: 1.15049886e-06
Iter: 985 loss: 1.15008083e-06
Iter: 986 loss: 1.15377168e-06
Iter: 987 loss: 1.15008731e-06
Iter: 988 loss: 1.14970328e-06
Iter: 989 loss: 1.15141415e-06
Iter: 990 loss: 1.1496312e-06
Iter: 991 loss: 1.14941895e-06
Iter: 992 loss: 1.14883255e-06
Iter: 993 loss: 1.15416174e-06
Iter: 994 loss: 1.14875115e-06
Iter: 995 loss: 1.14819068e-06
Iter: 996 loss: 1.1519013e-06
Iter: 997 loss: 1.1481136e-06
Iter: 998 loss: 1.14783211e-06
Iter: 999 loss: 1.14783029e-06
Iter: 1000 loss: 1.1475006e-06
Iter: 1001 loss: 1.14712782e-06
Iter: 1002 loss: 1.14708723e-06
Iter: 1003 loss: 1.14678164e-06
Iter: 1004 loss: 1.1467896e-06
Iter: 1005 loss: 1.14650777e-06
Iter: 1006 loss: 1.14609247e-06
Iter: 1007 loss: 1.1460935e-06
Iter: 1008 loss: 1.14569389e-06
Iter: 1009 loss: 1.14865463e-06
Iter: 1010 loss: 1.14568695e-06
Iter: 1011 loss: 1.14526586e-06
Iter: 1012 loss: 1.14583918e-06
Iter: 1013 loss: 1.14510533e-06
Iter: 1014 loss: 1.14466e-06
Iter: 1015 loss: 1.14604063e-06
Iter: 1016 loss: 1.144566e-06
Iter: 1017 loss: 1.14422323e-06
Iter: 1018 loss: 1.14434977e-06
Iter: 1019 loss: 1.144007e-06
Iter: 1020 loss: 1.14381191e-06
Iter: 1021 loss: 1.14377804e-06
Iter: 1022 loss: 1.14363183e-06
Iter: 1023 loss: 1.14328805e-06
Iter: 1024 loss: 1.14855618e-06
Iter: 1025 loss: 1.14329009e-06
Iter: 1026 loss: 1.14288355e-06
Iter: 1027 loss: 1.14255658e-06
Iter: 1028 loss: 1.14246e-06
Iter: 1029 loss: 1.14185229e-06
Iter: 1030 loss: 1.14493025e-06
Iter: 1031 loss: 1.14174691e-06
Iter: 1032 loss: 1.14124271e-06
Iter: 1033 loss: 1.14123281e-06
Iter: 1034 loss: 1.14091176e-06
Iter: 1035 loss: 1.14036015e-06
Iter: 1036 loss: 1.14037255e-06
Iter: 1037 loss: 1.14030934e-06
Iter: 1038 loss: 1.1401072e-06
Iter: 1039 loss: 1.13993656e-06
Iter: 1040 loss: 1.13955366e-06
Iter: 1041 loss: 1.14450393e-06
Iter: 1042 loss: 1.13952649e-06
Iter: 1043 loss: 1.13924739e-06
Iter: 1044 loss: 1.13925773e-06
Iter: 1045 loss: 1.13896579e-06
Iter: 1046 loss: 1.13962733e-06
Iter: 1047 loss: 1.13888268e-06
Iter: 1048 loss: 1.13863609e-06
Iter: 1049 loss: 1.13839258e-06
Iter: 1050 loss: 1.13835017e-06
Iter: 1051 loss: 1.13800297e-06
Iter: 1052 loss: 1.141555e-06
Iter: 1053 loss: 1.13798637e-06
Iter: 1054 loss: 1.13759711e-06
Iter: 1055 loss: 1.13748274e-06
Iter: 1056 loss: 1.13720535e-06
Iter: 1057 loss: 1.13685132e-06
Iter: 1058 loss: 1.13673445e-06
Iter: 1059 loss: 1.13650094e-06
Iter: 1060 loss: 1.13603892e-06
Iter: 1061 loss: 1.13743931e-06
Iter: 1062 loss: 1.13591318e-06
Iter: 1063 loss: 1.13567125e-06
Iter: 1064 loss: 1.13567523e-06
Iter: 1065 loss: 1.13535305e-06
Iter: 1066 loss: 1.13596639e-06
Iter: 1067 loss: 1.13525311e-06
Iter: 1068 loss: 1.13511226e-06
Iter: 1069 loss: 1.13565829e-06
Iter: 1070 loss: 1.13505496e-06
Iter: 1071 loss: 1.13485396e-06
Iter: 1072 loss: 1.1355703e-06
Iter: 1073 loss: 1.13478768e-06
Iter: 1074 loss: 1.13466501e-06
Iter: 1075 loss: 1.13445628e-06
Iter: 1076 loss: 1.13445356e-06
Iter: 1077 loss: 1.13418889e-06
Iter: 1078 loss: 1.1379916e-06
Iter: 1079 loss: 1.13419014e-06
Iter: 1080 loss: 1.13404121e-06
Iter: 1081 loss: 1.13391206e-06
Iter: 1082 loss: 1.13385181e-06
Iter: 1083 loss: 1.13361932e-06
Iter: 1084 loss: 1.13407168e-06
Iter: 1085 loss: 1.13350279e-06
Iter: 1086 loss: 1.13330191e-06
Iter: 1087 loss: 1.13624299e-06
Iter: 1088 loss: 1.13330475e-06
Iter: 1089 loss: 1.13313331e-06
Iter: 1090 loss: 1.13274029e-06
Iter: 1091 loss: 1.13694136e-06
Iter: 1092 loss: 1.13265287e-06
Iter: 1093 loss: 1.13221859e-06
Iter: 1094 loss: 1.13292845e-06
Iter: 1095 loss: 1.13204453e-06
Iter: 1096 loss: 1.1315974e-06
Iter: 1097 loss: 1.13414012e-06
Iter: 1098 loss: 1.13152123e-06
Iter: 1099 loss: 1.13141891e-06
Iter: 1100 loss: 1.13135297e-06
Iter: 1101 loss: 1.13122417e-06
Iter: 1102 loss: 1.13102851e-06
Iter: 1103 loss: 1.13100032e-06
Iter: 1104 loss: 1.13092392e-06
Iter: 1105 loss: 1.13090016e-06
Iter: 1106 loss: 1.13077783e-06
Iter: 1107 loss: 1.13045576e-06
Iter: 1108 loss: 1.13224485e-06
Iter: 1109 loss: 1.1303548e-06
Iter: 1110 loss: 1.13011197e-06
Iter: 1111 loss: 1.13009287e-06
Iter: 1112 loss: 1.12987698e-06
Iter: 1113 loss: 1.12967462e-06
Iter: 1114 loss: 1.12960402e-06
Iter: 1115 loss: 1.12925181e-06
Iter: 1116 loss: 1.12941018e-06
Iter: 1117 loss: 1.1289809e-06
Iter: 1118 loss: 1.12861972e-06
Iter: 1119 loss: 1.1300391e-06
Iter: 1120 loss: 1.12854536e-06
Iter: 1121 loss: 1.12819828e-06
Iter: 1122 loss: 1.12895032e-06
Iter: 1123 loss: 1.12806276e-06
Iter: 1124 loss: 1.12785324e-06
Iter: 1125 loss: 1.12762154e-06
Iter: 1126 loss: 1.12757414e-06
Iter: 1127 loss: 1.12718158e-06
Iter: 1128 loss: 1.12781436e-06
Iter: 1129 loss: 1.12701662e-06
Iter: 1130 loss: 1.12661337e-06
Iter: 1131 loss: 1.12851785e-06
Iter: 1132 loss: 1.12653549e-06
Iter: 1133 loss: 1.1259948e-06
Iter: 1134 loss: 1.12927069e-06
Iter: 1135 loss: 1.12593125e-06
Iter: 1136 loss: 1.12564203e-06
Iter: 1137 loss: 1.12556199e-06
Iter: 1138 loss: 1.12537532e-06
Iter: 1139 loss: 1.1249e-06
Iter: 1140 loss: 1.13076669e-06
Iter: 1141 loss: 1.12490466e-06
Iter: 1142 loss: 1.12473867e-06
Iter: 1143 loss: 1.12465034e-06
Iter: 1144 loss: 1.12456337e-06
Iter: 1145 loss: 1.12428245e-06
Iter: 1146 loss: 1.12635666e-06
Iter: 1147 loss: 1.12426e-06
Iter: 1148 loss: 1.1241184e-06
Iter: 1149 loss: 1.12417581e-06
Iter: 1150 loss: 1.12400915e-06
Iter: 1151 loss: 1.1238053e-06
Iter: 1152 loss: 1.12429984e-06
Iter: 1153 loss: 1.12372118e-06
Iter: 1154 loss: 1.12351006e-06
Iter: 1155 loss: 1.12388921e-06
Iter: 1156 loss: 1.12342411e-06
Iter: 1157 loss: 1.1231466e-06
Iter: 1158 loss: 1.12268458e-06
Iter: 1159 loss: 1.12267662e-06
Iter: 1160 loss: 1.12213911e-06
Iter: 1161 loss: 1.1226789e-06
Iter: 1162 loss: 1.12185444e-06
Iter: 1163 loss: 1.12127464e-06
Iter: 1164 loss: 1.12469479e-06
Iter: 1165 loss: 1.12120188e-06
Iter: 1166 loss: 1.12104397e-06
Iter: 1167 loss: 1.12096518e-06
Iter: 1168 loss: 1.12076907e-06
Iter: 1169 loss: 1.12057126e-06
Iter: 1170 loss: 1.12054067e-06
Iter: 1171 loss: 1.12037026e-06
Iter: 1172 loss: 1.12036287e-06
Iter: 1173 loss: 1.12015039e-06
Iter: 1174 loss: 1.11977033e-06
Iter: 1175 loss: 1.1270389e-06
Iter: 1176 loss: 1.11976385e-06
Iter: 1177 loss: 1.11962731e-06
Iter: 1178 loss: 1.11958957e-06
Iter: 1179 loss: 1.11942029e-06
Iter: 1180 loss: 1.11919871e-06
Iter: 1181 loss: 1.11920133e-06
Iter: 1182 loss: 1.11895929e-06
Iter: 1183 loss: 1.12207681e-06
Iter: 1184 loss: 1.11896395e-06
Iter: 1185 loss: 1.11878956e-06
Iter: 1186 loss: 1.11873192e-06
Iter: 1187 loss: 1.11865552e-06
Iter: 1188 loss: 1.11834572e-06
Iter: 1189 loss: 1.11868326e-06
Iter: 1190 loss: 1.11821589e-06
Iter: 1191 loss: 1.1179186e-06
Iter: 1192 loss: 1.11765735e-06
Iter: 1193 loss: 1.11758868e-06
Iter: 1194 loss: 1.11699114e-06
Iter: 1195 loss: 1.11693384e-06
Iter: 1196 loss: 1.11649138e-06
Iter: 1197 loss: 1.11589134e-06
Iter: 1198 loss: 1.12441035e-06
Iter: 1199 loss: 1.11589725e-06
Iter: 1200 loss: 1.11552708e-06
Iter: 1201 loss: 1.11553686e-06
Iter: 1202 loss: 1.11533939e-06
Iter: 1203 loss: 1.11515635e-06
Iter: 1204 loss: 1.11509712e-06
Iter: 1205 loss: 1.11488725e-06
Iter: 1206 loss: 1.11488635e-06
Iter: 1207 loss: 1.11477152e-06
Iter: 1208 loss: 1.11454722e-06
Iter: 1209 loss: 1.1175631e-06
Iter: 1210 loss: 1.11453119e-06
Iter: 1211 loss: 1.11425959e-06
Iter: 1212 loss: 1.11811914e-06
Iter: 1213 loss: 1.11425811e-06
Iter: 1214 loss: 1.11409736e-06
Iter: 1215 loss: 1.11378745e-06
Iter: 1216 loss: 1.12124735e-06
Iter: 1217 loss: 1.11379097e-06
Iter: 1218 loss: 1.11336431e-06
Iter: 1219 loss: 1.1172599e-06
Iter: 1220 loss: 1.11334e-06
Iter: 1221 loss: 1.11315251e-06
Iter: 1222 loss: 1.11319696e-06
Iter: 1223 loss: 1.1129905e-06
Iter: 1224 loss: 1.11267741e-06
Iter: 1225 loss: 1.11252371e-06
Iter: 1226 loss: 1.11235386e-06
Iter: 1227 loss: 1.1119464e-06
Iter: 1228 loss: 1.11382815e-06
Iter: 1229 loss: 1.11184886e-06
Iter: 1230 loss: 1.11152485e-06
Iter: 1231 loss: 1.11151826e-06
Iter: 1232 loss: 1.11125826e-06
Iter: 1233 loss: 1.11082545e-06
Iter: 1234 loss: 1.11254633e-06
Iter: 1235 loss: 1.11068755e-06
Iter: 1236 loss: 1.11033182e-06
Iter: 1237 loss: 1.11032773e-06
Iter: 1238 loss: 1.11016436e-06
Iter: 1239 loss: 1.11018653e-06
Iter: 1240 loss: 1.11003362e-06
Iter: 1241 loss: 1.10973838e-06
Iter: 1242 loss: 1.11042266e-06
Iter: 1243 loss: 1.10965175e-06
Iter: 1244 loss: 1.10945052e-06
Iter: 1245 loss: 1.10946985e-06
Iter: 1246 loss: 1.1093299e-06
Iter: 1247 loss: 1.10906808e-06
Iter: 1248 loss: 1.11163217e-06
Iter: 1249 loss: 1.1090624e-06
Iter: 1250 loss: 1.10890824e-06
Iter: 1251 loss: 1.10872327e-06
Iter: 1252 loss: 1.10871099e-06
Iter: 1253 loss: 1.10852204e-06
Iter: 1254 loss: 1.10850669e-06
Iter: 1255 loss: 1.10840483e-06
Iter: 1256 loss: 1.10816688e-06
Iter: 1257 loss: 1.11068664e-06
Iter: 1258 loss: 1.10816018e-06
Iter: 1259 loss: 1.10786016e-06
Iter: 1260 loss: 1.10989822e-06
Iter: 1261 loss: 1.10784572e-06
Iter: 1262 loss: 1.10761789e-06
Iter: 1263 loss: 1.10757514e-06
Iter: 1264 loss: 1.10742803e-06
Iter: 1265 loss: 1.1071163e-06
Iter: 1266 loss: 1.10747419e-06
Iter: 1267 loss: 1.10694225e-06
Iter: 1268 loss: 1.10657993e-06
Iter: 1269 loss: 1.10874316e-06
Iter: 1270 loss: 1.10652707e-06
Iter: 1271 loss: 1.10623625e-06
Iter: 1272 loss: 1.10692019e-06
Iter: 1273 loss: 1.10615838e-06
Iter: 1274 loss: 1.10586018e-06
Iter: 1275 loss: 1.10987162e-06
Iter: 1276 loss: 1.10585768e-06
Iter: 1277 loss: 1.10570454e-06
Iter: 1278 loss: 1.10540577e-06
Iter: 1279 loss: 1.10540191e-06
Iter: 1280 loss: 1.10536985e-06
Iter: 1281 loss: 1.10525048e-06
Iter: 1282 loss: 1.10515339e-06
Iter: 1283 loss: 1.10495432e-06
Iter: 1284 loss: 1.10885912e-06
Iter: 1285 loss: 1.10495148e-06
Iter: 1286 loss: 1.10482335e-06
Iter: 1287 loss: 1.1048121e-06
Iter: 1288 loss: 1.10469409e-06
Iter: 1289 loss: 1.1045513e-06
Iter: 1290 loss: 1.10454152e-06
Iter: 1291 loss: 1.10440828e-06
Iter: 1292 loss: 1.10606891e-06
Iter: 1293 loss: 1.10442534e-06
Iter: 1294 loss: 1.10429437e-06
Iter: 1295 loss: 1.10436838e-06
Iter: 1296 loss: 1.10421536e-06
Iter: 1297 loss: 1.10403289e-06
Iter: 1298 loss: 1.10393762e-06
Iter: 1299 loss: 1.10387521e-06
Iter: 1300 loss: 1.10360611e-06
Iter: 1301 loss: 1.10339602e-06
Iter: 1302 loss: 1.10331746e-06
Iter: 1303 loss: 1.10293195e-06
Iter: 1304 loss: 1.10388203e-06
Iter: 1305 loss: 1.10278916e-06
Iter: 1306 loss: 1.1025852e-06
Iter: 1307 loss: 1.10254371e-06
Iter: 1308 loss: 1.10226085e-06
Iter: 1309 loss: 1.10212932e-06
Iter: 1310 loss: 1.10202313e-06
Iter: 1311 loss: 1.10175301e-06
Iter: 1312 loss: 1.10333337e-06
Iter: 1313 loss: 1.10173494e-06
Iter: 1314 loss: 1.10144083e-06
Iter: 1315 loss: 1.10212181e-06
Iter: 1316 loss: 1.10135784e-06
Iter: 1317 loss: 1.1011698e-06
Iter: 1318 loss: 1.10136011e-06
Iter: 1319 loss: 1.10109568e-06
Iter: 1320 loss: 1.10086921e-06
Iter: 1321 loss: 1.10229189e-06
Iter: 1322 loss: 1.10082192e-06
Iter: 1323 loss: 1.10070334e-06
Iter: 1324 loss: 1.10048518e-06
Iter: 1325 loss: 1.10493568e-06
Iter: 1326 loss: 1.10048336e-06
Iter: 1327 loss: 1.10027622e-06
Iter: 1328 loss: 1.10027599e-06
Iter: 1329 loss: 1.10014457e-06
Iter: 1330 loss: 1.09993653e-06
Iter: 1331 loss: 1.09996517e-06
Iter: 1332 loss: 1.09966572e-06
Iter: 1333 loss: 1.09990617e-06
Iter: 1334 loss: 1.09948803e-06
Iter: 1335 loss: 1.09914208e-06
Iter: 1336 loss: 1.09929624e-06
Iter: 1337 loss: 1.09891664e-06
Iter: 1338 loss: 1.09852044e-06
Iter: 1339 loss: 1.09960729e-06
Iter: 1340 loss: 1.09839436e-06
Iter: 1341 loss: 1.0983988e-06
Iter: 1342 loss: 1.09823395e-06
Iter: 1343 loss: 1.09813448e-06
Iter: 1344 loss: 1.09785117e-06
Iter: 1345 loss: 1.10088536e-06
Iter: 1346 loss: 1.0978448e-06
Iter: 1347 loss: 1.0976986e-06
Iter: 1348 loss: 1.09766984e-06
Iter: 1349 loss: 1.09751704e-06
Iter: 1350 loss: 1.09719326e-06
Iter: 1351 loss: 1.10286578e-06
Iter: 1352 loss: 1.09717803e-06
Iter: 1353 loss: 1.09696384e-06
Iter: 1354 loss: 1.09696134e-06
Iter: 1355 loss: 1.09672021e-06
Iter: 1356 loss: 1.09625444e-06
Iter: 1357 loss: 1.10511405e-06
Iter: 1358 loss: 1.09627263e-06
Iter: 1359 loss: 1.09586983e-06
Iter: 1360 loss: 1.09742314e-06
Iter: 1361 loss: 1.09577593e-06
Iter: 1362 loss: 1.09533778e-06
Iter: 1363 loss: 1.09784264e-06
Iter: 1364 loss: 1.09528401e-06
Iter: 1365 loss: 1.09507312e-06
Iter: 1366 loss: 1.09475786e-06
Iter: 1367 loss: 1.10200062e-06
Iter: 1368 loss: 1.094764e-06
Iter: 1369 loss: 1.09439179e-06
Iter: 1370 loss: 1.09516327e-06
Iter: 1371 loss: 1.094254e-06
Iter: 1372 loss: 1.09393545e-06
Iter: 1373 loss: 1.09666485e-06
Iter: 1374 loss: 1.09391067e-06
Iter: 1375 loss: 1.09368011e-06
Iter: 1376 loss: 1.09485381e-06
Iter: 1377 loss: 1.09362907e-06
Iter: 1378 loss: 1.09340749e-06
Iter: 1379 loss: 1.09469704e-06
Iter: 1380 loss: 1.09336338e-06
Iter: 1381 loss: 1.09316761e-06
Iter: 1382 loss: 1.09296809e-06
Iter: 1383 loss: 1.09294535e-06
Iter: 1384 loss: 1.09272e-06
Iter: 1385 loss: 1.09271718e-06
Iter: 1386 loss: 1.09251346e-06
Iter: 1387 loss: 1.09224698e-06
Iter: 1388 loss: 1.0922364e-06
Iter: 1389 loss: 1.09208941e-06
Iter: 1390 loss: 1.09207735e-06
Iter: 1391 loss: 1.09191637e-06
Iter: 1392 loss: 1.0915594e-06
Iter: 1393 loss: 1.09761527e-06
Iter: 1394 loss: 1.09155462e-06
Iter: 1395 loss: 1.09128814e-06
Iter: 1396 loss: 1.09357836e-06
Iter: 1397 loss: 1.0912845e-06
Iter: 1398 loss: 1.09106372e-06
Iter: 1399 loss: 1.09253153e-06
Iter: 1400 loss: 1.09105133e-06
Iter: 1401 loss: 1.09089194e-06
Iter: 1402 loss: 1.0905751e-06
Iter: 1403 loss: 1.0932082e-06
Iter: 1404 loss: 1.09053826e-06
Iter: 1405 loss: 1.09016855e-06
Iter: 1406 loss: 1.09066946e-06
Iter: 1407 loss: 1.08999257e-06
Iter: 1408 loss: 1.0894812e-06
Iter: 1409 loss: 1.09141388e-06
Iter: 1410 loss: 1.08935933e-06
Iter: 1411 loss: 1.0889778e-06
Iter: 1412 loss: 1.0889737e-06
Iter: 1413 loss: 1.08865709e-06
Iter: 1414 loss: 1.08924132e-06
Iter: 1415 loss: 1.08849849e-06
Iter: 1416 loss: 1.0881854e-06
Iter: 1417 loss: 1.08846234e-06
Iter: 1418 loss: 1.08799986e-06
Iter: 1419 loss: 1.0877859e-06
Iter: 1420 loss: 1.08779e-06
Iter: 1421 loss: 1.08762356e-06
Iter: 1422 loss: 1.08738823e-06
Iter: 1423 loss: 1.08737208e-06
Iter: 1424 loss: 1.08718245e-06
Iter: 1425 loss: 1.08719314e-06
Iter: 1426 loss: 1.08701659e-06
Iter: 1427 loss: 1.0866687e-06
Iter: 1428 loss: 1.09156258e-06
Iter: 1429 loss: 1.08664904e-06
Iter: 1430 loss: 1.08643815e-06
Iter: 1431 loss: 1.0864419e-06
Iter: 1432 loss: 1.0861977e-06
Iter: 1433 loss: 1.08578445e-06
Iter: 1434 loss: 1.08578979e-06
Iter: 1435 loss: 1.08534994e-06
Iter: 1436 loss: 1.08508834e-06
Iter: 1437 loss: 1.08489041e-06
Iter: 1438 loss: 1.0844044e-06
Iter: 1439 loss: 1.0872991e-06
Iter: 1440 loss: 1.08432232e-06
Iter: 1441 loss: 1.08403015e-06
Iter: 1442 loss: 1.08688948e-06
Iter: 1443 loss: 1.08403628e-06
Iter: 1444 loss: 1.08376e-06
Iter: 1445 loss: 1.0858e-06
Iter: 1446 loss: 1.08374797e-06
Iter: 1447 loss: 1.08353993e-06
Iter: 1448 loss: 1.08389338e-06
Iter: 1449 loss: 1.08348627e-06
Iter: 1450 loss: 1.08332961e-06
Iter: 1451 loss: 1.08404015e-06
Iter: 1452 loss: 1.08329402e-06
Iter: 1453 loss: 1.08310257e-06
Iter: 1454 loss: 1.08309e-06
Iter: 1455 loss: 1.08291192e-06
Iter: 1456 loss: 1.08265704e-06
Iter: 1457 loss: 1.08317249e-06
Iter: 1458 loss: 1.08255222e-06
Iter: 1459 loss: 1.08224344e-06
Iter: 1460 loss: 1.08355141e-06
Iter: 1461 loss: 1.08216364e-06
Iter: 1462 loss: 1.08197924e-06
Iter: 1463 loss: 1.08173572e-06
Iter: 1464 loss: 1.08169991e-06
Iter: 1465 loss: 1.08138352e-06
Iter: 1466 loss: 1.0865549e-06
Iter: 1467 loss: 1.08138806e-06
Iter: 1468 loss: 1.08120594e-06
Iter: 1469 loss: 1.08076392e-06
Iter: 1470 loss: 1.08522613e-06
Iter: 1471 loss: 1.08069776e-06
Iter: 1472 loss: 1.08018105e-06
Iter: 1473 loss: 1.08069219e-06
Iter: 1474 loss: 1.07986534e-06
Iter: 1475 loss: 1.07917049e-06
Iter: 1476 loss: 1.081351e-06
Iter: 1477 loss: 1.07895949e-06
Iter: 1478 loss: 1.07875462e-06
Iter: 1479 loss: 1.07868141e-06
Iter: 1480 loss: 1.07839332e-06
Iter: 1481 loss: 1.07833648e-06
Iter: 1482 loss: 1.07813844e-06
Iter: 1483 loss: 1.07781091e-06
Iter: 1484 loss: 1.07955464e-06
Iter: 1485 loss: 1.07777555e-06
Iter: 1486 loss: 1.07744381e-06
Iter: 1487 loss: 1.07786229e-06
Iter: 1488 loss: 1.0772734e-06
Iter: 1489 loss: 1.0768731e-06
Iter: 1490 loss: 1.07711094e-06
Iter: 1491 loss: 1.07660867e-06
Iter: 1492 loss: 1.07621008e-06
Iter: 1493 loss: 1.08045583e-06
Iter: 1494 loss: 1.07621804e-06
Iter: 1495 loss: 1.07588312e-06
Iter: 1496 loss: 1.07536687e-06
Iter: 1497 loss: 1.0753696e-06
Iter: 1498 loss: 1.07494293e-06
Iter: 1499 loss: 1.0812812e-06
Iter: 1500 loss: 1.07493634e-06
Iter: 1501 loss: 1.07454207e-06
Iter: 1502 loss: 1.07464814e-06
Iter: 1503 loss: 1.07422488e-06
Iter: 1504 loss: 1.07396477e-06
Iter: 1505 loss: 1.0736693e-06
Iter: 1506 loss: 1.07360097e-06
Iter: 1507 loss: 1.07316896e-06
Iter: 1508 loss: 1.07350763e-06
Iter: 1509 loss: 1.07290805e-06
Iter: 1510 loss: 1.07232756e-06
Iter: 1511 loss: 1.07687856e-06
Iter: 1512 loss: 1.0722797e-06
Iter: 1513 loss: 1.07198048e-06
Iter: 1514 loss: 1.0719649e-06
Iter: 1515 loss: 1.07173446e-06
Iter: 1516 loss: 1.07124174e-06
Iter: 1517 loss: 1.08047652e-06
Iter: 1518 loss: 1.07126129e-06
Iter: 1519 loss: 1.07083224e-06
Iter: 1520 loss: 1.07083747e-06
Iter: 1521 loss: 1.07057213e-06
Iter: 1522 loss: 1.07030132e-06
Iter: 1523 loss: 1.07025153e-06
Iter: 1524 loss: 1.0699132e-06
Iter: 1525 loss: 1.07393078e-06
Iter: 1526 loss: 1.06990342e-06
Iter: 1527 loss: 1.06964217e-06
Iter: 1528 loss: 1.06983509e-06
Iter: 1529 loss: 1.06949858e-06
Iter: 1530 loss: 1.0692346e-06
Iter: 1531 loss: 1.06929224e-06
Iter: 1532 loss: 1.06900632e-06
Iter: 1533 loss: 1.0686623e-06
Iter: 1534 loss: 1.0736228e-06
Iter: 1535 loss: 1.06866048e-06
Iter: 1536 loss: 1.06845755e-06
Iter: 1537 loss: 1.0680551e-06
Iter: 1538 loss: 1.07477513e-06
Iter: 1539 loss: 1.0680526e-06
Iter: 1540 loss: 1.06758785e-06
Iter: 1541 loss: 1.06785637e-06
Iter: 1542 loss: 1.06730545e-06
Iter: 1543 loss: 1.06667562e-06
Iter: 1544 loss: 1.06730499e-06
Iter: 1545 loss: 1.06631751e-06
Iter: 1546 loss: 1.06612015e-06
Iter: 1547 loss: 1.06602852e-06
Iter: 1548 loss: 1.06563584e-06
Iter: 1549 loss: 1.0654278e-06
Iter: 1550 loss: 1.06527045e-06
Iter: 1551 loss: 1.06482037e-06
Iter: 1552 loss: 1.06746063e-06
Iter: 1553 loss: 1.06477023e-06
Iter: 1554 loss: 1.06425455e-06
Iter: 1555 loss: 1.06547191e-06
Iter: 1556 loss: 1.06407992e-06
Iter: 1557 loss: 1.06382129e-06
Iter: 1558 loss: 1.06451853e-06
Iter: 1559 loss: 1.06373193e-06
Iter: 1560 loss: 1.06341452e-06
Iter: 1561 loss: 1.06429752e-06
Iter: 1562 loss: 1.06332323e-06
Iter: 1563 loss: 1.06304935e-06
Iter: 1564 loss: 1.06352445e-06
Iter: 1565 loss: 1.06295738e-06
Iter: 1566 loss: 1.0627632e-06
Iter: 1567 loss: 1.06429502e-06
Iter: 1568 loss: 1.0627632e-06
Iter: 1569 loss: 1.06255038e-06
Iter: 1570 loss: 1.06244022e-06
Iter: 1571 loss: 1.06233506e-06
Iter: 1572 loss: 1.06211417e-06
Iter: 1573 loss: 1.06169568e-06
Iter: 1574 loss: 1.0617e-06
Iter: 1575 loss: 1.06111247e-06
Iter: 1576 loss: 1.06245579e-06
Iter: 1577 loss: 1.06089703e-06
Iter: 1578 loss: 1.06043512e-06
Iter: 1579 loss: 1.06317771e-06
Iter: 1580 loss: 1.06038237e-06
Iter: 1581 loss: 1.06012021e-06
Iter: 1582 loss: 1.06008849e-06
Iter: 1583 loss: 1.05990523e-06
Iter: 1584 loss: 1.05966626e-06
Iter: 1585 loss: 1.05965648e-06
Iter: 1586 loss: 1.05950653e-06
Iter: 1587 loss: 1.05948016e-06
Iter: 1588 loss: 1.05936931e-06
Iter: 1589 loss: 1.05901563e-06
Iter: 1590 loss: 1.06134257e-06
Iter: 1591 loss: 1.05897743e-06
Iter: 1592 loss: 1.05862114e-06
Iter: 1593 loss: 1.05862364e-06
Iter: 1594 loss: 1.05837171e-06
Iter: 1595 loss: 1.0578633e-06
Iter: 1596 loss: 1.05787035e-06
Iter: 1597 loss: 1.0575593e-06
Iter: 1598 loss: 1.05756021e-06
Iter: 1599 loss: 1.05732374e-06
Iter: 1600 loss: 1.0571315e-06
Iter: 1601 loss: 1.05707204e-06
Iter: 1602 loss: 1.05669187e-06
Iter: 1603 loss: 1.05763752e-06
Iter: 1604 loss: 1.05658819e-06
Iter: 1605 loss: 1.05631125e-06
Iter: 1606 loss: 1.05611525e-06
Iter: 1607 loss: 1.05601055e-06
Iter: 1608 loss: 1.05566392e-06
Iter: 1609 loss: 1.05668073e-06
Iter: 1610 loss: 1.05551817e-06
Iter: 1611 loss: 1.05511913e-06
Iter: 1612 loss: 1.05786603e-06
Iter: 1613 loss: 1.05508809e-06
Iter: 1614 loss: 1.05472748e-06
Iter: 1615 loss: 1.05907884e-06
Iter: 1616 loss: 1.05474442e-06
Iter: 1617 loss: 1.05455183e-06
Iter: 1618 loss: 1.05435151e-06
Iter: 1619 loss: 1.05434378e-06
Iter: 1620 loss: 1.05399818e-06
Iter: 1621 loss: 1.05740253e-06
Iter: 1622 loss: 1.05395861e-06
Iter: 1623 loss: 1.05378786e-06
Iter: 1624 loss: 1.05353547e-06
Iter: 1625 loss: 1.05354775e-06
Iter: 1626 loss: 1.05315473e-06
Iter: 1627 loss: 1.05660297e-06
Iter: 1628 loss: 1.05312677e-06
Iter: 1629 loss: 1.05296044e-06
Iter: 1630 loss: 1.05271954e-06
Iter: 1631 loss: 1.05271988e-06
Iter: 1632 loss: 1.05246556e-06
Iter: 1633 loss: 1.05245772e-06
Iter: 1634 loss: 1.05227639e-06
Iter: 1635 loss: 1.05187155e-06
Iter: 1636 loss: 1.05806794e-06
Iter: 1637 loss: 1.05182721e-06
Iter: 1638 loss: 1.05154925e-06
Iter: 1639 loss: 1.05382105e-06
Iter: 1640 loss: 1.05151616e-06
Iter: 1641 loss: 1.0512772e-06
Iter: 1642 loss: 1.05192532e-06
Iter: 1643 loss: 1.05121717e-06
Iter: 1644 loss: 1.05103163e-06
Iter: 1645 loss: 1.05101208e-06
Iter: 1646 loss: 1.05088043e-06
Iter: 1647 loss: 1.05078686e-06
Iter: 1648 loss: 1.05074798e-06
Iter: 1649 loss: 1.05056165e-06
Iter: 1650 loss: 1.05036281e-06
Iter: 1651 loss: 1.05033791e-06
Iter: 1652 loss: 1.0501559e-06
Iter: 1653 loss: 1.05296635e-06
Iter: 1654 loss: 1.05014578e-06
Iter: 1655 loss: 1.04994569e-06
Iter: 1656 loss: 1.04951391e-06
Iter: 1657 loss: 1.056153e-06
Iter: 1658 loss: 1.04950209e-06
Iter: 1659 loss: 1.04917376e-06
Iter: 1660 loss: 1.04917626e-06
Iter: 1661 loss: 1.04885441e-06
Iter: 1662 loss: 1.04890978e-06
Iter: 1663 loss: 1.0486051e-06
Iter: 1664 loss: 1.04839228e-06
Iter: 1665 loss: 1.04888693e-06
Iter: 1666 loss: 1.04831611e-06
Iter: 1667 loss: 1.04801984e-06
Iter: 1668 loss: 1.04912965e-06
Iter: 1669 loss: 1.04794628e-06
Iter: 1670 loss: 1.04777052e-06
Iter: 1671 loss: 1.04749847e-06
Iter: 1672 loss: 1.04749506e-06
Iter: 1673 loss: 1.04712706e-06
Iter: 1674 loss: 1.04814762e-06
Iter: 1675 loss: 1.0469978e-06
Iter: 1676 loss: 1.04660148e-06
Iter: 1677 loss: 1.0480029e-06
Iter: 1678 loss: 1.04649098e-06
Iter: 1679 loss: 1.04616493e-06
Iter: 1680 loss: 1.04609182e-06
Iter: 1681 loss: 1.04586604e-06
Iter: 1682 loss: 1.04550668e-06
Iter: 1683 loss: 1.04814922e-06
Iter: 1684 loss: 1.04547109e-06
Iter: 1685 loss: 1.04530807e-06
Iter: 1686 loss: 1.04526532e-06
Iter: 1687 loss: 1.0451156e-06
Iter: 1688 loss: 1.04480364e-06
Iter: 1689 loss: 1.05025219e-06
Iter: 1690 loss: 1.04479864e-06
Iter: 1691 loss: 1.04463822e-06
Iter: 1692 loss: 1.04462856e-06
Iter: 1693 loss: 1.04449532e-06
Iter: 1694 loss: 1.04412015e-06
Iter: 1695 loss: 1.04660421e-06
Iter: 1696 loss: 1.04404842e-06
Iter: 1697 loss: 1.04382855e-06
Iter: 1698 loss: 1.04382809e-06
Iter: 1699 loss: 1.04356127e-06
Iter: 1700 loss: 1.0434519e-06
Iter: 1701 loss: 1.04329581e-06
Iter: 1702 loss: 1.04308083e-06
Iter: 1703 loss: 1.04375431e-06
Iter: 1704 loss: 1.0430025e-06
Iter: 1705 loss: 1.0427184e-06
Iter: 1706 loss: 1.0437343e-06
Iter: 1707 loss: 1.04260835e-06
Iter: 1708 loss: 1.04241815e-06
Iter: 1709 loss: 1.04207834e-06
Iter: 1710 loss: 1.04917808e-06
Iter: 1711 loss: 1.04207766e-06
Iter: 1712 loss: 1.04164155e-06
Iter: 1713 loss: 1.04188689e-06
Iter: 1714 loss: 1.04139053e-06
Iter: 1715 loss: 1.04105698e-06
Iter: 1716 loss: 1.04541141e-06
Iter: 1717 loss: 1.04105459e-06
Iter: 1718 loss: 1.04082847e-06
Iter: 1719 loss: 1.04146602e-06
Iter: 1720 loss: 1.04074365e-06
Iter: 1721 loss: 1.04047876e-06
Iter: 1722 loss: 1.04235176e-06
Iter: 1723 loss: 1.04044943e-06
Iter: 1724 loss: 1.04034461e-06
Iter: 1725 loss: 1.04042647e-06
Iter: 1726 loss: 1.04026128e-06
Iter: 1727 loss: 1.04009757e-06
Iter: 1728 loss: 1.04069375e-06
Iter: 1729 loss: 1.04003379e-06
Iter: 1730 loss: 1.03991488e-06
Iter: 1731 loss: 1.03967125e-06
Iter: 1732 loss: 1.04417245e-06
Iter: 1733 loss: 1.03966647e-06
Iter: 1734 loss: 1.03954983e-06
Iter: 1735 loss: 1.03950811e-06
Iter: 1736 loss: 1.03934815e-06
Iter: 1737 loss: 1.03908212e-06
Iter: 1738 loss: 1.04396531e-06
Iter: 1739 loss: 1.03906837e-06
Iter: 1740 loss: 1.03890011e-06
Iter: 1741 loss: 1.03890761e-06
Iter: 1742 loss: 1.03871571e-06
Iter: 1743 loss: 1.03831394e-06
Iter: 1744 loss: 1.04426272e-06
Iter: 1745 loss: 1.0383169e-06
Iter: 1746 loss: 1.03799562e-06
Iter: 1747 loss: 1.03834168e-06
Iter: 1748 loss: 1.03779348e-06
Iter: 1749 loss: 1.03740717e-06
Iter: 1750 loss: 1.03966818e-06
Iter: 1751 loss: 1.03736443e-06
Iter: 1752 loss: 1.03705122e-06
Iter: 1753 loss: 1.03750676e-06
Iter: 1754 loss: 1.0368974e-06
Iter: 1755 loss: 1.03668174e-06
Iter: 1756 loss: 1.03667765e-06
Iter: 1757 loss: 1.03645402e-06
Iter: 1758 loss: 1.03689626e-06
Iter: 1759 loss: 1.0363832e-06
Iter: 1760 loss: 1.03624893e-06
Iter: 1761 loss: 1.03678281e-06
Iter: 1762 loss: 1.03620073e-06
Iter: 1763 loss: 1.03603054e-06
Iter: 1764 loss: 1.03646562e-06
Iter: 1765 loss: 1.03599166e-06
Iter: 1766 loss: 1.03585649e-06
Iter: 1767 loss: 1.03570721e-06
Iter: 1768 loss: 1.03567982e-06
Iter: 1769 loss: 1.03550212e-06
Iter: 1770 loss: 1.03549462e-06
Iter: 1771 loss: 1.03533921e-06
Iter: 1772 loss: 1.03511161e-06
Iter: 1773 loss: 1.03510615e-06
Iter: 1774 loss: 1.03487162e-06
Iter: 1775 loss: 1.0373177e-06
Iter: 1776 loss: 1.03486695e-06
Iter: 1777 loss: 1.03464185e-06
Iter: 1778 loss: 1.03447042e-06
Iter: 1779 loss: 1.03436582e-06
Iter: 1780 loss: 1.03413083e-06
Iter: 1781 loss: 1.03400066e-06
Iter: 1782 loss: 1.03390357e-06
Iter: 1783 loss: 1.03357434e-06
Iter: 1784 loss: 1.03398202e-06
Iter: 1785 loss: 1.03340903e-06
Iter: 1786 loss: 1.03307457e-06
Iter: 1787 loss: 1.0366681e-06
Iter: 1788 loss: 1.03304399e-06
Iter: 1789 loss: 1.03271987e-06
Iter: 1790 loss: 1.0353674e-06
Iter: 1791 loss: 1.03273305e-06
Iter: 1792 loss: 1.03248158e-06
Iter: 1793 loss: 1.03241973e-06
Iter: 1794 loss: 1.03224625e-06
Iter: 1795 loss: 1.03187449e-06
Iter: 1796 loss: 1.03401e-06
Iter: 1797 loss: 1.03182197e-06
Iter: 1798 loss: 1.03148773e-06
Iter: 1799 loss: 1.03123034e-06
Iter: 1800 loss: 1.03113871e-06
Iter: 1801 loss: 1.03086745e-06
Iter: 1802 loss: 1.03446587e-06
Iter: 1803 loss: 1.03085392e-06
Iter: 1804 loss: 1.03064974e-06
Iter: 1805 loss: 1.03161983e-06
Iter: 1806 loss: 1.03058903e-06
Iter: 1807 loss: 1.03045863e-06
Iter: 1808 loss: 1.03036405e-06
Iter: 1809 loss: 1.03032323e-06
Iter: 1810 loss: 1.03014747e-06
Iter: 1811 loss: 1.03013792e-06
Iter: 1812 loss: 1.03005391e-06
Iter: 1813 loss: 1.02985291e-06
Iter: 1814 loss: 1.03306388e-06
Iter: 1815 loss: 1.02982688e-06
Iter: 1816 loss: 1.02957597e-06
Iter: 1817 loss: 1.0291817e-06
Iter: 1818 loss: 1.02915806e-06
Iter: 1819 loss: 1.02858439e-06
Iter: 1820 loss: 1.03208322e-06
Iter: 1821 loss: 1.02850026e-06
Iter: 1822 loss: 1.02814829e-06
Iter: 1823 loss: 1.03121579e-06
Iter: 1824 loss: 1.02813158e-06
Iter: 1825 loss: 1.0278161e-06
Iter: 1826 loss: 1.03215871e-06
Iter: 1827 loss: 1.0278153e-06
Iter: 1828 loss: 1.02767058e-06
Iter: 1829 loss: 1.02765568e-06
Iter: 1830 loss: 1.02756678e-06
Iter: 1831 loss: 1.02728518e-06
Iter: 1832 loss: 1.02809815e-06
Iter: 1833 loss: 1.02723163e-06
Iter: 1834 loss: 1.02704166e-06
Iter: 1835 loss: 1.02695674e-06
Iter: 1836 loss: 1.02685112e-06
Iter: 1837 loss: 1.02661647e-06
Iter: 1838 loss: 1.02881108e-06
Iter: 1839 loss: 1.02659828e-06
Iter: 1840 loss: 1.0263293e-06
Iter: 1841 loss: 1.02640411e-06
Iter: 1842 loss: 1.02612262e-06
Iter: 1843 loss: 1.02586387e-06
Iter: 1844 loss: 1.02629122e-06
Iter: 1845 loss: 1.0257761e-06
Iter: 1846 loss: 1.02558283e-06
Iter: 1847 loss: 1.02557897e-06
Iter: 1848 loss: 1.02544436e-06
Iter: 1849 loss: 1.02518118e-06
Iter: 1850 loss: 1.02759088e-06
Iter: 1851 loss: 1.02512422e-06
Iter: 1852 loss: 1.0247669e-06
Iter: 1853 loss: 1.02590298e-06
Iter: 1854 loss: 1.0246597e-06
Iter: 1855 loss: 1.02435138e-06
Iter: 1856 loss: 1.02445938e-06
Iter: 1857 loss: 1.02412469e-06
Iter: 1858 loss: 1.02385252e-06
Iter: 1859 loss: 1.02385911e-06
Iter: 1860 loss: 1.02360752e-06
Iter: 1861 loss: 1.02489298e-06
Iter: 1862 loss: 1.02354784e-06
Iter: 1863 loss: 1.02342597e-06
Iter: 1864 loss: 1.02350918e-06
Iter: 1865 loss: 1.02331956e-06
Iter: 1866 loss: 1.02313709e-06
Iter: 1867 loss: 1.02412764e-06
Iter: 1868 loss: 1.02309605e-06
Iter: 1869 loss: 1.02296474e-06
Iter: 1870 loss: 1.02273134e-06
Iter: 1871 loss: 1.02273816e-06
Iter: 1872 loss: 1.02247691e-06
Iter: 1873 loss: 1.02578269e-06
Iter: 1874 loss: 1.02248009e-06
Iter: 1875 loss: 1.02225749e-06
Iter: 1876 loss: 1.02229819e-06
Iter: 1877 loss: 1.02207105e-06
Iter: 1878 loss: 1.02182025e-06
Iter: 1879 loss: 1.02200602e-06
Iter: 1880 loss: 1.02165859e-06
Iter: 1881 loss: 1.02146169e-06
Iter: 1882 loss: 1.02144361e-06
Iter: 1883 loss: 1.02128388e-06
Iter: 1884 loss: 1.02098522e-06
Iter: 1885 loss: 1.02720878e-06
Iter: 1886 loss: 1.02099193e-06
Iter: 1887 loss: 1.02078582e-06
Iter: 1888 loss: 1.02155332e-06
Iter: 1889 loss: 1.02076172e-06
Iter: 1890 loss: 1.02056401e-06
Iter: 1891 loss: 1.02076433e-06
Iter: 1892 loss: 1.02045851e-06
Iter: 1893 loss: 1.02027548e-06
Iter: 1894 loss: 1.02065258e-06
Iter: 1895 loss: 1.02018907e-06
Iter: 1896 loss: 1.01996e-06
Iter: 1897 loss: 1.01994158e-06
Iter: 1898 loss: 1.0198263e-06
Iter: 1899 loss: 1.0195572e-06
Iter: 1900 loss: 1.02433e-06
Iter: 1901 loss: 1.01956175e-06
Iter: 1902 loss: 1.01921592e-06
Iter: 1903 loss: 1.02206536e-06
Iter: 1904 loss: 1.01916123e-06
Iter: 1905 loss: 1.01896592e-06
Iter: 1906 loss: 1.01879982e-06
Iter: 1907 loss: 1.01875173e-06
Iter: 1908 loss: 1.01863861e-06
Iter: 1909 loss: 1.0186111e-06
Iter: 1910 loss: 1.01850219e-06
Iter: 1911 loss: 1.01825844e-06
Iter: 1912 loss: 1.02059721e-06
Iter: 1913 loss: 1.01823764e-06
Iter: 1914 loss: 1.01813248e-06
Iter: 1915 loss: 1.0181152e-06
Iter: 1916 loss: 1.01798094e-06
Iter: 1917 loss: 1.01791227e-06
Iter: 1918 loss: 1.01785849e-06
Iter: 1919 loss: 1.01765568e-06
Iter: 1920 loss: 1.01762669e-06
Iter: 1921 loss: 1.01750425e-06
Iter: 1922 loss: 1.01716694e-06
Iter: 1923 loss: 1.01729324e-06
Iter: 1924 loss: 1.01697697e-06
Iter: 1925 loss: 1.01666387e-06
Iter: 1926 loss: 1.01869034e-06
Iter: 1927 loss: 1.01664682e-06
Iter: 1928 loss: 1.01648061e-06
Iter: 1929 loss: 1.01646901e-06
Iter: 1930 loss: 1.01628768e-06
Iter: 1931 loss: 1.01645969e-06
Iter: 1932 loss: 1.01618957e-06
Iter: 1933 loss: 1.01605565e-06
Iter: 1934 loss: 1.0164174e-06
Iter: 1935 loss: 1.0160154e-06
Iter: 1936 loss: 1.01582987e-06
Iter: 1937 loss: 1.01588398e-06
Iter: 1938 loss: 1.01569788e-06
Iter: 1939 loss: 1.01552564e-06
Iter: 1940 loss: 1.0156391e-06
Iter: 1941 loss: 1.01540809e-06
Iter: 1942 loss: 1.01517423e-06
Iter: 1943 loss: 1.0176982e-06
Iter: 1944 loss: 1.01516389e-06
Iter: 1945 loss: 1.01502462e-06
Iter: 1946 loss: 1.01483056e-06
Iter: 1947 loss: 1.01484079e-06
Iter: 1948 loss: 1.01474939e-06
Iter: 1949 loss: 1.01469971e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2
+ date
Mon Oct 26 09:42:36 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.2_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.2_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac70cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94d2850730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94d2850ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac73e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac73e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac6c80d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac63da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac612ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac612d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac612598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac5d5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac5b2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac5a98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac5440d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac53b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac53b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac4fdf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac4fdae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac4b2400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac4628c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac471950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac423158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac471840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac3baae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac3836a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac394bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac394840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac3500d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac3501e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac3109d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac2ca2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac310bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac310268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac282840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac24f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f94ac237f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.63443269e-06
Iter: 2 loss: 3.50488654e-06
Iter: 3 loss: 2.32550292e-06
Iter: 4 loss: 1.95513621e-06
Iter: 5 loss: 1.84641033e-06
Iter: 6 loss: 1.62342849e-06
Iter: 7 loss: 1.47654282e-06
Iter: 8 loss: 1.46465948e-06
Iter: 9 loss: 1.4073637e-06
Iter: 10 loss: 1.37280654e-06
Iter: 11 loss: 1.34924483e-06
Iter: 12 loss: 1.30245849e-06
Iter: 13 loss: 1.30204512e-06
Iter: 14 loss: 1.26860948e-06
Iter: 15 loss: 1.26011651e-06
Iter: 16 loss: 1.2391215e-06
Iter: 17 loss: 1.21579365e-06
Iter: 18 loss: 1.34672655e-06
Iter: 19 loss: 1.21252594e-06
Iter: 20 loss: 1.19505967e-06
Iter: 21 loss: 1.17764841e-06
Iter: 22 loss: 1.17404102e-06
Iter: 23 loss: 1.17835202e-06
Iter: 24 loss: 1.16289345e-06
Iter: 25 loss: 1.1558227e-06
Iter: 26 loss: 1.14136947e-06
Iter: 27 loss: 1.40282168e-06
Iter: 28 loss: 1.14110912e-06
Iter: 29 loss: 1.12913324e-06
Iter: 30 loss: 1.13886199e-06
Iter: 31 loss: 1.12195426e-06
Iter: 32 loss: 1.115548e-06
Iter: 33 loss: 1.13921772e-06
Iter: 34 loss: 1.1139723e-06
Iter: 35 loss: 1.10823976e-06
Iter: 36 loss: 1.10016799e-06
Iter: 37 loss: 1.0998483e-06
Iter: 38 loss: 1.09091445e-06
Iter: 39 loss: 1.17369768e-06
Iter: 40 loss: 1.09050325e-06
Iter: 41 loss: 1.08186282e-06
Iter: 42 loss: 1.12586144e-06
Iter: 43 loss: 1.08043992e-06
Iter: 44 loss: 1.07458072e-06
Iter: 45 loss: 1.06918321e-06
Iter: 46 loss: 1.06779339e-06
Iter: 47 loss: 1.05728077e-06
Iter: 48 loss: 1.15948194e-06
Iter: 49 loss: 1.05691015e-06
Iter: 50 loss: 1.04969945e-06
Iter: 51 loss: 1.04148762e-06
Iter: 52 loss: 1.04042806e-06
Iter: 53 loss: 1.03111802e-06
Iter: 54 loss: 1.07503729e-06
Iter: 55 loss: 1.02940112e-06
Iter: 56 loss: 1.02270678e-06
Iter: 57 loss: 1.05431843e-06
Iter: 58 loss: 1.02148738e-06
Iter: 59 loss: 1.01883052e-06
Iter: 60 loss: 1.01846035e-06
Iter: 61 loss: 1.01638909e-06
Iter: 62 loss: 1.01246235e-06
Iter: 63 loss: 1.09623761e-06
Iter: 64 loss: 1.01243279e-06
Iter: 65 loss: 1.01009118e-06
Iter: 66 loss: 1.04708897e-06
Iter: 67 loss: 1.01009061e-06
Iter: 68 loss: 1.00763918e-06
Iter: 69 loss: 1.01382034e-06
Iter: 70 loss: 1.00678426e-06
Iter: 71 loss: 1.00527745e-06
Iter: 72 loss: 1.00213356e-06
Iter: 73 loss: 1.05693312e-06
Iter: 74 loss: 1.00203692e-06
Iter: 75 loss: 9.99107e-07
Iter: 76 loss: 9.99068902e-07
Iter: 77 loss: 9.96428753e-07
Iter: 78 loss: 9.96224117e-07
Iter: 79 loss: 9.94221409e-07
Iter: 80 loss: 9.91887873e-07
Iter: 81 loss: 1.00896e-06
Iter: 82 loss: 9.9167255e-07
Iter: 83 loss: 9.8906537e-07
Iter: 84 loss: 9.89382e-07
Iter: 85 loss: 9.87074827e-07
Iter: 86 loss: 9.84692178e-07
Iter: 87 loss: 9.86346663e-07
Iter: 88 loss: 9.83189238e-07
Iter: 89 loss: 9.80424147e-07
Iter: 90 loss: 9.907601e-07
Iter: 91 loss: 9.79774086e-07
Iter: 92 loss: 9.77684294e-07
Iter: 93 loss: 9.88432e-07
Iter: 94 loss: 9.7736438e-07
Iter: 95 loss: 9.74813e-07
Iter: 96 loss: 9.82077381e-07
Iter: 97 loss: 9.73997e-07
Iter: 98 loss: 9.71850568e-07
Iter: 99 loss: 9.6993665e-07
Iter: 100 loss: 9.69356734e-07
Iter: 101 loss: 9.68092536e-07
Iter: 102 loss: 9.67981236e-07
Iter: 103 loss: 9.66418e-07
Iter: 104 loss: 9.65737399e-07
Iter: 105 loss: 9.64933861e-07
Iter: 106 loss: 9.63801881e-07
Iter: 107 loss: 9.65725121e-07
Iter: 108 loss: 9.63311e-07
Iter: 109 loss: 9.62153194e-07
Iter: 110 loss: 9.77829359e-07
Iter: 111 loss: 9.62182639e-07
Iter: 112 loss: 9.61644105e-07
Iter: 113 loss: 9.6140991e-07
Iter: 114 loss: 9.61122737e-07
Iter: 115 loss: 9.60308284e-07
Iter: 116 loss: 9.6695112e-07
Iter: 117 loss: 9.60264401e-07
Iter: 118 loss: 9.59614454e-07
Iter: 119 loss: 9.58671421e-07
Iter: 120 loss: 9.58649139e-07
Iter: 121 loss: 9.57638576e-07
Iter: 122 loss: 9.59132421e-07
Iter: 123 loss: 9.57122779e-07
Iter: 124 loss: 9.55648e-07
Iter: 125 loss: 9.58829332e-07
Iter: 126 loss: 9.55047881e-07
Iter: 127 loss: 9.54166808e-07
Iter: 128 loss: 9.54102461e-07
Iter: 129 loss: 9.5342989e-07
Iter: 130 loss: 9.52614585e-07
Iter: 131 loss: 9.52500216e-07
Iter: 132 loss: 9.51516938e-07
Iter: 133 loss: 9.53055746e-07
Iter: 134 loss: 9.51035872e-07
Iter: 135 loss: 9.50114952e-07
Iter: 136 loss: 9.50104493e-07
Iter: 137 loss: 9.49617231e-07
Iter: 138 loss: 9.48308866e-07
Iter: 139 loss: 9.56943495e-07
Iter: 140 loss: 9.47985e-07
Iter: 141 loss: 9.4712118e-07
Iter: 142 loss: 9.47016531e-07
Iter: 143 loss: 9.46085436e-07
Iter: 144 loss: 9.45356305e-07
Iter: 145 loss: 9.45091358e-07
Iter: 146 loss: 9.44354611e-07
Iter: 147 loss: 9.44353133e-07
Iter: 148 loss: 9.43662371e-07
Iter: 149 loss: 9.43383668e-07
Iter: 150 loss: 9.42998099e-07
Iter: 151 loss: 9.42362931e-07
Iter: 152 loss: 9.43041073e-07
Iter: 153 loss: 9.42021757e-07
Iter: 154 loss: 9.41341114e-07
Iter: 155 loss: 9.45746081e-07
Iter: 156 loss: 9.41269036e-07
Iter: 157 loss: 9.40754489e-07
Iter: 158 loss: 9.45298211e-07
Iter: 159 loss: 9.40733912e-07
Iter: 160 loss: 9.40234088e-07
Iter: 161 loss: 9.40375628e-07
Iter: 162 loss: 9.3987785e-07
Iter: 163 loss: 9.39304357e-07
Iter: 164 loss: 9.39321239e-07
Iter: 165 loss: 9.38869164e-07
Iter: 166 loss: 9.38491667e-07
Iter: 167 loss: 9.3846279e-07
Iter: 168 loss: 9.37943582e-07
Iter: 169 loss: 9.36788069e-07
Iter: 170 loss: 9.50803383e-07
Iter: 171 loss: 9.36674383e-07
Iter: 172 loss: 9.35923708e-07
Iter: 173 loss: 9.41950759e-07
Iter: 174 loss: 9.35881587e-07
Iter: 175 loss: 9.34943841e-07
Iter: 176 loss: 9.36767208e-07
Iter: 177 loss: 9.3456697e-07
Iter: 178 loss: 9.3385222e-07
Iter: 179 loss: 9.34783884e-07
Iter: 180 loss: 9.33480692e-07
Iter: 181 loss: 9.32590069e-07
Iter: 182 loss: 9.38705739e-07
Iter: 183 loss: 9.3250992e-07
Iter: 184 loss: 9.31972238e-07
Iter: 185 loss: 9.31053876e-07
Iter: 186 loss: 9.31050579e-07
Iter: 187 loss: 9.30295187e-07
Iter: 188 loss: 9.35832873e-07
Iter: 189 loss: 9.30220835e-07
Iter: 190 loss: 9.29687872e-07
Iter: 191 loss: 9.3523181e-07
Iter: 192 loss: 9.29676276e-07
Iter: 193 loss: 9.29246539e-07
Iter: 194 loss: 9.30993679e-07
Iter: 195 loss: 9.29130579e-07
Iter: 196 loss: 9.28757686e-07
Iter: 197 loss: 9.28364784e-07
Iter: 198 loss: 9.28313625e-07
Iter: 199 loss: 9.27726433e-07
Iter: 200 loss: 9.28628822e-07
Iter: 201 loss: 9.27463589e-07
Iter: 202 loss: 9.26806138e-07
Iter: 203 loss: 9.26792609e-07
Iter: 204 loss: 9.26421251e-07
Iter: 205 loss: 9.25455879e-07
Iter: 206 loss: 9.33042259e-07
Iter: 207 loss: 9.25271763e-07
Iter: 208 loss: 9.24807523e-07
Iter: 209 loss: 9.24665528e-07
Iter: 210 loss: 9.24097606e-07
Iter: 211 loss: 9.23709536e-07
Iter: 212 loss: 9.23499101e-07
Iter: 213 loss: 9.2300445e-07
Iter: 214 loss: 9.28315501e-07
Iter: 215 loss: 9.22986146e-07
Iter: 216 loss: 9.2247376e-07
Iter: 217 loss: 9.22221602e-07
Iter: 218 loss: 9.21981893e-07
Iter: 219 loss: 9.21392711e-07
Iter: 220 loss: 9.20762659e-07
Iter: 221 loss: 9.20682737e-07
Iter: 222 loss: 9.20008461e-07
Iter: 223 loss: 9.3018366e-07
Iter: 224 loss: 9.19999536e-07
Iter: 225 loss: 9.19465037e-07
Iter: 226 loss: 9.21519927e-07
Iter: 227 loss: 9.1933839e-07
Iter: 228 loss: 9.18714591e-07
Iter: 229 loss: 9.19491072e-07
Iter: 230 loss: 9.18408091e-07
Iter: 231 loss: 9.17881891e-07
Iter: 232 loss: 9.1770562e-07
Iter: 233 loss: 9.17390139e-07
Iter: 234 loss: 9.16740419e-07
Iter: 235 loss: 9.20756236e-07
Iter: 236 loss: 9.16669705e-07
Iter: 237 loss: 9.16328304e-07
Iter: 238 loss: 9.16301587e-07
Iter: 239 loss: 9.16145211e-07
Iter: 240 loss: 9.15717123e-07
Iter: 241 loss: 9.178151e-07
Iter: 242 loss: 9.15587293e-07
Iter: 243 loss: 9.15421595e-07
Iter: 244 loss: 9.15298187e-07
Iter: 245 loss: 9.15052453e-07
Iter: 246 loss: 9.14490045e-07
Iter: 247 loss: 9.21047672e-07
Iter: 248 loss: 9.14436043e-07
Iter: 249 loss: 9.14008695e-07
Iter: 250 loss: 9.13996132e-07
Iter: 251 loss: 9.13560541e-07
Iter: 252 loss: 9.12690439e-07
Iter: 253 loss: 9.28164923e-07
Iter: 254 loss: 9.1266844e-07
Iter: 255 loss: 9.11906227e-07
Iter: 256 loss: 9.14403586e-07
Iter: 257 loss: 9.11682491e-07
Iter: 258 loss: 9.11404527e-07
Iter: 259 loss: 9.11275e-07
Iter: 260 loss: 9.1094978e-07
Iter: 261 loss: 9.11199038e-07
Iter: 262 loss: 9.1077527e-07
Iter: 263 loss: 9.10429094e-07
Iter: 264 loss: 9.10656e-07
Iter: 265 loss: 9.10230824e-07
Iter: 266 loss: 9.09893402e-07
Iter: 267 loss: 9.10249e-07
Iter: 268 loss: 9.09718096e-07
Iter: 269 loss: 9.09537903e-07
Iter: 270 loss: 9.09502262e-07
Iter: 271 loss: 9.09313144e-07
Iter: 272 loss: 9.09071503e-07
Iter: 273 loss: 9.09041034e-07
Iter: 274 loss: 9.08774496e-07
Iter: 275 loss: 9.08728339e-07
Iter: 276 loss: 9.08534389e-07
Iter: 277 loss: 9.08038544e-07
Iter: 278 loss: 9.12331075e-07
Iter: 279 loss: 9.0803735e-07
Iter: 280 loss: 9.07837773e-07
Iter: 281 loss: 9.07816684e-07
Iter: 282 loss: 9.07691401e-07
Iter: 283 loss: 9.07379786e-07
Iter: 284 loss: 9.09727532e-07
Iter: 285 loss: 9.07370691e-07
Iter: 286 loss: 9.07203912e-07
Iter: 287 loss: 9.06857736e-07
Iter: 288 loss: 9.1287643e-07
Iter: 289 loss: 9.06844662e-07
Iter: 290 loss: 9.0643789e-07
Iter: 291 loss: 9.06901732e-07
Iter: 292 loss: 9.06190337e-07
Iter: 293 loss: 9.05630941e-07
Iter: 294 loss: 9.06583296e-07
Iter: 295 loss: 9.05388106e-07
Iter: 296 loss: 9.0499833e-07
Iter: 297 loss: 9.04959506e-07
Iter: 298 loss: 9.04572062e-07
Iter: 299 loss: 9.04260617e-07
Iter: 300 loss: 9.04145963e-07
Iter: 301 loss: 9.03669616e-07
Iter: 302 loss: 9.06231435e-07
Iter: 303 loss: 9.03599641e-07
Iter: 304 loss: 9.0325608e-07
Iter: 305 loss: 9.0756555e-07
Iter: 306 loss: 9.03262446e-07
Iter: 307 loss: 9.03073214e-07
Iter: 308 loss: 9.02855447e-07
Iter: 309 loss: 9.02798661e-07
Iter: 310 loss: 9.02587828e-07
Iter: 311 loss: 9.03835e-07
Iter: 312 loss: 9.02559862e-07
Iter: 313 loss: 9.02320153e-07
Iter: 314 loss: 9.03029218e-07
Iter: 315 loss: 9.02213856e-07
Iter: 316 loss: 9.02100794e-07
Iter: 317 loss: 9.02262514e-07
Iter: 318 loss: 9.0203622e-07
Iter: 319 loss: 9.01821466e-07
Iter: 320 loss: 9.01973863e-07
Iter: 321 loss: 9.0165679e-07
Iter: 322 loss: 9.01408441e-07
Iter: 323 loss: 9.0122893e-07
Iter: 324 loss: 9.0114861e-07
Iter: 325 loss: 9.00779185e-07
Iter: 326 loss: 9.00566079e-07
Iter: 327 loss: 9.0039e-07
Iter: 328 loss: 8.99818133e-07
Iter: 329 loss: 9.0381e-07
Iter: 330 loss: 8.99756458e-07
Iter: 331 loss: 8.99545853e-07
Iter: 332 loss: 8.99512372e-07
Iter: 333 loss: 8.99276756e-07
Iter: 334 loss: 8.98913413e-07
Iter: 335 loss: 8.98905569e-07
Iter: 336 loss: 8.9858213e-07
Iter: 337 loss: 9.01191925e-07
Iter: 338 loss: 8.98570249e-07
Iter: 339 loss: 8.98196618e-07
Iter: 340 loss: 8.99402664e-07
Iter: 341 loss: 8.98113399e-07
Iter: 342 loss: 8.97927862e-07
Iter: 343 loss: 8.9787352e-07
Iter: 344 loss: 8.97767563e-07
Iter: 345 loss: 8.97495113e-07
Iter: 346 loss: 8.99354347e-07
Iter: 347 loss: 8.97453106e-07
Iter: 348 loss: 8.97190318e-07
Iter: 349 loss: 8.97411383e-07
Iter: 350 loss: 8.97042582e-07
Iter: 351 loss: 8.96797815e-07
Iter: 352 loss: 8.977226e-07
Iter: 353 loss: 8.96760412e-07
Iter: 354 loss: 8.96497227e-07
Iter: 355 loss: 8.9722613e-07
Iter: 356 loss: 8.96428787e-07
Iter: 357 loss: 8.96198799e-07
Iter: 358 loss: 8.958433e-07
Iter: 359 loss: 8.95858e-07
Iter: 360 loss: 8.95497635e-07
Iter: 361 loss: 8.96096196e-07
Iter: 362 loss: 8.95321534e-07
Iter: 363 loss: 8.94903849e-07
Iter: 364 loss: 8.96825782e-07
Iter: 365 loss: 8.94839332e-07
Iter: 366 loss: 8.94638447e-07
Iter: 367 loss: 8.94590357e-07
Iter: 368 loss: 8.94435345e-07
Iter: 369 loss: 8.94083882e-07
Iter: 370 loss: 9.00421128e-07
Iter: 371 loss: 8.94064044e-07
Iter: 372 loss: 8.9395985e-07
Iter: 373 loss: 8.93901301e-07
Iter: 374 loss: 8.93752372e-07
Iter: 375 loss: 8.93378228e-07
Iter: 376 loss: 8.98386588e-07
Iter: 377 loss: 8.93363676e-07
Iter: 378 loss: 8.93058086e-07
Iter: 379 loss: 8.96475569e-07
Iter: 380 loss: 8.93051492e-07
Iter: 381 loss: 8.92801779e-07
Iter: 382 loss: 8.93544552e-07
Iter: 383 loss: 8.92695653e-07
Iter: 384 loss: 8.92438891e-07
Iter: 385 loss: 8.92531943e-07
Iter: 386 loss: 8.92242838e-07
Iter: 387 loss: 8.92009837e-07
Iter: 388 loss: 8.95068183e-07
Iter: 389 loss: 8.92029e-07
Iter: 390 loss: 8.91843797e-07
Iter: 391 loss: 8.91583e-07
Iter: 392 loss: 8.915585e-07
Iter: 393 loss: 8.91260697e-07
Iter: 394 loss: 8.91411275e-07
Iter: 395 loss: 8.91079424e-07
Iter: 396 loss: 8.90730234e-07
Iter: 397 loss: 8.91272293e-07
Iter: 398 loss: 8.90558454e-07
Iter: 399 loss: 8.90256558e-07
Iter: 400 loss: 8.90251954e-07
Iter: 401 loss: 8.89964895e-07
Iter: 402 loss: 8.91161562e-07
Iter: 403 loss: 8.89902253e-07
Iter: 404 loss: 8.89725357e-07
Iter: 405 loss: 8.89609566e-07
Iter: 406 loss: 8.89556645e-07
Iter: 407 loss: 8.89332739e-07
Iter: 408 loss: 8.89323132e-07
Iter: 409 loss: 8.89213197e-07
Iter: 410 loss: 8.8888163e-07
Iter: 411 loss: 8.916345e-07
Iter: 412 loss: 8.88834506e-07
Iter: 413 loss: 8.88731734e-07
Iter: 414 loss: 8.88672503e-07
Iter: 415 loss: 8.88508453e-07
Iter: 416 loss: 8.8853443e-07
Iter: 417 loss: 8.88433192e-07
Iter: 418 loss: 8.8824163e-07
Iter: 419 loss: 8.88674947e-07
Iter: 420 loss: 8.88196723e-07
Iter: 421 loss: 8.87987255e-07
Iter: 422 loss: 8.88181489e-07
Iter: 423 loss: 8.8785e-07
Iter: 424 loss: 8.87637611e-07
Iter: 425 loss: 8.87596514e-07
Iter: 426 loss: 8.87444799e-07
Iter: 427 loss: 8.87105898e-07
Iter: 428 loss: 8.87358567e-07
Iter: 429 loss: 8.86893247e-07
Iter: 430 loss: 8.86548719e-07
Iter: 431 loss: 8.87047122e-07
Iter: 432 loss: 8.86374153e-07
Iter: 433 loss: 8.86205498e-07
Iter: 434 loss: 8.86171165e-07
Iter: 435 loss: 8.85990801e-07
Iter: 436 loss: 8.86383134e-07
Iter: 437 loss: 8.85896952e-07
Iter: 438 loss: 8.85771783e-07
Iter: 439 loss: 8.85828683e-07
Iter: 440 loss: 8.85672193e-07
Iter: 441 loss: 8.85489499e-07
Iter: 442 loss: 8.87468389e-07
Iter: 443 loss: 8.8547614e-07
Iter: 444 loss: 8.85384793e-07
Iter: 445 loss: 8.85090458e-07
Iter: 446 loss: 8.86866701e-07
Iter: 447 loss: 8.84979556e-07
Iter: 448 loss: 8.84907422e-07
Iter: 449 loss: 8.84826306e-07
Iter: 450 loss: 8.84654e-07
Iter: 451 loss: 8.84402482e-07
Iter: 452 loss: 8.84385145e-07
Iter: 453 loss: 8.84133271e-07
Iter: 454 loss: 8.87384203e-07
Iter: 455 loss: 8.84151177e-07
Iter: 456 loss: 8.83957796e-07
Iter: 457 loss: 8.83855819e-07
Iter: 458 loss: 8.83773509e-07
Iter: 459 loss: 8.8358621e-07
Iter: 460 loss: 8.83993607e-07
Iter: 461 loss: 8.83488099e-07
Iter: 462 loss: 8.83291705e-07
Iter: 463 loss: 8.83932898e-07
Iter: 464 loss: 8.83221162e-07
Iter: 465 loss: 8.83018515e-07
Iter: 466 loss: 8.82949507e-07
Iter: 467 loss: 8.82825134e-07
Iter: 468 loss: 8.82701784e-07
Iter: 469 loss: 8.82666541e-07
Iter: 470 loss: 8.82511358e-07
Iter: 471 loss: 8.82377321e-07
Iter: 472 loss: 8.82312065e-07
Iter: 473 loss: 8.82118343e-07
Iter: 474 loss: 8.82812515e-07
Iter: 475 loss: 8.82071959e-07
Iter: 476 loss: 8.81817641e-07
Iter: 477 loss: 8.82969971e-07
Iter: 478 loss: 8.81769097e-07
Iter: 479 loss: 8.81646542e-07
Iter: 480 loss: 8.81390349e-07
Iter: 481 loss: 8.84823635e-07
Iter: 482 loss: 8.81367782e-07
Iter: 483 loss: 8.81307415e-07
Iter: 484 loss: 8.81222718e-07
Iter: 485 loss: 8.81089534e-07
Iter: 486 loss: 8.80946345e-07
Iter: 487 loss: 8.80942935e-07
Iter: 488 loss: 8.80720108e-07
Iter: 489 loss: 8.81885398e-07
Iter: 490 loss: 8.8071e-07
Iter: 491 loss: 8.80467098e-07
Iter: 492 loss: 8.80534e-07
Iter: 493 loss: 8.80285484e-07
Iter: 494 loss: 8.80071184e-07
Iter: 495 loss: 8.80009793e-07
Iter: 496 loss: 8.79865524e-07
Iter: 497 loss: 8.79603476e-07
Iter: 498 loss: 8.81821279e-07
Iter: 499 loss: 8.7959927e-07
Iter: 500 loss: 8.79405206e-07
Iter: 501 loss: 8.79561469e-07
Iter: 502 loss: 8.7928521e-07
Iter: 503 loss: 8.7910189e-07
Iter: 504 loss: 8.80173218e-07
Iter: 505 loss: 8.79071536e-07
Iter: 506 loss: 8.78969502e-07
Iter: 507 loss: 8.78954438e-07
Iter: 508 loss: 8.78859055e-07
Iter: 509 loss: 8.78675621e-07
Iter: 510 loss: 8.82501809e-07
Iter: 511 loss: 8.78682783e-07
Iter: 512 loss: 8.78482467e-07
Iter: 513 loss: 8.78999174e-07
Iter: 514 loss: 8.78428807e-07
Iter: 515 loss: 8.7823588e-07
Iter: 516 loss: 8.810145e-07
Iter: 517 loss: 8.78231901e-07
Iter: 518 loss: 8.78118385e-07
Iter: 519 loss: 8.77816603e-07
Iter: 520 loss: 8.80389507e-07
Iter: 521 loss: 8.77770788e-07
Iter: 522 loss: 8.77727416e-07
Iter: 523 loss: 8.77640105e-07
Iter: 524 loss: 8.77526531e-07
Iter: 525 loss: 8.77383741e-07
Iter: 526 loss: 8.77366347e-07
Iter: 527 loss: 8.77216053e-07
Iter: 528 loss: 8.77181435e-07
Iter: 529 loss: 8.77133e-07
Iter: 530 loss: 8.76924616e-07
Iter: 531 loss: 8.79624395e-07
Iter: 532 loss: 8.76935246e-07
Iter: 533 loss: 8.76815136e-07
Iter: 534 loss: 8.76715603e-07
Iter: 535 loss: 8.76688432e-07
Iter: 536 loss: 8.76557e-07
Iter: 537 loss: 8.76396882e-07
Iter: 538 loss: 8.7635641e-07
Iter: 539 loss: 8.76140064e-07
Iter: 540 loss: 8.77041373e-07
Iter: 541 loss: 8.76083902e-07
Iter: 542 loss: 8.75923035e-07
Iter: 543 loss: 8.7813919e-07
Iter: 544 loss: 8.75922808e-07
Iter: 545 loss: 8.75794285e-07
Iter: 546 loss: 8.76838385e-07
Iter: 547 loss: 8.75797241e-07
Iter: 548 loss: 8.75702199e-07
Iter: 549 loss: 8.75623527e-07
Iter: 550 loss: 8.756125e-07
Iter: 551 loss: 8.7543134e-07
Iter: 552 loss: 8.76198328e-07
Iter: 553 loss: 8.75369892e-07
Iter: 554 loss: 8.75273429e-07
Iter: 555 loss: 8.75135584e-07
Iter: 556 loss: 8.7512791e-07
Iter: 557 loss: 8.74890134e-07
Iter: 558 loss: 8.75426338e-07
Iter: 559 loss: 8.74821467e-07
Iter: 560 loss: 8.74585908e-07
Iter: 561 loss: 8.77355831e-07
Iter: 562 loss: 8.74585908e-07
Iter: 563 loss: 8.74482907e-07
Iter: 564 loss: 8.74216937e-07
Iter: 565 loss: 8.76445142e-07
Iter: 566 loss: 8.74193347e-07
Iter: 567 loss: 8.74170951e-07
Iter: 568 loss: 8.74075397e-07
Iter: 569 loss: 8.73967565e-07
Iter: 570 loss: 8.73868771e-07
Iter: 571 loss: 8.73841373e-07
Iter: 572 loss: 8.73711429e-07
Iter: 573 loss: 8.73780948e-07
Iter: 574 loss: 8.73612237e-07
Iter: 575 loss: 8.73462e-07
Iter: 576 loss: 8.73410102e-07
Iter: 577 loss: 8.73321483e-07
Iter: 578 loss: 8.73112299e-07
Iter: 579 loss: 8.75785759e-07
Iter: 580 loss: 8.73096042e-07
Iter: 581 loss: 8.72964335e-07
Iter: 582 loss: 8.72949499e-07
Iter: 583 loss: 8.72868441e-07
Iter: 584 loss: 8.72847068e-07
Iter: 585 loss: 8.72778969e-07
Iter: 586 loss: 8.72657893e-07
Iter: 587 loss: 8.73559884e-07
Iter: 588 loss: 8.72625037e-07
Iter: 589 loss: 8.72546821e-07
Iter: 590 loss: 8.72384589e-07
Iter: 591 loss: 8.72380724e-07
Iter: 592 loss: 8.7220684e-07
Iter: 593 loss: 8.72681539e-07
Iter: 594 loss: 8.72171086e-07
Iter: 595 loss: 8.71995212e-07
Iter: 596 loss: 8.72576493e-07
Iter: 597 loss: 8.719569e-07
Iter: 598 loss: 8.71799898e-07
Iter: 599 loss: 8.71811494e-07
Iter: 600 loss: 8.71723103e-07
Iter: 601 loss: 8.71452698e-07
Iter: 602 loss: 8.73509862e-07
Iter: 603 loss: 8.71429449e-07
Iter: 604 loss: 8.71129714e-07
Iter: 605 loss: 8.71849e-07
Iter: 606 loss: 8.71034842e-07
Iter: 607 loss: 8.7081628e-07
Iter: 608 loss: 8.72656244e-07
Iter: 609 loss: 8.70825545e-07
Iter: 610 loss: 8.70666327e-07
Iter: 611 loss: 8.70663257e-07
Iter: 612 loss: 8.70568329e-07
Iter: 613 loss: 8.70473229e-07
Iter: 614 loss: 8.70461236e-07
Iter: 615 loss: 8.70382905e-07
Iter: 616 loss: 8.70378472e-07
Iter: 617 loss: 8.70286044e-07
Iter: 618 loss: 8.70225506e-07
Iter: 619 loss: 8.70172e-07
Iter: 620 loss: 8.70110853e-07
Iter: 621 loss: 8.70100848e-07
Iter: 622 loss: 8.7005742e-07
Iter: 623 loss: 8.6995675e-07
Iter: 624 loss: 8.70610847e-07
Iter: 625 loss: 8.69927305e-07
Iter: 626 loss: 8.69785e-07
Iter: 627 loss: 8.71148416e-07
Iter: 628 loss: 8.69796622e-07
Iter: 629 loss: 8.69687426e-07
Iter: 630 loss: 8.70870792e-07
Iter: 631 loss: 8.69686062e-07
Iter: 632 loss: 8.69585e-07
Iter: 633 loss: 8.69624273e-07
Iter: 634 loss: 8.69538212e-07
Iter: 635 loss: 8.69446808e-07
Iter: 636 loss: 8.69301061e-07
Iter: 637 loss: 8.69299356e-07
Iter: 638 loss: 8.6907562e-07
Iter: 639 loss: 8.69331757e-07
Iter: 640 loss: 8.6895858e-07
Iter: 641 loss: 8.68755649e-07
Iter: 642 loss: 8.69737505e-07
Iter: 643 loss: 8.68743143e-07
Iter: 644 loss: 8.68587563e-07
Iter: 645 loss: 8.68605866e-07
Iter: 646 loss: 8.68523784e-07
Iter: 647 loss: 8.68395e-07
Iter: 648 loss: 8.713439e-07
Iter: 649 loss: 8.68396342e-07
Iter: 650 loss: 8.68326936e-07
Iter: 651 loss: 8.68307779e-07
Iter: 652 loss: 8.68251277e-07
Iter: 653 loss: 8.68199947e-07
Iter: 654 loss: 8.68192728e-07
Iter: 655 loss: 8.68077223e-07
Iter: 656 loss: 8.68693121e-07
Iter: 657 loss: 8.68064717e-07
Iter: 658 loss: 8.68008897e-07
Iter: 659 loss: 8.6789089e-07
Iter: 660 loss: 8.67901e-07
Iter: 661 loss: 8.67759468e-07
Iter: 662 loss: 8.67951258e-07
Iter: 663 loss: 8.67686254e-07
Iter: 664 loss: 8.67671588e-07
Iter: 665 loss: 8.6760781e-07
Iter: 666 loss: 8.67524363e-07
Iter: 667 loss: 8.67430344e-07
Iter: 668 loss: 8.67427104e-07
Iter: 669 loss: 8.673004e-07
Iter: 670 loss: 8.67316146e-07
Iter: 671 loss: 8.67195354e-07
Iter: 672 loss: 8.67043696e-07
Iter: 673 loss: 8.6770558e-07
Iter: 674 loss: 8.67030735e-07
Iter: 675 loss: 8.66861342e-07
Iter: 676 loss: 8.67637709e-07
Iter: 677 loss: 8.66833602e-07
Iter: 678 loss: 8.66744585e-07
Iter: 679 loss: 8.6785667e-07
Iter: 680 loss: 8.66724349e-07
Iter: 681 loss: 8.66671712e-07
Iter: 682 loss: 8.66662e-07
Iter: 683 loss: 8.6660657e-07
Iter: 684 loss: 8.66529376e-07
Iter: 685 loss: 8.67254357e-07
Iter: 686 loss: 8.66539892e-07
Iter: 687 loss: 8.66450591e-07
Iter: 688 loss: 8.66397613e-07
Iter: 689 loss: 8.66344919e-07
Iter: 690 loss: 8.66300525e-07
Iter: 691 loss: 8.66297967e-07
Iter: 692 loss: 8.66234473e-07
Iter: 693 loss: 8.66138407e-07
Iter: 694 loss: 8.67777885e-07
Iter: 695 loss: 8.66146479e-07
Iter: 696 loss: 8.66021537e-07
Iter: 697 loss: 8.66248456e-07
Iter: 698 loss: 8.65965944e-07
Iter: 699 loss: 8.65880224e-07
Iter: 700 loss: 8.65892275e-07
Iter: 701 loss: 8.65777679e-07
Iter: 702 loss: 8.65641823e-07
Iter: 703 loss: 8.65632501e-07
Iter: 704 loss: 8.65482491e-07
Iter: 705 loss: 8.65445941e-07
Iter: 706 loss: 8.65366e-07
Iter: 707 loss: 8.65194579e-07
Iter: 708 loss: 8.66457867e-07
Iter: 709 loss: 8.65171671e-07
Iter: 710 loss: 8.6501251e-07
Iter: 711 loss: 8.65920072e-07
Iter: 712 loss: 8.65003585e-07
Iter: 713 loss: 8.64899789e-07
Iter: 714 loss: 8.65991467e-07
Iter: 715 loss: 8.64899732e-07
Iter: 716 loss: 8.64819697e-07
Iter: 717 loss: 8.64741594e-07
Iter: 718 loss: 8.64725621e-07
Iter: 719 loss: 8.64604544e-07
Iter: 720 loss: 8.64615117e-07
Iter: 721 loss: 8.64533831e-07
Iter: 722 loss: 8.64524395e-07
Iter: 723 loss: 8.64452034e-07
Iter: 724 loss: 8.64355684e-07
Iter: 725 loss: 8.65165816e-07
Iter: 726 loss: 8.64344656e-07
Iter: 727 loss: 8.64274114e-07
Iter: 728 loss: 8.64091703e-07
Iter: 729 loss: 8.67512426e-07
Iter: 730 loss: 8.64131835e-07
Iter: 731 loss: 8.6399956e-07
Iter: 732 loss: 8.65127504e-07
Iter: 733 loss: 8.64013828e-07
Iter: 734 loss: 8.63912703e-07
Iter: 735 loss: 8.64326353e-07
Iter: 736 loss: 8.63892296e-07
Iter: 737 loss: 8.63811465e-07
Iter: 738 loss: 8.63709602e-07
Iter: 739 loss: 8.63705452e-07
Iter: 740 loss: 8.63595744e-07
Iter: 741 loss: 8.63591708e-07
Iter: 742 loss: 8.63527191e-07
Iter: 743 loss: 8.63326591e-07
Iter: 744 loss: 8.64101196e-07
Iter: 745 loss: 8.63284868e-07
Iter: 746 loss: 8.63153787e-07
Iter: 747 loss: 8.65061281e-07
Iter: 748 loss: 8.63152309e-07
Iter: 749 loss: 8.63050332e-07
Iter: 750 loss: 8.63341256e-07
Iter: 751 loss: 8.62994966e-07
Iter: 752 loss: 8.62907086e-07
Iter: 753 loss: 8.62982347e-07
Iter: 754 loss: 8.62859395e-07
Iter: 755 loss: 8.62691365e-07
Iter: 756 loss: 8.63703463e-07
Iter: 757 loss: 8.6267903e-07
Iter: 758 loss: 8.62558807e-07
Iter: 759 loss: 8.62683351e-07
Iter: 760 loss: 8.62501111e-07
Iter: 761 loss: 8.62370712e-07
Iter: 762 loss: 8.62825971e-07
Iter: 763 loss: 8.62340698e-07
Iter: 764 loss: 8.62228717e-07
Iter: 765 loss: 8.62008847e-07
Iter: 766 loss: 8.66867538e-07
Iter: 767 loss: 8.62004697e-07
Iter: 768 loss: 8.61925059e-07
Iter: 769 loss: 8.61876e-07
Iter: 770 loss: 8.61768683e-07
Iter: 771 loss: 8.6160162e-07
Iter: 772 loss: 8.61591616e-07
Iter: 773 loss: 8.61414378e-07
Iter: 774 loss: 8.61564047e-07
Iter: 775 loss: 8.6134105e-07
Iter: 776 loss: 8.61160345e-07
Iter: 777 loss: 8.61789772e-07
Iter: 778 loss: 8.61120384e-07
Iter: 779 loss: 8.61010335e-07
Iter: 780 loss: 8.61095714e-07
Iter: 781 loss: 8.60908131e-07
Iter: 782 loss: 8.60806e-07
Iter: 783 loss: 8.61986678e-07
Iter: 784 loss: 8.60767841e-07
Iter: 785 loss: 8.60677915e-07
Iter: 786 loss: 8.61751914e-07
Iter: 787 loss: 8.60657167e-07
Iter: 788 loss: 8.60549619e-07
Iter: 789 loss: 8.60488797e-07
Iter: 790 loss: 8.60444743e-07
Iter: 791 loss: 8.60288765e-07
Iter: 792 loss: 8.6209667e-07
Iter: 793 loss: 8.60303203e-07
Iter: 794 loss: 8.60207251e-07
Iter: 795 loss: 8.60154557e-07
Iter: 796 loss: 8.60128523e-07
Iter: 797 loss: 8.59985789e-07
Iter: 798 loss: 8.61297849e-07
Iter: 799 loss: 8.59993e-07
Iter: 800 loss: 8.59912461e-07
Iter: 801 loss: 8.59763873e-07
Iter: 802 loss: 8.60951332e-07
Iter: 803 loss: 8.59719535e-07
Iter: 804 loss: 8.59676447e-07
Iter: 805 loss: 8.59619945e-07
Iter: 806 loss: 8.59515e-07
Iter: 807 loss: 8.59417241e-07
Iter: 808 loss: 8.59386887e-07
Iter: 809 loss: 8.59271381e-07
Iter: 810 loss: 8.59332943e-07
Iter: 811 loss: 8.59177931e-07
Iter: 812 loss: 8.59042075e-07
Iter: 813 loss: 8.59650299e-07
Iter: 814 loss: 8.58986539e-07
Iter: 815 loss: 8.5882948e-07
Iter: 816 loss: 8.59174179e-07
Iter: 817 loss: 8.58783892e-07
Iter: 818 loss: 8.5868e-07
Iter: 819 loss: 8.59743636e-07
Iter: 820 loss: 8.58668955e-07
Iter: 821 loss: 8.58583064e-07
Iter: 822 loss: 8.58886892e-07
Iter: 823 loss: 8.58533099e-07
Iter: 824 loss: 8.58472845e-07
Iter: 825 loss: 8.58864666e-07
Iter: 826 loss: 8.5846591e-07
Iter: 827 loss: 8.58381554e-07
Iter: 828 loss: 8.58317321e-07
Iter: 829 loss: 8.58291855e-07
Iter: 830 loss: 8.58216538e-07
Iter: 831 loss: 8.59235229e-07
Iter: 832 loss: 8.58203862e-07
Iter: 833 loss: 8.58127578e-07
Iter: 834 loss: 8.58040153e-07
Iter: 835 loss: 8.58040323e-07
Iter: 836 loss: 8.57913506e-07
Iter: 837 loss: 8.58372e-07
Iter: 838 loss: 8.57883833e-07
Iter: 839 loss: 8.57751445e-07
Iter: 840 loss: 8.58433168e-07
Iter: 841 loss: 8.57725e-07
Iter: 842 loss: 8.57632131e-07
Iter: 843 loss: 8.57512248e-07
Iter: 844 loss: 8.57491841e-07
Iter: 845 loss: 8.57371845e-07
Iter: 846 loss: 8.57881e-07
Iter: 847 loss: 8.57349505e-07
Iter: 848 loss: 8.57212399e-07
Iter: 849 loss: 8.57533166e-07
Iter: 850 loss: 8.57195687e-07
Iter: 851 loss: 8.57060286e-07
Iter: 852 loss: 8.57369571e-07
Iter: 853 loss: 8.5704437e-07
Iter: 854 loss: 8.56949327e-07
Iter: 855 loss: 8.56941369e-07
Iter: 856 loss: 8.5688157e-07
Iter: 857 loss: 8.56898168e-07
Iter: 858 loss: 8.56820861e-07
Iter: 859 loss: 8.56738779e-07
Iter: 860 loss: 8.57331202e-07
Iter: 861 loss: 8.56734687e-07
Iter: 862 loss: 8.56658289e-07
Iter: 863 loss: 8.5660389e-07
Iter: 864 loss: 8.56579561e-07
Iter: 865 loss: 8.56457063e-07
Iter: 866 loss: 8.57671864e-07
Iter: 867 loss: 8.56441488e-07
Iter: 868 loss: 8.5639806e-07
Iter: 869 loss: 8.5629506e-07
Iter: 870 loss: 8.57844213e-07
Iter: 871 loss: 8.56279257e-07
Iter: 872 loss: 8.56228553e-07
Iter: 873 loss: 8.56207407e-07
Iter: 874 loss: 8.5615261e-07
Iter: 875 loss: 8.56062854e-07
Iter: 876 loss: 8.56066549e-07
Iter: 877 loss: 8.55972417e-07
Iter: 878 loss: 8.55936264e-07
Iter: 879 loss: 8.55874532e-07
Iter: 880 loss: 8.55728956e-07
Iter: 881 loss: 8.56392205e-07
Iter: 882 loss: 8.55697522e-07
Iter: 883 loss: 8.55562e-07
Iter: 884 loss: 8.5628983e-07
Iter: 885 loss: 8.55550752e-07
Iter: 886 loss: 8.55432177e-07
Iter: 887 loss: 8.55842529e-07
Iter: 888 loss: 8.55389544e-07
Iter: 889 loss: 8.55294388e-07
Iter: 890 loss: 8.56180066e-07
Iter: 891 loss: 8.55295525e-07
Iter: 892 loss: 8.55228677e-07
Iter: 893 loss: 8.55402277e-07
Iter: 894 loss: 8.55219128e-07
Iter: 895 loss: 8.55154326e-07
Iter: 896 loss: 8.55055305e-07
Iter: 897 loss: 8.55047347e-07
Iter: 898 loss: 8.54978566e-07
Iter: 899 loss: 8.56064446e-07
Iter: 900 loss: 8.54973905e-07
Iter: 901 loss: 8.54875736e-07
Iter: 902 loss: 8.54827761e-07
Iter: 903 loss: 8.54780296e-07
Iter: 904 loss: 8.54664052e-07
Iter: 905 loss: 8.54579923e-07
Iter: 906 loss: 8.54531038e-07
Iter: 907 loss: 8.5438262e-07
Iter: 908 loss: 8.54375969e-07
Iter: 909 loss: 8.54298492e-07
Iter: 910 loss: 8.54121e-07
Iter: 911 loss: 8.56259589e-07
Iter: 912 loss: 8.54093059e-07
Iter: 913 loss: 8.53947142e-07
Iter: 914 loss: 8.55128746e-07
Iter: 915 loss: 8.53919857e-07
Iter: 916 loss: 8.53820097e-07
Iter: 917 loss: 8.53984716e-07
Iter: 918 loss: 8.53799747e-07
Iter: 919 loss: 8.53703682e-07
Iter: 920 loss: 8.54146776e-07
Iter: 921 loss: 8.53683332e-07
Iter: 922 loss: 8.53622396e-07
Iter: 923 loss: 8.53895244e-07
Iter: 924 loss: 8.5358306e-07
Iter: 925 loss: 8.5352724e-07
Iter: 926 loss: 8.53781387e-07
Iter: 927 loss: 8.53480969e-07
Iter: 928 loss: 8.53396955e-07
Iter: 929 loss: 8.54331e-07
Iter: 930 loss: 8.53408e-07
Iter: 931 loss: 8.53336701e-07
Iter: 932 loss: 8.53341589e-07
Iter: 933 loss: 8.53291e-07
Iter: 934 loss: 8.53245751e-07
Iter: 935 loss: 8.53873189e-07
Iter: 936 loss: 8.53245353e-07
Iter: 937 loss: 8.5319823e-07
Iter: 938 loss: 8.53139341e-07
Iter: 939 loss: 8.53132349e-07
Iter: 940 loss: 8.53034749e-07
Iter: 941 loss: 8.5363e-07
Iter: 942 loss: 8.53035658e-07
Iter: 943 loss: 8.52977337e-07
Iter: 944 loss: 8.52842959e-07
Iter: 945 loss: 8.55118401e-07
Iter: 946 loss: 8.52847904e-07
Iter: 947 loss: 8.52705966e-07
Iter: 948 loss: 8.52982225e-07
Iter: 949 loss: 8.52630933e-07
Iter: 950 loss: 8.52546691e-07
Iter: 951 loss: 8.52508151e-07
Iter: 952 loss: 8.52476433e-07
Iter: 953 loss: 8.52318863e-07
Iter: 954 loss: 8.53344545e-07
Iter: 955 loss: 8.52265657e-07
Iter: 956 loss: 8.52127755e-07
Iter: 957 loss: 8.53165261e-07
Iter: 958 loss: 8.52083076e-07
Iter: 959 loss: 8.51981326e-07
Iter: 960 loss: 8.51922948e-07
Iter: 961 loss: 8.5186997e-07
Iter: 962 loss: 8.51773279e-07
Iter: 963 loss: 8.5173815e-07
Iter: 964 loss: 8.51681136e-07
Iter: 965 loss: 8.52144e-07
Iter: 966 loss: 8.51647656e-07
Iter: 967 loss: 8.51595416e-07
Iter: 968 loss: 8.51564891e-07
Iter: 969 loss: 8.51514244e-07
Iter: 970 loss: 8.51449499e-07
Iter: 971 loss: 8.51431253e-07
Iter: 972 loss: 8.51393e-07
Iter: 973 loss: 8.51337347e-07
Iter: 974 loss: 8.51318305e-07
Iter: 975 loss: 8.51265497e-07
Iter: 976 loss: 8.51276e-07
Iter: 977 loss: 8.51234745e-07
Iter: 978 loss: 8.51154198e-07
Iter: 979 loss: 8.52760593e-07
Iter: 980 loss: 8.51138225e-07
Iter: 981 loss: 8.51022435e-07
Iter: 982 loss: 8.50998504e-07
Iter: 983 loss: 8.50905e-07
Iter: 984 loss: 8.50874244e-07
Iter: 985 loss: 8.50839228e-07
Iter: 986 loss: 8.50753e-07
Iter: 987 loss: 8.50528295e-07
Iter: 988 loss: 8.52949199e-07
Iter: 989 loss: 8.50499077e-07
Iter: 990 loss: 8.50299671e-07
Iter: 991 loss: 8.50617369e-07
Iter: 992 loss: 8.50203776e-07
Iter: 993 loss: 8.49966455e-07
Iter: 994 loss: 8.50586616e-07
Iter: 995 loss: 8.49899379e-07
Iter: 996 loss: 8.49721175e-07
Iter: 997 loss: 8.50902097e-07
Iter: 998 loss: 8.49693549e-07
Iter: 999 loss: 8.49590208e-07
Iter: 1000 loss: 8.51178356e-07
Iter: 1001 loss: 8.49570483e-07
Iter: 1002 loss: 8.49475327e-07
Iter: 1003 loss: 8.50108222e-07
Iter: 1004 loss: 8.49475157e-07
Iter: 1005 loss: 8.49433377e-07
Iter: 1006 loss: 8.49491471e-07
Iter: 1007 loss: 8.49397907e-07
Iter: 1008 loss: 8.49325147e-07
Iter: 1009 loss: 8.49645062e-07
Iter: 1010 loss: 8.49313e-07
Iter: 1011 loss: 8.4922965e-07
Iter: 1012 loss: 8.49215326e-07
Iter: 1013 loss: 8.49186392e-07
Iter: 1014 loss: 8.49120738e-07
Iter: 1015 loss: 8.49113462e-07
Iter: 1016 loss: 8.49054175e-07
Iter: 1017 loss: 8.48925879e-07
Iter: 1018 loss: 8.49757157e-07
Iter: 1019 loss: 8.48929062e-07
Iter: 1020 loss: 8.48788886e-07
Iter: 1021 loss: 8.49648472e-07
Iter: 1022 loss: 8.48769673e-07
Iter: 1023 loss: 8.48698278e-07
Iter: 1024 loss: 8.48697709e-07
Iter: 1025 loss: 8.48629838e-07
Iter: 1026 loss: 8.48530931e-07
Iter: 1027 loss: 8.48508876e-07
Iter: 1028 loss: 8.48414913e-07
Iter: 1029 loss: 8.48269337e-07
Iter: 1030 loss: 8.48239324e-07
Iter: 1031 loss: 8.48073341e-07
Iter: 1032 loss: 8.49048547e-07
Iter: 1033 loss: 8.48026843e-07
Iter: 1034 loss: 8.47946808e-07
Iter: 1035 loss: 8.47920433e-07
Iter: 1036 loss: 8.47836873e-07
Iter: 1037 loss: 8.47877573e-07
Iter: 1038 loss: 8.47760361e-07
Iter: 1039 loss: 8.47675324e-07
Iter: 1040 loss: 8.47984097e-07
Iter: 1041 loss: 8.47646902e-07
Iter: 1042 loss: 8.47554361e-07
Iter: 1043 loss: 8.47929414e-07
Iter: 1044 loss: 8.47539e-07
Iter: 1045 loss: 8.47462957e-07
Iter: 1046 loss: 8.47712045e-07
Iter: 1047 loss: 8.47445449e-07
Iter: 1048 loss: 8.4737519e-07
Iter: 1049 loss: 8.47604156e-07
Iter: 1050 loss: 8.47355523e-07
Iter: 1051 loss: 8.47311753e-07
Iter: 1052 loss: 8.47168053e-07
Iter: 1053 loss: 8.48416448e-07
Iter: 1054 loss: 8.47142474e-07
Iter: 1055 loss: 8.47016622e-07
Iter: 1056 loss: 8.47739045e-07
Iter: 1057 loss: 8.47003776e-07
Iter: 1058 loss: 8.46918908e-07
Iter: 1059 loss: 8.46932153e-07
Iter: 1060 loss: 8.46857631e-07
Iter: 1061 loss: 8.46791409e-07
Iter: 1062 loss: 8.46785156e-07
Iter: 1063 loss: 8.46707167e-07
Iter: 1064 loss: 8.46809e-07
Iter: 1065 loss: 8.46637818e-07
Iter: 1066 loss: 8.46545845e-07
Iter: 1067 loss: 8.46564376e-07
Iter: 1068 loss: 8.46483715e-07
Iter: 1069 loss: 8.46382704e-07
Iter: 1070 loss: 8.46379294e-07
Iter: 1071 loss: 8.46287946e-07
Iter: 1072 loss: 8.46246508e-07
Iter: 1073 loss: 8.46210071e-07
Iter: 1074 loss: 8.46100477e-07
Iter: 1075 loss: 8.46670332e-07
Iter: 1076 loss: 8.46084731e-07
Iter: 1077 loss: 8.45940349e-07
Iter: 1078 loss: 8.46035277e-07
Iter: 1079 loss: 8.45877139e-07
Iter: 1080 loss: 8.45751629e-07
Iter: 1081 loss: 8.46919079e-07
Iter: 1082 loss: 8.45772661e-07
Iter: 1083 loss: 8.45696718e-07
Iter: 1084 loss: 8.45700583e-07
Iter: 1085 loss: 8.45641637e-07
Iter: 1086 loss: 8.45541535e-07
Iter: 1087 loss: 8.45538523e-07
Iter: 1088 loss: 8.45467639e-07
Iter: 1089 loss: 8.4538209e-07
Iter: 1090 loss: 8.45599061e-07
Iter: 1091 loss: 8.45334739e-07
Iter: 1092 loss: 8.45272e-07
Iter: 1093 loss: 8.45271074e-07
Iter: 1094 loss: 8.45230147e-07
Iter: 1095 loss: 8.45104296e-07
Iter: 1096 loss: 8.46361388e-07
Iter: 1097 loss: 8.45120042e-07
Iter: 1098 loss: 8.45013176e-07
Iter: 1099 loss: 8.45240947e-07
Iter: 1100 loss: 8.44961619e-07
Iter: 1101 loss: 8.44851115e-07
Iter: 1102 loss: 8.45982299e-07
Iter: 1103 loss: 8.44855208e-07
Iter: 1104 loss: 8.44786882e-07
Iter: 1105 loss: 8.44795693e-07
Iter: 1106 loss: 8.44756073e-07
Iter: 1107 loss: 8.44702413e-07
Iter: 1108 loss: 8.44698e-07
Iter: 1109 loss: 8.44634712e-07
Iter: 1110 loss: 8.45488842e-07
Iter: 1111 loss: 8.44631131e-07
Iter: 1112 loss: 8.4457406e-07
Iter: 1113 loss: 8.44544843e-07
Iter: 1114 loss: 8.44535577e-07
Iter: 1115 loss: 8.44436e-07
Iter: 1116 loss: 8.44778583e-07
Iter: 1117 loss: 8.44406088e-07
Iter: 1118 loss: 8.4430269e-07
Iter: 1119 loss: 8.44260626e-07
Iter: 1120 loss: 8.44235615e-07
Iter: 1121 loss: 8.44092483e-07
Iter: 1122 loss: 8.44341344e-07
Iter: 1123 loss: 8.44048259e-07
Iter: 1124 loss: 8.43972089e-07
Iter: 1125 loss: 8.45296029e-07
Iter: 1126 loss: 8.43950431e-07
Iter: 1127 loss: 8.4389228e-07
Iter: 1128 loss: 8.4416331e-07
Iter: 1129 loss: 8.43841917e-07
Iter: 1130 loss: 8.43787291e-07
Iter: 1131 loss: 8.43740395e-07
Iter: 1132 loss: 8.43733062e-07
Iter: 1133 loss: 8.43678208e-07
Iter: 1134 loss: 8.4373039e-07
Iter: 1135 loss: 8.43629664e-07
Iter: 1136 loss: 8.43647342e-07
Iter: 1137 loss: 8.43573048e-07
Iter: 1138 loss: 8.43542296e-07
Iter: 1139 loss: 8.43512794e-07
Iter: 1140 loss: 8.43509497e-07
Iter: 1141 loss: 8.43466808e-07
Iter: 1142 loss: 8.43662065e-07
Iter: 1143 loss: 8.43425426e-07
Iter: 1144 loss: 8.43356815e-07
Iter: 1145 loss: 8.43369151e-07
Iter: 1146 loss: 8.43311909e-07
Iter: 1147 loss: 8.43253872e-07
Iter: 1148 loss: 8.43900352e-07
Iter: 1149 loss: 8.43257908e-07
Iter: 1150 loss: 8.43188104e-07
Iter: 1151 loss: 8.43122905e-07
Iter: 1152 loss: 8.43118301e-07
Iter: 1153 loss: 8.43020189e-07
Iter: 1154 loss: 8.4320277e-07
Iter: 1155 loss: 8.42973463e-07
Iter: 1156 loss: 8.42895702e-07
Iter: 1157 loss: 8.43063049e-07
Iter: 1158 loss: 8.42876659e-07
Iter: 1159 loss: 8.42846248e-07
Iter: 1160 loss: 8.42838119e-07
Iter: 1161 loss: 8.4279759e-07
Iter: 1162 loss: 8.42693964e-07
Iter: 1163 loss: 8.43784051e-07
Iter: 1164 loss: 8.42697716e-07
Iter: 1165 loss: 8.42569136e-07
Iter: 1166 loss: 8.42696522e-07
Iter: 1167 loss: 8.42500299e-07
Iter: 1168 loss: 8.42405825e-07
Iter: 1169 loss: 8.428608e-07
Iter: 1170 loss: 8.42383542e-07
Iter: 1171 loss: 8.42248596e-07
Iter: 1172 loss: 8.43846124e-07
Iter: 1173 loss: 8.42240127e-07
Iter: 1174 loss: 8.42163558e-07
Iter: 1175 loss: 8.4209978e-07
Iter: 1176 loss: 8.42084e-07
Iter: 1177 loss: 8.41975634e-07
Iter: 1178 loss: 8.43160592e-07
Iter: 1179 loss: 8.4195176e-07
Iter: 1180 loss: 8.41881842e-07
Iter: 1181 loss: 8.41932319e-07
Iter: 1182 loss: 8.41845406e-07
Iter: 1183 loss: 8.4178464e-07
Iter: 1184 loss: 8.42643828e-07
Iter: 1185 loss: 8.4177384e-07
Iter: 1186 loss: 8.41728e-07
Iter: 1187 loss: 8.41646681e-07
Iter: 1188 loss: 8.41659414e-07
Iter: 1189 loss: 8.41544704e-07
Iter: 1190 loss: 8.41595124e-07
Iter: 1191 loss: 8.41469387e-07
Iter: 1192 loss: 8.41376789e-07
Iter: 1193 loss: 8.41371957e-07
Iter: 1194 loss: 8.41274812e-07
Iter: 1195 loss: 8.41273447e-07
Iter: 1196 loss: 8.41187216e-07
Iter: 1197 loss: 8.41104907e-07
Iter: 1198 loss: 8.41138103e-07
Iter: 1199 loss: 8.41059943e-07
Iter: 1200 loss: 8.4094313e-07
Iter: 1201 loss: 8.41193355e-07
Iter: 1202 loss: 8.40925225e-07
Iter: 1203 loss: 8.4087975e-07
Iter: 1204 loss: 8.40879181e-07
Iter: 1205 loss: 8.40825351e-07
Iter: 1206 loss: 8.40745713e-07
Iter: 1207 loss: 8.40736789e-07
Iter: 1208 loss: 8.40661755e-07
Iter: 1209 loss: 8.40658572e-07
Iter: 1210 loss: 8.40595419e-07
Iter: 1211 loss: 8.40532834e-07
Iter: 1212 loss: 8.40528401e-07
Iter: 1213 loss: 8.40425173e-07
Iter: 1214 loss: 8.41078361e-07
Iter: 1215 loss: 8.4042108e-07
Iter: 1216 loss: 8.40318194e-07
Iter: 1217 loss: 8.40272833e-07
Iter: 1218 loss: 8.40210646e-07
Iter: 1219 loss: 8.40082123e-07
Iter: 1220 loss: 8.40235373e-07
Iter: 1221 loss: 8.39990605e-07
Iter: 1222 loss: 8.39882887e-07
Iter: 1223 loss: 8.40287271e-07
Iter: 1224 loss: 8.39854295e-07
Iter: 1225 loss: 8.39818938e-07
Iter: 1226 loss: 8.39799782e-07
Iter: 1227 loss: 8.39761583e-07
Iter: 1228 loss: 8.39657673e-07
Iter: 1229 loss: 8.41718474e-07
Iter: 1230 loss: 8.39656195e-07
Iter: 1231 loss: 8.39590371e-07
Iter: 1232 loss: 8.39602251e-07
Iter: 1233 loss: 8.39503571e-07
Iter: 1234 loss: 8.39423706e-07
Iter: 1235 loss: 8.40273515e-07
Iter: 1236 loss: 8.39418419e-07
Iter: 1237 loss: 8.39316272e-07
Iter: 1238 loss: 8.39539211e-07
Iter: 1239 loss: 8.39268864e-07
Iter: 1240 loss: 8.39192126e-07
Iter: 1241 loss: 8.39310587e-07
Iter: 1242 loss: 8.39139375e-07
Iter: 1243 loss: 8.39031088e-07
Iter: 1244 loss: 8.39779261e-07
Iter: 1245 loss: 8.39039842e-07
Iter: 1246 loss: 8.38994083e-07
Iter: 1247 loss: 8.39008067e-07
Iter: 1248 loss: 8.38948836e-07
Iter: 1249 loss: 8.38861069e-07
Iter: 1250 loss: 8.39367374e-07
Iter: 1251 loss: 8.3886863e-07
Iter: 1252 loss: 8.38824747e-07
Iter: 1253 loss: 8.38748235e-07
Iter: 1254 loss: 8.38752669e-07
Iter: 1255 loss: 8.38666892e-07
Iter: 1256 loss: 8.38794222e-07
Iter: 1257 loss: 8.38640574e-07
Iter: 1258 loss: 8.38556048e-07
Iter: 1259 loss: 8.38898927e-07
Iter: 1260 loss: 8.3854286e-07
Iter: 1261 loss: 8.38416213e-07
Iter: 1262 loss: 8.38827589e-07
Iter: 1263 loss: 8.38384153e-07
Iter: 1264 loss: 8.38301844e-07
Iter: 1265 loss: 8.38178835e-07
Iter: 1266 loss: 8.38163146e-07
Iter: 1267 loss: 8.38027461e-07
Iter: 1268 loss: 8.3858481e-07
Iter: 1269 loss: 8.37981077e-07
Iter: 1270 loss: 8.37896096e-07
Iter: 1271 loss: 8.37889218e-07
Iter: 1272 loss: 8.37824132e-07
Iter: 1273 loss: 8.37757682e-07
Iter: 1274 loss: 8.37752623e-07
Iter: 1275 loss: 8.37659229e-07
Iter: 1276 loss: 8.38897904e-07
Iter: 1277 loss: 8.37669e-07
Iter: 1278 loss: 8.37634389e-07
Iter: 1279 loss: 8.37640869e-07
Iter: 1280 loss: 8.37606194e-07
Iter: 1281 loss: 8.37542416e-07
Iter: 1282 loss: 8.38047299e-07
Iter: 1283 loss: 8.37553e-07
Iter: 1284 loss: 8.37502625e-07
Iter: 1285 loss: 8.37437938e-07
Iter: 1286 loss: 8.3742e-07
Iter: 1287 loss: 8.37368191e-07
Iter: 1288 loss: 8.37485118e-07
Iter: 1289 loss: 8.37362506e-07
Iter: 1290 loss: 8.37235859e-07
Iter: 1291 loss: 8.37286279e-07
Iter: 1292 loss: 8.37160428e-07
Iter: 1293 loss: 8.37140078e-07
Iter: 1294 loss: 8.37134849e-07
Iter: 1295 loss: 8.37069422e-07
Iter: 1296 loss: 8.369733e-07
Iter: 1297 loss: 8.38979759e-07
Iter: 1298 loss: 8.36963579e-07
Iter: 1299 loss: 8.36861716e-07
Iter: 1300 loss: 8.36835284e-07
Iter: 1301 loss: 8.36784579e-07
Iter: 1302 loss: 8.36741037e-07
Iter: 1303 loss: 8.36700451e-07
Iter: 1304 loss: 8.3666e-07
Iter: 1305 loss: 8.36603363e-07
Iter: 1306 loss: 8.36595632e-07
Iter: 1307 loss: 8.36489562e-07
Iter: 1308 loss: 8.36848244e-07
Iter: 1309 loss: 8.36489448e-07
Iter: 1310 loss: 8.36378888e-07
Iter: 1311 loss: 8.36497179e-07
Iter: 1312 loss: 8.36323125e-07
Iter: 1313 loss: 8.36237689e-07
Iter: 1314 loss: 8.36302831e-07
Iter: 1315 loss: 8.36191e-07
Iter: 1316 loss: 8.36109393e-07
Iter: 1317 loss: 8.36099048e-07
Iter: 1318 loss: 8.36054483e-07
Iter: 1319 loss: 8.3594864e-07
Iter: 1320 loss: 8.359454e-07
Iter: 1321 loss: 8.35896e-07
Iter: 1322 loss: 8.36361096e-07
Iter: 1323 loss: 8.35863602e-07
Iter: 1324 loss: 8.35826199e-07
Iter: 1325 loss: 8.36012305e-07
Iter: 1326 loss: 8.3581574e-07
Iter: 1327 loss: 8.35758783e-07
Iter: 1328 loss: 8.36240702e-07
Iter: 1329 loss: 8.35771061e-07
Iter: 1330 loss: 8.35724677e-07
Iter: 1331 loss: 8.3564214e-07
Iter: 1332 loss: 8.37036055e-07
Iter: 1333 loss: 8.35648279e-07
Iter: 1334 loss: 8.35554829e-07
Iter: 1335 loss: 8.36199831e-07
Iter: 1336 loss: 8.35544256e-07
Iter: 1337 loss: 8.35467631e-07
Iter: 1338 loss: 8.35889864e-07
Iter: 1339 loss: 8.35463084e-07
Iter: 1340 loss: 8.35410503e-07
Iter: 1341 loss: 8.35435117e-07
Iter: 1342 loss: 8.35352921e-07
Iter: 1343 loss: 8.35283174e-07
Iter: 1344 loss: 8.35743663e-07
Iter: 1345 loss: 8.35272e-07
Iter: 1346 loss: 8.35218543e-07
Iter: 1347 loss: 8.35103208e-07
Iter: 1348 loss: 8.37010361e-07
Iter: 1349 loss: 8.35106619e-07
Iter: 1350 loss: 8.35033404e-07
Iter: 1351 loss: 8.35020387e-07
Iter: 1352 loss: 8.34945695e-07
Iter: 1353 loss: 8.34905336e-07
Iter: 1354 loss: 8.34886805e-07
Iter: 1355 loss: 8.34797788e-07
Iter: 1356 loss: 8.34780053e-07
Iter: 1357 loss: 8.34725256e-07
Iter: 1358 loss: 8.34615889e-07
Iter: 1359 loss: 8.35315916e-07
Iter: 1360 loss: 8.3460634e-07
Iter: 1361 loss: 8.34517493e-07
Iter: 1362 loss: 8.34900902e-07
Iter: 1363 loss: 8.34520506e-07
Iter: 1364 loss: 8.34434786e-07
Iter: 1365 loss: 8.3448316e-07
Iter: 1366 loss: 8.34375442e-07
Iter: 1367 loss: 8.34311891e-07
Iter: 1368 loss: 8.34480943e-07
Iter: 1369 loss: 8.34284208e-07
Iter: 1370 loss: 8.34246521e-07
Iter: 1371 loss: 8.34251068e-07
Iter: 1372 loss: 8.3421719e-07
Iter: 1373 loss: 8.34178536e-07
Iter: 1374 loss: 8.34182629e-07
Iter: 1375 loss: 8.34122261e-07
Iter: 1376 loss: 8.34612194e-07
Iter: 1377 loss: 8.3411021e-07
Iter: 1378 loss: 8.34058824e-07
Iter: 1379 loss: 8.34038701e-07
Iter: 1380 loss: 8.34017214e-07
Iter: 1381 loss: 8.33960257e-07
Iter: 1382 loss: 8.34350146e-07
Iter: 1383 loss: 8.33958e-07
Iter: 1384 loss: 8.33878062e-07
Iter: 1385 loss: 8.33876243e-07
Iter: 1386 loss: 8.33838101e-07
Iter: 1387 loss: 8.33738852e-07
Iter: 1388 loss: 8.3388619e-07
Iter: 1389 loss: 8.33685931e-07
Iter: 1390 loss: 8.33623176e-07
Iter: 1391 loss: 8.34222476e-07
Iter: 1392 loss: 8.33626814e-07
Iter: 1393 loss: 8.3355576e-07
Iter: 1394 loss: 8.33662966e-07
Iter: 1395 loss: 8.33541264e-07
Iter: 1396 loss: 8.33459e-07
Iter: 1397 loss: 8.33741183e-07
Iter: 1398 loss: 8.33434797e-07
Iter: 1399 loss: 8.33374656e-07
Iter: 1400 loss: 8.33369711e-07
Iter: 1401 loss: 8.33333615e-07
Iter: 1402 loss: 8.33285412e-07
Iter: 1403 loss: 8.33288482e-07
Iter: 1404 loss: 8.33245849e-07
Iter: 1405 loss: 8.33168315e-07
Iter: 1406 loss: 8.33174454e-07
Iter: 1407 loss: 8.33092429e-07
Iter: 1408 loss: 8.33945137e-07
Iter: 1409 loss: 8.3310681e-07
Iter: 1410 loss: 8.33060426e-07
Iter: 1411 loss: 8.33030583e-07
Iter: 1412 loss: 8.33012564e-07
Iter: 1413 loss: 8.32948444e-07
Iter: 1414 loss: 8.33176841e-07
Iter: 1415 loss: 8.32937e-07
Iter: 1416 loss: 8.32847149e-07
Iter: 1417 loss: 8.33615445e-07
Iter: 1418 loss: 8.32844535e-07
Iter: 1419 loss: 8.32830779e-07
Iter: 1420 loss: 8.32807473e-07
Iter: 1421 loss: 8.32782e-07
Iter: 1422 loss: 8.32747844e-07
Iter: 1423 loss: 8.3301552e-07
Iter: 1424 loss: 8.32728233e-07
Iter: 1425 loss: 8.32693445e-07
Iter: 1426 loss: 8.32909677e-07
Iter: 1427 loss: 8.32683384e-07
Iter: 1428 loss: 8.32653313e-07
Iter: 1429 loss: 8.32820092e-07
Iter: 1430 loss: 8.32644e-07
Iter: 1431 loss: 8.32630633e-07
Iter: 1432 loss: 8.32568958e-07
Iter: 1433 loss: 8.32570322e-07
Iter: 1434 loss: 8.32514274e-07
Iter: 1435 loss: 8.32528372e-07
Iter: 1436 loss: 8.32469482e-07
Iter: 1437 loss: 8.3245709e-07
Iter: 1438 loss: 8.32450837e-07
Iter: 1439 loss: 8.32389389e-07
Iter: 1440 loss: 8.32663773e-07
Iter: 1441 loss: 8.32375576e-07
Iter: 1442 loss: 8.32316573e-07
Iter: 1443 loss: 8.32378362e-07
Iter: 1444 loss: 8.32269961e-07
Iter: 1445 loss: 8.32213573e-07
Iter: 1446 loss: 8.32219939e-07
Iter: 1447 loss: 8.321785e-07
Iter: 1448 loss: 8.32116427e-07
Iter: 1449 loss: 8.33178319e-07
Iter: 1450 loss: 8.32119895e-07
Iter: 1451 loss: 8.32056855e-07
Iter: 1452 loss: 8.31989269e-07
Iter: 1453 loss: 8.31996715e-07
Iter: 1454 loss: 8.31938792e-07
Iter: 1455 loss: 8.3240559e-07
Iter: 1456 loss: 8.31928219e-07
Iter: 1457 loss: 8.31884108e-07
Iter: 1458 loss: 8.32331e-07
Iter: 1459 loss: 8.3186444e-07
Iter: 1460 loss: 8.31843124e-07
Iter: 1461 loss: 8.31914917e-07
Iter: 1462 loss: 8.31833404e-07
Iter: 1463 loss: 8.3179259e-07
Iter: 1464 loss: 8.31841703e-07
Iter: 1465 loss: 8.31783723e-07
Iter: 1466 loss: 8.31745922e-07
Iter: 1467 loss: 8.32025762e-07
Iter: 1468 loss: 8.31746434e-07
Iter: 1469 loss: 8.31707894e-07
Iter: 1470 loss: 8.31743193e-07
Iter: 1471 loss: 8.31686066e-07
Iter: 1472 loss: 8.31655484e-07
Iter: 1473 loss: 8.3164349e-07
Iter: 1474 loss: 8.3161359e-07
Iter: 1475 loss: 8.3157812e-07
Iter: 1476 loss: 8.31860802e-07
Iter: 1477 loss: 8.31571242e-07
Iter: 1478 loss: 8.31529e-07
Iter: 1479 loss: 8.31460966e-07
Iter: 1480 loss: 8.31456248e-07
Iter: 1481 loss: 8.31393209e-07
Iter: 1482 loss: 8.32253249e-07
Iter: 1483 loss: 8.31400371e-07
Iter: 1484 loss: 8.31354214e-07
Iter: 1485 loss: 8.31289526e-07
Iter: 1486 loss: 8.31295949e-07
Iter: 1487 loss: 8.3121995e-07
Iter: 1488 loss: 8.31322268e-07
Iter: 1489 loss: 8.31170098e-07
Iter: 1490 loss: 8.31120815e-07
Iter: 1491 loss: 8.31674356e-07
Iter: 1492 loss: 8.31106604e-07
Iter: 1493 loss: 8.31035834e-07
Iter: 1494 loss: 8.31180046e-07
Iter: 1495 loss: 8.31037255e-07
Iter: 1496 loss: 8.30950967e-07
Iter: 1497 loss: 8.3106022e-07
Iter: 1498 loss: 8.30928116e-07
Iter: 1499 loss: 8.30878093e-07
Iter: 1500 loss: 8.31074772e-07
Iter: 1501 loss: 8.30883323e-07
Iter: 1502 loss: 8.30811132e-07
Iter: 1503 loss: 8.31251555e-07
Iter: 1504 loss: 8.30789304e-07
Iter: 1505 loss: 8.30730187e-07
Iter: 1506 loss: 8.30690851e-07
Iter: 1507 loss: 8.32251203e-07
Iter: 1508 loss: 8.30683803e-07
Iter: 1509 loss: 8.30622298e-07
Iter: 1510 loss: 8.30622753e-07
Iter: 1511 loss: 8.30602e-07
Iter: 1512 loss: 8.30521515e-07
Iter: 1513 loss: 8.30512931e-07
Iter: 1514 loss: 8.30486215e-07
Iter: 1515 loss: 8.31182604e-07
Iter: 1516 loss: 8.30493718e-07
Iter: 1517 loss: 8.30455178e-07
Iter: 1518 loss: 8.3042471e-07
Iter: 1519 loss: 8.30402087e-07
Iter: 1520 loss: 8.3034206e-07
Iter: 1521 loss: 8.30368435e-07
Iter: 1522 loss: 8.30324893e-07
Iter: 1523 loss: 8.30235763e-07
Iter: 1524 loss: 8.30854901e-07
Iter: 1525 loss: 8.30244971e-07
Iter: 1526 loss: 8.30175054e-07
Iter: 1527 loss: 8.30326428e-07
Iter: 1528 loss: 8.30158911e-07
Iter: 1529 loss: 8.30074782e-07
Iter: 1530 loss: 8.30105705e-07
Iter: 1531 loss: 8.30014073e-07
Iter: 1532 loss: 8.29945748e-07
Iter: 1533 loss: 8.30238832e-07
Iter: 1534 loss: 8.29902e-07
Iter: 1535 loss: 8.29818759e-07
Iter: 1536 loss: 8.31018838e-07
Iter: 1537 loss: 8.29820067e-07
Iter: 1538 loss: 8.29778401e-07
Iter: 1539 loss: 8.29657665e-07
Iter: 1540 loss: 8.31506725e-07
Iter: 1541 loss: 8.29654198e-07
Iter: 1542 loss: 8.29637713e-07
Iter: 1543 loss: 8.29607188e-07
Iter: 1544 loss: 8.29570808e-07
Iter: 1545 loss: 8.29517262e-07
Iter: 1546 loss: 8.29498504e-07
Iter: 1547 loss: 8.29466671e-07
Iter: 1548 loss: 8.30087743e-07
Iter: 1549 loss: 8.29456098e-07
Iter: 1550 loss: 8.29384476e-07
Iter: 1551 loss: 8.29486794e-07
Iter: 1552 loss: 8.2936765e-07
Iter: 1553 loss: 8.29305691e-07
Iter: 1554 loss: 8.292335e-07
Iter: 1555 loss: 8.29223666e-07
Iter: 1556 loss: 8.291276e-07
Iter: 1557 loss: 8.29958935e-07
Iter: 1558 loss: 8.29131295e-07
Iter: 1559 loss: 8.2903648e-07
Iter: 1560 loss: 8.295616e-07
Iter: 1561 loss: 8.29046826e-07
Iter: 1562 loss: 8.29002772e-07
Iter: 1563 loss: 8.29010332e-07
Iter: 1564 loss: 8.28953262e-07
Iter: 1565 loss: 8.28889085e-07
Iter: 1566 loss: 8.29116516e-07
Iter: 1567 loss: 8.28858504e-07
Iter: 1568 loss: 8.28813e-07
Iter: 1569 loss: 8.28812631e-07
Iter: 1570 loss: 8.2876727e-07
Iter: 1571 loss: 8.28718214e-07
Iter: 1572 loss: 8.28717702e-07
Iter: 1573 loss: 8.2865904e-07
Iter: 1574 loss: 8.29407327e-07
Iter: 1575 loss: 8.28657e-07
Iter: 1576 loss: 8.28603106e-07
Iter: 1577 loss: 8.28507154e-07
Iter: 1578 loss: 8.30647934e-07
Iter: 1579 loss: 8.28501356e-07
Iter: 1580 loss: 8.28360328e-07
Iter: 1581 loss: 8.2895383e-07
Iter: 1582 loss: 8.28346856e-07
Iter: 1583 loss: 8.28240388e-07
Iter: 1584 loss: 8.28806037e-07
Iter: 1585 loss: 8.28227542e-07
Iter: 1586 loss: 8.28151542e-07
Iter: 1587 loss: 8.28064742e-07
Iter: 1588 loss: 8.2804786e-07
Iter: 1589 loss: 8.27977829e-07
Iter: 1590 loss: 8.28738337e-07
Iter: 1591 loss: 8.27951396e-07
Iter: 1592 loss: 8.2789137e-07
Iter: 1593 loss: 8.28401539e-07
Iter: 1594 loss: 8.2790126e-07
Iter: 1595 loss: 8.27848339e-07
Iter: 1596 loss: 8.27841689e-07
Iter: 1597 loss: 8.27823669e-07
Iter: 1598 loss: 8.27759322e-07
Iter: 1599 loss: 8.27973452e-07
Iter: 1600 loss: 8.27743349e-07
Iter: 1601 loss: 8.27677582e-07
Iter: 1602 loss: 8.28296265e-07
Iter: 1603 loss: 8.27686279e-07
Iter: 1604 loss: 8.27632164e-07
Iter: 1605 loss: 8.27549457e-07
Iter: 1606 loss: 8.27557187e-07
Iter: 1607 loss: 8.27463055e-07
Iter: 1608 loss: 8.28090947e-07
Iter: 1609 loss: 8.27466522e-07
Iter: 1610 loss: 8.27375857e-07
Iter: 1611 loss: 8.27391432e-07
Iter: 1612 loss: 8.27296049e-07
Iter: 1613 loss: 8.27216923e-07
Iter: 1614 loss: 8.27341523e-07
Iter: 1615 loss: 8.27186852e-07
Iter: 1616 loss: 8.27073507e-07
Iter: 1617 loss: 8.28002726e-07
Iter: 1618 loss: 8.27101871e-07
Iter: 1619 loss: 8.27016038e-07
Iter: 1620 loss: 8.26975224e-07
Iter: 1621 loss: 8.26962719e-07
Iter: 1622 loss: 8.26876487e-07
Iter: 1623 loss: 8.27187137e-07
Iter: 1624 loss: 8.26862163e-07
Iter: 1625 loss: 8.26829933e-07
Iter: 1626 loss: 8.27165763e-07
Iter: 1627 loss: 8.26810719e-07
Iter: 1628 loss: 8.26742e-07
Iter: 1629 loss: 8.26687597e-07
Iter: 1630 loss: 8.26678502e-07
Iter: 1631 loss: 8.26587495e-07
Iter: 1632 loss: 8.26842779e-07
Iter: 1633 loss: 8.26575388e-07
Iter: 1634 loss: 8.26483188e-07
Iter: 1635 loss: 8.27329188e-07
Iter: 1636 loss: 8.26467499e-07
Iter: 1637 loss: 8.26406279e-07
Iter: 1638 loss: 8.2635836e-07
Iter: 1639 loss: 8.26312e-07
Iter: 1640 loss: 8.26235407e-07
Iter: 1641 loss: 8.26512462e-07
Iter: 1642 loss: 8.26213807e-07
Iter: 1643 loss: 8.26141331e-07
Iter: 1644 loss: 8.26497455e-07
Iter: 1645 loss: 8.261311e-07
Iter: 1646 loss: 8.26046289e-07
Iter: 1647 loss: 8.25984444e-07
Iter: 1648 loss: 8.25988e-07
Iter: 1649 loss: 8.25885195e-07
Iter: 1650 loss: 8.25888833e-07
Iter: 1651 loss: 8.25813061e-07
Iter: 1652 loss: 8.25819541e-07
Iter: 1653 loss: 8.2576355e-07
Iter: 1654 loss: 8.25711879e-07
Iter: 1655 loss: 8.25783218e-07
Iter: 1656 loss: 8.25646111e-07
Iter: 1657 loss: 8.25553684e-07
Iter: 1658 loss: 8.2627264e-07
Iter: 1659 loss: 8.25565735e-07
Iter: 1660 loss: 8.25466088e-07
Iter: 1661 loss: 8.2545489e-07
Iter: 1662 loss: 8.25380766e-07
Iter: 1663 loss: 8.25262816e-07
Iter: 1664 loss: 8.25382529e-07
Iter: 1665 loss: 8.25211885e-07
Iter: 1666 loss: 8.25096322e-07
Iter: 1667 loss: 8.25084953e-07
Iter: 1668 loss: 8.25015832e-07
Iter: 1669 loss: 8.25151119e-07
Iter: 1670 loss: 8.24995368e-07
Iter: 1671 loss: 8.24947449e-07
Iter: 1672 loss: 8.24972176e-07
Iter: 1673 loss: 8.24917947e-07
Iter: 1674 loss: 8.24815515e-07
Iter: 1675 loss: 8.25627e-07
Iter: 1676 loss: 8.24824667e-07
Iter: 1677 loss: 8.2479e-07
Iter: 1678 loss: 8.24706319e-07
Iter: 1679 loss: 8.26623477e-07
Iter: 1680 loss: 8.24709e-07
Iter: 1681 loss: 8.24658628e-07
Iter: 1682 loss: 8.24640324e-07
Iter: 1683 loss: 8.24593826e-07
Iter: 1684 loss: 8.24522431e-07
Iter: 1685 loss: 8.24501967e-07
Iter: 1686 loss: 8.24424092e-07
Iter: 1687 loss: 8.24575636e-07
Iter: 1688 loss: 8.24403628e-07
Iter: 1689 loss: 8.24325298e-07
Iter: 1690 loss: 8.25186362e-07
Iter: 1691 loss: 8.24315237e-07
Iter: 1692 loss: 8.24245035e-07
Iter: 1693 loss: 8.24275844e-07
Iter: 1694 loss: 8.24245149e-07
Iter: 1695 loss: 8.24153176e-07
Iter: 1696 loss: 8.24192455e-07
Iter: 1697 loss: 8.24089966e-07
Iter: 1698 loss: 8.24040058e-07
Iter: 1699 loss: 8.24037215e-07
Iter: 1700 loss: 8.23958032e-07
Iter: 1701 loss: 8.24058e-07
Iter: 1702 loss: 8.23944617e-07
Iter: 1703 loss: 8.239e-07
Iter: 1704 loss: 8.23830874e-07
Iter: 1705 loss: 8.23814844e-07
Iter: 1706 loss: 8.23760956e-07
Iter: 1707 loss: 8.23754249e-07
Iter: 1708 loss: 8.23714913e-07
Iter: 1709 loss: 8.23625896e-07
Iter: 1710 loss: 8.25165671e-07
Iter: 1711 loss: 8.23624873e-07
Iter: 1712 loss: 8.23584514e-07
Iter: 1713 loss: 8.2356911e-07
Iter: 1714 loss: 8.2354228e-07
Iter: 1715 loss: 8.23535515e-07
Iter: 1716 loss: 8.23509424e-07
Iter: 1717 loss: 8.2343638e-07
Iter: 1718 loss: 8.23537334e-07
Iter: 1719 loss: 8.23423477e-07
Iter: 1720 loss: 8.2337192e-07
Iter: 1721 loss: 8.23869186e-07
Iter: 1722 loss: 8.23365554e-07
Iter: 1723 loss: 8.23343271e-07
Iter: 1724 loss: 8.23366463e-07
Iter: 1725 loss: 8.23303765e-07
Iter: 1726 loss: 8.23259199e-07
Iter: 1727 loss: 8.23240498e-07
Iter: 1728 loss: 8.23192e-07
Iter: 1729 loss: 8.23144774e-07
Iter: 1730 loss: 8.23737366e-07
Iter: 1731 loss: 8.23132666e-07
Iter: 1732 loss: 8.23077244e-07
Iter: 1733 loss: 8.23226628e-07
Iter: 1734 loss: 8.23044616e-07
Iter: 1735 loss: 8.22966399e-07
Iter: 1736 loss: 8.22928939e-07
Iter: 1737 loss: 8.22904326e-07
Iter: 1738 loss: 8.22813263e-07
Iter: 1739 loss: 8.22822301e-07
Iter: 1740 loss: 8.22754885e-07
Iter: 1741 loss: 8.22677066e-07
Iter: 1742 loss: 8.22667403e-07
Iter: 1743 loss: 8.22609877e-07
Iter: 1744 loss: 8.22603226e-07
Iter: 1745 loss: 8.22553204e-07
Iter: 1746 loss: 8.22567358e-07
Iter: 1747 loss: 8.22496929e-07
Iter: 1748 loss: 8.22433e-07
Iter: 1749 loss: 8.22436164e-07
Iter: 1750 loss: 8.22374261e-07
Iter: 1751 loss: 8.2231594e-07
Iter: 1752 loss: 8.23107769e-07
Iter: 1753 loss: 8.22306447e-07
Iter: 1754 loss: 8.22240906e-07
Iter: 1755 loss: 8.22467427e-07
Iter: 1756 loss: 8.22196341e-07
Iter: 1757 loss: 8.22139896e-07
Iter: 1758 loss: 8.2216485e-07
Iter: 1759 loss: 8.22103516e-07
Iter: 1760 loss: 8.22081e-07
Iter: 1761 loss: 8.22610559e-07
Iter: 1762 loss: 8.22065942e-07
Iter: 1763 loss: 8.21985964e-07
Iter: 1764 loss: 8.2226552e-07
Iter: 1765 loss: 8.2196766e-07
Iter: 1766 loss: 8.21919741e-07
Iter: 1767 loss: 8.21905928e-07
Iter: 1768 loss: 8.21899391e-07
Iter: 1769 loss: 8.21846697e-07
Iter: 1770 loss: 8.21841923e-07
Iter: 1771 loss: 8.21835783e-07
Iter: 1772 loss: 8.21757908e-07
Iter: 1773 loss: 8.21770527e-07
Iter: 1774 loss: 8.21712661e-07
Iter: 1775 loss: 8.21834249e-07
Iter: 1776 loss: 8.21676622e-07
Iter: 1777 loss: 8.21598178e-07
Iter: 1778 loss: 8.21948106e-07
Iter: 1779 loss: 8.21595393e-07
Iter: 1780 loss: 8.2154736e-07
Iter: 1781 loss: 8.2143481e-07
Iter: 1782 loss: 8.21457832e-07
Iter: 1783 loss: 8.21301455e-07
Iter: 1784 loss: 8.21947651e-07
Iter: 1785 loss: 8.21305093e-07
Iter: 1786 loss: 8.21176059e-07
Iter: 1787 loss: 8.22302923e-07
Iter: 1788 loss: 8.21199308e-07
Iter: 1789 loss: 8.21113417e-07
Iter: 1790 loss: 8.21027754e-07
Iter: 1791 loss: 8.21011213e-07
Iter: 1792 loss: 8.20883884e-07
Iter: 1793 loss: 8.21589367e-07
Iter: 1794 loss: 8.20855234e-07
Iter: 1795 loss: 8.20791854e-07
Iter: 1796 loss: 8.2213478e-07
Iter: 1797 loss: 8.20793332e-07
Iter: 1798 loss: 8.20719094e-07
Iter: 1799 loss: 8.20673847e-07
Iter: 1800 loss: 8.20678736e-07
Iter: 1801 loss: 8.20594323e-07
Iter: 1802 loss: 8.21080107e-07
Iter: 1803 loss: 8.20595289e-07
Iter: 1804 loss: 8.20531454e-07
Iter: 1805 loss: 8.2066839e-07
Iter: 1806 loss: 8.20495188e-07
Iter: 1807 loss: 8.20446189e-07
Iter: 1808 loss: 8.20485354e-07
Iter: 1809 loss: 8.204251e-07
Iter: 1810 loss: 8.20374908e-07
Iter: 1811 loss: 8.21036622e-07
Iter: 1812 loss: 8.20383434e-07
Iter: 1813 loss: 8.2036081e-07
Iter: 1814 loss: 8.20263665e-07
Iter: 1815 loss: 8.21335448e-07
Iter: 1816 loss: 8.20257242e-07
Iter: 1817 loss: 8.20206651e-07
Iter: 1818 loss: 8.20815785e-07
Iter: 1819 loss: 8.20181924e-07
Iter: 1820 loss: 8.20131902e-07
Iter: 1821 loss: 8.20781793e-07
Iter: 1822 loss: 8.20129515e-07
Iter: 1823 loss: 8.20083187e-07
Iter: 1824 loss: 8.19962565e-07
Iter: 1825 loss: 8.2148938e-07
Iter: 1826 loss: 8.19969841e-07
Iter: 1827 loss: 8.19838533e-07
Iter: 1828 loss: 8.20534638e-07
Iter: 1829 loss: 8.1983444e-07
Iter: 1830 loss: 8.19756906e-07
Iter: 1831 loss: 8.19749289e-07
Iter: 1832 loss: 8.1970569e-07
Iter: 1833 loss: 8.19624745e-07
Iter: 1834 loss: 8.19633215e-07
Iter: 1835 loss: 8.19549655e-07
Iter: 1836 loss: 8.19913907e-07
Iter: 1837 loss: 8.19531124e-07
Iter: 1838 loss: 8.19467402e-07
Iter: 1839 loss: 8.19996e-07
Iter: 1840 loss: 8.19470188e-07
Iter: 1841 loss: 8.19383729e-07
Iter: 1842 loss: 8.19257878e-07
Iter: 1843 loss: 8.21638082e-07
Iter: 1844 loss: 8.19269701e-07
Iter: 1845 loss: 8.19180855e-07
Iter: 1846 loss: 8.19187107e-07
Iter: 1847 loss: 8.1910855e-07
Iter: 1848 loss: 8.19031754e-07
Iter: 1849 loss: 8.19000491e-07
Iter: 1850 loss: 8.18912213e-07
Iter: 1851 loss: 8.19063757e-07
Iter: 1852 loss: 8.18864805e-07
Iter: 1853 loss: 8.18789e-07
Iter: 1854 loss: 8.18785907e-07
Iter: 1855 loss: 8.18705189e-07
Iter: 1856 loss: 8.18695753e-07
Iter: 1857 loss: 8.18675062e-07
Iter: 1858 loss: 8.18618105e-07
Iter: 1859 loss: 8.18651529e-07
Iter: 1860 loss: 8.18554156e-07
Iter: 1861 loss: 8.18541821e-07
Iter: 1862 loss: 8.18518686e-07
Iter: 1863 loss: 8.1848458e-07
Iter: 1864 loss: 8.18462922e-07
Iter: 1865 loss: 8.18466447e-07
Iter: 1866 loss: 8.18395279e-07
Iter: 1867 loss: 8.18408466e-07
Iter: 1868 loss: 8.18350145e-07
Iter: 1869 loss: 8.18299384e-07
Iter: 1870 loss: 8.19065122e-07
Iter: 1871 loss: 8.18294836e-07
Iter: 1872 loss: 8.18238789e-07
Iter: 1873 loss: 8.18117257e-07
Iter: 1874 loss: 8.20170271e-07
Iter: 1875 loss: 8.18111062e-07
Iter: 1876 loss: 8.18042963e-07
Iter: 1877 loss: 8.18039439e-07
Iter: 1878 loss: 8.17950195e-07
Iter: 1879 loss: 8.17951218e-07
Iter: 1880 loss: 8.17883233e-07
Iter: 1881 loss: 8.17777959e-07
Iter: 1882 loss: 8.1775795e-07
Iter: 1883 loss: 8.1769673e-07
Iter: 1884 loss: 8.17633861e-07
Iter: 1885 loss: 8.17634032e-07
Iter: 1886 loss: 8.17553428e-07
Iter: 1887 loss: 8.17701789e-07
Iter: 1888 loss: 8.17508919e-07
Iter: 1889 loss: 8.17469413e-07
Iter: 1890 loss: 8.17409898e-07
Iter: 1891 loss: 8.1740211e-07
Iter: 1892 loss: 8.17334126e-07
Iter: 1893 loss: 8.18136698e-07
Iter: 1894 loss: 8.17327077e-07
Iter: 1895 loss: 8.17261082e-07
Iter: 1896 loss: 8.17731689e-07
Iter: 1897 loss: 8.17230443e-07
Iter: 1898 loss: 8.17166665e-07
Iter: 1899 loss: 8.17177295e-07
Iter: 1900 loss: 8.17151317e-07
Iter: 1901 loss: 8.17090381e-07
Iter: 1902 loss: 8.17673083e-07
Iter: 1903 loss: 8.17095042e-07
Iter: 1904 loss: 8.17030809e-07
Iter: 1905 loss: 8.17030468e-07
Iter: 1906 loss: 8.1699767e-07
Iter: 1907 loss: 8.16937188e-07
Iter: 1908 loss: 8.17052523e-07
Iter: 1909 loss: 8.16915929e-07
Iter: 1910 loss: 8.16855504e-07
Iter: 1911 loss: 8.17310934e-07
Iter: 1912 loss: 8.16851184e-07
Iter: 1913 loss: 8.16811e-07
Iter: 1914 loss: 8.16758075e-07
Iter: 1915 loss: 8.17872262e-07
Iter: 1916 loss: 8.16735792e-07
Iter: 1917 loss: 8.16634042e-07
Iter: 1918 loss: 8.17096804e-07
Iter: 1919 loss: 8.16602039e-07
Iter: 1920 loss: 8.16514842e-07
Iter: 1921 loss: 8.16518082e-07
Iter: 1922 loss: 8.16462205e-07
Iter: 1923 loss: 8.16348063e-07
Iter: 1924 loss: 8.18175238e-07
Iter: 1925 loss: 8.1632453e-07
Iter: 1926 loss: 8.16212832e-07
Iter: 1927 loss: 8.17236582e-07
Iter: 1928 loss: 8.16194813e-07
Iter: 1929 loss: 8.16137515e-07
Iter: 1930 loss: 8.16125237e-07
Iter: 1931 loss: 8.1607709e-07
Iter: 1932 loss: 8.15999329e-07
Iter: 1933 loss: 8.15990916e-07
Iter: 1934 loss: 8.15903491e-07
Iter: 1935 loss: 8.16485681e-07
Iter: 1936 loss: 8.15913495e-07
Iter: 1937 loss: 8.15832209e-07
Iter: 1938 loss: 8.16023089e-07
Iter: 1939 loss: 8.15821295e-07
Iter: 1940 loss: 8.15741828e-07
Iter: 1941 loss: 8.15687258e-07
Iter: 1942 loss: 8.15670319e-07
Iter: 1943 loss: 8.15578517e-07
Iter: 1944 loss: 8.15568512e-07
Iter: 1945 loss: 8.15509225e-07
Iter: 1946 loss: 8.1540793e-07
Iter: 1947 loss: 8.15412363e-07
Iter: 1948 loss: 8.15263149e-07
Iter: 1949 loss: 8.15850399e-07
Iter: 1950 loss: 8.15247859e-07
Iter: 1951 loss: 8.15187491e-07
Iter: 1952 loss: 8.16195438e-07
Iter: 1953 loss: 8.15177373e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6
+ date
Mon Oct 26 09:54:31 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.6_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.6_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.6_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.6_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b644a3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b64563730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b8bf806a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b6450b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b8bff0bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b644f76a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b6445af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b644111e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b64411268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b64411620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b643ec2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b643ab9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b643c38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b64378048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b6437cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b64407f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b64403e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b64329620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b64403f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b6427e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b6425d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b6421b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b64329378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b641e8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b641b5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b641bbb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b641bb730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b6416d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b6416d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b64111950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b640f0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b640a9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b64091598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b6410ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b640850d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4b64012f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.0390579e-06
Iter: 2 loss: 3.44598584e-06
Iter: 3 loss: 1.96033443e-06
Iter: 4 loss: 1.63380514e-06
Iter: 5 loss: 1.96708515e-06
Iter: 6 loss: 1.4513505e-06
Iter: 7 loss: 1.29685418e-06
Iter: 8 loss: 2.98843065e-06
Iter: 9 loss: 1.29354714e-06
Iter: 10 loss: 1.20165328e-06
Iter: 11 loss: 1.16659589e-06
Iter: 12 loss: 1.11645932e-06
Iter: 13 loss: 1.06301934e-06
Iter: 14 loss: 1.55673968e-06
Iter: 15 loss: 1.0606791e-06
Iter: 16 loss: 1.03256048e-06
Iter: 17 loss: 1.25233771e-06
Iter: 18 loss: 1.03057141e-06
Iter: 19 loss: 1.01081298e-06
Iter: 20 loss: 1.03690809e-06
Iter: 21 loss: 1.00082593e-06
Iter: 22 loss: 9.8877e-07
Iter: 23 loss: 1.07552114e-06
Iter: 24 loss: 9.87760586e-07
Iter: 25 loss: 9.76352567e-07
Iter: 26 loss: 1.01639478e-06
Iter: 27 loss: 9.73363e-07
Iter: 28 loss: 9.66978405e-07
Iter: 29 loss: 9.57693373e-07
Iter: 30 loss: 9.57414613e-07
Iter: 31 loss: 9.44748251e-07
Iter: 32 loss: 9.71066811e-07
Iter: 33 loss: 9.39740858e-07
Iter: 34 loss: 9.30093279e-07
Iter: 35 loss: 9.95048254e-07
Iter: 36 loss: 9.29111764e-07
Iter: 37 loss: 9.21504181e-07
Iter: 38 loss: 9.34830439e-07
Iter: 39 loss: 9.18140131e-07
Iter: 40 loss: 9.21889864e-07
Iter: 41 loss: 9.15469968e-07
Iter: 42 loss: 9.13393478e-07
Iter: 43 loss: 9.08591176e-07
Iter: 44 loss: 9.67505116e-07
Iter: 45 loss: 9.08195261e-07
Iter: 46 loss: 9.04675517e-07
Iter: 47 loss: 9.04650506e-07
Iter: 48 loss: 9.01353815e-07
Iter: 49 loss: 8.97184293e-07
Iter: 50 loss: 8.96889e-07
Iter: 51 loss: 8.91860282e-07
Iter: 52 loss: 8.94067512e-07
Iter: 53 loss: 8.88455588e-07
Iter: 54 loss: 8.8619754e-07
Iter: 55 loss: 8.85483587e-07
Iter: 56 loss: 8.82833319e-07
Iter: 57 loss: 8.76380909e-07
Iter: 58 loss: 9.43942723e-07
Iter: 59 loss: 8.75632736e-07
Iter: 60 loss: 8.73806357e-07
Iter: 61 loss: 8.72115777e-07
Iter: 62 loss: 8.69963173e-07
Iter: 63 loss: 8.69737164e-07
Iter: 64 loss: 8.68167376e-07
Iter: 65 loss: 8.64839e-07
Iter: 66 loss: 8.635173e-07
Iter: 67 loss: 8.61697742e-07
Iter: 68 loss: 8.58946692e-07
Iter: 69 loss: 8.74729608e-07
Iter: 70 loss: 8.58566125e-07
Iter: 71 loss: 8.56082238e-07
Iter: 72 loss: 8.58348187e-07
Iter: 73 loss: 8.5464103e-07
Iter: 74 loss: 8.5482e-07
Iter: 75 loss: 8.53758706e-07
Iter: 76 loss: 8.52731887e-07
Iter: 77 loss: 8.50559331e-07
Iter: 78 loss: 8.85059535e-07
Iter: 79 loss: 8.50468439e-07
Iter: 80 loss: 8.4935084e-07
Iter: 81 loss: 8.49356752e-07
Iter: 82 loss: 8.48103e-07
Iter: 83 loss: 8.46799367e-07
Iter: 84 loss: 8.46575574e-07
Iter: 85 loss: 8.44766362e-07
Iter: 86 loss: 8.44651e-07
Iter: 87 loss: 8.4329929e-07
Iter: 88 loss: 8.4115868e-07
Iter: 89 loss: 8.57061877e-07
Iter: 90 loss: 8.40982352e-07
Iter: 91 loss: 8.38990843e-07
Iter: 92 loss: 8.51470361e-07
Iter: 93 loss: 8.38785581e-07
Iter: 94 loss: 8.37549635e-07
Iter: 95 loss: 8.37536788e-07
Iter: 96 loss: 8.36548566e-07
Iter: 97 loss: 8.35028459e-07
Iter: 98 loss: 8.52737401e-07
Iter: 99 loss: 8.34985315e-07
Iter: 100 loss: 8.3383452e-07
Iter: 101 loss: 8.31787816e-07
Iter: 102 loss: 8.31769967e-07
Iter: 103 loss: 8.29590419e-07
Iter: 104 loss: 8.40569328e-07
Iter: 105 loss: 8.29221221e-07
Iter: 106 loss: 8.27305428e-07
Iter: 107 loss: 8.31367629e-07
Iter: 108 loss: 8.2657948e-07
Iter: 109 loss: 8.26233475e-07
Iter: 110 loss: 8.2578083e-07
Iter: 111 loss: 8.250272e-07
Iter: 112 loss: 8.23599748e-07
Iter: 113 loss: 8.5445e-07
Iter: 114 loss: 8.23573e-07
Iter: 115 loss: 8.22460038e-07
Iter: 116 loss: 8.3019097e-07
Iter: 117 loss: 8.22354536e-07
Iter: 118 loss: 8.21406161e-07
Iter: 119 loss: 8.27839074e-07
Iter: 120 loss: 8.21299182e-07
Iter: 121 loss: 8.20836249e-07
Iter: 122 loss: 8.19736329e-07
Iter: 123 loss: 8.31907585e-07
Iter: 124 loss: 8.19628042e-07
Iter: 125 loss: 8.18999865e-07
Iter: 126 loss: 8.1895223e-07
Iter: 127 loss: 8.18238391e-07
Iter: 128 loss: 8.18652211e-07
Iter: 129 loss: 8.17777902e-07
Iter: 130 loss: 8.17052069e-07
Iter: 131 loss: 8.18417107e-07
Iter: 132 loss: 8.1671908e-07
Iter: 133 loss: 8.1601371e-07
Iter: 134 loss: 8.2372992e-07
Iter: 135 loss: 8.15976136e-07
Iter: 136 loss: 8.15351882e-07
Iter: 137 loss: 8.14260886e-07
Iter: 138 loss: 8.40465759e-07
Iter: 139 loss: 8.14234909e-07
Iter: 140 loss: 8.13248107e-07
Iter: 141 loss: 8.13907661e-07
Iter: 142 loss: 8.12635903e-07
Iter: 143 loss: 8.11637278e-07
Iter: 144 loss: 8.11617952e-07
Iter: 145 loss: 8.10951349e-07
Iter: 146 loss: 8.19498155e-07
Iter: 147 loss: 8.10965275e-07
Iter: 148 loss: 8.10608185e-07
Iter: 149 loss: 8.09825167e-07
Iter: 150 loss: 8.20927767e-07
Iter: 151 loss: 8.09778e-07
Iter: 152 loss: 8.09209268e-07
Iter: 153 loss: 8.09199832e-07
Iter: 154 loss: 8.08616846e-07
Iter: 155 loss: 8.08309096e-07
Iter: 156 loss: 8.08061543e-07
Iter: 157 loss: 8.07444167e-07
Iter: 158 loss: 8.06785124e-07
Iter: 159 loss: 8.06675871e-07
Iter: 160 loss: 8.05806962e-07
Iter: 161 loss: 8.17741e-07
Iter: 162 loss: 8.05808781e-07
Iter: 163 loss: 8.05144396e-07
Iter: 164 loss: 8.10178449e-07
Iter: 165 loss: 8.05112222e-07
Iter: 166 loss: 8.04709543e-07
Iter: 167 loss: 8.04356034e-07
Iter: 168 loss: 8.04242461e-07
Iter: 169 loss: 8.03940452e-07
Iter: 170 loss: 8.03892362e-07
Iter: 171 loss: 8.03672947e-07
Iter: 172 loss: 8.03026069e-07
Iter: 173 loss: 8.06092203e-07
Iter: 174 loss: 8.02814384e-07
Iter: 175 loss: 8.02300065e-07
Iter: 176 loss: 8.09768949e-07
Iter: 177 loss: 8.02297848e-07
Iter: 178 loss: 8.01910574e-07
Iter: 179 loss: 8.04598358e-07
Iter: 180 loss: 8.01864928e-07
Iter: 181 loss: 8.01428314e-07
Iter: 182 loss: 8.01405804e-07
Iter: 183 loss: 8.01059741e-07
Iter: 184 loss: 8.00698e-07
Iter: 185 loss: 8.01605779e-07
Iter: 186 loss: 8.00564692e-07
Iter: 187 loss: 8.00193561e-07
Iter: 188 loss: 8.0279807e-07
Iter: 189 loss: 8.00162582e-07
Iter: 190 loss: 7.99819531e-07
Iter: 191 loss: 7.99397128e-07
Iter: 192 loss: 7.99380359e-07
Iter: 193 loss: 7.98907195e-07
Iter: 194 loss: 7.98978e-07
Iter: 195 loss: 7.98547489e-07
Iter: 196 loss: 7.98208248e-07
Iter: 197 loss: 7.98209555e-07
Iter: 198 loss: 7.97773282e-07
Iter: 199 loss: 7.97658402e-07
Iter: 200 loss: 7.97422558e-07
Iter: 201 loss: 7.96997199e-07
Iter: 202 loss: 7.99165207e-07
Iter: 203 loss: 7.96929839e-07
Iter: 204 loss: 7.96466168e-07
Iter: 205 loss: 7.9770291e-07
Iter: 206 loss: 7.96324457e-07
Iter: 207 loss: 7.95911205e-07
Iter: 208 loss: 7.95118865e-07
Iter: 209 loss: 8.09959886e-07
Iter: 210 loss: 7.95083508e-07
Iter: 211 loss: 7.94475227e-07
Iter: 212 loss: 7.94482844e-07
Iter: 213 loss: 7.94137236e-07
Iter: 214 loss: 7.94120353e-07
Iter: 215 loss: 7.93932145e-07
Iter: 216 loss: 7.93440051e-07
Iter: 217 loss: 7.97878897e-07
Iter: 218 loss: 7.93335744e-07
Iter: 219 loss: 7.93018501e-07
Iter: 220 loss: 7.93005711e-07
Iter: 221 loss: 7.92720925e-07
Iter: 222 loss: 7.9305147e-07
Iter: 223 loss: 7.92562332e-07
Iter: 224 loss: 7.92192736e-07
Iter: 225 loss: 7.9184224e-07
Iter: 226 loss: 7.91764478e-07
Iter: 227 loss: 7.91271873e-07
Iter: 228 loss: 7.92003789e-07
Iter: 229 loss: 7.91003231e-07
Iter: 230 loss: 7.90711283e-07
Iter: 231 loss: 7.90655e-07
Iter: 232 loss: 7.9036937e-07
Iter: 233 loss: 7.90478452e-07
Iter: 234 loss: 7.90179797e-07
Iter: 235 loss: 7.89799401e-07
Iter: 236 loss: 7.89900412e-07
Iter: 237 loss: 7.89561625e-07
Iter: 238 loss: 7.8887382e-07
Iter: 239 loss: 7.91578032e-07
Iter: 240 loss: 7.88711702e-07
Iter: 241 loss: 7.88405089e-07
Iter: 242 loss: 7.87749059e-07
Iter: 243 loss: 8.01063265e-07
Iter: 244 loss: 7.8773337e-07
Iter: 245 loss: 7.87840804e-07
Iter: 246 loss: 7.87484282e-07
Iter: 247 loss: 7.87228373e-07
Iter: 248 loss: 7.87433351e-07
Iter: 249 loss: 7.87066e-07
Iter: 250 loss: 7.86848659e-07
Iter: 251 loss: 7.86337296e-07
Iter: 252 loss: 7.9292812e-07
Iter: 253 loss: 7.8628409e-07
Iter: 254 loss: 7.86327632e-07
Iter: 255 loss: 7.86060582e-07
Iter: 256 loss: 7.85844e-07
Iter: 257 loss: 7.85441614e-07
Iter: 258 loss: 7.92207743e-07
Iter: 259 loss: 7.85431666e-07
Iter: 260 loss: 7.84973963e-07
Iter: 261 loss: 7.86882879e-07
Iter: 262 loss: 7.84844588e-07
Iter: 263 loss: 7.84428437e-07
Iter: 264 loss: 7.84326e-07
Iter: 265 loss: 7.84068675e-07
Iter: 266 loss: 7.83594146e-07
Iter: 267 loss: 7.83592441e-07
Iter: 268 loss: 7.83143832e-07
Iter: 269 loss: 7.83635187e-07
Iter: 270 loss: 7.82922825e-07
Iter: 271 loss: 7.82586426e-07
Iter: 272 loss: 7.83846872e-07
Iter: 273 loss: 7.82501729e-07
Iter: 274 loss: 7.82092457e-07
Iter: 275 loss: 7.82754171e-07
Iter: 276 loss: 7.81901747e-07
Iter: 277 loss: 7.81530332e-07
Iter: 278 loss: 7.81532378e-07
Iter: 279 loss: 7.81247536e-07
Iter: 280 loss: 7.80818709e-07
Iter: 281 loss: 7.81849963e-07
Iter: 282 loss: 7.80647724e-07
Iter: 283 loss: 7.80275286e-07
Iter: 284 loss: 7.80272842e-07
Iter: 285 loss: 7.80128232e-07
Iter: 286 loss: 7.79653362e-07
Iter: 287 loss: 7.80407731e-07
Iter: 288 loss: 7.79317929e-07
Iter: 289 loss: 7.79209643e-07
Iter: 290 loss: 7.79030074e-07
Iter: 291 loss: 7.78732e-07
Iter: 292 loss: 7.79032e-07
Iter: 293 loss: 7.78570723e-07
Iter: 294 loss: 7.78300489e-07
Iter: 295 loss: 7.77853188e-07
Iter: 296 loss: 7.77824084e-07
Iter: 297 loss: 7.77351374e-07
Iter: 298 loss: 7.78771152e-07
Iter: 299 loss: 7.7717084e-07
Iter: 300 loss: 7.76765376e-07
Iter: 301 loss: 7.76757815e-07
Iter: 302 loss: 7.76433637e-07
Iter: 303 loss: 7.77398327e-07
Iter: 304 loss: 7.76345701e-07
Iter: 305 loss: 7.76101786e-07
Iter: 306 loss: 7.76119805e-07
Iter: 307 loss: 7.75901583e-07
Iter: 308 loss: 7.75557055e-07
Iter: 309 loss: 7.78596814e-07
Iter: 310 loss: 7.75543e-07
Iter: 311 loss: 7.75301714e-07
Iter: 312 loss: 7.74902e-07
Iter: 313 loss: 7.74907164e-07
Iter: 314 loss: 7.74429054e-07
Iter: 315 loss: 7.7736388e-07
Iter: 316 loss: 7.74403475e-07
Iter: 317 loss: 7.74253522e-07
Iter: 318 loss: 7.74199066e-07
Iter: 319 loss: 7.74070941e-07
Iter: 320 loss: 7.73649901e-07
Iter: 321 loss: 7.74272621e-07
Iter: 322 loss: 7.73353634e-07
Iter: 323 loss: 7.73054069e-07
Iter: 324 loss: 7.73038437e-07
Iter: 325 loss: 7.72786734e-07
Iter: 326 loss: 7.75139142e-07
Iter: 327 loss: 7.72787814e-07
Iter: 328 loss: 7.72629676e-07
Iter: 329 loss: 7.72424528e-07
Iter: 330 loss: 7.72411852e-07
Iter: 331 loss: 7.72196813e-07
Iter: 332 loss: 7.72514284e-07
Iter: 333 loss: 7.72095802e-07
Iter: 334 loss: 7.71874454e-07
Iter: 335 loss: 7.74336115e-07
Iter: 336 loss: 7.71852683e-07
Iter: 337 loss: 7.71601435e-07
Iter: 338 loss: 7.71511679e-07
Iter: 339 loss: 7.71384407e-07
Iter: 340 loss: 7.71108034e-07
Iter: 341 loss: 7.72407816e-07
Iter: 342 loss: 7.7106165e-07
Iter: 343 loss: 7.70721613e-07
Iter: 344 loss: 7.71173063e-07
Iter: 345 loss: 7.70552674e-07
Iter: 346 loss: 7.70220765e-07
Iter: 347 loss: 7.70916586e-07
Iter: 348 loss: 7.7010634e-07
Iter: 349 loss: 7.69775e-07
Iter: 350 loss: 7.70178531e-07
Iter: 351 loss: 7.69632607e-07
Iter: 352 loss: 7.69311669e-07
Iter: 353 loss: 7.69314966e-07
Iter: 354 loss: 7.69226347e-07
Iter: 355 loss: 7.68978907e-07
Iter: 356 loss: 7.7073463e-07
Iter: 357 loss: 7.68937809e-07
Iter: 358 loss: 7.68679342e-07
Iter: 359 loss: 7.70012e-07
Iter: 360 loss: 7.68650239e-07
Iter: 361 loss: 7.68584471e-07
Iter: 362 loss: 7.68562302e-07
Iter: 363 loss: 7.68477e-07
Iter: 364 loss: 7.68287464e-07
Iter: 365 loss: 7.69309509e-07
Iter: 366 loss: 7.68202653e-07
Iter: 367 loss: 7.67991651e-07
Iter: 368 loss: 7.68487098e-07
Iter: 369 loss: 7.67918323e-07
Iter: 370 loss: 7.67647407e-07
Iter: 371 loss: 7.68566679e-07
Iter: 372 loss: 7.67567258e-07
Iter: 373 loss: 7.67398e-07
Iter: 374 loss: 7.6738047e-07
Iter: 375 loss: 7.67246377e-07
Iter: 376 loss: 7.6693749e-07
Iter: 377 loss: 7.73613181e-07
Iter: 378 loss: 7.66894857e-07
Iter: 379 loss: 7.66705512e-07
Iter: 380 loss: 7.66687833e-07
Iter: 381 loss: 7.66487119e-07
Iter: 382 loss: 7.66204437e-07
Iter: 383 loss: 7.66177436e-07
Iter: 384 loss: 7.6606517e-07
Iter: 385 loss: 7.66012931e-07
Iter: 386 loss: 7.65861444e-07
Iter: 387 loss: 7.65697905e-07
Iter: 388 loss: 7.65650839e-07
Iter: 389 loss: 7.65459561e-07
Iter: 390 loss: 7.65480706e-07
Iter: 391 loss: 7.65304549e-07
Iter: 392 loss: 7.65071775e-07
Iter: 393 loss: 7.65866957e-07
Iter: 394 loss: 7.65018228e-07
Iter: 395 loss: 7.64865e-07
Iter: 396 loss: 7.648581e-07
Iter: 397 loss: 7.64745664e-07
Iter: 398 loss: 7.64669949e-07
Iter: 399 loss: 7.64599804e-07
Iter: 400 loss: 7.64454683e-07
Iter: 401 loss: 7.6459105e-07
Iter: 402 loss: 7.64392439e-07
Iter: 403 loss: 7.64221795e-07
Iter: 404 loss: 7.64231686e-07
Iter: 405 loss: 7.64071615e-07
Iter: 406 loss: 7.63974811e-07
Iter: 407 loss: 7.63981802e-07
Iter: 408 loss: 7.63807634e-07
Iter: 409 loss: 7.63658704e-07
Iter: 410 loss: 7.63619937e-07
Iter: 411 loss: 7.63460491e-07
Iter: 412 loss: 7.64237711e-07
Iter: 413 loss: 7.63406206e-07
Iter: 414 loss: 7.63230446e-07
Iter: 415 loss: 7.64073661e-07
Iter: 416 loss: 7.63212768e-07
Iter: 417 loss: 7.63001196e-07
Iter: 418 loss: 7.63230219e-07
Iter: 419 loss: 7.62952368e-07
Iter: 420 loss: 7.62723971e-07
Iter: 421 loss: 7.64770107e-07
Iter: 422 loss: 7.62711636e-07
Iter: 423 loss: 7.62620573e-07
Iter: 424 loss: 7.62349714e-07
Iter: 425 loss: 7.63364596e-07
Iter: 426 loss: 7.62218804e-07
Iter: 427 loss: 7.62170203e-07
Iter: 428 loss: 7.62079594e-07
Iter: 429 loss: 7.61958916e-07
Iter: 430 loss: 7.6207823e-07
Iter: 431 loss: 7.61910201e-07
Iter: 432 loss: 7.61735e-07
Iter: 433 loss: 7.61900083e-07
Iter: 434 loss: 7.61652132e-07
Iter: 435 loss: 7.61507749e-07
Iter: 436 loss: 7.61554816e-07
Iter: 437 loss: 7.6141373e-07
Iter: 438 loss: 7.6116828e-07
Iter: 439 loss: 7.6154663e-07
Iter: 440 loss: 7.61060278e-07
Iter: 441 loss: 7.61000251e-07
Iter: 442 loss: 7.60959438e-07
Iter: 443 loss: 7.60886905e-07
Iter: 444 loss: 7.60698754e-07
Iter: 445 loss: 7.63208504e-07
Iter: 446 loss: 7.60690511e-07
Iter: 447 loss: 7.60598198e-07
Iter: 448 loss: 7.60581202e-07
Iter: 449 loss: 7.60468083e-07
Iter: 450 loss: 7.60452053e-07
Iter: 451 loss: 7.60396347e-07
Iter: 452 loss: 7.60314322e-07
Iter: 453 loss: 7.60307785e-07
Iter: 454 loss: 7.60254807e-07
Iter: 455 loss: 7.60130433e-07
Iter: 456 loss: 7.60119633e-07
Iter: 457 loss: 7.59995714e-07
Iter: 458 loss: 7.60095872e-07
Iter: 459 loss: 7.59921818e-07
Iter: 460 loss: 7.59778118e-07
Iter: 461 loss: 7.60965804e-07
Iter: 462 loss: 7.59756404e-07
Iter: 463 loss: 7.5966625e-07
Iter: 464 loss: 7.59599857e-07
Iter: 465 loss: 7.59558361e-07
Iter: 466 loss: 7.59368277e-07
Iter: 467 loss: 7.59261809e-07
Iter: 468 loss: 7.5917842e-07
Iter: 469 loss: 7.58874876e-07
Iter: 470 loss: 7.60819489e-07
Iter: 471 loss: 7.58863791e-07
Iter: 472 loss: 7.58681438e-07
Iter: 473 loss: 7.60026126e-07
Iter: 474 loss: 7.58669898e-07
Iter: 475 loss: 7.58529438e-07
Iter: 476 loss: 7.58524607e-07
Iter: 477 loss: 7.58365104e-07
Iter: 478 loss: 7.58177464e-07
Iter: 479 loss: 7.58694739e-07
Iter: 480 loss: 7.58120223e-07
Iter: 481 loss: 7.57940711e-07
Iter: 482 loss: 7.59175e-07
Iter: 483 loss: 7.57920816e-07
Iter: 484 loss: 7.57879e-07
Iter: 485 loss: 7.58849239e-07
Iter: 486 loss: 7.57894782e-07
Iter: 487 loss: 7.57824409e-07
Iter: 488 loss: 7.57674911e-07
Iter: 489 loss: 7.60533908e-07
Iter: 490 loss: 7.57695943e-07
Iter: 491 loss: 7.57549401e-07
Iter: 492 loss: 7.57970554e-07
Iter: 493 loss: 7.57486873e-07
Iter: 494 loss: 7.57405871e-07
Iter: 495 loss: 7.57410362e-07
Iter: 496 loss: 7.57358066e-07
Iter: 497 loss: 7.57186513e-07
Iter: 498 loss: 7.59465195e-07
Iter: 499 loss: 7.57183329e-07
Iter: 500 loss: 7.57019507e-07
Iter: 501 loss: 7.58328e-07
Iter: 502 loss: 7.57021667e-07
Iter: 503 loss: 7.56891495e-07
Iter: 504 loss: 7.56758482e-07
Iter: 505 loss: 7.56740349e-07
Iter: 506 loss: 7.56587497e-07
Iter: 507 loss: 7.56575218e-07
Iter: 508 loss: 7.56473526e-07
Iter: 509 loss: 7.56827831e-07
Iter: 510 loss: 7.56422e-07
Iter: 511 loss: 7.56319423e-07
Iter: 512 loss: 7.56190843e-07
Iter: 513 loss: 7.56174131e-07
Iter: 514 loss: 7.56057659e-07
Iter: 515 loss: 7.56042141e-07
Iter: 516 loss: 7.55961082e-07
Iter: 517 loss: 7.55943404e-07
Iter: 518 loss: 7.55881445e-07
Iter: 519 loss: 7.55660835e-07
Iter: 520 loss: 7.56238705e-07
Iter: 521 loss: 7.55587735e-07
Iter: 522 loss: 7.5549849e-07
Iter: 523 loss: 7.55457052e-07
Iter: 524 loss: 7.55368774e-07
Iter: 525 loss: 7.55250312e-07
Iter: 526 loss: 7.56134284e-07
Iter: 527 loss: 7.55207111e-07
Iter: 528 loss: 7.55036183e-07
Iter: 529 loss: 7.55091037e-07
Iter: 530 loss: 7.54923462e-07
Iter: 531 loss: 7.54772259e-07
Iter: 532 loss: 7.55057897e-07
Iter: 533 loss: 7.54740199e-07
Iter: 534 loss: 7.54574842e-07
Iter: 535 loss: 7.54713824e-07
Iter: 536 loss: 7.54496966e-07
Iter: 537 loss: 7.54375606e-07
Iter: 538 loss: 7.55225642e-07
Iter: 539 loss: 7.54361395e-07
Iter: 540 loss: 7.54247367e-07
Iter: 541 loss: 7.54610369e-07
Iter: 542 loss: 7.5423e-07
Iter: 543 loss: 7.54099347e-07
Iter: 544 loss: 7.5413908e-07
Iter: 545 loss: 7.54041366e-07
Iter: 546 loss: 7.53923587e-07
Iter: 547 loss: 7.54371968e-07
Iter: 548 loss: 7.53883796e-07
Iter: 549 loss: 7.53706558e-07
Iter: 550 loss: 7.54307734e-07
Iter: 551 loss: 7.53699055e-07
Iter: 552 loss: 7.53611346e-07
Iter: 553 loss: 7.53618224e-07
Iter: 554 loss: 7.53541769e-07
Iter: 555 loss: 7.53398353e-07
Iter: 556 loss: 7.54244866e-07
Iter: 557 loss: 7.53359586e-07
Iter: 558 loss: 7.53237487e-07
Iter: 559 loss: 7.53221116e-07
Iter: 560 loss: 7.53117945e-07
Iter: 561 loss: 7.53343045e-07
Iter: 562 loss: 7.53067809e-07
Iter: 563 loss: 7.52974302e-07
Iter: 564 loss: 7.52922404e-07
Iter: 565 loss: 7.52889207e-07
Iter: 566 loss: 7.52742949e-07
Iter: 567 loss: 7.53490895e-07
Iter: 568 loss: 7.52728226e-07
Iter: 569 loss: 7.52610447e-07
Iter: 570 loss: 7.52567871e-07
Iter: 571 loss: 7.52499091e-07
Iter: 572 loss: 7.52378696e-07
Iter: 573 loss: 7.52375399e-07
Iter: 574 loss: 7.52265805e-07
Iter: 575 loss: 7.52461688e-07
Iter: 576 loss: 7.52192136e-07
Iter: 577 loss: 7.52082258e-07
Iter: 578 loss: 7.52009896e-07
Iter: 579 loss: 7.51961181e-07
Iter: 580 loss: 7.51849257e-07
Iter: 581 loss: 7.51853122e-07
Iter: 582 loss: 7.51778e-07
Iter: 583 loss: 7.52009441e-07
Iter: 584 loss: 7.51753e-07
Iter: 585 loss: 7.51641949e-07
Iter: 586 loss: 7.5154162e-07
Iter: 587 loss: 7.51515529e-07
Iter: 588 loss: 7.51403775e-07
Iter: 589 loss: 7.51568336e-07
Iter: 590 loss: 7.51338462e-07
Iter: 591 loss: 7.51254333e-07
Iter: 592 loss: 7.51258426e-07
Iter: 593 loss: 7.51162702e-07
Iter: 594 loss: 7.51010134e-07
Iter: 595 loss: 7.53961501e-07
Iter: 596 loss: 7.50997174e-07
Iter: 597 loss: 7.50872118e-07
Iter: 598 loss: 7.51796051e-07
Iter: 599 loss: 7.50847619e-07
Iter: 600 loss: 7.50711251e-07
Iter: 601 loss: 7.50558229e-07
Iter: 602 loss: 7.50529807e-07
Iter: 603 loss: 7.50330287e-07
Iter: 604 loss: 7.51614e-07
Iter: 605 loss: 7.50308573e-07
Iter: 606 loss: 7.50175e-07
Iter: 607 loss: 7.50175161e-07
Iter: 608 loss: 7.50041067e-07
Iter: 609 loss: 7.50067e-07
Iter: 610 loss: 7.49967967e-07
Iter: 611 loss: 7.49866274e-07
Iter: 612 loss: 7.49888272e-07
Iter: 613 loss: 7.49812159e-07
Iter: 614 loss: 7.49650894e-07
Iter: 615 loss: 7.5123495e-07
Iter: 616 loss: 7.49650951e-07
Iter: 617 loss: 7.49572337e-07
Iter: 618 loss: 7.50115305e-07
Iter: 619 loss: 7.4958291e-07
Iter: 620 loss: 7.49535786e-07
Iter: 621 loss: 7.49379637e-07
Iter: 622 loss: 7.50611e-07
Iter: 623 loss: 7.49366905e-07
Iter: 624 loss: 7.49261915e-07
Iter: 625 loss: 7.50348363e-07
Iter: 626 loss: 7.49231049e-07
Iter: 627 loss: 7.49133903e-07
Iter: 628 loss: 7.49671585e-07
Iter: 629 loss: 7.49093658e-07
Iter: 630 loss: 7.49007427e-07
Iter: 631 loss: 7.488959e-07
Iter: 632 loss: 7.48882542e-07
Iter: 633 loss: 7.48737421e-07
Iter: 634 loss: 7.49059382e-07
Iter: 635 loss: 7.48715479e-07
Iter: 636 loss: 7.48563366e-07
Iter: 637 loss: 7.49479852e-07
Iter: 638 loss: 7.48557341e-07
Iter: 639 loss: 7.48461048e-07
Iter: 640 loss: 7.48425862e-07
Iter: 641 loss: 7.48365323e-07
Iter: 642 loss: 7.48269827e-07
Iter: 643 loss: 7.48250272e-07
Iter: 644 loss: 7.48147045e-07
Iter: 645 loss: 7.48104526e-07
Iter: 646 loss: 7.48032562e-07
Iter: 647 loss: 7.47944512e-07
Iter: 648 loss: 7.47979527e-07
Iter: 649 loss: 7.47855495e-07
Iter: 650 loss: 7.47783e-07
Iter: 651 loss: 7.47737943e-07
Iter: 652 loss: 7.47694969e-07
Iter: 653 loss: 7.47695196e-07
Iter: 654 loss: 7.47645913e-07
Iter: 655 loss: 7.47540184e-07
Iter: 656 loss: 7.47828835e-07
Iter: 657 loss: 7.47526428e-07
Iter: 658 loss: 7.47436957e-07
Iter: 659 loss: 7.47308377e-07
Iter: 660 loss: 7.47313152e-07
Iter: 661 loss: 7.47246645e-07
Iter: 662 loss: 7.47211857e-07
Iter: 663 loss: 7.47146373e-07
Iter: 664 loss: 7.47084073e-07
Iter: 665 loss: 7.47057641e-07
Iter: 666 loss: 7.4699733e-07
Iter: 667 loss: 7.47174511e-07
Iter: 668 loss: 7.46964361e-07
Iter: 669 loss: 7.46868409e-07
Iter: 670 loss: 7.46818273e-07
Iter: 671 loss: 7.46755177e-07
Iter: 672 loss: 7.46642172e-07
Iter: 673 loss: 7.48126922e-07
Iter: 674 loss: 7.46651381e-07
Iter: 675 loss: 7.46585101e-07
Iter: 676 loss: 7.46488809e-07
Iter: 677 loss: 7.46492844e-07
Iter: 678 loss: 7.46413775e-07
Iter: 679 loss: 7.46365174e-07
Iter: 680 loss: 7.46335331e-07
Iter: 681 loss: 7.46223691e-07
Iter: 682 loss: 7.47657396e-07
Iter: 683 loss: 7.46227101e-07
Iter: 684 loss: 7.46186743e-07
Iter: 685 loss: 7.46146839e-07
Iter: 686 loss: 7.46109e-07
Iter: 687 loss: 7.46037642e-07
Iter: 688 loss: 7.48036712e-07
Iter: 689 loss: 7.46052e-07
Iter: 690 loss: 7.45938905e-07
Iter: 691 loss: 7.45951127e-07
Iter: 692 loss: 7.45882e-07
Iter: 693 loss: 7.45776788e-07
Iter: 694 loss: 7.46895694e-07
Iter: 695 loss: 7.45769512e-07
Iter: 696 loss: 7.45707e-07
Iter: 697 loss: 7.45654518e-07
Iter: 698 loss: 7.4565e-07
Iter: 699 loss: 7.45538387e-07
Iter: 700 loss: 7.46775413e-07
Iter: 701 loss: 7.45552143e-07
Iter: 702 loss: 7.45489615e-07
Iter: 703 loss: 7.45451075e-07
Iter: 704 loss: 7.45411512e-07
Iter: 705 loss: 7.45315e-07
Iter: 706 loss: 7.45352281e-07
Iter: 707 loss: 7.45231205e-07
Iter: 708 loss: 7.45158559e-07
Iter: 709 loss: 7.45118e-07
Iter: 710 loss: 7.45062039e-07
Iter: 711 loss: 7.44928172e-07
Iter: 712 loss: 7.46044066e-07
Iter: 713 loss: 7.4492408e-07
Iter: 714 loss: 7.44846943e-07
Iter: 715 loss: 7.45438854e-07
Iter: 716 loss: 7.44829322e-07
Iter: 717 loss: 7.44763497e-07
Iter: 718 loss: 7.44850126e-07
Iter: 719 loss: 7.44739e-07
Iter: 720 loss: 7.44645831e-07
Iter: 721 loss: 7.4549223e-07
Iter: 722 loss: 7.44627755e-07
Iter: 723 loss: 7.44583531e-07
Iter: 724 loss: 7.44483145e-07
Iter: 725 loss: 7.46590501e-07
Iter: 726 loss: 7.44457452e-07
Iter: 727 loss: 7.44392878e-07
Iter: 728 loss: 7.45592843e-07
Iter: 729 loss: 7.44369402e-07
Iter: 730 loss: 7.442884e-07
Iter: 731 loss: 7.44305737e-07
Iter: 732 loss: 7.4423275e-07
Iter: 733 loss: 7.44161923e-07
Iter: 734 loss: 7.44373665e-07
Iter: 735 loss: 7.44115653e-07
Iter: 736 loss: 7.44055455e-07
Iter: 737 loss: 7.44893327e-07
Iter: 738 loss: 7.44038175e-07
Iter: 739 loss: 7.43983605e-07
Iter: 740 loss: 7.43835e-07
Iter: 741 loss: 7.4536797e-07
Iter: 742 loss: 7.43813189e-07
Iter: 743 loss: 7.43672331e-07
Iter: 744 loss: 7.45699e-07
Iter: 745 loss: 7.43674718e-07
Iter: 746 loss: 7.43531928e-07
Iter: 747 loss: 7.43755095e-07
Iter: 748 loss: 7.43452e-07
Iter: 749 loss: 7.43365604e-07
Iter: 750 loss: 7.44086378e-07
Iter: 751 loss: 7.43378905e-07
Iter: 752 loss: 7.43278918e-07
Iter: 753 loss: 7.43279656e-07
Iter: 754 loss: 7.43204396e-07
Iter: 755 loss: 7.43155738e-07
Iter: 756 loss: 7.43158694e-07
Iter: 757 loss: 7.43102305e-07
Iter: 758 loss: 7.43025453e-07
Iter: 759 loss: 7.44998488e-07
Iter: 760 loss: 7.43014766e-07
Iter: 761 loss: 7.42925408e-07
Iter: 762 loss: 7.43072349e-07
Iter: 763 loss: 7.42878683e-07
Iter: 764 loss: 7.42767668e-07
Iter: 765 loss: 7.43870373e-07
Iter: 766 loss: 7.42777956e-07
Iter: 767 loss: 7.42703e-07
Iter: 768 loss: 7.42623513e-07
Iter: 769 loss: 7.42615839e-07
Iter: 770 loss: 7.42539612e-07
Iter: 771 loss: 7.43506689e-07
Iter: 772 loss: 7.42545069e-07
Iter: 773 loss: 7.42472878e-07
Iter: 774 loss: 7.42509258e-07
Iter: 775 loss: 7.42374198e-07
Iter: 776 loss: 7.42312864e-07
Iter: 777 loss: 7.42229759e-07
Iter: 778 loss: 7.42218219e-07
Iter: 779 loss: 7.42149723e-07
Iter: 780 loss: 7.42132158e-07
Iter: 781 loss: 7.42076509e-07
Iter: 782 loss: 7.42267332e-07
Iter: 783 loss: 7.42039106e-07
Iter: 784 loss: 7.41986355e-07
Iter: 785 loss: 7.4202967e-07
Iter: 786 loss: 7.41943154e-07
Iter: 787 loss: 7.41874942e-07
Iter: 788 loss: 7.42796374e-07
Iter: 789 loss: 7.41854535e-07
Iter: 790 loss: 7.41805877e-07
Iter: 791 loss: 7.41864255e-07
Iter: 792 loss: 7.41756764e-07
Iter: 793 loss: 7.41709414e-07
Iter: 794 loss: 7.4157731e-07
Iter: 795 loss: 7.43816827e-07
Iter: 796 loss: 7.41582085e-07
Iter: 797 loss: 7.41515407e-07
Iter: 798 loss: 7.41525923e-07
Iter: 799 loss: 7.41433837e-07
Iter: 800 loss: 7.41403369e-07
Iter: 801 loss: 7.41347264e-07
Iter: 802 loss: 7.41251483e-07
Iter: 803 loss: 7.41357439e-07
Iter: 804 loss: 7.41211466e-07
Iter: 805 loss: 7.41145527e-07
Iter: 806 loss: 7.4112404e-07
Iter: 807 loss: 7.41064e-07
Iter: 808 loss: 7.40957603e-07
Iter: 809 loss: 7.43601902e-07
Iter: 810 loss: 7.40970904e-07
Iter: 811 loss: 7.40839937e-07
Iter: 812 loss: 7.40951577e-07
Iter: 813 loss: 7.40773089e-07
Iter: 814 loss: 7.40670032e-07
Iter: 815 loss: 7.40677706e-07
Iter: 816 loss: 7.40589201e-07
Iter: 817 loss: 7.40592327e-07
Iter: 818 loss: 7.40527241e-07
Iter: 819 loss: 7.40405369e-07
Iter: 820 loss: 7.40943563e-07
Iter: 821 loss: 7.40403607e-07
Iter: 822 loss: 7.40317887e-07
Iter: 823 loss: 7.41075041e-07
Iter: 824 loss: 7.40289693e-07
Iter: 825 loss: 7.40253199e-07
Iter: 826 loss: 7.40198516e-07
Iter: 827 loss: 7.40190956e-07
Iter: 828 loss: 7.40102678e-07
Iter: 829 loss: 7.4027173e-07
Iter: 830 loss: 7.40074483e-07
Iter: 831 loss: 7.40021676e-07
Iter: 832 loss: 7.4002628e-07
Iter: 833 loss: 7.39999564e-07
Iter: 834 loss: 7.39897075e-07
Iter: 835 loss: 7.41139957e-07
Iter: 836 loss: 7.39910149e-07
Iter: 837 loss: 7.39814823e-07
Iter: 838 loss: 7.40250186e-07
Iter: 839 loss: 7.39785719e-07
Iter: 840 loss: 7.39701647e-07
Iter: 841 loss: 7.40320502e-07
Iter: 842 loss: 7.39676125e-07
Iter: 843 loss: 7.39635936e-07
Iter: 844 loss: 7.39533164e-07
Iter: 845 loss: 7.4153877e-07
Iter: 846 loss: 7.39519919e-07
Iter: 847 loss: 7.39412826e-07
Iter: 848 loss: 7.39538223e-07
Iter: 849 loss: 7.39300617e-07
Iter: 850 loss: 7.3922854e-07
Iter: 851 loss: 7.39229677e-07
Iter: 852 loss: 7.39136908e-07
Iter: 853 loss: 7.39157542e-07
Iter: 854 loss: 7.39069037e-07
Iter: 855 loss: 7.39024756e-07
Iter: 856 loss: 7.3979993e-07
Iter: 857 loss: 7.39008271e-07
Iter: 858 loss: 7.38969447e-07
Iter: 859 loss: 7.39010829e-07
Iter: 860 loss: 7.38906465e-07
Iter: 861 loss: 7.38869062e-07
Iter: 862 loss: 7.38930623e-07
Iter: 863 loss: 7.38843539e-07
Iter: 864 loss: 7.3879545e-07
Iter: 865 loss: 7.38844278e-07
Iter: 866 loss: 7.38757535e-07
Iter: 867 loss: 7.38692279e-07
Iter: 868 loss: 7.39092229e-07
Iter: 869 loss: 7.38660049e-07
Iter: 870 loss: 7.38617814e-07
Iter: 871 loss: 7.38591609e-07
Iter: 872 loss: 7.38552444e-07
Iter: 873 loss: 7.38486847e-07
Iter: 874 loss: 7.38635208e-07
Iter: 875 loss: 7.38463712e-07
Iter: 876 loss: 7.38402605e-07
Iter: 877 loss: 7.38400615e-07
Iter: 878 loss: 7.38366e-07
Iter: 879 loss: 7.38265612e-07
Iter: 880 loss: 7.39169195e-07
Iter: 881 loss: 7.38250094e-07
Iter: 882 loss: 7.38165568e-07
Iter: 883 loss: 7.38788685e-07
Iter: 884 loss: 7.38165681e-07
Iter: 885 loss: 7.38110259e-07
Iter: 886 loss: 7.38784138e-07
Iter: 887 loss: 7.38099175e-07
Iter: 888 loss: 7.38048243e-07
Iter: 889 loss: 7.3801e-07
Iter: 890 loss: 7.37980713e-07
Iter: 891 loss: 7.37951723e-07
Iter: 892 loss: 7.3793376e-07
Iter: 893 loss: 7.37896471e-07
Iter: 894 loss: 7.37765163e-07
Iter: 895 loss: 7.39264351e-07
Iter: 896 loss: 7.37751748e-07
Iter: 897 loss: 7.3762e-07
Iter: 898 loss: 7.37951609e-07
Iter: 899 loss: 7.37561891e-07
Iter: 900 loss: 7.3743854e-07
Iter: 901 loss: 7.38579047e-07
Iter: 902 loss: 7.37450705e-07
Iter: 903 loss: 7.37376354e-07
Iter: 904 loss: 7.37715823e-07
Iter: 905 loss: 7.37364189e-07
Iter: 906 loss: 7.37294386e-07
Iter: 907 loss: 7.37211906e-07
Iter: 908 loss: 7.3719184e-07
Iter: 909 loss: 7.37116352e-07
Iter: 910 loss: 7.3744792e-07
Iter: 911 loss: 7.37095661e-07
Iter: 912 loss: 7.37042342e-07
Iter: 913 loss: 7.37034611e-07
Iter: 914 loss: 7.37001585e-07
Iter: 915 loss: 7.36956906e-07
Iter: 916 loss: 7.36930929e-07
Iter: 917 loss: 7.36883635e-07
Iter: 918 loss: 7.36888921e-07
Iter: 919 loss: 7.36819857e-07
Iter: 920 loss: 7.3673084e-07
Iter: 921 loss: 7.36918309e-07
Iter: 922 loss: 7.36689e-07
Iter: 923 loss: 7.36627726e-07
Iter: 924 loss: 7.36630398e-07
Iter: 925 loss: 7.36560935e-07
Iter: 926 loss: 7.3653996e-07
Iter: 927 loss: 7.36523248e-07
Iter: 928 loss: 7.36452648e-07
Iter: 929 loss: 7.3646595e-07
Iter: 930 loss: 7.36445088e-07
Iter: 931 loss: 7.36375341e-07
Iter: 932 loss: 7.37547566e-07
Iter: 933 loss: 7.3637284e-07
Iter: 934 loss: 7.36266315e-07
Iter: 935 loss: 7.36368918e-07
Iter: 936 loss: 7.36224592e-07
Iter: 937 loss: 7.3617241e-07
Iter: 938 loss: 7.36178094e-07
Iter: 939 loss: 7.36120171e-07
Iter: 940 loss: 7.36112497e-07
Iter: 941 loss: 7.36110678e-07
Iter: 942 loss: 7.36041784e-07
Iter: 943 loss: 7.36174343e-07
Iter: 944 loss: 7.36013362e-07
Iter: 945 loss: 7.35963908e-07
Iter: 946 loss: 7.36080665e-07
Iter: 947 loss: 7.35925084e-07
Iter: 948 loss: 7.35847493e-07
Iter: 949 loss: 7.35891263e-07
Iter: 950 loss: 7.35811454e-07
Iter: 951 loss: 7.3572528e-07
Iter: 952 loss: 7.35709762e-07
Iter: 953 loss: 7.35623416e-07
Iter: 954 loss: 7.35556284e-07
Iter: 955 loss: 7.35845788e-07
Iter: 956 loss: 7.35500521e-07
Iter: 957 loss: 7.35428159e-07
Iter: 958 loss: 7.35441745e-07
Iter: 959 loss: 7.3537e-07
Iter: 960 loss: 7.35418666e-07
Iter: 961 loss: 7.3530191e-07
Iter: 962 loss: 7.35261665e-07
Iter: 963 loss: 7.3591e-07
Iter: 964 loss: 7.35262063e-07
Iter: 965 loss: 7.3522591e-07
Iter: 966 loss: 7.35212893e-07
Iter: 967 loss: 7.3519459e-07
Iter: 968 loss: 7.35146841e-07
Iter: 969 loss: 7.35083177e-07
Iter: 970 loss: 7.35069932e-07
Iter: 971 loss: 7.35017409e-07
Iter: 972 loss: 7.35018261e-07
Iter: 973 loss: 7.34955847e-07
Iter: 974 loss: 7.34964374e-07
Iter: 975 loss: 7.34933451e-07
Iter: 976 loss: 7.34868138e-07
Iter: 977 loss: 7.35002345e-07
Iter: 978 loss: 7.34837954e-07
Iter: 979 loss: 7.34753712e-07
Iter: 980 loss: 7.35337494e-07
Iter: 981 loss: 7.3477014e-07
Iter: 982 loss: 7.34715059e-07
Iter: 983 loss: 7.34688797e-07
Iter: 984 loss: 7.34653781e-07
Iter: 985 loss: 7.34604157e-07
Iter: 986 loss: 7.34615753e-07
Iter: 987 loss: 7.34534296e-07
Iter: 988 loss: 7.34452044e-07
Iter: 989 loss: 7.35237165e-07
Iter: 990 loss: 7.34443404e-07
Iter: 991 loss: 7.34390142e-07
Iter: 992 loss: 7.34526907e-07
Iter: 993 loss: 7.34357513e-07
Iter: 994 loss: 7.34269292e-07
Iter: 995 loss: 7.34991488e-07
Iter: 996 loss: 7.34263381e-07
Iter: 997 loss: 7.34207504e-07
Iter: 998 loss: 7.34370246e-07
Iter: 999 loss: 7.34202445e-07
Iter: 1000 loss: 7.34152195e-07
Iter: 1001 loss: 7.34184709e-07
Iter: 1002 loss: 7.34099103e-07
Iter: 1003 loss: 7.34049081e-07
Iter: 1004 loss: 7.33969841e-07
Iter: 1005 loss: 7.33954948e-07
Iter: 1006 loss: 7.33870138e-07
Iter: 1007 loss: 7.34551179e-07
Iter: 1008 loss: 7.3387514e-07
Iter: 1009 loss: 7.33763557e-07
Iter: 1010 loss: 7.33932e-07
Iter: 1011 loss: 7.33716945e-07
Iter: 1012 loss: 7.33664706e-07
Iter: 1013 loss: 7.33736329e-07
Iter: 1014 loss: 7.33627132e-07
Iter: 1015 loss: 7.33543857e-07
Iter: 1016 loss: 7.33980698e-07
Iter: 1017 loss: 7.33540617e-07
Iter: 1018 loss: 7.33462457e-07
Iter: 1019 loss: 7.33366278e-07
Iter: 1020 loss: 7.33345814e-07
Iter: 1021 loss: 7.3324577e-07
Iter: 1022 loss: 7.33276238e-07
Iter: 1023 loss: 7.3319336e-07
Iter: 1024 loss: 7.33055799e-07
Iter: 1025 loss: 7.33764352e-07
Iter: 1026 loss: 7.33047e-07
Iter: 1027 loss: 7.32948365e-07
Iter: 1028 loss: 7.33505487e-07
Iter: 1029 loss: 7.32939213e-07
Iter: 1030 loss: 7.32882768e-07
Iter: 1031 loss: 7.33675961e-07
Iter: 1032 loss: 7.32877197e-07
Iter: 1033 loss: 7.32832632e-07
Iter: 1034 loss: 7.32862588e-07
Iter: 1035 loss: 7.32801e-07
Iter: 1036 loss: 7.32714284e-07
Iter: 1037 loss: 7.33086154e-07
Iter: 1038 loss: 7.32727e-07
Iter: 1039 loss: 7.3267438e-07
Iter: 1040 loss: 7.32584795e-07
Iter: 1041 loss: 7.34619903e-07
Iter: 1042 loss: 7.32588774e-07
Iter: 1043 loss: 7.3251e-07
Iter: 1044 loss: 7.32697345e-07
Iter: 1045 loss: 7.32485773e-07
Iter: 1046 loss: 7.32418755e-07
Iter: 1047 loss: 7.32408182e-07
Iter: 1048 loss: 7.3235276e-07
Iter: 1049 loss: 7.32247827e-07
Iter: 1050 loss: 7.33716092e-07
Iter: 1051 loss: 7.32258627e-07
Iter: 1052 loss: 7.32154263e-07
Iter: 1053 loss: 7.32168132e-07
Iter: 1054 loss: 7.32090484e-07
Iter: 1055 loss: 7.32016531e-07
Iter: 1056 loss: 7.3199584e-07
Iter: 1057 loss: 7.31891703e-07
Iter: 1058 loss: 7.31801e-07
Iter: 1059 loss: 7.31772388e-07
Iter: 1060 loss: 7.31591854e-07
Iter: 1061 loss: 7.32149772e-07
Iter: 1062 loss: 7.31538421e-07
Iter: 1063 loss: 7.31439911e-07
Iter: 1064 loss: 7.31441446e-07
Iter: 1065 loss: 7.31396199e-07
Iter: 1066 loss: 7.31561613e-07
Iter: 1067 loss: 7.31345892e-07
Iter: 1068 loss: 7.31305e-07
Iter: 1069 loss: 7.31643809e-07
Iter: 1070 loss: 7.31295e-07
Iter: 1071 loss: 7.31250339e-07
Iter: 1072 loss: 7.31230443e-07
Iter: 1073 loss: 7.31207479e-07
Iter: 1074 loss: 7.31137845e-07
Iter: 1075 loss: 7.31060254e-07
Iter: 1076 loss: 7.31045361e-07
Iter: 1077 loss: 7.3098181e-07
Iter: 1078 loss: 7.31995e-07
Iter: 1079 loss: 7.30973966e-07
Iter: 1080 loss: 7.30910415e-07
Iter: 1081 loss: 7.31140801e-07
Iter: 1082 loss: 7.30867669e-07
Iter: 1083 loss: 7.30826969e-07
Iter: 1084 loss: 7.30929969e-07
Iter: 1085 loss: 7.30828e-07
Iter: 1086 loss: 7.30745455e-07
Iter: 1087 loss: 7.30879208e-07
Iter: 1088 loss: 7.30727038e-07
Iter: 1089 loss: 7.30651e-07
Iter: 1090 loss: 7.30639385e-07
Iter: 1091 loss: 7.30568672e-07
Iter: 1092 loss: 7.30465842e-07
Iter: 1093 loss: 7.30391207e-07
Iter: 1094 loss: 7.30339821e-07
Iter: 1095 loss: 7.30180091e-07
Iter: 1096 loss: 7.30960096e-07
Iter: 1097 loss: 7.30171905e-07
Iter: 1098 loss: 7.30036788e-07
Iter: 1099 loss: 7.30279965e-07
Iter: 1100 loss: 7.2992782e-07
Iter: 1101 loss: 7.29900819e-07
Iter: 1102 loss: 7.29876206e-07
Iter: 1103 loss: 7.2978969e-07
Iter: 1104 loss: 7.29715566e-07
Iter: 1105 loss: 7.29719773e-07
Iter: 1106 loss: 7.29599e-07
Iter: 1107 loss: 7.30623185e-07
Iter: 1108 loss: 7.29570161e-07
Iter: 1109 loss: 7.29494218e-07
Iter: 1110 loss: 7.29392923e-07
Iter: 1111 loss: 7.29395424e-07
Iter: 1112 loss: 7.2928583e-07
Iter: 1113 loss: 7.30165652e-07
Iter: 1114 loss: 7.29270312e-07
Iter: 1115 loss: 7.2916373e-07
Iter: 1116 loss: 7.2941674e-07
Iter: 1117 loss: 7.29129624e-07
Iter: 1118 loss: 7.29045951e-07
Iter: 1119 loss: 7.29170097e-07
Iter: 1120 loss: 7.29013379e-07
Iter: 1121 loss: 7.28927546e-07
Iter: 1122 loss: 7.29454314e-07
Iter: 1123 loss: 7.28921464e-07
Iter: 1124 loss: 7.2886121e-07
Iter: 1125 loss: 7.28804594e-07
Iter: 1126 loss: 7.28813234e-07
Iter: 1127 loss: 7.28725468e-07
Iter: 1128 loss: 7.28849727e-07
Iter: 1129 loss: 7.28652253e-07
Iter: 1130 loss: 7.28589043e-07
Iter: 1131 loss: 7.28895031e-07
Iter: 1132 loss: 7.28554198e-07
Iter: 1133 loss: 7.28436191e-07
Iter: 1134 loss: 7.28465e-07
Iter: 1135 loss: 7.28367695e-07
Iter: 1136 loss: 7.28298801e-07
Iter: 1137 loss: 7.28287887e-07
Iter: 1138 loss: 7.28211e-07
Iter: 1139 loss: 7.28088594e-07
Iter: 1140 loss: 7.30213287e-07
Iter: 1141 loss: 7.28086036e-07
Iter: 1142 loss: 7.27943075e-07
Iter: 1143 loss: 7.298176e-07
Iter: 1144 loss: 7.27943643e-07
Iter: 1145 loss: 7.27876e-07
Iter: 1146 loss: 7.27716952e-07
Iter: 1147 loss: 7.29562885e-07
Iter: 1148 loss: 7.2771121e-07
Iter: 1149 loss: 7.27499241e-07
Iter: 1150 loss: 7.29309932e-07
Iter: 1151 loss: 7.27502481e-07
Iter: 1152 loss: 7.27404426e-07
Iter: 1153 loss: 7.27397833e-07
Iter: 1154 loss: 7.27346674e-07
Iter: 1155 loss: 7.27225711e-07
Iter: 1156 loss: 7.28815792e-07
Iter: 1157 loss: 7.27215649e-07
Iter: 1158 loss: 7.27167901e-07
Iter: 1159 loss: 7.27139764e-07
Iter: 1160 loss: 7.270894e-07
Iter: 1161 loss: 7.27025963e-07
Iter: 1162 loss: 7.27029942e-07
Iter: 1163 loss: 7.26947405e-07
Iter: 1164 loss: 7.27175689e-07
Iter: 1165 loss: 7.26930352e-07
Iter: 1166 loss: 7.26839289e-07
Iter: 1167 loss: 7.26728672e-07
Iter: 1168 loss: 7.26681719e-07
Iter: 1169 loss: 7.26664211e-07
Iter: 1170 loss: 7.26631129e-07
Iter: 1171 loss: 7.26556721e-07
Iter: 1172 loss: 7.26784549e-07
Iter: 1173 loss: 7.26517101e-07
Iter: 1174 loss: 7.26452413e-07
Iter: 1175 loss: 7.26451162e-07
Iter: 1176 loss: 7.26401254e-07
Iter: 1177 loss: 7.26305814e-07
Iter: 1178 loss: 7.26587871e-07
Iter: 1179 loss: 7.26296207e-07
Iter: 1180 loss: 7.26207645e-07
Iter: 1181 loss: 7.26211738e-07
Iter: 1182 loss: 7.2615876e-07
Iter: 1183 loss: 7.26064968e-07
Iter: 1184 loss: 7.26782901e-07
Iter: 1185 loss: 7.26063092e-07
Iter: 1186 loss: 7.25972e-07
Iter: 1187 loss: 7.26326732e-07
Iter: 1188 loss: 7.25970551e-07
Iter: 1189 loss: 7.25902964e-07
Iter: 1190 loss: 7.25828613e-07
Iter: 1191 loss: 7.2582867e-07
Iter: 1192 loss: 7.25758127e-07
Iter: 1193 loss: 7.25755e-07
Iter: 1194 loss: 7.25686505e-07
Iter: 1195 loss: 7.25551615e-07
Iter: 1196 loss: 7.27643055e-07
Iter: 1197 loss: 7.25530185e-07
Iter: 1198 loss: 7.25385e-07
Iter: 1199 loss: 7.26018925e-07
Iter: 1200 loss: 7.25379095e-07
Iter: 1201 loss: 7.25258303e-07
Iter: 1202 loss: 7.2565166e-07
Iter: 1203 loss: 7.25252448e-07
Iter: 1204 loss: 7.25210612e-07
Iter: 1205 loss: 7.25186e-07
Iter: 1206 loss: 7.25151892e-07
Iter: 1207 loss: 7.25160135e-07
Iter: 1208 loss: 7.25120572e-07
Iter: 1209 loss: 7.25069413e-07
Iter: 1210 loss: 7.25260804e-07
Iter: 1211 loss: 7.2503974e-07
Iter: 1212 loss: 7.25013251e-07
Iter: 1213 loss: 7.25049745e-07
Iter: 1214 loss: 7.24989491e-07
Iter: 1215 loss: 7.24920142e-07
Iter: 1216 loss: 7.24872621e-07
Iter: 1217 loss: 7.24859888e-07
Iter: 1218 loss: 7.24779284e-07
Iter: 1219 loss: 7.24997335e-07
Iter: 1220 loss: 7.24749839e-07
Iter: 1221 loss: 7.24667643e-07
Iter: 1222 loss: 7.25570544e-07
Iter: 1223 loss: 7.2468265e-07
Iter: 1224 loss: 7.24641609e-07
Iter: 1225 loss: 7.24537927e-07
Iter: 1226 loss: 7.26079e-07
Iter: 1227 loss: 7.24533834e-07
Iter: 1228 loss: 7.2445107e-07
Iter: 1229 loss: 7.24437825e-07
Iter: 1230 loss: 7.24387917e-07
Iter: 1231 loss: 7.24407528e-07
Iter: 1232 loss: 7.24342101e-07
Iter: 1233 loss: 7.24285826e-07
Iter: 1234 loss: 7.24177312e-07
Iter: 1235 loss: 7.24162874e-07
Iter: 1236 loss: 7.24023948e-07
Iter: 1237 loss: 7.24514507e-07
Iter: 1238 loss: 7.23993935e-07
Iter: 1239 loss: 7.2395494e-07
Iter: 1240 loss: 7.23924131e-07
Iter: 1241 loss: 7.23858648e-07
Iter: 1242 loss: 7.23778498e-07
Iter: 1243 loss: 7.23774633e-07
Iter: 1244 loss: 7.23700737e-07
Iter: 1245 loss: 7.23978189e-07
Iter: 1246 loss: 7.23654921e-07
Iter: 1247 loss: 7.23559538e-07
Iter: 1248 loss: 7.2357642e-07
Iter: 1249 loss: 7.23492235e-07
Iter: 1250 loss: 7.23360245e-07
Iter: 1251 loss: 7.23753942e-07
Iter: 1252 loss: 7.23367862e-07
Iter: 1253 loss: 7.23270091e-07
Iter: 1254 loss: 7.23317896e-07
Iter: 1255 loss: 7.23215749e-07
Iter: 1256 loss: 7.23167432e-07
Iter: 1257 loss: 7.23532e-07
Iter: 1258 loss: 7.23137589e-07
Iter: 1259 loss: 7.23074209e-07
Iter: 1260 loss: 7.23823746e-07
Iter: 1261 loss: 7.23081257e-07
Iter: 1262 loss: 7.23039193e-07
Iter: 1263 loss: 7.23001961e-07
Iter: 1264 loss: 7.22993263e-07
Iter: 1265 loss: 7.22923915e-07
Iter: 1266 loss: 7.23563403e-07
Iter: 1267 loss: 7.22923915e-07
Iter: 1268 loss: 7.22859568e-07
Iter: 1269 loss: 7.2279903e-07
Iter: 1270 loss: 7.22790105e-07
Iter: 1271 loss: 7.2269853e-07
Iter: 1272 loss: 7.22687162e-07
Iter: 1273 loss: 7.22617528e-07
Iter: 1274 loss: 7.22498498e-07
Iter: 1275 loss: 7.2337275e-07
Iter: 1276 loss: 7.22485254e-07
Iter: 1277 loss: 7.22451148e-07
Iter: 1278 loss: 7.22438415e-07
Iter: 1279 loss: 7.22394418e-07
Iter: 1280 loss: 7.22324501e-07
Iter: 1281 loss: 7.22839673e-07
Iter: 1282 loss: 7.22268567e-07
Iter: 1283 loss: 7.22202913e-07
Iter: 1284 loss: 7.23067046e-07
Iter: 1285 loss: 7.22181312e-07
Iter: 1286 loss: 7.22097184e-07
Iter: 1287 loss: 7.22452796e-07
Iter: 1288 loss: 7.2208627e-07
Iter: 1289 loss: 7.22036475e-07
Iter: 1290 loss: 7.21967751e-07
Iter: 1291 loss: 7.21961442e-07
Iter: 1292 loss: 7.2188891e-07
Iter: 1293 loss: 7.22482071e-07
Iter: 1294 loss: 7.21896527e-07
Iter: 1295 loss: 7.2184389e-07
Iter: 1296 loss: 7.22019934e-07
Iter: 1297 loss: 7.21827519e-07
Iter: 1298 loss: 7.21764422e-07
Iter: 1299 loss: 7.21726678e-07
Iter: 1300 loss: 7.21687513e-07
Iter: 1301 loss: 7.21599577e-07
Iter: 1302 loss: 7.21916194e-07
Iter: 1303 loss: 7.21593779e-07
Iter: 1304 loss: 7.21507718e-07
Iter: 1305 loss: 7.22114862e-07
Iter: 1306 loss: 7.2152e-07
Iter: 1307 loss: 7.21458605e-07
Iter: 1308 loss: 7.21421429e-07
Iter: 1309 loss: 7.21381411e-07
Iter: 1310 loss: 7.21313711e-07
Iter: 1311 loss: 7.21455649e-07
Iter: 1312 loss: 7.21327808e-07
Iter: 1313 loss: 7.21272727e-07
Iter: 1314 loss: 7.2126852e-07
Iter: 1315 loss: 7.21227252e-07
Iter: 1316 loss: 7.21168476e-07
Iter: 1317 loss: 7.21672222e-07
Iter: 1318 loss: 7.21179504e-07
Iter: 1319 loss: 7.21089236e-07
Iter: 1320 loss: 7.21374136e-07
Iter: 1321 loss: 7.2108287e-07
Iter: 1322 loss: 7.21047456e-07
Iter: 1323 loss: 7.210636e-07
Iter: 1324 loss: 7.2099914e-07
Iter: 1325 loss: 7.20878234e-07
Iter: 1326 loss: 7.21844572e-07
Iter: 1327 loss: 7.20884827e-07
Iter: 1328 loss: 7.20774779e-07
Iter: 1329 loss: 7.20771823e-07
Iter: 1330 loss: 7.20691332e-07
Iter: 1331 loss: 7.20878688e-07
Iter: 1332 loss: 7.20672574e-07
Iter: 1333 loss: 7.20582761e-07
Iter: 1334 loss: 7.20581738e-07
Iter: 1335 loss: 7.20517278e-07
Iter: 1336 loss: 7.20475555e-07
Iter: 1337 loss: 7.20703611e-07
Iter: 1338 loss: 7.20422463e-07
Iter: 1339 loss: 7.20360674e-07
Iter: 1340 loss: 7.2050824e-07
Iter: 1341 loss: 7.20317757e-07
Iter: 1342 loss: 7.2024261e-07
Iter: 1343 loss: 7.21199399e-07
Iter: 1344 loss: 7.20245112e-07
Iter: 1345 loss: 7.2021146e-07
Iter: 1346 loss: 7.20202308e-07
Iter: 1347 loss: 7.2016212e-07
Iter: 1348 loss: 7.20146886e-07
Iter: 1349 loss: 7.20139155e-07
Iter: 1350 loss: 7.2011693e-07
Iter: 1351 loss: 7.200764e-07
Iter: 1352 loss: 7.20546041e-07
Iter: 1353 loss: 7.20050252e-07
Iter: 1354 loss: 7.2000239e-07
Iter: 1355 loss: 7.20224193e-07
Iter: 1356 loss: 7.19991363e-07
Iter: 1357 loss: 7.19925197e-07
Iter: 1358 loss: 7.20491471e-07
Iter: 1359 loss: 7.19908826e-07
Iter: 1360 loss: 7.19887339e-07
Iter: 1361 loss: 7.19819582e-07
Iter: 1362 loss: 7.19826062e-07
Iter: 1363 loss: 7.19770355e-07
Iter: 1364 loss: 7.19778313e-07
Iter: 1365 loss: 7.19731474e-07
Iter: 1366 loss: 7.19762681e-07
Iter: 1367 loss: 7.19718798e-07
Iter: 1368 loss: 7.19699699e-07
Iter: 1369 loss: 7.19646437e-07
Iter: 1370 loss: 7.19640298e-07
Iter: 1371 loss: 7.19573791e-07
Iter: 1372 loss: 7.19660193e-07
Iter: 1373 loss: 7.19532636e-07
Iter: 1374 loss: 7.1948557e-07
Iter: 1375 loss: 7.20232265e-07
Iter: 1376 loss: 7.19483751e-07
Iter: 1377 loss: 7.19434524e-07
Iter: 1378 loss: 7.19538718e-07
Iter: 1379 loss: 7.19388197e-07
Iter: 1380 loss: 7.19315892e-07
Iter: 1381 loss: 7.19757395e-07
Iter: 1382 loss: 7.19335731e-07
Iter: 1383 loss: 7.19279171e-07
Iter: 1384 loss: 7.19313675e-07
Iter: 1385 loss: 7.19231252e-07
Iter: 1386 loss: 7.19180548e-07
Iter: 1387 loss: 7.19076468e-07
Iter: 1388 loss: 7.19080617e-07
Iter: 1389 loss: 7.19060154e-07
Iter: 1390 loss: 7.19043101e-07
Iter: 1391 loss: 7.19018146e-07
Iter: 1392 loss: 7.189401e-07
Iter: 1393 loss: 7.18941521e-07
Iter: 1394 loss: 7.18900196e-07
Iter: 1395 loss: 7.19551508e-07
Iter: 1396 loss: 7.18918955e-07
Iter: 1397 loss: 7.18857223e-07
Iter: 1398 loss: 7.19080504e-07
Iter: 1399 loss: 7.18848e-07
Iter: 1400 loss: 7.18837896e-07
Iter: 1401 loss: 7.18786396e-07
Iter: 1402 loss: 7.19557e-07
Iter: 1403 loss: 7.18768092e-07
Iter: 1404 loss: 7.18692036e-07
Iter: 1405 loss: 7.18930551e-07
Iter: 1406 loss: 7.18652416e-07
Iter: 1407 loss: 7.18585682e-07
Iter: 1408 loss: 7.1893362e-07
Iter: 1409 loss: 7.18568174e-07
Iter: 1410 loss: 7.18518663e-07
Iter: 1411 loss: 7.19222726e-07
Iter: 1412 loss: 7.18511274e-07
Iter: 1413 loss: 7.18474723e-07
Iter: 1414 loss: 7.18485e-07
Iter: 1415 loss: 7.18443232e-07
Iter: 1416 loss: 7.18392755e-07
Iter: 1417 loss: 7.18917931e-07
Iter: 1418 loss: 7.18403669e-07
Iter: 1419 loss: 7.18386445e-07
Iter: 1420 loss: 7.18291346e-07
Iter: 1421 loss: 7.19963055e-07
Iter: 1422 loss: 7.18282877e-07
Iter: 1423 loss: 7.18234674e-07
Iter: 1424 loss: 7.18620186e-07
Iter: 1425 loss: 7.18236151e-07
Iter: 1426 loss: 7.1817243e-07
Iter: 1427 loss: 7.18261447e-07
Iter: 1428 loss: 7.18144236e-07
Iter: 1429 loss: 7.18106151e-07
Iter: 1430 loss: 7.18112574e-07
Iter: 1431 loss: 7.18071817e-07
Iter: 1432 loss: 7.18022818e-07
Iter: 1433 loss: 7.18017077e-07
Iter: 1434 loss: 7.17993316e-07
Iter: 1435 loss: 7.1796012e-07
Iter: 1436 loss: 7.17954663e-07
Iter: 1437 loss: 7.17883154e-07
Iter: 1438 loss: 7.17990247e-07
Iter: 1439 loss: 7.17864168e-07
Iter: 1440 loss: 7.17784417e-07
Iter: 1441 loss: 7.1807824e-07
Iter: 1442 loss: 7.17781234e-07
Iter: 1443 loss: 7.17712851e-07
Iter: 1444 loss: 7.18222225e-07
Iter: 1445 loss: 7.17730472e-07
Iter: 1446 loss: 7.17666e-07
Iter: 1447 loss: 7.17717e-07
Iter: 1448 loss: 7.17614455e-07
Iter: 1449 loss: 7.17555622e-07
Iter: 1450 loss: 7.17967794e-07
Iter: 1451 loss: 7.17554e-07
Iter: 1452 loss: 7.17457795e-07
Iter: 1453 loss: 7.17448756e-07
Iter: 1454 loss: 7.17383045e-07
Iter: 1455 loss: 7.17309319e-07
Iter: 1456 loss: 7.17368323e-07
Iter: 1457 loss: 7.17279534e-07
Iter: 1458 loss: 7.17253e-07
Iter: 1459 loss: 7.17239629e-07
Iter: 1460 loss: 7.17208877e-07
Iter: 1461 loss: 7.17163459e-07
Iter: 1462 loss: 7.17154194e-07
Iter: 1463 loss: 7.1712077e-07
Iter: 1464 loss: 7.17246849e-07
Iter: 1465 loss: 7.17107e-07
Iter: 1466 loss: 7.17051876e-07
Iter: 1467 loss: 7.17510545e-07
Iter: 1468 loss: 7.17047158e-07
Iter: 1469 loss: 7.17020555e-07
Iter: 1470 loss: 7.16956606e-07
Iter: 1471 loss: 7.17567559e-07
Iter: 1472 loss: 7.16958709e-07
Iter: 1473 loss: 7.168548e-07
Iter: 1474 loss: 7.17344165e-07
Iter: 1475 loss: 7.16841953e-07
Iter: 1476 loss: 7.16774e-07
Iter: 1477 loss: 7.16790055e-07
Iter: 1478 loss: 7.16726277e-07
Iter: 1479 loss: 7.16671e-07
Iter: 1480 loss: 7.1666193e-07
Iter: 1481 loss: 7.16609179e-07
Iter: 1482 loss: 7.16616e-07
Iter: 1483 loss: 7.16571662e-07
Iter: 1484 loss: 7.1652272e-07
Iter: 1485 loss: 7.16950865e-07
Iter: 1486 loss: 7.16512432e-07
Iter: 1487 loss: 7.16479121e-07
Iter: 1488 loss: 7.16460704e-07
Iter: 1489 loss: 7.16454224e-07
Iter: 1490 loss: 7.16410511e-07
Iter: 1491 loss: 7.16742591e-07
Iter: 1492 loss: 7.16405339e-07
Iter: 1493 loss: 7.16344175e-07
Iter: 1494 loss: 7.16280169e-07
Iter: 1495 loss: 7.17964895e-07
Iter: 1496 loss: 7.16277157e-07
Iter: 1497 loss: 7.1621e-07
Iter: 1498 loss: 7.16638738e-07
Iter: 1499 loss: 7.16205705e-07
Iter: 1500 loss: 7.16153806e-07
Iter: 1501 loss: 7.16552393e-07
Iter: 1502 loss: 7.16138914e-07
Iter: 1503 loss: 7.1608514e-07
Iter: 1504 loss: 7.1607144e-07
Iter: 1505 loss: 7.16041768e-07
Iter: 1506 loss: 7.15962642e-07
Iter: 1507 loss: 7.15957185e-07
Iter: 1508 loss: 7.15916599e-07
Iter: 1509 loss: 7.15838894e-07
Iter: 1510 loss: 7.16252487e-07
Iter: 1511 loss: 7.15807e-07
Iter: 1512 loss: 7.15756528e-07
Iter: 1513 loss: 7.1600283e-07
Iter: 1514 loss: 7.15748627e-07
Iter: 1515 loss: 7.1569491e-07
Iter: 1516 loss: 7.1651408e-07
Iter: 1517 loss: 7.15696956e-07
Iter: 1518 loss: 7.15645683e-07
Iter: 1519 loss: 7.15682063e-07
Iter: 1520 loss: 7.15639e-07
Iter: 1521 loss: 7.15597707e-07
Iter: 1522 loss: 7.15763633e-07
Iter: 1523 loss: 7.15567069e-07
Iter: 1524 loss: 7.15534782e-07
Iter: 1525 loss: 7.15494e-07
Iter: 1526 loss: 7.15492263e-07
Iter: 1527 loss: 7.15410806e-07
Iter: 1528 loss: 7.1594377e-07
Iter: 1529 loss: 7.15396595e-07
Iter: 1530 loss: 7.15316901e-07
Iter: 1531 loss: 7.15319857e-07
Iter: 1532 loss: 7.15264378e-07
Iter: 1533 loss: 7.15150691e-07
Iter: 1534 loss: 7.15339525e-07
Iter: 1535 loss: 7.15128806e-07
Iter: 1536 loss: 7.15077647e-07
Iter: 1537 loss: 7.15087822e-07
Iter: 1538 loss: 7.15003239e-07
Iter: 1539 loss: 7.14942075e-07
Iter: 1540 loss: 7.14973112e-07
Iter: 1541 loss: 7.14880457e-07
Iter: 1542 loss: 7.14908765e-07
Iter: 1543 loss: 7.14831231e-07
Iter: 1544 loss: 7.14729708e-07
Iter: 1545 loss: 7.14719931e-07
Iter: 1546 loss: 7.14673547e-07
Iter: 1547 loss: 7.14588623e-07
Iter: 1548 loss: 7.14570604e-07
Iter: 1549 loss: 7.14548662e-07
Iter: 1550 loss: 7.15200883e-07
Iter: 1551 loss: 7.14549742e-07
Iter: 1552 loss: 7.14519217e-07
Iter: 1553 loss: 7.14497105e-07
Iter: 1554 loss: 7.14489772e-07
Iter: 1555 loss: 7.14407179e-07
Iter: 1556 loss: 7.14627674e-07
Iter: 1557 loss: 7.14375574e-07
Iter: 1558 loss: 7.14351302e-07
Iter: 1559 loss: 7.14271096e-07
Iter: 1560 loss: 7.14275643e-07
Iter: 1561 loss: 7.14204873e-07
Iter: 1562 loss: 7.14222892e-07
Iter: 1563 loss: 7.14178555e-07
Iter: 1564 loss: 7.14146836e-07
Iter: 1565 loss: 7.14149053e-07
Iter: 1566 loss: 7.14099826e-07
Iter: 1567 loss: 7.14235171e-07
Iter: 1568 loss: 7.14061684e-07
Iter: 1569 loss: 7.14049463e-07
Iter: 1570 loss: 7.14033035e-07
Iter: 1571 loss: 7.14004273e-07
Iter: 1572 loss: 7.13948225e-07
Iter: 1573 loss: 7.14673206e-07
Iter: 1574 loss: 7.13978523e-07
Iter: 1575 loss: 7.13910481e-07
Iter: 1576 loss: 7.13974885e-07
Iter: 1577 loss: 7.13880866e-07
Iter: 1578 loss: 7.13773034e-07
Iter: 1579 loss: 7.13753138e-07
Iter: 1580 loss: 7.13702093e-07
Iter: 1581 loss: 7.13625582e-07
Iter: 1582 loss: 7.14855389e-07
Iter: 1583 loss: 7.13612053e-07
Iter: 1584 loss: 7.13556346e-07
Iter: 1585 loss: 7.13540601e-07
Iter: 1586 loss: 7.13513032e-07
Iter: 1587 loss: 7.13494387e-07
Iter: 1588 loss: 7.13479437e-07
Iter: 1589 loss: 7.13439704e-07
Iter: 1590 loss: 7.13698171e-07
Iter: 1591 loss: 7.13424072e-07
Iter: 1592 loss: 7.13374504e-07
Iter: 1593 loss: 7.13305781e-07
Iter: 1594 loss: 7.13294753e-07
Iter: 1595 loss: 7.13239103e-07
Iter: 1596 loss: 7.13246493e-07
Iter: 1597 loss: 7.13223471e-07
Iter: 1598 loss: 7.13133204e-07
Iter: 1599 loss: 7.14125463e-07
Iter: 1600 loss: 7.13156055e-07
Iter: 1601 loss: 7.13087559e-07
Iter: 1602 loss: 7.13670488e-07
Iter: 1603 loss: 7.131041e-07
Iter: 1604 loss: 7.13036343e-07
Iter: 1605 loss: 7.13530881e-07
Iter: 1606 loss: 7.13036343e-07
Iter: 1607 loss: 7.13006216e-07
Iter: 1608 loss: 7.12955909e-07
Iter: 1609 loss: 7.14213797e-07
Iter: 1610 loss: 7.12961366e-07
Iter: 1611 loss: 7.12900544e-07
Iter: 1612 loss: 7.13009797e-07
Iter: 1613 loss: 7.12862231e-07
Iter: 1614 loss: 7.12792257e-07
Iter: 1615 loss: 7.1274269e-07
Iter: 1616 loss: 7.12724272e-07
Iter: 1617 loss: 7.12613144e-07
Iter: 1618 loss: 7.13025202e-07
Iter: 1619 loss: 7.12590918e-07
Iter: 1620 loss: 7.12553799e-07
Iter: 1621 loss: 7.12519295e-07
Iter: 1622 loss: 7.12493261e-07
Iter: 1623 loss: 7.12442215e-07
Iter: 1624 loss: 7.12438123e-07
Iter: 1625 loss: 7.12396059e-07
Iter: 1626 loss: 7.12825909e-07
Iter: 1627 loss: 7.12375254e-07
Iter: 1628 loss: 7.12337055e-07
Iter: 1629 loss: 7.12290159e-07
Iter: 1630 loss: 7.12272367e-07
Iter: 1631 loss: 7.1220353e-07
Iter: 1632 loss: 7.12616611e-07
Iter: 1633 loss: 7.1218318e-07
Iter: 1634 loss: 7.12163569e-07
Iter: 1635 loss: 7.12362692e-07
Iter: 1636 loss: 7.12166639e-07
Iter: 1637 loss: 7.1212321e-07
Iter: 1638 loss: 7.12045335e-07
Iter: 1639 loss: 7.13707209e-07
Iter: 1640 loss: 7.12035671e-07
Iter: 1641 loss: 7.11996279e-07
Iter: 1642 loss: 7.11977918e-07
Iter: 1643 loss: 7.11943812e-07
Iter: 1644 loss: 7.11857297e-07
Iter: 1645 loss: 7.11864232e-07
Iter: 1646 loss: 7.11806592e-07
Iter: 1647 loss: 7.11895609e-07
Iter: 1648 loss: 7.11740086e-07
Iter: 1649 loss: 7.11680798e-07
Iter: 1650 loss: 7.11995426e-07
Iter: 1651 loss: 7.11664541e-07
Iter: 1652 loss: 7.11585244e-07
Iter: 1653 loss: 7.12121107e-07
Iter: 1654 loss: 7.11594851e-07
Iter: 1655 loss: 7.1156245e-07
Iter: 1656 loss: 7.11926589e-07
Iter: 1657 loss: 7.1153795e-07
Iter: 1658 loss: 7.11523398e-07
Iter: 1659 loss: 7.11513223e-07
Iter: 1660 loss: 7.11487246e-07
Iter: 1661 loss: 7.11449445e-07
Iter: 1662 loss: 7.11523626e-07
Iter: 1663 loss: 7.11416646e-07
Iter: 1664 loss: 7.11385837e-07
Iter: 1665 loss: 7.11378618e-07
Iter: 1666 loss: 7.11371854e-07
Iter: 1667 loss: 7.11271127e-07
Iter: 1668 loss: 7.11942278e-07
Iter: 1669 loss: 7.11297503e-07
Iter: 1670 loss: 7.11252426e-07
Iter: 1671 loss: 7.11224516e-07
Iter: 1672 loss: 7.11194218e-07
Iter: 1673 loss: 7.11146072e-07
Iter: 1674 loss: 7.11239068e-07
Iter: 1675 loss: 7.11112136e-07
Iter: 1676 loss: 7.11037615e-07
Iter: 1677 loss: 7.11727239e-07
Iter: 1678 loss: 7.11044095e-07
Iter: 1679 loss: 7.11005839e-07
Iter: 1680 loss: 7.10951667e-07
Iter: 1681 loss: 7.10945471e-07
Iter: 1682 loss: 7.10869131e-07
Iter: 1683 loss: 7.11032214e-07
Iter: 1684 loss: 7.10824736e-07
Iter: 1685 loss: 7.10757206e-07
Iter: 1686 loss: 7.11141638e-07
Iter: 1687 loss: 7.1074e-07
Iter: 1688 loss: 7.10673817e-07
Iter: 1689 loss: 7.11558755e-07
Iter: 1690 loss: 7.10674044e-07
Iter: 1691 loss: 7.10609697e-07
Iter: 1692 loss: 7.10588779e-07
Iter: 1693 loss: 7.10593611e-07
Iter: 1694 loss: 7.10519657e-07
Iter: 1695 loss: 7.11058419e-07
Iter: 1696 loss: 7.105279e-07
Iter: 1697 loss: 7.10470317e-07
Iter: 1698 loss: 7.10389145e-07
Iter: 1699 loss: 7.1039176e-07
Iter: 1700 loss: 7.10310928e-07
Iter: 1701 loss: 7.10431095e-07
Iter: 1702 loss: 7.10261929e-07
Iter: 1703 loss: 7.10211339e-07
Iter: 1704 loss: 7.10222821e-07
Iter: 1705 loss: 7.10165295e-07
Iter: 1706 loss: 7.1016575e-07
Iter: 1707 loss: 7.10118115e-07
Iter: 1708 loss: 7.10057634e-07
Iter: 1709 loss: 7.10053e-07
Iter: 1710 loss: 7.10012046e-07
Iter: 1711 loss: 7.0995344e-07
Iter: 1712 loss: 7.09936e-07
Iter: 1713 loss: 7.09899268e-07
Iter: 1714 loss: 7.09779727e-07
Iter: 1715 loss: 7.11621396e-07
Iter: 1716 loss: 7.09803e-07
Iter: 1717 loss: 7.097089e-07
Iter: 1718 loss: 7.09961341e-07
Iter: 1719 loss: 7.09673316e-07
Iter: 1720 loss: 7.09582537e-07
Iter: 1721 loss: 7.10270456e-07
Iter: 1722 loss: 7.09593337e-07
Iter: 1723 loss: 7.09487779e-07
Iter: 1724 loss: 7.09938661e-07
Iter: 1725 loss: 7.09466178e-07
Iter: 1726 loss: 7.09415474e-07
Iter: 1727 loss: 7.09337201e-07
Iter: 1728 loss: 7.09346295e-07
Iter: 1729 loss: 7.09208223e-07
Iter: 1730 loss: 7.10261702e-07
Iter: 1731 loss: 7.09207825e-07
Iter: 1732 loss: 7.09121764e-07
Iter: 1733 loss: 7.09011829e-07
Iter: 1734 loss: 7.09040705e-07
Iter: 1735 loss: 7.08934579e-07
Iter: 1736 loss: 7.09798314e-07
Iter: 1737 loss: 7.08926791e-07
Iter: 1738 loss: 7.08847665e-07
Iter: 1739 loss: 7.0912e-07
Iter: 1740 loss: 7.08848461e-07
Iter: 1741 loss: 7.08746711e-07
Iter: 1742 loss: 7.08717039e-07
Iter: 1743 loss: 7.08730681e-07
Iter: 1744 loss: 7.08613811e-07
Iter: 1745 loss: 7.09657797e-07
Iter: 1746 loss: 7.08607558e-07
Iter: 1747 loss: 7.08551738e-07
Iter: 1748 loss: 7.08840446e-07
Iter: 1749 loss: 7.08548043e-07
Iter: 1750 loss: 7.08500693e-07
Iter: 1751 loss: 7.0838621e-07
Iter: 1752 loss: 7.10309905e-07
Iter: 1753 loss: 7.08380298e-07
Iter: 1754 loss: 7.08264452e-07
Iter: 1755 loss: 7.08663492e-07
Iter: 1756 loss: 7.08251719e-07
Iter: 1757 loss: 7.082715e-07
Iter: 1758 loss: 7.0820181e-07
Iter: 1759 loss: 7.08188622e-07
Iter: 1760 loss: 7.08097446e-07
Iter: 1761 loss: 7.08943844e-07
Iter: 1762 loss: 7.08095683e-07
Iter: 1763 loss: 7.08015364e-07
Iter: 1764 loss: 7.08553898e-07
Iter: 1765 loss: 7.080111e-07
Iter: 1766 loss: 7.0795636e-07
Iter: 1767 loss: 7.0822216e-07
Iter: 1768 loss: 7.07971481e-07
Iter: 1769 loss: 7.07920719e-07
Iter: 1770 loss: 7.07817094e-07
Iter: 1771 loss: 7.07828349e-07
Iter: 1772 loss: 7.07732056e-07
Iter: 1773 loss: 7.08712605e-07
Iter: 1774 loss: 7.07743311e-07
Iter: 1775 loss: 7.07661343e-07
Iter: 1776 loss: 7.0781482e-07
Iter: 1777 loss: 7.07599213e-07
Iter: 1778 loss: 7.07539527e-07
Iter: 1779 loss: 7.07451704e-07
Iter: 1780 loss: 7.07445849e-07
Iter: 1781 loss: 7.07364052e-07
Iter: 1782 loss: 7.07352456e-07
Iter: 1783 loss: 7.07271226e-07
Iter: 1784 loss: 7.07161746e-07
Iter: 1785 loss: 7.07149638e-07
Iter: 1786 loss: 7.07032768e-07
Iter: 1787 loss: 7.07316133e-07
Iter: 1788 loss: 7.0702032e-07
Iter: 1789 loss: 7.06939772e-07
Iter: 1790 loss: 7.07209097e-07
Iter: 1791 loss: 7.06935168e-07
Iter: 1792 loss: 7.068287e-07
Iter: 1793 loss: 7.07729896e-07
Iter: 1794 loss: 7.06833646e-07
Iter: 1795 loss: 7.06785158e-07
Iter: 1796 loss: 7.06664935e-07
Iter: 1797 loss: 7.09148e-07
Iter: 1798 loss: 7.06694266e-07
Iter: 1799 loss: 7.06573189e-07
Iter: 1800 loss: 7.06629464e-07
Iter: 1801 loss: 7.06519074e-07
Iter: 1802 loss: 7.06530614e-07
Iter: 1803 loss: 7.06476953e-07
Iter: 1804 loss: 7.06447281e-07
Iter: 1805 loss: 7.06352694e-07
Iter: 1806 loss: 7.07683739e-07
Iter: 1807 loss: 7.06356104e-07
Iter: 1808 loss: 7.06319e-07
Iter: 1809 loss: 7.0631836e-07
Iter: 1810 loss: 7.06274818e-07
Iter: 1811 loss: 7.06205242e-07
Iter: 1812 loss: 7.06206379e-07
Iter: 1813 loss: 7.06124638e-07
Iter: 1814 loss: 7.06251853e-07
Iter: 1815 loss: 7.06108153e-07
Iter: 1816 loss: 7.0602016e-07
Iter: 1817 loss: 7.0602573e-07
Iter: 1818 loss: 7.06001742e-07
Iter: 1819 loss: 7.05913294e-07
Iter: 1820 loss: 7.0696467e-07
Iter: 1821 loss: 7.05897207e-07
Iter: 1822 loss: 7.05807e-07
Iter: 1823 loss: 7.06162439e-07
Iter: 1824 loss: 7.05792786e-07
Iter: 1825 loss: 7.05706384e-07
Iter: 1826 loss: 7.05948821e-07
Iter: 1827 loss: 7.05668072e-07
Iter: 1828 loss: 7.05567629e-07
Iter: 1829 loss: 7.05561206e-07
Iter: 1830 loss: 7.05533807e-07
Iter: 1831 loss: 7.05445e-07
Iter: 1832 loss: 7.06103492e-07
Iter: 1833 loss: 7.05416824e-07
Iter: 1834 loss: 7.05320645e-07
Iter: 1835 loss: 7.06364176e-07
Iter: 1836 loss: 7.05329967e-07
Iter: 1837 loss: 7.05255786e-07
Iter: 1838 loss: 7.05711216e-07
Iter: 1839 loss: 7.05226796e-07
Iter: 1840 loss: 7.05200307e-07
Iter: 1841 loss: 7.052098e-07
Iter: 1842 loss: 7.05178536e-07
Iter: 1843 loss: 7.05106174e-07
Iter: 1844 loss: 7.05297111e-07
Iter: 1845 loss: 7.05093385e-07
Iter: 1846 loss: 7.0504467e-07
Iter: 1847 loss: 7.05051093e-07
Iter: 1848 loss: 7.04990327e-07
Iter: 1849 loss: 7.04940248e-07
Iter: 1850 loss: 7.05579282e-07
Iter: 1851 loss: 7.04950821e-07
Iter: 1852 loss: 7.04916e-07
Iter: 1853 loss: 7.04898412e-07
Iter: 1854 loss: 7.0484549e-07
Iter: 1855 loss: 7.04815761e-07
Iter: 1856 loss: 7.04813942e-07
Iter: 1857 loss: 7.04770059e-07
Iter: 1858 loss: 7.04701e-07
Iter: 1859 loss: 7.05110267e-07
Iter: 1860 loss: 7.04695765e-07
Iter: 1861 loss: 7.04630793e-07
Iter: 1862 loss: 7.05279831e-07
Iter: 1863 loss: 7.04628576e-07
Iter: 1864 loss: 7.04577928e-07
Iter: 1865 loss: 7.04540298e-07
Iter: 1866 loss: 7.04547574e-07
Iter: 1867 loss: 7.04472427e-07
Iter: 1868 loss: 7.04515685e-07
Iter: 1869 loss: 7.04446677e-07
Iter: 1870 loss: 7.043908e-07
Iter: 1871 loss: 7.04997888e-07
Iter: 1872 loss: 7.04402737e-07
Iter: 1873 loss: 7.04319461e-07
Iter: 1874 loss: 7.04299282e-07
Iter: 1875 loss: 7.04279159e-07
Iter: 1876 loss: 7.04203615e-07
Iter: 1877 loss: 7.04744707e-07
Iter: 1878 loss: 7.04196736e-07
Iter: 1879 loss: 7.04156378e-07
Iter: 1880 loss: 7.04151319e-07
Iter: 1881 loss: 7.0411977e-07
Iter: 1882 loss: 7.04050763e-07
Iter: 1883 loss: 7.04083334e-07
Iter: 1884 loss: 7.03998239e-07
Iter: 1885 loss: 7.03947478e-07
Iter: 1886 loss: 7.03959927e-07
Iter: 1887 loss: 7.03911155e-07
Iter: 1888 loss: 7.03847093e-07
Iter: 1889 loss: 7.03848229e-07
Iter: 1890 loss: 7.03786782e-07
Iter: 1891 loss: 7.03847206e-07
Iter: 1892 loss: 7.03745513e-07
Iter: 1893 loss: 7.03678552e-07
Iter: 1894 loss: 7.04486752e-07
Iter: 1895 loss: 7.03686112e-07
Iter: 1896 loss: 7.03635578e-07
Iter: 1897 loss: 7.03962257e-07
Iter: 1898 loss: 7.03627279e-07
Iter: 1899 loss: 7.03597834e-07
Iter: 1900 loss: 7.03570379e-07
Iter: 1901 loss: 7.0356333e-07
Iter: 1902 loss: 7.0351939e-07
Iter: 1903 loss: 7.03527689e-07
Iter: 1904 loss: 7.03457317e-07
Iter: 1905 loss: 7.03398484e-07
Iter: 1906 loss: 7.03406045e-07
Iter: 1907 loss: 7.03362673e-07
Iter: 1908 loss: 7.03347553e-07
Iter: 1909 loss: 7.0334471e-07
Iter: 1910 loss: 7.03275418e-07
Iter: 1911 loss: 7.0383021e-07
Iter: 1912 loss: 7.03267688e-07
Iter: 1913 loss: 7.03216642e-07
Iter: 1914 loss: 7.03223236e-07
Iter: 1915 loss: 7.03148885e-07
Iter: 1916 loss: 7.03121373e-07
Iter: 1917 loss: 7.03371938e-07
Iter: 1918 loss: 7.03124442e-07
Iter: 1919 loss: 7.03081128e-07
Iter: 1920 loss: 7.03567821e-07
Iter: 1921 loss: 7.03076637e-07
Iter: 1922 loss: 7.03056912e-07
Iter: 1923 loss: 7.02987e-07
Iter: 1924 loss: 7.03619037e-07
Iter: 1925 loss: 7.02982902e-07
Iter: 1926 loss: 7.028857e-07
Iter: 1927 loss: 7.03632168e-07
Iter: 1928 loss: 7.02892635e-07
Iter: 1929 loss: 7.02869329e-07
Iter: 1930 loss: 7.0285023e-07
Iter: 1931 loss: 7.02847331e-07
Iter: 1932 loss: 7.02779403e-07
Iter: 1933 loss: 7.02785371e-07
Iter: 1934 loss: 7.02733473e-07
Iter: 1935 loss: 7.02714601e-07
Iter: 1936 loss: 7.027013e-07
Iter: 1937 loss: 7.0264673e-07
Iter: 1938 loss: 7.03024057e-07
Iter: 1939 loss: 7.02612795e-07
Iter: 1940 loss: 7.02561124e-07
Iter: 1941 loss: 7.02832438e-07
Iter: 1942 loss: 7.02567263e-07
Iter: 1943 loss: 7.02527132e-07
Iter: 1944 loss: 7.02517582e-07
Iter: 1945 loss: 7.02482453e-07
Iter: 1946 loss: 7.02431407e-07
Iter: 1947 loss: 7.02876491e-07
Iter: 1948 loss: 7.02422085e-07
Iter: 1949 loss: 7.02405714e-07
Iter: 1950 loss: 7.02346938e-07
Iter: 1951 loss: 7.02353077e-07
Iter: 1952 loss: 7.02303282e-07
Iter: 1953 loss: 7.02628824e-07
Iter: 1954 loss: 7.0227992e-07
Iter: 1955 loss: 7.02252919e-07
Iter: 1956 loss: 7.02453747e-07
Iter: 1957 loss: 7.02240527e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2
+ date
Mon Oct 26 10:05:46 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573ee5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573e78a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573ee5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573ec7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573ec7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573e781e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573ec72f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573d461e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573d46950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573d46b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573ce4268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573cc59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573ca9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573cefe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573c62378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573cfa488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573c62598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573c22620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573c22598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573b6f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573b7b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573b340d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573b7b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573b33840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573aa0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573a96ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573a96620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65585ef2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65585ef598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65585aa8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f65585cc1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6573a988c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6558564620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f655857f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f655850aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653004df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.4246251e-06
Iter: 2 loss: 1.81630276e-05
Iter: 3 loss: 3.57313479e-06
Iter: 4 loss: 2.69930206e-06
Iter: 5 loss: 3.22470805e-06
Iter: 6 loss: 2.13778776e-06
Iter: 7 loss: 1.72156604e-06
Iter: 8 loss: 4.91142328e-06
Iter: 9 loss: 1.69053942e-06
Iter: 10 loss: 1.48284676e-06
Iter: 11 loss: 1.57727413e-06
Iter: 12 loss: 1.34193556e-06
Iter: 13 loss: 1.21632934e-06
Iter: 14 loss: 2.99965e-06
Iter: 15 loss: 1.21606843e-06
Iter: 16 loss: 1.16313788e-06
Iter: 17 loss: 1.35105267e-06
Iter: 18 loss: 1.14962677e-06
Iter: 19 loss: 1.10148483e-06
Iter: 20 loss: 1.13264525e-06
Iter: 21 loss: 1.07095e-06
Iter: 22 loss: 1.04048013e-06
Iter: 23 loss: 1.04044193e-06
Iter: 24 loss: 1.02090814e-06
Iter: 25 loss: 1.00446186e-06
Iter: 26 loss: 9.99041e-07
Iter: 27 loss: 9.80002e-07
Iter: 28 loss: 1.06854463e-06
Iter: 29 loss: 9.76519573e-07
Iter: 30 loss: 9.61555088e-07
Iter: 31 loss: 9.96766e-07
Iter: 32 loss: 9.5610153e-07
Iter: 33 loss: 9.41786311e-07
Iter: 34 loss: 9.71293275e-07
Iter: 35 loss: 9.36040124e-07
Iter: 36 loss: 9.20717525e-07
Iter: 37 loss: 9.73955139e-07
Iter: 38 loss: 9.16702447e-07
Iter: 39 loss: 9.35593e-07
Iter: 40 loss: 9.13379495e-07
Iter: 41 loss: 9.11883603e-07
Iter: 42 loss: 9.07024059e-07
Iter: 43 loss: 9.11829602e-07
Iter: 44 loss: 9.03179284e-07
Iter: 45 loss: 8.98109533e-07
Iter: 46 loss: 8.97855557e-07
Iter: 47 loss: 8.93860943e-07
Iter: 48 loss: 8.95878088e-07
Iter: 49 loss: 8.91254274e-07
Iter: 50 loss: 8.87257102e-07
Iter: 51 loss: 8.80177652e-07
Iter: 52 loss: 8.80188225e-07
Iter: 53 loss: 8.77539037e-07
Iter: 54 loss: 8.75728688e-07
Iter: 55 loss: 8.72639191e-07
Iter: 56 loss: 8.69155372e-07
Iter: 57 loss: 8.68682832e-07
Iter: 58 loss: 8.62989623e-07
Iter: 59 loss: 8.93627885e-07
Iter: 60 loss: 8.62137711e-07
Iter: 61 loss: 8.578495e-07
Iter: 62 loss: 8.70982944e-07
Iter: 63 loss: 8.56584e-07
Iter: 64 loss: 8.52311359e-07
Iter: 65 loss: 8.54217035e-07
Iter: 66 loss: 8.49416779e-07
Iter: 67 loss: 8.44633291e-07
Iter: 68 loss: 8.67839731e-07
Iter: 69 loss: 8.43789053e-07
Iter: 70 loss: 8.40220764e-07
Iter: 71 loss: 8.37970902e-07
Iter: 72 loss: 8.36592903e-07
Iter: 73 loss: 8.35168748e-07
Iter: 74 loss: 8.34418415e-07
Iter: 75 loss: 8.32121259e-07
Iter: 76 loss: 8.42093812e-07
Iter: 77 loss: 8.31691182e-07
Iter: 78 loss: 8.30362296e-07
Iter: 79 loss: 8.27085671e-07
Iter: 80 loss: 8.59368583e-07
Iter: 81 loss: 8.26649853e-07
Iter: 82 loss: 8.25087682e-07
Iter: 83 loss: 8.24798576e-07
Iter: 84 loss: 8.22792686e-07
Iter: 85 loss: 8.18708315e-07
Iter: 86 loss: 8.93716219e-07
Iter: 87 loss: 8.18668241e-07
Iter: 88 loss: 8.16615568e-07
Iter: 89 loss: 8.35727519e-07
Iter: 90 loss: 8.1655503e-07
Iter: 91 loss: 8.14270606e-07
Iter: 92 loss: 8.19455408e-07
Iter: 93 loss: 8.1343893e-07
Iter: 94 loss: 8.11685368e-07
Iter: 95 loss: 8.14813916e-07
Iter: 96 loss: 8.1093151e-07
Iter: 97 loss: 8.08925677e-07
Iter: 98 loss: 8.14928626e-07
Iter: 99 loss: 8.08329901e-07
Iter: 100 loss: 8.06091407e-07
Iter: 101 loss: 8.12024723e-07
Iter: 102 loss: 8.05364266e-07
Iter: 103 loss: 8.03796297e-07
Iter: 104 loss: 8.05291393e-07
Iter: 105 loss: 8.02950808e-07
Iter: 106 loss: 8.00809175e-07
Iter: 107 loss: 8.07132892e-07
Iter: 108 loss: 8.00101759e-07
Iter: 109 loss: 8.00017176e-07
Iter: 110 loss: 7.99490692e-07
Iter: 111 loss: 7.98868427e-07
Iter: 112 loss: 7.97634129e-07
Iter: 113 loss: 8.20230355e-07
Iter: 114 loss: 7.97600364e-07
Iter: 115 loss: 7.96479867e-07
Iter: 116 loss: 7.96497829e-07
Iter: 117 loss: 7.95576e-07
Iter: 118 loss: 7.94577545e-07
Iter: 119 loss: 7.94590335e-07
Iter: 120 loss: 7.93638719e-07
Iter: 121 loss: 7.9167188e-07
Iter: 122 loss: 8.25040786e-07
Iter: 123 loss: 7.91666594e-07
Iter: 124 loss: 7.89985506e-07
Iter: 125 loss: 7.98630822e-07
Iter: 126 loss: 7.89665876e-07
Iter: 127 loss: 7.88238367e-07
Iter: 128 loss: 8.03127705e-07
Iter: 129 loss: 7.881697e-07
Iter: 130 loss: 7.87126e-07
Iter: 131 loss: 7.86563248e-07
Iter: 132 loss: 7.86061719e-07
Iter: 133 loss: 7.85357656e-07
Iter: 134 loss: 7.8534282e-07
Iter: 135 loss: 7.8463637e-07
Iter: 136 loss: 7.84000292e-07
Iter: 137 loss: 7.83834366e-07
Iter: 138 loss: 7.828869e-07
Iter: 139 loss: 7.8950336e-07
Iter: 140 loss: 7.82799134e-07
Iter: 141 loss: 7.82288112e-07
Iter: 142 loss: 7.85463158e-07
Iter: 143 loss: 7.82214443e-07
Iter: 144 loss: 7.81560288e-07
Iter: 145 loss: 7.84532062e-07
Iter: 146 loss: 7.81427616e-07
Iter: 147 loss: 7.80886637e-07
Iter: 148 loss: 7.79784159e-07
Iter: 149 loss: 8.0034971e-07
Iter: 150 loss: 7.79764093e-07
Iter: 151 loss: 7.79072195e-07
Iter: 152 loss: 7.83945779e-07
Iter: 153 loss: 7.79011827e-07
Iter: 154 loss: 7.78172e-07
Iter: 155 loss: 7.80470714e-07
Iter: 156 loss: 7.77935725e-07
Iter: 157 loss: 7.77106607e-07
Iter: 158 loss: 7.76687671e-07
Iter: 159 loss: 7.76345928e-07
Iter: 160 loss: 7.75579565e-07
Iter: 161 loss: 7.79148081e-07
Iter: 162 loss: 7.7542461e-07
Iter: 163 loss: 7.74496812e-07
Iter: 164 loss: 7.78794742e-07
Iter: 165 loss: 7.74352316e-07
Iter: 166 loss: 7.73847773e-07
Iter: 167 loss: 7.73054182e-07
Iter: 168 loss: 7.73011323e-07
Iter: 169 loss: 7.72023952e-07
Iter: 170 loss: 7.85723103e-07
Iter: 171 loss: 7.72022645e-07
Iter: 172 loss: 7.71351267e-07
Iter: 173 loss: 7.70918462e-07
Iter: 174 loss: 7.70700353e-07
Iter: 175 loss: 7.69643179e-07
Iter: 176 loss: 7.73348347e-07
Iter: 177 loss: 7.6936135e-07
Iter: 178 loss: 7.69392841e-07
Iter: 179 loss: 7.68998234e-07
Iter: 180 loss: 7.68717427e-07
Iter: 181 loss: 7.68092718e-07
Iter: 182 loss: 7.75942794e-07
Iter: 183 loss: 7.68019959e-07
Iter: 184 loss: 7.67189817e-07
Iter: 185 loss: 7.67533379e-07
Iter: 186 loss: 7.66636617e-07
Iter: 187 loss: 7.6626236e-07
Iter: 188 loss: 7.66232802e-07
Iter: 189 loss: 7.65760774e-07
Iter: 190 loss: 7.65105483e-07
Iter: 191 loss: 7.65082859e-07
Iter: 192 loss: 7.64358902e-07
Iter: 193 loss: 7.66524067e-07
Iter: 194 loss: 7.64137667e-07
Iter: 195 loss: 7.63624939e-07
Iter: 196 loss: 7.68370455e-07
Iter: 197 loss: 7.63609137e-07
Iter: 198 loss: 7.630656e-07
Iter: 199 loss: 7.62702314e-07
Iter: 200 loss: 7.62490743e-07
Iter: 201 loss: 7.61844149e-07
Iter: 202 loss: 7.63754542e-07
Iter: 203 loss: 7.61651e-07
Iter: 204 loss: 7.61049705e-07
Iter: 205 loss: 7.66090295e-07
Iter: 206 loss: 7.60979447e-07
Iter: 207 loss: 7.60549369e-07
Iter: 208 loss: 7.60328135e-07
Iter: 209 loss: 7.60065916e-07
Iter: 210 loss: 7.59620093e-07
Iter: 211 loss: 7.64560468e-07
Iter: 212 loss: 7.59624925e-07
Iter: 213 loss: 7.58946385e-07
Iter: 214 loss: 7.59148747e-07
Iter: 215 loss: 7.58472254e-07
Iter: 216 loss: 7.57908083e-07
Iter: 217 loss: 7.58025863e-07
Iter: 218 loss: 7.57456633e-07
Iter: 219 loss: 7.56892575e-07
Iter: 220 loss: 7.59089346e-07
Iter: 221 loss: 7.56777126e-07
Iter: 222 loss: 7.56168106e-07
Iter: 223 loss: 7.59419095e-07
Iter: 224 loss: 7.56110069e-07
Iter: 225 loss: 7.555563e-07
Iter: 226 loss: 7.55327278e-07
Iter: 227 loss: 7.55086603e-07
Iter: 228 loss: 7.5451328e-07
Iter: 229 loss: 7.54622874e-07
Iter: 230 loss: 7.54116684e-07
Iter: 231 loss: 7.5338e-07
Iter: 232 loss: 7.64233505e-07
Iter: 233 loss: 7.53396307e-07
Iter: 234 loss: 7.5292121e-07
Iter: 235 loss: 7.52626363e-07
Iter: 236 loss: 7.52413087e-07
Iter: 237 loss: 7.51777861e-07
Iter: 238 loss: 7.5370383e-07
Iter: 239 loss: 7.51555831e-07
Iter: 240 loss: 7.51182597e-07
Iter: 241 loss: 7.51176685e-07
Iter: 242 loss: 7.50902132e-07
Iter: 243 loss: 7.50199092e-07
Iter: 244 loss: 7.56737393e-07
Iter: 245 loss: 7.50084e-07
Iter: 246 loss: 7.50671632e-07
Iter: 247 loss: 7.49816081e-07
Iter: 248 loss: 7.49613889e-07
Iter: 249 loss: 7.49242531e-07
Iter: 250 loss: 7.5369303e-07
Iter: 251 loss: 7.49171704e-07
Iter: 252 loss: 7.48752313e-07
Iter: 253 loss: 7.48820298e-07
Iter: 254 loss: 7.48370269e-07
Iter: 255 loss: 7.4812948e-07
Iter: 256 loss: 7.48091793e-07
Iter: 257 loss: 7.47860668e-07
Iter: 258 loss: 7.47825197e-07
Iter: 259 loss: 7.47693548e-07
Iter: 260 loss: 7.4727518e-07
Iter: 261 loss: 7.4665428e-07
Iter: 262 loss: 7.46638364e-07
Iter: 263 loss: 7.46269e-07
Iter: 264 loss: 7.46232104e-07
Iter: 265 loss: 7.4579475e-07
Iter: 266 loss: 7.45723924e-07
Iter: 267 loss: 7.45444765e-07
Iter: 268 loss: 7.44905208e-07
Iter: 269 loss: 7.46384e-07
Iter: 270 loss: 7.44728482e-07
Iter: 271 loss: 7.44336262e-07
Iter: 272 loss: 7.46084538e-07
Iter: 273 loss: 7.44256567e-07
Iter: 274 loss: 7.4373736e-07
Iter: 275 loss: 7.43957798e-07
Iter: 276 loss: 7.43381634e-07
Iter: 277 loss: 7.43003284e-07
Iter: 278 loss: 7.42986572e-07
Iter: 279 loss: 7.42656084e-07
Iter: 280 loss: 7.4351135e-07
Iter: 281 loss: 7.42508462e-07
Iter: 282 loss: 7.42288933e-07
Iter: 283 loss: 7.41868575e-07
Iter: 284 loss: 7.48039781e-07
Iter: 285 loss: 7.41842143e-07
Iter: 286 loss: 7.41498752e-07
Iter: 287 loss: 7.46174919e-07
Iter: 288 loss: 7.41506369e-07
Iter: 289 loss: 7.41122676e-07
Iter: 290 loss: 7.41859e-07
Iter: 291 loss: 7.4104139e-07
Iter: 292 loss: 7.40641099e-07
Iter: 293 loss: 7.40983523e-07
Iter: 294 loss: 7.40460905e-07
Iter: 295 loss: 7.40035716e-07
Iter: 296 loss: 7.40371433e-07
Iter: 297 loss: 7.39830568e-07
Iter: 298 loss: 7.39349105e-07
Iter: 299 loss: 7.40345627e-07
Iter: 300 loss: 7.39184316e-07
Iter: 301 loss: 7.38990877e-07
Iter: 302 loss: 7.38954895e-07
Iter: 303 loss: 7.38725e-07
Iter: 304 loss: 7.38361905e-07
Iter: 305 loss: 7.45309649e-07
Iter: 306 loss: 7.38374354e-07
Iter: 307 loss: 7.37919891e-07
Iter: 308 loss: 7.39879454e-07
Iter: 309 loss: 7.37832124e-07
Iter: 310 loss: 7.37507094e-07
Iter: 311 loss: 7.40315102e-07
Iter: 312 loss: 7.37469406e-07
Iter: 313 loss: 7.37208097e-07
Iter: 314 loss: 7.39876782e-07
Iter: 315 loss: 7.3720787e-07
Iter: 316 loss: 7.36981178e-07
Iter: 317 loss: 7.37712696e-07
Iter: 318 loss: 7.36926779e-07
Iter: 319 loss: 7.36830543e-07
Iter: 320 loss: 7.36507957e-07
Iter: 321 loss: 7.37271421e-07
Iter: 322 loss: 7.36292691e-07
Iter: 323 loss: 7.35910191e-07
Iter: 324 loss: 7.41010808e-07
Iter: 325 loss: 7.35905246e-07
Iter: 326 loss: 7.35592266e-07
Iter: 327 loss: 7.37793812e-07
Iter: 328 loss: 7.35581295e-07
Iter: 329 loss: 7.35230856e-07
Iter: 330 loss: 7.34967387e-07
Iter: 331 loss: 7.34906337e-07
Iter: 332 loss: 7.3453532e-07
Iter: 333 loss: 7.34765877e-07
Iter: 334 loss: 7.34327e-07
Iter: 335 loss: 7.33834099e-07
Iter: 336 loss: 7.35934236e-07
Iter: 337 loss: 7.33722686e-07
Iter: 338 loss: 7.3334877e-07
Iter: 339 loss: 7.33665502e-07
Iter: 340 loss: 7.33108891e-07
Iter: 341 loss: 7.32801709e-07
Iter: 342 loss: 7.32784827e-07
Iter: 343 loss: 7.32593321e-07
Iter: 344 loss: 7.32546255e-07
Iter: 345 loss: 7.32413582e-07
Iter: 346 loss: 7.32234128e-07
Iter: 347 loss: 7.33382308e-07
Iter: 348 loss: 7.32242199e-07
Iter: 349 loss: 7.31999364e-07
Iter: 350 loss: 7.33379807e-07
Iter: 351 loss: 7.31970147e-07
Iter: 352 loss: 7.31825253e-07
Iter: 353 loss: 7.3180388e-07
Iter: 354 loss: 7.31680871e-07
Iter: 355 loss: 7.31458e-07
Iter: 356 loss: 7.31293085e-07
Iter: 357 loss: 7.31207933e-07
Iter: 358 loss: 7.30880913e-07
Iter: 359 loss: 7.31296097e-07
Iter: 360 loss: 7.30684405e-07
Iter: 361 loss: 7.30521037e-07
Iter: 362 loss: 7.30473459e-07
Iter: 363 loss: 7.30299632e-07
Iter: 364 loss: 7.29938847e-07
Iter: 365 loss: 7.3747367e-07
Iter: 366 loss: 7.2996886e-07
Iter: 367 loss: 7.29684302e-07
Iter: 368 loss: 7.30425882e-07
Iter: 369 loss: 7.2958062e-07
Iter: 370 loss: 7.29261956e-07
Iter: 371 loss: 7.29606711e-07
Iter: 372 loss: 7.29062094e-07
Iter: 373 loss: 7.28968701e-07
Iter: 374 loss: 7.28913619e-07
Iter: 375 loss: 7.28775e-07
Iter: 376 loss: 7.28727059e-07
Iter: 377 loss: 7.28690793e-07
Iter: 378 loss: 7.2845188e-07
Iter: 379 loss: 7.28662371e-07
Iter: 380 loss: 7.2828243e-07
Iter: 381 loss: 7.28043346e-07
Iter: 382 loss: 7.29541739e-07
Iter: 383 loss: 7.28048406e-07
Iter: 384 loss: 7.27766e-07
Iter: 385 loss: 7.28197051e-07
Iter: 386 loss: 7.27609063e-07
Iter: 387 loss: 7.27390784e-07
Iter: 388 loss: 7.27146812e-07
Iter: 389 loss: 7.27090821e-07
Iter: 390 loss: 7.26715257e-07
Iter: 391 loss: 7.29122405e-07
Iter: 392 loss: 7.26691781e-07
Iter: 393 loss: 7.26434109e-07
Iter: 394 loss: 7.26277563e-07
Iter: 395 loss: 7.2617911e-07
Iter: 396 loss: 7.26082362e-07
Iter: 397 loss: 7.26035864e-07
Iter: 398 loss: 7.25894211e-07
Iter: 399 loss: 7.25607777e-07
Iter: 400 loss: 7.29257863e-07
Iter: 401 loss: 7.25631196e-07
Iter: 402 loss: 7.25317705e-07
Iter: 403 loss: 7.25815312e-07
Iter: 404 loss: 7.25149221e-07
Iter: 405 loss: 7.24883364e-07
Iter: 406 loss: 7.25872439e-07
Iter: 407 loss: 7.24792e-07
Iter: 408 loss: 7.24542e-07
Iter: 409 loss: 7.26340659e-07
Iter: 410 loss: 7.24533322e-07
Iter: 411 loss: 7.24309075e-07
Iter: 412 loss: 7.25755854e-07
Iter: 413 loss: 7.24310212e-07
Iter: 414 loss: 7.24120525e-07
Iter: 415 loss: 7.23736e-07
Iter: 416 loss: 7.30105569e-07
Iter: 417 loss: 7.23730636e-07
Iter: 418 loss: 7.23815504e-07
Iter: 419 loss: 7.23530889e-07
Iter: 420 loss: 7.23431128e-07
Iter: 421 loss: 7.23167602e-07
Iter: 422 loss: 7.26759595e-07
Iter: 423 loss: 7.23144467e-07
Iter: 424 loss: 7.22853e-07
Iter: 425 loss: 7.23323e-07
Iter: 426 loss: 7.22664595e-07
Iter: 427 loss: 7.22464506e-07
Iter: 428 loss: 7.24931e-07
Iter: 429 loss: 7.22464279e-07
Iter: 430 loss: 7.22285336e-07
Iter: 431 loss: 7.22234176e-07
Iter: 432 loss: 7.22127766e-07
Iter: 433 loss: 7.21872539e-07
Iter: 434 loss: 7.22854111e-07
Iter: 435 loss: 7.21817401e-07
Iter: 436 loss: 7.2158241e-07
Iter: 437 loss: 7.23480298e-07
Iter: 438 loss: 7.21599122e-07
Iter: 439 loss: 7.21412562e-07
Iter: 440 loss: 7.20976459e-07
Iter: 441 loss: 7.25097607e-07
Iter: 442 loss: 7.20922912e-07
Iter: 443 loss: 7.20595949e-07
Iter: 444 loss: 7.22558923e-07
Iter: 445 loss: 7.20563662e-07
Iter: 446 loss: 7.2023505e-07
Iter: 447 loss: 7.22249069e-07
Iter: 448 loss: 7.20195544e-07
Iter: 449 loss: 7.19913317e-07
Iter: 450 loss: 7.22330071e-07
Iter: 451 loss: 7.19879154e-07
Iter: 452 loss: 7.19700438e-07
Iter: 453 loss: 7.19735965e-07
Iter: 454 loss: 7.19594823e-07
Iter: 455 loss: 7.19266154e-07
Iter: 456 loss: 7.21430638e-07
Iter: 457 loss: 7.19259106e-07
Iter: 458 loss: 7.19090963e-07
Iter: 459 loss: 7.18875867e-07
Iter: 460 loss: 7.24377742e-07
Iter: 461 loss: 7.18890249e-07
Iter: 462 loss: 7.18617571e-07
Iter: 463 loss: 7.19577e-07
Iter: 464 loss: 7.18574483e-07
Iter: 465 loss: 7.18358422e-07
Iter: 466 loss: 7.19833508e-07
Iter: 467 loss: 7.18326191e-07
Iter: 468 loss: 7.18167826e-07
Iter: 469 loss: 7.18149693e-07
Iter: 470 loss: 7.18022363e-07
Iter: 471 loss: 7.17764408e-07
Iter: 472 loss: 7.1912325e-07
Iter: 473 loss: 7.17745763e-07
Iter: 474 loss: 7.1753243e-07
Iter: 475 loss: 7.19253876e-07
Iter: 476 loss: 7.17535158e-07
Iter: 477 loss: 7.17439036e-07
Iter: 478 loss: 7.17130888e-07
Iter: 479 loss: 7.19257059e-07
Iter: 480 loss: 7.17060402e-07
Iter: 481 loss: 7.16678642e-07
Iter: 482 loss: 7.19092554e-07
Iter: 483 loss: 7.16639249e-07
Iter: 484 loss: 7.16269e-07
Iter: 485 loss: 7.16534828e-07
Iter: 486 loss: 7.16082241e-07
Iter: 487 loss: 7.15882493e-07
Iter: 488 loss: 7.15790065e-07
Iter: 489 loss: 7.15671604e-07
Iter: 490 loss: 7.16217073e-07
Iter: 491 loss: 7.15658302e-07
Iter: 492 loss: 7.15489307e-07
Iter: 493 loss: 7.15726969e-07
Iter: 494 loss: 7.15420242e-07
Iter: 495 loss: 7.1526955e-07
Iter: 496 loss: 7.15075942e-07
Iter: 497 loss: 7.15047236e-07
Iter: 498 loss: 7.14884e-07
Iter: 499 loss: 7.16980708e-07
Iter: 500 loss: 7.14885914e-07
Iter: 501 loss: 7.14695545e-07
Iter: 502 loss: 7.14801899e-07
Iter: 503 loss: 7.14641317e-07
Iter: 504 loss: 7.14409566e-07
Iter: 505 loss: 7.1488796e-07
Iter: 506 loss: 7.14320265e-07
Iter: 507 loss: 7.14233465e-07
Iter: 508 loss: 7.14207658e-07
Iter: 509 loss: 7.14092266e-07
Iter: 510 loss: 7.13950953e-07
Iter: 511 loss: 7.13907866e-07
Iter: 512 loss: 7.1366469e-07
Iter: 513 loss: 7.13988e-07
Iter: 514 loss: 7.13566237e-07
Iter: 515 loss: 7.13310783e-07
Iter: 516 loss: 7.13413e-07
Iter: 517 loss: 7.1312013e-07
Iter: 518 loss: 7.12888891e-07
Iter: 519 loss: 7.16216618e-07
Iter: 520 loss: 7.12918563e-07
Iter: 521 loss: 7.12748204e-07
Iter: 522 loss: 7.1445362e-07
Iter: 523 loss: 7.12756844e-07
Iter: 524 loss: 7.12611723e-07
Iter: 525 loss: 7.12989618e-07
Iter: 526 loss: 7.12569886e-07
Iter: 527 loss: 7.12409701e-07
Iter: 528 loss: 7.12474161e-07
Iter: 529 loss: 7.12300334e-07
Iter: 530 loss: 7.12191195e-07
Iter: 531 loss: 7.11938185e-07
Iter: 532 loss: 7.14538828e-07
Iter: 533 loss: 7.11913174e-07
Iter: 534 loss: 7.11839959e-07
Iter: 535 loss: 7.11779478e-07
Iter: 536 loss: 7.11664541e-07
Iter: 537 loss: 7.11440237e-07
Iter: 538 loss: 7.11449275e-07
Iter: 539 loss: 7.11253165e-07
Iter: 540 loss: 7.13260079e-07
Iter: 541 loss: 7.11242365e-07
Iter: 542 loss: 7.11082748e-07
Iter: 543 loss: 7.12621613e-07
Iter: 544 loss: 7.11076041e-07
Iter: 545 loss: 7.10958375e-07
Iter: 546 loss: 7.10675636e-07
Iter: 547 loss: 7.10680183e-07
Iter: 548 loss: 7.10448148e-07
Iter: 549 loss: 7.11363896e-07
Iter: 550 loss: 7.10395057e-07
Iter: 551 loss: 7.10171093e-07
Iter: 552 loss: 7.10476229e-07
Iter: 553 loss: 7.10095264e-07
Iter: 554 loss: 7.09894607e-07
Iter: 555 loss: 7.12345866e-07
Iter: 556 loss: 7.09894493e-07
Iter: 557 loss: 7.09789788e-07
Iter: 558 loss: 7.11089626e-07
Iter: 559 loss: 7.09735048e-07
Iter: 560 loss: 7.09644496e-07
Iter: 561 loss: 7.09831966e-07
Iter: 562 loss: 7.09590267e-07
Iter: 563 loss: 7.09527058e-07
Iter: 564 loss: 7.09264725e-07
Iter: 565 loss: 7.12404471e-07
Iter: 566 loss: 7.09242386e-07
Iter: 567 loss: 7.09024846e-07
Iter: 568 loss: 7.09923597e-07
Iter: 569 loss: 7.08990058e-07
Iter: 570 loss: 7.08796961e-07
Iter: 571 loss: 7.09761252e-07
Iter: 572 loss: 7.08744494e-07
Iter: 573 loss: 7.0854577e-07
Iter: 574 loss: 7.09440314e-07
Iter: 575 loss: 7.08501261e-07
Iter: 576 loss: 7.08333459e-07
Iter: 577 loss: 7.08277184e-07
Iter: 578 loss: 7.0818e-07
Iter: 579 loss: 7.07957099e-07
Iter: 580 loss: 7.10062523e-07
Iter: 581 loss: 7.07906622e-07
Iter: 582 loss: 7.07787763e-07
Iter: 583 loss: 7.07729384e-07
Iter: 584 loss: 7.07651793e-07
Iter: 585 loss: 7.07489448e-07
Iter: 586 loss: 7.07930781e-07
Iter: 587 loss: 7.07388381e-07
Iter: 588 loss: 7.07230129e-07
Iter: 589 loss: 7.07824313e-07
Iter: 590 loss: 7.07190679e-07
Iter: 591 loss: 7.07044137e-07
Iter: 592 loss: 7.07031461e-07
Iter: 593 loss: 7.06901e-07
Iter: 594 loss: 7.07039703e-07
Iter: 595 loss: 7.068287e-07
Iter: 596 loss: 7.06666583e-07
Iter: 597 loss: 7.0701708e-07
Iter: 598 loss: 7.06622359e-07
Iter: 599 loss: 7.06555056e-07
Iter: 600 loss: 7.06271464e-07
Iter: 601 loss: 7.0936494e-07
Iter: 602 loss: 7.06254127e-07
Iter: 603 loss: 7.06050685e-07
Iter: 604 loss: 7.08635184e-07
Iter: 605 loss: 7.0603852e-07
Iter: 606 loss: 7.05920513e-07
Iter: 607 loss: 7.06988374e-07
Iter: 608 loss: 7.05909088e-07
Iter: 609 loss: 7.05732759e-07
Iter: 610 loss: 7.05616628e-07
Iter: 611 loss: 7.05586899e-07
Iter: 612 loss: 7.05463606e-07
Iter: 613 loss: 7.0547685e-07
Iter: 614 loss: 7.05380444e-07
Iter: 615 loss: 7.05228672e-07
Iter: 616 loss: 7.08761831e-07
Iter: 617 loss: 7.05225375e-07
Iter: 618 loss: 7.05010962e-07
Iter: 619 loss: 7.05610205e-07
Iter: 620 loss: 7.04985268e-07
Iter: 621 loss: 7.04820764e-07
Iter: 622 loss: 7.04938486e-07
Iter: 623 loss: 7.04719127e-07
Iter: 624 loss: 7.04478509e-07
Iter: 625 loss: 7.06690685e-07
Iter: 626 loss: 7.04489764e-07
Iter: 627 loss: 7.0429644e-07
Iter: 628 loss: 7.06161416e-07
Iter: 629 loss: 7.0429337e-07
Iter: 630 loss: 7.04203558e-07
Iter: 631 loss: 7.04258184e-07
Iter: 632 loss: 7.04168087e-07
Iter: 633 loss: 7.0402848e-07
Iter: 634 loss: 7.03814521e-07
Iter: 635 loss: 7.03837372e-07
Iter: 636 loss: 7.03647288e-07
Iter: 637 loss: 7.04197305e-07
Iter: 638 loss: 7.03581236e-07
Iter: 639 loss: 7.03411729e-07
Iter: 640 loss: 7.04128297e-07
Iter: 641 loss: 7.03351134e-07
Iter: 642 loss: 7.0322892e-07
Iter: 643 loss: 7.05077866e-07
Iter: 644 loss: 7.03189812e-07
Iter: 645 loss: 7.03120804e-07
Iter: 646 loss: 7.03009903e-07
Iter: 647 loss: 7.02989098e-07
Iter: 648 loss: 7.02861257e-07
Iter: 649 loss: 7.02865236e-07
Iter: 650 loss: 7.02801e-07
Iter: 651 loss: 7.02642694e-07
Iter: 652 loss: 7.05185755e-07
Iter: 653 loss: 7.0264241e-07
Iter: 654 loss: 7.02460966e-07
Iter: 655 loss: 7.02940213e-07
Iter: 656 loss: 7.02348643e-07
Iter: 657 loss: 7.02223929e-07
Iter: 658 loss: 7.03879209e-07
Iter: 659 loss: 7.02209093e-07
Iter: 660 loss: 7.02124339e-07
Iter: 661 loss: 7.02115699e-07
Iter: 662 loss: 7.02072612e-07
Iter: 663 loss: 7.02010084e-07
Iter: 664 loss: 7.01983367e-07
Iter: 665 loss: 7.01897193e-07
Iter: 666 loss: 7.02217562e-07
Iter: 667 loss: 7.0186519e-07
Iter: 668 loss: 7.01775662e-07
Iter: 669 loss: 7.01651629e-07
Iter: 670 loss: 7.01654812e-07
Iter: 671 loss: 7.01464785e-07
Iter: 672 loss: 7.01894919e-07
Iter: 673 loss: 7.01390263e-07
Iter: 674 loss: 7.01248155e-07
Iter: 675 loss: 7.01930844e-07
Iter: 676 loss: 7.01219619e-07
Iter: 677 loss: 7.01111162e-07
Iter: 678 loss: 7.01923341e-07
Iter: 679 loss: 7.01064607e-07
Iter: 680 loss: 7.00956718e-07
Iter: 681 loss: 7.01264526e-07
Iter: 682 loss: 7.00923067e-07
Iter: 683 loss: 7.00776127e-07
Iter: 684 loss: 7.01197962e-07
Iter: 685 loss: 7.00764247e-07
Iter: 686 loss: 7.00657949e-07
Iter: 687 loss: 7.00362591e-07
Iter: 688 loss: 7.04430363e-07
Iter: 689 loss: 7.00330531e-07
Iter: 690 loss: 7.0014022e-07
Iter: 691 loss: 7.00151645e-07
Iter: 692 loss: 7.00021587e-07
Iter: 693 loss: 7.00031308e-07
Iter: 694 loss: 6.99916598e-07
Iter: 695 loss: 6.9984e-07
Iter: 696 loss: 6.99828774e-07
Iter: 697 loss: 6.99669499e-07
Iter: 698 loss: 7.00324108e-07
Iter: 699 loss: 6.99641873e-07
Iter: 700 loss: 6.99525231e-07
Iter: 701 loss: 6.99419729e-07
Iter: 702 loss: 6.99398754e-07
Iter: 703 loss: 6.99217e-07
Iter: 704 loss: 6.99513407e-07
Iter: 705 loss: 6.99136706e-07
Iter: 706 loss: 6.99005113e-07
Iter: 707 loss: 6.9941143e-07
Iter: 708 loss: 6.98943381e-07
Iter: 709 loss: 6.98826852e-07
Iter: 710 loss: 7.00126634e-07
Iter: 711 loss: 6.98796839e-07
Iter: 712 loss: 6.98719418e-07
Iter: 713 loss: 6.99031091e-07
Iter: 714 loss: 6.98702365e-07
Iter: 715 loss: 6.98608e-07
Iter: 716 loss: 6.98605675e-07
Iter: 717 loss: 6.98516146e-07
Iter: 718 loss: 6.9837472e-07
Iter: 719 loss: 6.98703616e-07
Iter: 720 loss: 6.9826541e-07
Iter: 721 loss: 6.98114206e-07
Iter: 722 loss: 6.97885071e-07
Iter: 723 loss: 6.97889277e-07
Iter: 724 loss: 6.97866085e-07
Iter: 725 loss: 6.97786618e-07
Iter: 726 loss: 6.97734151e-07
Iter: 727 loss: 6.98530471e-07
Iter: 728 loss: 6.97730513e-07
Iter: 729 loss: 6.97686232e-07
Iter: 730 loss: 6.97608357e-07
Iter: 731 loss: 6.99037116e-07
Iter: 732 loss: 6.97619726e-07
Iter: 733 loss: 6.97455903e-07
Iter: 734 loss: 6.97829819e-07
Iter: 735 loss: 6.97383143e-07
Iter: 736 loss: 6.97300948e-07
Iter: 737 loss: 6.97530425e-07
Iter: 738 loss: 6.97280939e-07
Iter: 739 loss: 6.97210226e-07
Iter: 740 loss: 6.97112455e-07
Iter: 741 loss: 6.9708841e-07
Iter: 742 loss: 6.96944e-07
Iter: 743 loss: 6.97874611e-07
Iter: 744 loss: 6.96906568e-07
Iter: 745 loss: 6.9680425e-07
Iter: 746 loss: 6.97726534e-07
Iter: 747 loss: 6.96761333e-07
Iter: 748 loss: 6.96635425e-07
Iter: 749 loss: 6.96819257e-07
Iter: 750 loss: 6.96609789e-07
Iter: 751 loss: 6.96494908e-07
Iter: 752 loss: 6.96854499e-07
Iter: 753 loss: 6.96470465e-07
Iter: 754 loss: 6.96337679e-07
Iter: 755 loss: 6.96216887e-07
Iter: 756 loss: 6.96186476e-07
Iter: 757 loss: 6.96081202e-07
Iter: 758 loss: 6.96984614e-07
Iter: 759 loss: 6.96051927e-07
Iter: 760 loss: 6.96008328e-07
Iter: 761 loss: 6.96000598e-07
Iter: 762 loss: 6.95956828e-07
Iter: 763 loss: 6.95802953e-07
Iter: 764 loss: 6.97006215e-07
Iter: 765 loss: 6.95800338e-07
Iter: 766 loss: 6.95701488e-07
Iter: 767 loss: 6.96765596e-07
Iter: 768 loss: 6.9570973e-07
Iter: 769 loss: 6.95640097e-07
Iter: 770 loss: 6.95668348e-07
Iter: 771 loss: 6.95621054e-07
Iter: 772 loss: 6.95539143e-07
Iter: 773 loss: 6.95458255e-07
Iter: 774 loss: 6.95412268e-07
Iter: 775 loss: 6.95314952e-07
Iter: 776 loss: 6.959e-07
Iter: 777 loss: 6.95279425e-07
Iter: 778 loss: 6.95153062e-07
Iter: 779 loss: 6.9534633e-07
Iter: 780 loss: 6.95119411e-07
Iter: 781 loss: 6.94964911e-07
Iter: 782 loss: 6.96576535e-07
Iter: 783 loss: 6.94951325e-07
Iter: 784 loss: 6.94917389e-07
Iter: 785 loss: 6.9482428e-07
Iter: 786 loss: 6.94800292e-07
Iter: 787 loss: 6.94650453e-07
Iter: 788 loss: 6.95018912e-07
Iter: 789 loss: 6.94559276e-07
Iter: 790 loss: 6.94455e-07
Iter: 791 loss: 6.94692744e-07
Iter: 792 loss: 6.94422738e-07
Iter: 793 loss: 6.94357709e-07
Iter: 794 loss: 6.95611675e-07
Iter: 795 loss: 6.94358448e-07
Iter: 796 loss: 6.94254197e-07
Iter: 797 loss: 6.94195e-07
Iter: 798 loss: 6.9415e-07
Iter: 799 loss: 6.94089124e-07
Iter: 800 loss: 6.94324456e-07
Iter: 801 loss: 6.94063033e-07
Iter: 802 loss: 6.93972822e-07
Iter: 803 loss: 6.93887614e-07
Iter: 804 loss: 6.93867037e-07
Iter: 805 loss: 6.93754657e-07
Iter: 806 loss: 6.94256869e-07
Iter: 807 loss: 6.93731181e-07
Iter: 808 loss: 6.93632e-07
Iter: 809 loss: 6.9360965e-07
Iter: 810 loss: 6.93529955e-07
Iter: 811 loss: 6.93362722e-07
Iter: 812 loss: 6.94185417e-07
Iter: 813 loss: 6.93310483e-07
Iter: 814 loss: 6.9322158e-07
Iter: 815 loss: 6.93241645e-07
Iter: 816 loss: 6.93168033e-07
Iter: 817 loss: 6.92974652e-07
Iter: 818 loss: 6.96295785e-07
Iter: 819 loss: 6.92980905e-07
Iter: 820 loss: 6.92849312e-07
Iter: 821 loss: 6.94910625e-07
Iter: 822 loss: 6.92860738e-07
Iter: 823 loss: 6.92759045e-07
Iter: 824 loss: 6.92666163e-07
Iter: 825 loss: 6.92659114e-07
Iter: 826 loss: 6.92514845e-07
Iter: 827 loss: 6.93986181e-07
Iter: 828 loss: 6.92530477e-07
Iter: 829 loss: 6.92433218e-07
Iter: 830 loss: 6.92876768e-07
Iter: 831 loss: 6.92408889e-07
Iter: 832 loss: 6.92344315e-07
Iter: 833 loss: 6.92206527e-07
Iter: 834 loss: 6.9444377e-07
Iter: 835 loss: 6.92216418e-07
Iter: 836 loss: 6.92084313e-07
Iter: 837 loss: 6.93483912e-07
Iter: 838 loss: 6.92094e-07
Iter: 839 loss: 6.92047365e-07
Iter: 840 loss: 6.92006e-07
Iter: 841 loss: 6.91924129e-07
Iter: 842 loss: 6.91799414e-07
Iter: 843 loss: 6.91687887e-07
Iter: 844 loss: 6.91675609e-07
Iter: 845 loss: 6.91480182e-07
Iter: 846 loss: 6.93085781e-07
Iter: 847 loss: 6.9148922e-07
Iter: 848 loss: 6.91381672e-07
Iter: 849 loss: 6.91719151e-07
Iter: 850 loss: 6.91334549e-07
Iter: 851 loss: 6.91243372e-07
Iter: 852 loss: 6.91722107e-07
Iter: 853 loss: 6.91198466e-07
Iter: 854 loss: 6.91114394e-07
Iter: 855 loss: 6.91330399e-07
Iter: 856 loss: 6.91104617e-07
Iter: 857 loss: 6.91015316e-07
Iter: 858 loss: 6.91026e-07
Iter: 859 loss: 6.90927664e-07
Iter: 860 loss: 6.90822503e-07
Iter: 861 loss: 6.91139576e-07
Iter: 862 loss: 6.908366e-07
Iter: 863 loss: 6.9083211e-07
Iter: 864 loss: 6.90802835e-07
Iter: 865 loss: 6.90805905e-07
Iter: 866 loss: 6.90793513e-07
Iter: 867 loss: 6.90792319e-07
Iter: 868 loss: 6.90771117e-07
Iter: 869 loss: 6.9078078e-07
Iter: 870 loss: 6.90792149e-07
Iter: 871 loss: 6.90797606e-07
Iter: 872 loss: 6.90793058e-07
Iter: 873 loss: 6.90793797e-07
Iter: 874 loss: 6.90797606e-07
Iter: 875 loss: 6.90799482e-07
Iter: 876 loss: 6.90796924e-07
Iter: 877 loss: 6.90803859e-07
Iter: 878 loss: 6.90810964e-07
Iter: 879 loss: 6.9080437e-07
Iter: 880 loss: 6.9079988e-07
Iter: 881 loss: 6.9080437e-07
Iter: 882 loss: 6.90800789e-07
Iter: 883 loss: 6.90801471e-07
Iter: 884 loss: 6.90802267e-07
Iter: 885 loss: 6.908042e-07
Iter: 886 loss: 6.90802267e-07
Iter: 887 loss: 6.90804541e-07
Iter: 888 loss: 6.90804541e-07
Iter: 889 loss: 6.90804541e-07
Iter: 890 loss: 6.90802267e-07
Iter: 891 loss: 6.90597403e-07
Iter: 892 loss: 6.92971184e-07
Iter: 893 loss: 6.90553634e-07
Iter: 894 loss: 6.9053732e-07
Iter: 895 loss: 6.9053192e-07
Iter: 896 loss: 6.90478487e-07
Iter: 897 loss: 6.9031978e-07
Iter: 898 loss: 6.91724324e-07
Iter: 899 loss: 6.90329e-07
Iter: 900 loss: 6.90203933e-07
Iter: 901 loss: 6.90492129e-07
Iter: 902 loss: 6.90133049e-07
Iter: 903 loss: 6.90016464e-07
Iter: 904 loss: 6.90223374e-07
Iter: 905 loss: 6.89994806e-07
Iter: 906 loss: 6.89936826e-07
Iter: 907 loss: 6.89915396e-07
Iter: 908 loss: 6.89853039e-07
Iter: 909 loss: 6.89836156e-07
Iter: 910 loss: 6.89812396e-07
Iter: 911 loss: 6.89704507e-07
Iter: 912 loss: 6.89924207e-07
Iter: 913 loss: 6.8967654e-07
Iter: 914 loss: 6.89566036e-07
Iter: 915 loss: 6.89778574e-07
Iter: 916 loss: 6.89545857e-07
Iter: 917 loss: 6.89470767e-07
Iter: 918 loss: 6.89731e-07
Iter: 919 loss: 6.89420347e-07
Iter: 920 loss: 6.89353215e-07
Iter: 921 loss: 6.89632429e-07
Iter: 922 loss: 6.89295177e-07
Iter: 923 loss: 6.89232422e-07
Iter: 924 loss: 6.89242711e-07
Iter: 925 loss: 6.89239357e-07
Iter: 926 loss: 6.89141302e-07
Iter: 927 loss: 6.89033243e-07
Iter: 928 loss: 6.91814591e-07
Iter: 929 loss: 6.89019885e-07
Iter: 930 loss: 6.88936098e-07
Iter: 931 loss: 6.89566889e-07
Iter: 932 loss: 6.8890995e-07
Iter: 933 loss: 6.88808768e-07
Iter: 934 loss: 6.90005663e-07
Iter: 935 loss: 6.88837076e-07
Iter: 936 loss: 6.88738396e-07
Iter: 937 loss: 6.88816215e-07
Iter: 938 loss: 6.88690648e-07
Iter: 939 loss: 6.88632838e-07
Iter: 940 loss: 6.88488228e-07
Iter: 941 loss: 6.90576371e-07
Iter: 942 loss: 6.88463956e-07
Iter: 943 loss: 6.88482544e-07
Iter: 944 loss: 6.88435421e-07
Iter: 945 loss: 6.88366697e-07
Iter: 946 loss: 6.88522846e-07
Iter: 947 loss: 6.88321052e-07
Iter: 948 loss: 6.88259433e-07
Iter: 949 loss: 6.8824562e-07
Iter: 950 loss: 6.88215209e-07
Iter: 951 loss: 6.88142336e-07
Iter: 952 loss: 6.88883802e-07
Iter: 953 loss: 6.88103626e-07
Iter: 954 loss: 6.8807077e-07
Iter: 955 loss: 6.88415753e-07
Iter: 956 loss: 6.88082309e-07
Iter: 957 loss: 6.8800091e-07
Iter: 958 loss: 6.88222599e-07
Iter: 959 loss: 6.88025352e-07
Iter: 960 loss: 6.87958732e-07
Iter: 961 loss: 6.87821057e-07
Iter: 962 loss: 6.88624652e-07
Iter: 963 loss: 6.8782424e-07
Iter: 964 loss: 6.8768469e-07
Iter: 965 loss: 6.88595378e-07
Iter: 966 loss: 6.87671275e-07
Iter: 967 loss: 6.87609599e-07
Iter: 968 loss: 6.88019e-07
Iter: 969 loss: 6.87561453e-07
Iter: 970 loss: 6.87500915e-07
Iter: 971 loss: 6.8742213e-07
Iter: 972 loss: 6.87418151e-07
Iter: 973 loss: 6.87349313e-07
Iter: 974 loss: 6.87338968e-07
Iter: 975 loss: 6.87290083e-07
Iter: 976 loss: 6.87391662e-07
Iter: 977 loss: 6.87253532e-07
Iter: 978 loss: 6.87191e-07
Iter: 979 loss: 6.871403e-07
Iter: 980 loss: 6.8706936e-07
Iter: 981 loss: 6.87057423e-07
Iter: 982 loss: 6.87037698e-07
Iter: 983 loss: 6.86994383e-07
Iter: 984 loss: 6.86923102e-07
Iter: 985 loss: 6.86918497e-07
Iter: 986 loss: 6.86800377e-07
Iter: 987 loss: 6.86971362e-07
Iter: 988 loss: 6.8680265e-07
Iter: 989 loss: 6.86710109e-07
Iter: 990 loss: 6.87567933e-07
Iter: 991 loss: 6.86709541e-07
Iter: 992 loss: 6.86629676e-07
Iter: 993 loss: 6.86765702e-07
Iter: 994 loss: 6.86616e-07
Iter: 995 loss: 6.86498311e-07
Iter: 996 loss: 6.86635133e-07
Iter: 997 loss: 6.86467729e-07
Iter: 998 loss: 6.86402132e-07
Iter: 999 loss: 6.86337785e-07
Iter: 1000 loss: 6.86356429e-07
Iter: 1001 loss: 6.8627088e-07
Iter: 1002 loss: 6.87048555e-07
Iter: 1003 loss: 6.86243482e-07
Iter: 1004 loss: 6.8614429e-07
Iter: 1005 loss: 6.86137241e-07
Iter: 1006 loss: 6.8611871e-07
Iter: 1007 loss: 6.85976033e-07
Iter: 1008 loss: 6.87418606e-07
Iter: 1009 loss: 6.85997634e-07
Iter: 1010 loss: 6.85928399e-07
Iter: 1011 loss: 6.86214889e-07
Iter: 1012 loss: 6.85922885e-07
Iter: 1013 loss: 6.85847738e-07
Iter: 1014 loss: 6.85749569e-07
Iter: 1015 loss: 6.85744453e-07
Iter: 1016 loss: 6.85622e-07
Iter: 1017 loss: 6.86604267e-07
Iter: 1018 loss: 6.85633324e-07
Iter: 1019 loss: 6.8558063e-07
Iter: 1020 loss: 6.86007411e-07
Iter: 1021 loss: 6.85558462e-07
Iter: 1022 loss: 6.85495252e-07
Iter: 1023 loss: 6.85403677e-07
Iter: 1024 loss: 6.85407144e-07
Iter: 1025 loss: 6.8541317e-07
Iter: 1026 loss: 6.85376676e-07
Iter: 1027 loss: 6.85334271e-07
Iter: 1028 loss: 6.85261568e-07
Iter: 1029 loss: 6.86855401e-07
Iter: 1030 loss: 6.85234681e-07
Iter: 1031 loss: 6.85182158e-07
Iter: 1032 loss: 6.85321766e-07
Iter: 1033 loss: 6.85158625e-07
Iter: 1034 loss: 6.85080295e-07
Iter: 1035 loss: 6.85435793e-07
Iter: 1036 loss: 6.85038742e-07
Iter: 1037 loss: 6.85011e-07
Iter: 1038 loss: 6.84847464e-07
Iter: 1039 loss: 6.87763077e-07
Iter: 1040 loss: 6.84856673e-07
Iter: 1041 loss: 6.84760494e-07
Iter: 1042 loss: 6.86146961e-07
Iter: 1043 loss: 6.84728775e-07
Iter: 1044 loss: 6.8465306e-07
Iter: 1045 loss: 6.85361329e-07
Iter: 1046 loss: 6.84633676e-07
Iter: 1047 loss: 6.84564668e-07
Iter: 1048 loss: 6.84566771e-07
Iter: 1049 loss: 6.84501913e-07
Iter: 1050 loss: 6.84434781e-07
Iter: 1051 loss: 6.85064322e-07
Iter: 1052 loss: 6.84403858e-07
Iter: 1053 loss: 6.84387487e-07
Iter: 1054 loss: 6.84459565e-07
Iter: 1055 loss: 6.84315864e-07
Iter: 1056 loss: 6.84273402e-07
Iter: 1057 loss: 6.84524309e-07
Iter: 1058 loss: 6.84271754e-07
Iter: 1059 loss: 6.84188194e-07
Iter: 1060 loss: 6.84231054e-07
Iter: 1061 loss: 6.84170232e-07
Iter: 1062 loss: 6.84141412e-07
Iter: 1063 loss: 6.84101906e-07
Iter: 1064 loss: 6.8408184e-07
Iter: 1065 loss: 6.84023291e-07
Iter: 1066 loss: 6.84548581e-07
Iter: 1067 loss: 6.84009649e-07
Iter: 1068 loss: 6.83932399e-07
Iter: 1069 loss: 6.83977873e-07
Iter: 1070 loss: 6.83880103e-07
Iter: 1071 loss: 6.83780513e-07
Iter: 1072 loss: 6.84667839e-07
Iter: 1073 loss: 6.8377426e-07
Iter: 1074 loss: 6.83689734e-07
Iter: 1075 loss: 6.83649432e-07
Iter: 1076 loss: 6.83598955e-07
Iter: 1077 loss: 6.83465601e-07
Iter: 1078 loss: 6.83496921e-07
Iter: 1079 loss: 6.83387384e-07
Iter: 1080 loss: 6.83280405e-07
Iter: 1081 loss: 6.83959513e-07
Iter: 1082 loss: 6.83258918e-07
Iter: 1083 loss: 6.83191047e-07
Iter: 1084 loss: 6.84338602e-07
Iter: 1085 loss: 6.83196106e-07
Iter: 1086 loss: 6.83152621e-07
Iter: 1087 loss: 6.83098506e-07
Iter: 1088 loss: 6.83063945e-07
Iter: 1089 loss: 6.8301847e-07
Iter: 1090 loss: 6.83031317e-07
Iter: 1091 loss: 6.82954692e-07
Iter: 1092 loss: 6.82892676e-07
Iter: 1093 loss: 6.84078202e-07
Iter: 1094 loss: 6.82895916e-07
Iter: 1095 loss: 6.82803147e-07
Iter: 1096 loss: 6.8299687e-07
Iter: 1097 loss: 6.8283515e-07
Iter: 1098 loss: 6.82766256e-07
Iter: 1099 loss: 6.83240728e-07
Iter: 1100 loss: 6.82747896e-07
Iter: 1101 loss: 6.82744485e-07
Iter: 1102 loss: 6.82632958e-07
Iter: 1103 loss: 6.82771542e-07
Iter: 1104 loss: 6.82604195e-07
Iter: 1105 loss: 6.82517907e-07
Iter: 1106 loss: 6.8367774e-07
Iter: 1107 loss: 6.82524956e-07
Iter: 1108 loss: 6.82445318e-07
Iter: 1109 loss: 6.83049507e-07
Iter: 1110 loss: 6.82416839e-07
Iter: 1111 loss: 6.82385803e-07
Iter: 1112 loss: 6.82330779e-07
Iter: 1113 loss: 6.82310429e-07
Iter: 1114 loss: 6.82231814e-07
Iter: 1115 loss: 6.82421e-07
Iter: 1116 loss: 6.82219365e-07
Iter: 1117 loss: 6.821341e-07
Iter: 1118 loss: 6.82783877e-07
Iter: 1119 loss: 6.8214365e-07
Iter: 1120 loss: 6.82109203e-07
Iter: 1121 loss: 6.82117843e-07
Iter: 1122 loss: 6.8208567e-07
Iter: 1123 loss: 6.81967e-07
Iter: 1124 loss: 6.82168775e-07
Iter: 1125 loss: 6.81960501e-07
Iter: 1126 loss: 6.81874781e-07
Iter: 1127 loss: 6.81888423e-07
Iter: 1128 loss: 6.81820836e-07
Iter: 1129 loss: 6.81741369e-07
Iter: 1130 loss: 6.82254949e-07
Iter: 1131 loss: 6.81745519e-07
Iter: 1132 loss: 6.81646725e-07
Iter: 1133 loss: 6.8247823e-07
Iter: 1134 loss: 6.81670144e-07
Iter: 1135 loss: 6.81612732e-07
Iter: 1136 loss: 6.82011546e-07
Iter: 1137 loss: 6.8162e-07
Iter: 1138 loss: 6.81589e-07
Iter: 1139 loss: 6.81480628e-07
Iter: 1140 loss: 6.82048551e-07
Iter: 1141 loss: 6.81437598e-07
Iter: 1142 loss: 6.81361371e-07
Iter: 1143 loss: 6.81578911e-07
Iter: 1144 loss: 6.81308e-07
Iter: 1145 loss: 6.81210338e-07
Iter: 1146 loss: 6.81710958e-07
Iter: 1147 loss: 6.81192e-07
Iter: 1148 loss: 6.81155882e-07
Iter: 1149 loss: 6.81844085e-07
Iter: 1150 loss: 6.81121605e-07
Iter: 1151 loss: 6.81070105e-07
Iter: 1152 loss: 6.81061692e-07
Iter: 1153 loss: 6.810061e-07
Iter: 1154 loss: 6.8096324e-07
Iter: 1155 loss: 6.81078859e-07
Iter: 1156 loss: 6.80927258e-07
Iter: 1157 loss: 6.80857625e-07
Iter: 1158 loss: 6.81306801e-07
Iter: 1159 loss: 6.80859671e-07
Iter: 1160 loss: 6.80746211e-07
Iter: 1161 loss: 6.80873313e-07
Iter: 1162 loss: 6.80705455e-07
Iter: 1163 loss: 6.8064071e-07
Iter: 1164 loss: 6.80884341e-07
Iter: 1165 loss: 6.80640142e-07
Iter: 1166 loss: 6.80529752e-07
Iter: 1167 loss: 6.80556241e-07
Iter: 1168 loss: 6.80528331e-07
Iter: 1169 loss: 6.80452729e-07
Iter: 1170 loss: 6.80449773e-07
Iter: 1171 loss: 6.80390485e-07
Iter: 1172 loss: 6.80336711e-07
Iter: 1173 loss: 6.80294363e-07
Iter: 1174 loss: 6.80227e-07
Iter: 1175 loss: 6.80555786e-07
Iter: 1176 loss: 6.80218136e-07
Iter: 1177 loss: 6.80181813e-07
Iter: 1178 loss: 6.80075e-07
Iter: 1179 loss: 6.80042888e-07
Iter: 1180 loss: 6.8003277e-07
Iter: 1181 loss: 6.80327389e-07
Iter: 1182 loss: 6.8002862e-07
Iter: 1183 loss: 6.79909817e-07
Iter: 1184 loss: 6.80299706e-07
Iter: 1185 loss: 6.79893219e-07
Iter: 1186 loss: 6.79817049e-07
Iter: 1187 loss: 6.79823358e-07
Iter: 1188 loss: 6.79753214e-07
Iter: 1189 loss: 6.79672439e-07
Iter: 1190 loss: 6.79776463e-07
Iter: 1191 loss: 6.7957842e-07
Iter: 1192 loss: 6.79553125e-07
Iter: 1193 loss: 6.79516859e-07
Iter: 1194 loss: 6.79484231e-07
Iter: 1195 loss: 6.79391633e-07
Iter: 1196 loss: 6.79404138e-07
Iter: 1197 loss: 6.79235427e-07
Iter: 1198 loss: 6.79451659e-07
Iter: 1199 loss: 6.79216043e-07
Iter: 1200 loss: 6.79125378e-07
Iter: 1201 loss: 6.80320909e-07
Iter: 1202 loss: 6.79138e-07
Iter: 1203 loss: 6.79070922e-07
Iter: 1204 loss: 6.7970791e-07
Iter: 1205 loss: 6.79078312e-07
Iter: 1206 loss: 6.79007e-07
Iter: 1207 loss: 6.78942115e-07
Iter: 1208 loss: 6.78950528e-07
Iter: 1209 loss: 6.7887305e-07
Iter: 1210 loss: 6.79329e-07
Iter: 1211 loss: 6.78877655e-07
Iter: 1212 loss: 6.78800802e-07
Iter: 1213 loss: 6.78763229e-07
Iter: 1214 loss: 6.78761523e-07
Iter: 1215 loss: 6.78663582e-07
Iter: 1216 loss: 6.78881463e-07
Iter: 1217 loss: 6.78631409e-07
Iter: 1218 loss: 6.78538e-07
Iter: 1219 loss: 6.78998219e-07
Iter: 1220 loss: 6.78520053e-07
Iter: 1221 loss: 6.78417791e-07
Iter: 1222 loss: 6.78354e-07
Iter: 1223 loss: 6.78362198e-07
Iter: 1224 loss: 6.78261131e-07
Iter: 1225 loss: 6.78916649e-07
Iter: 1226 loss: 6.7821577e-07
Iter: 1227 loss: 6.78136757e-07
Iter: 1228 loss: 6.7900703e-07
Iter: 1229 loss: 6.78110268e-07
Iter: 1230 loss: 6.78040465e-07
Iter: 1231 loss: 6.78080653e-07
Iter: 1232 loss: 6.78005279e-07
Iter: 1233 loss: 6.77936498e-07
Iter: 1234 loss: 6.77910748e-07
Iter: 1235 loss: 6.77833782e-07
Iter: 1236 loss: 6.7786948e-07
Iter: 1237 loss: 6.77795299e-07
Iter: 1238 loss: 6.77767844e-07
Iter: 1239 loss: 6.77796663e-07
Iter: 1240 loss: 6.77746698e-07
Iter: 1241 loss: 6.77682692e-07
Iter: 1242 loss: 6.77752e-07
Iter: 1243 loss: 6.77694288e-07
Iter: 1244 loss: 6.77638297e-07
Iter: 1245 loss: 6.77620108e-07
Iter: 1246 loss: 6.77577077e-07
Iter: 1247 loss: 6.77504318e-07
Iter: 1248 loss: 6.77563037e-07
Iter: 1249 loss: 6.77456228e-07
Iter: 1250 loss: 6.77355729e-07
Iter: 1251 loss: 6.77562923e-07
Iter: 1252 loss: 6.77319576e-07
Iter: 1253 loss: 6.77194123e-07
Iter: 1254 loss: 6.78056153e-07
Iter: 1255 loss: 6.77204582e-07
Iter: 1256 loss: 6.77086916e-07
Iter: 1257 loss: 6.76992499e-07
Iter: 1258 loss: 6.76960212e-07
Iter: 1259 loss: 6.76853233e-07
Iter: 1260 loss: 6.77832304e-07
Iter: 1261 loss: 6.76864147e-07
Iter: 1262 loss: 6.76786726e-07
Iter: 1263 loss: 6.77511707e-07
Iter: 1264 loss: 6.7675569e-07
Iter: 1265 loss: 6.76704644e-07
Iter: 1266 loss: 6.76601e-07
Iter: 1267 loss: 6.76589e-07
Iter: 1268 loss: 6.76476418e-07
Iter: 1269 loss: 6.77229195e-07
Iter: 1270 loss: 6.76471814e-07
Iter: 1271 loss: 6.76439754e-07
Iter: 1272 loss: 6.76463401e-07
Iter: 1273 loss: 6.76422644e-07
Iter: 1274 loss: 6.76391892e-07
Iter: 1275 loss: 6.76406387e-07
Iter: 1276 loss: 6.76305945e-07
Iter: 1277 loss: 6.76652462e-07
Iter: 1278 loss: 6.7631629e-07
Iter: 1279 loss: 6.76269053e-07
Iter: 1280 loss: 6.76247794e-07
Iter: 1281 loss: 6.7620033e-07
Iter: 1282 loss: 6.76123932e-07
Iter: 1283 loss: 6.7615133e-07
Iter: 1284 loss: 6.76086756e-07
Iter: 1285 loss: 6.76015475e-07
Iter: 1286 loss: 6.76409513e-07
Iter: 1287 loss: 6.75987053e-07
Iter: 1288 loss: 6.75869501e-07
Iter: 1289 loss: 6.7603537e-07
Iter: 1290 loss: 6.75831359e-07
Iter: 1291 loss: 6.75709316e-07
Iter: 1292 loss: 6.75819251e-07
Iter: 1293 loss: 6.7565918e-07
Iter: 1294 loss: 6.75521733e-07
Iter: 1295 loss: 6.75570561e-07
Iter: 1296 loss: 6.75433625e-07
Iter: 1297 loss: 6.75359843e-07
Iter: 1298 loss: 6.75373144e-07
Iter: 1299 loss: 6.75284582e-07
Iter: 1300 loss: 6.75352737e-07
Iter: 1301 loss: 6.75253546e-07
Iter: 1302 loss: 6.75209378e-07
Iter: 1303 loss: 6.75215233e-07
Iter: 1304 loss: 6.75163619e-07
Iter: 1305 loss: 6.75078809e-07
Iter: 1306 loss: 6.75082674e-07
Iter: 1307 loss: 6.75050842e-07
Iter: 1308 loss: 6.75028616e-07
Iter: 1309 loss: 6.75007641e-07
Iter: 1310 loss: 6.74943749e-07
Iter: 1311 loss: 6.75034e-07
Iter: 1312 loss: 6.74934881e-07
Iter: 1313 loss: 6.74852117e-07
Iter: 1314 loss: 6.75016e-07
Iter: 1315 loss: 6.7481858e-07
Iter: 1316 loss: 6.7475878e-07
Iter: 1317 loss: 6.74687385e-07
Iter: 1318 loss: 6.74661123e-07
Iter: 1319 loss: 6.74592911e-07
Iter: 1320 loss: 6.7501486e-07
Iter: 1321 loss: 6.74577848e-07
Iter: 1322 loss: 6.74505827e-07
Iter: 1323 loss: 6.74690227e-07
Iter: 1324 loss: 6.74485818e-07
Iter: 1325 loss: 6.74398393e-07
Iter: 1326 loss: 6.74789931e-07
Iter: 1327 loss: 6.74395e-07
Iter: 1328 loss: 6.74320518e-07
Iter: 1329 loss: 6.74282205e-07
Iter: 1330 loss: 6.74272655e-07
Iter: 1331 loss: 6.74171304e-07
Iter: 1332 loss: 6.74928629e-07
Iter: 1333 loss: 6.74127307e-07
Iter: 1334 loss: 6.74077683e-07
Iter: 1335 loss: 6.74816306e-07
Iter: 1336 loss: 6.74090529e-07
Iter: 1337 loss: 6.74040564e-07
Iter: 1338 loss: 6.74011289e-07
Iter: 1339 loss: 6.7397832e-07
Iter: 1340 loss: 6.73912723e-07
Iter: 1341 loss: 6.73937848e-07
Iter: 1342 loss: 6.73813929e-07
Iter: 1343 loss: 6.73779766e-07
Iter: 1344 loss: 6.73764816e-07
Iter: 1345 loss: 6.736974e-07
Iter: 1346 loss: 6.73712748e-07
Iter: 1347 loss: 6.73701322e-07
Iter: 1348 loss: 6.73588318e-07
Iter: 1349 loss: 6.73896523e-07
Iter: 1350 loss: 6.73593831e-07
Iter: 1351 loss: 6.73518e-07
Iter: 1352 loss: 6.73481964e-07
Iter: 1353 loss: 6.73466332e-07
Iter: 1354 loss: 6.73356794e-07
Iter: 1355 loss: 6.73396244e-07
Iter: 1356 loss: 6.73323086e-07
Iter: 1357 loss: 6.73206557e-07
Iter: 1358 loss: 6.73484067e-07
Iter: 1359 loss: 6.73179102e-07
Iter: 1360 loss: 6.73064392e-07
Iter: 1361 loss: 6.73480201e-07
Iter: 1362 loss: 6.73045861e-07
Iter: 1363 loss: 6.72911e-07
Iter: 1364 loss: 6.735238e-07
Iter: 1365 loss: 6.72902104e-07
Iter: 1366 loss: 6.72848842e-07
Iter: 1367 loss: 6.72744e-07
Iter: 1368 loss: 6.72756528e-07
Iter: 1369 loss: 6.72670751e-07
Iter: 1370 loss: 6.73288696e-07
Iter: 1371 loss: 6.72651538e-07
Iter: 1372 loss: 6.72540409e-07
Iter: 1373 loss: 6.7327278e-07
Iter: 1374 loss: 6.72528813e-07
Iter: 1375 loss: 6.72387216e-07
Iter: 1376 loss: 6.72714862e-07
Iter: 1377 loss: 6.72369708e-07
Iter: 1378 loss: 6.72283647e-07
Iter: 1379 loss: 6.72767783e-07
Iter: 1380 loss: 6.72293481e-07
Iter: 1381 loss: 6.72215947e-07
Iter: 1382 loss: 6.72130341e-07
Iter: 1383 loss: 6.72116471e-07
Iter: 1384 loss: 6.72035412e-07
Iter: 1385 loss: 6.72113856e-07
Iter: 1386 loss: 6.72007047e-07
Iter: 1387 loss: 6.71913e-07
Iter: 1388 loss: 6.72078784e-07
Iter: 1389 loss: 6.7186545e-07
Iter: 1390 loss: 6.71811449e-07
Iter: 1391 loss: 6.72086571e-07
Iter: 1392 loss: 6.71781549e-07
Iter: 1393 loss: 6.71728969e-07
Iter: 1394 loss: 6.71712769e-07
Iter: 1395 loss: 6.71683324e-07
Iter: 1396 loss: 6.71630914e-07
Iter: 1397 loss: 6.71638759e-07
Iter: 1398 loss: 6.71559633e-07
Iter: 1399 loss: 6.71738462e-07
Iter: 1400 loss: 6.71534792e-07
Iter: 1401 loss: 6.71466353e-07
Iter: 1402 loss: 6.72179e-07
Iter: 1403 loss: 6.71469365e-07
Iter: 1404 loss: 6.71428495e-07
Iter: 1405 loss: 6.71342434e-07
Iter: 1406 loss: 6.73428247e-07
Iter: 1407 loss: 6.71308385e-07
Iter: 1408 loss: 6.71259386e-07
Iter: 1409 loss: 6.71685e-07
Iter: 1410 loss: 6.71218459e-07
Iter: 1411 loss: 6.71151327e-07
Iter: 1412 loss: 6.71673888e-07
Iter: 1413 loss: 6.711482e-07
Iter: 1414 loss: 6.71113639e-07
Iter: 1415 loss: 6.71640407e-07
Iter: 1416 loss: 6.71108296e-07
Iter: 1417 loss: 6.71033945e-07
Iter: 1418 loss: 6.71340388e-07
Iter: 1419 loss: 6.71024907e-07
Iter: 1420 loss: 6.70990687e-07
Iter: 1421 loss: 6.70923157e-07
Iter: 1422 loss: 6.71860562e-07
Iter: 1423 loss: 6.70895474e-07
Iter: 1424 loss: 6.70791849e-07
Iter: 1425 loss: 6.71178668e-07
Iter: 1426 loss: 6.70789746e-07
Iter: 1427 loss: 6.70698228e-07
Iter: 1428 loss: 6.70810209e-07
Iter: 1429 loss: 6.7065713e-07
Iter: 1430 loss: 6.70575332e-07
Iter: 1431 loss: 6.71294401e-07
Iter: 1432 loss: 6.70555096e-07
Iter: 1433 loss: 6.70509849e-07
Iter: 1434 loss: 6.71033433e-07
Iter: 1435 loss: 6.70500754e-07
Iter: 1436 loss: 6.70437e-07
Iter: 1437 loss: 6.70271504e-07
Iter: 1438 loss: 6.71424345e-07
Iter: 1439 loss: 6.70305951e-07
Iter: 1440 loss: 6.70227109e-07
Iter: 1441 loss: 6.70238819e-07
Iter: 1442 loss: 6.701909e-07
Iter: 1443 loss: 6.70221311e-07
Iter: 1444 loss: 6.70115071e-07
Iter: 1445 loss: 6.70060672e-07
Iter: 1446 loss: 6.70162422e-07
Iter: 1447 loss: 6.70049872e-07
Iter: 1448 loss: 6.69978931e-07
Iter: 1449 loss: 6.70043221e-07
Iter: 1450 loss: 6.69929705e-07
Iter: 1451 loss: 6.6987775e-07
Iter: 1452 loss: 6.69878773e-07
Iter: 1453 loss: 6.69832161e-07
Iter: 1454 loss: 6.69905376e-07
Iter: 1455 loss: 6.69807605e-07
Iter: 1456 loss: 6.69759345e-07
Iter: 1457 loss: 6.69828466e-07
Iter: 1458 loss: 6.69727569e-07
Iter: 1459 loss: 6.69679e-07
Iter: 1460 loss: 6.69577787e-07
Iter: 1461 loss: 6.69573467e-07
Iter: 1462 loss: 6.69485644e-07
Iter: 1463 loss: 6.70097506e-07
Iter: 1464 loss: 6.69459553e-07
Iter: 1465 loss: 6.69394922e-07
Iter: 1466 loss: 6.69458132e-07
Iter: 1467 loss: 6.69352914e-07
Iter: 1468 loss: 6.69222288e-07
Iter: 1469 loss: 6.6986496e-07
Iter: 1470 loss: 6.69228598e-07
Iter: 1471 loss: 6.69177837e-07
Iter: 1472 loss: 6.69346321e-07
Iter: 1473 loss: 6.69151746e-07
Iter: 1474 loss: 6.69055339e-07
Iter: 1475 loss: 6.69323697e-07
Iter: 1476 loss: 6.69049086e-07
Iter: 1477 loss: 6.68992e-07
Iter: 1478 loss: 6.69039082e-07
Iter: 1479 loss: 6.68936366e-07
Iter: 1480 loss: 6.68904931e-07
Iter: 1481 loss: 6.69108886e-07
Iter: 1482 loss: 6.6888623e-07
Iter: 1483 loss: 6.68810458e-07
Iter: 1484 loss: 6.68915902e-07
Iter: 1485 loss: 6.68780558e-07
Iter: 1486 loss: 6.68718144e-07
Iter: 1487 loss: 6.69045e-07
Iter: 1488 loss: 6.68689893e-07
Iter: 1489 loss: 6.68586551e-07
Iter: 1490 loss: 6.68916755e-07
Iter: 1491 loss: 6.68579673e-07
Iter: 1492 loss: 6.6856137e-07
Iter: 1493 loss: 6.68553639e-07
Iter: 1494 loss: 6.68537211e-07
Iter: 1495 loss: 6.68480766e-07
Iter: 1496 loss: 6.68589109e-07
Iter: 1497 loss: 6.68440748e-07
Iter: 1498 loss: 6.68390157e-07
Iter: 1499 loss: 6.68379869e-07
Iter: 1500 loss: 6.68332177e-07
Iter: 1501 loss: 6.68272037e-07
Iter: 1502 loss: 6.68308758e-07
Iter: 1503 loss: 6.68227472e-07
Iter: 1504 loss: 6.68137602e-07
Iter: 1505 loss: 6.68514531e-07
Iter: 1506 loss: 6.68108e-07
Iter: 1507 loss: 6.68053417e-07
Iter: 1508 loss: 6.68357757e-07
Iter: 1509 loss: 6.68065866e-07
Iter: 1510 loss: 6.67976451e-07
Iter: 1511 loss: 6.68270332e-07
Iter: 1512 loss: 6.67982e-07
Iter: 1513 loss: 6.67931658e-07
Iter: 1514 loss: 6.67948939e-07
Iter: 1515 loss: 6.67908182e-07
Iter: 1516 loss: 6.67842301e-07
Iter: 1517 loss: 6.67878851e-07
Iter: 1518 loss: 6.67836957e-07
Iter: 1519 loss: 6.6773049e-07
Iter: 1520 loss: 6.68240261e-07
Iter: 1521 loss: 6.6771895e-07
Iter: 1522 loss: 6.67711163e-07
Iter: 1523 loss: 6.67653069e-07
Iter: 1524 loss: 6.67657901e-07
Iter: 1525 loss: 6.67627035e-07
Iter: 1526 loss: 6.67590939e-07
Iter: 1527 loss: 6.67598613e-07
Iter: 1528 loss: 6.67613904e-07
Iter: 1529 loss: 6.67621521e-07
Iter: 1530 loss: 6.67594236e-07
Iter: 1531 loss: 6.67588154e-07
Iter: 1532 loss: 6.67585539e-07
Iter: 1533 loss: 6.6758605e-07
Iter: 1534 loss: 6.67588608e-07
Iter: 1535 loss: 6.67590541e-07
Iter: 1536 loss: 6.67589e-07
Iter: 1537 loss: 6.67600204e-07
Iter: 1538 loss: 6.67588665e-07
Iter: 1539 loss: 6.67594463e-07
Iter: 1540 loss: 6.67587528e-07
Iter: 1541 loss: 6.6759219e-07
Iter: 1542 loss: 6.67586164e-07
Iter: 1543 loss: 6.67589688e-07
Iter: 1544 loss: 6.675902e-07
Iter: 1545 loss: 6.67592531e-07
Iter: 1546 loss: 6.67590029e-07
Iter: 1547 loss: 6.6759e-07
Iter: 1548 loss: 6.67592644e-07
Iter: 1549 loss: 6.67592644e-07
Iter: 1550 loss: 6.67592644e-07
Iter: 1551 loss: 6.6759e-07
Iter: 1552 loss: 6.67592474e-07
Iter: 1553 loss: 6.67592474e-07
Iter: 1554 loss: 6.6759e-07
Iter: 1555 loss: 6.67592474e-07
Iter: 1556 loss: 6.67453548e-07
Iter: 1557 loss: 6.69630595e-07
Iter: 1558 loss: 6.67447637e-07
Iter: 1559 loss: 6.67400968e-07
Iter: 1560 loss: 6.67812174e-07
Iter: 1561 loss: 6.67383688e-07
Iter: 1562 loss: 6.67336e-07
Iter: 1563 loss: 6.67756751e-07
Iter: 1564 loss: 6.67340032e-07
Iter: 1565 loss: 6.67294898e-07
Iter: 1566 loss: 6.67291772e-07
Iter: 1567 loss: 6.67288305e-07
Iter: 1568 loss: 6.67254483e-07
Iter: 1569 loss: 6.67360666e-07
Iter: 1570 loss: 6.6720753e-07
Iter: 1571 loss: 6.67127779e-07
Iter: 1572 loss: 6.67376412e-07
Iter: 1573 loss: 6.67107656e-07
Iter: 1574 loss: 6.67035238e-07
Iter: 1575 loss: 6.67173083e-07
Iter: 1576 loss: 6.67046322e-07
Iter: 1577 loss: 6.670017e-07
Iter: 1578 loss: 6.67005907e-07
Iter: 1579 loss: 6.66975779e-07
Iter: 1580 loss: 6.66891879e-07
Iter: 1581 loss: 6.67270456e-07
Iter: 1582 loss: 6.66903247e-07
Iter: 1583 loss: 6.66864594e-07
Iter: 1584 loss: 6.67210372e-07
Iter: 1585 loss: 6.66832818e-07
Iter: 1586 loss: 6.66791038e-07
Iter: 1587 loss: 6.66865787e-07
Iter: 1588 loss: 6.66766709e-07
Iter: 1589 loss: 6.66744882e-07
Iter: 1590 loss: 6.66699066e-07
Iter: 1591 loss: 6.66684173e-07
Iter: 1592 loss: 6.66615506e-07
Iter: 1593 loss: 6.66714129e-07
Iter: 1594 loss: 6.66637675e-07
Iter: 1595 loss: 6.66560254e-07
Iter: 1596 loss: 6.66791323e-07
Iter: 1597 loss: 6.66539e-07
Iter: 1598 loss: 6.66488347e-07
Iter: 1599 loss: 6.66979247e-07
Iter: 1600 loss: 6.66489086e-07
Iter: 1601 loss: 6.66452934e-07
Iter: 1602 loss: 6.66347091e-07
Iter: 1603 loss: 6.67596908e-07
Iter: 1604 loss: 6.66377332e-07
Iter: 1605 loss: 6.66309234e-07
Iter: 1606 loss: 6.66311621e-07
Iter: 1607 loss: 6.66248241e-07
Iter: 1608 loss: 6.6626734e-07
Iter: 1609 loss: 6.6620396e-07
Iter: 1610 loss: 6.66156666e-07
Iter: 1611 loss: 6.66239089e-07
Iter: 1612 loss: 6.66130518e-07
Iter: 1613 loss: 6.66063897e-07
Iter: 1614 loss: 6.66340839e-07
Iter: 1615 loss: 6.66066114e-07
Iter: 1616 loss: 6.65996652e-07
Iter: 1617 loss: 6.66203846e-07
Iter: 1618 loss: 6.65975222e-07
Iter: 1619 loss: 6.65917185e-07
Iter: 1620 loss: 6.65936795e-07
Iter: 1621 loss: 6.65892514e-07
Iter: 1622 loss: 6.65883135e-07
Iter: 1623 loss: 6.65846756e-07
Iter: 1624 loss: 6.65817765e-07
Iter: 1625 loss: 6.65740799e-07
Iter: 1626 loss: 6.65721586e-07
Iter: 1627 loss: 6.65645416e-07
Iter: 1628 loss: 6.66090159e-07
Iter: 1629 loss: 6.65628761e-07
Iter: 1630 loss: 6.65604034e-07
Iter: 1631 loss: 6.65777691e-07
Iter: 1632 loss: 6.65586072e-07
Iter: 1633 loss: 6.65490177e-07
Iter: 1634 loss: 6.65701577e-07
Iter: 1635 loss: 6.65464881e-07
Iter: 1636 loss: 6.65431571e-07
Iter: 1637 loss: 6.65359e-07
Iter: 1638 loss: 6.66763924e-07
Iter: 1639 loss: 6.65342043e-07
Iter: 1640 loss: 6.65308789e-07
Iter: 1641 loss: 6.65280368e-07
Iter: 1642 loss: 6.65231e-07
Iter: 1643 loss: 6.65366883e-07
Iter: 1644 loss: 6.65233244e-07
Iter: 1645 loss: 6.65200787e-07
Iter: 1646 loss: 6.65126777e-07
Iter: 1647 loss: 6.65129392e-07
Iter: 1648 loss: 6.65068455e-07
Iter: 1649 loss: 6.65498817e-07
Iter: 1650 loss: 6.65067205e-07
Iter: 1651 loss: 6.6502e-07
Iter: 1652 loss: 6.65242965e-07
Iter: 1653 loss: 6.65021162e-07
Iter: 1654 loss: 6.6495295e-07
Iter: 1655 loss: 6.64939193e-07
Iter: 1656 loss: 6.64940103e-07
Iter: 1657 loss: 6.64941e-07
Iter: 1658 loss: 6.64954825e-07
Iter: 1659 loss: 6.64951244e-07
Iter: 1660 loss: 6.64957952e-07
Iter: 1661 loss: 6.64953177e-07
Iter: 1662 loss: 6.64942377e-07
Iter: 1663 loss: 6.64944594e-07
Iter: 1664 loss: 6.64942547e-07
Iter: 1665 loss: 6.64950733e-07
Iter: 1666 loss: 6.64948061e-07
Iter: 1667 loss: 6.64934532e-07
Iter: 1668 loss: 6.6494988e-07
Iter: 1669 loss: 6.64945674e-07
Iter: 1670 loss: 6.64938909e-07
Iter: 1671 loss: 6.6494124e-07
Iter: 1672 loss: 6.6494124e-07
Iter: 1673 loss: 6.6494e-07
Iter: 1674 loss: 6.64938796e-07
Iter: 1675 loss: 6.64938625e-07
Iter: 1676 loss: 6.64938625e-07
Iter: 1677 loss: 6.64939535e-07
Iter: 1678 loss: 6.6494e-07
Iter: 1679 loss: 6.6494e-07
Iter: 1680 loss: 6.64939535e-07
Iter: 1681 loss: 6.64939535e-07
Iter: 1682 loss: 6.64939535e-07
Iter: 1683 loss: 6.64939535e-07
Iter: 1684 loss: 6.64939535e-07
Iter: 1685 loss: 6.64939535e-07
Iter: 1686 loss: 6.6494e-07
Iter: 1687 loss: 6.64939535e-07
Iter: 1688 loss: 6.6494e-07
Iter: 1689 loss: 6.64939535e-07
Iter: 1690 loss: 6.64939535e-07
Iter: 1691 loss: 6.6494e-07
Iter: 1692 loss: 6.67398467e-07
Iter: 1693 loss: 6.64936522e-07
Iter: 1694 loss: 6.64865752e-07
Iter: 1695 loss: 6.65533662e-07
Iter: 1696 loss: 6.64872232e-07
Iter: 1697 loss: 6.64806748e-07
Iter: 1698 loss: 6.64878428e-07
Iter: 1699 loss: 6.64762865e-07
Iter: 1700 loss: 6.6468408e-07
Iter: 1701 loss: 6.65464711e-07
Iter: 1702 loss: 6.64689537e-07
Iter: 1703 loss: 6.64614e-07
Iter: 1704 loss: 6.64744e-07
Iter: 1705 loss: 6.64597223e-07
Iter: 1706 loss: 6.64515255e-07
Iter: 1707 loss: 6.6465293e-07
Iter: 1708 loss: 6.64480467e-07
Iter: 1709 loss: 6.64441586e-07
Iter: 1710 loss: 6.64346203e-07
Iter: 1711 loss: 6.6430232e-07
Iter: 1712 loss: 6.64622917e-07
Iter: 1713 loss: 6.64302434e-07
Iter: 1714 loss: 6.64177605e-07
Iter: 1715 loss: 6.64267645e-07
Iter: 1716 loss: 6.64125764e-07
Iter: 1717 loss: 6.64060394e-07
Iter: 1718 loss: 6.64509571e-07
Iter: 1719 loss: 6.64054824e-07
Iter: 1720 loss: 6.64023048e-07
Iter: 1721 loss: 6.64033564e-07
Iter: 1722 loss: 6.63975243e-07
Iter: 1723 loss: 6.63983087e-07
Iter: 1724 loss: 6.63948413e-07
Iter: 1725 loss: 6.63877358e-07
Iter: 1726 loss: 6.64050731e-07
Iter: 1727 loss: 6.63899129e-07
Iter: 1728 loss: 6.63855644e-07
Iter: 1729 loss: 6.63918286e-07
Iter: 1730 loss: 6.63832e-07
Iter: 1731 loss: 6.63800847e-07
Iter: 1732 loss: 6.64254628e-07
Iter: 1733 loss: 6.63810852e-07
Iter: 1734 loss: 6.63736728e-07
Iter: 1735 loss: 6.63691139e-07
Iter: 1736 loss: 6.64710456e-07
Iter: 1737 loss: 6.63693868e-07
Iter: 1738 loss: 6.63642368e-07
Iter: 1739 loss: 6.63646517e-07
Iter: 1740 loss: 6.63596325e-07
Iter: 1741 loss: 6.63625258e-07
Iter: 1742 loss: 6.63564435e-07
Iter: 1743 loss: 6.6353266e-07
Iter: 1744 loss: 6.63473713e-07
Iter: 1745 loss: 6.63459502e-07
Iter: 1746 loss: 6.63409537e-07
Iter: 1747 loss: 6.63646802e-07
Iter: 1748 loss: 6.63379069e-07
Iter: 1749 loss: 6.63312e-07
Iter: 1750 loss: 6.63294941e-07
Iter: 1751 loss: 6.63257595e-07
Iter: 1752 loss: 6.63287324e-07
Iter: 1753 loss: 6.63216724e-07
Iter: 1754 loss: 6.63172e-07
Iter: 1755 loss: 6.6316e-07
Iter: 1756 loss: 6.6316511e-07
Iter: 1757 loss: 6.63124183e-07
Iter: 1758 loss: 6.63057222e-07
Iter: 1759 loss: 6.6305563e-07
Iter: 1760 loss: 6.62984689e-07
Iter: 1761 loss: 6.6369455e-07
Iter: 1762 loss: 6.62981165e-07
Iter: 1763 loss: 6.62920911e-07
Iter: 1764 loss: 6.63285959e-07
Iter: 1765 loss: 6.62905e-07
Iter: 1766 loss: 6.62865887e-07
Iter: 1767 loss: 6.62926141e-07
Iter: 1768 loss: 6.62847469e-07
Iter: 1769 loss: 6.62793695e-07
Iter: 1770 loss: 6.62806e-07
Iter: 1771 loss: 6.62738898e-07
Iter: 1772 loss: 6.62705247e-07
Iter: 1773 loss: 6.63372589e-07
Iter: 1774 loss: 6.62719401e-07
Iter: 1775 loss: 6.62682282e-07
Iter: 1776 loss: 6.62657044e-07
Iter: 1777 loss: 6.6261714e-07
Iter: 1778 loss: 6.62580931e-07
Iter: 1779 loss: 6.62559728e-07
Iter: 1780 loss: 6.62530738e-07
Iter: 1781 loss: 6.62462071e-07
Iter: 1782 loss: 6.6304051e-07
Iter: 1783 loss: 6.62447746e-07
Iter: 1784 loss: 6.62440129e-07
Iter: 1785 loss: 6.6241671e-07
Iter: 1786 loss: 6.62384195e-07
Iter: 1787 loss: 6.62530113e-07
Iter: 1788 loss: 6.6236e-07
Iter: 1789 loss: 6.62329626e-07
Iter: 1790 loss: 6.62280229e-07
Iter: 1791 loss: 6.62796e-07
Iter: 1792 loss: 6.62270622e-07
Iter: 1793 loss: 6.6225482e-07
Iter: 1794 loss: 6.63044034e-07
Iter: 1795 loss: 6.62242542e-07
Iter: 1796 loss: 6.62189109e-07
Iter: 1797 loss: 6.62377488e-07
Iter: 1798 loss: 6.62169043e-07
Iter: 1799 loss: 6.62165633e-07
Iter: 1800 loss: 6.6221719e-07
Iter: 1801 loss: 6.62131697e-07
Iter: 1802 loss: 6.62090542e-07
Iter: 1803 loss: 6.62132607e-07
Iter: 1804 loss: 6.62050297e-07
Iter: 1805 loss: 6.61995784e-07
Iter: 1806 loss: 6.62048478e-07
Iter: 1807 loss: 6.61974184e-07
Iter: 1808 loss: 6.61935132e-07
Iter: 1809 loss: 6.62511468e-07
Iter: 1810 loss: 6.61917397e-07
Iter: 1811 loss: 6.61873287e-07
Iter: 1812 loss: 6.61810532e-07
Iter: 1813 loss: 6.61826959e-07
Iter: 1814 loss: 6.61754825e-07
Iter: 1815 loss: 6.61887441e-07
Iter: 1816 loss: 6.61745958e-07
Iter: 1817 loss: 6.61662739e-07
Iter: 1818 loss: 6.61853846e-07
Iter: 1819 loss: 6.6162e-07
Iter: 1820 loss: 6.61538138e-07
Iter: 1821 loss: 6.61825311e-07
Iter: 1822 loss: 6.61537456e-07
Iter: 1823 loss: 6.6152694e-07
Iter: 1824 loss: 6.61503748e-07
Iter: 1825 loss: 6.61485046e-07
Iter: 1826 loss: 6.61383126e-07
Iter: 1827 loss: 6.62129082e-07
Iter: 1828 loss: 6.61403305e-07
Iter: 1829 loss: 6.6132111e-07
Iter: 1830 loss: 6.61263357e-07
Iter: 1831 loss: 6.61253637e-07
Iter: 1832 loss: 6.61141e-07
Iter: 1833 loss: 6.619e-07
Iter: 1834 loss: 6.61131139e-07
Iter: 1835 loss: 6.61063325e-07
Iter: 1836 loss: 6.61287e-07
Iter: 1837 loss: 6.60999603e-07
Iter: 1838 loss: 6.60949468e-07
Iter: 1839 loss: 6.60956744e-07
Iter: 1840 loss: 6.60891033e-07
Iter: 1841 loss: 6.60837088e-07
Iter: 1842 loss: 6.61620618e-07
Iter: 1843 loss: 6.60811963e-07
Iter: 1844 loss: 6.60791898e-07
Iter: 1845 loss: 6.60769899e-07
Iter: 1846 loss: 6.60722208e-07
Iter: 1847 loss: 6.60675425e-07
Iter: 1848 loss: 6.61146373e-07
Iter: 1849 loss: 6.60686794e-07
Iter: 1850 loss: 6.60639e-07
Iter: 1851 loss: 6.60649903e-07
Iter: 1852 loss: 6.60585556e-07
Iter: 1853 loss: 6.60524847e-07
Iter: 1854 loss: 6.60509954e-07
Iter: 1855 loss: 6.60465616e-07
Iter: 1856 loss: 6.60392288e-07
Iter: 1857 loss: 6.61028e-07
Iter: 1858 loss: 6.60397404e-07
Iter: 1859 loss: 6.60346871e-07
Iter: 1860 loss: 6.60486307e-07
Iter: 1861 loss: 6.60326066e-07
Iter: 1862 loss: 6.60354544e-07
Iter: 1863 loss: 6.60306853e-07
Iter: 1864 loss: 6.602898e-07
Iter: 1865 loss: 6.60274168e-07
Iter: 1866 loss: 6.60646606e-07
Iter: 1867 loss: 6.60275873e-07
Iter: 1868 loss: 6.60213118e-07
Iter: 1869 loss: 6.60189528e-07
Iter: 1870 loss: 6.60181513e-07
Iter: 1871 loss: 6.60117166e-07
Iter: 1872 loss: 6.601216e-07
Iter: 1873 loss: 6.60082151e-07
Iter: 1874 loss: 6.60011665e-07
Iter: 1875 loss: 6.61276886e-07
Iter: 1876 loss: 6.60042474e-07
Iter: 1877 loss: 6.59940326e-07
Iter: 1878 loss: 6.61147055e-07
Iter: 1879 loss: 6.59932311e-07
Iter: 1880 loss: 6.59889281e-07
Iter: 1881 loss: 6.5991037e-07
Iter: 1882 loss: 6.59824309e-07
Iter: 1883 loss: 6.59786565e-07
Iter: 1884 loss: 6.60042588e-07
Iter: 1885 loss: 6.59782472e-07
Iter: 1886 loss: 6.59705165e-07
Iter: 1887 loss: 6.60089e-07
Iter: 1888 loss: 6.59677312e-07
Iter: 1889 loss: 6.59629336e-07
Iter: 1890 loss: 6.5956965e-07
Iter: 1891 loss: 6.60947762e-07
Iter: 1892 loss: 6.59581815e-07
Iter: 1893 loss: 6.59524e-07
Iter: 1894 loss: 6.59550039e-07
Iter: 1895 loss: 6.59511045e-07
Iter: 1896 loss: 6.59609896e-07
Iter: 1897 loss: 6.59503883e-07
Iter: 1898 loss: 6.59474381e-07
Iter: 1899 loss: 6.59831e-07
Iter: 1900 loss: 6.59474722e-07
Iter: 1901 loss: 6.59419072e-07
Iter: 1902 loss: 6.59404805e-07
Iter: 1903 loss: 6.59925092e-07
Iter: 1904 loss: 6.59383659e-07
Iter: 1905 loss: 6.59361604e-07
Iter: 1906 loss: 6.59388036e-07
Iter: 1907 loss: 6.59329487e-07
Iter: 1908 loss: 6.59263e-07
Iter: 1909 loss: 6.59262469e-07
Iter: 1910 loss: 6.5924047e-07
Iter: 1911 loss: 6.59150714e-07
Iter: 1912 loss: 6.59179932e-07
Iter: 1913 loss: 6.59100294e-07
Iter: 1914 loss: 6.59208467e-07
Iter: 1915 loss: 6.59114335e-07
Iter: 1916 loss: 6.59022476e-07
Iter: 1917 loss: 6.59145883e-07
Iter: 1918 loss: 6.59006616e-07
Iter: 1919 loss: 6.58975296e-07
Iter: 1920 loss: 6.58976433e-07
Iter: 1921 loss: 6.58966769e-07
Iter: 1922 loss: 6.58907425e-07
Iter: 1923 loss: 6.58895033e-07
Iter: 1924 loss: 6.58834779e-07
Iter: 1925 loss: 6.58929252e-07
Iter: 1926 loss: 6.58829663e-07
Iter: 1927 loss: 6.58780493e-07
Iter: 1928 loss: 6.58925899e-07
Iter: 1929 loss: 6.58793283e-07
Iter: 1930 loss: 6.58733256e-07
Iter: 1931 loss: 6.58746671e-07
Iter: 1932 loss: 6.58700174e-07
Iter: 1933 loss: 6.58687384e-07
Iter: 1934 loss: 6.58663453e-07
Iter: 1935 loss: 6.5864424e-07
Iter: 1936 loss: 6.58696194e-07
Iter: 1937 loss: 6.58606211e-07
Iter: 1938 loss: 6.58577278e-07
Iter: 1939 loss: 6.58530041e-07
Iter: 1940 loss: 6.5855204e-07
Iter: 1941 loss: 6.5847388e-07
Iter: 1942 loss: 6.58525778e-07
Iter: 1943 loss: 6.58450631e-07
Iter: 1944 loss: 6.5839015e-07
Iter: 1945 loss: 6.58384693e-07
Iter: 1946 loss: 6.58346607e-07
Iter: 1947 loss: 6.58306305e-07
Iter: 1948 loss: 6.58322392e-07
Iter: 1949 loss: 6.5827021e-07
Iter: 1950 loss: 6.58322506e-07
Iter: 1951 loss: 6.58219562e-07
Iter: 1952 loss: 6.58202111e-07
Iter: 1953 loss: 6.58536806e-07
Iter: 1954 loss: 6.58194494e-07
Iter: 1955 loss: 6.58124463e-07
Iter: 1956 loss: 6.58203533e-07
Iter: 1957 loss: 6.5812992e-07
Iter: 1958 loss: 6.58103943e-07
Iter: 1959 loss: 6.58098202e-07
Iter: 1960 loss: 6.58080296e-07
Iter: 1961 loss: 6.58028455e-07
Iter: 1962 loss: 6.58207341e-07
Iter: 1963 loss: 6.58008162e-07
Iter: 1964 loss: 6.57980763e-07
Iter: 1965 loss: 6.58200634e-07
Iter: 1966 loss: 6.57974169e-07
Iter: 1967 loss: 6.57939665e-07
Iter: 1968 loss: 6.57915848e-07
Iter: 1969 loss: 6.57890212e-07
Iter: 1970 loss: 6.57859232e-07
Iter: 1971 loss: 6.58811814e-07
Iter: 1972 loss: 6.57833425e-07
Iter: 1973 loss: 6.57795567e-07
Iter: 1974 loss: 6.57932446e-07
Iter: 1975 loss: 6.57778e-07
Iter: 1976 loss: 6.57712178e-07
Iter: 1977 loss: 6.57794089e-07
Iter: 1978 loss: 6.57667272e-07
Iter: 1979 loss: 6.57651185e-07
Iter: 1980 loss: 6.57660905e-07
Iter: 1981 loss: 6.57622195e-07
Iter: 1982 loss: 6.57574e-07
Iter: 1983 loss: 6.58441877e-07
Iter: 1984 loss: 6.57583e-07
Iter: 1985 loss: 6.57513169e-07
Iter: 1986 loss: 6.57809665e-07
Iter: 1987 loss: 6.57491285e-07
Iter: 1988 loss: 6.57458031e-07
Iter: 1989 loss: 6.57455303e-07
Iter: 1990 loss: 6.57408634e-07
Iter: 1991 loss: 6.5735145e-07
Iter: 1992 loss: 6.58256738e-07
Iter: 1993 loss: 6.57324051e-07
Iter: 1994 loss: 6.57291707e-07
Iter: 1995 loss: 6.57547389e-07
Iter: 1996 loss: 6.57249927e-07
Iter: 1997 loss: 6.57216e-07
Iter: 1998 loss: 6.57810347e-07
Iter: 1999 loss: 6.57232249e-07
Iter: 2000 loss: 6.57206158e-07
Iter: 2001 loss: 6.57387716e-07
Iter: 2002 loss: 6.57188764e-07
Iter: 2003 loss: 6.5714022e-07
Iter: 2004 loss: 6.5724646e-07
Iter: 2005 loss: 6.57121291e-07
Iter: 2006 loss: 6.57093551e-07
Iter: 2007 loss: 6.57018518e-07
Iter: 2008 loss: 6.57888279e-07
Iter: 2009 loss: 6.57031592e-07
Iter: 2010 loss: 6.5699885e-07
Iter: 2011 loss: 6.57333089e-07
Iter: 2012 loss: 6.56946099e-07
Iter: 2013 loss: 6.56911936e-07
Iter: 2014 loss: 6.57055466e-07
Iter: 2015 loss: 6.56877887e-07
Iter: 2016 loss: 6.56825875e-07
Iter: 2017 loss: 6.57065186e-07
Iter: 2018 loss: 6.56815075e-07
Iter: 2019 loss: 6.56782163e-07
Iter: 2020 loss: 6.56839688e-07
Iter: 2021 loss: 6.56744874e-07
Iter: 2022 loss: 6.56708607e-07
Iter: 2023 loss: 6.56650172e-07
Iter: 2024 loss: 6.56657903e-07
Iter: 2025 loss: 6.56594125e-07
Iter: 2026 loss: 6.57352246e-07
Iter: 2027 loss: 6.56597877e-07
Iter: 2028 loss: 6.5653046e-07
Iter: 2029 loss: 6.56802342e-07
Iter: 2030 loss: 6.56531654e-07
Iter: 2031 loss: 6.56481859e-07
Iter: 2032 loss: 6.56419616e-07
Iter: 2033 loss: 6.56408247e-07
Iter: 2034 loss: 6.56382042e-07
Iter: 2035 loss: 6.56838e-07
Iter: 2036 loss: 6.5638028e-07
Iter: 2037 loss: 6.56327927e-07
Iter: 2038 loss: 6.56340035e-07
Iter: 2039 loss: 6.562816e-07
Iter: 2040 loss: 6.5626682e-07
Iter: 2041 loss: 6.56260454e-07
Iter: 2042 loss: 6.56223e-07
Iter: 2043 loss: 6.56239649e-07
Iter: 2044 loss: 6.56202644e-07
Iter: 2045 loss: 6.56130169e-07
Iter: 2046 loss: 6.56148245e-07
Iter: 2047 loss: 6.56090606e-07
Iter: 2048 loss: 6.56060877e-07
Iter: 2049 loss: 6.56730663e-07
Iter: 2050 loss: 6.56037798e-07
Iter: 2051 loss: 6.56002271e-07
Iter: 2052 loss: 6.56134432e-07
Iter: 2053 loss: 6.55979534e-07
Iter: 2054 loss: 6.55944632e-07
Iter: 2055 loss: 6.55911549e-07
Iter: 2056 loss: 6.55902056e-07
Iter: 2057 loss: 6.5582185e-07
Iter: 2058 loss: 6.55950316e-07
Iter: 2059 loss: 6.55794338e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4
+ date
Mon Oct 26 10:17:25 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.4_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.4_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f801378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f80b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f8f38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f813598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5be9ee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f8f30d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f7a2d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f76a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f76a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f76a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f71b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f6f99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f6e98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f6a7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f69cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f724f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f72fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f682510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f682048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f5ff400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f5cba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f5800d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f5cb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f517a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f4f1488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f4f0b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f4f0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f492378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f492048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f44d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f418268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc14f69f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc14f89598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc2f434d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc14f9f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc14f1cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.5036282e-05
Iter: 2 loss: 0.000300940243
Iter: 3 loss: 4.10399734e-05
Iter: 4 loss: 3.04754431e-05
Iter: 5 loss: 0.000105019601
Iter: 6 loss: 2.95569662e-05
Iter: 7 loss: 2.53819017e-05
Iter: 8 loss: 3.45611115e-05
Iter: 9 loss: 2.37772492e-05
Iter: 10 loss: 2.07603262e-05
Iter: 11 loss: 1.9432493e-05
Iter: 12 loss: 1.78930968e-05
Iter: 13 loss: 1.40640059e-05
Iter: 14 loss: 2.89295203e-05
Iter: 15 loss: 1.31730385e-05
Iter: 16 loss: 1.17674163e-05
Iter: 17 loss: 1.54369773e-05
Iter: 18 loss: 1.12895e-05
Iter: 19 loss: 9.91029538e-06
Iter: 20 loss: 1.38054338e-05
Iter: 21 loss: 9.47242188e-06
Iter: 22 loss: 8.46655621e-06
Iter: 23 loss: 1.81576179e-05
Iter: 24 loss: 8.42864e-06
Iter: 25 loss: 7.97981102e-06
Iter: 26 loss: 8.67707058e-06
Iter: 27 loss: 7.76905472e-06
Iter: 28 loss: 7.2974135e-06
Iter: 29 loss: 1.04277988e-05
Iter: 30 loss: 7.24915253e-06
Iter: 31 loss: 6.92898266e-06
Iter: 32 loss: 6.96177494e-06
Iter: 33 loss: 6.68235816e-06
Iter: 34 loss: 6.42638952e-06
Iter: 35 loss: 6.91583818e-06
Iter: 36 loss: 6.31921148e-06
Iter: 37 loss: 6.53852112e-06
Iter: 38 loss: 6.25348821e-06
Iter: 39 loss: 6.17257137e-06
Iter: 40 loss: 5.98323641e-06
Iter: 41 loss: 8.25454572e-06
Iter: 42 loss: 5.96733435e-06
Iter: 43 loss: 5.85407179e-06
Iter: 44 loss: 5.85287944e-06
Iter: 45 loss: 5.785756e-06
Iter: 46 loss: 5.66010976e-06
Iter: 47 loss: 8.46802504e-06
Iter: 48 loss: 5.65967184e-06
Iter: 49 loss: 5.49606239e-06
Iter: 50 loss: 6.25827943e-06
Iter: 51 loss: 5.46581214e-06
Iter: 52 loss: 5.34646915e-06
Iter: 53 loss: 5.39731946e-06
Iter: 54 loss: 5.26474105e-06
Iter: 55 loss: 5.13104442e-06
Iter: 56 loss: 6.13877364e-06
Iter: 57 loss: 5.12094903e-06
Iter: 58 loss: 5.00013084e-06
Iter: 59 loss: 5.23468134e-06
Iter: 60 loss: 4.94979577e-06
Iter: 61 loss: 4.8205593e-06
Iter: 62 loss: 5.26820168e-06
Iter: 63 loss: 4.78668608e-06
Iter: 64 loss: 4.69059205e-06
Iter: 65 loss: 5.11176177e-06
Iter: 66 loss: 4.67151767e-06
Iter: 67 loss: 4.56586304e-06
Iter: 68 loss: 4.5402976e-06
Iter: 69 loss: 4.47311686e-06
Iter: 70 loss: 4.35684842e-06
Iter: 71 loss: 4.84731254e-06
Iter: 72 loss: 4.33268178e-06
Iter: 73 loss: 4.47885668e-06
Iter: 74 loss: 4.30321234e-06
Iter: 75 loss: 4.29313241e-06
Iter: 76 loss: 4.2580682e-06
Iter: 77 loss: 4.23656365e-06
Iter: 78 loss: 4.2143829e-06
Iter: 79 loss: 4.12765e-06
Iter: 80 loss: 4.9590235e-06
Iter: 81 loss: 4.12432928e-06
Iter: 82 loss: 4.08541791e-06
Iter: 83 loss: 4.1117637e-06
Iter: 84 loss: 4.06099844e-06
Iter: 85 loss: 4.01296165e-06
Iter: 86 loss: 4.06812251e-06
Iter: 87 loss: 3.98722705e-06
Iter: 88 loss: 3.92447146e-06
Iter: 89 loss: 4.00894169e-06
Iter: 90 loss: 3.89296292e-06
Iter: 91 loss: 3.8240596e-06
Iter: 92 loss: 4.25128e-06
Iter: 93 loss: 3.81592827e-06
Iter: 94 loss: 3.77328388e-06
Iter: 95 loss: 3.96883024e-06
Iter: 96 loss: 3.76511935e-06
Iter: 97 loss: 3.7237528e-06
Iter: 98 loss: 3.71123861e-06
Iter: 99 loss: 3.68658129e-06
Iter: 100 loss: 3.63556887e-06
Iter: 101 loss: 4.11137444e-06
Iter: 102 loss: 3.63346135e-06
Iter: 103 loss: 3.60428e-06
Iter: 104 loss: 3.5737919e-06
Iter: 105 loss: 3.56848932e-06
Iter: 106 loss: 3.55932934e-06
Iter: 107 loss: 3.54671829e-06
Iter: 108 loss: 3.51704148e-06
Iter: 109 loss: 3.49407719e-06
Iter: 110 loss: 3.48503772e-06
Iter: 111 loss: 3.46817342e-06
Iter: 112 loss: 3.47118475e-06
Iter: 113 loss: 3.45592775e-06
Iter: 114 loss: 3.42100611e-06
Iter: 115 loss: 3.46082402e-06
Iter: 116 loss: 3.40262204e-06
Iter: 117 loss: 3.37667529e-06
Iter: 118 loss: 3.38980385e-06
Iter: 119 loss: 3.35955929e-06
Iter: 120 loss: 3.33503226e-06
Iter: 121 loss: 3.4063878e-06
Iter: 122 loss: 3.32724721e-06
Iter: 123 loss: 3.30135595e-06
Iter: 124 loss: 3.38257e-06
Iter: 125 loss: 3.29386148e-06
Iter: 126 loss: 3.2625403e-06
Iter: 127 loss: 3.34225342e-06
Iter: 128 loss: 3.25156e-06
Iter: 129 loss: 3.23259792e-06
Iter: 130 loss: 3.24835764e-06
Iter: 131 loss: 3.22125743e-06
Iter: 132 loss: 3.19707283e-06
Iter: 133 loss: 3.35127743e-06
Iter: 134 loss: 3.19457604e-06
Iter: 135 loss: 3.1681011e-06
Iter: 136 loss: 3.17766762e-06
Iter: 137 loss: 3.14956651e-06
Iter: 138 loss: 3.12325778e-06
Iter: 139 loss: 3.23891095e-06
Iter: 140 loss: 3.11817439e-06
Iter: 141 loss: 3.10881956e-06
Iter: 142 loss: 3.10371092e-06
Iter: 143 loss: 3.09313555e-06
Iter: 144 loss: 3.07720893e-06
Iter: 145 loss: 3.0769902e-06
Iter: 146 loss: 3.06128732e-06
Iter: 147 loss: 3.02871945e-06
Iter: 148 loss: 3.59078285e-06
Iter: 149 loss: 3.02804392e-06
Iter: 150 loss: 3.0216745e-06
Iter: 151 loss: 3.01207365e-06
Iter: 152 loss: 2.99688054e-06
Iter: 153 loss: 3.00974853e-06
Iter: 154 loss: 2.98796135e-06
Iter: 155 loss: 2.97089309e-06
Iter: 156 loss: 2.98519353e-06
Iter: 157 loss: 2.96071721e-06
Iter: 158 loss: 2.94124106e-06
Iter: 159 loss: 2.97508313e-06
Iter: 160 loss: 2.93254971e-06
Iter: 161 loss: 2.91543688e-06
Iter: 162 loss: 3.05975027e-06
Iter: 163 loss: 2.91446031e-06
Iter: 164 loss: 2.89943137e-06
Iter: 165 loss: 2.9012067e-06
Iter: 166 loss: 2.88804381e-06
Iter: 167 loss: 2.87353942e-06
Iter: 168 loss: 2.98102759e-06
Iter: 169 loss: 2.87227886e-06
Iter: 170 loss: 2.85986835e-06
Iter: 171 loss: 2.90142725e-06
Iter: 172 loss: 2.85635656e-06
Iter: 173 loss: 2.84699559e-06
Iter: 174 loss: 2.91931474e-06
Iter: 175 loss: 2.84645262e-06
Iter: 176 loss: 2.83662735e-06
Iter: 177 loss: 2.91576612e-06
Iter: 178 loss: 2.83582631e-06
Iter: 179 loss: 2.83186637e-06
Iter: 180 loss: 2.82509814e-06
Iter: 181 loss: 2.82509563e-06
Iter: 182 loss: 2.81669554e-06
Iter: 183 loss: 2.79906067e-06
Iter: 184 loss: 3.09040456e-06
Iter: 185 loss: 2.79860365e-06
Iter: 186 loss: 2.80518952e-06
Iter: 187 loss: 2.79062579e-06
Iter: 188 loss: 2.78161633e-06
Iter: 189 loss: 2.79325604e-06
Iter: 190 loss: 2.77684762e-06
Iter: 191 loss: 2.7672445e-06
Iter: 192 loss: 2.75292518e-06
Iter: 193 loss: 2.75266802e-06
Iter: 194 loss: 2.73112664e-06
Iter: 195 loss: 2.89138779e-06
Iter: 196 loss: 2.72934199e-06
Iter: 197 loss: 2.72063789e-06
Iter: 198 loss: 2.71923022e-06
Iter: 199 loss: 2.71246972e-06
Iter: 200 loss: 2.70425153e-06
Iter: 201 loss: 2.70325177e-06
Iter: 202 loss: 2.69386101e-06
Iter: 203 loss: 2.7595629e-06
Iter: 204 loss: 2.69298152e-06
Iter: 205 loss: 2.68435656e-06
Iter: 206 loss: 2.69990778e-06
Iter: 207 loss: 2.68076042e-06
Iter: 208 loss: 2.67852556e-06
Iter: 209 loss: 2.6765224e-06
Iter: 210 loss: 2.67280211e-06
Iter: 211 loss: 2.66327152e-06
Iter: 212 loss: 2.73179626e-06
Iter: 213 loss: 2.6609116e-06
Iter: 214 loss: 2.64853065e-06
Iter: 215 loss: 2.67510177e-06
Iter: 216 loss: 2.64356413e-06
Iter: 217 loss: 2.63472111e-06
Iter: 218 loss: 2.65927156e-06
Iter: 219 loss: 2.63174343e-06
Iter: 220 loss: 2.62446792e-06
Iter: 221 loss: 2.62448475e-06
Iter: 222 loss: 2.61752871e-06
Iter: 223 loss: 2.61221476e-06
Iter: 224 loss: 2.60993e-06
Iter: 225 loss: 2.6013563e-06
Iter: 226 loss: 2.61694231e-06
Iter: 227 loss: 2.59762965e-06
Iter: 228 loss: 2.59082708e-06
Iter: 229 loss: 2.59306535e-06
Iter: 230 loss: 2.58600767e-06
Iter: 231 loss: 2.57414217e-06
Iter: 232 loss: 2.58958653e-06
Iter: 233 loss: 2.56828207e-06
Iter: 234 loss: 2.55590794e-06
Iter: 235 loss: 2.72492866e-06
Iter: 236 loss: 2.55577265e-06
Iter: 237 loss: 2.55009127e-06
Iter: 238 loss: 2.54328279e-06
Iter: 239 loss: 2.54254428e-06
Iter: 240 loss: 2.53085545e-06
Iter: 241 loss: 2.5396148e-06
Iter: 242 loss: 2.52351651e-06
Iter: 243 loss: 2.55496775e-06
Iter: 244 loss: 2.52173572e-06
Iter: 245 loss: 2.51978031e-06
Iter: 246 loss: 2.51309893e-06
Iter: 247 loss: 2.51013148e-06
Iter: 248 loss: 2.50505946e-06
Iter: 249 loss: 2.49595632e-06
Iter: 250 loss: 2.57761167e-06
Iter: 251 loss: 2.49563891e-06
Iter: 252 loss: 2.48761694e-06
Iter: 253 loss: 2.53686176e-06
Iter: 254 loss: 2.48664583e-06
Iter: 255 loss: 2.48146102e-06
Iter: 256 loss: 2.54265706e-06
Iter: 257 loss: 2.48137212e-06
Iter: 258 loss: 2.47672619e-06
Iter: 259 loss: 2.47096659e-06
Iter: 260 loss: 2.47041726e-06
Iter: 261 loss: 2.46441164e-06
Iter: 262 loss: 2.48229344e-06
Iter: 263 loss: 2.4625956e-06
Iter: 264 loss: 2.45520778e-06
Iter: 265 loss: 2.49419077e-06
Iter: 266 loss: 2.45408864e-06
Iter: 267 loss: 2.44881903e-06
Iter: 268 loss: 2.44805506e-06
Iter: 269 loss: 2.44434386e-06
Iter: 270 loss: 2.43869135e-06
Iter: 271 loss: 2.44021157e-06
Iter: 272 loss: 2.43466e-06
Iter: 273 loss: 2.4287026e-06
Iter: 274 loss: 2.43281693e-06
Iter: 275 loss: 2.42505939e-06
Iter: 276 loss: 2.42779834e-06
Iter: 277 loss: 2.4225119e-06
Iter: 278 loss: 2.42011401e-06
Iter: 279 loss: 2.418431e-06
Iter: 280 loss: 2.4174044e-06
Iter: 281 loss: 2.41379257e-06
Iter: 282 loss: 2.40754252e-06
Iter: 283 loss: 2.40755526e-06
Iter: 284 loss: 2.40144254e-06
Iter: 285 loss: 2.41243129e-06
Iter: 286 loss: 2.39872043e-06
Iter: 287 loss: 2.39608062e-06
Iter: 288 loss: 2.39552082e-06
Iter: 289 loss: 2.39218889e-06
Iter: 290 loss: 2.39106862e-06
Iter: 291 loss: 2.38909251e-06
Iter: 292 loss: 2.38402299e-06
Iter: 293 loss: 2.39300448e-06
Iter: 294 loss: 2.38180723e-06
Iter: 295 loss: 2.37668769e-06
Iter: 296 loss: 2.37039967e-06
Iter: 297 loss: 2.36986125e-06
Iter: 298 loss: 2.36205756e-06
Iter: 299 loss: 2.394484e-06
Iter: 300 loss: 2.36046458e-06
Iter: 301 loss: 2.35545031e-06
Iter: 302 loss: 2.35543757e-06
Iter: 303 loss: 2.35121934e-06
Iter: 304 loss: 2.3469961e-06
Iter: 305 loss: 2.34628328e-06
Iter: 306 loss: 2.34174581e-06
Iter: 307 loss: 2.37998916e-06
Iter: 308 loss: 2.34170102e-06
Iter: 309 loss: 2.33962783e-06
Iter: 310 loss: 2.33921696e-06
Iter: 311 loss: 2.33787887e-06
Iter: 312 loss: 2.33581682e-06
Iter: 313 loss: 2.3358532e-06
Iter: 314 loss: 2.33342871e-06
Iter: 315 loss: 2.3278817e-06
Iter: 316 loss: 2.41481484e-06
Iter: 317 loss: 2.32772754e-06
Iter: 318 loss: 2.32483126e-06
Iter: 319 loss: 2.32437651e-06
Iter: 320 loss: 2.32192315e-06
Iter: 321 loss: 2.33088576e-06
Iter: 322 loss: 2.3213297e-06
Iter: 323 loss: 2.31901095e-06
Iter: 324 loss: 2.336541e-06
Iter: 325 loss: 2.3190612e-06
Iter: 326 loss: 2.31739796e-06
Iter: 327 loss: 2.31331251e-06
Iter: 328 loss: 2.35149764e-06
Iter: 329 loss: 2.3127493e-06
Iter: 330 loss: 2.30734395e-06
Iter: 331 loss: 2.35583366e-06
Iter: 332 loss: 2.30718069e-06
Iter: 333 loss: 2.30220758e-06
Iter: 334 loss: 2.32222e-06
Iter: 335 loss: 2.30096657e-06
Iter: 336 loss: 2.29745342e-06
Iter: 337 loss: 2.30081059e-06
Iter: 338 loss: 2.2953468e-06
Iter: 339 loss: 2.2920035e-06
Iter: 340 loss: 2.31802369e-06
Iter: 341 loss: 2.29172156e-06
Iter: 342 loss: 2.29001694e-06
Iter: 343 loss: 2.28999079e-06
Iter: 344 loss: 2.28786644e-06
Iter: 345 loss: 2.28401586e-06
Iter: 346 loss: 2.36962637e-06
Iter: 347 loss: 2.28407544e-06
Iter: 348 loss: 2.28127828e-06
Iter: 349 loss: 2.29898251e-06
Iter: 350 loss: 2.2807676e-06
Iter: 351 loss: 2.27890723e-06
Iter: 352 loss: 2.27470582e-06
Iter: 353 loss: 2.33274704e-06
Iter: 354 loss: 2.27448027e-06
Iter: 355 loss: 2.26944985e-06
Iter: 356 loss: 2.33645187e-06
Iter: 357 loss: 2.26945963e-06
Iter: 358 loss: 2.26681232e-06
Iter: 359 loss: 2.2845486e-06
Iter: 360 loss: 2.26660268e-06
Iter: 361 loss: 2.26422208e-06
Iter: 362 loss: 2.26406337e-06
Iter: 363 loss: 2.26211341e-06
Iter: 364 loss: 2.25921713e-06
Iter: 365 loss: 2.25521535e-06
Iter: 366 loss: 2.25500116e-06
Iter: 367 loss: 2.25063422e-06
Iter: 368 loss: 2.2773761e-06
Iter: 369 loss: 2.24999781e-06
Iter: 370 loss: 2.24575729e-06
Iter: 371 loss: 2.26448105e-06
Iter: 372 loss: 2.24505e-06
Iter: 373 loss: 2.24275777e-06
Iter: 374 loss: 2.24794485e-06
Iter: 375 loss: 2.24187806e-06
Iter: 376 loss: 2.24005043e-06
Iter: 377 loss: 2.25826261e-06
Iter: 378 loss: 2.23996813e-06
Iter: 379 loss: 2.23794814e-06
Iter: 380 loss: 2.23780307e-06
Iter: 381 loss: 2.23620441e-06
Iter: 382 loss: 2.23420056e-06
Iter: 383 loss: 2.23355892e-06
Iter: 384 loss: 2.23232382e-06
Iter: 385 loss: 2.22861854e-06
Iter: 386 loss: 2.23088045e-06
Iter: 387 loss: 2.22613198e-06
Iter: 388 loss: 2.22274684e-06
Iter: 389 loss: 2.22998415e-06
Iter: 390 loss: 2.22126732e-06
Iter: 391 loss: 2.21829146e-06
Iter: 392 loss: 2.21829441e-06
Iter: 393 loss: 2.21651158e-06
Iter: 394 loss: 2.22138488e-06
Iter: 395 loss: 2.21593245e-06
Iter: 396 loss: 2.21387927e-06
Iter: 397 loss: 2.21087589e-06
Iter: 398 loss: 2.21079927e-06
Iter: 399 loss: 2.20802485e-06
Iter: 400 loss: 2.21058508e-06
Iter: 401 loss: 2.20634297e-06
Iter: 402 loss: 2.2022914e-06
Iter: 403 loss: 2.21977302e-06
Iter: 404 loss: 2.2015065e-06
Iter: 405 loss: 2.19812819e-06
Iter: 406 loss: 2.22629615e-06
Iter: 407 loss: 2.19799767e-06
Iter: 408 loss: 2.19733352e-06
Iter: 409 loss: 2.19694857e-06
Iter: 410 loss: 2.19619051e-06
Iter: 411 loss: 2.1957228e-06
Iter: 412 loss: 2.19536969e-06
Iter: 413 loss: 2.19406138e-06
Iter: 414 loss: 2.19168805e-06
Iter: 415 loss: 2.24156224e-06
Iter: 416 loss: 2.19160415e-06
Iter: 417 loss: 2.18841842e-06
Iter: 418 loss: 2.20064612e-06
Iter: 419 loss: 2.18759237e-06
Iter: 420 loss: 2.18408832e-06
Iter: 421 loss: 2.18772652e-06
Iter: 422 loss: 2.18202467e-06
Iter: 423 loss: 2.18032937e-06
Iter: 424 loss: 2.20240645e-06
Iter: 425 loss: 2.18026616e-06
Iter: 426 loss: 2.17817114e-06
Iter: 427 loss: 2.17935485e-06
Iter: 428 loss: 2.17703155e-06
Iter: 429 loss: 2.17462366e-06
Iter: 430 loss: 2.18526952e-06
Iter: 431 loss: 2.17415732e-06
Iter: 432 loss: 2.17228398e-06
Iter: 433 loss: 2.17061324e-06
Iter: 434 loss: 2.17020283e-06
Iter: 435 loss: 2.1678602e-06
Iter: 436 loss: 2.17372894e-06
Iter: 437 loss: 2.16724175e-06
Iter: 438 loss: 2.16446256e-06
Iter: 439 loss: 2.17937259e-06
Iter: 440 loss: 2.16398644e-06
Iter: 441 loss: 2.16239528e-06
Iter: 442 loss: 2.18575883e-06
Iter: 443 loss: 2.16242529e-06
Iter: 444 loss: 2.1607525e-06
Iter: 445 loss: 2.16399098e-06
Iter: 446 loss: 2.16001376e-06
Iter: 447 loss: 2.15851014e-06
Iter: 448 loss: 2.15543923e-06
Iter: 449 loss: 2.21550113e-06
Iter: 450 loss: 2.15541422e-06
Iter: 451 loss: 2.15315367e-06
Iter: 452 loss: 2.16895614e-06
Iter: 453 loss: 2.15306318e-06
Iter: 454 loss: 2.15118371e-06
Iter: 455 loss: 2.15768546e-06
Iter: 456 loss: 2.15067166e-06
Iter: 457 loss: 2.14897773e-06
Iter: 458 loss: 2.14754323e-06
Iter: 459 loss: 2.14703e-06
Iter: 460 loss: 2.14544571e-06
Iter: 461 loss: 2.14520196e-06
Iter: 462 loss: 2.14387819e-06
Iter: 463 loss: 2.14335773e-06
Iter: 464 loss: 2.14271449e-06
Iter: 465 loss: 2.1405931e-06
Iter: 466 loss: 2.1450976e-06
Iter: 467 loss: 2.13984504e-06
Iter: 468 loss: 2.13806402e-06
Iter: 469 loss: 2.13446583e-06
Iter: 470 loss: 2.21071491e-06
Iter: 471 loss: 2.13446492e-06
Iter: 472 loss: 2.13174167e-06
Iter: 473 loss: 2.16715648e-06
Iter: 474 loss: 2.13175099e-06
Iter: 475 loss: 2.12945906e-06
Iter: 476 loss: 2.13561407e-06
Iter: 477 loss: 2.12879445e-06
Iter: 478 loss: 2.1277724e-06
Iter: 479 loss: 2.1276619e-06
Iter: 480 loss: 2.12625e-06
Iter: 481 loss: 2.12444093e-06
Iter: 482 loss: 2.12434952e-06
Iter: 483 loss: 2.12311647e-06
Iter: 484 loss: 2.12153327e-06
Iter: 485 loss: 2.12145346e-06
Iter: 486 loss: 2.11881434e-06
Iter: 487 loss: 2.12225723e-06
Iter: 488 loss: 2.11738779e-06
Iter: 489 loss: 2.11506608e-06
Iter: 490 loss: 2.11503766e-06
Iter: 491 loss: 2.11353654e-06
Iter: 492 loss: 2.1111041e-06
Iter: 493 loss: 2.11105839e-06
Iter: 494 loss: 2.10908593e-06
Iter: 495 loss: 2.10906546e-06
Iter: 496 loss: 2.107206e-06
Iter: 497 loss: 2.11291e-06
Iter: 498 loss: 2.10669941e-06
Iter: 499 loss: 2.10552253e-06
Iter: 500 loss: 2.10762937e-06
Iter: 501 loss: 2.10501548e-06
Iter: 502 loss: 2.10335338e-06
Iter: 503 loss: 2.10150938e-06
Iter: 504 loss: 2.10142798e-06
Iter: 505 loss: 2.09860445e-06
Iter: 506 loss: 2.10589178e-06
Iter: 507 loss: 2.09760924e-06
Iter: 508 loss: 2.09545692e-06
Iter: 509 loss: 2.1264259e-06
Iter: 510 loss: 2.09544396e-06
Iter: 511 loss: 2.09449217e-06
Iter: 512 loss: 2.09436053e-06
Iter: 513 loss: 2.09375094e-06
Iter: 514 loss: 2.09159748e-06
Iter: 515 loss: 2.10164171e-06
Iter: 516 loss: 2.0908119e-06
Iter: 517 loss: 2.08916049e-06
Iter: 518 loss: 2.09558584e-06
Iter: 519 loss: 2.08882284e-06
Iter: 520 loss: 2.0872917e-06
Iter: 521 loss: 2.09232394e-06
Iter: 522 loss: 2.08686879e-06
Iter: 523 loss: 2.08535903e-06
Iter: 524 loss: 2.09519158e-06
Iter: 525 loss: 2.08520669e-06
Iter: 526 loss: 2.08409892e-06
Iter: 527 loss: 2.08807523e-06
Iter: 528 loss: 2.08378697e-06
Iter: 529 loss: 2.0830671e-06
Iter: 530 loss: 2.08147321e-06
Iter: 531 loss: 2.1055364e-06
Iter: 532 loss: 2.08135953e-06
Iter: 533 loss: 2.0806458e-06
Iter: 534 loss: 2.08016422e-06
Iter: 535 loss: 2.07921107e-06
Iter: 536 loss: 2.07916264e-06
Iter: 537 loss: 2.07819062e-06
Iter: 538 loss: 2.07744461e-06
Iter: 539 loss: 2.08400365e-06
Iter: 540 loss: 2.07732683e-06
Iter: 541 loss: 2.07637049e-06
Iter: 542 loss: 2.0747998e-06
Iter: 543 loss: 2.11483757e-06
Iter: 544 loss: 2.07488756e-06
Iter: 545 loss: 2.07393714e-06
Iter: 546 loss: 2.07382868e-06
Iter: 547 loss: 2.07279982e-06
Iter: 548 loss: 2.07737457e-06
Iter: 549 loss: 2.07256039e-06
Iter: 550 loss: 2.07213884e-06
Iter: 551 loss: 2.07152311e-06
Iter: 552 loss: 2.07151061e-06
Iter: 553 loss: 2.07032258e-06
Iter: 554 loss: 2.06874984e-06
Iter: 555 loss: 2.06871482e-06
Iter: 556 loss: 2.06678988e-06
Iter: 557 loss: 2.06625464e-06
Iter: 558 loss: 2.06507843e-06
Iter: 559 loss: 2.06241066e-06
Iter: 560 loss: 2.08392521e-06
Iter: 561 loss: 2.06214622e-06
Iter: 562 loss: 2.06054528e-06
Iter: 563 loss: 2.06056257e-06
Iter: 564 loss: 2.05944343e-06
Iter: 565 loss: 2.05899937e-06
Iter: 566 loss: 2.05831884e-06
Iter: 567 loss: 2.05701463e-06
Iter: 568 loss: 2.07239464e-06
Iter: 569 loss: 2.05696278e-06
Iter: 570 loss: 2.05555739e-06
Iter: 571 loss: 2.05534639e-06
Iter: 572 loss: 2.05432184e-06
Iter: 573 loss: 2.0528903e-06
Iter: 574 loss: 2.05338847e-06
Iter: 575 loss: 2.05171591e-06
Iter: 576 loss: 2.04958405e-06
Iter: 577 loss: 2.0509674e-06
Iter: 578 loss: 2.04802063e-06
Iter: 579 loss: 2.04840171e-06
Iter: 580 loss: 2.04677804e-06
Iter: 581 loss: 2.0456946e-06
Iter: 582 loss: 2.04558501e-06
Iter: 583 loss: 2.04491653e-06
Iter: 584 loss: 2.04410298e-06
Iter: 585 loss: 2.04178559e-06
Iter: 586 loss: 2.05199308e-06
Iter: 587 loss: 2.04072057e-06
Iter: 588 loss: 2.03672857e-06
Iter: 589 loss: 2.05591505e-06
Iter: 590 loss: 2.03608352e-06
Iter: 591 loss: 2.0349155e-06
Iter: 592 loss: 2.03477657e-06
Iter: 593 loss: 2.03333047e-06
Iter: 594 loss: 2.03349714e-06
Iter: 595 loss: 2.03237369e-06
Iter: 596 loss: 2.03093396e-06
Iter: 597 loss: 2.0383286e-06
Iter: 598 loss: 2.03067657e-06
Iter: 599 loss: 2.02900173e-06
Iter: 600 loss: 2.0303678e-06
Iter: 601 loss: 2.02803426e-06
Iter: 602 loss: 2.02589626e-06
Iter: 603 loss: 2.0281027e-06
Iter: 604 loss: 2.02480442e-06
Iter: 605 loss: 2.02381489e-06
Iter: 606 loss: 2.0235243e-06
Iter: 607 loss: 2.0227244e-06
Iter: 608 loss: 2.02238562e-06
Iter: 609 loss: 2.0217683e-06
Iter: 610 loss: 2.02114256e-06
Iter: 611 loss: 2.03046966e-06
Iter: 612 loss: 2.02108049e-06
Iter: 613 loss: 2.02028718e-06
Iter: 614 loss: 2.02330739e-06
Iter: 615 loss: 2.02015053e-06
Iter: 616 loss: 2.01967987e-06
Iter: 617 loss: 2.01883063e-06
Iter: 618 loss: 2.03992568e-06
Iter: 619 loss: 2.01880471e-06
Iter: 620 loss: 2.01782223e-06
Iter: 621 loss: 2.01824901e-06
Iter: 622 loss: 2.01728017e-06
Iter: 623 loss: 2.0161292e-06
Iter: 624 loss: 2.01641365e-06
Iter: 625 loss: 2.01536932e-06
Iter: 626 loss: 2.01454077e-06
Iter: 627 loss: 2.01456601e-06
Iter: 628 loss: 2.01381408e-06
Iter: 629 loss: 2.01439389e-06
Iter: 630 loss: 2.01345483e-06
Iter: 631 loss: 2.01262446e-06
Iter: 632 loss: 2.01202329e-06
Iter: 633 loss: 2.01179523e-06
Iter: 634 loss: 2.01037983e-06
Iter: 635 loss: 2.02593719e-06
Iter: 636 loss: 2.01038574e-06
Iter: 637 loss: 2.00958016e-06
Iter: 638 loss: 2.00897966e-06
Iter: 639 loss: 2.0088039e-06
Iter: 640 loss: 2.00808518e-06
Iter: 641 loss: 2.00803538e-06
Iter: 642 loss: 2.00720638e-06
Iter: 643 loss: 2.0068594e-06
Iter: 644 loss: 2.00635304e-06
Iter: 645 loss: 2.00650106e-06
Iter: 646 loss: 2.0059947e-06
Iter: 647 loss: 2.00569775e-06
Iter: 648 loss: 2.00525619e-06
Iter: 649 loss: 2.00528666e-06
Iter: 650 loss: 2.004517e-06
Iter: 651 loss: 2.00387558e-06
Iter: 652 loss: 2.00358181e-06
Iter: 653 loss: 2.00196655e-06
Iter: 654 loss: 2.00122281e-06
Iter: 655 loss: 2.00041723e-06
Iter: 656 loss: 1.99913575e-06
Iter: 657 loss: 1.99907208e-06
Iter: 658 loss: 1.99791248e-06
Iter: 659 loss: 1.99935403e-06
Iter: 660 loss: 1.99729129e-06
Iter: 661 loss: 1.99593524e-06
Iter: 662 loss: 1.99842862e-06
Iter: 663 loss: 1.9952995e-06
Iter: 664 loss: 1.99416809e-06
Iter: 665 loss: 1.99922738e-06
Iter: 666 loss: 1.99404963e-06
Iter: 667 loss: 1.99278952e-06
Iter: 668 loss: 1.99430406e-06
Iter: 669 loss: 1.99204669e-06
Iter: 670 loss: 1.99074157e-06
Iter: 671 loss: 1.99132728e-06
Iter: 672 loss: 1.98977386e-06
Iter: 673 loss: 1.98930161e-06
Iter: 674 loss: 1.98892485e-06
Iter: 675 loss: 1.98815314e-06
Iter: 676 loss: 1.98700741e-06
Iter: 677 loss: 1.9868969e-06
Iter: 678 loss: 1.9863437e-06
Iter: 679 loss: 1.98606949e-06
Iter: 680 loss: 1.98565363e-06
Iter: 681 loss: 1.98472844e-06
Iter: 682 loss: 1.99981514e-06
Iter: 683 loss: 1.98466432e-06
Iter: 684 loss: 1.98388489e-06
Iter: 685 loss: 1.98348971e-06
Iter: 686 loss: 1.98313865e-06
Iter: 687 loss: 1.98188309e-06
Iter: 688 loss: 1.98576913e-06
Iter: 689 loss: 1.98146381e-06
Iter: 690 loss: 1.98058615e-06
Iter: 691 loss: 1.99083684e-06
Iter: 692 loss: 1.98060707e-06
Iter: 693 loss: 1.97973623e-06
Iter: 694 loss: 1.97971144e-06
Iter: 695 loss: 1.97918553e-06
Iter: 696 loss: 1.97819622e-06
Iter: 697 loss: 1.97995269e-06
Iter: 698 loss: 1.97761869e-06
Iter: 699 loss: 1.97688473e-06
Iter: 700 loss: 1.9864583e-06
Iter: 701 loss: 1.97683971e-06
Iter: 702 loss: 1.97603322e-06
Iter: 703 loss: 1.97591885e-06
Iter: 704 loss: 1.97534382e-06
Iter: 705 loss: 1.97417967e-06
Iter: 706 loss: 1.97464169e-06
Iter: 707 loss: 1.97326608e-06
Iter: 708 loss: 1.97329837e-06
Iter: 709 loss: 1.97288705e-06
Iter: 710 loss: 1.97230315e-06
Iter: 711 loss: 1.97283703e-06
Iter: 712 loss: 1.97204645e-06
Iter: 713 loss: 1.97142481e-06
Iter: 714 loss: 1.97459894e-06
Iter: 715 loss: 1.9711988e-06
Iter: 716 loss: 1.97089503e-06
Iter: 717 loss: 1.97004465e-06
Iter: 718 loss: 1.9785075e-06
Iter: 719 loss: 1.96993801e-06
Iter: 720 loss: 1.96906012e-06
Iter: 721 loss: 1.97606e-06
Iter: 722 loss: 1.96895348e-06
Iter: 723 loss: 1.96837914e-06
Iter: 724 loss: 1.96699284e-06
Iter: 725 loss: 1.98429984e-06
Iter: 726 loss: 1.96689666e-06
Iter: 727 loss: 1.96578e-06
Iter: 728 loss: 1.96574069e-06
Iter: 729 loss: 1.96461474e-06
Iter: 730 loss: 1.96824021e-06
Iter: 731 loss: 1.96436804e-06
Iter: 732 loss: 1.96321685e-06
Iter: 733 loss: 1.96326255e-06
Iter: 734 loss: 1.9623285e-06
Iter: 735 loss: 1.96155452e-06
Iter: 736 loss: 1.96153792e-06
Iter: 737 loss: 1.96082237e-06
Iter: 738 loss: 1.96111614e-06
Iter: 739 loss: 1.96040401e-06
Iter: 740 loss: 1.9594836e-06
Iter: 741 loss: 1.95919506e-06
Iter: 742 loss: 1.95873804e-06
Iter: 743 loss: 1.95766575e-06
Iter: 744 loss: 1.96411179e-06
Iter: 745 loss: 1.95752114e-06
Iter: 746 loss: 1.95733287e-06
Iter: 747 loss: 1.95696293e-06
Iter: 748 loss: 1.95664052e-06
Iter: 749 loss: 1.95706434e-06
Iter: 750 loss: 1.95646021e-06
Iter: 751 loss: 1.95600387e-06
Iter: 752 loss: 1.95503162e-06
Iter: 753 loss: 1.9753661e-06
Iter: 754 loss: 1.95502435e-06
Iter: 755 loss: 1.95426219e-06
Iter: 756 loss: 1.95418102e-06
Iter: 757 loss: 1.95369739e-06
Iter: 758 loss: 1.952817e-06
Iter: 759 loss: 1.96036217e-06
Iter: 760 loss: 1.95290386e-06
Iter: 761 loss: 1.95189023e-06
Iter: 762 loss: 1.95142729e-06
Iter: 763 loss: 1.95095436e-06
Iter: 764 loss: 1.94993254e-06
Iter: 765 loss: 1.9514514e-06
Iter: 766 loss: 1.94946051e-06
Iter: 767 loss: 1.94838412e-06
Iter: 768 loss: 1.95673965e-06
Iter: 769 loss: 1.94828431e-06
Iter: 770 loss: 1.94732024e-06
Iter: 771 loss: 1.95274447e-06
Iter: 772 loss: 1.94715471e-06
Iter: 773 loss: 1.94646873e-06
Iter: 774 loss: 1.94539325e-06
Iter: 775 loss: 1.94535596e-06
Iter: 776 loss: 1.94378958e-06
Iter: 777 loss: 1.95727262e-06
Iter: 778 loss: 1.9436734e-06
Iter: 779 loss: 1.94277254e-06
Iter: 780 loss: 1.94341874e-06
Iter: 781 loss: 1.94204972e-06
Iter: 782 loss: 1.94323684e-06
Iter: 783 loss: 1.94180257e-06
Iter: 784 loss: 1.94145514e-06
Iter: 785 loss: 1.94095583e-06
Iter: 786 loss: 1.94689028e-06
Iter: 787 loss: 1.94095173e-06
Iter: 788 loss: 1.94016866e-06
Iter: 789 loss: 1.94455311e-06
Iter: 790 loss: 1.93992355e-06
Iter: 791 loss: 1.93937603e-06
Iter: 792 loss: 1.93962455e-06
Iter: 793 loss: 1.93893311e-06
Iter: 794 loss: 1.93823553e-06
Iter: 795 loss: 1.93827645e-06
Iter: 796 loss: 1.93748565e-06
Iter: 797 loss: 1.93639153e-06
Iter: 798 loss: 1.94442805e-06
Iter: 799 loss: 1.93619553e-06
Iter: 800 loss: 1.93510141e-06
Iter: 801 loss: 1.93795904e-06
Iter: 802 loss: 1.93478718e-06
Iter: 803 loss: 1.93402343e-06
Iter: 804 loss: 1.93226947e-06
Iter: 805 loss: 1.95746907e-06
Iter: 806 loss: 1.93231972e-06
Iter: 807 loss: 1.93085748e-06
Iter: 808 loss: 1.94203676e-06
Iter: 809 loss: 1.93069673e-06
Iter: 810 loss: 1.92986818e-06
Iter: 811 loss: 1.92982225e-06
Iter: 812 loss: 1.92910352e-06
Iter: 813 loss: 1.92877656e-06
Iter: 814 loss: 1.92846665e-06
Iter: 815 loss: 1.92815764e-06
Iter: 816 loss: 1.92804805e-06
Iter: 817 loss: 1.92759398e-06
Iter: 818 loss: 1.92840412e-06
Iter: 819 loss: 1.92732932e-06
Iter: 820 loss: 1.92681728e-06
Iter: 821 loss: 1.92630341e-06
Iter: 822 loss: 1.92621042e-06
Iter: 823 loss: 1.92551761e-06
Iter: 824 loss: 1.93116671e-06
Iter: 825 loss: 1.92546167e-06
Iter: 826 loss: 1.92486164e-06
Iter: 827 loss: 1.92375228e-06
Iter: 828 loss: 1.95072221e-06
Iter: 829 loss: 1.92374591e-06
Iter: 830 loss: 1.92257016e-06
Iter: 831 loss: 1.92545372e-06
Iter: 832 loss: 1.92202924e-06
Iter: 833 loss: 1.92091306e-06
Iter: 834 loss: 1.9311351e-06
Iter: 835 loss: 1.92080233e-06
Iter: 836 loss: 1.91979962e-06
Iter: 837 loss: 1.92022821e-06
Iter: 838 loss: 1.91907657e-06
Iter: 839 loss: 1.91782692e-06
Iter: 840 loss: 1.91967615e-06
Iter: 841 loss: 1.91738809e-06
Iter: 842 loss: 1.91601703e-06
Iter: 843 loss: 1.9192164e-06
Iter: 844 loss: 1.91570462e-06
Iter: 845 loss: 1.91489107e-06
Iter: 846 loss: 1.92680091e-06
Iter: 847 loss: 1.91480945e-06
Iter: 848 loss: 1.91400477e-06
Iter: 849 loss: 1.91385425e-06
Iter: 850 loss: 1.91341542e-06
Iter: 851 loss: 1.91225718e-06
Iter: 852 loss: 1.91601521e-06
Iter: 853 loss: 1.91205231e-06
Iter: 854 loss: 1.91169147e-06
Iter: 855 loss: 1.911455e-06
Iter: 856 loss: 1.91099707e-06
Iter: 857 loss: 1.9116751e-06
Iter: 858 loss: 1.91083609e-06
Iter: 859 loss: 1.91029358e-06
Iter: 860 loss: 1.91024651e-06
Iter: 861 loss: 1.90990204e-06
Iter: 862 loss: 1.90914352e-06
Iter: 863 loss: 1.90925516e-06
Iter: 864 loss: 1.90853029e-06
Iter: 865 loss: 1.90770379e-06
Iter: 866 loss: 1.90805235e-06
Iter: 867 loss: 1.90713536e-06
Iter: 868 loss: 1.90638411e-06
Iter: 869 loss: 1.90985588e-06
Iter: 870 loss: 1.90636183e-06
Iter: 871 loss: 1.90521064e-06
Iter: 872 loss: 1.90655521e-06
Iter: 873 loss: 1.90481217e-06
Iter: 874 loss: 1.90406547e-06
Iter: 875 loss: 1.90505921e-06
Iter: 876 loss: 1.90371497e-06
Iter: 877 loss: 1.90287642e-06
Iter: 878 loss: 1.90392552e-06
Iter: 879 loss: 1.90242929e-06
Iter: 880 loss: 1.90151877e-06
Iter: 881 loss: 1.9088327e-06
Iter: 882 loss: 1.90138439e-06
Iter: 883 loss: 1.90047763e-06
Iter: 884 loss: 1.90206219e-06
Iter: 885 loss: 1.90004755e-06
Iter: 886 loss: 1.89928869e-06
Iter: 887 loss: 1.90047092e-06
Iter: 888 loss: 1.8988344e-06
Iter: 889 loss: 1.89929358e-06
Iter: 890 loss: 1.89853267e-06
Iter: 891 loss: 1.89838011e-06
Iter: 892 loss: 1.89802518e-06
Iter: 893 loss: 1.89806599e-06
Iter: 894 loss: 1.89772948e-06
Iter: 895 loss: 1.89797822e-06
Iter: 896 loss: 1.89740854e-06
Iter: 897 loss: 1.89667992e-06
Iter: 898 loss: 1.89717548e-06
Iter: 899 loss: 1.89637058e-06
Iter: 900 loss: 1.89565787e-06
Iter: 901 loss: 1.89510683e-06
Iter: 902 loss: 1.89501043e-06
Iter: 903 loss: 1.89410105e-06
Iter: 904 loss: 1.89636319e-06
Iter: 905 loss: 1.89365699e-06
Iter: 906 loss: 1.89293314e-06
Iter: 907 loss: 1.89285788e-06
Iter: 908 loss: 1.89226216e-06
Iter: 909 loss: 1.89176421e-06
Iter: 910 loss: 1.89160357e-06
Iter: 911 loss: 1.89065122e-06
Iter: 912 loss: 1.89135744e-06
Iter: 913 loss: 1.89014634e-06
Iter: 914 loss: 1.88911031e-06
Iter: 915 loss: 1.89740172e-06
Iter: 916 loss: 1.88909144e-06
Iter: 917 loss: 1.88836702e-06
Iter: 918 loss: 1.88939327e-06
Iter: 919 loss: 1.88811919e-06
Iter: 920 loss: 1.88763374e-06
Iter: 921 loss: 1.88766921e-06
Iter: 922 loss: 1.88703279e-06
Iter: 923 loss: 1.88935621e-06
Iter: 924 loss: 1.88696197e-06
Iter: 925 loss: 1.88659817e-06
Iter: 926 loss: 1.88628587e-06
Iter: 927 loss: 1.88615309e-06
Iter: 928 loss: 1.8857163e-06
Iter: 929 loss: 1.88842705e-06
Iter: 930 loss: 1.88568924e-06
Iter: 931 loss: 1.8852686e-06
Iter: 932 loss: 1.88539866e-06
Iter: 933 loss: 1.88491811e-06
Iter: 934 loss: 1.88441493e-06
Iter: 935 loss: 1.88503668e-06
Iter: 936 loss: 1.88413355e-06
Iter: 937 loss: 1.88346712e-06
Iter: 938 loss: 1.8838723e-06
Iter: 939 loss: 1.88306808e-06
Iter: 940 loss: 1.8823805e-06
Iter: 941 loss: 1.88340175e-06
Iter: 942 loss: 1.88207753e-06
Iter: 943 loss: 1.88132776e-06
Iter: 944 loss: 1.88714671e-06
Iter: 945 loss: 1.88113222e-06
Iter: 946 loss: 1.88038553e-06
Iter: 947 loss: 1.88020647e-06
Iter: 948 loss: 1.8797316e-06
Iter: 949 loss: 1.8788046e-06
Iter: 950 loss: 1.87807677e-06
Iter: 951 loss: 1.87772287e-06
Iter: 952 loss: 1.87657781e-06
Iter: 953 loss: 1.8901768e-06
Iter: 954 loss: 1.87658236e-06
Iter: 955 loss: 1.87580576e-06
Iter: 956 loss: 1.87920591e-06
Iter: 957 loss: 1.87577848e-06
Iter: 958 loss: 1.87542196e-06
Iter: 959 loss: 1.87530088e-06
Iter: 960 loss: 1.87500018e-06
Iter: 961 loss: 1.87444482e-06
Iter: 962 loss: 1.88135141e-06
Iter: 963 loss: 1.87426883e-06
Iter: 964 loss: 1.8738765e-06
Iter: 965 loss: 1.87804244e-06
Iter: 966 loss: 1.87374383e-06
Iter: 967 loss: 1.87339629e-06
Iter: 968 loss: 1.87425599e-06
Iter: 969 loss: 1.87323371e-06
Iter: 970 loss: 1.87267824e-06
Iter: 971 loss: 1.87213914e-06
Iter: 972 loss: 1.87196531e-06
Iter: 973 loss: 1.87113812e-06
Iter: 974 loss: 1.87439423e-06
Iter: 975 loss: 1.87094065e-06
Iter: 976 loss: 1.87021e-06
Iter: 977 loss: 1.87363798e-06
Iter: 978 loss: 1.87007504e-06
Iter: 979 loss: 1.86944055e-06
Iter: 980 loss: 1.86887814e-06
Iter: 981 loss: 1.86864827e-06
Iter: 982 loss: 1.86803993e-06
Iter: 983 loss: 1.86804323e-06
Iter: 984 loss: 1.86755665e-06
Iter: 985 loss: 1.8666492e-06
Iter: 986 loss: 1.88215552e-06
Iter: 987 loss: 1.86666693e-06
Iter: 988 loss: 1.86573868e-06
Iter: 989 loss: 1.87751721e-06
Iter: 990 loss: 1.86567286e-06
Iter: 991 loss: 1.86565421e-06
Iter: 992 loss: 1.86543957e-06
Iter: 993 loss: 1.86522971e-06
Iter: 994 loss: 1.8648426e-06
Iter: 995 loss: 1.87533306e-06
Iter: 996 loss: 1.86486398e-06
Iter: 997 loss: 1.86438274e-06
Iter: 998 loss: 1.86449245e-06
Iter: 999 loss: 1.86413899e-06
Iter: 1000 loss: 1.86354714e-06
Iter: 1001 loss: 1.86984903e-06
Iter: 1002 loss: 1.86353645e-06
Iter: 1003 loss: 1.86310365e-06
Iter: 1004 loss: 1.86310047e-06
Iter: 1005 loss: 1.86272325e-06
Iter: 1006 loss: 1.86216141e-06
Iter: 1007 loss: 1.86319312e-06
Iter: 1008 loss: 1.86201885e-06
Iter: 1009 loss: 1.86146985e-06
Iter: 1010 loss: 1.86263128e-06
Iter: 1011 loss: 1.86119087e-06
Iter: 1012 loss: 1.86044849e-06
Iter: 1013 loss: 1.860574e-06
Iter: 1014 loss: 1.85986892e-06
Iter: 1015 loss: 1.85923295e-06
Iter: 1016 loss: 1.86293391e-06
Iter: 1017 loss: 1.85919623e-06
Iter: 1018 loss: 1.85845306e-06
Iter: 1019 loss: 1.85952206e-06
Iter: 1020 loss: 1.8580738e-06
Iter: 1021 loss: 1.85734029e-06
Iter: 1022 loss: 1.85771887e-06
Iter: 1023 loss: 1.8569018e-06
Iter: 1024 loss: 1.8583537e-06
Iter: 1025 loss: 1.85675128e-06
Iter: 1026 loss: 1.85656199e-06
Iter: 1027 loss: 1.85622662e-06
Iter: 1028 loss: 1.86026443e-06
Iter: 1029 loss: 1.85616091e-06
Iter: 1030 loss: 1.85554859e-06
Iter: 1031 loss: 1.85600175e-06
Iter: 1032 loss: 1.85535009e-06
Iter: 1033 loss: 1.85470708e-06
Iter: 1034 loss: 1.85994509e-06
Iter: 1035 loss: 1.85471595e-06
Iter: 1036 loss: 1.85418139e-06
Iter: 1037 loss: 1.8563444e-06
Iter: 1038 loss: 1.85406464e-06
Iter: 1039 loss: 1.85370777e-06
Iter: 1040 loss: 1.85366684e-06
Iter: 1041 loss: 1.85345516e-06
Iter: 1042 loss: 1.85299223e-06
Iter: 1043 loss: 1.8530036e-06
Iter: 1044 loss: 1.85256715e-06
Iter: 1045 loss: 1.85187207e-06
Iter: 1046 loss: 1.85705915e-06
Iter: 1047 loss: 1.85190152e-06
Iter: 1048 loss: 1.8514263e-06
Iter: 1049 loss: 1.85108911e-06
Iter: 1050 loss: 1.85101385e-06
Iter: 1051 loss: 1.85024646e-06
Iter: 1052 loss: 1.85184888e-06
Iter: 1053 loss: 1.84993428e-06
Iter: 1054 loss: 1.84926273e-06
Iter: 1055 loss: 1.85940667e-06
Iter: 1056 loss: 1.8493032e-06
Iter: 1057 loss: 1.84897488e-06
Iter: 1058 loss: 1.84808641e-06
Iter: 1059 loss: 1.85694944e-06
Iter: 1060 loss: 1.84783676e-06
Iter: 1061 loss: 1.84693249e-06
Iter: 1062 loss: 1.85710098e-06
Iter: 1063 loss: 1.8468711e-06
Iter: 1064 loss: 1.84798239e-06
Iter: 1065 loss: 1.84676855e-06
Iter: 1066 loss: 1.84670557e-06
Iter: 1067 loss: 1.84634371e-06
Iter: 1068 loss: 1.84591408e-06
Iter: 1069 loss: 1.84585406e-06
Iter: 1070 loss: 1.84479757e-06
Iter: 1071 loss: 1.85128704e-06
Iter: 1072 loss: 1.84461715e-06
Iter: 1073 loss: 1.84420469e-06
Iter: 1074 loss: 1.84568023e-06
Iter: 1075 loss: 1.84401654e-06
Iter: 1076 loss: 1.84342434e-06
Iter: 1077 loss: 1.85092154e-06
Iter: 1078 loss: 1.84340126e-06
Iter: 1079 loss: 1.84286182e-06
Iter: 1080 loss: 1.84264854e-06
Iter: 1081 loss: 1.84230976e-06
Iter: 1082 loss: 1.84115322e-06
Iter: 1083 loss: 1.8523117e-06
Iter: 1084 loss: 1.84112139e-06
Iter: 1085 loss: 1.84042983e-06
Iter: 1086 loss: 1.84541739e-06
Iter: 1087 loss: 1.8403781e-06
Iter: 1088 loss: 1.83954251e-06
Iter: 1089 loss: 1.83924305e-06
Iter: 1090 loss: 1.8386736e-06
Iter: 1091 loss: 1.83795896e-06
Iter: 1092 loss: 1.84165424e-06
Iter: 1093 loss: 1.83780014e-06
Iter: 1094 loss: 1.83737257e-06
Iter: 1095 loss: 1.84165276e-06
Iter: 1096 loss: 1.83734699e-06
Iter: 1097 loss: 1.83709221e-06
Iter: 1098 loss: 1.84160342e-06
Iter: 1099 loss: 1.83704083e-06
Iter: 1100 loss: 1.83667134e-06
Iter: 1101 loss: 1.83644738e-06
Iter: 1102 loss: 1.83627037e-06
Iter: 1103 loss: 1.8359392e-06
Iter: 1104 loss: 1.83546467e-06
Iter: 1105 loss: 1.8354142e-06
Iter: 1106 loss: 1.83493466e-06
Iter: 1107 loss: 1.83959435e-06
Iter: 1108 loss: 1.83484667e-06
Iter: 1109 loss: 1.83453426e-06
Iter: 1110 loss: 1.83589827e-06
Iter: 1111 loss: 1.83440011e-06
Iter: 1112 loss: 1.83412112e-06
Iter: 1113 loss: 1.83535053e-06
Iter: 1114 loss: 1.83406655e-06
Iter: 1115 loss: 1.83365842e-06
Iter: 1116 loss: 1.83422253e-06
Iter: 1117 loss: 1.83344264e-06
Iter: 1118 loss: 1.83308896e-06
Iter: 1119 loss: 1.83249438e-06
Iter: 1120 loss: 1.84422379e-06
Iter: 1121 loss: 1.83249165e-06
Iter: 1122 loss: 1.83203827e-06
Iter: 1123 loss: 1.83196971e-06
Iter: 1124 loss: 1.83167094e-06
Iter: 1125 loss: 1.83329621e-06
Iter: 1126 loss: 1.83158227e-06
Iter: 1127 loss: 1.83118118e-06
Iter: 1128 loss: 1.83059615e-06
Iter: 1129 loss: 1.83066879e-06
Iter: 1130 loss: 1.83040595e-06
Iter: 1131 loss: 1.83026509e-06
Iter: 1132 loss: 1.8299736e-06
Iter: 1133 loss: 1.83264183e-06
Iter: 1134 loss: 1.82988288e-06
Iter: 1135 loss: 1.82977715e-06
Iter: 1136 loss: 1.82915744e-06
Iter: 1137 loss: 1.8315618e-06
Iter: 1138 loss: 1.82892506e-06
Iter: 1139 loss: 1.82846918e-06
Iter: 1140 loss: 1.82916608e-06
Iter: 1141 loss: 1.82828944e-06
Iter: 1142 loss: 1.82749045e-06
Iter: 1143 loss: 1.83315206e-06
Iter: 1144 loss: 1.82745055e-06
Iter: 1145 loss: 1.82692133e-06
Iter: 1146 loss: 1.82875829e-06
Iter: 1147 loss: 1.82677195e-06
Iter: 1148 loss: 1.82642475e-06
Iter: 1149 loss: 1.83133375e-06
Iter: 1150 loss: 1.82646738e-06
Iter: 1151 loss: 1.82617657e-06
Iter: 1152 loss: 1.82619351e-06
Iter: 1153 loss: 1.82596477e-06
Iter: 1154 loss: 1.82567214e-06
Iter: 1155 loss: 1.82498161e-06
Iter: 1156 loss: 1.83233158e-06
Iter: 1157 loss: 1.82493636e-06
Iter: 1158 loss: 1.82459723e-06
Iter: 1159 loss: 1.82458029e-06
Iter: 1160 loss: 1.82416761e-06
Iter: 1161 loss: 1.82511167e-06
Iter: 1162 loss: 1.82408917e-06
Iter: 1163 loss: 1.82359895e-06
Iter: 1164 loss: 1.82309532e-06
Iter: 1165 loss: 1.82306576e-06
Iter: 1166 loss: 1.82432791e-06
Iter: 1167 loss: 1.82296685e-06
Iter: 1168 loss: 1.82284884e-06
Iter: 1169 loss: 1.82254212e-06
Iter: 1170 loss: 1.82285521e-06
Iter: 1171 loss: 1.82232543e-06
Iter: 1172 loss: 1.82176598e-06
Iter: 1173 loss: 1.82201416e-06
Iter: 1174 loss: 1.82144686e-06
Iter: 1175 loss: 1.82104236e-06
Iter: 1176 loss: 1.82684437e-06
Iter: 1177 loss: 1.82099393e-06
Iter: 1178 loss: 1.82062161e-06
Iter: 1179 loss: 1.82039707e-06
Iter: 1180 loss: 1.82027907e-06
Iter: 1181 loss: 1.8196431e-06
Iter: 1182 loss: 1.82764143e-06
Iter: 1183 loss: 1.81974906e-06
Iter: 1184 loss: 1.81937878e-06
Iter: 1185 loss: 1.82105794e-06
Iter: 1186 loss: 1.81931091e-06
Iter: 1187 loss: 1.81884343e-06
Iter: 1188 loss: 1.8186272e-06
Iter: 1189 loss: 1.81850226e-06
Iter: 1190 loss: 1.81791825e-06
Iter: 1191 loss: 1.81903579e-06
Iter: 1192 loss: 1.81765108e-06
Iter: 1193 loss: 1.81729706e-06
Iter: 1194 loss: 1.81751329e-06
Iter: 1195 loss: 1.81696305e-06
Iter: 1196 loss: 1.81630674e-06
Iter: 1197 loss: 1.81871815e-06
Iter: 1198 loss: 1.81620351e-06
Iter: 1199 loss: 1.8158828e-06
Iter: 1200 loss: 1.81593805e-06
Iter: 1201 loss: 1.81555356e-06
Iter: 1202 loss: 1.8158662e-06
Iter: 1203 loss: 1.81539531e-06
Iter: 1204 loss: 1.81525843e-06
Iter: 1205 loss: 1.81477287e-06
Iter: 1206 loss: 1.81775738e-06
Iter: 1207 loss: 1.8146095e-06
Iter: 1208 loss: 1.81388828e-06
Iter: 1209 loss: 1.81871565e-06
Iter: 1210 loss: 1.81388702e-06
Iter: 1211 loss: 1.81334576e-06
Iter: 1212 loss: 1.81585096e-06
Iter: 1213 loss: 1.81333587e-06
Iter: 1214 loss: 1.81302084e-06
Iter: 1215 loss: 1.81591906e-06
Iter: 1216 loss: 1.81297105e-06
Iter: 1217 loss: 1.81279734e-06
Iter: 1218 loss: 1.81429368e-06
Iter: 1219 loss: 1.81278506e-06
Iter: 1220 loss: 1.81248083e-06
Iter: 1221 loss: 1.81256235e-06
Iter: 1222 loss: 1.81240421e-06
Iter: 1223 loss: 1.81197493e-06
Iter: 1224 loss: 1.8126716e-06
Iter: 1225 loss: 1.81186761e-06
Iter: 1226 loss: 1.81152632e-06
Iter: 1227 loss: 1.81157657e-06
Iter: 1228 loss: 1.81128325e-06
Iter: 1229 loss: 1.81092355e-06
Iter: 1230 loss: 1.81229416e-06
Iter: 1231 loss: 1.81078258e-06
Iter: 1232 loss: 1.81043686e-06
Iter: 1233 loss: 1.81408632e-06
Iter: 1234 loss: 1.81040377e-06
Iter: 1235 loss: 1.81004884e-06
Iter: 1236 loss: 1.81345877e-06
Iter: 1237 loss: 1.81007522e-06
Iter: 1238 loss: 1.80995914e-06
Iter: 1239 loss: 1.80966708e-06
Iter: 1240 loss: 1.81119935e-06
Iter: 1241 loss: 1.80954908e-06
Iter: 1242 loss: 1.80927873e-06
Iter: 1243 loss: 1.80930874e-06
Iter: 1244 loss: 1.80889811e-06
Iter: 1245 loss: 1.80854272e-06
Iter: 1246 loss: 1.81226574e-06
Iter: 1247 loss: 1.80850657e-06
Iter: 1248 loss: 1.80824782e-06
Iter: 1249 loss: 1.80853374e-06
Iter: 1250 loss: 1.80808854e-06
Iter: 1251 loss: 1.80766767e-06
Iter: 1252 loss: 1.80952622e-06
Iter: 1253 loss: 1.80760389e-06
Iter: 1254 loss: 1.80725783e-06
Iter: 1255 loss: 1.80928271e-06
Iter: 1256 loss: 1.80730876e-06
Iter: 1257 loss: 1.80703159e-06
Iter: 1258 loss: 1.80673669e-06
Iter: 1259 loss: 1.80670429e-06
Iter: 1260 loss: 1.80624954e-06
Iter: 1261 loss: 1.80714596e-06
Iter: 1262 loss: 1.80595589e-06
Iter: 1263 loss: 1.80548318e-06
Iter: 1264 loss: 1.80562233e-06
Iter: 1265 loss: 1.8052159e-06
Iter: 1266 loss: 1.80477559e-06
Iter: 1267 loss: 1.80470647e-06
Iter: 1268 loss: 1.80460393e-06
Iter: 1269 loss: 1.80461257e-06
Iter: 1270 loss: 1.80454333e-06
Iter: 1271 loss: 1.80418533e-06
Iter: 1272 loss: 1.80544077e-06
Iter: 1273 loss: 1.80405141e-06
Iter: 1274 loss: 1.80351185e-06
Iter: 1275 loss: 1.8036352e-06
Iter: 1276 loss: 1.80314942e-06
Iter: 1277 loss: 1.8028411e-06
Iter: 1278 loss: 1.80286679e-06
Iter: 1279 loss: 1.80250618e-06
Iter: 1280 loss: 1.80235713e-06
Iter: 1281 loss: 1.80217239e-06
Iter: 1282 loss: 1.80169695e-06
Iter: 1283 loss: 1.80521909e-06
Iter: 1284 loss: 1.80163238e-06
Iter: 1285 loss: 1.80124619e-06
Iter: 1286 loss: 1.80214863e-06
Iter: 1287 loss: 1.80122743e-06
Iter: 1288 loss: 1.80080428e-06
Iter: 1289 loss: 1.80211623e-06
Iter: 1290 loss: 1.80069082e-06
Iter: 1291 loss: 1.80027723e-06
Iter: 1292 loss: 1.8003301e-06
Iter: 1293 loss: 1.80005486e-06
Iter: 1294 loss: 1.79952099e-06
Iter: 1295 loss: 1.80385632e-06
Iter: 1296 loss: 1.79958465e-06
Iter: 1297 loss: 1.79936148e-06
Iter: 1298 loss: 1.79935796e-06
Iter: 1299 loss: 1.79913241e-06
Iter: 1300 loss: 1.79925792e-06
Iter: 1301 loss: 1.79908511e-06
Iter: 1302 loss: 1.79898279e-06
Iter: 1303 loss: 1.79872484e-06
Iter: 1304 loss: 1.80160373e-06
Iter: 1305 loss: 1.79872393e-06
Iter: 1306 loss: 1.79841823e-06
Iter: 1307 loss: 1.79836218e-06
Iter: 1308 loss: 1.79816493e-06
Iter: 1309 loss: 1.79791346e-06
Iter: 1310 loss: 1.79822666e-06
Iter: 1311 loss: 1.79756989e-06
Iter: 1312 loss: 1.79709639e-06
Iter: 1313 loss: 1.79674225e-06
Iter: 1314 loss: 1.79669496e-06
Iter: 1315 loss: 1.79637868e-06
Iter: 1316 loss: 1.79626522e-06
Iter: 1317 loss: 1.79589892e-06
Iter: 1318 loss: 1.79609231e-06
Iter: 1319 loss: 1.79586482e-06
Iter: 1320 loss: 1.79549227e-06
Iter: 1321 loss: 1.79599908e-06
Iter: 1322 loss: 1.79527376e-06
Iter: 1323 loss: 1.79477843e-06
Iter: 1324 loss: 1.79523136e-06
Iter: 1325 loss: 1.79457038e-06
Iter: 1326 loss: 1.79422318e-06
Iter: 1327 loss: 1.7964785e-06
Iter: 1328 loss: 1.79416702e-06
Iter: 1329 loss: 1.79366157e-06
Iter: 1330 loss: 1.79372023e-06
Iter: 1331 loss: 1.793362e-06
Iter: 1332 loss: 1.79322456e-06
Iter: 1333 loss: 1.79299661e-06
Iter: 1334 loss: 1.79293511e-06
Iter: 1335 loss: 1.79261178e-06
Iter: 1336 loss: 1.79699134e-06
Iter: 1337 loss: 1.79267204e-06
Iter: 1338 loss: 1.79217352e-06
Iter: 1339 loss: 1.79220388e-06
Iter: 1340 loss: 1.79184121e-06
Iter: 1341 loss: 1.79126744e-06
Iter: 1342 loss: 1.79818585e-06
Iter: 1343 loss: 1.79129631e-06
Iter: 1344 loss: 1.7909008e-06
Iter: 1345 loss: 1.79010522e-06
Iter: 1346 loss: 1.80664279e-06
Iter: 1347 loss: 1.79014501e-06
Iter: 1348 loss: 1.78963501e-06
Iter: 1349 loss: 1.79712924e-06
Iter: 1350 loss: 1.78966661e-06
Iter: 1351 loss: 1.78924574e-06
Iter: 1352 loss: 1.78932635e-06
Iter: 1353 loss: 1.78910886e-06
Iter: 1354 loss: 1.78862888e-06
Iter: 1355 loss: 1.79949166e-06
Iter: 1356 loss: 1.78857636e-06
Iter: 1357 loss: 1.78813411e-06
Iter: 1358 loss: 1.79372535e-06
Iter: 1359 loss: 1.78816958e-06
Iter: 1360 loss: 1.78786502e-06
Iter: 1361 loss: 1.78734922e-06
Iter: 1362 loss: 1.78734126e-06
Iter: 1363 loss: 1.78698633e-06
Iter: 1364 loss: 1.79320773e-06
Iter: 1365 loss: 1.78695973e-06
Iter: 1366 loss: 1.78662481e-06
Iter: 1367 loss: 1.78743448e-06
Iter: 1368 loss: 1.78653e-06
Iter: 1369 loss: 1.78611367e-06
Iter: 1370 loss: 1.7909806e-06
Iter: 1371 loss: 1.78616915e-06
Iter: 1372 loss: 1.78605592e-06
Iter: 1373 loss: 1.78576477e-06
Iter: 1374 loss: 1.78993389e-06
Iter: 1375 loss: 1.7857385e-06
Iter: 1376 loss: 1.78539653e-06
Iter: 1377 loss: 1.7860915e-06
Iter: 1378 loss: 1.78523055e-06
Iter: 1379 loss: 1.78493326e-06
Iter: 1380 loss: 1.78910295e-06
Iter: 1381 loss: 1.78496225e-06
Iter: 1382 loss: 1.78466462e-06
Iter: 1383 loss: 1.78445396e-06
Iter: 1384 loss: 1.78439836e-06
Iter: 1385 loss: 1.78401547e-06
Iter: 1386 loss: 1.78445259e-06
Iter: 1387 loss: 1.78380287e-06
Iter: 1388 loss: 1.78340395e-06
Iter: 1389 loss: 1.78333403e-06
Iter: 1390 loss: 1.78320147e-06
Iter: 1391 loss: 1.78330288e-06
Iter: 1392 loss: 1.78302707e-06
Iter: 1393 loss: 1.78276582e-06
Iter: 1394 loss: 1.78313769e-06
Iter: 1395 loss: 1.78259984e-06
Iter: 1396 loss: 1.7823329e-06
Iter: 1397 loss: 1.78163805e-06
Iter: 1398 loss: 1.7815579e-06
Iter: 1399 loss: 1.78188179e-06
Iter: 1400 loss: 1.78140522e-06
Iter: 1401 loss: 1.78119706e-06
Iter: 1402 loss: 1.78278015e-06
Iter: 1403 loss: 1.78116818e-06
Iter: 1404 loss: 1.78099094e-06
Iter: 1405 loss: 1.78058622e-06
Iter: 1406 loss: 1.7881498e-06
Iter: 1407 loss: 1.7806035e-06
Iter: 1408 loss: 1.78020525e-06
Iter: 1409 loss: 1.77995787e-06
Iter: 1410 loss: 1.77989773e-06
Iter: 1411 loss: 1.77939842e-06
Iter: 1412 loss: 1.78569462e-06
Iter: 1413 loss: 1.77941752e-06
Iter: 1414 loss: 1.77897186e-06
Iter: 1415 loss: 1.7787213e-06
Iter: 1416 loss: 1.77844618e-06
Iter: 1417 loss: 1.7779779e-06
Iter: 1418 loss: 1.77982474e-06
Iter: 1419 loss: 1.77789957e-06
Iter: 1420 loss: 1.77759273e-06
Iter: 1421 loss: 1.78182188e-06
Iter: 1422 loss: 1.77757136e-06
Iter: 1423 loss: 1.77724314e-06
Iter: 1424 loss: 1.77745096e-06
Iter: 1425 loss: 1.7770825e-06
Iter: 1426 loss: 1.77680738e-06
Iter: 1427 loss: 1.77736365e-06
Iter: 1428 loss: 1.77659808e-06
Iter: 1429 loss: 1.77631864e-06
Iter: 1430 loss: 1.77738525e-06
Iter: 1431 loss: 1.77626748e-06
Iter: 1432 loss: 1.77588117e-06
Iter: 1433 loss: 1.77581489e-06
Iter: 1434 loss: 1.77562424e-06
Iter: 1435 loss: 1.77591062e-06
Iter: 1436 loss: 1.77558957e-06
Iter: 1437 loss: 1.77539334e-06
Iter: 1438 loss: 1.77519314e-06
Iter: 1439 loss: 1.77518007e-06
Iter: 1440 loss: 1.77500397e-06
Iter: 1441 loss: 1.77489244e-06
Iter: 1442 loss: 1.77472793e-06
Iter: 1443 loss: 1.77431127e-06
Iter: 1444 loss: 1.77416791e-06
Iter: 1445 loss: 1.77400852e-06
Iter: 1446 loss: 1.77349932e-06
Iter: 1447 loss: 1.77353627e-06
Iter: 1448 loss: 1.77319123e-06
Iter: 1449 loss: 1.77267225e-06
Iter: 1450 loss: 1.77267077e-06
Iter: 1451 loss: 1.7720406e-06
Iter: 1452 loss: 1.77282561e-06
Iter: 1453 loss: 1.77167396e-06
Iter: 1454 loss: 1.77125469e-06
Iter: 1455 loss: 1.7786615e-06
Iter: 1456 loss: 1.77113623e-06
Iter: 1457 loss: 1.77075253e-06
Iter: 1458 loss: 1.77475351e-06
Iter: 1459 loss: 1.7706891e-06
Iter: 1460 loss: 1.77038726e-06
Iter: 1461 loss: 1.77001994e-06
Iter: 1462 loss: 1.76994695e-06
Iter: 1463 loss: 1.76930712e-06
Iter: 1464 loss: 1.77079403e-06
Iter: 1465 loss: 1.76906974e-06
Iter: 1466 loss: 1.76867024e-06
Iter: 1467 loss: 1.77077425e-06
Iter: 1468 loss: 1.76850904e-06
Iter: 1469 loss: 1.7686757e-06
Iter: 1470 loss: 1.76843537e-06
Iter: 1471 loss: 1.76831804e-06
Iter: 1472 loss: 1.76803042e-06
Iter: 1473 loss: 1.76929291e-06
Iter: 1474 loss: 1.76783215e-06
Iter: 1475 loss: 1.76743902e-06
Iter: 1476 loss: 1.76774313e-06
Iter: 1477 loss: 1.76716856e-06
Iter: 1478 loss: 1.76693572e-06
Iter: 1479 loss: 1.76685489e-06
Iter: 1480 loss: 1.76670551e-06
Iter: 1481 loss: 1.76633193e-06
Iter: 1482 loss: 1.76632579e-06
Iter: 1483 loss: 1.76597291e-06
Iter: 1484 loss: 1.7691425e-06
Iter: 1485 loss: 1.76590618e-06
Iter: 1486 loss: 1.76547519e-06
Iter: 1487 loss: 1.76760307e-06
Iter: 1488 loss: 1.76545313e-06
Iter: 1489 loss: 1.76521098e-06
Iter: 1490 loss: 1.76541175e-06
Iter: 1491 loss: 1.76509627e-06
Iter: 1492 loss: 1.7647867e-06
Iter: 1493 loss: 1.7680984e-06
Iter: 1494 loss: 1.76476817e-06
Iter: 1495 loss: 1.76462345e-06
Iter: 1496 loss: 1.76428159e-06
Iter: 1497 loss: 1.76873345e-06
Iter: 1498 loss: 1.76437902e-06
Iter: 1499 loss: 1.76387857e-06
Iter: 1500 loss: 1.76807248e-06
Iter: 1501 loss: 1.76380331e-06
Iter: 1502 loss: 1.76341177e-06
Iter: 1503 loss: 1.76427943e-06
Iter: 1504 loss: 1.76327762e-06
Iter: 1505 loss: 1.76324215e-06
Iter: 1506 loss: 1.76314575e-06
Iter: 1507 loss: 1.76299955e-06
Iter: 1508 loss: 1.76273079e-06
Iter: 1509 loss: 1.76278672e-06
Iter: 1510 loss: 1.76233323e-06
Iter: 1511 loss: 1.76287426e-06
Iter: 1512 loss: 1.76228809e-06
Iter: 1513 loss: 1.7618629e-06
Iter: 1514 loss: 1.76195897e-06
Iter: 1515 loss: 1.7616976e-06
Iter: 1516 loss: 1.76134108e-06
Iter: 1517 loss: 1.76632057e-06
Iter: 1518 loss: 1.76136348e-06
Iter: 1519 loss: 1.76102026e-06
Iter: 1520 loss: 1.76113349e-06
Iter: 1521 loss: 1.76086951e-06
Iter: 1522 loss: 1.76062213e-06
Iter: 1523 loss: 1.76437732e-06
Iter: 1524 loss: 1.76075014e-06
Iter: 1525 loss: 1.76045887e-06
Iter: 1526 loss: 1.76079038e-06
Iter: 1527 loss: 1.76036724e-06
Iter: 1528 loss: 1.7602772e-06
Iter: 1529 loss: 1.75975401e-06
Iter: 1530 loss: 1.7684979e-06
Iter: 1531 loss: 1.75973469e-06
Iter: 1532 loss: 1.75949765e-06
Iter: 1533 loss: 1.76372589e-06
Iter: 1534 loss: 1.75942057e-06
Iter: 1535 loss: 1.75925811e-06
Iter: 1536 loss: 1.75915955e-06
Iter: 1537 loss: 1.75907462e-06
Iter: 1538 loss: 1.75876733e-06
Iter: 1539 loss: 1.76009712e-06
Iter: 1540 loss: 1.75866762e-06
Iter: 1541 loss: 1.7581923e-06
Iter: 1542 loss: 1.76055346e-06
Iter: 1543 loss: 1.75820196e-06
Iter: 1544 loss: 1.75799312e-06
Iter: 1545 loss: 1.75764012e-06
Iter: 1546 loss: 1.75768275e-06
Iter: 1547 loss: 1.75734044e-06
Iter: 1548 loss: 1.75749392e-06
Iter: 1549 loss: 1.7572e-06
Iter: 1550 loss: 1.75679088e-06
Iter: 1551 loss: 1.75790217e-06
Iter: 1552 loss: 1.75659773e-06
Iter: 1553 loss: 1.7561274e-06
Iter: 1554 loss: 1.76057756e-06
Iter: 1555 loss: 1.75614161e-06
Iter: 1556 loss: 1.7558948e-06
Iter: 1557 loss: 1.75629384e-06
Iter: 1558 loss: 1.75574837e-06
Iter: 1559 loss: 1.7554371e-06
Iter: 1560 loss: 1.75760124e-06
Iter: 1561 loss: 1.75531795e-06
Iter: 1562 loss: 1.75511195e-06
Iter: 1563 loss: 1.75536127e-06
Iter: 1564 loss: 1.75493e-06
Iter: 1565 loss: 1.7546887e-06
Iter: 1566 loss: 1.75460013e-06
Iter: 1567 loss: 1.75435412e-06
Iter: 1568 loss: 1.7539769e-06
Iter: 1569 loss: 1.75521427e-06
Iter: 1570 loss: 1.75396235e-06
Iter: 1571 loss: 1.75345417e-06
Iter: 1572 loss: 1.75384503e-06
Iter: 1573 loss: 1.75329512e-06
Iter: 1574 loss: 1.75318405e-06
Iter: 1575 loss: 1.7530092e-06
Iter: 1576 loss: 1.75279979e-06
Iter: 1577 loss: 1.7524751e-06
Iter: 1578 loss: 1.761818e-06
Iter: 1579 loss: 1.75246805e-06
Iter: 1580 loss: 1.75214757e-06
Iter: 1581 loss: 1.75178559e-06
Iter: 1582 loss: 1.75188666e-06
Iter: 1583 loss: 1.75133016e-06
Iter: 1584 loss: 1.75211699e-06
Iter: 1585 loss: 1.75112905e-06
Iter: 1586 loss: 1.75075229e-06
Iter: 1587 loss: 1.75436367e-06
Iter: 1588 loss: 1.75075115e-06
Iter: 1589 loss: 1.75044238e-06
Iter: 1590 loss: 1.75104356e-06
Iter: 1591 loss: 1.75031983e-06
Iter: 1592 loss: 1.75004834e-06
Iter: 1593 loss: 1.75004834e-06
Iter: 1594 loss: 1.74995102e-06
Iter: 1595 loss: 1.74964032e-06
Iter: 1596 loss: 1.74967317e-06
Iter: 1597 loss: 1.74954664e-06
Iter: 1598 loss: 1.74937281e-06
Iter: 1599 loss: 1.74933894e-06
Iter: 1600 loss: 1.74911315e-06
Iter: 1601 loss: 1.7492697e-06
Iter: 1602 loss: 1.74886441e-06
Iter: 1603 loss: 1.74868114e-06
Iter: 1604 loss: 1.75050945e-06
Iter: 1605 loss: 1.74864931e-06
Iter: 1606 loss: 1.74847696e-06
Iter: 1607 loss: 1.74914067e-06
Iter: 1608 loss: 1.74842251e-06
Iter: 1609 loss: 1.74818365e-06
Iter: 1610 loss: 1.75096307e-06
Iter: 1611 loss: 1.74824106e-06
Iter: 1612 loss: 1.74800789e-06
Iter: 1613 loss: 1.74758463e-06
Iter: 1614 loss: 1.74847628e-06
Iter: 1615 loss: 1.74741183e-06
Iter: 1616 loss: 1.74703575e-06
Iter: 1617 loss: 1.74696072e-06
Iter: 1618 loss: 1.74676279e-06
Iter: 1619 loss: 1.74691081e-06
Iter: 1620 loss: 1.74650347e-06
Iter: 1621 loss: 1.74624324e-06
Iter: 1622 loss: 1.74729917e-06
Iter: 1623 loss: 1.74624108e-06
Iter: 1624 loss: 1.74591582e-06
Iter: 1625 loss: 1.74714228e-06
Iter: 1626 loss: 1.74584852e-06
Iter: 1627 loss: 1.74554486e-06
Iter: 1628 loss: 1.74595743e-06
Iter: 1629 loss: 1.74535876e-06
Iter: 1630 loss: 1.7450127e-06
Iter: 1631 loss: 1.74899583e-06
Iter: 1632 loss: 1.74495403e-06
Iter: 1633 loss: 1.74480897e-06
Iter: 1634 loss: 1.74452634e-06
Iter: 1635 loss: 1.74447803e-06
Iter: 1636 loss: 1.74412253e-06
Iter: 1637 loss: 1.74501679e-06
Iter: 1638 loss: 1.74408831e-06
Iter: 1639 loss: 1.74400293e-06
Iter: 1640 loss: 1.74363629e-06
Iter: 1641 loss: 1.74713523e-06
Iter: 1642 loss: 1.74360775e-06
Iter: 1643 loss: 1.74389834e-06
Iter: 1644 loss: 1.74352226e-06
Iter: 1645 loss: 1.74333138e-06
Iter: 1646 loss: 1.74322486e-06
Iter: 1647 loss: 1.74315232e-06
Iter: 1648 loss: 1.74294985e-06
Iter: 1649 loss: 1.74263039e-06
Iter: 1650 loss: 1.74253682e-06
Iter: 1651 loss: 1.74209981e-06
Iter: 1652 loss: 1.74374486e-06
Iter: 1653 loss: 1.74194736e-06
Iter: 1654 loss: 1.74157697e-06
Iter: 1655 loss: 1.74160391e-06
Iter: 1656 loss: 1.74136721e-06
Iter: 1657 loss: 1.74150614e-06
Iter: 1658 loss: 1.74124807e-06
Iter: 1659 loss: 1.74083e-06
Iter: 1660 loss: 1.74067191e-06
Iter: 1661 loss: 1.74044885e-06
Iter: 1662 loss: 1.73993692e-06
Iter: 1663 loss: 1.74026366e-06
Iter: 1664 loss: 1.73969613e-06
Iter: 1665 loss: 1.73957881e-06
Iter: 1666 loss: 1.73929607e-06
Iter: 1667 loss: 1.73915555e-06
Iter: 1668 loss: 1.73877993e-06
Iter: 1669 loss: 1.74409229e-06
Iter: 1670 loss: 1.73874696e-06
Iter: 1671 loss: 1.73868591e-06
Iter: 1672 loss: 1.73847616e-06
Iter: 1673 loss: 1.73819512e-06
Iter: 1674 loss: 1.73994613e-06
Iter: 1675 loss: 1.73820888e-06
Iter: 1676 loss: 1.73797957e-06
Iter: 1677 loss: 1.73779404e-06
Iter: 1678 loss: 1.74555475e-06
Iter: 1679 loss: 1.73776516e-06
Iter: 1680 loss: 1.73745536e-06
Iter: 1681 loss: 1.73711601e-06
Iter: 1682 loss: 1.73717285e-06
Iter: 1683 loss: 1.73678404e-06
Iter: 1684 loss: 1.7376201e-06
Iter: 1685 loss: 1.73669116e-06
Iter: 1686 loss: 1.73651267e-06
Iter: 1687 loss: 1.73639262e-06
Iter: 1688 loss: 1.73619424e-06
Iter: 1689 loss: 1.73585363e-06
Iter: 1690 loss: 1.73652916e-06
Iter: 1691 loss: 1.7357014e-06
Iter: 1692 loss: 1.73529929e-06
Iter: 1693 loss: 1.73773799e-06
Iter: 1694 loss: 1.73523119e-06
Iter: 1695 loss: 1.73503088e-06
Iter: 1696 loss: 1.73551723e-06
Iter: 1697 loss: 1.73486319e-06
Iter: 1698 loss: 1.73448916e-06
Iter: 1699 loss: 1.7350726e-06
Iter: 1700 loss: 1.73426315e-06
Iter: 1701 loss: 1.73405419e-06
Iter: 1702 loss: 1.73408432e-06
Iter: 1703 loss: 1.73389981e-06
Iter: 1704 loss: 1.73353931e-06
Iter: 1705 loss: 1.74006959e-06
Iter: 1706 loss: 1.73359149e-06
Iter: 1707 loss: 1.73323576e-06
Iter: 1708 loss: 1.73591718e-06
Iter: 1709 loss: 1.73322849e-06
Iter: 1710 loss: 1.73307535e-06
Iter: 1711 loss: 1.73297713e-06
Iter: 1712 loss: 1.73289288e-06
Iter: 1713 loss: 1.73253989e-06
Iter: 1714 loss: 1.73719786e-06
Iter: 1715 loss: 1.73255216e-06
Iter: 1716 loss: 1.7323581e-06
Iter: 1717 loss: 1.73246838e-06
Iter: 1718 loss: 1.73206297e-06
Iter: 1719 loss: 1.73186072e-06
Iter: 1720 loss: 1.73516923e-06
Iter: 1721 loss: 1.7319104e-06
Iter: 1722 loss: 1.73165631e-06
Iter: 1723 loss: 1.73119838e-06
Iter: 1724 loss: 1.73130786e-06
Iter: 1725 loss: 1.73090416e-06
Iter: 1726 loss: 1.73422245e-06
Iter: 1727 loss: 1.73086505e-06
Iter: 1728 loss: 1.73054445e-06
Iter: 1729 loss: 1.73152876e-06
Iter: 1730 loss: 1.73047306e-06
Iter: 1731 loss: 1.73021226e-06
Iter: 1732 loss: 1.72983482e-06
Iter: 1733 loss: 1.7385953e-06
Iter: 1734 loss: 1.72979253e-06
Iter: 1735 loss: 1.72945511e-06
Iter: 1736 loss: 1.73157957e-06
Iter: 1737 loss: 1.72937575e-06
Iter: 1738 loss: 1.72921932e-06
Iter: 1739 loss: 1.73095964e-06
Iter: 1740 loss: 1.72916361e-06
Iter: 1741 loss: 1.72894e-06
Iter: 1742 loss: 1.72875241e-06
Iter: 1743 loss: 1.72877094e-06
Iter: 1744 loss: 1.72847081e-06
Iter: 1745 loss: 1.72869454e-06
Iter: 1746 loss: 1.72837929e-06
Iter: 1747 loss: 1.72822774e-06
Iter: 1748 loss: 1.72818488e-06
Iter: 1749 loss: 1.72809882e-06
Iter: 1750 loss: 1.72792738e-06
Iter: 1751 loss: 1.72793045e-06
Iter: 1752 loss: 1.72773025e-06
Iter: 1753 loss: 1.72755472e-06
Iter: 1754 loss: 1.72755313e-06
Iter: 1755 loss: 1.72728551e-06
Iter: 1756 loss: 1.7300473e-06
Iter: 1757 loss: 1.72725572e-06
Iter: 1758 loss: 1.72698128e-06
Iter: 1759 loss: 1.72702698e-06
Iter: 1760 loss: 1.72681928e-06
Iter: 1761 loss: 1.72658633e-06
Iter: 1762 loss: 1.72686123e-06
Iter: 1763 loss: 1.72653483e-06
Iter: 1764 loss: 1.72628643e-06
Iter: 1765 loss: 1.7290738e-06
Iter: 1766 loss: 1.72630894e-06
Iter: 1767 loss: 1.72607463e-06
Iter: 1768 loss: 1.72643968e-06
Iter: 1769 loss: 1.72603154e-06
Iter: 1770 loss: 1.72573186e-06
Iter: 1771 loss: 1.72632895e-06
Iter: 1772 loss: 1.72570708e-06
Iter: 1773 loss: 1.72541911e-06
Iter: 1774 loss: 1.72620275e-06
Iter: 1775 loss: 1.72536522e-06
Iter: 1776 loss: 1.72512978e-06
Iter: 1777 loss: 1.72515252e-06
Iter: 1778 loss: 1.72494447e-06
Iter: 1779 loss: 1.724767e-06
Iter: 1780 loss: 1.72481646e-06
Iter: 1781 loss: 1.72473028e-06
Iter: 1782 loss: 1.72466457e-06
Iter: 1783 loss: 1.72464e-06
Iter: 1784 loss: 1.72442878e-06
Iter: 1785 loss: 1.72421198e-06
Iter: 1786 loss: 1.72421e-06
Iter: 1787 loss: 1.72393072e-06
Iter: 1788 loss: 1.72387945e-06
Iter: 1789 loss: 1.72368618e-06
Iter: 1790 loss: 1.72345528e-06
Iter: 1791 loss: 1.72343778e-06
Iter: 1792 loss: 1.72325326e-06
Iter: 1793 loss: 1.72296291e-06
Iter: 1794 loss: 1.72968203e-06
Iter: 1795 loss: 1.72295574e-06
Iter: 1796 loss: 1.72265413e-06
Iter: 1797 loss: 1.72391333e-06
Iter: 1798 loss: 1.72270688e-06
Iter: 1799 loss: 1.72232762e-06
Iter: 1800 loss: 1.72424052e-06
Iter: 1801 loss: 1.72225168e-06
Iter: 1802 loss: 1.72203283e-06
Iter: 1803 loss: 1.72271609e-06
Iter: 1804 loss: 1.72196314e-06
Iter: 1805 loss: 1.72172645e-06
Iter: 1806 loss: 1.72210207e-06
Iter: 1807 loss: 1.72165164e-06
Iter: 1808 loss: 1.72136083e-06
Iter: 1809 loss: 1.72271245e-06
Iter: 1810 loss: 1.72127068e-06
Iter: 1811 loss: 1.72108753e-06
Iter: 1812 loss: 1.72163448e-06
Iter: 1813 loss: 1.72107173e-06
Iter: 1814 loss: 1.72079172e-06
Iter: 1815 loss: 1.72161072e-06
Iter: 1816 loss: 1.72075693e-06
Iter: 1817 loss: 1.72057241e-06
Iter: 1818 loss: 1.72064392e-06
Iter: 1819 loss: 1.72048453e-06
Iter: 1820 loss: 1.72018053e-06
Iter: 1821 loss: 1.72000409e-06
Iter: 1822 loss: 1.71995589e-06
Iter: 1823 loss: 1.71968327e-06
Iter: 1824 loss: 1.72120963e-06
Iter: 1825 loss: 1.71974716e-06
Iter: 1826 loss: 1.71943952e-06
Iter: 1827 loss: 1.7212285e-06
Iter: 1828 loss: 1.71931924e-06
Iter: 1829 loss: 1.71911609e-06
Iter: 1830 loss: 1.71877e-06
Iter: 1831 loss: 1.72505338e-06
Iter: 1832 loss: 1.71866304e-06
Iter: 1833 loss: 1.71832096e-06
Iter: 1834 loss: 1.71834495e-06
Iter: 1835 loss: 1.71817214e-06
Iter: 1836 loss: 1.720917e-06
Iter: 1837 loss: 1.71814872e-06
Iter: 1838 loss: 1.71789952e-06
Iter: 1839 loss: 1.71768784e-06
Iter: 1840 loss: 1.71764555e-06
Iter: 1841 loss: 1.71726538e-06
Iter: 1842 loss: 1.71824058e-06
Iter: 1843 loss: 1.71712077e-06
Iter: 1844 loss: 1.71700458e-06
Iter: 1845 loss: 1.71700037e-06
Iter: 1846 loss: 1.7168195e-06
Iter: 1847 loss: 1.71707484e-06
Iter: 1848 loss: 1.71671775e-06
Iter: 1849 loss: 1.71645229e-06
Iter: 1850 loss: 1.71738475e-06
Iter: 1851 loss: 1.71643671e-06
Iter: 1852 loss: 1.71639351e-06
Iter: 1853 loss: 1.71618876e-06
Iter: 1854 loss: 1.71615932e-06
Iter: 1855 loss: 1.71593751e-06
Iter: 1856 loss: 1.71663191e-06
Iter: 1857 loss: 1.71593319e-06
Iter: 1858 loss: 1.71565136e-06
Iter: 1859 loss: 1.71537499e-06
Iter: 1860 loss: 1.71530735e-06
Iter: 1861 loss: 1.71499016e-06
Iter: 1862 loss: 1.71630302e-06
Iter: 1863 loss: 1.71491638e-06
Iter: 1864 loss: 1.71458555e-06
Iter: 1865 loss: 1.71466104e-06
Iter: 1866 loss: 1.71434829e-06
Iter: 1867 loss: 1.71427e-06
Iter: 1868 loss: 1.71405179e-06
Iter: 1869 loss: 1.71393015e-06
Iter: 1870 loss: 1.71380793e-06
Iter: 1871 loss: 1.71375859e-06
Iter: 1872 loss: 1.71364388e-06
Iter: 1873 loss: 1.71353145e-06
Iter: 1874 loss: 1.71340787e-06
Iter: 1875 loss: 1.71316424e-06
Iter: 1876 loss: 1.71350916e-06
Iter: 1877 loss: 1.71306135e-06
Iter: 1878 loss: 1.71273973e-06
Iter: 1879 loss: 1.7127303e-06
Iter: 1880 loss: 1.71265856e-06
Iter: 1881 loss: 1.71337706e-06
Iter: 1882 loss: 1.71258171e-06
Iter: 1883 loss: 1.71246768e-06
Iter: 1884 loss: 1.7121497e-06
Iter: 1885 loss: 1.71818681e-06
Iter: 1886 loss: 1.71215925e-06
Iter: 1887 loss: 1.71187116e-06
Iter: 1888 loss: 1.71340957e-06
Iter: 1889 loss: 1.71192414e-06
Iter: 1890 loss: 1.7116796e-06
Iter: 1891 loss: 1.71174361e-06
Iter: 1892 loss: 1.71147622e-06
Iter: 1893 loss: 1.7112061e-06
Iter: 1894 loss: 1.71122917e-06
Iter: 1895 loss: 1.71097713e-06
Iter: 1896 loss: 1.71066392e-06
Iter: 1897 loss: 1.71181682e-06
Iter: 1898 loss: 1.71051204e-06
Iter: 1899 loss: 1.71035776e-06
Iter: 1900 loss: 1.71390798e-06
Iter: 1901 loss: 1.71021e-06
Iter: 1902 loss: 1.71002762e-06
Iter: 1903 loss: 1.71002807e-06
Iter: 1904 loss: 1.70991348e-06
Iter: 1905 loss: 1.70967837e-06
Iter: 1906 loss: 1.71270619e-06
Iter: 1907 loss: 1.70963426e-06
Iter: 1908 loss: 1.70939052e-06
Iter: 1909 loss: 1.71051863e-06
Iter: 1910 loss: 1.70943019e-06
Iter: 1911 loss: 1.70920816e-06
Iter: 1912 loss: 1.70953626e-06
Iter: 1913 loss: 1.70920521e-06
Iter: 1914 loss: 1.70899875e-06
Iter: 1915 loss: 1.70970497e-06
Iter: 1916 loss: 1.70903672e-06
Iter: 1917 loss: 1.70885e-06
Iter: 1918 loss: 1.70879503e-06
Iter: 1919 loss: 1.70873079e-06
Iter: 1920 loss: 1.70854457e-06
Iter: 1921 loss: 1.71059764e-06
Iter: 1922 loss: 1.70849762e-06
Iter: 1923 loss: 1.70835438e-06
Iter: 1924 loss: 1.70813655e-06
Iter: 1925 loss: 1.70987687e-06
Iter: 1926 loss: 1.70805276e-06
Iter: 1927 loss: 1.70778424e-06
Iter: 1928 loss: 1.71062698e-06
Iter: 1929 loss: 1.70776241e-06
Iter: 1930 loss: 1.70753515e-06
Iter: 1931 loss: 1.70880026e-06
Iter: 1932 loss: 1.70755391e-06
Iter: 1933 loss: 1.70735916e-06
Iter: 1934 loss: 1.70674082e-06
Iter: 1935 loss: 1.71341128e-06
Iter: 1936 loss: 1.70677765e-06
Iter: 1937 loss: 1.70642193e-06
Iter: 1938 loss: 1.70760711e-06
Iter: 1939 loss: 1.70628073e-06
Iter: 1940 loss: 1.70588055e-06
Iter: 1941 loss: 1.71105262e-06
Iter: 1942 loss: 1.7059017e-06
Iter: 1943 loss: 1.7057589e-06
Iter: 1944 loss: 1.70733415e-06
Iter: 1945 loss: 1.70574026e-06
Iter: 1946 loss: 1.70555563e-06
Iter: 1947 loss: 1.70667499e-06
Iter: 1948 loss: 1.70554449e-06
Iter: 1949 loss: 1.70543774e-06
Iter: 1950 loss: 1.70539715e-06
Iter: 1951 loss: 1.70534145e-06
Iter: 1952 loss: 1.70509907e-06
Iter: 1953 loss: 1.7062556e-06
Iter: 1954 loss: 1.70513442e-06
Iter: 1955 loss: 1.70493684e-06
Iter: 1956 loss: 1.70497128e-06
Iter: 1957 loss: 1.70483054e-06
Iter: 1958 loss: 1.70465341e-06
Iter: 1959 loss: 1.70504381e-06
Iter: 1960 loss: 1.70457145e-06
Iter: 1961 loss: 1.70435669e-06
Iter: 1962 loss: 1.70477256e-06
Iter: 1963 loss: 1.70433009e-06
Iter: 1964 loss: 1.70419094e-06
Iter: 1965 loss: 1.70389353e-06
Iter: 1966 loss: 1.7098937e-06
Iter: 1967 loss: 1.70392332e-06
Iter: 1968 loss: 1.70359363e-06
Iter: 1969 loss: 1.70360659e-06
Iter: 1970 loss: 1.7034979e-06
Iter: 1971 loss: 1.70346141e-06
Iter: 1972 loss: 1.70333396e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8
+ date
Mon Oct 26 10:28:33 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.8_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.8_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e48454950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e483eea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e483339d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e484042f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e484549d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e48454598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e482d4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e482b67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e482b6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e482b6620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e482782f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e48205ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e48234840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e481c4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e481d2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e48234950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e482b6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e4812bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e48128b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e4812b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e480ef158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e4805c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e480ef6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e48099bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e480999d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e48099ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e48099950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e3024aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e302390d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e30239a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e30237378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e30239e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e30239c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e301b78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e30158158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4e30116f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 7.7752029e-06
Iter: 2 loss: 1.96437395e-05
Iter: 3 loss: 6.19759976e-06
Iter: 4 loss: 5.29950603e-06
Iter: 5 loss: 5.40982182e-06
Iter: 6 loss: 4.61380932e-06
Iter: 7 loss: 4.02205296e-06
Iter: 8 loss: 3.4234838e-06
Iter: 9 loss: 3.30583816e-06
Iter: 10 loss: 2.68118515e-06
Iter: 11 loss: 9.58818509e-06
Iter: 12 loss: 2.66851384e-06
Iter: 13 loss: 2.51747451e-06
Iter: 14 loss: 3.91638878e-06
Iter: 15 loss: 2.51101687e-06
Iter: 16 loss: 2.38894177e-06
Iter: 17 loss: 2.28157933e-06
Iter: 18 loss: 2.24957967e-06
Iter: 19 loss: 2.11103247e-06
Iter: 20 loss: 3.41420264e-06
Iter: 21 loss: 2.10539201e-06
Iter: 22 loss: 2.02757451e-06
Iter: 23 loss: 1.97907366e-06
Iter: 24 loss: 1.94800805e-06
Iter: 25 loss: 1.87988223e-06
Iter: 26 loss: 1.87986336e-06
Iter: 27 loss: 1.8357905e-06
Iter: 28 loss: 1.8675903e-06
Iter: 29 loss: 1.80849884e-06
Iter: 30 loss: 1.7657826e-06
Iter: 31 loss: 2.0844509e-06
Iter: 32 loss: 1.76236472e-06
Iter: 33 loss: 1.74016589e-06
Iter: 34 loss: 1.95591201e-06
Iter: 35 loss: 1.73929595e-06
Iter: 36 loss: 1.71579768e-06
Iter: 37 loss: 1.80137044e-06
Iter: 38 loss: 1.70993303e-06
Iter: 39 loss: 1.68777342e-06
Iter: 40 loss: 1.63901359e-06
Iter: 41 loss: 2.357109e-06
Iter: 42 loss: 1.63683558e-06
Iter: 43 loss: 1.59986394e-06
Iter: 44 loss: 1.90852734e-06
Iter: 45 loss: 1.59761908e-06
Iter: 46 loss: 1.57211821e-06
Iter: 47 loss: 1.5947237e-06
Iter: 48 loss: 1.55717976e-06
Iter: 49 loss: 1.52745451e-06
Iter: 50 loss: 1.73925537e-06
Iter: 51 loss: 1.5247449e-06
Iter: 52 loss: 1.51102518e-06
Iter: 53 loss: 1.54711984e-06
Iter: 54 loss: 1.50637197e-06
Iter: 55 loss: 1.48735398e-06
Iter: 56 loss: 1.47728701e-06
Iter: 57 loss: 1.46864329e-06
Iter: 58 loss: 1.45252443e-06
Iter: 59 loss: 1.62138076e-06
Iter: 60 loss: 1.45206889e-06
Iter: 61 loss: 1.43777538e-06
Iter: 62 loss: 1.42402291e-06
Iter: 63 loss: 1.42089311e-06
Iter: 64 loss: 1.41061537e-06
Iter: 65 loss: 1.41054443e-06
Iter: 66 loss: 1.40128282e-06
Iter: 67 loss: 1.40580994e-06
Iter: 68 loss: 1.39513156e-06
Iter: 69 loss: 1.39055737e-06
Iter: 70 loss: 1.38923201e-06
Iter: 71 loss: 1.38502855e-06
Iter: 72 loss: 1.42435249e-06
Iter: 73 loss: 1.38481255e-06
Iter: 74 loss: 1.38254541e-06
Iter: 75 loss: 1.37504378e-06
Iter: 76 loss: 1.38344308e-06
Iter: 77 loss: 1.36931192e-06
Iter: 78 loss: 1.35739992e-06
Iter: 79 loss: 1.43697798e-06
Iter: 80 loss: 1.35631785e-06
Iter: 81 loss: 1.34863978e-06
Iter: 82 loss: 1.38165819e-06
Iter: 83 loss: 1.34713036e-06
Iter: 84 loss: 1.33814024e-06
Iter: 85 loss: 1.33519939e-06
Iter: 86 loss: 1.32995365e-06
Iter: 87 loss: 1.3234876e-06
Iter: 88 loss: 1.32344815e-06
Iter: 89 loss: 1.31790728e-06
Iter: 90 loss: 1.31427691e-06
Iter: 91 loss: 1.31213574e-06
Iter: 92 loss: 1.30659384e-06
Iter: 93 loss: 1.38758401e-06
Iter: 94 loss: 1.30656861e-06
Iter: 95 loss: 1.30182639e-06
Iter: 96 loss: 1.30118678e-06
Iter: 97 loss: 1.29787372e-06
Iter: 98 loss: 1.29113391e-06
Iter: 99 loss: 1.31843808e-06
Iter: 100 loss: 1.28960494e-06
Iter: 101 loss: 1.28566512e-06
Iter: 102 loss: 1.28505417e-06
Iter: 103 loss: 1.2823009e-06
Iter: 104 loss: 1.28257136e-06
Iter: 105 loss: 1.27941462e-06
Iter: 106 loss: 1.27715282e-06
Iter: 107 loss: 1.27358521e-06
Iter: 108 loss: 1.2735627e-06
Iter: 109 loss: 1.27074918e-06
Iter: 110 loss: 1.27526118e-06
Iter: 111 loss: 1.26950033e-06
Iter: 112 loss: 1.26618761e-06
Iter: 113 loss: 1.26133307e-06
Iter: 114 loss: 1.26112673e-06
Iter: 115 loss: 1.25760744e-06
Iter: 116 loss: 1.2575656e-06
Iter: 117 loss: 1.25446923e-06
Iter: 118 loss: 1.25039321e-06
Iter: 119 loss: 1.25015549e-06
Iter: 120 loss: 1.2447581e-06
Iter: 121 loss: 1.30655269e-06
Iter: 122 loss: 1.24471535e-06
Iter: 123 loss: 1.2417654e-06
Iter: 124 loss: 1.24529538e-06
Iter: 125 loss: 1.2402005e-06
Iter: 126 loss: 1.23607947e-06
Iter: 127 loss: 1.24994165e-06
Iter: 128 loss: 1.23497284e-06
Iter: 129 loss: 1.23244808e-06
Iter: 130 loss: 1.25344218e-06
Iter: 131 loss: 1.23225027e-06
Iter: 132 loss: 1.23029076e-06
Iter: 133 loss: 1.22815322e-06
Iter: 134 loss: 1.22785957e-06
Iter: 135 loss: 1.22478298e-06
Iter: 136 loss: 1.26249643e-06
Iter: 137 loss: 1.22474614e-06
Iter: 138 loss: 1.22283723e-06
Iter: 139 loss: 1.22285951e-06
Iter: 140 loss: 1.22219512e-06
Iter: 141 loss: 1.22009374e-06
Iter: 142 loss: 1.22076574e-06
Iter: 143 loss: 1.21802043e-06
Iter: 144 loss: 1.21473818e-06
Iter: 145 loss: 1.21474466e-06
Iter: 146 loss: 1.21331425e-06
Iter: 147 loss: 1.21314145e-06
Iter: 148 loss: 1.21217465e-06
Iter: 149 loss: 1.20944424e-06
Iter: 150 loss: 1.20829304e-06
Iter: 151 loss: 1.20691641e-06
Iter: 152 loss: 1.20398977e-06
Iter: 153 loss: 1.25027861e-06
Iter: 154 loss: 1.20398215e-06
Iter: 155 loss: 1.20173354e-06
Iter: 156 loss: 1.20083905e-06
Iter: 157 loss: 1.19962124e-06
Iter: 158 loss: 1.19674928e-06
Iter: 159 loss: 1.22649044e-06
Iter: 160 loss: 1.19666754e-06
Iter: 161 loss: 1.19496872e-06
Iter: 162 loss: 1.19918047e-06
Iter: 163 loss: 1.19438846e-06
Iter: 164 loss: 1.19232755e-06
Iter: 165 loss: 1.19225638e-06
Iter: 166 loss: 1.19059405e-06
Iter: 167 loss: 1.18833259e-06
Iter: 168 loss: 1.21426967e-06
Iter: 169 loss: 1.18831974e-06
Iter: 170 loss: 1.187762e-06
Iter: 171 loss: 1.18758305e-06
Iter: 172 loss: 1.18682624e-06
Iter: 173 loss: 1.18456296e-06
Iter: 174 loss: 1.195393e-06
Iter: 175 loss: 1.18376374e-06
Iter: 176 loss: 1.1816395e-06
Iter: 177 loss: 1.19633683e-06
Iter: 178 loss: 1.18138928e-06
Iter: 179 loss: 1.18023104e-06
Iter: 180 loss: 1.18340847e-06
Iter: 181 loss: 1.17988657e-06
Iter: 182 loss: 1.17845502e-06
Iter: 183 loss: 1.17889454e-06
Iter: 184 loss: 1.17748345e-06
Iter: 185 loss: 1.17606623e-06
Iter: 186 loss: 1.18079424e-06
Iter: 187 loss: 1.17568914e-06
Iter: 188 loss: 1.1740924e-06
Iter: 189 loss: 1.17539832e-06
Iter: 190 loss: 1.17318973e-06
Iter: 191 loss: 1.17167315e-06
Iter: 192 loss: 1.1903752e-06
Iter: 193 loss: 1.17161107e-06
Iter: 194 loss: 1.1704185e-06
Iter: 195 loss: 1.1690679e-06
Iter: 196 loss: 1.16876936e-06
Iter: 197 loss: 1.16721424e-06
Iter: 198 loss: 1.16719843e-06
Iter: 199 loss: 1.16638944e-06
Iter: 200 loss: 1.16791261e-06
Iter: 201 loss: 1.16607328e-06
Iter: 202 loss: 1.16513581e-06
Iter: 203 loss: 1.1687531e-06
Iter: 204 loss: 1.16488275e-06
Iter: 205 loss: 1.16354556e-06
Iter: 206 loss: 1.1684192e-06
Iter: 207 loss: 1.16324213e-06
Iter: 208 loss: 1.16265778e-06
Iter: 209 loss: 1.16220735e-06
Iter: 210 loss: 1.16196316e-06
Iter: 211 loss: 1.16107753e-06
Iter: 212 loss: 1.15952241e-06
Iter: 213 loss: 1.15951525e-06
Iter: 214 loss: 1.15791511e-06
Iter: 215 loss: 1.15791181e-06
Iter: 216 loss: 1.15700914e-06
Iter: 217 loss: 1.15834268e-06
Iter: 218 loss: 1.15664375e-06
Iter: 219 loss: 1.15562057e-06
Iter: 220 loss: 1.15428645e-06
Iter: 221 loss: 1.15416958e-06
Iter: 222 loss: 1.15299827e-06
Iter: 223 loss: 1.15300361e-06
Iter: 224 loss: 1.15191051e-06
Iter: 225 loss: 1.1513755e-06
Iter: 226 loss: 1.15087846e-06
Iter: 227 loss: 1.14914906e-06
Iter: 228 loss: 1.16196406e-06
Iter: 229 loss: 1.14899774e-06
Iter: 230 loss: 1.14817954e-06
Iter: 231 loss: 1.15088824e-06
Iter: 232 loss: 1.14787031e-06
Iter: 233 loss: 1.14680677e-06
Iter: 234 loss: 1.14891759e-06
Iter: 235 loss: 1.14639931e-06
Iter: 236 loss: 1.14638942e-06
Iter: 237 loss: 1.14588522e-06
Iter: 238 loss: 1.14569e-06
Iter: 239 loss: 1.14510976e-06
Iter: 240 loss: 1.15239209e-06
Iter: 241 loss: 1.14508e-06
Iter: 242 loss: 1.14438092e-06
Iter: 243 loss: 1.14352235e-06
Iter: 244 loss: 1.14343925e-06
Iter: 245 loss: 1.14233035e-06
Iter: 246 loss: 1.15411808e-06
Iter: 247 loss: 1.14232716e-06
Iter: 248 loss: 1.14156364e-06
Iter: 249 loss: 1.14164482e-06
Iter: 250 loss: 1.14099828e-06
Iter: 251 loss: 1.13961437e-06
Iter: 252 loss: 1.14282079e-06
Iter: 253 loss: 1.13913302e-06
Iter: 254 loss: 1.13812689e-06
Iter: 255 loss: 1.14006025e-06
Iter: 256 loss: 1.13772307e-06
Iter: 257 loss: 1.13642648e-06
Iter: 258 loss: 1.13707029e-06
Iter: 259 loss: 1.13547731e-06
Iter: 260 loss: 1.13461556e-06
Iter: 261 loss: 1.1346367e-06
Iter: 262 loss: 1.13383965e-06
Iter: 263 loss: 1.13339695e-06
Iter: 264 loss: 1.13307544e-06
Iter: 265 loss: 1.13215924e-06
Iter: 266 loss: 1.14182444e-06
Iter: 267 loss: 1.13213628e-06
Iter: 268 loss: 1.13177668e-06
Iter: 269 loss: 1.13174281e-06
Iter: 270 loss: 1.13132955e-06
Iter: 271 loss: 1.13138776e-06
Iter: 272 loss: 1.13095894e-06
Iter: 273 loss: 1.13050169e-06
Iter: 274 loss: 1.12938778e-06
Iter: 275 loss: 1.14397596e-06
Iter: 276 loss: 1.12918588e-06
Iter: 277 loss: 1.12849887e-06
Iter: 278 loss: 1.12843554e-06
Iter: 279 loss: 1.12797363e-06
Iter: 280 loss: 1.12730868e-06
Iter: 281 loss: 1.12717555e-06
Iter: 282 loss: 1.12641214e-06
Iter: 283 loss: 1.13674776e-06
Iter: 284 loss: 1.12639623e-06
Iter: 285 loss: 1.12588248e-06
Iter: 286 loss: 1.12580813e-06
Iter: 287 loss: 1.12546127e-06
Iter: 288 loss: 1.12462021e-06
Iter: 289 loss: 1.12625173e-06
Iter: 290 loss: 1.12429461e-06
Iter: 291 loss: 1.1235054e-06
Iter: 292 loss: 1.12430439e-06
Iter: 293 loss: 1.12309885e-06
Iter: 294 loss: 1.12208386e-06
Iter: 295 loss: 1.12625617e-06
Iter: 296 loss: 1.12181147e-06
Iter: 297 loss: 1.12121086e-06
Iter: 298 loss: 1.13001454e-06
Iter: 299 loss: 1.12123303e-06
Iter: 300 loss: 1.12081318e-06
Iter: 301 loss: 1.12025987e-06
Iter: 302 loss: 1.1201256e-06
Iter: 303 loss: 1.12036582e-06
Iter: 304 loss: 1.11981194e-06
Iter: 305 loss: 1.11954319e-06
Iter: 306 loss: 1.11917609e-06
Iter: 307 loss: 1.11916916e-06
Iter: 308 loss: 1.11878126e-06
Iter: 309 loss: 1.11860504e-06
Iter: 310 loss: 1.11835789e-06
Iter: 311 loss: 1.11773602e-06
Iter: 312 loss: 1.11864165e-06
Iter: 313 loss: 1.11738984e-06
Iter: 314 loss: 1.11671261e-06
Iter: 315 loss: 1.11884697e-06
Iter: 316 loss: 1.1165879e-06
Iter: 317 loss: 1.11588929e-06
Iter: 318 loss: 1.11744544e-06
Iter: 319 loss: 1.11564032e-06
Iter: 320 loss: 1.11505028e-06
Iter: 321 loss: 1.11922964e-06
Iter: 322 loss: 1.1150139e-06
Iter: 323 loss: 1.11456723e-06
Iter: 324 loss: 1.11449617e-06
Iter: 325 loss: 1.11419342e-06
Iter: 326 loss: 1.11353143e-06
Iter: 327 loss: 1.11428358e-06
Iter: 328 loss: 1.1130071e-06
Iter: 329 loss: 1.11237716e-06
Iter: 330 loss: 1.11346594e-06
Iter: 331 loss: 1.11209943e-06
Iter: 332 loss: 1.11127395e-06
Iter: 333 loss: 1.11776228e-06
Iter: 334 loss: 1.11120733e-06
Iter: 335 loss: 1.11071245e-06
Iter: 336 loss: 1.11340944e-06
Iter: 337 loss: 1.11062911e-06
Iter: 338 loss: 1.11051509e-06
Iter: 339 loss: 1.1104222e-06
Iter: 340 loss: 1.1101929e-06
Iter: 341 loss: 1.10965584e-06
Iter: 342 loss: 1.11329268e-06
Iter: 343 loss: 1.10947599e-06
Iter: 344 loss: 1.10892552e-06
Iter: 345 loss: 1.11174097e-06
Iter: 346 loss: 1.10884275e-06
Iter: 347 loss: 1.10840938e-06
Iter: 348 loss: 1.10810481e-06
Iter: 349 loss: 1.10791257e-06
Iter: 350 loss: 1.10703718e-06
Iter: 351 loss: 1.1098706e-06
Iter: 352 loss: 1.10682276e-06
Iter: 353 loss: 1.10631663e-06
Iter: 354 loss: 1.10920121e-06
Iter: 355 loss: 1.10629583e-06
Iter: 356 loss: 1.10577275e-06
Iter: 357 loss: 1.10668498e-06
Iter: 358 loss: 1.10555175e-06
Iter: 359 loss: 1.10507835e-06
Iter: 360 loss: 1.10713802e-06
Iter: 361 loss: 1.10492454e-06
Iter: 362 loss: 1.10454732e-06
Iter: 363 loss: 1.1046659e-06
Iter: 364 loss: 1.10429846e-06
Iter: 365 loss: 1.10362635e-06
Iter: 366 loss: 1.10586086e-06
Iter: 367 loss: 1.103464e-06
Iter: 368 loss: 1.1029432e-06
Iter: 369 loss: 1.10279461e-06
Iter: 370 loss: 1.10247356e-06
Iter: 371 loss: 1.10158214e-06
Iter: 372 loss: 1.10927272e-06
Iter: 373 loss: 1.10162364e-06
Iter: 374 loss: 1.10136068e-06
Iter: 375 loss: 1.10130247e-06
Iter: 376 loss: 1.10105816e-06
Iter: 377 loss: 1.10102678e-06
Iter: 378 loss: 1.10079964e-06
Iter: 379 loss: 1.10053861e-06
Iter: 380 loss: 1.09987195e-06
Iter: 381 loss: 1.10745964e-06
Iter: 382 loss: 1.09980886e-06
Iter: 383 loss: 1.0993474e-06
Iter: 384 loss: 1.09934228e-06
Iter: 385 loss: 1.09902135e-06
Iter: 386 loss: 1.09851396e-06
Iter: 387 loss: 1.09850384e-06
Iter: 388 loss: 1.09772918e-06
Iter: 389 loss: 1.10023097e-06
Iter: 390 loss: 1.09749431e-06
Iter: 391 loss: 1.09687846e-06
Iter: 392 loss: 1.10012058e-06
Iter: 393 loss: 1.09677899e-06
Iter: 394 loss: 1.09620078e-06
Iter: 395 loss: 1.09709549e-06
Iter: 396 loss: 1.09592474e-06
Iter: 397 loss: 1.09543907e-06
Iter: 398 loss: 1.09891039e-06
Iter: 399 loss: 1.09532596e-06
Iter: 400 loss: 1.09495863e-06
Iter: 401 loss: 1.09477105e-06
Iter: 402 loss: 1.09456084e-06
Iter: 403 loss: 1.09392624e-06
Iter: 404 loss: 1.09679695e-06
Iter: 405 loss: 1.09379289e-06
Iter: 406 loss: 1.0932539e-06
Iter: 407 loss: 1.09377766e-06
Iter: 408 loss: 1.09299629e-06
Iter: 409 loss: 1.09254177e-06
Iter: 410 loss: 1.09252619e-06
Iter: 411 loss: 1.09215136e-06
Iter: 412 loss: 1.09486268e-06
Iter: 413 loss: 1.0920952e-06
Iter: 414 loss: 1.09186726e-06
Iter: 415 loss: 1.09165182e-06
Iter: 416 loss: 1.09161408e-06
Iter: 417 loss: 1.09126e-06
Iter: 418 loss: 1.0907429e-06
Iter: 419 loss: 1.09070015e-06
Iter: 420 loss: 1.09024211e-06
Iter: 421 loss: 1.09020425e-06
Iter: 422 loss: 1.08989718e-06
Iter: 423 loss: 1.08944243e-06
Iter: 424 loss: 1.08931272e-06
Iter: 425 loss: 1.08873871e-06
Iter: 426 loss: 1.09301652e-06
Iter: 427 loss: 1.0885733e-06
Iter: 428 loss: 1.0882286e-06
Iter: 429 loss: 1.09103928e-06
Iter: 430 loss: 1.08823701e-06
Iter: 431 loss: 1.08787663e-06
Iter: 432 loss: 1.08770303e-06
Iter: 433 loss: 1.08757592e-06
Iter: 434 loss: 1.08722202e-06
Iter: 435 loss: 1.09065638e-06
Iter: 436 loss: 1.08720462e-06
Iter: 437 loss: 1.08692132e-06
Iter: 438 loss: 1.08680717e-06
Iter: 439 loss: 1.08662312e-06
Iter: 440 loss: 1.0861163e-06
Iter: 441 loss: 1.08790209e-06
Iter: 442 loss: 1.08599238e-06
Iter: 443 loss: 1.08603945e-06
Iter: 444 loss: 1.08589688e-06
Iter: 445 loss: 1.08569293e-06
Iter: 446 loss: 1.08534687e-06
Iter: 447 loss: 1.09232553e-06
Iter: 448 loss: 1.08538143e-06
Iter: 449 loss: 1.08484755e-06
Iter: 450 loss: 1.08531435e-06
Iter: 451 loss: 1.08465565e-06
Iter: 452 loss: 1.08428696e-06
Iter: 453 loss: 1.08535801e-06
Iter: 454 loss: 1.08408881e-06
Iter: 455 loss: 1.08364748e-06
Iter: 456 loss: 1.08329391e-06
Iter: 457 loss: 1.08317954e-06
Iter: 458 loss: 1.08253903e-06
Iter: 459 loss: 1.09090638e-06
Iter: 460 loss: 1.08254733e-06
Iter: 461 loss: 1.08217546e-06
Iter: 462 loss: 1.08217534e-06
Iter: 463 loss: 1.08191841e-06
Iter: 464 loss: 1.08138204e-06
Iter: 465 loss: 1.08352947e-06
Iter: 466 loss: 1.08121037e-06
Iter: 467 loss: 1.08084043e-06
Iter: 468 loss: 1.0832706e-06
Iter: 469 loss: 1.08074869e-06
Iter: 470 loss: 1.08034851e-06
Iter: 471 loss: 1.07985352e-06
Iter: 472 loss: 1.07979258e-06
Iter: 473 loss: 1.07923358e-06
Iter: 474 loss: 1.08586767e-06
Iter: 475 loss: 1.07927349e-06
Iter: 476 loss: 1.07886e-06
Iter: 477 loss: 1.07887809e-06
Iter: 478 loss: 1.07851e-06
Iter: 479 loss: 1.07857136e-06
Iter: 480 loss: 1.07829646e-06
Iter: 481 loss: 1.07804681e-06
Iter: 482 loss: 1.07780033e-06
Iter: 483 loss: 1.07778942e-06
Iter: 484 loss: 1.07750793e-06
Iter: 485 loss: 1.07727237e-06
Iter: 486 loss: 1.07725646e-06
Iter: 487 loss: 1.07666438e-06
Iter: 488 loss: 1.07774827e-06
Iter: 489 loss: 1.07637925e-06
Iter: 490 loss: 1.07595611e-06
Iter: 491 loss: 1.07762821e-06
Iter: 492 loss: 1.07588039e-06
Iter: 493 loss: 1.07544747e-06
Iter: 494 loss: 1.07583787e-06
Iter: 495 loss: 1.07518656e-06
Iter: 496 loss: 1.07461028e-06
Iter: 497 loss: 1.07719893e-06
Iter: 498 loss: 1.07447704e-06
Iter: 499 loss: 1.0741287e-06
Iter: 500 loss: 1.07469839e-06
Iter: 501 loss: 1.07396772e-06
Iter: 502 loss: 1.0734102e-06
Iter: 503 loss: 1.07402673e-06
Iter: 504 loss: 1.07308665e-06
Iter: 505 loss: 1.07266305e-06
Iter: 506 loss: 1.07582559e-06
Iter: 507 loss: 1.0726236e-06
Iter: 508 loss: 1.07228061e-06
Iter: 509 loss: 1.07189726e-06
Iter: 510 loss: 1.07187793e-06
Iter: 511 loss: 1.0717024e-06
Iter: 512 loss: 1.07158326e-06
Iter: 513 loss: 1.07131439e-06
Iter: 514 loss: 1.07330789e-06
Iter: 515 loss: 1.07129483e-06
Iter: 516 loss: 1.0712e-06
Iter: 517 loss: 1.07087351e-06
Iter: 518 loss: 1.07382607e-06
Iter: 519 loss: 1.07086635e-06
Iter: 520 loss: 1.07047526e-06
Iter: 521 loss: 1.0719192e-06
Iter: 522 loss: 1.07037397e-06
Iter: 523 loss: 1.07010976e-06
Iter: 524 loss: 1.07105552e-06
Iter: 525 loss: 1.06997982e-06
Iter: 526 loss: 1.06963944e-06
Iter: 527 loss: 1.06923972e-06
Iter: 528 loss: 1.06915377e-06
Iter: 529 loss: 1.06877235e-06
Iter: 530 loss: 1.07500512e-06
Iter: 531 loss: 1.06876155e-06
Iter: 532 loss: 1.06838661e-06
Iter: 533 loss: 1.0682852e-06
Iter: 534 loss: 1.06806147e-06
Iter: 535 loss: 1.06754828e-06
Iter: 536 loss: 1.07128676e-06
Iter: 537 loss: 1.06754055e-06
Iter: 538 loss: 1.06727725e-06
Iter: 539 loss: 1.06821494e-06
Iter: 540 loss: 1.06718426e-06
Iter: 541 loss: 1.06689993e-06
Iter: 542 loss: 1.06649588e-06
Iter: 543 loss: 1.06645666e-06
Iter: 544 loss: 1.06608718e-06
Iter: 545 loss: 1.06873904e-06
Iter: 546 loss: 1.06598281e-06
Iter: 547 loss: 1.06595485e-06
Iter: 548 loss: 1.06581854e-06
Iter: 549 loss: 1.06564721e-06
Iter: 550 loss: 1.06521838e-06
Iter: 551 loss: 1.07054018e-06
Iter: 552 loss: 1.06514335e-06
Iter: 553 loss: 1.06478046e-06
Iter: 554 loss: 1.0666655e-06
Iter: 555 loss: 1.06474317e-06
Iter: 556 loss: 1.06454377e-06
Iter: 557 loss: 1.0644651e-06
Iter: 558 loss: 1.06433629e-06
Iter: 559 loss: 1.06388e-06
Iter: 560 loss: 1.06623656e-06
Iter: 561 loss: 1.06386e-06
Iter: 562 loss: 1.06364098e-06
Iter: 563 loss: 1.06392281e-06
Iter: 564 loss: 1.06351013e-06
Iter: 565 loss: 1.06321e-06
Iter: 566 loss: 1.06299603e-06
Iter: 567 loss: 1.06279867e-06
Iter: 568 loss: 1.06246944e-06
Iter: 569 loss: 1.0624434e-06
Iter: 570 loss: 1.06224195e-06
Iter: 571 loss: 1.06180698e-06
Iter: 572 loss: 1.06181096e-06
Iter: 573 loss: 1.06135462e-06
Iter: 574 loss: 1.06586708e-06
Iter: 575 loss: 1.0613345e-06
Iter: 576 loss: 1.0610247e-06
Iter: 577 loss: 1.06115351e-06
Iter: 578 loss: 1.06082587e-06
Iter: 579 loss: 1.06027824e-06
Iter: 580 loss: 1.06056757e-06
Iter: 581 loss: 1.05999743e-06
Iter: 582 loss: 1.06001221e-06
Iter: 583 loss: 1.0598053e-06
Iter: 584 loss: 1.05958168e-06
Iter: 585 loss: 1.05935305e-06
Iter: 586 loss: 1.05930326e-06
Iter: 587 loss: 1.05908771e-06
Iter: 588 loss: 1.05892377e-06
Iter: 589 loss: 1.05881077e-06
Iter: 590 loss: 1.05856395e-06
Iter: 591 loss: 1.05856043e-06
Iter: 592 loss: 1.05834692e-06
Iter: 593 loss: 1.05799768e-06
Iter: 594 loss: 1.06136417e-06
Iter: 595 loss: 1.05796425e-06
Iter: 596 loss: 1.05771778e-06
Iter: 597 loss: 1.05944321e-06
Iter: 598 loss: 1.05768299e-06
Iter: 599 loss: 1.05744959e-06
Iter: 600 loss: 1.05703134e-06
Iter: 601 loss: 1.06606944e-06
Iter: 602 loss: 1.05701076e-06
Iter: 603 loss: 1.05668551e-06
Iter: 604 loss: 1.05669267e-06
Iter: 605 loss: 1.05646939e-06
Iter: 606 loss: 1.05638401e-06
Iter: 607 loss: 1.05629124e-06
Iter: 608 loss: 1.05598156e-06
Iter: 609 loss: 1.05790969e-06
Iter: 610 loss: 1.05590743e-06
Iter: 611 loss: 1.05575748e-06
Iter: 612 loss: 1.05612799e-06
Iter: 613 loss: 1.05563754e-06
Iter: 614 loss: 1.05535833e-06
Iter: 615 loss: 1.05630136e-06
Iter: 616 loss: 1.0553631e-06
Iter: 617 loss: 1.05513436e-06
Iter: 618 loss: 1.05695221e-06
Iter: 619 loss: 1.05509298e-06
Iter: 620 loss: 1.05502249e-06
Iter: 621 loss: 1.05481922e-06
Iter: 622 loss: 1.0548074e-06
Iter: 623 loss: 1.05457411e-06
Iter: 624 loss: 1.05464892e-06
Iter: 625 loss: 1.05441109e-06
Iter: 626 loss: 1.05422157e-06
Iter: 627 loss: 1.05634933e-06
Iter: 628 loss: 1.05419599e-06
Iter: 629 loss: 1.05406389e-06
Iter: 630 loss: 1.0539361e-06
Iter: 631 loss: 1.05382719e-06
Iter: 632 loss: 1.053598e-06
Iter: 633 loss: 1.05541312e-06
Iter: 634 loss: 1.05357094e-06
Iter: 635 loss: 1.05340087e-06
Iter: 636 loss: 1.05431036e-06
Iter: 637 loss: 1.05336392e-06
Iter: 638 loss: 1.05314359e-06
Iter: 639 loss: 1.05286563e-06
Iter: 640 loss: 1.06011521e-06
Iter: 641 loss: 1.05286779e-06
Iter: 642 loss: 1.05266304e-06
Iter: 643 loss: 1.05261461e-06
Iter: 644 loss: 1.05246181e-06
Iter: 645 loss: 1.05272829e-06
Iter: 646 loss: 1.0523172e-06
Iter: 647 loss: 1.05205731e-06
Iter: 648 loss: 1.05272193e-06
Iter: 649 loss: 1.05194556e-06
Iter: 650 loss: 1.05196409e-06
Iter: 651 loss: 1.05187667e-06
Iter: 652 loss: 1.05179856e-06
Iter: 653 loss: 1.05165805e-06
Iter: 654 loss: 1.05205163e-06
Iter: 655 loss: 1.051548e-06
Iter: 656 loss: 1.05121262e-06
Iter: 657 loss: 1.0523122e-06
Iter: 658 loss: 1.05112167e-06
Iter: 659 loss: 1.05093341e-06
Iter: 660 loss: 1.05204026e-06
Iter: 661 loss: 1.05089566e-06
Iter: 662 loss: 1.05071581e-06
Iter: 663 loss: 1.05057927e-06
Iter: 664 loss: 1.0504948e-06
Iter: 665 loss: 1.05016204e-06
Iter: 666 loss: 1.05254526e-06
Iter: 667 loss: 1.05015647e-06
Iter: 668 loss: 1.04991091e-06
Iter: 669 loss: 1.05007859e-06
Iter: 670 loss: 1.04975516e-06
Iter: 671 loss: 1.04941455e-06
Iter: 672 loss: 1.05017966e-06
Iter: 673 loss: 1.04931905e-06
Iter: 674 loss: 1.04907e-06
Iter: 675 loss: 1.04970366e-06
Iter: 676 loss: 1.04897958e-06
Iter: 677 loss: 1.04867945e-06
Iter: 678 loss: 1.04954484e-06
Iter: 679 loss: 1.04859612e-06
Iter: 680 loss: 1.04834407e-06
Iter: 681 loss: 1.04992341e-06
Iter: 682 loss: 1.04829428e-06
Iter: 683 loss: 1.04822686e-06
Iter: 684 loss: 1.04819878e-06
Iter: 685 loss: 1.04808692e-06
Iter: 686 loss: 1.04782589e-06
Iter: 687 loss: 1.05159438e-06
Iter: 688 loss: 1.04778883e-06
Iter: 689 loss: 1.04751484e-06
Iter: 690 loss: 1.04818628e-06
Iter: 691 loss: 1.04748142e-06
Iter: 692 loss: 1.04729429e-06
Iter: 693 loss: 1.04723699e-06
Iter: 694 loss: 1.04713286e-06
Iter: 695 loss: 1.04679305e-06
Iter: 696 loss: 1.04879473e-06
Iter: 697 loss: 1.04677042e-06
Iter: 698 loss: 1.04658102e-06
Iter: 699 loss: 1.047002e-06
Iter: 700 loss: 1.0464754e-06
Iter: 701 loss: 1.0461863e-06
Iter: 702 loss: 1.04633136e-06
Iter: 703 loss: 1.04602964e-06
Iter: 704 loss: 1.04562571e-06
Iter: 705 loss: 1.0482695e-06
Iter: 706 loss: 1.04564947e-06
Iter: 707 loss: 1.04539436e-06
Iter: 708 loss: 1.04525486e-06
Iter: 709 loss: 1.04521268e-06
Iter: 710 loss: 1.04489959e-06
Iter: 711 loss: 1.04731316e-06
Iter: 712 loss: 1.04491505e-06
Iter: 713 loss: 1.04460992e-06
Iter: 714 loss: 1.04550509e-06
Iter: 715 loss: 1.04457968e-06
Iter: 716 loss: 1.04438357e-06
Iter: 717 loss: 1.04596165e-06
Iter: 718 loss: 1.04434355e-06
Iter: 719 loss: 1.04414676e-06
Iter: 720 loss: 1.04545495e-06
Iter: 721 loss: 1.04412106e-06
Iter: 722 loss: 1.04403784e-06
Iter: 723 loss: 1.04380354e-06
Iter: 724 loss: 1.04711557e-06
Iter: 725 loss: 1.04378717e-06
Iter: 726 loss: 1.0434826e-06
Iter: 727 loss: 1.0432924e-06
Iter: 728 loss: 1.04323203e-06
Iter: 729 loss: 1.04303899e-06
Iter: 730 loss: 1.04295782e-06
Iter: 731 loss: 1.04279468e-06
Iter: 732 loss: 1.04264632e-06
Iter: 733 loss: 1.04256503e-06
Iter: 734 loss: 1.04227752e-06
Iter: 735 loss: 1.04476976e-06
Iter: 736 loss: 1.04227775e-06
Iter: 737 loss: 1.04209653e-06
Iter: 738 loss: 1.04219771e-06
Iter: 739 loss: 1.04195806e-06
Iter: 740 loss: 1.0416793e-06
Iter: 741 loss: 1.04206504e-06
Iter: 742 loss: 1.04155265e-06
Iter: 743 loss: 1.04128412e-06
Iter: 744 loss: 1.04246146e-06
Iter: 745 loss: 1.04124535e-06
Iter: 746 loss: 1.0410655e-06
Iter: 747 loss: 1.04079891e-06
Iter: 748 loss: 1.04078936e-06
Iter: 749 loss: 1.04063861e-06
Iter: 750 loss: 1.04056107e-06
Iter: 751 loss: 1.0404209e-06
Iter: 752 loss: 1.0423264e-06
Iter: 753 loss: 1.04041465e-06
Iter: 754 loss: 1.04029095e-06
Iter: 755 loss: 1.04014543e-06
Iter: 756 loss: 1.0426013e-06
Iter: 757 loss: 1.04007199e-06
Iter: 758 loss: 1.03986952e-06
Iter: 759 loss: 1.04002061e-06
Iter: 760 loss: 1.03969671e-06
Iter: 761 loss: 1.03943466e-06
Iter: 762 loss: 1.0398096e-06
Iter: 763 loss: 1.03929892e-06
Iter: 764 loss: 1.03903699e-06
Iter: 765 loss: 1.0416702e-06
Iter: 766 loss: 1.03901618e-06
Iter: 767 loss: 1.03887828e-06
Iter: 768 loss: 1.03922468e-06
Iter: 769 loss: 1.03887407e-06
Iter: 770 loss: 1.03858724e-06
Iter: 771 loss: 1.03881575e-06
Iter: 772 loss: 1.03849914e-06
Iter: 773 loss: 1.03825278e-06
Iter: 774 loss: 1.03970979e-06
Iter: 775 loss: 1.03822595e-06
Iter: 776 loss: 1.03810089e-06
Iter: 777 loss: 1.03816592e-06
Iter: 778 loss: 1.03799368e-06
Iter: 779 loss: 1.03766502e-06
Iter: 780 loss: 1.03775096e-06
Iter: 781 loss: 1.03745094e-06
Iter: 782 loss: 1.03723482e-06
Iter: 783 loss: 1.03890932e-06
Iter: 784 loss: 1.03720026e-06
Iter: 785 loss: 1.03707634e-06
Iter: 786 loss: 1.0370195e-06
Iter: 787 loss: 1.03692776e-06
Iter: 788 loss: 1.03671925e-06
Iter: 789 loss: 1.0421245e-06
Iter: 790 loss: 1.03674472e-06
Iter: 791 loss: 1.03649654e-06
Iter: 792 loss: 1.03690252e-06
Iter: 793 loss: 1.03645084e-06
Iter: 794 loss: 1.03624484e-06
Iter: 795 loss: 1.03598029e-06
Iter: 796 loss: 1.03598484e-06
Iter: 797 loss: 1.03558955e-06
Iter: 798 loss: 1.03882985e-06
Iter: 799 loss: 1.03557909e-06
Iter: 800 loss: 1.03543744e-06
Iter: 801 loss: 1.03580624e-06
Iter: 802 loss: 1.03528612e-06
Iter: 803 loss: 1.03508387e-06
Iter: 804 loss: 1.03707634e-06
Iter: 805 loss: 1.03508216e-06
Iter: 806 loss: 1.03499224e-06
Iter: 807 loss: 1.0349803e-06
Iter: 808 loss: 1.03481796e-06
Iter: 809 loss: 1.03467949e-06
Iter: 810 loss: 1.03536695e-06
Iter: 811 loss: 1.03458592e-06
Iter: 812 loss: 1.03444393e-06
Iter: 813 loss: 1.03488378e-06
Iter: 814 loss: 1.03433081e-06
Iter: 815 loss: 1.03415368e-06
Iter: 816 loss: 1.03380285e-06
Iter: 817 loss: 1.03380535e-06
Iter: 818 loss: 1.03381149e-06
Iter: 819 loss: 1.03368916e-06
Iter: 820 loss: 1.03351601e-06
Iter: 821 loss: 1.03452885e-06
Iter: 822 loss: 1.03349703e-06
Iter: 823 loss: 1.03342813e-06
Iter: 824 loss: 1.03327568e-06
Iter: 825 loss: 1.03535444e-06
Iter: 826 loss: 1.03324601e-06
Iter: 827 loss: 1.03296202e-06
Iter: 828 loss: 1.03332547e-06
Iter: 829 loss: 1.03286618e-06
Iter: 830 loss: 1.03271645e-06
Iter: 831 loss: 1.03389743e-06
Iter: 832 loss: 1.03264347e-06
Iter: 833 loss: 1.03242792e-06
Iter: 834 loss: 1.03213722e-06
Iter: 835 loss: 1.03212642e-06
Iter: 836 loss: 1.03195396e-06
Iter: 837 loss: 1.03190087e-06
Iter: 838 loss: 1.03179195e-06
Iter: 839 loss: 1.03217894e-06
Iter: 840 loss: 1.03169532e-06
Iter: 841 loss: 1.03152774e-06
Iter: 842 loss: 1.03200136e-06
Iter: 843 loss: 1.03147954e-06
Iter: 844 loss: 1.03128821e-06
Iter: 845 loss: 1.03148045e-06
Iter: 846 loss: 1.0311669e-06
Iter: 847 loss: 1.03094135e-06
Iter: 848 loss: 1.03156128e-06
Iter: 849 loss: 1.03082107e-06
Iter: 850 loss: 1.03066134e-06
Iter: 851 loss: 1.03086131e-06
Iter: 852 loss: 1.03054322e-06
Iter: 853 loss: 1.03039827e-06
Iter: 854 loss: 1.03269372e-06
Iter: 855 loss: 1.03041179e-06
Iter: 856 loss: 1.03020909e-06
Iter: 857 loss: 1.03133152e-06
Iter: 858 loss: 1.03018135e-06
Iter: 859 loss: 1.03012985e-06
Iter: 860 loss: 1.02995261e-06
Iter: 861 loss: 1.03083539e-06
Iter: 862 loss: 1.02986007e-06
Iter: 863 loss: 1.02958916e-06
Iter: 864 loss: 1.03020477e-06
Iter: 865 loss: 1.02945569e-06
Iter: 866 loss: 1.02926026e-06
Iter: 867 loss: 1.03146863e-06
Iter: 868 loss: 1.02926833e-06
Iter: 869 loss: 1.02908007e-06
Iter: 870 loss: 1.02893273e-06
Iter: 871 loss: 1.02891215e-06
Iter: 872 loss: 1.02862589e-06
Iter: 873 loss: 1.03030663e-06
Iter: 874 loss: 1.02857916e-06
Iter: 875 loss: 1.02840522e-06
Iter: 876 loss: 1.02864658e-06
Iter: 877 loss: 1.02832587e-06
Iter: 878 loss: 1.02815375e-06
Iter: 879 loss: 1.02832564e-06
Iter: 880 loss: 1.02798106e-06
Iter: 881 loss: 1.02782678e-06
Iter: 882 loss: 1.0278136e-06
Iter: 883 loss: 1.02771173e-06
Iter: 884 loss: 1.02753688e-06
Iter: 885 loss: 1.02749254e-06
Iter: 886 loss: 1.02726585e-06
Iter: 887 loss: 1.02820627e-06
Iter: 888 loss: 1.02726221e-06
Iter: 889 loss: 1.02720037e-06
Iter: 890 loss: 1.02715785e-06
Iter: 891 loss: 1.02708088e-06
Iter: 892 loss: 1.02691979e-06
Iter: 893 loss: 1.02688693e-06
Iter: 894 loss: 1.02671765e-06
Iter: 895 loss: 1.02683566e-06
Iter: 896 loss: 1.02660238e-06
Iter: 897 loss: 1.02647209e-06
Iter: 898 loss: 1.02646311e-06
Iter: 899 loss: 1.02632487e-06
Iter: 900 loss: 1.02605713e-06
Iter: 901 loss: 1.02675835e-06
Iter: 902 loss: 1.02599301e-06
Iter: 903 loss: 1.02576564e-06
Iter: 904 loss: 1.02634772e-06
Iter: 905 loss: 1.02568094e-06
Iter: 906 loss: 1.02549e-06
Iter: 907 loss: 1.02699664e-06
Iter: 908 loss: 1.02543106e-06
Iter: 909 loss: 1.02526303e-06
Iter: 910 loss: 1.02556305e-06
Iter: 911 loss: 1.02516742e-06
Iter: 912 loss: 1.02499075e-06
Iter: 913 loss: 1.02492231e-06
Iter: 914 loss: 1.02478577e-06
Iter: 915 loss: 1.02461468e-06
Iter: 916 loss: 1.02739511e-06
Iter: 917 loss: 1.02461763e-06
Iter: 918 loss: 1.0244238e-06
Iter: 919 loss: 1.02466947e-06
Iter: 920 loss: 1.02434967e-06
Iter: 921 loss: 1.02409194e-06
Iter: 922 loss: 1.02453646e-06
Iter: 923 loss: 1.02396518e-06
Iter: 924 loss: 1.02392323e-06
Iter: 925 loss: 1.02394574e-06
Iter: 926 loss: 1.02380204e-06
Iter: 927 loss: 1.0238532e-06
Iter: 928 loss: 1.02373656e-06
Iter: 929 loss: 1.02366778e-06
Iter: 930 loss: 1.02347224e-06
Iter: 931 loss: 1.02695617e-06
Iter: 932 loss: 1.02345484e-06
Iter: 933 loss: 1.02327067e-06
Iter: 934 loss: 1.02435695e-06
Iter: 935 loss: 1.0232518e-06
Iter: 936 loss: 1.0230234e-06
Iter: 937 loss: 1.02309059e-06
Iter: 938 loss: 1.02294507e-06
Iter: 939 loss: 1.02276715e-06
Iter: 940 loss: 1.02529771e-06
Iter: 941 loss: 1.02275476e-06
Iter: 942 loss: 1.02262652e-06
Iter: 943 loss: 1.02240836e-06
Iter: 944 loss: 1.02241438e-06
Iter: 945 loss: 1.02217223e-06
Iter: 946 loss: 1.02385593e-06
Iter: 947 loss: 1.02217666e-06
Iter: 948 loss: 1.02199317e-06
Iter: 949 loss: 1.02309855e-06
Iter: 950 loss: 1.02196668e-06
Iter: 951 loss: 1.02176546e-06
Iter: 952 loss: 1.0216686e-06
Iter: 953 loss: 1.0216296e-06
Iter: 954 loss: 1.02145418e-06
Iter: 955 loss: 1.02224544e-06
Iter: 956 loss: 1.02141473e-06
Iter: 957 loss: 1.0211495e-06
Iter: 958 loss: 1.02185368e-06
Iter: 959 loss: 1.02106742e-06
Iter: 960 loss: 1.0210008e-06
Iter: 961 loss: 1.02098761e-06
Iter: 962 loss: 1.02090371e-06
Iter: 963 loss: 1.02076206e-06
Iter: 964 loss: 1.02075455e-06
Iter: 965 loss: 1.02064826e-06
Iter: 966 loss: 1.020431e-06
Iter: 967 loss: 1.02289425e-06
Iter: 968 loss: 1.02043111e-06
Iter: 969 loss: 1.02010972e-06
Iter: 970 loss: 1.02331808e-06
Iter: 971 loss: 1.02011404e-06
Iter: 972 loss: 1.01987985e-06
Iter: 973 loss: 1.01991714e-06
Iter: 974 loss: 1.01975093e-06
Iter: 975 loss: 1.01942328e-06
Iter: 976 loss: 1.02157401e-06
Iter: 977 loss: 1.01936223e-06
Iter: 978 loss: 1.01913542e-06
Iter: 979 loss: 1.02000433e-06
Iter: 980 loss: 1.01909222e-06
Iter: 981 loss: 1.01890282e-06
Iter: 982 loss: 1.01892942e-06
Iter: 983 loss: 1.01878163e-06
Iter: 984 loss: 1.01856472e-06
Iter: 985 loss: 1.02071863e-06
Iter: 986 loss: 1.01860019e-06
Iter: 987 loss: 1.01835712e-06
Iter: 988 loss: 1.01820581e-06
Iter: 989 loss: 1.01815658e-06
Iter: 990 loss: 1.0178876e-06
Iter: 991 loss: 1.01971386e-06
Iter: 992 loss: 1.01781325e-06
Iter: 993 loss: 1.01761236e-06
Iter: 994 loss: 1.0174931e-06
Iter: 995 loss: 1.01745468e-06
Iter: 996 loss: 1.01761134e-06
Iter: 997 loss: 1.01732985e-06
Iter: 998 loss: 1.01725152e-06
Iter: 999 loss: 1.01716955e-06
Iter: 1000 loss: 1.01715091e-06
Iter: 1001 loss: 1.01704904e-06
Iter: 1002 loss: 1.01683679e-06
Iter: 1003 loss: 1.01685919e-06
Iter: 1004 loss: 1.01664932e-06
Iter: 1005 loss: 1.01709986e-06
Iter: 1006 loss: 1.0165229e-06
Iter: 1007 loss: 1.01638045e-06
Iter: 1008 loss: 1.01768694e-06
Iter: 1009 loss: 1.01637011e-06
Iter: 1010 loss: 1.01620276e-06
Iter: 1011 loss: 1.01592479e-06
Iter: 1012 loss: 1.01594924e-06
Iter: 1013 loss: 1.01576438e-06
Iter: 1014 loss: 1.0184267e-06
Iter: 1015 loss: 1.015723e-06
Iter: 1016 loss: 1.01559294e-06
Iter: 1017 loss: 1.01543856e-06
Iter: 1018 loss: 1.01542321e-06
Iter: 1019 loss: 1.015198e-06
Iter: 1020 loss: 1.01750811e-06
Iter: 1021 loss: 1.01511546e-06
Iter: 1022 loss: 1.01497358e-06
Iter: 1023 loss: 1.0149854e-06
Iter: 1024 loss: 1.01487649e-06
Iter: 1025 loss: 1.01463968e-06
Iter: 1026 loss: 1.01607316e-06
Iter: 1027 loss: 1.01458e-06
Iter: 1028 loss: 1.0144455e-06
Iter: 1029 loss: 1.01564524e-06
Iter: 1030 loss: 1.01446403e-06
Iter: 1031 loss: 1.01436058e-06
Iter: 1032 loss: 1.01581247e-06
Iter: 1033 loss: 1.01435091e-06
Iter: 1034 loss: 1.01428736e-06
Iter: 1035 loss: 1.01411e-06
Iter: 1036 loss: 1.01458e-06
Iter: 1037 loss: 1.01397848e-06
Iter: 1038 loss: 1.01378077e-06
Iter: 1039 loss: 1.01376258e-06
Iter: 1040 loss: 1.01365845e-06
Iter: 1041 loss: 1.01352725e-06
Iter: 1042 loss: 1.01353771e-06
Iter: 1043 loss: 1.01321609e-06
Iter: 1044 loss: 1.01420574e-06
Iter: 1045 loss: 1.01320393e-06
Iter: 1046 loss: 1.01301111e-06
Iter: 1047 loss: 1.01362048e-06
Iter: 1048 loss: 1.01295e-06
Iter: 1049 loss: 1.01280284e-06
Iter: 1050 loss: 1.01260162e-06
Iter: 1051 loss: 1.01256728e-06
Iter: 1052 loss: 1.01230432e-06
Iter: 1053 loss: 1.01520959e-06
Iter: 1054 loss: 1.01231399e-06
Iter: 1055 loss: 1.01211413e-06
Iter: 1056 loss: 1.0122294e-06
Iter: 1057 loss: 1.01201204e-06
Iter: 1058 loss: 1.011839e-06
Iter: 1059 loss: 1.01370142e-06
Iter: 1060 loss: 1.0118124e-06
Iter: 1061 loss: 1.01164471e-06
Iter: 1062 loss: 1.01165938e-06
Iter: 1063 loss: 1.01153364e-06
Iter: 1064 loss: 1.01151727e-06
Iter: 1065 loss: 1.01144906e-06
Iter: 1066 loss: 1.01134538e-06
Iter: 1067 loss: 1.01125033e-06
Iter: 1068 loss: 1.01123851e-06
Iter: 1069 loss: 1.0111321e-06
Iter: 1070 loss: 1.0109834e-06
Iter: 1071 loss: 1.0156391e-06
Iter: 1072 loss: 1.01098135e-06
Iter: 1073 loss: 1.01067553e-06
Iter: 1074 loss: 1.01189289e-06
Iter: 1075 loss: 1.01057e-06
Iter: 1076 loss: 1.01029764e-06
Iter: 1077 loss: 1.01135925e-06
Iter: 1078 loss: 1.01029968e-06
Iter: 1079 loss: 1.01010141e-06
Iter: 1080 loss: 1.0099152e-06
Iter: 1081 loss: 1.00985335e-06
Iter: 1082 loss: 1.00963734e-06
Iter: 1083 loss: 1.0125807e-06
Iter: 1084 loss: 1.00960244e-06
Iter: 1085 loss: 1.0094609e-06
Iter: 1086 loss: 1.00948114e-06
Iter: 1087 loss: 1.00934972e-06
Iter: 1088 loss: 1.00908289e-06
Iter: 1089 loss: 1.00910552e-06
Iter: 1090 loss: 1.00895659e-06
Iter: 1091 loss: 1.00869238e-06
Iter: 1092 loss: 1.01157616e-06
Iter: 1093 loss: 1.00875536e-06
Iter: 1094 loss: 1.00852617e-06
Iter: 1095 loss: 1.00845273e-06
Iter: 1096 loss: 1.00833904e-06
Iter: 1097 loss: 1.00834473e-06
Iter: 1098 loss: 1.00823763e-06
Iter: 1099 loss: 1.00821944e-06
Iter: 1100 loss: 1.00825901e-06
Iter: 1101 loss: 1.00813236e-06
Iter: 1102 loss: 1.00804505e-06
Iter: 1103 loss: 1.00787406e-06
Iter: 1104 loss: 1.00786815e-06
Iter: 1105 loss: 1.00770944e-06
Iter: 1106 loss: 1.00894204e-06
Iter: 1107 loss: 1.00771013e-06
Iter: 1108 loss: 1.00761417e-06
Iter: 1109 loss: 1.00786247e-06
Iter: 1110 loss: 1.00751743e-06
Iter: 1111 loss: 1.00739908e-06
Iter: 1112 loss: 1.00730585e-06
Iter: 1113 loss: 1.0072107e-06
Iter: 1114 loss: 1.00703278e-06
Iter: 1115 loss: 1.00780551e-06
Iter: 1116 loss: 1.00701811e-06
Iter: 1117 loss: 1.00675641e-06
Iter: 1118 loss: 1.00729267e-06
Iter: 1119 loss: 1.00665443e-06
Iter: 1120 loss: 1.00646537e-06
Iter: 1121 loss: 1.00761213e-06
Iter: 1122 loss: 1.00649186e-06
Iter: 1123 loss: 1.00629472e-06
Iter: 1124 loss: 1.00605803e-06
Iter: 1125 loss: 1.00606485e-06
Iter: 1126 loss: 1.00591342e-06
Iter: 1127 loss: 1.0058568e-06
Iter: 1128 loss: 1.00570367e-06
Iter: 1129 loss: 1.00567377e-06
Iter: 1130 loss: 1.0055993e-06
Iter: 1131 loss: 1.00562954e-06
Iter: 1132 loss: 1.00554325e-06
Iter: 1133 loss: 1.00541638e-06
Iter: 1134 loss: 1.00530588e-06
Iter: 1135 loss: 1.00529212e-06
Iter: 1136 loss: 1.00519196e-06
Iter: 1137 loss: 1.00498642e-06
Iter: 1138 loss: 1.00501165e-06
Iter: 1139 loss: 1.00476279e-06
Iter: 1140 loss: 1.00629427e-06
Iter: 1141 loss: 1.00471482e-06
Iter: 1142 loss: 1.00458522e-06
Iter: 1143 loss: 1.00530588e-06
Iter: 1144 loss: 1.00457044e-06
Iter: 1145 loss: 1.00440752e-06
Iter: 1146 loss: 1.00423188e-06
Iter: 1147 loss: 1.00423358e-06
Iter: 1148 loss: 1.00403508e-06
Iter: 1149 loss: 1.00403145e-06
Iter: 1150 loss: 1.00391071e-06
Iter: 1151 loss: 1.00402622e-06
Iter: 1152 loss: 1.0038159e-06
Iter: 1153 loss: 1.00367083e-06
Iter: 1154 loss: 1.00396983e-06
Iter: 1155 loss: 1.00362081e-06
Iter: 1156 loss: 1.00333818e-06
Iter: 1157 loss: 1.00340094e-06
Iter: 1158 loss: 1.00324064e-06
Iter: 1159 loss: 1.00300929e-06
Iter: 1160 loss: 1.00299405e-06
Iter: 1161 loss: 1.00288185e-06
Iter: 1162 loss: 1.00296654e-06
Iter: 1163 loss: 1.00287036e-06
Iter: 1164 loss: 1.00271245e-06
Iter: 1165 loss: 1.0027436e-06
Iter: 1166 loss: 1.00268767e-06
Iter: 1167 loss: 1.00256557e-06
Iter: 1168 loss: 1.00393117e-06
Iter: 1169 loss: 1.00255875e-06
Iter: 1170 loss: 1.00237116e-06
Iter: 1171 loss: 1.00271006e-06
Iter: 1172 loss: 1.0022984e-06
Iter: 1173 loss: 1.0021929e-06
Iter: 1174 loss: 1.00222042e-06
Iter: 1175 loss: 1.00199725e-06
Iter: 1176 loss: 1.00185048e-06
Iter: 1177 loss: 1.00285456e-06
Iter: 1178 loss: 1.00184229e-06
Iter: 1179 loss: 1.00174736e-06
Iter: 1180 loss: 1.00166721e-06
Iter: 1181 loss: 1.00159571e-06
Iter: 1182 loss: 1.00151431e-06
Iter: 1183 loss: 1.00366265e-06
Iter: 1184 loss: 1.00149964e-06
Iter: 1185 loss: 1.0013913e-06
Iter: 1186 loss: 1.00135162e-06
Iter: 1187 loss: 1.00124953e-06
Iter: 1188 loss: 1.00104887e-06
Iter: 1189 loss: 1.00148191e-06
Iter: 1190 loss: 1.00097918e-06
Iter: 1191 loss: 1.00078637e-06
Iter: 1192 loss: 1.00123475e-06
Iter: 1193 loss: 1.00073737e-06
Iter: 1194 loss: 1.0005474e-06
Iter: 1195 loss: 1.00171883e-06
Iter: 1196 loss: 1.00053808e-06
Iter: 1197 loss: 1.00055058e-06
Iter: 1198 loss: 1.00044053e-06
Iter: 1199 loss: 1.0004029e-06
Iter: 1200 loss: 1.00032207e-06
Iter: 1201 loss: 1.00076932e-06
Iter: 1202 loss: 1.00029115e-06
Iter: 1203 loss: 1.00011471e-06
Iter: 1204 loss: 1.00036743e-06
Iter: 1205 loss: 1.00002342e-06
Iter: 1206 loss: 9.9988074e-07
Iter: 1207 loss: 1.0009885e-06
Iter: 1208 loss: 9.99871418e-07
Iter: 1209 loss: 9.99722602e-07
Iter: 1210 loss: 9.99626081e-07
Iter: 1211 loss: 9.99601411e-07
Iter: 1212 loss: 9.9947033e-07
Iter: 1213 loss: 1.00064653e-06
Iter: 1214 loss: 9.9943577e-07
Iter: 1215 loss: 9.99251711e-07
Iter: 1216 loss: 9.9963313e-07
Iter: 1217 loss: 9.99160648e-07
Iter: 1218 loss: 9.99082886e-07
Iter: 1219 loss: 1.00010266e-06
Iter: 1220 loss: 9.9909721e-07
Iter: 1221 loss: 9.98975338e-07
Iter: 1222 loss: 9.98940095e-07
Iter: 1223 loss: 9.98941459e-07
Iter: 1224 loss: 9.98712153e-07
Iter: 1225 loss: 9.98760925e-07
Iter: 1226 loss: 9.98614155e-07
Iter: 1227 loss: 9.98446126e-07
Iter: 1228 loss: 9.98880296e-07
Iter: 1229 loss: 9.98381779e-07
Iter: 1230 loss: 9.98260703e-07
Iter: 1231 loss: 9.99578106e-07
Iter: 1232 loss: 9.98180326e-07
Iter: 1233 loss: 9.98145197e-07
Iter: 1234 loss: 9.98124733e-07
Iter: 1235 loss: 9.98123e-07
Iter: 1236 loss: 9.97979214e-07
Iter: 1237 loss: 9.9835438e-07
Iter: 1238 loss: 9.97974666e-07
Iter: 1239 loss: 9.97816642e-07
Iter: 1240 loss: 9.98177256e-07
Iter: 1241 loss: 9.97757752e-07
Iter: 1242 loss: 9.97673624e-07
Iter: 1243 loss: 9.97883262e-07
Iter: 1244 loss: 9.97673396e-07
Iter: 1245 loss: 9.97517418e-07
Iter: 1246 loss: 9.97851885e-07
Iter: 1247 loss: 9.97504912e-07
Iter: 1248 loss: 9.97352458e-07
Iter: 1249 loss: 9.98045607e-07
Iter: 1250 loss: 9.97303687e-07
Iter: 1251 loss: 9.97273e-07
Iter: 1252 loss: 9.97128836e-07
Iter: 1253 loss: 9.97142592e-07
Iter: 1254 loss: 9.96990593e-07
Iter: 1255 loss: 9.9862234e-07
Iter: 1256 loss: 9.96962626e-07
Iter: 1257 loss: 9.96890776e-07
Iter: 1258 loss: 9.96991e-07
Iter: 1259 loss: 9.96871904e-07
Iter: 1260 loss: 9.96702283e-07
Iter: 1261 loss: 9.96926133e-07
Iter: 1262 loss: 9.96666358e-07
Iter: 1263 loss: 9.96535846e-07
Iter: 1264 loss: 9.97740131e-07
Iter: 1265 loss: 9.96499693e-07
Iter: 1266 loss: 9.96426479e-07
Iter: 1267 loss: 9.96425456e-07
Iter: 1268 loss: 9.96330527e-07
Iter: 1269 loss: 9.96135668e-07
Iter: 1270 loss: 9.97859161e-07
Iter: 1271 loss: 9.96082e-07
Iter: 1272 loss: 9.95948767e-07
Iter: 1273 loss: 9.9687486e-07
Iter: 1274 loss: 9.95929213e-07
Iter: 1275 loss: 9.958e-07
Iter: 1276 loss: 9.9579745e-07
Iter: 1277 loss: 9.95731625e-07
Iter: 1278 loss: 9.95567461e-07
Iter: 1279 loss: 9.96136123e-07
Iter: 1280 loss: 9.95513119e-07
Iter: 1281 loss: 9.95458322e-07
Iter: 1282 loss: 9.95873279e-07
Iter: 1283 loss: 9.95421374e-07
Iter: 1284 loss: 9.95321557e-07
Iter: 1285 loss: 9.95227765e-07
Iter: 1286 loss: 9.95213e-07
Iter: 1287 loss: 9.95086793e-07
Iter: 1288 loss: 9.95847813e-07
Iter: 1289 loss: 9.95037681e-07
Iter: 1290 loss: 9.94917286e-07
Iter: 1291 loss: 9.95039613e-07
Iter: 1292 loss: 9.94875336e-07
Iter: 1293 loss: 9.94744596e-07
Iter: 1294 loss: 9.95814503e-07
Iter: 1295 loss: 9.9472652e-07
Iter: 1296 loss: 9.94642278e-07
Iter: 1297 loss: 9.94558604e-07
Iter: 1298 loss: 9.9457327e-07
Iter: 1299 loss: 9.94415814e-07
Iter: 1300 loss: 9.95641813e-07
Iter: 1301 loss: 9.94422862e-07
Iter: 1302 loss: 9.9435033e-07
Iter: 1303 loss: 9.95666142e-07
Iter: 1304 loss: 9.94319635e-07
Iter: 1305 loss: 9.94284164e-07
Iter: 1306 loss: 9.94226e-07
Iter: 1307 loss: 9.95383516e-07
Iter: 1308 loss: 9.94201628e-07
Iter: 1309 loss: 9.940768e-07
Iter: 1310 loss: 9.93919684e-07
Iter: 1311 loss: 9.93948106e-07
Iter: 1312 loss: 9.93781214e-07
Iter: 1313 loss: 9.94510174e-07
Iter: 1314 loss: 9.93791218e-07
Iter: 1315 loss: 9.93600906e-07
Iter: 1316 loss: 9.93823505e-07
Iter: 1317 loss: 9.9354429e-07
Iter: 1318 loss: 9.93422418e-07
Iter: 1319 loss: 9.94839638e-07
Iter: 1320 loss: 9.93436743e-07
Iter: 1321 loss: 9.93310778e-07
Iter: 1322 loss: 9.93262688e-07
Iter: 1323 loss: 9.9319368e-07
Iter: 1324 loss: 9.9306e-07
Iter: 1325 loss: 9.94876245e-07
Iter: 1326 loss: 9.93051e-07
Iter: 1327 loss: 9.92934247e-07
Iter: 1328 loss: 9.93104436e-07
Iter: 1329 loss: 9.92891e-07
Iter: 1330 loss: 9.92810101e-07
Iter: 1331 loss: 9.927669e-07
Iter: 1332 loss: 9.92639684e-07
Iter: 1333 loss: 9.92601599e-07
Iter: 1334 loss: 9.92603191e-07
Iter: 1335 loss: 9.92524747e-07
Iter: 1336 loss: 9.93003255e-07
Iter: 1337 loss: 9.92544074e-07
Iter: 1338 loss: 9.9248291e-07
Iter: 1339 loss: 9.92372406e-07
Iter: 1340 loss: 9.94419565e-07
Iter: 1341 loss: 9.92367632e-07
Iter: 1342 loss: 9.9226e-07
Iter: 1343 loss: 9.92404239e-07
Iter: 1344 loss: 9.92116497e-07
Iter: 1345 loss: 9.92008495e-07
Iter: 1346 loss: 9.91894e-07
Iter: 1347 loss: 9.91909246e-07
Iter: 1348 loss: 9.91683464e-07
Iter: 1349 loss: 9.93877279e-07
Iter: 1350 loss: 9.91671186e-07
Iter: 1351 loss: 9.91574097e-07
Iter: 1352 loss: 9.91903789e-07
Iter: 1353 loss: 9.9151373e-07
Iter: 1354 loss: 9.91422e-07
Iter: 1355 loss: 9.91500656e-07
Iter: 1356 loss: 9.91304e-07
Iter: 1357 loss: 9.91239858e-07
Iter: 1358 loss: 9.91728484e-07
Iter: 1359 loss: 9.91157776e-07
Iter: 1360 loss: 9.91076831e-07
Iter: 1361 loss: 9.91143111e-07
Iter: 1362 loss: 9.90970307e-07
Iter: 1363 loss: 9.908224e-07
Iter: 1364 loss: 9.91834895e-07
Iter: 1365 loss: 9.90828198e-07
Iter: 1366 loss: 9.90679382e-07
Iter: 1367 loss: 9.90765102e-07
Iter: 1368 loss: 9.90614353e-07
Iter: 1369 loss: 9.90576154e-07
Iter: 1370 loss: 9.90528747e-07
Iter: 1371 loss: 9.90468607e-07
Iter: 1372 loss: 9.90711555e-07
Iter: 1373 loss: 9.90461103e-07
Iter: 1374 loss: 9.90406079e-07
Iter: 1375 loss: 9.90326157e-07
Iter: 1376 loss: 9.9103e-07
Iter: 1377 loss: 9.90279659e-07
Iter: 1378 loss: 9.90112767e-07
Iter: 1379 loss: 9.90198146e-07
Iter: 1380 loss: 9.89990326e-07
Iter: 1381 loss: 9.89923819e-07
Iter: 1382 loss: 9.89928139e-07
Iter: 1383 loss: 9.89743739e-07
Iter: 1384 loss: 9.89631758e-07
Iter: 1385 loss: 9.8965711e-07
Iter: 1386 loss: 9.89534328e-07
Iter: 1387 loss: 9.90340368e-07
Iter: 1388 loss: 9.8945543e-07
Iter: 1389 loss: 9.89344585e-07
Iter: 1390 loss: 9.89250566e-07
Iter: 1391 loss: 9.89200089e-07
Iter: 1392 loss: 9.89036835e-07
Iter: 1393 loss: 9.89055138e-07
Iter: 1394 loss: 9.88938154e-07
Iter: 1395 loss: 9.88968736e-07
Iter: 1396 loss: 9.88877446e-07
Iter: 1397 loss: 9.88751253e-07
Iter: 1398 loss: 9.89081514e-07
Iter: 1399 loss: 9.88669171e-07
Iter: 1400 loss: 9.88705779e-07
Iter: 1401 loss: 9.88627562e-07
Iter: 1402 loss: 9.88618467e-07
Iter: 1403 loss: 9.88668376e-07
Iter: 1404 loss: 9.88571401e-07
Iter: 1405 loss: 9.88496254e-07
Iter: 1406 loss: 9.88434294e-07
Iter: 1407 loss: 9.91036359e-07
Iter: 1408 loss: 9.88457e-07
Iter: 1409 loss: 9.88305374e-07
Iter: 1410 loss: 9.88959869e-07
Iter: 1411 loss: 9.88327201e-07
Iter: 1412 loss: 9.88293323e-07
Iter: 1413 loss: 9.88227839e-07
Iter: 1414 loss: 9.88156216e-07
Iter: 1415 loss: 9.88077431e-07
Iter: 1416 loss: 9.89055252e-07
Iter: 1417 loss: 9.8804685e-07
Iter: 1418 loss: 9.87941689e-07
Iter: 1419 loss: 9.88121883e-07
Iter: 1420 loss: 9.87942713e-07
Iter: 1421 loss: 9.8783562e-07
Iter: 1422 loss: 9.87711701e-07
Iter: 1423 loss: 9.87683279e-07
Iter: 1424 loss: 9.87571639e-07
Iter: 1425 loss: 9.8856458e-07
Iter: 1426 loss: 9.87567319e-07
Iter: 1427 loss: 9.87491e-07
Iter: 1428 loss: 9.87848352e-07
Iter: 1429 loss: 9.87429303e-07
Iter: 1430 loss: 9.87359726e-07
Iter: 1431 loss: 9.87735802e-07
Iter: 1432 loss: 9.87356884e-07
Iter: 1433 loss: 9.87258545e-07
Iter: 1434 loss: 9.87178851e-07
Iter: 1435 loss: 9.87154408e-07
Iter: 1436 loss: 9.87152589e-07
Iter: 1437 loss: 9.87009344e-07
Iter: 1438 loss: 9.87045269e-07
Iter: 1439 loss: 9.87012868e-07
Iter: 1440 loss: 9.87053681e-07
Iter: 1441 loss: 9.87039584e-07
Iter: 1442 loss: 9.87035833e-07
Iter: 1443 loss: 9.87045496e-07
Iter: 1444 loss: 9.87016165e-07
Iter: 1445 loss: 9.87044928e-07
Iter: 1446 loss: 9.87036e-07
Iter: 1447 loss: 9.87030262e-07
Iter: 1448 loss: 9.87026624e-07
Iter: 1449 loss: 9.87019e-07
Iter: 1450 loss: 9.87016392e-07
Iter: 1451 loss: 9.87012754e-07
Iter: 1452 loss: 9.87011163e-07
Iter: 1453 loss: 9.87010708e-07
Iter: 1454 loss: 9.87008889e-07
Iter: 1455 loss: 9.87009571e-07
Iter: 1456 loss: 9.87009457e-07
Iter: 1457 loss: 9.87009571e-07
Iter: 1458 loss: 9.87010708e-07
Iter: 1459 loss: 9.87009571e-07
Iter: 1460 loss: 9.87009571e-07
Iter: 1461 loss: 9.87009571e-07
Iter: 1462 loss: 9.87010708e-07
Iter: 1463 loss: 9.87009571e-07
Iter: 1464 loss: 9.87010708e-07
Iter: 1465 loss: 9.87009571e-07
Iter: 1466 loss: 9.87010708e-07
Iter: 1467 loss: 9.87010708e-07
Iter: 1468 loss: 9.87010708e-07
Iter: 1469 loss: 9.87009571e-07
Iter: 1470 loss: 9.87010708e-07
Iter: 1471 loss: 9.87009571e-07
Iter: 1472 loss: 9.87010708e-07
Iter: 1473 loss: 9.87010708e-07
Iter: 1474 loss: 9.87010708e-07
Iter: 1475 loss: 9.87010708e-07
Iter: 1476 loss: 9.87010708e-07
Iter: 1477 loss: 9.87009571e-07
Iter: 1478 loss: 9.87010708e-07
Iter: 1479 loss: 9.91959837e-07
Iter: 1480 loss: 9.8704777e-07
Iter: 1481 loss: 9.87065164e-07
Iter: 1482 loss: 9.87039698e-07
Iter: 1483 loss: 9.87050612e-07
Iter: 1484 loss: 9.87041e-07
Iter: 1485 loss: 9.87046178e-07
Iter: 1486 loss: 9.87041176e-07
Iter: 1487 loss: 9.87034241e-07
Iter: 1488 loss: 9.87023896e-07
Iter: 1489 loss: 9.87009116e-07
Iter: 1490 loss: 9.87016e-07
Iter: 1491 loss: 9.87018893e-07
Iter: 1492 loss: 9.87019689e-07
Iter: 1493 loss: 9.87012299e-07
Iter: 1494 loss: 9.8701e-07
Iter: 1495 loss: 9.87013777e-07
Iter: 1496 loss: 9.8701139e-07
Iter: 1497 loss: 9.87010708e-07
Iter: 1498 loss: 9.8701e-07
Iter: 1499 loss: 9.87009571e-07
Iter: 1500 loss: 9.8701048e-07
Iter: 1501 loss: 9.87010708e-07
Iter: 1502 loss: 9.87010594e-07
Iter: 1503 loss: 9.87010594e-07
Iter: 1504 loss: 9.87010708e-07
Iter: 1505 loss: 9.87010594e-07
Iter: 1506 loss: 9.87010708e-07
Iter: 1507 loss: 9.87010708e-07
Iter: 1508 loss: 9.87010594e-07
Iter: 1509 loss: 9.87010708e-07
Iter: 1510 loss: 9.87010594e-07
Iter: 1511 loss: 9.87010594e-07
Iter: 1512 loss: 9.87010594e-07
Iter: 1513 loss: 9.87010708e-07
Iter: 1514 loss: 9.87010594e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3
+ date
Mon Oct 26 10:38:12 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi3_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi3_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi3_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/300_300_300_1 --optimizer lbfgs --function f1 --psi 0 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi3_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0d04c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0d042840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0d042378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cfd4048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cfd42f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cf21840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cfd46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0ce7b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0ce7b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0ce7b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0ce4c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0ce0ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0ce28598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0ce53598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cddb2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0ce45510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cda86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cd88ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cd88730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0ccd1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0ccdf378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0ccdaf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0ccdfea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0ccaf7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cbbd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cbcda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cbcd7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cc0b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cbf07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cbac400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cb88158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cb44f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ae6e8b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3b0cb55488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ae6e5dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ae6e07ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.30534294e-06
Iter: 2 loss: 2.7785668e-06
Iter: 3 loss: 2.49651703e-06
Iter: 4 loss: 2.00649902e-06
Iter: 5 loss: 4.67338259e-06
Iter: 6 loss: 1.93462711e-06
Iter: 7 loss: 1.78883465e-06
Iter: 8 loss: 1.53792212e-06
Iter: 9 loss: 1.53769417e-06
Iter: 10 loss: 1.33054414e-06
Iter: 11 loss: 1.32967443e-06
Iter: 12 loss: 1.28974852e-06
Iter: 13 loss: 1.27718124e-06
Iter: 14 loss: 1.25370138e-06
Iter: 15 loss: 1.19346907e-06
Iter: 16 loss: 1.29858313e-06
Iter: 17 loss: 1.16686135e-06
Iter: 18 loss: 1.12128657e-06
Iter: 19 loss: 1.43978855e-06
Iter: 20 loss: 1.11706299e-06
Iter: 21 loss: 1.0719815e-06
Iter: 22 loss: 1.14946874e-06
Iter: 23 loss: 1.05188428e-06
Iter: 24 loss: 1.03316006e-06
Iter: 25 loss: 1.0323547e-06
Iter: 26 loss: 1.023305e-06
Iter: 27 loss: 1.01029832e-06
Iter: 28 loss: 1.0099169e-06
Iter: 29 loss: 9.88739544e-07
Iter: 30 loss: 1.06669e-06
Iter: 31 loss: 9.83527116e-07
Iter: 32 loss: 9.69318535e-07
Iter: 33 loss: 9.66026391e-07
Iter: 34 loss: 9.56973167e-07
Iter: 35 loss: 9.37841548e-07
Iter: 36 loss: 1.14713816e-06
Iter: 37 loss: 9.3749253e-07
Iter: 38 loss: 9.40743291e-07
Iter: 39 loss: 9.33442266e-07
Iter: 40 loss: 9.29875966e-07
Iter: 41 loss: 9.19479248e-07
Iter: 42 loss: 9.59982344e-07
Iter: 43 loss: 9.15096166e-07
Iter: 44 loss: 9.07032586e-07
Iter: 45 loss: 1.00763373e-06
Iter: 46 loss: 9.06942091e-07
Iter: 47 loss: 9.01308397e-07
Iter: 48 loss: 9.09766754e-07
Iter: 49 loss: 8.98595317e-07
Iter: 50 loss: 8.89244916e-07
Iter: 51 loss: 8.90179535e-07
Iter: 52 loss: 8.8200045e-07
Iter: 53 loss: 8.77112257e-07
Iter: 54 loss: 8.87262445e-07
Iter: 55 loss: 8.75165597e-07
Iter: 56 loss: 8.70223289e-07
Iter: 57 loss: 9.10122424e-07
Iter: 58 loss: 8.69897e-07
Iter: 59 loss: 8.67062624e-07
Iter: 60 loss: 8.8384661e-07
Iter: 61 loss: 8.66699168e-07
Iter: 62 loss: 8.63681862e-07
Iter: 63 loss: 8.60221e-07
Iter: 64 loss: 8.59768477e-07
Iter: 65 loss: 8.5653943e-07
Iter: 66 loss: 8.95972391e-07
Iter: 67 loss: 8.56512656e-07
Iter: 68 loss: 8.5340389e-07
Iter: 69 loss: 8.49236642e-07
Iter: 70 loss: 8.49000173e-07
Iter: 71 loss: 8.45122258e-07
Iter: 72 loss: 8.45143575e-07
Iter: 73 loss: 8.42621148e-07
Iter: 74 loss: 8.36728702e-07
Iter: 75 loss: 9.07250467e-07
Iter: 76 loss: 8.36240474e-07
Iter: 77 loss: 8.342e-07
Iter: 78 loss: 8.32964e-07
Iter: 79 loss: 8.31443231e-07
Iter: 80 loss: 8.31317834e-07
Iter: 81 loss: 8.29898e-07
Iter: 82 loss: 8.26404403e-07
Iter: 83 loss: 8.61155115e-07
Iter: 84 loss: 8.25998086e-07
Iter: 85 loss: 8.24344681e-07
Iter: 86 loss: 8.35176479e-07
Iter: 87 loss: 8.24127369e-07
Iter: 88 loss: 8.22088111e-07
Iter: 89 loss: 8.23863445e-07
Iter: 90 loss: 8.20899572e-07
Iter: 91 loss: 8.19173124e-07
Iter: 92 loss: 8.25861775e-07
Iter: 93 loss: 8.18814556e-07
Iter: 94 loss: 8.17208559e-07
Iter: 95 loss: 8.14908958e-07
Iter: 96 loss: 8.14856776e-07
Iter: 97 loss: 8.12385792e-07
Iter: 98 loss: 8.48410423e-07
Iter: 99 loss: 8.12348901e-07
Iter: 100 loss: 8.11002906e-07
Iter: 101 loss: 8.13823533e-07
Iter: 102 loss: 8.10448228e-07
Iter: 103 loss: 8.08604966e-07
Iter: 104 loss: 8.14473e-07
Iter: 105 loss: 8.08042444e-07
Iter: 106 loss: 8.07122547e-07
Iter: 107 loss: 8.06967e-07
Iter: 108 loss: 8.06314802e-07
Iter: 109 loss: 8.04483534e-07
Iter: 110 loss: 8.07855827e-07
Iter: 111 loss: 8.03711259e-07
Iter: 112 loss: 8.02579393e-07
Iter: 113 loss: 8.08216669e-07
Iter: 114 loss: 8.02385671e-07
Iter: 115 loss: 8.01440706e-07
Iter: 116 loss: 8.01434453e-07
Iter: 117 loss: 8.00538658e-07
Iter: 118 loss: 7.9972142e-07
Iter: 119 loss: 7.99537588e-07
Iter: 120 loss: 7.98692383e-07
Iter: 121 loss: 7.96821439e-07
Iter: 122 loss: 8.19643049e-07
Iter: 123 loss: 7.96629877e-07
Iter: 124 loss: 7.95850838e-07
Iter: 125 loss: 7.953862e-07
Iter: 126 loss: 7.94585674e-07
Iter: 127 loss: 7.95934341e-07
Iter: 128 loss: 7.94241771e-07
Iter: 129 loss: 7.93271056e-07
Iter: 130 loss: 7.91365835e-07
Iter: 131 loss: 8.31069485e-07
Iter: 132 loss: 7.91324283e-07
Iter: 133 loss: 7.90285753e-07
Iter: 134 loss: 7.90277e-07
Iter: 135 loss: 7.89262629e-07
Iter: 136 loss: 7.88333182e-07
Iter: 137 loss: 7.88075226e-07
Iter: 138 loss: 7.87330521e-07
Iter: 139 loss: 7.87169824e-07
Iter: 140 loss: 7.86538294e-07
Iter: 141 loss: 7.85662792e-07
Iter: 142 loss: 7.8561186e-07
Iter: 143 loss: 7.84765803e-07
Iter: 144 loss: 7.95233177e-07
Iter: 145 loss: 7.84749943e-07
Iter: 146 loss: 7.84257963e-07
Iter: 147 loss: 7.83726477e-07
Iter: 148 loss: 7.83624671e-07
Iter: 149 loss: 7.85030863e-07
Iter: 150 loss: 7.83449764e-07
Iter: 151 loss: 7.83270934e-07
Iter: 152 loss: 7.82990753e-07
Iter: 153 loss: 7.87329782e-07
Iter: 154 loss: 7.82986149e-07
Iter: 155 loss: 7.82477628e-07
Iter: 156 loss: 7.81511858e-07
Iter: 157 loss: 8.02299e-07
Iter: 158 loss: 7.81525557e-07
Iter: 159 loss: 7.80359926e-07
Iter: 160 loss: 7.89225169e-07
Iter: 161 loss: 7.8028836e-07
Iter: 162 loss: 7.79658649e-07
Iter: 163 loss: 7.78831122e-07
Iter: 164 loss: 7.78750291e-07
Iter: 165 loss: 7.77585342e-07
Iter: 166 loss: 7.87794761e-07
Iter: 167 loss: 7.77516448e-07
Iter: 168 loss: 7.768773e-07
Iter: 169 loss: 7.77476089e-07
Iter: 170 loss: 7.76472461e-07
Iter: 171 loss: 7.75496346e-07
Iter: 172 loss: 7.77354444e-07
Iter: 173 loss: 7.75116064e-07
Iter: 174 loss: 7.74465e-07
Iter: 175 loss: 7.74468958e-07
Iter: 176 loss: 7.73967713e-07
Iter: 177 loss: 7.73176509e-07
Iter: 178 loss: 7.73166221e-07
Iter: 179 loss: 7.72700957e-07
Iter: 180 loss: 7.73938893e-07
Iter: 181 loss: 7.72579369e-07
Iter: 182 loss: 7.72107e-07
Iter: 183 loss: 7.73072145e-07
Iter: 184 loss: 7.71951534e-07
Iter: 185 loss: 7.71477914e-07
Iter: 186 loss: 7.71493433e-07
Iter: 187 loss: 7.71291866e-07
Iter: 188 loss: 7.70938641e-07
Iter: 189 loss: 7.74573209e-07
Iter: 190 loss: 7.70866109e-07
Iter: 191 loss: 7.70245833e-07
Iter: 192 loss: 7.71673228e-07
Iter: 193 loss: 7.70018971e-07
Iter: 194 loss: 7.69382666e-07
Iter: 195 loss: 7.74205375e-07
Iter: 196 loss: 7.69345547e-07
Iter: 197 loss: 7.68976861e-07
Iter: 198 loss: 7.68327482e-07
Iter: 199 loss: 7.68319637e-07
Iter: 200 loss: 7.67404e-07
Iter: 201 loss: 7.71287148e-07
Iter: 202 loss: 7.67208917e-07
Iter: 203 loss: 7.66592e-07
Iter: 204 loss: 7.67801794e-07
Iter: 205 loss: 7.66380595e-07
Iter: 206 loss: 7.65515665e-07
Iter: 207 loss: 7.67245922e-07
Iter: 208 loss: 7.65135951e-07
Iter: 209 loss: 7.64550691e-07
Iter: 210 loss: 7.64017955e-07
Iter: 211 loss: 7.63879314e-07
Iter: 212 loss: 7.63543085e-07
Iter: 213 loss: 7.63381763e-07
Iter: 214 loss: 7.63036098e-07
Iter: 215 loss: 7.6331122e-07
Iter: 216 loss: 7.62821628e-07
Iter: 217 loss: 7.62501486e-07
Iter: 218 loss: 7.6755606e-07
Iter: 219 loss: 7.62495347e-07
Iter: 220 loss: 7.62196692e-07
Iter: 221 loss: 7.62913544e-07
Iter: 222 loss: 7.62048217e-07
Iter: 223 loss: 7.61893546e-07
Iter: 224 loss: 7.61405545e-07
Iter: 225 loss: 7.63471348e-07
Iter: 226 loss: 7.61196e-07
Iter: 227 loss: 7.60585237e-07
Iter: 228 loss: 7.60589444e-07
Iter: 229 loss: 7.60259525e-07
Iter: 230 loss: 7.62331297e-07
Iter: 231 loss: 7.602182e-07
Iter: 232 loss: 7.5993205e-07
Iter: 233 loss: 7.59364411e-07
Iter: 234 loss: 7.69268e-07
Iter: 235 loss: 7.59337411e-07
Iter: 236 loss: 7.58939791e-07
Iter: 237 loss: 7.58913529e-07
Iter: 238 loss: 7.58626584e-07
Iter: 239 loss: 7.58239594e-07
Iter: 240 loss: 7.58211399e-07
Iter: 241 loss: 7.57769726e-07
Iter: 242 loss: 7.57758926e-07
Iter: 243 loss: 7.57496707e-07
Iter: 244 loss: 7.56976931e-07
Iter: 245 loss: 7.67448569e-07
Iter: 246 loss: 7.569638e-07
Iter: 247 loss: 7.5670323e-07
Iter: 248 loss: 7.56671739e-07
Iter: 249 loss: 7.56399686e-07
Iter: 250 loss: 7.56182374e-07
Iter: 251 loss: 7.56136956e-07
Iter: 252 loss: 7.55969097e-07
Iter: 253 loss: 7.55934821e-07
Iter: 254 loss: 7.55793849e-07
Iter: 255 loss: 7.57195153e-07
Iter: 256 loss: 7.55781457e-07
Iter: 257 loss: 7.55639462e-07
Iter: 258 loss: 7.55383269e-07
Iter: 259 loss: 7.60827447e-07
Iter: 260 loss: 7.55391511e-07
Iter: 261 loss: 7.55069721e-07
Iter: 262 loss: 7.55777194e-07
Iter: 263 loss: 7.54993494e-07
Iter: 264 loss: 7.54802102e-07
Iter: 265 loss: 7.55912311e-07
Iter: 266 loss: 7.54728831e-07
Iter: 267 loss: 7.54457801e-07
Iter: 268 loss: 7.54085477e-07
Iter: 269 loss: 7.54079338e-07
Iter: 270 loss: 7.53680638e-07
Iter: 271 loss: 7.54848088e-07
Iter: 272 loss: 7.53575819e-07
Iter: 273 loss: 7.53122208e-07
Iter: 274 loss: 7.53540803e-07
Iter: 275 loss: 7.52840549e-07
Iter: 276 loss: 7.52550932e-07
Iter: 277 loss: 7.52569122e-07
Iter: 278 loss: 7.52306505e-07
Iter: 279 loss: 7.51859943e-07
Iter: 280 loss: 7.51926677e-07
Iter: 281 loss: 7.51546622e-07
Iter: 282 loss: 7.56076247e-07
Iter: 283 loss: 7.51549805e-07
Iter: 284 loss: 7.51219e-07
Iter: 285 loss: 7.51273546e-07
Iter: 286 loss: 7.51001494e-07
Iter: 287 loss: 7.50858817e-07
Iter: 288 loss: 7.50781055e-07
Iter: 289 loss: 7.50637241e-07
Iter: 290 loss: 7.51244386e-07
Iter: 291 loss: 7.50604158e-07
Iter: 292 loss: 7.50494451e-07
Iter: 293 loss: 7.50237461e-07
Iter: 294 loss: 7.55395718e-07
Iter: 295 loss: 7.5021768e-07
Iter: 296 loss: 7.49835579e-07
Iter: 297 loss: 7.50636218e-07
Iter: 298 loss: 7.49695118e-07
Iter: 299 loss: 7.49505602e-07
Iter: 300 loss: 7.49514e-07
Iter: 301 loss: 7.49372305e-07
Iter: 302 loss: 7.48956438e-07
Iter: 303 loss: 7.51247967e-07
Iter: 304 loss: 7.48798698e-07
Iter: 305 loss: 7.48541822e-07
Iter: 306 loss: 7.48521416e-07
Iter: 307 loss: 7.48307116e-07
Iter: 308 loss: 7.48186494e-07
Iter: 309 loss: 7.48066896e-07
Iter: 310 loss: 7.47837362e-07
Iter: 311 loss: 7.47852141e-07
Iter: 312 loss: 7.47658703e-07
Iter: 313 loss: 7.47412969e-07
Iter: 314 loss: 7.47410184e-07
Iter: 315 loss: 7.47043202e-07
Iter: 316 loss: 7.50362233e-07
Iter: 317 loss: 7.47019e-07
Iter: 318 loss: 7.46859257e-07
Iter: 319 loss: 7.48218895e-07
Iter: 320 loss: 7.46861474e-07
Iter: 321 loss: 7.46634782e-07
Iter: 322 loss: 7.47049057e-07
Iter: 323 loss: 7.46542e-07
Iter: 324 loss: 7.46352157e-07
Iter: 325 loss: 7.46511773e-07
Iter: 326 loss: 7.46187482e-07
Iter: 327 loss: 7.46037699e-07
Iter: 328 loss: 7.46312821e-07
Iter: 329 loss: 7.45936e-07
Iter: 330 loss: 7.45723867e-07
Iter: 331 loss: 7.45915031e-07
Iter: 332 loss: 7.45604211e-07
Iter: 333 loss: 7.45342732e-07
Iter: 334 loss: 7.47277682e-07
Iter: 335 loss: 7.45358307e-07
Iter: 336 loss: 7.45203636e-07
Iter: 337 loss: 7.44888098e-07
Iter: 338 loss: 7.49131686e-07
Iter: 339 loss: 7.44890599e-07
Iter: 340 loss: 7.44579665e-07
Iter: 341 loss: 7.48652042e-07
Iter: 342 loss: 7.44585236e-07
Iter: 343 loss: 7.44427382e-07
Iter: 344 loss: 7.44408453e-07
Iter: 345 loss: 7.44313866e-07
Iter: 346 loss: 7.43994406e-07
Iter: 347 loss: 7.45249395e-07
Iter: 348 loss: 7.43935288e-07
Iter: 349 loss: 7.43745318e-07
Iter: 350 loss: 7.43697e-07
Iter: 351 loss: 7.43583428e-07
Iter: 352 loss: 7.4329e-07
Iter: 353 loss: 7.45320278e-07
Iter: 354 loss: 7.43223552e-07
Iter: 355 loss: 7.43226281e-07
Iter: 356 loss: 7.4314238e-07
Iter: 357 loss: 7.4309e-07
Iter: 358 loss: 7.42846112e-07
Iter: 359 loss: 7.43895498e-07
Iter: 360 loss: 7.4277e-07
Iter: 361 loss: 7.42392501e-07
Iter: 362 loss: 7.43675855e-07
Iter: 363 loss: 7.42327757e-07
Iter: 364 loss: 7.42142106e-07
Iter: 365 loss: 7.42109137e-07
Iter: 366 loss: 7.41998406e-07
Iter: 367 loss: 7.41690883e-07
Iter: 368 loss: 7.43528346e-07
Iter: 369 loss: 7.4162665e-07
Iter: 370 loss: 7.4139956e-07
Iter: 371 loss: 7.42553084e-07
Iter: 372 loss: 7.41384383e-07
Iter: 373 loss: 7.41173949e-07
Iter: 374 loss: 7.40828114e-07
Iter: 375 loss: 7.45432772e-07
Iter: 376 loss: 7.40786334e-07
Iter: 377 loss: 7.40541054e-07
Iter: 378 loss: 7.40563451e-07
Iter: 379 loss: 7.40364726e-07
Iter: 380 loss: 7.40274174e-07
Iter: 381 loss: 7.4017612e-07
Iter: 382 loss: 7.40007351e-07
Iter: 383 loss: 7.4217354e-07
Iter: 384 loss: 7.39977736e-07
Iter: 385 loss: 7.39765369e-07
Iter: 386 loss: 7.39577331e-07
Iter: 387 loss: 7.39501729e-07
Iter: 388 loss: 7.39268899e-07
Iter: 389 loss: 7.39277482e-07
Iter: 390 loss: 7.39176585e-07
Iter: 391 loss: 7.39183747e-07
Iter: 392 loss: 7.39100642e-07
Iter: 393 loss: 7.38747872e-07
Iter: 394 loss: 7.40404118e-07
Iter: 395 loss: 7.38695348e-07
Iter: 396 loss: 7.38468771e-07
Iter: 397 loss: 7.38488893e-07
Iter: 398 loss: 7.38340873e-07
Iter: 399 loss: 7.38174094e-07
Iter: 400 loss: 7.38127937e-07
Iter: 401 loss: 7.37906134e-07
Iter: 402 loss: 7.39264e-07
Iter: 403 loss: 7.3785742e-07
Iter: 404 loss: 7.37677283e-07
Iter: 405 loss: 7.37949904e-07
Iter: 406 loss: 7.37620439e-07
Iter: 407 loss: 7.37385e-07
Iter: 408 loss: 7.38499125e-07
Iter: 409 loss: 7.37295352e-07
Iter: 410 loss: 7.37091227e-07
Iter: 411 loss: 7.373377e-07
Iter: 412 loss: 7.3706542e-07
Iter: 413 loss: 7.3689489e-07
Iter: 414 loss: 7.3675011e-07
Iter: 415 loss: 7.36682352e-07
Iter: 416 loss: 7.36464528e-07
Iter: 417 loss: 7.38654e-07
Iter: 418 loss: 7.36502329e-07
Iter: 419 loss: 7.36293373e-07
Iter: 420 loss: 7.36101526e-07
Iter: 421 loss: 7.36036782e-07
Iter: 422 loss: 7.35950096e-07
Iter: 423 loss: 7.35884896e-07
Iter: 424 loss: 7.35741594e-07
Iter: 425 loss: 7.35838285e-07
Iter: 426 loss: 7.35605624e-07
Iter: 427 loss: 7.35482161e-07
Iter: 428 loss: 7.35273147e-07
Iter: 429 loss: 7.35275478e-07
Iter: 430 loss: 7.35050094e-07
Iter: 431 loss: 7.35954359e-07
Iter: 432 loss: 7.35047138e-07
Iter: 433 loss: 7.34835112e-07
Iter: 434 loss: 7.35245521e-07
Iter: 435 loss: 7.34736659e-07
Iter: 436 loss: 7.34598643e-07
Iter: 437 loss: 7.36360107e-07
Iter: 438 loss: 7.34612513e-07
Iter: 439 loss: 7.3450866e-07
Iter: 440 loss: 7.34307889e-07
Iter: 441 loss: 7.37985488e-07
Iter: 442 loss: 7.34286289e-07
Iter: 443 loss: 7.34198352e-07
Iter: 444 loss: 7.34137757e-07
Iter: 445 loss: 7.34070113e-07
Iter: 446 loss: 7.34108653e-07
Iter: 447 loss: 7.34038622e-07
Iter: 448 loss: 7.33892534e-07
Iter: 449 loss: 7.33566139e-07
Iter: 450 loss: 7.37229357e-07
Iter: 451 loss: 7.3353533e-07
Iter: 452 loss: 7.33378499e-07
Iter: 453 loss: 7.33368e-07
Iter: 454 loss: 7.33183e-07
Iter: 455 loss: 7.33050683e-07
Iter: 456 loss: 7.32982812e-07
Iter: 457 loss: 7.33099682e-07
Iter: 458 loss: 7.32898968e-07
Iter: 459 loss: 7.32823537e-07
Iter: 460 loss: 7.32697742e-07
Iter: 461 loss: 7.32680405e-07
Iter: 462 loss: 7.32506351e-07
Iter: 463 loss: 7.32258627e-07
Iter: 464 loss: 7.32248509e-07
Iter: 465 loss: 7.32086164e-07
Iter: 466 loss: 7.33792831e-07
Iter: 467 loss: 7.32090029e-07
Iter: 468 loss: 7.3190995e-07
Iter: 469 loss: 7.31705939e-07
Iter: 470 loss: 7.31725891e-07
Iter: 471 loss: 7.31523642e-07
Iter: 472 loss: 7.33086267e-07
Iter: 473 loss: 7.31515115e-07
Iter: 474 loss: 7.3135709e-07
Iter: 475 loss: 7.3188022e-07
Iter: 476 loss: 7.31296439e-07
Iter: 477 loss: 7.31079524e-07
Iter: 478 loss: 7.3123897e-07
Iter: 479 loss: 7.30971e-07
Iter: 480 loss: 7.30824922e-07
Iter: 481 loss: 7.3151233e-07
Iter: 482 loss: 7.30781153e-07
Iter: 483 loss: 7.30602267e-07
Iter: 484 loss: 7.30768875e-07
Iter: 485 loss: 7.3052081e-07
Iter: 486 loss: 7.30339e-07
Iter: 487 loss: 7.31596799e-07
Iter: 488 loss: 7.30311342e-07
Iter: 489 loss: 7.30217266e-07
Iter: 490 loss: 7.29999499e-07
Iter: 491 loss: 7.30010697e-07
Iter: 492 loss: 7.29939757e-07
Iter: 493 loss: 7.29892463e-07
Iter: 494 loss: 7.29859607e-07
Iter: 495 loss: 7.29829537e-07
Iter: 496 loss: 7.29826127e-07
Iter: 497 loss: 7.29702379e-07
Iter: 498 loss: 7.29672536e-07
Iter: 499 loss: 7.29530939e-07
Iter: 500 loss: 7.29391e-07
Iter: 501 loss: 7.2937479e-07
Iter: 502 loss: 7.29253429e-07
Iter: 503 loss: 7.29137582e-07
Iter: 504 loss: 7.29108308e-07
Iter: 505 loss: 7.28951875e-07
Iter: 506 loss: 7.2896205e-07
Iter: 507 loss: 7.28839382e-07
Iter: 508 loss: 7.28715122e-07
Iter: 509 loss: 7.28662599e-07
Iter: 510 loss: 7.28542204e-07
Iter: 511 loss: 7.2852788e-07
Iter: 512 loss: 7.28393672e-07
Iter: 513 loss: 7.28726775e-07
Iter: 514 loss: 7.28363e-07
Iter: 515 loss: 7.2818915e-07
Iter: 516 loss: 7.28022201e-07
Iter: 517 loss: 7.2802095e-07
Iter: 518 loss: 7.27825295e-07
Iter: 519 loss: 7.28912255e-07
Iter: 520 loss: 7.27795396e-07
Iter: 521 loss: 7.27637598e-07
Iter: 522 loss: 7.2777e-07
Iter: 523 loss: 7.27556426e-07
Iter: 524 loss: 7.27799716e-07
Iter: 525 loss: 7.27476049e-07
Iter: 526 loss: 7.27454335e-07
Iter: 527 loss: 7.27340762e-07
Iter: 528 loss: 7.28049827e-07
Iter: 529 loss: 7.27316092e-07
Iter: 530 loss: 7.27164775e-07
Iter: 531 loss: 7.2724572e-07
Iter: 532 loss: 7.27033239e-07
Iter: 533 loss: 7.26897667e-07
Iter: 534 loss: 7.27411589e-07
Iter: 535 loss: 7.26839346e-07
Iter: 536 loss: 7.26690928e-07
Iter: 537 loss: 7.26614303e-07
Iter: 538 loss: 7.26567691e-07
Iter: 539 loss: 7.2637738e-07
Iter: 540 loss: 7.26372093e-07
Iter: 541 loss: 7.26208668e-07
Iter: 542 loss: 7.26007215e-07
Iter: 543 loss: 7.2600875e-07
Iter: 544 loss: 7.25886423e-07
Iter: 545 loss: 7.26602082e-07
Iter: 546 loss: 7.25853567e-07
Iter: 547 loss: 7.25713e-07
Iter: 548 loss: 7.25497728e-07
Iter: 549 loss: 7.25461632e-07
Iter: 550 loss: 7.2532265e-07
Iter: 551 loss: 7.27407894e-07
Iter: 552 loss: 7.25321229e-07
Iter: 553 loss: 7.25182133e-07
Iter: 554 loss: 7.24983124e-07
Iter: 555 loss: 7.25003588e-07
Iter: 556 loss: 7.25081122e-07
Iter: 557 loss: 7.24931226e-07
Iter: 558 loss: 7.24826e-07
Iter: 559 loss: 7.2505884e-07
Iter: 560 loss: 7.24827487e-07
Iter: 561 loss: 7.24763368e-07
Iter: 562 loss: 7.24668e-07
Iter: 563 loss: 7.2666387e-07
Iter: 564 loss: 7.24664801e-07
Iter: 565 loss: 7.24546567e-07
Iter: 566 loss: 7.25619032e-07
Iter: 567 loss: 7.2455282e-07
Iter: 568 loss: 7.24422364e-07
Iter: 569 loss: 7.24354607e-07
Iter: 570 loss: 7.24303561e-07
Iter: 571 loss: 7.24132747e-07
Iter: 572 loss: 7.24605741e-07
Iter: 573 loss: 7.24070674e-07
Iter: 574 loss: 7.23975518e-07
Iter: 575 loss: 7.2435455e-07
Iter: 576 loss: 7.23933567e-07
Iter: 577 loss: 7.23807943e-07
Iter: 578 loss: 7.23996095e-07
Iter: 579 loss: 7.23753715e-07
Iter: 580 loss: 7.23616722e-07
Iter: 581 loss: 7.25164853e-07
Iter: 582 loss: 7.23619564e-07
Iter: 583 loss: 7.23512812e-07
Iter: 584 loss: 7.23287712e-07
Iter: 585 loss: 7.27216275e-07
Iter: 586 loss: 7.23336825e-07
Iter: 587 loss: 7.23149128e-07
Iter: 588 loss: 7.24626432e-07
Iter: 589 loss: 7.23151061e-07
Iter: 590 loss: 7.23014409e-07
Iter: 591 loss: 7.22833e-07
Iter: 592 loss: 7.22840696e-07
Iter: 593 loss: 7.22617756e-07
Iter: 594 loss: 7.22620825e-07
Iter: 595 loss: 7.22582399e-07
Iter: 596 loss: 7.22536242e-07
Iter: 597 loss: 7.22508389e-07
Iter: 598 loss: 7.22314383e-07
Iter: 599 loss: 7.22866844e-07
Iter: 600 loss: 7.22227242e-07
Iter: 601 loss: 7.22113896e-07
Iter: 602 loss: 7.23242579e-07
Iter: 603 loss: 7.22083144e-07
Iter: 604 loss: 7.21936317e-07
Iter: 605 loss: 7.22074105e-07
Iter: 606 loss: 7.21857361e-07
Iter: 607 loss: 7.21689389e-07
Iter: 608 loss: 7.22402149e-07
Iter: 609 loss: 7.21644142e-07
Iter: 610 loss: 7.21529943e-07
Iter: 611 loss: 7.21767e-07
Iter: 612 loss: 7.21522383e-07
Iter: 613 loss: 7.21379593e-07
Iter: 614 loss: 7.21489812e-07
Iter: 615 loss: 7.21333095e-07
Iter: 616 loss: 7.2121037e-07
Iter: 617 loss: 7.21575248e-07
Iter: 618 loss: 7.21128174e-07
Iter: 619 loss: 7.2094133e-07
Iter: 620 loss: 7.21273409e-07
Iter: 621 loss: 7.20877e-07
Iter: 622 loss: 7.20763069e-07
Iter: 623 loss: 7.21457866e-07
Iter: 624 loss: 7.20708385e-07
Iter: 625 loss: 7.20555647e-07
Iter: 626 loss: 7.20406319e-07
Iter: 627 loss: 7.20379603e-07
Iter: 628 loss: 7.20217542e-07
Iter: 629 loss: 7.20229764e-07
Iter: 630 loss: 7.20150524e-07
Iter: 631 loss: 7.20141315e-07
Iter: 632 loss: 7.20079697e-07
Iter: 633 loss: 7.19914169e-07
Iter: 634 loss: 7.22693699e-07
Iter: 635 loss: 7.19914738e-07
Iter: 636 loss: 7.19795082e-07
Iter: 637 loss: 7.19885804e-07
Iter: 638 loss: 7.19734771e-07
Iter: 639 loss: 7.19583909e-07
Iter: 640 loss: 7.19926049e-07
Iter: 641 loss: 7.19537297e-07
Iter: 642 loss: 7.19398201e-07
Iter: 643 loss: 7.20788421e-07
Iter: 644 loss: 7.19413777e-07
Iter: 645 loss: 7.19347327e-07
Iter: 646 loss: 7.1921e-07
Iter: 647 loss: 7.19249385e-07
Iter: 648 loss: 7.19048728e-07
Iter: 649 loss: 7.20052242e-07
Iter: 650 loss: 7.19042305e-07
Iter: 651 loss: 7.18958859e-07
Iter: 652 loss: 7.19003651e-07
Iter: 653 loss: 7.1885313e-07
Iter: 654 loss: 7.18675096e-07
Iter: 655 loss: 7.18869501e-07
Iter: 656 loss: 7.1858193e-07
Iter: 657 loss: 7.18483534e-07
Iter: 658 loss: 7.18497063e-07
Iter: 659 loss: 7.18405374e-07
Iter: 660 loss: 7.183761e-07
Iter: 661 loss: 7.18327783e-07
Iter: 662 loss: 7.18202443e-07
Iter: 663 loss: 7.18953572e-07
Iter: 664 loss: 7.18155775e-07
Iter: 665 loss: 7.18116439e-07
Iter: 666 loss: 7.18105412e-07
Iter: 667 loss: 7.18088074e-07
Iter: 668 loss: 7.17943522e-07
Iter: 669 loss: 7.18138381e-07
Iter: 670 loss: 7.17858939e-07
Iter: 671 loss: 7.17738601e-07
Iter: 672 loss: 7.19467494e-07
Iter: 673 loss: 7.17731609e-07
Iter: 674 loss: 7.17691364e-07
Iter: 675 loss: 7.17540047e-07
Iter: 676 loss: 7.17533624e-07
Iter: 677 loss: 7.17446198e-07
Iter: 678 loss: 7.17434148e-07
Iter: 679 loss: 7.1736622e-07
Iter: 680 loss: 7.17447278e-07
Iter: 681 loss: 7.17346e-07
Iter: 682 loss: 7.17271178e-07
Iter: 683 loss: 7.17112925e-07
Iter: 684 loss: 7.17126e-07
Iter: 685 loss: 7.16988211e-07
Iter: 686 loss: 7.1810706e-07
Iter: 687 loss: 7.16990144e-07
Iter: 688 loss: 7.16857301e-07
Iter: 689 loss: 7.16621628e-07
Iter: 690 loss: 7.16634645e-07
Iter: 691 loss: 7.16392378e-07
Iter: 692 loss: 7.17863941e-07
Iter: 693 loss: 7.16403179e-07
Iter: 694 loss: 7.16204454e-07
Iter: 695 loss: 7.17012313e-07
Iter: 696 loss: 7.16131581e-07
Iter: 697 loss: 7.16005047e-07
Iter: 698 loss: 7.17693297e-07
Iter: 699 loss: 7.15990495e-07
Iter: 700 loss: 7.15942633e-07
Iter: 701 loss: 7.16469344e-07
Iter: 702 loss: 7.1594809e-07
Iter: 703 loss: 7.1585896e-07
Iter: 704 loss: 7.15632268e-07
Iter: 705 loss: 7.18728757e-07
Iter: 706 loss: 7.15662111e-07
Iter: 707 loss: 7.1546458e-07
Iter: 708 loss: 7.1602858e-07
Iter: 709 loss: 7.15460715e-07
Iter: 710 loss: 7.15363285e-07
Iter: 711 loss: 7.15223337e-07
Iter: 712 loss: 7.15263e-07
Iter: 713 loss: 7.15086117e-07
Iter: 714 loss: 7.17227294e-07
Iter: 715 loss: 7.1507327e-07
Iter: 716 loss: 7.1494685e-07
Iter: 717 loss: 7.15053488e-07
Iter: 718 loss: 7.14845953e-07
Iter: 719 loss: 7.14706061e-07
Iter: 720 loss: 7.15458953e-07
Iter: 721 loss: 7.14680482e-07
Iter: 722 loss: 7.1457589e-07
Iter: 723 loss: 7.1450944e-07
Iter: 724 loss: 7.1449449e-07
Iter: 725 loss: 7.14317139e-07
Iter: 726 loss: 7.15085662e-07
Iter: 727 loss: 7.1429838e-07
Iter: 728 loss: 7.14203964e-07
Iter: 729 loss: 7.14404052e-07
Iter: 730 loss: 7.14176849e-07
Iter: 731 loss: 7.14027351e-07
Iter: 732 loss: 7.14664282e-07
Iter: 733 loss: 7.1398415e-07
Iter: 734 loss: 7.13918e-07
Iter: 735 loss: 7.14780128e-07
Iter: 736 loss: 7.13917586e-07
Iter: 737 loss: 7.13867621e-07
Iter: 738 loss: 7.1400035e-07
Iter: 739 loss: 7.13815e-07
Iter: 740 loss: 7.13751888e-07
Iter: 741 loss: 7.14199246e-07
Iter: 742 loss: 7.13769168e-07
Iter: 743 loss: 7.13670829e-07
Iter: 744 loss: 7.13662814e-07
Iter: 745 loss: 7.13621944e-07
Iter: 746 loss: 7.13562486e-07
Iter: 747 loss: 7.13436748e-07
Iter: 748 loss: 7.14565488e-07
Iter: 749 loss: 7.1339e-07
Iter: 750 loss: 7.13284294e-07
Iter: 751 loss: 7.13286511e-07
Iter: 752 loss: 7.13170436e-07
Iter: 753 loss: 7.13217503e-07
Iter: 754 loss: 7.13077441e-07
Iter: 755 loss: 7.12977908e-07
Iter: 756 loss: 7.13302938e-07
Iter: 757 loss: 7.12953238e-07
Iter: 758 loss: 7.12852739e-07
Iter: 759 loss: 7.12949827e-07
Iter: 760 loss: 7.1277077e-07
Iter: 761 loss: 7.12675558e-07
Iter: 762 loss: 7.12670726e-07
Iter: 763 loss: 7.12629799e-07
Iter: 764 loss: 7.12660551e-07
Iter: 765 loss: 7.12560791e-07
Iter: 766 loss: 7.12504061e-07
Iter: 767 loss: 7.12910492e-07
Iter: 768 loss: 7.1246393e-07
Iter: 769 loss: 7.12404528e-07
Iter: 770 loss: 7.12873543e-07
Iter: 771 loss: 7.12412032e-07
Iter: 772 loss: 7.12347173e-07
Iter: 773 loss: 7.12662711e-07
Iter: 774 loss: 7.12328415e-07
Iter: 775 loss: 7.12271913e-07
Iter: 776 loss: 7.12218537e-07
Iter: 777 loss: 7.12206088e-07
Iter: 778 loss: 7.12104224e-07
Iter: 779 loss: 7.1208342e-07
Iter: 780 loss: 7.12067731e-07
Iter: 781 loss: 7.11951486e-07
Iter: 782 loss: 7.11900782e-07
Iter: 783 loss: 7.11831547e-07
Iter: 784 loss: 7.11730479e-07
Iter: 785 loss: 7.1318857e-07
Iter: 786 loss: 7.11718656e-07
Iter: 787 loss: 7.1161196e-07
Iter: 788 loss: 7.11460302e-07
Iter: 789 loss: 7.11415225e-07
Iter: 790 loss: 7.11293922e-07
Iter: 791 loss: 7.12943e-07
Iter: 792 loss: 7.11294774e-07
Iter: 793 loss: 7.11198481e-07
Iter: 794 loss: 7.11333541e-07
Iter: 795 loss: 7.11108328e-07
Iter: 796 loss: 7.11016924e-07
Iter: 797 loss: 7.1118842e-07
Iter: 798 loss: 7.10988388e-07
Iter: 799 loss: 7.10858899e-07
Iter: 800 loss: 7.11884866e-07
Iter: 801 loss: 7.1086788e-07
Iter: 802 loss: 7.1078091e-07
Iter: 803 loss: 7.10877657e-07
Iter: 804 loss: 7.10743507e-07
Iter: 805 loss: 7.10635675e-07
Iter: 806 loss: 7.10951781e-07
Iter: 807 loss: 7.10631639e-07
Iter: 808 loss: 7.10539211e-07
Iter: 809 loss: 7.11599625e-07
Iter: 810 loss: 7.10554389e-07
Iter: 811 loss: 7.10506129e-07
Iter: 812 loss: 7.10450706e-07
Iter: 813 loss: 7.10441554e-07
Iter: 814 loss: 7.10370102e-07
Iter: 815 loss: 7.10245217e-07
Iter: 816 loss: 7.10265454e-07
Iter: 817 loss: 7.10139375e-07
Iter: 818 loss: 7.10833092e-07
Iter: 819 loss: 7.10111635e-07
Iter: 820 loss: 7.10021141e-07
Iter: 821 loss: 7.09837252e-07
Iter: 822 loss: 7.09840606e-07
Iter: 823 loss: 7.09739425e-07
Iter: 824 loss: 7.09736696e-07
Iter: 825 loss: 7.09660242e-07
Iter: 826 loss: 7.0952234e-07
Iter: 827 loss: 7.09523704e-07
Iter: 828 loss: 7.09354595e-07
Iter: 829 loss: 7.11004247e-07
Iter: 830 loss: 7.09379435e-07
Iter: 831 loss: 7.09274786e-07
Iter: 832 loss: 7.09357778e-07
Iter: 833 loss: 7.09210099e-07
Iter: 834 loss: 7.09123526e-07
Iter: 835 loss: 7.09366191e-07
Iter: 836 loss: 7.09125231e-07
Iter: 837 loss: 7.0903684e-07
Iter: 838 loss: 7.0910113e-07
Iter: 839 loss: 7.0893816e-07
Iter: 840 loss: 7.08866878e-07
Iter: 841 loss: 7.09737151e-07
Iter: 842 loss: 7.08844368e-07
Iter: 843 loss: 7.08767288e-07
Iter: 844 loss: 7.08776724e-07
Iter: 845 loss: 7.08757966e-07
Iter: 846 loss: 7.08658035e-07
Iter: 847 loss: 7.09884e-07
Iter: 848 loss: 7.08664402e-07
Iter: 849 loss: 7.08573054e-07
Iter: 850 loss: 7.08462835e-07
Iter: 851 loss: 7.08456696e-07
Iter: 852 loss: 7.08368646e-07
Iter: 853 loss: 7.08367565e-07
Iter: 854 loss: 7.08290372e-07
Iter: 855 loss: 7.08165487e-07
Iter: 856 loss: 7.11090138e-07
Iter: 857 loss: 7.08138202e-07
Iter: 858 loss: 7.08015079e-07
Iter: 859 loss: 7.09113237e-07
Iter: 860 loss: 7.0801326e-07
Iter: 861 loss: 7.0791566e-07
Iter: 862 loss: 7.07817094e-07
Iter: 863 loss: 7.07827e-07
Iter: 864 loss: 7.07692607e-07
Iter: 865 loss: 7.08646667e-07
Iter: 866 loss: 7.07697382e-07
Iter: 867 loss: 7.07579147e-07
Iter: 868 loss: 7.07611207e-07
Iter: 869 loss: 7.0749968e-07
Iter: 870 loss: 7.07421464e-07
Iter: 871 loss: 7.07542767e-07
Iter: 872 loss: 7.07379286e-07
Iter: 873 loss: 7.07297374e-07
Iter: 874 loss: 7.07865809e-07
Iter: 875 loss: 7.07302206e-07
Iter: 876 loss: 7.0724e-07
Iter: 877 loss: 7.0724866e-07
Iter: 878 loss: 7.07212e-07
Iter: 879 loss: 7.07183347e-07
Iter: 880 loss: 7.07767242e-07
Iter: 881 loss: 7.07175218e-07
Iter: 882 loss: 7.07087e-07
Iter: 883 loss: 7.07120194e-07
Iter: 884 loss: 7.07000368e-07
Iter: 885 loss: 7.06910782e-07
Iter: 886 loss: 7.07344157e-07
Iter: 887 loss: 7.06878836e-07
Iter: 888 loss: 7.06785784e-07
Iter: 889 loss: 7.06961544e-07
Iter: 890 loss: 7.06738035e-07
Iter: 891 loss: 7.06593369e-07
Iter: 892 loss: 7.07307436e-07
Iter: 893 loss: 7.06561309e-07
Iter: 894 loss: 7.06510832e-07
Iter: 895 loss: 7.06365654e-07
Iter: 896 loss: 7.09693325e-07
Iter: 897 loss: 7.06373385e-07
Iter: 898 loss: 7.06161188e-07
Iter: 899 loss: 7.08212383e-07
Iter: 900 loss: 7.06180401e-07
Iter: 901 loss: 7.06086155e-07
Iter: 902 loss: 7.061156e-07
Iter: 903 loss: 7.06042044e-07
Iter: 904 loss: 7.05903062e-07
Iter: 905 loss: 7.06360424e-07
Iter: 906 loss: 7.05858668e-07
Iter: 907 loss: 7.05778461e-07
Iter: 908 loss: 7.05988668e-07
Iter: 909 loss: 7.05728553e-07
Iter: 910 loss: 7.05694049e-07
Iter: 911 loss: 7.05666139e-07
Iter: 912 loss: 7.05629873e-07
Iter: 913 loss: 7.05573484e-07
Iter: 914 loss: 7.05537786e-07
Iter: 915 loss: 7.0550368e-07
Iter: 916 loss: 7.05457296e-07
Iter: 917 loss: 7.05443313e-07
Iter: 918 loss: 7.05309844e-07
Iter: 919 loss: 7.05429329e-07
Iter: 920 loss: 7.05240438e-07
Iter: 921 loss: 7.05117884e-07
Iter: 922 loss: 7.05359696e-07
Iter: 923 loss: 7.05095715e-07
Iter: 924 loss: 7.04964407e-07
Iter: 925 loss: 7.05489299e-07
Iter: 926 loss: 7.04948206e-07
Iter: 927 loss: 7.04825254e-07
Iter: 928 loss: 7.05353727e-07
Iter: 929 loss: 7.04845888e-07
Iter: 930 loss: 7.04785862e-07
Iter: 931 loss: 7.04631816e-07
Iter: 932 loss: 7.07474669e-07
Iter: 933 loss: 7.04633749e-07
Iter: 934 loss: 7.04498916e-07
Iter: 935 loss: 7.06436253e-07
Iter: 936 loss: 7.04514605e-07
Iter: 937 loss: 7.04489139e-07
Iter: 938 loss: 7.04499087e-07
Iter: 939 loss: 7.04409388e-07
Iter: 940 loss: 7.04333e-07
Iter: 941 loss: 7.04807405e-07
Iter: 942 loss: 7.04308377e-07
Iter: 943 loss: 7.04233685e-07
Iter: 944 loss: 7.04291779e-07
Iter: 945 loss: 7.04187073e-07
Iter: 946 loss: 7.04094418e-07
Iter: 947 loss: 7.0521e-07
Iter: 948 loss: 7.04076797e-07
Iter: 949 loss: 7.04087029e-07
Iter: 950 loss: 7.04070885e-07
Iter: 951 loss: 7.04064803e-07
Iter: 952 loss: 7.04068725e-07
Iter: 953 loss: 7.04073386e-07
Iter: 954 loss: 7.04079469e-07
Iter: 955 loss: 7.04067304e-07
Iter: 956 loss: 7.04089359e-07
Iter: 957 loss: 7.04054173e-07
Iter: 958 loss: 7.04076058e-07
Iter: 959 loss: 7.04093509e-07
Iter: 960 loss: 7.04095953e-07
Iter: 961 loss: 7.04087938e-07
Iter: 962 loss: 7.0407782e-07
Iter: 963 loss: 7.04073727e-07
Iter: 964 loss: 7.04078218e-07
Iter: 965 loss: 7.04073898e-07
Iter: 966 loss: 7.04075546e-07
Iter: 967 loss: 7.04076797e-07
Iter: 968 loss: 7.04078616e-07
Iter: 969 loss: 7.04077081e-07
Iter: 970 loss: 7.04077934e-07
Iter: 971 loss: 7.04077e-07
Iter: 972 loss: 7.04077877e-07
Iter: 973 loss: 7.04077877e-07
Iter: 974 loss: 7.04077e-07
Iter: 975 loss: 7.03937701e-07
Iter: 976 loss: 7.04780746e-07
Iter: 977 loss: 7.03924968e-07
Iter: 978 loss: 7.03871649e-07
Iter: 979 loss: 7.03980731e-07
Iter: 980 loss: 7.03826117e-07
Iter: 981 loss: 7.03717433e-07
Iter: 982 loss: 7.04303261e-07
Iter: 983 loss: 7.03689125e-07
Iter: 984 loss: 7.03650187e-07
Iter: 985 loss: 7.03576234e-07
Iter: 986 loss: 7.03585272e-07
Iter: 987 loss: 7.03435035e-07
Iter: 988 loss: 7.03884609e-07
Iter: 989 loss: 7.03410478e-07
Iter: 990 loss: 7.0335642e-07
Iter: 991 loss: 7.03336468e-07
Iter: 992 loss: 7.03313219e-07
Iter: 993 loss: 7.03191859e-07
Iter: 994 loss: 7.03349428e-07
Iter: 995 loss: 7.03105229e-07
Iter: 996 loss: 7.02992111e-07
Iter: 997 loss: 7.03543265e-07
Iter: 998 loss: 7.02987961e-07
Iter: 999 loss: 7.029127e-07
Iter: 1000 loss: 7.02835337e-07
Iter: 1001 loss: 7.02820444e-07
Iter: 1002 loss: 7.02698912e-07
Iter: 1003 loss: 7.03738863e-07
Iter: 1004 loss: 7.02701868e-07
Iter: 1005 loss: 7.02654233e-07
Iter: 1006 loss: 7.02633201e-07
Iter: 1007 loss: 7.02584316e-07
Iter: 1008 loss: 7.02519e-07
Iter: 1009 loss: 7.02516786e-07
Iter: 1010 loss: 7.02455623e-07
Iter: 1011 loss: 7.02395141e-07
Iter: 1012 loss: 7.02369618e-07
Iter: 1013 loss: 7.02254511e-07
Iter: 1014 loss: 7.02428224e-07
Iter: 1015 loss: 7.02189766e-07
Iter: 1016 loss: 7.02127181e-07
Iter: 1017 loss: 7.02113084e-07
Iter: 1018 loss: 7.02042939e-07
Iter: 1019 loss: 7.01962392e-07
Iter: 1020 loss: 7.01972795e-07
Iter: 1021 loss: 7.01865e-07
Iter: 1022 loss: 7.02678676e-07
Iter: 1023 loss: 7.01891395e-07
Iter: 1024 loss: 7.01837e-07
Iter: 1025 loss: 7.01933686e-07
Iter: 1026 loss: 7.01794477e-07
Iter: 1027 loss: 7.01703812e-07
Iter: 1028 loss: 7.01614397e-07
Iter: 1029 loss: 7.01604222e-07
Iter: 1030 loss: 7.01542e-07
Iter: 1031 loss: 7.0234131e-07
Iter: 1032 loss: 7.01557838e-07
Iter: 1033 loss: 7.01423744e-07
Iter: 1034 loss: 7.01437784e-07
Iter: 1035 loss: 7.01371846e-07
Iter: 1036 loss: 7.01297949e-07
Iter: 1037 loss: 7.01870135e-07
Iter: 1038 loss: 7.01270892e-07
Iter: 1039 loss: 7.01197791e-07
Iter: 1040 loss: 7.0123042e-07
Iter: 1041 loss: 7.01167551e-07
Iter: 1042 loss: 7.01176191e-07
Iter: 1043 loss: 7.01129466e-07
Iter: 1044 loss: 7.01104398e-07
Iter: 1045 loss: 7.0102e-07
Iter: 1046 loss: 7.01418116e-07
Iter: 1047 loss: 7.01022373e-07
Iter: 1048 loss: 7.00942e-07
Iter: 1049 loss: 7.01115368e-07
Iter: 1050 loss: 7.00878331e-07
Iter: 1051 loss: 7.00824785e-07
Iter: 1052 loss: 7.01137765e-07
Iter: 1053 loss: 7.00790451e-07
Iter: 1054 loss: 7.00750775e-07
Iter: 1055 loss: 7.00913802e-07
Iter: 1056 loss: 7.00738951e-07
Iter: 1057 loss: 7.00695637e-07
Iter: 1058 loss: 7.00821943e-07
Iter: 1059 loss: 7.00655278e-07
Iter: 1060 loss: 7.00618102e-07
Iter: 1061 loss: 7.00815576e-07
Iter: 1062 loss: 7.00578084e-07
Iter: 1063 loss: 7.0053e-07
Iter: 1064 loss: 7.00529256e-07
Iter: 1065 loss: 7.0047281e-07
Iter: 1066 loss: 7.0041574e-07
Iter: 1067 loss: 7.00337e-07
Iter: 1068 loss: 7.00346845e-07
Iter: 1069 loss: 7.00289661e-07
Iter: 1070 loss: 7.00260443e-07
Iter: 1071 loss: 7.0024555e-07
Iter: 1072 loss: 7.00114e-07
Iter: 1073 loss: 7.00158694e-07
Iter: 1074 loss: 7.00079056e-07
Iter: 1075 loss: 7.00077521e-07
Iter: 1076 loss: 7.00003284e-07
Iter: 1077 loss: 7.00434668e-07
Iter: 1078 loss: 7.00045234e-07
Iter: 1079 loss: 7.00029432e-07
Iter: 1080 loss: 6.99905911e-07
Iter: 1081 loss: 7.00612077e-07
Iter: 1082 loss: 6.99886868e-07
Iter: 1083 loss: 6.99802854e-07
Iter: 1084 loss: 7.00505325e-07
Iter: 1085 loss: 6.99810755e-07
Iter: 1086 loss: 6.99774432e-07
Iter: 1087 loss: 6.99683085e-07
Iter: 1088 loss: 6.99651082e-07
Iter: 1089 loss: 6.99585826e-07
Iter: 1090 loss: 6.99561951e-07
Iter: 1091 loss: 6.99545126e-07
Iter: 1092 loss: 6.9970082e-07
Iter: 1093 loss: 6.99532507e-07
Iter: 1094 loss: 6.99468501e-07
Iter: 1095 loss: 6.99493285e-07
Iter: 1096 loss: 6.99432e-07
Iter: 1097 loss: 6.994e-07
Iter: 1098 loss: 6.9946725e-07
Iter: 1099 loss: 6.99357543e-07
Iter: 1100 loss: 6.99317866e-07
Iter: 1101 loss: 6.99467932e-07
Iter: 1102 loss: 6.9927637e-07
Iter: 1103 loss: 6.99217765e-07
Iter: 1104 loss: 6.99329576e-07
Iter: 1105 loss: 6.99184e-07
Iter: 1106 loss: 6.991296e-07
Iter: 1107 loss: 6.9901057e-07
Iter: 1108 loss: 6.99009888e-07
Iter: 1109 loss: 6.99075315e-07
Iter: 1110 loss: 6.98955319e-07
Iter: 1111 loss: 6.98955773e-07
Iter: 1112 loss: 6.98883241e-07
Iter: 1113 loss: 6.98908e-07
Iter: 1114 loss: 6.98825318e-07
Iter: 1115 loss: 6.98702593e-07
Iter: 1116 loss: 6.98684573e-07
Iter: 1117 loss: 6.98620227e-07
Iter: 1118 loss: 6.98599592e-07
Iter: 1119 loss: 6.98529902e-07
Iter: 1120 loss: 6.98451e-07
Iter: 1121 loss: 6.98480903e-07
Iter: 1122 loss: 6.98359884e-07
Iter: 1123 loss: 6.98384383e-07
Iter: 1124 loss: 6.98319e-07
Iter: 1125 loss: 6.98293206e-07
Iter: 1126 loss: 6.9825262e-07
Iter: 1127 loss: 6.98134841e-07
Iter: 1128 loss: 6.98321401e-07
Iter: 1129 loss: 6.98099e-07
Iter: 1130 loss: 6.98054464e-07
Iter: 1131 loss: 6.98294286e-07
Iter: 1132 loss: 6.98020813e-07
Iter: 1133 loss: 6.97990572e-07
Iter: 1134 loss: 6.97834935e-07
Iter: 1135 loss: 7.00560463e-07
Iter: 1136 loss: 6.97831069e-07
Iter: 1137 loss: 6.97735459e-07
Iter: 1138 loss: 6.98821736e-07
Iter: 1139 loss: 6.97710675e-07
Iter: 1140 loss: 6.97628138e-07
Iter: 1141 loss: 6.97524683e-07
Iter: 1142 loss: 6.97488701e-07
Iter: 1143 loss: 6.97443966e-07
Iter: 1144 loss: 6.97423161e-07
Iter: 1145 loss: 6.97365635e-07
Iter: 1146 loss: 6.97293899e-07
Iter: 1147 loss: 6.9730055e-07
Iter: 1148 loss: 6.97218866e-07
Iter: 1149 loss: 6.97066582e-07
Iter: 1150 loss: 6.97100404e-07
Iter: 1151 loss: 6.96989105e-07
Iter: 1152 loss: 6.97712153e-07
Iter: 1153 loss: 6.96975121e-07
Iter: 1154 loss: 6.9689e-07
Iter: 1155 loss: 6.96944653e-07
Iter: 1156 loss: 6.96871e-07
Iter: 1157 loss: 6.96764062e-07
Iter: 1158 loss: 6.96879226e-07
Iter: 1159 loss: 6.96733878e-07
Iter: 1160 loss: 6.9664361e-07
Iter: 1161 loss: 6.97034466e-07
Iter: 1162 loss: 6.96658958e-07
Iter: 1163 loss: 6.9661661e-07
Iter: 1164 loss: 6.96514348e-07
Iter: 1165 loss: 6.96479788e-07
Iter: 1166 loss: 6.96418624e-07
Iter: 1167 loss: 6.97430551e-07
Iter: 1168 loss: 6.96417942e-07
Iter: 1169 loss: 6.96365191e-07
Iter: 1170 loss: 6.96390146e-07
Iter: 1171 loss: 6.96320456e-07
Iter: 1172 loss: 6.96241443e-07
Iter: 1173 loss: 6.9648e-07
Iter: 1174 loss: 6.96221775e-07
Iter: 1175 loss: 6.96139239e-07
Iter: 1176 loss: 6.96295501e-07
Iter: 1177 loss: 6.96111158e-07
Iter: 1178 loss: 6.96033567e-07
Iter: 1179 loss: 6.96202846e-07
Iter: 1180 loss: 6.95983204e-07
Iter: 1181 loss: 6.95921244e-07
Iter: 1182 loss: 6.95949211e-07
Iter: 1183 loss: 6.95877134e-07
Iter: 1184 loss: 6.95788117e-07
Iter: 1185 loss: 6.95679432e-07
Iter: 1186 loss: 6.95686481e-07
Iter: 1187 loss: 6.9561014e-07
Iter: 1188 loss: 6.95587e-07
Iter: 1189 loss: 6.95527262e-07
Iter: 1190 loss: 6.95497306e-07
Iter: 1191 loss: 6.95473773e-07
Iter: 1192 loss: 6.95363838e-07
Iter: 1193 loss: 6.96005372e-07
Iter: 1194 loss: 6.95432675e-07
Iter: 1195 loss: 6.95328197e-07
Iter: 1196 loss: 6.95663e-07
Iter: 1197 loss: 6.95307108e-07
Iter: 1198 loss: 6.95269307e-07
Iter: 1199 loss: 6.95138226e-07
Iter: 1200 loss: 6.96742063e-07
Iter: 1201 loss: 6.95117819e-07
Iter: 1202 loss: 6.95023175e-07
Iter: 1203 loss: 6.95004701e-07
Iter: 1204 loss: 6.94952632e-07
Iter: 1205 loss: 6.94946777e-07
Iter: 1206 loss: 6.94867822e-07
Iter: 1207 loss: 6.9477295e-07
Iter: 1208 loss: 6.95286644e-07
Iter: 1209 loss: 6.94788e-07
Iter: 1210 loss: 6.94754874e-07
Iter: 1211 loss: 6.94750213e-07
Iter: 1212 loss: 6.94699395e-07
Iter: 1213 loss: 6.94621633e-07
Iter: 1214 loss: 6.95154824e-07
Iter: 1215 loss: 6.94569621e-07
Iter: 1216 loss: 6.94455139e-07
Iter: 1217 loss: 6.94741345e-07
Iter: 1218 loss: 6.94451e-07
Iter: 1219 loss: 6.94346681e-07
Iter: 1220 loss: 6.94589403e-07
Iter: 1221 loss: 6.94309733e-07
Iter: 1222 loss: 6.94196274e-07
Iter: 1223 loss: 6.94689504e-07
Iter: 1224 loss: 6.94203436e-07
Iter: 1225 loss: 6.94150913e-07
Iter: 1226 loss: 6.94681262e-07
Iter: 1227 loss: 6.94147673e-07
Iter: 1228 loss: 6.94067e-07
Iter: 1229 loss: 6.94069513e-07
Iter: 1230 loss: 6.94011874e-07
Iter: 1231 loss: 6.93946674e-07
Iter: 1232 loss: 6.94075197e-07
Iter: 1233 loss: 6.93905577e-07
Iter: 1234 loss: 6.9383043e-07
Iter: 1235 loss: 6.93867889e-07
Iter: 1236 loss: 6.93769039e-07
Iter: 1237 loss: 6.9363432e-07
Iter: 1238 loss: 6.94362598e-07
Iter: 1239 loss: 6.93622951e-07
Iter: 1240 loss: 6.93574407e-07
Iter: 1241 loss: 6.93550305e-07
Iter: 1242 loss: 6.93535185e-07
Iter: 1243 loss: 6.9363864e-07
Iter: 1244 loss: 6.93564516e-07
Iter: 1245 loss: 6.93552579e-07
Iter: 1246 loss: 6.93521e-07
Iter: 1247 loss: 6.93537629e-07
Iter: 1248 loss: 6.93545587e-07
Iter: 1249 loss: 6.93547918e-07
Iter: 1250 loss: 6.9356031e-07
Iter: 1251 loss: 6.93559457e-07
Iter: 1252 loss: 6.93557695e-07
Iter: 1253 loss: 6.93548e-07
Iter: 1254 loss: 6.9355815e-07
Iter: 1255 loss: 6.93543484e-07
Iter: 1256 loss: 6.93544166e-07
Iter: 1257 loss: 6.93547861e-07
Iter: 1258 loss: 6.93549055e-07
Iter: 1259 loss: 6.93550419e-07
Iter: 1260 loss: 6.93553943e-07
Iter: 1261 loss: 6.93554341e-07
Iter: 1262 loss: 6.93552693e-07
Iter: 1263 loss: 6.93551328e-07
Iter: 1264 loss: 6.93551613e-07
Iter: 1265 loss: 6.93551556e-07
Iter: 1266 loss: 6.93550419e-07
Iter: 1267 loss: 6.93551328e-07
Iter: 1268 loss: 6.93550419e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/300_300_300_1
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0
+ date
Mon Oct 26 10:46:31 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 0 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3bb1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3aed0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3b24ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3c0d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3b3d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3b40a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3ac7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3ab1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3a8c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3a78488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3a55ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3a508c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d39ea400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3a01e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d39ca8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d39cf378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d39ddd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d399d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d392c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3945c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3907730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d39191e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d38bfbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d38746a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3888158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3868b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d383c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d38100d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d3803ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d37ba598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d37bf048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d376ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d379c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d372bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d37499d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f68d36f8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.4848764e-06
Iter: 2 loss: 1.88363197e-06
Iter: 3 loss: 1.31961269e-06
Iter: 4 loss: 1.11532245e-06
Iter: 5 loss: 9.49511048e-07
Iter: 6 loss: 8.88852185e-07
Iter: 7 loss: 7.763648e-07
Iter: 8 loss: 1.16569163e-06
Iter: 9 loss: 7.46766148e-07
Iter: 10 loss: 6.40834969e-07
Iter: 11 loss: 1.87096578e-06
Iter: 12 loss: 6.39234372e-07
Iter: 13 loss: 6.18833383e-07
Iter: 14 loss: 6.43830163e-07
Iter: 15 loss: 6.08197183e-07
Iter: 16 loss: 6.03326328e-07
Iter: 17 loss: 6.02580485e-07
Iter: 18 loss: 5.97388407e-07
Iter: 19 loss: 5.91934793e-07
Iter: 20 loss: 5.91029334e-07
Iter: 21 loss: 5.8834587e-07
Iter: 22 loss: 5.86425472e-07
Iter: 23 loss: 5.85525754e-07
Iter: 24 loss: 5.80901656e-07
Iter: 25 loss: 6.38879101e-07
Iter: 26 loss: 5.80841572e-07
Iter: 27 loss: 5.77947446e-07
Iter: 28 loss: 5.75506704e-07
Iter: 29 loss: 5.74670082e-07
Iter: 30 loss: 5.71425858e-07
Iter: 31 loss: 5.70742429e-07
Iter: 32 loss: 5.68592554e-07
Iter: 33 loss: 5.64491302e-07
Iter: 34 loss: 5.565214e-07
Iter: 35 loss: 7.19891659e-07
Iter: 36 loss: 5.5647e-07
Iter: 37 loss: 5.51843414e-07
Iter: 38 loss: 5.73709372e-07
Iter: 39 loss: 5.51010771e-07
Iter: 40 loss: 5.47870172e-07
Iter: 41 loss: 5.66045287e-07
Iter: 42 loss: 5.47437651e-07
Iter: 43 loss: 5.46596766e-07
Iter: 44 loss: 5.46353e-07
Iter: 45 loss: 5.45152034e-07
Iter: 46 loss: 5.42289058e-07
Iter: 47 loss: 5.72897306e-07
Iter: 48 loss: 5.41970053e-07
Iter: 49 loss: 5.39633561e-07
Iter: 50 loss: 5.62572779e-07
Iter: 51 loss: 5.39552161e-07
Iter: 52 loss: 5.39029088e-07
Iter: 53 loss: 5.38801487e-07
Iter: 54 loss: 5.38178483e-07
Iter: 55 loss: 5.36282471e-07
Iter: 56 loss: 5.41526447e-07
Iter: 57 loss: 5.35273443e-07
Iter: 58 loss: 5.34338767e-07
Iter: 59 loss: 5.34289029e-07
Iter: 60 loss: 5.33549496e-07
Iter: 61 loss: 5.42056341e-07
Iter: 62 loss: 5.33521757e-07
Iter: 63 loss: 5.32925867e-07
Iter: 64 loss: 5.3144953e-07
Iter: 65 loss: 5.46774857e-07
Iter: 66 loss: 5.31279397e-07
Iter: 67 loss: 5.30911564e-07
Iter: 68 loss: 5.30413104e-07
Iter: 69 loss: 5.29818521e-07
Iter: 70 loss: 5.28025907e-07
Iter: 71 loss: 5.29937438e-07
Iter: 72 loss: 5.26598228e-07
Iter: 73 loss: 5.24866039e-07
Iter: 74 loss: 5.24847849e-07
Iter: 75 loss: 5.240056e-07
Iter: 76 loss: 5.24415384e-07
Iter: 77 loss: 5.23446488e-07
Iter: 78 loss: 5.23346e-07
Iter: 79 loss: 5.22934954e-07
Iter: 80 loss: 5.22745381e-07
Iter: 81 loss: 5.22488e-07
Iter: 82 loss: 5.22468213e-07
Iter: 83 loss: 5.22121297e-07
Iter: 84 loss: 5.23683866e-07
Iter: 85 loss: 5.22056e-07
Iter: 86 loss: 5.21651714e-07
Iter: 87 loss: 5.21295078e-07
Iter: 88 loss: 5.21194579e-07
Iter: 89 loss: 5.20663946e-07
Iter: 90 loss: 5.21740844e-07
Iter: 91 loss: 5.20468348e-07
Iter: 92 loss: 5.20156391e-07
Iter: 93 loss: 5.19810669e-07
Iter: 94 loss: 5.19765706e-07
Iter: 95 loss: 5.19182322e-07
Iter: 96 loss: 5.22790856e-07
Iter: 97 loss: 5.19114224e-07
Iter: 98 loss: 5.18862294e-07
Iter: 99 loss: 5.2216933e-07
Iter: 100 loss: 5.18850698e-07
Iter: 101 loss: 5.18553065e-07
Iter: 102 loss: 5.17905391e-07
Iter: 103 loss: 5.2927146e-07
Iter: 104 loss: 5.17904425e-07
Iter: 105 loss: 5.17276249e-07
Iter: 106 loss: 5.18112302e-07
Iter: 107 loss: 5.16956561e-07
Iter: 108 loss: 5.16174623e-07
Iter: 109 loss: 5.153193e-07
Iter: 110 loss: 5.15204363e-07
Iter: 111 loss: 5.16018588e-07
Iter: 112 loss: 5.14884732e-07
Iter: 113 loss: 5.14542478e-07
Iter: 114 loss: 5.14278e-07
Iter: 115 loss: 5.14175099e-07
Iter: 116 loss: 5.13937096e-07
Iter: 117 loss: 5.13946134e-07
Iter: 118 loss: 5.13764917e-07
Iter: 119 loss: 5.14186809e-07
Iter: 120 loss: 5.13695454e-07
Iter: 121 loss: 5.1359109e-07
Iter: 122 loss: 5.13901114e-07
Iter: 123 loss: 5.1355164e-07
Iter: 124 loss: 5.13407088e-07
Iter: 125 loss: 5.13214218e-07
Iter: 126 loss: 5.13185341e-07
Iter: 127 loss: 5.12927045e-07
Iter: 128 loss: 5.14097906e-07
Iter: 129 loss: 5.12866905e-07
Iter: 130 loss: 5.12666588e-07
Iter: 131 loss: 5.13795158e-07
Iter: 132 loss: 5.12637712e-07
Iter: 133 loss: 5.12369638e-07
Iter: 134 loss: 5.12505437e-07
Iter: 135 loss: 5.12166196e-07
Iter: 136 loss: 5.1191887e-07
Iter: 137 loss: 5.11866745e-07
Iter: 138 loss: 5.11710539e-07
Iter: 139 loss: 5.11400629e-07
Iter: 140 loss: 5.11297401e-07
Iter: 141 loss: 5.11115445e-07
Iter: 142 loss: 5.10834468e-07
Iter: 143 loss: 5.14572321e-07
Iter: 144 loss: 5.1084055e-07
Iter: 145 loss: 5.10532914e-07
Iter: 146 loss: 5.11708549e-07
Iter: 147 loss: 5.10479424e-07
Iter: 148 loss: 5.10217774e-07
Iter: 149 loss: 5.101341e-07
Iter: 150 loss: 5.0998392e-07
Iter: 151 loss: 5.09647521e-07
Iter: 152 loss: 5.14505189e-07
Iter: 153 loss: 5.09659856e-07
Iter: 154 loss: 5.09487847e-07
Iter: 155 loss: 5.09241204e-07
Iter: 156 loss: 5.09242568e-07
Iter: 157 loss: 5.08893891e-07
Iter: 158 loss: 5.12275847e-07
Iter: 159 loss: 5.08864389e-07
Iter: 160 loss: 5.08753715e-07
Iter: 161 loss: 5.0895585e-07
Iter: 162 loss: 5.08702e-07
Iter: 163 loss: 5.08617404e-07
Iter: 164 loss: 5.0880783e-07
Iter: 165 loss: 5.08573862e-07
Iter: 166 loss: 5.08484334e-07
Iter: 167 loss: 5.09568508e-07
Iter: 168 loss: 5.08471942e-07
Iter: 169 loss: 5.08404355e-07
Iter: 170 loss: 5.0831062e-07
Iter: 171 loss: 5.08314542e-07
Iter: 172 loss: 5.08186588e-07
Iter: 173 loss: 5.07754862e-07
Iter: 174 loss: 5.09756887e-07
Iter: 175 loss: 5.07612469e-07
Iter: 176 loss: 5.07096047e-07
Iter: 177 loss: 5.14407191e-07
Iter: 178 loss: 5.07103948e-07
Iter: 179 loss: 5.06777553e-07
Iter: 180 loss: 5.06777553e-07
Iter: 181 loss: 5.0650749e-07
Iter: 182 loss: 5.06164156e-07
Iter: 183 loss: 5.06149604e-07
Iter: 184 loss: 5.06109643e-07
Iter: 185 loss: 5.06012441e-07
Iter: 186 loss: 5.05898299e-07
Iter: 187 loss: 5.05716e-07
Iter: 188 loss: 5.08765709e-07
Iter: 189 loss: 5.05716685e-07
Iter: 190 loss: 5.05651542e-07
Iter: 191 loss: 5.05629373e-07
Iter: 192 loss: 5.05589071e-07
Iter: 193 loss: 5.0554172e-07
Iter: 194 loss: 5.0554911e-07
Iter: 195 loss: 5.05457592e-07
Iter: 196 loss: 5.05207197e-07
Iter: 197 loss: 5.06335709e-07
Iter: 198 loss: 5.05111075e-07
Iter: 199 loss: 5.04986474e-07
Iter: 200 loss: 5.04933382e-07
Iter: 201 loss: 5.04700949e-07
Iter: 202 loss: 5.06219408e-07
Iter: 203 loss: 5.0468276e-07
Iter: 204 loss: 5.04564696e-07
Iter: 205 loss: 5.04265245e-07
Iter: 206 loss: 5.09182087e-07
Iter: 207 loss: 5.04263085e-07
Iter: 208 loss: 5.04023e-07
Iter: 209 loss: 5.04166167e-07
Iter: 210 loss: 5.03881779e-07
Iter: 211 loss: 5.03571187e-07
Iter: 212 loss: 5.04914738e-07
Iter: 213 loss: 5.03520084e-07
Iter: 214 loss: 5.03236777e-07
Iter: 215 loss: 5.05191167e-07
Iter: 216 loss: 5.03219098e-07
Iter: 217 loss: 5.02960063e-07
Iter: 218 loss: 5.05228968e-07
Iter: 219 loss: 5.02934256e-07
Iter: 220 loss: 5.02804824e-07
Iter: 221 loss: 5.02631e-07
Iter: 222 loss: 5.02612863e-07
Iter: 223 loss: 5.02544196e-07
Iter: 224 loss: 5.02511739e-07
Iter: 225 loss: 5.02432954e-07
Iter: 226 loss: 5.02438638e-07
Iter: 227 loss: 5.02365424e-07
Iter: 228 loss: 5.02288344e-07
Iter: 229 loss: 5.02443243e-07
Iter: 230 loss: 5.02232353e-07
Iter: 231 loss: 5.02135549e-07
Iter: 232 loss: 5.03341482e-07
Iter: 233 loss: 5.02143394e-07
Iter: 234 loss: 5.02116791e-07
Iter: 235 loss: 5.02061766e-07
Iter: 236 loss: 5.02067564e-07
Iter: 237 loss: 5.01990371e-07
Iter: 238 loss: 5.02521e-07
Iter: 239 loss: 5.01971783e-07
Iter: 240 loss: 5.01908119e-07
Iter: 241 loss: 5.01727357e-07
Iter: 242 loss: 5.03025376e-07
Iter: 243 loss: 5.01699617e-07
Iter: 244 loss: 5.01454849e-07
Iter: 245 loss: 5.01580871e-07
Iter: 246 loss: 5.01306829e-07
Iter: 247 loss: 5.01024715e-07
Iter: 248 loss: 5.02308808e-07
Iter: 249 loss: 5.00970941e-07
Iter: 250 loss: 5.01059503e-07
Iter: 251 loss: 5.00926376e-07
Iter: 252 loss: 5.00854e-07
Iter: 253 loss: 5.00730948e-07
Iter: 254 loss: 5.02142484e-07
Iter: 255 loss: 5.00717647e-07
Iter: 256 loss: 5.00583042e-07
Iter: 257 loss: 5.01280056e-07
Iter: 258 loss: 5.00556553e-07
Iter: 259 loss: 5.00430531e-07
Iter: 260 loss: 5.01779539e-07
Iter: 261 loss: 5.00413307e-07
Iter: 262 loss: 5.00350495e-07
Iter: 263 loss: 5.00287683e-07
Iter: 264 loss: 5.00256249e-07
Iter: 265 loss: 5.00168426e-07
Iter: 266 loss: 5.00168312e-07
Iter: 267 loss: 5.00099759e-07
Iter: 268 loss: 4.99984e-07
Iter: 269 loss: 5.02229909e-07
Iter: 270 loss: 4.99997e-07
Iter: 271 loss: 4.99949e-07
Iter: 272 loss: 4.99944633e-07
Iter: 273 loss: 4.99908083e-07
Iter: 274 loss: 4.99872e-07
Iter: 275 loss: 4.9986636e-07
Iter: 276 loss: 4.9981918e-07
Iter: 277 loss: 4.99717373e-07
Iter: 278 loss: 5.01814611e-07
Iter: 279 loss: 4.99712087e-07
Iter: 280 loss: 4.99568444e-07
Iter: 281 loss: 4.99653197e-07
Iter: 282 loss: 4.99465386e-07
Iter: 283 loss: 4.9924688e-07
Iter: 284 loss: 4.99551788e-07
Iter: 285 loss: 4.99139162e-07
Iter: 286 loss: 4.99009275e-07
Iter: 287 loss: 4.98987561e-07
Iter: 288 loss: 4.98837665e-07
Iter: 289 loss: 4.99450039e-07
Iter: 290 loss: 4.98812369e-07
Iter: 291 loss: 4.98704821e-07
Iter: 292 loss: 4.98671284e-07
Iter: 293 loss: 4.98652867e-07
Iter: 294 loss: 4.98602162e-07
Iter: 295 loss: 4.98588633e-07
Iter: 296 loss: 4.9856169e-07
Iter: 297 loss: 4.98518261e-07
Iter: 298 loss: 4.98503312e-07
Iter: 299 loss: 4.9842123e-07
Iter: 300 loss: 4.98632744e-07
Iter: 301 loss: 4.98420832e-07
Iter: 302 loss: 4.98332213e-07
Iter: 303 loss: 4.98267752e-07
Iter: 304 loss: 4.98267525e-07
Iter: 305 loss: 4.9817254e-07
Iter: 306 loss: 4.99679516e-07
Iter: 307 loss: 4.98173e-07
Iter: 308 loss: 4.98134796e-07
Iter: 309 loss: 4.98038162e-07
Iter: 310 loss: 5.00009151e-07
Iter: 311 loss: 4.98051804e-07
Iter: 312 loss: 4.97888323e-07
Iter: 313 loss: 4.97925384e-07
Iter: 314 loss: 4.97834662e-07
Iter: 315 loss: 4.97683288e-07
Iter: 316 loss: 4.98220174e-07
Iter: 317 loss: 4.976431e-07
Iter: 318 loss: 4.97516453e-07
Iter: 319 loss: 4.98205395e-07
Iter: 320 loss: 4.97511053e-07
Iter: 321 loss: 4.97390602e-07
Iter: 322 loss: 4.9843544e-07
Iter: 323 loss: 4.97395149e-07
Iter: 324 loss: 4.97290841e-07
Iter: 325 loss: 4.97223652e-07
Iter: 326 loss: 4.97185567e-07
Iter: 327 loss: 4.97103883e-07
Iter: 328 loss: 4.97108e-07
Iter: 329 loss: 4.97023052e-07
Iter: 330 loss: 4.96885832e-07
Iter: 331 loss: 4.9687003e-07
Iter: 332 loss: 4.96826146e-07
Iter: 333 loss: 4.96801761e-07
Iter: 334 loss: 4.96768e-07
Iter: 335 loss: 4.96757423e-07
Iter: 336 loss: 4.96712232e-07
Iter: 337 loss: 4.96690348e-07
Iter: 338 loss: 4.97218764e-07
Iter: 339 loss: 4.96695066e-07
Iter: 340 loss: 4.96645498e-07
Iter: 341 loss: 4.96583539e-07
Iter: 342 loss: 4.97985241e-07
Iter: 343 loss: 4.96591952e-07
Iter: 344 loss: 4.9651419e-07
Iter: 345 loss: 4.96358325e-07
Iter: 346 loss: 4.9927786e-07
Iter: 347 loss: 4.96345592e-07
Iter: 348 loss: 4.96205e-07
Iter: 349 loss: 4.96157668e-07
Iter: 350 loss: 4.96040911e-07
Iter: 351 loss: 4.9588823e-07
Iter: 352 loss: 4.96568305e-07
Iter: 353 loss: 4.95828033e-07
Iter: 354 loss: 4.9568672e-07
Iter: 355 loss: 4.96191205e-07
Iter: 356 loss: 4.95654092e-07
Iter: 357 loss: 4.95494078e-07
Iter: 358 loss: 4.95495783e-07
Iter: 359 loss: 4.95453946e-07
Iter: 360 loss: 4.95376355e-07
Iter: 361 loss: 4.95379538e-07
Iter: 362 loss: 4.95295467e-07
Iter: 363 loss: 4.95298536e-07
Iter: 364 loss: 4.95249822e-07
Iter: 365 loss: 4.95166375e-07
Iter: 366 loss: 4.97058465e-07
Iter: 367 loss: 4.95169e-07
Iter: 368 loss: 4.9511209e-07
Iter: 369 loss: 4.95089409e-07
Iter: 370 loss: 4.95048255e-07
Iter: 371 loss: 4.94990672e-07
Iter: 372 loss: 4.94977144e-07
Iter: 373 loss: 4.94945652e-07
Iter: 374 loss: 4.94946562e-07
Iter: 375 loss: 4.94909159e-07
Iter: 376 loss: 4.94865617e-07
Iter: 377 loss: 4.94847484e-07
Iter: 378 loss: 4.9478524e-07
Iter: 379 loss: 4.94856863e-07
Iter: 380 loss: 4.9475e-07
Iter: 381 loss: 4.94671781e-07
Iter: 382 loss: 4.95330255e-07
Iter: 383 loss: 4.94666267e-07
Iter: 384 loss: 4.94607889e-07
Iter: 385 loss: 4.9458049e-07
Iter: 386 loss: 4.9453115e-07
Iter: 387 loss: 4.944078e-07
Iter: 388 loss: 4.95621123e-07
Iter: 389 loss: 4.94407345e-07
Iter: 390 loss: 4.94322762e-07
Iter: 391 loss: 4.94256653e-07
Iter: 392 loss: 4.94240567e-07
Iter: 393 loss: 4.94128e-07
Iter: 394 loss: 4.94139044e-07
Iter: 395 loss: 4.94055939e-07
Iter: 396 loss: 4.94075607e-07
Iter: 397 loss: 4.93992047e-07
Iter: 398 loss: 4.93959078e-07
Iter: 399 loss: 4.94571452e-07
Iter: 400 loss: 4.93965899e-07
Iter: 401 loss: 4.93920595e-07
Iter: 402 loss: 4.93967377e-07
Iter: 403 loss: 4.93897346e-07
Iter: 404 loss: 4.93870971e-07
Iter: 405 loss: 4.93911102e-07
Iter: 406 loss: 4.93869379e-07
Iter: 407 loss: 4.93837376e-07
Iter: 408 loss: 4.93923665e-07
Iter: 409 loss: 4.93823e-07
Iter: 410 loss: 4.93781158e-07
Iter: 411 loss: 4.93725452e-07
Iter: 412 loss: 4.9397795e-07
Iter: 413 loss: 4.93691346e-07
Iter: 414 loss: 4.93514278e-07
Iter: 415 loss: 4.93367224e-07
Iter: 416 loss: 4.93322887e-07
Iter: 417 loss: 4.93116318e-07
Iter: 418 loss: 4.93124787e-07
Iter: 419 loss: 4.93017637e-07
Iter: 420 loss: 4.93686457e-07
Iter: 421 loss: 4.93006e-07
Iter: 422 loss: 4.9288542e-07
Iter: 423 loss: 4.93451807e-07
Iter: 424 loss: 4.92864103e-07
Iter: 425 loss: 4.92816866e-07
Iter: 426 loss: 4.92794925e-07
Iter: 427 loss: 4.92769232e-07
Iter: 428 loss: 4.92714889e-07
Iter: 429 loss: 4.93233813e-07
Iter: 430 loss: 4.92716e-07
Iter: 431 loss: 4.92648383e-07
Iter: 432 loss: 4.92567779e-07
Iter: 433 loss: 4.92576646e-07
Iter: 434 loss: 4.92493427e-07
Iter: 435 loss: 4.93210052e-07
Iter: 436 loss: 4.92504e-07
Iter: 437 loss: 4.92465347e-07
Iter: 438 loss: 4.93026334e-07
Iter: 439 loss: 4.92460231e-07
Iter: 440 loss: 4.92435e-07
Iter: 441 loss: 4.9238065e-07
Iter: 442 loss: 4.93432935e-07
Iter: 443 loss: 4.92363597e-07
Iter: 444 loss: 4.92357e-07
Iter: 445 loss: 4.92344952e-07
Iter: 446 loss: 4.92318236e-07
Iter: 447 loss: 4.92254571e-07
Iter: 448 loss: 4.92310448e-07
Iter: 449 loss: 4.9221353e-07
Iter: 450 loss: 4.92058803e-07
Iter: 451 loss: 4.92045729e-07
Iter: 452 loss: 4.91938636e-07
Iter: 453 loss: 4.91747528e-07
Iter: 454 loss: 4.92280037e-07
Iter: 455 loss: 4.91678861e-07
Iter: 456 loss: 4.91669425e-07
Iter: 457 loss: 4.9160019e-07
Iter: 458 loss: 4.91536298e-07
Iter: 459 loss: 4.91371e-07
Iter: 460 loss: 4.93318339e-07
Iter: 461 loss: 4.91372361e-07
Iter: 462 loss: 4.91342234e-07
Iter: 463 loss: 4.91306537e-07
Iter: 464 loss: 4.91263563e-07
Iter: 465 loss: 4.91421133e-07
Iter: 466 loss: 4.91270157e-07
Iter: 467 loss: 4.91238325e-07
Iter: 468 loss: 4.91198875e-07
Iter: 469 loss: 4.91193077e-07
Iter: 470 loss: 4.91158232e-07
Iter: 471 loss: 4.91102526e-07
Iter: 472 loss: 4.9108786e-07
Iter: 473 loss: 4.91009e-07
Iter: 474 loss: 4.91165679e-07
Iter: 475 loss: 4.90981e-07
Iter: 476 loss: 4.90872878e-07
Iter: 477 loss: 4.91047331e-07
Iter: 478 loss: 4.90825641e-07
Iter: 479 loss: 4.90830871e-07
Iter: 480 loss: 4.90768912e-07
Iter: 481 loss: 4.90741627e-07
Iter: 482 loss: 4.90652837e-07
Iter: 483 loss: 4.90714228e-07
Iter: 484 loss: 4.90570528e-07
Iter: 485 loss: 4.90466277e-07
Iter: 486 loss: 4.90465254e-07
Iter: 487 loss: 4.90399202e-07
Iter: 488 loss: 4.90333491e-07
Iter: 489 loss: 4.90327295e-07
Iter: 490 loss: 4.90245043e-07
Iter: 491 loss: 4.90231969e-07
Iter: 492 loss: 4.9017217e-07
Iter: 493 loss: 4.90167906e-07
Iter: 494 loss: 4.90135051e-07
Iter: 495 loss: 4.90043703e-07
Iter: 496 loss: 4.90523178e-07
Iter: 497 loss: 4.9005007e-07
Iter: 498 loss: 4.89979925e-07
Iter: 499 loss: 4.89927174e-07
Iter: 500 loss: 4.89926208e-07
Iter: 501 loss: 4.89866352e-07
Iter: 502 loss: 4.89879653e-07
Iter: 503 loss: 4.89830313e-07
Iter: 504 loss: 4.89909382e-07
Iter: 505 loss: 4.8981326e-07
Iter: 506 loss: 4.89767785e-07
Iter: 507 loss: 4.89738568e-07
Iter: 508 loss: 4.89718559e-07
Iter: 509 loss: 4.89688773e-07
Iter: 510 loss: 4.89699801e-07
Iter: 511 loss: 4.89666832e-07
Iter: 512 loss: 4.89546323e-07
Iter: 513 loss: 4.90370837e-07
Iter: 514 loss: 4.89538593e-07
Iter: 515 loss: 4.89434569e-07
Iter: 516 loss: 4.8960726e-07
Iter: 517 loss: 4.89401032e-07
Iter: 518 loss: 4.89251192e-07
Iter: 519 loss: 4.89271486e-07
Iter: 520 loss: 4.89174738e-07
Iter: 521 loss: 4.8905224e-07
Iter: 522 loss: 4.90099637e-07
Iter: 523 loss: 4.89065428e-07
Iter: 524 loss: 4.88989258e-07
Iter: 525 loss: 4.89190313e-07
Iter: 526 loss: 4.88950604e-07
Iter: 527 loss: 4.88908597e-07
Iter: 528 loss: 4.8890297e-07
Iter: 529 loss: 4.88840726e-07
Iter: 530 loss: 4.88726187e-07
Iter: 531 loss: 4.91281867e-07
Iter: 532 loss: 4.8873062e-07
Iter: 533 loss: 4.88665364e-07
Iter: 534 loss: 4.89212709e-07
Iter: 535 loss: 4.88655132e-07
Iter: 536 loss: 4.88632395e-07
Iter: 537 loss: 4.88623641e-07
Iter: 538 loss: 4.88579701e-07
Iter: 539 loss: 4.88522176e-07
Iter: 540 loss: 4.89439344e-07
Iter: 541 loss: 4.88523256e-07
Iter: 542 loss: 4.88474484e-07
Iter: 543 loss: 4.88796616e-07
Iter: 544 loss: 4.88476871e-07
Iter: 545 loss: 4.88408773e-07
Iter: 546 loss: 4.88565e-07
Iter: 547 loss: 4.88415139e-07
Iter: 548 loss: 4.88375179e-07
Iter: 549 loss: 4.88317141e-07
Iter: 550 loss: 4.88327601e-07
Iter: 551 loss: 4.8824171e-07
Iter: 552 loss: 4.8811205e-07
Iter: 553 loss: 4.88106e-07
Iter: 554 loss: 4.88243359e-07
Iter: 555 loss: 4.88063904e-07
Iter: 556 loss: 4.88037e-07
Iter: 557 loss: 4.87954253e-07
Iter: 558 loss: 4.8963426e-07
Iter: 559 loss: 4.87950842e-07
Iter: 560 loss: 4.87882801e-07
Iter: 561 loss: 4.88030423e-07
Iter: 562 loss: 4.87855743e-07
Iter: 563 loss: 4.87798218e-07
Iter: 564 loss: 4.88056287e-07
Iter: 565 loss: 4.87797649e-07
Iter: 566 loss: 4.87719717e-07
Iter: 567 loss: 4.88254216e-07
Iter: 568 loss: 4.87699708e-07
Iter: 569 loss: 4.87686123e-07
Iter: 570 loss: 4.87648435e-07
Iter: 571 loss: 4.87644229e-07
Iter: 572 loss: 4.8761251e-07
Iter: 573 loss: 4.88048045e-07
Iter: 574 loss: 4.87617399e-07
Iter: 575 loss: 4.87576813e-07
Iter: 576 loss: 4.87511215e-07
Iter: 577 loss: 4.87531679e-07
Iter: 578 loss: 4.87500188e-07
Iter: 579 loss: 4.8749348e-07
Iter: 580 loss: 4.87462785e-07
Iter: 581 loss: 4.8742794e-07
Iter: 582 loss: 4.88238697e-07
Iter: 583 loss: 4.87411739e-07
Iter: 584 loss: 4.87369789e-07
Iter: 585 loss: 4.87423733e-07
Iter: 586 loss: 4.87328066e-07
Iter: 587 loss: 4.8726065e-07
Iter: 588 loss: 4.87347336e-07
Iter: 589 loss: 4.87235127e-07
Iter: 590 loss: 4.87117859e-07
Iter: 591 loss: 4.87378657e-07
Iter: 592 loss: 4.87073e-07
Iter: 593 loss: 4.87010482e-07
Iter: 594 loss: 4.86944714e-07
Iter: 595 loss: 4.86920726e-07
Iter: 596 loss: 4.86794136e-07
Iter: 597 loss: 4.86994679e-07
Iter: 598 loss: 4.8674616e-07
Iter: 599 loss: 4.86796e-07
Iter: 600 loss: 4.8669159e-07
Iter: 601 loss: 4.8668079e-07
Iter: 602 loss: 4.866572e-07
Iter: 603 loss: 4.87381328e-07
Iter: 604 loss: 4.86651174e-07
Iter: 605 loss: 4.86621559e-07
Iter: 606 loss: 4.86618774e-07
Iter: 607 loss: 4.86588704e-07
Iter: 608 loss: 4.86623378e-07
Iter: 609 loss: 4.86566e-07
Iter: 610 loss: 4.86559259e-07
Iter: 611 loss: 4.86595354e-07
Iter: 612 loss: 4.86555962e-07
Iter: 613 loss: 4.86534589e-07
Iter: 614 loss: 4.86494628e-07
Iter: 615 loss: 4.86487124e-07
Iter: 616 loss: 4.86433578e-07
Iter: 617 loss: 4.86404076e-07
Iter: 618 loss: 4.86373949e-07
Iter: 619 loss: 4.8629289e-07
Iter: 620 loss: 4.8636366e-07
Iter: 621 loss: 4.86233034e-07
Iter: 622 loss: 4.86137878e-07
Iter: 623 loss: 4.86128215e-07
Iter: 624 loss: 4.86116505e-07
Iter: 625 loss: 4.86068757e-07
Iter: 626 loss: 4.87483362e-07
Iter: 627 loss: 4.86060401e-07
Iter: 628 loss: 4.85995031e-07
Iter: 629 loss: 4.86059605e-07
Iter: 630 loss: 4.859358e-07
Iter: 631 loss: 4.86010663e-07
Iter: 632 loss: 4.85948249e-07
Iter: 633 loss: 4.85947567e-07
Iter: 634 loss: 4.85952228e-07
Iter: 635 loss: 4.85948533e-07
Iter: 636 loss: 4.8593779e-07
Iter: 637 loss: 4.85940745e-07
Iter: 638 loss: 4.8594336e-07
Iter: 639 loss: 4.85943929e-07
Iter: 640 loss: 4.85943474e-07
Iter: 641 loss: 4.85943076e-07
Iter: 642 loss: 4.85947282e-07
Iter: 643 loss: 4.85947226e-07
Iter: 644 loss: 4.8594103e-07
Iter: 645 loss: 4.85934606e-07
Iter: 646 loss: 4.85935061e-07
Iter: 647 loss: 4.85942792e-07
Iter: 648 loss: 4.85936653e-07
Iter: 649 loss: 4.85932446e-07
Iter: 650 loss: 4.85934095e-07
Iter: 651 loss: 4.85937505e-07
Iter: 652 loss: 4.85936766e-07
Iter: 653 loss: 4.85934606e-07
Iter: 654 loss: 4.85936255e-07
Iter: 655 loss: 4.85936141e-07
Iter: 656 loss: 4.85936425e-07
Iter: 657 loss: 4.85936141e-07
Iter: 658 loss: 4.85936141e-07
Iter: 659 loss: 4.85936425e-07
Iter: 660 loss: 4.85936141e-07
Iter: 661 loss: 4.85936425e-07
Iter: 662 loss: 4.85936425e-07
Iter: 663 loss: 4.85936141e-07
Iter: 664 loss: 4.85936141e-07
Iter: 665 loss: 4.85936141e-07
Iter: 666 loss: 4.85936141e-07
Iter: 667 loss: 4.85936141e-07
Iter: 668 loss: 4.85936425e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4
+ date
Mon Oct 26 10:59:01 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.4_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.4_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f350756b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35075849d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35076619d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3507616950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3507616598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3507661158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f350750e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35074d91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35074d9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35074d9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35074ae2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f350745b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f350745b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35074410d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3507445510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f350745b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35073f36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35073cb0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35073f39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3507323400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3507379a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35072d30d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35073799d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35072689d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35072688c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3507268bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3507268e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35071d69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35071d6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35071c8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35071932f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3507154f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3507154730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f35071bcd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34dfcc60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34dfc8ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.67651717e-06
Iter: 2 loss: 1.78359426e-06
Iter: 3 loss: 1.6743279e-06
Iter: 4 loss: 1.33885669e-06
Iter: 5 loss: 1.52145299e-06
Iter: 6 loss: 1.11818281e-06
Iter: 7 loss: 9.80873438e-07
Iter: 8 loss: 1.35722871e-06
Iter: 9 loss: 9.36167e-07
Iter: 10 loss: 8.27542465e-07
Iter: 11 loss: 8.27435315e-07
Iter: 12 loss: 7.97893733e-07
Iter: 13 loss: 7.93503432e-07
Iter: 14 loss: 7.72911e-07
Iter: 15 loss: 7.56262125e-07
Iter: 16 loss: 9.54748202e-07
Iter: 17 loss: 7.5605567e-07
Iter: 18 loss: 7.39274e-07
Iter: 19 loss: 7.75962064e-07
Iter: 20 loss: 7.32791875e-07
Iter: 21 loss: 7.24034408e-07
Iter: 22 loss: 7.30984709e-07
Iter: 23 loss: 7.1873967e-07
Iter: 24 loss: 7.07846766e-07
Iter: 25 loss: 8.19072739e-07
Iter: 26 loss: 7.07532536e-07
Iter: 27 loss: 6.99479301e-07
Iter: 28 loss: 6.79551476e-07
Iter: 29 loss: 8.74451e-07
Iter: 30 loss: 6.76923833e-07
Iter: 31 loss: 6.6023415e-07
Iter: 32 loss: 7.31311957e-07
Iter: 33 loss: 6.56739758e-07
Iter: 34 loss: 6.44288662e-07
Iter: 35 loss: 6.85433463e-07
Iter: 36 loss: 6.40863391e-07
Iter: 37 loss: 6.44103352e-07
Iter: 38 loss: 6.38835786e-07
Iter: 39 loss: 6.37064318e-07
Iter: 40 loss: 6.34314119e-07
Iter: 41 loss: 6.34266769e-07
Iter: 42 loss: 6.31091268e-07
Iter: 43 loss: 6.31742239e-07
Iter: 44 loss: 6.2874409e-07
Iter: 45 loss: 6.25991618e-07
Iter: 46 loss: 6.47278057e-07
Iter: 47 loss: 6.25792154e-07
Iter: 48 loss: 6.21937e-07
Iter: 49 loss: 6.25725306e-07
Iter: 50 loss: 6.19744242e-07
Iter: 51 loss: 6.16291572e-07
Iter: 52 loss: 6.14200303e-07
Iter: 53 loss: 6.12808662e-07
Iter: 54 loss: 6.11403323e-07
Iter: 55 loss: 6.10757525e-07
Iter: 56 loss: 6.08537562e-07
Iter: 57 loss: 6.03838259e-07
Iter: 58 loss: 6.81026677e-07
Iter: 59 loss: 6.03709339e-07
Iter: 60 loss: 6.01566853e-07
Iter: 61 loss: 6.01447312e-07
Iter: 62 loss: 5.9969409e-07
Iter: 63 loss: 6.06357219e-07
Iter: 64 loss: 5.99284135e-07
Iter: 65 loss: 5.98161e-07
Iter: 66 loss: 5.95283268e-07
Iter: 67 loss: 6.20264132e-07
Iter: 68 loss: 5.94794699e-07
Iter: 69 loss: 5.91668254e-07
Iter: 70 loss: 6.19377772e-07
Iter: 71 loss: 5.91530863e-07
Iter: 72 loss: 5.8937519e-07
Iter: 73 loss: 5.91728963e-07
Iter: 74 loss: 5.88155217e-07
Iter: 75 loss: 5.87249929e-07
Iter: 76 loss: 5.870059e-07
Iter: 77 loss: 5.86017507e-07
Iter: 78 loss: 5.89333467e-07
Iter: 79 loss: 5.85728344e-07
Iter: 80 loss: 5.8497119e-07
Iter: 81 loss: 5.83177268e-07
Iter: 82 loss: 6.04873435e-07
Iter: 83 loss: 5.83037775e-07
Iter: 84 loss: 5.82709617e-07
Iter: 85 loss: 5.81947234e-07
Iter: 86 loss: 5.8151619e-07
Iter: 87 loss: 5.80282801e-07
Iter: 88 loss: 5.85272801e-07
Iter: 89 loss: 5.79777463e-07
Iter: 90 loss: 5.78820959e-07
Iter: 91 loss: 5.78631671e-07
Iter: 92 loss: 5.77632761e-07
Iter: 93 loss: 5.76980597e-07
Iter: 94 loss: 5.76608898e-07
Iter: 95 loss: 5.75621e-07
Iter: 96 loss: 5.74983744e-07
Iter: 97 loss: 5.74592491e-07
Iter: 98 loss: 5.74410933e-07
Iter: 99 loss: 5.73832722e-07
Iter: 100 loss: 5.73507805e-07
Iter: 101 loss: 5.73076e-07
Iter: 102 loss: 5.73079092e-07
Iter: 103 loss: 5.72562726e-07
Iter: 104 loss: 5.71593773e-07
Iter: 105 loss: 5.92990375e-07
Iter: 106 loss: 5.71594683e-07
Iter: 107 loss: 5.70494421e-07
Iter: 108 loss: 5.81768802e-07
Iter: 109 loss: 5.70456052e-07
Iter: 110 loss: 5.69756253e-07
Iter: 111 loss: 5.74865e-07
Iter: 112 loss: 5.69697818e-07
Iter: 113 loss: 5.68658152e-07
Iter: 114 loss: 5.67347342e-07
Iter: 115 loss: 5.67233656e-07
Iter: 116 loss: 5.65847699e-07
Iter: 117 loss: 5.67852453e-07
Iter: 118 loss: 5.6515961e-07
Iter: 119 loss: 5.64938887e-07
Iter: 120 loss: 5.64581796e-07
Iter: 121 loss: 5.64054517e-07
Iter: 122 loss: 5.63040658e-07
Iter: 123 loss: 5.8361536e-07
Iter: 124 loss: 5.63046797e-07
Iter: 125 loss: 5.62923901e-07
Iter: 126 loss: 5.62793559e-07
Iter: 127 loss: 5.62525e-07
Iter: 128 loss: 5.62273726e-07
Iter: 129 loss: 5.62231492e-07
Iter: 130 loss: 5.61835691e-07
Iter: 131 loss: 5.61619345e-07
Iter: 132 loss: 5.61446655e-07
Iter: 133 loss: 5.61390721e-07
Iter: 134 loss: 5.61169372e-07
Iter: 135 loss: 5.61045852e-07
Iter: 136 loss: 5.60617423e-07
Iter: 137 loss: 5.60916419e-07
Iter: 138 loss: 5.60265335e-07
Iter: 139 loss: 5.59496812e-07
Iter: 140 loss: 5.64310426e-07
Iter: 141 loss: 5.59405748e-07
Iter: 142 loss: 5.5893571e-07
Iter: 143 loss: 5.59889145e-07
Iter: 144 loss: 5.58712713e-07
Iter: 145 loss: 5.58448505e-07
Iter: 146 loss: 5.58411102e-07
Iter: 147 loss: 5.58139845e-07
Iter: 148 loss: 5.57928388e-07
Iter: 149 loss: 5.57826411e-07
Iter: 150 loss: 5.57555154e-07
Iter: 151 loss: 5.58652118e-07
Iter: 152 loss: 5.5748734e-07
Iter: 153 loss: 5.57055159e-07
Iter: 154 loss: 5.57339945e-07
Iter: 155 loss: 5.56778218e-07
Iter: 156 loss: 5.56427835e-07
Iter: 157 loss: 5.57325222e-07
Iter: 158 loss: 5.56326597e-07
Iter: 159 loss: 5.56073246e-07
Iter: 160 loss: 5.56064663e-07
Iter: 161 loss: 5.55886061e-07
Iter: 162 loss: 5.55427448e-07
Iter: 163 loss: 5.60349e-07
Iter: 164 loss: 5.55367592e-07
Iter: 165 loss: 5.55153861e-07
Iter: 166 loss: 5.5515909e-07
Iter: 167 loss: 5.54912504e-07
Iter: 168 loss: 5.55443194e-07
Iter: 169 loss: 5.5482576e-07
Iter: 170 loss: 5.54667622e-07
Iter: 171 loss: 5.54318262e-07
Iter: 172 loss: 5.61850641e-07
Iter: 173 loss: 5.54330256e-07
Iter: 174 loss: 5.53956909e-07
Iter: 175 loss: 5.54718667e-07
Iter: 176 loss: 5.53808491e-07
Iter: 177 loss: 5.53368864e-07
Iter: 178 loss: 5.55805457e-07
Iter: 179 loss: 5.53328732e-07
Iter: 180 loss: 5.52987672e-07
Iter: 181 loss: 5.52997903e-07
Iter: 182 loss: 5.52850281e-07
Iter: 183 loss: 5.52485631e-07
Iter: 184 loss: 5.54437293e-07
Iter: 185 loss: 5.52381266e-07
Iter: 186 loss: 5.5191208e-07
Iter: 187 loss: 5.553851e-07
Iter: 188 loss: 5.5186274e-07
Iter: 189 loss: 5.5167493e-07
Iter: 190 loss: 5.52366885e-07
Iter: 191 loss: 5.51635253e-07
Iter: 192 loss: 5.51450853e-07
Iter: 193 loss: 5.54229359e-07
Iter: 194 loss: 5.51451535e-07
Iter: 195 loss: 5.5124849e-07
Iter: 196 loss: 5.50881623e-07
Iter: 197 loss: 5.50868265e-07
Iter: 198 loss: 5.50666584e-07
Iter: 199 loss: 5.51492406e-07
Iter: 200 loss: 5.50592688e-07
Iter: 201 loss: 5.50226616e-07
Iter: 202 loss: 5.50591e-07
Iter: 203 loss: 5.50030052e-07
Iter: 204 loss: 5.49739525e-07
Iter: 205 loss: 5.4980967e-07
Iter: 206 loss: 5.49522497e-07
Iter: 207 loss: 5.49302626e-07
Iter: 208 loss: 5.51251e-07
Iter: 209 loss: 5.4929717e-07
Iter: 210 loss: 5.49022786e-07
Iter: 211 loss: 5.49635388e-07
Iter: 212 loss: 5.48925186e-07
Iter: 213 loss: 5.4879871e-07
Iter: 214 loss: 5.48871071e-07
Iter: 215 loss: 5.48730441e-07
Iter: 216 loss: 5.48606238e-07
Iter: 217 loss: 5.50340474e-07
Iter: 218 loss: 5.48615617e-07
Iter: 219 loss: 5.48509547e-07
Iter: 220 loss: 5.48281378e-07
Iter: 221 loss: 5.52819301e-07
Iter: 222 loss: 5.48282344e-07
Iter: 223 loss: 5.48077765e-07
Iter: 224 loss: 5.48662456e-07
Iter: 225 loss: 5.4800563e-07
Iter: 226 loss: 5.47787067e-07
Iter: 227 loss: 5.47594141e-07
Iter: 228 loss: 5.47537923e-07
Iter: 229 loss: 5.47358752e-07
Iter: 230 loss: 5.47346644e-07
Iter: 231 loss: 5.47193508e-07
Iter: 232 loss: 5.49014089e-07
Iter: 233 loss: 5.47196919e-07
Iter: 234 loss: 5.47117452e-07
Iter: 235 loss: 5.46953231e-07
Iter: 236 loss: 5.50089624e-07
Iter: 237 loss: 5.46949536e-07
Iter: 238 loss: 5.46883598e-07
Iter: 239 loss: 5.46853641e-07
Iter: 240 loss: 5.46774743e-07
Iter: 241 loss: 5.46580168e-07
Iter: 242 loss: 5.49067522e-07
Iter: 243 loss: 5.46571641e-07
Iter: 244 loss: 5.46307547e-07
Iter: 245 loss: 5.46274237e-07
Iter: 246 loss: 5.4610291e-07
Iter: 247 loss: 5.46234446e-07
Iter: 248 loss: 5.45976263e-07
Iter: 249 loss: 5.45911e-07
Iter: 250 loss: 5.45697617e-07
Iter: 251 loss: 5.46875299e-07
Iter: 252 loss: 5.45637704e-07
Iter: 253 loss: 5.45429657e-07
Iter: 254 loss: 5.48018193e-07
Iter: 255 loss: 5.45425848e-07
Iter: 256 loss: 5.45277544e-07
Iter: 257 loss: 5.45299145e-07
Iter: 258 loss: 5.45230478e-07
Iter: 259 loss: 5.45042212e-07
Iter: 260 loss: 5.45706428e-07
Iter: 261 loss: 5.44939667e-07
Iter: 262 loss: 5.4467796e-07
Iter: 263 loss: 5.47354489e-07
Iter: 264 loss: 5.44675572e-07
Iter: 265 loss: 5.44479235e-07
Iter: 266 loss: 5.44214629e-07
Iter: 267 loss: 5.44187174e-07
Iter: 268 loss: 5.4403904e-07
Iter: 269 loss: 5.43971396e-07
Iter: 270 loss: 5.43824854e-07
Iter: 271 loss: 5.45038574e-07
Iter: 272 loss: 5.43805072e-07
Iter: 273 loss: 5.43738565e-07
Iter: 274 loss: 5.43574401e-07
Iter: 275 loss: 5.46343131e-07
Iter: 276 loss: 5.43580541e-07
Iter: 277 loss: 5.43455599e-07
Iter: 278 loss: 5.43450824e-07
Iter: 279 loss: 5.43409e-07
Iter: 280 loss: 5.43345152e-07
Iter: 281 loss: 5.44826776e-07
Iter: 282 loss: 5.43332476e-07
Iter: 283 loss: 5.43239e-07
Iter: 284 loss: 5.43834062e-07
Iter: 285 loss: 5.43257215e-07
Iter: 286 loss: 5.43129545e-07
Iter: 287 loss: 5.43082706e-07
Iter: 288 loss: 5.43003353e-07
Iter: 289 loss: 5.42923942e-07
Iter: 290 loss: 5.43670183e-07
Iter: 291 loss: 5.42914e-07
Iter: 292 loss: 5.42779048e-07
Iter: 293 loss: 5.42667749e-07
Iter: 294 loss: 5.42634098e-07
Iter: 295 loss: 5.42432474e-07
Iter: 296 loss: 5.42378302e-07
Iter: 297 loss: 5.42254497e-07
Iter: 298 loss: 5.42051453e-07
Iter: 299 loss: 5.42463738e-07
Iter: 300 loss: 5.41984605e-07
Iter: 301 loss: 5.41769964e-07
Iter: 302 loss: 5.43265401e-07
Iter: 303 loss: 5.41718464e-07
Iter: 304 loss: 5.4169476e-07
Iter: 305 loss: 5.4166884e-07
Iter: 306 loss: 5.41599547e-07
Iter: 307 loss: 5.41471081e-07
Iter: 308 loss: 5.41474321e-07
Iter: 309 loss: 5.41370184e-07
Iter: 310 loss: 5.42322198e-07
Iter: 311 loss: 5.41355291e-07
Iter: 312 loss: 5.41266161e-07
Iter: 313 loss: 5.41402073e-07
Iter: 314 loss: 5.41194879e-07
Iter: 315 loss: 5.41098e-07
Iter: 316 loss: 5.40930159e-07
Iter: 317 loss: 5.4093357e-07
Iter: 318 loss: 5.40866324e-07
Iter: 319 loss: 5.40829774e-07
Iter: 320 loss: 5.407461e-07
Iter: 321 loss: 5.40722965e-07
Iter: 322 loss: 5.40669362e-07
Iter: 323 loss: 5.4058296e-07
Iter: 324 loss: 5.41289751e-07
Iter: 325 loss: 5.40575854e-07
Iter: 326 loss: 5.40463361e-07
Iter: 327 loss: 5.40264864e-07
Iter: 328 loss: 5.43052238e-07
Iter: 329 loss: 5.40214728e-07
Iter: 330 loss: 5.40036638e-07
Iter: 331 loss: 5.40419819e-07
Iter: 332 loss: 5.39971779e-07
Iter: 333 loss: 5.39738551e-07
Iter: 334 loss: 5.39747703e-07
Iter: 335 loss: 5.39567054e-07
Iter: 336 loss: 5.39531e-07
Iter: 337 loss: 5.39479061e-07
Iter: 338 loss: 5.39407893e-07
Iter: 339 loss: 5.39628104e-07
Iter: 340 loss: 5.39367647e-07
Iter: 341 loss: 5.39309667e-07
Iter: 342 loss: 5.39377652e-07
Iter: 343 loss: 5.39281245e-07
Iter: 344 loss: 5.39190353e-07
Iter: 345 loss: 5.39902089e-07
Iter: 346 loss: 5.39195128e-07
Iter: 347 loss: 5.39157895e-07
Iter: 348 loss: 5.3903932e-07
Iter: 349 loss: 5.40326369e-07
Iter: 350 loss: 5.39019481e-07
Iter: 351 loss: 5.38886e-07
Iter: 352 loss: 5.39753273e-07
Iter: 353 loss: 5.38848781e-07
Iter: 354 loss: 5.38742e-07
Iter: 355 loss: 5.40515316e-07
Iter: 356 loss: 5.38747713e-07
Iter: 357 loss: 5.38666882e-07
Iter: 358 loss: 5.38563313e-07
Iter: 359 loss: 5.3855149e-07
Iter: 360 loss: 5.38379879e-07
Iter: 361 loss: 5.39854227e-07
Iter: 362 loss: 5.38390339e-07
Iter: 363 loss: 5.38290692e-07
Iter: 364 loss: 5.3821708e-07
Iter: 365 loss: 5.38187408e-07
Iter: 366 loss: 5.38084066e-07
Iter: 367 loss: 5.38192921e-07
Iter: 368 loss: 5.38047971e-07
Iter: 369 loss: 5.37928031e-07
Iter: 370 loss: 5.3840995e-07
Iter: 371 loss: 5.37894778e-07
Iter: 372 loss: 5.37820426e-07
Iter: 373 loss: 5.38654604e-07
Iter: 374 loss: 5.37830374e-07
Iter: 375 loss: 5.37751362e-07
Iter: 376 loss: 5.37665301e-07
Iter: 377 loss: 5.37676726e-07
Iter: 378 loss: 5.37591859e-07
Iter: 379 loss: 5.38425866e-07
Iter: 380 loss: 5.37592427e-07
Iter: 381 loss: 5.37512847e-07
Iter: 382 loss: 5.37585947e-07
Iter: 383 loss: 5.37454866e-07
Iter: 384 loss: 5.37394158e-07
Iter: 385 loss: 5.37247843e-07
Iter: 386 loss: 5.40371104e-07
Iter: 387 loss: 5.37232609e-07
Iter: 388 loss: 5.37227379e-07
Iter: 389 loss: 5.37170536e-07
Iter: 390 loss: 5.37094934e-07
Iter: 391 loss: 5.37120968e-07
Iter: 392 loss: 5.37054746e-07
Iter: 393 loss: 5.3697994e-07
Iter: 394 loss: 5.37043263e-07
Iter: 395 loss: 5.36931566e-07
Iter: 396 loss: 5.36832431e-07
Iter: 397 loss: 5.37360449e-07
Iter: 398 loss: 5.36780703e-07
Iter: 399 loss: 5.36716186e-07
Iter: 400 loss: 5.36587493e-07
Iter: 401 loss: 5.38491236e-07
Iter: 402 loss: 5.36574248e-07
Iter: 403 loss: 5.36457605e-07
Iter: 404 loss: 5.37136e-07
Iter: 405 loss: 5.36417929e-07
Iter: 406 loss: 5.36317089e-07
Iter: 407 loss: 5.36614152e-07
Iter: 408 loss: 5.36278264e-07
Iter: 409 loss: 5.36211701e-07
Iter: 410 loss: 5.36739606e-07
Iter: 411 loss: 5.36220909e-07
Iter: 412 loss: 5.36154971e-07
Iter: 413 loss: 5.36166908e-07
Iter: 414 loss: 5.36150424e-07
Iter: 415 loss: 5.36074651e-07
Iter: 416 loss: 5.36062203e-07
Iter: 417 loss: 5.36005928e-07
Iter: 418 loss: 5.35946185e-07
Iter: 419 loss: 5.35921913e-07
Iter: 420 loss: 5.35863762e-07
Iter: 421 loss: 5.35876893e-07
Iter: 422 loss: 5.35812433e-07
Iter: 423 loss: 5.35769345e-07
Iter: 424 loss: 5.35619961e-07
Iter: 425 loss: 5.37666324e-07
Iter: 426 loss: 5.35617346e-07
Iter: 427 loss: 5.35557547e-07
Iter: 428 loss: 5.35546235e-07
Iter: 429 loss: 5.35514459e-07
Iter: 430 loss: 5.35498e-07
Iter: 431 loss: 5.35476488e-07
Iter: 432 loss: 5.35416689e-07
Iter: 433 loss: 5.36118137e-07
Iter: 434 loss: 5.35419815e-07
Iter: 435 loss: 5.35381048e-07
Iter: 436 loss: 5.35637e-07
Iter: 437 loss: 5.35350068e-07
Iter: 438 loss: 5.35324e-07
Iter: 439 loss: 5.35590516e-07
Iter: 440 loss: 5.35310392e-07
Iter: 441 loss: 5.35254912e-07
Iter: 442 loss: 5.35166578e-07
Iter: 443 loss: 5.36544633e-07
Iter: 444 loss: 5.35121444e-07
Iter: 445 loss: 5.3507415e-07
Iter: 446 loss: 5.35068352e-07
Iter: 447 loss: 5.35001845e-07
Iter: 448 loss: 5.34992409e-07
Iter: 449 loss: 5.34959895e-07
Iter: 450 loss: 5.34886908e-07
Iter: 451 loss: 5.34984736e-07
Iter: 452 loss: 5.34865762e-07
Iter: 453 loss: 5.34807782e-07
Iter: 454 loss: 5.34787432e-07
Iter: 455 loss: 5.34763274e-07
Iter: 456 loss: 5.34725473e-07
Iter: 457 loss: 5.35458867e-07
Iter: 458 loss: 5.34714218e-07
Iter: 459 loss: 5.3464862e-07
Iter: 460 loss: 5.34710466e-07
Iter: 461 loss: 5.34584046e-07
Iter: 462 loss: 5.34537662e-07
Iter: 463 loss: 5.35070342e-07
Iter: 464 loss: 5.34517881e-07
Iter: 465 loss: 5.34447167e-07
Iter: 466 loss: 5.34449e-07
Iter: 467 loss: 5.34371566e-07
Iter: 468 loss: 5.34328592e-07
Iter: 469 loss: 5.34363721e-07
Iter: 470 loss: 5.3427982e-07
Iter: 471 loss: 5.34239746e-07
Iter: 472 loss: 5.34238552e-07
Iter: 473 loss: 5.34175683e-07
Iter: 474 loss: 5.34117e-07
Iter: 475 loss: 5.35673621e-07
Iter: 476 loss: 5.3411361e-07
Iter: 477 loss: 5.34064384e-07
Iter: 478 loss: 5.34050741e-07
Iter: 479 loss: 5.33997706e-07
Iter: 480 loss: 5.34079e-07
Iter: 481 loss: 5.33991624e-07
Iter: 482 loss: 5.3395695e-07
Iter: 483 loss: 5.3393e-07
Iter: 484 loss: 5.3390329e-07
Iter: 485 loss: 5.33866455e-07
Iter: 486 loss: 5.33856166e-07
Iter: 487 loss: 5.33815751e-07
Iter: 488 loss: 5.33765501e-07
Iter: 489 loss: 5.33762261e-07
Iter: 490 loss: 5.33681202e-07
Iter: 491 loss: 5.33642265e-07
Iter: 492 loss: 5.33625439e-07
Iter: 493 loss: 5.33532727e-07
Iter: 494 loss: 5.34354854e-07
Iter: 495 loss: 5.33516186e-07
Iter: 496 loss: 5.33469e-07
Iter: 497 loss: 5.33462867e-07
Iter: 498 loss: 5.33431034e-07
Iter: 499 loss: 5.33303933e-07
Iter: 500 loss: 5.34094625e-07
Iter: 501 loss: 5.33285402e-07
Iter: 502 loss: 5.33171487e-07
Iter: 503 loss: 5.33894251e-07
Iter: 504 loss: 5.33162961e-07
Iter: 505 loss: 5.33052798e-07
Iter: 506 loss: 5.33993898e-07
Iter: 507 loss: 5.33030743e-07
Iter: 508 loss: 5.3295048e-07
Iter: 509 loss: 5.32767444e-07
Iter: 510 loss: 5.35712502e-07
Iter: 511 loss: 5.32773811e-07
Iter: 512 loss: 5.32686272e-07
Iter: 513 loss: 5.3266092e-07
Iter: 514 loss: 5.32603849e-07
Iter: 515 loss: 5.32597824e-07
Iter: 516 loss: 5.32521824e-07
Iter: 517 loss: 5.32433944e-07
Iter: 518 loss: 5.3256133e-07
Iter: 519 loss: 5.32428146e-07
Iter: 520 loss: 5.32323043e-07
Iter: 521 loss: 5.33425464e-07
Iter: 522 loss: 5.32312242e-07
Iter: 523 loss: 5.32270462e-07
Iter: 524 loss: 5.32177296e-07
Iter: 525 loss: 5.32761874e-07
Iter: 526 loss: 5.32144782e-07
Iter: 527 loss: 5.32022852e-07
Iter: 528 loss: 5.32905233e-07
Iter: 529 loss: 5.32029674e-07
Iter: 530 loss: 5.31949354e-07
Iter: 531 loss: 5.31976468e-07
Iter: 532 loss: 5.3189865e-07
Iter: 533 loss: 5.31883757e-07
Iter: 534 loss: 5.31843739e-07
Iter: 535 loss: 5.31796104e-07
Iter: 536 loss: 5.31813271e-07
Iter: 537 loss: 5.31773424e-07
Iter: 538 loss: 5.31739204e-07
Iter: 539 loss: 5.31685714e-07
Iter: 540 loss: 5.31678097e-07
Iter: 541 loss: 5.31567707e-07
Iter: 542 loss: 5.32159333e-07
Iter: 543 loss: 5.31563728e-07
Iter: 544 loss: 5.3147437e-07
Iter: 545 loss: 5.31714591e-07
Iter: 546 loss: 5.3143475e-07
Iter: 547 loss: 5.31342437e-07
Iter: 548 loss: 5.32457648e-07
Iter: 549 loss: 5.3132851e-07
Iter: 550 loss: 5.31270359e-07
Iter: 551 loss: 5.31192939e-07
Iter: 552 loss: 5.31201124e-07
Iter: 553 loss: 5.31170883e-07
Iter: 554 loss: 5.31159344e-07
Iter: 555 loss: 5.31133082e-07
Iter: 556 loss: 5.31047704e-07
Iter: 557 loss: 5.31670366e-07
Iter: 558 loss: 5.31015758e-07
Iter: 559 loss: 5.30924467e-07
Iter: 560 loss: 5.31559238e-07
Iter: 561 loss: 5.30927423e-07
Iter: 562 loss: 5.30864668e-07
Iter: 563 loss: 5.3117742e-07
Iter: 564 loss: 5.30862621e-07
Iter: 565 loss: 5.30798616e-07
Iter: 566 loss: 5.31151215e-07
Iter: 567 loss: 5.30788782e-07
Iter: 568 loss: 5.30778038e-07
Iter: 569 loss: 5.3075928e-07
Iter: 570 loss: 5.30750185e-07
Iter: 571 loss: 5.30699822e-07
Iter: 572 loss: 5.30837383e-07
Iter: 573 loss: 5.30684929e-07
Iter: 574 loss: 5.30623367e-07
Iter: 575 loss: 5.30495925e-07
Iter: 576 loss: 5.30506043e-07
Iter: 577 loss: 5.30403895e-07
Iter: 578 loss: 5.30844829e-07
Iter: 579 loss: 5.30365071e-07
Iter: 580 loss: 5.30304419e-07
Iter: 581 loss: 5.30599152e-07
Iter: 582 loss: 5.30295324e-07
Iter: 583 loss: 5.30209036e-07
Iter: 584 loss: 5.30116381e-07
Iter: 585 loss: 5.30115358e-07
Iter: 586 loss: 5.3005e-07
Iter: 587 loss: 5.30043167e-07
Iter: 588 loss: 5.29977513e-07
Iter: 589 loss: 5.30020316e-07
Iter: 590 loss: 5.29963813e-07
Iter: 591 loss: 5.29903e-07
Iter: 592 loss: 5.29748831e-07
Iter: 593 loss: 5.31179523e-07
Iter: 594 loss: 5.29736383e-07
Iter: 595 loss: 5.29604904e-07
Iter: 596 loss: 5.31040428e-07
Iter: 597 loss: 5.29614908e-07
Iter: 598 loss: 5.29504803e-07
Iter: 599 loss: 5.29705858e-07
Iter: 600 loss: 5.29489171e-07
Iter: 601 loss: 5.29405668e-07
Iter: 602 loss: 5.30664238e-07
Iter: 603 loss: 5.29425336e-07
Iter: 604 loss: 5.29383726e-07
Iter: 605 loss: 5.29317845e-07
Iter: 606 loss: 5.29323529e-07
Iter: 607 loss: 5.29256e-07
Iter: 608 loss: 5.29255e-07
Iter: 609 loss: 5.29225076e-07
Iter: 610 loss: 5.29198815e-07
Iter: 611 loss: 5.29185513e-07
Iter: 612 loss: 5.29136287e-07
Iter: 613 loss: 5.29460806e-07
Iter: 614 loss: 5.29147428e-07
Iter: 615 loss: 5.29108e-07
Iter: 616 loss: 5.29193e-07
Iter: 617 loss: 5.29093825e-07
Iter: 618 loss: 5.29069212e-07
Iter: 619 loss: 5.29086833e-07
Iter: 620 loss: 5.29049885e-07
Iter: 621 loss: 5.29033e-07
Iter: 622 loss: 5.29030274e-07
Iter: 623 loss: 5.29011913e-07
Iter: 624 loss: 5.28972578e-07
Iter: 625 loss: 5.29201202e-07
Iter: 626 loss: 5.28952171e-07
Iter: 627 loss: 5.28903229e-07
Iter: 628 loss: 5.28998498e-07
Iter: 629 loss: 5.28879582e-07
Iter: 630 loss: 5.28811825e-07
Iter: 631 loss: 5.28791702e-07
Iter: 632 loss: 5.28760779e-07
Iter: 633 loss: 5.28722751e-07
Iter: 634 loss: 5.28697115e-07
Iter: 635 loss: 5.28671421e-07
Iter: 636 loss: 5.28627879e-07
Iter: 637 loss: 5.28611565e-07
Iter: 638 loss: 5.28564101e-07
Iter: 639 loss: 5.28641522e-07
Iter: 640 loss: 5.28518854e-07
Iter: 641 loss: 5.28461214e-07
Iter: 642 loss: 5.28903115e-07
Iter: 643 loss: 5.28467e-07
Iter: 644 loss: 5.28402609e-07
Iter: 645 loss: 5.28426881e-07
Iter: 646 loss: 5.28390672e-07
Iter: 647 loss: 5.28354235e-07
Iter: 648 loss: 5.28464398e-07
Iter: 649 loss: 5.28320811e-07
Iter: 650 loss: 5.28253736e-07
Iter: 651 loss: 5.28474516e-07
Iter: 652 loss: 5.28211331e-07
Iter: 653 loss: 5.28177907e-07
Iter: 654 loss: 5.28517035e-07
Iter: 655 loss: 5.28173359e-07
Iter: 656 loss: 5.28135729e-07
Iter: 657 loss: 5.28188707e-07
Iter: 658 loss: 5.28129e-07
Iter: 659 loss: 5.28081387e-07
Iter: 660 loss: 5.28045121e-07
Iter: 661 loss: 5.2806115e-07
Iter: 662 loss: 5.27995439e-07
Iter: 663 loss: 5.28226451e-07
Iter: 664 loss: 5.27981683e-07
Iter: 665 loss: 5.27943939e-07
Iter: 666 loss: 5.27942518e-07
Iter: 667 loss: 5.27901761e-07
Iter: 668 loss: 5.27851853e-07
Iter: 669 loss: 5.2785515e-07
Iter: 670 loss: 5.27782333e-07
Iter: 671 loss: 5.27814052e-07
Iter: 672 loss: 5.27758857e-07
Iter: 673 loss: 5.27726797e-07
Iter: 674 loss: 5.27712473e-07
Iter: 675 loss: 5.27672853e-07
Iter: 676 loss: 5.2763744e-07
Iter: 677 loss: 5.27636416e-07
Iter: 678 loss: 5.27559848e-07
Iter: 679 loss: 5.27680754e-07
Iter: 680 loss: 5.27574116e-07
Iter: 681 loss: 5.27536201e-07
Iter: 682 loss: 5.27917791e-07
Iter: 683 loss: 5.27526424e-07
Iter: 684 loss: 5.27499424e-07
Iter: 685 loss: 5.27578152e-07
Iter: 686 loss: 5.27483962e-07
Iter: 687 loss: 5.27451675e-07
Iter: 688 loss: 5.27481461e-07
Iter: 689 loss: 5.27431894e-07
Iter: 690 loss: 5.27379655e-07
Iter: 691 loss: 5.27299449e-07
Iter: 692 loss: 5.2891221e-07
Iter: 693 loss: 5.27308089e-07
Iter: 694 loss: 5.27211682e-07
Iter: 695 loss: 5.27739758e-07
Iter: 696 loss: 5.27173825e-07
Iter: 697 loss: 5.27116754e-07
Iter: 698 loss: 5.2712619e-07
Iter: 699 loss: 5.27082875e-07
Iter: 700 loss: 5.271595e-07
Iter: 701 loss: 5.27029101e-07
Iter: 702 loss: 5.26988742e-07
Iter: 703 loss: 5.26979079e-07
Iter: 704 loss: 5.26960889e-07
Iter: 705 loss: 5.26903591e-07
Iter: 706 loss: 5.27279269e-07
Iter: 707 loss: 5.26922122e-07
Iter: 708 loss: 5.26861868e-07
Iter: 709 loss: 5.27084183e-07
Iter: 710 loss: 5.26813e-07
Iter: 711 loss: 5.26797635e-07
Iter: 712 loss: 5.26740678e-07
Iter: 713 loss: 5.27745044e-07
Iter: 714 loss: 5.26752103e-07
Iter: 715 loss: 5.2668338e-07
Iter: 716 loss: 5.26688495e-07
Iter: 717 loss: 5.2665456e-07
Iter: 718 loss: 5.26646659e-07
Iter: 719 loss: 5.26607209e-07
Iter: 720 loss: 5.2655e-07
Iter: 721 loss: 5.2720776e-07
Iter: 722 loss: 5.26561337e-07
Iter: 723 loss: 5.26516374e-07
Iter: 724 loss: 5.26439692e-07
Iter: 725 loss: 5.27704401e-07
Iter: 726 loss: 5.26429858e-07
Iter: 727 loss: 5.26306394e-07
Iter: 728 loss: 5.2663529e-07
Iter: 729 loss: 5.26293775e-07
Iter: 730 loss: 5.26220276e-07
Iter: 731 loss: 5.26211e-07
Iter: 732 loss: 5.26170936e-07
Iter: 733 loss: 5.26105509e-07
Iter: 734 loss: 5.26090275e-07
Iter: 735 loss: 5.26010695e-07
Iter: 736 loss: 5.25963969e-07
Iter: 737 loss: 5.25947485e-07
Iter: 738 loss: 5.25871144e-07
Iter: 739 loss: 5.25762971e-07
Iter: 740 loss: 5.25766495e-07
Iter: 741 loss: 5.25755354e-07
Iter: 742 loss: 5.25693e-07
Iter: 743 loss: 5.25645419e-07
Iter: 744 loss: 5.25640075e-07
Iter: 745 loss: 5.25614269e-07
Iter: 746 loss: 5.2558255e-07
Iter: 747 loss: 5.25917926e-07
Iter: 748 loss: 5.25583e-07
Iter: 749 loss: 5.25548558e-07
Iter: 750 loss: 5.25571295e-07
Iter: 751 loss: 5.25535938e-07
Iter: 752 loss: 5.25484381e-07
Iter: 753 loss: 5.25549694e-07
Iter: 754 loss: 5.25470568e-07
Iter: 755 loss: 5.2542822e-07
Iter: 756 loss: 5.25626547e-07
Iter: 757 loss: 5.25421967e-07
Iter: 758 loss: 5.25394626e-07
Iter: 759 loss: 5.25340624e-07
Iter: 760 loss: 5.25342614e-07
Iter: 761 loss: 5.25322889e-07
Iter: 762 loss: 5.25545374e-07
Iter: 763 loss: 5.25304756e-07
Iter: 764 loss: 5.25273265e-07
Iter: 765 loss: 5.25302937e-07
Iter: 766 loss: 5.25215626e-07
Iter: 767 loss: 5.25143264e-07
Iter: 768 loss: 5.25851419e-07
Iter: 769 loss: 5.2514406e-07
Iter: 770 loss: 5.2509165e-07
Iter: 771 loss: 5.25071243e-07
Iter: 772 loss: 5.25042879e-07
Iter: 773 loss: 5.25040662e-07
Iter: 774 loss: 5.25041401e-07
Iter: 775 loss: 5.25010705e-07
Iter: 776 loss: 5.24974837e-07
Iter: 777 loss: 5.24951588e-07
Iter: 778 loss: 5.24959887e-07
Iter: 779 loss: 5.24962729e-07
Iter: 780 loss: 5.24953236e-07
Iter: 781 loss: 5.24951e-07
Iter: 782 loss: 5.24960114e-07
Iter: 783 loss: 5.24950394e-07
Iter: 784 loss: 5.24950451e-07
Iter: 785 loss: 5.24961877e-07
Iter: 786 loss: 5.24957841e-07
Iter: 787 loss: 5.24941811e-07
Iter: 788 loss: 5.24959e-07
Iter: 789 loss: 5.24948632e-07
Iter: 790 loss: 5.24958068e-07
Iter: 791 loss: 5.24947041e-07
Iter: 792 loss: 5.24950565e-07
Iter: 793 loss: 5.24957e-07
Iter: 794 loss: 5.24955965e-07
Iter: 795 loss: 5.24952952e-07
Iter: 796 loss: 5.24953236e-07
Iter: 797 loss: 5.24951929e-07
Iter: 798 loss: 5.249521e-07
Iter: 799 loss: 5.24951531e-07
Iter: 800 loss: 5.2495227e-07
Iter: 801 loss: 5.24952725e-07
Iter: 802 loss: 5.24952156e-07
Iter: 803 loss: 5.24951702e-07
Iter: 804 loss: 5.24951929e-07
Iter: 805 loss: 5.24952156e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8
+ date
Mon Oct 26 11:11:42 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.8_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi0.8_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe435079c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe435080730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434fcc6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe43500d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434fb0bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434ff86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434f55f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434f251e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434f25268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434f25620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434f012f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434ea1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434ea11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434e5d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434e6a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434e5d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434f07950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434e23268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434e23048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434e37ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434d84a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434d1a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434d849d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe434cdfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe40503c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe40505ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe40505a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe40500f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe40500f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe404fb8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe404f89268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe404f9ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe404f5d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe404faad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3c04370d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3c0405f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.39573944e-06
Iter: 2 loss: 2.60674301e-06
Iter: 3 loss: 1.73529111e-06
Iter: 4 loss: 1.4947841e-06
Iter: 5 loss: 1.36734707e-06
Iter: 6 loss: 1.25795191e-06
Iter: 7 loss: 1.11685858e-06
Iter: 8 loss: 1.11581608e-06
Iter: 9 loss: 1.00632053e-06
Iter: 10 loss: 9.14835e-07
Iter: 11 loss: 8.8369319e-07
Iter: 12 loss: 8.16351417e-07
Iter: 13 loss: 1.67780831e-06
Iter: 14 loss: 8.15811177e-07
Iter: 15 loss: 7.88373427e-07
Iter: 16 loss: 7.87878e-07
Iter: 17 loss: 7.77511104e-07
Iter: 18 loss: 7.62013713e-07
Iter: 19 loss: 7.61674357e-07
Iter: 20 loss: 7.43153691e-07
Iter: 21 loss: 9.78898584e-07
Iter: 22 loss: 7.43003795e-07
Iter: 23 loss: 7.32410854e-07
Iter: 24 loss: 7.29252179e-07
Iter: 25 loss: 7.22926757e-07
Iter: 26 loss: 7.11727353e-07
Iter: 27 loss: 7.10321729e-07
Iter: 28 loss: 7.0234114e-07
Iter: 29 loss: 6.8698921e-07
Iter: 30 loss: 7.11463031e-07
Iter: 31 loss: 6.79870823e-07
Iter: 32 loss: 6.69119913e-07
Iter: 33 loss: 8.11873292e-07
Iter: 34 loss: 6.6905568e-07
Iter: 35 loss: 6.63885885e-07
Iter: 36 loss: 6.6381449e-07
Iter: 37 loss: 6.59072725e-07
Iter: 38 loss: 6.59918385e-07
Iter: 39 loss: 6.55525241e-07
Iter: 40 loss: 6.52035965e-07
Iter: 41 loss: 6.65756943e-07
Iter: 42 loss: 6.51259086e-07
Iter: 43 loss: 6.46616058e-07
Iter: 44 loss: 6.56026373e-07
Iter: 45 loss: 6.44736133e-07
Iter: 46 loss: 6.42291866e-07
Iter: 47 loss: 6.41068368e-07
Iter: 48 loss: 6.39915811e-07
Iter: 49 loss: 6.36394134e-07
Iter: 50 loss: 6.91100354e-07
Iter: 51 loss: 6.36372306e-07
Iter: 52 loss: 6.33585501e-07
Iter: 53 loss: 6.26624e-07
Iter: 54 loss: 6.9396134e-07
Iter: 55 loss: 6.25704047e-07
Iter: 56 loss: 6.19009484e-07
Iter: 57 loss: 6.18954232e-07
Iter: 58 loss: 6.14332521e-07
Iter: 59 loss: 6.18231468e-07
Iter: 60 loss: 6.11613189e-07
Iter: 61 loss: 6.0796458e-07
Iter: 62 loss: 6.02817863e-07
Iter: 63 loss: 6.02613341e-07
Iter: 64 loss: 5.97396934e-07
Iter: 65 loss: 6.66811843e-07
Iter: 66 loss: 5.97377266e-07
Iter: 67 loss: 5.94760195e-07
Iter: 68 loss: 5.98101451e-07
Iter: 69 loss: 5.93448817e-07
Iter: 70 loss: 5.91568096e-07
Iter: 71 loss: 5.9155127e-07
Iter: 72 loss: 5.8965054e-07
Iter: 73 loss: 5.9511683e-07
Iter: 74 loss: 5.89080969e-07
Iter: 75 loss: 5.87928184e-07
Iter: 76 loss: 5.86857254e-07
Iter: 77 loss: 5.86589522e-07
Iter: 78 loss: 5.84319196e-07
Iter: 79 loss: 6.06942251e-07
Iter: 80 loss: 5.84274687e-07
Iter: 81 loss: 5.83206656e-07
Iter: 82 loss: 5.81721e-07
Iter: 83 loss: 5.81670179e-07
Iter: 84 loss: 5.80132735e-07
Iter: 85 loss: 5.80130518e-07
Iter: 86 loss: 5.78810159e-07
Iter: 87 loss: 5.77214792e-07
Iter: 88 loss: 5.77070409e-07
Iter: 89 loss: 5.75756246e-07
Iter: 90 loss: 5.90143827e-07
Iter: 91 loss: 5.75717934e-07
Iter: 92 loss: 5.74274168e-07
Iter: 93 loss: 5.73445277e-07
Iter: 94 loss: 5.72839e-07
Iter: 95 loss: 5.71397436e-07
Iter: 96 loss: 5.70489533e-07
Iter: 97 loss: 5.69898e-07
Iter: 98 loss: 5.67742916e-07
Iter: 99 loss: 5.81147e-07
Iter: 100 loss: 5.67487689e-07
Iter: 101 loss: 5.65612481e-07
Iter: 102 loss: 5.67030497e-07
Iter: 103 loss: 5.64483912e-07
Iter: 104 loss: 5.63378308e-07
Iter: 105 loss: 5.63316405e-07
Iter: 106 loss: 5.62099899e-07
Iter: 107 loss: 5.62448918e-07
Iter: 108 loss: 5.61224624e-07
Iter: 109 loss: 5.60052854e-07
Iter: 110 loss: 5.61181764e-07
Iter: 111 loss: 5.59375621e-07
Iter: 112 loss: 5.58345221e-07
Iter: 113 loss: 5.58323563e-07
Iter: 114 loss: 5.57856197e-07
Iter: 115 loss: 5.57279179e-07
Iter: 116 loss: 5.57220915e-07
Iter: 117 loss: 5.56681471e-07
Iter: 118 loss: 5.56724558e-07
Iter: 119 loss: 5.56148223e-07
Iter: 120 loss: 5.55102531e-07
Iter: 121 loss: 5.77499e-07
Iter: 122 loss: 5.55111512e-07
Iter: 123 loss: 5.54818371e-07
Iter: 124 loss: 5.54760618e-07
Iter: 125 loss: 5.54373059e-07
Iter: 126 loss: 5.53547295e-07
Iter: 127 loss: 5.63044466e-07
Iter: 128 loss: 5.53449752e-07
Iter: 129 loss: 5.52515758e-07
Iter: 130 loss: 5.55497877e-07
Iter: 131 loss: 5.52246206e-07
Iter: 132 loss: 5.51574544e-07
Iter: 133 loss: 5.5432406e-07
Iter: 134 loss: 5.51400944e-07
Iter: 135 loss: 5.50762934e-07
Iter: 136 loss: 5.51967901e-07
Iter: 137 loss: 5.50438e-07
Iter: 138 loss: 5.49975539e-07
Iter: 139 loss: 5.49909373e-07
Iter: 140 loss: 5.49564675e-07
Iter: 141 loss: 5.49016193e-07
Iter: 142 loss: 5.49020456e-07
Iter: 143 loss: 5.48370792e-07
Iter: 144 loss: 5.52780648e-07
Iter: 145 loss: 5.48297749e-07
Iter: 146 loss: 5.47439697e-07
Iter: 147 loss: 5.46653609e-07
Iter: 148 loss: 5.46455567e-07
Iter: 149 loss: 5.45184491e-07
Iter: 150 loss: 5.48292917e-07
Iter: 151 loss: 5.44723093e-07
Iter: 152 loss: 5.43996862e-07
Iter: 153 loss: 5.43908186e-07
Iter: 154 loss: 5.43600208e-07
Iter: 155 loss: 5.4268628e-07
Iter: 156 loss: 5.49498395e-07
Iter: 157 loss: 5.42516375e-07
Iter: 158 loss: 5.42096132e-07
Iter: 159 loss: 5.41926397e-07
Iter: 160 loss: 5.41573911e-07
Iter: 161 loss: 5.41382576e-07
Iter: 162 loss: 5.41266331e-07
Iter: 163 loss: 5.4090367e-07
Iter: 164 loss: 5.40886958e-07
Iter: 165 loss: 5.406269e-07
Iter: 166 loss: 5.40225869e-07
Iter: 167 loss: 5.41426857e-07
Iter: 168 loss: 5.40142821e-07
Iter: 169 loss: 5.39849168e-07
Iter: 170 loss: 5.42782061e-07
Iter: 171 loss: 5.39824839e-07
Iter: 172 loss: 5.39479345e-07
Iter: 173 loss: 5.39745e-07
Iter: 174 loss: 5.3925487e-07
Iter: 175 loss: 5.38940412e-07
Iter: 176 loss: 5.39789085e-07
Iter: 177 loss: 5.38839288e-07
Iter: 178 loss: 5.38564677e-07
Iter: 179 loss: 5.41293616e-07
Iter: 180 loss: 5.38552968e-07
Iter: 181 loss: 5.38313088e-07
Iter: 182 loss: 5.37893925e-07
Iter: 183 loss: 5.47136324e-07
Iter: 184 loss: 5.37904555e-07
Iter: 185 loss: 5.37660696e-07
Iter: 186 loss: 5.37637447e-07
Iter: 187 loss: 5.37400865e-07
Iter: 188 loss: 5.37178266e-07
Iter: 189 loss: 5.37125288e-07
Iter: 190 loss: 5.36859147e-07
Iter: 191 loss: 5.37497044e-07
Iter: 192 loss: 5.36720108e-07
Iter: 193 loss: 5.36389052e-07
Iter: 194 loss: 5.38192239e-07
Iter: 195 loss: 5.36320613e-07
Iter: 196 loss: 5.36064817e-07
Iter: 197 loss: 5.3566032e-07
Iter: 198 loss: 5.35647e-07
Iter: 199 loss: 5.35162769e-07
Iter: 200 loss: 5.35321703e-07
Iter: 201 loss: 5.34836261e-07
Iter: 202 loss: 5.34321487e-07
Iter: 203 loss: 5.40392534e-07
Iter: 204 loss: 5.34329047e-07
Iter: 205 loss: 5.34041305e-07
Iter: 206 loss: 5.3862334e-07
Iter: 207 loss: 5.34028629e-07
Iter: 208 loss: 5.33807963e-07
Iter: 209 loss: 5.33472644e-07
Iter: 210 loss: 5.33470029e-07
Iter: 211 loss: 5.33258628e-07
Iter: 212 loss: 5.33226171e-07
Iter: 213 loss: 5.3307997e-07
Iter: 214 loss: 5.33195873e-07
Iter: 215 loss: 5.32948093e-07
Iter: 216 loss: 5.32820195e-07
Iter: 217 loss: 5.33010507e-07
Iter: 218 loss: 5.32760964e-07
Iter: 219 loss: 5.32570596e-07
Iter: 220 loss: 5.3348333e-07
Iter: 221 loss: 5.32528759e-07
Iter: 222 loss: 5.32367039e-07
Iter: 223 loss: 5.32288709e-07
Iter: 224 loss: 5.32213221e-07
Iter: 225 loss: 5.32082424e-07
Iter: 226 loss: 5.33994069e-07
Iter: 227 loss: 5.32067816e-07
Iter: 228 loss: 5.31877618e-07
Iter: 229 loss: 5.31629e-07
Iter: 230 loss: 5.31644332e-07
Iter: 231 loss: 5.31406442e-07
Iter: 232 loss: 5.31531668e-07
Iter: 233 loss: 5.3125234e-07
Iter: 234 loss: 5.30904799e-07
Iter: 235 loss: 5.31538547e-07
Iter: 236 loss: 5.30734724e-07
Iter: 237 loss: 5.30470913e-07
Iter: 238 loss: 5.30455793e-07
Iter: 239 loss: 5.30234047e-07
Iter: 240 loss: 5.30421516e-07
Iter: 241 loss: 5.30085629e-07
Iter: 242 loss: 5.29851604e-07
Iter: 243 loss: 5.30020714e-07
Iter: 244 loss: 5.29716317e-07
Iter: 245 loss: 5.2938384e-07
Iter: 246 loss: 5.3087723e-07
Iter: 247 loss: 5.29299484e-07
Iter: 248 loss: 5.29071428e-07
Iter: 249 loss: 5.2914379e-07
Iter: 250 loss: 5.28915734e-07
Iter: 251 loss: 5.28678129e-07
Iter: 252 loss: 5.31861758e-07
Iter: 253 loss: 5.2866352e-07
Iter: 254 loss: 5.2852738e-07
Iter: 255 loss: 5.28375608e-07
Iter: 256 loss: 5.28307623e-07
Iter: 257 loss: 5.28154885e-07
Iter: 258 loss: 5.29666579e-07
Iter: 259 loss: 5.28142323e-07
Iter: 260 loss: 5.27984923e-07
Iter: 261 loss: 5.28318594e-07
Iter: 262 loss: 5.27916598e-07
Iter: 263 loss: 5.27822408e-07
Iter: 264 loss: 5.27701161e-07
Iter: 265 loss: 5.27702e-07
Iter: 266 loss: 5.27508064e-07
Iter: 267 loss: 5.27983843e-07
Iter: 268 loss: 5.27445764e-07
Iter: 269 loss: 5.27287625e-07
Iter: 270 loss: 5.28180124e-07
Iter: 271 loss: 5.27229702e-07
Iter: 272 loss: 5.27039447e-07
Iter: 273 loss: 5.28015335e-07
Iter: 274 loss: 5.27028249e-07
Iter: 275 loss: 5.2689461e-07
Iter: 276 loss: 5.26870622e-07
Iter: 277 loss: 5.26785129e-07
Iter: 278 loss: 5.26600502e-07
Iter: 279 loss: 5.28463261e-07
Iter: 280 loss: 5.26594476e-07
Iter: 281 loss: 5.26461633e-07
Iter: 282 loss: 5.26185147e-07
Iter: 283 loss: 5.26199e-07
Iter: 284 loss: 5.25997848e-07
Iter: 285 loss: 5.25985342e-07
Iter: 286 loss: 5.25839596e-07
Iter: 287 loss: 5.26224085e-07
Iter: 288 loss: 5.25769337e-07
Iter: 289 loss: 5.2569726e-07
Iter: 290 loss: 5.25719202e-07
Iter: 291 loss: 5.25622113e-07
Iter: 292 loss: 5.25457835e-07
Iter: 293 loss: 5.25769906e-07
Iter: 294 loss: 5.25393432e-07
Iter: 295 loss: 5.25184532e-07
Iter: 296 loss: 5.25102848e-07
Iter: 297 loss: 5.25024e-07
Iter: 298 loss: 5.24771281e-07
Iter: 299 loss: 5.247594e-07
Iter: 300 loss: 5.24551638e-07
Iter: 301 loss: 5.24232178e-07
Iter: 302 loss: 5.27155407e-07
Iter: 303 loss: 5.24187158e-07
Iter: 304 loss: 5.24046925e-07
Iter: 305 loss: 5.2402504e-07
Iter: 306 loss: 5.2390368e-07
Iter: 307 loss: 5.23746564e-07
Iter: 308 loss: 5.23731956e-07
Iter: 309 loss: 5.23589165e-07
Iter: 310 loss: 5.23576318e-07
Iter: 311 loss: 5.23439326e-07
Iter: 312 loss: 5.23376571e-07
Iter: 313 loss: 5.23329447e-07
Iter: 314 loss: 5.2321468e-07
Iter: 315 loss: 5.24035443e-07
Iter: 316 loss: 5.23217352e-07
Iter: 317 loss: 5.23115034e-07
Iter: 318 loss: 5.23322e-07
Iter: 319 loss: 5.23078427e-07
Iter: 320 loss: 5.2297122e-07
Iter: 321 loss: 5.22835876e-07
Iter: 322 loss: 5.22856055e-07
Iter: 323 loss: 5.22681262e-07
Iter: 324 loss: 5.24755819e-07
Iter: 325 loss: 5.22683308e-07
Iter: 326 loss: 5.2258531e-07
Iter: 327 loss: 5.22444907e-07
Iter: 328 loss: 5.2242217e-07
Iter: 329 loss: 5.22224241e-07
Iter: 330 loss: 5.22240782e-07
Iter: 331 loss: 5.22081223e-07
Iter: 332 loss: 5.2184788e-07
Iter: 333 loss: 5.23128335e-07
Iter: 334 loss: 5.21808715e-07
Iter: 335 loss: 5.21688776e-07
Iter: 336 loss: 5.21680533e-07
Iter: 337 loss: 5.21570655e-07
Iter: 338 loss: 5.21429115e-07
Iter: 339 loss: 5.21414222e-07
Iter: 340 loss: 5.21261484e-07
Iter: 341 loss: 5.22229243e-07
Iter: 342 loss: 5.21254265e-07
Iter: 343 loss: 5.21090328e-07
Iter: 344 loss: 5.21415245e-07
Iter: 345 loss: 5.2103735e-07
Iter: 346 loss: 5.20876142e-07
Iter: 347 loss: 5.20778372e-07
Iter: 348 loss: 5.20712888e-07
Iter: 349 loss: 5.2052485e-07
Iter: 350 loss: 5.20527863e-07
Iter: 351 loss: 5.20399055e-07
Iter: 352 loss: 5.20143089e-07
Iter: 353 loss: 5.2569294e-07
Iter: 354 loss: 5.20154686e-07
Iter: 355 loss: 5.19915829e-07
Iter: 356 loss: 5.19917705e-07
Iter: 357 loss: 5.19790319e-07
Iter: 358 loss: 5.2000405e-07
Iter: 359 loss: 5.19735e-07
Iter: 360 loss: 5.19631726e-07
Iter: 361 loss: 5.19614332e-07
Iter: 362 loss: 5.19547825e-07
Iter: 363 loss: 5.1938548e-07
Iter: 364 loss: 5.19720743e-07
Iter: 365 loss: 5.1934046e-07
Iter: 366 loss: 5.19258151e-07
Iter: 367 loss: 5.19255138e-07
Iter: 368 loss: 5.19212e-07
Iter: 369 loss: 5.19462105e-07
Iter: 370 loss: 5.19190507e-07
Iter: 371 loss: 5.19134119e-07
Iter: 372 loss: 5.18968648e-07
Iter: 373 loss: 5.21096069e-07
Iter: 374 loss: 5.19009063e-07
Iter: 375 loss: 5.18840693e-07
Iter: 376 loss: 5.21074185e-07
Iter: 377 loss: 5.18859e-07
Iter: 378 loss: 5.18795673e-07
Iter: 379 loss: 5.1863725e-07
Iter: 380 loss: 5.18638217e-07
Iter: 381 loss: 5.18555453e-07
Iter: 382 loss: 5.18553179e-07
Iter: 383 loss: 5.18427555e-07
Iter: 384 loss: 5.1833905e-07
Iter: 385 loss: 5.18303466e-07
Iter: 386 loss: 5.18190745e-07
Iter: 387 loss: 5.18942215e-07
Iter: 388 loss: 5.18190859e-07
Iter: 389 loss: 5.18105708e-07
Iter: 390 loss: 5.18654531e-07
Iter: 391 loss: 5.18081436e-07
Iter: 392 loss: 5.17994522e-07
Iter: 393 loss: 5.17872536e-07
Iter: 394 loss: 5.19952323e-07
Iter: 395 loss: 5.1786833e-07
Iter: 396 loss: 5.17673129e-07
Iter: 397 loss: 5.19105356e-07
Iter: 398 loss: 5.17653234e-07
Iter: 399 loss: 5.17536648e-07
Iter: 400 loss: 5.18110369e-07
Iter: 401 loss: 5.17518515e-07
Iter: 402 loss: 5.17405397e-07
Iter: 403 loss: 5.18411923e-07
Iter: 404 loss: 5.17402e-07
Iter: 405 loss: 5.17291369e-07
Iter: 406 loss: 5.17086335e-07
Iter: 407 loss: 5.20096762e-07
Iter: 408 loss: 5.17097078e-07
Iter: 409 loss: 5.17053309e-07
Iter: 410 loss: 5.16986518e-07
Iter: 411 loss: 5.16898e-07
Iter: 412 loss: 5.16805699e-07
Iter: 413 loss: 5.16799105e-07
Iter: 414 loss: 5.16690591e-07
Iter: 415 loss: 5.17128228e-07
Iter: 416 loss: 5.16684963e-07
Iter: 417 loss: 5.16591058e-07
Iter: 418 loss: 5.17533579e-07
Iter: 419 loss: 5.16577188e-07
Iter: 420 loss: 5.16556895e-07
Iter: 421 loss: 5.1649107e-07
Iter: 422 loss: 5.17866397e-07
Iter: 423 loss: 5.16476575e-07
Iter: 424 loss: 5.16413479e-07
Iter: 425 loss: 5.17491685e-07
Iter: 426 loss: 5.16413195e-07
Iter: 427 loss: 5.16342539e-07
Iter: 428 loss: 5.16254e-07
Iter: 429 loss: 5.16233911e-07
Iter: 430 loss: 5.16124487e-07
Iter: 431 loss: 5.16206285e-07
Iter: 432 loss: 5.16061277e-07
Iter: 433 loss: 5.15906e-07
Iter: 434 loss: 5.16091404e-07
Iter: 435 loss: 5.1582856e-07
Iter: 436 loss: 5.15734314e-07
Iter: 437 loss: 5.15718284e-07
Iter: 438 loss: 5.15620798e-07
Iter: 439 loss: 5.15691738e-07
Iter: 440 loss: 5.15549971e-07
Iter: 441 loss: 5.15448846e-07
Iter: 442 loss: 5.15413149e-07
Iter: 443 loss: 5.15377906e-07
Iter: 444 loss: 5.15313104e-07
Iter: 445 loss: 5.15305373e-07
Iter: 446 loss: 5.1525592e-07
Iter: 447 loss: 5.1517452e-07
Iter: 448 loss: 5.16536886e-07
Iter: 449 loss: 5.15197712e-07
Iter: 450 loss: 5.15125407e-07
Iter: 451 loss: 5.15136e-07
Iter: 452 loss: 5.15056968e-07
Iter: 453 loss: 5.14950216e-07
Iter: 454 loss: 5.1642e-07
Iter: 455 loss: 5.1496545e-07
Iter: 456 loss: 5.1484767e-07
Iter: 457 loss: 5.15059753e-07
Iter: 458 loss: 5.14809187e-07
Iter: 459 loss: 5.14679243e-07
Iter: 460 loss: 5.16079865e-07
Iter: 461 loss: 5.14685951e-07
Iter: 462 loss: 5.14633655e-07
Iter: 463 loss: 5.14525254e-07
Iter: 464 loss: 5.14525141e-07
Iter: 465 loss: 5.14406622e-07
Iter: 466 loss: 5.14370925e-07
Iter: 467 loss: 5.14309477e-07
Iter: 468 loss: 5.14193118e-07
Iter: 469 loss: 5.15048555e-07
Iter: 470 loss: 5.14177884e-07
Iter: 471 loss: 5.14122064e-07
Iter: 472 loss: 5.14111036e-07
Iter: 473 loss: 5.14064766e-07
Iter: 474 loss: 5.14078806e-07
Iter: 475 loss: 5.14052488e-07
Iter: 476 loss: 5.13967848e-07
Iter: 477 loss: 5.13870816e-07
Iter: 478 loss: 5.13876216e-07
Iter: 479 loss: 5.13779923e-07
Iter: 480 loss: 5.14262069e-07
Iter: 481 loss: 5.13756504e-07
Iter: 482 loss: 5.13667487e-07
Iter: 483 loss: 5.14723297e-07
Iter: 484 loss: 5.13683631e-07
Iter: 485 loss: 5.1359018e-07
Iter: 486 loss: 5.13674308e-07
Iter: 487 loss: 5.13547093e-07
Iter: 488 loss: 5.13461032e-07
Iter: 489 loss: 5.13571e-07
Iter: 490 loss: 5.13442956e-07
Iter: 491 loss: 5.13369059e-07
Iter: 492 loss: 5.13772363e-07
Iter: 493 loss: 5.13359396e-07
Iter: 494 loss: 5.1332745e-07
Iter: 495 loss: 5.13240877e-07
Iter: 496 loss: 5.13249e-07
Iter: 497 loss: 5.13182272e-07
Iter: 498 loss: 5.13796294e-07
Iter: 499 loss: 5.13193527e-07
Iter: 500 loss: 5.13108375e-07
Iter: 501 loss: 5.13278792e-07
Iter: 502 loss: 5.13075292e-07
Iter: 503 loss: 5.13010093e-07
Iter: 504 loss: 5.13017312e-07
Iter: 505 loss: 5.12994632e-07
Iter: 506 loss: 5.12920792e-07
Iter: 507 loss: 5.12930797e-07
Iter: 508 loss: 5.12861902e-07
Iter: 509 loss: 5.12776239e-07
Iter: 510 loss: 5.12794372e-07
Iter: 511 loss: 5.1275731e-07
Iter: 512 loss: 5.12698193e-07
Iter: 513 loss: 5.13352745e-07
Iter: 514 loss: 5.12669772e-07
Iter: 515 loss: 5.12598547e-07
Iter: 516 loss: 5.12840074e-07
Iter: 517 loss: 5.12542e-07
Iter: 518 loss: 5.12507881e-07
Iter: 519 loss: 5.12474969e-07
Iter: 520 loss: 5.12442966e-07
Iter: 521 loss: 5.12400334e-07
Iter: 522 loss: 5.12369354e-07
Iter: 523 loss: 5.12314728e-07
Iter: 524 loss: 5.12849056e-07
Iter: 525 loss: 5.12311658e-07
Iter: 526 loss: 5.12200472e-07
Iter: 527 loss: 5.12189672e-07
Iter: 528 loss: 5.12119357e-07
Iter: 529 loss: 5.12036081e-07
Iter: 530 loss: 5.12145164e-07
Iter: 531 loss: 5.12012434e-07
Iter: 532 loss: 5.11903181e-07
Iter: 533 loss: 5.12264251e-07
Iter: 534 loss: 5.11886356e-07
Iter: 535 loss: 5.11796088e-07
Iter: 536 loss: 5.12713086e-07
Iter: 537 loss: 5.1179876e-07
Iter: 538 loss: 5.11752489e-07
Iter: 539 loss: 5.11698488e-07
Iter: 540 loss: 5.12948873e-07
Iter: 541 loss: 5.11701046e-07
Iter: 542 loss: 5.11655855e-07
Iter: 543 loss: 5.12441602e-07
Iter: 544 loss: 5.11648864e-07
Iter: 545 loss: 5.11600206e-07
Iter: 546 loss: 5.11759652e-07
Iter: 547 loss: 5.11568089e-07
Iter: 548 loss: 5.11544158e-07
Iter: 549 loss: 5.11464e-07
Iter: 550 loss: 5.11477765e-07
Iter: 551 loss: 5.11391477e-07
Iter: 552 loss: 5.11332928e-07
Iter: 553 loss: 5.11306894e-07
Iter: 554 loss: 5.11190592e-07
Iter: 555 loss: 5.12287897e-07
Iter: 556 loss: 5.11201165e-07
Iter: 557 loss: 5.11142161e-07
Iter: 558 loss: 5.11144094e-07
Iter: 559 loss: 5.11103906e-07
Iter: 560 loss: 5.11059966e-07
Iter: 561 loss: 5.12091447e-07
Iter: 562 loss: 5.11048484e-07
Iter: 563 loss: 5.11015571e-07
Iter: 564 loss: 5.11006419e-07
Iter: 565 loss: 5.11001303e-07
Iter: 566 loss: 5.10933774e-07
Iter: 567 loss: 5.11940812e-07
Iter: 568 loss: 5.10929908e-07
Iter: 569 loss: 5.10865e-07
Iter: 570 loss: 5.10938094e-07
Iter: 571 loss: 5.10832592e-07
Iter: 572 loss: 5.10760117e-07
Iter: 573 loss: 5.11052463e-07
Iter: 574 loss: 5.10750738e-07
Iter: 575 loss: 5.10676728e-07
Iter: 576 loss: 5.11123062e-07
Iter: 577 loss: 5.1068082e-07
Iter: 578 loss: 5.10613518e-07
Iter: 579 loss: 5.10603627e-07
Iter: 580 loss: 5.1053803e-07
Iter: 581 loss: 5.10471068e-07
Iter: 582 loss: 5.10525865e-07
Iter: 583 loss: 5.10435939e-07
Iter: 584 loss: 5.10381199e-07
Iter: 585 loss: 5.10384268e-07
Iter: 586 loss: 5.10299913e-07
Iter: 587 loss: 5.10318898e-07
Iter: 588 loss: 5.10263362e-07
Iter: 589 loss: 5.10239943e-07
Iter: 590 loss: 5.10150073e-07
Iter: 591 loss: 5.10129439e-07
Iter: 592 loss: 5.10069412e-07
Iter: 593 loss: 5.10077143e-07
Iter: 594 loss: 5.09999381e-07
Iter: 595 loss: 5.10044856e-07
Iter: 596 loss: 5.09944073e-07
Iter: 597 loss: 5.09882568e-07
Iter: 598 loss: 5.09876486e-07
Iter: 599 loss: 5.09845506e-07
Iter: 600 loss: 5.09776214e-07
Iter: 601 loss: 5.09782524e-07
Iter: 602 loss: 5.0973631e-07
Iter: 603 loss: 5.09664233e-07
Iter: 604 loss: 5.10533823e-07
Iter: 605 loss: 5.09641495e-07
Iter: 606 loss: 5.09549636e-07
Iter: 607 loss: 5.09961524e-07
Iter: 608 loss: 5.09539518e-07
Iter: 609 loss: 5.09504e-07
Iter: 610 loss: 5.1036659e-07
Iter: 611 loss: 5.09502229e-07
Iter: 612 loss: 5.09461643e-07
Iter: 613 loss: 5.09423614e-07
Iter: 614 loss: 5.09413212e-07
Iter: 615 loss: 5.09355232e-07
Iter: 616 loss: 5.09296683e-07
Iter: 617 loss: 5.09275878e-07
Iter: 618 loss: 5.09214544e-07
Iter: 619 loss: 5.09194e-07
Iter: 620 loss: 5.09141159e-07
Iter: 621 loss: 5.09059817e-07
Iter: 622 loss: 5.09053848e-07
Iter: 623 loss: 5.0896341e-07
Iter: 624 loss: 5.0907164e-07
Iter: 625 loss: 5.08892185e-07
Iter: 626 loss: 5.0882079e-07
Iter: 627 loss: 5.08807659e-07
Iter: 628 loss: 5.087727e-07
Iter: 629 loss: 5.08758944e-07
Iter: 630 loss: 5.08734161e-07
Iter: 631 loss: 5.08685616e-07
Iter: 632 loss: 5.08841538e-07
Iter: 633 loss: 5.08677829e-07
Iter: 634 loss: 5.0860325e-07
Iter: 635 loss: 5.08797257e-07
Iter: 636 loss: 5.08608423e-07
Iter: 637 loss: 5.08546179e-07
Iter: 638 loss: 5.08507355e-07
Iter: 639 loss: 5.08497806e-07
Iter: 640 loss: 5.08428172e-07
Iter: 641 loss: 5.08392304e-07
Iter: 642 loss: 5.08362689e-07
Iter: 643 loss: 5.08267306e-07
Iter: 644 loss: 5.08951928e-07
Iter: 645 loss: 5.08270318e-07
Iter: 646 loss: 5.0817448e-07
Iter: 647 loss: 5.09146503e-07
Iter: 648 loss: 5.08176129e-07
Iter: 649 loss: 5.08123549e-07
Iter: 650 loss: 5.08052949e-07
Iter: 651 loss: 5.0804e-07
Iter: 652 loss: 5.07968934e-07
Iter: 653 loss: 5.08620417e-07
Iter: 654 loss: 5.07963421e-07
Iter: 655 loss: 5.07933e-07
Iter: 656 loss: 5.08505195e-07
Iter: 657 loss: 5.07918344e-07
Iter: 658 loss: 5.07906407e-07
Iter: 659 loss: 5.07826485e-07
Iter: 660 loss: 5.08243261e-07
Iter: 661 loss: 5.07803e-07
Iter: 662 loss: 5.07705181e-07
Iter: 663 loss: 5.08381163e-07
Iter: 664 loss: 5.0771763e-07
Iter: 665 loss: 5.07628329e-07
Iter: 666 loss: 5.08322444e-07
Iter: 667 loss: 5.07612754e-07
Iter: 668 loss: 5.07541e-07
Iter: 669 loss: 5.07499294e-07
Iter: 670 loss: 5.07474567e-07
Iter: 671 loss: 5.0740914e-07
Iter: 672 loss: 5.0742949e-07
Iter: 673 loss: 5.07335926e-07
Iter: 674 loss: 5.07292e-07
Iter: 675 loss: 5.07269078e-07
Iter: 676 loss: 5.07201e-07
Iter: 677 loss: 5.07173922e-07
Iter: 678 loss: 5.07123445e-07
Iter: 679 loss: 5.07027039e-07
Iter: 680 loss: 5.07869402e-07
Iter: 681 loss: 5.07031473e-07
Iter: 682 loss: 5.06965e-07
Iter: 683 loss: 5.07043922e-07
Iter: 684 loss: 5.06934953e-07
Iter: 685 loss: 5.06897436e-07
Iter: 686 loss: 5.0691267e-07
Iter: 687 loss: 5.06846163e-07
Iter: 688 loss: 5.06766241e-07
Iter: 689 loss: 5.07847517e-07
Iter: 690 loss: 5.06765502e-07
Iter: 691 loss: 5.06656761e-07
Iter: 692 loss: 5.06898e-07
Iter: 693 loss: 5.06650679e-07
Iter: 694 loss: 5.06597132e-07
Iter: 695 loss: 5.06578886e-07
Iter: 696 loss: 5.06558308e-07
Iter: 697 loss: 5.06509366e-07
Iter: 698 loss: 5.06544097e-07
Iter: 699 loss: 5.06462698e-07
Iter: 700 loss: 5.06668812e-07
Iter: 701 loss: 5.06453205e-07
Iter: 702 loss: 5.06377432e-07
Iter: 703 loss: 5.06730657e-07
Iter: 704 loss: 5.06369929e-07
Iter: 705 loss: 5.06353103e-07
Iter: 706 loss: 5.06356685e-07
Iter: 707 loss: 5.06321612e-07
Iter: 708 loss: 5.06261699e-07
Iter: 709 loss: 5.06289666e-07
Iter: 710 loss: 5.06227934e-07
Iter: 711 loss: 5.0619235e-07
Iter: 712 loss: 5.06130277e-07
Iter: 713 loss: 5.06149036e-07
Iter: 714 loss: 5.06025572e-07
Iter: 715 loss: 5.06207641e-07
Iter: 716 loss: 5.05988965e-07
Iter: 717 loss: 5.05895116e-07
Iter: 718 loss: 5.06141e-07
Iter: 719 loss: 5.05829632e-07
Iter: 720 loss: 5.05765399e-07
Iter: 721 loss: 5.06552283e-07
Iter: 722 loss: 5.05739081e-07
Iter: 723 loss: 5.05681e-07
Iter: 724 loss: 5.06145682e-07
Iter: 725 loss: 5.05660751e-07
Iter: 726 loss: 5.05600156e-07
Iter: 727 loss: 5.0557685e-07
Iter: 728 loss: 5.0557162e-07
Iter: 729 loss: 5.05482944e-07
Iter: 730 loss: 5.05669959e-07
Iter: 731 loss: 5.0547817e-07
Iter: 732 loss: 5.05446849e-07
Iter: 733 loss: 5.05449464e-07
Iter: 734 loss: 5.0543872e-07
Iter: 735 loss: 5.05363346e-07
Iter: 736 loss: 5.06016193e-07
Iter: 737 loss: 5.05365733e-07
Iter: 738 loss: 5.05344701e-07
Iter: 739 loss: 5.05315938e-07
Iter: 740 loss: 5.05300136e-07
Iter: 741 loss: 5.05266257e-07
Iter: 742 loss: 5.06120671e-07
Iter: 743 loss: 5.05244543e-07
Iter: 744 loss: 5.05184744e-07
Iter: 745 loss: 5.05625621e-07
Iter: 746 loss: 5.051686e-07
Iter: 747 loss: 5.05142111e-07
Iter: 748 loss: 5.05251933e-07
Iter: 749 loss: 5.05095784e-07
Iter: 750 loss: 5.05082085e-07
Iter: 751 loss: 5.05054231e-07
Iter: 752 loss: 5.05056107e-07
Iter: 753 loss: 5.04983177e-07
Iter: 754 loss: 5.05152457e-07
Iter: 755 loss: 5.04974594e-07
Iter: 756 loss: 5.04938839e-07
Iter: 757 loss: 5.05027856e-07
Iter: 758 loss: 5.0494009e-07
Iter: 759 loss: 5.04882792e-07
Iter: 760 loss: 5.05153594e-07
Iter: 761 loss: 5.04859827e-07
Iter: 762 loss: 5.04817194e-07
Iter: 763 loss: 5.04778257e-07
Iter: 764 loss: 5.04782747e-07
Iter: 765 loss: 5.04723289e-07
Iter: 766 loss: 5.05080266e-07
Iter: 767 loss: 5.04724255e-07
Iter: 768 loss: 5.04661898e-07
Iter: 769 loss: 5.04895e-07
Iter: 770 loss: 5.0467e-07
Iter: 771 loss: 5.04615286e-07
Iter: 772 loss: 5.04567083e-07
Iter: 773 loss: 5.04539287e-07
Iter: 774 loss: 5.04524451e-07
Iter: 775 loss: 5.04522632e-07
Iter: 776 loss: 5.04485286e-07
Iter: 777 loss: 5.0446863e-07
Iter: 778 loss: 5.04449304e-07
Iter: 779 loss: 5.04442426e-07
Iter: 780 loss: 5.04440152e-07
Iter: 781 loss: 5.04415766e-07
Iter: 782 loss: 5.04382569e-07
Iter: 783 loss: 5.04375407e-07
Iter: 784 loss: 5.04343348e-07
Iter: 785 loss: 5.0443731e-07
Iter: 786 loss: 5.04336867e-07
Iter: 787 loss: 5.04312766e-07
Iter: 788 loss: 5.04290483e-07
Iter: 789 loss: 5.04273316e-07
Iter: 790 loss: 5.04220679e-07
Iter: 791 loss: 5.04527179e-07
Iter: 792 loss: 5.04206469e-07
Iter: 793 loss: 5.0416088e-07
Iter: 794 loss: 5.04370803e-07
Iter: 795 loss: 5.04164404e-07
Iter: 796 loss: 5.04114553e-07
Iter: 797 loss: 5.04084255e-07
Iter: 798 loss: 5.04064531e-07
Iter: 799 loss: 5.04046227e-07
Iter: 800 loss: 5.04021557e-07
Iter: 801 loss: 5.04007744e-07
Iter: 802 loss: 5.03975855e-07
Iter: 803 loss: 5.03979663e-07
Iter: 804 loss: 5.03947945e-07
Iter: 805 loss: 5.04165371e-07
Iter: 806 loss: 5.03936178e-07
Iter: 807 loss: 5.0392174e-07
Iter: 808 loss: 5.0396784e-07
Iter: 809 loss: 5.03918727e-07
Iter: 810 loss: 5.03901731e-07
Iter: 811 loss: 5.03929e-07
Iter: 812 loss: 5.03882916e-07
Iter: 813 loss: 5.03877e-07
Iter: 814 loss: 5.03947092e-07
Iter: 815 loss: 5.0387365e-07
Iter: 816 loss: 5.03836304e-07
Iter: 817 loss: 5.0380811e-07
Iter: 818 loss: 5.04597836e-07
Iter: 819 loss: 5.0381044e-07
Iter: 820 loss: 5.03755359e-07
Iter: 821 loss: 5.03992112e-07
Iter: 822 loss: 5.03741376e-07
Iter: 823 loss: 5.03705223e-07
Iter: 824 loss: 5.03707952e-07
Iter: 825 loss: 5.03689535e-07
Iter: 826 loss: 5.03615524e-07
Iter: 827 loss: 5.03724948e-07
Iter: 828 loss: 5.03601939e-07
Iter: 829 loss: 5.03550496e-07
Iter: 830 loss: 5.03725516e-07
Iter: 831 loss: 5.03537478e-07
Iter: 832 loss: 5.03515651e-07
Iter: 833 loss: 5.03799185e-07
Iter: 834 loss: 5.03500473e-07
Iter: 835 loss: 5.03465287e-07
Iter: 836 loss: 5.03606657e-07
Iter: 837 loss: 5.03465742e-07
Iter: 838 loss: 5.03429646e-07
Iter: 839 loss: 5.03387184e-07
Iter: 840 loss: 5.04608806e-07
Iter: 841 loss: 5.03378033e-07
Iter: 842 loss: 5.03299702e-07
Iter: 843 loss: 5.03301806e-07
Iter: 844 loss: 5.03292426e-07
Iter: 845 loss: 5.03276965e-07
Iter: 846 loss: 5.03245587e-07
Iter: 847 loss: 5.03210913e-07
Iter: 848 loss: 5.03440504e-07
Iter: 849 loss: 5.03184651e-07
Iter: 850 loss: 5.03132355e-07
Iter: 851 loss: 5.03117121e-07
Iter: 852 loss: 5.03107344e-07
Iter: 853 loss: 5.03061358e-07
Iter: 854 loss: 5.03459773e-07
Iter: 855 loss: 5.03044134e-07
Iter: 856 loss: 5.03008494e-07
Iter: 857 loss: 5.03049534e-07
Iter: 858 loss: 5.02967055e-07
Iter: 859 loss: 5.02923626e-07
Iter: 860 loss: 5.02941191e-07
Iter: 861 loss: 5.02901116e-07
Iter: 862 loss: 5.02851435e-07
Iter: 863 loss: 5.03474553e-07
Iter: 864 loss: 5.02850298e-07
Iter: 865 loss: 5.02822672e-07
Iter: 866 loss: 5.02794535e-07
Iter: 867 loss: 5.02791863e-07
Iter: 868 loss: 5.02723083e-07
Iter: 869 loss: 5.03262072e-07
Iter: 870 loss: 5.02742296e-07
Iter: 871 loss: 5.02710805e-07
Iter: 872 loss: 5.02744228e-07
Iter: 873 loss: 5.02681246e-07
Iter: 874 loss: 5.02662942e-07
Iter: 875 loss: 5.02622811e-07
Iter: 876 loss: 5.02623493e-07
Iter: 877 loss: 5.02584271e-07
Iter: 878 loss: 5.02576711e-07
Iter: 879 loss: 5.02546584e-07
Iter: 880 loss: 5.02551757e-07
Iter: 881 loss: 5.02531407e-07
Iter: 882 loss: 5.02508385e-07
Iter: 883 loss: 5.02900434e-07
Iter: 884 loss: 5.02500257e-07
Iter: 885 loss: 5.02477519e-07
Iter: 886 loss: 5.02464673e-07
Iter: 887 loss: 5.02486273e-07
Iter: 888 loss: 5.02431135e-07
Iter: 889 loss: 5.02459955e-07
Iter: 890 loss: 5.02402486e-07
Iter: 891 loss: 5.02370369e-07
Iter: 892 loss: 5.02766738e-07
Iter: 893 loss: 5.02353657e-07
Iter: 894 loss: 5.02341322e-07
Iter: 895 loss: 5.0229238e-07
Iter: 896 loss: 5.0227743e-07
Iter: 897 loss: 5.0220541e-07
Iter: 898 loss: 5.02605076e-07
Iter: 899 loss: 5.02205e-07
Iter: 900 loss: 5.02165335e-07
Iter: 901 loss: 5.02232524e-07
Iter: 902 loss: 5.02124692e-07
Iter: 903 loss: 5.02089108e-07
Iter: 904 loss: 5.02066939e-07
Iter: 905 loss: 5.02065177e-07
Iter: 906 loss: 5.02038517e-07
Iter: 907 loss: 5.02029138e-07
Iter: 908 loss: 5.01989803e-07
Iter: 909 loss: 5.0228283e-07
Iter: 910 loss: 5.01979571e-07
Iter: 911 loss: 5.01946943e-07
Iter: 912 loss: 5.02121679e-07
Iter: 913 loss: 5.01944101e-07
Iter: 914 loss: 5.01903457e-07
Iter: 915 loss: 5.0193853e-07
Iter: 916 loss: 5.01902718e-07
Iter: 917 loss: 5.01867817e-07
Iter: 918 loss: 5.02028797e-07
Iter: 919 loss: 5.01862e-07
Iter: 920 loss: 5.01848376e-07
Iter: 921 loss: 5.01815407e-07
Iter: 922 loss: 5.01818874e-07
Iter: 923 loss: 5.0176e-07
Iter: 924 loss: 5.0222269e-07
Iter: 925 loss: 5.01789089e-07
Iter: 926 loss: 5.01743443e-07
Iter: 927 loss: 5.01793e-07
Iter: 928 loss: 5.01717523e-07
Iter: 929 loss: 5.01698821e-07
Iter: 930 loss: 5.01670115e-07
Iter: 931 loss: 5.01658121e-07
Iter: 932 loss: 5.01621344e-07
Iter: 933 loss: 5.01952513e-07
Iter: 934 loss: 5.01614295e-07
Iter: 935 loss: 5.01564841e-07
Iter: 936 loss: 5.01778345e-07
Iter: 937 loss: 5.01575528e-07
Iter: 938 loss: 5.01504644e-07
Iter: 939 loss: 5.01547049e-07
Iter: 940 loss: 5.01502825e-07
Iter: 941 loss: 5.01456498e-07
Iter: 942 loss: 5.01544775e-07
Iter: 943 loss: 5.01457578e-07
Iter: 944 loss: 5.0142927e-07
Iter: 945 loss: 5.01878276e-07
Iter: 946 loss: 5.01408067e-07
Iter: 947 loss: 5.01402269e-07
Iter: 948 loss: 5.01375325e-07
Iter: 949 loss: 5.01369414e-07
Iter: 950 loss: 5.01344857e-07
Iter: 951 loss: 5.01859176e-07
Iter: 952 loss: 5.01342583e-07
Iter: 953 loss: 5.01309671e-07
Iter: 954 loss: 5.01237821e-07
Iter: 955 loss: 5.01231511e-07
Iter: 956 loss: 5.01211105e-07
Iter: 957 loss: 5.01457748e-07
Iter: 958 loss: 5.01193369e-07
Iter: 959 loss: 5.01134309e-07
Iter: 960 loss: 5.01353838e-07
Iter: 961 loss: 5.01128397e-07
Iter: 962 loss: 5.0108406e-07
Iter: 963 loss: 5.01127431e-07
Iter: 964 loss: 5.01103955e-07
Iter: 965 loss: 5.01032787e-07
Iter: 966 loss: 5.01066211e-07
Iter: 967 loss: 5.01020963e-07
Iter: 968 loss: 5.00993451e-07
Iter: 969 loss: 5.00987653e-07
Iter: 970 loss: 5.00962756e-07
Iter: 971 loss: 5.01102932e-07
Iter: 972 loss: 5.0095241e-07
Iter: 973 loss: 5.00942576e-07
Iter: 974 loss: 5.00918e-07
Iter: 975 loss: 5.00900512e-07
Iter: 976 loss: 5.00878343e-07
Iter: 977 loss: 5.00890906e-07
Iter: 978 loss: 5.00836e-07
Iter: 979 loss: 5.008485e-07
Iter: 980 loss: 5.00835199e-07
Iter: 981 loss: 5.00815077e-07
Iter: 982 loss: 5.00880731e-07
Iter: 983 loss: 5.00789724e-07
Iter: 984 loss: 5.00789156e-07
Iter: 985 loss: 5.00880788e-07
Iter: 986 loss: 5.00761189e-07
Iter: 987 loss: 5.00756641e-07
Iter: 988 loss: 5.00715828e-07
Iter: 989 loss: 5.00701447e-07
Iter: 990 loss: 5.00676038e-07
Iter: 991 loss: 5.00694512e-07
Iter: 992 loss: 5.0066285e-07
Iter: 993 loss: 5.00645797e-07
Iter: 994 loss: 5.00636872e-07
Iter: 995 loss: 5.00601516e-07
Iter: 996 loss: 5.00567182e-07
Iter: 997 loss: 5.00564397e-07
Iter: 998 loss: 5.00514489e-07
Iter: 999 loss: 5.00817919e-07
Iter: 1000 loss: 5.00495617e-07
Iter: 1001 loss: 5.00478905e-07
Iter: 1002 loss: 5.00814622e-07
Iter: 1003 loss: 5.0049448e-07
Iter: 1004 loss: 5.00448095e-07
Iter: 1005 loss: 5.0043451e-07
Iter: 1006 loss: 5.00427745e-07
Iter: 1007 loss: 5.00392048e-07
Iter: 1008 loss: 5.00659667e-07
Iter: 1009 loss: 5.00384e-07
Iter: 1010 loss: 5.00335091e-07
Iter: 1011 loss: 5.00524948e-07
Iter: 1012 loss: 5.00350495e-07
Iter: 1013 loss: 5.00337649e-07
Iter: 1014 loss: 5.00329293e-07
Iter: 1015 loss: 5.00318492e-07
Iter: 1016 loss: 5.00296892e-07
Iter: 1017 loss: 5.00282681e-07
Iter: 1018 loss: 5.00265287e-07
Iter: 1019 loss: 5.00245108e-07
Iter: 1020 loss: 5.00247836e-07
Iter: 1021 loss: 5.00205658e-07
Iter: 1022 loss: 5.00337364e-07
Iter: 1023 loss: 5.00202191e-07
Iter: 1024 loss: 5.0019014e-07
Iter: 1025 loss: 5.00173542e-07
Iter: 1026 loss: 5.00160297e-07
Iter: 1027 loss: 5.00146825e-07
Iter: 1028 loss: 5.00219642e-07
Iter: 1029 loss: 5.00128351e-07
Iter: 1030 loss: 5.00085207e-07
Iter: 1031 loss: 5.00073952e-07
Iter: 1032 loss: 5.00046e-07
Iter: 1033 loss: 5.00010515e-07
Iter: 1034 loss: 5.00101578e-07
Iter: 1035 loss: 4.9998954e-07
Iter: 1036 loss: 4.99963392e-07
Iter: 1037 loss: 4.99979933e-07
Iter: 1038 loss: 4.99943951e-07
Iter: 1039 loss: 4.99985958e-07
Iter: 1040 loss: 4.99927523e-07
Iter: 1041 loss: 4.99930252e-07
Iter: 1042 loss: 4.99948e-07
Iter: 1043 loss: 4.99935197e-07
Iter: 1044 loss: 4.99932e-07
Iter: 1045 loss: 4.99946623e-07
Iter: 1046 loss: 4.99942416e-07
Iter: 1047 loss: 4.99936675e-07
Iter: 1048 loss: 4.99932753e-07
Iter: 1049 loss: 4.99933833e-07
Iter: 1050 loss: 4.99935823e-07
Iter: 1051 loss: 4.99925704e-07
Iter: 1052 loss: 4.99935368e-07
Iter: 1053 loss: 4.99930422e-07
Iter: 1054 loss: 4.99927864e-07
Iter: 1055 loss: 4.99927864e-07
Iter: 1056 loss: 4.99929229e-07
Iter: 1057 loss: 4.99929797e-07
Iter: 1058 loss: 4.99932526e-07
Iter: 1059 loss: 4.99931161e-07
Iter: 1060 loss: 4.99929911e-07
Iter: 1061 loss: 4.99928092e-07
Iter: 1062 loss: 4.99927864e-07
Iter: 1063 loss: 4.99927864e-07
Iter: 1064 loss: 4.99928035e-07
Iter: 1065 loss: 4.99928035e-07
Iter: 1066 loss: 4.99928092e-07
Iter: 1067 loss: 4.99928035e-07
Iter: 1068 loss: 4.99928092e-07
Iter: 1069 loss: 4.99928e-07
Iter: 1070 loss: 4.99928e-07
Iter: 1071 loss: 4.99928e-07
Iter: 1072 loss: 4.99928e-07
Iter: 1073 loss: 4.99928e-07
Iter: 1074 loss: 4.99928092e-07
Iter: 1075 loss: 4.99928e-07
Iter: 1076 loss: 8563.2207
Iter: 1077 loss: 4.99928e-07
Iter: 1078 loss: 0.0526491
Iter: 1079 loss: 4.9992525e-07
Iter: 1080 loss: 4.99911266e-07
Iter: 1081 loss: 4.99918656e-07
Iter: 1082 loss: 4.99892792e-07
Iter: 1083 loss: 5.00011254e-07
Iter: 1084 loss: 4.9988239e-07
Iter: 1085 loss: 4.99871192e-07
Iter: 1086 loss: 4.99838677e-07
Iter: 1087 loss: 4.99846351e-07
Iter: 1088 loss: 4.99791554e-07
Iter: 1089 loss: 4.9980531e-07
Iter: 1090 loss: 4.99755e-07
Iter: 1091 loss: 4.99759892e-07
Iter: 1092 loss: 4.99699922e-07
Iter: 1093 loss: 4.99689349e-07
Iter: 1094 loss: 4.99765292e-07
Iter: 1095 loss: 4.99662121e-07
Iter: 1096 loss: 4.99631597e-07
Iter: 1097 loss: 4.99917803e-07
Iter: 1098 loss: 4.99608859e-07
Iter: 1099 loss: 4.9959948e-07
Iter: 1100 loss: 4.99588509e-07
Iter: 1101 loss: 4.99552755e-07
Iter: 1102 loss: 4.99546445e-07
Iter: 1103 loss: 4.99545081e-07
Iter: 1104 loss: 4.99507848e-07
Iter: 1105 loss: 4.99769385e-07
Iter: 1106 loss: 4.99493e-07
Iter: 1107 loss: 4.9947846e-07
Iter: 1108 loss: 4.9962739e-07
Iter: 1109 loss: 4.99471753e-07
Iter: 1110 loss: 4.99462885e-07
Iter: 1111 loss: 4.99395298e-07
Iter: 1112 loss: 4.99963789e-07
Iter: 1113 loss: 4.99408429e-07
Iter: 1114 loss: 4.99391206e-07
Iter: 1115 loss: 4.99860334e-07
Iter: 1116 loss: 4.99387e-07
Iter: 1117 loss: 4.99344e-07
Iter: 1118 loss: 4.99408429e-07
Iter: 1119 loss: 4.99327086e-07
Iter: 1120 loss: 4.99303553e-07
Iter: 1121 loss: 4.99279849e-07
Iter: 1122 loss: 4.99243413e-07
Iter: 1123 loss: 4.99234773e-07
Iter: 1124 loss: 4.99559235e-07
Iter: 1125 loss: 4.99231874e-07
Iter: 1126 loss: 4.9916946e-07
Iter: 1127 loss: 4.99478233e-07
Iter: 1128 loss: 4.99173495e-07
Iter: 1129 loss: 4.99167356e-07
Iter: 1130 loss: 4.99134728e-07
Iter: 1131 loss: 4.99130351e-07
Iter: 1132 loss: 4.99107955e-07
Iter: 1133 loss: 4.99232897e-07
Iter: 1134 loss: 4.99083853e-07
Iter: 1135 loss: 4.99060661e-07
Iter: 1136 loss: 4.99149792e-07
Iter: 1137 loss: 4.99060604e-07
Iter: 1138 loss: 4.99047815e-07
Iter: 1139 loss: 4.99021553e-07
Iter: 1140 loss: 4.99024793e-07
Iter: 1141 loss: 4.98983923e-07
Iter: 1142 loss: 4.98983809e-07
Iter: 1143 loss: 4.98941404e-07
Iter: 1144 loss: 4.98881604e-07
Iter: 1145 loss: 4.988716e-07
Iter: 1146 loss: 4.98857162e-07
Iter: 1147 loss: 4.99015869e-07
Iter: 1148 loss: 4.98819361e-07
Iter: 1149 loss: 4.98796908e-07
Iter: 1150 loss: 4.98774796e-07
Iter: 1151 loss: 4.98756094e-07
Iter: 1152 loss: 4.98721363e-07
Iter: 1153 loss: 4.99688781e-07
Iter: 1154 loss: 4.98722e-07
Iter: 1155 loss: 4.98660256e-07
Iter: 1156 loss: 4.98814643e-07
Iter: 1157 loss: 4.98669749e-07
Iter: 1158 loss: 4.98618419e-07
Iter: 1159 loss: 4.98607733e-07
Iter: 1160 loss: 4.98584e-07
Iter: 1161 loss: 4.9856925e-07
Iter: 1162 loss: 4.98577606e-07
Iter: 1163 loss: 4.98560951e-07
Iter: 1164 loss: 4.98514453e-07
Iter: 1165 loss: 4.98513771e-07
Iter: 1166 loss: 4.98470285e-07
Iter: 1167 loss: 4.98670943e-07
Iter: 1168 loss: 4.98486429e-07
Iter: 1169 loss: 4.98438908e-07
Iter: 1170 loss: 4.98494387e-07
Iter: 1171 loss: 4.98443626e-07
Iter: 1172 loss: 4.98421e-07
Iter: 1173 loss: 4.98441409e-07
Iter: 1174 loss: 4.98383883e-07
Iter: 1175 loss: 4.98374334e-07
Iter: 1176 loss: 4.98375584e-07
Iter: 1177 loss: 4.98365523e-07
Iter: 1178 loss: 4.98319537e-07
Iter: 1179 loss: 4.98311181e-07
Iter: 1180 loss: 4.98280258e-07
Iter: 1181 loss: 4.98278325e-07
Iter: 1182 loss: 4.98239217e-07
Iter: 1183 loss: 4.98247687e-07
Iter: 1184 loss: 4.98219492e-07
Iter: 1185 loss: 4.98210738e-07
Iter: 1186 loss: 4.98156567e-07
Iter: 1187 loss: 4.98164809e-07
Iter: 1188 loss: 4.98123882e-07
Iter: 1189 loss: 4.98107795e-07
Iter: 1190 loss: 4.98079885e-07
Iter: 1191 loss: 4.98040322e-07
Iter: 1192 loss: 4.98321356e-07
Iter: 1193 loss: 4.98032762e-07
Iter: 1194 loss: 4.98007239e-07
Iter: 1195 loss: 4.97999395e-07
Iter: 1196 loss: 4.97992175e-07
Iter: 1197 loss: 4.97970063e-07
Iter: 1198 loss: 4.97962105e-07
Iter: 1199 loss: 4.97974895e-07
Iter: 1200 loss: 4.9796904e-07
Iter: 1201 loss: 4.97971541e-07
Iter: 1202 loss: 4.9797228e-07
Iter: 1203 loss: 4.97965061e-07
Iter: 1204 loss: 4.97977965e-07
Iter: 1205 loss: 4.97959604e-07
Iter: 1206 loss: 4.97963242e-07
Iter: 1207 loss: 4.97955796e-07
Iter: 1208 loss: 4.97960229e-07
Iter: 1209 loss: 4.97957444e-07
Iter: 1210 loss: 4.97961423e-07
Iter: 1211 loss: 4.97969893e-07
Iter: 1212 loss: 4.97964038e-07
Iter: 1213 loss: 4.97964947e-07
Iter: 1214 loss: 4.97964891e-07
Iter: 1215 loss: 4.97964493e-07
Iter: 1216 loss: 4.97965118e-07
Iter: 1217 loss: 4.97963e-07
Iter: 1218 loss: 4.97962674e-07
Iter: 1219 loss: 4.97962503e-07
Iter: 1220 loss: 4.97962333e-07
Iter: 1221 loss: 4.97962333e-07
Iter: 1222 loss: 4.97962844e-07
Iter: 1223 loss: 4.97962787e-07
Iter: 1224 loss: 4.97962787e-07
Iter: 1225 loss: 4.97962333e-07
Iter: 1226 loss: 4.97962333e-07
Iter: 1227 loss: 4.97962333e-07
Iter: 1228 loss: 4.97962787e-07
Iter: 1229 loss: 4.97962333e-07
Iter: 1230 loss: 4.97940277e-07
Iter: 1231 loss: 4.98238876e-07
Iter: 1232 loss: 4.97942096e-07
Iter: 1233 loss: 4.97930273e-07
Iter: 1234 loss: 4.97925271e-07
Iter: 1235 loss: 4.97900487e-07
Iter: 1236 loss: 4.97887754e-07
Iter: 1237 loss: 4.9795824e-07
Iter: 1238 loss: 4.97880251e-07
Iter: 1239 loss: 4.97846372e-07
Iter: 1240 loss: 4.97967847e-07
Iter: 1241 loss: 4.97839551e-07
Iter: 1242 loss: 4.978275e-07
Iter: 1243 loss: 4.97786687e-07
Iter: 1244 loss: 4.98678787e-07
Iter: 1245 loss: 4.97799647e-07
Iter: 1246 loss: 4.9778555e-07
Iter: 1247 loss: 4.97782594e-07
Iter: 1248 loss: 4.97767473e-07
Iter: 1249 loss: 4.97753945e-07
Iter: 1250 loss: 4.97740757e-07
Iter: 1251 loss: 4.97727456e-07
Iter: 1252 loss: 4.97666178e-07
Iter: 1253 loss: 4.97672545e-07
Iter: 1254 loss: 4.97627298e-07
Iter: 1255 loss: 4.97921519e-07
Iter: 1256 loss: 4.97645487e-07
Iter: 1257 loss: 4.97655094e-07
Iter: 1258 loss: 4.97653332e-07
Iter: 1259 loss: 4.97652081e-07
Iter: 1260 loss: 4.97665269e-07
Iter: 1261 loss: 4.97670726e-07
Iter: 1262 loss: 4.97648557e-07
Iter: 1263 loss: 4.97650888e-07
Iter: 1264 loss: 4.97636847e-07
Iter: 1265 loss: 4.97645317e-07
Iter: 1266 loss: 4.97654923e-07
Iter: 1267 loss: 4.97648784e-07
Iter: 1268 loss: 4.97648386e-07
Iter: 1269 loss: 4.97645715e-07
Iter: 1270 loss: 4.97650149e-07
Iter: 1271 loss: 4.97647363e-07
Iter: 1272 loss: 4.97642077e-07
Iter: 1273 loss: 4.97645715e-07
Iter: 1274 loss: 4.97648671e-07
Iter: 1275 loss: 4.97647534e-07
Iter: 1276 loss: 4.97646681e-07
Iter: 1277 loss: 4.97645658e-07
Iter: 1278 loss: 4.97646e-07
Iter: 1279 loss: 4.9764617e-07
Iter: 1280 loss: 4.9764617e-07
Iter: 1281 loss: 4.97646056e-07
Iter: 1282 loss: 4.97646056e-07
Iter: 1283 loss: 4.97646e-07
Iter: 1284 loss: 4.97646e-07
Iter: 1285 loss: 4.97646e-07
Iter: 1286 loss: 4.97646056e-07
Iter: 1287 loss: 4.97646056e-07
Iter: 1288 loss: 4.97646e-07
Iter: 1289 loss: 4.97642304e-07
Iter: 1290 loss: 4.97619851e-07
Iter: 1291 loss: 4.97589e-07
Iter: 1292 loss: 4.97623319e-07
Iter: 1293 loss: 4.97583642e-07
Iter: 1294 loss: 4.97521341e-07
Iter: 1295 loss: 4.97632584e-07
Iter: 1296 loss: 4.97521228e-07
Iter: 1297 loss: 4.97509291e-07
Iter: 1298 loss: 4.97806695e-07
Iter: 1299 loss: 4.97497524e-07
Iter: 1300 loss: 4.97488941e-07
Iter: 1301 loss: 4.97459496e-07
Iter: 1302 loss: 4.97438919e-07
Iter: 1303 loss: 4.97424764e-07
Iter: 1304 loss: 4.97401629e-07
Iter: 1305 loss: 4.97394069e-07
Iter: 1306 loss: 4.97351664e-07
Iter: 1307 loss: 4.97478368e-07
Iter: 1308 loss: 4.97354733e-07
Iter: 1309 loss: 4.97309031e-07
Iter: 1310 loss: 4.97354449e-07
Iter: 1311 loss: 4.97303517e-07
Iter: 1312 loss: 4.97268616e-07
Iter: 1313 loss: 4.97314943e-07
Iter: 1314 loss: 4.97261738e-07
Iter: 1315 loss: 4.97240649e-07
Iter: 1316 loss: 4.97245537e-07
Iter: 1317 loss: 4.97221e-07
Iter: 1318 loss: 4.97190229e-07
Iter: 1319 loss: 4.97358883e-07
Iter: 1320 loss: 4.97197e-07
Iter: 1321 loss: 4.97161921e-07
Iter: 1322 loss: 4.9713e-07
Iter: 1323 loss: 4.971468e-07
Iter: 1324 loss: 4.97088251e-07
Iter: 1325 loss: 4.97197561e-07
Iter: 1326 loss: 4.97100132e-07
Iter: 1327 loss: 4.97075632e-07
Iter: 1328 loss: 4.97098711e-07
Iter: 1329 loss: 4.97054543e-07
Iter: 1330 loss: 4.97023279e-07
Iter: 1331 loss: 4.97206202e-07
Iter: 1332 loss: 4.9701174e-07
Iter: 1333 loss: 4.9698815e-07
Iter: 1334 loss: 4.97253382e-07
Iter: 1335 loss: 4.96970699e-07
Iter: 1336 loss: 4.96956432e-07
Iter: 1337 loss: 4.96982466e-07
Iter: 1338 loss: 4.96941652e-07
Iter: 1339 loss: 4.96892483e-07
Iter: 1340 loss: 4.97071255e-07
Iter: 1341 loss: 4.96912378e-07
Iter: 1342 loss: 4.96891687e-07
Iter: 1343 loss: 4.96884866e-07
Iter: 1344 loss: 4.96858888e-07
Iter: 1345 loss: 4.96836719e-07
Iter: 1346 loss: 4.96932955e-07
Iter: 1347 loss: 4.96821883e-07
Iter: 1348 loss: 4.96795735e-07
Iter: 1349 loss: 4.96813584e-07
Iter: 1350 loss: 4.96773737e-07
Iter: 1351 loss: 4.96743382e-07
Iter: 1352 loss: 4.96860252e-07
Iter: 1353 loss: 4.96720077e-07
Iter: 1354 loss: 4.96695918e-07
Iter: 1355 loss: 4.96791472e-07
Iter: 1356 loss: 4.96680741e-07
Iter: 1357 loss: 4.96661073e-07
Iter: 1358 loss: 4.96675511e-07
Iter: 1359 loss: 4.96664029e-07
Iter: 1360 loss: 4.96618895e-07
Iter: 1361 loss: 4.96697794e-07
Iter: 1362 loss: 4.96591724e-07
Iter: 1363 loss: 4.96572397e-07
Iter: 1364 loss: 4.96638449e-07
Iter: 1365 loss: 4.96576376e-07
Iter: 1366 loss: 4.9654966e-07
Iter: 1367 loss: 4.96821144e-07
Iter: 1368 loss: 4.96548182e-07
Iter: 1369 loss: 4.96527548e-07
Iter: 1370 loss: 4.96521693e-07
Iter: 1371 loss: 4.96507653e-07
Iter: 1372 loss: 4.96476e-07
Iter: 1373 loss: 4.96547045e-07
Iter: 1374 loss: 4.96471955e-07
Iter: 1375 loss: 4.96449616e-07
Iter: 1376 loss: 4.96546818e-07
Iter: 1377 loss: 4.96421251e-07
Iter: 1378 loss: 4.96419602e-07
Iter: 1379 loss: 4.96409086e-07
Iter: 1380 loss: 4.96397206e-07
Iter: 1381 loss: 4.9637481e-07
Iter: 1382 loss: 4.96425059e-07
Iter: 1383 loss: 4.96342068e-07
Iter: 1384 loss: 4.9628494e-07
Iter: 1385 loss: 4.9639516e-07
Iter: 1386 loss: 4.96275788e-07
Iter: 1387 loss: 4.96250209e-07
Iter: 1388 loss: 4.96498956e-07
Iter: 1389 loss: 4.9626135e-07
Iter: 1390 loss: 4.96221e-07
Iter: 1391 loss: 4.96214398e-07
Iter: 1392 loss: 4.96217126e-07
Iter: 1393 loss: 4.96176142e-07
Iter: 1394 loss: 4.96221219e-07
Iter: 1395 loss: 4.96146583e-07
Iter: 1396 loss: 4.96111852e-07
Iter: 1397 loss: 4.96148402e-07
Iter: 1398 loss: 4.96100881e-07
Iter: 1399 loss: 4.96050802e-07
Iter: 1400 loss: 4.96354346e-07
Iter: 1401 loss: 4.96043185e-07
Iter: 1402 loss: 4.96008909e-07
Iter: 1403 loss: 4.96380608e-07
Iter: 1404 loss: 4.96008965e-07
Iter: 1405 loss: 4.96005953e-07
Iter: 1406 loss: 4.96014195e-07
Iter: 1407 loss: 4.95999757e-07
Iter: 1408 loss: 4.95941094e-07
Iter: 1409 loss: 4.96104e-07
Iter: 1410 loss: 4.95932909e-07
Iter: 1411 loss: 4.95916879e-07
Iter: 1412 loss: 4.95948143e-07
Iter: 1413 loss: 4.95913525e-07
Iter: 1414 loss: 4.95865606e-07
Iter: 1415 loss: 4.96026928e-07
Iter: 1416 loss: 4.95883796e-07
Iter: 1417 loss: 4.9584321e-07
Iter: 1418 loss: 4.95887321e-07
Iter: 1419 loss: 4.95814788e-07
Iter: 1420 loss: 4.95824281e-07
Iter: 1421 loss: 4.95834342e-07
Iter: 1422 loss: 4.95830818e-07
Iter: 1423 loss: 4.95827862e-07
Iter: 1424 loss: 4.95832239e-07
Iter: 1425 loss: 4.95826612e-07
Iter: 1426 loss: 4.95833433e-07
Iter: 1427 loss: 4.95823656e-07
Iter: 1428 loss: 4.95829e-07
Iter: 1429 loss: 4.95823542e-07
Iter: 1430 loss: 4.95827749e-07
Iter: 1431 loss: 4.95821041e-07
Iter: 1432 loss: 4.95814845e-07
Iter: 1433 loss: 4.95820473e-07
Iter: 1434 loss: 4.95819449e-07
Iter: 1435 loss: 4.95817403e-07
Iter: 1436 loss: 4.95815414e-07
Iter: 1437 loss: 4.95817176e-07
Iter: 1438 loss: 4.95816948e-07
Iter: 1439 loss: 4.95816266e-07
Iter: 1440 loss: 4.95816039e-07
Iter: 1441 loss: 4.95815243e-07
Iter: 1442 loss: 4.95815414e-07
Iter: 1443 loss: 4.95815357e-07
Iter: 1444 loss: 4.95815812e-07
Iter: 1445 loss: 4.95815243e-07
Iter: 1446 loss: 4.95815243e-07
Iter: 1447 loss: 4.95815812e-07
Iter: 1448 loss: 4.95815812e-07
Iter: 1449 loss: 4.95815812e-07
Iter: 1450 loss: 4.95815812e-07
Iter: 1451 loss: 4.95815243e-07
Iter: 1452 loss: 4.95815812e-07
Iter: 1453 loss: 4.95815812e-07
Iter: 1454 loss: 4.95815243e-07
Iter: 1455 loss: 4.95815812e-07
Iter: 1456 loss: 4.95815243e-07
Iter: 1457 loss: 1.70795931e-06
Iter: 1458 loss: 4.95835138e-07
Iter: 1459 loss: 4.95827294e-07
Iter: 1460 loss: 4.9583457e-07
Iter: 1461 loss: 4.95828886e-07
Iter: 1462 loss: 4.95812969e-07
Iter: 1463 loss: 4.95820927e-07
Iter: 1464 loss: 4.95823087e-07
Iter: 1465 loss: 4.95832e-07
Iter: 1466 loss: 4.95831387e-07
Iter: 1467 loss: 4.95823599e-07
Iter: 1468 loss: 4.95818654e-07
Iter: 1469 loss: 4.95814561e-07
Iter: 1470 loss: 4.95818142e-07
Iter: 1471 loss: 4.95814902e-07
Iter: 1472 loss: 4.9582e-07
Iter: 1473 loss: 4.95815414e-07
Iter: 1474 loss: 4.95818711e-07
Iter: 1475 loss: 4.95817972e-07
Iter: 1476 loss: 4.95815698e-07
Iter: 1477 loss: 4.95817744e-07
Iter: 1478 loss: 4.95816607e-07
Iter: 1479 loss: 4.95815698e-07
Iter: 1480 loss: 4.95816e-07
Iter: 1481 loss: 4.95815925e-07
Iter: 1482 loss: 4.95815698e-07
Iter: 1483 loss: 4.95815698e-07
Iter: 1484 loss: 4.95815698e-07
Iter: 1485 loss: 4.95815698e-07
Iter: 1486 loss: 4.95815698e-07
Iter: 1487 loss: 4.95815925e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2
+ date
Mon Oct 26 11:32:44 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.2_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.2_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4401b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d44042730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4410e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4410e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d4405ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d44137730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d307c0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d3078d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d3078d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d3078d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d307652f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d307119d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d30712d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d306db048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d306d3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d3076b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d3076cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d3067f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d3076cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d305df048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d305dfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d305ac0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d305df9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d3051d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d3051d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d3051d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d3051d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d30482950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d30482f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d304977b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d3044b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d30415f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d30415730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d30471d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d303bf0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d303b1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.04158698e-06
Iter: 2 loss: 1.95686471e-06
Iter: 3 loss: 1.83581074e-06
Iter: 4 loss: 1.43898205e-06
Iter: 5 loss: 2.76419382e-06
Iter: 6 loss: 1.33046376e-06
Iter: 7 loss: 1.16081651e-06
Iter: 8 loss: 2.52841e-06
Iter: 9 loss: 1.14974932e-06
Iter: 10 loss: 1.01688192e-06
Iter: 11 loss: 1.11083114e-06
Iter: 12 loss: 9.34431569e-07
Iter: 13 loss: 8.61915851e-07
Iter: 14 loss: 1.82209294e-06
Iter: 15 loss: 8.61538751e-07
Iter: 16 loss: 8.01826445e-07
Iter: 17 loss: 8.11463849e-07
Iter: 18 loss: 7.56821578e-07
Iter: 19 loss: 7.35678725e-07
Iter: 20 loss: 9.59198815e-07
Iter: 21 loss: 7.35151502e-07
Iter: 22 loss: 7.22629125e-07
Iter: 23 loss: 8.03278112e-07
Iter: 24 loss: 7.21239587e-07
Iter: 25 loss: 7.11455073e-07
Iter: 26 loss: 7.02483533e-07
Iter: 27 loss: 7.00083945e-07
Iter: 28 loss: 6.86474948e-07
Iter: 29 loss: 6.9265667e-07
Iter: 30 loss: 6.77260346e-07
Iter: 31 loss: 6.65517689e-07
Iter: 32 loss: 7.36612037e-07
Iter: 33 loss: 6.64106949e-07
Iter: 34 loss: 6.53595862e-07
Iter: 35 loss: 6.96106781e-07
Iter: 36 loss: 6.51306493e-07
Iter: 37 loss: 6.42908617e-07
Iter: 38 loss: 6.51692858e-07
Iter: 39 loss: 6.38244899e-07
Iter: 40 loss: 6.37045048e-07
Iter: 41 loss: 6.3510538e-07
Iter: 42 loss: 6.31444e-07
Iter: 43 loss: 6.26139354e-07
Iter: 44 loss: 6.2597735e-07
Iter: 45 loss: 6.21303911e-07
Iter: 46 loss: 6.21305958e-07
Iter: 47 loss: 6.17796104e-07
Iter: 48 loss: 6.16030775e-07
Iter: 49 loss: 6.14398687e-07
Iter: 50 loss: 6.09663857e-07
Iter: 51 loss: 6.36630716e-07
Iter: 52 loss: 6.0900777e-07
Iter: 53 loss: 6.03600938e-07
Iter: 54 loss: 6.00753879e-07
Iter: 55 loss: 5.98281758e-07
Iter: 56 loss: 5.92090373e-07
Iter: 57 loss: 5.89552769e-07
Iter: 58 loss: 5.86295869e-07
Iter: 59 loss: 5.77604567e-07
Iter: 60 loss: 5.77588253e-07
Iter: 61 loss: 5.73567036e-07
Iter: 62 loss: 5.68717837e-07
Iter: 63 loss: 5.68216649e-07
Iter: 64 loss: 5.62715513e-07
Iter: 65 loss: 5.87636123e-07
Iter: 66 loss: 5.61655156e-07
Iter: 67 loss: 5.57550152e-07
Iter: 68 loss: 5.71928922e-07
Iter: 69 loss: 5.564508e-07
Iter: 70 loss: 5.53791097e-07
Iter: 71 loss: 5.59076739e-07
Iter: 72 loss: 5.52693507e-07
Iter: 73 loss: 5.49502829e-07
Iter: 74 loss: 5.66430799e-07
Iter: 75 loss: 5.49018409e-07
Iter: 76 loss: 5.48460662e-07
Iter: 77 loss: 5.47871764e-07
Iter: 78 loss: 5.47138939e-07
Iter: 79 loss: 5.45728881e-07
Iter: 80 loss: 5.72909e-07
Iter: 81 loss: 5.45693524e-07
Iter: 82 loss: 5.44187287e-07
Iter: 83 loss: 5.65927337e-07
Iter: 84 loss: 5.44183706e-07
Iter: 85 loss: 5.43304452e-07
Iter: 86 loss: 5.421183e-07
Iter: 87 loss: 5.42048952e-07
Iter: 88 loss: 5.40413907e-07
Iter: 89 loss: 5.58792635e-07
Iter: 90 loss: 5.40377869e-07
Iter: 91 loss: 5.39527832e-07
Iter: 92 loss: 5.3857724e-07
Iter: 93 loss: 5.38450479e-07
Iter: 94 loss: 5.37235792e-07
Iter: 95 loss: 5.4369e-07
Iter: 96 loss: 5.37036954e-07
Iter: 97 loss: 5.35854156e-07
Iter: 98 loss: 5.42635291e-07
Iter: 99 loss: 5.35669642e-07
Iter: 100 loss: 5.34805906e-07
Iter: 101 loss: 5.3311345e-07
Iter: 102 loss: 5.70850602e-07
Iter: 103 loss: 5.33137779e-07
Iter: 104 loss: 5.3138757e-07
Iter: 105 loss: 5.30596367e-07
Iter: 106 loss: 5.29708132e-07
Iter: 107 loss: 5.27423936e-07
Iter: 108 loss: 5.27392217e-07
Iter: 109 loss: 5.2600808e-07
Iter: 110 loss: 5.25318342e-07
Iter: 111 loss: 5.24645088e-07
Iter: 112 loss: 5.24328414e-07
Iter: 113 loss: 5.23505264e-07
Iter: 114 loss: 5.23047675e-07
Iter: 115 loss: 5.2290693e-07
Iter: 116 loss: 5.22652613e-07
Iter: 117 loss: 5.22067808e-07
Iter: 118 loss: 5.26369945e-07
Iter: 119 loss: 5.2204183e-07
Iter: 120 loss: 5.21624202e-07
Iter: 121 loss: 5.21572758e-07
Iter: 122 loss: 5.21270408e-07
Iter: 123 loss: 5.20865399e-07
Iter: 124 loss: 5.24432039e-07
Iter: 125 loss: 5.20843059e-07
Iter: 126 loss: 5.20492677e-07
Iter: 127 loss: 5.20463459e-07
Iter: 128 loss: 5.20162928e-07
Iter: 129 loss: 5.19912e-07
Iter: 130 loss: 5.1961257e-07
Iter: 131 loss: 5.19567095e-07
Iter: 132 loss: 5.18960121e-07
Iter: 133 loss: 5.19382638e-07
Iter: 134 loss: 5.18595584e-07
Iter: 135 loss: 5.17896865e-07
Iter: 136 loss: 5.27607085e-07
Iter: 137 loss: 5.17901412e-07
Iter: 138 loss: 5.17336389e-07
Iter: 139 loss: 5.20374329e-07
Iter: 140 loss: 5.17247429e-07
Iter: 141 loss: 5.16824e-07
Iter: 142 loss: 5.15580382e-07
Iter: 143 loss: 5.20452545e-07
Iter: 144 loss: 5.15007969e-07
Iter: 145 loss: 5.14171347e-07
Iter: 146 loss: 5.14059309e-07
Iter: 147 loss: 5.1336059e-07
Iter: 148 loss: 5.13094164e-07
Iter: 149 loss: 5.12716383e-07
Iter: 150 loss: 5.1304221e-07
Iter: 151 loss: 5.12371059e-07
Iter: 152 loss: 5.12110148e-07
Iter: 153 loss: 5.11856456e-07
Iter: 154 loss: 5.11815585e-07
Iter: 155 loss: 5.11541771e-07
Iter: 156 loss: 5.15711349e-07
Iter: 157 loss: 5.11546318e-07
Iter: 158 loss: 5.11324117e-07
Iter: 159 loss: 5.10921e-07
Iter: 160 loss: 5.10915925e-07
Iter: 161 loss: 5.10435825e-07
Iter: 162 loss: 5.16983505e-07
Iter: 163 loss: 5.10430709e-07
Iter: 164 loss: 5.10169343e-07
Iter: 165 loss: 5.09838458e-07
Iter: 166 loss: 5.09805886e-07
Iter: 167 loss: 5.0935887e-07
Iter: 168 loss: 5.10300424e-07
Iter: 169 loss: 5.09194138e-07
Iter: 170 loss: 5.08980179e-07
Iter: 171 loss: 5.08933226e-07
Iter: 172 loss: 5.08727169e-07
Iter: 173 loss: 5.08626272e-07
Iter: 174 loss: 5.08524e-07
Iter: 175 loss: 5.08278333e-07
Iter: 176 loss: 5.08620587e-07
Iter: 177 loss: 5.08175617e-07
Iter: 178 loss: 5.07895209e-07
Iter: 179 loss: 5.08372352e-07
Iter: 180 loss: 5.07801587e-07
Iter: 181 loss: 5.07586833e-07
Iter: 182 loss: 5.08601715e-07
Iter: 183 loss: 5.0754403e-07
Iter: 184 loss: 5.07384868e-07
Iter: 185 loss: 5.07343543e-07
Iter: 186 loss: 5.072622e-07
Iter: 187 loss: 5.07088487e-07
Iter: 188 loss: 5.11065423e-07
Iter: 189 loss: 5.07087293e-07
Iter: 190 loss: 5.06771e-07
Iter: 191 loss: 5.08081939e-07
Iter: 192 loss: 5.06722e-07
Iter: 193 loss: 5.06485e-07
Iter: 194 loss: 5.06708034e-07
Iter: 195 loss: 5.06307174e-07
Iter: 196 loss: 5.06118283e-07
Iter: 197 loss: 5.06110382e-07
Iter: 198 loss: 5.06004767e-07
Iter: 199 loss: 5.05603225e-07
Iter: 200 loss: 5.06766355e-07
Iter: 201 loss: 5.05431331e-07
Iter: 202 loss: 5.0526512e-07
Iter: 203 loss: 5.05164962e-07
Iter: 204 loss: 5.0497988e-07
Iter: 205 loss: 5.0519435e-07
Iter: 206 loss: 5.04848344e-07
Iter: 207 loss: 5.04638137e-07
Iter: 208 loss: 5.04531897e-07
Iter: 209 loss: 5.04426566e-07
Iter: 210 loss: 5.04081584e-07
Iter: 211 loss: 5.04769048e-07
Iter: 212 loss: 5.03919182e-07
Iter: 213 loss: 5.03639455e-07
Iter: 214 loss: 5.03911508e-07
Iter: 215 loss: 5.03448746e-07
Iter: 216 loss: 5.03187607e-07
Iter: 217 loss: 5.03183173e-07
Iter: 218 loss: 5.02975809e-07
Iter: 219 loss: 5.04730451e-07
Iter: 220 loss: 5.02981152e-07
Iter: 221 loss: 5.02854277e-07
Iter: 222 loss: 5.02773787e-07
Iter: 223 loss: 5.02724333e-07
Iter: 224 loss: 5.02624232e-07
Iter: 225 loss: 5.02613716e-07
Iter: 226 loss: 5.02544879e-07
Iter: 227 loss: 5.02355931e-07
Iter: 228 loss: 5.04257798e-07
Iter: 229 loss: 5.02341436e-07
Iter: 230 loss: 5.021584e-07
Iter: 231 loss: 5.02161356e-07
Iter: 232 loss: 5.02092576e-07
Iter: 233 loss: 5.01991e-07
Iter: 234 loss: 5.01979912e-07
Iter: 235 loss: 5.01843715e-07
Iter: 236 loss: 5.02244916e-07
Iter: 237 loss: 5.01811428e-07
Iter: 238 loss: 5.0168228e-07
Iter: 239 loss: 5.03321e-07
Iter: 240 loss: 5.01683189e-07
Iter: 241 loss: 5.01571549e-07
Iter: 242 loss: 5.01394197e-07
Iter: 243 loss: 5.01399199e-07
Iter: 244 loss: 5.01203544e-07
Iter: 245 loss: 5.01012551e-07
Iter: 246 loss: 5.00971737e-07
Iter: 247 loss: 5.00635451e-07
Iter: 248 loss: 5.03837668e-07
Iter: 249 loss: 5.00627095e-07
Iter: 250 loss: 5.00394776e-07
Iter: 251 loss: 5.01929037e-07
Iter: 252 loss: 5.00388069e-07
Iter: 253 loss: 5.00210604e-07
Iter: 254 loss: 5.02519299e-07
Iter: 255 loss: 5.0021282e-07
Iter: 256 loss: 5.0011829e-07
Iter: 257 loss: 4.99976238e-07
Iter: 258 loss: 5.03028e-07
Iter: 259 loss: 4.9998215e-07
Iter: 260 loss: 4.99810199e-07
Iter: 261 loss: 4.9978803e-07
Iter: 262 loss: 4.99699865e-07
Iter: 263 loss: 4.99614544e-07
Iter: 264 loss: 4.9959641e-07
Iter: 265 loss: 4.99457599e-07
Iter: 266 loss: 4.9993946e-07
Iter: 267 loss: 4.99413602e-07
Iter: 268 loss: 4.99193789e-07
Iter: 269 loss: 4.98849204e-07
Iter: 270 loss: 4.98835107e-07
Iter: 271 loss: 4.9856817e-07
Iter: 272 loss: 4.98939642e-07
Iter: 273 loss: 4.98465965e-07
Iter: 274 loss: 4.98252462e-07
Iter: 275 loss: 5.00430758e-07
Iter: 276 loss: 4.98247573e-07
Iter: 277 loss: 4.98056e-07
Iter: 278 loss: 4.9912569e-07
Iter: 279 loss: 4.98038958e-07
Iter: 280 loss: 4.97910719e-07
Iter: 281 loss: 4.97975691e-07
Iter: 282 loss: 4.97811698e-07
Iter: 283 loss: 4.97742349e-07
Iter: 284 loss: 4.97647591e-07
Iter: 285 loss: 4.97625535e-07
Iter: 286 loss: 4.97475753e-07
Iter: 287 loss: 4.98561803e-07
Iter: 288 loss: 4.97452561e-07
Iter: 289 loss: 4.97362123e-07
Iter: 290 loss: 4.97360702e-07
Iter: 291 loss: 4.97284873e-07
Iter: 292 loss: 4.97225415e-07
Iter: 293 loss: 4.97209328e-07
Iter: 294 loss: 4.97102576e-07
Iter: 295 loss: 4.97184658e-07
Iter: 296 loss: 4.9705875e-07
Iter: 297 loss: 4.96873668e-07
Iter: 298 loss: 4.97160158e-07
Iter: 299 loss: 4.96798634e-07
Iter: 300 loss: 4.96680286e-07
Iter: 301 loss: 4.9675873e-07
Iter: 302 loss: 4.96604741e-07
Iter: 303 loss: 4.96444386e-07
Iter: 304 loss: 4.97544136e-07
Iter: 305 loss: 4.96443e-07
Iter: 306 loss: 4.96311e-07
Iter: 307 loss: 4.96185123e-07
Iter: 308 loss: 4.96121118e-07
Iter: 309 loss: 4.95947e-07
Iter: 310 loss: 4.96064501e-07
Iter: 311 loss: 4.95815527e-07
Iter: 312 loss: 4.95754762e-07
Iter: 313 loss: 4.95725089e-07
Iter: 314 loss: 4.95664153e-07
Iter: 315 loss: 4.95669042e-07
Iter: 316 loss: 4.95625216e-07
Iter: 317 loss: 4.95506299e-07
Iter: 318 loss: 4.95422341e-07
Iter: 319 loss: 4.95391532e-07
Iter: 320 loss: 4.95238737e-07
Iter: 321 loss: 4.95670406e-07
Iter: 322 loss: 4.95176153e-07
Iter: 323 loss: 4.95097595e-07
Iter: 324 loss: 4.95856e-07
Iter: 325 loss: 4.95069571e-07
Iter: 326 loss: 4.94992321e-07
Iter: 327 loss: 4.95002723e-07
Iter: 328 loss: 4.9497055e-07
Iter: 329 loss: 4.94915525e-07
Iter: 330 loss: 4.96049609e-07
Iter: 331 loss: 4.94912797e-07
Iter: 332 loss: 4.94812866e-07
Iter: 333 loss: 4.95406937e-07
Iter: 334 loss: 4.94803317e-07
Iter: 335 loss: 4.94715209e-07
Iter: 336 loss: 4.94753408e-07
Iter: 337 loss: 4.94654103e-07
Iter: 338 loss: 4.94579183e-07
Iter: 339 loss: 4.94414394e-07
Iter: 340 loss: 4.98108818e-07
Iter: 341 loss: 4.94419282e-07
Iter: 342 loss: 4.94227095e-07
Iter: 343 loss: 4.95655513e-07
Iter: 344 loss: 4.94206688e-07
Iter: 345 loss: 4.94063727e-07
Iter: 346 loss: 4.95153586e-07
Iter: 347 loss: 4.94048209e-07
Iter: 348 loss: 4.93912864e-07
Iter: 349 loss: 4.93970219e-07
Iter: 350 loss: 4.93821233e-07
Iter: 351 loss: 4.93704079e-07
Iter: 352 loss: 4.93658604e-07
Iter: 353 loss: 4.93565722e-07
Iter: 354 loss: 4.93498817e-07
Iter: 355 loss: 4.93486596e-07
Iter: 356 loss: 4.9338945e-07
Iter: 357 loss: 4.93410084e-07
Iter: 358 loss: 4.93334483e-07
Iter: 359 loss: 4.93211e-07
Iter: 360 loss: 4.93222444e-07
Iter: 361 loss: 4.93098241e-07
Iter: 362 loss: 4.93051232e-07
Iter: 363 loss: 4.93037305e-07
Iter: 364 loss: 4.92932713e-07
Iter: 365 loss: 4.92818117e-07
Iter: 366 loss: 4.92800893e-07
Iter: 367 loss: 4.92693459e-07
Iter: 368 loss: 4.93194079e-07
Iter: 369 loss: 4.92699087e-07
Iter: 370 loss: 4.92577215e-07
Iter: 371 loss: 4.93159405e-07
Iter: 372 loss: 4.92573065e-07
Iter: 373 loss: 4.92457389e-07
Iter: 374 loss: 4.92395145e-07
Iter: 375 loss: 4.92401e-07
Iter: 376 loss: 4.92293452e-07
Iter: 377 loss: 4.92281288e-07
Iter: 378 loss: 4.92208642e-07
Iter: 379 loss: 4.92110189e-07
Iter: 380 loss: 4.93712e-07
Iter: 381 loss: 4.92089157e-07
Iter: 382 loss: 4.92026857e-07
Iter: 383 loss: 4.92737286e-07
Iter: 384 loss: 4.92021798e-07
Iter: 385 loss: 4.91962453e-07
Iter: 386 loss: 4.91862693e-07
Iter: 387 loss: 4.9342691e-07
Iter: 388 loss: 4.91841206e-07
Iter: 389 loss: 4.91706e-07
Iter: 390 loss: 4.92340462e-07
Iter: 391 loss: 4.91667038e-07
Iter: 392 loss: 4.91557842e-07
Iter: 393 loss: 4.92674303e-07
Iter: 394 loss: 4.91543631e-07
Iter: 395 loss: 4.91472633e-07
Iter: 396 loss: 4.91379183e-07
Iter: 397 loss: 4.91386231e-07
Iter: 398 loss: 4.91242361e-07
Iter: 399 loss: 4.91246738e-07
Iter: 400 loss: 4.91162609e-07
Iter: 401 loss: 4.91011519e-07
Iter: 402 loss: 4.91006574e-07
Iter: 403 loss: 4.90896582e-07
Iter: 404 loss: 4.91588366e-07
Iter: 405 loss: 4.9088635e-07
Iter: 406 loss: 4.90782497e-07
Iter: 407 loss: 4.91451942e-07
Iter: 408 loss: 4.90770901e-07
Iter: 409 loss: 4.90706e-07
Iter: 410 loss: 4.90617481e-07
Iter: 411 loss: 4.90599064e-07
Iter: 412 loss: 4.90498564e-07
Iter: 413 loss: 4.90801767e-07
Iter: 414 loss: 4.90481284e-07
Iter: 415 loss: 4.90419e-07
Iter: 416 loss: 4.91114577e-07
Iter: 417 loss: 4.90414322e-07
Iter: 418 loss: 4.90363107e-07
Iter: 419 loss: 4.90371065e-07
Iter: 420 loss: 4.90329171e-07
Iter: 421 loss: 4.90262096e-07
Iter: 422 loss: 4.90270224e-07
Iter: 423 loss: 4.90208322e-07
Iter: 424 loss: 4.90115099e-07
Iter: 425 loss: 4.90204457e-07
Iter: 426 loss: 4.90051377e-07
Iter: 427 loss: 4.89942181e-07
Iter: 428 loss: 4.9127425e-07
Iter: 429 loss: 4.89937065e-07
Iter: 430 loss: 4.89882893e-07
Iter: 431 loss: 4.89820593e-07
Iter: 432 loss: 4.89796832e-07
Iter: 433 loss: 4.89749766e-07
Iter: 434 loss: 4.89741637e-07
Iter: 435 loss: 4.89711795e-07
Iter: 436 loss: 4.89581794e-07
Iter: 437 loss: 4.90599859e-07
Iter: 438 loss: 4.89561103e-07
Iter: 439 loss: 4.89439458e-07
Iter: 440 loss: 4.89857541e-07
Iter: 441 loss: 4.89406e-07
Iter: 442 loss: 4.89364538e-07
Iter: 443 loss: 4.89336969e-07
Iter: 444 loss: 4.89299794e-07
Iter: 445 loss: 4.89171725e-07
Iter: 446 loss: 4.90860145e-07
Iter: 447 loss: 4.8917974e-07
Iter: 448 loss: 4.89061904e-07
Iter: 449 loss: 4.89031436e-07
Iter: 450 loss: 4.8897931e-07
Iter: 451 loss: 4.88879095e-07
Iter: 452 loss: 4.88883074e-07
Iter: 453 loss: 4.88803607e-07
Iter: 454 loss: 4.89450599e-07
Iter: 455 loss: 4.88787691e-07
Iter: 456 loss: 4.88724822e-07
Iter: 457 loss: 4.8868776e-07
Iter: 458 loss: 4.88680371e-07
Iter: 459 loss: 4.88598175e-07
Iter: 460 loss: 4.88872274e-07
Iter: 461 loss: 4.8859431e-07
Iter: 462 loss: 4.88531555e-07
Iter: 463 loss: 4.89287572e-07
Iter: 464 loss: 4.88537466e-07
Iter: 465 loss: 4.88514843e-07
Iter: 466 loss: 4.88446517e-07
Iter: 467 loss: 4.90063314e-07
Iter: 468 loss: 4.88423836e-07
Iter: 469 loss: 4.88364776e-07
Iter: 470 loss: 4.88367277e-07
Iter: 471 loss: 4.88310775e-07
Iter: 472 loss: 4.88378589e-07
Iter: 473 loss: 4.88259275e-07
Iter: 474 loss: 4.88226135e-07
Iter: 475 loss: 4.88099658e-07
Iter: 476 loss: 4.89096237e-07
Iter: 477 loss: 4.88113e-07
Iter: 478 loss: 4.87995578e-07
Iter: 479 loss: 4.88997102e-07
Iter: 480 loss: 4.87985744e-07
Iter: 481 loss: 4.87873194e-07
Iter: 482 loss: 4.89087768e-07
Iter: 483 loss: 4.87876605e-07
Iter: 484 loss: 4.87807824e-07
Iter: 485 loss: 4.87683963e-07
Iter: 486 loss: 4.87670604e-07
Iter: 487 loss: 4.87583861e-07
Iter: 488 loss: 4.88433727e-07
Iter: 489 loss: 4.87581758e-07
Iter: 490 loss: 4.8749871e-07
Iter: 491 loss: 4.87718694e-07
Iter: 492 loss: 4.8746432e-07
Iter: 493 loss: 4.87422881e-07
Iter: 494 loss: 4.87381726e-07
Iter: 495 loss: 4.87363195e-07
Iter: 496 loss: 4.87313116e-07
Iter: 497 loss: 4.88061346e-07
Iter: 498 loss: 4.87299189e-07
Iter: 499 loss: 4.87265368e-07
Iter: 500 loss: 4.87423222e-07
Iter: 501 loss: 4.87228135e-07
Iter: 502 loss: 4.8718357e-07
Iter: 503 loss: 4.87130933e-07
Iter: 504 loss: 4.87137413e-07
Iter: 505 loss: 4.87088926e-07
Iter: 506 loss: 4.87079e-07
Iter: 507 loss: 4.87061584e-07
Iter: 508 loss: 4.86977228e-07
Iter: 509 loss: 4.87735292e-07
Iter: 510 loss: 4.86979275e-07
Iter: 511 loss: 4.86871727e-07
Iter: 512 loss: 4.87349325e-07
Iter: 513 loss: 4.86871102e-07
Iter: 514 loss: 4.86821136e-07
Iter: 515 loss: 4.87099e-07
Iter: 516 loss: 4.86805675e-07
Iter: 517 loss: 4.86738372e-07
Iter: 518 loss: 4.87246e-07
Iter: 519 loss: 4.86748036e-07
Iter: 520 loss: 4.86731e-07
Iter: 521 loss: 4.86675731e-07
Iter: 522 loss: 4.87511898e-07
Iter: 523 loss: 4.86668739e-07
Iter: 524 loss: 4.86625709e-07
Iter: 525 loss: 4.86907538e-07
Iter: 526 loss: 4.86571651e-07
Iter: 527 loss: 4.86518275e-07
Iter: 528 loss: 4.869039e-07
Iter: 529 loss: 4.86533565e-07
Iter: 530 loss: 4.864782e-07
Iter: 531 loss: 4.86440968e-07
Iter: 532 loss: 4.86413228e-07
Iter: 533 loss: 4.86305339e-07
Iter: 534 loss: 4.86286751e-07
Iter: 535 loss: 4.8622195e-07
Iter: 536 loss: 4.8615459e-07
Iter: 537 loss: 4.86125657e-07
Iter: 538 loss: 4.86118211e-07
Iter: 539 loss: 4.86045337e-07
Iter: 540 loss: 4.86032718e-07
Iter: 541 loss: 4.85952e-07
Iter: 542 loss: 4.86983367e-07
Iter: 543 loss: 4.85949158e-07
Iter: 544 loss: 4.85889927e-07
Iter: 545 loss: 4.85825353e-07
Iter: 546 loss: 4.85834278e-07
Iter: 547 loss: 4.85724854e-07
Iter: 548 loss: 4.85899875e-07
Iter: 549 loss: 4.85712576e-07
Iter: 550 loss: 4.85670398e-07
Iter: 551 loss: 4.86217516e-07
Iter: 552 loss: 4.85671649e-07
Iter: 553 loss: 4.85631404e-07
Iter: 554 loss: 4.85613725e-07
Iter: 555 loss: 4.85582177e-07
Iter: 556 loss: 4.85526812e-07
Iter: 557 loss: 4.85638452e-07
Iter: 558 loss: 4.8549407e-07
Iter: 559 loss: 4.8545661e-07
Iter: 560 loss: 4.85497822e-07
Iter: 561 loss: 4.85412727e-07
Iter: 562 loss: 4.85358896e-07
Iter: 563 loss: 4.85945918e-07
Iter: 564 loss: 4.85361056e-07
Iter: 565 loss: 4.85326041e-07
Iter: 566 loss: 4.85260955e-07
Iter: 567 loss: 4.86507815e-07
Iter: 568 loss: 4.85273631e-07
Iter: 569 loss: 4.85191e-07
Iter: 570 loss: 4.85694841e-07
Iter: 571 loss: 4.85177793e-07
Iter: 572 loss: 4.85130101e-07
Iter: 573 loss: 4.85252144e-07
Iter: 574 loss: 4.85068938e-07
Iter: 575 loss: 4.85023122e-07
Iter: 576 loss: 4.85817623e-07
Iter: 577 loss: 4.85027044e-07
Iter: 578 loss: 4.84973896e-07
Iter: 579 loss: 4.84973839e-07
Iter: 580 loss: 4.84962698e-07
Iter: 581 loss: 4.84859356e-07
Iter: 582 loss: 4.85259761e-07
Iter: 583 loss: 4.84856116e-07
Iter: 584 loss: 4.84813484e-07
Iter: 585 loss: 4.8473e-07
Iter: 586 loss: 4.86158115e-07
Iter: 587 loss: 4.84721284e-07
Iter: 588 loss: 4.84653867e-07
Iter: 589 loss: 4.84677287e-07
Iter: 590 loss: 4.84602197e-07
Iter: 591 loss: 4.84576617e-07
Iter: 592 loss: 4.84543307e-07
Iter: 593 loss: 4.84465318e-07
Iter: 594 loss: 4.84403301e-07
Iter: 595 loss: 4.84366922e-07
Iter: 596 loss: 4.84296322e-07
Iter: 597 loss: 4.85192913e-07
Iter: 598 loss: 4.8430951e-07
Iter: 599 loss: 4.84258351e-07
Iter: 600 loss: 4.84258578e-07
Iter: 601 loss: 4.84210545e-07
Iter: 602 loss: 4.84167231e-07
Iter: 603 loss: 4.8417337e-07
Iter: 604 loss: 4.84131647e-07
Iter: 605 loss: 4.84068494e-07
Iter: 606 loss: 4.84386078e-07
Iter: 607 loss: 4.84050588e-07
Iter: 608 loss: 4.84008183e-07
Iter: 609 loss: 4.83992039e-07
Iter: 610 loss: 4.83927806e-07
Iter: 611 loss: 4.83905296e-07
Iter: 612 loss: 4.83896372e-07
Iter: 613 loss: 4.83821225e-07
Iter: 614 loss: 4.84695079e-07
Iter: 615 loss: 4.83821e-07
Iter: 616 loss: 4.83791382e-07
Iter: 617 loss: 4.8369435e-07
Iter: 618 loss: 4.85435066e-07
Iter: 619 loss: 4.83703047e-07
Iter: 620 loss: 4.83630913e-07
Iter: 621 loss: 4.83988856e-07
Iter: 622 loss: 4.83583221e-07
Iter: 623 loss: 4.83526605e-07
Iter: 624 loss: 4.83524502e-07
Iter: 625 loss: 4.83496649e-07
Iter: 626 loss: 4.83430824e-07
Iter: 627 loss: 4.83445092e-07
Iter: 628 loss: 4.83407746e-07
Iter: 629 loss: 4.83649615e-07
Iter: 630 loss: 4.83399958e-07
Iter: 631 loss: 4.83334418e-07
Iter: 632 loss: 4.83453e-07
Iter: 633 loss: 4.83329302e-07
Iter: 634 loss: 4.83279e-07
Iter: 635 loss: 4.83233293e-07
Iter: 636 loss: 4.83201859e-07
Iter: 637 loss: 4.83091185e-07
Iter: 638 loss: 4.83201518e-07
Iter: 639 loss: 4.8305867e-07
Iter: 640 loss: 4.82991709e-07
Iter: 641 loss: 4.83705549e-07
Iter: 642 loss: 4.8297818e-07
Iter: 643 loss: 4.8285915e-07
Iter: 644 loss: 4.83603912e-07
Iter: 645 loss: 4.82853522e-07
Iter: 646 loss: 4.82757457e-07
Iter: 647 loss: 4.83038434e-07
Iter: 648 loss: 4.82770929e-07
Iter: 649 loss: 4.82701751e-07
Iter: 650 loss: 4.82631e-07
Iter: 651 loss: 4.82620521e-07
Iter: 652 loss: 4.82553162e-07
Iter: 653 loss: 4.82738e-07
Iter: 654 loss: 4.82492283e-07
Iter: 655 loss: 4.82484097e-07
Iter: 656 loss: 4.82472274e-07
Iter: 657 loss: 4.82440328e-07
Iter: 658 loss: 4.82416226e-07
Iter: 659 loss: 4.8240571e-07
Iter: 660 loss: 4.82363e-07
Iter: 661 loss: 4.82370524e-07
Iter: 662 loss: 4.8232539e-07
Iter: 663 loss: 4.82287362e-07
Iter: 664 loss: 4.82293046e-07
Iter: 665 loss: 4.82271389e-07
Iter: 666 loss: 4.82219434e-07
Iter: 667 loss: 4.82238534e-07
Iter: 668 loss: 4.82163273e-07
Iter: 669 loss: 4.82218638e-07
Iter: 670 loss: 4.82143037e-07
Iter: 671 loss: 4.8207454e-07
Iter: 672 loss: 4.82160203e-07
Iter: 673 loss: 4.82043e-07
Iter: 674 loss: 4.81997915e-07
Iter: 675 loss: 4.82013888e-07
Iter: 676 loss: 4.81960569e-07
Iter: 677 loss: 4.81944198e-07
Iter: 678 loss: 4.81937889e-07
Iter: 679 loss: 4.81879226e-07
Iter: 680 loss: 4.82194594e-07
Iter: 681 loss: 4.81856887e-07
Iter: 682 loss: 4.81820052e-07
Iter: 683 loss: 4.81798565e-07
Iter: 684 loss: 4.81776169e-07
Iter: 685 loss: 4.81730524e-07
Iter: 686 loss: 4.81706195e-07
Iter: 687 loss: 4.81693178e-07
Iter: 688 loss: 4.81644406e-07
Iter: 689 loss: 4.81640313e-07
Iter: 690 loss: 4.81605525e-07
Iter: 691 loss: 4.81545271e-07
Iter: 692 loss: 4.81953293e-07
Iter: 693 loss: 4.81543566e-07
Iter: 694 loss: 4.81478423e-07
Iter: 695 loss: 4.8243453e-07
Iter: 696 loss: 4.81478651e-07
Iter: 697 loss: 4.81413053e-07
Iter: 698 loss: 4.81487689e-07
Iter: 699 loss: 4.81376674e-07
Iter: 700 loss: 4.81299e-07
Iter: 701 loss: 4.81255256e-07
Iter: 702 loss: 4.81240363e-07
Iter: 703 loss: 4.81144298e-07
Iter: 704 loss: 4.81436359e-07
Iter: 705 loss: 4.81111556e-07
Iter: 706 loss: 4.8106e-07
Iter: 707 loss: 4.81385428e-07
Iter: 708 loss: 4.8104755e-07
Iter: 709 loss: 4.80986387e-07
Iter: 710 loss: 4.81153e-07
Iter: 711 loss: 4.80976951e-07
Iter: 712 loss: 4.80944436e-07
Iter: 713 loss: 4.80956601e-07
Iter: 714 loss: 4.80932897e-07
Iter: 715 loss: 4.80871e-07
Iter: 716 loss: 4.81434029e-07
Iter: 717 loss: 4.80868891e-07
Iter: 718 loss: 4.80827282e-07
Iter: 719 loss: 4.8081813e-07
Iter: 720 loss: 4.8079977e-07
Iter: 721 loss: 4.80769188e-07
Iter: 722 loss: 4.80760491e-07
Iter: 723 loss: 4.80722633e-07
Iter: 724 loss: 4.81103484e-07
Iter: 725 loss: 4.80726555e-07
Iter: 726 loss: 4.80689721e-07
Iter: 727 loss: 4.80699327e-07
Iter: 728 loss: 4.80649419e-07
Iter: 729 loss: 4.80616336e-07
Iter: 730 loss: 4.80528342e-07
Iter: 731 loss: 4.80561141e-07
Iter: 732 loss: 4.80485824e-07
Iter: 733 loss: 4.80719109e-07
Iter: 734 loss: 4.80446886e-07
Iter: 735 loss: 4.80411074e-07
Iter: 736 loss: 4.81219843e-07
Iter: 737 loss: 4.80421249e-07
Iter: 738 loss: 4.80337405e-07
Iter: 739 loss: 4.80566086e-07
Iter: 740 loss: 4.80331e-07
Iter: 741 loss: 4.803004e-07
Iter: 742 loss: 4.80211725e-07
Iter: 743 loss: 4.81967959e-07
Iter: 744 loss: 4.80217579e-07
Iter: 745 loss: 4.80087067e-07
Iter: 746 loss: 4.80182052e-07
Iter: 747 loss: 4.80047e-07
Iter: 748 loss: 4.79982077e-07
Iter: 749 loss: 4.79982077e-07
Iter: 750 loss: 4.79915798e-07
Iter: 751 loss: 4.80525671e-07
Iter: 752 loss: 4.79909716e-07
Iter: 753 loss: 4.79871119e-07
Iter: 754 loss: 4.79773632e-07
Iter: 755 loss: 4.81035784e-07
Iter: 756 loss: 4.79739e-07
Iter: 757 loss: 4.79705704e-07
Iter: 758 loss: 4.79685866e-07
Iter: 759 loss: 4.79663868e-07
Iter: 760 loss: 4.79786536e-07
Iter: 761 loss: 4.79658524e-07
Iter: 762 loss: 4.79611799e-07
Iter: 763 loss: 4.79694791e-07
Iter: 764 loss: 4.79588266e-07
Iter: 765 loss: 4.79568428e-07
Iter: 766 loss: 4.79614528e-07
Iter: 767 loss: 4.79549499e-07
Iter: 768 loss: 4.79538699e-07
Iter: 769 loss: 4.79512323e-07
Iter: 770 loss: 4.79482708e-07
Iter: 771 loss: 4.794407e-07
Iter: 772 loss: 4.79514938e-07
Iter: 773 loss: 4.79434e-07
Iter: 774 loss: 4.79395453e-07
Iter: 775 loss: 4.79816435e-07
Iter: 776 loss: 4.79395396e-07
Iter: 777 loss: 4.79360779e-07
Iter: 778 loss: 4.7930007e-07
Iter: 779 loss: 4.80555968e-07
Iter: 780 loss: 4.79276423e-07
Iter: 781 loss: 4.79184564e-07
Iter: 782 loss: 4.79399091e-07
Iter: 783 loss: 4.79137e-07
Iter: 784 loss: 4.79079063e-07
Iter: 785 loss: 4.7906866e-07
Iter: 786 loss: 4.79025516e-07
Iter: 787 loss: 4.79106632e-07
Iter: 788 loss: 4.79003916e-07
Iter: 789 loss: 4.78933885e-07
Iter: 790 loss: 4.78863967e-07
Iter: 791 loss: 4.78853053e-07
Iter: 792 loss: 4.78840661e-07
Iter: 793 loss: 4.7883276e-07
Iter: 794 loss: 4.78810193e-07
Iter: 795 loss: 4.78780521e-07
Iter: 796 loss: 4.78759887e-07
Iter: 797 loss: 4.78721745e-07
Iter: 798 loss: 4.78947e-07
Iter: 799 loss: 4.78700031e-07
Iter: 800 loss: 4.78640857e-07
Iter: 801 loss: 4.78769266e-07
Iter: 802 loss: 4.78642335e-07
Iter: 803 loss: 4.78592085e-07
Iter: 804 loss: 4.7857111e-07
Iter: 805 loss: 4.78580773e-07
Iter: 806 loss: 4.78529103e-07
Iter: 807 loss: 4.78527227e-07
Iter: 808 loss: 4.78497554e-07
Iter: 809 loss: 4.78499089e-07
Iter: 810 loss: 4.7847584e-07
Iter: 811 loss: 4.78430593e-07
Iter: 812 loss: 4.78433606e-07
Iter: 813 loss: 4.78413654e-07
Iter: 814 loss: 4.7836761e-07
Iter: 815 loss: 4.78522622e-07
Iter: 816 loss: 4.78357492e-07
Iter: 817 loss: 4.78330776e-07
Iter: 818 loss: 4.78326115e-07
Iter: 819 loss: 4.78291497e-07
Iter: 820 loss: 4.7828604e-07
Iter: 821 loss: 4.78715265e-07
Iter: 822 loss: 4.78273762e-07
Iter: 823 loss: 4.7821618e-07
Iter: 824 loss: 4.78372328e-07
Iter: 825 loss: 4.78213678e-07
Iter: 826 loss: 4.78173718e-07
Iter: 827 loss: 4.78188042e-07
Iter: 828 loss: 4.78150184e-07
Iter: 829 loss: 4.78086804e-07
Iter: 830 loss: 4.78655693e-07
Iter: 831 loss: 4.78071343e-07
Iter: 832 loss: 4.78044853e-07
Iter: 833 loss: 4.78043376e-07
Iter: 834 loss: 4.78009042e-07
Iter: 835 loss: 4.77924686e-07
Iter: 836 loss: 4.79456276e-07
Iter: 837 loss: 4.77931508e-07
Iter: 838 loss: 4.77839649e-07
Iter: 839 loss: 4.78195432e-07
Iter: 840 loss: 4.77811568e-07
Iter: 841 loss: 4.77749154e-07
Iter: 842 loss: 4.7866547e-07
Iter: 843 loss: 4.77755407e-07
Iter: 844 loss: 4.77717208e-07
Iter: 845 loss: 4.77702429e-07
Iter: 846 loss: 4.77665822e-07
Iter: 847 loss: 4.77597212e-07
Iter: 848 loss: 4.77589083e-07
Iter: 849 loss: 4.7754952e-07
Iter: 850 loss: 4.77529795e-07
Iter: 851 loss: 4.77504e-07
Iter: 852 loss: 4.77472213e-07
Iter: 853 loss: 4.77467665e-07
Iter: 854 loss: 4.77439528e-07
Iter: 855 loss: 4.77405933e-07
Iter: 856 loss: 4.7735432e-07
Iter: 857 loss: 4.77354661e-07
Iter: 858 loss: 4.77304411e-07
Iter: 859 loss: 4.77871311e-07
Iter: 860 loss: 4.77305e-07
Iter: 861 loss: 4.77280707e-07
Iter: 862 loss: 4.77246147e-07
Iter: 863 loss: 4.77195e-07
Iter: 864 loss: 4.77163553e-07
Iter: 865 loss: 4.77644619e-07
Iter: 866 loss: 4.77175092e-07
Iter: 867 loss: 4.77135757e-07
Iter: 868 loss: 4.7717748e-07
Iter: 869 loss: 4.77101707e-07
Iter: 870 loss: 4.7709176e-07
Iter: 871 loss: 4.77072e-07
Iter: 872 loss: 4.77055664e-07
Iter: 873 loss: 4.77001379e-07
Iter: 874 loss: 4.77118419e-07
Iter: 875 loss: 4.76999844e-07
Iter: 876 loss: 4.76941921e-07
Iter: 877 loss: 4.7749063e-07
Iter: 878 loss: 4.76959372e-07
Iter: 879 loss: 4.76918274e-07
Iter: 880 loss: 4.76837045e-07
Iter: 881 loss: 4.77709762e-07
Iter: 882 loss: 4.76835822e-07
Iter: 883 loss: 4.76776165e-07
Iter: 884 loss: 4.77152071e-07
Iter: 885 loss: 4.7678563e-07
Iter: 886 loss: 4.76697949e-07
Iter: 887 loss: 4.77539857e-07
Iter: 888 loss: 4.76697267e-07
Iter: 889 loss: 4.76654066e-07
Iter: 890 loss: 4.76579885e-07
Iter: 891 loss: 4.76592845e-07
Iter: 892 loss: 4.76502663e-07
Iter: 893 loss: 4.76848925e-07
Iter: 894 loss: 4.76518096e-07
Iter: 895 loss: 4.76457387e-07
Iter: 896 loss: 4.7711228e-07
Iter: 897 loss: 4.76447525e-07
Iter: 898 loss: 4.7641862e-07
Iter: 899 loss: 4.76441983e-07
Iter: 900 loss: 4.76411032e-07
Iter: 901 loss: 4.76380023e-07
Iter: 902 loss: 4.76551605e-07
Iter: 903 loss: 4.76388948e-07
Iter: 904 loss: 4.7635848e-07
Iter: 905 loss: 4.76376641e-07
Iter: 906 loss: 4.76340318e-07
Iter: 907 loss: 4.76301466e-07
Iter: 908 loss: 4.76263665e-07
Iter: 909 loss: 4.76269491e-07
Iter: 910 loss: 4.76220464e-07
Iter: 911 loss: 4.76213245e-07
Iter: 912 loss: 4.76176325e-07
Iter: 913 loss: 4.76144294e-07
Iter: 914 loss: 4.77392632e-07
Iter: 915 loss: 4.76139178e-07
Iter: 916 loss: 4.7605846e-07
Iter: 917 loss: 4.76215433e-07
Iter: 918 loss: 4.7605198e-07
Iter: 919 loss: 4.75995e-07
Iter: 920 loss: 4.75987235e-07
Iter: 921 loss: 4.7592971e-07
Iter: 922 loss: 4.759342e-07
Iter: 923 loss: 4.75911236e-07
Iter: 924 loss: 4.75855359e-07
Iter: 925 loss: 4.75778933e-07
Iter: 926 loss: 4.75754405e-07
Iter: 927 loss: 4.75747157e-07
Iter: 928 loss: 4.75705576e-07
Iter: 929 loss: 4.75708219e-07
Iter: 930 loss: 4.75698727e-07
Iter: 931 loss: 4.75702734e-07
Iter: 932 loss: 4.75680395e-07
Iter: 933 loss: 4.7571092e-07
Iter: 934 loss: 4.75682072e-07
Iter: 935 loss: 4.75693753e-07
Iter: 936 loss: 4.75697277e-07
Iter: 937 loss: 4.75703558e-07
Iter: 938 loss: 4.75693383e-07
Iter: 939 loss: 4.75705406e-07
Iter: 940 loss: 4.75702876e-07
Iter: 941 loss: 4.75699835e-07
Iter: 942 loss: 4.75710891e-07
Iter: 943 loss: 4.75707878e-07
Iter: 944 loss: 4.757064e-07
Iter: 945 loss: 4.7570407e-07
Iter: 946 loss: 4.75705775e-07
Iter: 947 loss: 4.75707509e-07
Iter: 948 loss: 4.75707338e-07
Iter: 949 loss: 4.75705463e-07
Iter: 950 loss: 4.7570586e-07
Iter: 951 loss: 4.75706599e-07
Iter: 952 loss: 4.75706599e-07
Iter: 953 loss: 4.7570586e-07
Iter: 954 loss: 4.75706599e-07
Iter: 955 loss: 4.75574467e-07
Iter: 956 loss: 4.7681857e-07
Iter: 957 loss: 4.7559314e-07
Iter: 958 loss: 4.75541e-07
Iter: 959 loss: 4.75545704e-07
Iter: 960 loss: 4.75479425e-07
Iter: 961 loss: 4.75476583e-07
Iter: 962 loss: 4.75455522e-07
Iter: 963 loss: 4.75422638e-07
Iter: 964 loss: 4.75409877e-07
Iter: 965 loss: 4.75378584e-07
Iter: 966 loss: 4.75346894e-07
Iter: 967 loss: 4.75769951e-07
Iter: 968 loss: 4.75338766e-07
Iter: 969 loss: 4.75291273e-07
Iter: 970 loss: 4.75337515e-07
Iter: 971 loss: 4.75242359e-07
Iter: 972 loss: 4.75204672e-07
Iter: 973 loss: 4.75266432e-07
Iter: 974 loss: 4.75204502e-07
Iter: 975 loss: 4.75219963e-07
Iter: 976 loss: 4.7519606e-07
Iter: 977 loss: 4.75195804e-07
Iter: 978 loss: 4.75139842e-07
Iter: 979 loss: 4.75331149e-07
Iter: 980 loss: 4.75127109e-07
Iter: 981 loss: 4.75091468e-07
Iter: 982 loss: 4.75089223e-07
Iter: 983 loss: 4.75064894e-07
Iter: 984 loss: 4.75096869e-07
Iter: 985 loss: 4.7504966e-07
Iter: 986 loss: 4.75022404e-07
Iter: 987 loss: 4.74969141e-07
Iter: 988 loss: 4.74958398e-07
Iter: 989 loss: 4.74918323e-07
Iter: 990 loss: 4.7491659e-07
Iter: 991 loss: 4.74885866e-07
Iter: 992 loss: 4.74829e-07
Iter: 993 loss: 4.75443642e-07
Iter: 994 loss: 4.74844938e-07
Iter: 995 loss: 4.74802846e-07
Iter: 996 loss: 4.75211664e-07
Iter: 997 loss: 4.74786759e-07
Iter: 998 loss: 4.74738783e-07
Iter: 999 loss: 4.74772349e-07
Iter: 1000 loss: 4.7470121e-07
Iter: 1001 loss: 4.74674152e-07
Iter: 1002 loss: 4.74679069e-07
Iter: 1003 loss: 4.74654342e-07
Iter: 1004 loss: 4.74575188e-07
Iter: 1005 loss: 4.74875065e-07
Iter: 1006 loss: 4.74562228e-07
Iter: 1007 loss: 4.74542361e-07
Iter: 1008 loss: 4.74537728e-07
Iter: 1009 loss: 4.74522921e-07
Iter: 1010 loss: 4.74496659e-07
Iter: 1011 loss: 4.74595083e-07
Iter: 1012 loss: 4.74457863e-07
Iter: 1013 loss: 4.74449109e-07
Iter: 1014 loss: 4.74430522e-07
Iter: 1015 loss: 4.74428589e-07
Iter: 1016 loss: 4.74387264e-07
Iter: 1017 loss: 4.74373564e-07
Iter: 1018 loss: 4.74360405e-07
Iter: 1019 loss: 4.7444118e-07
Iter: 1020 loss: 4.74353726e-07
Iter: 1021 loss: 4.74319535e-07
Iter: 1022 loss: 4.7457732e-07
Iter: 1023 loss: 4.74324224e-07
Iter: 1024 loss: 4.74291397e-07
Iter: 1025 loss: 4.74242427e-07
Iter: 1026 loss: 4.74243052e-07
Iter: 1027 loss: 4.74207411e-07
Iter: 1028 loss: 4.74479691e-07
Iter: 1029 loss: 4.7421679e-07
Iter: 1030 loss: 4.74183878e-07
Iter: 1031 loss: 4.74331046e-07
Iter: 1032 loss: 4.74184e-07
Iter: 1033 loss: 4.74146049e-07
Iter: 1034 loss: 4.74153467e-07
Iter: 1035 loss: 4.74622112e-07
Iter: 1036 loss: 4.74134623e-07
Iter: 1037 loss: 4.74091792e-07
Iter: 1038 loss: 4.74088353e-07
Iter: 1039 loss: 4.7404788e-07
Iter: 1040 loss: 4.74086534e-07
Iter: 1041 loss: 4.74040434e-07
Iter: 1042 loss: 4.7401943e-07
Iter: 1043 loss: 4.73978162e-07
Iter: 1044 loss: 4.7400215e-07
Iter: 1045 loss: 4.73905629e-07
Iter: 1046 loss: 4.74165e-07
Iter: 1047 loss: 4.73915065e-07
Iter: 1048 loss: 4.73862826e-07
Iter: 1049 loss: 4.73865413e-07
Iter: 1050 loss: 4.73820364e-07
Iter: 1051 loss: 4.73798053e-07
Iter: 1052 loss: 4.73805443e-07
Iter: 1053 loss: 4.73759343e-07
Iter: 1054 loss: 4.73747491e-07
Iter: 1055 loss: 4.73710628e-07
Iter: 1056 loss: 4.7372464e-07
Iter: 1057 loss: 4.73683883e-07
Iter: 1058 loss: 4.73661203e-07
Iter: 1059 loss: 4.7365171e-07
Iter: 1060 loss: 4.74072635e-07
Iter: 1061 loss: 4.73618911e-07
Iter: 1062 loss: 4.73573664e-07
Iter: 1063 loss: 4.73832586e-07
Iter: 1064 loss: 4.73587818e-07
Iter: 1065 loss: 4.73557435e-07
Iter: 1066 loss: 4.73650516e-07
Iter: 1067 loss: 4.73542912e-07
Iter: 1068 loss: 4.73508635e-07
Iter: 1069 loss: 4.73927884e-07
Iter: 1070 loss: 4.73477172e-07
Iter: 1071 loss: 4.73479162e-07
Iter: 1072 loss: 4.73500791e-07
Iter: 1073 loss: 4.7349269e-07
Iter: 1074 loss: 4.7348172e-07
Iter: 1075 loss: 4.73491838e-07
Iter: 1076 loss: 4.73491866e-07
Iter: 1077 loss: 4.73497437e-07
Iter: 1078 loss: 4.73494367e-07
Iter: 1079 loss: 4.7348783e-07
Iter: 1080 loss: 4.73487603e-07
Iter: 1081 loss: 4.734855e-07
Iter: 1082 loss: 4.73483652e-07
Iter: 1083 loss: 4.73477229e-07
Iter: 1084 loss: 4.73479304e-07
Iter: 1085 loss: 4.73479133e-07
Iter: 1086 loss: 4.73477257e-07
Iter: 1087 loss: 4.73478082e-07
Iter: 1088 loss: 4.73478e-07
Iter: 1089 loss: 4.73477769e-07
Iter: 1090 loss: 4.73477201e-07
Iter: 1091 loss: 4.73477598e-07
Iter: 1092 loss: 4.73477741e-07
Iter: 1093 loss: 4.73477598e-07
Iter: 1094 loss: 4.73477598e-07
Iter: 1095 loss: 4.73477741e-07
Iter: 1096 loss: 4.73477598e-07
Iter: 1097 loss: 4.73477598e-07
Iter: 1098 loss: 4.73477741e-07
Iter: 1099 loss: 4.73477741e-07
Iter: 1100 loss: 4.73477598e-07
Iter: 1101 loss: 4.73477741e-07
Iter: 1102 loss: 4.73477598e-07
Iter: 1103 loss: 4.73477741e-07
Iter: 1104 loss: 4.73477598e-07
Iter: 1105 loss: 4.73477741e-07
Iter: 1106 loss: 4.73477741e-07
Iter: 1107 loss: 4.73477598e-07
Iter: 1108 loss: 4.73508294e-07
Iter: 1109 loss: 4.73495732e-07
Iter: 1110 loss: 4.73480384e-07
Iter: 1111 loss: 4.73494509e-07
Iter: 1112 loss: 4.73502325e-07
Iter: 1113 loss: 4.73489962e-07
Iter: 1114 loss: 4.73488626e-07
Iter: 1115 loss: 4.73487887e-07
Iter: 1116 loss: 4.73476859e-07
Iter: 1117 loss: 4.73477911e-07
Iter: 1118 loss: 4.73475126e-07
Iter: 1119 loss: 4.73494197e-07
Iter: 1120 loss: 4.73480185e-07
Iter: 1121 loss: 4.73481634e-07
Iter: 1122 loss: 4.73479759e-07
Iter: 1123 loss: 4.73478281e-07
Iter: 1124 loss: 4.73475581e-07
Iter: 1125 loss: 4.73476206e-07
Iter: 1126 loss: 4.73477883e-07
Iter: 1127 loss: 4.73477513e-07
Iter: 1128 loss: 4.7347811e-07
Iter: 1129 loss: 4.73477428e-07
Iter: 1130 loss: 4.73477655e-07
Iter: 1131 loss: 4.7347811e-07
Iter: 1132 loss: 4.73477854e-07
Iter: 1133 loss: 4.7347811e-07
Iter: 1134 loss: 4.7347811e-07
Iter: 1135 loss: 4.73477854e-07
Iter: 1136 loss: 4.7347811e-07
Iter: 1137 loss: 4.7347811e-07
Iter: 1138 loss: 4.7347811e-07
Iter: 1139 loss: 4.73477854e-07
Iter: 1140 loss: 4.7347811e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6
+ date
Mon Oct 26 11:49:20 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.6_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.6_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.6_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.6 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi1.6_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f940176a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f940ef730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f940eb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f940eb1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5fbb253bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f9406e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f807cb730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f8077d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f8077d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f8077d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f807692f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f8072e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f8072e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f806de0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f806d9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f806de488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f8065cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f80662620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f80662378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f80628400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f805bda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f805840d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f805bd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f8051d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f8051d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f8051dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f8051d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f8047e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f8047ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f8048b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f80404268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f80442f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f80442730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f80435d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f803e90d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f803bbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.64883044e-06
Iter: 2 loss: 7.75274748e-06
Iter: 3 loss: 1.76170556e-06
Iter: 4 loss: 1.44828493e-06
Iter: 5 loss: 1.82841632e-06
Iter: 6 loss: 1.28377599e-06
Iter: 7 loss: 1.07285609e-06
Iter: 8 loss: 1.29289811e-06
Iter: 9 loss: 9.5592361e-07
Iter: 10 loss: 8.20388948e-07
Iter: 11 loss: 1.40109864e-06
Iter: 12 loss: 7.92396918e-07
Iter: 13 loss: 7.16168131e-07
Iter: 14 loss: 1.89894604e-06
Iter: 15 loss: 7.16157615e-07
Iter: 16 loss: 6.690492e-07
Iter: 17 loss: 6.57781698e-07
Iter: 18 loss: 6.27738586e-07
Iter: 19 loss: 6.10615871e-07
Iter: 20 loss: 8.10024289e-07
Iter: 21 loss: 6.10379914e-07
Iter: 22 loss: 5.95205847e-07
Iter: 23 loss: 6.33358241e-07
Iter: 24 loss: 5.89926e-07
Iter: 25 loss: 5.82552389e-07
Iter: 26 loss: 5.96974701e-07
Iter: 27 loss: 5.79497964e-07
Iter: 28 loss: 5.72863769e-07
Iter: 29 loss: 5.83508154e-07
Iter: 30 loss: 5.69750512e-07
Iter: 31 loss: 5.63562367e-07
Iter: 32 loss: 5.73234843e-07
Iter: 33 loss: 5.6062845e-07
Iter: 34 loss: 5.55114468e-07
Iter: 35 loss: 5.80246365e-07
Iter: 36 loss: 5.54068492e-07
Iter: 37 loss: 5.5169761e-07
Iter: 38 loss: 5.50952564e-07
Iter: 39 loss: 5.48290814e-07
Iter: 40 loss: 5.47128707e-07
Iter: 41 loss: 5.45770263e-07
Iter: 42 loss: 5.43200429e-07
Iter: 43 loss: 5.4712234e-07
Iter: 44 loss: 5.41939812e-07
Iter: 45 loss: 5.37622213e-07
Iter: 46 loss: 5.35587503e-07
Iter: 47 loss: 5.33428647e-07
Iter: 48 loss: 5.31844307e-07
Iter: 49 loss: 5.31454191e-07
Iter: 50 loss: 5.29636509e-07
Iter: 51 loss: 5.26253643e-07
Iter: 52 loss: 6.03762487e-07
Iter: 53 loss: 5.26270696e-07
Iter: 54 loss: 5.21994934e-07
Iter: 55 loss: 5.315257e-07
Iter: 56 loss: 5.20395076e-07
Iter: 57 loss: 5.17341448e-07
Iter: 58 loss: 5.43296835e-07
Iter: 59 loss: 5.17201556e-07
Iter: 60 loss: 5.13757641e-07
Iter: 61 loss: 5.13723421e-07
Iter: 62 loss: 5.10946506e-07
Iter: 63 loss: 5.07720301e-07
Iter: 64 loss: 5.04570721e-07
Iter: 65 loss: 5.0387149e-07
Iter: 66 loss: 4.98733527e-07
Iter: 67 loss: 5.44564045e-07
Iter: 68 loss: 4.98481882e-07
Iter: 69 loss: 4.94482151e-07
Iter: 70 loss: 5.00005171e-07
Iter: 71 loss: 4.92445395e-07
Iter: 72 loss: 4.89885338e-07
Iter: 73 loss: 4.9227009e-07
Iter: 74 loss: 4.88449473e-07
Iter: 75 loss: 4.89210777e-07
Iter: 76 loss: 4.86914132e-07
Iter: 77 loss: 4.86417093e-07
Iter: 78 loss: 4.85432338e-07
Iter: 79 loss: 5.04562422e-07
Iter: 80 loss: 4.85424948e-07
Iter: 81 loss: 4.8456161e-07
Iter: 82 loss: 4.90112257e-07
Iter: 83 loss: 4.84440932e-07
Iter: 84 loss: 4.83462543e-07
Iter: 85 loss: 4.83732322e-07
Iter: 86 loss: 4.82821e-07
Iter: 87 loss: 4.8142283e-07
Iter: 88 loss: 4.84027169e-07
Iter: 89 loss: 4.80813128e-07
Iter: 90 loss: 4.79424443e-07
Iter: 91 loss: 4.92811523e-07
Iter: 92 loss: 4.79349637e-07
Iter: 93 loss: 4.78755226e-07
Iter: 94 loss: 4.77103185e-07
Iter: 95 loss: 4.87485863e-07
Iter: 96 loss: 4.76687973e-07
Iter: 97 loss: 4.76519944e-07
Iter: 98 loss: 4.75757673e-07
Iter: 99 loss: 4.75053554e-07
Iter: 100 loss: 4.74207468e-07
Iter: 101 loss: 4.7414224e-07
Iter: 102 loss: 4.72815259e-07
Iter: 103 loss: 4.74919403e-07
Iter: 104 loss: 4.72239492e-07
Iter: 105 loss: 4.71023156e-07
Iter: 106 loss: 4.71845254e-07
Iter: 107 loss: 4.70214729e-07
Iter: 108 loss: 4.6902781e-07
Iter: 109 loss: 4.71788667e-07
Iter: 110 loss: 4.68573461e-07
Iter: 111 loss: 4.68395484e-07
Iter: 112 loss: 4.67853113e-07
Iter: 113 loss: 4.67489343e-07
Iter: 114 loss: 4.66742733e-07
Iter: 115 loss: 4.79313e-07
Iter: 116 loss: 4.66714795e-07
Iter: 117 loss: 4.65803396e-07
Iter: 118 loss: 4.65838e-07
Iter: 119 loss: 4.65091205e-07
Iter: 120 loss: 4.64434038e-07
Iter: 121 loss: 4.643382e-07
Iter: 122 loss: 4.63860147e-07
Iter: 123 loss: 4.62908417e-07
Iter: 124 loss: 4.80453082e-07
Iter: 125 loss: 4.62918081e-07
Iter: 126 loss: 4.61643367e-07
Iter: 127 loss: 4.74463491e-07
Iter: 128 loss: 4.61602696e-07
Iter: 129 loss: 4.61161079e-07
Iter: 130 loss: 4.60760873e-07
Iter: 131 loss: 4.6071068e-07
Iter: 132 loss: 4.60137869e-07
Iter: 133 loss: 4.62826279e-07
Iter: 134 loss: 4.60056242e-07
Iter: 135 loss: 4.59589842e-07
Iter: 136 loss: 4.60827778e-07
Iter: 137 loss: 4.59446284e-07
Iter: 138 loss: 4.59167325e-07
Iter: 139 loss: 4.59151465e-07
Iter: 140 loss: 4.58942566e-07
Iter: 141 loss: 4.58451154e-07
Iter: 142 loss: 4.62526629e-07
Iter: 143 loss: 4.583151e-07
Iter: 144 loss: 4.58332494e-07
Iter: 145 loss: 4.58052568e-07
Iter: 146 loss: 4.57818714e-07
Iter: 147 loss: 4.57744534e-07
Iter: 148 loss: 4.57581507e-07
Iter: 149 loss: 4.57249257e-07
Iter: 150 loss: 4.56741958e-07
Iter: 151 loss: 4.56724706e-07
Iter: 152 loss: 4.56215162e-07
Iter: 153 loss: 4.59073334e-07
Iter: 154 loss: 4.56143596e-07
Iter: 155 loss: 4.55611939e-07
Iter: 156 loss: 4.58883534e-07
Iter: 157 loss: 4.55540061e-07
Iter: 158 loss: 4.55201e-07
Iter: 159 loss: 4.55934099e-07
Iter: 160 loss: 4.55065674e-07
Iter: 161 loss: 4.54702359e-07
Iter: 162 loss: 4.55006045e-07
Iter: 163 loss: 4.54431927e-07
Iter: 164 loss: 4.53984569e-07
Iter: 165 loss: 4.55229355e-07
Iter: 166 loss: 4.53865852e-07
Iter: 167 loss: 4.53553639e-07
Iter: 168 loss: 4.52868335e-07
Iter: 169 loss: 4.63836386e-07
Iter: 170 loss: 4.52847871e-07
Iter: 171 loss: 4.52702466e-07
Iter: 172 loss: 4.5250323e-07
Iter: 173 loss: 4.52199288e-07
Iter: 174 loss: 4.52799156e-07
Iter: 175 loss: 4.52080656e-07
Iter: 176 loss: 4.51718734e-07
Iter: 177 loss: 4.51758041e-07
Iter: 178 loss: 4.51473824e-07
Iter: 179 loss: 4.51286411e-07
Iter: 180 loss: 4.51246194e-07
Iter: 181 loss: 4.50998613e-07
Iter: 182 loss: 4.50692369e-07
Iter: 183 loss: 4.50665823e-07
Iter: 184 loss: 4.50368702e-07
Iter: 185 loss: 4.50041398e-07
Iter: 186 loss: 4.50010816e-07
Iter: 187 loss: 4.49716083e-07
Iter: 188 loss: 4.49668249e-07
Iter: 189 loss: 4.49478193e-07
Iter: 190 loss: 4.50002432e-07
Iter: 191 loss: 4.49400318e-07
Iter: 192 loss: 4.49239849e-07
Iter: 193 loss: 4.49603476e-07
Iter: 194 loss: 4.49175673e-07
Iter: 195 loss: 4.48960066e-07
Iter: 196 loss: 4.49101947e-07
Iter: 197 loss: 4.48820458e-07
Iter: 198 loss: 4.48609626e-07
Iter: 199 loss: 4.48487754e-07
Iter: 200 loss: 4.48392541e-07
Iter: 201 loss: 4.48087064e-07
Iter: 202 loss: 4.49859783e-07
Iter: 203 loss: 4.4805725e-07
Iter: 204 loss: 4.47809526e-07
Iter: 205 loss: 4.49383066e-07
Iter: 206 loss: 4.47779655e-07
Iter: 207 loss: 4.47566293e-07
Iter: 208 loss: 4.48901858e-07
Iter: 209 loss: 4.47555237e-07
Iter: 210 loss: 4.47449395e-07
Iter: 211 loss: 4.47315188e-07
Iter: 212 loss: 4.47306888e-07
Iter: 213 loss: 4.47008e-07
Iter: 214 loss: 4.4874426e-07
Iter: 215 loss: 4.46978362e-07
Iter: 216 loss: 4.46867261e-07
Iter: 217 loss: 4.4668883e-07
Iter: 218 loss: 4.46663194e-07
Iter: 219 loss: 4.46473479e-07
Iter: 220 loss: 4.46628292e-07
Iter: 221 loss: 4.46301556e-07
Iter: 222 loss: 4.4613e-07
Iter: 223 loss: 4.46109482e-07
Iter: 224 loss: 4.45931704e-07
Iter: 225 loss: 4.45719053e-07
Iter: 226 loss: 4.45691512e-07
Iter: 227 loss: 4.45515241e-07
Iter: 228 loss: 4.4548716e-07
Iter: 229 loss: 4.4538e-07
Iter: 230 loss: 4.45230057e-07
Iter: 231 loss: 4.45189983e-07
Iter: 232 loss: 4.45010869e-07
Iter: 233 loss: 4.45692621e-07
Iter: 234 loss: 4.44954878e-07
Iter: 235 loss: 4.44783439e-07
Iter: 236 loss: 4.4439588e-07
Iter: 237 loss: 4.52179535e-07
Iter: 238 loss: 4.44372859e-07
Iter: 239 loss: 4.44455026e-07
Iter: 240 loss: 4.44224497e-07
Iter: 241 loss: 4.44086766e-07
Iter: 242 loss: 4.44017473e-07
Iter: 243 loss: 4.43953354e-07
Iter: 244 loss: 4.43819175e-07
Iter: 245 loss: 4.43819715e-07
Iter: 246 loss: 4.43744113e-07
Iter: 247 loss: 4.43652283e-07
Iter: 248 loss: 4.43618603e-07
Iter: 249 loss: 4.43465439e-07
Iter: 250 loss: 4.43456543e-07
Iter: 251 loss: 4.43360079e-07
Iter: 252 loss: 4.43123554e-07
Iter: 253 loss: 4.43468565e-07
Iter: 254 loss: 4.43034821e-07
Iter: 255 loss: 4.42883447e-07
Iter: 256 loss: 4.42850535e-07
Iter: 257 loss: 4.42725451e-07
Iter: 258 loss: 4.42810233e-07
Iter: 259 loss: 4.42669545e-07
Iter: 260 loss: 4.4250956e-07
Iter: 261 loss: 4.4244436e-07
Iter: 262 loss: 4.42395361e-07
Iter: 263 loss: 4.42211871e-07
Iter: 264 loss: 4.44841731e-07
Iter: 265 loss: 4.42194732e-07
Iter: 266 loss: 4.42060667e-07
Iter: 267 loss: 4.41873681e-07
Iter: 268 loss: 4.41853786e-07
Iter: 269 loss: 4.4170946e-07
Iter: 270 loss: 4.42246346e-07
Iter: 271 loss: 4.41655175e-07
Iter: 272 loss: 4.41469268e-07
Iter: 273 loss: 4.42161706e-07
Iter: 274 loss: 4.41458184e-07
Iter: 275 loss: 4.41370901e-07
Iter: 276 loss: 4.42833652e-07
Iter: 277 loss: 4.4137289e-07
Iter: 278 loss: 4.41261079e-07
Iter: 279 loss: 4.41171778e-07
Iter: 280 loss: 4.41155805e-07
Iter: 281 loss: 4.41161632e-07
Iter: 282 loss: 4.41101093e-07
Iter: 283 loss: 4.41028362e-07
Iter: 284 loss: 4.40903506e-07
Iter: 285 loss: 4.40949464e-07
Iter: 286 loss: 4.40776489e-07
Iter: 287 loss: 4.40556789e-07
Iter: 288 loss: 4.42488329e-07
Iter: 289 loss: 4.40574951e-07
Iter: 290 loss: 4.40410417e-07
Iter: 291 loss: 4.41051839e-07
Iter: 292 loss: 4.4039723e-07
Iter: 293 loss: 4.40181509e-07
Iter: 294 loss: 4.40407291e-07
Iter: 295 loss: 4.40128929e-07
Iter: 296 loss: 4.39990743e-07
Iter: 297 loss: 4.39841699e-07
Iter: 298 loss: 4.3979793e-07
Iter: 299 loss: 4.39632799e-07
Iter: 300 loss: 4.39639592e-07
Iter: 301 loss: 4.394835e-07
Iter: 302 loss: 4.3943183e-07
Iter: 303 loss: 4.39297338e-07
Iter: 304 loss: 4.39169952e-07
Iter: 305 loss: 4.39890044e-07
Iter: 306 loss: 4.3914082e-07
Iter: 307 loss: 4.39011274e-07
Iter: 308 loss: 4.38939452e-07
Iter: 309 loss: 4.38907477e-07
Iter: 310 loss: 4.38753801e-07
Iter: 311 loss: 4.38772673e-07
Iter: 312 loss: 4.38690222e-07
Iter: 313 loss: 4.38877947e-07
Iter: 314 loss: 4.38628064e-07
Iter: 315 loss: 4.38532652e-07
Iter: 316 loss: 4.38771224e-07
Iter: 317 loss: 4.38489934e-07
Iter: 318 loss: 4.384232e-07
Iter: 319 loss: 4.38775459e-07
Iter: 320 loss: 4.38384745e-07
Iter: 321 loss: 4.38328982e-07
Iter: 322 loss: 4.38135373e-07
Iter: 323 loss: 4.40090616e-07
Iter: 324 loss: 4.38107406e-07
Iter: 325 loss: 4.3809672e-07
Iter: 326 loss: 4.38004861e-07
Iter: 327 loss: 4.37983488e-07
Iter: 328 loss: 4.3780534e-07
Iter: 329 loss: 4.40398878e-07
Iter: 330 loss: 4.37797098e-07
Iter: 331 loss: 4.37655672e-07
Iter: 332 loss: 4.38889515e-07
Iter: 333 loss: 4.37656581e-07
Iter: 334 loss: 4.37566939e-07
Iter: 335 loss: 4.37339224e-07
Iter: 336 loss: 4.4088884e-07
Iter: 337 loss: 4.37347751e-07
Iter: 338 loss: 4.37334563e-07
Iter: 339 loss: 4.37252368e-07
Iter: 340 loss: 4.3715238e-07
Iter: 341 loss: 4.37110828e-07
Iter: 342 loss: 4.37070241e-07
Iter: 343 loss: 4.36935579e-07
Iter: 344 loss: 4.37009334e-07
Iter: 345 loss: 4.36864724e-07
Iter: 346 loss: 4.36706785e-07
Iter: 347 loss: 4.37833e-07
Iter: 348 loss: 4.36733103e-07
Iter: 349 loss: 4.36560583e-07
Iter: 350 loss: 4.36925802e-07
Iter: 351 loss: 4.36563624e-07
Iter: 352 loss: 4.36453803e-07
Iter: 353 loss: 4.36939615e-07
Iter: 354 loss: 4.36442519e-07
Iter: 355 loss: 4.36348472e-07
Iter: 356 loss: 4.36183825e-07
Iter: 357 loss: 4.3924581e-07
Iter: 358 loss: 4.3619778e-07
Iter: 359 loss: 4.36099299e-07
Iter: 360 loss: 4.37488325e-07
Iter: 361 loss: 4.3609225e-07
Iter: 362 loss: 4.35970463e-07
Iter: 363 loss: 4.3613332e-07
Iter: 364 loss: 4.35914075e-07
Iter: 365 loss: 4.35837791e-07
Iter: 366 loss: 4.35761251e-07
Iter: 367 loss: 4.35760541e-07
Iter: 368 loss: 4.35604136e-07
Iter: 369 loss: 4.36007525e-07
Iter: 370 loss: 4.35557865e-07
Iter: 371 loss: 4.35445372e-07
Iter: 372 loss: 4.35591517e-07
Iter: 373 loss: 4.35383555e-07
Iter: 374 loss: 4.35214531e-07
Iter: 375 loss: 4.36909119e-07
Iter: 376 loss: 4.35255657e-07
Iter: 377 loss: 4.35162e-07
Iter: 378 loss: 4.35154845e-07
Iter: 379 loss: 4.35105846e-07
Iter: 380 loss: 4.34973146e-07
Iter: 381 loss: 4.35046445e-07
Iter: 382 loss: 4.34859487e-07
Iter: 383 loss: 4.3480253e-07
Iter: 384 loss: 4.34802303e-07
Iter: 385 loss: 4.34745402e-07
Iter: 386 loss: 4.34840388e-07
Iter: 387 loss: 4.34721699e-07
Iter: 388 loss: 4.34692225e-07
Iter: 389 loss: 4.34573536e-07
Iter: 390 loss: 4.34563219e-07
Iter: 391 loss: 4.34495121e-07
Iter: 392 loss: 4.34718913e-07
Iter: 393 loss: 4.34466813e-07
Iter: 394 loss: 4.34347754e-07
Iter: 395 loss: 4.35452705e-07
Iter: 396 loss: 4.34345367e-07
Iter: 397 loss: 4.34303786e-07
Iter: 398 loss: 4.34250637e-07
Iter: 399 loss: 4.34236682e-07
Iter: 400 loss: 4.34128538e-07
Iter: 401 loss: 4.34075758e-07
Iter: 402 loss: 4.34062486e-07
Iter: 403 loss: 4.33895792e-07
Iter: 404 loss: 4.34294265e-07
Iter: 405 loss: 4.33816496e-07
Iter: 406 loss: 4.33621835e-07
Iter: 407 loss: 4.34870401e-07
Iter: 408 loss: 4.33638775e-07
Iter: 409 loss: 4.33490584e-07
Iter: 410 loss: 4.34826461e-07
Iter: 411 loss: 4.33500816e-07
Iter: 412 loss: 4.33406285e-07
Iter: 413 loss: 4.33251728e-07
Iter: 414 loss: 4.3720425e-07
Iter: 415 loss: 4.33239393e-07
Iter: 416 loss: 4.3313679e-07
Iter: 417 loss: 4.34378023e-07
Iter: 418 loss: 4.3313932e-07
Iter: 419 loss: 4.32964214e-07
Iter: 420 loss: 4.3406456e-07
Iter: 421 loss: 4.32966715e-07
Iter: 422 loss: 4.3292448e-07
Iter: 423 loss: 4.33555329e-07
Iter: 424 loss: 4.32894581e-07
Iter: 425 loss: 4.32858457e-07
Iter: 426 loss: 4.32776375e-07
Iter: 427 loss: 4.33350777e-07
Iter: 428 loss: 4.32743775e-07
Iter: 429 loss: 4.32624e-07
Iter: 430 loss: 4.33215973e-07
Iter: 431 loss: 4.32589104e-07
Iter: 432 loss: 4.32531806e-07
Iter: 433 loss: 4.3253084e-07
Iter: 434 loss: 4.32478544e-07
Iter: 435 loss: 4.32397655e-07
Iter: 436 loss: 4.33089298e-07
Iter: 437 loss: 4.32376027e-07
Iter: 438 loss: 4.32265097e-07
Iter: 439 loss: 4.32766086e-07
Iter: 440 loss: 4.32273538e-07
Iter: 441 loss: 4.32167781e-07
Iter: 442 loss: 4.32280075e-07
Iter: 443 loss: 4.32148312e-07
Iter: 444 loss: 4.32046221e-07
Iter: 445 loss: 4.32201716e-07
Iter: 446 loss: 4.31978606e-07
Iter: 447 loss: 4.31883194e-07
Iter: 448 loss: 4.32983143e-07
Iter: 449 loss: 4.31883592e-07
Iter: 450 loss: 4.31797361e-07
Iter: 451 loss: 4.31726448e-07
Iter: 452 loss: 4.31724e-07
Iter: 453 loss: 4.31653149e-07
Iter: 454 loss: 4.31648573e-07
Iter: 455 loss: 4.31595481e-07
Iter: 456 loss: 4.31688647e-07
Iter: 457 loss: 4.31558362e-07
Iter: 458 loss: 4.31490065e-07
Iter: 459 loss: 4.31562341e-07
Iter: 460 loss: 4.31439702e-07
Iter: 461 loss: 4.31380954e-07
Iter: 462 loss: 4.31310923e-07
Iter: 463 loss: 4.31296542e-07
Iter: 464 loss: 4.31228841e-07
Iter: 465 loss: 4.31965304e-07
Iter: 466 loss: 4.31209287e-07
Iter: 467 loss: 4.31120185e-07
Iter: 468 loss: 4.31579309e-07
Iter: 469 loss: 4.31114159e-07
Iter: 470 loss: 4.31064962e-07
Iter: 471 loss: 4.30960654e-07
Iter: 472 loss: 4.30950678e-07
Iter: 473 loss: 4.30878572e-07
Iter: 474 loss: 4.3088113e-07
Iter: 475 loss: 4.30829118e-07
Iter: 476 loss: 4.3081053e-07
Iter: 477 loss: 4.30782791e-07
Iter: 478 loss: 4.30713669e-07
Iter: 479 loss: 4.30670269e-07
Iter: 480 loss: 4.30659668e-07
Iter: 481 loss: 4.30544532e-07
Iter: 482 loss: 4.30720092e-07
Iter: 483 loss: 4.30536062e-07
Iter: 484 loss: 4.30447045e-07
Iter: 485 loss: 4.31091451e-07
Iter: 486 loss: 4.30436e-07
Iter: 487 loss: 4.30379259e-07
Iter: 488 loss: 4.30536403e-07
Iter: 489 loss: 4.30363457e-07
Iter: 490 loss: 4.30341231e-07
Iter: 491 loss: 4.30331227e-07
Iter: 492 loss: 4.30283876e-07
Iter: 493 loss: 4.30254431e-07
Iter: 494 loss: 4.30233115e-07
Iter: 495 loss: 4.30170815e-07
Iter: 496 loss: 4.30488683e-07
Iter: 497 loss: 4.30160782e-07
Iter: 498 loss: 4.3011994e-07
Iter: 499 loss: 4.30066592e-07
Iter: 500 loss: 4.30055621e-07
Iter: 501 loss: 4.30009436e-07
Iter: 502 loss: 4.30277481e-07
Iter: 503 loss: 4.29985846e-07
Iter: 504 loss: 4.29900695e-07
Iter: 505 loss: 4.30319744e-07
Iter: 506 loss: 4.29893845e-07
Iter: 507 loss: 4.2985593e-07
Iter: 508 loss: 4.29828219e-07
Iter: 509 loss: 4.29774559e-07
Iter: 510 loss: 4.29743e-07
Iter: 511 loss: 4.30543082e-07
Iter: 512 loss: 4.29737867e-07
Iter: 513 loss: 4.29684377e-07
Iter: 514 loss: 4.29643478e-07
Iter: 515 loss: 4.29613465e-07
Iter: 516 loss: 4.29537522e-07
Iter: 517 loss: 4.29742954e-07
Iter: 518 loss: 4.29514671e-07
Iter: 519 loss: 4.29432532e-07
Iter: 520 loss: 4.29299689e-07
Iter: 521 loss: 4.29293721e-07
Iter: 522 loss: 4.29187793e-07
Iter: 523 loss: 4.29183359e-07
Iter: 524 loss: 4.29135838e-07
Iter: 525 loss: 4.29435374e-07
Iter: 526 loss: 4.29116028e-07
Iter: 527 loss: 4.29027494e-07
Iter: 528 loss: 4.29119694e-07
Iter: 529 loss: 4.29017064e-07
Iter: 530 loss: 4.28939046e-07
Iter: 531 loss: 4.2919595e-07
Iter: 532 loss: 4.28914944e-07
Iter: 533 loss: 4.28914689e-07
Iter: 534 loss: 4.28937824e-07
Iter: 535 loss: 4.28905878e-07
Iter: 536 loss: 4.28833857e-07
Iter: 537 loss: 4.28969088e-07
Iter: 538 loss: 4.28802252e-07
Iter: 539 loss: 4.28777241e-07
Iter: 540 loss: 4.289885e-07
Iter: 541 loss: 4.28751719e-07
Iter: 542 loss: 4.28732449e-07
Iter: 543 loss: 4.28701469e-07
Iter: 544 loss: 4.28693681e-07
Iter: 545 loss: 4.28657e-07
Iter: 546 loss: 4.29256119e-07
Iter: 547 loss: 4.28653976e-07
Iter: 548 loss: 4.28620524e-07
Iter: 549 loss: 4.2859e-07
Iter: 550 loss: 4.28578517e-07
Iter: 551 loss: 4.28521162e-07
Iter: 552 loss: 4.28592557e-07
Iter: 553 loss: 4.28506468e-07
Iter: 554 loss: 4.28394515e-07
Iter: 555 loss: 4.28529177e-07
Iter: 556 loss: 4.28357907e-07
Iter: 557 loss: 4.28265821e-07
Iter: 558 loss: 4.28448573e-07
Iter: 559 loss: 4.2824621e-07
Iter: 560 loss: 4.28162792e-07
Iter: 561 loss: 4.28187718e-07
Iter: 562 loss: 4.281207e-07
Iter: 563 loss: 4.28399716e-07
Iter: 564 loss: 4.28124366e-07
Iter: 565 loss: 4.2811368e-07
Iter: 566 loss: 4.28031967e-07
Iter: 567 loss: 4.29202771e-07
Iter: 568 loss: 4.28048622e-07
Iter: 569 loss: 4.27984105e-07
Iter: 570 loss: 4.28369447e-07
Iter: 571 loss: 4.27949317e-07
Iter: 572 loss: 4.2789992e-07
Iter: 573 loss: 4.27947867e-07
Iter: 574 loss: 4.27873402e-07
Iter: 575 loss: 4.27832504e-07
Iter: 576 loss: 4.28187889e-07
Iter: 577 loss: 4.27829661e-07
Iter: 578 loss: 4.27793282e-07
Iter: 579 loss: 4.27826592e-07
Iter: 580 loss: 4.2777674e-07
Iter: 581 loss: 4.27726775e-07
Iter: 582 loss: 4.27840462e-07
Iter: 583 loss: 4.27748773e-07
Iter: 584 loss: 4.27677662e-07
Iter: 585 loss: 4.27678799e-07
Iter: 586 loss: 4.2763736e-07
Iter: 587 loss: 4.27553573e-07
Iter: 588 loss: 4.27662201e-07
Iter: 589 loss: 4.27541806e-07
Iter: 590 loss: 4.27478454e-07
Iter: 591 loss: 4.27709722e-07
Iter: 592 loss: 4.27504716e-07
Iter: 593 loss: 4.27450317e-07
Iter: 594 loss: 4.27442842e-07
Iter: 595 loss: 4.27388272e-07
Iter: 596 loss: 4.27359879e-07
Iter: 597 loss: 4.27346293e-07
Iter: 598 loss: 4.27338477e-07
Iter: 599 loss: 4.27543796e-07
Iter: 600 loss: 4.27310681e-07
Iter: 601 loss: 4.27277882e-07
Iter: 602 loss: 4.27197648e-07
Iter: 603 loss: 4.28072326e-07
Iter: 604 loss: 4.27196312e-07
Iter: 605 loss: 4.27123183e-07
Iter: 606 loss: 4.2714413e-07
Iter: 607 loss: 4.27055227e-07
Iter: 608 loss: 4.26984712e-07
Iter: 609 loss: 4.28038845e-07
Iter: 610 loss: 4.26979057e-07
Iter: 611 loss: 4.26900044e-07
Iter: 612 loss: 4.27033683e-07
Iter: 613 loss: 4.26841297e-07
Iter: 614 loss: 4.26784055e-07
Iter: 615 loss: 4.27151036e-07
Iter: 616 loss: 4.26762654e-07
Iter: 617 loss: 4.26752706e-07
Iter: 618 loss: 4.26794315e-07
Iter: 619 loss: 4.26716184e-07
Iter: 620 loss: 4.26648626e-07
Iter: 621 loss: 4.26605055e-07
Iter: 622 loss: 4.26594852e-07
Iter: 623 loss: 4.26559325e-07
Iter: 624 loss: 4.26550656e-07
Iter: 625 loss: 4.26513424e-07
Iter: 626 loss: 4.26599229e-07
Iter: 627 loss: 4.26465249e-07
Iter: 628 loss: 4.26422105e-07
Iter: 629 loss: 4.26475651e-07
Iter: 630 loss: 4.26423355e-07
Iter: 631 loss: 4.26401243e-07
Iter: 632 loss: 4.26424208e-07
Iter: 633 loss: 4.26353751e-07
Iter: 634 loss: 4.26341245e-07
Iter: 635 loss: 4.26348549e-07
Iter: 636 loss: 4.26331383e-07
Iter: 637 loss: 4.26260215e-07
Iter: 638 loss: 4.26603208e-07
Iter: 639 loss: 4.26260215e-07
Iter: 640 loss: 4.26207492e-07
Iter: 641 loss: 4.26459962e-07
Iter: 642 loss: 4.26207322e-07
Iter: 643 loss: 4.26155736e-07
Iter: 644 loss: 4.26147551e-07
Iter: 645 loss: 4.26113928e-07
Iter: 646 loss: 4.26059103e-07
Iter: 647 loss: 4.26768054e-07
Iter: 648 loss: 4.26059273e-07
Iter: 649 loss: 4.26012036e-07
Iter: 650 loss: 4.26109665e-07
Iter: 651 loss: 4.25973781e-07
Iter: 652 loss: 4.25958092e-07
Iter: 653 loss: 4.26092924e-07
Iter: 654 loss: 4.25974747e-07
Iter: 655 loss: 4.25962469e-07
Iter: 656 loss: 4.25922e-07
Iter: 657 loss: 4.25887862e-07
Iter: 658 loss: 4.25888317e-07
Iter: 659 loss: 4.2581587e-07
Iter: 660 loss: 4.26076554e-07
Iter: 661 loss: 4.25803108e-07
Iter: 662 loss: 4.25779774e-07
Iter: 663 loss: 4.26177422e-07
Iter: 664 loss: 4.2576724e-07
Iter: 665 loss: 4.25735351e-07
Iter: 666 loss: 4.25720543e-07
Iter: 667 loss: 4.25667707e-07
Iter: 668 loss: 4.25644771e-07
Iter: 669 loss: 4.25749363e-07
Iter: 670 loss: 4.25612143e-07
Iter: 671 loss: 4.2556826e-07
Iter: 672 loss: 4.25586222e-07
Iter: 673 loss: 4.25573546e-07
Iter: 674 loss: 4.25515339e-07
Iter: 675 loss: 4.25498456e-07
Iter: 676 loss: 4.25466112e-07
Iter: 677 loss: 4.254305e-07
Iter: 678 loss: 4.25435815e-07
Iter: 679 loss: 4.25375646e-07
Iter: 680 loss: 4.25797026e-07
Iter: 681 loss: 4.25358e-07
Iter: 682 loss: 4.25293081e-07
Iter: 683 loss: 4.25473445e-07
Iter: 684 loss: 4.25288221e-07
Iter: 685 loss: 4.2523962e-07
Iter: 686 loss: 4.25523751e-07
Iter: 687 loss: 4.25235498e-07
Iter: 688 loss: 4.25194287e-07
Iter: 689 loss: 4.25374822e-07
Iter: 690 loss: 4.25196106e-07
Iter: 691 loss: 4.25151939e-07
Iter: 692 loss: 4.251836e-07
Iter: 693 loss: 4.25104901e-07
Iter: 694 loss: 4.2510618e-07
Iter: 695 loss: 4.25107544e-07
Iter: 696 loss: 4.25109562e-07
Iter: 697 loss: 4.25111068e-07
Iter: 698 loss: 4.2512869e-07
Iter: 699 loss: 4.25120732e-07
Iter: 700 loss: 4.25118543e-07
Iter: 701 loss: 4.25124483e-07
Iter: 702 loss: 4.2511158e-07
Iter: 703 loss: 4.25113626e-07
Iter: 704 loss: 4.2510635e-07
Iter: 705 loss: 4.25106464e-07
Iter: 706 loss: 4.25105213e-07
Iter: 707 loss: 4.25105213e-07
Iter: 708 loss: 4.25115616e-07
Iter: 709 loss: 4.25114365e-07
Iter: 710 loss: 4.25107658e-07
Iter: 711 loss: 4.25106037e-07
Iter: 712 loss: 4.25106265e-07
Iter: 713 loss: 4.25104929e-07
Iter: 714 loss: 4.25105725e-07
Iter: 715 loss: 4.25105071e-07
Iter: 716 loss: 4.25105242e-07
Iter: 717 loss: 4.25106066e-07
Iter: 718 loss: 4.25105952e-07
Iter: 719 loss: 4.25105952e-07
Iter: 720 loss: 4.25105952e-07
Iter: 721 loss: 4.25105242e-07
Iter: 722 loss: 4.25105242e-07
Iter: 723 loss: 4.25105952e-07
Iter: 724 loss: 4.25105952e-07
Iter: 725 loss: 4.25105242e-07
Iter: 726 loss: 4.25105242e-07
Iter: 727 loss: 4.25105242e-07
Iter: 728 loss: 4.25105242e-07
Iter: 729 loss: 4.25105242e-07
Iter: 730 loss: 4.25105952e-07
Iter: 731 loss: 5.68313283e-07
Iter: 732 loss: 4.25116696e-07
Iter: 733 loss: 4.2512039e-07
Iter: 734 loss: 4.25089581e-07
Iter: 735 loss: 4.25118259e-07
Iter: 736 loss: 4.25105156e-07
Iter: 737 loss: 4.25110471e-07
Iter: 738 loss: 4.25124142e-07
Iter: 739 loss: 4.25096403e-07
Iter: 740 loss: 4.25119339e-07
Iter: 741 loss: 4.25113967e-07
Iter: 742 loss: 4.25113797e-07
Iter: 743 loss: 4.25110727e-07
Iter: 744 loss: 4.25105696e-07
Iter: 745 loss: 4.25106805e-07
Iter: 746 loss: 4.2510527e-07
Iter: 747 loss: 4.25106805e-07
Iter: 748 loss: 4.25107459e-07
Iter: 749 loss: 4.25108681e-07
Iter: 750 loss: 4.25107601e-07
Iter: 751 loss: 4.25107288e-07
Iter: 752 loss: 4.25106e-07
Iter: 753 loss: 4.25106236e-07
Iter: 754 loss: 4.25105668e-07
Iter: 755 loss: 4.25105782e-07
Iter: 756 loss: 4.25105782e-07
Iter: 757 loss: 4.25105668e-07
Iter: 758 loss: 4.25105782e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.6/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2
+ date
Mon Oct 26 12:01:25 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619b134f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619b1096a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619b0297b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619b0c7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619b040b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619b076840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619afdcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619af911e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619af91c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619af91598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619af5b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619af28a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619af096a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619aedb048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619aed4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619ae7b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619ae7be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619ae632f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619ae43840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619af098c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619addf620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619ad9b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619addff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619ad91a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619ad01488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619ad1fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619ad1f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619acbd378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f619acbd048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6173842950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6173849268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f617380c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f61737be730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f61737e5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f61737890d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f617372af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.03225113e-06
Iter: 2 loss: 2.64952687e-05
Iter: 3 loss: 2.2476529e-06
Iter: 4 loss: 1.95932512e-06
Iter: 5 loss: 1.53213182e-06
Iter: 6 loss: 1.52188181e-06
Iter: 7 loss: 1.23329175e-06
Iter: 8 loss: 2.99588146e-06
Iter: 9 loss: 1.19834635e-06
Iter: 10 loss: 9.74680233e-07
Iter: 11 loss: 1.50587152e-06
Iter: 12 loss: 8.93212075e-07
Iter: 13 loss: 8.22873062e-07
Iter: 14 loss: 1.82721658e-06
Iter: 15 loss: 8.22798143e-07
Iter: 16 loss: 7.85735e-07
Iter: 17 loss: 7.62902289e-07
Iter: 18 loss: 7.48023126e-07
Iter: 19 loss: 7.17263788e-07
Iter: 20 loss: 8.74128602e-07
Iter: 21 loss: 7.12177325e-07
Iter: 22 loss: 6.94473e-07
Iter: 23 loss: 7.1606155e-07
Iter: 24 loss: 6.85185626e-07
Iter: 25 loss: 6.66302e-07
Iter: 26 loss: 6.85386908e-07
Iter: 27 loss: 6.55703275e-07
Iter: 28 loss: 6.42349676e-07
Iter: 29 loss: 6.23903588e-07
Iter: 30 loss: 6.23109713e-07
Iter: 31 loss: 6.07317133e-07
Iter: 32 loss: 6.83527844e-07
Iter: 33 loss: 6.04564207e-07
Iter: 34 loss: 5.91869707e-07
Iter: 35 loss: 7.23016115e-07
Iter: 36 loss: 5.91446792e-07
Iter: 37 loss: 5.81753852e-07
Iter: 38 loss: 6.78724746e-07
Iter: 39 loss: 5.81402787e-07
Iter: 40 loss: 5.78212848e-07
Iter: 41 loss: 5.71158239e-07
Iter: 42 loss: 6.75570618e-07
Iter: 43 loss: 5.70816e-07
Iter: 44 loss: 5.63636092e-07
Iter: 45 loss: 6.6664677e-07
Iter: 46 loss: 5.63544859e-07
Iter: 47 loss: 5.58901377e-07
Iter: 48 loss: 5.7412916e-07
Iter: 49 loss: 5.57630301e-07
Iter: 50 loss: 5.54238795e-07
Iter: 51 loss: 5.55642316e-07
Iter: 52 loss: 5.5188093e-07
Iter: 53 loss: 5.47673949e-07
Iter: 54 loss: 5.7568684e-07
Iter: 55 loss: 5.47205218e-07
Iter: 56 loss: 5.44704619e-07
Iter: 57 loss: 5.4042323e-07
Iter: 58 loss: 5.40459723e-07
Iter: 59 loss: 5.37146548e-07
Iter: 60 loss: 5.36997049e-07
Iter: 61 loss: 5.33581442e-07
Iter: 62 loss: 5.3368592e-07
Iter: 63 loss: 5.30864e-07
Iter: 64 loss: 5.27371185e-07
Iter: 65 loss: 5.4410657e-07
Iter: 66 loss: 5.26777171e-07
Iter: 67 loss: 5.23233552e-07
Iter: 68 loss: 5.24033396e-07
Iter: 69 loss: 5.20631716e-07
Iter: 70 loss: 5.16207933e-07
Iter: 71 loss: 5.4542312e-07
Iter: 72 loss: 5.15741e-07
Iter: 73 loss: 5.14614385e-07
Iter: 74 loss: 5.1449581e-07
Iter: 75 loss: 5.13063696e-07
Iter: 76 loss: 5.08873313e-07
Iter: 77 loss: 5.21969866e-07
Iter: 78 loss: 5.06826041e-07
Iter: 79 loss: 5.03477906e-07
Iter: 80 loss: 5.03459489e-07
Iter: 81 loss: 5.00969804e-07
Iter: 82 loss: 5.16366867e-07
Iter: 83 loss: 5.00739e-07
Iter: 84 loss: 4.98387465e-07
Iter: 85 loss: 4.95678592e-07
Iter: 86 loss: 4.95352651e-07
Iter: 87 loss: 4.92965626e-07
Iter: 88 loss: 5.1962752e-07
Iter: 89 loss: 4.92936067e-07
Iter: 90 loss: 4.90845537e-07
Iter: 91 loss: 4.91782487e-07
Iter: 92 loss: 4.89395575e-07
Iter: 93 loss: 4.87445845e-07
Iter: 94 loss: 4.90419552e-07
Iter: 95 loss: 4.86547151e-07
Iter: 96 loss: 4.84401255e-07
Iter: 97 loss: 4.9693881e-07
Iter: 98 loss: 4.84070199e-07
Iter: 99 loss: 4.82412588e-07
Iter: 100 loss: 4.99102384e-07
Iter: 101 loss: 4.82316636e-07
Iter: 102 loss: 4.81159191e-07
Iter: 103 loss: 4.80259132e-07
Iter: 104 loss: 4.79864752e-07
Iter: 105 loss: 4.79089806e-07
Iter: 106 loss: 4.7908253e-07
Iter: 107 loss: 4.78441677e-07
Iter: 108 loss: 4.80215135e-07
Iter: 109 loss: 4.78199581e-07
Iter: 110 loss: 4.77295e-07
Iter: 111 loss: 4.77229833e-07
Iter: 112 loss: 4.76551264e-07
Iter: 113 loss: 4.7579573e-07
Iter: 114 loss: 4.77525134e-07
Iter: 115 loss: 4.75588195e-07
Iter: 116 loss: 4.74798867e-07
Iter: 117 loss: 4.74293813e-07
Iter: 118 loss: 4.73989871e-07
Iter: 119 loss: 4.72856669e-07
Iter: 120 loss: 4.72821284e-07
Iter: 121 loss: 4.72332289e-07
Iter: 122 loss: 4.71303792e-07
Iter: 123 loss: 4.88955948e-07
Iter: 124 loss: 4.713159e-07
Iter: 125 loss: 4.70254292e-07
Iter: 126 loss: 4.79938478e-07
Iter: 127 loss: 4.70166356e-07
Iter: 128 loss: 4.69346162e-07
Iter: 129 loss: 4.71994383e-07
Iter: 130 loss: 4.69098609e-07
Iter: 131 loss: 4.6825437e-07
Iter: 132 loss: 4.70390717e-07
Iter: 133 loss: 4.67934854e-07
Iter: 134 loss: 4.67139444e-07
Iter: 135 loss: 4.6856951e-07
Iter: 136 loss: 4.6677718e-07
Iter: 137 loss: 4.65573635e-07
Iter: 138 loss: 4.70810107e-07
Iter: 139 loss: 4.65346574e-07
Iter: 140 loss: 4.64672553e-07
Iter: 141 loss: 4.67193502e-07
Iter: 142 loss: 4.64472095e-07
Iter: 143 loss: 4.63737933e-07
Iter: 144 loss: 4.66024517e-07
Iter: 145 loss: 4.63557683e-07
Iter: 146 loss: 4.62625849e-07
Iter: 147 loss: 4.6121022e-07
Iter: 148 loss: 4.61170771e-07
Iter: 149 loss: 4.59933e-07
Iter: 150 loss: 4.63716731e-07
Iter: 151 loss: 4.59600045e-07
Iter: 152 loss: 4.58390105e-07
Iter: 153 loss: 4.6482279e-07
Iter: 154 loss: 4.58184218e-07
Iter: 155 loss: 4.57135542e-07
Iter: 156 loss: 4.65073867e-07
Iter: 157 loss: 4.57094984e-07
Iter: 158 loss: 4.56568102e-07
Iter: 159 loss: 4.55421912e-07
Iter: 160 loss: 4.65032798e-07
Iter: 161 loss: 4.55188115e-07
Iter: 162 loss: 4.53989344e-07
Iter: 163 loss: 4.54015549e-07
Iter: 164 loss: 4.5340127e-07
Iter: 165 loss: 4.5658993e-07
Iter: 166 loss: 4.53306825e-07
Iter: 167 loss: 4.5268331e-07
Iter: 168 loss: 4.52089182e-07
Iter: 169 loss: 4.51921323e-07
Iter: 170 loss: 4.51532969e-07
Iter: 171 loss: 4.51421158e-07
Iter: 172 loss: 4.51037465e-07
Iter: 173 loss: 4.51131115e-07
Iter: 174 loss: 4.50813189e-07
Iter: 175 loss: 4.50335136e-07
Iter: 176 loss: 4.51758e-07
Iter: 177 loss: 4.50172138e-07
Iter: 178 loss: 4.49691186e-07
Iter: 179 loss: 4.51923285e-07
Iter: 180 loss: 4.49611974e-07
Iter: 181 loss: 4.49198637e-07
Iter: 182 loss: 4.48389102e-07
Iter: 183 loss: 4.66135361e-07
Iter: 184 loss: 4.48357e-07
Iter: 185 loss: 4.47742025e-07
Iter: 186 loss: 4.54893723e-07
Iter: 187 loss: 4.47733385e-07
Iter: 188 loss: 4.47277102e-07
Iter: 189 loss: 4.50007121e-07
Iter: 190 loss: 4.47254223e-07
Iter: 191 loss: 4.46737744e-07
Iter: 192 loss: 4.46349702e-07
Iter: 193 loss: 4.46126478e-07
Iter: 194 loss: 4.45662494e-07
Iter: 195 loss: 4.45855108e-07
Iter: 196 loss: 4.45348689e-07
Iter: 197 loss: 4.44643376e-07
Iter: 198 loss: 4.49154896e-07
Iter: 199 loss: 4.44573629e-07
Iter: 200 loss: 4.44105297e-07
Iter: 201 loss: 4.47960758e-07
Iter: 202 loss: 4.44114e-07
Iter: 203 loss: 4.43749457e-07
Iter: 204 loss: 4.43622525e-07
Iter: 205 loss: 4.43440427e-07
Iter: 206 loss: 4.42906611e-07
Iter: 207 loss: 4.49673905e-07
Iter: 208 loss: 4.42942849e-07
Iter: 209 loss: 4.42761575e-07
Iter: 210 loss: 4.42874722e-07
Iter: 211 loss: 4.42570808e-07
Iter: 212 loss: 4.42246687e-07
Iter: 213 loss: 4.43170762e-07
Iter: 214 loss: 4.42200133e-07
Iter: 215 loss: 4.41817662e-07
Iter: 216 loss: 4.41585854e-07
Iter: 217 loss: 4.41484104e-07
Iter: 218 loss: 4.41076082e-07
Iter: 219 loss: 4.41964318e-07
Iter: 220 loss: 4.40917859e-07
Iter: 221 loss: 4.40544341e-07
Iter: 222 loss: 4.42313706e-07
Iter: 223 loss: 4.40433155e-07
Iter: 224 loss: 4.400612e-07
Iter: 225 loss: 4.42544717e-07
Iter: 226 loss: 4.39996825e-07
Iter: 227 loss: 4.39722157e-07
Iter: 228 loss: 4.39193911e-07
Iter: 229 loss: 4.50257346e-07
Iter: 230 loss: 4.3920835e-07
Iter: 231 loss: 4.3863372e-07
Iter: 232 loss: 4.42606279e-07
Iter: 233 loss: 4.38612e-07
Iter: 234 loss: 4.38209753e-07
Iter: 235 loss: 4.40129554e-07
Iter: 236 loss: 4.38150494e-07
Iter: 237 loss: 4.37772655e-07
Iter: 238 loss: 4.408667e-07
Iter: 239 loss: 4.3775276e-07
Iter: 240 loss: 4.37576972e-07
Iter: 241 loss: 4.39246321e-07
Iter: 242 loss: 4.37618667e-07
Iter: 243 loss: 4.3735912e-07
Iter: 244 loss: 4.37065523e-07
Iter: 245 loss: 4.37062567e-07
Iter: 246 loss: 4.36781022e-07
Iter: 247 loss: 4.40350476e-07
Iter: 248 loss: 4.36797706e-07
Iter: 249 loss: 4.36565102e-07
Iter: 250 loss: 4.37082036e-07
Iter: 251 loss: 4.36542905e-07
Iter: 252 loss: 4.36313655e-07
Iter: 253 loss: 4.35898272e-07
Iter: 254 loss: 4.42080136e-07
Iter: 255 loss: 4.35871698e-07
Iter: 256 loss: 4.3551546e-07
Iter: 257 loss: 4.39470057e-07
Iter: 258 loss: 4.35503267e-07
Iter: 259 loss: 4.35218595e-07
Iter: 260 loss: 4.37064216e-07
Iter: 261 loss: 4.35209415e-07
Iter: 262 loss: 4.3489689e-07
Iter: 263 loss: 4.34930143e-07
Iter: 264 loss: 4.34643e-07
Iter: 265 loss: 4.34363812e-07
Iter: 266 loss: 4.34318508e-07
Iter: 267 loss: 4.34205901e-07
Iter: 268 loss: 4.33803507e-07
Iter: 269 loss: 4.36676885e-07
Iter: 270 loss: 4.33786283e-07
Iter: 271 loss: 4.33445791e-07
Iter: 272 loss: 4.35121365e-07
Iter: 273 loss: 4.33424191e-07
Iter: 274 loss: 4.33165241e-07
Iter: 275 loss: 4.34636206e-07
Iter: 276 loss: 4.33192355e-07
Iter: 277 loss: 4.32956796e-07
Iter: 278 loss: 4.33784066e-07
Iter: 279 loss: 4.32880512e-07
Iter: 280 loss: 4.32818325e-07
Iter: 281 loss: 4.32590127e-07
Iter: 282 loss: 4.32517538e-07
Iter: 283 loss: 4.32283969e-07
Iter: 284 loss: 4.34762541e-07
Iter: 285 loss: 4.32287891e-07
Iter: 286 loss: 4.32064269e-07
Iter: 287 loss: 4.32109971e-07
Iter: 288 loss: 4.32016179e-07
Iter: 289 loss: 4.31724743e-07
Iter: 290 loss: 4.31345086e-07
Iter: 291 loss: 4.31297963e-07
Iter: 292 loss: 4.31023125e-07
Iter: 293 loss: 4.31054787e-07
Iter: 294 loss: 4.30840544e-07
Iter: 295 loss: 4.31730399e-07
Iter: 296 loss: 4.30778073e-07
Iter: 297 loss: 4.30542258e-07
Iter: 298 loss: 4.30174225e-07
Iter: 299 loss: 4.30124231e-07
Iter: 300 loss: 4.29851696e-07
Iter: 301 loss: 4.29856527e-07
Iter: 302 loss: 4.29571969e-07
Iter: 303 loss: 4.29169233e-07
Iter: 304 loss: 4.34426966e-07
Iter: 305 loss: 4.2911978e-07
Iter: 306 loss: 4.28899796e-07
Iter: 307 loss: 4.30616979e-07
Iter: 308 loss: 4.28914348e-07
Iter: 309 loss: 4.28734779e-07
Iter: 310 loss: 4.30055763e-07
Iter: 311 loss: 4.28723581e-07
Iter: 312 loss: 4.28473584e-07
Iter: 313 loss: 4.28378087e-07
Iter: 314 loss: 4.28297227e-07
Iter: 315 loss: 4.28087333e-07
Iter: 316 loss: 4.29222212e-07
Iter: 317 loss: 4.28066983e-07
Iter: 318 loss: 4.27826649e-07
Iter: 319 loss: 4.27795044e-07
Iter: 320 loss: 4.27606551e-07
Iter: 321 loss: 4.27359737e-07
Iter: 322 loss: 4.28205396e-07
Iter: 323 loss: 4.27287489e-07
Iter: 324 loss: 4.27049287e-07
Iter: 325 loss: 4.26835044e-07
Iter: 326 loss: 4.26734232e-07
Iter: 327 loss: 4.26520529e-07
Iter: 328 loss: 4.26539629e-07
Iter: 329 loss: 4.26324789e-07
Iter: 330 loss: 4.26231907e-07
Iter: 331 loss: 4.26157044e-07
Iter: 332 loss: 4.2591077e-07
Iter: 333 loss: 4.2661668e-07
Iter: 334 loss: 4.25805808e-07
Iter: 335 loss: 4.25548e-07
Iter: 336 loss: 4.25224016e-07
Iter: 337 loss: 4.25255678e-07
Iter: 338 loss: 4.25089183e-07
Iter: 339 loss: 4.24981778e-07
Iter: 340 loss: 4.24868546e-07
Iter: 341 loss: 4.25190933e-07
Iter: 342 loss: 4.24806927e-07
Iter: 343 loss: 4.24657401e-07
Iter: 344 loss: 4.25217252e-07
Iter: 345 loss: 4.24616985e-07
Iter: 346 loss: 4.24453049e-07
Iter: 347 loss: 4.24345046e-07
Iter: 348 loss: 4.24327e-07
Iter: 349 loss: 4.24098829e-07
Iter: 350 loss: 4.25104872e-07
Iter: 351 loss: 4.24069071e-07
Iter: 352 loss: 4.23938388e-07
Iter: 353 loss: 4.23885865e-07
Iter: 354 loss: 4.23759673e-07
Iter: 355 loss: 4.23595395e-07
Iter: 356 loss: 4.2388217e-07
Iter: 357 loss: 4.23519538e-07
Iter: 358 loss: 4.23311633e-07
Iter: 359 loss: 4.2353426e-07
Iter: 360 loss: 4.23169752e-07
Iter: 361 loss: 4.23036425e-07
Iter: 362 loss: 4.23457067e-07
Iter: 363 loss: 4.229193e-07
Iter: 364 loss: 4.22683172e-07
Iter: 365 loss: 4.24786265e-07
Iter: 366 loss: 4.22679136e-07
Iter: 367 loss: 4.22558344e-07
Iter: 368 loss: 4.22566359e-07
Iter: 369 loss: 4.22495276e-07
Iter: 370 loss: 4.22230642e-07
Iter: 371 loss: 4.22108911e-07
Iter: 372 loss: 4.22052238e-07
Iter: 373 loss: 4.22028307e-07
Iter: 374 loss: 4.21888956e-07
Iter: 375 loss: 4.21755431e-07
Iter: 376 loss: 4.21756852e-07
Iter: 377 loss: 4.21716805e-07
Iter: 378 loss: 4.21529109e-07
Iter: 379 loss: 4.21520781e-07
Iter: 380 loss: 4.21440745e-07
Iter: 381 loss: 4.21244778e-07
Iter: 382 loss: 4.22919811e-07
Iter: 383 loss: 4.21208199e-07
Iter: 384 loss: 4.21086753e-07
Iter: 385 loss: 4.21198536e-07
Iter: 386 loss: 4.21013738e-07
Iter: 387 loss: 4.20917104e-07
Iter: 388 loss: 4.20838035e-07
Iter: 389 loss: 4.20732732e-07
Iter: 390 loss: 4.20540459e-07
Iter: 391 loss: 4.20719402e-07
Iter: 392 loss: 4.20390705e-07
Iter: 393 loss: 4.20203719e-07
Iter: 394 loss: 4.22889e-07
Iter: 395 loss: 4.2013167e-07
Iter: 396 loss: 4.19970434e-07
Iter: 397 loss: 4.2065443e-07
Iter: 398 loss: 4.19930814e-07
Iter: 399 loss: 4.19808856e-07
Iter: 400 loss: 4.20056949e-07
Iter: 401 loss: 4.19671494e-07
Iter: 402 loss: 4.19539504e-07
Iter: 403 loss: 4.19973787e-07
Iter: 404 loss: 4.19510201e-07
Iter: 405 loss: 4.19408138e-07
Iter: 406 loss: 4.19522365e-07
Iter: 407 loss: 4.19271203e-07
Iter: 408 loss: 4.19214302e-07
Iter: 409 loss: 4.19199466e-07
Iter: 410 loss: 4.19160557e-07
Iter: 411 loss: 4.19113945e-07
Iter: 412 loss: 4.1912341e-07
Iter: 413 loss: 4.19010235e-07
Iter: 414 loss: 4.18970956e-07
Iter: 415 loss: 4.18909224e-07
Iter: 416 loss: 4.18829615e-07
Iter: 417 loss: 4.20042312e-07
Iter: 418 loss: 4.18799971e-07
Iter: 419 loss: 4.18689382e-07
Iter: 420 loss: 4.18642458e-07
Iter: 421 loss: 4.18591128e-07
Iter: 422 loss: 4.18452771e-07
Iter: 423 loss: 4.18496256e-07
Iter: 424 loss: 4.18366938e-07
Iter: 425 loss: 4.18189586e-07
Iter: 426 loss: 4.18232219e-07
Iter: 427 loss: 4.1803446e-07
Iter: 428 loss: 4.17902186e-07
Iter: 429 loss: 4.17925378e-07
Iter: 430 loss: 4.177802e-07
Iter: 431 loss: 4.17936633e-07
Iter: 432 loss: 4.17710254e-07
Iter: 433 loss: 4.17546346e-07
Iter: 434 loss: 4.18007971e-07
Iter: 435 loss: 4.17518834e-07
Iter: 436 loss: 4.1739392e-07
Iter: 437 loss: 4.17519e-07
Iter: 438 loss: 4.17364703e-07
Iter: 439 loss: 4.17222111e-07
Iter: 440 loss: 4.18636915e-07
Iter: 441 loss: 4.17254768e-07
Iter: 442 loss: 4.17097539e-07
Iter: 443 loss: 4.17441299e-07
Iter: 444 loss: 4.17038194e-07
Iter: 445 loss: 4.16967907e-07
Iter: 446 loss: 4.1703484e-07
Iter: 447 loss: 4.16934597e-07
Iter: 448 loss: 4.16889435e-07
Iter: 449 loss: 4.16841374e-07
Iter: 450 loss: 4.16732377e-07
Iter: 451 loss: 4.16540161e-07
Iter: 452 loss: 4.17173879e-07
Iter: 453 loss: 4.16491787e-07
Iter: 454 loss: 4.16385177e-07
Iter: 455 loss: 4.16193103e-07
Iter: 456 loss: 4.1616903e-07
Iter: 457 loss: 4.15963825e-07
Iter: 458 loss: 4.17730462e-07
Iter: 459 loss: 4.15997391e-07
Iter: 460 loss: 4.15818704e-07
Iter: 461 loss: 4.15829e-07
Iter: 462 loss: 4.15701521e-07
Iter: 463 loss: 4.15593092e-07
Iter: 464 loss: 4.15578882e-07
Iter: 465 loss: 4.15462182e-07
Iter: 466 loss: 4.15409033e-07
Iter: 467 loss: 4.15354464e-07
Iter: 468 loss: 4.15215595e-07
Iter: 469 loss: 4.15702459e-07
Iter: 470 loss: 4.1522344e-07
Iter: 471 loss: 4.15087584e-07
Iter: 472 loss: 4.15612391e-07
Iter: 473 loss: 4.15083889e-07
Iter: 474 loss: 4.14937659e-07
Iter: 475 loss: 4.1589945e-07
Iter: 476 loss: 4.1491279e-07
Iter: 477 loss: 4.1488687e-07
Iter: 478 loss: 4.14890962e-07
Iter: 479 loss: 4.14791259e-07
Iter: 480 loss: 4.14753174e-07
Iter: 481 loss: 4.14780885e-07
Iter: 482 loss: 4.14684621e-07
Iter: 483 loss: 4.1454669e-07
Iter: 484 loss: 4.15216931e-07
Iter: 485 loss: 4.14509969e-07
Iter: 486 loss: 4.14430247e-07
Iter: 487 loss: 4.14689111e-07
Iter: 488 loss: 4.14420356e-07
Iter: 489 loss: 4.14276627e-07
Iter: 490 loss: 4.14033707e-07
Iter: 491 loss: 4.17367232e-07
Iter: 492 loss: 4.14073781e-07
Iter: 493 loss: 4.13957764e-07
Iter: 494 loss: 4.15134735e-07
Iter: 495 loss: 4.13885971e-07
Iter: 496 loss: 4.13730845e-07
Iter: 497 loss: 4.14425216e-07
Iter: 498 loss: 4.13724848e-07
Iter: 499 loss: 4.13648479e-07
Iter: 500 loss: 4.13638531e-07
Iter: 501 loss: 4.13575407e-07
Iter: 502 loss: 4.13469564e-07
Iter: 503 loss: 4.14974238e-07
Iter: 504 loss: 4.13425596e-07
Iter: 505 loss: 4.1329605e-07
Iter: 506 loss: 4.1505956e-07
Iter: 507 loss: 4.13339137e-07
Iter: 508 loss: 4.13238979e-07
Iter: 509 loss: 4.140295e-07
Iter: 510 loss: 4.13224399e-07
Iter: 511 loss: 4.13154567e-07
Iter: 512 loss: 4.1317287e-07
Iter: 513 loss: 4.13136149e-07
Iter: 514 loss: 4.13048269e-07
Iter: 515 loss: 4.13042812e-07
Iter: 516 loss: 4.12991085e-07
Iter: 517 loss: 4.12927506e-07
Iter: 518 loss: 4.13210614e-07
Iter: 519 loss: 4.12890785e-07
Iter: 520 loss: 4.12778235e-07
Iter: 521 loss: 4.12914204e-07
Iter: 522 loss: 4.12728582e-07
Iter: 523 loss: 4.12600428e-07
Iter: 524 loss: 4.12765786e-07
Iter: 525 loss: 4.12516215e-07
Iter: 526 loss: 4.12427255e-07
Iter: 527 loss: 4.12663496e-07
Iter: 528 loss: 4.12368337e-07
Iter: 529 loss: 4.12250643e-07
Iter: 530 loss: 4.12163104e-07
Iter: 531 loss: 4.12088468e-07
Iter: 532 loss: 4.12030431e-07
Iter: 533 loss: 4.11939567e-07
Iter: 534 loss: 4.11856774e-07
Iter: 535 loss: 4.11918336e-07
Iter: 536 loss: 4.11861038e-07
Iter: 537 loss: 4.11761874e-07
Iter: 538 loss: 4.11760652e-07
Iter: 539 loss: 4.11657e-07
Iter: 540 loss: 4.11644862e-07
Iter: 541 loss: 4.11610017e-07
Iter: 542 loss: 4.11556186e-07
Iter: 543 loss: 4.11534018e-07
Iter: 544 loss: 4.11483114e-07
Iter: 545 loss: 4.11385685e-07
Iter: 546 loss: 4.11614934e-07
Iter: 547 loss: 4.1135678e-07
Iter: 548 loss: 4.11294934e-07
Iter: 549 loss: 4.11214671e-07
Iter: 550 loss: 4.13777855e-07
Iter: 551 loss: 4.11227404e-07
Iter: 552 loss: 4.11078446e-07
Iter: 553 loss: 4.1255845e-07
Iter: 554 loss: 4.1110161e-07
Iter: 555 loss: 4.11033909e-07
Iter: 556 loss: 4.11061762e-07
Iter: 557 loss: 4.10964816e-07
Iter: 558 loss: 4.10824015e-07
Iter: 559 loss: 4.10817279e-07
Iter: 560 loss: 4.10786356e-07
Iter: 561 loss: 4.10545169e-07
Iter: 562 loss: 4.10927129e-07
Iter: 563 loss: 4.10433984e-07
Iter: 564 loss: 4.10226249e-07
Iter: 565 loss: 4.1114555e-07
Iter: 566 loss: 4.10210049e-07
Iter: 567 loss: 4.10089427e-07
Iter: 568 loss: 4.1106307e-07
Iter: 569 loss: 4.10129246e-07
Iter: 570 loss: 4.1000402e-07
Iter: 571 loss: 4.09928191e-07
Iter: 572 loss: 4.09838435e-07
Iter: 573 loss: 4.09735037e-07
Iter: 574 loss: 4.10409484e-07
Iter: 575 loss: 4.09683253e-07
Iter: 576 loss: 4.09644713e-07
Iter: 577 loss: 4.10458881e-07
Iter: 578 loss: 4.0964639e-07
Iter: 579 loss: 4.09620384e-07
Iter: 580 loss: 4.09472335e-07
Iter: 581 loss: 4.09458664e-07
Iter: 582 loss: 4.09410859e-07
Iter: 583 loss: 4.09686379e-07
Iter: 584 loss: 4.09350832e-07
Iter: 585 loss: 4.09297172e-07
Iter: 586 loss: 4.09283359e-07
Iter: 587 loss: 4.09245956e-07
Iter: 588 loss: 4.09125789e-07
Iter: 589 loss: 4.09520226e-07
Iter: 590 loss: 4.09129314e-07
Iter: 591 loss: 4.09032737e-07
Iter: 592 loss: 4.09176e-07
Iter: 593 loss: 4.0895975e-07
Iter: 594 loss: 4.08823269e-07
Iter: 595 loss: 4.09134145e-07
Iter: 596 loss: 4.08732888e-07
Iter: 597 loss: 4.08663084e-07
Iter: 598 loss: 4.08581514e-07
Iter: 599 loss: 4.08577307e-07
Iter: 600 loss: 4.0842292e-07
Iter: 601 loss: 4.09340601e-07
Iter: 602 loss: 4.08448159e-07
Iter: 603 loss: 4.08291584e-07
Iter: 604 loss: 4.08313497e-07
Iter: 605 loss: 4.08177641e-07
Iter: 606 loss: 4.08077369e-07
Iter: 607 loss: 4.08319579e-07
Iter: 608 loss: 4.08052841e-07
Iter: 609 loss: 4.07926393e-07
Iter: 610 loss: 4.07934351e-07
Iter: 611 loss: 4.07907578e-07
Iter: 612 loss: 4.0799361e-07
Iter: 613 loss: 4.07830726e-07
Iter: 614 loss: 4.07815264e-07
Iter: 615 loss: 4.08437074e-07
Iter: 616 loss: 4.0779733e-07
Iter: 617 loss: 4.07757511e-07
Iter: 618 loss: 4.07697399e-07
Iter: 619 loss: 4.08459101e-07
Iter: 620 loss: 4.076675e-07
Iter: 621 loss: 4.07601732e-07
Iter: 622 loss: 4.08148026e-07
Iter: 623 loss: 4.07626487e-07
Iter: 624 loss: 4.07512601e-07
Iter: 625 loss: 4.07644649e-07
Iter: 626 loss: 4.07512516e-07
Iter: 627 loss: 4.07418497e-07
Iter: 628 loss: 4.07419662e-07
Iter: 629 loss: 4.07348153e-07
Iter: 630 loss: 4.07251719e-07
Iter: 631 loss: 4.07777463e-07
Iter: 632 loss: 4.07299979e-07
Iter: 633 loss: 4.07177538e-07
Iter: 634 loss: 4.0804639e-07
Iter: 635 loss: 4.07178561e-07
Iter: 636 loss: 4.07127686e-07
Iter: 637 loss: 4.07016955e-07
Iter: 638 loss: 4.09326447e-07
Iter: 639 loss: 4.07020082e-07
Iter: 640 loss: 4.06942831e-07
Iter: 641 loss: 4.07242e-07
Iter: 642 loss: 4.06924983e-07
Iter: 643 loss: 4.06837842e-07
Iter: 644 loss: 4.06723984e-07
Iter: 645 loss: 4.06700025e-07
Iter: 646 loss: 4.0655749e-07
Iter: 647 loss: 4.06888773e-07
Iter: 648 loss: 4.06537737e-07
Iter: 649 loss: 4.06461055e-07
Iter: 650 loss: 4.06454546e-07
Iter: 651 loss: 4.06395884e-07
Iter: 652 loss: 4.06513834e-07
Iter: 653 loss: 4.0636246e-07
Iter: 654 loss: 4.0632122e-07
Iter: 655 loss: 4.06436129e-07
Iter: 656 loss: 4.06298625e-07
Iter: 657 loss: 4.06249541e-07
Iter: 658 loss: 4.06143585e-07
Iter: 659 loss: 4.08218909e-07
Iter: 660 loss: 4.06144977e-07
Iter: 661 loss: 4.06027709e-07
Iter: 662 loss: 4.06914694e-07
Iter: 663 loss: 4.05995593e-07
Iter: 664 loss: 4.05969104e-07
Iter: 665 loss: 4.05895065e-07
Iter: 666 loss: 4.05896685e-07
Iter: 667 loss: 4.05791582e-07
Iter: 668 loss: 4.05815e-07
Iter: 669 loss: 4.05702963e-07
Iter: 670 loss: 4.05721e-07
Iter: 671 loss: 4.05690088e-07
Iter: 672 loss: 4.05577396e-07
Iter: 673 loss: 4.05722176e-07
Iter: 674 loss: 4.05565288e-07
Iter: 675 loss: 4.05500145e-07
Iter: 676 loss: 4.05906178e-07
Iter: 677 loss: 4.05495712e-07
Iter: 678 loss: 4.05367757e-07
Iter: 679 loss: 4.05387368e-07
Iter: 680 loss: 4.05370685e-07
Iter: 681 loss: 4.0526777e-07
Iter: 682 loss: 4.05937556e-07
Iter: 683 loss: 4.05246766e-07
Iter: 684 loss: 4.05184238e-07
Iter: 685 loss: 4.05774074e-07
Iter: 686 loss: 4.05163121e-07
Iter: 687 loss: 4.051291e-07
Iter: 688 loss: 4.05099598e-07
Iter: 689 loss: 4.05071034e-07
Iter: 690 loss: 4.04980057e-07
Iter: 691 loss: 4.05189809e-07
Iter: 692 loss: 4.04974855e-07
Iter: 693 loss: 4.04829564e-07
Iter: 694 loss: 4.0484727e-07
Iter: 695 loss: 4.04790825e-07
Iter: 696 loss: 4.04714228e-07
Iter: 697 loss: 4.04594459e-07
Iter: 698 loss: 4.04625581e-07
Iter: 699 loss: 4.0454006e-07
Iter: 700 loss: 4.0453358e-07
Iter: 701 loss: 4.04472e-07
Iter: 702 loss: 4.04638882e-07
Iter: 703 loss: 4.04475372e-07
Iter: 704 loss: 4.04390107e-07
Iter: 705 loss: 4.04754303e-07
Iter: 706 loss: 4.04366716e-07
Iter: 707 loss: 4.04317717e-07
Iter: 708 loss: 4.04307059e-07
Iter: 709 loss: 4.04283753e-07
Iter: 710 loss: 4.04176092e-07
Iter: 711 loss: 4.04462e-07
Iter: 712 loss: 4.04177285e-07
Iter: 713 loss: 4.04083039e-07
Iter: 714 loss: 4.04174074e-07
Iter: 715 loss: 4.04040634e-07
Iter: 716 loss: 4.0395372e-07
Iter: 717 loss: 4.04556147e-07
Iter: 718 loss: 4.03987485e-07
Iter: 719 loss: 4.03942522e-07
Iter: 720 loss: 4.04291882e-07
Iter: 721 loss: 4.0393013e-07
Iter: 722 loss: 4.03831109e-07
Iter: 723 loss: 4.03825794e-07
Iter: 724 loss: 4.0512117e-07
Iter: 725 loss: 4.03827727e-07
Iter: 726 loss: 4.0375923e-07
Iter: 727 loss: 4.04198062e-07
Iter: 728 loss: 4.03737516e-07
Iter: 729 loss: 4.03738113e-07
Iter: 730 loss: 4.03713301e-07
Iter: 731 loss: 4.03722595e-07
Iter: 732 loss: 4.03705826e-07
Iter: 733 loss: 4.03724528e-07
Iter: 734 loss: 4.03711226e-07
Iter: 735 loss: 4.03695594e-07
Iter: 736 loss: 4.03701506e-07
Iter: 737 loss: 4.03726688e-07
Iter: 738 loss: 4.03720776e-07
Iter: 739 loss: 4.03742149e-07
Iter: 740 loss: 4.03730098e-07
Iter: 741 loss: 4.03727711e-07
Iter: 742 loss: 4.03731804e-07
Iter: 743 loss: 4.03746014e-07
Iter: 744 loss: 4.03740557e-07
Iter: 745 loss: 4.03733907e-07
Iter: 746 loss: 4.03734248e-07
Iter: 747 loss: 4.03734475e-07
Iter: 748 loss: 4.03735498e-07
Iter: 749 loss: 4.03735555e-07
Iter: 750 loss: 4.03740728e-07
Iter: 751 loss: 4.03735612e-07
Iter: 752 loss: 4.03735612e-07
Iter: 753 loss: 4.03740728e-07
Iter: 754 loss: 4.03735612e-07
Iter: 755 loss: 4.03584181e-07
Iter: 756 loss: 4.04539094e-07
Iter: 757 loss: 4.03563661e-07
Iter: 758 loss: 4.03464e-07
Iter: 759 loss: 4.03786089e-07
Iter: 760 loss: 4.03419875e-07
Iter: 761 loss: 4.03342653e-07
Iter: 762 loss: 4.04433422e-07
Iter: 763 loss: 4.0335442e-07
Iter: 764 loss: 4.03321081e-07
Iter: 765 loss: 4.03423769e-07
Iter: 766 loss: 4.03304568e-07
Iter: 767 loss: 4.03251136e-07
Iter: 768 loss: 4.03167547e-07
Iter: 769 loss: 4.03156548e-07
Iter: 770 loss: 4.03127217e-07
Iter: 771 loss: 4.03102206e-07
Iter: 772 loss: 4.03059914e-07
Iter: 773 loss: 4.03005174e-07
Iter: 774 loss: 4.02938326e-07
Iter: 775 loss: 4.03009437e-07
Iter: 776 loss: 4.02955465e-07
Iter: 777 loss: 4.02970215e-07
Iter: 778 loss: 4.02972944e-07
Iter: 779 loss: 4.02963479e-07
Iter: 780 loss: 4.03012677e-07
Iter: 781 loss: 4.02996193e-07
Iter: 782 loss: 4.02983062e-07
Iter: 783 loss: 4.02968311e-07
Iter: 784 loss: 4.02975616e-07
Iter: 785 loss: 4.02961575e-07
Iter: 786 loss: 4.0297121e-07
Iter: 787 loss: 4.02970528e-07
Iter: 788 loss: 4.02951798e-07
Iter: 789 loss: 4.02948842e-07
Iter: 790 loss: 4.02933239e-07
Iter: 791 loss: 4.02932699e-07
Iter: 792 loss: 4.02942817e-07
Iter: 793 loss: 4.02937133e-07
Iter: 794 loss: 4.02941396e-07
Iter: 795 loss: 4.02941623e-07
Iter: 796 loss: 4.02940259e-07
Iter: 797 loss: 4.02940259e-07
Iter: 798 loss: 4.02937985e-07
Iter: 799 loss: 4.02937985e-07
Iter: 800 loss: 4.02937985e-07
Iter: 801 loss: 4.02937985e-07
Iter: 802 loss: 4.02940259e-07
Iter: 803 loss: 4.02937985e-07
Iter: 804 loss: 4.02937985e-07
Iter: 805 loss: 4.02937985e-07
Iter: 806 loss: 4.02940259e-07
Iter: 807 loss: 4.02937985e-07
Iter: 808 loss: 4.02937985e-07
Iter: 809 loss: 4.02940259e-07
Iter: 810 loss: 4.02940259e-07
Iter: 811 loss: 4.02937985e-07
Iter: 812 loss: 4.02937985e-07
Iter: 813 loss: 4.02937985e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4
+ date
Mon Oct 26 12:14:19 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.4_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.4_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770bb84488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770bb84a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770bb65730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770bb657b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770ba42950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770baa12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b9f3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770baa11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b9b4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b9b47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b972378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b960a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b953d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b8fd0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b90f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b953950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b8c9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b8fd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b82f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b81a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b80e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b784158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b80e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b737d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b737ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b737598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f770b737730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76e92569d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76e9256f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76e92589d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76e921a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76e9258bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76e9258268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76e91f5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76e919d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76c4285f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.62635939e-05
Iter: 2 loss: 0.00473887147
Iter: 3 loss: 5.00032911e-05
Iter: 4 loss: 4.45725782e-05
Iter: 5 loss: 4.65699413e-05
Iter: 6 loss: 4.0754061e-05
Iter: 7 loss: 3.50141636e-05
Iter: 8 loss: 3.59759433e-05
Iter: 9 loss: 3.07016307e-05
Iter: 10 loss: 2.58714099e-05
Iter: 11 loss: 5.81706117e-05
Iter: 12 loss: 2.53779017e-05
Iter: 13 loss: 2.20950969e-05
Iter: 14 loss: 1.83730535e-05
Iter: 15 loss: 1.78945884e-05
Iter: 16 loss: 1.40400298e-05
Iter: 17 loss: 3.06135626e-05
Iter: 18 loss: 1.32404512e-05
Iter: 19 loss: 1.14985251e-05
Iter: 20 loss: 2.16621265e-05
Iter: 21 loss: 1.12716116e-05
Iter: 22 loss: 9.92267724e-06
Iter: 23 loss: 1.48078516e-05
Iter: 24 loss: 9.58308192e-06
Iter: 25 loss: 8.67238e-06
Iter: 26 loss: 1.05948784e-05
Iter: 27 loss: 8.31276e-06
Iter: 28 loss: 7.66181256e-06
Iter: 29 loss: 1.01963387e-05
Iter: 30 loss: 7.51110565e-06
Iter: 31 loss: 6.76289e-06
Iter: 32 loss: 8.04722549e-06
Iter: 33 loss: 6.42848499e-06
Iter: 34 loss: 5.96680366e-06
Iter: 35 loss: 9.44481872e-06
Iter: 36 loss: 5.93118057e-06
Iter: 37 loss: 5.64939455e-06
Iter: 38 loss: 6.27398276e-06
Iter: 39 loss: 5.54333519e-06
Iter: 40 loss: 5.38334325e-06
Iter: 41 loss: 5.35544223e-06
Iter: 42 loss: 5.16461205e-06
Iter: 43 loss: 5.84690133e-06
Iter: 44 loss: 5.11652752e-06
Iter: 45 loss: 5.05775915e-06
Iter: 46 loss: 4.91146329e-06
Iter: 47 loss: 6.28843736e-06
Iter: 48 loss: 4.89057538e-06
Iter: 49 loss: 4.7435833e-06
Iter: 50 loss: 4.74275384e-06
Iter: 51 loss: 4.66314123e-06
Iter: 52 loss: 4.46719787e-06
Iter: 53 loss: 6.4009273e-06
Iter: 54 loss: 4.44186e-06
Iter: 55 loss: 4.29664397e-06
Iter: 56 loss: 5.79156858e-06
Iter: 57 loss: 4.29271086e-06
Iter: 58 loss: 4.18277705e-06
Iter: 59 loss: 4.51584492e-06
Iter: 60 loss: 4.14979104e-06
Iter: 61 loss: 4.03996546e-06
Iter: 62 loss: 4.22097673e-06
Iter: 63 loss: 3.99040573e-06
Iter: 64 loss: 3.90427522e-06
Iter: 65 loss: 4.71205e-06
Iter: 66 loss: 3.90082414e-06
Iter: 67 loss: 3.83909082e-06
Iter: 68 loss: 3.67701477e-06
Iter: 69 loss: 4.89328522e-06
Iter: 70 loss: 3.64412426e-06
Iter: 71 loss: 3.48101139e-06
Iter: 72 loss: 4.75483876e-06
Iter: 73 loss: 3.46935258e-06
Iter: 74 loss: 3.35474101e-06
Iter: 75 loss: 4.45741262e-06
Iter: 76 loss: 3.35071491e-06
Iter: 77 loss: 3.4042132e-06
Iter: 78 loss: 3.32842455e-06
Iter: 79 loss: 3.30407033e-06
Iter: 80 loss: 3.27057023e-06
Iter: 81 loss: 3.26912027e-06
Iter: 82 loss: 3.23329641e-06
Iter: 83 loss: 3.16981959e-06
Iter: 84 loss: 4.75510296e-06
Iter: 85 loss: 3.17006015e-06
Iter: 86 loss: 3.15807779e-06
Iter: 87 loss: 3.13788314e-06
Iter: 88 loss: 3.11199801e-06
Iter: 89 loss: 3.1323e-06
Iter: 90 loss: 3.09602774e-06
Iter: 91 loss: 3.05830167e-06
Iter: 92 loss: 3.02748049e-06
Iter: 93 loss: 3.01639193e-06
Iter: 94 loss: 2.95129121e-06
Iter: 95 loss: 3.50965411e-06
Iter: 96 loss: 2.94765823e-06
Iter: 97 loss: 2.91356537e-06
Iter: 98 loss: 3.08687027e-06
Iter: 99 loss: 2.90817934e-06
Iter: 100 loss: 2.87690659e-06
Iter: 101 loss: 2.94013444e-06
Iter: 102 loss: 2.86415298e-06
Iter: 103 loss: 2.82819633e-06
Iter: 104 loss: 2.83771374e-06
Iter: 105 loss: 2.80215727e-06
Iter: 106 loss: 2.77195795e-06
Iter: 107 loss: 2.75933508e-06
Iter: 108 loss: 2.74330387e-06
Iter: 109 loss: 2.73000182e-06
Iter: 110 loss: 2.7265362e-06
Iter: 111 loss: 2.71380532e-06
Iter: 112 loss: 2.70022338e-06
Iter: 113 loss: 2.6979e-06
Iter: 114 loss: 2.68010513e-06
Iter: 115 loss: 2.69946077e-06
Iter: 116 loss: 2.67084579e-06
Iter: 117 loss: 2.64822233e-06
Iter: 118 loss: 2.60606612e-06
Iter: 119 loss: 3.52306097e-06
Iter: 120 loss: 2.60577281e-06
Iter: 121 loss: 2.55981058e-06
Iter: 122 loss: 3.10140149e-06
Iter: 123 loss: 2.55918781e-06
Iter: 124 loss: 2.53148573e-06
Iter: 125 loss: 2.95178e-06
Iter: 126 loss: 2.53158919e-06
Iter: 127 loss: 2.51966708e-06
Iter: 128 loss: 2.48648985e-06
Iter: 129 loss: 2.67472092e-06
Iter: 130 loss: 2.47695743e-06
Iter: 131 loss: 2.44515331e-06
Iter: 132 loss: 2.92481536e-06
Iter: 133 loss: 2.44500211e-06
Iter: 134 loss: 2.4256351e-06
Iter: 135 loss: 2.52181235e-06
Iter: 136 loss: 2.42222723e-06
Iter: 137 loss: 2.40474333e-06
Iter: 138 loss: 2.3974244e-06
Iter: 139 loss: 2.3880109e-06
Iter: 140 loss: 2.36060419e-06
Iter: 141 loss: 2.49743016e-06
Iter: 142 loss: 2.35620632e-06
Iter: 143 loss: 2.33371838e-06
Iter: 144 loss: 2.36406368e-06
Iter: 145 loss: 2.3224386e-06
Iter: 146 loss: 2.35720381e-06
Iter: 147 loss: 2.31571448e-06
Iter: 148 loss: 2.3125117e-06
Iter: 149 loss: 2.30114756e-06
Iter: 150 loss: 2.29464877e-06
Iter: 151 loss: 2.28777753e-06
Iter: 152 loss: 2.26558177e-06
Iter: 153 loss: 2.41630914e-06
Iter: 154 loss: 2.26380962e-06
Iter: 155 loss: 2.24898827e-06
Iter: 156 loss: 2.25238182e-06
Iter: 157 loss: 2.23835286e-06
Iter: 158 loss: 2.21738719e-06
Iter: 159 loss: 2.37775544e-06
Iter: 160 loss: 2.21607866e-06
Iter: 161 loss: 2.2001193e-06
Iter: 162 loss: 2.38279063e-06
Iter: 163 loss: 2.19990443e-06
Iter: 164 loss: 2.19036065e-06
Iter: 165 loss: 2.17592742e-06
Iter: 166 loss: 2.17569e-06
Iter: 167 loss: 2.15937189e-06
Iter: 168 loss: 2.14454758e-06
Iter: 169 loss: 2.14051806e-06
Iter: 170 loss: 2.11353381e-06
Iter: 171 loss: 2.38091638e-06
Iter: 172 loss: 2.11234692e-06
Iter: 173 loss: 2.0906341e-06
Iter: 174 loss: 2.16670514e-06
Iter: 175 loss: 2.0846287e-06
Iter: 176 loss: 2.06274444e-06
Iter: 177 loss: 2.06400227e-06
Iter: 178 loss: 2.04595e-06
Iter: 179 loss: 2.02486626e-06
Iter: 180 loss: 2.22299423e-06
Iter: 181 loss: 2.0241996e-06
Iter: 182 loss: 2.02338583e-06
Iter: 183 loss: 2.01953935e-06
Iter: 184 loss: 2.01347848e-06
Iter: 185 loss: 1.99840906e-06
Iter: 186 loss: 2.1545934e-06
Iter: 187 loss: 1.99664e-06
Iter: 188 loss: 1.9858137e-06
Iter: 189 loss: 1.98184739e-06
Iter: 190 loss: 1.97583859e-06
Iter: 191 loss: 1.963572e-06
Iter: 192 loss: 1.99645e-06
Iter: 193 loss: 1.95944e-06
Iter: 194 loss: 1.94918084e-06
Iter: 195 loss: 1.94874929e-06
Iter: 196 loss: 1.94156814e-06
Iter: 197 loss: 1.93236565e-06
Iter: 198 loss: 1.93161395e-06
Iter: 199 loss: 1.91911568e-06
Iter: 200 loss: 1.93703227e-06
Iter: 201 loss: 1.91276831e-06
Iter: 202 loss: 1.89979687e-06
Iter: 203 loss: 1.91958634e-06
Iter: 204 loss: 1.89335765e-06
Iter: 205 loss: 1.8812982e-06
Iter: 206 loss: 2.01926605e-06
Iter: 207 loss: 1.8807707e-06
Iter: 208 loss: 1.874497e-06
Iter: 209 loss: 1.88030708e-06
Iter: 210 loss: 1.87063642e-06
Iter: 211 loss: 1.86185434e-06
Iter: 212 loss: 1.9094432e-06
Iter: 213 loss: 1.86029479e-06
Iter: 214 loss: 1.85643944e-06
Iter: 215 loss: 1.85607155e-06
Iter: 216 loss: 1.85120189e-06
Iter: 217 loss: 1.85658178e-06
Iter: 218 loss: 1.84824239e-06
Iter: 219 loss: 1.84522844e-06
Iter: 220 loss: 1.83643283e-06
Iter: 221 loss: 1.89122125e-06
Iter: 222 loss: 1.83401949e-06
Iter: 223 loss: 1.82284839e-06
Iter: 224 loss: 1.87691489e-06
Iter: 225 loss: 1.82076928e-06
Iter: 226 loss: 1.8154667e-06
Iter: 227 loss: 1.81426935e-06
Iter: 228 loss: 1.8084329e-06
Iter: 229 loss: 1.80795519e-06
Iter: 230 loss: 1.80330471e-06
Iter: 231 loss: 1.79580036e-06
Iter: 232 loss: 1.79216158e-06
Iter: 233 loss: 1.7882885e-06
Iter: 234 loss: 1.77637298e-06
Iter: 235 loss: 1.77987977e-06
Iter: 236 loss: 1.76812159e-06
Iter: 237 loss: 1.75463788e-06
Iter: 238 loss: 1.87142541e-06
Iter: 239 loss: 1.7535807e-06
Iter: 240 loss: 1.74508114e-06
Iter: 241 loss: 1.79754443e-06
Iter: 242 loss: 1.7439246e-06
Iter: 243 loss: 1.73651142e-06
Iter: 244 loss: 1.74155912e-06
Iter: 245 loss: 1.73202125e-06
Iter: 246 loss: 1.72191847e-06
Iter: 247 loss: 1.77258869e-06
Iter: 248 loss: 1.72034356e-06
Iter: 249 loss: 1.72026762e-06
Iter: 250 loss: 1.71581496e-06
Iter: 251 loss: 1.71456816e-06
Iter: 252 loss: 1.70989506e-06
Iter: 253 loss: 1.70903809e-06
Iter: 254 loss: 1.70504177e-06
Iter: 255 loss: 1.69509553e-06
Iter: 256 loss: 1.74219986e-06
Iter: 257 loss: 1.69338955e-06
Iter: 258 loss: 1.68781332e-06
Iter: 259 loss: 1.72013006e-06
Iter: 260 loss: 1.68679185e-06
Iter: 261 loss: 1.68317615e-06
Iter: 262 loss: 1.737763e-06
Iter: 263 loss: 1.6832671e-06
Iter: 264 loss: 1.68035024e-06
Iter: 265 loss: 1.67455244e-06
Iter: 266 loss: 1.78033451e-06
Iter: 267 loss: 1.67436838e-06
Iter: 268 loss: 1.66724965e-06
Iter: 269 loss: 1.67959706e-06
Iter: 270 loss: 1.66365339e-06
Iter: 271 loss: 1.65497772e-06
Iter: 272 loss: 1.6783423e-06
Iter: 273 loss: 1.65242795e-06
Iter: 274 loss: 1.64503922e-06
Iter: 275 loss: 1.65554e-06
Iter: 276 loss: 1.6414408e-06
Iter: 277 loss: 1.63713048e-06
Iter: 278 loss: 1.6662392e-06
Iter: 279 loss: 1.6367469e-06
Iter: 280 loss: 1.63156869e-06
Iter: 281 loss: 1.62718925e-06
Iter: 282 loss: 1.62591118e-06
Iter: 283 loss: 1.61757748e-06
Iter: 284 loss: 1.6439144e-06
Iter: 285 loss: 1.61482421e-06
Iter: 286 loss: 1.62486333e-06
Iter: 287 loss: 1.61264597e-06
Iter: 288 loss: 1.61110108e-06
Iter: 289 loss: 1.6082812e-06
Iter: 290 loss: 1.63825553e-06
Iter: 291 loss: 1.60794559e-06
Iter: 292 loss: 1.60448872e-06
Iter: 293 loss: 1.6011054e-06
Iter: 294 loss: 1.6005381e-06
Iter: 295 loss: 1.59546516e-06
Iter: 296 loss: 1.62853564e-06
Iter: 297 loss: 1.59483898e-06
Iter: 298 loss: 1.59079786e-06
Iter: 299 loss: 1.63149616e-06
Iter: 300 loss: 1.59062927e-06
Iter: 301 loss: 1.58714545e-06
Iter: 302 loss: 1.58870489e-06
Iter: 303 loss: 1.58468265e-06
Iter: 304 loss: 1.58096066e-06
Iter: 305 loss: 1.57576017e-06
Iter: 306 loss: 1.57536795e-06
Iter: 307 loss: 1.57002637e-06
Iter: 308 loss: 1.58392618e-06
Iter: 309 loss: 1.56809665e-06
Iter: 310 loss: 1.56127828e-06
Iter: 311 loss: 1.5846656e-06
Iter: 312 loss: 1.55950966e-06
Iter: 313 loss: 1.55303837e-06
Iter: 314 loss: 1.59907972e-06
Iter: 315 loss: 1.55278474e-06
Iter: 316 loss: 1.547068e-06
Iter: 317 loss: 1.54252643e-06
Iter: 318 loss: 1.54117788e-06
Iter: 319 loss: 1.53989424e-06
Iter: 320 loss: 1.53763699e-06
Iter: 321 loss: 1.5359343e-06
Iter: 322 loss: 1.53589656e-06
Iter: 323 loss: 1.53488e-06
Iter: 324 loss: 1.53190865e-06
Iter: 325 loss: 1.52983966e-06
Iter: 326 loss: 1.52792495e-06
Iter: 327 loss: 1.52005646e-06
Iter: 328 loss: 1.55967814e-06
Iter: 329 loss: 1.51889071e-06
Iter: 330 loss: 1.51511904e-06
Iter: 331 loss: 1.56493377e-06
Iter: 332 loss: 1.51504855e-06
Iter: 333 loss: 1.51234758e-06
Iter: 334 loss: 1.52981261e-06
Iter: 335 loss: 1.51206109e-06
Iter: 336 loss: 1.50929077e-06
Iter: 337 loss: 1.50543679e-06
Iter: 338 loss: 1.50547748e-06
Iter: 339 loss: 1.50011192e-06
Iter: 340 loss: 1.50421272e-06
Iter: 341 loss: 1.49658138e-06
Iter: 342 loss: 1.49175594e-06
Iter: 343 loss: 1.49827588e-06
Iter: 344 loss: 1.48947629e-06
Iter: 345 loss: 1.48387619e-06
Iter: 346 loss: 1.51076529e-06
Iter: 347 loss: 1.48317281e-06
Iter: 348 loss: 1.4780876e-06
Iter: 349 loss: 1.50264009e-06
Iter: 350 loss: 1.47730418e-06
Iter: 351 loss: 1.47325204e-06
Iter: 352 loss: 1.47507194e-06
Iter: 353 loss: 1.47091487e-06
Iter: 354 loss: 1.48203549e-06
Iter: 355 loss: 1.46973832e-06
Iter: 356 loss: 1.46920115e-06
Iter: 357 loss: 1.46708066e-06
Iter: 358 loss: 1.47224614e-06
Iter: 359 loss: 1.46610512e-06
Iter: 360 loss: 1.4638124e-06
Iter: 361 loss: 1.46035859e-06
Iter: 362 loss: 1.46014031e-06
Iter: 363 loss: 1.45696515e-06
Iter: 364 loss: 1.45688352e-06
Iter: 365 loss: 1.45524882e-06
Iter: 366 loss: 1.48227946e-06
Iter: 367 loss: 1.45514366e-06
Iter: 368 loss: 1.45353442e-06
Iter: 369 loss: 1.45238391e-06
Iter: 370 loss: 1.4519112e-06
Iter: 371 loss: 1.44913793e-06
Iter: 372 loss: 1.44588694e-06
Iter: 373 loss: 1.44546289e-06
Iter: 374 loss: 1.44096441e-06
Iter: 375 loss: 1.46432399e-06
Iter: 376 loss: 1.44033152e-06
Iter: 377 loss: 1.43688112e-06
Iter: 378 loss: 1.44238766e-06
Iter: 379 loss: 1.43533896e-06
Iter: 380 loss: 1.43170917e-06
Iter: 381 loss: 1.45232832e-06
Iter: 382 loss: 1.43111993e-06
Iter: 383 loss: 1.42779027e-06
Iter: 384 loss: 1.42678971e-06
Iter: 385 loss: 1.42500562e-06
Iter: 386 loss: 1.42081899e-06
Iter: 387 loss: 1.44753221e-06
Iter: 388 loss: 1.42029432e-06
Iter: 389 loss: 1.41815667e-06
Iter: 390 loss: 1.41772273e-06
Iter: 391 loss: 1.41659586e-06
Iter: 392 loss: 1.41338728e-06
Iter: 393 loss: 1.44207115e-06
Iter: 394 loss: 1.41310056e-06
Iter: 395 loss: 1.41087094e-06
Iter: 396 loss: 1.41138457e-06
Iter: 397 loss: 1.40934105e-06
Iter: 398 loss: 1.40575821e-06
Iter: 399 loss: 1.41895714e-06
Iter: 400 loss: 1.40518546e-06
Iter: 401 loss: 1.40328768e-06
Iter: 402 loss: 1.40312318e-06
Iter: 403 loss: 1.40255725e-06
Iter: 404 loss: 1.4086279e-06
Iter: 405 loss: 1.40221687e-06
Iter: 406 loss: 1.40179327e-06
Iter: 407 loss: 1.39888675e-06
Iter: 408 loss: 1.41553301e-06
Iter: 409 loss: 1.39836538e-06
Iter: 410 loss: 1.39513418e-06
Iter: 411 loss: 1.40793873e-06
Iter: 412 loss: 1.39454801e-06
Iter: 413 loss: 1.39200824e-06
Iter: 414 loss: 1.39746828e-06
Iter: 415 loss: 1.39038843e-06
Iter: 416 loss: 1.38724681e-06
Iter: 417 loss: 1.38920018e-06
Iter: 418 loss: 1.38511223e-06
Iter: 419 loss: 1.38233088e-06
Iter: 420 loss: 1.4008192e-06
Iter: 421 loss: 1.38195537e-06
Iter: 422 loss: 1.38169185e-06
Iter: 423 loss: 1.38090218e-06
Iter: 424 loss: 1.37993607e-06
Iter: 425 loss: 1.38007886e-06
Iter: 426 loss: 1.37896723e-06
Iter: 427 loss: 1.37776533e-06
Iter: 428 loss: 1.37550148e-06
Iter: 429 loss: 1.401394e-06
Iter: 430 loss: 1.37567156e-06
Iter: 431 loss: 1.37279017e-06
Iter: 432 loss: 1.37625545e-06
Iter: 433 loss: 1.37110544e-06
Iter: 434 loss: 1.36925985e-06
Iter: 435 loss: 1.36843789e-06
Iter: 436 loss: 1.36678943e-06
Iter: 437 loss: 1.37224106e-06
Iter: 438 loss: 1.3660981e-06
Iter: 439 loss: 1.36347012e-06
Iter: 440 loss: 1.36470123e-06
Iter: 441 loss: 1.36201129e-06
Iter: 442 loss: 1.35920345e-06
Iter: 443 loss: 1.35710877e-06
Iter: 444 loss: 1.35598361e-06
Iter: 445 loss: 1.353416e-06
Iter: 446 loss: 1.38526616e-06
Iter: 447 loss: 1.35354276e-06
Iter: 448 loss: 1.35186326e-06
Iter: 449 loss: 1.34908964e-06
Iter: 450 loss: 1.34895276e-06
Iter: 451 loss: 1.34584775e-06
Iter: 452 loss: 1.36348933e-06
Iter: 453 loss: 1.34533536e-06
Iter: 454 loss: 1.34418792e-06
Iter: 455 loss: 1.34422316e-06
Iter: 456 loss: 1.34326183e-06
Iter: 457 loss: 1.34965853e-06
Iter: 458 loss: 1.34316156e-06
Iter: 459 loss: 1.34236484e-06
Iter: 460 loss: 1.34017512e-06
Iter: 461 loss: 1.35885796e-06
Iter: 462 loss: 1.33973447e-06
Iter: 463 loss: 1.33722506e-06
Iter: 464 loss: 1.33965023e-06
Iter: 465 loss: 1.3357627e-06
Iter: 466 loss: 1.33336675e-06
Iter: 467 loss: 1.34295306e-06
Iter: 468 loss: 1.3328463e-06
Iter: 469 loss: 1.32989896e-06
Iter: 470 loss: 1.33254548e-06
Iter: 471 loss: 1.3282679e-06
Iter: 472 loss: 1.32606397e-06
Iter: 473 loss: 1.32344564e-06
Iter: 474 loss: 1.3229959e-06
Iter: 475 loss: 1.32367427e-06
Iter: 476 loss: 1.32116634e-06
Iter: 477 loss: 1.3197091e-06
Iter: 478 loss: 1.31854256e-06
Iter: 479 loss: 1.31799106e-06
Iter: 480 loss: 1.31641218e-06
Iter: 481 loss: 1.31729257e-06
Iter: 482 loss: 1.31532568e-06
Iter: 483 loss: 1.31261845e-06
Iter: 484 loss: 1.31515264e-06
Iter: 485 loss: 1.31122169e-06
Iter: 486 loss: 1.3094982e-06
Iter: 487 loss: 1.31952152e-06
Iter: 488 loss: 1.30934552e-06
Iter: 489 loss: 1.30817762e-06
Iter: 490 loss: 1.30816966e-06
Iter: 491 loss: 1.30738817e-06
Iter: 492 loss: 1.30555827e-06
Iter: 493 loss: 1.32692526e-06
Iter: 494 loss: 1.30548608e-06
Iter: 495 loss: 1.3034246e-06
Iter: 496 loss: 1.3051291e-06
Iter: 497 loss: 1.30234298e-06
Iter: 498 loss: 1.30075409e-06
Iter: 499 loss: 1.31014349e-06
Iter: 500 loss: 1.30015519e-06
Iter: 501 loss: 1.29843761e-06
Iter: 502 loss: 1.30034448e-06
Iter: 503 loss: 1.29763032e-06
Iter: 504 loss: 1.29569798e-06
Iter: 505 loss: 1.30021908e-06
Iter: 506 loss: 1.29511204e-06
Iter: 507 loss: 1.29351224e-06
Iter: 508 loss: 1.30001843e-06
Iter: 509 loss: 1.2931655e-06
Iter: 510 loss: 1.29131331e-06
Iter: 511 loss: 1.30024478e-06
Iter: 512 loss: 1.29117336e-06
Iter: 513 loss: 1.28961051e-06
Iter: 514 loss: 1.29995249e-06
Iter: 515 loss: 1.28921067e-06
Iter: 516 loss: 1.28830789e-06
Iter: 517 loss: 1.28607485e-06
Iter: 518 loss: 1.3063717e-06
Iter: 519 loss: 1.28573868e-06
Iter: 520 loss: 1.28350007e-06
Iter: 521 loss: 1.31564889e-06
Iter: 522 loss: 1.2835294e-06
Iter: 523 loss: 1.28255965e-06
Iter: 524 loss: 1.2826174e-06
Iter: 525 loss: 1.28170791e-06
Iter: 526 loss: 1.29137834e-06
Iter: 527 loss: 1.28158013e-06
Iter: 528 loss: 1.28128522e-06
Iter: 529 loss: 1.28023476e-06
Iter: 530 loss: 1.27845215e-06
Iter: 531 loss: 1.27839485e-06
Iter: 532 loss: 1.27592114e-06
Iter: 533 loss: 1.29161913e-06
Iter: 534 loss: 1.27579438e-06
Iter: 535 loss: 1.27423573e-06
Iter: 536 loss: 1.28749286e-06
Iter: 537 loss: 1.27414455e-06
Iter: 538 loss: 1.27289252e-06
Iter: 539 loss: 1.27212763e-06
Iter: 540 loss: 1.27175986e-06
Iter: 541 loss: 1.27023327e-06
Iter: 542 loss: 1.28016836e-06
Iter: 543 loss: 1.27010935e-06
Iter: 544 loss: 1.26875727e-06
Iter: 545 loss: 1.27081807e-06
Iter: 546 loss: 1.26786756e-06
Iter: 547 loss: 1.26649388e-06
Iter: 548 loss: 1.27163969e-06
Iter: 549 loss: 1.26629834e-06
Iter: 550 loss: 1.26497548e-06
Iter: 551 loss: 1.28502518e-06
Iter: 552 loss: 1.26494342e-06
Iter: 553 loss: 1.26450095e-06
Iter: 554 loss: 1.2632986e-06
Iter: 555 loss: 1.28001477e-06
Iter: 556 loss: 1.26336408e-06
Iter: 557 loss: 1.2622354e-06
Iter: 558 loss: 1.26954274e-06
Iter: 559 loss: 1.26232817e-06
Iter: 560 loss: 1.26171221e-06
Iter: 561 loss: 1.26155669e-06
Iter: 562 loss: 1.2612943e-06
Iter: 563 loss: 1.26034547e-06
Iter: 564 loss: 1.27566682e-06
Iter: 565 loss: 1.26024895e-06
Iter: 566 loss: 1.25949839e-06
Iter: 567 loss: 1.25960537e-06
Iter: 568 loss: 1.25881706e-06
Iter: 569 loss: 1.25761562e-06
Iter: 570 loss: 1.25637439e-06
Iter: 571 loss: 1.25583483e-06
Iter: 572 loss: 1.25391171e-06
Iter: 573 loss: 1.27982344e-06
Iter: 574 loss: 1.25410179e-06
Iter: 575 loss: 1.25299334e-06
Iter: 576 loss: 1.25621273e-06
Iter: 577 loss: 1.25274585e-06
Iter: 578 loss: 1.25144561e-06
Iter: 579 loss: 1.25027532e-06
Iter: 580 loss: 1.2500617e-06
Iter: 581 loss: 1.24857934e-06
Iter: 582 loss: 1.2696014e-06
Iter: 583 loss: 1.24837504e-06
Iter: 584 loss: 1.24729775e-06
Iter: 585 loss: 1.25559586e-06
Iter: 586 loss: 1.24733208e-06
Iter: 587 loss: 1.24612006e-06
Iter: 588 loss: 1.24832241e-06
Iter: 589 loss: 1.24549365e-06
Iter: 590 loss: 1.24481858e-06
Iter: 591 loss: 1.24524172e-06
Iter: 592 loss: 1.24440203e-06
Iter: 593 loss: 1.24372809e-06
Iter: 594 loss: 1.24369126e-06
Iter: 595 loss: 1.24342432e-06
Iter: 596 loss: 1.24281519e-06
Iter: 597 loss: 1.25369888e-06
Iter: 598 loss: 1.2427588e-06
Iter: 599 loss: 1.24198255e-06
Iter: 600 loss: 1.24094595e-06
Iter: 601 loss: 1.24086739e-06
Iter: 602 loss: 1.23952009e-06
Iter: 603 loss: 1.24511075e-06
Iter: 604 loss: 1.23908012e-06
Iter: 605 loss: 1.23792506e-06
Iter: 606 loss: 1.23916425e-06
Iter: 607 loss: 1.23734719e-06
Iter: 608 loss: 1.23582731e-06
Iter: 609 loss: 1.24200233e-06
Iter: 610 loss: 1.23563609e-06
Iter: 611 loss: 1.23414134e-06
Iter: 612 loss: 1.23735538e-06
Iter: 613 loss: 1.23364464e-06
Iter: 614 loss: 1.23238067e-06
Iter: 615 loss: 1.23584277e-06
Iter: 616 loss: 1.23228119e-06
Iter: 617 loss: 1.23122811e-06
Iter: 618 loss: 1.23490906e-06
Iter: 619 loss: 1.23070879e-06
Iter: 620 loss: 1.22985455e-06
Iter: 621 loss: 1.24390033e-06
Iter: 622 loss: 1.22986671e-06
Iter: 623 loss: 1.22918516e-06
Iter: 624 loss: 1.2283765e-06
Iter: 625 loss: 1.22835638e-06
Iter: 626 loss: 1.2279437e-06
Iter: 627 loss: 1.22779272e-06
Iter: 628 loss: 1.2272717e-06
Iter: 629 loss: 1.22586505e-06
Iter: 630 loss: 1.24787039e-06
Iter: 631 loss: 1.22590961e-06
Iter: 632 loss: 1.22489087e-06
Iter: 633 loss: 1.22609913e-06
Iter: 634 loss: 1.22443225e-06
Iter: 635 loss: 1.22325241e-06
Iter: 636 loss: 1.22420352e-06
Iter: 637 loss: 1.22269887e-06
Iter: 638 loss: 1.2215728e-06
Iter: 639 loss: 1.22504207e-06
Iter: 640 loss: 1.22128063e-06
Iter: 641 loss: 1.22012545e-06
Iter: 642 loss: 1.22247525e-06
Iter: 643 loss: 1.21958715e-06
Iter: 644 loss: 1.21846119e-06
Iter: 645 loss: 1.2199896e-06
Iter: 646 loss: 1.21777714e-06
Iter: 647 loss: 1.21609901e-06
Iter: 648 loss: 1.21967298e-06
Iter: 649 loss: 1.21528046e-06
Iter: 650 loss: 1.21417702e-06
Iter: 651 loss: 1.21744563e-06
Iter: 652 loss: 1.21384414e-06
Iter: 653 loss: 1.21293192e-06
Iter: 654 loss: 1.21291782e-06
Iter: 655 loss: 1.21213748e-06
Iter: 656 loss: 1.21312405e-06
Iter: 657 loss: 1.21148992e-06
Iter: 658 loss: 1.21092808e-06
Iter: 659 loss: 1.21273388e-06
Iter: 660 loss: 1.21053336e-06
Iter: 661 loss: 1.20988466e-06
Iter: 662 loss: 1.2098626e-06
Iter: 663 loss: 1.20965285e-06
Iter: 664 loss: 1.20912694e-06
Iter: 665 loss: 1.21020764e-06
Iter: 666 loss: 1.20847778e-06
Iter: 667 loss: 1.2071946e-06
Iter: 668 loss: 1.21084929e-06
Iter: 669 loss: 1.20688981e-06
Iter: 670 loss: 1.20599293e-06
Iter: 671 loss: 1.20977461e-06
Iter: 672 loss: 1.20572554e-06
Iter: 673 loss: 1.20483355e-06
Iter: 674 loss: 1.20611958e-06
Iter: 675 loss: 1.20435584e-06
Iter: 676 loss: 1.20294396e-06
Iter: 677 loss: 1.20382776e-06
Iter: 678 loss: 1.20221011e-06
Iter: 679 loss: 1.20069581e-06
Iter: 680 loss: 1.20535333e-06
Iter: 681 loss: 1.20049276e-06
Iter: 682 loss: 1.19915728e-06
Iter: 683 loss: 1.20396885e-06
Iter: 684 loss: 1.19890422e-06
Iter: 685 loss: 1.19767219e-06
Iter: 686 loss: 1.19907429e-06
Iter: 687 loss: 1.19725132e-06
Iter: 688 loss: 1.19586537e-06
Iter: 689 loss: 1.20101038e-06
Iter: 690 loss: 1.19591618e-06
Iter: 691 loss: 1.19516949e-06
Iter: 692 loss: 1.19496303e-06
Iter: 693 loss: 1.19468189e-06
Iter: 694 loss: 1.19666288e-06
Iter: 695 loss: 1.19471702e-06
Iter: 696 loss: 1.19422339e-06
Iter: 697 loss: 1.19379706e-06
Iter: 698 loss: 1.19375864e-06
Iter: 699 loss: 1.19351239e-06
Iter: 700 loss: 1.19299739e-06
Iter: 701 loss: 1.19297329e-06
Iter: 702 loss: 1.19219499e-06
Iter: 703 loss: 1.19285187e-06
Iter: 704 loss: 1.19171796e-06
Iter: 705 loss: 1.1910912e-06
Iter: 706 loss: 1.19479455e-06
Iter: 707 loss: 1.19084848e-06
Iter: 708 loss: 1.19022786e-06
Iter: 709 loss: 1.19090805e-06
Iter: 710 loss: 1.18984894e-06
Iter: 711 loss: 1.18876937e-06
Iter: 712 loss: 1.19065135e-06
Iter: 713 loss: 1.18848925e-06
Iter: 714 loss: 1.18772709e-06
Iter: 715 loss: 1.19184051e-06
Iter: 716 loss: 1.18752223e-06
Iter: 717 loss: 1.18689013e-06
Iter: 718 loss: 1.1875851e-06
Iter: 719 loss: 1.18644482e-06
Iter: 720 loss: 1.18572916e-06
Iter: 721 loss: 1.18837215e-06
Iter: 722 loss: 1.18560411e-06
Iter: 723 loss: 1.18537264e-06
Iter: 724 loss: 1.18521609e-06
Iter: 725 loss: 1.18483206e-06
Iter: 726 loss: 1.18482967e-06
Iter: 727 loss: 1.18434082e-06
Iter: 728 loss: 1.18393291e-06
Iter: 729 loss: 1.1838506e-06
Iter: 730 loss: 1.18390949e-06
Iter: 731 loss: 1.18394871e-06
Iter: 732 loss: 1.18393768e-06
Iter: 733 loss: 1.18381149e-06
Iter: 734 loss: 1.18380672e-06
Iter: 735 loss: 1.18388607e-06
Iter: 736 loss: 1.18400749e-06
Iter: 737 loss: 1.1839054e-06
Iter: 738 loss: 1.18387868e-06
Iter: 739 loss: 1.18386095e-06
Iter: 740 loss: 1.18392404e-06
Iter: 741 loss: 1.18387948e-06
Iter: 742 loss: 1.18393621e-06
Iter: 743 loss: 1.18395701e-06
Iter: 744 loss: 1.18386765e-06
Iter: 745 loss: 1.18384071e-06
Iter: 746 loss: 1.1838415e-06
Iter: 747 loss: 1.18385333e-06
Iter: 748 loss: 1.18383923e-06
Iter: 749 loss: 1.18384673e-06
Iter: 750 loss: 1.18384389e-06
Iter: 751 loss: 1.18384401e-06
Iter: 752 loss: 1.18385344e-06
Iter: 753 loss: 1.18385333e-06
Iter: 754 loss: 1.18384401e-06
Iter: 755 loss: 1.18385333e-06
Iter: 756 loss: 1.18385333e-06
Iter: 757 loss: 1.18384401e-06
Iter: 758 loss: 1.18253456e-06
Iter: 759 loss: 1.1826844e-06
Iter: 760 loss: 1.18182231e-06
Iter: 761 loss: 1.18312448e-06
Iter: 762 loss: 1.18131265e-06
Iter: 763 loss: 1.18064486e-06
Iter: 764 loss: 1.18062235e-06
Iter: 765 loss: 1.18019591e-06
Iter: 766 loss: 1.17963464e-06
Iter: 767 loss: 1.17966829e-06
Iter: 768 loss: 1.17914692e-06
Iter: 769 loss: 1.17891568e-06
Iter: 770 loss: 1.1786758e-06
Iter: 771 loss: 1.17815728e-06
Iter: 772 loss: 1.17876834e-06
Iter: 773 loss: 1.17762283e-06
Iter: 774 loss: 1.17699074e-06
Iter: 775 loss: 1.18062326e-06
Iter: 776 loss: 1.17679622e-06
Iter: 777 loss: 1.1763334e-06
Iter: 778 loss: 1.17575689e-06
Iter: 779 loss: 1.17552372e-06
Iter: 780 loss: 1.17470177e-06
Iter: 781 loss: 1.17754325e-06
Iter: 782 loss: 1.17450304e-06
Iter: 783 loss: 1.17399554e-06
Iter: 784 loss: 1.17394461e-06
Iter: 785 loss: 1.17352249e-06
Iter: 786 loss: 1.17360696e-06
Iter: 787 loss: 1.17318723e-06
Iter: 788 loss: 1.17276977e-06
Iter: 789 loss: 1.17474019e-06
Iter: 790 loss: 1.17264722e-06
Iter: 791 loss: 1.17251193e-06
Iter: 792 loss: 1.17196601e-06
Iter: 793 loss: 1.18299477e-06
Iter: 794 loss: 1.17157913e-06
Iter: 795 loss: 1.17081663e-06
Iter: 796 loss: 1.17069112e-06
Iter: 797 loss: 1.17058573e-06
Iter: 798 loss: 1.16959473e-06
Iter: 799 loss: 1.1758757e-06
Iter: 800 loss: 1.16934916e-06
Iter: 801 loss: 1.16847809e-06
Iter: 802 loss: 1.17280536e-06
Iter: 803 loss: 1.16831393e-06
Iter: 804 loss: 1.16759543e-06
Iter: 805 loss: 1.16959882e-06
Iter: 806 loss: 1.1671566e-06
Iter: 807 loss: 1.16641036e-06
Iter: 808 loss: 1.16706337e-06
Iter: 809 loss: 1.16621482e-06
Iter: 810 loss: 1.16520266e-06
Iter: 811 loss: 1.17297054e-06
Iter: 812 loss: 1.1652221e-06
Iter: 813 loss: 1.16454646e-06
Iter: 814 loss: 1.16512456e-06
Iter: 815 loss: 1.16443118e-06
Iter: 816 loss: 1.16367505e-06
Iter: 817 loss: 1.16831552e-06
Iter: 818 loss: 1.16352498e-06
Iter: 819 loss: 1.16310878e-06
Iter: 820 loss: 1.16997307e-06
Iter: 821 loss: 1.16301658e-06
Iter: 822 loss: 1.16281444e-06
Iter: 823 loss: 1.1628739e-06
Iter: 824 loss: 1.16266801e-06
Iter: 825 loss: 1.1621712e-06
Iter: 826 loss: 1.16225669e-06
Iter: 827 loss: 1.16173499e-06
Iter: 828 loss: 1.16115791e-06
Iter: 829 loss: 1.16086471e-06
Iter: 830 loss: 1.1606013e-06
Iter: 831 loss: 1.15977491e-06
Iter: 832 loss: 1.16335605e-06
Iter: 833 loss: 1.15974592e-06
Iter: 834 loss: 1.15881176e-06
Iter: 835 loss: 1.15916214e-06
Iter: 836 loss: 1.15825787e-06
Iter: 837 loss: 1.15729642e-06
Iter: 838 loss: 1.16154592e-06
Iter: 839 loss: 1.15707667e-06
Iter: 840 loss: 1.15650664e-06
Iter: 841 loss: 1.16186391e-06
Iter: 842 loss: 1.15637476e-06
Iter: 843 loss: 1.15591081e-06
Iter: 844 loss: 1.15751402e-06
Iter: 845 loss: 1.1557554e-06
Iter: 846 loss: 1.1551806e-06
Iter: 847 loss: 1.15513058e-06
Iter: 848 loss: 1.15483283e-06
Iter: 849 loss: 1.15425019e-06
Iter: 850 loss: 1.15422563e-06
Iter: 851 loss: 1.15385194e-06
Iter: 852 loss: 1.15838088e-06
Iter: 853 loss: 1.15376815e-06
Iter: 854 loss: 1.15362093e-06
Iter: 855 loss: 1.15336513e-06
Iter: 856 loss: 1.15329703e-06
Iter: 857 loss: 1.15274338e-06
Iter: 858 loss: 1.15398666e-06
Iter: 859 loss: 1.15270927e-06
Iter: 860 loss: 1.152305e-06
Iter: 861 loss: 1.1523282e-06
Iter: 862 loss: 1.1518174e-06
Iter: 863 loss: 1.151432e-06
Iter: 864 loss: 1.15114062e-06
Iter: 865 loss: 1.15066564e-06
Iter: 866 loss: 1.14990871e-06
Iter: 867 loss: 1.15262083e-06
Iter: 868 loss: 1.14978798e-06
Iter: 869 loss: 1.14905265e-06
Iter: 870 loss: 1.1533557e-06
Iter: 871 loss: 1.1489318e-06
Iter: 872 loss: 1.14820978e-06
Iter: 873 loss: 1.14819284e-06
Iter: 874 loss: 1.14778231e-06
Iter: 875 loss: 1.14673639e-06
Iter: 876 loss: 1.15013245e-06
Iter: 877 loss: 1.14675504e-06
Iter: 878 loss: 1.14606462e-06
Iter: 879 loss: 1.15270586e-06
Iter: 880 loss: 1.14597447e-06
Iter: 881 loss: 1.14574095e-06
Iter: 882 loss: 1.14566603e-06
Iter: 883 loss: 1.14526335e-06
Iter: 884 loss: 1.14572185e-06
Iter: 885 loss: 1.14501893e-06
Iter: 886 loss: 1.14490194e-06
Iter: 887 loss: 1.14457703e-06
Iter: 888 loss: 1.15079979e-06
Iter: 889 loss: 1.14469503e-06
Iter: 890 loss: 1.14425484e-06
Iter: 891 loss: 1.14506884e-06
Iter: 892 loss: 1.14431487e-06
Iter: 893 loss: 1.14384488e-06
Iter: 894 loss: 1.14396914e-06
Iter: 895 loss: 1.14363661e-06
Iter: 896 loss: 1.14315526e-06
Iter: 897 loss: 1.14323439e-06
Iter: 898 loss: 1.14286081e-06
Iter: 899 loss: 1.14248428e-06
Iter: 900 loss: 1.1425908e-06
Iter: 901 loss: 1.14192858e-06
Iter: 902 loss: 1.14111526e-06
Iter: 903 loss: 1.14268164e-06
Iter: 904 loss: 1.14097406e-06
Iter: 905 loss: 1.14024795e-06
Iter: 906 loss: 1.14296404e-06
Iter: 907 loss: 1.13999408e-06
Iter: 908 loss: 1.13948727e-06
Iter: 909 loss: 1.14160184e-06
Iter: 910 loss: 1.13915576e-06
Iter: 911 loss: 1.13876013e-06
Iter: 912 loss: 1.13938825e-06
Iter: 913 loss: 1.13835904e-06
Iter: 914 loss: 1.13805299e-06
Iter: 915 loss: 1.14427883e-06
Iter: 916 loss: 1.13789031e-06
Iter: 917 loss: 1.13771023e-06
Iter: 918 loss: 1.13781357e-06
Iter: 919 loss: 1.13740714e-06
Iter: 920 loss: 1.13725707e-06
Iter: 921 loss: 1.13728106e-06
Iter: 922 loss: 1.1370513e-06
Iter: 923 loss: 1.13743579e-06
Iter: 924 loss: 1.13670194e-06
Iter: 925 loss: 1.1363262e-06
Iter: 926 loss: 1.13688e-06
Iter: 927 loss: 1.13613623e-06
Iter: 928 loss: 1.13577994e-06
Iter: 929 loss: 1.13612805e-06
Iter: 930 loss: 1.13555802e-06
Iter: 931 loss: 1.13511032e-06
Iter: 932 loss: 1.13530723e-06
Iter: 933 loss: 1.13487772e-06
Iter: 934 loss: 1.13416547e-06
Iter: 935 loss: 1.1348227e-06
Iter: 936 loss: 1.13377678e-06
Iter: 937 loss: 1.13307124e-06
Iter: 938 loss: 1.13553187e-06
Iter: 939 loss: 1.1329073e-06
Iter: 940 loss: 1.13233045e-06
Iter: 941 loss: 1.13470355e-06
Iter: 942 loss: 1.13229316e-06
Iter: 943 loss: 1.13183228e-06
Iter: 944 loss: 1.13329202e-06
Iter: 945 loss: 1.13161877e-06
Iter: 946 loss: 1.13121723e-06
Iter: 947 loss: 1.13121257e-06
Iter: 948 loss: 1.13072679e-06
Iter: 949 loss: 1.13042643e-06
Iter: 950 loss: 1.13036981e-06
Iter: 951 loss: 1.12986436e-06
Iter: 952 loss: 1.13449573e-06
Iter: 953 loss: 1.12993587e-06
Iter: 954 loss: 1.12985902e-06
Iter: 955 loss: 1.12936073e-06
Iter: 956 loss: 1.13593319e-06
Iter: 957 loss: 1.1294444e-06
Iter: 958 loss: 1.12915757e-06
Iter: 959 loss: 1.13150179e-06
Iter: 960 loss: 1.12921316e-06
Iter: 961 loss: 1.1290665e-06
Iter: 962 loss: 1.12814655e-06
Iter: 963 loss: 1.1388496e-06
Iter: 964 loss: 1.12832549e-06
Iter: 965 loss: 1.12769237e-06
Iter: 966 loss: 1.13125964e-06
Iter: 967 loss: 1.12764371e-06
Iter: 968 loss: 1.12702094e-06
Iter: 969 loss: 1.1276436e-06
Iter: 970 loss: 1.12670182e-06
Iter: 971 loss: 1.12622911e-06
Iter: 972 loss: 1.12819396e-06
Iter: 973 loss: 1.12605278e-06
Iter: 974 loss: 1.12556131e-06
Iter: 975 loss: 1.12513931e-06
Iter: 976 loss: 1.12500334e-06
Iter: 977 loss: 1.12448038e-06
Iter: 978 loss: 1.12612884e-06
Iter: 979 loss: 1.12400994e-06
Iter: 980 loss: 1.12361204e-06
Iter: 981 loss: 1.12363705e-06
Iter: 982 loss: 1.12313137e-06
Iter: 983 loss: 1.12267753e-06
Iter: 984 loss: 1.12269322e-06
Iter: 985 loss: 1.12246585e-06
Iter: 986 loss: 1.12237376e-06
Iter: 987 loss: 1.12223483e-06
Iter: 988 loss: 1.12182215e-06
Iter: 989 loss: 1.12690384e-06
Iter: 990 loss: 1.121884e-06
Iter: 991 loss: 1.12131931e-06
Iter: 992 loss: 1.12300393e-06
Iter: 993 loss: 1.12136854e-06
Iter: 994 loss: 1.12071939e-06
Iter: 995 loss: 1.12388443e-06
Iter: 996 loss: 1.12052726e-06
Iter: 997 loss: 1.12019507e-06
Iter: 998 loss: 1.11995917e-06
Iter: 999 loss: 1.1199204e-06
Iter: 1000 loss: 1.11953295e-06
Iter: 1001 loss: 1.12003386e-06
Iter: 1002 loss: 1.11937447e-06
Iter: 1003 loss: 1.11875829e-06
Iter: 1004 loss: 1.1196571e-06
Iter: 1005 loss: 1.11838108e-06
Iter: 1006 loss: 1.11793361e-06
Iter: 1007 loss: 1.11999384e-06
Iter: 1008 loss: 1.11767827e-06
Iter: 1009 loss: 1.11747363e-06
Iter: 1010 loss: 1.11887277e-06
Iter: 1011 loss: 1.11725035e-06
Iter: 1012 loss: 1.11680174e-06
Iter: 1013 loss: 1.1188489e-06
Iter: 1014 loss: 1.11699455e-06
Iter: 1015 loss: 1.11664451e-06
Iter: 1016 loss: 1.11659961e-06
Iter: 1017 loss: 1.11635063e-06
Iter: 1018 loss: 1.11689155e-06
Iter: 1019 loss: 1.11647569e-06
Iter: 1020 loss: 1.11635939e-06
Iter: 1021 loss: 1.11646978e-06
Iter: 1022 loss: 1.11638974e-06
Iter: 1023 loss: 1.11649604e-06
Iter: 1024 loss: 1.11643203e-06
Iter: 1025 loss: 1.11641589e-06
Iter: 1026 loss: 1.11659028e-06
Iter: 1027 loss: 1.11649069e-06
Iter: 1028 loss: 1.11643021e-06
Iter: 1029 loss: 1.11638701e-06
Iter: 1030 loss: 1.11635836e-06
Iter: 1031 loss: 1.11633926e-06
Iter: 1032 loss: 1.11637416e-06
Iter: 1033 loss: 1.11636268e-06
Iter: 1034 loss: 1.11632505e-06
Iter: 1035 loss: 1.11634608e-06
Iter: 1036 loss: 1.11635029e-06
Iter: 1037 loss: 1.11636496e-06
Iter: 1038 loss: 1.11635018e-06
Iter: 1039 loss: 1.11635745e-06
Iter: 1040 loss: 1.1163504e-06
Iter: 1041 loss: 1.11635131e-06
Iter: 1042 loss: 1.11635745e-06
Iter: 1043 loss: 1.11635131e-06
Iter: 1044 loss: 1.11635131e-06
Iter: 1045 loss: 1.11635131e-06
Iter: 1046 loss: 1.11635131e-06
Iter: 1047 loss: 1.11635131e-06
Iter: 1048 loss: 1.11635131e-06
Iter: 1049 loss: 1.11635745e-06
Iter: 1050 loss: 1.11635745e-06
Iter: 1051 loss: 1.11635745e-06
Iter: 1052 loss: 1.11635131e-06
Iter: 1053 loss: 1.11635745e-06
Iter: 1054 loss: 1.11635131e-06
Iter: 1055 loss: 1.11635131e-06
Iter: 1056 loss: 1.11635745e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8
+ date
Mon Oct 26 12:29:53 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.8_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi2.8_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e8023598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e80059d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e8005268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7ff9488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7ff9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7f2c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7f2c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7e438c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7e43840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7e43378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7e2e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7dcc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7e01e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7e32840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7dbc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7e198c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7dbcd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7d47488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7d47598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7cb07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7ca1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7c1b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7ca1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7c758c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7be4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7bceae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7bce620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7ba52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7ba5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7b518c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7b801e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7bf78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75de164620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75e7b0f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75de148ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f75de0e3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1647.00635
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Traceback (most recent call last):
  File "biholoNN_train.py", line 169, in <module>
    results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=train_func, initial_position=init_params, max_iterations=max_epochs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 287, in minimize
    parallel_iterations=parallel_iterations)[0]
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 574, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2499, in while_loop_v2
    return_same_structure=True)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2735, in while_loop
    loop_vars = body(*loop_vars)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 260, in _body
    max_line_search_iterations)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 156, in line_search_step
    max_iterations=max_iterations)  # No search needed for these.
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 259, in hager_zhang
    threshold_use_approximate_wolfe_condition)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 609, in _prepare_args
    val_initial = value_and_gradients_function(initial_step_size)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 251, in _restricted_func
    objective_value, gradient = value_and_gradients_function(pt)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 814, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
	 [[Sum_1/_34]]
  (1) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_f_16876]

Function call stack:
f -> f

++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi2.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3
+ date
Mon Oct 26 12:32:43 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi3_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi3_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi3_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 0 --phi 3 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi0_phi3_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18d8fdb510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18d8fcba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18d8fcb048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18fd490510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18fd4902f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18d8f610d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18d8ecb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b4772620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b47729d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b47720d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b47472f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b46f5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b4729f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b46e6048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b46e9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b47296a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b473dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b4687268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b4687400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b46a6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b45e06a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b454d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b45e0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b455ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b44bb488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b44cab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b44ca840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b450c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b450c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b44ac950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b44a2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b444e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b43f9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b4442d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b43b60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f18b437cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1945.74915
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Traceback (most recent call last):
  File "biholoNN_train.py", line 169, in <module>
    results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=train_func, initial_position=init_params, max_iterations=max_epochs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 287, in minimize
    parallel_iterations=parallel_iterations)[0]
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 574, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2499, in while_loop_v2
    return_same_structure=True)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2735, in while_loop
    loop_vars = body(*loop_vars)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 260, in _body
    max_line_search_iterations)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 156, in line_search_step
    max_iterations=max_iterations)  # No search needed for these.
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 259, in hager_zhang
    threshold_use_approximate_wolfe_condition)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 609, in _prepare_args
    val_initial = value_and_gradients_function(initial_step_size)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 251, in _restricted_func
    objective_value, gradient = value_and_gradients_function(pt)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 814, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
	 [[Sum_1/_34]]
  (1) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_f_16876]

Function call stack:
f -> f

++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi3/500_500_500_500_1
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0
+ date
Mon Oct 26 12:35:10 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi 1 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a02e9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a0310c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a0310f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a02aa510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a039a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a039a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a0384378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a039ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a0267ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a0200730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a0200488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a0186f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a01370d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a015fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a015ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a015f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a00f5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a00e6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a004a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a0061488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32a0061620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3280448950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32803f39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32803a1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32803a1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f328038b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f328037a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f328033e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3280311378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32802e5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32802e59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32802b97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32802b9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f328026e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3280211ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f328023e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.44537489e-06
Iter: 2 loss: 2.70809892e-06
Iter: 3 loss: 2.67784299e-06
Iter: 4 loss: 2.39110045e-06
Iter: 5 loss: 3.41736791e-06
Iter: 6 loss: 2.31837498e-06
Iter: 7 loss: 2.13919384e-06
Iter: 8 loss: 2.28847375e-06
Iter: 9 loss: 2.03260743e-06
Iter: 10 loss: 1.9028904e-06
Iter: 11 loss: 1.89835191e-06
Iter: 12 loss: 1.85128761e-06
Iter: 13 loss: 1.78495168e-06
Iter: 14 loss: 1.78244045e-06
Iter: 15 loss: 1.75131049e-06
Iter: 16 loss: 2.19645699e-06
Iter: 17 loss: 1.75124774e-06
Iter: 18 loss: 1.70858777e-06
Iter: 19 loss: 1.59772321e-06
Iter: 20 loss: 2.47378512e-06
Iter: 21 loss: 1.57678801e-06
Iter: 22 loss: 1.49227139e-06
Iter: 23 loss: 1.8134283e-06
Iter: 24 loss: 1.4721162e-06
Iter: 25 loss: 1.46041373e-06
Iter: 26 loss: 1.44843739e-06
Iter: 27 loss: 1.42482509e-06
Iter: 28 loss: 1.44912565e-06
Iter: 29 loss: 1.41168573e-06
Iter: 30 loss: 1.39905819e-06
Iter: 31 loss: 1.37686197e-06
Iter: 32 loss: 1.3768497e-06
Iter: 33 loss: 1.35217624e-06
Iter: 34 loss: 1.33273261e-06
Iter: 35 loss: 1.32511e-06
Iter: 36 loss: 1.29264708e-06
Iter: 37 loss: 1.29043246e-06
Iter: 38 loss: 1.2560306e-06
Iter: 39 loss: 1.22094639e-06
Iter: 40 loss: 1.21421044e-06
Iter: 41 loss: 1.18493153e-06
Iter: 42 loss: 1.18408354e-06
Iter: 43 loss: 1.16203444e-06
Iter: 44 loss: 1.20547327e-06
Iter: 45 loss: 1.15297667e-06
Iter: 46 loss: 1.1335776e-06
Iter: 47 loss: 1.11184806e-06
Iter: 48 loss: 1.10890949e-06
Iter: 49 loss: 1.09807911e-06
Iter: 50 loss: 1.09686653e-06
Iter: 51 loss: 1.08329891e-06
Iter: 52 loss: 1.06194125e-06
Iter: 53 loss: 1.06173525e-06
Iter: 54 loss: 1.04127707e-06
Iter: 55 loss: 9.9804447e-07
Iter: 56 loss: 1.7055745e-06
Iter: 57 loss: 9.96788799e-07
Iter: 58 loss: 9.83285531e-07
Iter: 59 loss: 9.699462e-07
Iter: 60 loss: 9.43688065e-07
Iter: 61 loss: 1.00086288e-06
Iter: 62 loss: 9.3354555e-07
Iter: 63 loss: 9.24347205e-07
Iter: 64 loss: 9.03912678e-07
Iter: 65 loss: 1.19209938e-06
Iter: 66 loss: 9.02839133e-07
Iter: 67 loss: 8.81089818e-07
Iter: 68 loss: 9.97190909e-07
Iter: 69 loss: 8.7777579e-07
Iter: 70 loss: 8.71472423e-07
Iter: 71 loss: 8.66844061e-07
Iter: 72 loss: 8.60203045e-07
Iter: 73 loss: 8.43120631e-07
Iter: 74 loss: 9.84792791e-07
Iter: 75 loss: 8.40141581e-07
Iter: 76 loss: 8.22828042e-07
Iter: 77 loss: 8.22451284e-07
Iter: 78 loss: 8.12019834e-07
Iter: 79 loss: 8.15606199e-07
Iter: 80 loss: 8.04686351e-07
Iter: 81 loss: 7.96446386e-07
Iter: 82 loss: 8.11111249e-07
Iter: 83 loss: 7.92864057e-07
Iter: 84 loss: 7.874836e-07
Iter: 85 loss: 7.8746632e-07
Iter: 86 loss: 7.81844e-07
Iter: 87 loss: 7.65777941e-07
Iter: 88 loss: 8.39614302e-07
Iter: 89 loss: 7.59984175e-07
Iter: 90 loss: 7.42414159e-07
Iter: 91 loss: 7.60620537e-07
Iter: 92 loss: 7.32640785e-07
Iter: 93 loss: 7.29264684e-07
Iter: 94 loss: 7.2360092e-07
Iter: 95 loss: 7.14467433e-07
Iter: 96 loss: 7.28316934e-07
Iter: 97 loss: 7.10125619e-07
Iter: 98 loss: 7.05649654e-07
Iter: 99 loss: 6.97248652e-07
Iter: 100 loss: 8.82117945e-07
Iter: 101 loss: 6.97238079e-07
Iter: 102 loss: 6.88084128e-07
Iter: 103 loss: 6.89544606e-07
Iter: 104 loss: 6.81167194e-07
Iter: 105 loss: 6.74075636e-07
Iter: 106 loss: 6.71740338e-07
Iter: 107 loss: 6.68579617e-07
Iter: 108 loss: 6.66485676e-07
Iter: 109 loss: 6.65292703e-07
Iter: 110 loss: 6.5957073e-07
Iter: 111 loss: 7.00665339e-07
Iter: 112 loss: 6.59064199e-07
Iter: 113 loss: 6.56095665e-07
Iter: 114 loss: 6.52756398e-07
Iter: 115 loss: 6.52297558e-07
Iter: 116 loss: 6.48047887e-07
Iter: 117 loss: 6.80660207e-07
Iter: 118 loss: 6.47744287e-07
Iter: 119 loss: 6.4493895e-07
Iter: 120 loss: 6.74215585e-07
Iter: 121 loss: 6.44859313e-07
Iter: 122 loss: 6.42609393e-07
Iter: 123 loss: 6.36041364e-07
Iter: 124 loss: 6.60514e-07
Iter: 125 loss: 6.33166e-07
Iter: 126 loss: 6.23933715e-07
Iter: 127 loss: 6.38856932e-07
Iter: 128 loss: 6.19664206e-07
Iter: 129 loss: 6.24905908e-07
Iter: 130 loss: 6.17300941e-07
Iter: 131 loss: 6.14982696e-07
Iter: 132 loss: 6.13639259e-07
Iter: 133 loss: 6.12622443e-07
Iter: 134 loss: 6.10021e-07
Iter: 135 loss: 6.04159e-07
Iter: 136 loss: 6.82949462e-07
Iter: 137 loss: 6.03786589e-07
Iter: 138 loss: 5.97277676e-07
Iter: 139 loss: 6.17479031e-07
Iter: 140 loss: 5.95373763e-07
Iter: 141 loss: 5.89405147e-07
Iter: 142 loss: 5.89210345e-07
Iter: 143 loss: 5.86223166e-07
Iter: 144 loss: 5.84347276e-07
Iter: 145 loss: 5.83156748e-07
Iter: 146 loss: 5.79744892e-07
Iter: 147 loss: 5.7970567e-07
Iter: 148 loss: 5.78668221e-07
Iter: 149 loss: 5.76089576e-07
Iter: 150 loss: 5.99345299e-07
Iter: 151 loss: 5.75656543e-07
Iter: 152 loss: 5.7383977e-07
Iter: 153 loss: 5.73689249e-07
Iter: 154 loss: 5.71967917e-07
Iter: 155 loss: 5.71802673e-07
Iter: 156 loss: 5.70527561e-07
Iter: 157 loss: 5.6737548e-07
Iter: 158 loss: 5.61979732e-07
Iter: 159 loss: 5.61955858e-07
Iter: 160 loss: 5.56340183e-07
Iter: 161 loss: 5.60239187e-07
Iter: 162 loss: 5.52836809e-07
Iter: 163 loss: 5.55788461e-07
Iter: 164 loss: 5.50553e-07
Iter: 165 loss: 5.48726177e-07
Iter: 166 loss: 5.46295553e-07
Iter: 167 loss: 5.46156457e-07
Iter: 168 loss: 5.4465977e-07
Iter: 169 loss: 5.42475732e-07
Iter: 170 loss: 5.42407292e-07
Iter: 171 loss: 5.40611722e-07
Iter: 172 loss: 5.40489452e-07
Iter: 173 loss: 5.38006816e-07
Iter: 174 loss: 5.3583608e-07
Iter: 175 loss: 5.35172262e-07
Iter: 176 loss: 5.33402954e-07
Iter: 177 loss: 5.33378284e-07
Iter: 178 loss: 5.31529452e-07
Iter: 179 loss: 5.28099577e-07
Iter: 180 loss: 6.06551964e-07
Iter: 181 loss: 5.28100827e-07
Iter: 182 loss: 5.25518658e-07
Iter: 183 loss: 5.26931899e-07
Iter: 184 loss: 5.23819153e-07
Iter: 185 loss: 5.21336119e-07
Iter: 186 loss: 5.27493057e-07
Iter: 187 loss: 5.20471247e-07
Iter: 188 loss: 5.19837954e-07
Iter: 189 loss: 5.19255138e-07
Iter: 190 loss: 5.17864748e-07
Iter: 191 loss: 5.14312489e-07
Iter: 192 loss: 5.45562443e-07
Iter: 193 loss: 5.13761336e-07
Iter: 194 loss: 5.10725272e-07
Iter: 195 loss: 5.53399218e-07
Iter: 196 loss: 5.10724476e-07
Iter: 197 loss: 5.09265419e-07
Iter: 198 loss: 5.10408256e-07
Iter: 199 loss: 5.08396283e-07
Iter: 200 loss: 5.07242703e-07
Iter: 201 loss: 5.21204242e-07
Iter: 202 loss: 5.07226844e-07
Iter: 203 loss: 5.05849926e-07
Iter: 204 loss: 5.06798642e-07
Iter: 205 loss: 5.04992784e-07
Iter: 206 loss: 5.04090565e-07
Iter: 207 loss: 5.01551767e-07
Iter: 208 loss: 5.15253646e-07
Iter: 209 loss: 5.00789497e-07
Iter: 210 loss: 4.99677583e-07
Iter: 211 loss: 4.98957377e-07
Iter: 212 loss: 4.96834559e-07
Iter: 213 loss: 4.9846517e-07
Iter: 214 loss: 4.95549102e-07
Iter: 215 loss: 4.9480218e-07
Iter: 216 loss: 4.94779556e-07
Iter: 217 loss: 4.94102153e-07
Iter: 218 loss: 4.92780941e-07
Iter: 219 loss: 5.19354558e-07
Iter: 220 loss: 4.92770369e-07
Iter: 221 loss: 4.91609285e-07
Iter: 222 loss: 4.90967523e-07
Iter: 223 loss: 4.9044354e-07
Iter: 224 loss: 4.89389549e-07
Iter: 225 loss: 4.89138245e-07
Iter: 226 loss: 4.8840559e-07
Iter: 227 loss: 4.86547833e-07
Iter: 228 loss: 5.03960223e-07
Iter: 229 loss: 4.86278338e-07
Iter: 230 loss: 4.83502049e-07
Iter: 231 loss: 4.91010496e-07
Iter: 232 loss: 4.8259119e-07
Iter: 233 loss: 4.80517e-07
Iter: 234 loss: 4.83728627e-07
Iter: 235 loss: 4.79526534e-07
Iter: 236 loss: 4.7973208e-07
Iter: 237 loss: 4.78779498e-07
Iter: 238 loss: 4.78157176e-07
Iter: 239 loss: 4.7693328e-07
Iter: 240 loss: 5.00260512e-07
Iter: 241 loss: 4.76919411e-07
Iter: 242 loss: 4.75674625e-07
Iter: 243 loss: 4.74489781e-07
Iter: 244 loss: 4.74212158e-07
Iter: 245 loss: 4.73505793e-07
Iter: 246 loss: 4.73054115e-07
Iter: 247 loss: 4.7255341e-07
Iter: 248 loss: 4.70994564e-07
Iter: 249 loss: 4.74714511e-07
Iter: 250 loss: 4.70071797e-07
Iter: 251 loss: 4.68491e-07
Iter: 252 loss: 4.86159593e-07
Iter: 253 loss: 4.68462844e-07
Iter: 254 loss: 4.6777393e-07
Iter: 255 loss: 4.67656946e-07
Iter: 256 loss: 4.67351356e-07
Iter: 257 loss: 4.66470311e-07
Iter: 258 loss: 4.70052868e-07
Iter: 259 loss: 4.66111146e-07
Iter: 260 loss: 4.65595e-07
Iter: 261 loss: 4.65472056e-07
Iter: 262 loss: 4.64676276e-07
Iter: 263 loss: 4.62527453e-07
Iter: 264 loss: 4.78275865e-07
Iter: 265 loss: 4.62092146e-07
Iter: 266 loss: 4.60873196e-07
Iter: 267 loss: 4.60872712e-07
Iter: 268 loss: 4.59891282e-07
Iter: 269 loss: 4.61687478e-07
Iter: 270 loss: 4.59456373e-07
Iter: 271 loss: 4.58840105e-07
Iter: 272 loss: 4.5878744e-07
Iter: 273 loss: 4.58376547e-07
Iter: 274 loss: 4.57525402e-07
Iter: 275 loss: 4.72712372e-07
Iter: 276 loss: 4.57508406e-07
Iter: 277 loss: 4.56716521e-07
Iter: 278 loss: 4.56456462e-07
Iter: 279 loss: 4.55998588e-07
Iter: 280 loss: 4.54618771e-07
Iter: 281 loss: 4.71122689e-07
Iter: 282 loss: 4.54598393e-07
Iter: 283 loss: 4.53906267e-07
Iter: 284 loss: 4.53810173e-07
Iter: 285 loss: 4.533278e-07
Iter: 286 loss: 4.52554559e-07
Iter: 287 loss: 4.5810819e-07
Iter: 288 loss: 4.52494533e-07
Iter: 289 loss: 4.5210129e-07
Iter: 290 loss: 4.51393845e-07
Iter: 291 loss: 4.51403906e-07
Iter: 292 loss: 4.5100424e-07
Iter: 293 loss: 4.50942935e-07
Iter: 294 loss: 4.50422192e-07
Iter: 295 loss: 4.49553681e-07
Iter: 296 loss: 4.49544359e-07
Iter: 297 loss: 4.48650098e-07
Iter: 298 loss: 4.48391035e-07
Iter: 299 loss: 4.47832861e-07
Iter: 300 loss: 4.46662256e-07
Iter: 301 loss: 4.53925907e-07
Iter: 302 loss: 4.46535978e-07
Iter: 303 loss: 4.45167473e-07
Iter: 304 loss: 4.50076499e-07
Iter: 305 loss: 4.44829254e-07
Iter: 306 loss: 4.43768243e-07
Iter: 307 loss: 4.57266879e-07
Iter: 308 loss: 4.43768812e-07
Iter: 309 loss: 4.4347729e-07
Iter: 310 loss: 4.42746824e-07
Iter: 311 loss: 4.48525327e-07
Iter: 312 loss: 4.42605767e-07
Iter: 313 loss: 4.41982053e-07
Iter: 314 loss: 4.41974493e-07
Iter: 315 loss: 4.41252382e-07
Iter: 316 loss: 4.42127174e-07
Iter: 317 loss: 4.40852403e-07
Iter: 318 loss: 4.40412634e-07
Iter: 319 loss: 4.45094884e-07
Iter: 320 loss: 4.40397343e-07
Iter: 321 loss: 4.39923895e-07
Iter: 322 loss: 4.38807319e-07
Iter: 323 loss: 4.50541904e-07
Iter: 324 loss: 4.3866541e-07
Iter: 325 loss: 4.37830181e-07
Iter: 326 loss: 4.44566979e-07
Iter: 327 loss: 4.37780614e-07
Iter: 328 loss: 4.37292158e-07
Iter: 329 loss: 4.37279112e-07
Iter: 330 loss: 4.37083287e-07
Iter: 331 loss: 4.36594945e-07
Iter: 332 loss: 4.41099e-07
Iter: 333 loss: 4.3651761e-07
Iter: 334 loss: 4.35869453e-07
Iter: 335 loss: 4.34957116e-07
Iter: 336 loss: 4.34925312e-07
Iter: 337 loss: 4.34202803e-07
Iter: 338 loss: 4.34079197e-07
Iter: 339 loss: 4.33326875e-07
Iter: 340 loss: 4.37032725e-07
Iter: 341 loss: 4.33188177e-07
Iter: 342 loss: 4.32576485e-07
Iter: 343 loss: 4.33397275e-07
Iter: 344 loss: 4.32267313e-07
Iter: 345 loss: 4.3180637e-07
Iter: 346 loss: 4.31451099e-07
Iter: 347 loss: 4.31311378e-07
Iter: 348 loss: 4.31491515e-07
Iter: 349 loss: 4.31104752e-07
Iter: 350 loss: 4.30911825e-07
Iter: 351 loss: 4.30387956e-07
Iter: 352 loss: 4.34066038e-07
Iter: 353 loss: 4.30281801e-07
Iter: 354 loss: 4.29815429e-07
Iter: 355 loss: 4.29798519e-07
Iter: 356 loss: 4.29395982e-07
Iter: 357 loss: 4.28273495e-07
Iter: 358 loss: 4.34901239e-07
Iter: 359 loss: 4.2795682e-07
Iter: 360 loss: 4.28032308e-07
Iter: 361 loss: 4.27447759e-07
Iter: 362 loss: 4.27064606e-07
Iter: 363 loss: 4.26512599e-07
Iter: 364 loss: 4.26498531e-07
Iter: 365 loss: 4.25948258e-07
Iter: 366 loss: 4.26497138e-07
Iter: 367 loss: 4.25633857e-07
Iter: 368 loss: 4.25093617e-07
Iter: 369 loss: 4.24946847e-07
Iter: 370 loss: 4.24612324e-07
Iter: 371 loss: 4.24152091e-07
Iter: 372 loss: 4.2413231e-07
Iter: 373 loss: 4.23649794e-07
Iter: 374 loss: 4.23791505e-07
Iter: 375 loss: 4.23304868e-07
Iter: 376 loss: 4.22646252e-07
Iter: 377 loss: 4.23303618e-07
Iter: 378 loss: 4.22277452e-07
Iter: 379 loss: 4.21753072e-07
Iter: 380 loss: 4.23327066e-07
Iter: 381 loss: 4.21594478e-07
Iter: 382 loss: 4.21489858e-07
Iter: 383 loss: 4.21392258e-07
Iter: 384 loss: 4.21213116e-07
Iter: 385 loss: 4.2072196e-07
Iter: 386 loss: 4.24157577e-07
Iter: 387 loss: 4.20606227e-07
Iter: 388 loss: 4.20283641e-07
Iter: 389 loss: 4.20223e-07
Iter: 390 loss: 4.19975407e-07
Iter: 391 loss: 4.19336232e-07
Iter: 392 loss: 4.24386769e-07
Iter: 393 loss: 4.19214672e-07
Iter: 394 loss: 4.1892747e-07
Iter: 395 loss: 4.18802216e-07
Iter: 396 loss: 4.18483665e-07
Iter: 397 loss: 4.17906392e-07
Iter: 398 loss: 4.30819455e-07
Iter: 399 loss: 4.17905937e-07
Iter: 400 loss: 4.17473e-07
Iter: 401 loss: 4.17890107e-07
Iter: 402 loss: 4.17218075e-07
Iter: 403 loss: 4.16951508e-07
Iter: 404 loss: 4.20949448e-07
Iter: 405 loss: 4.16937496e-07
Iter: 406 loss: 4.16711686e-07
Iter: 407 loss: 4.16732405e-07
Iter: 408 loss: 4.16534306e-07
Iter: 409 loss: 4.16086664e-07
Iter: 410 loss: 4.15957743e-07
Iter: 411 loss: 4.15698764e-07
Iter: 412 loss: 4.15109753e-07
Iter: 413 loss: 4.14729783e-07
Iter: 414 loss: 4.14507e-07
Iter: 415 loss: 4.13899699e-07
Iter: 416 loss: 4.19054174e-07
Iter: 417 loss: 4.13865109e-07
Iter: 418 loss: 4.13474339e-07
Iter: 419 loss: 4.15688135e-07
Iter: 420 loss: 4.13422612e-07
Iter: 421 loss: 4.13162e-07
Iter: 422 loss: 4.13155476e-07
Iter: 423 loss: 4.13040084e-07
Iter: 424 loss: 4.12672733e-07
Iter: 425 loss: 4.13416444e-07
Iter: 426 loss: 4.12459201e-07
Iter: 427 loss: 4.12174131e-07
Iter: 428 loss: 4.12109955e-07
Iter: 429 loss: 4.11769747e-07
Iter: 430 loss: 4.1112915e-07
Iter: 431 loss: 4.24884263e-07
Iter: 432 loss: 4.11116133e-07
Iter: 433 loss: 4.10590701e-07
Iter: 434 loss: 4.14256533e-07
Iter: 435 loss: 4.10539656e-07
Iter: 436 loss: 4.10371e-07
Iter: 437 loss: 4.10354062e-07
Iter: 438 loss: 4.10206269e-07
Iter: 439 loss: 4.10161846e-07
Iter: 440 loss: 4.10084226e-07
Iter: 441 loss: 4.09837952e-07
Iter: 442 loss: 4.10222526e-07
Iter: 443 loss: 4.09712101e-07
Iter: 444 loss: 4.09433198e-07
Iter: 445 loss: 4.09274151e-07
Iter: 446 loss: 4.09164613e-07
Iter: 447 loss: 4.08807921e-07
Iter: 448 loss: 4.08797519e-07
Iter: 449 loss: 4.08576227e-07
Iter: 450 loss: 4.08024221e-07
Iter: 451 loss: 4.1322e-07
Iter: 452 loss: 4.07912609e-07
Iter: 453 loss: 4.07770585e-07
Iter: 454 loss: 4.07671621e-07
Iter: 455 loss: 4.07398574e-07
Iter: 456 loss: 4.0763365e-07
Iter: 457 loss: 4.07260075e-07
Iter: 458 loss: 4.07059019e-07
Iter: 459 loss: 4.06793959e-07
Iter: 460 loss: 4.06797483e-07
Iter: 461 loss: 4.06540835e-07
Iter: 462 loss: 4.06524208e-07
Iter: 463 loss: 4.06361863e-07
Iter: 464 loss: 4.06023844e-07
Iter: 465 loss: 4.11689683e-07
Iter: 466 loss: 4.06018387e-07
Iter: 467 loss: 4.05565686e-07
Iter: 468 loss: 4.0848289e-07
Iter: 469 loss: 4.0552095e-07
Iter: 470 loss: 4.05033688e-07
Iter: 471 loss: 4.07292077e-07
Iter: 472 loss: 4.04944217e-07
Iter: 473 loss: 4.0469871e-07
Iter: 474 loss: 4.05894724e-07
Iter: 475 loss: 4.04643913e-07
Iter: 476 loss: 4.04463549e-07
Iter: 477 loss: 4.04239017e-07
Iter: 478 loss: 4.04211278e-07
Iter: 479 loss: 4.04120499e-07
Iter: 480 loss: 4.04066498e-07
Iter: 481 loss: 4.03918818e-07
Iter: 482 loss: 4.03660408e-07
Iter: 483 loss: 4.09554389e-07
Iter: 484 loss: 4.03658646e-07
Iter: 485 loss: 4.03376418e-07
Iter: 486 loss: 4.03617491e-07
Iter: 487 loss: 4.03199721e-07
Iter: 488 loss: 4.0282282e-07
Iter: 489 loss: 4.07898426e-07
Iter: 490 loss: 4.02825037e-07
Iter: 491 loss: 4.02653853e-07
Iter: 492 loss: 4.02240374e-07
Iter: 493 loss: 4.06440279e-07
Iter: 494 loss: 4.02183275e-07
Iter: 495 loss: 4.02076438e-07
Iter: 496 loss: 4.0197267e-07
Iter: 497 loss: 4.01821779e-07
Iter: 498 loss: 4.01523522e-07
Iter: 499 loss: 4.07279799e-07
Iter: 500 loss: 4.01522698e-07
Iter: 501 loss: 4.01264231e-07
Iter: 502 loss: 4.01446869e-07
Iter: 503 loss: 4.0110109e-07
Iter: 504 loss: 4.00819772e-07
Iter: 505 loss: 4.03502895e-07
Iter: 506 loss: 4.0079783e-07
Iter: 507 loss: 4.00615022e-07
Iter: 508 loss: 4.00228146e-07
Iter: 509 loss: 4.06884226e-07
Iter: 510 loss: 4.00218966e-07
Iter: 511 loss: 3.99709165e-07
Iter: 512 loss: 4.04193827e-07
Iter: 513 loss: 3.99689782e-07
Iter: 514 loss: 3.9939016e-07
Iter: 515 loss: 4.01481941e-07
Iter: 516 loss: 3.99360431e-07
Iter: 517 loss: 3.99152384e-07
Iter: 518 loss: 4.02146725e-07
Iter: 519 loss: 3.99152896e-07
Iter: 520 loss: 3.99057683e-07
Iter: 521 loss: 3.98970883e-07
Iter: 522 loss: 3.9895292e-07
Iter: 523 loss: 3.98893604e-07
Iter: 524 loss: 3.98866632e-07
Iter: 525 loss: 3.98780571e-07
Iter: 526 loss: 3.98542568e-07
Iter: 527 loss: 3.99651015e-07
Iter: 528 loss: 3.98450368e-07
Iter: 529 loss: 3.98265882e-07
Iter: 530 loss: 3.98265712e-07
Iter: 531 loss: 3.98081937e-07
Iter: 532 loss: 3.97871702e-07
Iter: 533 loss: 3.97843e-07
Iter: 534 loss: 3.97547694e-07
Iter: 535 loss: 3.9789353e-07
Iter: 536 loss: 3.97412606e-07
Iter: 537 loss: 3.97254951e-07
Iter: 538 loss: 3.97207884e-07
Iter: 539 loss: 3.9709721e-07
Iter: 540 loss: 3.96929465e-07
Iter: 541 loss: 3.96936912e-07
Iter: 542 loss: 3.96687199e-07
Iter: 543 loss: 3.97134443e-07
Iter: 544 loss: 3.96613416e-07
Iter: 545 loss: 3.96377118e-07
Iter: 546 loss: 3.98543705e-07
Iter: 547 loss: 3.96357393e-07
Iter: 548 loss: 3.96190103e-07
Iter: 549 loss: 3.9729423e-07
Iter: 550 loss: 3.96171345e-07
Iter: 551 loss: 3.95969153e-07
Iter: 552 loss: 3.9555178e-07
Iter: 553 loss: 4.02863975e-07
Iter: 554 loss: 3.95544618e-07
Iter: 555 loss: 3.95224788e-07
Iter: 556 loss: 3.99261069e-07
Iter: 557 loss: 3.95215238e-07
Iter: 558 loss: 3.94845756e-07
Iter: 559 loss: 3.95365362e-07
Iter: 560 loss: 3.94673179e-07
Iter: 561 loss: 3.94513052e-07
Iter: 562 loss: 3.9458115e-07
Iter: 563 loss: 3.94410051e-07
Iter: 564 loss: 3.94166648e-07
Iter: 565 loss: 3.95850407e-07
Iter: 566 loss: 3.94143626e-07
Iter: 567 loss: 3.94030707e-07
Iter: 568 loss: 3.93852474e-07
Iter: 569 loss: 3.93855601e-07
Iter: 570 loss: 3.93698315e-07
Iter: 571 loss: 3.93698116e-07
Iter: 572 loss: 3.93550778e-07
Iter: 573 loss: 3.93337e-07
Iter: 574 loss: 3.93337501e-07
Iter: 575 loss: 3.93039727e-07
Iter: 576 loss: 3.93230778e-07
Iter: 577 loss: 3.92835034e-07
Iter: 578 loss: 3.92633439e-07
Iter: 579 loss: 3.9261721e-07
Iter: 580 loss: 3.92452364e-07
Iter: 581 loss: 3.93173366e-07
Iter: 582 loss: 3.92420702e-07
Iter: 583 loss: 3.92267737e-07
Iter: 584 loss: 3.9260658e-07
Iter: 585 loss: 3.92210836e-07
Iter: 586 loss: 3.92117613e-07
Iter: 587 loss: 3.92142681e-07
Iter: 588 loss: 3.92044342e-07
Iter: 589 loss: 3.91896094e-07
Iter: 590 loss: 3.93598896e-07
Iter: 591 loss: 3.91891206e-07
Iter: 592 loss: 3.91831065e-07
Iter: 593 loss: 3.91662724e-07
Iter: 594 loss: 3.92871073e-07
Iter: 595 loss: 3.91637457e-07
Iter: 596 loss: 3.91400476e-07
Iter: 597 loss: 3.94417157e-07
Iter: 598 loss: 3.91402239e-07
Iter: 599 loss: 3.91268941e-07
Iter: 600 loss: 3.90976822e-07
Iter: 601 loss: 3.95445852e-07
Iter: 602 loss: 3.90959059e-07
Iter: 603 loss: 3.90759453e-07
Iter: 604 loss: 3.90755361e-07
Iter: 605 loss: 3.90599467e-07
Iter: 606 loss: 3.91162814e-07
Iter: 607 loss: 3.90556266e-07
Iter: 608 loss: 3.90456421e-07
Iter: 609 loss: 3.90302148e-07
Iter: 610 loss: 3.9029527e-07
Iter: 611 loss: 3.90134858e-07
Iter: 612 loss: 3.91567823e-07
Iter: 613 loss: 3.90132868e-07
Iter: 614 loss: 3.89982063e-07
Iter: 615 loss: 3.91325159e-07
Iter: 616 loss: 3.8998121e-07
Iter: 617 loss: 3.89854392e-07
Iter: 618 loss: 3.89785697e-07
Iter: 619 loss: 3.89733657e-07
Iter: 620 loss: 3.89527884e-07
Iter: 621 loss: 3.89431904e-07
Iter: 622 loss: 3.89326203e-07
Iter: 623 loss: 3.89190689e-07
Iter: 624 loss: 3.89146834e-07
Iter: 625 loss: 3.89077e-07
Iter: 626 loss: 3.88889646e-07
Iter: 627 loss: 3.91076583e-07
Iter: 628 loss: 3.88872195e-07
Iter: 629 loss: 3.88790397e-07
Iter: 630 loss: 3.8878062e-07
Iter: 631 loss: 3.88694389e-07
Iter: 632 loss: 3.88525052e-07
Iter: 633 loss: 3.92327308e-07
Iter: 634 loss: 3.8853122e-07
Iter: 635 loss: 3.88384279e-07
Iter: 636 loss: 3.8855552e-07
Iter: 637 loss: 3.883207e-07
Iter: 638 loss: 3.88131355e-07
Iter: 639 loss: 3.89414083e-07
Iter: 640 loss: 3.88098584e-07
Iter: 641 loss: 3.87973955e-07
Iter: 642 loss: 3.87833637e-07
Iter: 643 loss: 3.87814566e-07
Iter: 644 loss: 3.87615e-07
Iter: 645 loss: 3.87889315e-07
Iter: 646 loss: 3.87509033e-07
Iter: 647 loss: 3.87381903e-07
Iter: 648 loss: 3.87378549e-07
Iter: 649 loss: 3.87293198e-07
Iter: 650 loss: 3.87608168e-07
Iter: 651 loss: 3.87279897e-07
Iter: 652 loss: 3.87196877e-07
Iter: 653 loss: 3.87154756e-07
Iter: 654 loss: 3.87125e-07
Iter: 655 loss: 3.87030468e-07
Iter: 656 loss: 3.87034675e-07
Iter: 657 loss: 3.86944919e-07
Iter: 658 loss: 3.86746e-07
Iter: 659 loss: 3.89298208e-07
Iter: 660 loss: 3.86733575e-07
Iter: 661 loss: 3.86559179e-07
Iter: 662 loss: 3.87626841e-07
Iter: 663 loss: 3.86536044e-07
Iter: 664 loss: 3.86286445e-07
Iter: 665 loss: 3.86532122e-07
Iter: 666 loss: 3.86157069e-07
Iter: 667 loss: 3.85981878e-07
Iter: 668 loss: 3.85944219e-07
Iter: 669 loss: 3.85822e-07
Iter: 670 loss: 3.85757062e-07
Iter: 671 loss: 3.85708319e-07
Iter: 672 loss: 3.85643943e-07
Iter: 673 loss: 3.85471594e-07
Iter: 674 loss: 3.87516451e-07
Iter: 675 loss: 3.85465228e-07
Iter: 676 loss: 3.85282902e-07
Iter: 677 loss: 3.86487159e-07
Iter: 678 loss: 3.85271164e-07
Iter: 679 loss: 3.85160405e-07
Iter: 680 loss: 3.85159467e-07
Iter: 681 loss: 3.85069342e-07
Iter: 682 loss: 3.85036e-07
Iter: 683 loss: 3.84968331e-07
Iter: 684 loss: 3.84817668e-07
Iter: 685 loss: 3.85067153e-07
Iter: 686 loss: 3.84747523e-07
Iter: 687 loss: 3.84642817e-07
Iter: 688 loss: 3.86117733e-07
Iter: 689 loss: 3.84642846e-07
Iter: 690 loss: 3.84540044e-07
Iter: 691 loss: 3.84534303e-07
Iter: 692 loss: 3.84464727e-07
Iter: 693 loss: 3.84363801e-07
Iter: 694 loss: 3.84381735e-07
Iter: 695 loss: 3.84300279e-07
Iter: 696 loss: 3.84171841e-07
Iter: 697 loss: 3.85397982e-07
Iter: 698 loss: 3.84157772e-07
Iter: 699 loss: 3.8406219e-07
Iter: 700 loss: 3.83820861e-07
Iter: 701 loss: 3.86076522e-07
Iter: 702 loss: 3.83799772e-07
Iter: 703 loss: 3.83730082e-07
Iter: 704 loss: 3.83652178e-07
Iter: 705 loss: 3.8356086e-07
Iter: 706 loss: 3.83340137e-07
Iter: 707 loss: 3.87352571e-07
Iter: 708 loss: 3.83339085e-07
Iter: 709 loss: 3.83132431e-07
Iter: 710 loss: 3.83518682e-07
Iter: 711 loss: 3.83055e-07
Iter: 712 loss: 3.8294047e-07
Iter: 713 loss: 3.83796191e-07
Iter: 714 loss: 3.82913925e-07
Iter: 715 loss: 3.8280524e-07
Iter: 716 loss: 3.83417131e-07
Iter: 717 loss: 3.82793331e-07
Iter: 718 loss: 3.82702069e-07
Iter: 719 loss: 3.8266765e-07
Iter: 720 loss: 3.82628087e-07
Iter: 721 loss: 3.8247029e-07
Iter: 722 loss: 3.82732935e-07
Iter: 723 loss: 3.82406313e-07
Iter: 724 loss: 3.8220702e-07
Iter: 725 loss: 3.83607585e-07
Iter: 726 loss: 3.82188944e-07
Iter: 727 loss: 3.82077587e-07
Iter: 728 loss: 3.81852033e-07
Iter: 729 loss: 3.86069445e-07
Iter: 730 loss: 3.81849873e-07
Iter: 731 loss: 3.81790443e-07
Iter: 732 loss: 3.81736186e-07
Iter: 733 loss: 3.81646601e-07
Iter: 734 loss: 3.81516088e-07
Iter: 735 loss: 3.81524188e-07
Iter: 736 loss: 3.81432415e-07
Iter: 737 loss: 3.82191899e-07
Iter: 738 loss: 3.81420392e-07
Iter: 739 loss: 3.81302385e-07
Iter: 740 loss: 3.8138208e-07
Iter: 741 loss: 3.81238578e-07
Iter: 742 loss: 3.81110681e-07
Iter: 743 loss: 3.80918692e-07
Iter: 744 loss: 3.80914742e-07
Iter: 745 loss: 3.80701039e-07
Iter: 746 loss: 3.8268729e-07
Iter: 747 loss: 3.80684696e-07
Iter: 748 loss: 3.80524767e-07
Iter: 749 loss: 3.81722174e-07
Iter: 750 loss: 3.80515132e-07
Iter: 751 loss: 3.80371944e-07
Iter: 752 loss: 3.8064502e-07
Iter: 753 loss: 3.80319733e-07
Iter: 754 loss: 3.80190301e-07
Iter: 755 loss: 3.80464115e-07
Iter: 756 loss: 3.80132974e-07
Iter: 757 loss: 3.80075647e-07
Iter: 758 loss: 3.8007164e-07
Iter: 759 loss: 3.80009112e-07
Iter: 760 loss: 3.79838298e-07
Iter: 761 loss: 3.81273423e-07
Iter: 762 loss: 3.79811581e-07
Iter: 763 loss: 3.79686156e-07
Iter: 764 loss: 3.81477662e-07
Iter: 765 loss: 3.79691443e-07
Iter: 766 loss: 3.79551267e-07
Iter: 767 loss: 3.79609276e-07
Iter: 768 loss: 3.79449034e-07
Iter: 769 loss: 3.79307721e-07
Iter: 770 loss: 3.79309654e-07
Iter: 771 loss: 3.79188947e-07
Iter: 772 loss: 3.79075402e-07
Iter: 773 loss: 3.79057809e-07
Iter: 774 loss: 3.7900088e-07
Iter: 775 loss: 3.78872301e-07
Iter: 776 loss: 3.81046448e-07
Iter: 777 loss: 3.78866787e-07
Iter: 778 loss: 3.78747046e-07
Iter: 779 loss: 3.79106268e-07
Iter: 780 loss: 3.78717374e-07
Iter: 781 loss: 3.78593029e-07
Iter: 782 loss: 3.79032628e-07
Iter: 783 loss: 3.78576033e-07
Iter: 784 loss: 3.78456605e-07
Iter: 785 loss: 3.78571087e-07
Iter: 786 loss: 3.7840033e-07
Iter: 787 loss: 3.78252395e-07
Iter: 788 loss: 3.78246909e-07
Iter: 789 loss: 3.78149025e-07
Iter: 790 loss: 3.77991171e-07
Iter: 791 loss: 3.79646536e-07
Iter: 792 loss: 3.7798884e-07
Iter: 793 loss: 3.77858e-07
Iter: 794 loss: 3.7817037e-07
Iter: 795 loss: 3.7780643e-07
Iter: 796 loss: 3.77751462e-07
Iter: 797 loss: 3.77743334e-07
Iter: 798 loss: 3.77688593e-07
Iter: 799 loss: 3.77606597e-07
Iter: 800 loss: 3.77600202e-07
Iter: 801 loss: 3.7756206e-07
Iter: 802 loss: 3.77428933e-07
Iter: 803 loss: 3.78875825e-07
Iter: 804 loss: 3.77425749e-07
Iter: 805 loss: 3.77359527e-07
Iter: 806 loss: 3.77343383e-07
Iter: 807 loss: 3.77268805e-07
Iter: 808 loss: 3.7710825e-07
Iter: 809 loss: 3.80069537e-07
Iter: 810 loss: 3.77110382e-07
Iter: 811 loss: 3.76934338e-07
Iter: 812 loss: 3.78621621e-07
Iter: 813 loss: 3.76921889e-07
Iter: 814 loss: 3.76805303e-07
Iter: 815 loss: 3.77382293e-07
Iter: 816 loss: 3.76774153e-07
Iter: 817 loss: 3.76671551e-07
Iter: 818 loss: 3.77255901e-07
Iter: 819 loss: 3.76653503e-07
Iter: 820 loss: 3.76558461e-07
Iter: 821 loss: 3.7660115e-07
Iter: 822 loss: 3.76520745e-07
Iter: 823 loss: 3.76400919e-07
Iter: 824 loss: 3.76283595e-07
Iter: 825 loss: 3.76264211e-07
Iter: 826 loss: 3.76207197e-07
Iter: 827 loss: 3.76163342e-07
Iter: 828 loss: 3.76090242e-07
Iter: 829 loss: 3.75953618e-07
Iter: 830 loss: 3.78272148e-07
Iter: 831 loss: 3.75950663e-07
Iter: 832 loss: 3.75845616e-07
Iter: 833 loss: 3.75836635e-07
Iter: 834 loss: 3.75752734e-07
Iter: 835 loss: 3.75986502e-07
Iter: 836 loss: 3.75714194e-07
Iter: 837 loss: 3.75645214e-07
Iter: 838 loss: 3.75594823e-07
Iter: 839 loss: 3.75568e-07
Iter: 840 loss: 3.75459166e-07
Iter: 841 loss: 3.76729474e-07
Iter: 842 loss: 3.75463401e-07
Iter: 843 loss: 3.75388481e-07
Iter: 844 loss: 3.75236141e-07
Iter: 845 loss: 3.76922543e-07
Iter: 846 loss: 3.75224204e-07
Iter: 847 loss: 3.75062029e-07
Iter: 848 loss: 3.76472769e-07
Iter: 849 loss: 3.75052224e-07
Iter: 850 loss: 3.74925747e-07
Iter: 851 loss: 3.74922877e-07
Iter: 852 loss: 3.74821298e-07
Iter: 853 loss: 3.74727591e-07
Iter: 854 loss: 3.74720543e-07
Iter: 855 loss: 3.74652245e-07
Iter: 856 loss: 3.74650426e-07
Iter: 857 loss: 3.7459273e-07
Iter: 858 loss: 3.74514826e-07
Iter: 859 loss: 3.74503486e-07
Iter: 860 loss: 3.74445278e-07
Iter: 861 loss: 3.74311696e-07
Iter: 862 loss: 3.74607168e-07
Iter: 863 loss: 3.74290437e-07
Iter: 864 loss: 3.74243058e-07
Iter: 865 loss: 3.74221372e-07
Iter: 866 loss: 3.7416973e-07
Iter: 867 loss: 3.74134572e-07
Iter: 868 loss: 3.74113938e-07
Iter: 869 loss: 3.74044816e-07
Iter: 870 loss: 3.74362202e-07
Iter: 871 loss: 3.74037711e-07
Iter: 872 loss: 3.73963701e-07
Iter: 873 loss: 3.74035295e-07
Iter: 874 loss: 3.73923427e-07
Iter: 875 loss: 3.73852572e-07
Iter: 876 loss: 3.73785099e-07
Iter: 877 loss: 3.73780551e-07
Iter: 878 loss: 3.73654075e-07
Iter: 879 loss: 3.74710766e-07
Iter: 880 loss: 3.73656576e-07
Iter: 881 loss: 3.73543571e-07
Iter: 882 loss: 3.73689488e-07
Iter: 883 loss: 3.73477548e-07
Iter: 884 loss: 3.73403196e-07
Iter: 885 loss: 3.74378601e-07
Iter: 886 loss: 3.7340709e-07
Iter: 887 loss: 3.73344221e-07
Iter: 888 loss: 3.73354624e-07
Iter: 889 loss: 3.73305966e-07
Iter: 890 loss: 3.73210582e-07
Iter: 891 loss: 3.7341934e-07
Iter: 892 loss: 3.73181251e-07
Iter: 893 loss: 3.73087943e-07
Iter: 894 loss: 3.73064552e-07
Iter: 895 loss: 3.73005406e-07
Iter: 896 loss: 3.72877054e-07
Iter: 897 loss: 3.73458022e-07
Iter: 898 loss: 3.72874069e-07
Iter: 899 loss: 3.7274512e-07
Iter: 900 loss: 3.73694832e-07
Iter: 901 loss: 3.72723605e-07
Iter: 902 loss: 3.72642887e-07
Iter: 903 loss: 3.72599061e-07
Iter: 904 loss: 3.72560805e-07
Iter: 905 loss: 3.72435466e-07
Iter: 906 loss: 3.73522653e-07
Iter: 907 loss: 3.72438734e-07
Iter: 908 loss: 3.72344601e-07
Iter: 909 loss: 3.72323285e-07
Iter: 910 loss: 3.7225729e-07
Iter: 911 loss: 3.72187344e-07
Iter: 912 loss: 3.72598947e-07
Iter: 913 loss: 3.72163271e-07
Iter: 914 loss: 3.72084656e-07
Iter: 915 loss: 3.72411108e-07
Iter: 916 loss: 3.72062772e-07
Iter: 917 loss: 3.71980207e-07
Iter: 918 loss: 3.72117199e-07
Iter: 919 loss: 3.71950563e-07
Iter: 920 loss: 3.71835313e-07
Iter: 921 loss: 3.7216796e-07
Iter: 922 loss: 3.71813172e-07
Iter: 923 loss: 3.71736235e-07
Iter: 924 loss: 3.72159917e-07
Iter: 925 loss: 3.71721768e-07
Iter: 926 loss: 3.71657052e-07
Iter: 927 loss: 3.71631529e-07
Iter: 928 loss: 3.71599754e-07
Iter: 929 loss: 3.71514375e-07
Iter: 930 loss: 3.71571133e-07
Iter: 931 loss: 3.71463614e-07
Iter: 932 loss: 3.71393469e-07
Iter: 933 loss: 3.71392275e-07
Iter: 934 loss: 3.71347369e-07
Iter: 935 loss: 3.71299052e-07
Iter: 936 loss: 3.71286717e-07
Iter: 937 loss: 3.71217197e-07
Iter: 938 loss: 3.71971709e-07
Iter: 939 loss: 3.71221518e-07
Iter: 940 loss: 3.71160183e-07
Iter: 941 loss: 3.71065511e-07
Iter: 942 loss: 3.71063777e-07
Iter: 943 loss: 3.70924852e-07
Iter: 944 loss: 3.71219244e-07
Iter: 945 loss: 3.70880684e-07
Iter: 946 loss: 3.70774245e-07
Iter: 947 loss: 3.72008031e-07
Iter: 948 loss: 3.70777656e-07
Iter: 949 loss: 3.70696711e-07
Iter: 950 loss: 3.70832566e-07
Iter: 951 loss: 3.70672439e-07
Iter: 952 loss: 3.70609229e-07
Iter: 953 loss: 3.71094188e-07
Iter: 954 loss: 3.70608205e-07
Iter: 955 loss: 3.70563839e-07
Iter: 956 loss: 3.70571627e-07
Iter: 957 loss: 3.70529165e-07
Iter: 958 loss: 3.70458565e-07
Iter: 959 loss: 3.70516602e-07
Iter: 960 loss: 3.70429376e-07
Iter: 961 loss: 3.70344e-07
Iter: 962 loss: 3.70322311e-07
Iter: 963 loss: 3.70284454e-07
Iter: 964 loss: 3.70187e-07
Iter: 965 loss: 3.71332447e-07
Iter: 966 loss: 3.70189525e-07
Iter: 967 loss: 3.70087918e-07
Iter: 968 loss: 3.70094313e-07
Iter: 969 loss: 3.70014561e-07
Iter: 970 loss: 3.69939272e-07
Iter: 971 loss: 3.70807e-07
Iter: 972 loss: 3.69942228e-07
Iter: 973 loss: 3.69882684e-07
Iter: 974 loss: 3.69977e-07
Iter: 975 loss: 3.69852216e-07
Iter: 976 loss: 3.69803246e-07
Iter: 977 loss: 3.69852557e-07
Iter: 978 loss: 3.69777638e-07
Iter: 979 loss: 3.69721135e-07
Iter: 980 loss: 3.70023912e-07
Iter: 981 loss: 3.69714513e-07
Iter: 982 loss: 3.69646187e-07
Iter: 983 loss: 3.69645193e-07
Iter: 984 loss: 3.6960077e-07
Iter: 985 loss: 3.69517295e-07
Iter: 986 loss: 3.70342775e-07
Iter: 987 loss: 3.69510673e-07
Iter: 988 loss: 3.69462953e-07
Iter: 989 loss: 3.69443853e-07
Iter: 990 loss: 3.69413e-07
Iter: 991 loss: 3.69316069e-07
Iter: 992 loss: 3.695869e-07
Iter: 993 loss: 3.69274801e-07
Iter: 994 loss: 3.69204201e-07
Iter: 995 loss: 3.69157078e-07
Iter: 996 loss: 3.69120187e-07
Iter: 997 loss: 3.69025145e-07
Iter: 998 loss: 3.69948935e-07
Iter: 999 loss: 3.69026935e-07
Iter: 1000 loss: 3.68939794e-07
Iter: 1001 loss: 3.69366376e-07
Iter: 1002 loss: 3.68923509e-07
Iter: 1003 loss: 3.68876101e-07
Iter: 1004 loss: 3.68869451e-07
Iter: 1005 loss: 3.6883722e-07
Iter: 1006 loss: 3.68722112e-07
Iter: 1007 loss: 3.69050952e-07
Iter: 1008 loss: 3.68703354e-07
Iter: 1009 loss: 3.68622267e-07
Iter: 1010 loss: 3.68581851e-07
Iter: 1011 loss: 3.6854442e-07
Iter: 1012 loss: 3.68449406e-07
Iter: 1013 loss: 3.69626349e-07
Iter: 1014 loss: 3.68446877e-07
Iter: 1015 loss: 3.68381848e-07
Iter: 1016 loss: 3.68468079e-07
Iter: 1017 loss: 3.68326766e-07
Iter: 1018 loss: 3.68267649e-07
Iter: 1019 loss: 3.68805445e-07
Iter: 1020 loss: 3.68264921e-07
Iter: 1021 loss: 3.68225358e-07
Iter: 1022 loss: 3.68227404e-07
Iter: 1023 loss: 3.68184686e-07
Iter: 1024 loss: 3.68117242e-07
Iter: 1025 loss: 3.68494028e-07
Iter: 1026 loss: 3.6812e-07
Iter: 1027 loss: 3.6807748e-07
Iter: 1028 loss: 3.68028566e-07
Iter: 1029 loss: 3.68033795e-07
Iter: 1030 loss: 3.67935513e-07
Iter: 1031 loss: 3.67961775e-07
Iter: 1032 loss: 3.67881626e-07
Iter: 1033 loss: 3.67819638e-07
Iter: 1034 loss: 3.67795735e-07
Iter: 1035 loss: 3.67769161e-07
Iter: 1036 loss: 3.67717547e-07
Iter: 1037 loss: 3.67712516e-07
Iter: 1038 loss: 3.67653513e-07
Iter: 1039 loss: 3.67657265e-07
Iter: 1040 loss: 3.67609658e-07
Iter: 1041 loss: 3.67551536e-07
Iter: 1042 loss: 3.67551934e-07
Iter: 1043 loss: 3.67486734e-07
Iter: 1044 loss: 3.68029845e-07
Iter: 1045 loss: 3.67492959e-07
Iter: 1046 loss: 3.67427305e-07
Iter: 1047 loss: 3.67651353e-07
Iter: 1048 loss: 3.67418238e-07
Iter: 1049 loss: 3.67349e-07
Iter: 1050 loss: 3.67486678e-07
Iter: 1051 loss: 3.6733735e-07
Iter: 1052 loss: 3.67280563e-07
Iter: 1053 loss: 3.67299691e-07
Iter: 1054 loss: 3.67224459e-07
Iter: 1055 loss: 3.67159231e-07
Iter: 1056 loss: 3.67724937e-07
Iter: 1057 loss: 3.67154144e-07
Iter: 1058 loss: 3.6710739e-07
Iter: 1059 loss: 3.67091218e-07
Iter: 1060 loss: 3.67062938e-07
Iter: 1061 loss: 3.67001917e-07
Iter: 1062 loss: 3.67023063e-07
Iter: 1063 loss: 3.66969402e-07
Iter: 1064 loss: 3.66923132e-07
Iter: 1065 loss: 3.66923558e-07
Iter: 1066 loss: 3.66876407e-07
Iter: 1067 loss: 3.66802169e-07
Iter: 1068 loss: 3.66799156e-07
Iter: 1069 loss: 3.66754904e-07
Iter: 1070 loss: 3.66759764e-07
Iter: 1071 loss: 3.66690244e-07
Iter: 1072 loss: 3.66592644e-07
Iter: 1073 loss: 3.68907195e-07
Iter: 1074 loss: 3.66592189e-07
Iter: 1075 loss: 3.66459886e-07
Iter: 1076 loss: 3.667827e-07
Iter: 1077 loss: 3.66421943e-07
Iter: 1078 loss: 3.66303766e-07
Iter: 1079 loss: 3.67366908e-07
Iter: 1080 loss: 3.66303482e-07
Iter: 1081 loss: 3.66234303e-07
Iter: 1082 loss: 3.66424217e-07
Iter: 1083 loss: 3.66215659e-07
Iter: 1084 loss: 3.66157366e-07
Iter: 1085 loss: 3.66287395e-07
Iter: 1086 loss: 3.66134287e-07
Iter: 1087 loss: 3.66075568e-07
Iter: 1088 loss: 3.66106434e-07
Iter: 1089 loss: 3.66038194e-07
Iter: 1090 loss: 3.65951e-07
Iter: 1091 loss: 3.66175442e-07
Iter: 1092 loss: 3.6590427e-07
Iter: 1093 loss: 3.65831141e-07
Iter: 1094 loss: 3.65737264e-07
Iter: 1095 loss: 3.65727374e-07
Iter: 1096 loss: 3.65585777e-07
Iter: 1097 loss: 3.66393181e-07
Iter: 1098 loss: 3.65560311e-07
Iter: 1099 loss: 3.65426331e-07
Iter: 1100 loss: 3.66776931e-07
Iter: 1101 loss: 3.65410983e-07
Iter: 1102 loss: 3.65361927e-07
Iter: 1103 loss: 3.65364485e-07
Iter: 1104 loss: 3.65329214e-07
Iter: 1105 loss: 3.65247161e-07
Iter: 1106 loss: 3.66037455e-07
Iter: 1107 loss: 3.65245853e-07
Iter: 1108 loss: 3.65212571e-07
Iter: 1109 loss: 3.6516542e-07
Iter: 1110 loss: 3.65164425e-07
Iter: 1111 loss: 3.6508689e-07
Iter: 1112 loss: 3.65605615e-07
Iter: 1113 loss: 3.65079131e-07
Iter: 1114 loss: 3.65021037e-07
Iter: 1115 loss: 3.65136572e-07
Iter: 1116 loss: 3.6499523e-07
Iter: 1117 loss: 3.64918805e-07
Iter: 1118 loss: 3.65234371e-07
Iter: 1119 loss: 3.64905361e-07
Iter: 1120 loss: 3.64848574e-07
Iter: 1121 loss: 3.64797415e-07
Iter: 1122 loss: 3.64786104e-07
Iter: 1123 loss: 3.6469649e-07
Iter: 1124 loss: 3.65755426e-07
Iter: 1125 loss: 3.64703453e-07
Iter: 1126 loss: 3.6464931e-07
Iter: 1127 loss: 3.64610912e-07
Iter: 1128 loss: 3.64598662e-07
Iter: 1129 loss: 3.64530024e-07
Iter: 1130 loss: 3.64508224e-07
Iter: 1131 loss: 3.64465706e-07
Iter: 1132 loss: 3.64408322e-07
Iter: 1133 loss: 3.64399966e-07
Iter: 1134 loss: 3.64331868e-07
Iter: 1135 loss: 3.64249104e-07
Iter: 1136 loss: 3.64235802e-07
Iter: 1137 loss: 3.64168727e-07
Iter: 1138 loss: 3.64795369e-07
Iter: 1139 loss: 3.64155596e-07
Iter: 1140 loss: 3.64065556e-07
Iter: 1141 loss: 3.64158893e-07
Iter: 1142 loss: 3.64017694e-07
Iter: 1143 loss: 3.63964148e-07
Iter: 1144 loss: 3.64166311e-07
Iter: 1145 loss: 3.63946242e-07
Iter: 1146 loss: 3.63884e-07
Iter: 1147 loss: 3.64036623e-07
Iter: 1148 loss: 3.63852621e-07
Iter: 1149 loss: 3.63805327e-07
Iter: 1150 loss: 3.64148548e-07
Iter: 1151 loss: 3.63799359e-07
Iter: 1152 loss: 3.63749251e-07
Iter: 1153 loss: 3.63768834e-07
Iter: 1154 loss: 3.63712985e-07
Iter: 1155 loss: 3.63645597e-07
Iter: 1156 loss: 3.63547798e-07
Iter: 1157 loss: 3.63551294e-07
Iter: 1158 loss: 3.63450084e-07
Iter: 1159 loss: 3.63446247e-07
Iter: 1160 loss: 3.63379087e-07
Iter: 1161 loss: 3.63270516e-07
Iter: 1162 loss: 3.6327674e-07
Iter: 1163 loss: 3.631921e-07
Iter: 1164 loss: 3.63996548e-07
Iter: 1165 loss: 3.63194033e-07
Iter: 1166 loss: 3.63115021e-07
Iter: 1167 loss: 3.63516676e-07
Iter: 1168 loss: 3.63101179e-07
Iter: 1169 loss: 3.63052578e-07
Iter: 1170 loss: 3.63002641e-07
Iter: 1171 loss: 3.63004773e-07
Iter: 1172 loss: 3.62962453e-07
Iter: 1173 loss: 3.62956854e-07
Iter: 1174 loss: 3.6291479e-07
Iter: 1175 loss: 3.6283808e-07
Iter: 1176 loss: 3.64079085e-07
Iter: 1177 loss: 3.62825233e-07
Iter: 1178 loss: 3.62748352e-07
Iter: 1179 loss: 3.62754207e-07
Iter: 1180 loss: 3.62692163e-07
Iter: 1181 loss: 3.62653083e-07
Iter: 1182 loss: 3.6263296e-07
Iter: 1183 loss: 3.62568016e-07
Iter: 1184 loss: 3.62568528e-07
Iter: 1185 loss: 3.6252527e-07
Iter: 1186 loss: 3.62481558e-07
Iter: 1187 loss: 3.62472e-07
Iter: 1188 loss: 3.62412834e-07
Iter: 1189 loss: 3.62390409e-07
Iter: 1190 loss: 3.62351329e-07
Iter: 1191 loss: 3.62253388e-07
Iter: 1192 loss: 3.62252251e-07
Iter: 1193 loss: 3.62181197e-07
Iter: 1194 loss: 3.62126798e-07
Iter: 1195 loss: 3.62113525e-07
Iter: 1196 loss: 3.62033859e-07
Iter: 1197 loss: 3.63195852e-07
Iter: 1198 loss: 3.62023741e-07
Iter: 1199 loss: 3.61963913e-07
Iter: 1200 loss: 3.61945e-07
Iter: 1201 loss: 3.61909969e-07
Iter: 1202 loss: 3.61864863e-07
Iter: 1203 loss: 3.62487611e-07
Iter: 1204 loss: 3.61859463e-07
Iter: 1205 loss: 3.61801256e-07
Iter: 1206 loss: 3.61764535e-07
Iter: 1207 loss: 3.61743815e-07
Iter: 1208 loss: 3.61691519e-07
Iter: 1209 loss: 3.62044943e-07
Iter: 1210 loss: 3.61684329e-07
Iter: 1211 loss: 3.61629418e-07
Iter: 1212 loss: 3.61809839e-07
Iter: 1213 loss: 3.61606965e-07
Iter: 1214 loss: 3.61564247e-07
Iter: 1215 loss: 3.61738131e-07
Iter: 1216 loss: 3.61559273e-07
Iter: 1217 loss: 3.61488389e-07
Iter: 1218 loss: 3.61464174e-07
Iter: 1219 loss: 3.61431375e-07
Iter: 1220 loss: 3.61360208e-07
Iter: 1221 loss: 3.61436065e-07
Iter: 1222 loss: 3.61328944e-07
Iter: 1223 loss: 3.61277557e-07
Iter: 1224 loss: 3.61298703e-07
Iter: 1225 loss: 3.61226313e-07
Iter: 1226 loss: 3.61187801e-07
Iter: 1227 loss: 3.61176e-07
Iter: 1228 loss: 3.61152871e-07
Iter: 1229 loss: 3.61114985e-07
Iter: 1230 loss: 3.61109386e-07
Iter: 1231 loss: 3.61028924e-07
Iter: 1232 loss: 3.61312487e-07
Iter: 1233 loss: 3.61010791e-07
Iter: 1234 loss: 3.60963156e-07
Iter: 1235 loss: 3.61021705e-07
Iter: 1236 loss: 3.60925839e-07
Iter: 1237 loss: 3.60860042e-07
Iter: 1238 loss: 3.61285174e-07
Iter: 1239 loss: 3.60853562e-07
Iter: 1240 loss: 3.60806268e-07
Iter: 1241 loss: 3.60676722e-07
Iter: 1242 loss: 3.62707198e-07
Iter: 1243 loss: 3.60680161e-07
Iter: 1244 loss: 3.60652507e-07
Iter: 1245 loss: 3.60619595e-07
Iter: 1246 loss: 3.60577303e-07
Iter: 1247 loss: 3.60540611e-07
Iter: 1248 loss: 3.60528503e-07
Iter: 1249 loss: 3.60465606e-07
Iter: 1250 loss: 3.61175182e-07
Iter: 1251 loss: 3.60461627e-07
Iter: 1252 loss: 3.60418341e-07
Iter: 1253 loss: 3.60364766e-07
Iter: 1254 loss: 3.6036505e-07
Iter: 1255 loss: 3.60275322e-07
Iter: 1256 loss: 3.60331114e-07
Iter: 1257 loss: 3.6022476e-07
Iter: 1258 loss: 3.60146146e-07
Iter: 1259 loss: 3.60503805e-07
Iter: 1260 loss: 3.6012591e-07
Iter: 1261 loss: 3.60027968e-07
Iter: 1262 loss: 3.606566e-07
Iter: 1263 loss: 3.60018504e-07
Iter: 1264 loss: 3.59973512e-07
Iter: 1265 loss: 3.60090723e-07
Iter: 1266 loss: 3.59954129e-07
Iter: 1267 loss: 3.59912121e-07
Iter: 1268 loss: 3.5994745e-07
Iter: 1269 loss: 3.5988333e-07
Iter: 1270 loss: 3.59831e-07
Iter: 1271 loss: 3.60216376e-07
Iter: 1272 loss: 3.59825322e-07
Iter: 1273 loss: 3.59773566e-07
Iter: 1274 loss: 3.59704273e-07
Iter: 1275 loss: 3.59692592e-07
Iter: 1276 loss: 3.5963626e-07
Iter: 1277 loss: 3.60448269e-07
Iter: 1278 loss: 3.59639188e-07
Iter: 1279 loss: 3.5957504e-07
Iter: 1280 loss: 3.5952209e-07
Iter: 1281 loss: 3.59518e-07
Iter: 1282 loss: 3.59459392e-07
Iter: 1283 loss: 3.59457459e-07
Iter: 1284 loss: 3.5941855e-07
Iter: 1285 loss: 3.59341016e-07
Iter: 1286 loss: 3.60743059e-07
Iter: 1287 loss: 3.59344256e-07
Iter: 1288 loss: 3.59265641e-07
Iter: 1289 loss: 3.59441316e-07
Iter: 1290 loss: 3.59225538e-07
Iter: 1291 loss: 3.59154967e-07
Iter: 1292 loss: 3.59060721e-07
Iter: 1293 loss: 3.59057424e-07
Iter: 1294 loss: 3.58994782e-07
Iter: 1295 loss: 3.58981e-07
Iter: 1296 loss: 3.58917305e-07
Iter: 1297 loss: 3.58817601e-07
Iter: 1298 loss: 3.5882249e-07
Iter: 1299 loss: 3.58718466e-07
Iter: 1300 loss: 3.59759412e-07
Iter: 1301 loss: 3.58708348e-07
Iter: 1302 loss: 3.58658667e-07
Iter: 1303 loss: 3.59355568e-07
Iter: 1304 loss: 3.5865105e-07
Iter: 1305 loss: 3.58601397e-07
Iter: 1306 loss: 3.58566552e-07
Iter: 1307 loss: 3.58548107e-07
Iter: 1308 loss: 3.58494276e-07
Iter: 1309 loss: 3.58786451e-07
Iter: 1310 loss: 3.58489103e-07
Iter: 1311 loss: 3.58414837e-07
Iter: 1312 loss: 3.58501921e-07
Iter: 1313 loss: 3.58385449e-07
Iter: 1314 loss: 3.58333523e-07
Iter: 1315 loss: 3.58782955e-07
Iter: 1316 loss: 3.58338298e-07
Iter: 1317 loss: 3.58286229e-07
Iter: 1318 loss: 3.58209149e-07
Iter: 1319 loss: 3.58214749e-07
Iter: 1320 loss: 3.58124652e-07
Iter: 1321 loss: 3.58709514e-07
Iter: 1322 loss: 3.58122065e-07
Iter: 1323 loss: 3.58072043e-07
Iter: 1324 loss: 3.58080371e-07
Iter: 1325 loss: 3.58028785e-07
Iter: 1326 loss: 3.57996043e-07
Iter: 1327 loss: 3.5799394e-07
Iter: 1328 loss: 3.57957731e-07
Iter: 1329 loss: 3.57964723e-07
Iter: 1330 loss: 3.5793164e-07
Iter: 1331 loss: 3.57900205e-07
Iter: 1332 loss: 3.57944373e-07
Iter: 1333 loss: 3.57874512e-07
Iter: 1334 loss: 3.57821648e-07
Iter: 1335 loss: 3.57928883e-07
Iter: 1336 loss: 3.57806073e-07
Iter: 1337 loss: 3.57746e-07
Iter: 1338 loss: 3.57883124e-07
Iter: 1339 loss: 3.57723934e-07
Iter: 1340 loss: 3.57679198e-07
Iter: 1341 loss: 3.57679e-07
Iter: 1342 loss: 3.57643017e-07
Iter: 1343 loss: 3.57557383e-07
Iter: 1344 loss: 3.57903701e-07
Iter: 1345 loss: 3.57536578e-07
Iter: 1346 loss: 3.57494713e-07
Iter: 1347 loss: 3.57736212e-07
Iter: 1348 loss: 3.57489171e-07
Iter: 1349 loss: 3.57434544e-07
Iter: 1350 loss: 3.57509293e-07
Iter: 1351 loss: 3.57411437e-07
Iter: 1352 loss: 3.57373608e-07
Iter: 1353 loss: 3.57301332e-07
Iter: 1354 loss: 3.57306476e-07
Iter: 1355 loss: 3.57213082e-07
Iter: 1356 loss: 3.57989649e-07
Iter: 1357 loss: 3.5720808e-07
Iter: 1358 loss: 3.57159365e-07
Iter: 1359 loss: 3.5715837e-07
Iter: 1360 loss: 3.57126851e-07
Iter: 1361 loss: 3.5709553e-07
Iter: 1362 loss: 3.57085753e-07
Iter: 1363 loss: 3.57031752e-07
Iter: 1364 loss: 3.57227322e-07
Iter: 1365 loss: 3.57017655e-07
Iter: 1366 loss: 3.56967803e-07
Iter: 1367 loss: 3.57099452e-07
Iter: 1368 loss: 3.56957344e-07
Iter: 1369 loss: 3.56900443e-07
Iter: 1370 loss: 3.57134127e-07
Iter: 1371 loss: 3.56885948e-07
Iter: 1372 loss: 3.56843145e-07
Iter: 1373 loss: 3.56800513e-07
Iter: 1374 loss: 3.56800967e-07
Iter: 1375 loss: 3.56710643e-07
Iter: 1376 loss: 3.5749548e-07
Iter: 1377 loss: 3.56708455e-07
Iter: 1378 loss: 3.56660792e-07
Iter: 1379 loss: 3.56741339e-07
Iter: 1380 loss: 3.56644819e-07
Iter: 1381 loss: 3.56596e-07
Iter: 1382 loss: 3.56752594e-07
Iter: 1383 loss: 3.56576e-07
Iter: 1384 loss: 3.56534883e-07
Iter: 1385 loss: 3.56483127e-07
Iter: 1386 loss: 3.56483838e-07
Iter: 1387 loss: 3.56426938e-07
Iter: 1388 loss: 3.56840928e-07
Iter: 1389 loss: 3.56420571e-07
Iter: 1390 loss: 3.56377797e-07
Iter: 1391 loss: 3.56468661e-07
Iter: 1392 loss: 3.56362392e-07
Iter: 1393 loss: 3.56307737e-07
Iter: 1394 loss: 3.56540909e-07
Iter: 1395 loss: 3.56288723e-07
Iter: 1396 loss: 3.5624933e-07
Iter: 1397 loss: 3.56270789e-07
Iter: 1398 loss: 3.56214116e-07
Iter: 1399 loss: 3.561548e-07
Iter: 1400 loss: 3.56303133e-07
Iter: 1401 loss: 3.56142095e-07
Iter: 1402 loss: 3.5608042e-07
Iter: 1403 loss: 3.56396527e-07
Iter: 1404 loss: 3.56070473e-07
Iter: 1405 loss: 3.56026078e-07
Iter: 1406 loss: 3.56024771e-07
Iter: 1407 loss: 3.55992427e-07
Iter: 1408 loss: 3.55939108e-07
Iter: 1409 loss: 3.56560236e-07
Iter: 1410 loss: 3.55942916e-07
Iter: 1411 loss: 3.55917848e-07
Iter: 1412 loss: 3.55899488e-07
Iter: 1413 loss: 3.55880303e-07
Iter: 1414 loss: 3.55828888e-07
Iter: 1415 loss: 3.56079624e-07
Iter: 1416 loss: 3.55818372e-07
Iter: 1417 loss: 3.55781282e-07
Iter: 1418 loss: 3.55715855e-07
Iter: 1419 loss: 3.57213565e-07
Iter: 1420 loss: 3.55712842e-07
Iter: 1421 loss: 3.55658045e-07
Iter: 1422 loss: 3.56446833e-07
Iter: 1423 loss: 3.55655516e-07
Iter: 1424 loss: 3.55618823e-07
Iter: 1425 loss: 3.55676491e-07
Iter: 1426 loss: 3.55600889e-07
Iter: 1427 loss: 3.55560076e-07
Iter: 1428 loss: 3.55857765e-07
Iter: 1429 loss: 3.5556269e-07
Iter: 1430 loss: 3.55531e-07
Iter: 1431 loss: 3.55513436e-07
Iter: 1432 loss: 3.55504312e-07
Iter: 1433 loss: 3.55448435e-07
Iter: 1434 loss: 3.55598843e-07
Iter: 1435 loss: 3.55437749e-07
Iter: 1436 loss: 3.55381161e-07
Iter: 1437 loss: 3.562551e-07
Iter: 1438 loss: 3.55383065e-07
Iter: 1439 loss: 3.55344753e-07
Iter: 1440 loss: 3.55330371e-07
Iter: 1441 loss: 3.55319088e-07
Iter: 1442 loss: 3.5528538e-07
Iter: 1443 loss: 3.55283504e-07
Iter: 1444 loss: 3.55268043e-07
Iter: 1445 loss: 3.5523658e-07
Iter: 1446 loss: 3.55227542e-07
Iter: 1447 loss: 3.55196846e-07
Iter: 1448 loss: 3.55536599e-07
Iter: 1449 loss: 3.5519642e-07
Iter: 1450 loss: 3.55174592e-07
Iter: 1451 loss: 3.55131249e-07
Iter: 1452 loss: 3.55992199e-07
Iter: 1453 loss: 3.55133551e-07
Iter: 1454 loss: 3.55080886e-07
Iter: 1455 loss: 3.55589464e-07
Iter: 1456 loss: 3.55071137e-07
Iter: 1457 loss: 3.55038623e-07
Iter: 1458 loss: 3.55226604e-07
Iter: 1459 loss: 3.5503956e-07
Iter: 1460 loss: 3.54996359e-07
Iter: 1461 loss: 3.55110302e-07
Iter: 1462 loss: 3.54974048e-07
Iter: 1463 loss: 3.54932126e-07
Iter: 1464 loss: 3.54946906e-07
Iter: 1465 loss: 3.54900862e-07
Iter: 1466 loss: 3.54846406e-07
Iter: 1467 loss: 3.54890147e-07
Iter: 1468 loss: 3.54818e-07
Iter: 1469 loss: 3.54764097e-07
Iter: 1470 loss: 3.54760516e-07
Iter: 1471 loss: 3.54728741e-07
Iter: 1472 loss: 3.54670789e-07
Iter: 1473 loss: 3.54675791e-07
Iter: 1474 loss: 3.54638047e-07
Iter: 1475 loss: 3.54638189e-07
Iter: 1476 loss: 3.54600559e-07
Iter: 1477 loss: 3.54529618e-07
Iter: 1478 loss: 3.54539111e-07
Iter: 1479 loss: 3.54473087e-07
Iter: 1480 loss: 3.55250847e-07
Iter: 1481 loss: 3.54473684e-07
Iter: 1482 loss: 3.54456631e-07
Iter: 1483 loss: 3.54402516e-07
Iter: 1484 loss: 3.54391773e-07
Iter: 1485 loss: 3.54339e-07
Iter: 1486 loss: 3.54353858e-07
Iter: 1487 loss: 3.54306735e-07
Iter: 1488 loss: 3.54239035e-07
Iter: 1489 loss: 3.5407146e-07
Iter: 1490 loss: 3.54963817e-07
Iter: 1491 loss: 3.54008108e-07
Iter: 1492 loss: 3.53791904e-07
Iter: 1493 loss: 3.54214308e-07
Iter: 1494 loss: 3.53709879e-07
Iter: 1495 loss: 3.53671851e-07
Iter: 1496 loss: 3.53642633e-07
Iter: 1497 loss: 3.53618333e-07
Iter: 1498 loss: 3.53651444e-07
Iter: 1499 loss: 3.53607618e-07
Iter: 1500 loss: 3.53570726e-07
Iter: 1501 loss: 3.53575558e-07
Iter: 1502 loss: 3.53549e-07
Iter: 1503 loss: 3.53498251e-07
Iter: 1504 loss: 3.53760527e-07
Iter: 1505 loss: 3.53492197e-07
Iter: 1506 loss: 3.53442204e-07
Iter: 1507 loss: 3.53460507e-07
Iter: 1508 loss: 3.53422877e-07
Iter: 1509 loss: 3.53349606e-07
Iter: 1510 loss: 3.53716814e-07
Iter: 1511 loss: 3.5334e-07
Iter: 1512 loss: 3.53306206e-07
Iter: 1513 loss: 3.53419665e-07
Iter: 1514 loss: 3.53300152e-07
Iter: 1515 loss: 3.53267126e-07
Iter: 1516 loss: 3.53292421e-07
Iter: 1517 loss: 3.53244559e-07
Iter: 1518 loss: 3.53204598e-07
Iter: 1519 loss: 3.5328182e-07
Iter: 1520 loss: 3.5319e-07
Iter: 1521 loss: 3.5314315e-07
Iter: 1522 loss: 3.53117883e-07
Iter: 1523 loss: 3.5308085e-07
Iter: 1524 loss: 3.53035119e-07
Iter: 1525 loss: 3.52935587e-07
Iter: 1526 loss: 3.54878182e-07
Iter: 1527 loss: 3.52949286e-07
Iter: 1528 loss: 3.52805955e-07
Iter: 1529 loss: 3.53110863e-07
Iter: 1530 loss: 3.52758235e-07
Iter: 1531 loss: 3.52660493e-07
Iter: 1532 loss: 3.52662397e-07
Iter: 1533 loss: 3.52602967e-07
Iter: 1534 loss: 3.52664529e-07
Iter: 1535 loss: 3.52578269e-07
Iter: 1536 loss: 3.52523273e-07
Iter: 1537 loss: 3.53195162e-07
Iter: 1538 loss: 3.52522108e-07
Iter: 1539 loss: 3.52493402e-07
Iter: 1540 loss: 3.52458926e-07
Iter: 1541 loss: 3.52438462e-07
Iter: 1542 loss: 3.52397279e-07
Iter: 1543 loss: 3.5303168e-07
Iter: 1544 loss: 3.52401344e-07
Iter: 1545 loss: 3.52361553e-07
Iter: 1546 loss: 3.52358228e-07
Iter: 1547 loss: 3.52335917e-07
Iter: 1548 loss: 3.52290385e-07
Iter: 1549 loss: 3.52744564e-07
Iter: 1550 loss: 3.52284e-07
Iter: 1551 loss: 3.52236157e-07
Iter: 1552 loss: 3.52136283e-07
Iter: 1553 loss: 3.53224607e-07
Iter: 1554 loss: 3.52122271e-07
Iter: 1555 loss: 3.5210445e-07
Iter: 1556 loss: 3.52093963e-07
Iter: 1557 loss: 3.52044282e-07
Iter: 1558 loss: 3.52063296e-07
Iter: 1559 loss: 3.52018077e-07
Iter: 1560 loss: 3.51973881e-07
Iter: 1561 loss: 3.5191286e-07
Iter: 1562 loss: 3.52942209e-07
Iter: 1563 loss: 3.51906522e-07
Iter: 1564 loss: 3.51803777e-07
Iter: 1565 loss: 3.51701658e-07
Iter: 1566 loss: 3.51676931e-07
Iter: 1567 loss: 3.5157575e-07
Iter: 1568 loss: 3.52835826e-07
Iter: 1569 loss: 3.51573021e-07
Iter: 1570 loss: 3.51478036e-07
Iter: 1571 loss: 3.52383296e-07
Iter: 1572 loss: 3.51482981e-07
Iter: 1573 loss: 3.51438075e-07
Iter: 1574 loss: 3.51503189e-07
Iter: 1575 loss: 3.51412467e-07
Iter: 1576 loss: 3.51369039e-07
Iter: 1577 loss: 3.51383846e-07
Iter: 1578 loss: 3.51339821e-07
Iter: 1579 loss: 3.5130563e-07
Iter: 1580 loss: 3.51753869e-07
Iter: 1581 loss: 3.51297e-07
Iter: 1582 loss: 3.51255807e-07
Iter: 1583 loss: 3.51178784e-07
Iter: 1584 loss: 3.53044385e-07
Iter: 1585 loss: 3.51179352e-07
Iter: 1586 loss: 3.51097e-07
Iter: 1587 loss: 3.51103267e-07
Iter: 1588 loss: 3.51044406e-07
Iter: 1589 loss: 3.51066547e-07
Iter: 1590 loss: 3.51003109e-07
Iter: 1591 loss: 3.50961813e-07
Iter: 1592 loss: 3.5093521e-07
Iter: 1593 loss: 3.50916537e-07
Iter: 1594 loss: 3.50907726e-07
Iter: 1595 loss: 3.50895704e-07
Iter: 1596 loss: 3.50863786e-07
Iter: 1597 loss: 3.50851394e-07
Iter: 1598 loss: 3.5083869e-07
Iter: 1599 loss: 3.50791908e-07
Iter: 1600 loss: 3.50789577e-07
Iter: 1601 loss: 3.507607e-07
Iter: 1602 loss: 3.506957e-07
Iter: 1603 loss: 3.51015387e-07
Iter: 1604 loss: 3.50689845e-07
Iter: 1605 loss: 3.50628909e-07
Iter: 1606 loss: 3.50988273e-07
Iter: 1607 loss: 3.50614982e-07
Iter: 1608 loss: 3.50569394e-07
Iter: 1609 loss: 3.50496748e-07
Iter: 1610 loss: 3.52159077e-07
Iter: 1611 loss: 3.50493764e-07
Iter: 1612 loss: 3.50429076e-07
Iter: 1613 loss: 3.50555069e-07
Iter: 1614 loss: 3.50390678e-07
Iter: 1615 loss: 3.5032815e-07
Iter: 1616 loss: 3.50329e-07
Iter: 1617 loss: 3.50270653e-07
Iter: 1618 loss: 3.50340883e-07
Iter: 1619 loss: 3.50247603e-07
Iter: 1620 loss: 3.5017564e-07
Iter: 1621 loss: 3.50109417e-07
Iter: 1622 loss: 3.50082303e-07
Iter: 1623 loss: 3.50008349e-07
Iter: 1624 loss: 3.51050431e-07
Iter: 1625 loss: 3.50008975e-07
Iter: 1626 loss: 3.49929962e-07
Iter: 1627 loss: 3.50005848e-07
Iter: 1628 loss: 3.49886363e-07
Iter: 1629 loss: 3.49843276e-07
Iter: 1630 loss: 3.49931497e-07
Iter: 1631 loss: 3.49828383e-07
Iter: 1632 loss: 3.49784557e-07
Iter: 1633 loss: 3.50104528e-07
Iter: 1634 loss: 3.49775064e-07
Iter: 1635 loss: 3.49753122e-07
Iter: 1636 loss: 3.49695227e-07
Iter: 1637 loss: 3.5056e-07
Iter: 1638 loss: 3.49695881e-07
Iter: 1639 loss: 3.49653192e-07
Iter: 1640 loss: 3.50217249e-07
Iter: 1641 loss: 3.49651117e-07
Iter: 1642 loss: 3.49577221e-07
Iter: 1643 loss: 3.49468451e-07
Iter: 1644 loss: 3.49472685e-07
Iter: 1645 loss: 3.49386369e-07
Iter: 1646 loss: 3.49295192e-07
Iter: 1647 loss: 3.49266656e-07
Iter: 1648 loss: 3.49185655e-07
Iter: 1649 loss: 3.4952896e-07
Iter: 1650 loss: 3.49170136e-07
Iter: 1651 loss: 3.4908939e-07
Iter: 1652 loss: 3.49441535e-07
Iter: 1653 loss: 3.49081063e-07
Iter: 1654 loss: 3.49038146e-07
Iter: 1655 loss: 3.48977608e-07
Iter: 1656 loss: 3.48975277e-07
Iter: 1657 loss: 3.48911954e-07
Iter: 1658 loss: 3.49154e-07
Iter: 1659 loss: 3.48893821e-07
Iter: 1660 loss: 3.48842377e-07
Iter: 1661 loss: 3.48839876e-07
Iter: 1662 loss: 3.48801507e-07
Iter: 1663 loss: 3.48863665e-07
Iter: 1664 loss: 3.48795197e-07
Iter: 1665 loss: 3.48763621e-07
Iter: 1666 loss: 3.48772232e-07
Iter: 1667 loss: 3.48727326e-07
Iter: 1668 loss: 3.48697142e-07
Iter: 1669 loss: 3.48696148e-07
Iter: 1670 loss: 3.48684864e-07
Iter: 1671 loss: 3.48633648e-07
Iter: 1672 loss: 3.49122615e-07
Iter: 1673 loss: 3.4862569e-07
Iter: 1674 loss: 3.48610172e-07
Iter: 1675 loss: 3.48608239e-07
Iter: 1676 loss: 3.4857726e-07
Iter: 1677 loss: 3.48520928e-07
Iter: 1678 loss: 3.49195886e-07
Iter: 1679 loss: 3.4851746e-07
Iter: 1680 loss: 3.48472213e-07
Iter: 1681 loss: 3.48958906e-07
Iter: 1682 loss: 3.48463971e-07
Iter: 1683 loss: 3.48423612e-07
Iter: 1684 loss: 3.48457633e-07
Iter: 1685 loss: 3.48393939e-07
Iter: 1686 loss: 3.48365234e-07
Iter: 1687 loss: 3.48396725e-07
Iter: 1688 loss: 3.48339881e-07
Iter: 1689 loss: 3.48305235e-07
Iter: 1690 loss: 3.48287102e-07
Iter: 1691 loss: 3.48283379e-07
Iter: 1692 loss: 3.48245777e-07
Iter: 1693 loss: 3.48429353e-07
Iter: 1694 loss: 3.48244839e-07
Iter: 1695 loss: 3.48204594e-07
Iter: 1696 loss: 3.48291138e-07
Iter: 1697 loss: 3.48192572e-07
Iter: 1698 loss: 3.481519e-07
Iter: 1699 loss: 3.48288722e-07
Iter: 1700 loss: 3.48141896e-07
Iter: 1701 loss: 3.48119329e-07
Iter: 1702 loss: 3.48070444e-07
Iter: 1703 loss: 3.48069818e-07
Iter: 1704 loss: 3.48034462e-07
Iter: 1705 loss: 3.48037304e-07
Iter: 1706 loss: 3.4801559e-07
Iter: 1707 loss: 3.4799416e-07
Iter: 1708 loss: 3.47988788e-07
Iter: 1709 loss: 3.47970342e-07
Iter: 1710 loss: 3.47974265e-07
Iter: 1711 loss: 3.47950873e-07
Iter: 1712 loss: 3.47906024e-07
Iter: 1713 loss: 3.48270504e-07
Iter: 1714 loss: 3.47898379e-07
Iter: 1715 loss: 3.47874675e-07
Iter: 1716 loss: 3.47884168e-07
Iter: 1717 loss: 3.47842445e-07
Iter: 1718 loss: 3.4782073e-07
Iter: 1719 loss: 3.47816922e-07
Iter: 1720 loss: 3.47770197e-07
Iter: 1721 loss: 3.47860066e-07
Iter: 1722 loss: 3.47766e-07
Iter: 1723 loss: 3.47720515e-07
Iter: 1724 loss: 3.47673193e-07
Iter: 1725 loss: 3.48808896e-07
Iter: 1726 loss: 3.47670721e-07
Iter: 1727 loss: 3.47611774e-07
Iter: 1728 loss: 3.47805269e-07
Iter: 1729 loss: 3.47583466e-07
Iter: 1730 loss: 3.47560331e-07
Iter: 1731 loss: 3.47563486e-07
Iter: 1732 loss: 3.47531568e-07
Iter: 1733 loss: 3.47475208e-07
Iter: 1734 loss: 3.48493728e-07
Iter: 1735 loss: 3.47477965e-07
Iter: 1736 loss: 3.47465289e-07
Iter: 1737 loss: 3.47446644e-07
Iter: 1738 loss: 3.4742331e-07
Iter: 1739 loss: 3.47408758e-07
Iter: 1740 loss: 3.47413021e-07
Iter: 1741 loss: 3.47381729e-07
Iter: 1742 loss: 3.47415892e-07
Iter: 1743 loss: 3.4736567e-07
Iter: 1744 loss: 3.47320565e-07
Iter: 1745 loss: 3.47554817e-07
Iter: 1746 loss: 3.47316188e-07
Iter: 1747 loss: 3.47297174e-07
Iter: 1748 loss: 3.47240274e-07
Iter: 1749 loss: 3.47736119e-07
Iter: 1750 loss: 3.47233652e-07
Iter: 1751 loss: 3.47164246e-07
Iter: 1752 loss: 3.47617345e-07
Iter: 1753 loss: 3.47160665e-07
Iter: 1754 loss: 3.47114764e-07
Iter: 1755 loss: 3.47104276e-07
Iter: 1756 loss: 3.47088701e-07
Iter: 1757 loss: 3.47037684e-07
Iter: 1758 loss: 3.47424958e-07
Iter: 1759 loss: 3.47021285e-07
Iter: 1760 loss: 3.46983484e-07
Iter: 1761 loss: 3.47268553e-07
Iter: 1762 loss: 3.46987889e-07
Iter: 1763 loss: 3.46942727e-07
Iter: 1764 loss: 3.47471598e-07
Iter: 1765 loss: 3.46941135e-07
Iter: 1766 loss: 3.46910667e-07
Iter: 1767 loss: 3.46958984e-07
Iter: 1768 loss: 3.46898133e-07
Iter: 1769 loss: 3.46858968e-07
Iter: 1770 loss: 3.46998945e-07
Iter: 1771 loss: 3.46858769e-07
Iter: 1772 loss: 3.46825573e-07
Iter: 1773 loss: 3.46832422e-07
Iter: 1774 loss: 3.46805621e-07
Iter: 1775 loss: 3.46762022e-07
Iter: 1776 loss: 3.46863544e-07
Iter: 1777 loss: 3.46747299e-07
Iter: 1778 loss: 3.46706e-07
Iter: 1779 loss: 3.47002214e-07
Iter: 1780 loss: 3.4669506e-07
Iter: 1781 loss: 3.46657714e-07
Iter: 1782 loss: 3.46615082e-07
Iter: 1783 loss: 3.46617071e-07
Iter: 1784 loss: 3.46561791e-07
Iter: 1785 loss: 3.46506511e-07
Iter: 1786 loss: 3.46488321e-07
Iter: 1787 loss: 3.46463253e-07
Iter: 1788 loss: 3.46450918e-07
Iter: 1789 loss: 3.4639362e-07
Iter: 1790 loss: 3.46373611e-07
Iter: 1791 loss: 3.46341295e-07
Iter: 1792 loss: 3.46294541e-07
Iter: 1793 loss: 3.46267939e-07
Iter: 1794 loss: 3.46253927e-07
Iter: 1795 loss: 3.46218172e-07
Iter: 1796 loss: 3.46220787e-07
Iter: 1797 loss: 3.46188955e-07
Iter: 1798 loss: 3.4633689e-07
Iter: 1799 loss: 3.46188756e-07
Iter: 1800 loss: 3.46159453e-07
Iter: 1801 loss: 3.46219906e-07
Iter: 1802 loss: 3.46158771e-07
Iter: 1803 loss: 3.46139188e-07
Iter: 1804 loss: 3.46148397e-07
Iter: 1805 loss: 3.46109857e-07
Iter: 1806 loss: 3.46079e-07
Iter: 1807 loss: 3.46126541e-07
Iter: 1808 loss: 3.4607649e-07
Iter: 1809 loss: 3.46030333e-07
Iter: 1810 loss: 3.46183299e-07
Iter: 1811 loss: 3.46028912e-07
Iter: 1812 loss: 3.45980055e-07
Iter: 1813 loss: 3.45984489e-07
Iter: 1814 loss: 3.45952571e-07
Iter: 1815 loss: 3.4590164e-07
Iter: 1816 loss: 3.45931682e-07
Iter: 1817 loss: 3.45869e-07
Iter: 1818 loss: 3.45826663e-07
Iter: 1819 loss: 3.45901071e-07
Iter: 1820 loss: 3.45811173e-07
Iter: 1821 loss: 3.45799265e-07
Iter: 1822 loss: 3.45794803e-07
Iter: 1823 loss: 3.45779256e-07
Iter: 1824 loss: 3.4573921e-07
Iter: 1825 loss: 3.45976673e-07
Iter: 1826 loss: 3.45720082e-07
Iter: 1827 loss: 3.45684583e-07
Iter: 1828 loss: 3.45701721e-07
Iter: 1829 loss: 3.45668354e-07
Iter: 1830 loss: 3.45629303e-07
Iter: 1831 loss: 3.45628791e-07
Iter: 1832 loss: 3.45601734e-07
Iter: 1833 loss: 3.4571957e-07
Iter: 1834 loss: 3.45593918e-07
Iter: 1835 loss: 3.45568424e-07
Iter: 1836 loss: 3.45622368e-07
Iter: 1837 loss: 3.45569788e-07
Iter: 1838 loss: 3.45561944e-07
Iter: 1839 loss: 3.45536108e-07
Iter: 1840 loss: 3.45532555e-07
Iter: 1841 loss: 3.45496773e-07
Iter: 1842 loss: 3.45643855e-07
Iter: 1843 loss: 3.45487138e-07
Iter: 1844 loss: 3.45446921e-07
Iter: 1845 loss: 3.45856961e-07
Iter: 1846 loss: 3.45453316e-07
Iter: 1847 loss: 3.45434955e-07
Iter: 1848 loss: 3.45402754e-07
Iter: 1849 loss: 3.45398092e-07
Iter: 1850 loss: 3.45356796e-07
Iter: 1851 loss: 3.45311122e-07
Iter: 1852 loss: 3.45296144e-07
Iter: 1853 loss: 3.45302567e-07
Iter: 1854 loss: 3.45271843e-07
Iter: 1855 loss: 3.45244359e-07
Iter: 1856 loss: 3.4523498e-07
Iter: 1857 loss: 3.45219917e-07
Iter: 1858 loss: 3.45192205e-07
Iter: 1859 loss: 3.45171031e-07
Iter: 1860 loss: 3.45153353e-07
Iter: 1861 loss: 3.4511018e-07
Iter: 1862 loss: 3.45123908e-07
Iter: 1863 loss: 3.45078092e-07
Iter: 1864 loss: 3.45019657e-07
Iter: 1865 loss: 3.45007493e-07
Iter: 1866 loss: 3.44992031e-07
Iter: 1867 loss: 3.45148607e-07
Iter: 1868 loss: 3.44981174e-07
Iter: 1869 loss: 3.44966395e-07
Iter: 1870 loss: 3.44974552e-07
Iter: 1871 loss: 3.44949143e-07
Iter: 1872 loss: 3.44907818e-07
Iter: 1873 loss: 3.44881954e-07
Iter: 1874 loss: 3.4486925e-07
Iter: 1875 loss: 3.44818773e-07
Iter: 1876 loss: 3.45064024e-07
Iter: 1877 loss: 3.44805756e-07
Iter: 1878 loss: 3.44780915e-07
Iter: 1879 loss: 3.45219291e-07
Iter: 1880 loss: 3.44785519e-07
Iter: 1881 loss: 3.44747548e-07
Iter: 1882 loss: 3.44789555e-07
Iter: 1883 loss: 3.44730893e-07
Iter: 1884 loss: 3.44705e-07
Iter: 1885 loss: 3.44660918e-07
Iter: 1886 loss: 3.44652875e-07
Iter: 1887 loss: 3.44608964e-07
Iter: 1888 loss: 3.44888292e-07
Iter: 1889 loss: 3.44606406e-07
Iter: 1890 loss: 3.4459768e-07
Iter: 1891 loss: 3.44583782e-07
Iter: 1892 loss: 3.44567866e-07
Iter: 1893 loss: 3.44544219e-07
Iter: 1894 loss: 3.44702983e-07
Iter: 1895 loss: 3.44539387e-07
Iter: 1896 loss: 3.44493969e-07
Iter: 1897 loss: 3.44763038e-07
Iter: 1898 loss: 3.44494708e-07
Iter: 1899 loss: 3.44481208e-07
Iter: 1900 loss: 3.44480611e-07
Iter: 1901 loss: 3.44459295e-07
Iter: 1902 loss: 3.44437467e-07
Iter: 1903 loss: 3.44429736e-07
Iter: 1904 loss: 3.44410921e-07
Iter: 1905 loss: 3.44415469e-07
Iter: 1906 loss: 3.44389377e-07
Iter: 1907 loss: 3.44376303e-07
Iter: 1908 loss: 3.44373063e-07
Iter: 1909 loss: 3.44358796e-07
Iter: 1910 loss: 3.44325599e-07
Iter: 1911 loss: 3.4433549e-07
Iter: 1912 loss: 3.44316078e-07
Iter: 1913 loss: 3.44314572e-07
Iter: 1914 loss: 3.44297604e-07
Iter: 1915 loss: 3.44288708e-07
Iter: 1916 loss: 3.44288651e-07
Iter: 1917 loss: 3.44261309e-07
Iter: 1918 loss: 3.44278703e-07
Iter: 1919 loss: 3.44250452e-07
Iter: 1920 loss: 3.442359e-07
Iter: 1921 loss: 3.44270632e-07
Iter: 1922 loss: 3.44226549e-07
Iter: 1923 loss: 3.44198156e-07
Iter: 1924 loss: 3.44338133e-07
Iter: 1925 loss: 3.44199293e-07
Iter: 1926 loss: 3.44174566e-07
Iter: 1927 loss: 3.4417269e-07
Iter: 1928 loss: 3.44157627e-07
Iter: 1929 loss: 3.44135231e-07
Iter: 1930 loss: 3.44278703e-07
Iter: 1931 loss: 3.44131848e-07
Iter: 1932 loss: 3.44110475e-07
Iter: 1933 loss: 3.4422041e-07
Iter: 1934 loss: 3.44093962e-07
Iter: 1935 loss: 3.4407168e-07
Iter: 1936 loss: 3.4401674e-07
Iter: 1937 loss: 3.44745274e-07
Iter: 1938 loss: 3.44005571e-07
Iter: 1939 loss: 3.43966207e-07
Iter: 1940 loss: 3.44420641e-07
Iter: 1941 loss: 3.43951399e-07
Iter: 1942 loss: 3.43929457e-07
Iter: 1943 loss: 3.44370562e-07
Iter: 1944 loss: 3.43929457e-07
Iter: 1945 loss: 3.43916611e-07
Iter: 1946 loss: 3.43911381e-07
Iter: 1947 loss: 3.43909591e-07
Iter: 1948 loss: 3.43888928e-07
Iter: 1949 loss: 3.43931021e-07
Iter: 1950 loss: 3.43889326e-07
Iter: 1951 loss: 3.43867043e-07
Iter: 1952 loss: 3.43860904e-07
Iter: 1953 loss: 3.43856897e-07
Iter: 1954 loss: 3.43804231e-07
Iter: 1955 loss: 3.43808551e-07
Iter: 1956 loss: 3.43775298e-07
Iter: 1957 loss: 3.43736275e-07
Iter: 1958 loss: 3.43749207e-07
Iter: 1959 loss: 3.43704073e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.4
+ date
Mon Oct 26 12:47:06 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.4/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.4_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 1 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.4_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72486811e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72487376a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72486dfd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7248762ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f724877b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f724877b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f724860cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72485977b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72485a8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7248574488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72485a8b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f724853ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72484d58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72484fc620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f724849d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7248483f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7248483bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7248483d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72483f0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f724848c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72483ee378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72483eec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7248363730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72483ee1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7248315840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f724831e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72482dd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72482a0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72482bb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f72482b9400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f724828c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f723e8e08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f723e8e0400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f723e88ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f723e89ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f723e83bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.36778873e-06
Iter: 2 loss: 3.8878743e-06
Iter: 3 loss: 3.09450161e-06
Iter: 4 loss: 2.86205227e-06
Iter: 5 loss: 2.53819371e-06
Iter: 6 loss: 2.52484142e-06
Iter: 7 loss: 2.39955762e-06
Iter: 8 loss: 2.34749086e-06
Iter: 9 loss: 2.27478426e-06
Iter: 10 loss: 2.12668283e-06
Iter: 11 loss: 4.81844654e-06
Iter: 12 loss: 2.12419582e-06
Iter: 13 loss: 2.07002086e-06
Iter: 14 loss: 2.05438801e-06
Iter: 15 loss: 2.01292414e-06
Iter: 16 loss: 2.08721463e-06
Iter: 17 loss: 1.99496117e-06
Iter: 18 loss: 1.94718677e-06
Iter: 19 loss: 1.83574957e-06
Iter: 20 loss: 3.18941647e-06
Iter: 21 loss: 1.82680537e-06
Iter: 22 loss: 1.78153437e-06
Iter: 23 loss: 1.76095e-06
Iter: 24 loss: 1.70045894e-06
Iter: 25 loss: 1.72156547e-06
Iter: 26 loss: 1.65789231e-06
Iter: 27 loss: 1.62274364e-06
Iter: 28 loss: 1.58731291e-06
Iter: 29 loss: 1.58028206e-06
Iter: 30 loss: 1.54495194e-06
Iter: 31 loss: 1.7532418e-06
Iter: 32 loss: 1.54038628e-06
Iter: 33 loss: 1.51278073e-06
Iter: 34 loss: 1.53732128e-06
Iter: 35 loss: 1.49666755e-06
Iter: 36 loss: 1.48441029e-06
Iter: 37 loss: 1.4766822e-06
Iter: 38 loss: 1.45819581e-06
Iter: 39 loss: 1.41096746e-06
Iter: 40 loss: 1.81618566e-06
Iter: 41 loss: 1.40317297e-06
Iter: 42 loss: 1.39647443e-06
Iter: 43 loss: 1.37825373e-06
Iter: 44 loss: 1.36122731e-06
Iter: 45 loss: 1.3250343e-06
Iter: 46 loss: 1.91557092e-06
Iter: 47 loss: 1.32398554e-06
Iter: 48 loss: 1.29272019e-06
Iter: 49 loss: 1.48103618e-06
Iter: 50 loss: 1.28887541e-06
Iter: 51 loss: 1.26607381e-06
Iter: 52 loss: 1.51162317e-06
Iter: 53 loss: 1.26554824e-06
Iter: 54 loss: 1.24567418e-06
Iter: 55 loss: 1.26306975e-06
Iter: 56 loss: 1.23405823e-06
Iter: 57 loss: 1.21719108e-06
Iter: 58 loss: 1.19690594e-06
Iter: 59 loss: 1.19488811e-06
Iter: 60 loss: 1.1801576e-06
Iter: 61 loss: 1.17657987e-06
Iter: 62 loss: 1.16008482e-06
Iter: 63 loss: 1.12453165e-06
Iter: 64 loss: 1.67268354e-06
Iter: 65 loss: 1.12318423e-06
Iter: 66 loss: 1.09315386e-06
Iter: 67 loss: 1.15534954e-06
Iter: 68 loss: 1.08113159e-06
Iter: 69 loss: 1.0570734e-06
Iter: 70 loss: 1.18293531e-06
Iter: 71 loss: 1.05328195e-06
Iter: 72 loss: 1.03088348e-06
Iter: 73 loss: 1.36924791e-06
Iter: 74 loss: 1.03088439e-06
Iter: 75 loss: 1.02067804e-06
Iter: 76 loss: 1.00605246e-06
Iter: 77 loss: 1.00557872e-06
Iter: 78 loss: 9.81346716e-07
Iter: 79 loss: 1.17171658e-06
Iter: 80 loss: 9.79645847e-07
Iter: 81 loss: 9.69230427e-07
Iter: 82 loss: 9.47122146e-07
Iter: 83 loss: 1.30461058e-06
Iter: 84 loss: 9.46431669e-07
Iter: 85 loss: 9.39001438e-07
Iter: 86 loss: 9.35577305e-07
Iter: 87 loss: 9.27716712e-07
Iter: 88 loss: 9.45018087e-07
Iter: 89 loss: 9.24701112e-07
Iter: 90 loss: 9.17711532e-07
Iter: 91 loss: 9.17773434e-07
Iter: 92 loss: 9.12201585e-07
Iter: 93 loss: 9.02699071e-07
Iter: 94 loss: 9.18629382e-07
Iter: 95 loss: 8.98440419e-07
Iter: 96 loss: 8.83016696e-07
Iter: 97 loss: 9.38740186e-07
Iter: 98 loss: 8.79152253e-07
Iter: 99 loss: 8.7074892e-07
Iter: 100 loss: 8.56752138e-07
Iter: 101 loss: 8.56710301e-07
Iter: 102 loss: 8.42519512e-07
Iter: 103 loss: 8.84158339e-07
Iter: 104 loss: 8.38116762e-07
Iter: 105 loss: 8.42218128e-07
Iter: 106 loss: 8.3342411e-07
Iter: 107 loss: 8.29960413e-07
Iter: 108 loss: 8.23403809e-07
Iter: 109 loss: 9.67047527e-07
Iter: 110 loss: 8.23390167e-07
Iter: 111 loss: 8.19477805e-07
Iter: 112 loss: 8.19146862e-07
Iter: 113 loss: 8.15250814e-07
Iter: 114 loss: 8.04624392e-07
Iter: 115 loss: 8.72537441e-07
Iter: 116 loss: 8.01922454e-07
Iter: 117 loss: 7.92144249e-07
Iter: 118 loss: 8.54018822e-07
Iter: 119 loss: 7.91043362e-07
Iter: 120 loss: 7.83789119e-07
Iter: 121 loss: 7.83786504e-07
Iter: 122 loss: 7.797222e-07
Iter: 123 loss: 7.75202693e-07
Iter: 124 loss: 7.74592138e-07
Iter: 125 loss: 7.6747051e-07
Iter: 126 loss: 7.87703641e-07
Iter: 127 loss: 7.65202628e-07
Iter: 128 loss: 7.62230684e-07
Iter: 129 loss: 7.61740125e-07
Iter: 130 loss: 7.59192346e-07
Iter: 131 loss: 7.52801554e-07
Iter: 132 loss: 8.09933681e-07
Iter: 133 loss: 7.5182669e-07
Iter: 134 loss: 7.44266458e-07
Iter: 135 loss: 7.39595293e-07
Iter: 136 loss: 7.3653672e-07
Iter: 137 loss: 7.33600075e-07
Iter: 138 loss: 7.31126647e-07
Iter: 139 loss: 7.26260396e-07
Iter: 140 loss: 7.50714e-07
Iter: 141 loss: 7.25436109e-07
Iter: 142 loss: 7.23327332e-07
Iter: 143 loss: 7.25155815e-07
Iter: 144 loss: 7.22072912e-07
Iter: 145 loss: 7.18166405e-07
Iter: 146 loss: 7.22293862e-07
Iter: 147 loss: 7.15993451e-07
Iter: 148 loss: 7.13352051e-07
Iter: 149 loss: 7.09096696e-07
Iter: 150 loss: 7.09074641e-07
Iter: 151 loss: 7.06095193e-07
Iter: 152 loss: 7.0573e-07
Iter: 153 loss: 7.02210912e-07
Iter: 154 loss: 6.95859853e-07
Iter: 155 loss: 8.50421088e-07
Iter: 156 loss: 6.95862923e-07
Iter: 157 loss: 6.90244519e-07
Iter: 158 loss: 7.37664436e-07
Iter: 159 loss: 6.8992091e-07
Iter: 160 loss: 6.87047475e-07
Iter: 161 loss: 7.14987777e-07
Iter: 162 loss: 6.86943167e-07
Iter: 163 loss: 6.84103725e-07
Iter: 164 loss: 6.86353928e-07
Iter: 165 loss: 6.82400525e-07
Iter: 166 loss: 6.79975301e-07
Iter: 167 loss: 6.75678564e-07
Iter: 168 loss: 6.75666968e-07
Iter: 169 loss: 6.70798954e-07
Iter: 170 loss: 6.88755563e-07
Iter: 171 loss: 6.69578412e-07
Iter: 172 loss: 6.68251687e-07
Iter: 173 loss: 6.67127779e-07
Iter: 174 loss: 6.65309244e-07
Iter: 175 loss: 6.61011597e-07
Iter: 176 loss: 7.09135463e-07
Iter: 177 loss: 6.60602325e-07
Iter: 178 loss: 6.5925235e-07
Iter: 179 loss: 6.5803772e-07
Iter: 180 loss: 6.56975431e-07
Iter: 181 loss: 6.54220401e-07
Iter: 182 loss: 6.74904072e-07
Iter: 183 loss: 6.53650602e-07
Iter: 184 loss: 6.50844072e-07
Iter: 185 loss: 6.67327811e-07
Iter: 186 loss: 6.5048016e-07
Iter: 187 loss: 6.47120032e-07
Iter: 188 loss: 6.64393326e-07
Iter: 189 loss: 6.46575074e-07
Iter: 190 loss: 6.44574527e-07
Iter: 191 loss: 6.42906571e-07
Iter: 192 loss: 6.42342229e-07
Iter: 193 loss: 6.39836401e-07
Iter: 194 loss: 6.6809713e-07
Iter: 195 loss: 6.3980923e-07
Iter: 196 loss: 6.37939365e-07
Iter: 197 loss: 6.48827552e-07
Iter: 198 loss: 6.37715971e-07
Iter: 199 loss: 6.36204049e-07
Iter: 200 loss: 6.34147796e-07
Iter: 201 loss: 6.34039225e-07
Iter: 202 loss: 6.32056299e-07
Iter: 203 loss: 6.32174874e-07
Iter: 204 loss: 6.30494696e-07
Iter: 205 loss: 6.2885357e-07
Iter: 206 loss: 6.28706232e-07
Iter: 207 loss: 6.26584722e-07
Iter: 208 loss: 6.24210088e-07
Iter: 209 loss: 6.23902736e-07
Iter: 210 loss: 6.21880076e-07
Iter: 211 loss: 6.21872232e-07
Iter: 212 loss: 6.19799152e-07
Iter: 213 loss: 6.16870466e-07
Iter: 214 loss: 6.16770535e-07
Iter: 215 loss: 6.14758619e-07
Iter: 216 loss: 6.15959607e-07
Iter: 217 loss: 6.13466057e-07
Iter: 218 loss: 6.1288813e-07
Iter: 219 loss: 6.1220652e-07
Iter: 220 loss: 6.11358757e-07
Iter: 221 loss: 6.09735366e-07
Iter: 222 loss: 6.43933731e-07
Iter: 223 loss: 6.09725475e-07
Iter: 224 loss: 6.08139089e-07
Iter: 225 loss: 6.15678914e-07
Iter: 226 loss: 6.07866582e-07
Iter: 227 loss: 6.06066e-07
Iter: 228 loss: 6.14870942e-07
Iter: 229 loss: 6.05755304e-07
Iter: 230 loss: 6.03947285e-07
Iter: 231 loss: 6.03637773e-07
Iter: 232 loss: 6.02420073e-07
Iter: 233 loss: 6.00613e-07
Iter: 234 loss: 6.00309761e-07
Iter: 235 loss: 5.99083137e-07
Iter: 236 loss: 5.97085887e-07
Iter: 237 loss: 6.09715e-07
Iter: 238 loss: 5.96864197e-07
Iter: 239 loss: 5.94772359e-07
Iter: 240 loss: 6.14921078e-07
Iter: 241 loss: 5.94694939e-07
Iter: 242 loss: 5.93681364e-07
Iter: 243 loss: 5.92885272e-07
Iter: 244 loss: 5.92573542e-07
Iter: 245 loss: 5.9042236e-07
Iter: 246 loss: 6.01465445e-07
Iter: 247 loss: 5.90107902e-07
Iter: 248 loss: 5.88997182e-07
Iter: 249 loss: 5.86402962e-07
Iter: 250 loss: 6.15984277e-07
Iter: 251 loss: 5.86172064e-07
Iter: 252 loss: 5.85150588e-07
Iter: 253 loss: 5.84779343e-07
Iter: 254 loss: 5.83378892e-07
Iter: 255 loss: 5.84823908e-07
Iter: 256 loss: 5.82569896e-07
Iter: 257 loss: 5.81659265e-07
Iter: 258 loss: 5.81107201e-07
Iter: 259 loss: 5.8074e-07
Iter: 260 loss: 5.79492394e-07
Iter: 261 loss: 5.79488812e-07
Iter: 262 loss: 5.78357913e-07
Iter: 263 loss: 5.78204094e-07
Iter: 264 loss: 5.77419883e-07
Iter: 265 loss: 5.75913759e-07
Iter: 266 loss: 5.75638126e-07
Iter: 267 loss: 5.7461159e-07
Iter: 268 loss: 5.73102284e-07
Iter: 269 loss: 5.75057584e-07
Iter: 270 loss: 5.72316424e-07
Iter: 271 loss: 5.7247837e-07
Iter: 272 loss: 5.71576493e-07
Iter: 273 loss: 5.71120268e-07
Iter: 274 loss: 5.70315194e-07
Iter: 275 loss: 5.70314626e-07
Iter: 276 loss: 5.69783538e-07
Iter: 277 loss: 5.69745566e-07
Iter: 278 loss: 5.69323674e-07
Iter: 279 loss: 5.6809796e-07
Iter: 280 loss: 5.72716203e-07
Iter: 281 loss: 5.67563802e-07
Iter: 282 loss: 5.65764708e-07
Iter: 283 loss: 5.71139708e-07
Iter: 284 loss: 5.65227481e-07
Iter: 285 loss: 5.64287802e-07
Iter: 286 loss: 5.64009497e-07
Iter: 287 loss: 5.63550316e-07
Iter: 288 loss: 5.62381842e-07
Iter: 289 loss: 5.71922214e-07
Iter: 290 loss: 5.62185505e-07
Iter: 291 loss: 5.61532374e-07
Iter: 292 loss: 5.6140027e-07
Iter: 293 loss: 5.60785e-07
Iter: 294 loss: 5.62331e-07
Iter: 295 loss: 5.60554554e-07
Iter: 296 loss: 5.60030799e-07
Iter: 297 loss: 5.59725777e-07
Iter: 298 loss: 5.59500847e-07
Iter: 299 loss: 5.58549e-07
Iter: 300 loss: 5.5707244e-07
Iter: 301 loss: 5.57055557e-07
Iter: 302 loss: 5.56250825e-07
Iter: 303 loss: 5.55971724e-07
Iter: 304 loss: 5.54806434e-07
Iter: 305 loss: 5.54915346e-07
Iter: 306 loss: 5.53938548e-07
Iter: 307 loss: 5.53350617e-07
Iter: 308 loss: 5.5334732e-07
Iter: 309 loss: 5.52749839e-07
Iter: 310 loss: 5.51642074e-07
Iter: 311 loss: 5.7666864e-07
Iter: 312 loss: 5.51636674e-07
Iter: 313 loss: 5.50635605e-07
Iter: 314 loss: 5.50859397e-07
Iter: 315 loss: 5.49914546e-07
Iter: 316 loss: 5.49681204e-07
Iter: 317 loss: 5.49338779e-07
Iter: 318 loss: 5.48775347e-07
Iter: 319 loss: 5.47427135e-07
Iter: 320 loss: 5.631465e-07
Iter: 321 loss: 5.47310378e-07
Iter: 322 loss: 5.46299418e-07
Iter: 323 loss: 5.55462407e-07
Iter: 324 loss: 5.4625383e-07
Iter: 325 loss: 5.45464331e-07
Iter: 326 loss: 5.53753807e-07
Iter: 327 loss: 5.45444038e-07
Iter: 328 loss: 5.45034368e-07
Iter: 329 loss: 5.4497059e-07
Iter: 330 loss: 5.44694672e-07
Iter: 331 loss: 5.44071327e-07
Iter: 332 loss: 5.43907163e-07
Iter: 333 loss: 5.43511419e-07
Iter: 334 loss: 5.42589135e-07
Iter: 335 loss: 5.44963427e-07
Iter: 336 loss: 5.42253929e-07
Iter: 337 loss: 5.41037593e-07
Iter: 338 loss: 5.51182438e-07
Iter: 339 loss: 5.40954716e-07
Iter: 340 loss: 5.40454835e-07
Iter: 341 loss: 5.4054982e-07
Iter: 342 loss: 5.40092515e-07
Iter: 343 loss: 5.39196321e-07
Iter: 344 loss: 5.42177645e-07
Iter: 345 loss: 5.38940185e-07
Iter: 346 loss: 5.38421205e-07
Iter: 347 loss: 5.37509891e-07
Iter: 348 loss: 5.60432682e-07
Iter: 349 loss: 5.37513699e-07
Iter: 350 loss: 5.36851587e-07
Iter: 351 loss: 5.46407136e-07
Iter: 352 loss: 5.36866366e-07
Iter: 353 loss: 5.36111543e-07
Iter: 354 loss: 5.37437e-07
Iter: 355 loss: 5.35800154e-07
Iter: 356 loss: 5.35257072e-07
Iter: 357 loss: 5.34362243e-07
Iter: 358 loss: 5.34360652e-07
Iter: 359 loss: 5.33969455e-07
Iter: 360 loss: 5.33726222e-07
Iter: 361 loss: 5.33294667e-07
Iter: 362 loss: 5.3277347e-07
Iter: 363 loss: 5.32718e-07
Iter: 364 loss: 5.32080094e-07
Iter: 365 loss: 5.33931143e-07
Iter: 366 loss: 5.31876e-07
Iter: 367 loss: 5.3124171e-07
Iter: 368 loss: 5.32308263e-07
Iter: 369 loss: 5.30955049e-07
Iter: 370 loss: 5.30583407e-07
Iter: 371 loss: 5.30523891e-07
Iter: 372 loss: 5.30254567e-07
Iter: 373 loss: 5.29650606e-07
Iter: 374 loss: 5.37946562e-07
Iter: 375 loss: 5.29637418e-07
Iter: 376 loss: 5.28831833e-07
Iter: 377 loss: 5.35863478e-07
Iter: 378 loss: 5.28793748e-07
Iter: 379 loss: 5.28361966e-07
Iter: 380 loss: 5.27289842e-07
Iter: 381 loss: 5.37375513e-07
Iter: 382 loss: 5.27132613e-07
Iter: 383 loss: 5.26484371e-07
Iter: 384 loss: 5.26479e-07
Iter: 385 loss: 5.26077429e-07
Iter: 386 loss: 5.30223e-07
Iter: 387 loss: 5.2608749e-07
Iter: 388 loss: 5.25821804e-07
Iter: 389 loss: 5.25223413e-07
Iter: 390 loss: 5.32181957e-07
Iter: 391 loss: 5.25176063e-07
Iter: 392 loss: 5.24708753e-07
Iter: 393 loss: 5.24695565e-07
Iter: 394 loss: 5.24204893e-07
Iter: 395 loss: 5.23603035e-07
Iter: 396 loss: 5.23540166e-07
Iter: 397 loss: 5.2274811e-07
Iter: 398 loss: 5.24167945e-07
Iter: 399 loss: 5.22411483e-07
Iter: 400 loss: 5.21488403e-07
Iter: 401 loss: 5.23819324e-07
Iter: 402 loss: 5.21189634e-07
Iter: 403 loss: 5.20918661e-07
Iter: 404 loss: 5.20821686e-07
Iter: 405 loss: 5.20507115e-07
Iter: 406 loss: 5.20208459e-07
Iter: 407 loss: 5.20145932e-07
Iter: 408 loss: 5.19911509e-07
Iter: 409 loss: 5.19901107e-07
Iter: 410 loss: 5.19731429e-07
Iter: 411 loss: 5.19239052e-07
Iter: 412 loss: 5.22464688e-07
Iter: 413 loss: 5.19129e-07
Iter: 414 loss: 5.1852669e-07
Iter: 415 loss: 5.19197101e-07
Iter: 416 loss: 5.18206832e-07
Iter: 417 loss: 5.17995886e-07
Iter: 418 loss: 5.17787555e-07
Iter: 419 loss: 5.17481453e-07
Iter: 420 loss: 5.16708724e-07
Iter: 421 loss: 5.24494226e-07
Iter: 422 loss: 5.16626415e-07
Iter: 423 loss: 5.1606446e-07
Iter: 424 loss: 5.16060254e-07
Iter: 425 loss: 5.15580894e-07
Iter: 426 loss: 5.17696e-07
Iter: 427 loss: 5.15481815e-07
Iter: 428 loss: 5.15243244e-07
Iter: 429 loss: 5.1497608e-07
Iter: 430 loss: 5.14950784e-07
Iter: 431 loss: 5.14464546e-07
Iter: 432 loss: 5.15618e-07
Iter: 433 loss: 5.14292481e-07
Iter: 434 loss: 5.13961481e-07
Iter: 435 loss: 5.13954774e-07
Iter: 436 loss: 5.13623945e-07
Iter: 437 loss: 5.13387818e-07
Iter: 438 loss: 5.13271743e-07
Iter: 439 loss: 5.12944155e-07
Iter: 440 loss: 5.16872888e-07
Iter: 441 loss: 5.1294046e-07
Iter: 442 loss: 5.12626627e-07
Iter: 443 loss: 5.12145903e-07
Iter: 444 loss: 5.121409e-07
Iter: 445 loss: 5.11693827e-07
Iter: 446 loss: 5.11667224e-07
Iter: 447 loss: 5.11306951e-07
Iter: 448 loss: 5.11264943e-07
Iter: 449 loss: 5.11065195e-07
Iter: 450 loss: 5.10828045e-07
Iter: 451 loss: 5.10283428e-07
Iter: 452 loss: 5.18396178e-07
Iter: 453 loss: 5.10259611e-07
Iter: 454 loss: 5.09743927e-07
Iter: 455 loss: 5.12262602e-07
Iter: 456 loss: 5.09650363e-07
Iter: 457 loss: 5.09159122e-07
Iter: 458 loss: 5.13745e-07
Iter: 459 loss: 5.09131439e-07
Iter: 460 loss: 5.08867629e-07
Iter: 461 loss: 5.08532821e-07
Iter: 462 loss: 5.08507696e-07
Iter: 463 loss: 5.08047492e-07
Iter: 464 loss: 5.09880465e-07
Iter: 465 loss: 5.07965524e-07
Iter: 466 loss: 5.07643449e-07
Iter: 467 loss: 5.09832375e-07
Iter: 468 loss: 5.07602635e-07
Iter: 469 loss: 5.07242e-07
Iter: 470 loss: 5.08150833e-07
Iter: 471 loss: 5.0709582e-07
Iter: 472 loss: 5.0682354e-07
Iter: 473 loss: 5.0724077e-07
Iter: 474 loss: 5.06688821e-07
Iter: 475 loss: 5.0625e-07
Iter: 476 loss: 5.06476624e-07
Iter: 477 loss: 5.05945422e-07
Iter: 478 loss: 5.05592425e-07
Iter: 479 loss: 5.05068101e-07
Iter: 480 loss: 5.05054345e-07
Iter: 481 loss: 5.04771094e-07
Iter: 482 loss: 5.04740456e-07
Iter: 483 loss: 5.04441914e-07
Iter: 484 loss: 5.05564856e-07
Iter: 485 loss: 5.04372338e-07
Iter: 486 loss: 5.04222e-07
Iter: 487 loss: 5.03935e-07
Iter: 488 loss: 5.09299412e-07
Iter: 489 loss: 5.03923388e-07
Iter: 490 loss: 5.03603417e-07
Iter: 491 loss: 5.03602791e-07
Iter: 492 loss: 5.03360639e-07
Iter: 493 loss: 5.02877356e-07
Iter: 494 loss: 5.1174e-07
Iter: 495 loss: 5.02849161e-07
Iter: 496 loss: 5.02354624e-07
Iter: 497 loss: 5.04827085e-07
Iter: 498 loss: 5.02242756e-07
Iter: 499 loss: 5.01889815e-07
Iter: 500 loss: 5.04415198e-07
Iter: 501 loss: 5.01851559e-07
Iter: 502 loss: 5.01562454e-07
Iter: 503 loss: 5.05241815e-07
Iter: 504 loss: 5.01565296e-07
Iter: 505 loss: 5.01426484e-07
Iter: 506 loss: 5.0137163e-07
Iter: 507 loss: 5.01300576e-07
Iter: 508 loss: 5.01081672e-07
Iter: 509 loss: 5.02692274e-07
Iter: 510 loss: 5.01062061e-07
Iter: 511 loss: 5.00941951e-07
Iter: 512 loss: 5.00603846e-07
Iter: 513 loss: 5.02756507e-07
Iter: 514 loss: 5.00518695e-07
Iter: 515 loss: 5.00125623e-07
Iter: 516 loss: 5.0382323e-07
Iter: 517 loss: 5.00098906e-07
Iter: 518 loss: 4.99809516e-07
Iter: 519 loss: 5.00588953e-07
Iter: 520 loss: 4.99702878e-07
Iter: 521 loss: 4.99440091e-07
Iter: 522 loss: 4.9913325e-07
Iter: 523 loss: 4.99098633e-07
Iter: 524 loss: 4.99056682e-07
Iter: 525 loss: 4.98893542e-07
Iter: 526 loss: 4.98726536e-07
Iter: 527 loss: 4.98408212e-07
Iter: 528 loss: 5.05580033e-07
Iter: 529 loss: 4.98403e-07
Iter: 530 loss: 4.9807e-07
Iter: 531 loss: 4.98921e-07
Iter: 532 loss: 4.97969552e-07
Iter: 533 loss: 4.97628491e-07
Iter: 534 loss: 4.97829319e-07
Iter: 535 loss: 4.97376391e-07
Iter: 536 loss: 4.97115707e-07
Iter: 537 loss: 4.9709962e-07
Iter: 538 loss: 4.96817108e-07
Iter: 539 loss: 4.96467806e-07
Iter: 540 loss: 4.96449047e-07
Iter: 541 loss: 4.96248902e-07
Iter: 542 loss: 4.96229632e-07
Iter: 543 loss: 4.96077917e-07
Iter: 544 loss: 4.95743507e-07
Iter: 545 loss: 5.00461681e-07
Iter: 546 loss: 4.95723611e-07
Iter: 547 loss: 4.95458835e-07
Iter: 548 loss: 4.95350946e-07
Iter: 549 loss: 4.95204063e-07
Iter: 550 loss: 4.95214294e-07
Iter: 551 loss: 4.95023585e-07
Iter: 552 loss: 4.94872438e-07
Iter: 553 loss: 4.94483629e-07
Iter: 554 loss: 4.98086138e-07
Iter: 555 loss: 4.94413598e-07
Iter: 556 loss: 4.94082428e-07
Iter: 557 loss: 4.96729285e-07
Iter: 558 loss: 4.94056394e-07
Iter: 559 loss: 4.93667e-07
Iter: 560 loss: 4.93917753e-07
Iter: 561 loss: 4.93430662e-07
Iter: 562 loss: 4.93236143e-07
Iter: 563 loss: 4.93144739e-07
Iter: 564 loss: 4.93043899e-07
Iter: 565 loss: 4.92791571e-07
Iter: 566 loss: 4.93060611e-07
Iter: 567 loss: 4.92665777e-07
Iter: 568 loss: 4.92396907e-07
Iter: 569 loss: 4.95245e-07
Iter: 570 loss: 4.92387642e-07
Iter: 571 loss: 4.92124059e-07
Iter: 572 loss: 4.94101073e-07
Iter: 573 loss: 4.92094841e-07
Iter: 574 loss: 4.91881906e-07
Iter: 575 loss: 4.91876108e-07
Iter: 576 loss: 4.91727519e-07
Iter: 577 loss: 4.91495598e-07
Iter: 578 loss: 4.93427e-07
Iter: 579 loss: 4.9148332e-07
Iter: 580 loss: 4.91254923e-07
Iter: 581 loss: 4.91079e-07
Iter: 582 loss: 4.91020501e-07
Iter: 583 loss: 4.9077596e-07
Iter: 584 loss: 4.90842467e-07
Iter: 585 loss: 4.90605657e-07
Iter: 586 loss: 4.90447349e-07
Iter: 587 loss: 4.90410571e-07
Iter: 588 loss: 4.90309503e-07
Iter: 589 loss: 4.8995696e-07
Iter: 590 loss: 4.91284823e-07
Iter: 591 loss: 4.89818376e-07
Iter: 592 loss: 4.89818831e-07
Iter: 593 loss: 4.89618287e-07
Iter: 594 loss: 4.89446052e-07
Iter: 595 loss: 4.89125682e-07
Iter: 596 loss: 4.95005281e-07
Iter: 597 loss: 4.89117383e-07
Iter: 598 loss: 4.88794626e-07
Iter: 599 loss: 4.88839078e-07
Iter: 600 loss: 4.88544401e-07
Iter: 601 loss: 4.88310548e-07
Iter: 602 loss: 4.89330318e-07
Iter: 603 loss: 4.88246883e-07
Iter: 604 loss: 4.88158321e-07
Iter: 605 loss: 4.88126432e-07
Iter: 606 loss: 4.87968236e-07
Iter: 607 loss: 4.8774416e-07
Iter: 608 loss: 4.87730745e-07
Iter: 609 loss: 4.87500131e-07
Iter: 610 loss: 4.89502668e-07
Iter: 611 loss: 4.87493537e-07
Iter: 612 loss: 4.87290833e-07
Iter: 613 loss: 4.88228e-07
Iter: 614 loss: 4.8725019e-07
Iter: 615 loss: 4.87090688e-07
Iter: 616 loss: 4.86840236e-07
Iter: 617 loss: 4.86846091e-07
Iter: 618 loss: 4.86642591e-07
Iter: 619 loss: 4.88943499e-07
Iter: 620 loss: 4.86642e-07
Iter: 621 loss: 4.86423346e-07
Iter: 622 loss: 4.87006048e-07
Iter: 623 loss: 4.86347176e-07
Iter: 624 loss: 4.86236786e-07
Iter: 625 loss: 4.86061822e-07
Iter: 626 loss: 4.86049601e-07
Iter: 627 loss: 4.85876058e-07
Iter: 628 loss: 4.85863382e-07
Iter: 629 loss: 4.85745886e-07
Iter: 630 loss: 4.85367195e-07
Iter: 631 loss: 4.86889e-07
Iter: 632 loss: 4.85223779e-07
Iter: 633 loss: 4.84767838e-07
Iter: 634 loss: 4.86179601e-07
Iter: 635 loss: 4.84638122e-07
Iter: 636 loss: 4.84315251e-07
Iter: 637 loss: 4.8568063e-07
Iter: 638 loss: 4.84247721e-07
Iter: 639 loss: 4.84148359e-07
Iter: 640 loss: 4.84049167e-07
Iter: 641 loss: 4.83974702e-07
Iter: 642 loss: 4.83747101e-07
Iter: 643 loss: 4.85319e-07
Iter: 644 loss: 4.83682243e-07
Iter: 645 loss: 4.83518761e-07
Iter: 646 loss: 4.83500969e-07
Iter: 647 loss: 4.83327881e-07
Iter: 648 loss: 4.83151553e-07
Iter: 649 loss: 4.8311631e-07
Iter: 650 loss: 4.82777182e-07
Iter: 651 loss: 4.83335725e-07
Iter: 652 loss: 4.82624387e-07
Iter: 653 loss: 4.82367454e-07
Iter: 654 loss: 4.83092776e-07
Iter: 655 loss: 4.82292705e-07
Iter: 656 loss: 4.82097391e-07
Iter: 657 loss: 4.84033819e-07
Iter: 658 loss: 4.82085852e-07
Iter: 659 loss: 4.81886218e-07
Iter: 660 loss: 4.82774908e-07
Iter: 661 loss: 4.81858933e-07
Iter: 662 loss: 4.81759912e-07
Iter: 663 loss: 4.81644179e-07
Iter: 664 loss: 4.81631446e-07
Iter: 665 loss: 4.81440793e-07
Iter: 666 loss: 4.83366364e-07
Iter: 667 loss: 4.81432494e-07
Iter: 668 loss: 4.81299253e-07
Iter: 669 loss: 4.81019242e-07
Iter: 670 loss: 4.86109911e-07
Iter: 671 loss: 4.81016855e-07
Iter: 672 loss: 4.80815174e-07
Iter: 673 loss: 4.80803351e-07
Iter: 674 loss: 4.80681081e-07
Iter: 675 loss: 4.80402548e-07
Iter: 676 loss: 4.84089469e-07
Iter: 677 loss: 4.8039243e-07
Iter: 678 loss: 4.80195524e-07
Iter: 679 loss: 4.81748714e-07
Iter: 680 loss: 4.80175345e-07
Iter: 681 loss: 4.79974517e-07
Iter: 682 loss: 4.80999063e-07
Iter: 683 loss: 4.79931714e-07
Iter: 684 loss: 4.79839628e-07
Iter: 685 loss: 4.79595428e-07
Iter: 686 loss: 4.83311169e-07
Iter: 687 loss: 4.79594064e-07
Iter: 688 loss: 4.79344351e-07
Iter: 689 loss: 4.79610208e-07
Iter: 690 loss: 4.79230096e-07
Iter: 691 loss: 4.79029666e-07
Iter: 692 loss: 4.79042285e-07
Iter: 693 loss: 4.78851348e-07
Iter: 694 loss: 4.78656091e-07
Iter: 695 loss: 4.7865592e-07
Iter: 696 loss: 4.78525806e-07
Iter: 697 loss: 4.78256425e-07
Iter: 698 loss: 4.83983683e-07
Iter: 699 loss: 4.78259039e-07
Iter: 700 loss: 4.78102663e-07
Iter: 701 loss: 4.78096183e-07
Iter: 702 loss: 4.77938102e-07
Iter: 703 loss: 4.78232892e-07
Iter: 704 loss: 4.77860908e-07
Iter: 705 loss: 4.77722097e-07
Iter: 706 loss: 4.77825e-07
Iter: 707 loss: 4.77653771e-07
Iter: 708 loss: 4.7739195e-07
Iter: 709 loss: 4.77563617e-07
Iter: 710 loss: 4.77246e-07
Iter: 711 loss: 4.77095682e-07
Iter: 712 loss: 4.76998196e-07
Iter: 713 loss: 4.76951527e-07
Iter: 714 loss: 4.76829541e-07
Iter: 715 loss: 4.76826585e-07
Iter: 716 loss: 4.76705679e-07
Iter: 717 loss: 4.7663076e-07
Iter: 718 loss: 4.76570875e-07
Iter: 719 loss: 4.7644923e-07
Iter: 720 loss: 4.76357087e-07
Iter: 721 loss: 4.7630931e-07
Iter: 722 loss: 4.76092254e-07
Iter: 723 loss: 4.78923766e-07
Iter: 724 loss: 4.76093817e-07
Iter: 725 loss: 4.75918341e-07
Iter: 726 loss: 4.75946166e-07
Iter: 727 loss: 4.75790699e-07
Iter: 728 loss: 4.75601382e-07
Iter: 729 loss: 4.77357503e-07
Iter: 730 loss: 4.75572e-07
Iter: 731 loss: 4.75431648e-07
Iter: 732 loss: 4.75426674e-07
Iter: 733 loss: 4.75327226e-07
Iter: 734 loss: 4.75267541e-07
Iter: 735 loss: 4.75248186e-07
Iter: 736 loss: 4.75165734e-07
Iter: 737 loss: 4.75069271e-07
Iter: 738 loss: 4.75070607e-07
Iter: 739 loss: 4.74953822e-07
Iter: 740 loss: 4.74941828e-07
Iter: 741 loss: 4.74866226e-07
Iter: 742 loss: 4.74669719e-07
Iter: 743 loss: 4.7680328e-07
Iter: 744 loss: 4.7463044e-07
Iter: 745 loss: 4.74485091e-07
Iter: 746 loss: 4.74489639e-07
Iter: 747 loss: 4.74368392e-07
Iter: 748 loss: 4.7436157e-07
Iter: 749 loss: 4.74285599e-07
Iter: 750 loss: 4.74121805e-07
Iter: 751 loss: 4.73846086e-07
Iter: 752 loss: 4.80833819e-07
Iter: 753 loss: 4.73845e-07
Iter: 754 loss: 4.73702784e-07
Iter: 755 loss: 4.73693916e-07
Iter: 756 loss: 4.73578666e-07
Iter: 757 loss: 4.73476803e-07
Iter: 758 loss: 4.73458414e-07
Iter: 759 loss: 4.73271939e-07
Iter: 760 loss: 4.73261451e-07
Iter: 761 loss: 4.7313037e-07
Iter: 762 loss: 4.72973625e-07
Iter: 763 loss: 4.72968935e-07
Iter: 764 loss: 4.72811763e-07
Iter: 765 loss: 4.72562732e-07
Iter: 766 loss: 4.72566967e-07
Iter: 767 loss: 4.72502023e-07
Iter: 768 loss: 4.72462432e-07
Iter: 769 loss: 4.72385722e-07
Iter: 770 loss: 4.72383533e-07
Iter: 771 loss: 4.72313246e-07
Iter: 772 loss: 4.72231079e-07
Iter: 773 loss: 4.72878526e-07
Iter: 774 loss: 4.72222553e-07
Iter: 775 loss: 4.721403e-07
Iter: 776 loss: 4.71947658e-07
Iter: 777 loss: 4.74772946e-07
Iter: 778 loss: 4.71950699e-07
Iter: 779 loss: 4.71834284e-07
Iter: 780 loss: 4.73728107e-07
Iter: 781 loss: 4.71832067e-07
Iter: 782 loss: 4.71689418e-07
Iter: 783 loss: 4.71587612e-07
Iter: 784 loss: 4.71540829e-07
Iter: 785 loss: 4.71405912e-07
Iter: 786 loss: 4.72003364e-07
Iter: 787 loss: 4.71371152e-07
Iter: 788 loss: 4.71209944e-07
Iter: 789 loss: 4.71536595e-07
Iter: 790 loss: 4.71133717e-07
Iter: 791 loss: 4.70975522e-07
Iter: 792 loss: 4.70741696e-07
Iter: 793 loss: 4.70736268e-07
Iter: 794 loss: 4.70517932e-07
Iter: 795 loss: 4.71786961e-07
Iter: 796 loss: 4.70486242e-07
Iter: 797 loss: 4.70364824e-07
Iter: 798 loss: 4.72254e-07
Iter: 799 loss: 4.70366984e-07
Iter: 800 loss: 4.70253781e-07
Iter: 801 loss: 4.70079158e-07
Iter: 802 loss: 4.70077538e-07
Iter: 803 loss: 4.69919399e-07
Iter: 804 loss: 4.72175742e-07
Iter: 805 loss: 4.69917381e-07
Iter: 806 loss: 4.69784879e-07
Iter: 807 loss: 4.70298801e-07
Iter: 808 loss: 4.69765723e-07
Iter: 809 loss: 4.69703082e-07
Iter: 810 loss: 4.69702456e-07
Iter: 811 loss: 4.69648967e-07
Iter: 812 loss: 4.69500321e-07
Iter: 813 loss: 4.69703309e-07
Iter: 814 loss: 4.69431143e-07
Iter: 815 loss: 4.69356365e-07
Iter: 816 loss: 4.69452374e-07
Iter: 817 loss: 4.6930802e-07
Iter: 818 loss: 4.6917765e-07
Iter: 819 loss: 4.69393569e-07
Iter: 820 loss: 4.69117253e-07
Iter: 821 loss: 4.69004192e-07
Iter: 822 loss: 4.68899117e-07
Iter: 823 loss: 4.68883485e-07
Iter: 824 loss: 4.68746634e-07
Iter: 825 loss: 4.68744275e-07
Iter: 826 loss: 4.68659579e-07
Iter: 827 loss: 4.68532505e-07
Iter: 828 loss: 4.68529606e-07
Iter: 829 loss: 4.68363083e-07
Iter: 830 loss: 4.68577042e-07
Iter: 831 loss: 4.68269889e-07
Iter: 832 loss: 4.68107487e-07
Iter: 833 loss: 4.70590663e-07
Iter: 834 loss: 4.68109704e-07
Iter: 835 loss: 4.68020744e-07
Iter: 836 loss: 4.6793636e-07
Iter: 837 loss: 4.67904044e-07
Iter: 838 loss: 4.67767734e-07
Iter: 839 loss: 4.68006306e-07
Iter: 840 loss: 4.67705377e-07
Iter: 841 loss: 4.67548062e-07
Iter: 842 loss: 4.69813699e-07
Iter: 843 loss: 4.67534363e-07
Iter: 844 loss: 4.67450718e-07
Iter: 845 loss: 4.67275612e-07
Iter: 846 loss: 4.70974271e-07
Iter: 847 loss: 4.67272855e-07
Iter: 848 loss: 4.67124437e-07
Iter: 849 loss: 4.67105963e-07
Iter: 850 loss: 4.67051933e-07
Iter: 851 loss: 4.66894335e-07
Iter: 852 loss: 4.67885627e-07
Iter: 853 loss: 4.66843773e-07
Iter: 854 loss: 4.66764703e-07
Iter: 855 loss: 4.66727e-07
Iter: 856 loss: 4.66638454e-07
Iter: 857 loss: 4.66498619e-07
Iter: 858 loss: 4.69743838e-07
Iter: 859 loss: 4.66489638e-07
Iter: 860 loss: 4.66350684e-07
Iter: 861 loss: 4.66451411e-07
Iter: 862 loss: 4.66274173e-07
Iter: 863 loss: 4.66120525e-07
Iter: 864 loss: 4.6698014e-07
Iter: 865 loss: 4.66107309e-07
Iter: 866 loss: 4.66014342e-07
Iter: 867 loss: 4.6593442e-07
Iter: 868 loss: 4.65909693e-07
Iter: 869 loss: 4.65732228e-07
Iter: 870 loss: 4.65730324e-07
Iter: 871 loss: 4.6560524e-07
Iter: 872 loss: 4.65432038e-07
Iter: 873 loss: 4.66536648e-07
Iter: 874 loss: 4.65404696e-07
Iter: 875 loss: 4.65279413e-07
Iter: 876 loss: 4.66444362e-07
Iter: 877 loss: 4.65274695e-07
Iter: 878 loss: 4.65126675e-07
Iter: 879 loss: 4.65482e-07
Iter: 880 loss: 4.65070258e-07
Iter: 881 loss: 4.649674e-07
Iter: 882 loss: 4.6501961e-07
Iter: 883 loss: 4.64899557e-07
Iter: 884 loss: 4.64747927e-07
Iter: 885 loss: 4.65982282e-07
Iter: 886 loss: 4.64744801e-07
Iter: 887 loss: 4.64636e-07
Iter: 888 loss: 4.64439665e-07
Iter: 889 loss: 4.6764049e-07
Iter: 890 loss: 4.64440575e-07
Iter: 891 loss: 4.64357129e-07
Iter: 892 loss: 4.6432487e-07
Iter: 893 loss: 4.64218033e-07
Iter: 894 loss: 4.64006348e-07
Iter: 895 loss: 4.67633441e-07
Iter: 896 loss: 4.64006348e-07
Iter: 897 loss: 4.63849261e-07
Iter: 898 loss: 4.64562049e-07
Iter: 899 loss: 4.63806884e-07
Iter: 900 loss: 4.63670403e-07
Iter: 901 loss: 4.65350354e-07
Iter: 902 loss: 4.63667561e-07
Iter: 903 loss: 4.63598553e-07
Iter: 904 loss: 4.63432315e-07
Iter: 905 loss: 4.65092796e-07
Iter: 906 loss: 4.63425977e-07
Iter: 907 loss: 4.63250785e-07
Iter: 908 loss: 4.63659319e-07
Iter: 909 loss: 4.63171659e-07
Iter: 910 loss: 4.6301e-07
Iter: 911 loss: 4.64597633e-07
Iter: 912 loss: 4.63004227e-07
Iter: 913 loss: 4.62866183e-07
Iter: 914 loss: 4.6410662e-07
Iter: 915 loss: 4.62871213e-07
Iter: 916 loss: 4.62781145e-07
Iter: 917 loss: 4.62886192e-07
Iter: 918 loss: 4.62728906e-07
Iter: 919 loss: 4.62636024e-07
Iter: 920 loss: 4.62994024e-07
Iter: 921 loss: 4.62614906e-07
Iter: 922 loss: 4.62498122e-07
Iter: 923 loss: 4.62813915e-07
Iter: 924 loss: 4.62480983e-07
Iter: 925 loss: 4.6239856e-07
Iter: 926 loss: 4.62306133e-07
Iter: 927 loss: 4.62303575e-07
Iter: 928 loss: 4.62174626e-07
Iter: 929 loss: 4.62150041e-07
Iter: 930 loss: 4.62072052e-07
Iter: 931 loss: 4.62006653e-07
Iter: 932 loss: 4.61959786e-07
Iter: 933 loss: 4.61884099e-07
Iter: 934 loss: 4.61707856e-07
Iter: 935 loss: 4.64835892e-07
Iter: 936 loss: 4.61708453e-07
Iter: 937 loss: 4.61629384e-07
Iter: 938 loss: 4.6160892e-07
Iter: 939 loss: 4.61520955e-07
Iter: 940 loss: 4.61451691e-07
Iter: 941 loss: 4.6141588e-07
Iter: 942 loss: 4.61315153e-07
Iter: 943 loss: 4.61252796e-07
Iter: 944 loss: 4.61215166e-07
Iter: 945 loss: 4.61117963e-07
Iter: 946 loss: 4.62511167e-07
Iter: 947 loss: 4.61117537e-07
Iter: 948 loss: 4.60999644e-07
Iter: 949 loss: 4.61289176e-07
Iter: 950 loss: 4.60969119e-07
Iter: 951 loss: 4.60878312e-07
Iter: 952 loss: 4.60964117e-07
Iter: 953 loss: 4.60810639e-07
Iter: 954 loss: 4.6072455e-07
Iter: 955 loss: 4.61735453e-07
Iter: 956 loss: 4.60732196e-07
Iter: 957 loss: 4.60656281e-07
Iter: 958 loss: 4.605821e-07
Iter: 959 loss: 4.60564308e-07
Iter: 960 loss: 4.60479441e-07
Iter: 961 loss: 4.60508829e-07
Iter: 962 loss: 4.60418306e-07
Iter: 963 loss: 4.60325197e-07
Iter: 964 loss: 4.60519232e-07
Iter: 965 loss: 4.60295951e-07
Iter: 966 loss: 4.6021961e-07
Iter: 967 loss: 4.60209691e-07
Iter: 968 loss: 4.60157025e-07
Iter: 969 loss: 4.60026854e-07
Iter: 970 loss: 4.61616452e-07
Iter: 971 loss: 4.60024637e-07
Iter: 972 loss: 4.59967168e-07
Iter: 973 loss: 4.5994193e-07
Iter: 974 loss: 4.59894693e-07
Iter: 975 loss: 4.59761253e-07
Iter: 976 loss: 4.62034166e-07
Iter: 977 loss: 4.59770774e-07
Iter: 978 loss: 4.59637363e-07
Iter: 979 loss: 4.59654984e-07
Iter: 980 loss: 4.59551927e-07
Iter: 981 loss: 4.59459073e-07
Iter: 982 loss: 4.60978441e-07
Iter: 983 loss: 4.59453787e-07
Iter: 984 loss: 4.59335524e-07
Iter: 985 loss: 4.59682639e-07
Iter: 986 loss: 4.59301049e-07
Iter: 987 loss: 4.59243893e-07
Iter: 988 loss: 4.59295165e-07
Iter: 989 loss: 4.59200862e-07
Iter: 990 loss: 4.59094906e-07
Iter: 991 loss: 4.59581059e-07
Iter: 992 loss: 4.59062335e-07
Iter: 993 loss: 4.58973659e-07
Iter: 994 loss: 4.58982782e-07
Iter: 995 loss: 4.58899876e-07
Iter: 996 loss: 4.58789827e-07
Iter: 997 loss: 4.5880779e-07
Iter: 998 loss: 4.58726504e-07
Iter: 999 loss: 4.58592154e-07
Iter: 1000 loss: 4.5860665e-07
Iter: 1001 loss: 4.58480429e-07
Iter: 1002 loss: 4.58311717e-07
Iter: 1003 loss: 4.59883296e-07
Iter: 1004 loss: 4.5830555e-07
Iter: 1005 loss: 4.58227305e-07
Iter: 1006 loss: 4.58219802e-07
Iter: 1007 loss: 4.58168358e-07
Iter: 1008 loss: 4.58021958e-07
Iter: 1009 loss: 4.59348485e-07
Iter: 1010 loss: 4.58003342e-07
Iter: 1011 loss: 4.57949568e-07
Iter: 1012 loss: 4.57910801e-07
Iter: 1013 loss: 4.57855322e-07
Iter: 1014 loss: 4.57706903e-07
Iter: 1015 loss: 4.59899468e-07
Iter: 1016 loss: 4.57703806e-07
Iter: 1017 loss: 4.5760487e-07
Iter: 1018 loss: 4.5760342e-07
Iter: 1019 loss: 4.57500448e-07
Iter: 1020 loss: 4.57516933e-07
Iter: 1021 loss: 4.5742803e-07
Iter: 1022 loss: 4.57340235e-07
Iter: 1023 loss: 4.57178544e-07
Iter: 1024 loss: 4.57184427e-07
Iter: 1025 loss: 4.57120308e-07
Iter: 1026 loss: 4.57074606e-07
Iter: 1027 loss: 4.56997157e-07
Iter: 1028 loss: 4.56946509e-07
Iter: 1029 loss: 4.5691425e-07
Iter: 1030 loss: 4.56811222e-07
Iter: 1031 loss: 4.56717771e-07
Iter: 1032 loss: 4.56668971e-07
Iter: 1033 loss: 4.56536185e-07
Iter: 1034 loss: 4.57133496e-07
Iter: 1035 loss: 4.56507195e-07
Iter: 1036 loss: 4.56417695e-07
Iter: 1037 loss: 4.57518865e-07
Iter: 1038 loss: 4.56409737e-07
Iter: 1039 loss: 4.5629389e-07
Iter: 1040 loss: 4.56456974e-07
Iter: 1041 loss: 4.56234318e-07
Iter: 1042 loss: 4.56152293e-07
Iter: 1043 loss: 4.56298835e-07
Iter: 1044 loss: 4.56116766e-07
Iter: 1045 loss: 4.5602286e-07
Iter: 1046 loss: 4.56759921e-07
Iter: 1047 loss: 4.56014334e-07
Iter: 1048 loss: 4.55953341e-07
Iter: 1049 loss: 4.55847811e-07
Iter: 1050 loss: 4.55849715e-07
Iter: 1051 loss: 4.55757402e-07
Iter: 1052 loss: 4.55747681e-07
Iter: 1053 loss: 4.55711643e-07
Iter: 1054 loss: 4.55634e-07
Iter: 1055 loss: 4.57198126e-07
Iter: 1056 loss: 4.55641384e-07
Iter: 1057 loss: 4.55585678e-07
Iter: 1058 loss: 4.56313046e-07
Iter: 1059 loss: 4.55581414e-07
Iter: 1060 loss: 4.55518375e-07
Iter: 1061 loss: 4.55437316e-07
Iter: 1062 loss: 4.55435952e-07
Iter: 1063 loss: 4.55345571e-07
Iter: 1064 loss: 4.55615776e-07
Iter: 1065 loss: 4.55319224e-07
Iter: 1066 loss: 4.55225404e-07
Iter: 1067 loss: 4.55172028e-07
Iter: 1068 loss: 4.55136188e-07
Iter: 1069 loss: 4.5500181e-07
Iter: 1070 loss: 4.55398094e-07
Iter: 1071 loss: 4.54964095e-07
Iter: 1072 loss: 4.5497e-07
Iter: 1073 loss: 4.54913589e-07
Iter: 1074 loss: 4.54884827e-07
Iter: 1075 loss: 4.54773527e-07
Iter: 1076 loss: 4.55527072e-07
Iter: 1077 loss: 4.54748943e-07
Iter: 1078 loss: 4.54684596e-07
Iter: 1079 loss: 4.54671124e-07
Iter: 1080 loss: 4.54609e-07
Iter: 1081 loss: 4.5451165e-07
Iter: 1082 loss: 4.56338739e-07
Iter: 1083 loss: 4.54504431e-07
Iter: 1084 loss: 4.54434144e-07
Iter: 1085 loss: 4.54440368e-07
Iter: 1086 loss: 4.54343194e-07
Iter: 1087 loss: 4.54267564e-07
Iter: 1088 loss: 4.54255257e-07
Iter: 1089 loss: 4.54126621e-07
Iter: 1090 loss: 4.54039537e-07
Iter: 1091 loss: 4.53998837e-07
Iter: 1092 loss: 4.53941141e-07
Iter: 1093 loss: 4.53927726e-07
Iter: 1094 loss: 4.53862896e-07
Iter: 1095 loss: 4.53768394e-07
Iter: 1096 loss: 4.53770127e-07
Iter: 1097 loss: 4.5367679e-07
Iter: 1098 loss: 4.53647715e-07
Iter: 1099 loss: 4.53583795e-07
Iter: 1100 loss: 4.5344521e-07
Iter: 1101 loss: 4.53643565e-07
Iter: 1102 loss: 4.53368244e-07
Iter: 1103 loss: 4.53217808e-07
Iter: 1104 loss: 4.53564127e-07
Iter: 1105 loss: 4.53170202e-07
Iter: 1106 loss: 4.53103496e-07
Iter: 1107 loss: 4.53082748e-07
Iter: 1108 loss: 4.53001661e-07
Iter: 1109 loss: 4.52958602e-07
Iter: 1110 loss: 4.52924041e-07
Iter: 1111 loss: 4.52861173e-07
Iter: 1112 loss: 4.53293353e-07
Iter: 1113 loss: 4.52858416e-07
Iter: 1114 loss: 4.52767608e-07
Iter: 1115 loss: 4.52689619e-07
Iter: 1116 loss: 4.52676147e-07
Iter: 1117 loss: 4.52590655e-07
Iter: 1118 loss: 4.52555e-07
Iter: 1119 loss: 4.52497318e-07
Iter: 1120 loss: 4.52406084e-07
Iter: 1121 loss: 4.52409e-07
Iter: 1122 loss: 4.52324e-07
Iter: 1123 loss: 4.52473159e-07
Iter: 1124 loss: 4.5228677e-07
Iter: 1125 loss: 4.52224413e-07
Iter: 1126 loss: 4.52092024e-07
Iter: 1127 loss: 4.54260373e-07
Iter: 1128 loss: 4.52096543e-07
Iter: 1129 loss: 4.51960943e-07
Iter: 1130 loss: 4.52818227e-07
Iter: 1131 loss: 4.519637e-07
Iter: 1132 loss: 4.51868573e-07
Iter: 1133 loss: 4.51866129e-07
Iter: 1134 loss: 4.51813435e-07
Iter: 1135 loss: 4.51702846e-07
Iter: 1136 loss: 4.52835167e-07
Iter: 1137 loss: 4.51683405e-07
Iter: 1138 loss: 4.51549198e-07
Iter: 1139 loss: 4.52477821e-07
Iter: 1140 loss: 4.51516826e-07
Iter: 1141 loss: 4.51442133e-07
Iter: 1142 loss: 4.51581059e-07
Iter: 1143 loss: 4.51396659e-07
Iter: 1144 loss: 4.51315145e-07
Iter: 1145 loss: 4.51310825e-07
Iter: 1146 loss: 4.51247672e-07
Iter: 1147 loss: 4.51135691e-07
Iter: 1148 loss: 4.52342505e-07
Iter: 1149 loss: 4.51128074e-07
Iter: 1150 loss: 4.51093541e-07
Iter: 1151 loss: 4.5105503e-07
Iter: 1152 loss: 4.51010578e-07
Iter: 1153 loss: 4.50918151e-07
Iter: 1154 loss: 4.5230658e-07
Iter: 1155 loss: 4.50918435e-07
Iter: 1156 loss: 4.50816685e-07
Iter: 1157 loss: 4.51707535e-07
Iter: 1158 loss: 4.50823222e-07
Iter: 1159 loss: 4.50721757e-07
Iter: 1160 loss: 4.51056593e-07
Iter: 1161 loss: 4.50698764e-07
Iter: 1162 loss: 4.50627226e-07
Iter: 1163 loss: 4.50571633e-07
Iter: 1164 loss: 4.50544235e-07
Iter: 1165 loss: 4.50463773e-07
Iter: 1166 loss: 4.50471248e-07
Iter: 1167 loss: 4.5036154e-07
Iter: 1168 loss: 4.50503933e-07
Iter: 1169 loss: 4.50327605e-07
Iter: 1170 loss: 4.50256948e-07
Iter: 1171 loss: 4.50111656e-07
Iter: 1172 loss: 4.50109326e-07
Iter: 1173 loss: 4.50024686e-07
Iter: 1174 loss: 4.51438666e-07
Iter: 1175 loss: 4.50021162e-07
Iter: 1176 loss: 4.49938966e-07
Iter: 1177 loss: 4.50381151e-07
Iter: 1178 loss: 4.49914921e-07
Iter: 1179 loss: 4.49818913e-07
Iter: 1180 loss: 4.49849608e-07
Iter: 1181 loss: 4.49745869e-07
Iter: 1182 loss: 4.49659694e-07
Iter: 1183 loss: 4.49976795e-07
Iter: 1184 loss: 4.49645427e-07
Iter: 1185 loss: 4.49533616e-07
Iter: 1186 loss: 4.49902529e-07
Iter: 1187 loss: 4.4952e-07
Iter: 1188 loss: 4.49464437e-07
Iter: 1189 loss: 4.49293566e-07
Iter: 1190 loss: 4.51068757e-07
Iter: 1191 loss: 4.49302433e-07
Iter: 1192 loss: 4.49276087e-07
Iter: 1193 loss: 4.49208017e-07
Iter: 1194 loss: 4.49144409e-07
Iter: 1195 loss: 4.49028846e-07
Iter: 1196 loss: 4.51492269e-07
Iter: 1197 loss: 4.49033905e-07
Iter: 1198 loss: 4.48935054e-07
Iter: 1199 loss: 4.48952449e-07
Iter: 1200 loss: 4.48849789e-07
Iter: 1201 loss: 4.48766428e-07
Iter: 1202 loss: 4.48763672e-07
Iter: 1203 loss: 4.48664878e-07
Iter: 1204 loss: 4.48857e-07
Iter: 1205 loss: 4.48639298e-07
Iter: 1206 loss: 4.48578476e-07
Iter: 1207 loss: 4.48442051e-07
Iter: 1208 loss: 4.49726741e-07
Iter: 1209 loss: 4.48424487e-07
Iter: 1210 loss: 4.4829909e-07
Iter: 1211 loss: 4.49440449e-07
Iter: 1212 loss: 4.48299659e-07
Iter: 1213 loss: 4.48286215e-07
Iter: 1214 loss: 4.48250944e-07
Iter: 1215 loss: 4.4822059e-07
Iter: 1216 loss: 4.48152917e-07
Iter: 1217 loss: 4.49069091e-07
Iter: 1218 loss: 4.48146807e-07
Iter: 1219 loss: 4.48066885e-07
Iter: 1220 loss: 4.48714786e-07
Iter: 1221 loss: 4.48047388e-07
Iter: 1222 loss: 4.47975765e-07
Iter: 1223 loss: 4.48087746e-07
Iter: 1224 loss: 4.47932791e-07
Iter: 1225 loss: 4.47838033e-07
Iter: 1226 loss: 4.47877852e-07
Iter: 1227 loss: 4.47780451e-07
Iter: 1228 loss: 4.4775004e-07
Iter: 1229 loss: 4.47739041e-07
Iter: 1230 loss: 4.47700245e-07
Iter: 1231 loss: 4.4759534e-07
Iter: 1232 loss: 4.49045842e-07
Iter: 1233 loss: 4.47574649e-07
Iter: 1234 loss: 4.47462384e-07
Iter: 1235 loss: 4.47517436e-07
Iter: 1236 loss: 4.47394171e-07
Iter: 1237 loss: 4.47317234e-07
Iter: 1238 loss: 4.48335641e-07
Iter: 1239 loss: 4.47310839e-07
Iter: 1240 loss: 4.47221225e-07
Iter: 1241 loss: 4.47821719e-07
Iter: 1242 loss: 4.47206503e-07
Iter: 1243 loss: 4.47143321e-07
Iter: 1244 loss: 4.47003117e-07
Iter: 1245 loss: 4.4838643e-07
Iter: 1246 loss: 4.46986434e-07
Iter: 1247 loss: 4.46855552e-07
Iter: 1248 loss: 4.47764535e-07
Iter: 1249 loss: 4.46841568e-07
Iter: 1250 loss: 4.46786714e-07
Iter: 1251 loss: 4.46777619e-07
Iter: 1252 loss: 4.46740671e-07
Iter: 1253 loss: 4.46670697e-07
Iter: 1254 loss: 4.47497086e-07
Iter: 1255 loss: 4.46627041e-07
Iter: 1256 loss: 4.46588047e-07
Iter: 1257 loss: 4.46579634e-07
Iter: 1258 loss: 4.46528247e-07
Iter: 1259 loss: 4.46439429e-07
Iter: 1260 loss: 4.46437809e-07
Iter: 1261 loss: 4.46319632e-07
Iter: 1262 loss: 4.46962815e-07
Iter: 1263 loss: 4.46313976e-07
Iter: 1264 loss: 4.46226039e-07
Iter: 1265 loss: 4.46747208e-07
Iter: 1266 loss: 4.46211487e-07
Iter: 1267 loss: 4.46106981e-07
Iter: 1268 loss: 4.45975672e-07
Iter: 1269 loss: 4.45968539e-07
Iter: 1270 loss: 4.45847206e-07
Iter: 1271 loss: 4.45962144e-07
Iter: 1272 loss: 4.45798776e-07
Iter: 1273 loss: 4.45684691e-07
Iter: 1274 loss: 4.46239653e-07
Iter: 1275 loss: 4.4566292e-07
Iter: 1276 loss: 4.45641717e-07
Iter: 1277 loss: 4.45603973e-07
Iter: 1278 loss: 4.45587261e-07
Iter: 1279 loss: 4.45501172e-07
Iter: 1280 loss: 4.45605338e-07
Iter: 1281 loss: 4.45444243e-07
Iter: 1282 loss: 4.45359433e-07
Iter: 1283 loss: 4.4536074e-07
Iter: 1284 loss: 4.45253761e-07
Iter: 1285 loss: 4.45285366e-07
Iter: 1286 loss: 4.45204961e-07
Iter: 1287 loss: 4.45136891e-07
Iter: 1288 loss: 4.45319131e-07
Iter: 1289 loss: 4.45114722e-07
Iter: 1290 loss: 4.45055662e-07
Iter: 1291 loss: 4.45690887e-07
Iter: 1292 loss: 4.45050517e-07
Iter: 1293 loss: 4.45010073e-07
Iter: 1294 loss: 4.44960477e-07
Iter: 1295 loss: 4.44962268e-07
Iter: 1296 loss: 4.44899229e-07
Iter: 1297 loss: 4.45609629e-07
Iter: 1298 loss: 4.44893061e-07
Iter: 1299 loss: 4.44855345e-07
Iter: 1300 loss: 4.44894852e-07
Iter: 1301 loss: 4.4480683e-07
Iter: 1302 loss: 4.44755244e-07
Iter: 1303 loss: 4.44631667e-07
Iter: 1304 loss: 4.44635361e-07
Iter: 1305 loss: 4.44509681e-07
Iter: 1306 loss: 4.44621747e-07
Iter: 1307 loss: 4.44422255e-07
Iter: 1308 loss: 4.44388917e-07
Iter: 1309 loss: 4.44367629e-07
Iter: 1310 loss: 4.44294443e-07
Iter: 1311 loss: 4.44406254e-07
Iter: 1312 loss: 4.44253146e-07
Iter: 1313 loss: 4.44195138e-07
Iter: 1314 loss: 4.44049647e-07
Iter: 1315 loss: 4.45808837e-07
Iter: 1316 loss: 4.44037028e-07
Iter: 1317 loss: 4.44057775e-07
Iter: 1318 loss: 4.43982771e-07
Iter: 1319 loss: 4.43932294e-07
Iter: 1320 loss: 4.43818919e-07
Iter: 1321 loss: 4.43816191e-07
Iter: 1322 loss: 4.43711883e-07
Iter: 1323 loss: 4.43947556e-07
Iter: 1324 loss: 4.43683263e-07
Iter: 1325 loss: 4.43536862e-07
Iter: 1326 loss: 4.44480293e-07
Iter: 1327 loss: 4.43544479e-07
Iter: 1328 loss: 4.43499573e-07
Iter: 1329 loss: 4.43396317e-07
Iter: 1330 loss: 4.4519885e-07
Iter: 1331 loss: 4.434022e-07
Iter: 1332 loss: 4.43331373e-07
Iter: 1333 loss: 4.4331864e-07
Iter: 1334 loss: 4.43265776e-07
Iter: 1335 loss: 4.43163231e-07
Iter: 1336 loss: 4.43172326e-07
Iter: 1337 loss: 4.43089192e-07
Iter: 1338 loss: 4.43186281e-07
Iter: 1339 loss: 4.43033514e-07
Iter: 1340 loss: 4.42961692e-07
Iter: 1341 loss: 4.42896351e-07
Iter: 1342 loss: 4.42862245e-07
Iter: 1343 loss: 4.4275788e-07
Iter: 1344 loss: 4.42955354e-07
Iter: 1345 loss: 4.42726559e-07
Iter: 1346 loss: 4.42659427e-07
Iter: 1347 loss: 4.42681426e-07
Iter: 1348 loss: 4.42607416e-07
Iter: 1349 loss: 4.42505666e-07
Iter: 1350 loss: 4.42619239e-07
Iter: 1351 loss: 4.42458514e-07
Iter: 1352 loss: 4.42340138e-07
Iter: 1353 loss: 4.43478484e-07
Iter: 1354 loss: 4.42326325e-07
Iter: 1355 loss: 4.42238047e-07
Iter: 1356 loss: 4.4215426e-07
Iter: 1357 loss: 4.42121234e-07
Iter: 1358 loss: 4.42015221e-07
Iter: 1359 loss: 4.42150508e-07
Iter: 1360 loss: 4.41953716e-07
Iter: 1361 loss: 4.41863193e-07
Iter: 1362 loss: 4.4184452e-07
Iter: 1363 loss: 4.41794157e-07
Iter: 1364 loss: 4.41700195e-07
Iter: 1365 loss: 4.43729675e-07
Iter: 1366 loss: 4.41681209e-07
Iter: 1367 loss: 4.41665236e-07
Iter: 1368 loss: 4.41638576e-07
Iter: 1369 loss: 4.41607824e-07
Iter: 1370 loss: 4.41485724e-07
Iter: 1371 loss: 4.42201667e-07
Iter: 1372 loss: 4.41447185e-07
Iter: 1373 loss: 4.41379939e-07
Iter: 1374 loss: 4.4137289e-07
Iter: 1375 loss: 4.41287966e-07
Iter: 1376 loss: 4.41412027e-07
Iter: 1377 loss: 4.41246186e-07
Iter: 1378 loss: 4.41188519e-07
Iter: 1379 loss: 4.41411885e-07
Iter: 1380 loss: 4.41186046e-07
Iter: 1381 loss: 4.41108455e-07
Iter: 1382 loss: 4.41449913e-07
Iter: 1383 loss: 4.41104248e-07
Iter: 1384 loss: 4.41042801e-07
Iter: 1385 loss: 4.40980727e-07
Iter: 1386 loss: 4.40969671e-07
Iter: 1387 loss: 4.40886936e-07
Iter: 1388 loss: 4.41173484e-07
Iter: 1389 loss: 4.40865392e-07
Iter: 1390 loss: 4.40771316e-07
Iter: 1391 loss: 4.41122893e-07
Iter: 1392 loss: 4.4075324e-07
Iter: 1393 loss: 4.40666042e-07
Iter: 1394 loss: 4.40739313e-07
Iter: 1395 loss: 4.4061909e-07
Iter: 1396 loss: 4.4051626e-07
Iter: 1397 loss: 4.41795919e-07
Iter: 1398 loss: 4.40517113e-07
Iter: 1399 loss: 4.40458507e-07
Iter: 1400 loss: 4.40410673e-07
Iter: 1401 loss: 4.40399475e-07
Iter: 1402 loss: 4.40343399e-07
Iter: 1403 loss: 4.40336407e-07
Iter: 1404 loss: 4.4030952e-07
Iter: 1405 loss: 4.40225534e-07
Iter: 1406 loss: 4.40577224e-07
Iter: 1407 loss: 4.40198875e-07
Iter: 1408 loss: 4.40181338e-07
Iter: 1409 loss: 4.40149051e-07
Iter: 1410 loss: 4.40114263e-07
Iter: 1411 loss: 4.40081834e-07
Iter: 1412 loss: 4.4006552e-07
Iter: 1413 loss: 4.40002111e-07
Iter: 1414 loss: 4.40105879e-07
Iter: 1415 loss: 4.39963657e-07
Iter: 1416 loss: 4.39910195e-07
Iter: 1417 loss: 4.39818479e-07
Iter: 1418 loss: 4.39811942e-07
Iter: 1419 loss: 4.3968825e-07
Iter: 1420 loss: 4.39918608e-07
Iter: 1421 loss: 4.39644168e-07
Iter: 1422 loss: 4.39553219e-07
Iter: 1423 loss: 4.39846474e-07
Iter: 1424 loss: 4.39537075e-07
Iter: 1425 loss: 4.39427026e-07
Iter: 1426 loss: 4.39881035e-07
Iter: 1427 loss: 4.3940139e-07
Iter: 1428 loss: 4.39318796e-07
Iter: 1429 loss: 4.39359837e-07
Iter: 1430 loss: 4.39250016e-07
Iter: 1431 loss: 4.39150142e-07
Iter: 1432 loss: 4.4007632e-07
Iter: 1433 loss: 4.39159692e-07
Iter: 1434 loss: 4.39036739e-07
Iter: 1435 loss: 4.39266614e-07
Iter: 1436 loss: 4.39001639e-07
Iter: 1437 loss: 4.38929135e-07
Iter: 1438 loss: 4.38868142e-07
Iter: 1439 loss: 4.38856603e-07
Iter: 1440 loss: 4.38845262e-07
Iter: 1441 loss: 4.38817892e-07
Iter: 1442 loss: 4.38784667e-07
Iter: 1443 loss: 4.38651398e-07
Iter: 1444 loss: 4.39206616e-07
Iter: 1445 loss: 4.38624852e-07
Iter: 1446 loss: 4.38553172e-07
Iter: 1447 loss: 4.3854385e-07
Iter: 1448 loss: 4.38465662e-07
Iter: 1449 loss: 4.38522221e-07
Iter: 1450 loss: 4.38401486e-07
Iter: 1451 loss: 4.38348366e-07
Iter: 1452 loss: 4.38455231e-07
Iter: 1453 loss: 4.38318068e-07
Iter: 1454 loss: 4.38255626e-07
Iter: 1455 loss: 4.38969408e-07
Iter: 1456 loss: 4.38254574e-07
Iter: 1457 loss: 4.3821521e-07
Iter: 1458 loss: 4.38185282e-07
Iter: 1459 loss: 4.38174141e-07
Iter: 1460 loss: 4.38107634e-07
Iter: 1461 loss: 4.38328186e-07
Iter: 1462 loss: 4.38094105e-07
Iter: 1463 loss: 4.38032771e-07
Iter: 1464 loss: 4.3812588e-07
Iter: 1465 loss: 4.38004463e-07
Iter: 1466 loss: 4.37935512e-07
Iter: 1467 loss: 4.38769604e-07
Iter: 1468 loss: 4.37933835e-07
Iter: 1469 loss: 4.3789629e-07
Iter: 1470 loss: 4.37803891e-07
Iter: 1471 loss: 4.39213807e-07
Iter: 1472 loss: 4.3779653e-07
Iter: 1473 loss: 4.37739516e-07
Iter: 1474 loss: 4.37728488e-07
Iter: 1475 loss: 4.37685344e-07
Iter: 1476 loss: 4.37626539e-07
Iter: 1477 loss: 4.37627705e-07
Iter: 1478 loss: 4.37583907e-07
Iter: 1479 loss: 4.37582912e-07
Iter: 1480 loss: 4.37538461e-07
Iter: 1481 loss: 4.37669087e-07
Iter: 1482 loss: 4.37532577e-07
Iter: 1483 loss: 4.37501e-07
Iter: 1484 loss: 4.37465218e-07
Iter: 1485 loss: 4.37462802e-07
Iter: 1486 loss: 4.3741656e-07
Iter: 1487 loss: 4.37415736e-07
Iter: 1488 loss: 4.37370261e-07
Iter: 1489 loss: 4.37277947e-07
Iter: 1490 loss: 4.38272906e-07
Iter: 1491 loss: 4.37263395e-07
Iter: 1492 loss: 4.371621e-07
Iter: 1493 loss: 4.37871961e-07
Iter: 1494 loss: 4.37142546e-07
Iter: 1495 loss: 4.3707e-07
Iter: 1496 loss: 4.37190607e-07
Iter: 1497 loss: 4.37044122e-07
Iter: 1498 loss: 4.36959226e-07
Iter: 1499 loss: 4.37730279e-07
Iter: 1500 loss: 4.36968236e-07
Iter: 1501 loss: 4.36917929e-07
Iter: 1502 loss: 4.36782e-07
Iter: 1503 loss: 4.38414077e-07
Iter: 1504 loss: 4.36793016e-07
Iter: 1505 loss: 4.36725315e-07
Iter: 1506 loss: 4.36714487e-07
Iter: 1507 loss: 4.36643177e-07
Iter: 1508 loss: 4.36620724e-07
Iter: 1509 loss: 4.36566211e-07
Iter: 1510 loss: 4.36480974e-07
Iter: 1511 loss: 4.36719318e-07
Iter: 1512 loss: 4.36457611e-07
Iter: 1513 loss: 4.36381981e-07
Iter: 1514 loss: 4.37135782e-07
Iter: 1515 loss: 4.36375387e-07
Iter: 1516 loss: 4.36325706e-07
Iter: 1517 loss: 4.36300894e-07
Iter: 1518 loss: 4.36263548e-07
Iter: 1519 loss: 4.3621435e-07
Iter: 1520 loss: 4.368639e-07
Iter: 1521 loss: 4.36223132e-07
Iter: 1522 loss: 4.36178141e-07
Iter: 1523 loss: 4.36142841e-07
Iter: 1524 loss: 4.36125447e-07
Iter: 1525 loss: 4.36069939e-07
Iter: 1526 loss: 4.36180414e-07
Iter: 1527 loss: 4.36040523e-07
Iter: 1528 loss: 4.35973334e-07
Iter: 1529 loss: 4.36090261e-07
Iter: 1530 loss: 4.35943662e-07
Iter: 1531 loss: 4.35878235e-07
Iter: 1532 loss: 4.35885369e-07
Iter: 1533 loss: 4.3583151e-07
Iter: 1534 loss: 4.35698865e-07
Iter: 1535 loss: 4.37428127e-07
Iter: 1536 loss: 4.35707932e-07
Iter: 1537 loss: 4.35632757e-07
Iter: 1538 loss: 4.36328492e-07
Iter: 1539 loss: 4.35620279e-07
Iter: 1540 loss: 4.35556444e-07
Iter: 1541 loss: 4.35552124e-07
Iter: 1542 loss: 4.35532229e-07
Iter: 1543 loss: 4.35488175e-07
Iter: 1544 loss: 4.3654785e-07
Iter: 1545 loss: 4.3548863e-07
Iter: 1546 loss: 4.35427722e-07
Iter: 1547 loss: 4.36260677e-07
Iter: 1548 loss: 4.3543335e-07
Iter: 1549 loss: 4.35396771e-07
Iter: 1550 loss: 4.35354167e-07
Iter: 1551 loss: 4.35358743e-07
Iter: 1552 loss: 4.35306e-07
Iter: 1553 loss: 4.35597968e-07
Iter: 1554 loss: 4.3529468e-07
Iter: 1555 loss: 4.35243038e-07
Iter: 1556 loss: 4.35199496e-07
Iter: 1557 loss: 4.3518682e-07
Iter: 1558 loss: 4.35095615e-07
Iter: 1559 loss: 4.3534078e-07
Iter: 1560 loss: 4.35065687e-07
Iter: 1561 loss: 4.34982724e-07
Iter: 1562 loss: 4.35345214e-07
Iter: 1563 loss: 4.34986163e-07
Iter: 1564 loss: 4.34947054e-07
Iter: 1565 loss: 4.35516881e-07
Iter: 1566 loss: 4.34948163e-07
Iter: 1567 loss: 4.34891149e-07
Iter: 1568 loss: 4.34854599e-07
Iter: 1569 loss: 4.34856133e-07
Iter: 1570 loss: 4.34796192e-07
Iter: 1571 loss: 4.34651156e-07
Iter: 1572 loss: 4.3748355e-07
Iter: 1573 loss: 4.34651611e-07
Iter: 1574 loss: 4.34680658e-07
Iter: 1575 loss: 4.34603976e-07
Iter: 1576 loss: 4.34573849e-07
Iter: 1577 loss: 4.34470167e-07
Iter: 1578 loss: 4.34961976e-07
Iter: 1579 loss: 4.34445042e-07
Iter: 1580 loss: 4.34481535e-07
Iter: 1581 loss: 4.34409685e-07
Iter: 1582 loss: 4.34390444e-07
Iter: 1583 loss: 4.3433937e-07
Iter: 1584 loss: 4.35032092e-07
Iter: 1585 loss: 4.34326694e-07
Iter: 1586 loss: 4.3426914e-07
Iter: 1587 loss: 4.34422361e-07
Iter: 1588 loss: 4.34231339e-07
Iter: 1589 loss: 4.34130186e-07
Iter: 1590 loss: 4.34800768e-07
Iter: 1591 loss: 4.34126548e-07
Iter: 1592 loss: 4.34076554e-07
Iter: 1593 loss: 4.34064162e-07
Iter: 1594 loss: 4.34021473e-07
Iter: 1595 loss: 4.33941068e-07
Iter: 1596 loss: 4.34224944e-07
Iter: 1597 loss: 4.33924981e-07
Iter: 1598 loss: 4.33827e-07
Iter: 1599 loss: 4.34088577e-07
Iter: 1600 loss: 4.33807429e-07
Iter: 1601 loss: 4.33748966e-07
Iter: 1602 loss: 4.34793208e-07
Iter: 1603 loss: 4.33738393e-07
Iter: 1604 loss: 4.33683681e-07
Iter: 1605 loss: 4.33572296e-07
Iter: 1606 loss: 4.34526385e-07
Iter: 1607 loss: 4.33549616e-07
Iter: 1608 loss: 4.33426521e-07
Iter: 1609 loss: 4.34223779e-07
Iter: 1610 loss: 4.33404722e-07
Iter: 1611 loss: 4.33298766e-07
Iter: 1612 loss: 4.34722836e-07
Iter: 1613 loss: 4.33299192e-07
Iter: 1614 loss: 4.33241496e-07
Iter: 1615 loss: 4.33087877e-07
Iter: 1616 loss: 4.35128356e-07
Iter: 1617 loss: 4.33095551e-07
Iter: 1618 loss: 4.33111381e-07
Iter: 1619 loss: 4.3303865e-07
Iter: 1620 loss: 4.33003549e-07
Iter: 1621 loss: 4.32922889e-07
Iter: 1622 loss: 4.34086871e-07
Iter: 1623 loss: 4.32916977e-07
Iter: 1624 loss: 4.32868546e-07
Iter: 1625 loss: 4.32865875e-07
Iter: 1626 loss: 4.32815057e-07
Iter: 1627 loss: 4.32821196e-07
Iter: 1628 loss: 4.32774129e-07
Iter: 1629 loss: 4.32725869e-07
Iter: 1630 loss: 4.32775948e-07
Iter: 1631 loss: 4.3269867e-07
Iter: 1632 loss: 4.32644782e-07
Iter: 1633 loss: 4.32916295e-07
Iter: 1634 loss: 4.32627786e-07
Iter: 1635 loss: 4.3257424e-07
Iter: 1636 loss: 4.33290666e-07
Iter: 1637 loss: 4.32571909e-07
Iter: 1638 loss: 4.32533596e-07
Iter: 1639 loss: 4.3246709e-07
Iter: 1640 loss: 4.3246331e-07
Iter: 1641 loss: 4.32397769e-07
Iter: 1642 loss: 4.32477e-07
Iter: 1643 loss: 4.32363976e-07
Iter: 1644 loss: 4.32330211e-07
Iter: 1645 loss: 4.3233797e-07
Iter: 1646 loss: 4.32291046e-07
Iter: 1647 loss: 4.32215415e-07
Iter: 1648 loss: 4.3377031e-07
Iter: 1649 loss: 4.32204075e-07
Iter: 1650 loss: 4.32184038e-07
Iter: 1651 loss: 4.32174318e-07
Iter: 1652 loss: 4.32133817e-07
Iter: 1653 loss: 4.32078167e-07
Iter: 1654 loss: 4.32073676e-07
Iter: 1655 loss: 4.32032436e-07
Iter: 1656 loss: 4.31907893e-07
Iter: 1657 loss: 4.33883685e-07
Iter: 1658 loss: 4.31888111e-07
Iter: 1659 loss: 4.32063814e-07
Iter: 1660 loss: 4.3184582e-07
Iter: 1661 loss: 4.31835957e-07
Iter: 1662 loss: 4.31814641e-07
Iter: 1663 loss: 4.32555368e-07
Iter: 1664 loss: 4.31807564e-07
Iter: 1665 loss: 4.31763823e-07
Iter: 1666 loss: 4.3176334e-07
Iter: 1667 loss: 4.31713715e-07
Iter: 1668 loss: 4.31675204e-07
Iter: 1669 loss: 4.31650278e-07
Iter: 1670 loss: 4.31630923e-07
Iter: 1671 loss: 4.3156146e-07
Iter: 1672 loss: 4.31563819e-07
Iter: 1673 loss: 4.31522608e-07
Iter: 1674 loss: 4.31475257e-07
Iter: 1675 loss: 4.31471051e-07
Iter: 1676 loss: 4.31383057e-07
Iter: 1677 loss: 4.31464485e-07
Iter: 1678 loss: 4.31353044e-07
Iter: 1679 loss: 4.31265391e-07
Iter: 1680 loss: 4.3127551e-07
Iter: 1681 loss: 4.31218325e-07
Iter: 1682 loss: 4.31253227e-07
Iter: 1683 loss: 4.31186152e-07
Iter: 1684 loss: 4.31147782e-07
Iter: 1685 loss: 4.31202551e-07
Iter: 1686 loss: 4.31130729e-07
Iter: 1687 loss: 4.31073943e-07
Iter: 1688 loss: 4.31113676e-07
Iter: 1689 loss: 4.31020737e-07
Iter: 1690 loss: 4.30952468e-07
Iter: 1691 loss: 4.30844125e-07
Iter: 1692 loss: 4.32926356e-07
Iter: 1693 loss: 4.30840771e-07
Iter: 1694 loss: 4.30708269e-07
Iter: 1695 loss: 4.31201386e-07
Iter: 1696 loss: 4.30674902e-07
Iter: 1697 loss: 4.30539416e-07
Iter: 1698 loss: 4.31416282e-07
Iter: 1699 loss: 4.30528161e-07
Iter: 1700 loss: 4.30417543e-07
Iter: 1701 loss: 4.31630212e-07
Iter: 1702 loss: 4.30412229e-07
Iter: 1703 loss: 4.30335717e-07
Iter: 1704 loss: 4.30624e-07
Iter: 1705 loss: 4.30310706e-07
Iter: 1706 loss: 4.30265914e-07
Iter: 1707 loss: 4.30376417e-07
Iter: 1708 loss: 4.30247184e-07
Iter: 1709 loss: 4.30173912e-07
Iter: 1710 loss: 4.30343e-07
Iter: 1711 loss: 4.30151658e-07
Iter: 1712 loss: 4.30102773e-07
Iter: 1713 loss: 4.30092427e-07
Iter: 1714 loss: 4.30040046e-07
Iter: 1715 loss: 4.29962228e-07
Iter: 1716 loss: 4.3065927e-07
Iter: 1717 loss: 4.29969134e-07
Iter: 1718 loss: 4.29915588e-07
Iter: 1719 loss: 4.30144269e-07
Iter: 1720 loss: 4.29907175e-07
Iter: 1721 loss: 4.29877616e-07
Iter: 1722 loss: 4.2976751e-07
Iter: 1723 loss: 4.30328612e-07
Iter: 1724 loss: 4.29741846e-07
Iter: 1725 loss: 4.29666898e-07
Iter: 1726 loss: 4.29665789e-07
Iter: 1727 loss: 4.29576488e-07
Iter: 1728 loss: 4.29946965e-07
Iter: 1729 loss: 4.29565631e-07
Iter: 1730 loss: 4.2951666e-07
Iter: 1731 loss: 4.29441684e-07
Iter: 1732 loss: 4.31070077e-07
Iter: 1733 loss: 4.29431e-07
Iter: 1734 loss: 4.29375262e-07
Iter: 1735 loss: 4.29369891e-07
Iter: 1736 loss: 4.29319897e-07
Iter: 1737 loss: 4.29476245e-07
Iter: 1738 loss: 4.29307192e-07
Iter: 1739 loss: 4.29274223e-07
Iter: 1740 loss: 4.29409965e-07
Iter: 1741 loss: 4.29258478e-07
Iter: 1742 loss: 4.29214253e-07
Iter: 1743 loss: 4.29390468e-07
Iter: 1744 loss: 4.29189072e-07
Iter: 1745 loss: 4.29150759e-07
Iter: 1746 loss: 4.29143796e-07
Iter: 1747 loss: 4.29112248e-07
Iter: 1748 loss: 4.29056939e-07
Iter: 1749 loss: 4.29060606e-07
Iter: 1750 loss: 4.29008281e-07
Iter: 1751 loss: 4.28932196e-07
Iter: 1752 loss: 4.28924295e-07
Iter: 1753 loss: 4.28881719e-07
Iter: 1754 loss: 4.28811177e-07
Iter: 1755 loss: 4.28813735e-07
Iter: 1756 loss: 4.28721506e-07
Iter: 1757 loss: 4.28873335e-07
Iter: 1758 loss: 4.28698741e-07
Iter: 1759 loss: 4.28661735e-07
Iter: 1760 loss: 4.2864616e-07
Iter: 1761 loss: 4.28622172e-07
Iter: 1762 loss: 4.28562458e-07
Iter: 1763 loss: 4.29442764e-07
Iter: 1764 loss: 4.28551743e-07
Iter: 1765 loss: 4.28501721e-07
Iter: 1766 loss: 4.28794237e-07
Iter: 1767 loss: 4.28495582e-07
Iter: 1768 loss: 4.28433765e-07
Iter: 1769 loss: 4.289613e-07
Iter: 1770 loss: 4.28445247e-07
Iter: 1771 loss: 4.28405627e-07
Iter: 1772 loss: 4.28415149e-07
Iter: 1773 loss: 4.2838289e-07
Iter: 1774 loss: 4.28336932e-07
Iter: 1775 loss: 4.28550152e-07
Iter: 1776 loss: 4.28327667e-07
Iter: 1777 loss: 4.28278781e-07
Iter: 1778 loss: 4.28317549e-07
Iter: 1779 loss: 4.28243425e-07
Iter: 1780 loss: 4.28189196e-07
Iter: 1781 loss: 4.28218385e-07
Iter: 1782 loss: 4.28153555e-07
Iter: 1783 loss: 4.28118369e-07
Iter: 1784 loss: 4.28115925e-07
Iter: 1785 loss: 4.28074543e-07
Iter: 1786 loss: 4.28023952e-07
Iter: 1787 loss: 4.28016733e-07
Iter: 1788 loss: 4.27953182e-07
Iter: 1789 loss: 4.27945281e-07
Iter: 1790 loss: 4.27911857e-07
Iter: 1791 loss: 4.27889404e-07
Iter: 1792 loss: 4.27876273e-07
Iter: 1793 loss: 4.27835261e-07
Iter: 1794 loss: 4.27757982e-07
Iter: 1795 loss: 4.27756703e-07
Iter: 1796 loss: 4.27695397e-07
Iter: 1797 loss: 4.27863824e-07
Iter: 1798 loss: 4.2767536e-07
Iter: 1799 loss: 4.27637644e-07
Iter: 1800 loss: 4.276433e-07
Iter: 1801 loss: 4.27618772e-07
Iter: 1802 loss: 4.27588759e-07
Iter: 1803 loss: 4.27572502e-07
Iter: 1804 loss: 4.27528562e-07
Iter: 1805 loss: 4.27871043e-07
Iter: 1806 loss: 4.27535412e-07
Iter: 1807 loss: 4.27497184e-07
Iter: 1808 loss: 4.27587821e-07
Iter: 1809 loss: 4.27487578e-07
Iter: 1810 loss: 4.27458872e-07
Iter: 1811 loss: 4.27429796e-07
Iter: 1812 loss: 4.27419934e-07
Iter: 1813 loss: 4.27373777e-07
Iter: 1814 loss: 4.27738485e-07
Iter: 1815 loss: 4.27361925e-07
Iter: 1816 loss: 4.27307526e-07
Iter: 1817 loss: 4.27383696e-07
Iter: 1818 loss: 4.27282487e-07
Iter: 1819 loss: 4.27228457e-07
Iter: 1820 loss: 4.27170534e-07
Iter: 1821 loss: 4.27168288e-07
Iter: 1822 loss: 4.27126452e-07
Iter: 1823 loss: 4.27109967e-07
Iter: 1824 loss: 4.27067619e-07
Iter: 1825 loss: 4.27108745e-07
Iter: 1826 loss: 4.27040845e-07
Iter: 1827 loss: 4.26996564e-07
Iter: 1828 loss: 4.26945348e-07
Iter: 1829 loss: 4.26936424e-07
Iter: 1830 loss: 4.26904734e-07
Iter: 1831 loss: 4.26900556e-07
Iter: 1832 loss: 4.26852068e-07
Iter: 1833 loss: 4.2682808e-07
Iter: 1834 loss: 4.26812733e-07
Iter: 1835 loss: 4.26753672e-07
Iter: 1836 loss: 4.26979426e-07
Iter: 1837 loss: 4.26738723e-07
Iter: 1838 loss: 4.26677104e-07
Iter: 1839 loss: 4.26936225e-07
Iter: 1840 loss: 4.2665863e-07
Iter: 1841 loss: 4.26617817e-07
Iter: 1842 loss: 4.26667384e-07
Iter: 1843 loss: 4.26600906e-07
Iter: 1844 loss: 4.26555459e-07
Iter: 1845 loss: 4.26677644e-07
Iter: 1846 loss: 4.26537696e-07
Iter: 1847 loss: 4.26485798e-07
Iter: 1848 loss: 4.26777262e-07
Iter: 1849 loss: 4.26479943e-07
Iter: 1850 loss: 4.26445979e-07
Iter: 1851 loss: 4.26370036e-07
Iter: 1852 loss: 4.26373646e-07
Iter: 1853 loss: 4.2631919e-07
Iter: 1854 loss: 4.2673264e-07
Iter: 1855 loss: 4.26296424e-07
Iter: 1856 loss: 4.26250438e-07
Iter: 1857 loss: 4.2694063e-07
Iter: 1858 loss: 4.26256e-07
Iter: 1859 loss: 4.26222499e-07
Iter: 1860 loss: 4.26158209e-07
Iter: 1861 loss: 4.26631516e-07
Iter: 1862 loss: 4.26143629e-07
Iter: 1863 loss: 4.26070585e-07
Iter: 1864 loss: 4.26839421e-07
Iter: 1865 loss: 4.2606257e-07
Iter: 1866 loss: 4.25992937e-07
Iter: 1867 loss: 4.26549661e-07
Iter: 1868 loss: 4.25994443e-07
Iter: 1869 loss: 4.2595147e-07
Iter: 1870 loss: 4.25901874e-07
Iter: 1871 loss: 4.25907416e-07
Iter: 1872 loss: 4.25844775e-07
Iter: 1873 loss: 4.26471757e-07
Iter: 1874 loss: 4.25838323e-07
Iter: 1875 loss: 4.25794838e-07
Iter: 1876 loss: 4.25735294e-07
Iter: 1877 loss: 4.25726739e-07
Iter: 1878 loss: 4.25635733e-07
Iter: 1879 loss: 4.25995609e-07
Iter: 1880 loss: 4.25620215e-07
Iter: 1881 loss: 4.25536967e-07
Iter: 1882 loss: 4.26139877e-07
Iter: 1883 loss: 4.25531965e-07
Iter: 1884 loss: 4.25479101e-07
Iter: 1885 loss: 4.25428709e-07
Iter: 1886 loss: 4.25406711e-07
Iter: 1887 loss: 4.25352141e-07
Iter: 1888 loss: 4.25512923e-07
Iter: 1889 loss: 4.2532551e-07
Iter: 1890 loss: 4.25282281e-07
Iter: 1891 loss: 4.25274465e-07
Iter: 1892 loss: 4.25250278e-07
Iter: 1893 loss: 4.25196731e-07
Iter: 1894 loss: 4.25776619e-07
Iter: 1895 loss: 4.25167684e-07
Iter: 1896 loss: 4.25105441e-07
Iter: 1897 loss: 4.25537337e-07
Iter: 1898 loss: 4.25111637e-07
Iter: 1899 loss: 4.25056669e-07
Iter: 1900 loss: 4.2573248e-07
Iter: 1901 loss: 4.25050018e-07
Iter: 1902 loss: 4.25009e-07
Iter: 1903 loss: 4.24924195e-07
Iter: 1904 loss: 4.26439811e-07
Iter: 1905 loss: 4.24921978e-07
Iter: 1906 loss: 4.24875907e-07
Iter: 1907 loss: 4.24861867e-07
Iter: 1908 loss: 4.24814715e-07
Iter: 1909 loss: 4.24811844e-07
Iter: 1910 loss: 4.24780296e-07
Iter: 1911 loss: 4.24744883e-07
Iter: 1912 loss: 4.24942243e-07
Iter: 1913 loss: 4.24727489e-07
Iter: 1914 loss: 4.24682867e-07
Iter: 1915 loss: 4.24928601e-07
Iter: 1916 loss: 4.24671953e-07
Iter: 1917 loss: 4.24622272e-07
Iter: 1918 loss: 4.24655326e-07
Iter: 1919 loss: 4.24607492e-07
Iter: 1920 loss: 4.24565e-07
Iter: 1921 loss: 4.24539138e-07
Iter: 1922 loss: 4.24523876e-07
Iter: 1923 loss: 4.24482067e-07
Iter: 1924 loss: 4.24479651e-07
Iter: 1925 loss: 4.2443861e-07
Iter: 1926 loss: 4.24504663e-07
Iter: 1927 loss: 4.24409933e-07
Iter: 1928 loss: 4.24396148e-07
Iter: 1929 loss: 4.24339902e-07
Iter: 1930 loss: 4.25001218e-07
Iter: 1931 loss: 4.24330892e-07
Iter: 1932 loss: 4.24283314e-07
Iter: 1933 loss: 4.24282e-07
Iter: 1934 loss: 4.2422198e-07
Iter: 1935 loss: 4.2416346e-07
Iter: 1936 loss: 4.24152404e-07
Iter: 1937 loss: 4.24076546e-07
Iter: 1938 loss: 4.24656605e-07
Iter: 1939 loss: 4.2407973e-07
Iter: 1940 loss: 4.24024563e-07
Iter: 1941 loss: 4.24114489e-07
Iter: 1942 loss: 4.240012e-07
Iter: 1943 loss: 4.23964536e-07
Iter: 1944 loss: 4.2407467e-07
Iter: 1945 loss: 4.23931965e-07
Iter: 1946 loss: 4.23894733e-07
Iter: 1947 loss: 4.24145838e-07
Iter: 1948 loss: 4.23885751e-07
Iter: 1949 loss: 4.23824304e-07
Iter: 1950 loss: 4.2393583e-07
Iter: 1951 loss: 4.23803328e-07
Iter: 1952 loss: 4.23778033e-07
Iter: 1953 loss: 4.23731933e-07
Iter: 1954 loss: 4.2372335e-07
Iter: 1955 loss: 4.23672702e-07
Iter: 1956 loss: 4.23992645e-07
Iter: 1957 loss: 4.23666e-07
Iter: 1958 loss: 4.23623646e-07
Iter: 1959 loss: 4.24269274e-07
Iter: 1960 loss: 4.23620577e-07
Iter: 1961 loss: 4.23589256e-07
Iter: 1962 loss: 4.23489666e-07
Iter: 1963 loss: 4.24077513e-07
Iter: 1964 loss: 4.23472642e-07
Iter: 1965 loss: 4.23384904e-07
Iter: 1966 loss: 4.24203733e-07
Iter: 1967 loss: 4.23380783e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.8
+ date
Mon Oct 26 12:58:54 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.8/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.8_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 1 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.8_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa96440d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9643c2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9643e3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa964490e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa96433f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa96433fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa96431fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9642a4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9642a9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9642a9488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa964255950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa964223e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9641ee9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9642238c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9641cfae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa964168c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa96418a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa96417f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa96415bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9640fa730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa964103510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9640ce620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9640ae048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9640830d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa964083048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa964062378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa963fd2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa963fd26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa96401e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa96401e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa963f9f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa963f9f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa93a2e91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa93a283598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa93a2b3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa93a26ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.06394884e-06
Iter: 2 loss: 3.27128123e-06
Iter: 3 loss: 3.05095e-06
Iter: 4 loss: 2.6535813e-06
Iter: 5 loss: 3.00313263e-06
Iter: 6 loss: 2.42059377e-06
Iter: 7 loss: 2.26729162e-06
Iter: 8 loss: 3.28128635e-06
Iter: 9 loss: 2.25140911e-06
Iter: 10 loss: 2.16283979e-06
Iter: 11 loss: 2.87142279e-06
Iter: 12 loss: 2.15694581e-06
Iter: 13 loss: 2.09393625e-06
Iter: 14 loss: 2.04890057e-06
Iter: 15 loss: 2.02694446e-06
Iter: 16 loss: 1.99623378e-06
Iter: 17 loss: 1.99255919e-06
Iter: 18 loss: 1.96322162e-06
Iter: 19 loss: 1.93085862e-06
Iter: 20 loss: 1.92619314e-06
Iter: 21 loss: 1.8750693e-06
Iter: 22 loss: 1.83175143e-06
Iter: 23 loss: 1.81750238e-06
Iter: 24 loss: 1.76117214e-06
Iter: 25 loss: 1.75438697e-06
Iter: 26 loss: 1.73146373e-06
Iter: 27 loss: 1.67232929e-06
Iter: 28 loss: 2.16105877e-06
Iter: 29 loss: 1.66191489e-06
Iter: 30 loss: 1.61146545e-06
Iter: 31 loss: 1.79074163e-06
Iter: 32 loss: 1.5985521e-06
Iter: 33 loss: 1.55675161e-06
Iter: 34 loss: 1.66171344e-06
Iter: 35 loss: 1.54209306e-06
Iter: 36 loss: 1.50575465e-06
Iter: 37 loss: 1.71189686e-06
Iter: 38 loss: 1.50074436e-06
Iter: 39 loss: 1.44963838e-06
Iter: 40 loss: 1.59225146e-06
Iter: 41 loss: 1.43315481e-06
Iter: 42 loss: 1.40958252e-06
Iter: 43 loss: 1.37234235e-06
Iter: 44 loss: 1.37195627e-06
Iter: 45 loss: 1.35294795e-06
Iter: 46 loss: 1.34536e-06
Iter: 47 loss: 1.32484161e-06
Iter: 48 loss: 1.28917645e-06
Iter: 49 loss: 1.28918464e-06
Iter: 50 loss: 1.26444525e-06
Iter: 51 loss: 1.26441546e-06
Iter: 52 loss: 1.24593748e-06
Iter: 53 loss: 1.30332501e-06
Iter: 54 loss: 1.24055077e-06
Iter: 55 loss: 1.22211486e-06
Iter: 56 loss: 1.20841401e-06
Iter: 57 loss: 1.20231107e-06
Iter: 58 loss: 1.17963668e-06
Iter: 59 loss: 1.3627141e-06
Iter: 60 loss: 1.17814886e-06
Iter: 61 loss: 1.15262833e-06
Iter: 62 loss: 1.19175661e-06
Iter: 63 loss: 1.14056922e-06
Iter: 64 loss: 1.11944109e-06
Iter: 65 loss: 1.08439747e-06
Iter: 66 loss: 1.08428776e-06
Iter: 67 loss: 1.05189974e-06
Iter: 68 loss: 1.09311577e-06
Iter: 69 loss: 1.03522439e-06
Iter: 70 loss: 1.01981789e-06
Iter: 71 loss: 1.0167654e-06
Iter: 72 loss: 9.99316399e-07
Iter: 73 loss: 1.11126747e-06
Iter: 74 loss: 9.97403617e-07
Iter: 75 loss: 9.8744863e-07
Iter: 76 loss: 9.64601895e-07
Iter: 77 loss: 1.25543318e-06
Iter: 78 loss: 9.62993e-07
Iter: 79 loss: 9.56453e-07
Iter: 80 loss: 9.51718846e-07
Iter: 81 loss: 9.40353061e-07
Iter: 82 loss: 9.25892891e-07
Iter: 83 loss: 9.24812241e-07
Iter: 84 loss: 9.14705652e-07
Iter: 85 loss: 9.96390099e-07
Iter: 86 loss: 9.14036207e-07
Iter: 87 loss: 9.01994269e-07
Iter: 88 loss: 9.11492236e-07
Iter: 89 loss: 8.94725758e-07
Iter: 90 loss: 8.85363079e-07
Iter: 91 loss: 9.07657636e-07
Iter: 92 loss: 8.81985727e-07
Iter: 93 loss: 8.72725082e-07
Iter: 94 loss: 9.23625691e-07
Iter: 95 loss: 8.71393411e-07
Iter: 96 loss: 8.61905335e-07
Iter: 97 loss: 8.90934e-07
Iter: 98 loss: 8.59059355e-07
Iter: 99 loss: 8.521572e-07
Iter: 100 loss: 8.36735637e-07
Iter: 101 loss: 1.05234801e-06
Iter: 102 loss: 8.35927779e-07
Iter: 103 loss: 8.21815945e-07
Iter: 104 loss: 8.9239586e-07
Iter: 105 loss: 8.19428578e-07
Iter: 106 loss: 8.05958734e-07
Iter: 107 loss: 8.42794293e-07
Iter: 108 loss: 8.01528813e-07
Iter: 109 loss: 8.09352571e-07
Iter: 110 loss: 7.98130429e-07
Iter: 111 loss: 7.95532173e-07
Iter: 112 loss: 7.87608656e-07
Iter: 113 loss: 8.0673351e-07
Iter: 114 loss: 7.8307437e-07
Iter: 115 loss: 7.7521031e-07
Iter: 116 loss: 7.75217813e-07
Iter: 117 loss: 7.66278959e-07
Iter: 118 loss: 7.79327763e-07
Iter: 119 loss: 7.61957665e-07
Iter: 120 loss: 7.5618118e-07
Iter: 121 loss: 7.55692554e-07
Iter: 122 loss: 7.5138297e-07
Iter: 123 loss: 7.4671766e-07
Iter: 124 loss: 7.4626405e-07
Iter: 125 loss: 7.43058877e-07
Iter: 126 loss: 7.36308152e-07
Iter: 127 loss: 8.49437811e-07
Iter: 128 loss: 7.36144216e-07
Iter: 129 loss: 7.31128807e-07
Iter: 130 loss: 7.58419674e-07
Iter: 131 loss: 7.30389786e-07
Iter: 132 loss: 7.27895213e-07
Iter: 133 loss: 7.27609063e-07
Iter: 134 loss: 7.25664961e-07
Iter: 135 loss: 7.22235598e-07
Iter: 136 loss: 7.22232528e-07
Iter: 137 loss: 7.16959903e-07
Iter: 138 loss: 7.17110765e-07
Iter: 139 loss: 7.12759061e-07
Iter: 140 loss: 7.06270555e-07
Iter: 141 loss: 7.10438712e-07
Iter: 142 loss: 7.02127181e-07
Iter: 143 loss: 6.95274593e-07
Iter: 144 loss: 7.52826281e-07
Iter: 145 loss: 6.9491e-07
Iter: 146 loss: 6.89656531e-07
Iter: 147 loss: 6.89651642e-07
Iter: 148 loss: 6.87244665e-07
Iter: 149 loss: 6.82165705e-07
Iter: 150 loss: 7.64123399e-07
Iter: 151 loss: 6.8202155e-07
Iter: 152 loss: 6.79915502e-07
Iter: 153 loss: 6.79587856e-07
Iter: 154 loss: 6.76878187e-07
Iter: 155 loss: 6.73230602e-07
Iter: 156 loss: 6.73028637e-07
Iter: 157 loss: 6.69466715e-07
Iter: 158 loss: 6.7680287e-07
Iter: 159 loss: 6.68039206e-07
Iter: 160 loss: 6.63856554e-07
Iter: 161 loss: 7.06906178e-07
Iter: 162 loss: 6.63721266e-07
Iter: 163 loss: 6.61426327e-07
Iter: 164 loss: 6.55533199e-07
Iter: 165 loss: 7.07862e-07
Iter: 166 loss: 6.54638e-07
Iter: 167 loss: 6.51359e-07
Iter: 168 loss: 6.51090318e-07
Iter: 169 loss: 6.47171817e-07
Iter: 170 loss: 6.54999553e-07
Iter: 171 loss: 6.45563603e-07
Iter: 172 loss: 6.43930434e-07
Iter: 173 loss: 6.41270674e-07
Iter: 174 loss: 6.41258907e-07
Iter: 175 loss: 6.37669075e-07
Iter: 176 loss: 6.48049308e-07
Iter: 177 loss: 6.36527545e-07
Iter: 178 loss: 6.32102115e-07
Iter: 179 loss: 6.4689e-07
Iter: 180 loss: 6.30886802e-07
Iter: 181 loss: 6.28807697e-07
Iter: 182 loss: 6.285876e-07
Iter: 183 loss: 6.2648985e-07
Iter: 184 loss: 6.22277241e-07
Iter: 185 loss: 7.02209377e-07
Iter: 186 loss: 6.22248649e-07
Iter: 187 loss: 6.18924958e-07
Iter: 188 loss: 6.40479698e-07
Iter: 189 loss: 6.18578611e-07
Iter: 190 loss: 6.16408784e-07
Iter: 191 loss: 6.1640452e-07
Iter: 192 loss: 6.15557269e-07
Iter: 193 loss: 6.13408304e-07
Iter: 194 loss: 6.30807e-07
Iter: 195 loss: 6.12988117e-07
Iter: 196 loss: 6.12193787e-07
Iter: 197 loss: 6.11534631e-07
Iter: 198 loss: 6.10642928e-07
Iter: 199 loss: 6.08068376e-07
Iter: 200 loss: 6.19359867e-07
Iter: 201 loss: 6.07092659e-07
Iter: 202 loss: 6.04964043e-07
Iter: 203 loss: 6.04800448e-07
Iter: 204 loss: 6.02553769e-07
Iter: 205 loss: 6.04774584e-07
Iter: 206 loss: 6.01291958e-07
Iter: 207 loss: 5.9948286e-07
Iter: 208 loss: 5.96342829e-07
Iter: 209 loss: 5.96346126e-07
Iter: 210 loss: 5.94183518e-07
Iter: 211 loss: 6.12153485e-07
Iter: 212 loss: 5.94049766e-07
Iter: 213 loss: 5.93106392e-07
Iter: 214 loss: 5.92911817e-07
Iter: 215 loss: 5.91935418e-07
Iter: 216 loss: 5.92740832e-07
Iter: 217 loss: 5.913422e-07
Iter: 218 loss: 5.90108584e-07
Iter: 219 loss: 5.87588261e-07
Iter: 220 loss: 6.35400568e-07
Iter: 221 loss: 5.87586101e-07
Iter: 222 loss: 5.85896373e-07
Iter: 223 loss: 5.85791895e-07
Iter: 224 loss: 5.83980295e-07
Iter: 225 loss: 5.83425503e-07
Iter: 226 loss: 5.8236219e-07
Iter: 227 loss: 5.80496248e-07
Iter: 228 loss: 5.81833319e-07
Iter: 229 loss: 5.79329821e-07
Iter: 230 loss: 5.77190804e-07
Iter: 231 loss: 6.10819029e-07
Iter: 232 loss: 5.77194669e-07
Iter: 233 loss: 5.76434218e-07
Iter: 234 loss: 5.75179115e-07
Iter: 235 loss: 5.75170873e-07
Iter: 236 loss: 5.7464905e-07
Iter: 237 loss: 5.7440792e-07
Iter: 238 loss: 5.73738816e-07
Iter: 239 loss: 5.71955582e-07
Iter: 240 loss: 5.83185169e-07
Iter: 241 loss: 5.71496457e-07
Iter: 242 loss: 5.69653253e-07
Iter: 243 loss: 5.71782721e-07
Iter: 244 loss: 5.68688506e-07
Iter: 245 loss: 5.66799713e-07
Iter: 246 loss: 5.67628945e-07
Iter: 247 loss: 5.65518917e-07
Iter: 248 loss: 5.65168193e-07
Iter: 249 loss: 5.64606125e-07
Iter: 250 loss: 5.63672643e-07
Iter: 251 loss: 5.63624781e-07
Iter: 252 loss: 5.62905598e-07
Iter: 253 loss: 5.62132755e-07
Iter: 254 loss: 5.63595108e-07
Iter: 255 loss: 5.61817217e-07
Iter: 256 loss: 5.60876401e-07
Iter: 257 loss: 5.68368591e-07
Iter: 258 loss: 5.60793183e-07
Iter: 259 loss: 5.60148919e-07
Iter: 260 loss: 5.58974932e-07
Iter: 261 loss: 5.86153874e-07
Iter: 262 loss: 5.58974534e-07
Iter: 263 loss: 5.5789684e-07
Iter: 264 loss: 5.74055377e-07
Iter: 265 loss: 5.57883e-07
Iter: 266 loss: 5.56882753e-07
Iter: 267 loss: 5.57986937e-07
Iter: 268 loss: 5.56356099e-07
Iter: 269 loss: 5.55148461e-07
Iter: 270 loss: 5.55203e-07
Iter: 271 loss: 5.54205e-07
Iter: 272 loss: 5.53274958e-07
Iter: 273 loss: 5.53272514e-07
Iter: 274 loss: 5.52416964e-07
Iter: 275 loss: 5.51797257e-07
Iter: 276 loss: 5.51519292e-07
Iter: 277 loss: 5.50551192e-07
Iter: 278 loss: 5.49021593e-07
Iter: 279 loss: 5.49017386e-07
Iter: 280 loss: 5.47024115e-07
Iter: 281 loss: 5.53636937e-07
Iter: 282 loss: 5.46467732e-07
Iter: 283 loss: 5.46705564e-07
Iter: 284 loss: 5.456792e-07
Iter: 285 loss: 5.44967065e-07
Iter: 286 loss: 5.43317697e-07
Iter: 287 loss: 5.61629804e-07
Iter: 288 loss: 5.43154556e-07
Iter: 289 loss: 5.42101816e-07
Iter: 290 loss: 5.57862336e-07
Iter: 291 loss: 5.42089253e-07
Iter: 292 loss: 5.41062889e-07
Iter: 293 loss: 5.43929559e-07
Iter: 294 loss: 5.40750818e-07
Iter: 295 loss: 5.40091378e-07
Iter: 296 loss: 5.39342238e-07
Iter: 297 loss: 5.39237249e-07
Iter: 298 loss: 5.38456732e-07
Iter: 299 loss: 5.38437689e-07
Iter: 300 loss: 5.37676215e-07
Iter: 301 loss: 5.36974426e-07
Iter: 302 loss: 5.36809125e-07
Iter: 303 loss: 5.35800098e-07
Iter: 304 loss: 5.4089503e-07
Iter: 305 loss: 5.35648326e-07
Iter: 306 loss: 5.34776177e-07
Iter: 307 loss: 5.39854341e-07
Iter: 308 loss: 5.34682954e-07
Iter: 309 loss: 5.33767206e-07
Iter: 310 loss: 5.32273361e-07
Iter: 311 loss: 5.32268302e-07
Iter: 312 loss: 5.31138e-07
Iter: 313 loss: 5.33508342e-07
Iter: 314 loss: 5.30691693e-07
Iter: 315 loss: 5.29592853e-07
Iter: 316 loss: 5.32853164e-07
Iter: 317 loss: 5.29251281e-07
Iter: 318 loss: 5.29113436e-07
Iter: 319 loss: 5.28724684e-07
Iter: 320 loss: 5.28338546e-07
Iter: 321 loss: 5.27256816e-07
Iter: 322 loss: 5.33229581e-07
Iter: 323 loss: 5.26941392e-07
Iter: 324 loss: 5.26404904e-07
Iter: 325 loss: 5.26312533e-07
Iter: 326 loss: 5.25619953e-07
Iter: 327 loss: 5.2488673e-07
Iter: 328 loss: 5.24755819e-07
Iter: 329 loss: 5.23972801e-07
Iter: 330 loss: 5.2511416e-07
Iter: 331 loss: 5.23625886e-07
Iter: 332 loss: 5.22984465e-07
Iter: 333 loss: 5.22957748e-07
Iter: 334 loss: 5.22547e-07
Iter: 335 loss: 5.22014943e-07
Iter: 336 loss: 5.21973107e-07
Iter: 337 loss: 5.21252161e-07
Iter: 338 loss: 5.25959535e-07
Iter: 339 loss: 5.21173774e-07
Iter: 340 loss: 5.20366143e-07
Iter: 341 loss: 5.20804178e-07
Iter: 342 loss: 5.19828404e-07
Iter: 343 loss: 5.1894358e-07
Iter: 344 loss: 5.20039407e-07
Iter: 345 loss: 5.18495085e-07
Iter: 346 loss: 5.17658236e-07
Iter: 347 loss: 5.17070248e-07
Iter: 348 loss: 5.16780403e-07
Iter: 349 loss: 5.15568217e-07
Iter: 350 loss: 5.19926516e-07
Iter: 351 loss: 5.15225679e-07
Iter: 352 loss: 5.15452712e-07
Iter: 353 loss: 5.14735859e-07
Iter: 354 loss: 5.14459487e-07
Iter: 355 loss: 5.13720693e-07
Iter: 356 loss: 5.18032834e-07
Iter: 357 loss: 5.13514124e-07
Iter: 358 loss: 5.12682846e-07
Iter: 359 loss: 5.19480864e-07
Iter: 360 loss: 5.12638337e-07
Iter: 361 loss: 5.11729695e-07
Iter: 362 loss: 5.15084935e-07
Iter: 363 loss: 5.11484927e-07
Iter: 364 loss: 5.11016196e-07
Iter: 365 loss: 5.10135692e-07
Iter: 366 loss: 5.28248165e-07
Iter: 367 loss: 5.10120458e-07
Iter: 368 loss: 5.09894392e-07
Iter: 369 loss: 5.09602273e-07
Iter: 370 loss: 5.09200788e-07
Iter: 371 loss: 5.08711196e-07
Iter: 372 loss: 5.08672e-07
Iter: 373 loss: 5.08273331e-07
Iter: 374 loss: 5.08274525e-07
Iter: 375 loss: 5.07946879e-07
Iter: 376 loss: 5.07621735e-07
Iter: 377 loss: 5.07526636e-07
Iter: 378 loss: 5.0702738e-07
Iter: 379 loss: 5.08008839e-07
Iter: 380 loss: 5.06809556e-07
Iter: 381 loss: 5.06243055e-07
Iter: 382 loss: 5.06530284e-07
Iter: 383 loss: 5.0585254e-07
Iter: 384 loss: 5.05106755e-07
Iter: 385 loss: 5.07413858e-07
Iter: 386 loss: 5.04873583e-07
Iter: 387 loss: 5.04041111e-07
Iter: 388 loss: 5.15344254e-07
Iter: 389 loss: 5.04040258e-07
Iter: 390 loss: 5.03679644e-07
Iter: 391 loss: 5.02785383e-07
Iter: 392 loss: 5.10704695e-07
Iter: 393 loss: 5.02639637e-07
Iter: 394 loss: 5.02073135e-07
Iter: 395 loss: 5.02049147e-07
Iter: 396 loss: 5.01512e-07
Iter: 397 loss: 5.04011041e-07
Iter: 398 loss: 5.01408522e-07
Iter: 399 loss: 5.01158695e-07
Iter: 400 loss: 5.00533133e-07
Iter: 401 loss: 5.06392496e-07
Iter: 402 loss: 5.00421777e-07
Iter: 403 loss: 5.00073611e-07
Iter: 404 loss: 4.99998237e-07
Iter: 405 loss: 4.99512566e-07
Iter: 406 loss: 4.99070325e-07
Iter: 407 loss: 4.98950271e-07
Iter: 408 loss: 4.98394286e-07
Iter: 409 loss: 4.98675945e-07
Iter: 410 loss: 4.98016675e-07
Iter: 411 loss: 4.97758663e-07
Iter: 412 loss: 4.97648e-07
Iter: 413 loss: 4.97335e-07
Iter: 414 loss: 4.96631856e-07
Iter: 415 loss: 5.05088906e-07
Iter: 416 loss: 4.96555913e-07
Iter: 417 loss: 4.9601033e-07
Iter: 418 loss: 4.97152143e-07
Iter: 419 loss: 4.95794438e-07
Iter: 420 loss: 4.95350776e-07
Iter: 421 loss: 4.95347763e-07
Iter: 422 loss: 4.94942242e-07
Iter: 423 loss: 4.97008159e-07
Iter: 424 loss: 4.94894834e-07
Iter: 425 loss: 4.94547635e-07
Iter: 426 loss: 4.94142284e-07
Iter: 427 loss: 4.94114943e-07
Iter: 428 loss: 4.93619041e-07
Iter: 429 loss: 4.94992833e-07
Iter: 430 loss: 4.93469e-07
Iter: 431 loss: 4.92967843e-07
Iter: 432 loss: 4.98936515e-07
Iter: 433 loss: 4.92959089e-07
Iter: 434 loss: 4.92702441e-07
Iter: 435 loss: 4.92264519e-07
Iter: 436 loss: 5.0347478e-07
Iter: 437 loss: 4.92264e-07
Iter: 438 loss: 4.91826029e-07
Iter: 439 loss: 4.92643608e-07
Iter: 440 loss: 4.91643618e-07
Iter: 441 loss: 4.91396747e-07
Iter: 442 loss: 4.913922e-07
Iter: 443 loss: 4.91046535e-07
Iter: 444 loss: 4.90508171e-07
Iter: 445 loss: 4.90491971e-07
Iter: 446 loss: 4.90109528e-07
Iter: 447 loss: 4.95016479e-07
Iter: 448 loss: 4.90098614e-07
Iter: 449 loss: 4.89690649e-07
Iter: 450 loss: 4.89900913e-07
Iter: 451 loss: 4.89425531e-07
Iter: 452 loss: 4.89027457e-07
Iter: 453 loss: 4.88636772e-07
Iter: 454 loss: 4.88545083e-07
Iter: 455 loss: 4.88118644e-07
Iter: 456 loss: 4.90402954e-07
Iter: 457 loss: 4.88041564e-07
Iter: 458 loss: 4.87717045e-07
Iter: 459 loss: 4.87711816e-07
Iter: 460 loss: 4.87575903e-07
Iter: 461 loss: 4.87180159e-07
Iter: 462 loss: 4.88934461e-07
Iter: 463 loss: 4.8703339e-07
Iter: 464 loss: 4.86707052e-07
Iter: 465 loss: 4.8668187e-07
Iter: 466 loss: 4.86337683e-07
Iter: 467 loss: 4.86407316e-07
Iter: 468 loss: 4.86080921e-07
Iter: 469 loss: 4.85707517e-07
Iter: 470 loss: 4.8605483e-07
Iter: 471 loss: 4.85488385e-07
Iter: 472 loss: 4.85064675e-07
Iter: 473 loss: 4.85670625e-07
Iter: 474 loss: 4.84858333e-07
Iter: 475 loss: 4.84539157e-07
Iter: 476 loss: 4.88414287e-07
Iter: 477 loss: 4.84544614e-07
Iter: 478 loss: 4.84253064e-07
Iter: 479 loss: 4.85713713e-07
Iter: 480 loss: 4.84213842e-07
Iter: 481 loss: 4.84040754e-07
Iter: 482 loss: 4.83779559e-07
Iter: 483 loss: 4.83761653e-07
Iter: 484 loss: 4.83364829e-07
Iter: 485 loss: 4.87228e-07
Iter: 486 loss: 4.83353745e-07
Iter: 487 loss: 4.83134272e-07
Iter: 488 loss: 4.82599319e-07
Iter: 489 loss: 4.87425325e-07
Iter: 490 loss: 4.82526616e-07
Iter: 491 loss: 4.81939765e-07
Iter: 492 loss: 4.86142937e-07
Iter: 493 loss: 4.81897644e-07
Iter: 494 loss: 4.81675556e-07
Iter: 495 loss: 4.81601205e-07
Iter: 496 loss: 4.8144932e-07
Iter: 497 loss: 4.81057e-07
Iter: 498 loss: 4.85142209e-07
Iter: 499 loss: 4.81002417e-07
Iter: 500 loss: 4.80783399e-07
Iter: 501 loss: 4.83867211e-07
Iter: 502 loss: 4.8077959e-07
Iter: 503 loss: 4.80524477e-07
Iter: 504 loss: 4.80818926e-07
Iter: 505 loss: 4.80379526e-07
Iter: 506 loss: 4.80133906e-07
Iter: 507 loss: 4.79832352e-07
Iter: 508 loss: 4.79801201e-07
Iter: 509 loss: 4.79318714e-07
Iter: 510 loss: 4.81505424e-07
Iter: 511 loss: 4.79220716e-07
Iter: 512 loss: 4.78785864e-07
Iter: 513 loss: 4.78910124e-07
Iter: 514 loss: 4.78460663e-07
Iter: 515 loss: 4.78340382e-07
Iter: 516 loss: 4.78207539e-07
Iter: 517 loss: 4.78014499e-07
Iter: 518 loss: 4.77823846e-07
Iter: 519 loss: 4.77784738e-07
Iter: 520 loss: 4.77590788e-07
Iter: 521 loss: 4.79973437e-07
Iter: 522 loss: 4.77587264e-07
Iter: 523 loss: 4.77384219e-07
Iter: 524 loss: 4.7715173e-07
Iter: 525 loss: 4.77126662e-07
Iter: 526 loss: 4.7686683e-07
Iter: 527 loss: 4.76666742e-07
Iter: 528 loss: 4.76575821e-07
Iter: 529 loss: 4.7627168e-07
Iter: 530 loss: 4.80738379e-07
Iter: 531 loss: 4.7627131e-07
Iter: 532 loss: 4.75913083e-07
Iter: 533 loss: 4.77642743e-07
Iter: 534 loss: 4.75848765e-07
Iter: 535 loss: 4.75669594e-07
Iter: 536 loss: 4.75304716e-07
Iter: 537 loss: 4.80824042e-07
Iter: 538 loss: 4.75289283e-07
Iter: 539 loss: 4.74888822e-07
Iter: 540 loss: 4.7671756e-07
Iter: 541 loss: 4.74814158e-07
Iter: 542 loss: 4.74586017e-07
Iter: 543 loss: 4.74568083e-07
Iter: 544 loss: 4.74382375e-07
Iter: 545 loss: 4.73996522e-07
Iter: 546 loss: 4.8076538e-07
Iter: 547 loss: 4.74002604e-07
Iter: 548 loss: 4.73635879e-07
Iter: 549 loss: 4.75559403e-07
Iter: 550 loss: 4.73588671e-07
Iter: 551 loss: 4.73272848e-07
Iter: 552 loss: 4.7413775e-07
Iter: 553 loss: 4.73141199e-07
Iter: 554 loss: 4.72814065e-07
Iter: 555 loss: 4.75926e-07
Iter: 556 loss: 4.72800167e-07
Iter: 557 loss: 4.72603944e-07
Iter: 558 loss: 4.72316515e-07
Iter: 559 loss: 4.72308272e-07
Iter: 560 loss: 4.71943736e-07
Iter: 561 loss: 4.77604146e-07
Iter: 562 loss: 4.71949278e-07
Iter: 563 loss: 4.71760131e-07
Iter: 564 loss: 4.71489e-07
Iter: 565 loss: 4.71484952e-07
Iter: 566 loss: 4.71200195e-07
Iter: 567 loss: 4.72332943e-07
Iter: 568 loss: 4.71126157e-07
Iter: 569 loss: 4.70792372e-07
Iter: 570 loss: 4.73378918e-07
Iter: 571 loss: 4.70786603e-07
Iter: 572 loss: 4.70623121e-07
Iter: 573 loss: 4.70266627e-07
Iter: 574 loss: 4.76076309e-07
Iter: 575 loss: 4.70270777e-07
Iter: 576 loss: 4.70063185e-07
Iter: 577 loss: 4.70056818e-07
Iter: 578 loss: 4.69843229e-07
Iter: 579 loss: 4.69772772e-07
Iter: 580 loss: 4.69645727e-07
Iter: 581 loss: 4.69368445e-07
Iter: 582 loss: 4.69708084e-07
Iter: 583 loss: 4.69225398e-07
Iter: 584 loss: 4.68953573e-07
Iter: 585 loss: 4.69788517e-07
Iter: 586 loss: 4.68856371e-07
Iter: 587 loss: 4.68622687e-07
Iter: 588 loss: 4.71351e-07
Iter: 589 loss: 4.68626e-07
Iter: 590 loss: 4.68410377e-07
Iter: 591 loss: 4.68579856e-07
Iter: 592 loss: 4.68281399e-07
Iter: 593 loss: 4.68056328e-07
Iter: 594 loss: 4.68242206e-07
Iter: 595 loss: 4.67907682e-07
Iter: 596 loss: 4.67530072e-07
Iter: 597 loss: 4.68585029e-07
Iter: 598 loss: 4.67400724e-07
Iter: 599 loss: 4.6719e-07
Iter: 600 loss: 4.66915878e-07
Iter: 601 loss: 4.66894164e-07
Iter: 602 loss: 4.66816516e-07
Iter: 603 loss: 4.66715619e-07
Iter: 604 loss: 4.6656055e-07
Iter: 605 loss: 4.66344204e-07
Iter: 606 loss: 4.66333717e-07
Iter: 607 loss: 4.66110208e-07
Iter: 608 loss: 4.67180172e-07
Iter: 609 loss: 4.66087101e-07
Iter: 610 loss: 4.65876411e-07
Iter: 611 loss: 4.66754727e-07
Iter: 612 loss: 4.65828521e-07
Iter: 613 loss: 4.65665664e-07
Iter: 614 loss: 4.65501728e-07
Iter: 615 loss: 4.65476575e-07
Iter: 616 loss: 4.65168426e-07
Iter: 617 loss: 4.65821358e-07
Iter: 618 loss: 4.65060339e-07
Iter: 619 loss: 4.64754663e-07
Iter: 620 loss: 4.66317545e-07
Iter: 621 loss: 4.64699099e-07
Iter: 622 loss: 4.6437e-07
Iter: 623 loss: 4.65874905e-07
Iter: 624 loss: 4.64324387e-07
Iter: 625 loss: 4.64076237e-07
Iter: 626 loss: 4.63845083e-07
Iter: 627 loss: 4.63774654e-07
Iter: 628 loss: 4.63620268e-07
Iter: 629 loss: 4.63581614e-07
Iter: 630 loss: 4.63452409e-07
Iter: 631 loss: 4.63212643e-07
Iter: 632 loss: 4.66664972e-07
Iter: 633 loss: 4.63205794e-07
Iter: 634 loss: 4.62998457e-07
Iter: 635 loss: 4.65191135e-07
Iter: 636 loss: 4.62994194e-07
Iter: 637 loss: 4.62735045e-07
Iter: 638 loss: 4.63057461e-07
Iter: 639 loss: 4.62595921e-07
Iter: 640 loss: 4.62411521e-07
Iter: 641 loss: 4.62193555e-07
Iter: 642 loss: 4.62174398e-07
Iter: 643 loss: 4.61851158e-07
Iter: 644 loss: 4.66716671e-07
Iter: 645 loss: 4.61848487e-07
Iter: 646 loss: 4.61629327e-07
Iter: 647 loss: 4.61383706e-07
Iter: 648 loss: 4.61360742e-07
Iter: 649 loss: 4.61048472e-07
Iter: 650 loss: 4.62269554e-07
Iter: 651 loss: 4.60980743e-07
Iter: 652 loss: 4.60725744e-07
Iter: 653 loss: 4.61356876e-07
Iter: 654 loss: 4.60641246e-07
Iter: 655 loss: 4.60468755e-07
Iter: 656 loss: 4.60471767e-07
Iter: 657 loss: 4.60341084e-07
Iter: 658 loss: 4.60182406e-07
Iter: 659 loss: 4.60168451e-07
Iter: 660 loss: 4.59990929e-07
Iter: 661 loss: 4.61486025e-07
Iter: 662 loss: 4.59988598e-07
Iter: 663 loss: 4.59768557e-07
Iter: 664 loss: 4.59677722e-07
Iter: 665 loss: 4.5957762e-07
Iter: 666 loss: 4.59319835e-07
Iter: 667 loss: 4.5923133e-07
Iter: 668 loss: 4.59102637e-07
Iter: 669 loss: 4.58942196e-07
Iter: 670 loss: 4.58865145e-07
Iter: 671 loss: 4.58730369e-07
Iter: 672 loss: 4.58438166e-07
Iter: 673 loss: 4.61765e-07
Iter: 674 loss: 4.58409318e-07
Iter: 675 loss: 4.58357107e-07
Iter: 676 loss: 4.58263457e-07
Iter: 677 loss: 4.58168586e-07
Iter: 678 loss: 4.58063226e-07
Iter: 679 loss: 4.58030627e-07
Iter: 680 loss: 4.57862512e-07
Iter: 681 loss: 4.57914496e-07
Iter: 682 loss: 4.57738054e-07
Iter: 683 loss: 4.57497606e-07
Iter: 684 loss: 4.58847524e-07
Iter: 685 loss: 4.57457674e-07
Iter: 686 loss: 4.57273075e-07
Iter: 687 loss: 4.5917028e-07
Iter: 688 loss: 4.57266083e-07
Iter: 689 loss: 4.57088277e-07
Iter: 690 loss: 4.56896856e-07
Iter: 691 loss: 4.56864598e-07
Iter: 692 loss: 4.56565886e-07
Iter: 693 loss: 4.57339297e-07
Iter: 694 loss: 4.56470048e-07
Iter: 695 loss: 4.56152804e-07
Iter: 696 loss: 4.59616444e-07
Iter: 697 loss: 4.56136434e-07
Iter: 698 loss: 4.55997139e-07
Iter: 699 loss: 4.55792843e-07
Iter: 700 loss: 4.55782185e-07
Iter: 701 loss: 4.55758368e-07
Iter: 702 loss: 4.55697744e-07
Iter: 703 loss: 4.55606198e-07
Iter: 704 loss: 4.55380956e-07
Iter: 705 loss: 4.58255727e-07
Iter: 706 loss: 4.55378284e-07
Iter: 707 loss: 4.55224722e-07
Iter: 708 loss: 4.57215521e-07
Iter: 709 loss: 4.55222448e-07
Iter: 710 loss: 4.55025599e-07
Iter: 711 loss: 4.54839579e-07
Iter: 712 loss: 4.54789983e-07
Iter: 713 loss: 4.54495876e-07
Iter: 714 loss: 4.54698039e-07
Iter: 715 loss: 4.54296412e-07
Iter: 716 loss: 4.5397644e-07
Iter: 717 loss: 4.55282134e-07
Iter: 718 loss: 4.53902601e-07
Iter: 719 loss: 4.53680286e-07
Iter: 720 loss: 4.53666985e-07
Iter: 721 loss: 4.5353471e-07
Iter: 722 loss: 4.53510665e-07
Iter: 723 loss: 4.53403e-07
Iter: 724 loss: 4.53227642e-07
Iter: 725 loss: 4.53667496e-07
Iter: 726 loss: 4.5318464e-07
Iter: 727 loss: 4.53055975e-07
Iter: 728 loss: 4.54677235e-07
Iter: 729 loss: 4.53049381e-07
Iter: 730 loss: 4.52941833e-07
Iter: 731 loss: 4.52732934e-07
Iter: 732 loss: 4.55718208e-07
Iter: 733 loss: 4.52720144e-07
Iter: 734 loss: 4.5241481e-07
Iter: 735 loss: 4.52479441e-07
Iter: 736 loss: 4.5220321e-07
Iter: 737 loss: 4.51983624e-07
Iter: 738 loss: 4.51897506e-07
Iter: 739 loss: 4.51793085e-07
Iter: 740 loss: 4.51538824e-07
Iter: 741 loss: 4.53732213e-07
Iter: 742 loss: 4.51487836e-07
Iter: 743 loss: 4.51451399e-07
Iter: 744 loss: 4.51326628e-07
Iter: 745 loss: 4.5124429e-07
Iter: 746 loss: 4.51120883e-07
Iter: 747 loss: 4.51111475e-07
Iter: 748 loss: 4.50952342e-07
Iter: 749 loss: 4.51187873e-07
Iter: 750 loss: 4.50854827e-07
Iter: 751 loss: 4.506403e-07
Iter: 752 loss: 4.51154136e-07
Iter: 753 loss: 4.50575101e-07
Iter: 754 loss: 4.50397266e-07
Iter: 755 loss: 4.52051353e-07
Iter: 756 loss: 4.50375e-07
Iter: 757 loss: 4.5019047e-07
Iter: 758 loss: 4.5091349e-07
Iter: 759 loss: 4.50144626e-07
Iter: 760 loss: 4.49972333e-07
Iter: 761 loss: 4.49926745e-07
Iter: 762 loss: 4.49830281e-07
Iter: 763 loss: 4.49614873e-07
Iter: 764 loss: 4.51564745e-07
Iter: 765 loss: 4.49608365e-07
Iter: 766 loss: 4.49488255e-07
Iter: 767 loss: 4.49315451e-07
Iter: 768 loss: 4.49300074e-07
Iter: 769 loss: 4.49105016e-07
Iter: 770 loss: 4.49474499e-07
Iter: 771 loss: 4.49000595e-07
Iter: 772 loss: 4.49011395e-07
Iter: 773 loss: 4.48933577e-07
Iter: 774 loss: 4.48855189e-07
Iter: 775 loss: 4.4866681e-07
Iter: 776 loss: 4.50302792e-07
Iter: 777 loss: 4.4864089e-07
Iter: 778 loss: 4.48423577e-07
Iter: 779 loss: 4.4829136e-07
Iter: 780 loss: 4.48208425e-07
Iter: 781 loss: 4.47919803e-07
Iter: 782 loss: 4.50724315e-07
Iter: 783 loss: 4.4790221e-07
Iter: 784 loss: 4.47787272e-07
Iter: 785 loss: 4.47771527e-07
Iter: 786 loss: 4.47675461e-07
Iter: 787 loss: 4.47515532e-07
Iter: 788 loss: 4.47531136e-07
Iter: 789 loss: 4.47380586e-07
Iter: 790 loss: 4.47947798e-07
Iter: 791 loss: 4.47360122e-07
Iter: 792 loss: 4.47225858e-07
Iter: 793 loss: 4.48315689e-07
Iter: 794 loss: 4.47222249e-07
Iter: 795 loss: 4.47107652e-07
Iter: 796 loss: 4.47144771e-07
Iter: 797 loss: 4.47037706e-07
Iter: 798 loss: 4.46879312e-07
Iter: 799 loss: 4.4697191e-07
Iter: 800 loss: 4.46779779e-07
Iter: 801 loss: 4.46595578e-07
Iter: 802 loss: 4.48279962e-07
Iter: 803 loss: 4.46583954e-07
Iter: 804 loss: 4.46430647e-07
Iter: 805 loss: 4.46222487e-07
Iter: 806 loss: 4.46210777e-07
Iter: 807 loss: 4.46205291e-07
Iter: 808 loss: 4.46136312e-07
Iter: 809 loss: 4.46054287e-07
Iter: 810 loss: 4.45864032e-07
Iter: 811 loss: 4.48158858e-07
Iter: 812 loss: 4.45865339e-07
Iter: 813 loss: 4.45701488e-07
Iter: 814 loss: 4.45801589e-07
Iter: 815 loss: 4.45591354e-07
Iter: 816 loss: 4.45409029e-07
Iter: 817 loss: 4.45879721e-07
Iter: 818 loss: 4.45329903e-07
Iter: 819 loss: 4.45196179e-07
Iter: 820 loss: 4.45184185e-07
Iter: 821 loss: 4.45062113e-07
Iter: 822 loss: 4.44859097e-07
Iter: 823 loss: 4.44854919e-07
Iter: 824 loss: 4.44675663e-07
Iter: 825 loss: 4.45571828e-07
Iter: 826 loss: 4.44651732e-07
Iter: 827 loss: 4.44515308e-07
Iter: 828 loss: 4.45895807e-07
Iter: 829 loss: 4.44514285e-07
Iter: 830 loss: 4.44395539e-07
Iter: 831 loss: 4.44493224e-07
Iter: 832 loss: 4.44327782e-07
Iter: 833 loss: 4.44230949e-07
Iter: 834 loss: 4.44522357e-07
Iter: 835 loss: 4.44215715e-07
Iter: 836 loss: 4.44099896e-07
Iter: 837 loss: 4.44504735e-07
Iter: 838 loss: 4.44063915e-07
Iter: 839 loss: 4.43967224e-07
Iter: 840 loss: 4.4391868e-07
Iter: 841 loss: 4.43891167e-07
Iter: 842 loss: 4.4381278e-07
Iter: 843 loss: 4.43797262e-07
Iter: 844 loss: 4.4373769e-07
Iter: 845 loss: 4.43567046e-07
Iter: 846 loss: 4.44461364e-07
Iter: 847 loss: 4.43499516e-07
Iter: 848 loss: 4.4326552e-07
Iter: 849 loss: 4.43739111e-07
Iter: 850 loss: 4.43181818e-07
Iter: 851 loss: 4.42987243e-07
Iter: 852 loss: 4.45856784e-07
Iter: 853 loss: 4.42987556e-07
Iter: 854 loss: 4.42819328e-07
Iter: 855 loss: 4.43890571e-07
Iter: 856 loss: 4.42801024e-07
Iter: 857 loss: 4.42712633e-07
Iter: 858 loss: 4.4260392e-07
Iter: 859 loss: 4.42602527e-07
Iter: 860 loss: 4.424964e-07
Iter: 861 loss: 4.44067183e-07
Iter: 862 loss: 4.42493445e-07
Iter: 863 loss: 4.42392547e-07
Iter: 864 loss: 4.42599372e-07
Iter: 865 loss: 4.42361795e-07
Iter: 866 loss: 4.42254702e-07
Iter: 867 loss: 4.42264167e-07
Iter: 868 loss: 4.42176912e-07
Iter: 869 loss: 4.42033524e-07
Iter: 870 loss: 4.42820976e-07
Iter: 871 loss: 4.42013771e-07
Iter: 872 loss: 4.41878e-07
Iter: 873 loss: 4.41923078e-07
Iter: 874 loss: 4.41774262e-07
Iter: 875 loss: 4.41659921e-07
Iter: 876 loss: 4.42558417e-07
Iter: 877 loss: 4.41652077e-07
Iter: 878 loss: 4.41509343e-07
Iter: 879 loss: 4.41688826e-07
Iter: 880 loss: 4.4142547e-07
Iter: 881 loss: 4.41332475e-07
Iter: 882 loss: 4.41211e-07
Iter: 883 loss: 4.41193691e-07
Iter: 884 loss: 4.41051156e-07
Iter: 885 loss: 4.41289501e-07
Iter: 886 loss: 4.40986412e-07
Iter: 887 loss: 4.40917432e-07
Iter: 888 loss: 4.40893757e-07
Iter: 889 loss: 4.40791439e-07
Iter: 890 loss: 4.40655441e-07
Iter: 891 loss: 4.44202044e-07
Iter: 892 loss: 4.4064663e-07
Iter: 893 loss: 4.40514668e-07
Iter: 894 loss: 4.40678491e-07
Iter: 895 loss: 4.40418148e-07
Iter: 896 loss: 4.40286613e-07
Iter: 897 loss: 4.40275e-07
Iter: 898 loss: 4.401968e-07
Iter: 899 loss: 4.40248471e-07
Iter: 900 loss: 4.40131089e-07
Iter: 901 loss: 4.40031044e-07
Iter: 902 loss: 4.40356757e-07
Iter: 903 loss: 4.39995262e-07
Iter: 904 loss: 4.39893711e-07
Iter: 905 loss: 4.40090332e-07
Iter: 906 loss: 4.39852499e-07
Iter: 907 loss: 4.39736112e-07
Iter: 908 loss: 4.39734777e-07
Iter: 909 loss: 4.39658805e-07
Iter: 910 loss: 4.3952133e-07
Iter: 911 loss: 4.39528264e-07
Iter: 912 loss: 4.39438196e-07
Iter: 913 loss: 4.39232025e-07
Iter: 914 loss: 4.41272e-07
Iter: 915 loss: 4.39188227e-07
Iter: 916 loss: 4.38993538e-07
Iter: 917 loss: 4.3946028e-07
Iter: 918 loss: 4.38922058e-07
Iter: 919 loss: 4.38781655e-07
Iter: 920 loss: 4.38784468e-07
Iter: 921 loss: 4.38634515e-07
Iter: 922 loss: 4.38970119e-07
Iter: 923 loss: 4.38590916e-07
Iter: 924 loss: 4.38489053e-07
Iter: 925 loss: 4.38348707e-07
Iter: 926 loss: 4.3834018e-07
Iter: 927 loss: 4.38224959e-07
Iter: 928 loss: 4.38227573e-07
Iter: 929 loss: 4.38084754e-07
Iter: 930 loss: 4.38117382e-07
Iter: 931 loss: 4.37983147e-07
Iter: 932 loss: 4.37828817e-07
Iter: 933 loss: 4.38339157e-07
Iter: 934 loss: 4.37770751e-07
Iter: 935 loss: 4.37633958e-07
Iter: 936 loss: 4.38390515e-07
Iter: 937 loss: 4.37622788e-07
Iter: 938 loss: 4.37519418e-07
Iter: 939 loss: 4.37547897e-07
Iter: 940 loss: 4.37450524e-07
Iter: 941 loss: 4.37340645e-07
Iter: 942 loss: 4.38415015e-07
Iter: 943 loss: 4.37349627e-07
Iter: 944 loss: 4.372381e-07
Iter: 945 loss: 4.37293949e-07
Iter: 946 loss: 4.3716355e-07
Iter: 947 loss: 4.3707459e-07
Iter: 948 loss: 4.36931145e-07
Iter: 949 loss: 4.36933306e-07
Iter: 950 loss: 4.36729692e-07
Iter: 951 loss: 4.36815668e-07
Iter: 952 loss: 4.36573146e-07
Iter: 953 loss: 4.36490495e-07
Iter: 954 loss: 4.36420947e-07
Iter: 955 loss: 4.36313655e-07
Iter: 956 loss: 4.36113766e-07
Iter: 957 loss: 4.40374151e-07
Iter: 958 loss: 4.36107541e-07
Iter: 959 loss: 4.35938858e-07
Iter: 960 loss: 4.36398693e-07
Iter: 961 loss: 4.35883635e-07
Iter: 962 loss: 4.35780464e-07
Iter: 963 loss: 4.35771113e-07
Iter: 964 loss: 4.35698723e-07
Iter: 965 loss: 4.35640345e-07
Iter: 966 loss: 4.35619114e-07
Iter: 967 loss: 4.35515375e-07
Iter: 968 loss: 4.36164413e-07
Iter: 969 loss: 4.35499771e-07
Iter: 970 loss: 4.35392053e-07
Iter: 971 loss: 4.35450914e-07
Iter: 972 loss: 4.35328644e-07
Iter: 973 loss: 4.35194949e-07
Iter: 974 loss: 4.35185711e-07
Iter: 975 loss: 4.3509641e-07
Iter: 976 loss: 4.34935629e-07
Iter: 977 loss: 4.34930882e-07
Iter: 978 loss: 4.34815092e-07
Iter: 979 loss: 4.34619068e-07
Iter: 980 loss: 4.38608794e-07
Iter: 981 loss: 4.34623587e-07
Iter: 982 loss: 4.34451465e-07
Iter: 983 loss: 4.34809579e-07
Iter: 984 loss: 4.34384106e-07
Iter: 985 loss: 4.3430029e-07
Iter: 986 loss: 4.3430461e-07
Iter: 987 loss: 4.34201183e-07
Iter: 988 loss: 4.34317229e-07
Iter: 989 loss: 4.34151332e-07
Iter: 990 loss: 4.34065328e-07
Iter: 991 loss: 4.33919695e-07
Iter: 992 loss: 4.3749435e-07
Iter: 993 loss: 4.3391637e-07
Iter: 994 loss: 4.33764797e-07
Iter: 995 loss: 4.35685109e-07
Iter: 996 loss: 4.3375934e-07
Iter: 997 loss: 4.33608562e-07
Iter: 998 loss: 4.3399848e-07
Iter: 999 loss: 4.33550042e-07
Iter: 1000 loss: 4.33441812e-07
Iter: 1001 loss: 4.33679418e-07
Iter: 1002 loss: 4.3339179e-07
Iter: 1003 loss: 4.33268724e-07
Iter: 1004 loss: 4.33868252e-07
Iter: 1005 loss: 4.33241155e-07
Iter: 1006 loss: 4.33142787e-07
Iter: 1007 loss: 4.33240018e-07
Iter: 1008 loss: 4.33096858e-07
Iter: 1009 loss: 4.33013042e-07
Iter: 1010 loss: 4.33526e-07
Iter: 1011 loss: 4.3300841e-07
Iter: 1012 loss: 4.32913566e-07
Iter: 1013 loss: 4.3323891e-07
Iter: 1014 loss: 4.32886395e-07
Iter: 1015 loss: 4.3282347e-07
Iter: 1016 loss: 4.32730928e-07
Iter: 1017 loss: 4.34858208e-07
Iter: 1018 loss: 4.3272513e-07
Iter: 1019 loss: 4.32576599e-07
Iter: 1020 loss: 4.32772026e-07
Iter: 1021 loss: 4.32498581e-07
Iter: 1022 loss: 4.32425793e-07
Iter: 1023 loss: 4.3240351e-07
Iter: 1024 loss: 4.32324441e-07
Iter: 1025 loss: 4.32140098e-07
Iter: 1026 loss: 4.34715133e-07
Iter: 1027 loss: 4.32136574e-07
Iter: 1028 loss: 4.31999212e-07
Iter: 1029 loss: 4.32137057e-07
Iter: 1030 loss: 4.31906585e-07
Iter: 1031 loss: 4.31878959e-07
Iter: 1032 loss: 4.31823565e-07
Iter: 1033 loss: 4.31753818e-07
Iter: 1034 loss: 4.31701665e-07
Iter: 1035 loss: 4.31671253e-07
Iter: 1036 loss: 4.31613017e-07
Iter: 1037 loss: 4.32153456e-07
Iter: 1038 loss: 4.31616286e-07
Iter: 1039 loss: 4.31536904e-07
Iter: 1040 loss: 4.31436661e-07
Iter: 1041 loss: 4.31435183e-07
Iter: 1042 loss: 4.31311264e-07
Iter: 1043 loss: 4.31918863e-07
Iter: 1044 loss: 4.31283183e-07
Iter: 1045 loss: 4.31179672e-07
Iter: 1046 loss: 4.32112387e-07
Iter: 1047 loss: 4.31171145e-07
Iter: 1048 loss: 4.31089148e-07
Iter: 1049 loss: 4.30954486e-07
Iter: 1050 loss: 4.30943544e-07
Iter: 1051 loss: 4.30808939e-07
Iter: 1052 loss: 4.30895e-07
Iter: 1053 loss: 4.30718217e-07
Iter: 1054 loss: 4.30624596e-07
Iter: 1055 loss: 4.30631928e-07
Iter: 1056 loss: 4.30527166e-07
Iter: 1057 loss: 4.30742489e-07
Iter: 1058 loss: 4.30490445e-07
Iter: 1059 loss: 4.30416634e-07
Iter: 1060 loss: 4.30261764e-07
Iter: 1061 loss: 4.33864471e-07
Iter: 1062 loss: 4.3026256e-07
Iter: 1063 loss: 4.30098567e-07
Iter: 1064 loss: 4.3076605e-07
Iter: 1065 loss: 4.30077137e-07
Iter: 1066 loss: 4.29931049e-07
Iter: 1067 loss: 4.29922665e-07
Iter: 1068 loss: 4.29835495e-07
Iter: 1069 loss: 4.29676902e-07
Iter: 1070 loss: 4.3331417e-07
Iter: 1071 loss: 4.29690488e-07
Iter: 1072 loss: 4.29580211e-07
Iter: 1073 loss: 4.29574e-07
Iter: 1074 loss: 4.29499778e-07
Iter: 1075 loss: 4.29413149e-07
Iter: 1076 loss: 4.29397801e-07
Iter: 1077 loss: 4.2930418e-07
Iter: 1078 loss: 4.30287884e-07
Iter: 1079 loss: 4.29291418e-07
Iter: 1080 loss: 4.29232756e-07
Iter: 1081 loss: 4.29756312e-07
Iter: 1082 loss: 4.29229289e-07
Iter: 1083 loss: 4.29173383e-07
Iter: 1084 loss: 4.29051113e-07
Iter: 1085 loss: 4.30570481e-07
Iter: 1086 loss: 4.29041e-07
Iter: 1087 loss: 4.28907867e-07
Iter: 1088 loss: 4.29180346e-07
Iter: 1089 loss: 4.28856083e-07
Iter: 1090 loss: 4.28766924e-07
Iter: 1091 loss: 4.28759421e-07
Iter: 1092 loss: 4.28675264e-07
Iter: 1093 loss: 4.28532161e-07
Iter: 1094 loss: 4.32040281e-07
Iter: 1095 loss: 4.28535145e-07
Iter: 1096 loss: 4.28393719e-07
Iter: 1097 loss: 4.28652868e-07
Iter: 1098 loss: 4.28340627e-07
Iter: 1099 loss: 4.28299671e-07
Iter: 1100 loss: 4.28287905e-07
Iter: 1101 loss: 4.28228475e-07
Iter: 1102 loss: 4.28208182e-07
Iter: 1103 loss: 4.28173308e-07
Iter: 1104 loss: 4.28105238e-07
Iter: 1105 loss: 4.28179192e-07
Iter: 1106 loss: 4.28084718e-07
Iter: 1107 loss: 4.27974044e-07
Iter: 1108 loss: 4.28338581e-07
Iter: 1109 loss: 4.27957332e-07
Iter: 1110 loss: 4.27857572e-07
Iter: 1111 loss: 4.27793964e-07
Iter: 1112 loss: 4.2776179e-07
Iter: 1113 loss: 4.27652509e-07
Iter: 1114 loss: 4.29503586e-07
Iter: 1115 loss: 4.27649894e-07
Iter: 1116 loss: 4.27552e-07
Iter: 1117 loss: 4.27590209e-07
Iter: 1118 loss: 4.27468933e-07
Iter: 1119 loss: 4.27370082e-07
Iter: 1120 loss: 4.27259749e-07
Iter: 1121 loss: 4.27236188e-07
Iter: 1122 loss: 4.27136627e-07
Iter: 1123 loss: 4.28755982e-07
Iter: 1124 loss: 4.27136087e-07
Iter: 1125 loss: 4.27032603e-07
Iter: 1126 loss: 4.27561218e-07
Iter: 1127 loss: 4.2703013e-07
Iter: 1128 loss: 4.26978545e-07
Iter: 1129 loss: 4.26882849e-07
Iter: 1130 loss: 4.28445048e-07
Iter: 1131 loss: 4.26876113e-07
Iter: 1132 loss: 4.26715928e-07
Iter: 1133 loss: 4.26987981e-07
Iter: 1134 loss: 4.2665539e-07
Iter: 1135 loss: 4.26511235e-07
Iter: 1136 loss: 4.26512031e-07
Iter: 1137 loss: 4.26437197e-07
Iter: 1138 loss: 4.26282099e-07
Iter: 1139 loss: 4.28859806e-07
Iter: 1140 loss: 4.26279655e-07
Iter: 1141 loss: 4.26189672e-07
Iter: 1142 loss: 4.26176086e-07
Iter: 1143 loss: 4.26098325e-07
Iter: 1144 loss: 4.26014e-07
Iter: 1145 loss: 4.26005613e-07
Iter: 1146 loss: 4.25919836e-07
Iter: 1147 loss: 4.26862726e-07
Iter: 1148 loss: 4.25921826e-07
Iter: 1149 loss: 4.25842444e-07
Iter: 1150 loss: 4.25969e-07
Iter: 1151 loss: 4.25805524e-07
Iter: 1152 loss: 4.25742599e-07
Iter: 1153 loss: 4.25701899e-07
Iter: 1154 loss: 4.25647954e-07
Iter: 1155 loss: 4.25533926e-07
Iter: 1156 loss: 4.25523297e-07
Iter: 1157 loss: 4.25443432e-07
Iter: 1158 loss: 4.25335372e-07
Iter: 1159 loss: 4.25324373e-07
Iter: 1160 loss: 4.25250079e-07
Iter: 1161 loss: 4.25102769e-07
Iter: 1162 loss: 4.27794362e-07
Iter: 1163 loss: 4.25096943e-07
Iter: 1164 loss: 4.24963162e-07
Iter: 1165 loss: 4.2527688e-07
Iter: 1166 loss: 4.24909786e-07
Iter: 1167 loss: 4.24841289e-07
Iter: 1168 loss: 4.24840209e-07
Iter: 1169 loss: 4.24757161e-07
Iter: 1170 loss: 4.24899639e-07
Iter: 1171 loss: 4.24724533e-07
Iter: 1172 loss: 4.24647681e-07
Iter: 1173 loss: 4.24549768e-07
Iter: 1174 loss: 4.24541611e-07
Iter: 1175 loss: 4.24478969e-07
Iter: 1176 loss: 4.24471409e-07
Iter: 1177 loss: 4.24407233e-07
Iter: 1178 loss: 4.24336804e-07
Iter: 1179 loss: 4.24334075e-07
Iter: 1180 loss: 4.24253273e-07
Iter: 1181 loss: 4.24251311e-07
Iter: 1182 loss: 4.24195377e-07
Iter: 1183 loss: 4.24162323e-07
Iter: 1184 loss: 4.2413177e-07
Iter: 1185 loss: 4.24051734e-07
Iter: 1186 loss: 4.24020442e-07
Iter: 1187 loss: 4.23969624e-07
Iter: 1188 loss: 4.23838202e-07
Iter: 1189 loss: 4.23949871e-07
Iter: 1190 loss: 4.23773827e-07
Iter: 1191 loss: 4.23681115e-07
Iter: 1192 loss: 4.23680575e-07
Iter: 1193 loss: 4.2356703e-07
Iter: 1194 loss: 4.23514052e-07
Iter: 1195 loss: 4.23445641e-07
Iter: 1196 loss: 4.23345909e-07
Iter: 1197 loss: 4.23428162e-07
Iter: 1198 loss: 4.23283836e-07
Iter: 1199 loss: 4.23214487e-07
Iter: 1200 loss: 4.23203062e-07
Iter: 1201 loss: 4.23145138e-07
Iter: 1202 loss: 4.23066922e-07
Iter: 1203 loss: 4.23048334e-07
Iter: 1204 loss: 4.22948574e-07
Iter: 1205 loss: 4.22975177e-07
Iter: 1206 loss: 4.22869221e-07
Iter: 1207 loss: 4.22793676e-07
Iter: 1208 loss: 4.22783927e-07
Iter: 1209 loss: 4.22727624e-07
Iter: 1210 loss: 4.22586538e-07
Iter: 1211 loss: 4.25577809e-07
Iter: 1212 loss: 4.22571247e-07
Iter: 1213 loss: 4.22480269e-07
Iter: 1214 loss: 4.22475779e-07
Iter: 1215 loss: 4.22362916e-07
Iter: 1216 loss: 4.22462705e-07
Iter: 1217 loss: 4.22300843e-07
Iter: 1218 loss: 4.22223962e-07
Iter: 1219 loss: 4.22440678e-07
Iter: 1220 loss: 4.22190737e-07
Iter: 1221 loss: 4.22117893e-07
Iter: 1222 loss: 4.22080348e-07
Iter: 1223 loss: 4.22037402e-07
Iter: 1224 loss: 4.2193372e-07
Iter: 1225 loss: 4.22201936e-07
Iter: 1226 loss: 4.21892281e-07
Iter: 1227 loss: 4.21860278e-07
Iter: 1228 loss: 4.21838564e-07
Iter: 1229 loss: 4.21799598e-07
Iter: 1230 loss: 4.21700065e-07
Iter: 1231 loss: 4.22540381e-07
Iter: 1232 loss: 4.2167818e-07
Iter: 1233 loss: 4.21607126e-07
Iter: 1234 loss: 4.2159968e-07
Iter: 1235 loss: 4.21494434e-07
Iter: 1236 loss: 4.21416985e-07
Iter: 1237 loss: 4.21404536e-07
Iter: 1238 loss: 4.21280618e-07
Iter: 1239 loss: 4.21463426e-07
Iter: 1240 loss: 4.21222467e-07
Iter: 1241 loss: 4.21141493e-07
Iter: 1242 loss: 4.21142317e-07
Iter: 1243 loss: 4.21062907e-07
Iter: 1244 loss: 4.20919889e-07
Iter: 1245 loss: 4.230005e-07
Iter: 1246 loss: 4.20916024e-07
Iter: 1247 loss: 4.20861625e-07
Iter: 1248 loss: 4.20846646e-07
Iter: 1249 loss: 4.20787273e-07
Iter: 1250 loss: 4.20826666e-07
Iter: 1251 loss: 4.20745124e-07
Iter: 1252 loss: 4.20673928e-07
Iter: 1253 loss: 4.20781532e-07
Iter: 1254 loss: 4.20649769e-07
Iter: 1255 loss: 4.20573286e-07
Iter: 1256 loss: 4.20512322e-07
Iter: 1257 loss: 4.20493848e-07
Iter: 1258 loss: 4.20395e-07
Iter: 1259 loss: 4.20962579e-07
Iter: 1260 loss: 4.20370952e-07
Iter: 1261 loss: 4.20267952e-07
Iter: 1262 loss: 4.21025618e-07
Iter: 1263 loss: 4.20253315e-07
Iter: 1264 loss: 4.20197381e-07
Iter: 1265 loss: 4.20095716e-07
Iter: 1266 loss: 4.22625362e-07
Iter: 1267 loss: 4.20107114e-07
Iter: 1268 loss: 4.20062179e-07
Iter: 1269 loss: 4.2004541e-07
Iter: 1270 loss: 4.19991807e-07
Iter: 1271 loss: 4.19916432e-07
Iter: 1272 loss: 4.19895343e-07
Iter: 1273 loss: 4.19829348e-07
Iter: 1274 loss: 4.19685648e-07
Iter: 1275 loss: 4.23120127e-07
Iter: 1276 loss: 4.19676098e-07
Iter: 1277 loss: 4.1955866e-07
Iter: 1278 loss: 4.19549821e-07
Iter: 1279 loss: 4.19440596e-07
Iter: 1280 loss: 4.19760966e-07
Iter: 1281 loss: 4.19399669e-07
Iter: 1282 loss: 4.19341461e-07
Iter: 1283 loss: 4.19345724e-07
Iter: 1284 loss: 4.19299113e-07
Iter: 1285 loss: 4.19193015e-07
Iter: 1286 loss: 4.20168476e-07
Iter: 1287 loss: 4.19188211e-07
Iter: 1288 loss: 4.19141514e-07
Iter: 1289 loss: 4.19063781e-07
Iter: 1290 loss: 4.19062246e-07
Iter: 1291 loss: 4.1896169e-07
Iter: 1292 loss: 4.19333674e-07
Iter: 1293 loss: 4.18943927e-07
Iter: 1294 loss: 4.18845786e-07
Iter: 1295 loss: 4.18900356e-07
Iter: 1296 loss: 4.18792951e-07
Iter: 1297 loss: 4.18694697e-07
Iter: 1298 loss: 4.18692395e-07
Iter: 1299 loss: 4.18627963e-07
Iter: 1300 loss: 4.18502026e-07
Iter: 1301 loss: 4.21365655e-07
Iter: 1302 loss: 4.18500235e-07
Iter: 1303 loss: 4.18430091e-07
Iter: 1304 loss: 4.18423724e-07
Iter: 1305 loss: 4.18345167e-07
Iter: 1306 loss: 4.18262033e-07
Iter: 1307 loss: 4.18247396e-07
Iter: 1308 loss: 4.18158749e-07
Iter: 1309 loss: 4.18542299e-07
Iter: 1310 loss: 4.18142378e-07
Iter: 1311 loss: 4.18070215e-07
Iter: 1312 loss: 4.18169122e-07
Iter: 1313 loss: 4.18027696e-07
Iter: 1314 loss: 4.17919e-07
Iter: 1315 loss: 4.17946893e-07
Iter: 1316 loss: 4.17861486e-07
Iter: 1317 loss: 4.17725403e-07
Iter: 1318 loss: 4.18343745e-07
Iter: 1319 loss: 4.17711362e-07
Iter: 1320 loss: 4.17585596e-07
Iter: 1321 loss: 4.18174977e-07
Iter: 1322 loss: 4.17558113e-07
Iter: 1323 loss: 4.17484557e-07
Iter: 1324 loss: 4.17396961e-07
Iter: 1325 loss: 4.17396791e-07
Iter: 1326 loss: 4.17262243e-07
Iter: 1327 loss: 4.17832666e-07
Iter: 1328 loss: 4.17240756e-07
Iter: 1329 loss: 4.17169e-07
Iter: 1330 loss: 4.17389231e-07
Iter: 1331 loss: 4.17132412e-07
Iter: 1332 loss: 4.17068804e-07
Iter: 1333 loss: 4.17870609e-07
Iter: 1334 loss: 4.17074801e-07
Iter: 1335 loss: 4.17009687e-07
Iter: 1336 loss: 4.169965e-07
Iter: 1337 loss: 4.1696174e-07
Iter: 1338 loss: 4.16873377e-07
Iter: 1339 loss: 4.16739795e-07
Iter: 1340 loss: 4.1672925e-07
Iter: 1341 loss: 4.1660013e-07
Iter: 1342 loss: 4.16596833e-07
Iter: 1343 loss: 4.1652163e-07
Iter: 1344 loss: 4.16393704e-07
Iter: 1345 loss: 4.19115338e-07
Iter: 1346 loss: 4.16391799e-07
Iter: 1347 loss: 4.16302584e-07
Iter: 1348 loss: 4.16298747e-07
Iter: 1349 loss: 4.16209332e-07
Iter: 1350 loss: 4.16268847e-07
Iter: 1351 loss: 4.16151977e-07
Iter: 1352 loss: 4.16083566e-07
Iter: 1353 loss: 4.16107184e-07
Iter: 1354 loss: 4.16034254e-07
Iter: 1355 loss: 4.15961722e-07
Iter: 1356 loss: 4.16317619e-07
Iter: 1357 loss: 4.15950865e-07
Iter: 1358 loss: 4.15847609e-07
Iter: 1359 loss: 4.16068531e-07
Iter: 1360 loss: 4.15813759e-07
Iter: 1361 loss: 4.15736338e-07
Iter: 1362 loss: 4.15682848e-07
Iter: 1363 loss: 4.1566625e-07
Iter: 1364 loss: 4.15537187e-07
Iter: 1365 loss: 4.15886291e-07
Iter: 1366 loss: 4.15487364e-07
Iter: 1367 loss: 4.15392549e-07
Iter: 1368 loss: 4.15689584e-07
Iter: 1369 loss: 4.15351764e-07
Iter: 1370 loss: 4.15270506e-07
Iter: 1371 loss: 4.15266783e-07
Iter: 1372 loss: 4.15206955e-07
Iter: 1373 loss: 4.15152158e-07
Iter: 1374 loss: 4.15152726e-07
Iter: 1375 loss: 4.15084912e-07
Iter: 1376 loss: 4.15674378e-07
Iter: 1377 loss: 4.15088721e-07
Iter: 1378 loss: 4.15016302e-07
Iter: 1379 loss: 4.15087328e-07
Iter: 1380 loss: 4.14977933e-07
Iter: 1381 loss: 4.14895965e-07
Iter: 1382 loss: 4.14769602e-07
Iter: 1383 loss: 4.17813879e-07
Iter: 1384 loss: 4.14776622e-07
Iter: 1385 loss: 4.14641789e-07
Iter: 1386 loss: 4.15474886e-07
Iter: 1387 loss: 4.14609417e-07
Iter: 1388 loss: 4.14480382e-07
Iter: 1389 loss: 4.14981741e-07
Iter: 1390 loss: 4.1443414e-07
Iter: 1391 loss: 4.14379372e-07
Iter: 1392 loss: 4.14373545e-07
Iter: 1393 loss: 4.14329605e-07
Iter: 1394 loss: 4.14254487e-07
Iter: 1395 loss: 4.15737674e-07
Iter: 1396 loss: 4.1424704e-07
Iter: 1397 loss: 4.14167175e-07
Iter: 1398 loss: 4.14862541e-07
Iter: 1399 loss: 4.14162e-07
Iter: 1400 loss: 4.14092881e-07
Iter: 1401 loss: 4.14325854e-07
Iter: 1402 loss: 4.14079864e-07
Iter: 1403 loss: 4.1399656e-07
Iter: 1404 loss: 4.13941819e-07
Iter: 1405 loss: 4.1390274e-07
Iter: 1406 loss: 4.13777173e-07
Iter: 1407 loss: 4.14012334e-07
Iter: 1408 loss: 4.13719135e-07
Iter: 1409 loss: 4.13643392e-07
Iter: 1410 loss: 4.13630318e-07
Iter: 1411 loss: 4.13575293e-07
Iter: 1412 loss: 4.13454472e-07
Iter: 1413 loss: 4.15600454e-07
Iter: 1414 loss: 4.13449811e-07
Iter: 1415 loss: 4.13355338e-07
Iter: 1416 loss: 4.14171325e-07
Iter: 1417 loss: 4.13342207e-07
Iter: 1418 loss: 4.13259897e-07
Iter: 1419 loss: 4.13934544e-07
Iter: 1420 loss: 4.13263763e-07
Iter: 1421 loss: 4.13203622e-07
Iter: 1422 loss: 4.13104146e-07
Iter: 1423 loss: 4.13103578e-07
Iter: 1424 loss: 4.12996144e-07
Iter: 1425 loss: 4.14051556e-07
Iter: 1426 loss: 4.12996087e-07
Iter: 1427 loss: 4.12909969e-07
Iter: 1428 loss: 4.13394815e-07
Iter: 1429 loss: 4.12888795e-07
Iter: 1430 loss: 4.12819617e-07
Iter: 1431 loss: 4.12739439e-07
Iter: 1432 loss: 4.12738814e-07
Iter: 1433 loss: 4.12639793e-07
Iter: 1434 loss: 4.13928319e-07
Iter: 1435 loss: 4.12636155e-07
Iter: 1436 loss: 4.12555806e-07
Iter: 1437 loss: 4.1276931e-07
Iter: 1438 loss: 4.12535428e-07
Iter: 1439 loss: 4.12452096e-07
Iter: 1440 loss: 4.12410884e-07
Iter: 1441 loss: 4.12377972e-07
Iter: 1442 loss: 4.12301546e-07
Iter: 1443 loss: 4.13214195e-07
Iter: 1444 loss: 4.12305695e-07
Iter: 1445 loss: 4.12225916e-07
Iter: 1446 loss: 4.12421741e-07
Iter: 1447 loss: 4.1219505e-07
Iter: 1448 loss: 4.12128657e-07
Iter: 1449 loss: 4.12048877e-07
Iter: 1450 loss: 4.12037593e-07
Iter: 1451 loss: 4.11973474e-07
Iter: 1452 loss: 4.11958496e-07
Iter: 1453 loss: 4.11896451e-07
Iter: 1454 loss: 4.11816245e-07
Iter: 1455 loss: 4.11812039e-07
Iter: 1456 loss: 4.1173098e-07
Iter: 1457 loss: 4.11953806e-07
Iter: 1458 loss: 4.11700455e-07
Iter: 1459 loss: 4.11650319e-07
Iter: 1460 loss: 4.11652024e-07
Iter: 1461 loss: 4.11608738e-07
Iter: 1462 loss: 4.11573865e-07
Iter: 1463 loss: 4.11578696e-07
Iter: 1464 loss: 4.11516453e-07
Iter: 1465 loss: 4.11579265e-07
Iter: 1466 loss: 4.11494085e-07
Iter: 1467 loss: 4.11430932e-07
Iter: 1468 loss: 4.11915209e-07
Iter: 1469 loss: 4.11417716e-07
Iter: 1470 loss: 4.11363374e-07
Iter: 1471 loss: 4.11378664e-07
Iter: 1472 loss: 4.11317359e-07
Iter: 1473 loss: 4.11248266e-07
Iter: 1474 loss: 4.11229109e-07
Iter: 1475 loss: 4.11175364e-07
Iter: 1476 loss: 4.11107465e-07
Iter: 1477 loss: 4.11108772e-07
Iter: 1478 loss: 4.11063724e-07
Iter: 1479 loss: 4.11007591e-07
Iter: 1480 loss: 4.11010035e-07
Iter: 1481 loss: 4.10955522e-07
Iter: 1482 loss: 4.11460746e-07
Iter: 1483 loss: 4.10956574e-07
Iter: 1484 loss: 4.10916925e-07
Iter: 1485 loss: 4.11055908e-07
Iter: 1486 loss: 4.10890834e-07
Iter: 1487 loss: 4.108594e-07
Iter: 1488 loss: 4.10819609e-07
Iter: 1489 loss: 4.10824498e-07
Iter: 1490 loss: 4.1077061e-07
Iter: 1491 loss: 4.10772088e-07
Iter: 1492 loss: 4.10724283e-07
Iter: 1493 loss: 4.10690348e-07
Iter: 1494 loss: 4.10673323e-07
Iter: 1495 loss: 4.10614092e-07
Iter: 1496 loss: 4.10601615e-07
Iter: 1497 loss: 4.10555515e-07
Iter: 1498 loss: 4.10509045e-07
Iter: 1499 loss: 4.10499467e-07
Iter: 1500 loss: 4.10462349e-07
Iter: 1501 loss: 4.1045368e-07
Iter: 1502 loss: 4.10424605e-07
Iter: 1503 loss: 4.10347411e-07
Iter: 1504 loss: 4.10439924e-07
Iter: 1505 loss: 4.10321405e-07
Iter: 1506 loss: 4.10288379e-07
Iter: 1507 loss: 4.10757593e-07
Iter: 1508 loss: 4.10276073e-07
Iter: 1509 loss: 4.10219343e-07
Iter: 1510 loss: 4.10317e-07
Iter: 1511 loss: 4.10206127e-07
Iter: 1512 loss: 4.10175801e-07
Iter: 1513 loss: 4.1013314e-07
Iter: 1514 loss: 4.1012143e-07
Iter: 1515 loss: 4.10070442e-07
Iter: 1516 loss: 4.1063214e-07
Iter: 1517 loss: 4.10081498e-07
Iter: 1518 loss: 4.10030395e-07
Iter: 1519 loss: 4.09949195e-07
Iter: 1520 loss: 4.09954737e-07
Iter: 1521 loss: 4.09893232e-07
Iter: 1522 loss: 4.10409882e-07
Iter: 1523 loss: 4.09888059e-07
Iter: 1524 loss: 4.09819222e-07
Iter: 1525 loss: 4.1020445e-07
Iter: 1526 loss: 4.09818199e-07
Iter: 1527 loss: 4.09776248e-07
Iter: 1528 loss: 4.09725487e-07
Iter: 1529 loss: 4.09724635e-07
Iter: 1530 loss: 4.09676346e-07
Iter: 1531 loss: 4.10118957e-07
Iter: 1532 loss: 4.0967592e-07
Iter: 1533 loss: 4.09649488e-07
Iter: 1534 loss: 4.09924951e-07
Iter: 1535 loss: 4.09631411e-07
Iter: 1536 loss: 4.09604951e-07
Iter: 1537 loss: 4.09566582e-07
Iter: 1538 loss: 4.09571214e-07
Iter: 1539 loss: 4.09495868e-07
Iter: 1540 loss: 4.09692575e-07
Iter: 1541 loss: 4.09482141e-07
Iter: 1542 loss: 4.09414895e-07
Iter: 1543 loss: 4.10080759e-07
Iter: 1544 loss: 4.09415748e-07
Iter: 1545 loss: 4.09364873e-07
Iter: 1546 loss: 4.09282109e-07
Iter: 1547 loss: 4.10699471e-07
Iter: 1548 loss: 4.09274804e-07
Iter: 1549 loss: 4.09205825e-07
Iter: 1550 loss: 4.09201334e-07
Iter: 1551 loss: 4.09160037e-07
Iter: 1552 loss: 4.09139943e-07
Iter: 1553 loss: 4.09110186e-07
Iter: 1554 loss: 4.0904871e-07
Iter: 1555 loss: 4.09141904e-07
Iter: 1556 loss: 4.09036943e-07
Iter: 1557 loss: 4.09000108e-07
Iter: 1558 loss: 4.09000506e-07
Iter: 1559 loss: 4.08969839e-07
Iter: 1560 loss: 4.08909386e-07
Iter: 1561 loss: 4.09852333e-07
Iter: 1562 loss: 4.08894437e-07
Iter: 1563 loss: 4.08833046e-07
Iter: 1564 loss: 4.09046777e-07
Iter: 1565 loss: 4.08807608e-07
Iter: 1566 loss: 4.08768074e-07
Iter: 1567 loss: 4.08750083e-07
Iter: 1568 loss: 4.08719416e-07
Iter: 1569 loss: 4.08648532e-07
Iter: 1570 loss: 4.10188022e-07
Iter: 1571 loss: 4.08659218e-07
Iter: 1572 loss: 4.08596975e-07
Iter: 1573 loss: 4.0910669e-07
Iter: 1574 loss: 4.08588591e-07
Iter: 1575 loss: 4.08551784e-07
Iter: 1576 loss: 4.08970436e-07
Iter: 1577 loss: 4.08551443e-07
Iter: 1578 loss: 4.08523e-07
Iter: 1579 loss: 4.08513188e-07
Iter: 1580 loss: 4.08491815e-07
Iter: 1581 loss: 4.0846669e-07
Iter: 1582 loss: 4.08602091e-07
Iter: 1583 loss: 4.08456373e-07
Iter: 1584 loss: 4.08413428e-07
Iter: 1585 loss: 4.08501961e-07
Iter: 1586 loss: 4.08401831e-07
Iter: 1587 loss: 4.08369829e-07
Iter: 1588 loss: 4.08319437e-07
Iter: 1589 loss: 4.08319835e-07
Iter: 1590 loss: 4.08270125e-07
Iter: 1591 loss: 4.08263475e-07
Iter: 1592 loss: 4.08224821e-07
Iter: 1593 loss: 4.08182188e-07
Iter: 1594 loss: 4.08170479e-07
Iter: 1595 loss: 4.08122332e-07
Iter: 1596 loss: 4.08098458e-07
Iter: 1597 loss: 4.08072367e-07
Iter: 1598 loss: 4.08019474e-07
Iter: 1599 loss: 4.08024931e-07
Iter: 1600 loss: 4.07978064e-07
Iter: 1601 loss: 4.08207569e-07
Iter: 1602 loss: 4.07977126e-07
Iter: 1603 loss: 4.07954929e-07
Iter: 1604 loss: 4.07941172e-07
Iter: 1605 loss: 4.07934465e-07
Iter: 1606 loss: 4.07901041e-07
Iter: 1607 loss: 4.0828192e-07
Iter: 1608 loss: 4.07911045e-07
Iter: 1609 loss: 4.07865258e-07
Iter: 1610 loss: 4.07874154e-07
Iter: 1611 loss: 4.07847438e-07
Iter: 1612 loss: 4.07803157e-07
Iter: 1613 loss: 4.07802418e-07
Iter: 1614 loss: 4.077786e-07
Iter: 1615 loss: 4.07727327e-07
Iter: 1616 loss: 4.08193785e-07
Iter: 1617 loss: 4.07719256e-07
Iter: 1618 loss: 4.07684581e-07
Iter: 1619 loss: 4.07627965e-07
Iter: 1620 loss: 4.07618984e-07
Iter: 1621 loss: 4.07587493e-07
Iter: 1622 loss: 4.07586981e-07
Iter: 1623 loss: 4.07550033e-07
Iter: 1624 loss: 4.07585787e-07
Iter: 1625 loss: 4.07535879e-07
Iter: 1626 loss: 4.07496316e-07
Iter: 1627 loss: 4.07498646e-07
Iter: 1628 loss: 4.07469315e-07
Iter: 1629 loss: 4.07436e-07
Iter: 1630 loss: 4.07402297e-07
Iter: 1631 loss: 4.0739269e-07
Iter: 1632 loss: 4.07362933e-07
Iter: 1633 loss: 4.07359067e-07
Iter: 1634 loss: 4.07320897e-07
Iter: 1635 loss: 4.07265219e-07
Iter: 1636 loss: 4.08538767e-07
Iter: 1637 loss: 4.07261155e-07
Iter: 1638 loss: 4.07206812e-07
Iter: 1639 loss: 4.0792267e-07
Iter: 1640 loss: 4.07203544e-07
Iter: 1641 loss: 4.07151674e-07
Iter: 1642 loss: 4.07259847e-07
Iter: 1643 loss: 4.07130642e-07
Iter: 1644 loss: 4.07079966e-07
Iter: 1645 loss: 4.07222558e-07
Iter: 1646 loss: 4.07070445e-07
Iter: 1647 loss: 4.07044695e-07
Iter: 1648 loss: 4.07390132e-07
Iter: 1649 loss: 4.07048901e-07
Iter: 1650 loss: 4.07023492e-07
Iter: 1651 loss: 4.06986146e-07
Iter: 1652 loss: 4.06981599e-07
Iter: 1653 loss: 4.06944793e-07
Iter: 1654 loss: 4.07036794e-07
Iter: 1655 loss: 4.06927086e-07
Iter: 1656 loss: 4.06882975e-07
Iter: 1657 loss: 4.07215e-07
Iter: 1658 loss: 4.06873198e-07
Iter: 1659 loss: 4.06834971e-07
Iter: 1660 loss: 4.06793646e-07
Iter: 1661 loss: 4.06785034e-07
Iter: 1662 loss: 4.06731459e-07
Iter: 1663 loss: 4.06728873e-07
Iter: 1664 loss: 4.06674e-07
Iter: 1665 loss: 4.06608308e-07
Iter: 1666 loss: 4.06833692e-07
Iter: 1667 loss: 4.06597053e-07
Iter: 1668 loss: 4.06568233e-07
Iter: 1669 loss: 4.06560673e-07
Iter: 1670 loss: 4.06531882e-07
Iter: 1671 loss: 4.06466256e-07
Iter: 1672 loss: 4.07324279e-07
Iter: 1673 loss: 4.06470662e-07
Iter: 1674 loss: 4.06418053e-07
Iter: 1675 loss: 4.06504796e-07
Iter: 1676 loss: 4.06395372e-07
Iter: 1677 loss: 4.06339126e-07
Iter: 1678 loss: 4.06736149e-07
Iter: 1679 loss: 4.06338614e-07
Iter: 1680 loss: 4.06296493e-07
Iter: 1681 loss: 4.06839661e-07
Iter: 1682 loss: 4.06298795e-07
Iter: 1683 loss: 4.06264689e-07
Iter: 1684 loss: 4.06220465e-07
Iter: 1685 loss: 4.06213644e-07
Iter: 1686 loss: 4.06165356e-07
Iter: 1687 loss: 4.0619986e-07
Iter: 1688 loss: 4.06142135e-07
Iter: 1689 loss: 4.06154186e-07
Iter: 1690 loss: 4.0611107e-07
Iter: 1691 loss: 4.06098366e-07
Iter: 1692 loss: 4.06073383e-07
Iter: 1693 loss: 4.0647376e-07
Iter: 1694 loss: 4.06064885e-07
Iter: 1695 loss: 4.06028562e-07
Iter: 1696 loss: 4.062627e-07
Iter: 1697 loss: 4.06031148e-07
Iter: 1698 loss: 4.06000424e-07
Iter: 1699 loss: 4.06116669e-07
Iter: 1700 loss: 4.05988601e-07
Iter: 1701 loss: 4.05980643e-07
Iter: 1702 loss: 4.05968649e-07
Iter: 1703 loss: 4.05953699e-07
Iter: 1704 loss: 4.05911351e-07
Iter: 1705 loss: 4.06134802e-07
Iter: 1706 loss: 4.05900238e-07
Iter: 1707 loss: 4.05869258e-07
Iter: 1708 loss: 4.05788739e-07
Iter: 1709 loss: 4.07416792e-07
Iter: 1710 loss: 4.0578314e-07
Iter: 1711 loss: 4.05738461e-07
Iter: 1712 loss: 4.06022764e-07
Iter: 1713 loss: 4.05726553e-07
Iter: 1714 loss: 4.05682272e-07
Iter: 1715 loss: 4.05853143e-07
Iter: 1716 loss: 4.05668914e-07
Iter: 1717 loss: 4.05637707e-07
Iter: 1718 loss: 4.05805963e-07
Iter: 1719 loss: 4.05618692e-07
Iter: 1720 loss: 4.05592431e-07
Iter: 1721 loss: 4.05990249e-07
Iter: 1722 loss: 4.05592175e-07
Iter: 1723 loss: 4.05582455e-07
Iter: 1724 loss: 4.05551191e-07
Iter: 1725 loss: 4.06452955e-07
Iter: 1726 loss: 4.05553635e-07
Iter: 1727 loss: 4.05498042e-07
Iter: 1728 loss: 4.05569835e-07
Iter: 1729 loss: 4.05477351e-07
Iter: 1730 loss: 4.05436367e-07
Iter: 1731 loss: 4.05432e-07
Iter: 1732 loss: 4.05405018e-07
Iter: 1733 loss: 4.05372305e-07
Iter: 1734 loss: 4.05359884e-07
Iter: 1735 loss: 4.0532521e-07
Iter: 1736 loss: 4.05642425e-07
Iter: 1737 loss: 4.05314495e-07
Iter: 1738 loss: 4.05292099e-07
Iter: 1739 loss: 4.05395809e-07
Iter: 1740 loss: 4.05280446e-07
Iter: 1741 loss: 4.05241e-07
Iter: 1742 loss: 4.05232697e-07
Iter: 1743 loss: 4.05212489e-07
Iter: 1744 loss: 4.05179833e-07
Iter: 1745 loss: 4.05180685e-07
Iter: 1746 loss: 4.05161245e-07
Iter: 1747 loss: 4.0511145e-07
Iter: 1748 loss: 4.05677923e-07
Iter: 1749 loss: 4.0509488e-07
Iter: 1750 loss: 4.05032097e-07
Iter: 1751 loss: 4.05213484e-07
Iter: 1752 loss: 4.0501277e-07
Iter: 1753 loss: 4.04965817e-07
Iter: 1754 loss: 4.0533692e-07
Iter: 1755 loss: 4.04965817e-07
Iter: 1756 loss: 4.04901414e-07
Iter: 1757 loss: 4.05269361e-07
Iter: 1758 loss: 4.04899595e-07
Iter: 1759 loss: 4.04848663e-07
Iter: 1760 loss: 4.05026867e-07
Iter: 1761 loss: 4.04835788e-07
Iter: 1762 loss: 4.04800915e-07
Iter: 1763 loss: 4.04783464e-07
Iter: 1764 loss: 4.04766581e-07
Iter: 1765 loss: 4.04743815e-07
Iter: 1766 loss: 4.04747112e-07
Iter: 1767 loss: 4.04699222e-07
Iter: 1768 loss: 4.04657726e-07
Iter: 1769 loss: 4.04644311e-07
Iter: 1770 loss: 4.04606112e-07
Iter: 1771 loss: 4.0460003e-07
Iter: 1772 loss: 4.04556317e-07
Iter: 1773 loss: 4.04517209e-07
Iter: 1774 loss: 4.0517773e-07
Iter: 1775 loss: 4.04517436e-07
Iter: 1776 loss: 4.04474406e-07
Iter: 1777 loss: 4.04675916e-07
Iter: 1778 loss: 4.04468807e-07
Iter: 1779 loss: 4.04437571e-07
Iter: 1780 loss: 4.04411168e-07
Iter: 1781 loss: 4.0439636e-07
Iter: 1782 loss: 4.04354182e-07
Iter: 1783 loss: 4.0505077e-07
Iter: 1784 loss: 4.043535e-07
Iter: 1785 loss: 4.04326897e-07
Iter: 1786 loss: 4.04265506e-07
Iter: 1787 loss: 4.05258135e-07
Iter: 1788 loss: 4.04253655e-07
Iter: 1789 loss: 4.04172567e-07
Iter: 1790 loss: 4.04225716e-07
Iter: 1791 loss: 4.04122602e-07
Iter: 1792 loss: 4.04031823e-07
Iter: 1793 loss: 4.05151695e-07
Iter: 1794 loss: 4.04036371e-07
Iter: 1795 loss: 4.03983677e-07
Iter: 1796 loss: 4.04501975e-07
Iter: 1797 loss: 4.03970034e-07
Iter: 1798 loss: 4.03914441e-07
Iter: 1799 loss: 4.03897701e-07
Iter: 1800 loss: 4.03879113e-07
Iter: 1801 loss: 4.03837817e-07
Iter: 1802 loss: 4.03835713e-07
Iter: 1803 loss: 4.03788704e-07
Iter: 1804 loss: 4.03757383e-07
Iter: 1805 loss: 4.03756133e-07
Iter: 1806 loss: 4.03700824e-07
Iter: 1807 loss: 4.03853392e-07
Iter: 1808 loss: 4.03678087e-07
Iter: 1809 loss: 4.03618344e-07
Iter: 1810 loss: 4.04195021e-07
Iter: 1811 loss: 4.03620447e-07
Iter: 1812 loss: 4.03577729e-07
Iter: 1813 loss: 4.03515799e-07
Iter: 1814 loss: 4.035019e-07
Iter: 1815 loss: 4.03436758e-07
Iter: 1816 loss: 4.03957e-07
Iter: 1817 loss: 4.03429254e-07
Iter: 1818 loss: 4.03341289e-07
Iter: 1819 loss: 4.03423542e-07
Iter: 1820 loss: 4.03309855e-07
Iter: 1821 loss: 4.0325105e-07
Iter: 1822 loss: 4.03189858e-07
Iter: 1823 loss: 4.03168485e-07
Iter: 1824 loss: 4.03100273e-07
Iter: 1825 loss: 4.03619225e-07
Iter: 1826 loss: 4.03093907e-07
Iter: 1827 loss: 4.03042918e-07
Iter: 1828 loss: 4.03743968e-07
Iter: 1829 loss: 4.03034051e-07
Iter: 1830 loss: 4.02993749e-07
Iter: 1831 loss: 4.03009864e-07
Iter: 1832 loss: 4.02970784e-07
Iter: 1833 loss: 4.02911951e-07
Iter: 1834 loss: 4.03138586e-07
Iter: 1835 loss: 4.0290837e-07
Iter: 1836 loss: 4.02845956e-07
Iter: 1837 loss: 4.03035898e-07
Iter: 1838 loss: 4.02835241e-07
Iter: 1839 loss: 4.02784224e-07
Iter: 1840 loss: 4.02673493e-07
Iter: 1841 loss: 4.04361771e-07
Iter: 1842 loss: 4.02662664e-07
Iter: 1843 loss: 4.02579303e-07
Iter: 1844 loss: 4.02577825e-07
Iter: 1845 loss: 4.02514843e-07
Iter: 1846 loss: 4.02830267e-07
Iter: 1847 loss: 4.02508874e-07
Iter: 1848 loss: 4.02456863e-07
Iter: 1849 loss: 4.02465929e-07
Iter: 1850 loss: 4.02409569e-07
Iter: 1851 loss: 4.02352043e-07
Iter: 1852 loss: 4.02735282e-07
Iter: 1853 loss: 4.02344568e-07
Iter: 1854 loss: 4.02286e-07
Iter: 1855 loss: 4.02244041e-07
Iter: 1856 loss: 4.02220479e-07
Iter: 1857 loss: 4.02158634e-07
Iter: 1858 loss: 4.02216244e-07
Iter: 1859 loss: 4.02126886e-07
Iter: 1860 loss: 4.02056571e-07
Iter: 1861 loss: 4.02573619e-07
Iter: 1862 loss: 4.0204398e-07
Iter: 1863 loss: 4.01966872e-07
Iter: 1864 loss: 4.02368727e-07
Iter: 1865 loss: 4.01943907e-07
Iter: 1866 loss: 4.01891668e-07
Iter: 1867 loss: 4.01896074e-07
Iter: 1868 loss: 4.01852731e-07
Iter: 1869 loss: 4.01772297e-07
Iter: 1870 loss: 4.02565263e-07
Iter: 1871 loss: 4.01778266e-07
Iter: 1872 loss: 4.01710338e-07
Iter: 1873 loss: 4.01673276e-07
Iter: 1874 loss: 4.01650283e-07
Iter: 1875 loss: 4.01572436e-07
Iter: 1876 loss: 4.01674583e-07
Iter: 1877 loss: 4.01541399e-07
Iter: 1878 loss: 4.01475916e-07
Iter: 1879 loss: 4.01477507e-07
Iter: 1880 loss: 4.01426064e-07
Iter: 1881 loss: 4.01448546e-07
Iter: 1882 loss: 4.01398609e-07
Iter: 1883 loss: 4.01344778e-07
Iter: 1884 loss: 4.01523948e-07
Iter: 1885 loss: 4.01328975e-07
Iter: 1886 loss: 4.01260223e-07
Iter: 1887 loss: 4.01244961e-07
Iter: 1888 loss: 4.01220888e-07
Iter: 1889 loss: 4.01136901e-07
Iter: 1890 loss: 4.01168336e-07
Iter: 1891 loss: 4.01080058e-07
Iter: 1892 loss: 4.00979559e-07
Iter: 1893 loss: 4.01306721e-07
Iter: 1894 loss: 4.00958555e-07
Iter: 1895 loss: 4.00916861e-07
Iter: 1896 loss: 4.0091598e-07
Iter: 1897 loss: 4.00869624e-07
Iter: 1898 loss: 4.00803913e-07
Iter: 1899 loss: 4.00814457e-07
Iter: 1900 loss: 4.00739623e-07
Iter: 1901 loss: 4.01563028e-07
Iter: 1902 loss: 4.00756e-07
Iter: 1903 loss: 4.00707876e-07
Iter: 1904 loss: 4.00759802e-07
Iter: 1905 loss: 4.00682438e-07
Iter: 1906 loss: 4.0062551e-07
Iter: 1907 loss: 4.00588249e-07
Iter: 1908 loss: 4.00570912e-07
Iter: 1909 loss: 4.00501307e-07
Iter: 1910 loss: 4.00920271e-07
Iter: 1911 loss: 4.00492354e-07
Iter: 1912 loss: 4.00410471e-07
Iter: 1913 loss: 4.00487636e-07
Iter: 1914 loss: 4.00365167e-07
Iter: 1915 loss: 4.00296727e-07
Iter: 1916 loss: 4.00771967e-07
Iter: 1917 loss: 4.00291015e-07
Iter: 1918 loss: 4.00241873e-07
Iter: 1919 loss: 4.00292635e-07
Iter: 1920 loss: 4.00215526e-07
Iter: 1921 loss: 4.00149816e-07
Iter: 1922 loss: 4.00089135e-07
Iter: 1923 loss: 4.00079216e-07
Iter: 1924 loss: 3.99978774e-07
Iter: 1925 loss: 4.00305737e-07
Iter: 1926 loss: 3.99954e-07
Iter: 1927 loss: 3.9988663e-07
Iter: 1928 loss: 3.99884811e-07
Iter: 1929 loss: 3.99823904e-07
Iter: 1930 loss: 3.99853832e-07
Iter: 1931 loss: 3.99780163e-07
Iter: 1932 loss: 3.9971e-07
Iter: 1933 loss: 3.99925057e-07
Iter: 1934 loss: 3.99703936e-07
Iter: 1935 loss: 3.99623133e-07
Iter: 1936 loss: 3.99900102e-07
Iter: 1937 loss: 3.99604346e-07
Iter: 1938 loss: 3.99544717e-07
Iter: 1939 loss: 3.99574049e-07
Iter: 1940 loss: 3.99511748e-07
Iter: 1941 loss: 3.99444446e-07
Iter: 1942 loss: 3.99647774e-07
Iter: 1943 loss: 3.99428984e-07
Iter: 1944 loss: 3.99351791e-07
Iter: 1945 loss: 3.99707943e-07
Iter: 1946 loss: 3.99343151e-07
Iter: 1947 loss: 3.99293015e-07
Iter: 1948 loss: 3.99333288e-07
Iter: 1949 loss: 3.99249188e-07
Iter: 1950 loss: 3.99183193e-07
Iter: 1951 loss: 3.99426881e-07
Iter: 1952 loss: 3.99158523e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.2
+ date
Mon Oct 26 13:10:44 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.2/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.2_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 1 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.2_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ce017c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ce01ada60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ce01ade18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ce01ad268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ce02732f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ce0273158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ce01078c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ce00b6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ce01072f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ce0079378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ce00377b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ce0051488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1cc00b8268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1cc00c77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1cc00678c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1cc00671e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1cc007d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1cc00b8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c7ec8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c78f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c78a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c78aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c7007b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c737ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c7377b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c6bdae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c692598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c661268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c661158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c653598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c6531e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c5f61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c5e9378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c58b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c5a7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c6c570378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.43555234e-06
Iter: 2 loss: 5.81704353e-06
Iter: 3 loss: 3.72681961e-06
Iter: 4 loss: 3.48149865e-06
Iter: 5 loss: 3.1578229e-06
Iter: 6 loss: 3.13837154e-06
Iter: 7 loss: 2.94956271e-06
Iter: 8 loss: 2.94786969e-06
Iter: 9 loss: 2.83141276e-06
Iter: 10 loss: 2.86666454e-06
Iter: 11 loss: 2.74812555e-06
Iter: 12 loss: 2.64867458e-06
Iter: 13 loss: 3.43120746e-06
Iter: 14 loss: 2.64179198e-06
Iter: 15 loss: 2.56000612e-06
Iter: 16 loss: 2.73675914e-06
Iter: 17 loss: 2.52827795e-06
Iter: 18 loss: 2.4514502e-06
Iter: 19 loss: 2.43782688e-06
Iter: 20 loss: 2.38561506e-06
Iter: 21 loss: 2.27879536e-06
Iter: 22 loss: 2.25664053e-06
Iter: 23 loss: 2.18626565e-06
Iter: 24 loss: 2.22415474e-06
Iter: 25 loss: 2.13647945e-06
Iter: 26 loss: 2.09809332e-06
Iter: 27 loss: 1.99901706e-06
Iter: 28 loss: 2.80857012e-06
Iter: 29 loss: 1.9812187e-06
Iter: 30 loss: 1.89869888e-06
Iter: 31 loss: 2.40938311e-06
Iter: 32 loss: 1.88888703e-06
Iter: 33 loss: 1.83096427e-06
Iter: 34 loss: 1.90673609e-06
Iter: 35 loss: 1.80145571e-06
Iter: 36 loss: 1.82767212e-06
Iter: 37 loss: 1.7804183e-06
Iter: 38 loss: 1.76534718e-06
Iter: 39 loss: 1.72104865e-06
Iter: 40 loss: 1.88299475e-06
Iter: 41 loss: 1.70157819e-06
Iter: 42 loss: 1.67735732e-06
Iter: 43 loss: 1.66945574e-06
Iter: 44 loss: 1.6401525e-06
Iter: 45 loss: 1.60417858e-06
Iter: 46 loss: 1.60095328e-06
Iter: 47 loss: 1.55474606e-06
Iter: 48 loss: 1.69636792e-06
Iter: 49 loss: 1.54106738e-06
Iter: 50 loss: 1.49202083e-06
Iter: 51 loss: 1.9570532e-06
Iter: 52 loss: 1.49005882e-06
Iter: 53 loss: 1.45670299e-06
Iter: 54 loss: 1.43318812e-06
Iter: 55 loss: 1.4213731e-06
Iter: 56 loss: 1.38625626e-06
Iter: 57 loss: 1.52159282e-06
Iter: 58 loss: 1.37801703e-06
Iter: 59 loss: 1.35424295e-06
Iter: 60 loss: 1.71352804e-06
Iter: 61 loss: 1.35422374e-06
Iter: 62 loss: 1.32988237e-06
Iter: 63 loss: 1.29461705e-06
Iter: 64 loss: 1.29355772e-06
Iter: 65 loss: 1.26496604e-06
Iter: 66 loss: 1.25872202e-06
Iter: 67 loss: 1.24009694e-06
Iter: 68 loss: 1.1932184e-06
Iter: 69 loss: 1.33663855e-06
Iter: 70 loss: 1.17930324e-06
Iter: 71 loss: 1.19637366e-06
Iter: 72 loss: 1.1646689e-06
Iter: 73 loss: 1.15442776e-06
Iter: 74 loss: 1.13133626e-06
Iter: 75 loss: 1.44689568e-06
Iter: 76 loss: 1.13004558e-06
Iter: 77 loss: 1.11358509e-06
Iter: 78 loss: 1.30734543e-06
Iter: 79 loss: 1.11337681e-06
Iter: 80 loss: 1.09722964e-06
Iter: 81 loss: 1.12946395e-06
Iter: 82 loss: 1.09060136e-06
Iter: 83 loss: 1.07938718e-06
Iter: 84 loss: 1.06763832e-06
Iter: 85 loss: 1.06560469e-06
Iter: 86 loss: 1.05163542e-06
Iter: 87 loss: 1.05096569e-06
Iter: 88 loss: 1.04154128e-06
Iter: 89 loss: 1.01656167e-06
Iter: 90 loss: 1.19774472e-06
Iter: 91 loss: 1.01121896e-06
Iter: 92 loss: 9.86836767e-07
Iter: 93 loss: 1.36677522e-06
Iter: 94 loss: 9.86823579e-07
Iter: 95 loss: 9.7232919e-07
Iter: 96 loss: 1.09339055e-06
Iter: 97 loss: 9.71471877e-07
Iter: 98 loss: 9.57513e-07
Iter: 99 loss: 9.6460235e-07
Iter: 100 loss: 9.48213426e-07
Iter: 101 loss: 9.36953597e-07
Iter: 102 loss: 9.33451247e-07
Iter: 103 loss: 9.26854227e-07
Iter: 104 loss: 9.13441056e-07
Iter: 105 loss: 9.79521815e-07
Iter: 106 loss: 9.11167831e-07
Iter: 107 loss: 9.09920232e-07
Iter: 108 loss: 9.0642834e-07
Iter: 109 loss: 9.02331863e-07
Iter: 110 loss: 8.90103479e-07
Iter: 111 loss: 9.28932423e-07
Iter: 112 loss: 8.84135829e-07
Iter: 113 loss: 8.77917728e-07
Iter: 114 loss: 8.76651654e-07
Iter: 115 loss: 8.68012137e-07
Iter: 116 loss: 8.5873387e-07
Iter: 117 loss: 8.57294197e-07
Iter: 118 loss: 8.46586e-07
Iter: 119 loss: 8.85673e-07
Iter: 120 loss: 8.43931957e-07
Iter: 121 loss: 8.36367747e-07
Iter: 122 loss: 8.36364222e-07
Iter: 123 loss: 8.31690727e-07
Iter: 124 loss: 8.23243e-07
Iter: 125 loss: 1.0271342e-06
Iter: 126 loss: 8.23223218e-07
Iter: 127 loss: 8.15376552e-07
Iter: 128 loss: 8.44182807e-07
Iter: 129 loss: 8.13464339e-07
Iter: 130 loss: 8.07027902e-07
Iter: 131 loss: 8.06961737e-07
Iter: 132 loss: 8.03367925e-07
Iter: 133 loss: 7.99440215e-07
Iter: 134 loss: 7.98859105e-07
Iter: 135 loss: 7.91593607e-07
Iter: 136 loss: 7.92540504e-07
Iter: 137 loss: 7.86062628e-07
Iter: 138 loss: 7.77882178e-07
Iter: 139 loss: 8.02681e-07
Iter: 140 loss: 7.75464855e-07
Iter: 141 loss: 7.72168278e-07
Iter: 142 loss: 7.70635097e-07
Iter: 143 loss: 7.68255404e-07
Iter: 144 loss: 7.62335617e-07
Iter: 145 loss: 8.19751222e-07
Iter: 146 loss: 7.61561296e-07
Iter: 147 loss: 7.57417638e-07
Iter: 148 loss: 7.57404564e-07
Iter: 149 loss: 7.53034442e-07
Iter: 150 loss: 7.54127086e-07
Iter: 151 loss: 7.49845356e-07
Iter: 152 loss: 7.45681291e-07
Iter: 153 loss: 7.48853893e-07
Iter: 154 loss: 7.43129931e-07
Iter: 155 loss: 7.38432846e-07
Iter: 156 loss: 8.03346893e-07
Iter: 157 loss: 7.38426365e-07
Iter: 158 loss: 7.35267349e-07
Iter: 159 loss: 7.27676706e-07
Iter: 160 loss: 8.08161e-07
Iter: 161 loss: 7.26836333e-07
Iter: 162 loss: 7.22122934e-07
Iter: 163 loss: 7.22026243e-07
Iter: 164 loss: 7.16500779e-07
Iter: 165 loss: 7.20916e-07
Iter: 166 loss: 7.1314895e-07
Iter: 167 loss: 7.09497e-07
Iter: 168 loss: 7.16165687e-07
Iter: 169 loss: 7.07917934e-07
Iter: 170 loss: 7.03405931e-07
Iter: 171 loss: 7.05187745e-07
Iter: 172 loss: 7.00280111e-07
Iter: 173 loss: 6.97446808e-07
Iter: 174 loss: 6.97269e-07
Iter: 175 loss: 6.93831623e-07
Iter: 176 loss: 6.91535206e-07
Iter: 177 loss: 6.90255433e-07
Iter: 178 loss: 6.86269573e-07
Iter: 179 loss: 6.85258556e-07
Iter: 180 loss: 6.82761083e-07
Iter: 181 loss: 6.79899074e-07
Iter: 182 loss: 6.7948406e-07
Iter: 183 loss: 6.76737443e-07
Iter: 184 loss: 6.73268346e-07
Iter: 185 loss: 6.72987e-07
Iter: 186 loss: 6.70239388e-07
Iter: 187 loss: 6.96261679e-07
Iter: 188 loss: 6.70164184e-07
Iter: 189 loss: 6.66812412e-07
Iter: 190 loss: 6.64535548e-07
Iter: 191 loss: 6.6331279e-07
Iter: 192 loss: 6.60448222e-07
Iter: 193 loss: 6.67984409e-07
Iter: 194 loss: 6.59452439e-07
Iter: 195 loss: 6.57306e-07
Iter: 196 loss: 6.57262945e-07
Iter: 197 loss: 6.55389385e-07
Iter: 198 loss: 6.51611117e-07
Iter: 199 loss: 7.22723939e-07
Iter: 200 loss: 6.51579853e-07
Iter: 201 loss: 6.48214495e-07
Iter: 202 loss: 6.60633077e-07
Iter: 203 loss: 6.47357524e-07
Iter: 204 loss: 6.4395914e-07
Iter: 205 loss: 6.60069873e-07
Iter: 206 loss: 6.43340059e-07
Iter: 207 loss: 6.4152556e-07
Iter: 208 loss: 6.41421593e-07
Iter: 209 loss: 6.40171208e-07
Iter: 210 loss: 6.3771239e-07
Iter: 211 loss: 6.85558e-07
Iter: 212 loss: 6.37682092e-07
Iter: 213 loss: 6.35218953e-07
Iter: 214 loss: 6.37259177e-07
Iter: 215 loss: 6.33707543e-07
Iter: 216 loss: 6.30982186e-07
Iter: 217 loss: 6.30960301e-07
Iter: 218 loss: 6.29715373e-07
Iter: 219 loss: 6.26900942e-07
Iter: 220 loss: 6.63050741e-07
Iter: 221 loss: 6.26709777e-07
Iter: 222 loss: 6.23667631e-07
Iter: 223 loss: 6.23678773e-07
Iter: 224 loss: 6.21697438e-07
Iter: 225 loss: 6.1916478e-07
Iter: 226 loss: 6.18980096e-07
Iter: 227 loss: 6.16663158e-07
Iter: 228 loss: 6.36238155e-07
Iter: 229 loss: 6.1652463e-07
Iter: 230 loss: 6.1425385e-07
Iter: 231 loss: 6.25514303e-07
Iter: 232 loss: 6.13853445e-07
Iter: 233 loss: 6.12450606e-07
Iter: 234 loss: 6.09649533e-07
Iter: 235 loss: 6.62668526e-07
Iter: 236 loss: 6.09621111e-07
Iter: 237 loss: 6.07155471e-07
Iter: 238 loss: 6.22442e-07
Iter: 239 loss: 6.06842264e-07
Iter: 240 loss: 6.04890147e-07
Iter: 241 loss: 6.34213393e-07
Iter: 242 loss: 6.04890147e-07
Iter: 243 loss: 6.02810928e-07
Iter: 244 loss: 6.0184766e-07
Iter: 245 loss: 6.00810154e-07
Iter: 246 loss: 5.98452459e-07
Iter: 247 loss: 5.99600639e-07
Iter: 248 loss: 5.96862321e-07
Iter: 249 loss: 5.94547828e-07
Iter: 250 loss: 6.16955617e-07
Iter: 251 loss: 5.94498e-07
Iter: 252 loss: 5.9230274e-07
Iter: 253 loss: 5.99401119e-07
Iter: 254 loss: 5.91648302e-07
Iter: 255 loss: 5.90537297e-07
Iter: 256 loss: 5.90521722e-07
Iter: 257 loss: 5.89639797e-07
Iter: 258 loss: 5.87856277e-07
Iter: 259 loss: 6.02437922e-07
Iter: 260 loss: 5.87739578e-07
Iter: 261 loss: 5.86883175e-07
Iter: 262 loss: 5.84820668e-07
Iter: 263 loss: 6.08211963e-07
Iter: 264 loss: 5.84631778e-07
Iter: 265 loss: 5.83773158e-07
Iter: 266 loss: 5.83247129e-07
Iter: 267 loss: 5.82131861e-07
Iter: 268 loss: 5.7994032e-07
Iter: 269 loss: 6.22577204e-07
Iter: 270 loss: 5.79908829e-07
Iter: 271 loss: 5.77550452e-07
Iter: 272 loss: 5.79813218e-07
Iter: 273 loss: 5.76176149e-07
Iter: 274 loss: 5.7374956e-07
Iter: 275 loss: 5.92076958e-07
Iter: 276 loss: 5.73528723e-07
Iter: 277 loss: 5.7246848e-07
Iter: 278 loss: 5.72310796e-07
Iter: 279 loss: 5.71545456e-07
Iter: 280 loss: 5.70446559e-07
Iter: 281 loss: 5.70402221e-07
Iter: 282 loss: 5.69200324e-07
Iter: 283 loss: 5.72620138e-07
Iter: 284 loss: 5.68829535e-07
Iter: 285 loss: 5.67879624e-07
Iter: 286 loss: 5.80082087e-07
Iter: 287 loss: 5.6786223e-07
Iter: 288 loss: 5.66993265e-07
Iter: 289 loss: 5.65520622e-07
Iter: 290 loss: 5.65522612e-07
Iter: 291 loss: 5.64212769e-07
Iter: 292 loss: 5.80009271e-07
Iter: 293 loss: 5.64188724e-07
Iter: 294 loss: 5.62898094e-07
Iter: 295 loss: 5.62951186e-07
Iter: 296 loss: 5.61875254e-07
Iter: 297 loss: 5.60490776e-07
Iter: 298 loss: 5.61001514e-07
Iter: 299 loss: 5.59508067e-07
Iter: 300 loss: 5.58695206e-07
Iter: 301 loss: 5.58489432e-07
Iter: 302 loss: 5.57874614e-07
Iter: 303 loss: 5.56625139e-07
Iter: 304 loss: 5.81205541e-07
Iter: 305 loss: 5.5660513e-07
Iter: 306 loss: 5.55537554e-07
Iter: 307 loss: 5.54615724e-07
Iter: 308 loss: 5.54326107e-07
Iter: 309 loss: 5.53427526e-07
Iter: 310 loss: 5.53192422e-07
Iter: 311 loss: 5.52148094e-07
Iter: 312 loss: 5.53369e-07
Iter: 313 loss: 5.51594e-07
Iter: 314 loss: 5.5050333e-07
Iter: 315 loss: 5.48680532e-07
Iter: 316 loss: 5.48692583e-07
Iter: 317 loss: 5.46570959e-07
Iter: 318 loss: 5.69728627e-07
Iter: 319 loss: 5.46536057e-07
Iter: 320 loss: 5.4519171e-07
Iter: 321 loss: 5.54026087e-07
Iter: 322 loss: 5.45064154e-07
Iter: 323 loss: 5.44250838e-07
Iter: 324 loss: 5.4357929e-07
Iter: 325 loss: 5.43367037e-07
Iter: 326 loss: 5.42352836e-07
Iter: 327 loss: 5.42338341e-07
Iter: 328 loss: 5.41716815e-07
Iter: 329 loss: 5.40536e-07
Iter: 330 loss: 5.64882384e-07
Iter: 331 loss: 5.40541123e-07
Iter: 332 loss: 5.39698249e-07
Iter: 333 loss: 5.39698931e-07
Iter: 334 loss: 5.38835479e-07
Iter: 335 loss: 5.39333314e-07
Iter: 336 loss: 5.38262839e-07
Iter: 337 loss: 5.37308551e-07
Iter: 338 loss: 5.35707898e-07
Iter: 339 loss: 5.35714776e-07
Iter: 340 loss: 5.34005153e-07
Iter: 341 loss: 5.44473323e-07
Iter: 342 loss: 5.33806e-07
Iter: 343 loss: 5.32670299e-07
Iter: 344 loss: 5.32609135e-07
Iter: 345 loss: 5.32072534e-07
Iter: 346 loss: 5.31249611e-07
Iter: 347 loss: 5.31241881e-07
Iter: 348 loss: 5.30369732e-07
Iter: 349 loss: 5.35387e-07
Iter: 350 loss: 5.30248371e-07
Iter: 351 loss: 5.29477916e-07
Iter: 352 loss: 5.33410912e-07
Iter: 353 loss: 5.29359e-07
Iter: 354 loss: 5.28746568e-07
Iter: 355 loss: 5.29005206e-07
Iter: 356 loss: 5.28307851e-07
Iter: 357 loss: 5.27648126e-07
Iter: 358 loss: 5.30921625e-07
Iter: 359 loss: 5.27536486e-07
Iter: 360 loss: 5.26705776e-07
Iter: 361 loss: 5.26765575e-07
Iter: 362 loss: 5.26066856e-07
Iter: 363 loss: 5.25184362e-07
Iter: 364 loss: 5.25429584e-07
Iter: 365 loss: 5.24558857e-07
Iter: 366 loss: 5.23829726e-07
Iter: 367 loss: 5.23766403e-07
Iter: 368 loss: 5.23219171e-07
Iter: 369 loss: 5.22070422e-07
Iter: 370 loss: 5.39694e-07
Iter: 371 loss: 5.22011192e-07
Iter: 372 loss: 5.2106077e-07
Iter: 373 loss: 5.25175551e-07
Iter: 374 loss: 5.20878586e-07
Iter: 375 loss: 5.20456e-07
Iter: 376 loss: 5.20405e-07
Iter: 377 loss: 5.19919524e-07
Iter: 378 loss: 5.19405035e-07
Iter: 379 loss: 5.1931579e-07
Iter: 380 loss: 5.186713e-07
Iter: 381 loss: 5.18794081e-07
Iter: 382 loss: 5.18175739e-07
Iter: 383 loss: 5.17313367e-07
Iter: 384 loss: 5.27963607e-07
Iter: 385 loss: 5.17319165e-07
Iter: 386 loss: 5.16694968e-07
Iter: 387 loss: 5.16983448e-07
Iter: 388 loss: 5.16278419e-07
Iter: 389 loss: 5.15492502e-07
Iter: 390 loss: 5.16398472e-07
Iter: 391 loss: 5.15073225e-07
Iter: 392 loss: 5.1435984e-07
Iter: 393 loss: 5.24575626e-07
Iter: 394 loss: 5.14351314e-07
Iter: 395 loss: 5.13912539e-07
Iter: 396 loss: 5.13052328e-07
Iter: 397 loss: 5.29963131e-07
Iter: 398 loss: 5.13043517e-07
Iter: 399 loss: 5.12488214e-07
Iter: 400 loss: 5.12464226e-07
Iter: 401 loss: 5.11897667e-07
Iter: 402 loss: 5.12071608e-07
Iter: 403 loss: 5.11501241e-07
Iter: 404 loss: 5.108717e-07
Iter: 405 loss: 5.09832546e-07
Iter: 406 loss: 5.09828283e-07
Iter: 407 loss: 5.09189192e-07
Iter: 408 loss: 5.09163328e-07
Iter: 409 loss: 5.08491439e-07
Iter: 410 loss: 5.11659664e-07
Iter: 411 loss: 5.08368657e-07
Iter: 412 loss: 5.07952791e-07
Iter: 413 loss: 5.07315917e-07
Iter: 414 loss: 5.07315e-07
Iter: 415 loss: 5.06708147e-07
Iter: 416 loss: 5.15164061e-07
Iter: 417 loss: 5.06715764e-07
Iter: 418 loss: 5.0611186e-07
Iter: 419 loss: 5.06363392e-07
Iter: 420 loss: 5.05705657e-07
Iter: 421 loss: 5.05092316e-07
Iter: 422 loss: 5.0712265e-07
Iter: 423 loss: 5.04899674e-07
Iter: 424 loss: 5.04391437e-07
Iter: 425 loss: 5.07210302e-07
Iter: 426 loss: 5.04326749e-07
Iter: 427 loss: 5.03747628e-07
Iter: 428 loss: 5.03135766e-07
Iter: 429 loss: 5.03028559e-07
Iter: 430 loss: 5.02254579e-07
Iter: 431 loss: 5.053314e-07
Iter: 432 loss: 5.02064154e-07
Iter: 433 loss: 5.01367424e-07
Iter: 434 loss: 5.09857387e-07
Iter: 435 loss: 5.01362138e-07
Iter: 436 loss: 5.010059e-07
Iter: 437 loss: 5.0039489e-07
Iter: 438 loss: 5.00395174e-07
Iter: 439 loss: 4.99737439e-07
Iter: 440 loss: 5.01137947e-07
Iter: 441 loss: 4.9946982e-07
Iter: 442 loss: 4.99322937e-07
Iter: 443 loss: 4.99126259e-07
Iter: 444 loss: 4.9888763e-07
Iter: 445 loss: 4.98249619e-07
Iter: 446 loss: 5.02535272e-07
Iter: 447 loss: 4.98099496e-07
Iter: 448 loss: 4.97379176e-07
Iter: 449 loss: 5.02419255e-07
Iter: 450 loss: 4.97319661e-07
Iter: 451 loss: 4.96624921e-07
Iter: 452 loss: 5.00493911e-07
Iter: 453 loss: 4.96515668e-07
Iter: 454 loss: 4.95943311e-07
Iter: 455 loss: 4.95682173e-07
Iter: 456 loss: 4.95407448e-07
Iter: 457 loss: 4.94762503e-07
Iter: 458 loss: 5.01064619e-07
Iter: 459 loss: 4.94735218e-07
Iter: 460 loss: 4.943048e-07
Iter: 461 loss: 4.96221446e-07
Iter: 462 loss: 4.94205153e-07
Iter: 463 loss: 4.93872903e-07
Iter: 464 loss: 4.93525931e-07
Iter: 465 loss: 4.93454309e-07
Iter: 466 loss: 4.93124389e-07
Iter: 467 loss: 4.93117284e-07
Iter: 468 loss: 4.92781624e-07
Iter: 469 loss: 4.92252e-07
Iter: 470 loss: 4.92258494e-07
Iter: 471 loss: 4.91677156e-07
Iter: 472 loss: 4.91337516e-07
Iter: 473 loss: 4.91093829e-07
Iter: 474 loss: 4.91032438e-07
Iter: 475 loss: 4.90741172e-07
Iter: 476 loss: 4.90362595e-07
Iter: 477 loss: 4.90034154e-07
Iter: 478 loss: 4.89943886e-07
Iter: 479 loss: 4.89483e-07
Iter: 480 loss: 4.89282911e-07
Iter: 481 loss: 4.89057072e-07
Iter: 482 loss: 4.8875404e-07
Iter: 483 loss: 4.88689921e-07
Iter: 484 loss: 4.88422643e-07
Iter: 485 loss: 4.8810864e-07
Iter: 486 loss: 4.88067826e-07
Iter: 487 loss: 4.87631951e-07
Iter: 488 loss: 4.89325316e-07
Iter: 489 loss: 4.87501154e-07
Iter: 490 loss: 4.87079831e-07
Iter: 491 loss: 4.88851526e-07
Iter: 492 loss: 4.86988142e-07
Iter: 493 loss: 4.86566e-07
Iter: 494 loss: 4.86377417e-07
Iter: 495 loss: 4.86151521e-07
Iter: 496 loss: 4.85580074e-07
Iter: 497 loss: 4.88063108e-07
Iter: 498 loss: 4.8547156e-07
Iter: 499 loss: 4.84767952e-07
Iter: 500 loss: 4.87076932e-07
Iter: 501 loss: 4.84590942e-07
Iter: 502 loss: 4.8411664e-07
Iter: 503 loss: 4.83812187e-07
Iter: 504 loss: 4.83651206e-07
Iter: 505 loss: 4.8315087e-07
Iter: 506 loss: 4.85394082e-07
Iter: 507 loss: 4.83041731e-07
Iter: 508 loss: 4.82571352e-07
Iter: 509 loss: 4.89376134e-07
Iter: 510 loss: 4.82572545e-07
Iter: 511 loss: 4.82386326e-07
Iter: 512 loss: 4.82026e-07
Iter: 513 loss: 4.88401554e-07
Iter: 514 loss: 4.82013547e-07
Iter: 515 loss: 4.81618372e-07
Iter: 516 loss: 4.84878569e-07
Iter: 517 loss: 4.81601205e-07
Iter: 518 loss: 4.81157372e-07
Iter: 519 loss: 4.81799134e-07
Iter: 520 loss: 4.80944493e-07
Iter: 521 loss: 4.80554661e-07
Iter: 522 loss: 4.80819e-07
Iter: 523 loss: 4.80310575e-07
Iter: 524 loss: 4.79771074e-07
Iter: 525 loss: 4.82476366e-07
Iter: 526 loss: 4.79671883e-07
Iter: 527 loss: 4.79213782e-07
Iter: 528 loss: 4.80390554e-07
Iter: 529 loss: 4.79049277e-07
Iter: 530 loss: 4.7870742e-07
Iter: 531 loss: 4.79015682e-07
Iter: 532 loss: 4.78496759e-07
Iter: 533 loss: 4.78214702e-07
Iter: 534 loss: 4.7821834e-07
Iter: 535 loss: 4.78017114e-07
Iter: 536 loss: 4.77628134e-07
Iter: 537 loss: 4.84986629e-07
Iter: 538 loss: 4.77604488e-07
Iter: 539 loss: 4.77142407e-07
Iter: 540 loss: 4.779439e-07
Iter: 541 loss: 4.76934e-07
Iter: 542 loss: 4.76854325e-07
Iter: 543 loss: 4.76718355e-07
Iter: 544 loss: 4.76537082e-07
Iter: 545 loss: 4.76009632e-07
Iter: 546 loss: 4.78356924e-07
Iter: 547 loss: 4.75795673e-07
Iter: 548 loss: 4.75255433e-07
Iter: 549 loss: 4.80338201e-07
Iter: 550 loss: 4.75236021e-07
Iter: 551 loss: 4.74766182e-07
Iter: 552 loss: 4.79618848e-07
Iter: 553 loss: 4.74745661e-07
Iter: 554 loss: 4.74482164e-07
Iter: 555 loss: 4.74223071e-07
Iter: 556 loss: 4.74163272e-07
Iter: 557 loss: 4.73818545e-07
Iter: 558 loss: 4.77777405e-07
Iter: 559 loss: 4.73822467e-07
Iter: 560 loss: 4.7355519e-07
Iter: 561 loss: 4.73930527e-07
Iter: 562 loss: 4.73438746e-07
Iter: 563 loss: 4.73147963e-07
Iter: 564 loss: 4.73247951e-07
Iter: 565 loss: 4.72926956e-07
Iter: 566 loss: 4.72614033e-07
Iter: 567 loss: 4.7532086e-07
Iter: 568 loss: 4.72576829e-07
Iter: 569 loss: 4.72230738e-07
Iter: 570 loss: 4.71872625e-07
Iter: 571 loss: 4.71776559e-07
Iter: 572 loss: 4.71255504e-07
Iter: 573 loss: 4.71395623e-07
Iter: 574 loss: 4.70879911e-07
Iter: 575 loss: 4.70670017e-07
Iter: 576 loss: 4.70592738e-07
Iter: 577 loss: 4.70288683e-07
Iter: 578 loss: 4.70101469e-07
Iter: 579 loss: 4.69974282e-07
Iter: 580 loss: 4.69654594e-07
Iter: 581 loss: 4.69503107e-07
Iter: 582 loss: 4.69341188e-07
Iter: 583 loss: 4.69145874e-07
Iter: 584 loss: 4.69090423e-07
Iter: 585 loss: 4.68861714e-07
Iter: 586 loss: 4.6853819e-07
Iter: 587 loss: 4.68521478e-07
Iter: 588 loss: 4.68174363e-07
Iter: 589 loss: 4.69546421e-07
Iter: 590 loss: 4.68083442e-07
Iter: 591 loss: 4.67695486e-07
Iter: 592 loss: 4.69416875e-07
Iter: 593 loss: 4.67615735e-07
Iter: 594 loss: 4.67243098e-07
Iter: 595 loss: 4.67364572e-07
Iter: 596 loss: 4.67019106e-07
Iter: 597 loss: 4.66644565e-07
Iter: 598 loss: 4.68674614e-07
Iter: 599 loss: 4.66578655e-07
Iter: 600 loss: 4.66242341e-07
Iter: 601 loss: 4.68677115e-07
Iter: 602 loss: 4.66215056e-07
Iter: 603 loss: 4.66004479e-07
Iter: 604 loss: 4.65673025e-07
Iter: 605 loss: 4.65672144e-07
Iter: 606 loss: 4.65334722e-07
Iter: 607 loss: 4.67441396e-07
Iter: 608 loss: 4.65293425e-07
Iter: 609 loss: 4.65021486e-07
Iter: 610 loss: 4.69069107e-07
Iter: 611 loss: 4.65034816e-07
Iter: 612 loss: 4.64874972e-07
Iter: 613 loss: 4.64472e-07
Iter: 614 loss: 4.66321069e-07
Iter: 615 loss: 4.64318191e-07
Iter: 616 loss: 4.63921594e-07
Iter: 617 loss: 4.63917161e-07
Iter: 618 loss: 4.63551714e-07
Iter: 619 loss: 4.65457447e-07
Iter: 620 loss: 4.63504023e-07
Iter: 621 loss: 4.63278695e-07
Iter: 622 loss: 4.62804138e-07
Iter: 623 loss: 4.70897589e-07
Iter: 624 loss: 4.62790297e-07
Iter: 625 loss: 4.62648586e-07
Iter: 626 loss: 4.62542772e-07
Iter: 627 loss: 4.62381394e-07
Iter: 628 loss: 4.62192531e-07
Iter: 629 loss: 4.62165758e-07
Iter: 630 loss: 4.61841068e-07
Iter: 631 loss: 4.6236687e-07
Iter: 632 loss: 4.61675313e-07
Iter: 633 loss: 4.61377169e-07
Iter: 634 loss: 4.64771574e-07
Iter: 635 loss: 4.61377056e-07
Iter: 636 loss: 4.61125325e-07
Iter: 637 loss: 4.60954908e-07
Iter: 638 loss: 4.60879846e-07
Iter: 639 loss: 4.60575166e-07
Iter: 640 loss: 4.6066765e-07
Iter: 641 loss: 4.60355352e-07
Iter: 642 loss: 4.60144349e-07
Iter: 643 loss: 4.60096885e-07
Iter: 644 loss: 4.59870705e-07
Iter: 645 loss: 4.59450177e-07
Iter: 646 loss: 4.68750386e-07
Iter: 647 loss: 4.59454583e-07
Iter: 648 loss: 4.59163289e-07
Iter: 649 loss: 4.60029497e-07
Iter: 650 loss: 4.59104712e-07
Iter: 651 loss: 4.58813076e-07
Iter: 652 loss: 4.61897116e-07
Iter: 653 loss: 4.58815748e-07
Iter: 654 loss: 4.58611282e-07
Iter: 655 loss: 4.58336672e-07
Iter: 656 loss: 4.58317686e-07
Iter: 657 loss: 4.58038585e-07
Iter: 658 loss: 4.59547152e-07
Iter: 659 loss: 4.5798123e-07
Iter: 660 loss: 4.57624481e-07
Iter: 661 loss: 4.58038983e-07
Iter: 662 loss: 4.57410749e-07
Iter: 663 loss: 4.57108627e-07
Iter: 664 loss: 4.5812692e-07
Iter: 665 loss: 4.57048088e-07
Iter: 666 loss: 4.56750172e-07
Iter: 667 loss: 4.58106598e-07
Iter: 668 loss: 4.56716208e-07
Iter: 669 loss: 4.56418718e-07
Iter: 670 loss: 4.57010458e-07
Iter: 671 loss: 4.56300768e-07
Iter: 672 loss: 4.56058274e-07
Iter: 673 loss: 4.55841018e-07
Iter: 674 loss: 4.55795174e-07
Iter: 675 loss: 4.55519398e-07
Iter: 676 loss: 4.59222775e-07
Iter: 677 loss: 4.55519228e-07
Iter: 678 loss: 4.55221823e-07
Iter: 679 loss: 4.5600251e-07
Iter: 680 loss: 4.55122745e-07
Iter: 681 loss: 4.54919245e-07
Iter: 682 loss: 4.5449022e-07
Iter: 683 loss: 4.6176217e-07
Iter: 684 loss: 4.54493176e-07
Iter: 685 loss: 4.54359281e-07
Iter: 686 loss: 4.54280411e-07
Iter: 687 loss: 4.54063922e-07
Iter: 688 loss: 4.53905074e-07
Iter: 689 loss: 4.53848202e-07
Iter: 690 loss: 4.53569157e-07
Iter: 691 loss: 4.53885775e-07
Iter: 692 loss: 4.53410621e-07
Iter: 693 loss: 4.53194303e-07
Iter: 694 loss: 4.53191461e-07
Iter: 695 loss: 4.5303986e-07
Iter: 696 loss: 4.52789e-07
Iter: 697 loss: 4.52796456e-07
Iter: 698 loss: 4.52573147e-07
Iter: 699 loss: 4.55619443e-07
Iter: 700 loss: 4.52583095e-07
Iter: 701 loss: 4.52358591e-07
Iter: 702 loss: 4.52636243e-07
Iter: 703 loss: 4.52245956e-07
Iter: 704 loss: 4.51971232e-07
Iter: 705 loss: 4.51876872e-07
Iter: 706 loss: 4.51715948e-07
Iter: 707 loss: 4.51407914e-07
Iter: 708 loss: 4.51748491e-07
Iter: 709 loss: 4.5124915e-07
Iter: 710 loss: 4.51016092e-07
Iter: 711 loss: 4.50986619e-07
Iter: 712 loss: 4.50826633e-07
Iter: 713 loss: 4.50570695e-07
Iter: 714 loss: 4.50571349e-07
Iter: 715 loss: 4.50328969e-07
Iter: 716 loss: 4.50277923e-07
Iter: 717 loss: 4.50117909e-07
Iter: 718 loss: 4.50059076e-07
Iter: 719 loss: 4.49968951e-07
Iter: 720 loss: 4.49841025e-07
Iter: 721 loss: 4.49595859e-07
Iter: 722 loss: 4.54469813e-07
Iter: 723 loss: 4.49593472e-07
Iter: 724 loss: 4.49307606e-07
Iter: 725 loss: 4.4936877e-07
Iter: 726 loss: 4.49112065e-07
Iter: 727 loss: 4.48753099e-07
Iter: 728 loss: 4.54252245e-07
Iter: 729 loss: 4.48748494e-07
Iter: 730 loss: 4.4856688e-07
Iter: 731 loss: 4.48243782e-07
Iter: 732 loss: 4.56174803e-07
Iter: 733 loss: 4.48247647e-07
Iter: 734 loss: 4.47887601e-07
Iter: 735 loss: 4.48686961e-07
Iter: 736 loss: 4.47770276e-07
Iter: 737 loss: 4.47616e-07
Iter: 738 loss: 4.47603867e-07
Iter: 739 loss: 4.4744678e-07
Iter: 740 loss: 4.47584597e-07
Iter: 741 loss: 4.47346764e-07
Iter: 742 loss: 4.47186721e-07
Iter: 743 loss: 4.47366602e-07
Iter: 744 loss: 4.47082215e-07
Iter: 745 loss: 4.46874424e-07
Iter: 746 loss: 4.46837248e-07
Iter: 747 loss: 4.46710033e-07
Iter: 748 loss: 4.46585631e-07
Iter: 749 loss: 4.46558261e-07
Iter: 750 loss: 4.46378e-07
Iter: 751 loss: 4.46102916e-07
Iter: 752 loss: 4.46086631e-07
Iter: 753 loss: 4.45817648e-07
Iter: 754 loss: 4.46289334e-07
Iter: 755 loss: 4.45712203e-07
Iter: 756 loss: 4.45631031e-07
Iter: 757 loss: 4.45580469e-07
Iter: 758 loss: 4.45480083e-07
Iter: 759 loss: 4.45209821e-07
Iter: 760 loss: 4.48154566e-07
Iter: 761 loss: 4.45174038e-07
Iter: 762 loss: 4.45015e-07
Iter: 763 loss: 4.45020447e-07
Iter: 764 loss: 4.4485e-07
Iter: 765 loss: 4.44890674e-07
Iter: 766 loss: 4.44716477e-07
Iter: 767 loss: 4.44533612e-07
Iter: 768 loss: 4.4437752e-07
Iter: 769 loss: 4.44320904e-07
Iter: 770 loss: 4.43964325e-07
Iter: 771 loss: 4.44738788e-07
Iter: 772 loss: 4.43854503e-07
Iter: 773 loss: 4.43662827e-07
Iter: 774 loss: 4.43632445e-07
Iter: 775 loss: 4.43514153e-07
Iter: 776 loss: 4.43394214e-07
Iter: 777 loss: 4.43387307e-07
Iter: 778 loss: 4.43203334e-07
Iter: 779 loss: 4.43673628e-07
Iter: 780 loss: 4.43133217e-07
Iter: 781 loss: 4.42989915e-07
Iter: 782 loss: 4.4424479e-07
Iter: 783 loss: 4.42964733e-07
Iter: 784 loss: 4.42828878e-07
Iter: 785 loss: 4.43210496e-07
Iter: 786 loss: 4.42775786e-07
Iter: 787 loss: 4.42655761e-07
Iter: 788 loss: 4.4246795e-07
Iter: 789 loss: 4.42454166e-07
Iter: 790 loss: 4.42278917e-07
Iter: 791 loss: 4.42264195e-07
Iter: 792 loss: 4.42108018e-07
Iter: 793 loss: 4.41766815e-07
Iter: 794 loss: 4.46847082e-07
Iter: 795 loss: 4.41764428e-07
Iter: 796 loss: 4.41474356e-07
Iter: 797 loss: 4.43002335e-07
Iter: 798 loss: 4.41459548e-07
Iter: 799 loss: 4.41273215e-07
Iter: 800 loss: 4.41271425e-07
Iter: 801 loss: 4.41156601e-07
Iter: 802 loss: 4.40941847e-07
Iter: 803 loss: 4.45120037e-07
Iter: 804 loss: 4.40936446e-07
Iter: 805 loss: 4.40733629e-07
Iter: 806 loss: 4.42220426e-07
Iter: 807 loss: 4.40720726e-07
Iter: 808 loss: 4.40513503e-07
Iter: 809 loss: 4.41583722e-07
Iter: 810 loss: 4.4048457e-07
Iter: 811 loss: 4.40326318e-07
Iter: 812 loss: 4.40117475e-07
Iter: 813 loss: 4.40102355e-07
Iter: 814 loss: 4.39810776e-07
Iter: 815 loss: 4.41057068e-07
Iter: 816 loss: 4.39759532e-07
Iter: 817 loss: 4.39537814e-07
Iter: 818 loss: 4.42884698e-07
Iter: 819 loss: 4.39528e-07
Iter: 820 loss: 4.39367454e-07
Iter: 821 loss: 4.39423047e-07
Iter: 822 loss: 4.3924814e-07
Iter: 823 loss: 4.39098216e-07
Iter: 824 loss: 4.3903816e-07
Iter: 825 loss: 4.3894488e-07
Iter: 826 loss: 4.38764289e-07
Iter: 827 loss: 4.3876264e-07
Iter: 828 loss: 4.38692837e-07
Iter: 829 loss: 4.38557549e-07
Iter: 830 loss: 4.40093686e-07
Iter: 831 loss: 4.38520487e-07
Iter: 832 loss: 4.38310252e-07
Iter: 833 loss: 4.39875663e-07
Iter: 834 loss: 4.38311076e-07
Iter: 835 loss: 4.38094503e-07
Iter: 836 loss: 4.38875588e-07
Iter: 837 loss: 4.38043685e-07
Iter: 838 loss: 4.3788296e-07
Iter: 839 loss: 4.37612869e-07
Iter: 840 loss: 4.44252805e-07
Iter: 841 loss: 4.37605593e-07
Iter: 842 loss: 4.37447511e-07
Iter: 843 loss: 4.37416077e-07
Iter: 844 loss: 4.37223349e-07
Iter: 845 loss: 4.37006747e-07
Iter: 846 loss: 4.36983385e-07
Iter: 847 loss: 4.36753879e-07
Iter: 848 loss: 4.372273e-07
Iter: 849 loss: 4.36652186e-07
Iter: 850 loss: 4.36512494e-07
Iter: 851 loss: 4.36499676e-07
Iter: 852 loss: 4.36359471e-07
Iter: 853 loss: 4.36503967e-07
Iter: 854 loss: 4.36286513e-07
Iter: 855 loss: 4.36133774e-07
Iter: 856 loss: 4.36251753e-07
Iter: 857 loss: 4.36056553e-07
Iter: 858 loss: 4.3589651e-07
Iter: 859 loss: 4.36218215e-07
Iter: 860 loss: 4.35820795e-07
Iter: 861 loss: 4.3554968e-07
Iter: 862 loss: 4.3596043e-07
Iter: 863 loss: 4.35440654e-07
Iter: 864 loss: 4.35257277e-07
Iter: 865 loss: 4.34985282e-07
Iter: 866 loss: 4.34968911e-07
Iter: 867 loss: 4.34869463e-07
Iter: 868 loss: 4.34801905e-07
Iter: 869 loss: 4.34684353e-07
Iter: 870 loss: 4.34690548e-07
Iter: 871 loss: 4.34588657e-07
Iter: 872 loss: 4.34460816e-07
Iter: 873 loss: 4.34446491e-07
Iter: 874 loss: 4.34352188e-07
Iter: 875 loss: 4.342e-07
Iter: 876 loss: 4.34198881e-07
Iter: 877 loss: 4.34106397e-07
Iter: 878 loss: 4.33891159e-07
Iter: 879 loss: 4.36818027e-07
Iter: 880 loss: 4.33875414e-07
Iter: 881 loss: 4.33665207e-07
Iter: 882 loss: 4.33654748e-07
Iter: 883 loss: 4.33485724e-07
Iter: 884 loss: 4.33235869e-07
Iter: 885 loss: 4.35867634e-07
Iter: 886 loss: 4.33248118e-07
Iter: 887 loss: 4.33054481e-07
Iter: 888 loss: 4.34772687e-07
Iter: 889 loss: 4.33038963e-07
Iter: 890 loss: 4.32963333e-07
Iter: 891 loss: 4.32743548e-07
Iter: 892 loss: 4.34229094e-07
Iter: 893 loss: 4.32693668e-07
Iter: 894 loss: 4.32675506e-07
Iter: 895 loss: 4.32596721e-07
Iter: 896 loss: 4.32494232e-07
Iter: 897 loss: 4.32396831e-07
Iter: 898 loss: 4.32366221e-07
Iter: 899 loss: 4.32224397e-07
Iter: 900 loss: 4.32353232e-07
Iter: 901 loss: 4.32131912e-07
Iter: 902 loss: 4.31942112e-07
Iter: 903 loss: 4.33201819e-07
Iter: 904 loss: 4.31941345e-07
Iter: 905 loss: 4.31788436e-07
Iter: 906 loss: 4.32321144e-07
Iter: 907 loss: 4.31758508e-07
Iter: 908 loss: 4.31631406e-07
Iter: 909 loss: 4.31439901e-07
Iter: 910 loss: 4.31440355e-07
Iter: 911 loss: 4.31328772e-07
Iter: 912 loss: 4.31318483e-07
Iter: 913 loss: 4.31201272e-07
Iter: 914 loss: 4.31090655e-07
Iter: 915 loss: 4.31084e-07
Iter: 916 loss: 4.3094343e-07
Iter: 917 loss: 4.30895255e-07
Iter: 918 loss: 4.30834916e-07
Iter: 919 loss: 4.30765454e-07
Iter: 920 loss: 4.30734644e-07
Iter: 921 loss: 4.30630621e-07
Iter: 922 loss: 4.30457334e-07
Iter: 923 loss: 4.34802217e-07
Iter: 924 loss: 4.30451877e-07
Iter: 925 loss: 4.3028075e-07
Iter: 926 loss: 4.30708496e-07
Iter: 927 loss: 4.30217284e-07
Iter: 928 loss: 4.30013387e-07
Iter: 929 loss: 4.3208297e-07
Iter: 930 loss: 4.30002643e-07
Iter: 931 loss: 4.2985414e-07
Iter: 932 loss: 4.29683155e-07
Iter: 933 loss: 4.29677868e-07
Iter: 934 loss: 4.29463341e-07
Iter: 935 loss: 4.30025494e-07
Iter: 936 loss: 4.29399364e-07
Iter: 937 loss: 4.29174122e-07
Iter: 938 loss: 4.30880334e-07
Iter: 939 loss: 4.29161929e-07
Iter: 940 loss: 4.2901604e-07
Iter: 941 loss: 4.294223e-07
Iter: 942 loss: 4.28956781e-07
Iter: 943 loss: 4.28807056e-07
Iter: 944 loss: 4.28705562e-07
Iter: 945 loss: 4.28656392e-07
Iter: 946 loss: 4.28485919e-07
Iter: 947 loss: 4.28479439e-07
Iter: 948 loss: 4.28405912e-07
Iter: 949 loss: 4.28207954e-07
Iter: 950 loss: 4.31038416e-07
Iter: 951 loss: 4.28190958e-07
Iter: 952 loss: 4.27997946e-07
Iter: 953 loss: 4.29142858e-07
Iter: 954 loss: 4.2798456e-07
Iter: 955 loss: 4.27813745e-07
Iter: 956 loss: 4.2782591e-07
Iter: 957 loss: 4.27748517e-07
Iter: 958 loss: 4.27565624e-07
Iter: 959 loss: 4.29691113e-07
Iter: 960 loss: 4.27549708e-07
Iter: 961 loss: 4.27391228e-07
Iter: 962 loss: 4.29574e-07
Iter: 963 loss: 4.27395747e-07
Iter: 964 loss: 4.27217799e-07
Iter: 965 loss: 4.27331713e-07
Iter: 966 loss: 4.27114202e-07
Iter: 967 loss: 4.26955069e-07
Iter: 968 loss: 4.26849425e-07
Iter: 969 loss: 4.26811255e-07
Iter: 970 loss: 4.26611905e-07
Iter: 971 loss: 4.28511498e-07
Iter: 972 loss: 4.26590816e-07
Iter: 973 loss: 4.26407496e-07
Iter: 974 loss: 4.26501458e-07
Iter: 975 loss: 4.26264364e-07
Iter: 976 loss: 4.26084057e-07
Iter: 977 loss: 4.27180595e-07
Iter: 978 loss: 4.26067572e-07
Iter: 979 loss: 4.25940812e-07
Iter: 980 loss: 4.2622861e-07
Iter: 981 loss: 4.25878795e-07
Iter: 982 loss: 4.25728672e-07
Iter: 983 loss: 4.26605084e-07
Iter: 984 loss: 4.2570997e-07
Iter: 985 loss: 4.25605663e-07
Iter: 986 loss: 4.25504027e-07
Iter: 987 loss: 4.25488565e-07
Iter: 988 loss: 4.25367631e-07
Iter: 989 loss: 4.26234834e-07
Iter: 990 loss: 4.25354926e-07
Iter: 991 loss: 4.25195594e-07
Iter: 992 loss: 4.2544238e-07
Iter: 993 loss: 4.25128405e-07
Iter: 994 loss: 4.24992948e-07
Iter: 995 loss: 4.24878635e-07
Iter: 996 loss: 4.24856239e-07
Iter: 997 loss: 4.24704069e-07
Iter: 998 loss: 4.24700602e-07
Iter: 999 loss: 4.24560199e-07
Iter: 1000 loss: 4.2438495e-07
Iter: 1001 loss: 4.24361872e-07
Iter: 1002 loss: 4.24211692e-07
Iter: 1003 loss: 4.24360508e-07
Iter: 1004 loss: 4.24096925e-07
Iter: 1005 loss: 4.24044373e-07
Iter: 1006 loss: 4.24018822e-07
Iter: 1007 loss: 4.23937024e-07
Iter: 1008 loss: 4.23854829e-07
Iter: 1009 loss: 4.23843119e-07
Iter: 1010 loss: 4.23700499e-07
Iter: 1011 loss: 4.24161385e-07
Iter: 1012 loss: 4.23665512e-07
Iter: 1013 loss: 4.23553047e-07
Iter: 1014 loss: 4.24308666e-07
Iter: 1015 loss: 4.23520675e-07
Iter: 1016 loss: 4.2338138e-07
Iter: 1017 loss: 4.23408267e-07
Iter: 1018 loss: 4.23275679e-07
Iter: 1019 loss: 4.2310117e-07
Iter: 1020 loss: 4.2313502e-07
Iter: 1021 loss: 4.22976484e-07
Iter: 1022 loss: 4.2280513e-07
Iter: 1023 loss: 4.23068855e-07
Iter: 1024 loss: 4.22717591e-07
Iter: 1025 loss: 4.22583526e-07
Iter: 1026 loss: 4.22565876e-07
Iter: 1027 loss: 4.22514063e-07
Iter: 1028 loss: 4.22365162e-07
Iter: 1029 loss: 4.23395875e-07
Iter: 1030 loss: 4.22327787e-07
Iter: 1031 loss: 4.22137049e-07
Iter: 1032 loss: 4.2277577e-07
Iter: 1033 loss: 4.22097514e-07
Iter: 1034 loss: 4.2201097e-07
Iter: 1035 loss: 4.21986556e-07
Iter: 1036 loss: 4.2190652e-07
Iter: 1037 loss: 4.21691595e-07
Iter: 1038 loss: 4.22683144e-07
Iter: 1039 loss: 4.21611304e-07
Iter: 1040 loss: 4.21401808e-07
Iter: 1041 loss: 4.24483886e-07
Iter: 1042 loss: 4.21397772e-07
Iter: 1043 loss: 4.21215958e-07
Iter: 1044 loss: 4.22553228e-07
Iter: 1045 loss: 4.21191373e-07
Iter: 1046 loss: 4.21090363e-07
Iter: 1047 loss: 4.20971674e-07
Iter: 1048 loss: 4.20963943e-07
Iter: 1049 loss: 4.20840365e-07
Iter: 1050 loss: 4.20829082e-07
Iter: 1051 loss: 4.20749871e-07
Iter: 1052 loss: 4.2070269e-07
Iter: 1053 loss: 4.20664094e-07
Iter: 1054 loss: 4.20552965e-07
Iter: 1055 loss: 4.20676258e-07
Iter: 1056 loss: 4.20483047e-07
Iter: 1057 loss: 4.20338552e-07
Iter: 1058 loss: 4.20251638e-07
Iter: 1059 loss: 4.2018786e-07
Iter: 1060 loss: 4.19945593e-07
Iter: 1061 loss: 4.20533212e-07
Iter: 1062 loss: 4.1985524e-07
Iter: 1063 loss: 4.19810419e-07
Iter: 1064 loss: 4.19729417e-07
Iter: 1065 loss: 4.19612888e-07
Iter: 1066 loss: 4.19402909e-07
Iter: 1067 loss: 4.22358255e-07
Iter: 1068 loss: 4.1939245e-07
Iter: 1069 loss: 4.19209442e-07
Iter: 1070 loss: 4.19530664e-07
Iter: 1071 loss: 4.19153594e-07
Iter: 1072 loss: 4.19076457e-07
Iter: 1073 loss: 4.19036041e-07
Iter: 1074 loss: 4.18963452e-07
Iter: 1075 loss: 4.18836748e-07
Iter: 1076 loss: 4.18835839e-07
Iter: 1077 loss: 4.18678837e-07
Iter: 1078 loss: 4.18660221e-07
Iter: 1079 loss: 4.18562905e-07
Iter: 1080 loss: 4.18482699e-07
Iter: 1081 loss: 4.18452373e-07
Iter: 1082 loss: 4.18353352e-07
Iter: 1083 loss: 4.18108073e-07
Iter: 1084 loss: 4.21404962e-07
Iter: 1085 loss: 4.18091872e-07
Iter: 1086 loss: 4.17963491e-07
Iter: 1087 loss: 4.17947206e-07
Iter: 1088 loss: 4.17816722e-07
Iter: 1089 loss: 4.17808963e-07
Iter: 1090 loss: 4.17704484e-07
Iter: 1091 loss: 4.17567492e-07
Iter: 1092 loss: 4.17576643e-07
Iter: 1093 loss: 4.17481459e-07
Iter: 1094 loss: 4.17324515e-07
Iter: 1095 loss: 4.18350282e-07
Iter: 1096 loss: 4.17317125e-07
Iter: 1097 loss: 4.1718954e-07
Iter: 1098 loss: 4.17748311e-07
Iter: 1099 loss: 4.17159299e-07
Iter: 1100 loss: 4.17028161e-07
Iter: 1101 loss: 4.1766171e-07
Iter: 1102 loss: 4.17001388e-07
Iter: 1103 loss: 4.16914645e-07
Iter: 1104 loss: 4.16751448e-07
Iter: 1105 loss: 4.20177741e-07
Iter: 1106 loss: 4.16756166e-07
Iter: 1107 loss: 4.16640262e-07
Iter: 1108 loss: 4.16633469e-07
Iter: 1109 loss: 4.16516571e-07
Iter: 1110 loss: 4.16327168e-07
Iter: 1111 loss: 4.20273182e-07
Iter: 1112 loss: 4.16330494e-07
Iter: 1113 loss: 4.16150499e-07
Iter: 1114 loss: 4.1726e-07
Iter: 1115 loss: 4.16121537e-07
Iter: 1116 loss: 4.16010153e-07
Iter: 1117 loss: 4.16010607e-07
Iter: 1118 loss: 4.15950694e-07
Iter: 1119 loss: 4.15807477e-07
Iter: 1120 loss: 4.17262754e-07
Iter: 1121 loss: 4.15824672e-07
Iter: 1122 loss: 4.15728664e-07
Iter: 1123 loss: 4.15707575e-07
Iter: 1124 loss: 4.15657155e-07
Iter: 1125 loss: 4.15513796e-07
Iter: 1126 loss: 4.18109238e-07
Iter: 1127 loss: 4.15508509e-07
Iter: 1128 loss: 4.15340423e-07
Iter: 1129 loss: 4.15195956e-07
Iter: 1130 loss: 4.15148293e-07
Iter: 1131 loss: 4.1492126e-07
Iter: 1132 loss: 4.14920294e-07
Iter: 1133 loss: 4.14778469e-07
Iter: 1134 loss: 4.14781624e-07
Iter: 1135 loss: 4.14685417e-07
Iter: 1136 loss: 4.14602738e-07
Iter: 1137 loss: 4.14578039e-07
Iter: 1138 loss: 4.14480724e-07
Iter: 1139 loss: 4.14952751e-07
Iter: 1140 loss: 4.144527e-07
Iter: 1141 loss: 4.14342e-07
Iter: 1142 loss: 4.14608621e-07
Iter: 1143 loss: 4.14309568e-07
Iter: 1144 loss: 4.14228168e-07
Iter: 1145 loss: 4.1413665e-07
Iter: 1146 loss: 4.14107433e-07
Iter: 1147 loss: 4.14017961e-07
Iter: 1148 loss: 4.14016085e-07
Iter: 1149 loss: 4.13923715e-07
Iter: 1150 loss: 4.13842429e-07
Iter: 1151 loss: 4.13802098e-07
Iter: 1152 loss: 4.13675195e-07
Iter: 1153 loss: 4.14045331e-07
Iter: 1154 loss: 4.13628726e-07
Iter: 1155 loss: 4.13464079e-07
Iter: 1156 loss: 4.14408248e-07
Iter: 1157 loss: 4.13451772e-07
Iter: 1158 loss: 4.13349e-07
Iter: 1159 loss: 4.13206237e-07
Iter: 1160 loss: 4.13205072e-07
Iter: 1161 loss: 4.1303781e-07
Iter: 1162 loss: 4.13191117e-07
Iter: 1163 loss: 4.12956609e-07
Iter: 1164 loss: 4.12801398e-07
Iter: 1165 loss: 4.1506388e-07
Iter: 1166 loss: 4.12796311e-07
Iter: 1167 loss: 4.12712666e-07
Iter: 1168 loss: 4.12709e-07
Iter: 1169 loss: 4.12630499e-07
Iter: 1170 loss: 4.12489669e-07
Iter: 1171 loss: 4.14739674e-07
Iter: 1172 loss: 4.12481853e-07
Iter: 1173 loss: 4.12302029e-07
Iter: 1174 loss: 4.13035423e-07
Iter: 1175 loss: 4.1227679e-07
Iter: 1176 loss: 4.12092902e-07
Iter: 1177 loss: 4.13274506e-07
Iter: 1178 loss: 4.12069625e-07
Iter: 1179 loss: 4.11938402e-07
Iter: 1180 loss: 4.11799277e-07
Iter: 1181 loss: 4.11769804e-07
Iter: 1182 loss: 4.11647022e-07
Iter: 1183 loss: 4.12867e-07
Iter: 1184 loss: 4.11649893e-07
Iter: 1185 loss: 4.11517846e-07
Iter: 1186 loss: 4.12025202e-07
Iter: 1187 loss: 4.11501048e-07
Iter: 1188 loss: 4.11423e-07
Iter: 1189 loss: 4.11401061e-07
Iter: 1190 loss: 4.11348e-07
Iter: 1191 loss: 4.11258e-07
Iter: 1192 loss: 4.12433138e-07
Iter: 1193 loss: 4.11245651e-07
Iter: 1194 loss: 4.11165558e-07
Iter: 1195 loss: 4.11021972e-07
Iter: 1196 loss: 4.14581336e-07
Iter: 1197 loss: 4.11019784e-07
Iter: 1198 loss: 4.10884667e-07
Iter: 1199 loss: 4.1076936e-07
Iter: 1200 loss: 4.10716382e-07
Iter: 1201 loss: 4.10489974e-07
Iter: 1202 loss: 4.1218135e-07
Iter: 1203 loss: 4.10473689e-07
Iter: 1204 loss: 4.10360883e-07
Iter: 1205 loss: 4.10364123e-07
Iter: 1206 loss: 4.1026675e-07
Iter: 1207 loss: 4.10696316e-07
Iter: 1208 loss: 4.10240347e-07
Iter: 1209 loss: 4.10172589e-07
Iter: 1210 loss: 4.10042958e-07
Iter: 1211 loss: 4.12957831e-07
Iter: 1212 loss: 4.10040855e-07
Iter: 1213 loss: 4.09897979e-07
Iter: 1214 loss: 4.10548353e-07
Iter: 1215 loss: 4.09876577e-07
Iter: 1216 loss: 4.09791227e-07
Iter: 1217 loss: 4.11201313e-07
Iter: 1218 loss: 4.09788669e-07
Iter: 1219 loss: 4.09703205e-07
Iter: 1220 loss: 4.09535232e-07
Iter: 1221 loss: 4.13393082e-07
Iter: 1222 loss: 4.09536767e-07
Iter: 1223 loss: 4.09364873e-07
Iter: 1224 loss: 4.09535147e-07
Iter: 1225 loss: 4.09257751e-07
Iter: 1226 loss: 4.09179876e-07
Iter: 1227 loss: 4.09164556e-07
Iter: 1228 loss: 4.09070083e-07
Iter: 1229 loss: 4.09007527e-07
Iter: 1230 loss: 4.0898567e-07
Iter: 1231 loss: 4.08887388e-07
Iter: 1232 loss: 4.09645338e-07
Iter: 1233 loss: 4.0889438e-07
Iter: 1234 loss: 4.08802919e-07
Iter: 1235 loss: 4.08809512e-07
Iter: 1236 loss: 4.08728965e-07
Iter: 1237 loss: 4.08604535e-07
Iter: 1238 loss: 4.08412603e-07
Iter: 1239 loss: 4.08411864e-07
Iter: 1240 loss: 4.08172e-07
Iter: 1241 loss: 4.08577705e-07
Iter: 1242 loss: 4.08070832e-07
Iter: 1243 loss: 4.08046503e-07
Iter: 1244 loss: 4.07985283e-07
Iter: 1245 loss: 4.0787566e-07
Iter: 1246 loss: 4.07931736e-07
Iter: 1247 loss: 4.07795937e-07
Iter: 1248 loss: 4.07705187e-07
Iter: 1249 loss: 4.07733751e-07
Iter: 1250 loss: 4.07629301e-07
Iter: 1251 loss: 4.07539716e-07
Iter: 1252 loss: 4.08109315e-07
Iter: 1253 loss: 4.07518854e-07
Iter: 1254 loss: 4.07427933e-07
Iter: 1255 loss: 4.07988608e-07
Iter: 1256 loss: 4.07402638e-07
Iter: 1257 loss: 4.07311688e-07
Iter: 1258 loss: 4.07258199e-07
Iter: 1259 loss: 4.07216191e-07
Iter: 1260 loss: 4.07115095e-07
Iter: 1261 loss: 4.07049043e-07
Iter: 1262 loss: 4.07009566e-07
Iter: 1263 loss: 4.06969036e-07
Iter: 1264 loss: 4.0693152e-07
Iter: 1265 loss: 4.0687479e-07
Iter: 1266 loss: 4.06749223e-07
Iter: 1267 loss: 4.08385375e-07
Iter: 1268 loss: 4.06758147e-07
Iter: 1269 loss: 4.06664924e-07
Iter: 1270 loss: 4.06662593e-07
Iter: 1271 loss: 4.06583354e-07
Iter: 1272 loss: 4.06475351e-07
Iter: 1273 loss: 4.06488653e-07
Iter: 1274 loss: 4.06356548e-07
Iter: 1275 loss: 4.07113021e-07
Iter: 1276 loss: 4.06368827e-07
Iter: 1277 loss: 4.06240645e-07
Iter: 1278 loss: 4.06605182e-07
Iter: 1279 loss: 4.06220124e-07
Iter: 1280 loss: 4.06119625e-07
Iter: 1281 loss: 4.06033962e-07
Iter: 1282 loss: 4.06007615e-07
Iter: 1283 loss: 4.0587048e-07
Iter: 1284 loss: 4.06209409e-07
Iter: 1285 loss: 4.05815769e-07
Iter: 1286 loss: 4.05704327e-07
Iter: 1287 loss: 4.06600037e-07
Iter: 1288 loss: 4.05695232e-07
Iter: 1289 loss: 4.05592516e-07
Iter: 1290 loss: 4.05755429e-07
Iter: 1291 loss: 4.05532262e-07
Iter: 1292 loss: 4.05430853e-07
Iter: 1293 loss: 4.05385492e-07
Iter: 1294 loss: 4.05333594e-07
Iter: 1295 loss: 4.05219851e-07
Iter: 1296 loss: 4.06210972e-07
Iter: 1297 loss: 4.05218856e-07
Iter: 1298 loss: 4.05118044e-07
Iter: 1299 loss: 4.05377932e-07
Iter: 1300 loss: 4.05078026e-07
Iter: 1301 loss: 4.04990146e-07
Iter: 1302 loss: 4.05076037e-07
Iter: 1303 loss: 4.04943677e-07
Iter: 1304 loss: 4.04829819e-07
Iter: 1305 loss: 4.05334731e-07
Iter: 1306 loss: 4.04807793e-07
Iter: 1307 loss: 4.04714e-07
Iter: 1308 loss: 4.04647e-07
Iter: 1309 loss: 4.04619527e-07
Iter: 1310 loss: 4.04547961e-07
Iter: 1311 loss: 4.04537218e-07
Iter: 1312 loss: 4.04473809e-07
Iter: 1313 loss: 4.04387833e-07
Iter: 1314 loss: 4.04370155e-07
Iter: 1315 loss: 4.04262892e-07
Iter: 1316 loss: 4.04476225e-07
Iter: 1317 loss: 4.04214e-07
Iter: 1318 loss: 4.04119874e-07
Iter: 1319 loss: 4.05209448e-07
Iter: 1320 loss: 4.04122432e-07
Iter: 1321 loss: 4.0404791e-07
Iter: 1322 loss: 4.0424294e-07
Iter: 1323 loss: 4.04035688e-07
Iter: 1324 loss: 4.03965032e-07
Iter: 1325 loss: 4.03888208e-07
Iter: 1326 loss: 4.03867e-07
Iter: 1327 loss: 4.03750079e-07
Iter: 1328 loss: 4.04087388e-07
Iter: 1329 loss: 4.03716683e-07
Iter: 1330 loss: 4.03620959e-07
Iter: 1331 loss: 4.03621215e-07
Iter: 1332 loss: 4.0353936e-07
Iter: 1333 loss: 4.03425418e-07
Iter: 1334 loss: 4.03433717e-07
Iter: 1335 loss: 4.03331626e-07
Iter: 1336 loss: 4.03333445e-07
Iter: 1337 loss: 4.03283224e-07
Iter: 1338 loss: 4.03133924e-07
Iter: 1339 loss: 4.05067226e-07
Iter: 1340 loss: 4.03128979e-07
Iter: 1341 loss: 4.03095953e-07
Iter: 1342 loss: 4.03077308e-07
Iter: 1343 loss: 4.03018845e-07
Iter: 1344 loss: 4.03027087e-07
Iter: 1345 loss: 4.02973797e-07
Iter: 1346 loss: 4.02911411e-07
Iter: 1347 loss: 4.02937189e-07
Iter: 1348 loss: 4.02867698e-07
Iter: 1349 loss: 4.0277763e-07
Iter: 1350 loss: 4.0337784e-07
Iter: 1351 loss: 4.02768336e-07
Iter: 1352 loss: 4.02679e-07
Iter: 1353 loss: 4.02812276e-07
Iter: 1354 loss: 4.02627421e-07
Iter: 1355 loss: 4.02498529e-07
Iter: 1356 loss: 4.02658173e-07
Iter: 1357 loss: 4.02450127e-07
Iter: 1358 loss: 4.02320751e-07
Iter: 1359 loss: 4.02245519e-07
Iter: 1360 loss: 4.02191631e-07
Iter: 1361 loss: 4.0212592e-07
Iter: 1362 loss: 4.02093121e-07
Iter: 1363 loss: 4.02029173e-07
Iter: 1364 loss: 4.0202022e-07
Iter: 1365 loss: 4.01956441e-07
Iter: 1366 loss: 4.01886098e-07
Iter: 1367 loss: 4.02304636e-07
Iter: 1368 loss: 4.01883256e-07
Iter: 1369 loss: 4.01802424e-07
Iter: 1370 loss: 4.01890929e-07
Iter: 1371 loss: 4.01762236e-07
Iter: 1372 loss: 4.0170616e-07
Iter: 1373 loss: 4.01573e-07
Iter: 1374 loss: 4.04105265e-07
Iter: 1375 loss: 4.01571e-07
Iter: 1376 loss: 4.01542138e-07
Iter: 1377 loss: 4.01482794e-07
Iter: 1378 loss: 4.01413104e-07
Iter: 1379 loss: 4.01265822e-07
Iter: 1380 loss: 4.02981073e-07
Iter: 1381 loss: 4.01252123e-07
Iter: 1382 loss: 4.01099953e-07
Iter: 1383 loss: 4.01589233e-07
Iter: 1384 loss: 4.01056809e-07
Iter: 1385 loss: 4.00957589e-07
Iter: 1386 loss: 4.0094011e-07
Iter: 1387 loss: 4.008904e-07
Iter: 1388 loss: 4.00836683e-07
Iter: 1389 loss: 4.00811984e-07
Iter: 1390 loss: 4.00722229e-07
Iter: 1391 loss: 4.00753208e-07
Iter: 1392 loss: 4.00662486e-07
Iter: 1393 loss: 4.0055545e-07
Iter: 1394 loss: 4.00832192e-07
Iter: 1395 loss: 4.00515205e-07
Iter: 1396 loss: 4.00403195e-07
Iter: 1397 loss: 4.00403621e-07
Iter: 1398 loss: 4.00337115e-07
Iter: 1399 loss: 4.00244289e-07
Iter: 1400 loss: 4.0024716e-07
Iter: 1401 loss: 4.00165391e-07
Iter: 1402 loss: 4.00168574e-07
Iter: 1403 loss: 4.00082655e-07
Iter: 1404 loss: 3.99938585e-07
Iter: 1405 loss: 4.03329835e-07
Iter: 1406 loss: 3.99936852e-07
Iter: 1407 loss: 3.99815463e-07
Iter: 1408 loss: 4.00502017e-07
Iter: 1409 loss: 3.99809892e-07
Iter: 1410 loss: 3.99670199e-07
Iter: 1411 loss: 4.00373438e-07
Iter: 1412 loss: 3.99668494e-07
Iter: 1413 loss: 3.99599401e-07
Iter: 1414 loss: 3.99515585e-07
Iter: 1415 loss: 4.01669354e-07
Iter: 1416 loss: 3.99511578e-07
Iter: 1417 loss: 3.99426682e-07
Iter: 1418 loss: 4.00645149e-07
Iter: 1419 loss: 3.99425858e-07
Iter: 1420 loss: 3.99320754e-07
Iter: 1421 loss: 3.99476164e-07
Iter: 1422 loss: 3.99295288e-07
Iter: 1423 loss: 3.99212809e-07
Iter: 1424 loss: 3.99278179e-07
Iter: 1425 loss: 3.9915767e-07
Iter: 1426 loss: 3.99040829e-07
Iter: 1427 loss: 3.98987737e-07
Iter: 1428 loss: 3.98933395e-07
Iter: 1429 loss: 3.98812233e-07
Iter: 1430 loss: 3.98820902e-07
Iter: 1431 loss: 3.987181e-07
Iter: 1432 loss: 3.98846055e-07
Iter: 1433 loss: 3.98676207e-07
Iter: 1434 loss: 3.98586963e-07
Iter: 1435 loss: 3.98863563e-07
Iter: 1436 loss: 3.98568574e-07
Iter: 1437 loss: 3.98482115e-07
Iter: 1438 loss: 3.98761586e-07
Iter: 1439 loss: 3.98466881e-07
Iter: 1440 loss: 3.98396537e-07
Iter: 1441 loss: 3.98325795e-07
Iter: 1442 loss: 3.983059e-07
Iter: 1443 loss: 3.9830968e-07
Iter: 1444 loss: 3.982762e-07
Iter: 1445 loss: 3.98225154e-07
Iter: 1446 loss: 3.98101463e-07
Iter: 1447 loss: 3.98432974e-07
Iter: 1448 loss: 3.98034786e-07
Iter: 1449 loss: 3.97890119e-07
Iter: 1450 loss: 3.99226707e-07
Iter: 1451 loss: 3.97880228e-07
Iter: 1452 loss: 3.97812641e-07
Iter: 1453 loss: 3.98988e-07
Iter: 1454 loss: 3.9780906e-07
Iter: 1455 loss: 3.97737e-07
Iter: 1456 loss: 3.97619601e-07
Iter: 1457 loss: 3.99944611e-07
Iter: 1458 loss: 3.97622983e-07
Iter: 1459 loss: 3.97471837e-07
Iter: 1460 loss: 3.98297743e-07
Iter: 1461 loss: 3.97454386e-07
Iter: 1462 loss: 3.97348458e-07
Iter: 1463 loss: 3.97759266e-07
Iter: 1464 loss: 3.97333395e-07
Iter: 1465 loss: 3.97256201e-07
Iter: 1466 loss: 3.97832338e-07
Iter: 1467 loss: 3.97249721e-07
Iter: 1468 loss: 3.97181338e-07
Iter: 1469 loss: 3.97116253e-07
Iter: 1470 loss: 3.9710352e-07
Iter: 1471 loss: 3.9701635e-07
Iter: 1472 loss: 3.97925e-07
Iter: 1473 loss: 3.97009131e-07
Iter: 1474 loss: 3.96932251e-07
Iter: 1475 loss: 3.96795201e-07
Iter: 1476 loss: 3.96794547e-07
Iter: 1477 loss: 3.96700273e-07
Iter: 1478 loss: 3.98356946e-07
Iter: 1479 loss: 3.96694674e-07
Iter: 1480 loss: 3.96584198e-07
Iter: 1481 loss: 3.9679523e-07
Iter: 1482 loss: 3.96536166e-07
Iter: 1483 loss: 3.96481454e-07
Iter: 1484 loss: 3.96421797e-07
Iter: 1485 loss: 3.96403664e-07
Iter: 1486 loss: 3.96335565e-07
Iter: 1487 loss: 3.96336816e-07
Iter: 1488 loss: 3.96287106e-07
Iter: 1489 loss: 3.96244502e-07
Iter: 1490 loss: 3.96227335e-07
Iter: 1491 loss: 3.96153098e-07
Iter: 1492 loss: 3.9621716e-07
Iter: 1493 loss: 3.96116036e-07
Iter: 1494 loss: 3.95985552e-07
Iter: 1495 loss: 3.96413043e-07
Iter: 1496 loss: 3.95951872e-07
Iter: 1497 loss: 3.95835627e-07
Iter: 1498 loss: 3.96626433e-07
Iter: 1499 loss: 3.95829829e-07
Iter: 1500 loss: 3.95737942e-07
Iter: 1501 loss: 3.95623204e-07
Iter: 1502 loss: 3.95620759e-07
Iter: 1503 loss: 3.95523784e-07
Iter: 1504 loss: 3.9552458e-07
Iter: 1505 loss: 3.95417828e-07
Iter: 1506 loss: 3.95416066e-07
Iter: 1507 loss: 3.95326964e-07
Iter: 1508 loss: 3.95255711e-07
Iter: 1509 loss: 3.95665126e-07
Iter: 1510 loss: 3.95242836e-07
Iter: 1511 loss: 3.95172606e-07
Iter: 1512 loss: 3.95932318e-07
Iter: 1513 loss: 3.95169224e-07
Iter: 1514 loss: 3.95122015e-07
Iter: 1515 loss: 3.95024045e-07
Iter: 1516 loss: 3.96437372e-07
Iter: 1517 loss: 3.95014041e-07
Iter: 1518 loss: 3.94945715e-07
Iter: 1519 loss: 3.94937956e-07
Iter: 1520 loss: 3.9485704e-07
Iter: 1521 loss: 3.9480426e-07
Iter: 1522 loss: 3.94777032e-07
Iter: 1523 loss: 3.94656524e-07
Iter: 1524 loss: 3.94668859e-07
Iter: 1525 loss: 3.9456836e-07
Iter: 1526 loss: 3.9446013e-07
Iter: 1527 loss: 3.95093309e-07
Iter: 1528 loss: 3.9442e-07
Iter: 1529 loss: 3.94350963e-07
Iter: 1530 loss: 3.9434957e-07
Iter: 1531 loss: 3.94290566e-07
Iter: 1532 loss: 3.94199816e-07
Iter: 1533 loss: 3.94191602e-07
Iter: 1534 loss: 3.94094627e-07
Iter: 1535 loss: 3.94481134e-07
Iter: 1536 loss: 3.94078342e-07
Iter: 1537 loss: 3.93950501e-07
Iter: 1538 loss: 3.94461466e-07
Iter: 1539 loss: 3.93940184e-07
Iter: 1540 loss: 3.93852076e-07
Iter: 1541 loss: 3.93838434e-07
Iter: 1542 loss: 3.9378773e-07
Iter: 1543 loss: 3.937065e-07
Iter: 1544 loss: 3.93721621e-07
Iter: 1545 loss: 3.93652954e-07
Iter: 1546 loss: 3.93530058e-07
Iter: 1547 loss: 3.95419249e-07
Iter: 1548 loss: 3.93530115e-07
Iter: 1549 loss: 3.93408982e-07
Iter: 1550 loss: 3.93485834e-07
Iter: 1551 loss: 3.93325507e-07
Iter: 1552 loss: 3.93271193e-07
Iter: 1553 loss: 3.93235865e-07
Iter: 1554 loss: 3.93164896e-07
Iter: 1555 loss: 3.93101e-07
Iter: 1556 loss: 3.93091966e-07
Iter: 1557 loss: 3.93009742e-07
Iter: 1558 loss: 3.93165351e-07
Iter: 1559 loss: 3.9295918e-07
Iter: 1560 loss: 3.92859619e-07
Iter: 1561 loss: 3.93740066e-07
Iter: 1562 loss: 3.92861239e-07
Iter: 1563 loss: 3.92768158e-07
Iter: 1564 loss: 3.9320318e-07
Iter: 1565 loss: 3.92756363e-07
Iter: 1566 loss: 3.9267718e-07
Iter: 1567 loss: 3.92605443e-07
Iter: 1568 loss: 3.92592455e-07
Iter: 1569 loss: 3.92527596e-07
Iter: 1570 loss: 3.92527028e-07
Iter: 1571 loss: 3.92485504e-07
Iter: 1572 loss: 3.92391883e-07
Iter: 1573 loss: 3.93811433e-07
Iter: 1574 loss: 3.92378126e-07
Iter: 1575 loss: 3.92352888e-07
Iter: 1576 loss: 3.92337768e-07
Iter: 1577 loss: 3.92298574e-07
Iter: 1578 loss: 3.92212684e-07
Iter: 1579 loss: 3.93158189e-07
Iter: 1580 loss: 3.92179629e-07
Iter: 1581 loss: 3.92094705e-07
Iter: 1582 loss: 3.92117e-07
Iter: 1583 loss: 3.92026607e-07
Iter: 1584 loss: 3.91928126e-07
Iter: 1585 loss: 3.92976517e-07
Iter: 1586 loss: 3.91925454e-07
Iter: 1587 loss: 3.91835e-07
Iter: 1588 loss: 3.92270778e-07
Iter: 1589 loss: 3.91798039e-07
Iter: 1590 loss: 3.91730737e-07
Iter: 1591 loss: 3.91627509e-07
Iter: 1592 loss: 3.91629783e-07
Iter: 1593 loss: 3.91532694e-07
Iter: 1594 loss: 3.92212343e-07
Iter: 1595 loss: 3.91514561e-07
Iter: 1596 loss: 3.91462407e-07
Iter: 1597 loss: 3.92343679e-07
Iter: 1598 loss: 3.91469797e-07
Iter: 1599 loss: 3.91412868e-07
Iter: 1600 loss: 3.91433701e-07
Iter: 1601 loss: 3.91379388e-07
Iter: 1602 loss: 3.91309072e-07
Iter: 1603 loss: 3.91404171e-07
Iter: 1604 loss: 3.91288467e-07
Iter: 1605 loss: 3.91222159e-07
Iter: 1606 loss: 3.9188177e-07
Iter: 1607 loss: 3.91205219e-07
Iter: 1608 loss: 3.91150081e-07
Iter: 1609 loss: 3.91075162e-07
Iter: 1610 loss: 3.931832e-07
Iter: 1611 loss: 3.91071922e-07
Iter: 1612 loss: 3.90961475e-07
Iter: 1613 loss: 3.926001e-07
Iter: 1614 loss: 3.90954654e-07
Iter: 1615 loss: 3.90912589e-07
Iter: 1616 loss: 3.9077392e-07
Iter: 1617 loss: 3.92746159e-07
Iter: 1618 loss: 3.90778837e-07
Iter: 1619 loss: 3.90652133e-07
Iter: 1620 loss: 3.90732026e-07
Iter: 1621 loss: 3.90558171e-07
Iter: 1622 loss: 3.90630106e-07
Iter: 1623 loss: 3.90488765e-07
Iter: 1624 loss: 3.90464066e-07
Iter: 1625 loss: 3.90425896e-07
Iter: 1626 loss: 3.90420098e-07
Iter: 1627 loss: 3.90360753e-07
Iter: 1628 loss: 3.90251955e-07
Iter: 1629 loss: 3.90246896e-07
Iter: 1630 loss: 3.90121841e-07
Iter: 1631 loss: 3.90607681e-07
Iter: 1632 loss: 3.90078753e-07
Iter: 1633 loss: 3.89974957e-07
Iter: 1634 loss: 3.91316547e-07
Iter: 1635 loss: 3.89971433e-07
Iter: 1636 loss: 3.89846036e-07
Iter: 1637 loss: 3.89831513e-07
Iter: 1638 loss: 3.89742326e-07
Iter: 1639 loss: 3.8964788e-07
Iter: 1640 loss: 3.89954607e-07
Iter: 1641 loss: 3.89624631e-07
Iter: 1642 loss: 3.89533369e-07
Iter: 1643 loss: 3.90186926e-07
Iter: 1644 loss: 3.89541924e-07
Iter: 1645 loss: 3.89459274e-07
Iter: 1646 loss: 3.89616559e-07
Iter: 1647 loss: 3.89432643e-07
Iter: 1648 loss: 3.89367472e-07
Iter: 1649 loss: 3.89953755e-07
Iter: 1650 loss: 3.89364942e-07
Iter: 1651 loss: 3.89319723e-07
Iter: 1652 loss: 3.89265267e-07
Iter: 1653 loss: 3.89268365e-07
Iter: 1654 loss: 3.89177018e-07
Iter: 1655 loss: 3.89112813e-07
Iter: 1656 loss: 3.89080981e-07
Iter: 1657 loss: 3.88995318e-07
Iter: 1658 loss: 3.90142532e-07
Iter: 1659 loss: 3.88973831e-07
Iter: 1660 loss: 3.88857217e-07
Iter: 1661 loss: 3.89113325e-07
Iter: 1662 loss: 3.88811685e-07
Iter: 1663 loss: 3.88723379e-07
Iter: 1664 loss: 3.88573824e-07
Iter: 1665 loss: 3.88579792e-07
Iter: 1666 loss: 3.88430323e-07
Iter: 1667 loss: 3.89300794e-07
Iter: 1668 loss: 3.88436689e-07
Iter: 1669 loss: 3.8831476e-07
Iter: 1670 loss: 3.88577888e-07
Iter: 1671 loss: 3.88270706e-07
Iter: 1672 loss: 3.88243222e-07
Iter: 1673 loss: 3.88207781e-07
Iter: 1674 loss: 3.88171088e-07
Iter: 1675 loss: 3.88085311e-07
Iter: 1676 loss: 3.89302102e-07
Iter: 1677 loss: 3.88057458e-07
Iter: 1678 loss: 3.87936154e-07
Iter: 1679 loss: 3.89025956e-07
Iter: 1680 loss: 3.87946557e-07
Iter: 1681 loss: 3.8783952e-07
Iter: 1682 loss: 3.88208207e-07
Iter: 1683 loss: 3.87812321e-07
Iter: 1684 loss: 3.87732769e-07
Iter: 1685 loss: 3.8809938e-07
Iter: 1686 loss: 3.87710031e-07
Iter: 1687 loss: 3.87634429e-07
Iter: 1688 loss: 3.87834234e-07
Iter: 1689 loss: 3.87593161e-07
Iter: 1690 loss: 3.87526796e-07
Iter: 1691 loss: 3.8749792e-07
Iter: 1692 loss: 3.87474074e-07
Iter: 1693 loss: 3.87392305e-07
Iter: 1694 loss: 3.87747946e-07
Iter: 1695 loss: 3.87366981e-07
Iter: 1696 loss: 3.87301895e-07
Iter: 1697 loss: 3.87302777e-07
Iter: 1698 loss: 3.87260059e-07
Iter: 1699 loss: 3.87151488e-07
Iter: 1700 loss: 3.88419124e-07
Iter: 1701 loss: 3.87131479e-07
Iter: 1702 loss: 3.87028081e-07
Iter: 1703 loss: 3.87070912e-07
Iter: 1704 loss: 3.86941622e-07
Iter: 1705 loss: 3.86791783e-07
Iter: 1706 loss: 3.87664329e-07
Iter: 1707 loss: 3.8675546e-07
Iter: 1708 loss: 3.86643592e-07
Iter: 1709 loss: 3.87735525e-07
Iter: 1710 loss: 3.86640579e-07
Iter: 1711 loss: 3.86506315e-07
Iter: 1712 loss: 3.86849479e-07
Iter: 1713 loss: 3.86466922e-07
Iter: 1714 loss: 3.86390838e-07
Iter: 1715 loss: 3.86395016e-07
Iter: 1716 loss: 3.86328111e-07
Iter: 1717 loss: 3.86276042e-07
Iter: 1718 loss: 3.86270045e-07
Iter: 1719 loss: 3.86228237e-07
Iter: 1720 loss: 3.86163379e-07
Iter: 1721 loss: 3.86160536e-07
Iter: 1722 loss: 3.86062538e-07
Iter: 1723 loss: 3.87031378e-07
Iter: 1724 loss: 3.86071235e-07
Iter: 1725 loss: 3.86029399e-07
Iter: 1726 loss: 3.85944759e-07
Iter: 1727 loss: 3.85946578e-07
Iter: 1728 loss: 3.8583056e-07
Iter: 1729 loss: 3.86244096e-07
Iter: 1730 loss: 3.85810893e-07
Iter: 1731 loss: 3.85712355e-07
Iter: 1732 loss: 3.86493127e-07
Iter: 1733 loss: 3.85697547e-07
Iter: 1734 loss: 3.85643318e-07
Iter: 1735 loss: 3.85564618e-07
Iter: 1736 loss: 3.85561407e-07
Iter: 1737 loss: 3.85480746e-07
Iter: 1738 loss: 3.85613e-07
Iter: 1739 loss: 3.85462926e-07
Iter: 1740 loss: 3.85366945e-07
Iter: 1741 loss: 3.85392411e-07
Iter: 1742 loss: 3.85291e-07
Iter: 1743 loss: 3.85259284e-07
Iter: 1744 loss: 3.85232113e-07
Iter: 1745 loss: 3.85176691e-07
Iter: 1746 loss: 3.85061867e-07
Iter: 1747 loss: 3.87028138e-07
Iter: 1748 loss: 3.85050726e-07
Iter: 1749 loss: 3.84936413e-07
Iter: 1750 loss: 3.85719432e-07
Iter: 1751 loss: 3.84933031e-07
Iter: 1752 loss: 3.84834351e-07
Iter: 1753 loss: 3.856224e-07
Iter: 1754 loss: 3.84826961e-07
Iter: 1755 loss: 3.84766395e-07
Iter: 1756 loss: 3.84811813e-07
Iter: 1757 loss: 3.84732232e-07
Iter: 1758 loss: 3.84630027e-07
Iter: 1759 loss: 3.84946077e-07
Iter: 1760 loss: 3.84602686e-07
Iter: 1761 loss: 3.84566391e-07
Iter: 1762 loss: 3.84549423e-07
Iter: 1763 loss: 3.84517705e-07
Iter: 1764 loss: 3.84453244e-07
Iter: 1765 loss: 3.85189026e-07
Iter: 1766 loss: 3.84453585e-07
Iter: 1767 loss: 3.84392592e-07
Iter: 1768 loss: 3.84375596e-07
Iter: 1769 loss: 3.84345725e-07
Iter: 1770 loss: 3.84266457e-07
Iter: 1771 loss: 3.84314944e-07
Iter: 1772 loss: 3.84210239e-07
Iter: 1773 loss: 3.84117357e-07
Iter: 1774 loss: 3.84074468e-07
Iter: 1775 loss: 3.8401015e-07
Iter: 1776 loss: 3.83879751e-07
Iter: 1777 loss: 3.84372242e-07
Iter: 1778 loss: 3.83837147e-07
Iter: 1779 loss: 3.83735539e-07
Iter: 1780 loss: 3.84324494e-07
Iter: 1781 loss: 3.83716696e-07
Iter: 1782 loss: 3.83634699e-07
Iter: 1783 loss: 3.84365649e-07
Iter: 1784 loss: 3.83625917e-07
Iter: 1785 loss: 3.83563076e-07
Iter: 1786 loss: 3.83810914e-07
Iter: 1787 loss: 3.83525958e-07
Iter: 1788 loss: 3.83483325e-07
Iter: 1789 loss: 3.8336745e-07
Iter: 1790 loss: 3.85239161e-07
Iter: 1791 loss: 3.83357019e-07
Iter: 1792 loss: 3.83266e-07
Iter: 1793 loss: 3.84081943e-07
Iter: 1794 loss: 3.8326084e-07
Iter: 1795 loss: 3.83163353e-07
Iter: 1796 loss: 3.840521e-07
Iter: 1797 loss: 3.83160227e-07
Iter: 1798 loss: 3.83085222e-07
Iter: 1799 loss: 3.82998621e-07
Iter: 1800 loss: 3.82989185e-07
Iter: 1801 loss: 3.82959456e-07
Iter: 1802 loss: 3.82934275e-07
Iter: 1803 loss: 3.82886753e-07
Iter: 1804 loss: 3.82784265e-07
Iter: 1805 loss: 3.84053067e-07
Iter: 1806 loss: 3.8277858e-07
Iter: 1807 loss: 3.82698573e-07
Iter: 1808 loss: 3.83664712e-07
Iter: 1809 loss: 3.82693599e-07
Iter: 1810 loss: 3.82623426e-07
Iter: 1811 loss: 3.82742456e-07
Iter: 1812 loss: 3.82593896e-07
Iter: 1813 loss: 3.82514969e-07
Iter: 1814 loss: 3.82515793e-07
Iter: 1815 loss: 3.82447752e-07
Iter: 1816 loss: 3.82358223e-07
Iter: 1817 loss: 3.82294246e-07
Iter: 1818 loss: 3.82257241e-07
Iter: 1819 loss: 3.82116184e-07
Iter: 1820 loss: 3.8288033e-07
Iter: 1821 loss: 3.82100382e-07
Iter: 1822 loss: 3.82031885e-07
Iter: 1823 loss: 3.82028475e-07
Iter: 1824 loss: 3.81952816e-07
Iter: 1825 loss: 3.81911946e-07
Iter: 1826 loss: 3.81879403e-07
Iter: 1827 loss: 3.8179158e-07
Iter: 1828 loss: 3.81688238e-07
Iter: 1829 loss: 3.81677978e-07
Iter: 1830 loss: 3.81634635e-07
Iter: 1831 loss: 3.8161204e-07
Iter: 1832 loss: 3.81537234e-07
Iter: 1833 loss: 3.81467e-07
Iter: 1834 loss: 3.81460495e-07
Iter: 1835 loss: 3.81361644e-07
Iter: 1836 loss: 3.81960547e-07
Iter: 1837 loss: 3.81358234e-07
Iter: 1838 loss: 3.81254324e-07
Iter: 1839 loss: 3.8173377e-07
Iter: 1840 loss: 3.81237669e-07
Iter: 1841 loss: 3.81183384e-07
Iter: 1842 loss: 3.81130974e-07
Iter: 1843 loss: 3.81114177e-07
Iter: 1844 loss: 3.81001456e-07
Iter: 1845 loss: 3.81603684e-07
Iter: 1846 loss: 3.81000632e-07
Iter: 1847 loss: 3.80939809e-07
Iter: 1848 loss: 3.80956834e-07
Iter: 1849 loss: 3.80913434e-07
Iter: 1850 loss: 3.8084e-07
Iter: 1851 loss: 3.80929635e-07
Iter: 1852 loss: 3.80795825e-07
Iter: 1853 loss: 3.80704023e-07
Iter: 1854 loss: 3.8072767e-07
Iter: 1855 loss: 3.80622168e-07
Iter: 1856 loss: 3.80592923e-07
Iter: 1857 loss: 3.80577916e-07
Iter: 1858 loss: 3.80523716e-07
Iter: 1859 loss: 3.8041162e-07
Iter: 1860 loss: 3.82325027e-07
Iter: 1861 loss: 3.8040389e-07
Iter: 1862 loss: 3.8029475e-07
Iter: 1863 loss: 3.80631803e-07
Iter: 1864 loss: 3.80259735e-07
Iter: 1865 loss: 3.80202522e-07
Iter: 1866 loss: 3.80208206e-07
Iter: 1867 loss: 3.80141103e-07
Iter: 1868 loss: 3.80148379e-07
Iter: 1869 loss: 3.80097958e-07
Iter: 1870 loss: 3.80047823e-07
Iter: 1871 loss: 3.80516553e-07
Iter: 1872 loss: 3.80047027e-07
Iter: 1873 loss: 3.79987398e-07
Iter: 1874 loss: 3.79883318e-07
Iter: 1875 loss: 3.79888974e-07
Iter: 1876 loss: 3.79802486e-07
Iter: 1877 loss: 3.801905e-07
Iter: 1878 loss: 3.79795864e-07
Iter: 1879 loss: 3.79698974e-07
Iter: 1880 loss: 3.79724412e-07
Iter: 1881 loss: 3.79617916e-07
Iter: 1882 loss: 3.79517758e-07
Iter: 1883 loss: 3.79641619e-07
Iter: 1884 loss: 3.79450114e-07
Iter: 1885 loss: 3.79355782e-07
Iter: 1886 loss: 3.79740356e-07
Iter: 1887 loss: 3.79334921e-07
Iter: 1888 loss: 3.79249173e-07
Iter: 1889 loss: 3.7957841e-07
Iter: 1890 loss: 3.7922905e-07
Iter: 1891 loss: 3.79161349e-07
Iter: 1892 loss: 3.79160042e-07
Iter: 1893 loss: 3.79113715e-07
Iter: 1894 loss: 3.79007076e-07
Iter: 1895 loss: 3.80450956e-07
Iter: 1896 loss: 3.7900594e-07
Iter: 1897 loss: 3.78916525e-07
Iter: 1898 loss: 3.79079978e-07
Iter: 1899 loss: 3.78873182e-07
Iter: 1900 loss: 3.78751224e-07
Iter: 1901 loss: 3.79313406e-07
Iter: 1902 loss: 3.78729851e-07
Iter: 1903 loss: 3.78621e-07
Iter: 1904 loss: 3.79924074e-07
Iter: 1905 loss: 3.78613947e-07
Iter: 1906 loss: 3.7855e-07
Iter: 1907 loss: 3.78560912e-07
Iter: 1908 loss: 3.78502847e-07
Iter: 1909 loss: 3.78429377e-07
Iter: 1910 loss: 3.79121218e-07
Iter: 1911 loss: 3.78430173e-07
Iter: 1912 loss: 3.78399932e-07
Iter: 1913 loss: 3.78348716e-07
Iter: 1914 loss: 3.79640085e-07
Iter: 1915 loss: 3.78347352e-07
Iter: 1916 loss: 3.78291588e-07
Iter: 1917 loss: 3.78290792e-07
Iter: 1918 loss: 3.78244408e-07
Iter: 1919 loss: 3.78203481e-07
Iter: 1920 loss: 3.78193107e-07
Iter: 1921 loss: 3.78135e-07
Iter: 1922 loss: 3.78151896e-07
Iter: 1923 loss: 3.78117932e-07
Iter: 1924 loss: 3.78059781e-07
Iter: 1925 loss: 3.78050572e-07
Iter: 1926 loss: 3.78006064e-07
Iter: 1927 loss: 3.77914972e-07
Iter: 1928 loss: 3.77968945e-07
Iter: 1929 loss: 3.77859408e-07
Iter: 1930 loss: 3.77760557e-07
Iter: 1931 loss: 3.7783667e-07
Iter: 1932 loss: 3.7770414e-07
Iter: 1933 loss: 3.77595285e-07
Iter: 1934 loss: 3.78107529e-07
Iter: 1935 loss: 3.77585138e-07
Iter: 1936 loss: 3.77506552e-07
Iter: 1937 loss: 3.78324387e-07
Iter: 1938 loss: 3.77507405e-07
Iter: 1939 loss: 3.77441921e-07
Iter: 1940 loss: 3.77775734e-07
Iter: 1941 loss: 3.77431803e-07
Iter: 1942 loss: 3.77379251e-07
Iter: 1943 loss: 3.77328064e-07
Iter: 1944 loss: 3.77314024e-07
Iter: 1945 loss: 3.77230435e-07
Iter: 1946 loss: 3.78345874e-07
Iter: 1947 loss: 3.77233448e-07
Iter: 1948 loss: 3.77191498e-07
Iter: 1949 loss: 3.77130505e-07
Iter: 1950 loss: 3.77137212e-07
Iter: 1951 loss: 3.77071274e-07
Iter: 1952 loss: 3.77999783e-07
Iter: 1953 loss: 3.77076645e-07
Iter: 1954 loss: 3.7699823e-07
Iter: 1955 loss: 3.76973475e-07
Iter: 1956 loss: 3.76947526e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.6
+ date
Mon Oct 26 13:22:15 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.6/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.6_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.6_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.6_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi 1 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.6_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8368a2f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83689b5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83689a4bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8368aa2e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8368a9e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8368961bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83688b6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83688e3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83688e3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83688b6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8368871950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83688136a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8368813730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8368823378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8368823ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8368797b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8368776488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83687760d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8368705598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f836870d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8368700378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83686d1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8345a22840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83459cd950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83459cda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83459f6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f834598f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f834597a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f834596c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f834597a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8345932bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83201d5158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83201ef400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8320199950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83201bd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83201bd2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 8.43825819e-06
Iter: 2 loss: 8.63803052e-06
Iter: 3 loss: 5.50495133e-06
Iter: 4 loss: 5.02081e-06
Iter: 5 loss: 6.71134785e-06
Iter: 6 loss: 4.89429704e-06
Iter: 7 loss: 4.56913494e-06
Iter: 8 loss: 5.20365074e-06
Iter: 9 loss: 4.43457975e-06
Iter: 10 loss: 4.20230072e-06
Iter: 11 loss: 4.70986697e-06
Iter: 12 loss: 4.11268229e-06
Iter: 13 loss: 3.93592745e-06
Iter: 14 loss: 5.3118797e-06
Iter: 15 loss: 3.92316269e-06
Iter: 16 loss: 3.81418067e-06
Iter: 17 loss: 3.74445199e-06
Iter: 18 loss: 3.70197517e-06
Iter: 19 loss: 3.55828183e-06
Iter: 20 loss: 4.28936255e-06
Iter: 21 loss: 3.53466157e-06
Iter: 22 loss: 3.43147531e-06
Iter: 23 loss: 4.56098905e-06
Iter: 24 loss: 3.42928183e-06
Iter: 25 loss: 3.33701792e-06
Iter: 26 loss: 3.16793694e-06
Iter: 27 loss: 7.10874883e-06
Iter: 28 loss: 3.16769751e-06
Iter: 29 loss: 3.03631691e-06
Iter: 30 loss: 3.29385693e-06
Iter: 31 loss: 2.98189252e-06
Iter: 32 loss: 2.82684869e-06
Iter: 33 loss: 3.43431884e-06
Iter: 34 loss: 2.79118376e-06
Iter: 35 loss: 2.70592136e-06
Iter: 36 loss: 3.04322566e-06
Iter: 37 loss: 2.68656868e-06
Iter: 38 loss: 2.59799026e-06
Iter: 39 loss: 2.63250695e-06
Iter: 40 loss: 2.53659346e-06
Iter: 41 loss: 2.53813e-06
Iter: 42 loss: 2.49256527e-06
Iter: 43 loss: 2.44215721e-06
Iter: 44 loss: 2.36189271e-06
Iter: 45 loss: 2.36116148e-06
Iter: 46 loss: 2.29293482e-06
Iter: 47 loss: 3.1666691e-06
Iter: 48 loss: 2.2923623e-06
Iter: 49 loss: 2.23834263e-06
Iter: 50 loss: 2.21318692e-06
Iter: 51 loss: 2.18654918e-06
Iter: 52 loss: 2.12599798e-06
Iter: 53 loss: 2.43915679e-06
Iter: 54 loss: 2.116308e-06
Iter: 55 loss: 2.05516199e-06
Iter: 56 loss: 2.21044274e-06
Iter: 57 loss: 2.03401214e-06
Iter: 58 loss: 1.99533633e-06
Iter: 59 loss: 1.94809809e-06
Iter: 60 loss: 1.94373115e-06
Iter: 61 loss: 1.91616937e-06
Iter: 62 loss: 1.90441074e-06
Iter: 63 loss: 1.88023694e-06
Iter: 64 loss: 1.86298928e-06
Iter: 65 loss: 1.8545046e-06
Iter: 66 loss: 1.81077849e-06
Iter: 67 loss: 1.7754237e-06
Iter: 68 loss: 1.76235744e-06
Iter: 69 loss: 1.70152123e-06
Iter: 70 loss: 2.17664683e-06
Iter: 71 loss: 1.69725627e-06
Iter: 72 loss: 1.65039955e-06
Iter: 73 loss: 1.61080584e-06
Iter: 74 loss: 1.59782871e-06
Iter: 75 loss: 1.53857263e-06
Iter: 76 loss: 1.85025556e-06
Iter: 77 loss: 1.52932603e-06
Iter: 78 loss: 1.56259273e-06
Iter: 79 loss: 1.51230392e-06
Iter: 80 loss: 1.50232643e-06
Iter: 81 loss: 1.48091965e-06
Iter: 82 loss: 1.82110466e-06
Iter: 83 loss: 1.48022093e-06
Iter: 84 loss: 1.45975957e-06
Iter: 85 loss: 1.61971184e-06
Iter: 86 loss: 1.45830177e-06
Iter: 87 loss: 1.4366916e-06
Iter: 88 loss: 1.44190744e-06
Iter: 89 loss: 1.42088879e-06
Iter: 90 loss: 1.40146676e-06
Iter: 91 loss: 1.42396902e-06
Iter: 92 loss: 1.39119845e-06
Iter: 93 loss: 1.3634691e-06
Iter: 94 loss: 1.52576183e-06
Iter: 95 loss: 1.35982623e-06
Iter: 96 loss: 1.34363131e-06
Iter: 97 loss: 1.30969238e-06
Iter: 98 loss: 1.88618742e-06
Iter: 99 loss: 1.30883723e-06
Iter: 100 loss: 1.29331158e-06
Iter: 101 loss: 1.2850719e-06
Iter: 102 loss: 1.27279077e-06
Iter: 103 loss: 1.25555744e-06
Iter: 104 loss: 1.25488873e-06
Iter: 105 loss: 1.23326447e-06
Iter: 106 loss: 1.25791507e-06
Iter: 107 loss: 1.22155473e-06
Iter: 108 loss: 1.20598349e-06
Iter: 109 loss: 1.28265833e-06
Iter: 110 loss: 1.20329378e-06
Iter: 111 loss: 1.18517346e-06
Iter: 112 loss: 1.22072731e-06
Iter: 113 loss: 1.17776221e-06
Iter: 114 loss: 1.15968919e-06
Iter: 115 loss: 1.15600915e-06
Iter: 116 loss: 1.14397858e-06
Iter: 117 loss: 1.11665327e-06
Iter: 118 loss: 1.1815531e-06
Iter: 119 loss: 1.10675137e-06
Iter: 120 loss: 1.09233156e-06
Iter: 121 loss: 1.09083771e-06
Iter: 122 loss: 1.07558049e-06
Iter: 123 loss: 1.15504099e-06
Iter: 124 loss: 1.07312803e-06
Iter: 125 loss: 1.06686412e-06
Iter: 126 loss: 1.0591865e-06
Iter: 127 loss: 1.05849676e-06
Iter: 128 loss: 1.04951278e-06
Iter: 129 loss: 1.07880157e-06
Iter: 130 loss: 1.04702963e-06
Iter: 131 loss: 1.03615707e-06
Iter: 132 loss: 1.11552981e-06
Iter: 133 loss: 1.03520699e-06
Iter: 134 loss: 1.03044556e-06
Iter: 135 loss: 1.01772753e-06
Iter: 136 loss: 1.10762289e-06
Iter: 137 loss: 1.01494254e-06
Iter: 138 loss: 1.00822876e-06
Iter: 139 loss: 1.00557395e-06
Iter: 140 loss: 9.98092446e-07
Iter: 141 loss: 9.92632067e-07
Iter: 142 loss: 9.90106e-07
Iter: 143 loss: 9.80591608e-07
Iter: 144 loss: 1.0217409e-06
Iter: 145 loss: 9.78652224e-07
Iter: 146 loss: 9.6892154e-07
Iter: 147 loss: 1.03355251e-06
Iter: 148 loss: 9.67919505e-07
Iter: 149 loss: 9.62288823e-07
Iter: 150 loss: 9.52178652e-07
Iter: 151 loss: 1.20219056e-06
Iter: 152 loss: 9.52166033e-07
Iter: 153 loss: 9.41655e-07
Iter: 154 loss: 9.47128683e-07
Iter: 155 loss: 9.34678724e-07
Iter: 156 loss: 9.21778451e-07
Iter: 157 loss: 1.0932738e-06
Iter: 158 loss: 9.21713763e-07
Iter: 159 loss: 9.183766e-07
Iter: 160 loss: 9.17523721e-07
Iter: 161 loss: 9.13520921e-07
Iter: 162 loss: 9.04675744e-07
Iter: 163 loss: 1.03380808e-06
Iter: 164 loss: 9.04252943e-07
Iter: 165 loss: 8.95094331e-07
Iter: 166 loss: 9.26513053e-07
Iter: 167 loss: 8.92666435e-07
Iter: 168 loss: 8.86439295e-07
Iter: 169 loss: 9.03946102e-07
Iter: 170 loss: 8.84518386e-07
Iter: 171 loss: 8.75591809e-07
Iter: 172 loss: 9.09439223e-07
Iter: 173 loss: 8.73454781e-07
Iter: 174 loss: 8.69379505e-07
Iter: 175 loss: 8.616949e-07
Iter: 176 loss: 1.02930812e-06
Iter: 177 loss: 8.61675915e-07
Iter: 178 loss: 8.59662e-07
Iter: 179 loss: 8.57906571e-07
Iter: 180 loss: 8.54625341e-07
Iter: 181 loss: 8.53877054e-07
Iter: 182 loss: 8.51755601e-07
Iter: 183 loss: 8.46909415e-07
Iter: 184 loss: 8.5535055e-07
Iter: 185 loss: 8.44755675e-07
Iter: 186 loss: 8.39344e-07
Iter: 187 loss: 8.85024292e-07
Iter: 188 loss: 8.3902205e-07
Iter: 189 loss: 8.35831372e-07
Iter: 190 loss: 8.26486257e-07
Iter: 191 loss: 8.62556703e-07
Iter: 192 loss: 8.22560082e-07
Iter: 193 loss: 8.15027192e-07
Iter: 194 loss: 8.14976488e-07
Iter: 195 loss: 8.09116273e-07
Iter: 196 loss: 8.30106785e-07
Iter: 197 loss: 8.07676884e-07
Iter: 198 loss: 8.07947799e-07
Iter: 199 loss: 8.05493755e-07
Iter: 200 loss: 8.04113597e-07
Iter: 201 loss: 7.99659801e-07
Iter: 202 loss: 8.08272716e-07
Iter: 203 loss: 7.96813481e-07
Iter: 204 loss: 7.91675348e-07
Iter: 205 loss: 8.01251304e-07
Iter: 206 loss: 7.89473e-07
Iter: 207 loss: 7.84968961e-07
Iter: 208 loss: 7.84897395e-07
Iter: 209 loss: 7.81107246e-07
Iter: 210 loss: 7.97396751e-07
Iter: 211 loss: 7.80340883e-07
Iter: 212 loss: 7.77275488e-07
Iter: 213 loss: 7.73414e-07
Iter: 214 loss: 7.7308232e-07
Iter: 215 loss: 7.68073733e-07
Iter: 216 loss: 7.97187624e-07
Iter: 217 loss: 7.67426286e-07
Iter: 218 loss: 7.64730771e-07
Iter: 219 loss: 7.85457303e-07
Iter: 220 loss: 7.64498509e-07
Iter: 221 loss: 7.6158733e-07
Iter: 222 loss: 7.65313e-07
Iter: 223 loss: 7.60058128e-07
Iter: 224 loss: 7.57088458e-07
Iter: 225 loss: 7.60181649e-07
Iter: 226 loss: 7.55431188e-07
Iter: 227 loss: 7.52507617e-07
Iter: 228 loss: 7.92750882e-07
Iter: 229 loss: 7.52473e-07
Iter: 230 loss: 7.50617403e-07
Iter: 231 loss: 7.45743e-07
Iter: 232 loss: 7.83806627e-07
Iter: 233 loss: 7.44804424e-07
Iter: 234 loss: 7.39043e-07
Iter: 235 loss: 7.89786668e-07
Iter: 236 loss: 7.38744916e-07
Iter: 237 loss: 7.36979189e-07
Iter: 238 loss: 7.36155e-07
Iter: 239 loss: 7.34961645e-07
Iter: 240 loss: 7.31498631e-07
Iter: 241 loss: 7.46074306e-07
Iter: 242 loss: 7.30134047e-07
Iter: 243 loss: 7.25575319e-07
Iter: 244 loss: 7.30367447e-07
Iter: 245 loss: 7.2309922e-07
Iter: 246 loss: 7.19337891e-07
Iter: 247 loss: 7.75062e-07
Iter: 248 loss: 7.19336299e-07
Iter: 249 loss: 7.15249769e-07
Iter: 250 loss: 7.27249073e-07
Iter: 251 loss: 7.13965505e-07
Iter: 252 loss: 7.10345603e-07
Iter: 253 loss: 7.09024562e-07
Iter: 254 loss: 7.07043341e-07
Iter: 255 loss: 7.03730507e-07
Iter: 256 loss: 7.08379844e-07
Iter: 257 loss: 7.02118598e-07
Iter: 258 loss: 6.99120733e-07
Iter: 259 loss: 6.99057523e-07
Iter: 260 loss: 6.97398832e-07
Iter: 261 loss: 6.98696795e-07
Iter: 262 loss: 6.96442e-07
Iter: 263 loss: 6.94282676e-07
Iter: 264 loss: 6.97846531e-07
Iter: 265 loss: 6.9332765e-07
Iter: 266 loss: 6.91310959e-07
Iter: 267 loss: 7.09619428e-07
Iter: 268 loss: 6.91227115e-07
Iter: 269 loss: 6.89936314e-07
Iter: 270 loss: 6.87465729e-07
Iter: 271 loss: 7.36720324e-07
Iter: 272 loss: 6.87445834e-07
Iter: 273 loss: 6.86148667e-07
Iter: 274 loss: 6.85813916e-07
Iter: 275 loss: 6.84256406e-07
Iter: 276 loss: 6.85721e-07
Iter: 277 loss: 6.83351459e-07
Iter: 278 loss: 6.82154393e-07
Iter: 279 loss: 6.79150162e-07
Iter: 280 loss: 7.0612748e-07
Iter: 281 loss: 6.78668698e-07
Iter: 282 loss: 6.75597335e-07
Iter: 283 loss: 6.94320647e-07
Iter: 284 loss: 6.75206365e-07
Iter: 285 loss: 6.72916599e-07
Iter: 286 loss: 6.91648381e-07
Iter: 287 loss: 6.7278711e-07
Iter: 288 loss: 6.71437419e-07
Iter: 289 loss: 6.71429461e-07
Iter: 290 loss: 6.70408667e-07
Iter: 291 loss: 6.67875156e-07
Iter: 292 loss: 6.91547029e-07
Iter: 293 loss: 6.67522499e-07
Iter: 294 loss: 6.6488883e-07
Iter: 295 loss: 6.7244855e-07
Iter: 296 loss: 6.64082449e-07
Iter: 297 loss: 6.61138074e-07
Iter: 298 loss: 6.67828317e-07
Iter: 299 loss: 6.59982561e-07
Iter: 300 loss: 6.58494798e-07
Iter: 301 loss: 6.58131739e-07
Iter: 302 loss: 6.56984867e-07
Iter: 303 loss: 6.56107602e-07
Iter: 304 loss: 6.55761198e-07
Iter: 305 loss: 6.54070618e-07
Iter: 306 loss: 6.62671596e-07
Iter: 307 loss: 6.5382784e-07
Iter: 308 loss: 6.51947175e-07
Iter: 309 loss: 6.53820393e-07
Iter: 310 loss: 6.50895458e-07
Iter: 311 loss: 6.49789968e-07
Iter: 312 loss: 6.64891957e-07
Iter: 313 loss: 6.49779395e-07
Iter: 314 loss: 6.48770083e-07
Iter: 315 loss: 6.50602146e-07
Iter: 316 loss: 6.48325681e-07
Iter: 317 loss: 6.47248044e-07
Iter: 318 loss: 6.46116575e-07
Iter: 319 loss: 6.45939963e-07
Iter: 320 loss: 6.44505178e-07
Iter: 321 loss: 6.4260206e-07
Iter: 322 loss: 6.42486498e-07
Iter: 323 loss: 6.3989819e-07
Iter: 324 loss: 6.56727309e-07
Iter: 325 loss: 6.39651262e-07
Iter: 326 loss: 6.37851144e-07
Iter: 327 loss: 6.3781124e-07
Iter: 328 loss: 6.36733262e-07
Iter: 329 loss: 6.36069217e-07
Iter: 330 loss: 6.35646643e-07
Iter: 331 loss: 6.34402113e-07
Iter: 332 loss: 6.32912e-07
Iter: 333 loss: 6.32741376e-07
Iter: 334 loss: 6.31478315e-07
Iter: 335 loss: 6.31468765e-07
Iter: 336 loss: 6.30024829e-07
Iter: 337 loss: 6.32824595e-07
Iter: 338 loss: 6.29401086e-07
Iter: 339 loss: 6.2803133e-07
Iter: 340 loss: 6.2775473e-07
Iter: 341 loss: 6.26834321e-07
Iter: 342 loss: 6.25026416e-07
Iter: 343 loss: 6.46825868e-07
Iter: 344 loss: 6.25019482e-07
Iter: 345 loss: 6.23774838e-07
Iter: 346 loss: 6.2589794e-07
Iter: 347 loss: 6.23188612e-07
Iter: 348 loss: 6.21923959e-07
Iter: 349 loss: 6.3418986e-07
Iter: 350 loss: 6.21878485e-07
Iter: 351 loss: 6.21255253e-07
Iter: 352 loss: 6.19459229e-07
Iter: 353 loss: 6.28023884e-07
Iter: 354 loss: 6.18830086e-07
Iter: 355 loss: 6.17524677e-07
Iter: 356 loss: 6.17493811e-07
Iter: 357 loss: 6.1656317e-07
Iter: 358 loss: 6.16719262e-07
Iter: 359 loss: 6.1586627e-07
Iter: 360 loss: 6.14853377e-07
Iter: 361 loss: 6.30211161e-07
Iter: 362 loss: 6.14835415e-07
Iter: 363 loss: 6.13922111e-07
Iter: 364 loss: 6.14671762e-07
Iter: 365 loss: 6.13353109e-07
Iter: 366 loss: 6.12375743e-07
Iter: 367 loss: 6.10531117e-07
Iter: 368 loss: 6.51389769e-07
Iter: 369 loss: 6.10521397e-07
Iter: 370 loss: 6.09093604e-07
Iter: 371 loss: 6.0906018e-07
Iter: 372 loss: 6.0774363e-07
Iter: 373 loss: 6.12503754e-07
Iter: 374 loss: 6.07431502e-07
Iter: 375 loss: 6.06601361e-07
Iter: 376 loss: 6.06320896e-07
Iter: 377 loss: 6.05870866e-07
Iter: 378 loss: 6.04797208e-07
Iter: 379 loss: 6.04797606e-07
Iter: 380 loss: 6.04268735e-07
Iter: 381 loss: 6.06071637e-07
Iter: 382 loss: 6.04135153e-07
Iter: 383 loss: 6.03437456e-07
Iter: 384 loss: 6.02485784e-07
Iter: 385 loss: 6.02427292e-07
Iter: 386 loss: 6.01337206e-07
Iter: 387 loss: 6.01630916e-07
Iter: 388 loss: 6.00560611e-07
Iter: 389 loss: 5.99122586e-07
Iter: 390 loss: 5.99924533e-07
Iter: 391 loss: 5.98221732e-07
Iter: 392 loss: 5.97015969e-07
Iter: 393 loss: 5.96995051e-07
Iter: 394 loss: 5.95971301e-07
Iter: 395 loss: 5.99350642e-07
Iter: 396 loss: 5.95709594e-07
Iter: 397 loss: 5.94457504e-07
Iter: 398 loss: 5.93951313e-07
Iter: 399 loss: 5.93329105e-07
Iter: 400 loss: 5.92257777e-07
Iter: 401 loss: 5.93649133e-07
Iter: 402 loss: 5.9175e-07
Iter: 403 loss: 5.9097988e-07
Iter: 404 loss: 5.90955779e-07
Iter: 405 loss: 5.90239608e-07
Iter: 406 loss: 5.89679416e-07
Iter: 407 loss: 5.8948217e-07
Iter: 408 loss: 5.88553e-07
Iter: 409 loss: 5.92757033e-07
Iter: 410 loss: 5.88353373e-07
Iter: 411 loss: 5.87464e-07
Iter: 412 loss: 5.9598085e-07
Iter: 413 loss: 5.87427792e-07
Iter: 414 loss: 5.8691711e-07
Iter: 415 loss: 5.88746502e-07
Iter: 416 loss: 5.8678711e-07
Iter: 417 loss: 5.8617627e-07
Iter: 418 loss: 5.84600457e-07
Iter: 419 loss: 5.98451777e-07
Iter: 420 loss: 5.84356087e-07
Iter: 421 loss: 5.82994858e-07
Iter: 422 loss: 5.87812281e-07
Iter: 423 loss: 5.82622533e-07
Iter: 424 loss: 5.81211907e-07
Iter: 425 loss: 5.85621535e-07
Iter: 426 loss: 5.80821734e-07
Iter: 427 loss: 5.80115056e-07
Iter: 428 loss: 5.80056167e-07
Iter: 429 loss: 5.7945897e-07
Iter: 430 loss: 5.80210212e-07
Iter: 431 loss: 5.79156449e-07
Iter: 432 loss: 5.78528955e-07
Iter: 433 loss: 5.78286176e-07
Iter: 434 loss: 5.77941194e-07
Iter: 435 loss: 5.77112246e-07
Iter: 436 loss: 5.79984544e-07
Iter: 437 loss: 5.76891239e-07
Iter: 438 loss: 5.76276534e-07
Iter: 439 loss: 5.85313273e-07
Iter: 440 loss: 5.76262096e-07
Iter: 441 loss: 5.75664899e-07
Iter: 442 loss: 5.74533487e-07
Iter: 443 loss: 5.9769684e-07
Iter: 444 loss: 5.74524677e-07
Iter: 445 loss: 5.74150931e-07
Iter: 446 loss: 5.73967213e-07
Iter: 447 loss: 5.73461648e-07
Iter: 448 loss: 5.73133775e-07
Iter: 449 loss: 5.72872295e-07
Iter: 450 loss: 5.72030785e-07
Iter: 451 loss: 5.76135164e-07
Iter: 452 loss: 5.7185008e-07
Iter: 453 loss: 5.71261694e-07
Iter: 454 loss: 5.70402e-07
Iter: 455 loss: 5.7035561e-07
Iter: 456 loss: 5.69202257e-07
Iter: 457 loss: 5.69691565e-07
Iter: 458 loss: 5.68410769e-07
Iter: 459 loss: 5.67589268e-07
Iter: 460 loss: 5.77188302e-07
Iter: 461 loss: 5.67568577e-07
Iter: 462 loss: 5.66881567e-07
Iter: 463 loss: 5.74646492e-07
Iter: 464 loss: 5.66887365e-07
Iter: 465 loss: 5.6643546e-07
Iter: 466 loss: 5.66105427e-07
Iter: 467 loss: 5.65953485e-07
Iter: 468 loss: 5.6514051e-07
Iter: 469 loss: 5.65214805e-07
Iter: 470 loss: 5.64535412e-07
Iter: 471 loss: 5.63810602e-07
Iter: 472 loss: 5.74197315e-07
Iter: 473 loss: 5.63820095e-07
Iter: 474 loss: 5.63184528e-07
Iter: 475 loss: 5.64623065e-07
Iter: 476 loss: 5.62947093e-07
Iter: 477 loss: 5.62256503e-07
Iter: 478 loss: 5.61970467e-07
Iter: 479 loss: 5.6163e-07
Iter: 480 loss: 5.61222578e-07
Iter: 481 loss: 5.6109e-07
Iter: 482 loss: 5.6073111e-07
Iter: 483 loss: 5.60277442e-07
Iter: 484 loss: 5.60228273e-07
Iter: 485 loss: 5.59427463e-07
Iter: 486 loss: 5.60803301e-07
Iter: 487 loss: 5.59028877e-07
Iter: 488 loss: 5.58320323e-07
Iter: 489 loss: 5.57692147e-07
Iter: 490 loss: 5.57513886e-07
Iter: 491 loss: 5.56673626e-07
Iter: 492 loss: 5.57698513e-07
Iter: 493 loss: 5.56237637e-07
Iter: 494 loss: 5.55365659e-07
Iter: 495 loss: 5.66976667e-07
Iter: 496 loss: 5.55350766e-07
Iter: 497 loss: 5.54503572e-07
Iter: 498 loss: 5.57815781e-07
Iter: 499 loss: 5.54290409e-07
Iter: 500 loss: 5.53665757e-07
Iter: 501 loss: 5.53366e-07
Iter: 502 loss: 5.53060431e-07
Iter: 503 loss: 5.52278038e-07
Iter: 504 loss: 5.54956387e-07
Iter: 505 loss: 5.52049414e-07
Iter: 506 loss: 5.51290327e-07
Iter: 507 loss: 5.58632962e-07
Iter: 508 loss: 5.51262701e-07
Iter: 509 loss: 5.50745767e-07
Iter: 510 loss: 5.52560721e-07
Iter: 511 loss: 5.5062452e-07
Iter: 512 loss: 5.50242362e-07
Iter: 513 loss: 5.50382197e-07
Iter: 514 loss: 5.49955416e-07
Iter: 515 loss: 5.49152105e-07
Iter: 516 loss: 5.51223081e-07
Iter: 517 loss: 5.48897049e-07
Iter: 518 loss: 5.48471576e-07
Iter: 519 loss: 5.50966774e-07
Iter: 520 loss: 5.48424737e-07
Iter: 521 loss: 5.48035189e-07
Iter: 522 loss: 5.47477043e-07
Iter: 523 loss: 5.47472894e-07
Iter: 524 loss: 5.46789295e-07
Iter: 525 loss: 5.47341244e-07
Iter: 526 loss: 5.46365868e-07
Iter: 527 loss: 5.45515036e-07
Iter: 528 loss: 5.46354499e-07
Iter: 529 loss: 5.45013563e-07
Iter: 530 loss: 5.44750037e-07
Iter: 531 loss: 5.44576949e-07
Iter: 532 loss: 5.44086e-07
Iter: 533 loss: 5.43363967e-07
Iter: 534 loss: 5.43336284e-07
Iter: 535 loss: 5.4269492e-07
Iter: 536 loss: 5.45092803e-07
Iter: 537 loss: 5.42533144e-07
Iter: 538 loss: 5.42024452e-07
Iter: 539 loss: 5.45175794e-07
Iter: 540 loss: 5.41973122e-07
Iter: 541 loss: 5.41467898e-07
Iter: 542 loss: 5.42399903e-07
Iter: 543 loss: 5.41237569e-07
Iter: 544 loss: 5.40682038e-07
Iter: 545 loss: 5.4167424e-07
Iter: 546 loss: 5.40413794e-07
Iter: 547 loss: 5.40137e-07
Iter: 548 loss: 5.40111e-07
Iter: 549 loss: 5.3985741e-07
Iter: 550 loss: 5.39460757e-07
Iter: 551 loss: 5.39459052e-07
Iter: 552 loss: 5.38960307e-07
Iter: 553 loss: 5.40873e-07
Iter: 554 loss: 5.38844574e-07
Iter: 555 loss: 5.38259428e-07
Iter: 556 loss: 5.381915e-07
Iter: 557 loss: 5.37797291e-07
Iter: 558 loss: 5.37237838e-07
Iter: 559 loss: 5.36717721e-07
Iter: 560 loss: 5.36592779e-07
Iter: 561 loss: 5.35789411e-07
Iter: 562 loss: 5.41855457e-07
Iter: 563 loss: 5.35725519e-07
Iter: 564 loss: 5.3545773e-07
Iter: 565 loss: 5.35379115e-07
Iter: 566 loss: 5.3507415e-07
Iter: 567 loss: 5.34768901e-07
Iter: 568 loss: 5.34724791e-07
Iter: 569 loss: 5.34204673e-07
Iter: 570 loss: 5.33484467e-07
Iter: 571 loss: 5.33467528e-07
Iter: 572 loss: 5.33377943e-07
Iter: 573 loss: 5.33083494e-07
Iter: 574 loss: 5.32754484e-07
Iter: 575 loss: 5.32095e-07
Iter: 576 loss: 5.32091917e-07
Iter: 577 loss: 5.31508476e-07
Iter: 578 loss: 5.41177428e-07
Iter: 579 loss: 5.31511375e-07
Iter: 580 loss: 5.31078e-07
Iter: 581 loss: 5.32692866e-07
Iter: 582 loss: 5.30976649e-07
Iter: 583 loss: 5.30639852e-07
Iter: 584 loss: 5.30229499e-07
Iter: 585 loss: 5.30187094e-07
Iter: 586 loss: 5.29598537e-07
Iter: 587 loss: 5.34892592e-07
Iter: 588 loss: 5.29573924e-07
Iter: 589 loss: 5.29266629e-07
Iter: 590 loss: 5.28782493e-07
Iter: 591 loss: 5.28762939e-07
Iter: 592 loss: 5.28197404e-07
Iter: 593 loss: 5.30132752e-07
Iter: 594 loss: 5.28029886e-07
Iter: 595 loss: 5.2743394e-07
Iter: 596 loss: 5.27402733e-07
Iter: 597 loss: 5.26980727e-07
Iter: 598 loss: 5.26707197e-07
Iter: 599 loss: 5.2650239e-07
Iter: 600 loss: 5.26212375e-07
Iter: 601 loss: 5.25755e-07
Iter: 602 loss: 5.25754103e-07
Iter: 603 loss: 5.25197663e-07
Iter: 604 loss: 5.24644747e-07
Iter: 605 loss: 5.24516111e-07
Iter: 606 loss: 5.23894641e-07
Iter: 607 loss: 5.33218781e-07
Iter: 608 loss: 5.23900269e-07
Iter: 609 loss: 5.23575522e-07
Iter: 610 loss: 5.26994e-07
Iter: 611 loss: 5.23566655e-07
Iter: 612 loss: 5.23215931e-07
Iter: 613 loss: 5.22888513e-07
Iter: 614 loss: 5.22795688e-07
Iter: 615 loss: 5.22613107e-07
Iter: 616 loss: 5.22563653e-07
Iter: 617 loss: 5.22362598e-07
Iter: 618 loss: 5.22118626e-07
Iter: 619 loss: 5.22070877e-07
Iter: 620 loss: 5.21656375e-07
Iter: 621 loss: 5.21525578e-07
Iter: 622 loss: 5.21277e-07
Iter: 623 loss: 5.20699359e-07
Iter: 624 loss: 5.26056965e-07
Iter: 625 loss: 5.20661956e-07
Iter: 626 loss: 5.20242e-07
Iter: 627 loss: 5.20079539e-07
Iter: 628 loss: 5.19847788e-07
Iter: 629 loss: 5.19325908e-07
Iter: 630 loss: 5.20899221e-07
Iter: 631 loss: 5.19165155e-07
Iter: 632 loss: 5.18727347e-07
Iter: 633 loss: 5.18913794e-07
Iter: 634 loss: 5.18412719e-07
Iter: 635 loss: 5.18428408e-07
Iter: 636 loss: 5.18186425e-07
Iter: 637 loss: 5.18013849e-07
Iter: 638 loss: 5.17545061e-07
Iter: 639 loss: 5.19516846e-07
Iter: 640 loss: 5.17363787e-07
Iter: 641 loss: 5.16719865e-07
Iter: 642 loss: 5.20838512e-07
Iter: 643 loss: 5.16658133e-07
Iter: 644 loss: 5.16272621e-07
Iter: 645 loss: 5.21484594e-07
Iter: 646 loss: 5.16266482e-07
Iter: 647 loss: 5.15885631e-07
Iter: 648 loss: 5.15917122e-07
Iter: 649 loss: 5.15589e-07
Iter: 650 loss: 5.15399449e-07
Iter: 651 loss: 5.15361421e-07
Iter: 652 loss: 5.15180773e-07
Iter: 653 loss: 5.14745352e-07
Iter: 654 loss: 5.20064305e-07
Iter: 655 loss: 5.14701924e-07
Iter: 656 loss: 5.14383942e-07
Iter: 657 loss: 5.17273463e-07
Iter: 658 loss: 5.14351e-07
Iter: 659 loss: 5.14034241e-07
Iter: 660 loss: 5.14530313e-07
Iter: 661 loss: 5.1388156e-07
Iter: 662 loss: 5.13492068e-07
Iter: 663 loss: 5.13720465e-07
Iter: 664 loss: 5.13234227e-07
Iter: 665 loss: 5.12851898e-07
Iter: 666 loss: 5.13489795e-07
Iter: 667 loss: 5.12709278e-07
Iter: 668 loss: 5.12220311e-07
Iter: 669 loss: 5.1274958e-07
Iter: 670 loss: 5.11954795e-07
Iter: 671 loss: 5.11754251e-07
Iter: 672 loss: 5.11678138e-07
Iter: 673 loss: 5.11427402e-07
Iter: 674 loss: 5.10840209e-07
Iter: 675 loss: 5.16419277e-07
Iter: 676 loss: 5.1077177e-07
Iter: 677 loss: 5.10139671e-07
Iter: 678 loss: 5.12809322e-07
Iter: 679 loss: 5.10030816e-07
Iter: 680 loss: 5.09794631e-07
Iter: 681 loss: 5.09769279e-07
Iter: 682 loss: 5.09507117e-07
Iter: 683 loss: 5.09537585e-07
Iter: 684 loss: 5.09294864e-07
Iter: 685 loss: 5.09086249e-07
Iter: 686 loss: 5.09101e-07
Iter: 687 loss: 5.08925154e-07
Iter: 688 loss: 5.0851304e-07
Iter: 689 loss: 5.14025658e-07
Iter: 690 loss: 5.08479e-07
Iter: 691 loss: 5.08149924e-07
Iter: 692 loss: 5.09628194e-07
Iter: 693 loss: 5.08089101e-07
Iter: 694 loss: 5.07745654e-07
Iter: 695 loss: 5.10024847e-07
Iter: 696 loss: 5.07707227e-07
Iter: 697 loss: 5.07456946e-07
Iter: 698 loss: 5.06895503e-07
Iter: 699 loss: 5.12884299e-07
Iter: 700 loss: 5.06825e-07
Iter: 701 loss: 5.06242145e-07
Iter: 702 loss: 5.13809e-07
Iter: 703 loss: 5.0621918e-07
Iter: 704 loss: 5.05802518e-07
Iter: 705 loss: 5.09286679e-07
Iter: 706 loss: 5.05780179e-07
Iter: 707 loss: 5.05367439e-07
Iter: 708 loss: 5.06391189e-07
Iter: 709 loss: 5.05212597e-07
Iter: 710 loss: 5.04958052e-07
Iter: 711 loss: 5.04972832e-07
Iter: 712 loss: 5.04758077e-07
Iter: 713 loss: 5.04452487e-07
Iter: 714 loss: 5.04878e-07
Iter: 715 loss: 5.04281843e-07
Iter: 716 loss: 5.04036279e-07
Iter: 717 loss: 5.04045659e-07
Iter: 718 loss: 5.03797537e-07
Iter: 719 loss: 5.03641957e-07
Iter: 720 loss: 5.03545493e-07
Iter: 721 loss: 5.03257752e-07
Iter: 722 loss: 5.03256956e-07
Iter: 723 loss: 5.03124966e-07
Iter: 724 loss: 5.02726152e-07
Iter: 725 loss: 5.05785067e-07
Iter: 726 loss: 5.02652085e-07
Iter: 727 loss: 5.02278112e-07
Iter: 728 loss: 5.07366224e-07
Iter: 729 loss: 5.02277544e-07
Iter: 730 loss: 5.01973e-07
Iter: 731 loss: 5.025978e-07
Iter: 732 loss: 5.01852128e-07
Iter: 733 loss: 5.01578768e-07
Iter: 734 loss: 5.01188083e-07
Iter: 735 loss: 5.01171257e-07
Iter: 736 loss: 5.00718329e-07
Iter: 737 loss: 5.03241154e-07
Iter: 738 loss: 5.00643353e-07
Iter: 739 loss: 5.00280635e-07
Iter: 740 loss: 5.00278475e-07
Iter: 741 loss: 5.00056672e-07
Iter: 742 loss: 4.99709813e-07
Iter: 743 loss: 4.99711177e-07
Iter: 744 loss: 4.99329644e-07
Iter: 745 loss: 5.00743909e-07
Iter: 746 loss: 4.99227554e-07
Iter: 747 loss: 4.98847271e-07
Iter: 748 loss: 4.99970611e-07
Iter: 749 loss: 4.98744043e-07
Iter: 750 loss: 4.98434133e-07
Iter: 751 loss: 5.03055617e-07
Iter: 752 loss: 4.98427198e-07
Iter: 753 loss: 4.98237341e-07
Iter: 754 loss: 4.9882118e-07
Iter: 755 loss: 4.98194936e-07
Iter: 756 loss: 4.97972565e-07
Iter: 757 loss: 4.97953238e-07
Iter: 758 loss: 4.97787141e-07
Iter: 759 loss: 4.97511223e-07
Iter: 760 loss: 4.9733319e-07
Iter: 761 loss: 4.9724332e-07
Iter: 762 loss: 4.96963139e-07
Iter: 763 loss: 5.00609417e-07
Iter: 764 loss: 4.969678e-07
Iter: 765 loss: 4.96643906e-07
Iter: 766 loss: 4.96397e-07
Iter: 767 loss: 4.96310747e-07
Iter: 768 loss: 4.95925633e-07
Iter: 769 loss: 4.96708083e-07
Iter: 770 loss: 4.95773065e-07
Iter: 771 loss: 4.9541211e-07
Iter: 772 loss: 4.97443068e-07
Iter: 773 loss: 4.95383119e-07
Iter: 774 loss: 4.95081338e-07
Iter: 775 loss: 4.97608823e-07
Iter: 776 loss: 4.95050301e-07
Iter: 777 loss: 4.94887672e-07
Iter: 778 loss: 4.94427809e-07
Iter: 779 loss: 4.97941755e-07
Iter: 780 loss: 4.94320716e-07
Iter: 781 loss: 4.94046162e-07
Iter: 782 loss: 4.94028086e-07
Iter: 783 loss: 4.93833227e-07
Iter: 784 loss: 4.96222e-07
Iter: 785 loss: 4.93819e-07
Iter: 786 loss: 4.93639391e-07
Iter: 787 loss: 4.93769051e-07
Iter: 788 loss: 4.93526613e-07
Iter: 789 loss: 4.93269226e-07
Iter: 790 loss: 4.94325377e-07
Iter: 791 loss: 4.93192829e-07
Iter: 792 loss: 4.92981258e-07
Iter: 793 loss: 4.92716708e-07
Iter: 794 loss: 4.9267868e-07
Iter: 795 loss: 4.92349614e-07
Iter: 796 loss: 4.93604546e-07
Iter: 797 loss: 4.92276627e-07
Iter: 798 loss: 4.92016397e-07
Iter: 799 loss: 4.94577876e-07
Iter: 800 loss: 4.91992125e-07
Iter: 801 loss: 4.91790729e-07
Iter: 802 loss: 4.91747585e-07
Iter: 803 loss: 4.91602e-07
Iter: 804 loss: 4.91335641e-07
Iter: 805 loss: 4.91357468e-07
Iter: 806 loss: 4.91133392e-07
Iter: 807 loss: 4.9091517e-07
Iter: 808 loss: 4.9093228e-07
Iter: 809 loss: 4.90661e-07
Iter: 810 loss: 4.90506409e-07
Iter: 811 loss: 4.90421314e-07
Iter: 812 loss: 4.9018206e-07
Iter: 813 loss: 4.90098955e-07
Iter: 814 loss: 4.89980039e-07
Iter: 815 loss: 4.89612717e-07
Iter: 816 loss: 4.91184437e-07
Iter: 817 loss: 4.89527849e-07
Iter: 818 loss: 4.89212596e-07
Iter: 819 loss: 4.89200602e-07
Iter: 820 loss: 4.89033141e-07
Iter: 821 loss: 4.89097829e-07
Iter: 822 loss: 4.88902401e-07
Iter: 823 loss: 4.88593e-07
Iter: 824 loss: 4.88622788e-07
Iter: 825 loss: 4.88344199e-07
Iter: 826 loss: 4.88089142e-07
Iter: 827 loss: 4.88235059e-07
Iter: 828 loss: 4.87956356e-07
Iter: 829 loss: 4.87712271e-07
Iter: 830 loss: 4.90346565e-07
Iter: 831 loss: 4.87697093e-07
Iter: 832 loss: 4.87487341e-07
Iter: 833 loss: 4.8798438e-07
Iter: 834 loss: 4.87426064e-07
Iter: 835 loss: 4.87235525e-07
Iter: 836 loss: 4.87087902e-07
Iter: 837 loss: 4.87026455e-07
Iter: 838 loss: 4.86669762e-07
Iter: 839 loss: 4.87704e-07
Iter: 840 loss: 4.86570855e-07
Iter: 841 loss: 4.86328418e-07
Iter: 842 loss: 4.86318584e-07
Iter: 843 loss: 4.86081149e-07
Iter: 844 loss: 4.85596047e-07
Iter: 845 loss: 4.93494781e-07
Iter: 846 loss: 4.85582405e-07
Iter: 847 loss: 4.85095256e-07
Iter: 848 loss: 4.86826e-07
Iter: 849 loss: 4.84973441e-07
Iter: 850 loss: 4.847235e-07
Iter: 851 loss: 4.88280534e-07
Iter: 852 loss: 4.84733164e-07
Iter: 853 loss: 4.84476629e-07
Iter: 854 loss: 4.8559906e-07
Iter: 855 loss: 4.84420411e-07
Iter: 856 loss: 4.84261136e-07
Iter: 857 loss: 4.84746238e-07
Iter: 858 loss: 4.84233283e-07
Iter: 859 loss: 4.84054738e-07
Iter: 860 loss: 4.84065936e-07
Iter: 861 loss: 4.83920871e-07
Iter: 862 loss: 4.83706799e-07
Iter: 863 loss: 4.83336066e-07
Iter: 864 loss: 4.83331519e-07
Iter: 865 loss: 4.8303e-07
Iter: 866 loss: 4.83026724e-07
Iter: 867 loss: 4.82686346e-07
Iter: 868 loss: 4.82393375e-07
Iter: 869 loss: 4.8232863e-07
Iter: 870 loss: 4.81934876e-07
Iter: 871 loss: 4.82867279e-07
Iter: 872 loss: 4.81788675e-07
Iter: 873 loss: 4.81470465e-07
Iter: 874 loss: 4.85384703e-07
Iter: 875 loss: 4.81485927e-07
Iter: 876 loss: 4.81168513e-07
Iter: 877 loss: 4.82393602e-07
Iter: 878 loss: 4.81114512e-07
Iter: 879 loss: 4.80937274e-07
Iter: 880 loss: 4.80953702e-07
Iter: 881 loss: 4.80841209e-07
Iter: 882 loss: 4.80593371e-07
Iter: 883 loss: 4.80757933e-07
Iter: 884 loss: 4.80462859e-07
Iter: 885 loss: 4.80329732e-07
Iter: 886 loss: 4.80291419e-07
Iter: 887 loss: 4.80146241e-07
Iter: 888 loss: 4.79918526e-07
Iter: 889 loss: 4.79896585e-07
Iter: 890 loss: 4.79682058e-07
Iter: 891 loss: 4.82828682e-07
Iter: 892 loss: 4.79685184e-07
Iter: 893 loss: 4.79539835e-07
Iter: 894 loss: 4.79224923e-07
Iter: 895 loss: 4.84807401e-07
Iter: 896 loss: 4.79203777e-07
Iter: 897 loss: 4.78828042e-07
Iter: 898 loss: 4.79768232e-07
Iter: 899 loss: 4.78711286e-07
Iter: 900 loss: 4.7848755e-07
Iter: 901 loss: 4.78487209e-07
Iter: 902 loss: 4.78281606e-07
Iter: 903 loss: 4.77983576e-07
Iter: 904 loss: 4.77991705e-07
Iter: 905 loss: 4.77699359e-07
Iter: 906 loss: 4.79192408e-07
Iter: 907 loss: 4.77662354e-07
Iter: 908 loss: 4.77549236e-07
Iter: 909 loss: 4.77518256e-07
Iter: 910 loss: 4.77423214e-07
Iter: 911 loss: 4.77106141e-07
Iter: 912 loss: 4.78042693e-07
Iter: 913 loss: 4.76938681e-07
Iter: 914 loss: 4.7653e-07
Iter: 915 loss: 4.82333519e-07
Iter: 916 loss: 4.76532819e-07
Iter: 917 loss: 4.76309282e-07
Iter: 918 loss: 4.77741253e-07
Iter: 919 loss: 4.7626753e-07
Iter: 920 loss: 4.75997041e-07
Iter: 921 loss: 4.76677258e-07
Iter: 922 loss: 4.75901629e-07
Iter: 923 loss: 4.75697021e-07
Iter: 924 loss: 4.76566811e-07
Iter: 925 loss: 4.7568011e-07
Iter: 926 loss: 4.75495028e-07
Iter: 927 loss: 4.75579782e-07
Iter: 928 loss: 4.75374975e-07
Iter: 929 loss: 4.75178211e-07
Iter: 930 loss: 4.74882455e-07
Iter: 931 loss: 4.74873502e-07
Iter: 932 loss: 4.74668667e-07
Iter: 933 loss: 4.7466358e-07
Iter: 934 loss: 4.74480203e-07
Iter: 935 loss: 4.74545857e-07
Iter: 936 loss: 4.74336389e-07
Iter: 937 loss: 4.74151875e-07
Iter: 938 loss: 4.74130957e-07
Iter: 939 loss: 4.74e-07
Iter: 940 loss: 4.73783359e-07
Iter: 941 loss: 4.73800526e-07
Iter: 942 loss: 4.73558856e-07
Iter: 943 loss: 4.73267562e-07
Iter: 944 loss: 4.73267306e-07
Iter: 945 loss: 4.72956799e-07
Iter: 946 loss: 4.7359589e-07
Iter: 947 loss: 4.72849393e-07
Iter: 948 loss: 4.72553609e-07
Iter: 949 loss: 4.74545828e-07
Iter: 950 loss: 4.72516433e-07
Iter: 951 loss: 4.72282409e-07
Iter: 952 loss: 4.75502816e-07
Iter: 953 loss: 4.72294914e-07
Iter: 954 loss: 4.7216497e-07
Iter: 955 loss: 4.72181853e-07
Iter: 956 loss: 4.72086981e-07
Iter: 957 loss: 4.71918156e-07
Iter: 958 loss: 4.72409056e-07
Iter: 959 loss: 4.71864809e-07
Iter: 960 loss: 4.71696097e-07
Iter: 961 loss: 4.71553619e-07
Iter: 962 loss: 4.7149075e-07
Iter: 963 loss: 4.71279861e-07
Iter: 964 loss: 4.71648377e-07
Iter: 965 loss: 4.71187889e-07
Iter: 966 loss: 4.70929223e-07
Iter: 967 loss: 4.73728022e-07
Iter: 968 loss: 4.70929734e-07
Iter: 969 loss: 4.70737689e-07
Iter: 970 loss: 4.70467739e-07
Iter: 971 loss: 4.7047223e-07
Iter: 972 loss: 4.70141515e-07
Iter: 973 loss: 4.72347466e-07
Iter: 974 loss: 4.70116106e-07
Iter: 975 loss: 4.69891376e-07
Iter: 976 loss: 4.73135941e-07
Iter: 977 loss: 4.69893962e-07
Iter: 978 loss: 4.6975515e-07
Iter: 979 loss: 4.69468887e-07
Iter: 980 loss: 4.73152539e-07
Iter: 981 loss: 4.69444387e-07
Iter: 982 loss: 4.69203201e-07
Iter: 983 loss: 4.71534179e-07
Iter: 984 loss: 4.69190951e-07
Iter: 985 loss: 4.69072205e-07
Iter: 986 loss: 4.69064219e-07
Iter: 987 loss: 4.68960025e-07
Iter: 988 loss: 4.68799016e-07
Iter: 989 loss: 4.68803307e-07
Iter: 990 loss: 4.68618111e-07
Iter: 991 loss: 4.6968745e-07
Iter: 992 loss: 4.68590827e-07
Iter: 993 loss: 4.6840691e-07
Iter: 994 loss: 4.68387043e-07
Iter: 995 loss: 4.68247265e-07
Iter: 996 loss: 4.68009148e-07
Iter: 997 loss: 4.68110528e-07
Iter: 998 loss: 4.67828897e-07
Iter: 999 loss: 4.6763364e-07
Iter: 1000 loss: 4.70821192e-07
Iter: 1001 loss: 4.67639069e-07
Iter: 1002 loss: 4.67454953e-07
Iter: 1003 loss: 4.67451599e-07
Iter: 1004 loss: 4.67313498e-07
Iter: 1005 loss: 4.67048665e-07
Iter: 1006 loss: 4.67197424e-07
Iter: 1007 loss: 4.66902094e-07
Iter: 1008 loss: 4.66815436e-07
Iter: 1009 loss: 4.66775703e-07
Iter: 1010 loss: 4.66668666e-07
Iter: 1011 loss: 4.66412132e-07
Iter: 1012 loss: 4.69807475e-07
Iter: 1013 loss: 4.66385018e-07
Iter: 1014 loss: 4.66108304e-07
Iter: 1015 loss: 4.66557424e-07
Iter: 1016 loss: 4.65991491e-07
Iter: 1017 loss: 4.65816441e-07
Iter: 1018 loss: 4.65795125e-07
Iter: 1019 loss: 4.65594269e-07
Iter: 1020 loss: 4.65695592e-07
Iter: 1021 loss: 4.65455514e-07
Iter: 1022 loss: 4.65288622e-07
Iter: 1023 loss: 4.65969464e-07
Iter: 1024 loss: 4.65246558e-07
Iter: 1025 loss: 4.65069093e-07
Iter: 1026 loss: 4.65008895e-07
Iter: 1027 loss: 4.64899585e-07
Iter: 1028 loss: 4.64655784e-07
Iter: 1029 loss: 4.65471089e-07
Iter: 1030 loss: 4.64599935e-07
Iter: 1031 loss: 4.64404792e-07
Iter: 1032 loss: 4.64439779e-07
Iter: 1033 loss: 4.64273455e-07
Iter: 1034 loss: 4.64015e-07
Iter: 1035 loss: 4.671071e-07
Iter: 1036 loss: 4.64025277e-07
Iter: 1037 loss: 4.63846249e-07
Iter: 1038 loss: 4.63699223e-07
Iter: 1039 loss: 4.6366884e-07
Iter: 1040 loss: 4.63470599e-07
Iter: 1041 loss: 4.64449954e-07
Iter: 1042 loss: 4.63418587e-07
Iter: 1043 loss: 4.63150457e-07
Iter: 1044 loss: 4.64132199e-07
Iter: 1045 loss: 4.63079658e-07
Iter: 1046 loss: 4.62925328e-07
Iter: 1047 loss: 4.62713672e-07
Iter: 1048 loss: 4.62694601e-07
Iter: 1049 loss: 4.62457876e-07
Iter: 1050 loss: 4.6467423e-07
Iter: 1051 loss: 4.62464357e-07
Iter: 1052 loss: 4.62276518e-07
Iter: 1053 loss: 4.64895436e-07
Iter: 1054 loss: 4.62285982e-07
Iter: 1055 loss: 4.62200376e-07
Iter: 1056 loss: 4.62023138e-07
Iter: 1057 loss: 4.62025355e-07
Iter: 1058 loss: 4.61861504e-07
Iter: 1059 loss: 4.64201207e-07
Iter: 1060 loss: 4.61865795e-07
Iter: 1061 loss: 4.61781696e-07
Iter: 1062 loss: 4.61651496e-07
Iter: 1063 loss: 4.61641946e-07
Iter: 1064 loss: 4.61468403e-07
Iter: 1065 loss: 4.62030783e-07
Iter: 1066 loss: 4.6138922e-07
Iter: 1067 loss: 4.61232446e-07
Iter: 1068 loss: 4.62614963e-07
Iter: 1069 loss: 4.61211783e-07
Iter: 1070 loss: 4.61063e-07
Iter: 1071 loss: 4.6095613e-07
Iter: 1072 loss: 4.60882632e-07
Iter: 1073 loss: 4.60646135e-07
Iter: 1074 loss: 4.60904062e-07
Iter: 1075 loss: 4.60524177e-07
Iter: 1076 loss: 4.60365072e-07
Iter: 1077 loss: 4.6035376e-07
Iter: 1078 loss: 4.60227511e-07
Iter: 1079 loss: 4.60024921e-07
Iter: 1080 loss: 4.60004884e-07
Iter: 1081 loss: 4.59837139e-07
Iter: 1082 loss: 4.59957306e-07
Iter: 1083 loss: 4.59709241e-07
Iter: 1084 loss: 4.59637931e-07
Iter: 1085 loss: 4.59593622e-07
Iter: 1086 loss: 4.59490934e-07
Iter: 1087 loss: 4.59300537e-07
Iter: 1088 loss: 4.59307046e-07
Iter: 1089 loss: 4.59202226e-07
Iter: 1090 loss: 4.59200692e-07
Iter: 1091 loss: 4.59092178e-07
Iter: 1092 loss: 4.58868e-07
Iter: 1093 loss: 4.62063355e-07
Iter: 1094 loss: 4.58856192e-07
Iter: 1095 loss: 4.58630893e-07
Iter: 1096 loss: 4.59663e-07
Iter: 1097 loss: 4.58568735e-07
Iter: 1098 loss: 4.58345767e-07
Iter: 1099 loss: 4.58861052e-07
Iter: 1100 loss: 4.58261155e-07
Iter: 1101 loss: 4.57957356e-07
Iter: 1102 loss: 4.59290078e-07
Iter: 1103 loss: 4.57897187e-07
Iter: 1104 loss: 4.5771435e-07
Iter: 1105 loss: 4.58097247e-07
Iter: 1106 loss: 4.5763386e-07
Iter: 1107 loss: 4.57440876e-07
Iter: 1108 loss: 4.57799331e-07
Iter: 1109 loss: 4.57363853e-07
Iter: 1110 loss: 4.57098878e-07
Iter: 1111 loss: 4.57973698e-07
Iter: 1112 loss: 4.57013243e-07
Iter: 1113 loss: 4.56828275e-07
Iter: 1114 loss: 4.5656941e-07
Iter: 1115 loss: 4.56556052e-07
Iter: 1116 loss: 4.564981e-07
Iter: 1117 loss: 4.56415762e-07
Iter: 1118 loss: 4.56286784e-07
Iter: 1119 loss: 4.56306452e-07
Iter: 1120 loss: 4.56181368e-07
Iter: 1121 loss: 4.56076066e-07
Iter: 1122 loss: 4.55953284e-07
Iter: 1123 loss: 4.55928102e-07
Iter: 1124 loss: 4.55654572e-07
Iter: 1125 loss: 4.57400233e-07
Iter: 1126 loss: 4.55615293e-07
Iter: 1127 loss: 4.55473071e-07
Iter: 1128 loss: 4.55286596e-07
Iter: 1129 loss: 4.55278297e-07
Iter: 1130 loss: 4.55045722e-07
Iter: 1131 loss: 4.56139759e-07
Iter: 1132 loss: 4.54997917e-07
Iter: 1133 loss: 4.54847282e-07
Iter: 1134 loss: 4.57330401e-07
Iter: 1135 loss: 4.54854444e-07
Iter: 1136 loss: 4.54723249e-07
Iter: 1137 loss: 4.54700256e-07
Iter: 1138 loss: 4.54636222e-07
Iter: 1139 loss: 4.54468136e-07
Iter: 1140 loss: 4.55000361e-07
Iter: 1141 loss: 4.54415158e-07
Iter: 1142 loss: 4.54311134e-07
Iter: 1143 loss: 4.55779741e-07
Iter: 1144 loss: 4.54303517e-07
Iter: 1145 loss: 4.54201881e-07
Iter: 1146 loss: 4.54058e-07
Iter: 1147 loss: 4.5407441e-07
Iter: 1148 loss: 4.53916584e-07
Iter: 1149 loss: 4.54587e-07
Iter: 1150 loss: 4.53884326e-07
Iter: 1151 loss: 4.53732127e-07
Iter: 1152 loss: 4.55586218e-07
Iter: 1153 loss: 4.53731445e-07
Iter: 1154 loss: 4.53647544e-07
Iter: 1155 loss: 4.53394648e-07
Iter: 1156 loss: 4.54824715e-07
Iter: 1157 loss: 4.53312907e-07
Iter: 1158 loss: 4.5324856e-07
Iter: 1159 loss: 4.53148743e-07
Iter: 1160 loss: 4.53044379e-07
Iter: 1161 loss: 4.52749077e-07
Iter: 1162 loss: 4.56191856e-07
Iter: 1163 loss: 4.52743365e-07
Iter: 1164 loss: 4.52486972e-07
Iter: 1165 loss: 4.53265329e-07
Iter: 1166 loss: 4.52406823e-07
Iter: 1167 loss: 4.5217638e-07
Iter: 1168 loss: 4.53511e-07
Iter: 1169 loss: 4.52131417e-07
Iter: 1170 loss: 4.51883039e-07
Iter: 1171 loss: 4.52960251e-07
Iter: 1172 loss: 4.5183981e-07
Iter: 1173 loss: 4.51703158e-07
Iter: 1174 loss: 4.51619599e-07
Iter: 1175 loss: 4.51558407e-07
Iter: 1176 loss: 4.51345443e-07
Iter: 1177 loss: 4.51925303e-07
Iter: 1178 loss: 4.51255062e-07
Iter: 1179 loss: 4.51083281e-07
Iter: 1180 loss: 4.51076232e-07
Iter: 1181 loss: 4.50953905e-07
Iter: 1182 loss: 4.50831948e-07
Iter: 1183 loss: 4.50797444e-07
Iter: 1184 loss: 4.50586583e-07
Iter: 1185 loss: 4.50592381e-07
Iter: 1186 loss: 4.50416394e-07
Iter: 1187 loss: 4.50420288e-07
Iter: 1188 loss: 4.5027781e-07
Iter: 1189 loss: 4.50227219e-07
Iter: 1190 loss: 4.50038726e-07
Iter: 1191 loss: 4.51109116e-07
Iter: 1192 loss: 4.49985407e-07
Iter: 1193 loss: 4.49874506e-07
Iter: 1194 loss: 4.4986615e-07
Iter: 1195 loss: 4.49748427e-07
Iter: 1196 loss: 4.49630363e-07
Iter: 1197 loss: 4.49619847e-07
Iter: 1198 loss: 4.49447555e-07
Iter: 1199 loss: 4.50061123e-07
Iter: 1200 loss: 4.49437266e-07
Iter: 1201 loss: 4.49279696e-07
Iter: 1202 loss: 4.50677845e-07
Iter: 1203 loss: 4.49288677e-07
Iter: 1204 loss: 4.49152424e-07
Iter: 1205 loss: 4.489643e-07
Iter: 1206 loss: 4.48968194e-07
Iter: 1207 loss: 4.48774273e-07
Iter: 1208 loss: 4.49764968e-07
Iter: 1209 loss: 4.48704611e-07
Iter: 1210 loss: 4.48542636e-07
Iter: 1211 loss: 4.48851097e-07
Iter: 1212 loss: 4.48456831e-07
Iter: 1213 loss: 4.48216326e-07
Iter: 1214 loss: 4.49669955e-07
Iter: 1215 loss: 4.48186313e-07
Iter: 1216 loss: 4.48065265e-07
Iter: 1217 loss: 4.48092806e-07
Iter: 1218 loss: 4.47969228e-07
Iter: 1219 loss: 4.47859236e-07
Iter: 1220 loss: 4.47854092e-07
Iter: 1221 loss: 4.47750381e-07
Iter: 1222 loss: 4.4771798e-07
Iter: 1223 loss: 4.47681401e-07
Iter: 1224 loss: 4.47562513e-07
Iter: 1225 loss: 4.47484524e-07
Iter: 1226 loss: 4.47472928e-07
Iter: 1227 loss: 4.4729552e-07
Iter: 1228 loss: 4.49457019e-07
Iter: 1229 loss: 4.47289608e-07
Iter: 1230 loss: 4.47205196e-07
Iter: 1231 loss: 4.47009654e-07
Iter: 1232 loss: 4.49364109e-07
Iter: 1233 loss: 4.47005618e-07
Iter: 1234 loss: 4.46726119e-07
Iter: 1235 loss: 4.47377317e-07
Iter: 1236 loss: 4.46635084e-07
Iter: 1237 loss: 4.46479049e-07
Iter: 1238 loss: 4.46459552e-07
Iter: 1239 loss: 4.46322247e-07
Iter: 1240 loss: 4.4615922e-07
Iter: 1241 loss: 4.46154502e-07
Iter: 1242 loss: 4.45957085e-07
Iter: 1243 loss: 4.46764801e-07
Iter: 1244 loss: 4.45937076e-07
Iter: 1245 loss: 4.45769615e-07
Iter: 1246 loss: 4.46996125e-07
Iter: 1247 loss: 4.4575296e-07
Iter: 1248 loss: 4.45647032e-07
Iter: 1249 loss: 4.46300561e-07
Iter: 1250 loss: 4.45631429e-07
Iter: 1251 loss: 4.45534113e-07
Iter: 1252 loss: 4.45459364e-07
Iter: 1253 loss: 4.45433841e-07
Iter: 1254 loss: 4.45347951e-07
Iter: 1255 loss: 4.45333939e-07
Iter: 1256 loss: 4.45249555e-07
Iter: 1257 loss: 4.45060664e-07
Iter: 1258 loss: 4.47941488e-07
Iter: 1259 loss: 4.45051285e-07
Iter: 1260 loss: 4.44878566e-07
Iter: 1261 loss: 4.45823389e-07
Iter: 1262 loss: 4.44846762e-07
Iter: 1263 loss: 4.44659463e-07
Iter: 1264 loss: 4.45244098e-07
Iter: 1265 loss: 4.44605917e-07
Iter: 1266 loss: 4.44437461e-07
Iter: 1267 loss: 4.44309393e-07
Iter: 1268 loss: 4.4426946e-07
Iter: 1269 loss: 4.44102142e-07
Iter: 1270 loss: 4.45406954e-07
Iter: 1271 loss: 4.44076193e-07
Iter: 1272 loss: 4.43925131e-07
Iter: 1273 loss: 4.44407362e-07
Iter: 1274 loss: 4.43875479e-07
Iter: 1275 loss: 4.43731722e-07
Iter: 1276 loss: 4.43822273e-07
Iter: 1277 loss: 4.43641454e-07
Iter: 1278 loss: 4.43506792e-07
Iter: 1279 loss: 4.43522083e-07
Iter: 1280 loss: 4.43380543e-07
Iter: 1281 loss: 4.43224167e-07
Iter: 1282 loss: 4.45437081e-07
Iter: 1283 loss: 4.43220927e-07
Iter: 1284 loss: 4.43063755e-07
Iter: 1285 loss: 4.43296642e-07
Iter: 1286 loss: 4.42982866e-07
Iter: 1287 loss: 4.42809039e-07
Iter: 1288 loss: 4.42917951e-07
Iter: 1289 loss: 4.42695921e-07
Iter: 1290 loss: 4.42564783e-07
Iter: 1291 loss: 4.42538692e-07
Iter: 1292 loss: 4.42471077e-07
Iter: 1293 loss: 4.42294379e-07
Iter: 1294 loss: 4.44350917e-07
Iter: 1295 loss: 4.42273574e-07
Iter: 1296 loss: 4.42116686e-07
Iter: 1297 loss: 4.43564204e-07
Iter: 1298 loss: 4.42103214e-07
Iter: 1299 loss: 4.41944e-07
Iter: 1300 loss: 4.42492137e-07
Iter: 1301 loss: 4.41921486e-07
Iter: 1302 loss: 4.41831617e-07
Iter: 1303 loss: 4.41741975e-07
Iter: 1304 loss: 4.41704572e-07
Iter: 1305 loss: 4.41583097e-07
Iter: 1306 loss: 4.43132137e-07
Iter: 1307 loss: 4.41586479e-07
Iter: 1308 loss: 4.41481291e-07
Iter: 1309 loss: 4.41515908e-07
Iter: 1310 loss: 4.41409583e-07
Iter: 1311 loss: 4.41240502e-07
Iter: 1312 loss: 4.41347311e-07
Iter: 1313 loss: 4.41141765e-07
Iter: 1314 loss: 4.40966261e-07
Iter: 1315 loss: 4.41719664e-07
Iter: 1316 loss: 4.40950089e-07
Iter: 1317 loss: 4.4077359e-07
Iter: 1318 loss: 4.41754196e-07
Iter: 1319 loss: 4.40756025e-07
Iter: 1320 loss: 4.40588678e-07
Iter: 1321 loss: 4.40796157e-07
Iter: 1322 loss: 4.4051734e-07
Iter: 1323 loss: 4.40416954e-07
Iter: 1324 loss: 4.42001e-07
Iter: 1325 loss: 4.4042477e-07
Iter: 1326 loss: 4.40323674e-07
Iter: 1327 loss: 4.40308213e-07
Iter: 1328 loss: 4.40256542e-07
Iter: 1329 loss: 4.40128645e-07
Iter: 1330 loss: 4.39963e-07
Iter: 1331 loss: 4.39955528e-07
Iter: 1332 loss: 4.39883706e-07
Iter: 1333 loss: 4.3984619e-07
Iter: 1334 loss: 4.39779399e-07
Iter: 1335 loss: 4.39673869e-07
Iter: 1336 loss: 4.39678331e-07
Iter: 1337 loss: 4.39539122e-07
Iter: 1338 loss: 4.39807593e-07
Iter: 1339 loss: 4.39488247e-07
Iter: 1340 loss: 4.39310526e-07
Iter: 1341 loss: 4.40142571e-07
Iter: 1342 loss: 4.39295746e-07
Iter: 1343 loss: 4.39176603e-07
Iter: 1344 loss: 4.39161852e-07
Iter: 1345 loss: 4.39071073e-07
Iter: 1346 loss: 4.38916629e-07
Iter: 1347 loss: 4.39409064e-07
Iter: 1348 loss: 4.38874395e-07
Iter: 1349 loss: 4.38742745e-07
Iter: 1350 loss: 4.39340852e-07
Iter: 1351 loss: 4.38708526e-07
Iter: 1352 loss: 4.385995e-07
Iter: 1353 loss: 4.38810162e-07
Iter: 1354 loss: 4.38522392e-07
Iter: 1355 loss: 4.38401486e-07
Iter: 1356 loss: 4.39126239e-07
Iter: 1357 loss: 4.3838827e-07
Iter: 1358 loss: 4.38272139e-07
Iter: 1359 loss: 4.38870245e-07
Iter: 1360 loss: 4.38244058e-07
Iter: 1361 loss: 4.38162886e-07
Iter: 1362 loss: 4.37960779e-07
Iter: 1363 loss: 4.41456166e-07
Iter: 1364 loss: 4.37961717e-07
Iter: 1365 loss: 4.37808865e-07
Iter: 1366 loss: 4.3983556e-07
Iter: 1367 loss: 4.37812361e-07
Iter: 1368 loss: 4.37692194e-07
Iter: 1369 loss: 4.38060738e-07
Iter: 1370 loss: 4.37668689e-07
Iter: 1371 loss: 4.37562846e-07
Iter: 1372 loss: 4.37417441e-07
Iter: 1373 loss: 4.37415707e-07
Iter: 1374 loss: 4.37249298e-07
Iter: 1375 loss: 4.39227648e-07
Iter: 1376 loss: 4.37261917e-07
Iter: 1377 loss: 4.37120775e-07
Iter: 1378 loss: 4.37349541e-07
Iter: 1379 loss: 4.37066717e-07
Iter: 1380 loss: 4.36949506e-07
Iter: 1381 loss: 4.37139931e-07
Iter: 1382 loss: 4.3690514e-07
Iter: 1383 loss: 4.36761695e-07
Iter: 1384 loss: 4.37168296e-07
Iter: 1385 loss: 4.36732705e-07
Iter: 1386 loss: 4.36582411e-07
Iter: 1387 loss: 4.3714266e-07
Iter: 1388 loss: 4.36556149e-07
Iter: 1389 loss: 4.36441098e-07
Iter: 1390 loss: 4.36675293e-07
Iter: 1391 loss: 4.36376155e-07
Iter: 1392 loss: 4.36286228e-07
Iter: 1393 loss: 4.37775e-07
Iter: 1394 loss: 4.36288076e-07
Iter: 1395 loss: 4.36204658e-07
Iter: 1396 loss: 4.3606147e-07
Iter: 1397 loss: 4.39257121e-07
Iter: 1398 loss: 4.36057235e-07
Iter: 1399 loss: 4.35890399e-07
Iter: 1400 loss: 4.36374307e-07
Iter: 1401 loss: 4.35842935e-07
Iter: 1402 loss: 4.35743914e-07
Iter: 1403 loss: 4.35748717e-07
Iter: 1404 loss: 4.35660951e-07
Iter: 1405 loss: 4.35529444e-07
Iter: 1406 loss: 4.38670185e-07
Iter: 1407 loss: 4.35534901e-07
Iter: 1408 loss: 4.35397283e-07
Iter: 1409 loss: 4.36427456e-07
Iter: 1410 loss: 4.35392224e-07
Iter: 1411 loss: 4.35268817e-07
Iter: 1412 loss: 4.35663821e-07
Iter: 1413 loss: 4.35241674e-07
Iter: 1414 loss: 4.35119659e-07
Iter: 1415 loss: 4.35198615e-07
Iter: 1416 loss: 4.35045934e-07
Iter: 1417 loss: 4.34931962e-07
Iter: 1418 loss: 4.355513e-07
Iter: 1419 loss: 4.34905786e-07
Iter: 1420 loss: 4.34785193e-07
Iter: 1421 loss: 4.35170364e-07
Iter: 1422 loss: 4.3474904e-07
Iter: 1423 loss: 4.34631659e-07
Iter: 1424 loss: 4.34742446e-07
Iter: 1425 loss: 4.34570609e-07
Iter: 1426 loss: 4.34465306e-07
Iter: 1427 loss: 4.34462436e-07
Iter: 1428 loss: 4.34379814e-07
Iter: 1429 loss: 4.34284743e-07
Iter: 1430 loss: 4.34283834e-07
Iter: 1431 loss: 4.34146045e-07
Iter: 1432 loss: 4.34326068e-07
Iter: 1433 loss: 4.34087212e-07
Iter: 1434 loss: 4.33981114e-07
Iter: 1435 loss: 4.35074412e-07
Iter: 1436 loss: 4.33996547e-07
Iter: 1437 loss: 4.33886953e-07
Iter: 1438 loss: 4.33914721e-07
Iter: 1439 loss: 4.33810612e-07
Iter: 1440 loss: 4.33714121e-07
Iter: 1441 loss: 4.33745271e-07
Iter: 1442 loss: 4.33632e-07
Iter: 1443 loss: 4.3349732e-07
Iter: 1444 loss: 4.34941938e-07
Iter: 1445 loss: 4.33493824e-07
Iter: 1446 loss: 4.33386163e-07
Iter: 1447 loss: 4.33369166e-07
Iter: 1448 loss: 4.33274238e-07
Iter: 1449 loss: 4.33125791e-07
Iter: 1450 loss: 4.33858304e-07
Iter: 1451 loss: 4.33089156e-07
Iter: 1452 loss: 4.3298391e-07
Iter: 1453 loss: 4.33816695e-07
Iter: 1454 loss: 4.32983313e-07
Iter: 1455 loss: 4.32898304e-07
Iter: 1456 loss: 4.3301128e-07
Iter: 1457 loss: 4.32856183e-07
Iter: 1458 loss: 4.32773845e-07
Iter: 1459 loss: 4.33457814e-07
Iter: 1460 loss: 4.32767024e-07
Iter: 1461 loss: 4.3270677e-07
Iter: 1462 loss: 4.32832564e-07
Iter: 1463 loss: 4.32659363e-07
Iter: 1464 loss: 4.32613206e-07
Iter: 1465 loss: 4.32531863e-07
Iter: 1466 loss: 4.32518902e-07
Iter: 1467 loss: 4.32421416e-07
Iter: 1468 loss: 4.32897821e-07
Iter: 1469 loss: 4.3239703e-07
Iter: 1470 loss: 4.3228917e-07
Iter: 1471 loss: 4.32747811e-07
Iter: 1472 loss: 4.32258787e-07
Iter: 1473 loss: 4.32146408e-07
Iter: 1474 loss: 4.32078593e-07
Iter: 1475 loss: 4.32048864e-07
Iter: 1476 loss: 4.31934126e-07
Iter: 1477 loss: 4.33417426e-07
Iter: 1478 loss: 4.31926225e-07
Iter: 1479 loss: 4.31816602e-07
Iter: 1480 loss: 4.31860315e-07
Iter: 1481 loss: 4.31737931e-07
Iter: 1482 loss: 4.31617906e-07
Iter: 1483 loss: 4.32050456e-07
Iter: 1484 loss: 4.31586244e-07
Iter: 1485 loss: 4.31482505e-07
Iter: 1486 loss: 4.31998274e-07
Iter: 1487 loss: 4.31464343e-07
Iter: 1488 loss: 4.31353641e-07
Iter: 1489 loss: 4.31468578e-07
Iter: 1490 loss: 4.31309275e-07
Iter: 1491 loss: 4.31213437e-07
Iter: 1492 loss: 4.32011632e-07
Iter: 1493 loss: 4.31211703e-07
Iter: 1494 loss: 4.31138744e-07
Iter: 1495 loss: 4.31472557e-07
Iter: 1496 loss: 4.31129735e-07
Iter: 1497 loss: 4.31087301e-07
Iter: 1498 loss: 4.30973614e-07
Iter: 1499 loss: 4.33012929e-07
Iter: 1500 loss: 4.30968043e-07
Iter: 1501 loss: 4.3083844e-07
Iter: 1502 loss: 4.31619071e-07
Iter: 1503 loss: 4.30825082e-07
Iter: 1504 loss: 4.30722821e-07
Iter: 1505 loss: 4.31630326e-07
Iter: 1506 loss: 4.30728562e-07
Iter: 1507 loss: 4.30640569e-07
Iter: 1508 loss: 4.30549335e-07
Iter: 1509 loss: 4.30540979e-07
Iter: 1510 loss: 4.30424535e-07
Iter: 1511 loss: 4.31130786e-07
Iter: 1512 loss: 4.3042067e-07
Iter: 1513 loss: 4.3031477e-07
Iter: 1514 loss: 4.30828777e-07
Iter: 1515 loss: 4.30292459e-07
Iter: 1516 loss: 4.30231864e-07
Iter: 1517 loss: 4.3030073e-07
Iter: 1518 loss: 4.30196479e-07
Iter: 1519 loss: 4.30119087e-07
Iter: 1520 loss: 4.30597481e-07
Iter: 1521 loss: 4.3011e-07
Iter: 1522 loss: 4.30046782e-07
Iter: 1523 loss: 4.30167034e-07
Iter: 1524 loss: 4.3003547e-07
Iter: 1525 loss: 4.29978058e-07
Iter: 1526 loss: 4.30095099e-07
Iter: 1527 loss: 4.29940826e-07
Iter: 1528 loss: 4.29875854e-07
Iter: 1529 loss: 4.30424109e-07
Iter: 1530 loss: 4.29871534e-07
Iter: 1531 loss: 4.29809688e-07
Iter: 1532 loss: 4.29702766e-07
Iter: 1533 loss: 4.32124153e-07
Iter: 1534 loss: 4.29705352e-07
Iter: 1535 loss: 4.29580211e-07
Iter: 1536 loss: 4.29966917e-07
Iter: 1537 loss: 4.29560771e-07
Iter: 1538 loss: 4.2945905e-07
Iter: 1539 loss: 4.30395517e-07
Iter: 1540 loss: 4.29456378e-07
Iter: 1541 loss: 4.29367276e-07
Iter: 1542 loss: 4.2930003e-07
Iter: 1543 loss: 4.29263224e-07
Iter: 1544 loss: 4.29159456e-07
Iter: 1545 loss: 4.29481787e-07
Iter: 1546 loss: 4.2913706e-07
Iter: 1547 loss: 4.29051966e-07
Iter: 1548 loss: 4.30252754e-07
Iter: 1549 loss: 4.29046338e-07
Iter: 1550 loss: 4.29008026e-07
Iter: 1551 loss: 4.2896076e-07
Iter: 1552 loss: 4.28945128e-07
Iter: 1553 loss: 4.28880696e-07
Iter: 1554 loss: 4.29298495e-07
Iter: 1555 loss: 4.28858669e-07
Iter: 1556 loss: 4.28804526e-07
Iter: 1557 loss: 4.28932481e-07
Iter: 1558 loss: 4.28781561e-07
Iter: 1559 loss: 4.28715254e-07
Iter: 1560 loss: 4.2874737e-07
Iter: 1561 loss: 4.28670717e-07
Iter: 1562 loss: 4.28585565e-07
Iter: 1563 loss: 4.28589885e-07
Iter: 1564 loss: 4.28538186e-07
Iter: 1565 loss: 4.28435953e-07
Iter: 1566 loss: 4.28435158e-07
Iter: 1567 loss: 4.28325791e-07
Iter: 1568 loss: 4.28593921e-07
Iter: 1569 loss: 4.28294356e-07
Iter: 1570 loss: 4.28198547e-07
Iter: 1571 loss: 4.2907314e-07
Iter: 1572 loss: 4.28204515e-07
Iter: 1573 loss: 4.28138833e-07
Iter: 1574 loss: 4.28131955e-07
Iter: 1575 loss: 4.28077897e-07
Iter: 1576 loss: 4.27990926e-07
Iter: 1577 loss: 4.28167056e-07
Iter: 1578 loss: 4.27946418e-07
Iter: 1579 loss: 4.27882554e-07
Iter: 1580 loss: 4.28858812e-07
Iter: 1581 loss: 4.27883549e-07
Iter: 1582 loss: 4.27837506e-07
Iter: 1583 loss: 4.27779099e-07
Iter: 1584 loss: 4.27747466e-07
Iter: 1585 loss: 4.27675559e-07
Iter: 1586 loss: 4.28131472e-07
Iter: 1587 loss: 4.2765987e-07
Iter: 1588 loss: 4.27580233e-07
Iter: 1589 loss: 4.27981831e-07
Iter: 1590 loss: 4.27551811e-07
Iter: 1591 loss: 4.27506677e-07
Iter: 1592 loss: 4.27508866e-07
Iter: 1593 loss: 4.27461259e-07
Iter: 1594 loss: 4.27386539e-07
Iter: 1595 loss: 4.27394241e-07
Iter: 1596 loss: 4.27337625e-07
Iter: 1597 loss: 4.27251564e-07
Iter: 1598 loss: 4.2725469e-07
Iter: 1599 loss: 4.271742e-07
Iter: 1600 loss: 4.27358827e-07
Iter: 1601 loss: 4.2712557e-07
Iter: 1602 loss: 4.27053891e-07
Iter: 1603 loss: 4.27679367e-07
Iter: 1604 loss: 4.27048974e-07
Iter: 1605 loss: 4.26977408e-07
Iter: 1606 loss: 4.2705625e-07
Iter: 1607 loss: 4.26941966e-07
Iter: 1608 loss: 4.26860197e-07
Iter: 1609 loss: 4.26950407e-07
Iter: 1610 loss: 4.2680324e-07
Iter: 1611 loss: 4.26733891e-07
Iter: 1612 loss: 4.27157829e-07
Iter: 1613 loss: 4.2672e-07
Iter: 1614 loss: 4.26641208e-07
Iter: 1615 loss: 4.2666656e-07
Iter: 1616 loss: 4.26578367e-07
Iter: 1617 loss: 4.26485371e-07
Iter: 1618 loss: 4.26910674e-07
Iter: 1619 loss: 4.26442824e-07
Iter: 1620 loss: 4.26364835e-07
Iter: 1621 loss: 4.26815802e-07
Iter: 1622 loss: 4.2636151e-07
Iter: 1623 loss: 4.26290569e-07
Iter: 1624 loss: 4.26309526e-07
Iter: 1625 loss: 4.26245293e-07
Iter: 1626 loss: 4.26182652e-07
Iter: 1627 loss: 4.26184044e-07
Iter: 1628 loss: 4.26114354e-07
Iter: 1629 loss: 4.26061632e-07
Iter: 1630 loss: 4.26044267e-07
Iter: 1631 loss: 4.25969432e-07
Iter: 1632 loss: 4.25937685e-07
Iter: 1633 loss: 4.2588772e-07
Iter: 1634 loss: 4.25792933e-07
Iter: 1635 loss: 4.26659426e-07
Iter: 1636 loss: 4.25786538e-07
Iter: 1637 loss: 4.25690786e-07
Iter: 1638 loss: 4.26163496e-07
Iter: 1639 loss: 4.25670464e-07
Iter: 1640 loss: 4.25617174e-07
Iter: 1641 loss: 4.25544329e-07
Iter: 1642 loss: 4.25541486e-07
Iter: 1643 loss: 4.25420353e-07
Iter: 1644 loss: 4.26047e-07
Iter: 1645 loss: 4.25406654e-07
Iter: 1646 loss: 4.2532406e-07
Iter: 1647 loss: 4.25721623e-07
Iter: 1648 loss: 4.25305586e-07
Iter: 1649 loss: 4.25240501e-07
Iter: 1650 loss: 4.25245901e-07
Iter: 1651 loss: 4.25189569e-07
Iter: 1652 loss: 4.25071448e-07
Iter: 1653 loss: 4.25868848e-07
Iter: 1654 loss: 4.2507321e-07
Iter: 1655 loss: 4.24996983e-07
Iter: 1656 loss: 4.25059397e-07
Iter: 1657 loss: 4.24956852e-07
Iter: 1658 loss: 4.24897678e-07
Iter: 1659 loss: 4.25632322e-07
Iter: 1660 loss: 4.24903504e-07
Iter: 1661 loss: 4.24828187e-07
Iter: 1662 loss: 4.2480616e-07
Iter: 1663 loss: 4.24774868e-07
Iter: 1664 loss: 4.24696395e-07
Iter: 1665 loss: 4.24686618e-07
Iter: 1666 loss: 4.24628979e-07
Iter: 1667 loss: 4.24522057e-07
Iter: 1668 loss: 4.25052463e-07
Iter: 1669 loss: 4.24512479e-07
Iter: 1670 loss: 4.24451457e-07
Iter: 1671 loss: 4.25043936e-07
Iter: 1672 loss: 4.24446313e-07
Iter: 1673 loss: 4.24353175e-07
Iter: 1674 loss: 4.24287691e-07
Iter: 1675 loss: 4.24278682e-07
Iter: 1676 loss: 4.24174459e-07
Iter: 1677 loss: 4.2485442e-07
Iter: 1678 loss: 4.24153882e-07
Iter: 1679 loss: 4.24080781e-07
Iter: 1680 loss: 4.24436223e-07
Iter: 1681 loss: 4.24056907e-07
Iter: 1682 loss: 4.23990059e-07
Iter: 1683 loss: 4.23964e-07
Iter: 1684 loss: 4.2390235e-07
Iter: 1685 loss: 4.23824787e-07
Iter: 1686 loss: 4.24916976e-07
Iter: 1687 loss: 4.23813105e-07
Iter: 1688 loss: 4.23738044e-07
Iter: 1689 loss: 4.23782581e-07
Iter: 1690 loss: 4.23679182e-07
Iter: 1691 loss: 4.23611397e-07
Iter: 1692 loss: 4.2422721e-07
Iter: 1693 loss: 4.23625636e-07
Iter: 1694 loss: 4.23563222e-07
Iter: 1695 loss: 4.2374154e-07
Iter: 1696 loss: 4.23541195e-07
Iter: 1697 loss: 4.23511977e-07
Iter: 1698 loss: 4.23429753e-07
Iter: 1699 loss: 4.23425888e-07
Iter: 1700 loss: 4.23314845e-07
Iter: 1701 loss: 4.23786048e-07
Iter: 1702 loss: 4.23311633e-07
Iter: 1703 loss: 4.23229636e-07
Iter: 1704 loss: 4.24210157e-07
Iter: 1705 loss: 4.23225913e-07
Iter: 1706 loss: 4.23199936e-07
Iter: 1707 loss: 4.2309145e-07
Iter: 1708 loss: 4.25190166e-07
Iter: 1709 loss: 4.23081246e-07
Iter: 1710 loss: 4.22981316e-07
Iter: 1711 loss: 4.23654626e-07
Iter: 1712 loss: 4.22966821e-07
Iter: 1713 loss: 4.22854583e-07
Iter: 1714 loss: 4.23529059e-07
Iter: 1715 loss: 4.22841595e-07
Iter: 1716 loss: 4.22778015e-07
Iter: 1717 loss: 4.22742573e-07
Iter: 1718 loss: 4.22709377e-07
Iter: 1719 loss: 4.22617e-07
Iter: 1720 loss: 4.23470141e-07
Iter: 1721 loss: 4.22609787e-07
Iter: 1722 loss: 4.2252907e-07
Iter: 1723 loss: 4.22688913e-07
Iter: 1724 loss: 4.22514944e-07
Iter: 1725 loss: 4.22451308e-07
Iter: 1726 loss: 4.22802657e-07
Iter: 1727 loss: 4.22446e-07
Iter: 1728 loss: 4.22412342e-07
Iter: 1729 loss: 4.22583412e-07
Iter: 1730 loss: 4.22402195e-07
Iter: 1731 loss: 4.22349615e-07
Iter: 1732 loss: 4.22266282e-07
Iter: 1733 loss: 4.22259774e-07
Iter: 1734 loss: 4.2216061e-07
Iter: 1735 loss: 4.22530746e-07
Iter: 1736 loss: 4.22147025e-07
Iter: 1737 loss: 4.2204806e-07
Iter: 1738 loss: 4.229571e-07
Iter: 1739 loss: 4.22046867e-07
Iter: 1740 loss: 4.21964785e-07
Iter: 1741 loss: 4.2185971e-07
Iter: 1742 loss: 4.21856612e-07
Iter: 1743 loss: 4.21731841e-07
Iter: 1744 loss: 4.22352031e-07
Iter: 1745 loss: 4.21721353e-07
Iter: 1746 loss: 4.21630375e-07
Iter: 1747 loss: 4.22280948e-07
Iter: 1748 loss: 4.21619916e-07
Iter: 1749 loss: 4.21544343e-07
Iter: 1750 loss: 4.2158328e-07
Iter: 1751 loss: 4.21497049e-07
Iter: 1752 loss: 4.21427671e-07
Iter: 1753 loss: 4.22060907e-07
Iter: 1754 loss: 4.21419031e-07
Iter: 1755 loss: 4.21345135e-07
Iter: 1756 loss: 4.21459106e-07
Iter: 1757 loss: 4.21321374e-07
Iter: 1758 loss: 4.21259784e-07
Iter: 1759 loss: 4.21439267e-07
Iter: 1760 loss: 4.21237786e-07
Iter: 1761 loss: 4.21175287e-07
Iter: 1762 loss: 4.21612157e-07
Iter: 1763 loss: 4.21168863e-07
Iter: 1764 loss: 4.21115089e-07
Iter: 1765 loss: 4.2102829e-07
Iter: 1766 loss: 4.2102468e-07
Iter: 1767 loss: 4.20926369e-07
Iter: 1768 loss: 4.21115686e-07
Iter: 1769 loss: 4.20893201e-07
Iter: 1770 loss: 4.20775621e-07
Iter: 1771 loss: 4.21809375e-07
Iter: 1772 loss: 4.2077437e-07
Iter: 1773 loss: 4.20684273e-07
Iter: 1774 loss: 4.20676e-07
Iter: 1775 loss: 4.20600941e-07
Iter: 1776 loss: 4.20474464e-07
Iter: 1777 loss: 4.20683307e-07
Iter: 1778 loss: 4.20437743e-07
Iter: 1779 loss: 4.20324227e-07
Iter: 1780 loss: 4.21484117e-07
Iter: 1781 loss: 4.20327069e-07
Iter: 1782 loss: 4.20256413e-07
Iter: 1783 loss: 4.20268947e-07
Iter: 1784 loss: 4.20183426e-07
Iter: 1785 loss: 4.20093329e-07
Iter: 1786 loss: 4.2059736e-07
Iter: 1787 loss: 4.2007369e-07
Iter: 1788 loss: 4.1998436e-07
Iter: 1789 loss: 4.20290519e-07
Iter: 1790 loss: 4.19961623e-07
Iter: 1791 loss: 4.19899749e-07
Iter: 1792 loss: 4.19984303e-07
Iter: 1793 loss: 4.19865188e-07
Iter: 1794 loss: 4.19789245e-07
Iter: 1795 loss: 4.20457241e-07
Iter: 1796 loss: 4.19782168e-07
Iter: 1797 loss: 4.19726064e-07
Iter: 1798 loss: 4.19605385e-07
Iter: 1799 loss: 4.19615901e-07
Iter: 1800 loss: 4.1949329e-07
Iter: 1801 loss: 4.19737489e-07
Iter: 1802 loss: 4.19455517e-07
Iter: 1803 loss: 4.1933751e-07
Iter: 1804 loss: 4.19998855e-07
Iter: 1805 loss: 4.19330888e-07
Iter: 1806 loss: 4.19204923e-07
Iter: 1807 loss: 4.19508069e-07
Iter: 1808 loss: 4.1917113e-07
Iter: 1809 loss: 4.19091322e-07
Iter: 1810 loss: 4.19188069e-07
Iter: 1811 loss: 4.19039e-07
Iter: 1812 loss: 4.18936281e-07
Iter: 1813 loss: 4.19652736e-07
Iter: 1814 loss: 4.18942477e-07
Iter: 1815 loss: 4.18871821e-07
Iter: 1816 loss: 4.18887112e-07
Iter: 1817 loss: 4.18818956e-07
Iter: 1818 loss: 4.18709021e-07
Iter: 1819 loss: 4.19204667e-07
Iter: 1820 loss: 4.18700438e-07
Iter: 1821 loss: 4.18623017e-07
Iter: 1822 loss: 4.18868098e-07
Iter: 1823 loss: 4.18588257e-07
Iter: 1824 loss: 4.18521211e-07
Iter: 1825 loss: 4.18501941e-07
Iter: 1826 loss: 4.1847079e-07
Iter: 1827 loss: 4.18361054e-07
Iter: 1828 loss: 4.18365261e-07
Iter: 1829 loss: 4.18291222e-07
Iter: 1830 loss: 4.18215649e-07
Iter: 1831 loss: 4.18224e-07
Iter: 1832 loss: 4.18105145e-07
Iter: 1833 loss: 4.18130924e-07
Iter: 1834 loss: 4.18045744e-07
Iter: 1835 loss: 4.17922593e-07
Iter: 1836 loss: 4.19015464e-07
Iter: 1837 loss: 4.17923076e-07
Iter: 1838 loss: 4.17819876e-07
Iter: 1839 loss: 4.18277494e-07
Iter: 1840 loss: 4.17787618e-07
Iter: 1841 loss: 4.17711448e-07
Iter: 1842 loss: 4.17637068e-07
Iter: 1843 loss: 4.17608362e-07
Iter: 1844 loss: 4.1748504e-07
Iter: 1845 loss: 4.18693588e-07
Iter: 1846 loss: 4.17498427e-07
Iter: 1847 loss: 4.17385053e-07
Iter: 1848 loss: 4.17442664e-07
Iter: 1849 loss: 4.17330455e-07
Iter: 1850 loss: 4.17211368e-07
Iter: 1851 loss: 4.17615809e-07
Iter: 1852 loss: 4.17164358e-07
Iter: 1853 loss: 4.17050586e-07
Iter: 1854 loss: 4.17557544e-07
Iter: 1855 loss: 4.17036972e-07
Iter: 1856 loss: 4.16932664e-07
Iter: 1857 loss: 4.17014519e-07
Iter: 1858 loss: 4.16854903e-07
Iter: 1859 loss: 4.16780068e-07
Iter: 1860 loss: 4.16788e-07
Iter: 1861 loss: 4.16708872e-07
Iter: 1862 loss: 4.16620765e-07
Iter: 1863 loss: 4.1661923e-07
Iter: 1864 loss: 4.164948e-07
Iter: 1865 loss: 4.16641285e-07
Iter: 1866 loss: 4.16423916e-07
Iter: 1867 loss: 4.16320091e-07
Iter: 1868 loss: 4.16991497e-07
Iter: 1869 loss: 4.16306307e-07
Iter: 1870 loss: 4.16210383e-07
Iter: 1871 loss: 4.16655752e-07
Iter: 1872 loss: 4.16192506e-07
Iter: 1873 loss: 4.16106872e-07
Iter: 1874 loss: 4.16048181e-07
Iter: 1875 loss: 4.16009016e-07
Iter: 1876 loss: 4.15909398e-07
Iter: 1877 loss: 4.1672115e-07
Iter: 1878 loss: 4.1588379e-07
Iter: 1879 loss: 4.15818079e-07
Iter: 1880 loss: 4.15891236e-07
Iter: 1881 loss: 4.15736224e-07
Iter: 1882 loss: 4.15641807e-07
Iter: 1883 loss: 4.15996794e-07
Iter: 1884 loss: 4.15604262e-07
Iter: 1885 loss: 4.15498448e-07
Iter: 1886 loss: 4.16116563e-07
Iter: 1887 loss: 4.15486255e-07
Iter: 1888 loss: 4.15412103e-07
Iter: 1889 loss: 4.15439558e-07
Iter: 1890 loss: 4.15352531e-07
Iter: 1891 loss: 4.15298445e-07
Iter: 1892 loss: 4.15300491e-07
Iter: 1893 loss: 4.15245324e-07
Iter: 1894 loss: 4.15166198e-07
Iter: 1895 loss: 4.15156052e-07
Iter: 1896 loss: 4.15070531e-07
Iter: 1897 loss: 4.15033526e-07
Iter: 1898 loss: 4.1496213e-07
Iter: 1899 loss: 4.14819908e-07
Iter: 1900 loss: 4.15367197e-07
Iter: 1901 loss: 4.14792368e-07
Iter: 1902 loss: 4.14668648e-07
Iter: 1903 loss: 4.15994975e-07
Iter: 1904 loss: 4.14667511e-07
Iter: 1905 loss: 4.14562635e-07
Iter: 1906 loss: 4.14454064e-07
Iter: 1907 loss: 4.14447982e-07
Iter: 1908 loss: 4.14307294e-07
Iter: 1909 loss: 4.1532212e-07
Iter: 1910 loss: 4.14287058e-07
Iter: 1911 loss: 4.14182409e-07
Iter: 1912 loss: 4.14564767e-07
Iter: 1913 loss: 4.14164788e-07
Iter: 1914 loss: 4.14058576e-07
Iter: 1915 loss: 4.14104704e-07
Iter: 1916 loss: 4.1399332e-07
Iter: 1917 loss: 4.13871646e-07
Iter: 1918 loss: 4.14780857e-07
Iter: 1919 loss: 4.13852661e-07
Iter: 1920 loss: 4.13745624e-07
Iter: 1921 loss: 4.13886653e-07
Iter: 1922 loss: 4.13689662e-07
Iter: 1923 loss: 4.13603402e-07
Iter: 1924 loss: 4.14388069e-07
Iter: 1925 loss: 4.136119e-07
Iter: 1926 loss: 4.13518848e-07
Iter: 1927 loss: 4.13467518e-07
Iter: 1928 loss: 4.13434464e-07
Iter: 1929 loss: 4.13292128e-07
Iter: 1930 loss: 4.13325239e-07
Iter: 1931 loss: 4.13200212e-07
Iter: 1932 loss: 4.13054181e-07
Iter: 1933 loss: 4.13531097e-07
Iter: 1934 loss: 4.1301459e-07
Iter: 1935 loss: 4.12873561e-07
Iter: 1936 loss: 4.14681438e-07
Iter: 1937 loss: 4.12871771e-07
Iter: 1938 loss: 4.12783436e-07
Iter: 1939 loss: 4.12700359e-07
Iter: 1940 loss: 4.12674353e-07
Iter: 1941 loss: 4.12556744e-07
Iter: 1942 loss: 4.13362301e-07
Iter: 1943 loss: 4.12540714e-07
Iter: 1944 loss: 4.12437373e-07
Iter: 1945 loss: 4.12736028e-07
Iter: 1946 loss: 4.12413357e-07
Iter: 1947 loss: 4.12308935e-07
Iter: 1948 loss: 4.123537e-07
Iter: 1949 loss: 4.12249193e-07
Iter: 1950 loss: 4.12137865e-07
Iter: 1951 loss: 4.13228179e-07
Iter: 1952 loss: 4.12132465e-07
Iter: 1953 loss: 4.12029465e-07
Iter: 1954 loss: 4.12065958e-07
Iter: 1955 loss: 4.11982114e-07
Iter: 1956 loss: 4.11889687e-07
Iter: 1957 loss: 4.12657727e-07
Iter: 1958 loss: 4.11859077e-07
Iter: 1959 loss: 4.11776767e-07
Iter: 1960 loss: 4.11877977e-07
Iter: 1961 loss: 4.11724045e-07
Iter: 1962 loss: 4.11623915e-07
Iter: 1963 loss: 4.11574973e-07
Iter: 1964 loss: 4.1152461e-07
Iter: 1965 loss: 4.11407598e-07
Iter: 1966 loss: 4.1168758e-07
Iter: 1967 loss: 4.11355643e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2
+ date
Mon Oct 26 13:33:32 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi 1 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6406d71e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6406afd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6406afea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6406af400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe664eca9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe64066ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe64063fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6405dd8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe64060a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe64060a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe64060af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe64060a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe64060a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe640516c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe640498d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6404f5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe640500730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6404d3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6404889d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6404d3158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe640411840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe640411620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe64038c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6403c6620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6403c66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6403637b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe640329620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6403297b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6402e7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6402e7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6402ba510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe64026a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe64026a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe640211730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe64021a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe6401e3378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.53302608e-05
Iter: 2 loss: 3.46294037e-05
Iter: 3 loss: 1.22563124e-05
Iter: 4 loss: 1.17103882e-05
Iter: 5 loss: 1.06608104e-05
Iter: 6 loss: 3.25622241e-05
Iter: 7 loss: 1.06553816e-05
Iter: 8 loss: 9.74903e-06
Iter: 9 loss: 1.45196682e-05
Iter: 10 loss: 9.60776561e-06
Iter: 11 loss: 9.09909159e-06
Iter: 12 loss: 1.08140948e-05
Iter: 13 loss: 8.96145411e-06
Iter: 14 loss: 8.59477e-06
Iter: 15 loss: 1.02461072e-05
Iter: 16 loss: 8.52372614e-06
Iter: 17 loss: 8.08838558e-06
Iter: 18 loss: 8.18021e-06
Iter: 19 loss: 7.76606157e-06
Iter: 20 loss: 7.38660447e-06
Iter: 21 loss: 9.52542541e-06
Iter: 22 loss: 7.33336037e-06
Iter: 23 loss: 7.01715226e-06
Iter: 24 loss: 8.05031414e-06
Iter: 25 loss: 6.92889898e-06
Iter: 26 loss: 6.63149922e-06
Iter: 27 loss: 7.02835769e-06
Iter: 28 loss: 6.48181549e-06
Iter: 29 loss: 6.20858873e-06
Iter: 30 loss: 6.28918679e-06
Iter: 31 loss: 6.01207876e-06
Iter: 32 loss: 5.63402864e-06
Iter: 33 loss: 6.34294702e-06
Iter: 34 loss: 5.47391528e-06
Iter: 35 loss: 5.1879324e-06
Iter: 36 loss: 5.61874913e-06
Iter: 37 loss: 5.05157141e-06
Iter: 38 loss: 5.37609685e-06
Iter: 39 loss: 4.95492e-06
Iter: 40 loss: 4.92111758e-06
Iter: 41 loss: 4.81895358e-06
Iter: 42 loss: 5.09036e-06
Iter: 43 loss: 4.76322157e-06
Iter: 44 loss: 4.60295223e-06
Iter: 45 loss: 5.1676534e-06
Iter: 46 loss: 4.5615825e-06
Iter: 47 loss: 4.40842132e-06
Iter: 48 loss: 5.52195888e-06
Iter: 49 loss: 4.39550058e-06
Iter: 50 loss: 4.2778056e-06
Iter: 51 loss: 4.29321e-06
Iter: 52 loss: 4.188e-06
Iter: 53 loss: 4.06004119e-06
Iter: 54 loss: 5.79396419e-06
Iter: 55 loss: 4.05952278e-06
Iter: 56 loss: 3.97834719e-06
Iter: 57 loss: 3.9714364e-06
Iter: 58 loss: 3.91144249e-06
Iter: 59 loss: 3.83276529e-06
Iter: 60 loss: 4.33379546e-06
Iter: 61 loss: 3.8241019e-06
Iter: 62 loss: 3.73864623e-06
Iter: 63 loss: 3.81513428e-06
Iter: 64 loss: 3.68884685e-06
Iter: 65 loss: 3.60202284e-06
Iter: 66 loss: 3.93292703e-06
Iter: 67 loss: 3.5813764e-06
Iter: 68 loss: 3.49711e-06
Iter: 69 loss: 3.44416753e-06
Iter: 70 loss: 3.41068107e-06
Iter: 71 loss: 3.34079141e-06
Iter: 72 loss: 3.3365568e-06
Iter: 73 loss: 3.27103862e-06
Iter: 74 loss: 3.66178983e-06
Iter: 75 loss: 3.26265535e-06
Iter: 76 loss: 3.22793585e-06
Iter: 77 loss: 3.12273824e-06
Iter: 78 loss: 3.40725182e-06
Iter: 79 loss: 3.06618858e-06
Iter: 80 loss: 2.97769589e-06
Iter: 81 loss: 2.97703286e-06
Iter: 82 loss: 2.90087701e-06
Iter: 83 loss: 3.28826127e-06
Iter: 84 loss: 2.88839146e-06
Iter: 85 loss: 2.84288808e-06
Iter: 86 loss: 2.99958469e-06
Iter: 87 loss: 2.83086911e-06
Iter: 88 loss: 2.78437869e-06
Iter: 89 loss: 2.90997468e-06
Iter: 90 loss: 2.76896753e-06
Iter: 91 loss: 2.71693762e-06
Iter: 92 loss: 2.73872342e-06
Iter: 93 loss: 2.6811831e-06
Iter: 94 loss: 2.64240316e-06
Iter: 95 loss: 2.99295493e-06
Iter: 96 loss: 2.64058417e-06
Iter: 97 loss: 2.59902527e-06
Iter: 98 loss: 2.59638227e-06
Iter: 99 loss: 2.56495605e-06
Iter: 100 loss: 2.52262726e-06
Iter: 101 loss: 2.71315503e-06
Iter: 102 loss: 2.51444908e-06
Iter: 103 loss: 2.48356082e-06
Iter: 104 loss: 2.63080665e-06
Iter: 105 loss: 2.47808885e-06
Iter: 106 loss: 2.45149954e-06
Iter: 107 loss: 2.82074279e-06
Iter: 108 loss: 2.45136835e-06
Iter: 109 loss: 2.4325218e-06
Iter: 110 loss: 2.39323595e-06
Iter: 111 loss: 3.06264701e-06
Iter: 112 loss: 2.39237306e-06
Iter: 113 loss: 2.35635389e-06
Iter: 114 loss: 2.35203265e-06
Iter: 115 loss: 2.32608954e-06
Iter: 116 loss: 2.29553507e-06
Iter: 117 loss: 2.29485818e-06
Iter: 118 loss: 2.26843827e-06
Iter: 119 loss: 2.31478862e-06
Iter: 120 loss: 2.25675035e-06
Iter: 121 loss: 2.23102916e-06
Iter: 122 loss: 2.28686781e-06
Iter: 123 loss: 2.22095969e-06
Iter: 124 loss: 2.18567675e-06
Iter: 125 loss: 2.25174836e-06
Iter: 126 loss: 2.17077e-06
Iter: 127 loss: 2.14586589e-06
Iter: 128 loss: 2.21383198e-06
Iter: 129 loss: 2.13768e-06
Iter: 130 loss: 2.11313341e-06
Iter: 131 loss: 2.23605502e-06
Iter: 132 loss: 2.10898338e-06
Iter: 133 loss: 2.08276151e-06
Iter: 134 loss: 2.0825637e-06
Iter: 135 loss: 2.06172899e-06
Iter: 136 loss: 2.03505692e-06
Iter: 137 loss: 2.12435089e-06
Iter: 138 loss: 2.02770161e-06
Iter: 139 loss: 2.0111479e-06
Iter: 140 loss: 2.00997601e-06
Iter: 141 loss: 1.99499095e-06
Iter: 142 loss: 1.98096086e-06
Iter: 143 loss: 1.97754252e-06
Iter: 144 loss: 1.96079782e-06
Iter: 145 loss: 1.93732421e-06
Iter: 146 loss: 1.93644587e-06
Iter: 147 loss: 1.90696414e-06
Iter: 148 loss: 2.13242356e-06
Iter: 149 loss: 1.90483456e-06
Iter: 150 loss: 1.88817887e-06
Iter: 151 loss: 2.05257038e-06
Iter: 152 loss: 1.88767694e-06
Iter: 153 loss: 1.87030378e-06
Iter: 154 loss: 1.85272643e-06
Iter: 155 loss: 1.84923078e-06
Iter: 156 loss: 1.83237694e-06
Iter: 157 loss: 1.83200632e-06
Iter: 158 loss: 1.81987491e-06
Iter: 159 loss: 1.80379197e-06
Iter: 160 loss: 1.80285906e-06
Iter: 161 loss: 1.78703249e-06
Iter: 162 loss: 1.96711335e-06
Iter: 163 loss: 1.78680818e-06
Iter: 164 loss: 1.77266043e-06
Iter: 165 loss: 1.79105416e-06
Iter: 166 loss: 1.76545188e-06
Iter: 167 loss: 1.74891272e-06
Iter: 168 loss: 1.75234879e-06
Iter: 169 loss: 1.73668207e-06
Iter: 170 loss: 1.73283661e-06
Iter: 171 loss: 1.72830528e-06
Iter: 172 loss: 1.72045907e-06
Iter: 173 loss: 1.7188097e-06
Iter: 174 loss: 1.71373154e-06
Iter: 175 loss: 1.70383782e-06
Iter: 176 loss: 1.68769338e-06
Iter: 177 loss: 1.68754673e-06
Iter: 178 loss: 1.67188512e-06
Iter: 179 loss: 1.75266405e-06
Iter: 180 loss: 1.66933978e-06
Iter: 181 loss: 1.65512745e-06
Iter: 182 loss: 1.69766599e-06
Iter: 183 loss: 1.65086237e-06
Iter: 184 loss: 1.63747904e-06
Iter: 185 loss: 1.77460606e-06
Iter: 186 loss: 1.63712502e-06
Iter: 187 loss: 1.62932793e-06
Iter: 188 loss: 1.62924766e-06
Iter: 189 loss: 1.62313654e-06
Iter: 190 loss: 1.60989111e-06
Iter: 191 loss: 1.65949905e-06
Iter: 192 loss: 1.60664149e-06
Iter: 193 loss: 1.59752358e-06
Iter: 194 loss: 1.59929186e-06
Iter: 195 loss: 1.59065883e-06
Iter: 196 loss: 1.58120724e-06
Iter: 197 loss: 1.72210844e-06
Iter: 198 loss: 1.58121543e-06
Iter: 199 loss: 1.57385e-06
Iter: 200 loss: 1.56201395e-06
Iter: 201 loss: 1.56189458e-06
Iter: 202 loss: 1.55509781e-06
Iter: 203 loss: 1.55420571e-06
Iter: 204 loss: 1.5469675e-06
Iter: 205 loss: 1.57146224e-06
Iter: 206 loss: 1.5450255e-06
Iter: 207 loss: 1.53905171e-06
Iter: 208 loss: 1.52593384e-06
Iter: 209 loss: 1.71961108e-06
Iter: 210 loss: 1.52536256e-06
Iter: 211 loss: 1.51460631e-06
Iter: 212 loss: 1.59184469e-06
Iter: 213 loss: 1.51367703e-06
Iter: 214 loss: 1.50529843e-06
Iter: 215 loss: 1.51127e-06
Iter: 216 loss: 1.50007702e-06
Iter: 217 loss: 1.49295215e-06
Iter: 218 loss: 1.4928446e-06
Iter: 219 loss: 1.48795641e-06
Iter: 220 loss: 1.48534878e-06
Iter: 221 loss: 1.48307811e-06
Iter: 222 loss: 1.47486844e-06
Iter: 223 loss: 1.51174186e-06
Iter: 224 loss: 1.47334458e-06
Iter: 225 loss: 1.46574814e-06
Iter: 226 loss: 1.47680362e-06
Iter: 227 loss: 1.46206628e-06
Iter: 228 loss: 1.45656918e-06
Iter: 229 loss: 1.47633659e-06
Iter: 230 loss: 1.45515128e-06
Iter: 231 loss: 1.44743012e-06
Iter: 232 loss: 1.45362742e-06
Iter: 233 loss: 1.44284616e-06
Iter: 234 loss: 1.4369889e-06
Iter: 235 loss: 1.46143361e-06
Iter: 236 loss: 1.43581883e-06
Iter: 237 loss: 1.42999988e-06
Iter: 238 loss: 1.5015861e-06
Iter: 239 loss: 1.42992462e-06
Iter: 240 loss: 1.42639578e-06
Iter: 241 loss: 1.41980263e-06
Iter: 242 loss: 1.5710807e-06
Iter: 243 loss: 1.41981525e-06
Iter: 244 loss: 1.41278861e-06
Iter: 245 loss: 1.41210717e-06
Iter: 246 loss: 1.40698671e-06
Iter: 247 loss: 1.39688302e-06
Iter: 248 loss: 1.44817682e-06
Iter: 249 loss: 1.39521342e-06
Iter: 250 loss: 1.38899713e-06
Iter: 251 loss: 1.4409577e-06
Iter: 252 loss: 1.38871167e-06
Iter: 253 loss: 1.38216831e-06
Iter: 254 loss: 1.38678172e-06
Iter: 255 loss: 1.37803124e-06
Iter: 256 loss: 1.37204347e-06
Iter: 257 loss: 1.40981047e-06
Iter: 258 loss: 1.37137022e-06
Iter: 259 loss: 1.36556139e-06
Iter: 260 loss: 1.3687461e-06
Iter: 261 loss: 1.36174936e-06
Iter: 262 loss: 1.35479831e-06
Iter: 263 loss: 1.364127e-06
Iter: 264 loss: 1.35136838e-06
Iter: 265 loss: 1.34519701e-06
Iter: 266 loss: 1.43080945e-06
Iter: 267 loss: 1.34519951e-06
Iter: 268 loss: 1.34085656e-06
Iter: 269 loss: 1.3356796e-06
Iter: 270 loss: 1.3351588e-06
Iter: 271 loss: 1.33517165e-06
Iter: 272 loss: 1.33213314e-06
Iter: 273 loss: 1.32961486e-06
Iter: 274 loss: 1.32418154e-06
Iter: 275 loss: 1.40868553e-06
Iter: 276 loss: 1.32396985e-06
Iter: 277 loss: 1.31898764e-06
Iter: 278 loss: 1.32295577e-06
Iter: 279 loss: 1.31596039e-06
Iter: 280 loss: 1.30875958e-06
Iter: 281 loss: 1.32014111e-06
Iter: 282 loss: 1.30538399e-06
Iter: 283 loss: 1.2991859e-06
Iter: 284 loss: 1.33344111e-06
Iter: 285 loss: 1.29827686e-06
Iter: 286 loss: 1.2926447e-06
Iter: 287 loss: 1.33084632e-06
Iter: 288 loss: 1.29204432e-06
Iter: 289 loss: 1.28730858e-06
Iter: 290 loss: 1.28924921e-06
Iter: 291 loss: 1.28397642e-06
Iter: 292 loss: 1.2791354e-06
Iter: 293 loss: 1.32077037e-06
Iter: 294 loss: 1.27892781e-06
Iter: 295 loss: 1.27476483e-06
Iter: 296 loss: 1.27128226e-06
Iter: 297 loss: 1.27023623e-06
Iter: 298 loss: 1.26654641e-06
Iter: 299 loss: 1.26651935e-06
Iter: 300 loss: 1.26354598e-06
Iter: 301 loss: 1.26345481e-06
Iter: 302 loss: 1.26116186e-06
Iter: 303 loss: 1.25720703e-06
Iter: 304 loss: 1.28121894e-06
Iter: 305 loss: 1.25671113e-06
Iter: 306 loss: 1.25214672e-06
Iter: 307 loss: 1.26943382e-06
Iter: 308 loss: 1.25108522e-06
Iter: 309 loss: 1.24903931e-06
Iter: 310 loss: 1.24328699e-06
Iter: 311 loss: 1.27560031e-06
Iter: 312 loss: 1.24159783e-06
Iter: 313 loss: 1.23522898e-06
Iter: 314 loss: 1.32186369e-06
Iter: 315 loss: 1.23520283e-06
Iter: 316 loss: 1.2307853e-06
Iter: 317 loss: 1.22935398e-06
Iter: 318 loss: 1.22676022e-06
Iter: 319 loss: 1.22274037e-06
Iter: 320 loss: 1.22259792e-06
Iter: 321 loss: 1.21974199e-06
Iter: 322 loss: 1.22363417e-06
Iter: 323 loss: 1.21831852e-06
Iter: 324 loss: 1.21501807e-06
Iter: 325 loss: 1.21764674e-06
Iter: 326 loss: 1.21305163e-06
Iter: 327 loss: 1.20845721e-06
Iter: 328 loss: 1.22457323e-06
Iter: 329 loss: 1.20725963e-06
Iter: 330 loss: 1.20432151e-06
Iter: 331 loss: 1.20994173e-06
Iter: 332 loss: 1.20304082e-06
Iter: 333 loss: 1.19935157e-06
Iter: 334 loss: 1.21556764e-06
Iter: 335 loss: 1.1986026e-06
Iter: 336 loss: 1.19589981e-06
Iter: 337 loss: 1.20343952e-06
Iter: 338 loss: 1.19494837e-06
Iter: 339 loss: 1.19226661e-06
Iter: 340 loss: 1.22824736e-06
Iter: 341 loss: 1.19224376e-06
Iter: 342 loss: 1.19080028e-06
Iter: 343 loss: 1.18697483e-06
Iter: 344 loss: 1.21573419e-06
Iter: 345 loss: 1.1861481e-06
Iter: 346 loss: 1.18193168e-06
Iter: 347 loss: 1.1933148e-06
Iter: 348 loss: 1.18054106e-06
Iter: 349 loss: 1.17578656e-06
Iter: 350 loss: 1.19067931e-06
Iter: 351 loss: 1.17440663e-06
Iter: 352 loss: 1.17008756e-06
Iter: 353 loss: 1.1928e-06
Iter: 354 loss: 1.1694583e-06
Iter: 355 loss: 1.16620481e-06
Iter: 356 loss: 1.19529409e-06
Iter: 357 loss: 1.16597096e-06
Iter: 358 loss: 1.16342756e-06
Iter: 359 loss: 1.16135038e-06
Iter: 360 loss: 1.16054798e-06
Iter: 361 loss: 1.1567746e-06
Iter: 362 loss: 1.19221409e-06
Iter: 363 loss: 1.15668081e-06
Iter: 364 loss: 1.15438161e-06
Iter: 365 loss: 1.1567497e-06
Iter: 366 loss: 1.1530575e-06
Iter: 367 loss: 1.15062244e-06
Iter: 368 loss: 1.16199953e-06
Iter: 369 loss: 1.15021453e-06
Iter: 370 loss: 1.14757324e-06
Iter: 371 loss: 1.14999079e-06
Iter: 372 loss: 1.14603438e-06
Iter: 373 loss: 1.14503928e-06
Iter: 374 loss: 1.14459669e-06
Iter: 375 loss: 1.14336467e-06
Iter: 376 loss: 1.13984697e-06
Iter: 377 loss: 1.15391526e-06
Iter: 378 loss: 1.13838291e-06
Iter: 379 loss: 1.13439319e-06
Iter: 380 loss: 1.15413832e-06
Iter: 381 loss: 1.13374654e-06
Iter: 382 loss: 1.13011595e-06
Iter: 383 loss: 1.13077681e-06
Iter: 384 loss: 1.12750217e-06
Iter: 385 loss: 1.12432986e-06
Iter: 386 loss: 1.12426051e-06
Iter: 387 loss: 1.12235898e-06
Iter: 388 loss: 1.1256036e-06
Iter: 389 loss: 1.12150258e-06
Iter: 390 loss: 1.11852478e-06
Iter: 391 loss: 1.12152861e-06
Iter: 392 loss: 1.11691702e-06
Iter: 393 loss: 1.11445183e-06
Iter: 394 loss: 1.12758778e-06
Iter: 395 loss: 1.11409281e-06
Iter: 396 loss: 1.11168947e-06
Iter: 397 loss: 1.11380132e-06
Iter: 398 loss: 1.11023087e-06
Iter: 399 loss: 1.10758901e-06
Iter: 400 loss: 1.11465783e-06
Iter: 401 loss: 1.10664894e-06
Iter: 402 loss: 1.10396479e-06
Iter: 403 loss: 1.12279486e-06
Iter: 404 loss: 1.10375072e-06
Iter: 405 loss: 1.10194037e-06
Iter: 406 loss: 1.11098427e-06
Iter: 407 loss: 1.10166945e-06
Iter: 408 loss: 1.09922257e-06
Iter: 409 loss: 1.09876407e-06
Iter: 410 loss: 1.09717735e-06
Iter: 411 loss: 1.09527673e-06
Iter: 412 loss: 1.092601e-06
Iter: 413 loss: 1.09256553e-06
Iter: 414 loss: 1.08924598e-06
Iter: 415 loss: 1.11161148e-06
Iter: 416 loss: 1.08892732e-06
Iter: 417 loss: 1.08665233e-06
Iter: 418 loss: 1.08976474e-06
Iter: 419 loss: 1.08555537e-06
Iter: 420 loss: 1.08246945e-06
Iter: 421 loss: 1.09916141e-06
Iter: 422 loss: 1.0820371e-06
Iter: 423 loss: 1.08003155e-06
Iter: 424 loss: 1.09799134e-06
Iter: 425 loss: 1.07992935e-06
Iter: 426 loss: 1.07855521e-06
Iter: 427 loss: 1.07661128e-06
Iter: 428 loss: 1.07652113e-06
Iter: 429 loss: 1.07373808e-06
Iter: 430 loss: 1.09522057e-06
Iter: 431 loss: 1.0734949e-06
Iter: 432 loss: 1.07180722e-06
Iter: 433 loss: 1.07518008e-06
Iter: 434 loss: 1.07109179e-06
Iter: 435 loss: 1.06952587e-06
Iter: 436 loss: 1.07812241e-06
Iter: 437 loss: 1.06931918e-06
Iter: 438 loss: 1.06773075e-06
Iter: 439 loss: 1.07190863e-06
Iter: 440 loss: 1.06718369e-06
Iter: 441 loss: 1.06534253e-06
Iter: 442 loss: 1.07730898e-06
Iter: 443 loss: 1.06516063e-06
Iter: 444 loss: 1.06421248e-06
Iter: 445 loss: 1.06183074e-06
Iter: 446 loss: 1.08736117e-06
Iter: 447 loss: 1.06158109e-06
Iter: 448 loss: 1.05899403e-06
Iter: 449 loss: 1.06493553e-06
Iter: 450 loss: 1.05803679e-06
Iter: 451 loss: 1.05561867e-06
Iter: 452 loss: 1.06613982e-06
Iter: 453 loss: 1.05510708e-06
Iter: 454 loss: 1.05272807e-06
Iter: 455 loss: 1.06384948e-06
Iter: 456 loss: 1.05231675e-06
Iter: 457 loss: 1.05045751e-06
Iter: 458 loss: 1.06581888e-06
Iter: 459 loss: 1.05036133e-06
Iter: 460 loss: 1.0490794e-06
Iter: 461 loss: 1.04992534e-06
Iter: 462 loss: 1.048277e-06
Iter: 463 loss: 1.04654373e-06
Iter: 464 loss: 1.04942637e-06
Iter: 465 loss: 1.04578805e-06
Iter: 466 loss: 1.04403716e-06
Iter: 467 loss: 1.05493064e-06
Iter: 468 loss: 1.04383435e-06
Iter: 469 loss: 1.04270202e-06
Iter: 470 loss: 1.04365085e-06
Iter: 471 loss: 1.04200478e-06
Iter: 472 loss: 1.04024025e-06
Iter: 473 loss: 1.04716707e-06
Iter: 474 loss: 1.03987509e-06
Iter: 475 loss: 1.03838329e-06
Iter: 476 loss: 1.05559104e-06
Iter: 477 loss: 1.03836283e-06
Iter: 478 loss: 1.03733385e-06
Iter: 479 loss: 1.03542038e-06
Iter: 480 loss: 1.07950814e-06
Iter: 481 loss: 1.03542266e-06
Iter: 482 loss: 1.03328421e-06
Iter: 483 loss: 1.03172533e-06
Iter: 484 loss: 1.0310323e-06
Iter: 485 loss: 1.02857462e-06
Iter: 486 loss: 1.03837306e-06
Iter: 487 loss: 1.02799231e-06
Iter: 488 loss: 1.02554804e-06
Iter: 489 loss: 1.04578703e-06
Iter: 490 loss: 1.02540832e-06
Iter: 491 loss: 1.02392721e-06
Iter: 492 loss: 1.03474292e-06
Iter: 493 loss: 1.02385047e-06
Iter: 494 loss: 1.02236322e-06
Iter: 495 loss: 1.02206218e-06
Iter: 496 loss: 1.0210648e-06
Iter: 497 loss: 1.01906312e-06
Iter: 498 loss: 1.02882359e-06
Iter: 499 loss: 1.01876617e-06
Iter: 500 loss: 1.01726346e-06
Iter: 501 loss: 1.02258878e-06
Iter: 502 loss: 1.01689545e-06
Iter: 503 loss: 1.01530054e-06
Iter: 504 loss: 1.01541093e-06
Iter: 505 loss: 1.01407272e-06
Iter: 506 loss: 1.01243268e-06
Iter: 507 loss: 1.03711386e-06
Iter: 508 loss: 1.01242279e-06
Iter: 509 loss: 1.01137448e-06
Iter: 510 loss: 1.01862986e-06
Iter: 511 loss: 1.01132878e-06
Iter: 512 loss: 1.01016667e-06
Iter: 513 loss: 1.00838895e-06
Iter: 514 loss: 1.00836269e-06
Iter: 515 loss: 1.00657985e-06
Iter: 516 loss: 1.00870739e-06
Iter: 517 loss: 1.00566388e-06
Iter: 518 loss: 1.00396528e-06
Iter: 519 loss: 1.00341117e-06
Iter: 520 loss: 1.0024977e-06
Iter: 521 loss: 1.00051795e-06
Iter: 522 loss: 1.02528622e-06
Iter: 523 loss: 1.00046736e-06
Iter: 524 loss: 9.98891892e-07
Iter: 525 loss: 1.00511238e-06
Iter: 526 loss: 9.98539e-07
Iter: 527 loss: 9.96878157e-07
Iter: 528 loss: 1.0024213e-06
Iter: 529 loss: 9.96425e-07
Iter: 530 loss: 9.95179e-07
Iter: 531 loss: 9.98848e-07
Iter: 532 loss: 9.9483168e-07
Iter: 533 loss: 9.93500635e-07
Iter: 534 loss: 9.94708671e-07
Iter: 535 loss: 9.92791911e-07
Iter: 536 loss: 9.91136289e-07
Iter: 537 loss: 9.97936809e-07
Iter: 538 loss: 9.9079773e-07
Iter: 539 loss: 9.89727141e-07
Iter: 540 loss: 9.94835204e-07
Iter: 541 loss: 9.89500222e-07
Iter: 542 loss: 9.88306e-07
Iter: 543 loss: 9.92665719e-07
Iter: 544 loss: 9.8799444e-07
Iter: 545 loss: 9.86466375e-07
Iter: 546 loss: 9.87601e-07
Iter: 547 loss: 9.85537326e-07
Iter: 548 loss: 9.84426492e-07
Iter: 549 loss: 9.84614e-07
Iter: 550 loss: 9.83603059e-07
Iter: 551 loss: 9.82271558e-07
Iter: 552 loss: 9.81643097e-07
Iter: 553 loss: 9.81010317e-07
Iter: 554 loss: 9.79151764e-07
Iter: 555 loss: 9.84495e-07
Iter: 556 loss: 9.78617436e-07
Iter: 557 loss: 9.76763658e-07
Iter: 558 loss: 9.96414e-07
Iter: 559 loss: 9.76716365e-07
Iter: 560 loss: 9.75347348e-07
Iter: 561 loss: 9.81595122e-07
Iter: 562 loss: 9.75051307e-07
Iter: 563 loss: 9.73959345e-07
Iter: 564 loss: 9.74906243e-07
Iter: 565 loss: 9.73286092e-07
Iter: 566 loss: 9.71917871e-07
Iter: 567 loss: 9.76579599e-07
Iter: 568 loss: 9.71542e-07
Iter: 569 loss: 9.70315796e-07
Iter: 570 loss: 9.74328e-07
Iter: 571 loss: 9.6995484e-07
Iter: 572 loss: 9.68669156e-07
Iter: 573 loss: 9.69286248e-07
Iter: 574 loss: 9.67777851e-07
Iter: 575 loss: 9.66350626e-07
Iter: 576 loss: 9.66376092e-07
Iter: 577 loss: 9.65418508e-07
Iter: 578 loss: 9.68742711e-07
Iter: 579 loss: 9.65220806e-07
Iter: 580 loss: 9.64493324e-07
Iter: 581 loss: 9.63001867e-07
Iter: 582 loss: 9.83381824e-07
Iter: 583 loss: 9.6285e-07
Iter: 584 loss: 9.60986767e-07
Iter: 585 loss: 9.67772849e-07
Iter: 586 loss: 9.60570446e-07
Iter: 587 loss: 9.59170166e-07
Iter: 588 loss: 9.58773626e-07
Iter: 589 loss: 9.57990551e-07
Iter: 590 loss: 9.56085e-07
Iter: 591 loss: 9.75583e-07
Iter: 592 loss: 9.56019676e-07
Iter: 593 loss: 9.54533789e-07
Iter: 594 loss: 9.65972e-07
Iter: 595 loss: 9.54459e-07
Iter: 596 loss: 9.53398228e-07
Iter: 597 loss: 9.54222742e-07
Iter: 598 loss: 9.52783864e-07
Iter: 599 loss: 9.51432355e-07
Iter: 600 loss: 9.54214329e-07
Iter: 601 loss: 9.50903257e-07
Iter: 602 loss: 9.49656169e-07
Iter: 603 loss: 9.56188273e-07
Iter: 604 loss: 9.49450452e-07
Iter: 605 loss: 9.48325805e-07
Iter: 606 loss: 9.49313687e-07
Iter: 607 loss: 9.47677222e-07
Iter: 608 loss: 9.46721514e-07
Iter: 609 loss: 9.4672356e-07
Iter: 610 loss: 9.45943668e-07
Iter: 611 loss: 9.4848042e-07
Iter: 612 loss: 9.45740908e-07
Iter: 613 loss: 9.45041506e-07
Iter: 614 loss: 9.44161798e-07
Iter: 615 loss: 9.44113424e-07
Iter: 616 loss: 9.42705185e-07
Iter: 617 loss: 9.41925521e-07
Iter: 618 loss: 9.4133236e-07
Iter: 619 loss: 9.39970107e-07
Iter: 620 loss: 9.50212154e-07
Iter: 621 loss: 9.39839481e-07
Iter: 622 loss: 9.38542087e-07
Iter: 623 loss: 9.39527808e-07
Iter: 624 loss: 9.37827394e-07
Iter: 625 loss: 9.3701334e-07
Iter: 626 loss: 9.36924607e-07
Iter: 627 loss: 9.36141305e-07
Iter: 628 loss: 9.35472485e-07
Iter: 629 loss: 9.3527558e-07
Iter: 630 loss: 9.34013372e-07
Iter: 631 loss: 9.4086397e-07
Iter: 632 loss: 9.33842216e-07
Iter: 633 loss: 9.32905e-07
Iter: 634 loss: 9.35478113e-07
Iter: 635 loss: 9.32618264e-07
Iter: 636 loss: 9.31572117e-07
Iter: 637 loss: 9.32798173e-07
Iter: 638 loss: 9.31019429e-07
Iter: 639 loss: 9.30058718e-07
Iter: 640 loss: 9.42096165e-07
Iter: 641 loss: 9.30039221e-07
Iter: 642 loss: 9.29338512e-07
Iter: 643 loss: 9.33702268e-07
Iter: 644 loss: 9.29291332e-07
Iter: 645 loss: 9.28687427e-07
Iter: 646 loss: 9.27638212e-07
Iter: 647 loss: 9.27645658e-07
Iter: 648 loss: 9.26188591e-07
Iter: 649 loss: 9.26736902e-07
Iter: 650 loss: 9.25164102e-07
Iter: 651 loss: 9.24019787e-07
Iter: 652 loss: 9.26156758e-07
Iter: 653 loss: 9.23533378e-07
Iter: 654 loss: 9.22367917e-07
Iter: 655 loss: 9.29904672e-07
Iter: 656 loss: 9.22251957e-07
Iter: 657 loss: 9.21357184e-07
Iter: 658 loss: 9.2345806e-07
Iter: 659 loss: 9.21012941e-07
Iter: 660 loss: 9.19723675e-07
Iter: 661 loss: 9.24443498e-07
Iter: 662 loss: 9.19464924e-07
Iter: 663 loss: 9.18560318e-07
Iter: 664 loss: 9.20385958e-07
Iter: 665 loss: 9.18242108e-07
Iter: 666 loss: 9.17205853e-07
Iter: 667 loss: 9.18561454e-07
Iter: 668 loss: 9.16719046e-07
Iter: 669 loss: 9.15567398e-07
Iter: 670 loss: 9.21288688e-07
Iter: 671 loss: 9.15313478e-07
Iter: 672 loss: 9.14575594e-07
Iter: 673 loss: 9.19118065e-07
Iter: 674 loss: 9.14465431e-07
Iter: 675 loss: 9.13815768e-07
Iter: 676 loss: 9.19389322e-07
Iter: 677 loss: 9.13788199e-07
Iter: 678 loss: 9.13204417e-07
Iter: 679 loss: 9.12724488e-07
Iter: 680 loss: 9.12541623e-07
Iter: 681 loss: 9.11824714e-07
Iter: 682 loss: 9.12648375e-07
Iter: 683 loss: 9.11394466e-07
Iter: 684 loss: 9.10480935e-07
Iter: 685 loss: 9.09518803e-07
Iter: 686 loss: 9.09345829e-07
Iter: 687 loss: 9.08277855e-07
Iter: 688 loss: 9.17323291e-07
Iter: 689 loss: 9.08176105e-07
Iter: 690 loss: 9.07066578e-07
Iter: 691 loss: 9.07811341e-07
Iter: 692 loss: 9.06386845e-07
Iter: 693 loss: 9.05376055e-07
Iter: 694 loss: 9.0536e-07
Iter: 695 loss: 9.04678e-07
Iter: 696 loss: 9.03907392e-07
Iter: 697 loss: 9.03809735e-07
Iter: 698 loss: 9.02580155e-07
Iter: 699 loss: 9.09558764e-07
Iter: 700 loss: 9.02413433e-07
Iter: 701 loss: 9.01680835e-07
Iter: 702 loss: 9.05141576e-07
Iter: 703 loss: 9.01499448e-07
Iter: 704 loss: 9.00806526e-07
Iter: 705 loss: 9.01926228e-07
Iter: 706 loss: 9.0045188e-07
Iter: 707 loss: 8.99650104e-07
Iter: 708 loss: 9.08361301e-07
Iter: 709 loss: 8.99653969e-07
Iter: 710 loss: 8.99019255e-07
Iter: 711 loss: 8.99141753e-07
Iter: 712 loss: 8.98494704e-07
Iter: 713 loss: 8.97813379e-07
Iter: 714 loss: 8.9772459e-07
Iter: 715 loss: 8.972454e-07
Iter: 716 loss: 8.96146048e-07
Iter: 717 loss: 8.96810491e-07
Iter: 718 loss: 8.95427263e-07
Iter: 719 loss: 8.94356617e-07
Iter: 720 loss: 8.95893436e-07
Iter: 721 loss: 8.93833771e-07
Iter: 722 loss: 8.92658477e-07
Iter: 723 loss: 8.9808475e-07
Iter: 724 loss: 8.92484422e-07
Iter: 725 loss: 8.91539628e-07
Iter: 726 loss: 9.00959606e-07
Iter: 727 loss: 8.9152411e-07
Iter: 728 loss: 8.90615524e-07
Iter: 729 loss: 8.90724664e-07
Iter: 730 loss: 8.89878947e-07
Iter: 731 loss: 8.88802788e-07
Iter: 732 loss: 8.93005222e-07
Iter: 733 loss: 8.88538466e-07
Iter: 734 loss: 8.87658302e-07
Iter: 735 loss: 8.92071967e-07
Iter: 736 loss: 8.8748493e-07
Iter: 737 loss: 8.86750172e-07
Iter: 738 loss: 8.87585202e-07
Iter: 739 loss: 8.86330952e-07
Iter: 740 loss: 8.85463407e-07
Iter: 741 loss: 8.94799712e-07
Iter: 742 loss: 8.85427539e-07
Iter: 743 loss: 8.84697556e-07
Iter: 744 loss: 8.86317935e-07
Iter: 745 loss: 8.84378323e-07
Iter: 746 loss: 8.83834389e-07
Iter: 747 loss: 8.83448479e-07
Iter: 748 loss: 8.83207917e-07
Iter: 749 loss: 8.82269546e-07
Iter: 750 loss: 8.83125153e-07
Iter: 751 loss: 8.81732092e-07
Iter: 752 loss: 8.80776042e-07
Iter: 753 loss: 8.83342807e-07
Iter: 754 loss: 8.80460732e-07
Iter: 755 loss: 8.79422714e-07
Iter: 756 loss: 8.78819833e-07
Iter: 757 loss: 8.78397032e-07
Iter: 758 loss: 8.77639081e-07
Iter: 759 loss: 8.77526418e-07
Iter: 760 loss: 8.76752665e-07
Iter: 761 loss: 8.77623393e-07
Iter: 762 loss: 8.76262902e-07
Iter: 763 loss: 8.75370688e-07
Iter: 764 loss: 8.78347e-07
Iter: 765 loss: 8.75154456e-07
Iter: 766 loss: 8.74611601e-07
Iter: 767 loss: 8.77519369e-07
Iter: 768 loss: 8.74501211e-07
Iter: 769 loss: 8.73868e-07
Iter: 770 loss: 8.73531462e-07
Iter: 771 loss: 8.73268959e-07
Iter: 772 loss: 8.72719738e-07
Iter: 773 loss: 8.72633734e-07
Iter: 774 loss: 8.72223723e-07
Iter: 775 loss: 8.73296642e-07
Iter: 776 loss: 8.72103897e-07
Iter: 777 loss: 8.7161277e-07
Iter: 778 loss: 8.70976805e-07
Iter: 779 loss: 8.70959241e-07
Iter: 780 loss: 8.70065e-07
Iter: 781 loss: 8.72398971e-07
Iter: 782 loss: 8.69776386e-07
Iter: 783 loss: 8.68971767e-07
Iter: 784 loss: 8.69263886e-07
Iter: 785 loss: 8.68418738e-07
Iter: 786 loss: 8.67189556e-07
Iter: 787 loss: 8.69498876e-07
Iter: 788 loss: 8.66717187e-07
Iter: 789 loss: 8.65941502e-07
Iter: 790 loss: 8.70288602e-07
Iter: 791 loss: 8.6584464e-07
Iter: 792 loss: 8.65169341e-07
Iter: 793 loss: 8.73486499e-07
Iter: 794 loss: 8.6514143e-07
Iter: 795 loss: 8.64695437e-07
Iter: 796 loss: 8.64528829e-07
Iter: 797 loss: 8.64286676e-07
Iter: 798 loss: 8.63547314e-07
Iter: 799 loss: 8.65196114e-07
Iter: 800 loss: 8.63252694e-07
Iter: 801 loss: 8.62382308e-07
Iter: 802 loss: 8.65527682e-07
Iter: 803 loss: 8.62151865e-07
Iter: 804 loss: 8.61523461e-07
Iter: 805 loss: 8.66313826e-07
Iter: 806 loss: 8.61505271e-07
Iter: 807 loss: 8.60844e-07
Iter: 808 loss: 8.62550678e-07
Iter: 809 loss: 8.60642103e-07
Iter: 810 loss: 8.59962938e-07
Iter: 811 loss: 8.60098908e-07
Iter: 812 loss: 8.59484715e-07
Iter: 813 loss: 8.58902126e-07
Iter: 814 loss: 8.6025085e-07
Iter: 815 loss: 8.58686235e-07
Iter: 816 loss: 8.58072042e-07
Iter: 817 loss: 8.57437215e-07
Iter: 818 loss: 8.57317559e-07
Iter: 819 loss: 8.5611e-07
Iter: 820 loss: 8.60554451e-07
Iter: 821 loss: 8.5584162e-07
Iter: 822 loss: 8.54789221e-07
Iter: 823 loss: 8.55874532e-07
Iter: 824 loss: 8.54237442e-07
Iter: 825 loss: 8.53726078e-07
Iter: 826 loss: 8.53632912e-07
Iter: 827 loss: 8.53054757e-07
Iter: 828 loss: 8.52417656e-07
Iter: 829 loss: 8.5231369e-07
Iter: 830 loss: 8.5138538e-07
Iter: 831 loss: 8.54867778e-07
Iter: 832 loss: 8.51176424e-07
Iter: 833 loss: 8.50324113e-07
Iter: 834 loss: 8.54666268e-07
Iter: 835 loss: 8.50207471e-07
Iter: 836 loss: 8.49495279e-07
Iter: 837 loss: 8.51966888e-07
Iter: 838 loss: 8.49363232e-07
Iter: 839 loss: 8.48751881e-07
Iter: 840 loss: 8.54349764e-07
Iter: 841 loss: 8.48670652e-07
Iter: 842 loss: 8.48241086e-07
Iter: 843 loss: 8.48188279e-07
Iter: 844 loss: 8.47846877e-07
Iter: 845 loss: 8.47311185e-07
Iter: 846 loss: 8.47883086e-07
Iter: 847 loss: 8.47021397e-07
Iter: 848 loss: 8.46385262e-07
Iter: 849 loss: 8.46410785e-07
Iter: 850 loss: 8.45857357e-07
Iter: 851 loss: 8.45015279e-07
Iter: 852 loss: 8.47525257e-07
Iter: 853 loss: 8.44758347e-07
Iter: 854 loss: 8.43848682e-07
Iter: 855 loss: 8.44091119e-07
Iter: 856 loss: 8.4322653e-07
Iter: 857 loss: 8.42442205e-07
Iter: 858 loss: 8.54641883e-07
Iter: 859 loss: 8.4244266e-07
Iter: 860 loss: 8.4200542e-07
Iter: 861 loss: 8.47827778e-07
Iter: 862 loss: 8.42015538e-07
Iter: 863 loss: 8.41709948e-07
Iter: 864 loss: 8.40895041e-07
Iter: 865 loss: 8.48736477e-07
Iter: 866 loss: 8.40816e-07
Iter: 867 loss: 8.40267035e-07
Iter: 868 loss: 8.40193e-07
Iter: 869 loss: 8.39756581e-07
Iter: 870 loss: 8.39457e-07
Iter: 871 loss: 8.39282734e-07
Iter: 872 loss: 8.38643359e-07
Iter: 873 loss: 8.38637447e-07
Iter: 874 loss: 8.38215612e-07
Iter: 875 loss: 8.38219648e-07
Iter: 876 loss: 8.37894959e-07
Iter: 877 loss: 8.37423954e-07
Iter: 878 loss: 8.37796165e-07
Iter: 879 loss: 8.37087441e-07
Iter: 880 loss: 8.36391791e-07
Iter: 881 loss: 8.36642926e-07
Iter: 882 loss: 8.3591442e-07
Iter: 883 loss: 8.35225705e-07
Iter: 884 loss: 8.38154961e-07
Iter: 885 loss: 8.3510173e-07
Iter: 886 loss: 8.34383854e-07
Iter: 887 loss: 8.34248453e-07
Iter: 888 loss: 8.33814568e-07
Iter: 889 loss: 8.32989258e-07
Iter: 890 loss: 8.38417861e-07
Iter: 891 loss: 8.32899673e-07
Iter: 892 loss: 8.32271724e-07
Iter: 893 loss: 8.37098469e-07
Iter: 894 loss: 8.32209878e-07
Iter: 895 loss: 8.31515933e-07
Iter: 896 loss: 8.31666398e-07
Iter: 897 loss: 8.31074544e-07
Iter: 898 loss: 8.30436534e-07
Iter: 899 loss: 8.31177e-07
Iter: 900 loss: 8.30064891e-07
Iter: 901 loss: 8.29153123e-07
Iter: 902 loss: 8.34378568e-07
Iter: 903 loss: 8.29025225e-07
Iter: 904 loss: 8.28621751e-07
Iter: 905 loss: 8.28605152e-07
Iter: 906 loss: 8.2829547e-07
Iter: 907 loss: 8.27963618e-07
Iter: 908 loss: 8.2785283e-07
Iter: 909 loss: 8.2727297e-07
Iter: 910 loss: 8.27941108e-07
Iter: 911 loss: 8.27008307e-07
Iter: 912 loss: 8.26375128e-07
Iter: 913 loss: 8.28193492e-07
Iter: 914 loss: 8.26138944e-07
Iter: 915 loss: 8.2566936e-07
Iter: 916 loss: 8.25592224e-07
Iter: 917 loss: 8.25287316e-07
Iter: 918 loss: 8.24417782e-07
Iter: 919 loss: 8.26302539e-07
Iter: 920 loss: 8.24064955e-07
Iter: 921 loss: 8.23295636e-07
Iter: 922 loss: 8.24022891e-07
Iter: 923 loss: 8.22805873e-07
Iter: 924 loss: 8.22087031e-07
Iter: 925 loss: 8.22079528e-07
Iter: 926 loss: 8.21541448e-07
Iter: 927 loss: 8.24134304e-07
Iter: 928 loss: 8.21423328e-07
Iter: 929 loss: 8.20999333e-07
Iter: 930 loss: 8.20380251e-07
Iter: 931 loss: 8.20391051e-07
Iter: 932 loss: 8.19763954e-07
Iter: 933 loss: 8.29290229e-07
Iter: 934 loss: 8.19763613e-07
Iter: 935 loss: 8.19334389e-07
Iter: 936 loss: 8.21042647e-07
Iter: 937 loss: 8.19219281e-07
Iter: 938 loss: 8.18729177e-07
Iter: 939 loss: 8.22488119e-07
Iter: 940 loss: 8.18702574e-07
Iter: 941 loss: 8.18448939e-07
Iter: 942 loss: 8.17963041e-07
Iter: 943 loss: 8.28208954e-07
Iter: 944 loss: 8.17956789e-07
Iter: 945 loss: 8.17291948e-07
Iter: 946 loss: 8.19951595e-07
Iter: 947 loss: 8.17117211e-07
Iter: 948 loss: 8.16527518e-07
Iter: 949 loss: 8.16752845e-07
Iter: 950 loss: 8.16187253e-07
Iter: 951 loss: 8.15481599e-07
Iter: 952 loss: 8.16985619e-07
Iter: 953 loss: 8.15168278e-07
Iter: 954 loss: 8.14435793e-07
Iter: 955 loss: 8.16772683e-07
Iter: 956 loss: 8.14214104e-07
Iter: 957 loss: 8.13537611e-07
Iter: 958 loss: 8.16487955e-07
Iter: 959 loss: 8.13369866e-07
Iter: 960 loss: 8.12947e-07
Iter: 961 loss: 8.19892875e-07
Iter: 962 loss: 8.12964231e-07
Iter: 963 loss: 8.12627491e-07
Iter: 964 loss: 8.12054395e-07
Iter: 965 loss: 8.23518349e-07
Iter: 966 loss: 8.12050359e-07
Iter: 967 loss: 8.11338623e-07
Iter: 968 loss: 8.1578321e-07
Iter: 969 loss: 8.11239147e-07
Iter: 970 loss: 8.10756831e-07
Iter: 971 loss: 8.16427587e-07
Iter: 972 loss: 8.1073506e-07
Iter: 973 loss: 8.10375468e-07
Iter: 974 loss: 8.12904489e-07
Iter: 975 loss: 8.10333631e-07
Iter: 976 loss: 8.10001e-07
Iter: 977 loss: 8.09387871e-07
Iter: 978 loss: 8.21456695e-07
Iter: 979 loss: 8.09387643e-07
Iter: 980 loss: 8.08811819e-07
Iter: 981 loss: 8.13057682e-07
Iter: 982 loss: 8.0878425e-07
Iter: 983 loss: 8.08254924e-07
Iter: 984 loss: 8.07790116e-07
Iter: 985 loss: 8.07648576e-07
Iter: 986 loss: 8.0698203e-07
Iter: 987 loss: 8.10959534e-07
Iter: 988 loss: 8.0690279e-07
Iter: 989 loss: 8.06380626e-07
Iter: 990 loss: 8.0707008e-07
Iter: 991 loss: 8.06152684e-07
Iter: 992 loss: 8.0541912e-07
Iter: 993 loss: 8.07028073e-07
Iter: 994 loss: 8.05101e-07
Iter: 995 loss: 8.04719662e-07
Iter: 996 loss: 8.04696924e-07
Iter: 997 loss: 8.04306524e-07
Iter: 998 loss: 8.03520152e-07
Iter: 999 loss: 8.19090133e-07
Iter: 1000 loss: 8.03512194e-07
Iter: 1001 loss: 8.02787326e-07
Iter: 1002 loss: 8.06966966e-07
Iter: 1003 loss: 8.02709451e-07
Iter: 1004 loss: 8.02019599e-07
Iter: 1005 loss: 8.07298477e-07
Iter: 1006 loss: 8.01987937e-07
Iter: 1007 loss: 8.01547515e-07
Iter: 1008 loss: 8.05787238e-07
Iter: 1009 loss: 8.01521082e-07
Iter: 1010 loss: 8.011001e-07
Iter: 1011 loss: 8.0120293e-07
Iter: 1012 loss: 8.00817e-07
Iter: 1013 loss: 8.00470502e-07
Iter: 1014 loss: 8.00504154e-07
Iter: 1015 loss: 8.00169573e-07
Iter: 1016 loss: 7.99619897e-07
Iter: 1017 loss: 8.00964244e-07
Iter: 1018 loss: 7.99431518e-07
Iter: 1019 loss: 7.98955455e-07
Iter: 1020 loss: 7.99433565e-07
Iter: 1021 loss: 7.98682152e-07
Iter: 1022 loss: 7.98180395e-07
Iter: 1023 loss: 7.98802375e-07
Iter: 1024 loss: 7.97916528e-07
Iter: 1025 loss: 7.97170912e-07
Iter: 1026 loss: 7.99516158e-07
Iter: 1027 loss: 7.96933875e-07
Iter: 1028 loss: 7.96412564e-07
Iter: 1029 loss: 8.00861926e-07
Iter: 1030 loss: 7.96357483e-07
Iter: 1031 loss: 7.95837764e-07
Iter: 1032 loss: 7.97052621e-07
Iter: 1033 loss: 7.95576e-07
Iter: 1034 loss: 7.95111e-07
Iter: 1035 loss: 7.94891264e-07
Iter: 1036 loss: 7.94663777e-07
Iter: 1037 loss: 7.94174639e-07
Iter: 1038 loss: 8.01173883e-07
Iter: 1039 loss: 7.94174071e-07
Iter: 1040 loss: 7.93744675e-07
Iter: 1041 loss: 7.95531378e-07
Iter: 1042 loss: 7.93673678e-07
Iter: 1043 loss: 7.93186246e-07
Iter: 1044 loss: 7.94042705e-07
Iter: 1045 loss: 7.92980359e-07
Iter: 1046 loss: 7.92578476e-07
Iter: 1047 loss: 7.92567e-07
Iter: 1048 loss: 7.92300398e-07
Iter: 1049 loss: 7.91864636e-07
Iter: 1050 loss: 7.93641334e-07
Iter: 1051 loss: 7.91717582e-07
Iter: 1052 loss: 7.9128381e-07
Iter: 1053 loss: 7.91397611e-07
Iter: 1054 loss: 7.90918307e-07
Iter: 1055 loss: 7.90401e-07
Iter: 1056 loss: 7.90626473e-07
Iter: 1057 loss: 7.90029844e-07
Iter: 1058 loss: 7.89431795e-07
Iter: 1059 loss: 7.95082542e-07
Iter: 1060 loss: 7.89416845e-07
Iter: 1061 loss: 7.8895448e-07
Iter: 1062 loss: 7.89423098e-07
Iter: 1063 loss: 7.88687089e-07
Iter: 1064 loss: 7.88099e-07
Iter: 1065 loss: 7.9356181e-07
Iter: 1066 loss: 7.88076704e-07
Iter: 1067 loss: 7.87683234e-07
Iter: 1068 loss: 7.87721206e-07
Iter: 1069 loss: 7.87411295e-07
Iter: 1070 loss: 7.86915336e-07
Iter: 1071 loss: 7.87536464e-07
Iter: 1072 loss: 7.86661928e-07
Iter: 1073 loss: 7.86256805e-07
Iter: 1074 loss: 7.8625294e-07
Iter: 1075 loss: 7.86012208e-07
Iter: 1076 loss: 7.87024419e-07
Iter: 1077 loss: 7.8590773e-07
Iter: 1078 loss: 7.85666202e-07
Iter: 1079 loss: 7.8518292e-07
Iter: 1080 loss: 7.93564141e-07
Iter: 1081 loss: 7.85182237e-07
Iter: 1082 loss: 7.84716462e-07
Iter: 1083 loss: 7.89740398e-07
Iter: 1084 loss: 7.84689689e-07
Iter: 1085 loss: 7.8437273e-07
Iter: 1086 loss: 7.84273766e-07
Iter: 1087 loss: 7.84082886e-07
Iter: 1088 loss: 7.83543555e-07
Iter: 1089 loss: 7.8329316e-07
Iter: 1090 loss: 7.83e-07
Iter: 1091 loss: 7.82436246e-07
Iter: 1092 loss: 7.89053615e-07
Iter: 1093 loss: 7.82417487e-07
Iter: 1094 loss: 7.81890606e-07
Iter: 1095 loss: 7.81687731e-07
Iter: 1096 loss: 7.81397944e-07
Iter: 1097 loss: 7.80948426e-07
Iter: 1098 loss: 7.80915911e-07
Iter: 1099 loss: 7.80621576e-07
Iter: 1100 loss: 7.8074288e-07
Iter: 1101 loss: 7.80402161e-07
Iter: 1102 loss: 7.80023356e-07
Iter: 1103 loss: 7.80062578e-07
Iter: 1104 loss: 7.79712082e-07
Iter: 1105 loss: 7.79451057e-07
Iter: 1106 loss: 7.79398e-07
Iter: 1107 loss: 7.791391e-07
Iter: 1108 loss: 7.79394668e-07
Iter: 1109 loss: 7.79034167e-07
Iter: 1110 loss: 7.78642629e-07
Iter: 1111 loss: 7.78493472e-07
Iter: 1112 loss: 7.78316121e-07
Iter: 1113 loss: 7.77944479e-07
Iter: 1114 loss: 7.78827371e-07
Iter: 1115 loss: 7.7783568e-07
Iter: 1116 loss: 7.77361095e-07
Iter: 1117 loss: 7.78035258e-07
Iter: 1118 loss: 7.77207219e-07
Iter: 1119 loss: 7.76621391e-07
Iter: 1120 loss: 7.76598426e-07
Iter: 1121 loss: 7.76225534e-07
Iter: 1122 loss: 7.75554213e-07
Iter: 1123 loss: 7.78171795e-07
Iter: 1124 loss: 7.75413525e-07
Iter: 1125 loss: 7.74830767e-07
Iter: 1126 loss: 7.76581032e-07
Iter: 1127 loss: 7.74633577e-07
Iter: 1128 loss: 7.74184343e-07
Iter: 1129 loss: 7.79637162e-07
Iter: 1130 loss: 7.74140915e-07
Iter: 1131 loss: 7.73810712e-07
Iter: 1132 loss: 7.75005162e-07
Iter: 1133 loss: 7.73716351e-07
Iter: 1134 loss: 7.73386e-07
Iter: 1135 loss: 7.73091529e-07
Iter: 1136 loss: 7.73008651e-07
Iter: 1137 loss: 7.72663e-07
Iter: 1138 loss: 7.72629619e-07
Iter: 1139 loss: 7.72334829e-07
Iter: 1140 loss: 7.73630234e-07
Iter: 1141 loss: 7.72257692e-07
Iter: 1142 loss: 7.71959492e-07
Iter: 1143 loss: 7.71987629e-07
Iter: 1144 loss: 7.71718874e-07
Iter: 1145 loss: 7.71379064e-07
Iter: 1146 loss: 7.71297721e-07
Iter: 1147 loss: 7.71015493e-07
Iter: 1148 loss: 7.70631232e-07
Iter: 1149 loss: 7.73706e-07
Iter: 1150 loss: 7.70620431e-07
Iter: 1151 loss: 7.70151e-07
Iter: 1152 loss: 7.69703036e-07
Iter: 1153 loss: 7.69616236e-07
Iter: 1154 loss: 7.69017731e-07
Iter: 1155 loss: 7.70792894e-07
Iter: 1156 loss: 7.68833956e-07
Iter: 1157 loss: 7.68195e-07
Iter: 1158 loss: 7.70302336e-07
Iter: 1159 loss: 7.67987785e-07
Iter: 1160 loss: 7.6754668e-07
Iter: 1161 loss: 7.72095632e-07
Iter: 1162 loss: 7.67568338e-07
Iter: 1163 loss: 7.67181746e-07
Iter: 1164 loss: 7.68163829e-07
Iter: 1165 loss: 7.67021504e-07
Iter: 1166 loss: 7.66619223e-07
Iter: 1167 loss: 7.67073061e-07
Iter: 1168 loss: 7.66394805e-07
Iter: 1169 loss: 7.66060566e-07
Iter: 1170 loss: 7.6789911e-07
Iter: 1171 loss: 7.66014523e-07
Iter: 1172 loss: 7.6566e-07
Iter: 1173 loss: 7.67636038e-07
Iter: 1174 loss: 7.6558274e-07
Iter: 1175 loss: 7.65297727e-07
Iter: 1176 loss: 7.65559037e-07
Iter: 1177 loss: 7.65108439e-07
Iter: 1178 loss: 7.64791707e-07
Iter: 1179 loss: 7.65131915e-07
Iter: 1180 loss: 7.6466506e-07
Iter: 1181 loss: 7.6433605e-07
Iter: 1182 loss: 7.64552965e-07
Iter: 1183 loss: 7.6412789e-07
Iter: 1184 loss: 7.63691673e-07
Iter: 1185 loss: 7.65950801e-07
Iter: 1186 loss: 7.63606181e-07
Iter: 1187 loss: 7.63293258e-07
Iter: 1188 loss: 7.62724767e-07
Iter: 1189 loss: 7.62743298e-07
Iter: 1190 loss: 7.6212865e-07
Iter: 1191 loss: 7.67415031e-07
Iter: 1192 loss: 7.62073739e-07
Iter: 1193 loss: 7.61674642e-07
Iter: 1194 loss: 7.63660637e-07
Iter: 1195 loss: 7.61594265e-07
Iter: 1196 loss: 7.61197214e-07
Iter: 1197 loss: 7.63933031e-07
Iter: 1198 loss: 7.61179933e-07
Iter: 1199 loss: 7.60898786e-07
Iter: 1200 loss: 7.61172828e-07
Iter: 1201 loss: 7.60733201e-07
Iter: 1202 loss: 7.60361331e-07
Iter: 1203 loss: 7.6047121e-07
Iter: 1204 loss: 7.6010474e-07
Iter: 1205 loss: 7.59828254e-07
Iter: 1206 loss: 7.59791192e-07
Iter: 1207 loss: 7.59557281e-07
Iter: 1208 loss: 7.59435636e-07
Iter: 1209 loss: 7.59326667e-07
Iter: 1210 loss: 7.58935471e-07
Iter: 1211 loss: 7.59103216e-07
Iter: 1212 loss: 7.58671206e-07
Iter: 1213 loss: 7.58235217e-07
Iter: 1214 loss: 7.58490103e-07
Iter: 1215 loss: 7.57943667e-07
Iter: 1216 loss: 7.57448106e-07
Iter: 1217 loss: 7.61121555e-07
Iter: 1218 loss: 7.57392172e-07
Iter: 1219 loss: 7.56990801e-07
Iter: 1220 loss: 7.57028374e-07
Iter: 1221 loss: 7.56661734e-07
Iter: 1222 loss: 7.56149575e-07
Iter: 1223 loss: 7.56410202e-07
Iter: 1224 loss: 7.55793167e-07
Iter: 1225 loss: 7.5533012e-07
Iter: 1226 loss: 7.59257659e-07
Iter: 1227 loss: 7.55318e-07
Iter: 1228 loss: 7.54968767e-07
Iter: 1229 loss: 7.57573673e-07
Iter: 1230 loss: 7.54901578e-07
Iter: 1231 loss: 7.54590474e-07
Iter: 1232 loss: 7.55469e-07
Iter: 1233 loss: 7.54492646e-07
Iter: 1234 loss: 7.541679e-07
Iter: 1235 loss: 7.53764198e-07
Iter: 1236 loss: 7.53737766e-07
Iter: 1237 loss: 7.53480208e-07
Iter: 1238 loss: 7.53405857e-07
Iter: 1239 loss: 7.53112715e-07
Iter: 1240 loss: 7.53532163e-07
Iter: 1241 loss: 7.52980327e-07
Iter: 1242 loss: 7.527359e-07
Iter: 1243 loss: 7.52890628e-07
Iter: 1244 loss: 7.52589244e-07
Iter: 1245 loss: 7.52257506e-07
Iter: 1246 loss: 7.51898369e-07
Iter: 1247 loss: 7.51822e-07
Iter: 1248 loss: 7.51486368e-07
Iter: 1249 loss: 7.5148705e-07
Iter: 1250 loss: 7.51161394e-07
Iter: 1251 loss: 7.50978245e-07
Iter: 1252 loss: 7.50843071e-07
Iter: 1253 loss: 7.5038588e-07
Iter: 1254 loss: 7.5124683e-07
Iter: 1255 loss: 7.50159927e-07
Iter: 1256 loss: 7.49740593e-07
Iter: 1257 loss: 7.50402364e-07
Iter: 1258 loss: 7.49558239e-07
Iter: 1259 loss: 7.49219907e-07
Iter: 1260 loss: 7.54210078e-07
Iter: 1261 loss: 7.49233095e-07
Iter: 1262 loss: 7.48922389e-07
Iter: 1263 loss: 7.49371793e-07
Iter: 1264 loss: 7.48799948e-07
Iter: 1265 loss: 7.48398861e-07
Iter: 1266 loss: 7.48692514e-07
Iter: 1267 loss: 7.48196726e-07
Iter: 1268 loss: 7.47963895e-07
Iter: 1269 loss: 7.51367736e-07
Iter: 1270 loss: 7.47962758e-07
Iter: 1271 loss: 7.47663364e-07
Iter: 1272 loss: 7.48742593e-07
Iter: 1273 loss: 7.47604076e-07
Iter: 1274 loss: 7.47373235e-07
Iter: 1275 loss: 7.47087654e-07
Iter: 1276 loss: 7.47074409e-07
Iter: 1277 loss: 7.46592718e-07
Iter: 1278 loss: 7.47984132e-07
Iter: 1279 loss: 7.46438559e-07
Iter: 1280 loss: 7.46098863e-07
Iter: 1281 loss: 7.46005696e-07
Iter: 1282 loss: 7.45768148e-07
Iter: 1283 loss: 7.45211651e-07
Iter: 1284 loss: 7.49886851e-07
Iter: 1285 loss: 7.45217108e-07
Iter: 1286 loss: 7.4488247e-07
Iter: 1287 loss: 7.44896909e-07
Iter: 1288 loss: 7.44640147e-07
Iter: 1289 loss: 7.44242e-07
Iter: 1290 loss: 7.45040893e-07
Iter: 1291 loss: 7.44080808e-07
Iter: 1292 loss: 7.43649423e-07
Iter: 1293 loss: 7.45141847e-07
Iter: 1294 loss: 7.43573878e-07
Iter: 1295 loss: 7.43244186e-07
Iter: 1296 loss: 7.4705207e-07
Iter: 1297 loss: 7.43238161e-07
Iter: 1298 loss: 7.42953262e-07
Iter: 1299 loss: 7.42782561e-07
Iter: 1300 loss: 7.42644e-07
Iter: 1301 loss: 7.42286147e-07
Iter: 1302 loss: 7.44629e-07
Iter: 1303 loss: 7.42275574e-07
Iter: 1304 loss: 7.42013071e-07
Iter: 1305 loss: 7.41995848e-07
Iter: 1306 loss: 7.41816848e-07
Iter: 1307 loss: 7.4146385e-07
Iter: 1308 loss: 7.47259605e-07
Iter: 1309 loss: 7.41458052e-07
Iter: 1310 loss: 7.41056169e-07
Iter: 1311 loss: 7.4380182e-07
Iter: 1312 loss: 7.40990686e-07
Iter: 1313 loss: 7.407059e-07
Iter: 1314 loss: 7.40639223e-07
Iter: 1315 loss: 7.40474263e-07
Iter: 1316 loss: 7.40084545e-07
Iter: 1317 loss: 7.42046041e-07
Iter: 1318 loss: 7.39963525e-07
Iter: 1319 loss: 7.3965515e-07
Iter: 1320 loss: 7.41927067e-07
Iter: 1321 loss: 7.39609106e-07
Iter: 1322 loss: 7.39461029e-07
Iter: 1323 loss: 7.3899e-07
Iter: 1324 loss: 7.45130933e-07
Iter: 1325 loss: 7.38952167e-07
Iter: 1326 loss: 7.38441486e-07
Iter: 1327 loss: 7.42864586e-07
Iter: 1328 loss: 7.38418919e-07
Iter: 1329 loss: 7.38039262e-07
Iter: 1330 loss: 7.39578354e-07
Iter: 1331 loss: 7.37956725e-07
Iter: 1332 loss: 7.37516132e-07
Iter: 1333 loss: 7.39481379e-07
Iter: 1334 loss: 7.37413757e-07
Iter: 1335 loss: 7.37134769e-07
Iter: 1336 loss: 7.37060816e-07
Iter: 1337 loss: 7.36909669e-07
Iter: 1338 loss: 7.36666721e-07
Iter: 1339 loss: 7.36601692e-07
Iter: 1340 loss: 7.36414336e-07
Iter: 1341 loss: 7.36278821e-07
Iter: 1342 loss: 7.3617332e-07
Iter: 1343 loss: 7.35918206e-07
Iter: 1344 loss: 7.36345442e-07
Iter: 1345 loss: 7.3581441e-07
Iter: 1346 loss: 7.35450726e-07
Iter: 1347 loss: 7.3561614e-07
Iter: 1348 loss: 7.35206186e-07
Iter: 1349 loss: 7.34776791e-07
Iter: 1350 loss: 7.36203219e-07
Iter: 1351 loss: 7.34690218e-07
Iter: 1352 loss: 7.3438332e-07
Iter: 1353 loss: 7.36100105e-07
Iter: 1354 loss: 7.34343701e-07
Iter: 1355 loss: 7.34017249e-07
Iter: 1356 loss: 7.33657032e-07
Iter: 1357 loss: 7.33617412e-07
Iter: 1358 loss: 7.3312674e-07
Iter: 1359 loss: 7.34117521e-07
Iter: 1360 loss: 7.3294018e-07
Iter: 1361 loss: 7.32585249e-07
Iter: 1362 loss: 7.37442917e-07
Iter: 1363 loss: 7.32590593e-07
Iter: 1364 loss: 7.32321723e-07
Iter: 1365 loss: 7.34097398e-07
Iter: 1366 loss: 7.32287731e-07
Iter: 1367 loss: 7.32083151e-07
Iter: 1368 loss: 7.31837702e-07
Iter: 1369 loss: 7.31779949e-07
Iter: 1370 loss: 7.31565592e-07
Iter: 1371 loss: 7.31562181e-07
Iter: 1372 loss: 7.31327191e-07
Iter: 1373 loss: 7.31455145e-07
Iter: 1374 loss: 7.31173941e-07
Iter: 1375 loss: 7.30884892e-07
Iter: 1376 loss: 7.30861302e-07
Iter: 1377 loss: 7.30666e-07
Iter: 1378 loss: 7.30295e-07
Iter: 1379 loss: 7.31786713e-07
Iter: 1380 loss: 7.30205443e-07
Iter: 1381 loss: 7.29826866e-07
Iter: 1382 loss: 7.30405191e-07
Iter: 1383 loss: 7.29629448e-07
Iter: 1384 loss: 7.29322096e-07
Iter: 1385 loss: 7.2995e-07
Iter: 1386 loss: 7.29155e-07
Iter: 1387 loss: 7.28761222e-07
Iter: 1388 loss: 7.29987448e-07
Iter: 1389 loss: 7.28611155e-07
Iter: 1390 loss: 7.28284135e-07
Iter: 1391 loss: 7.28411123e-07
Iter: 1392 loss: 7.28084274e-07
Iter: 1393 loss: 7.27580868e-07
Iter: 1394 loss: 7.2837463e-07
Iter: 1395 loss: 7.27367592e-07
Iter: 1396 loss: 7.27173585e-07
Iter: 1397 loss: 7.2711083e-07
Iter: 1398 loss: 7.26925e-07
Iter: 1399 loss: 7.26686721e-07
Iter: 1400 loss: 7.26636074e-07
Iter: 1401 loss: 7.26303313e-07
Iter: 1402 loss: 7.28365421e-07
Iter: 1403 loss: 7.26258804e-07
Iter: 1404 loss: 7.25968221e-07
Iter: 1405 loss: 7.30128704e-07
Iter: 1406 loss: 7.25974473e-07
Iter: 1407 loss: 7.25871587e-07
Iter: 1408 loss: 7.2561636e-07
Iter: 1409 loss: 7.29528779e-07
Iter: 1410 loss: 7.25637165e-07
Iter: 1411 loss: 7.25300538e-07
Iter: 1412 loss: 7.2656286e-07
Iter: 1413 loss: 7.25241421e-07
Iter: 1414 loss: 7.24963797e-07
Iter: 1415 loss: 7.25588222e-07
Iter: 1416 loss: 7.24828112e-07
Iter: 1417 loss: 7.24534516e-07
Iter: 1418 loss: 7.24209258e-07
Iter: 1419 loss: 7.24127858e-07
Iter: 1420 loss: 7.23645201e-07
Iter: 1421 loss: 7.30181512e-07
Iter: 1422 loss: 7.23653e-07
Iter: 1423 loss: 7.23400603e-07
Iter: 1424 loss: 7.23347284e-07
Iter: 1425 loss: 7.2315197e-07
Iter: 1426 loss: 7.22740253e-07
Iter: 1427 loss: 7.23041808e-07
Iter: 1428 loss: 7.22507593e-07
Iter: 1429 loss: 7.2214641e-07
Iter: 1430 loss: 7.26289613e-07
Iter: 1431 loss: 7.22118386e-07
Iter: 1432 loss: 7.21733556e-07
Iter: 1433 loss: 7.22591494e-07
Iter: 1434 loss: 7.2158025e-07
Iter: 1435 loss: 7.21349295e-07
Iter: 1436 loss: 7.22128846e-07
Iter: 1437 loss: 7.21240383e-07
Iter: 1438 loss: 7.21047172e-07
Iter: 1439 loss: 7.210582e-07
Iter: 1440 loss: 7.20896196e-07
Iter: 1441 loss: 7.20584922e-07
Iter: 1442 loss: 7.24404686e-07
Iter: 1443 loss: 7.20579692e-07
Iter: 1444 loss: 7.20283651e-07
Iter: 1445 loss: 7.22682387e-07
Iter: 1446 loss: 7.20280639e-07
Iter: 1447 loss: 7.19951913e-07
Iter: 1448 loss: 7.19914055e-07
Iter: 1449 loss: 7.19724312e-07
Iter: 1450 loss: 7.19233299e-07
Iter: 1451 loss: 7.20255116e-07
Iter: 1452 loss: 7.19049297e-07
Iter: 1453 loss: 7.18695e-07
Iter: 1454 loss: 7.21231345e-07
Iter: 1455 loss: 7.18695e-07
Iter: 1456 loss: 7.18370416e-07
Iter: 1457 loss: 7.183923e-07
Iter: 1458 loss: 7.1812741e-07
Iter: 1459 loss: 7.1780704e-07
Iter: 1460 loss: 7.18540832e-07
Iter: 1461 loss: 7.17684657e-07
Iter: 1462 loss: 7.17314663e-07
Iter: 1463 loss: 7.17637e-07
Iter: 1464 loss: 7.17059663e-07
Iter: 1465 loss: 7.16722184e-07
Iter: 1466 loss: 7.16689726e-07
Iter: 1467 loss: 7.16536135e-07
Iter: 1468 loss: 7.16328202e-07
Iter: 1469 loss: 7.16322575e-07
Iter: 1470 loss: 7.16093211e-07
Iter: 1471 loss: 7.16101738e-07
Iter: 1472 loss: 7.15872034e-07
Iter: 1473 loss: 7.15675583e-07
Iter: 1474 loss: 7.15623685e-07
Iter: 1475 loss: 7.15409101e-07
Iter: 1476 loss: 7.15358283e-07
Iter: 1477 loss: 7.15258864e-07
Iter: 1478 loss: 7.1478172e-07
Iter: 1479 loss: 7.15624e-07
Iter: 1480 loss: 7.14607154e-07
Iter: 1481 loss: 7.14190378e-07
Iter: 1482 loss: 7.15634201e-07
Iter: 1483 loss: 7.1410841e-07
Iter: 1484 loss: 7.1369e-07
Iter: 1485 loss: 7.13909628e-07
Iter: 1486 loss: 7.13498423e-07
Iter: 1487 loss: 7.13061183e-07
Iter: 1488 loss: 7.17209275e-07
Iter: 1489 loss: 7.13087729e-07
Iter: 1490 loss: 7.12802716e-07
Iter: 1491 loss: 7.12593192e-07
Iter: 1492 loss: 7.1255954e-07
Iter: 1493 loss: 7.12127e-07
Iter: 1494 loss: 7.13423e-07
Iter: 1495 loss: 7.1199247e-07
Iter: 1496 loss: 7.1179e-07
Iter: 1497 loss: 7.11755092e-07
Iter: 1498 loss: 7.11589848e-07
Iter: 1499 loss: 7.11211328e-07
Iter: 1500 loss: 7.19495e-07
Iter: 1501 loss: 7.11206837e-07
Iter: 1502 loss: 7.10981567e-07
Iter: 1503 loss: 7.10978895e-07
Iter: 1504 loss: 7.10706e-07
Iter: 1505 loss: 7.11420284e-07
Iter: 1506 loss: 7.10622317e-07
Iter: 1507 loss: 7.10437575e-07
Iter: 1508 loss: 7.10042741e-07
Iter: 1509 loss: 7.15921942e-07
Iter: 1510 loss: 7.10031827e-07
Iter: 1511 loss: 7.09683491e-07
Iter: 1512 loss: 7.15204578e-07
Iter: 1513 loss: 7.09688265e-07
Iter: 1514 loss: 7.09423887e-07
Iter: 1515 loss: 7.09387791e-07
Iter: 1516 loss: 7.0917929e-07
Iter: 1517 loss: 7.08793436e-07
Iter: 1518 loss: 7.09993913e-07
Iter: 1519 loss: 7.08684297e-07
Iter: 1520 loss: 7.08369271e-07
Iter: 1521 loss: 7.1067825e-07
Iter: 1522 loss: 7.08381094e-07
Iter: 1523 loss: 7.08101311e-07
Iter: 1524 loss: 7.08062601e-07
Iter: 1525 loss: 7.07901336e-07
Iter: 1526 loss: 7.07557e-07
Iter: 1527 loss: 7.08794801e-07
Iter: 1528 loss: 7.0750832e-07
Iter: 1529 loss: 7.07201593e-07
Iter: 1530 loss: 7.08101481e-07
Iter: 1531 loss: 7.07137133e-07
Iter: 1532 loss: 7.06816763e-07
Iter: 1533 loss: 7.08751827e-07
Iter: 1534 loss: 7.06782942e-07
Iter: 1535 loss: 7.06565061e-07
Iter: 1536 loss: 7.06668629e-07
Iter: 1537 loss: 7.0644569e-07
Iter: 1538 loss: 7.06205e-07
Iter: 1539 loss: 7.09537176e-07
Iter: 1540 loss: 7.06221e-07
Iter: 1541 loss: 7.06086439e-07
Iter: 1542 loss: 7.05697062e-07
Iter: 1543 loss: 7.09423318e-07
Iter: 1544 loss: 7.05642492e-07
Iter: 1545 loss: 7.05235379e-07
Iter: 1546 loss: 7.08238474e-07
Iter: 1547 loss: 7.05194338e-07
Iter: 1548 loss: 7.04880335e-07
Iter: 1549 loss: 7.05968773e-07
Iter: 1550 loss: 7.04769604e-07
Iter: 1551 loss: 7.04502952e-07
Iter: 1552 loss: 7.046267e-07
Iter: 1553 loss: 7.04327363e-07
Iter: 1554 loss: 7.04019271e-07
Iter: 1555 loss: 7.06236051e-07
Iter: 1556 loss: 7.03974592e-07
Iter: 1557 loss: 7.03704245e-07
Iter: 1558 loss: 7.03998296e-07
Iter: 1559 loss: 7.03596129e-07
Iter: 1560 loss: 7.03260412e-07
Iter: 1561 loss: 7.03628075e-07
Iter: 1562 loss: 7.03097783e-07
Iter: 1563 loss: 7.02736031e-07
Iter: 1564 loss: 7.03205842e-07
Iter: 1565 loss: 7.02592956e-07
Iter: 1566 loss: 7.02247121e-07
Iter: 1567 loss: 7.07199774e-07
Iter: 1568 loss: 7.02236775e-07
Iter: 1569 loss: 7.02002865e-07
Iter: 1570 loss: 7.02235639e-07
Iter: 1571 loss: 7.01849103e-07
Iter: 1572 loss: 7.01719671e-07
Iter: 1573 loss: 7.01690794e-07
Iter: 1574 loss: 7.0155761e-07
Iter: 1575 loss: 7.01201884e-07
Iter: 1576 loss: 7.05041e-07
Iter: 1577 loss: 7.01148565e-07
Iter: 1578 loss: 7.00865144e-07
Iter: 1579 loss: 7.02648e-07
Iter: 1580 loss: 7.0083729e-07
Iter: 1581 loss: 7.00577061e-07
Iter: 1582 loss: 7.0150395e-07
Iter: 1583 loss: 7.00527607e-07
Iter: 1584 loss: 7.0030103e-07
Iter: 1585 loss: 7.00503392e-07
Iter: 1586 loss: 7.00161536e-07
Iter: 1587 loss: 6.99913926e-07
Iter: 1588 loss: 7.00304042e-07
Iter: 1589 loss: 6.99794327e-07
Iter: 1590 loss: 6.9945736e-07
Iter: 1591 loss: 7.0042006e-07
Iter: 1592 loss: 6.99329e-07
Iter: 1593 loss: 6.99068e-07
Iter: 1594 loss: 7.00051146e-07
Iter: 1595 loss: 6.99006762e-07
Iter: 1596 loss: 6.98725785e-07
Iter: 1597 loss: 6.98615054e-07
Iter: 1598 loss: 6.98485053e-07
Iter: 1599 loss: 6.98274448e-07
Iter: 1600 loss: 6.98255292e-07
Iter: 1601 loss: 6.98068675e-07
Iter: 1602 loss: 6.98425424e-07
Iter: 1603 loss: 6.97985399e-07
Iter: 1604 loss: 6.97807536e-07
Iter: 1605 loss: 6.98744657e-07
Iter: 1606 loss: 6.9774444e-07
Iter: 1607 loss: 6.97481369e-07
Iter: 1608 loss: 6.97729547e-07
Iter: 1609 loss: 6.97362225e-07
Iter: 1610 loss: 6.97211476e-07
Iter: 1611 loss: 6.97011274e-07
Iter: 1612 loss: 6.97014855e-07
Iter: 1613 loss: 6.96711311e-07
Iter: 1614 loss: 6.98173494e-07
Iter: 1615 loss: 6.96664586e-07
Iter: 1616 loss: 6.96418226e-07
Iter: 1617 loss: 6.97508767e-07
Iter: 1618 loss: 6.96357461e-07
Iter: 1619 loss: 6.96148163e-07
Iter: 1620 loss: 6.95964445e-07
Iter: 1621 loss: 6.95935114e-07
Iter: 1622 loss: 6.95559493e-07
Iter: 1623 loss: 6.98362044e-07
Iter: 1624 loss: 6.95522886e-07
Iter: 1625 loss: 6.95294545e-07
Iter: 1626 loss: 6.95849678e-07
Iter: 1627 loss: 6.95206836e-07
Iter: 1628 loss: 6.94999358e-07
Iter: 1629 loss: 6.95185122e-07
Iter: 1630 loss: 6.94826895e-07
Iter: 1631 loss: 6.94541598e-07
Iter: 1632 loss: 6.9540431e-07
Iter: 1633 loss: 6.9447276e-07
Iter: 1634 loss: 6.94157109e-07
Iter: 1635 loss: 6.97051632e-07
Iter: 1636 loss: 6.9412954e-07
Iter: 1637 loss: 6.93960146e-07
Iter: 1638 loss: 6.94309392e-07
Iter: 1639 loss: 6.93909101e-07
Iter: 1640 loss: 6.93700827e-07
Iter: 1641 loss: 6.95426e-07
Iter: 1642 loss: 6.93673314e-07
Iter: 1643 loss: 6.93580319e-07
Iter: 1644 loss: 6.9326245e-07
Iter: 1645 loss: 6.96702045e-07
Iter: 1646 loss: 6.93245738e-07
Iter: 1647 loss: 6.92967888e-07
Iter: 1648 loss: 6.95238384e-07
Iter: 1649 loss: 6.92929575e-07
Iter: 1650 loss: 6.92719709e-07
Iter: 1651 loss: 6.9326029e-07
Iter: 1652 loss: 6.92637343e-07
Iter: 1653 loss: 6.92425317e-07
Iter: 1654 loss: 6.92588287e-07
Iter: 1655 loss: 6.9230407e-07
Iter: 1656 loss: 6.92033666e-07
Iter: 1657 loss: 6.92580954e-07
Iter: 1658 loss: 6.91936862e-07
Iter: 1659 loss: 6.91608136e-07
Iter: 1660 loss: 6.93480558e-07
Iter: 1661 loss: 6.91595e-07
Iter: 1662 loss: 6.91335e-07
Iter: 1663 loss: 6.90950628e-07
Iter: 1664 loss: 6.90935565e-07
Iter: 1665 loss: 6.90501281e-07
Iter: 1666 loss: 6.95402093e-07
Iter: 1667 loss: 6.90490594e-07
Iter: 1668 loss: 6.90226443e-07
Iter: 1669 loss: 6.92247227e-07
Iter: 1670 loss: 6.90198476e-07
Iter: 1671 loss: 6.89927e-07
Iter: 1672 loss: 6.89925173e-07
Iter: 1673 loss: 6.89735e-07
Iter: 1674 loss: 6.89574961e-07
Iter: 1675 loss: 6.89545914e-07
Iter: 1676 loss: 6.89448029e-07
Iter: 1677 loss: 6.89259195e-07
Iter: 1678 loss: 6.92088065e-07
Iter: 1679 loss: 6.89249418e-07
Iter: 1680 loss: 6.89013518e-07
Iter: 1681 loss: 6.8936896e-07
Iter: 1682 loss: 6.8889841e-07
Iter: 1683 loss: 6.88673595e-07
Iter: 1684 loss: 6.90732122e-07
Iter: 1685 loss: 6.88633691e-07
Iter: 1686 loss: 6.88477826e-07
Iter: 1687 loss: 6.88265288e-07
Iter: 1688 loss: 6.88261935e-07
Iter: 1689 loss: 6.87960437e-07
Iter: 1690 loss: 6.89656247e-07
Iter: 1691 loss: 6.87896886e-07
Iter: 1692 loss: 6.87721695e-07
Iter: 1693 loss: 6.88526313e-07
Iter: 1694 loss: 6.87706461e-07
Iter: 1695 loss: 6.87418037e-07
Iter: 1696 loss: 6.87279396e-07
Iter: 1697 loss: 6.87193676e-07
Iter: 1698 loss: 6.8689576e-07
Iter: 1699 loss: 6.89857075e-07
Iter: 1700 loss: 6.8690224e-07
Iter: 1701 loss: 6.86664123e-07
Iter: 1702 loss: 6.87140471e-07
Iter: 1703 loss: 6.86580051e-07
Iter: 1704 loss: 6.86314365e-07
Iter: 1705 loss: 6.87515e-07
Iter: 1706 loss: 6.862698e-07
Iter: 1707 loss: 6.86130932e-07
Iter: 1708 loss: 6.86145881e-07
Iter: 1709 loss: 6.86046235e-07
Iter: 1710 loss: 6.8580357e-07
Iter: 1711 loss: 6.89268234e-07
Iter: 1712 loss: 6.85791065e-07
Iter: 1713 loss: 6.85566249e-07
Iter: 1714 loss: 6.86147928e-07
Iter: 1715 loss: 6.85476266e-07
Iter: 1716 loss: 6.85216492e-07
Iter: 1717 loss: 6.86717499e-07
Iter: 1718 loss: 6.85239172e-07
Iter: 1719 loss: 6.84999122e-07
Iter: 1720 loss: 6.84868041e-07
Iter: 1721 loss: 6.84773113e-07
Iter: 1722 loss: 6.84469967e-07
Iter: 1723 loss: 6.85995701e-07
Iter: 1724 loss: 6.84456609e-07
Iter: 1725 loss: 6.84228e-07
Iter: 1726 loss: 6.84957513e-07
Iter: 1727 loss: 6.84165173e-07
Iter: 1728 loss: 6.83910798e-07
Iter: 1729 loss: 6.84250324e-07
Iter: 1730 loss: 6.83800238e-07
Iter: 1731 loss: 6.83512212e-07
Iter: 1732 loss: 6.84062172e-07
Iter: 1733 loss: 6.83420581e-07
Iter: 1734 loss: 6.83130679e-07
Iter: 1735 loss: 6.84619692e-07
Iter: 1736 loss: 6.8310726e-07
Iter: 1737 loss: 6.82883069e-07
Iter: 1738 loss: 6.84015788e-07
Iter: 1739 loss: 6.82814e-07
Iter: 1740 loss: 6.82630571e-07
Iter: 1741 loss: 6.83946269e-07
Iter: 1742 loss: 6.82614086e-07
Iter: 1743 loss: 6.8242116e-07
Iter: 1744 loss: 6.82589189e-07
Iter: 1745 loss: 6.82305256e-07
Iter: 1746 loss: 6.82175539e-07
Iter: 1747 loss: 6.81938e-07
Iter: 1748 loss: 6.81933557e-07
Iter: 1749 loss: 6.81673214e-07
Iter: 1750 loss: 6.84304268e-07
Iter: 1751 loss: 6.81666279e-07
Iter: 1752 loss: 6.81447375e-07
Iter: 1753 loss: 6.81754955e-07
Iter: 1754 loss: 6.81350116e-07
Iter: 1755 loss: 6.8107073e-07
Iter: 1756 loss: 6.8102338e-07
Iter: 1757 loss: 6.80852679e-07
Iter: 1758 loss: 6.80587505e-07
Iter: 1759 loss: 6.82984307e-07
Iter: 1760 loss: 6.80568405e-07
Iter: 1761 loss: 6.80314201e-07
Iter: 1762 loss: 6.80488938e-07
Iter: 1763 loss: 6.8014856e-07
Iter: 1764 loss: 6.79815344e-07
Iter: 1765 loss: 6.81178e-07
Iter: 1766 loss: 6.79753271e-07
Iter: 1767 loss: 6.79510833e-07
Iter: 1768 loss: 6.80032372e-07
Iter: 1769 loss: 6.79386062e-07
Iter: 1770 loss: 6.7913777e-07
Iter: 1771 loss: 6.81386098e-07
Iter: 1772 loss: 6.79115715e-07
Iter: 1773 loss: 6.78955303e-07
Iter: 1774 loss: 6.80188236e-07
Iter: 1775 loss: 6.78957917e-07
Iter: 1776 loss: 6.78794777e-07
Iter: 1777 loss: 6.791891e-07
Iter: 1778 loss: 6.78747938e-07
Iter: 1779 loss: 6.78593437e-07
Iter: 1780 loss: 6.78270965e-07
Iter: 1781 loss: 6.82174971e-07
Iter: 1782 loss: 6.78272045e-07
Iter: 1783 loss: 6.78044e-07
Iter: 1784 loss: 6.78037338e-07
Iter: 1785 loss: 6.77880848e-07
Iter: 1786 loss: 6.78393121e-07
Iter: 1787 loss: 6.77822413e-07
Iter: 1788 loss: 6.77629259e-07
Iter: 1789 loss: 6.77559569e-07
Iter: 1790 loss: 6.77446e-07
Iter: 1791 loss: 6.77184914e-07
Iter: 1792 loss: 6.77917853e-07
Iter: 1793 loss: 6.77122898e-07
Iter: 1794 loss: 6.76815318e-07
Iter: 1795 loss: 6.77989249e-07
Iter: 1796 loss: 6.76772288e-07
Iter: 1797 loss: 6.76534967e-07
Iter: 1798 loss: 6.77096295e-07
Iter: 1799 loss: 6.76482045e-07
Iter: 1800 loss: 6.76232958e-07
Iter: 1801 loss: 6.76433672e-07
Iter: 1802 loss: 6.76055095e-07
Iter: 1803 loss: 6.75852448e-07
Iter: 1804 loss: 6.75849151e-07
Iter: 1805 loss: 6.75704541e-07
Iter: 1806 loss: 6.76242735e-07
Iter: 1807 loss: 6.75672425e-07
Iter: 1808 loss: 6.75528497e-07
Iter: 1809 loss: 6.76101195e-07
Iter: 1810 loss: 6.75497063e-07
Iter: 1811 loss: 6.75348701e-07
Iter: 1812 loss: 6.75072613e-07
Iter: 1813 loss: 6.80599e-07
Iter: 1814 loss: 6.75068577e-07
Iter: 1815 loss: 6.74830744e-07
Iter: 1816 loss: 6.75912702e-07
Iter: 1817 loss: 6.74760599e-07
Iter: 1818 loss: 6.74533169e-07
Iter: 1819 loss: 6.75554759e-07
Iter: 1820 loss: 6.74493208e-07
Iter: 1821 loss: 6.74248213e-07
Iter: 1822 loss: 6.74331091e-07
Iter: 1823 loss: 6.74050682e-07
Iter: 1824 loss: 6.73773798e-07
Iter: 1825 loss: 6.74446824e-07
Iter: 1826 loss: 6.73662726e-07
Iter: 1827 loss: 6.73415741e-07
Iter: 1828 loss: 6.74964781e-07
Iter: 1829 loss: 6.73351224e-07
Iter: 1830 loss: 6.7307576e-07
Iter: 1831 loss: 6.73413069e-07
Iter: 1832 loss: 6.72921146e-07
Iter: 1833 loss: 6.72653073e-07
Iter: 1834 loss: 6.73545628e-07
Iter: 1835 loss: 6.7257389e-07
Iter: 1836 loss: 6.72367378e-07
Iter: 1837 loss: 6.73746513e-07
Iter: 1838 loss: 6.72310591e-07
Iter: 1839 loss: 6.72094302e-07
Iter: 1840 loss: 6.73133059e-07
Iter: 1841 loss: 6.72017961e-07
Iter: 1842 loss: 6.71851922e-07
Iter: 1843 loss: 6.73598947e-07
Iter: 1844 loss: 6.7185232e-07
Iter: 1845 loss: 6.71762e-07
Iter: 1846 loss: 6.71600844e-07
Iter: 1847 loss: 6.71608916e-07
Iter: 1848 loss: 6.71397629e-07
Iter: 1849 loss: 6.71785756e-07
Iter: 1850 loss: 6.71309465e-07
Iter: 1851 loss: 6.71135354e-07
Iter: 1852 loss: 6.72046554e-07
Iter: 1853 loss: 6.71102839e-07
Iter: 1854 loss: 6.70862505e-07
Iter: 1855 loss: 6.70984718e-07
Iter: 1856 loss: 6.70696636e-07
Iter: 1857 loss: 6.70503482e-07
Iter: 1858 loss: 6.71104033e-07
Iter: 1859 loss: 6.70443342e-07
Iter: 1860 loss: 6.70176746e-07
Iter: 1861 loss: 6.70668157e-07
Iter: 1862 loss: 6.70103645e-07
Iter: 1863 loss: 6.69856e-07
Iter: 1864 loss: 6.71003363e-07
Iter: 1865 loss: 6.6980158e-07
Iter: 1866 loss: 6.69600183e-07
Iter: 1867 loss: 6.69648387e-07
Iter: 1868 loss: 6.69462395e-07
Iter: 1869 loss: 6.69220924e-07
Iter: 1870 loss: 6.71186172e-07
Iter: 1871 loss: 6.69166525e-07
Iter: 1872 loss: 6.69023848e-07
Iter: 1873 loss: 6.70726536e-07
Iter: 1874 loss: 6.69011627e-07
Iter: 1875 loss: 6.68911184e-07
Iter: 1876 loss: 6.69552946e-07
Iter: 1877 loss: 6.68886173e-07
Iter: 1878 loss: 6.68757366e-07
Iter: 1879 loss: 6.68535336e-07
Iter: 1880 loss: 6.73386353e-07
Iter: 1881 loss: 6.68562961e-07
Iter: 1882 loss: 6.68384928e-07
Iter: 1883 loss: 6.6914447e-07
Iter: 1884 loss: 6.68330472e-07
Iter: 1885 loss: 6.68132884e-07
Iter: 1886 loss: 6.6841767e-07
Iter: 1887 loss: 6.68052849e-07
Iter: 1888 loss: 6.67806376e-07
Iter: 1889 loss: 6.68144594e-07
Iter: 1890 loss: 6.6769735e-07
Iter: 1891 loss: 6.67468612e-07
Iter: 1892 loss: 6.67936945e-07
Iter: 1893 loss: 6.6736925e-07
Iter: 1894 loss: 6.67144491e-07
Iter: 1895 loss: 6.67846734e-07
Iter: 1896 loss: 6.67069685e-07
Iter: 1897 loss: 6.66871188e-07
Iter: 1898 loss: 6.67883228e-07
Iter: 1899 loss: 6.66812298e-07
Iter: 1900 loss: 6.66619655e-07
Iter: 1901 loss: 6.66565654e-07
Iter: 1902 loss: 6.66437359e-07
Iter: 1903 loss: 6.66185883e-07
Iter: 1904 loss: 6.68271241e-07
Iter: 1905 loss: 6.6614848e-07
Iter: 1906 loss: 6.65969253e-07
Iter: 1907 loss: 6.67329346e-07
Iter: 1908 loss: 6.65924119e-07
Iter: 1909 loss: 6.6576365e-07
Iter: 1910 loss: 6.66465098e-07
Iter: 1911 loss: 6.65725111e-07
Iter: 1912 loss: 6.6556106e-07
Iter: 1913 loss: 6.65471418e-07
Iter: 1914 loss: 6.65378479e-07
Iter: 1915 loss: 6.65191408e-07
Iter: 1916 loss: 6.65362677e-07
Iter: 1917 loss: 6.65056859e-07
Iter: 1918 loss: 6.64822551e-07
Iter: 1919 loss: 6.65466e-07
Iter: 1920 loss: 6.64759796e-07
Iter: 1921 loss: 6.64530489e-07
Iter: 1922 loss: 6.66033486e-07
Iter: 1923 loss: 6.6450275e-07
Iter: 1924 loss: 6.64349159e-07
Iter: 1925 loss: 6.64179481e-07
Iter: 1926 loss: 6.64151173e-07
Iter: 1927 loss: 6.63856667e-07
Iter: 1928 loss: 6.650763e-07
Iter: 1929 loss: 6.63773164e-07
Iter: 1930 loss: 6.63562957e-07
Iter: 1931 loss: 6.65017524e-07
Iter: 1932 loss: 6.63533683e-07
Iter: 1933 loss: 6.63353262e-07
Iter: 1934 loss: 6.63452283e-07
Iter: 1935 loss: 6.63238154e-07
Iter: 1936 loss: 6.6299566e-07
Iter: 1937 loss: 6.63576657e-07
Iter: 1938 loss: 6.62891239e-07
Iter: 1939 loss: 6.62727302e-07
Iter: 1940 loss: 6.62715706e-07
Iter: 1941 loss: 6.62567686e-07
Iter: 1942 loss: 6.6306734e-07
Iter: 1943 loss: 6.62547e-07
Iter: 1944 loss: 6.62407217e-07
Iter: 1945 loss: 6.624565e-07
Iter: 1946 loss: 6.62295065e-07
Iter: 1947 loss: 6.62130105e-07
Iter: 1948 loss: 6.62122943e-07
Iter: 1949 loss: 6.6202881e-07
Iter: 1950 loss: 6.61878857e-07
Iter: 1951 loss: 6.62541311e-07
Iter: 1952 loss: 6.61806951e-07
Iter: 1953 loss: 6.61643185e-07
Iter: 1954 loss: 6.62294212e-07
Iter: 1955 loss: 6.61587251e-07
Iter: 1956 loss: 6.6142286e-07
Iter: 1957 loss: 6.61515628e-07
Iter: 1958 loss: 6.61299168e-07
Iter: 1959 loss: 6.61082197e-07
Iter: 1960 loss: 6.61292347e-07
Iter: 1961 loss: 6.60953333e-07
Iter: 1962 loss: 6.60663147e-07
Iter: 1963 loss: 6.62691889e-07
Iter: 1964 loss: 6.60653768e-07
Iter: 1965 loss: 6.60452429e-07
Iter: 1966 loss: 6.60731871e-07
Iter: 1967 loss: 6.60375349e-07
Iter: 1968 loss: 6.60111823e-07
Iter: 1969 loss: 6.60645469e-07
Iter: 1970 loss: 6.60061346e-07
Iter: 1971 loss: 6.59900252e-07
Iter: 1972 loss: 6.59891612e-07
Iter: 1973 loss: 6.5975496e-07
Iter: 1974 loss: 6.59964144e-07
Iter: 1975 loss: 6.59692319e-07
Iter: 1976 loss: 6.59519287e-07
Iter: 1977 loss: 6.60052706e-07
Iter: 1978 loss: 6.59421687e-07
Iter: 1979 loss: 6.59305215e-07
Iter: 1980 loss: 6.59285774e-07
Iter: 1981 loss: 6.5920193e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.4
+ date
Mon Oct 26 13:44:53 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.4/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.4_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 1 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.4_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26586851e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f265864a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f265864c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2658732e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26585f6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26585d2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26585b38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f265854b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2658547488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f265854b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26584e4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26584d7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26584d7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2658484620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2658468840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2658468158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26584477b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f265841db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f265837a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f265837e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f265837b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2658339840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26582fb7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2658329730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2658329510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26582edd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26582816a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f265825d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f265824e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f265825d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2658214bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26581f6158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26581d8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2658181950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26581ac9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f26581d8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000220994843
Iter: 2 loss: 0.00158195733
Iter: 3 loss: 0.000178993971
Iter: 4 loss: 0.000151054453
Iter: 5 loss: 0.000195024593
Iter: 6 loss: 0.000138191579
Iter: 7 loss: 0.000125203573
Iter: 8 loss: 0.000145957689
Iter: 9 loss: 0.00011914783
Iter: 10 loss: 0.000107216212
Iter: 11 loss: 0.000139080861
Iter: 12 loss: 0.000103223443
Iter: 13 loss: 9.16993e-05
Iter: 14 loss: 9.28150257e-05
Iter: 15 loss: 8.28256743e-05
Iter: 16 loss: 7.54043431e-05
Iter: 17 loss: 0.00010226811
Iter: 18 loss: 7.35107606e-05
Iter: 19 loss: 6.86138446e-05
Iter: 20 loss: 6.6270979e-05
Iter: 21 loss: 6.39148057e-05
Iter: 22 loss: 5.69294789e-05
Iter: 23 loss: 7.37403243e-05
Iter: 24 loss: 5.44058712e-05
Iter: 25 loss: 5.10859682e-05
Iter: 26 loss: 6.79886725e-05
Iter: 27 loss: 5.05442367e-05
Iter: 28 loss: 4.80833805e-05
Iter: 29 loss: 4.71156818e-05
Iter: 30 loss: 4.57875649e-05
Iter: 31 loss: 4.37130293e-05
Iter: 32 loss: 4.94800188e-05
Iter: 33 loss: 4.30478685e-05
Iter: 34 loss: 4.22340672e-05
Iter: 35 loss: 5.43830247e-05
Iter: 36 loss: 4.22325247e-05
Iter: 37 loss: 4.16057264e-05
Iter: 38 loss: 4.07576554e-05
Iter: 39 loss: 4.07144034e-05
Iter: 40 loss: 3.9191189e-05
Iter: 41 loss: 3.9084669e-05
Iter: 42 loss: 3.79397097e-05
Iter: 43 loss: 3.64675216e-05
Iter: 44 loss: 4.79434784e-05
Iter: 45 loss: 3.63606232e-05
Iter: 46 loss: 3.50616319e-05
Iter: 47 loss: 3.9971259e-05
Iter: 48 loss: 3.47475725e-05
Iter: 49 loss: 3.38278296e-05
Iter: 50 loss: 3.37620913e-05
Iter: 51 loss: 3.30748262e-05
Iter: 52 loss: 3.16956211e-05
Iter: 53 loss: 3.91007707e-05
Iter: 54 loss: 3.14835852e-05
Iter: 55 loss: 3.07121e-05
Iter: 56 loss: 3.10535943e-05
Iter: 57 loss: 3.01863e-05
Iter: 58 loss: 2.91678261e-05
Iter: 59 loss: 3.38641585e-05
Iter: 60 loss: 2.89778345e-05
Iter: 61 loss: 2.7797716e-05
Iter: 62 loss: 3.10192299e-05
Iter: 63 loss: 2.74107733e-05
Iter: 64 loss: 2.67519099e-05
Iter: 65 loss: 2.69360298e-05
Iter: 66 loss: 2.6276397e-05
Iter: 67 loss: 2.62034409e-05
Iter: 68 loss: 2.58464697e-05
Iter: 69 loss: 2.54710503e-05
Iter: 70 loss: 2.54828883e-05
Iter: 71 loss: 2.51737e-05
Iter: 72 loss: 2.4807232e-05
Iter: 73 loss: 2.50993544e-05
Iter: 74 loss: 2.45863721e-05
Iter: 75 loss: 2.41819835e-05
Iter: 76 loss: 2.4026529e-05
Iter: 77 loss: 2.38068569e-05
Iter: 78 loss: 2.32960519e-05
Iter: 79 loss: 3.13461715e-05
Iter: 80 loss: 2.32962611e-05
Iter: 81 loss: 2.29535544e-05
Iter: 82 loss: 2.27039345e-05
Iter: 83 loss: 2.25874555e-05
Iter: 84 loss: 2.2163078e-05
Iter: 85 loss: 2.34022427e-05
Iter: 86 loss: 2.20333259e-05
Iter: 87 loss: 2.1484e-05
Iter: 88 loss: 2.31115446e-05
Iter: 89 loss: 2.13150415e-05
Iter: 90 loss: 2.08553738e-05
Iter: 91 loss: 2.14948486e-05
Iter: 92 loss: 2.06278855e-05
Iter: 93 loss: 2.03295822e-05
Iter: 94 loss: 2.4286328e-05
Iter: 95 loss: 2.03278578e-05
Iter: 96 loss: 2.00422255e-05
Iter: 97 loss: 1.95033535e-05
Iter: 98 loss: 3.13687e-05
Iter: 99 loss: 1.95016401e-05
Iter: 100 loss: 1.95482025e-05
Iter: 101 loss: 1.92985572e-05
Iter: 102 loss: 1.90850296e-05
Iter: 103 loss: 2.00132417e-05
Iter: 104 loss: 1.90424962e-05
Iter: 105 loss: 1.89101556e-05
Iter: 106 loss: 1.86274956e-05
Iter: 107 loss: 2.31654794e-05
Iter: 108 loss: 1.86179313e-05
Iter: 109 loss: 1.82095464e-05
Iter: 110 loss: 1.95620705e-05
Iter: 111 loss: 1.80976058e-05
Iter: 112 loss: 1.79551298e-05
Iter: 113 loss: 1.79184644e-05
Iter: 114 loss: 1.77796574e-05
Iter: 115 loss: 1.78048e-05
Iter: 116 loss: 1.76762442e-05
Iter: 117 loss: 1.74660272e-05
Iter: 118 loss: 1.70678e-05
Iter: 119 loss: 2.58045438e-05
Iter: 120 loss: 1.70662624e-05
Iter: 121 loss: 1.67171147e-05
Iter: 122 loss: 1.95764096e-05
Iter: 123 loss: 1.66958635e-05
Iter: 124 loss: 1.639947e-05
Iter: 125 loss: 1.82799013e-05
Iter: 126 loss: 1.63661953e-05
Iter: 127 loss: 1.60038981e-05
Iter: 128 loss: 1.68555889e-05
Iter: 129 loss: 1.58718885e-05
Iter: 130 loss: 1.57065806e-05
Iter: 131 loss: 1.68303686e-05
Iter: 132 loss: 1.56906499e-05
Iter: 133 loss: 1.55415019e-05
Iter: 134 loss: 1.5511223e-05
Iter: 135 loss: 1.54127265e-05
Iter: 136 loss: 1.53035126e-05
Iter: 137 loss: 1.52773755e-05
Iter: 138 loss: 1.51907316e-05
Iter: 139 loss: 1.50009973e-05
Iter: 140 loss: 1.77704314e-05
Iter: 141 loss: 1.49927528e-05
Iter: 142 loss: 1.48038571e-05
Iter: 143 loss: 1.57802497e-05
Iter: 144 loss: 1.47733663e-05
Iter: 145 loss: 1.4584597e-05
Iter: 146 loss: 1.5178327e-05
Iter: 147 loss: 1.45295489e-05
Iter: 148 loss: 1.42594163e-05
Iter: 149 loss: 1.53286255e-05
Iter: 150 loss: 1.41975197e-05
Iter: 151 loss: 1.40070933e-05
Iter: 152 loss: 1.48512136e-05
Iter: 153 loss: 1.39687163e-05
Iter: 154 loss: 1.38295964e-05
Iter: 155 loss: 1.35499904e-05
Iter: 156 loss: 1.87577516e-05
Iter: 157 loss: 1.35458813e-05
Iter: 158 loss: 1.33173e-05
Iter: 159 loss: 1.33132426e-05
Iter: 160 loss: 1.32109335e-05
Iter: 161 loss: 1.40807551e-05
Iter: 162 loss: 1.32047762e-05
Iter: 163 loss: 1.30939188e-05
Iter: 164 loss: 1.34106722e-05
Iter: 165 loss: 1.30588642e-05
Iter: 166 loss: 1.29708833e-05
Iter: 167 loss: 1.3294115e-05
Iter: 168 loss: 1.29494128e-05
Iter: 169 loss: 1.28770371e-05
Iter: 170 loss: 1.32468176e-05
Iter: 171 loss: 1.28651682e-05
Iter: 172 loss: 1.27978037e-05
Iter: 173 loss: 1.28616384e-05
Iter: 174 loss: 1.27586936e-05
Iter: 175 loss: 1.27125359e-05
Iter: 176 loss: 1.25822626e-05
Iter: 177 loss: 1.32516943e-05
Iter: 178 loss: 1.25409506e-05
Iter: 179 loss: 1.238001e-05
Iter: 180 loss: 1.45787471e-05
Iter: 181 loss: 1.23788195e-05
Iter: 182 loss: 1.22841975e-05
Iter: 183 loss: 1.31850429e-05
Iter: 184 loss: 1.22808278e-05
Iter: 185 loss: 1.21859466e-05
Iter: 186 loss: 1.22661e-05
Iter: 187 loss: 1.2129758e-05
Iter: 188 loss: 1.2024444e-05
Iter: 189 loss: 1.21264384e-05
Iter: 190 loss: 1.19648485e-05
Iter: 191 loss: 1.18523603e-05
Iter: 192 loss: 1.20969817e-05
Iter: 193 loss: 1.18085791e-05
Iter: 194 loss: 1.16960036e-05
Iter: 195 loss: 1.16260489e-05
Iter: 196 loss: 1.1580667e-05
Iter: 197 loss: 1.15386229e-05
Iter: 198 loss: 1.15013954e-05
Iter: 199 loss: 1.14464619e-05
Iter: 200 loss: 1.18173448e-05
Iter: 201 loss: 1.14407812e-05
Iter: 202 loss: 1.13962888e-05
Iter: 203 loss: 1.17009422e-05
Iter: 204 loss: 1.13920742e-05
Iter: 205 loss: 1.13547321e-05
Iter: 206 loss: 1.1348684e-05
Iter: 207 loss: 1.1323e-05
Iter: 208 loss: 1.12693251e-05
Iter: 209 loss: 1.12199032e-05
Iter: 210 loss: 1.1206992e-05
Iter: 211 loss: 1.11439576e-05
Iter: 212 loss: 1.11150748e-05
Iter: 213 loss: 1.10830752e-05
Iter: 214 loss: 1.09660123e-05
Iter: 215 loss: 1.13647584e-05
Iter: 216 loss: 1.09338798e-05
Iter: 217 loss: 1.08774866e-05
Iter: 218 loss: 1.08651802e-05
Iter: 219 loss: 1.08283457e-05
Iter: 220 loss: 1.07655542e-05
Iter: 221 loss: 1.07654178e-05
Iter: 222 loss: 1.0657177e-05
Iter: 223 loss: 1.06420557e-05
Iter: 224 loss: 1.05652471e-05
Iter: 225 loss: 1.04570463e-05
Iter: 226 loss: 1.11097543e-05
Iter: 227 loss: 1.04436422e-05
Iter: 228 loss: 1.03839584e-05
Iter: 229 loss: 1.0407035e-05
Iter: 230 loss: 1.03428647e-05
Iter: 231 loss: 1.03199836e-05
Iter: 232 loss: 1.03010725e-05
Iter: 233 loss: 1.0276297e-05
Iter: 234 loss: 1.04684123e-05
Iter: 235 loss: 1.02745917e-05
Iter: 236 loss: 1.02525555e-05
Iter: 237 loss: 1.02368922e-05
Iter: 238 loss: 1.02293707e-05
Iter: 239 loss: 1.01963069e-05
Iter: 240 loss: 1.02174126e-05
Iter: 241 loss: 1.0175334e-05
Iter: 242 loss: 1.01301175e-05
Iter: 243 loss: 1.00488032e-05
Iter: 244 loss: 1.20308196e-05
Iter: 245 loss: 1.00487741e-05
Iter: 246 loss: 9.97409916e-06
Iter: 247 loss: 1.03462116e-05
Iter: 248 loss: 9.96115887e-06
Iter: 249 loss: 9.90917852e-06
Iter: 250 loss: 9.90882836e-06
Iter: 251 loss: 9.85962652e-06
Iter: 252 loss: 9.93613412e-06
Iter: 253 loss: 9.83643e-06
Iter: 254 loss: 9.80008645e-06
Iter: 255 loss: 9.831223e-06
Iter: 256 loss: 9.77888067e-06
Iter: 257 loss: 9.7308357e-06
Iter: 258 loss: 9.8276887e-06
Iter: 259 loss: 9.71107693e-06
Iter: 260 loss: 9.65346408e-06
Iter: 261 loss: 9.64928768e-06
Iter: 262 loss: 9.60579382e-06
Iter: 263 loss: 9.55955238e-06
Iter: 264 loss: 1.0114285e-05
Iter: 265 loss: 9.55913129e-06
Iter: 266 loss: 9.54065763e-06
Iter: 267 loss: 9.5356645e-06
Iter: 268 loss: 9.51819675e-06
Iter: 269 loss: 9.49717833e-06
Iter: 270 loss: 9.49559944e-06
Iter: 271 loss: 9.46486e-06
Iter: 272 loss: 9.46134787e-06
Iter: 273 loss: 9.43903069e-06
Iter: 274 loss: 9.3993367e-06
Iter: 275 loss: 9.53008202e-06
Iter: 276 loss: 9.38818448e-06
Iter: 277 loss: 9.3522749e-06
Iter: 278 loss: 9.28831832e-06
Iter: 279 loss: 9.28838745e-06
Iter: 280 loss: 9.24567757e-06
Iter: 281 loss: 9.24395863e-06
Iter: 282 loss: 9.21010178e-06
Iter: 283 loss: 9.46516e-06
Iter: 284 loss: 9.20735692e-06
Iter: 285 loss: 9.17842772e-06
Iter: 286 loss: 9.1209422e-06
Iter: 287 loss: 1.02586328e-05
Iter: 288 loss: 9.12046562e-06
Iter: 289 loss: 9.07629601e-06
Iter: 290 loss: 9.62447939e-06
Iter: 291 loss: 9.07591311e-06
Iter: 292 loss: 9.04383705e-06
Iter: 293 loss: 9.10416e-06
Iter: 294 loss: 9.03070122e-06
Iter: 295 loss: 8.99399947e-06
Iter: 296 loss: 8.97798418e-06
Iter: 297 loss: 8.95920857e-06
Iter: 298 loss: 8.92279604e-06
Iter: 299 loss: 9.36030847e-06
Iter: 300 loss: 8.92246862e-06
Iter: 301 loss: 8.8924744e-06
Iter: 302 loss: 8.89243256e-06
Iter: 303 loss: 8.87753777e-06
Iter: 304 loss: 8.83512439e-06
Iter: 305 loss: 9.04034459e-06
Iter: 306 loss: 8.82051e-06
Iter: 307 loss: 8.78547326e-06
Iter: 308 loss: 8.78547053e-06
Iter: 309 loss: 8.76562444e-06
Iter: 310 loss: 8.73621e-06
Iter: 311 loss: 8.73534e-06
Iter: 312 loss: 8.69285577e-06
Iter: 313 loss: 8.98573126e-06
Iter: 314 loss: 8.68887582e-06
Iter: 315 loss: 8.66047594e-06
Iter: 316 loss: 9.07584945e-06
Iter: 317 loss: 8.6604623e-06
Iter: 318 loss: 8.64201684e-06
Iter: 319 loss: 8.70867734e-06
Iter: 320 loss: 8.63737114e-06
Iter: 321 loss: 8.62013167e-06
Iter: 322 loss: 8.57977284e-06
Iter: 323 loss: 9.07171761e-06
Iter: 324 loss: 8.57702798e-06
Iter: 325 loss: 8.51552795e-06
Iter: 326 loss: 8.75960905e-06
Iter: 327 loss: 8.50138531e-06
Iter: 328 loss: 8.46671537e-06
Iter: 329 loss: 8.98111739e-06
Iter: 330 loss: 8.46658259e-06
Iter: 331 loss: 8.4414105e-06
Iter: 332 loss: 8.43999078e-06
Iter: 333 loss: 8.42101508e-06
Iter: 334 loss: 8.40462417e-06
Iter: 335 loss: 8.40172379e-06
Iter: 336 loss: 8.37869902e-06
Iter: 337 loss: 8.39223594e-06
Iter: 338 loss: 8.36375693e-06
Iter: 339 loss: 8.3513014e-06
Iter: 340 loss: 8.32307614e-06
Iter: 341 loss: 8.73221143e-06
Iter: 342 loss: 8.32212572e-06
Iter: 343 loss: 8.28768589e-06
Iter: 344 loss: 8.56469e-06
Iter: 345 loss: 8.28527845e-06
Iter: 346 loss: 8.26431915e-06
Iter: 347 loss: 8.31172292e-06
Iter: 348 loss: 8.25607276e-06
Iter: 349 loss: 8.23650407e-06
Iter: 350 loss: 8.31477428e-06
Iter: 351 loss: 8.23228402e-06
Iter: 352 loss: 8.20824789e-06
Iter: 353 loss: 8.31364923e-06
Iter: 354 loss: 8.20390233e-06
Iter: 355 loss: 8.18309672e-06
Iter: 356 loss: 8.2166307e-06
Iter: 357 loss: 8.17361615e-06
Iter: 358 loss: 8.15259409e-06
Iter: 359 loss: 8.14667146e-06
Iter: 360 loss: 8.13393854e-06
Iter: 361 loss: 8.10199708e-06
Iter: 362 loss: 8.13213774e-06
Iter: 363 loss: 8.0835e-06
Iter: 364 loss: 8.05326817e-06
Iter: 365 loss: 8.51609275e-06
Iter: 366 loss: 8.05353375e-06
Iter: 367 loss: 8.03382591e-06
Iter: 368 loss: 8.09622543e-06
Iter: 369 loss: 8.02860814e-06
Iter: 370 loss: 8.00939415e-06
Iter: 371 loss: 8.0094178e-06
Iter: 372 loss: 8.00258204e-06
Iter: 373 loss: 7.98458677e-06
Iter: 374 loss: 8.11737664e-06
Iter: 375 loss: 7.98053e-06
Iter: 376 loss: 7.96414133e-06
Iter: 377 loss: 8.00744783e-06
Iter: 378 loss: 7.95855885e-06
Iter: 379 loss: 7.93689378e-06
Iter: 380 loss: 7.96370114e-06
Iter: 381 loss: 7.92591e-06
Iter: 382 loss: 7.90237391e-06
Iter: 383 loss: 7.94972766e-06
Iter: 384 loss: 7.89253409e-06
Iter: 385 loss: 7.8767971e-06
Iter: 386 loss: 7.87650242e-06
Iter: 387 loss: 7.8622279e-06
Iter: 388 loss: 7.85958855e-06
Iter: 389 loss: 7.8498e-06
Iter: 390 loss: 7.82944244e-06
Iter: 391 loss: 7.84349322e-06
Iter: 392 loss: 7.8167368e-06
Iter: 393 loss: 7.79416314e-06
Iter: 394 loss: 7.84350414e-06
Iter: 395 loss: 7.78553476e-06
Iter: 396 loss: 7.7627019e-06
Iter: 397 loss: 7.77014066e-06
Iter: 398 loss: 7.74670661e-06
Iter: 399 loss: 7.72451494e-06
Iter: 400 loss: 7.72424119e-06
Iter: 401 loss: 7.72096337e-06
Iter: 402 loss: 7.71687428e-06
Iter: 403 loss: 7.71172472e-06
Iter: 404 loss: 7.69832877e-06
Iter: 405 loss: 7.81413291e-06
Iter: 406 loss: 7.69604685e-06
Iter: 407 loss: 7.6819e-06
Iter: 408 loss: 7.65583627e-06
Iter: 409 loss: 8.26269206e-06
Iter: 410 loss: 7.65531331e-06
Iter: 411 loss: 7.63245953e-06
Iter: 412 loss: 7.63202388e-06
Iter: 413 loss: 7.61635056e-06
Iter: 414 loss: 7.62701347e-06
Iter: 415 loss: 7.60584544e-06
Iter: 416 loss: 7.58551414e-06
Iter: 417 loss: 7.6195447e-06
Iter: 418 loss: 7.5758594e-06
Iter: 419 loss: 7.56344707e-06
Iter: 420 loss: 7.56182817e-06
Iter: 421 loss: 7.5538178e-06
Iter: 422 loss: 7.54914799e-06
Iter: 423 loss: 7.54548864e-06
Iter: 424 loss: 7.53303675e-06
Iter: 425 loss: 7.52454525e-06
Iter: 426 loss: 7.52007463e-06
Iter: 427 loss: 7.49657193e-06
Iter: 428 loss: 7.58480473e-06
Iter: 429 loss: 7.49096807e-06
Iter: 430 loss: 7.47457852e-06
Iter: 431 loss: 7.5123753e-06
Iter: 432 loss: 7.46839714e-06
Iter: 433 loss: 7.46803607e-06
Iter: 434 loss: 7.46104524e-06
Iter: 435 loss: 7.45313218e-06
Iter: 436 loss: 7.44448562e-06
Iter: 437 loss: 7.44325189e-06
Iter: 438 loss: 7.43282135e-06
Iter: 439 loss: 7.41876192e-06
Iter: 440 loss: 7.41815074e-06
Iter: 441 loss: 7.40188261e-06
Iter: 442 loss: 7.42867633e-06
Iter: 443 loss: 7.39453162e-06
Iter: 444 loss: 7.37783103e-06
Iter: 445 loss: 7.52041706e-06
Iter: 446 loss: 7.37673417e-06
Iter: 447 loss: 7.3628421e-06
Iter: 448 loss: 7.37081882e-06
Iter: 449 loss: 7.3536371e-06
Iter: 450 loss: 7.34152854e-06
Iter: 451 loss: 7.43137389e-06
Iter: 452 loss: 7.34071818e-06
Iter: 453 loss: 7.32512581e-06
Iter: 454 loss: 7.32299713e-06
Iter: 455 loss: 7.31185446e-06
Iter: 456 loss: 7.29565318e-06
Iter: 457 loss: 7.36613856e-06
Iter: 458 loss: 7.29248768e-06
Iter: 459 loss: 7.28136274e-06
Iter: 460 loss: 7.28834539e-06
Iter: 461 loss: 7.27439283e-06
Iter: 462 loss: 7.25969812e-06
Iter: 463 loss: 7.29817748e-06
Iter: 464 loss: 7.2545854e-06
Iter: 465 loss: 7.24679558e-06
Iter: 466 loss: 7.24642e-06
Iter: 467 loss: 7.23785e-06
Iter: 468 loss: 7.27500174e-06
Iter: 469 loss: 7.23596759e-06
Iter: 470 loss: 7.23162339e-06
Iter: 471 loss: 7.22130335e-06
Iter: 472 loss: 7.34447212e-06
Iter: 473 loss: 7.2203361e-06
Iter: 474 loss: 7.20480239e-06
Iter: 475 loss: 7.19701575e-06
Iter: 476 loss: 7.18997217e-06
Iter: 477 loss: 7.17408375e-06
Iter: 478 loss: 7.31760974e-06
Iter: 479 loss: 7.17348303e-06
Iter: 480 loss: 7.15728493e-06
Iter: 481 loss: 7.18535694e-06
Iter: 482 loss: 7.15012038e-06
Iter: 483 loss: 7.13566078e-06
Iter: 484 loss: 7.16343129e-06
Iter: 485 loss: 7.12992642e-06
Iter: 486 loss: 7.11607754e-06
Iter: 487 loss: 7.32121498e-06
Iter: 488 loss: 7.11609573e-06
Iter: 489 loss: 7.10693075e-06
Iter: 490 loss: 7.09479355e-06
Iter: 491 loss: 7.09393044e-06
Iter: 492 loss: 7.0777769e-06
Iter: 493 loss: 7.12627707e-06
Iter: 494 loss: 7.07275603e-06
Iter: 495 loss: 7.0594092e-06
Iter: 496 loss: 7.12312567e-06
Iter: 497 loss: 7.05724733e-06
Iter: 498 loss: 7.04728791e-06
Iter: 499 loss: 7.08142488e-06
Iter: 500 loss: 7.04487775e-06
Iter: 501 loss: 7.03577598e-06
Iter: 502 loss: 7.03563092e-06
Iter: 503 loss: 7.03051955e-06
Iter: 504 loss: 7.01737e-06
Iter: 505 loss: 7.12938e-06
Iter: 506 loss: 7.01543286e-06
Iter: 507 loss: 7.00428427e-06
Iter: 508 loss: 7.05341927e-06
Iter: 509 loss: 7.00177679e-06
Iter: 510 loss: 6.99162501e-06
Iter: 511 loss: 6.98158601e-06
Iter: 512 loss: 6.97935502e-06
Iter: 513 loss: 6.96525512e-06
Iter: 514 loss: 7.1688346e-06
Iter: 515 loss: 6.96505185e-06
Iter: 516 loss: 6.95488234e-06
Iter: 517 loss: 6.97088308e-06
Iter: 518 loss: 6.94994924e-06
Iter: 519 loss: 6.94095615e-06
Iter: 520 loss: 7.00381315e-06
Iter: 521 loss: 6.94001119e-06
Iter: 522 loss: 6.92899857e-06
Iter: 523 loss: 6.93029597e-06
Iter: 524 loss: 6.92068e-06
Iter: 525 loss: 6.91114792e-06
Iter: 526 loss: 6.91578953e-06
Iter: 527 loss: 6.90493926e-06
Iter: 528 loss: 6.89127182e-06
Iter: 529 loss: 6.93218954e-06
Iter: 530 loss: 6.88737236e-06
Iter: 531 loss: 6.87517922e-06
Iter: 532 loss: 6.93098355e-06
Iter: 533 loss: 6.87314605e-06
Iter: 534 loss: 6.87424745e-06
Iter: 535 loss: 6.86838439e-06
Iter: 536 loss: 6.86510703e-06
Iter: 537 loss: 6.85654777e-06
Iter: 538 loss: 6.91684909e-06
Iter: 539 loss: 6.85491204e-06
Iter: 540 loss: 6.84529e-06
Iter: 541 loss: 6.86184558e-06
Iter: 542 loss: 6.84102542e-06
Iter: 543 loss: 6.8305676e-06
Iter: 544 loss: 6.8407744e-06
Iter: 545 loss: 6.82480095e-06
Iter: 546 loss: 6.81526035e-06
Iter: 547 loss: 6.87176407e-06
Iter: 548 loss: 6.81402616e-06
Iter: 549 loss: 6.80298217e-06
Iter: 550 loss: 6.81200481e-06
Iter: 551 loss: 6.79707591e-06
Iter: 552 loss: 6.78532615e-06
Iter: 553 loss: 6.86184239e-06
Iter: 554 loss: 6.78392553e-06
Iter: 555 loss: 6.77545222e-06
Iter: 556 loss: 6.85619216e-06
Iter: 557 loss: 6.77515527e-06
Iter: 558 loss: 6.76948093e-06
Iter: 559 loss: 6.75605042e-06
Iter: 560 loss: 6.89876924e-06
Iter: 561 loss: 6.75410229e-06
Iter: 562 loss: 6.74043622e-06
Iter: 563 loss: 6.8825475e-06
Iter: 564 loss: 6.73995555e-06
Iter: 565 loss: 6.73155409e-06
Iter: 566 loss: 6.77493335e-06
Iter: 567 loss: 6.73044769e-06
Iter: 568 loss: 6.72772785e-06
Iter: 569 loss: 6.72685383e-06
Iter: 570 loss: 6.7224446e-06
Iter: 571 loss: 6.71205453e-06
Iter: 572 loss: 6.83973531e-06
Iter: 573 loss: 6.71146336e-06
Iter: 574 loss: 6.70385543e-06
Iter: 575 loss: 6.71977614e-06
Iter: 576 loss: 6.70071586e-06
Iter: 577 loss: 6.69281644e-06
Iter: 578 loss: 6.70199643e-06
Iter: 579 loss: 6.68850726e-06
Iter: 580 loss: 6.67944914e-06
Iter: 581 loss: 6.68693428e-06
Iter: 582 loss: 6.67412132e-06
Iter: 583 loss: 6.66513824e-06
Iter: 584 loss: 6.78607967e-06
Iter: 585 loss: 6.66512824e-06
Iter: 586 loss: 6.65927109e-06
Iter: 587 loss: 6.66209144e-06
Iter: 588 loss: 6.65547122e-06
Iter: 589 loss: 6.64881918e-06
Iter: 590 loss: 6.72261103e-06
Iter: 591 loss: 6.64864092e-06
Iter: 592 loss: 6.64236541e-06
Iter: 593 loss: 6.63525498e-06
Iter: 594 loss: 6.63420542e-06
Iter: 595 loss: 6.62664888e-06
Iter: 596 loss: 6.64265826e-06
Iter: 597 loss: 6.62357888e-06
Iter: 598 loss: 6.61391459e-06
Iter: 599 loss: 6.62125785e-06
Iter: 600 loss: 6.60771184e-06
Iter: 601 loss: 6.60423302e-06
Iter: 602 loss: 6.60158958e-06
Iter: 603 loss: 6.59553552e-06
Iter: 604 loss: 6.61357717e-06
Iter: 605 loss: 6.59349553e-06
Iter: 606 loss: 6.59062334e-06
Iter: 607 loss: 6.58215549e-06
Iter: 608 loss: 6.62047114e-06
Iter: 609 loss: 6.57893315e-06
Iter: 610 loss: 6.56818793e-06
Iter: 611 loss: 6.65306834e-06
Iter: 612 loss: 6.56754946e-06
Iter: 613 loss: 6.55931308e-06
Iter: 614 loss: 6.5802692e-06
Iter: 615 loss: 6.5564991e-06
Iter: 616 loss: 6.54751329e-06
Iter: 617 loss: 6.55146596e-06
Iter: 618 loss: 6.54134601e-06
Iter: 619 loss: 6.5318327e-06
Iter: 620 loss: 6.66403321e-06
Iter: 621 loss: 6.53206098e-06
Iter: 622 loss: 6.52599283e-06
Iter: 623 loss: 6.536201e-06
Iter: 624 loss: 6.52397785e-06
Iter: 625 loss: 6.51690334e-06
Iter: 626 loss: 6.54883388e-06
Iter: 627 loss: 6.51540176e-06
Iter: 628 loss: 6.51059781e-06
Iter: 629 loss: 6.49941194e-06
Iter: 630 loss: 6.65229754e-06
Iter: 631 loss: 6.49901904e-06
Iter: 632 loss: 6.48885907e-06
Iter: 633 loss: 6.61467811e-06
Iter: 634 loss: 6.48859168e-06
Iter: 635 loss: 6.48210698e-06
Iter: 636 loss: 6.50521952e-06
Iter: 637 loss: 6.4803844e-06
Iter: 638 loss: 6.47575143e-06
Iter: 639 loss: 6.47520392e-06
Iter: 640 loss: 6.47270372e-06
Iter: 641 loss: 6.46431636e-06
Iter: 642 loss: 6.49377807e-06
Iter: 643 loss: 6.46047738e-06
Iter: 644 loss: 6.45378623e-06
Iter: 645 loss: 6.50254242e-06
Iter: 646 loss: 6.45334876e-06
Iter: 647 loss: 6.44810098e-06
Iter: 648 loss: 6.4514652e-06
Iter: 649 loss: 6.44474403e-06
Iter: 650 loss: 6.43838393e-06
Iter: 651 loss: 6.46293756e-06
Iter: 652 loss: 6.43696512e-06
Iter: 653 loss: 6.43147132e-06
Iter: 654 loss: 6.43984367e-06
Iter: 655 loss: 6.42856412e-06
Iter: 656 loss: 6.42134e-06
Iter: 657 loss: 6.45433647e-06
Iter: 658 loss: 6.41988709e-06
Iter: 659 loss: 6.41317911e-06
Iter: 660 loss: 6.45826185e-06
Iter: 661 loss: 6.41261431e-06
Iter: 662 loss: 6.4076894e-06
Iter: 663 loss: 6.4181786e-06
Iter: 664 loss: 6.40573853e-06
Iter: 665 loss: 6.40118378e-06
Iter: 666 loss: 6.39475593e-06
Iter: 667 loss: 6.39459086e-06
Iter: 668 loss: 6.38748588e-06
Iter: 669 loss: 6.40187955e-06
Iter: 670 loss: 6.38483743e-06
Iter: 671 loss: 6.38337042e-06
Iter: 672 loss: 6.38171878e-06
Iter: 673 loss: 6.37796802e-06
Iter: 674 loss: 6.38453821e-06
Iter: 675 loss: 6.37617541e-06
Iter: 676 loss: 6.37398625e-06
Iter: 677 loss: 6.36933328e-06
Iter: 678 loss: 6.46756234e-06
Iter: 679 loss: 6.36919958e-06
Iter: 680 loss: 6.36461846e-06
Iter: 681 loss: 6.36670529e-06
Iter: 682 loss: 6.36138566e-06
Iter: 683 loss: 6.35439756e-06
Iter: 684 loss: 6.36754248e-06
Iter: 685 loss: 6.35145625e-06
Iter: 686 loss: 6.34634e-06
Iter: 687 loss: 6.34630942e-06
Iter: 688 loss: 6.34273829e-06
Iter: 689 loss: 6.34133266e-06
Iter: 690 loss: 6.33951913e-06
Iter: 691 loss: 6.33442869e-06
Iter: 692 loss: 6.36585901e-06
Iter: 693 loss: 6.33379204e-06
Iter: 694 loss: 6.32831598e-06
Iter: 695 loss: 6.34234857e-06
Iter: 696 loss: 6.32611045e-06
Iter: 697 loss: 6.32167848e-06
Iter: 698 loss: 6.33943091e-06
Iter: 699 loss: 6.32035608e-06
Iter: 700 loss: 6.31623698e-06
Iter: 701 loss: 6.31008561e-06
Iter: 702 loss: 6.31013063e-06
Iter: 703 loss: 6.30174827e-06
Iter: 704 loss: 6.34168191e-06
Iter: 705 loss: 6.30046679e-06
Iter: 706 loss: 6.29469559e-06
Iter: 707 loss: 6.29974647e-06
Iter: 708 loss: 6.29140231e-06
Iter: 709 loss: 6.30163595e-06
Iter: 710 loss: 6.28960697e-06
Iter: 711 loss: 6.28802e-06
Iter: 712 loss: 6.28362841e-06
Iter: 713 loss: 6.29306805e-06
Iter: 714 loss: 6.28085763e-06
Iter: 715 loss: 6.27638565e-06
Iter: 716 loss: 6.27641612e-06
Iter: 717 loss: 6.27274676e-06
Iter: 718 loss: 6.26653582e-06
Iter: 719 loss: 6.29716124e-06
Iter: 720 loss: 6.26574592e-06
Iter: 721 loss: 6.25924167e-06
Iter: 722 loss: 6.27161853e-06
Iter: 723 loss: 6.25695566e-06
Iter: 724 loss: 6.25047687e-06
Iter: 725 loss: 6.25990469e-06
Iter: 726 loss: 6.24693757e-06
Iter: 727 loss: 6.24104177e-06
Iter: 728 loss: 6.2409772e-06
Iter: 729 loss: 6.23608912e-06
Iter: 730 loss: 6.24529321e-06
Iter: 731 loss: 6.23400183e-06
Iter: 732 loss: 6.22986772e-06
Iter: 733 loss: 6.24740824e-06
Iter: 734 loss: 6.22875768e-06
Iter: 735 loss: 6.22472226e-06
Iter: 736 loss: 6.21902927e-06
Iter: 737 loss: 6.21875279e-06
Iter: 738 loss: 6.21220352e-06
Iter: 739 loss: 6.24428458e-06
Iter: 740 loss: 6.21129129e-06
Iter: 741 loss: 6.20904302e-06
Iter: 742 loss: 6.20847322e-06
Iter: 743 loss: 6.20505671e-06
Iter: 744 loss: 6.21146137e-06
Iter: 745 loss: 6.20349783e-06
Iter: 746 loss: 6.20107312e-06
Iter: 747 loss: 6.19519415e-06
Iter: 748 loss: 6.26476412e-06
Iter: 749 loss: 6.19473394e-06
Iter: 750 loss: 6.18905779e-06
Iter: 751 loss: 6.20199535e-06
Iter: 752 loss: 6.18675222e-06
Iter: 753 loss: 6.17960404e-06
Iter: 754 loss: 6.17941441e-06
Iter: 755 loss: 6.17392e-06
Iter: 756 loss: 6.16598209e-06
Iter: 757 loss: 6.20760648e-06
Iter: 758 loss: 6.16472153e-06
Iter: 759 loss: 6.15698264e-06
Iter: 760 loss: 6.17156411e-06
Iter: 761 loss: 6.1535211e-06
Iter: 762 loss: 6.14825876e-06
Iter: 763 loss: 6.14780856e-06
Iter: 764 loss: 6.14467626e-06
Iter: 765 loss: 6.14196233e-06
Iter: 766 loss: 6.14115743e-06
Iter: 767 loss: 6.13530028e-06
Iter: 768 loss: 6.15165391e-06
Iter: 769 loss: 6.13331395e-06
Iter: 770 loss: 6.12757503e-06
Iter: 771 loss: 6.11846644e-06
Iter: 772 loss: 6.11836185e-06
Iter: 773 loss: 6.11469932e-06
Iter: 774 loss: 6.11287032e-06
Iter: 775 loss: 6.11026871e-06
Iter: 776 loss: 6.11006726e-06
Iter: 777 loss: 6.10834195e-06
Iter: 778 loss: 6.10318466e-06
Iter: 779 loss: 6.12216854e-06
Iter: 780 loss: 6.10112374e-06
Iter: 781 loss: 6.09430572e-06
Iter: 782 loss: 6.10247116e-06
Iter: 783 loss: 6.09070912e-06
Iter: 784 loss: 6.082455e-06
Iter: 785 loss: 6.10625466e-06
Iter: 786 loss: 6.0798161e-06
Iter: 787 loss: 6.07076e-06
Iter: 788 loss: 6.07342099e-06
Iter: 789 loss: 6.06421236e-06
Iter: 790 loss: 6.05321384e-06
Iter: 791 loss: 6.10363077e-06
Iter: 792 loss: 6.0510697e-06
Iter: 793 loss: 6.04472552e-06
Iter: 794 loss: 6.12063741e-06
Iter: 795 loss: 6.04473735e-06
Iter: 796 loss: 6.03811304e-06
Iter: 797 loss: 6.05706146e-06
Iter: 798 loss: 6.03613489e-06
Iter: 799 loss: 6.03077569e-06
Iter: 800 loss: 6.03408944e-06
Iter: 801 loss: 6.0272514e-06
Iter: 802 loss: 6.02092859e-06
Iter: 803 loss: 6.04089701e-06
Iter: 804 loss: 6.01937609e-06
Iter: 805 loss: 6.01234342e-06
Iter: 806 loss: 6.01394e-06
Iter: 807 loss: 6.00696876e-06
Iter: 808 loss: 6.01425563e-06
Iter: 809 loss: 6.00435033e-06
Iter: 810 loss: 6.0024563e-06
Iter: 811 loss: 5.99645045e-06
Iter: 812 loss: 6.02989348e-06
Iter: 813 loss: 5.99496207e-06
Iter: 814 loss: 5.98831275e-06
Iter: 815 loss: 6.00017847e-06
Iter: 816 loss: 5.98534689e-06
Iter: 817 loss: 5.9778813e-06
Iter: 818 loss: 5.97047256e-06
Iter: 819 loss: 5.96856353e-06
Iter: 820 loss: 5.95783513e-06
Iter: 821 loss: 6.0608927e-06
Iter: 822 loss: 5.95752408e-06
Iter: 823 loss: 5.94962967e-06
Iter: 824 loss: 5.96708924e-06
Iter: 825 loss: 5.94653147e-06
Iter: 826 loss: 5.93820505e-06
Iter: 827 loss: 5.94198627e-06
Iter: 828 loss: 5.93214418e-06
Iter: 829 loss: 5.92901506e-06
Iter: 830 loss: 5.92779452e-06
Iter: 831 loss: 5.92300603e-06
Iter: 832 loss: 5.91985e-06
Iter: 833 loss: 5.91836215e-06
Iter: 834 loss: 5.9112358e-06
Iter: 835 loss: 5.90925129e-06
Iter: 836 loss: 5.90534137e-06
Iter: 837 loss: 5.89593083e-06
Iter: 838 loss: 6.01122065e-06
Iter: 839 loss: 5.89568845e-06
Iter: 840 loss: 5.89388492e-06
Iter: 841 loss: 5.89329193e-06
Iter: 842 loss: 5.89061e-06
Iter: 843 loss: 5.8839496e-06
Iter: 844 loss: 5.97403323e-06
Iter: 845 loss: 5.88347257e-06
Iter: 846 loss: 5.87711293e-06
Iter: 847 loss: 5.87765589e-06
Iter: 848 loss: 5.87227714e-06
Iter: 849 loss: 5.86413444e-06
Iter: 850 loss: 5.89098227e-06
Iter: 851 loss: 5.86208625e-06
Iter: 852 loss: 5.85554471e-06
Iter: 853 loss: 5.85524958e-06
Iter: 854 loss: 5.85046473e-06
Iter: 855 loss: 5.8412943e-06
Iter: 856 loss: 5.88830062e-06
Iter: 857 loss: 5.83942528e-06
Iter: 858 loss: 5.83180599e-06
Iter: 859 loss: 5.86145416e-06
Iter: 860 loss: 5.83014844e-06
Iter: 861 loss: 5.82348e-06
Iter: 862 loss: 5.83779683e-06
Iter: 863 loss: 5.82080065e-06
Iter: 864 loss: 5.81322502e-06
Iter: 865 loss: 5.88573948e-06
Iter: 866 loss: 5.81266522e-06
Iter: 867 loss: 5.80776396e-06
Iter: 868 loss: 5.80519e-06
Iter: 869 loss: 5.80261894e-06
Iter: 870 loss: 5.797006e-06
Iter: 871 loss: 5.82169787e-06
Iter: 872 loss: 5.79594689e-06
Iter: 873 loss: 5.79157e-06
Iter: 874 loss: 5.85919133e-06
Iter: 875 loss: 5.79165089e-06
Iter: 876 loss: 5.78656864e-06
Iter: 877 loss: 5.78834897e-06
Iter: 878 loss: 5.78318031e-06
Iter: 879 loss: 5.77893843e-06
Iter: 880 loss: 5.77256742e-06
Iter: 881 loss: 5.77255878e-06
Iter: 882 loss: 5.76660295e-06
Iter: 883 loss: 5.79620337e-06
Iter: 884 loss: 5.76570892e-06
Iter: 885 loss: 5.75966305e-06
Iter: 886 loss: 5.75654258e-06
Iter: 887 loss: 5.75371268e-06
Iter: 888 loss: 5.74508795e-06
Iter: 889 loss: 5.77263472e-06
Iter: 890 loss: 5.74266323e-06
Iter: 891 loss: 5.73454099e-06
Iter: 892 loss: 5.7569473e-06
Iter: 893 loss: 5.73181751e-06
Iter: 894 loss: 5.72311046e-06
Iter: 895 loss: 5.77325591e-06
Iter: 896 loss: 5.72199451e-06
Iter: 897 loss: 5.71749024e-06
Iter: 898 loss: 5.78924028e-06
Iter: 899 loss: 5.71748888e-06
Iter: 900 loss: 5.7132288e-06
Iter: 901 loss: 5.70801058e-06
Iter: 902 loss: 5.70754e-06
Iter: 903 loss: 5.70042039e-06
Iter: 904 loss: 5.71098508e-06
Iter: 905 loss: 5.69688473e-06
Iter: 906 loss: 5.69283065e-06
Iter: 907 loss: 5.69221766e-06
Iter: 908 loss: 5.68798259e-06
Iter: 909 loss: 5.71003693e-06
Iter: 910 loss: 5.68736687e-06
Iter: 911 loss: 5.68502946e-06
Iter: 912 loss: 5.67920597e-06
Iter: 913 loss: 5.71710916e-06
Iter: 914 loss: 5.67759116e-06
Iter: 915 loss: 5.66989638e-06
Iter: 916 loss: 5.6862109e-06
Iter: 917 loss: 5.66695508e-06
Iter: 918 loss: 5.6580966e-06
Iter: 919 loss: 5.69007807e-06
Iter: 920 loss: 5.65545952e-06
Iter: 921 loss: 5.64865468e-06
Iter: 922 loss: 5.65658365e-06
Iter: 923 loss: 5.64526545e-06
Iter: 924 loss: 5.63705635e-06
Iter: 925 loss: 5.6593417e-06
Iter: 926 loss: 5.6343215e-06
Iter: 927 loss: 5.6275112e-06
Iter: 928 loss: 5.66854442e-06
Iter: 929 loss: 5.62679315e-06
Iter: 930 loss: 5.62114292e-06
Iter: 931 loss: 5.65847631e-06
Iter: 932 loss: 5.62066452e-06
Iter: 933 loss: 5.61499e-06
Iter: 934 loss: 5.62817058e-06
Iter: 935 loss: 5.61313527e-06
Iter: 936 loss: 5.60856552e-06
Iter: 937 loss: 5.61109118e-06
Iter: 938 loss: 5.60563512e-06
Iter: 939 loss: 5.6017011e-06
Iter: 940 loss: 5.64365473e-06
Iter: 941 loss: 5.60143644e-06
Iter: 942 loss: 5.59742421e-06
Iter: 943 loss: 5.62259629e-06
Iter: 944 loss: 5.59709042e-06
Iter: 945 loss: 5.59379805e-06
Iter: 946 loss: 5.58553438e-06
Iter: 947 loss: 5.6359072e-06
Iter: 948 loss: 5.58312604e-06
Iter: 949 loss: 5.57609246e-06
Iter: 950 loss: 5.60526087e-06
Iter: 951 loss: 5.57439398e-06
Iter: 952 loss: 5.56833629e-06
Iter: 953 loss: 5.59981709e-06
Iter: 954 loss: 5.56736904e-06
Iter: 955 loss: 5.56163104e-06
Iter: 956 loss: 5.55730639e-06
Iter: 957 loss: 5.55519e-06
Iter: 958 loss: 5.54727103e-06
Iter: 959 loss: 5.60113949e-06
Iter: 960 loss: 5.54635108e-06
Iter: 961 loss: 5.54081498e-06
Iter: 962 loss: 5.55057431e-06
Iter: 963 loss: 5.53858717e-06
Iter: 964 loss: 5.53175505e-06
Iter: 965 loss: 5.575e-06
Iter: 966 loss: 5.53071368e-06
Iter: 967 loss: 5.52545e-06
Iter: 968 loss: 5.57486828e-06
Iter: 969 loss: 5.52513e-06
Iter: 970 loss: 5.5221908e-06
Iter: 971 loss: 5.5190435e-06
Iter: 972 loss: 5.51844641e-06
Iter: 973 loss: 5.51387257e-06
Iter: 974 loss: 5.53768405e-06
Iter: 975 loss: 5.51317771e-06
Iter: 976 loss: 5.50993e-06
Iter: 977 loss: 5.50956884e-06
Iter: 978 loss: 5.50778714e-06
Iter: 979 loss: 5.50346704e-06
Iter: 980 loss: 5.54370763e-06
Iter: 981 loss: 5.50277946e-06
Iter: 982 loss: 5.49868946e-06
Iter: 983 loss: 5.50110099e-06
Iter: 984 loss: 5.49593642e-06
Iter: 985 loss: 5.49073275e-06
Iter: 986 loss: 5.50839286e-06
Iter: 987 loss: 5.48949629e-06
Iter: 988 loss: 5.4845359e-06
Iter: 989 loss: 5.50554114e-06
Iter: 990 loss: 5.48325e-06
Iter: 991 loss: 5.47886248e-06
Iter: 992 loss: 5.48004527e-06
Iter: 993 loss: 5.47575837e-06
Iter: 994 loss: 5.46991396e-06
Iter: 995 loss: 5.48067146e-06
Iter: 996 loss: 5.46765614e-06
Iter: 997 loss: 5.46371666e-06
Iter: 998 loss: 5.46378942e-06
Iter: 999 loss: 5.46080355e-06
Iter: 1000 loss: 5.48144226e-06
Iter: 1001 loss: 5.46061801e-06
Iter: 1002 loss: 5.45807234e-06
Iter: 1003 loss: 5.45578041e-06
Iter: 1004 loss: 5.45539024e-06
Iter: 1005 loss: 5.45152034e-06
Iter: 1006 loss: 5.46222236e-06
Iter: 1007 loss: 5.45013609e-06
Iter: 1008 loss: 5.45221701e-06
Iter: 1009 loss: 5.44887e-06
Iter: 1010 loss: 5.44801651e-06
Iter: 1011 loss: 5.44539e-06
Iter: 1012 loss: 5.4542561e-06
Iter: 1013 loss: 5.44420891e-06
Iter: 1014 loss: 5.440309e-06
Iter: 1015 loss: 5.44318664e-06
Iter: 1016 loss: 5.43776969e-06
Iter: 1017 loss: 5.43435817e-06
Iter: 1018 loss: 5.43879742e-06
Iter: 1019 loss: 5.43258056e-06
Iter: 1020 loss: 5.42838188e-06
Iter: 1021 loss: 5.4501229e-06
Iter: 1022 loss: 5.42759472e-06
Iter: 1023 loss: 5.42351518e-06
Iter: 1024 loss: 5.42787939e-06
Iter: 1025 loss: 5.42157341e-06
Iter: 1026 loss: 5.41790587e-06
Iter: 1027 loss: 5.42865564e-06
Iter: 1028 loss: 5.41693771e-06
Iter: 1029 loss: 5.41314694e-06
Iter: 1030 loss: 5.41809277e-06
Iter: 1031 loss: 5.41143709e-06
Iter: 1032 loss: 5.40872907e-06
Iter: 1033 loss: 5.40861856e-06
Iter: 1034 loss: 5.40626e-06
Iter: 1035 loss: 5.40587e-06
Iter: 1036 loss: 5.40434758e-06
Iter: 1037 loss: 5.40169913e-06
Iter: 1038 loss: 5.40529027e-06
Iter: 1039 loss: 5.40072506e-06
Iter: 1040 loss: 5.39987104e-06
Iter: 1041 loss: 5.39934035e-06
Iter: 1042 loss: 5.39771145e-06
Iter: 1043 loss: 5.39427583e-06
Iter: 1044 loss: 5.45669172e-06
Iter: 1045 loss: 5.39437315e-06
Iter: 1046 loss: 5.39217217e-06
Iter: 1047 loss: 5.39688608e-06
Iter: 1048 loss: 5.3912745e-06
Iter: 1049 loss: 5.38918539e-06
Iter: 1050 loss: 5.38691938e-06
Iter: 1051 loss: 5.38657969e-06
Iter: 1052 loss: 5.38305176e-06
Iter: 1053 loss: 5.40300698e-06
Iter: 1054 loss: 5.38254699e-06
Iter: 1055 loss: 5.38006452e-06
Iter: 1056 loss: 5.39548546e-06
Iter: 1057 loss: 5.3795693e-06
Iter: 1058 loss: 5.37710912e-06
Iter: 1059 loss: 5.37406049e-06
Iter: 1060 loss: 5.37369488e-06
Iter: 1061 loss: 5.36947209e-06
Iter: 1062 loss: 5.39288612e-06
Iter: 1063 loss: 5.36891139e-06
Iter: 1064 loss: 5.36706557e-06
Iter: 1065 loss: 5.36706375e-06
Iter: 1066 loss: 5.36506377e-06
Iter: 1067 loss: 5.36533389e-06
Iter: 1068 loss: 5.36366406e-06
Iter: 1069 loss: 5.36131665e-06
Iter: 1070 loss: 5.36505513e-06
Iter: 1071 loss: 5.36023072e-06
Iter: 1072 loss: 5.35933123e-06
Iter: 1073 loss: 5.35906793e-06
Iter: 1074 loss: 5.35776235e-06
Iter: 1075 loss: 5.3581507e-06
Iter: 1076 loss: 5.35671461e-06
Iter: 1077 loss: 5.35567233e-06
Iter: 1078 loss: 5.35379786e-06
Iter: 1079 loss: 5.3537442e-06
Iter: 1080 loss: 5.35116033e-06
Iter: 1081 loss: 5.35364688e-06
Iter: 1082 loss: 5.34959554e-06
Iter: 1083 loss: 5.34698893e-06
Iter: 1084 loss: 5.35575236e-06
Iter: 1085 loss: 5.34614719e-06
Iter: 1086 loss: 5.34312312e-06
Iter: 1087 loss: 5.34759238e-06
Iter: 1088 loss: 5.34172932e-06
Iter: 1089 loss: 5.33836283e-06
Iter: 1090 loss: 5.36186189e-06
Iter: 1091 loss: 5.3380918e-06
Iter: 1092 loss: 5.33607226e-06
Iter: 1093 loss: 5.33544971e-06
Iter: 1094 loss: 5.33406728e-06
Iter: 1095 loss: 5.33034927e-06
Iter: 1096 loss: 5.33837192e-06
Iter: 1097 loss: 5.3291069e-06
Iter: 1098 loss: 5.32711329e-06
Iter: 1099 loss: 5.32658896e-06
Iter: 1100 loss: 5.32523063e-06
Iter: 1101 loss: 5.32207605e-06
Iter: 1102 loss: 5.36552625e-06
Iter: 1103 loss: 5.32190325e-06
Iter: 1104 loss: 5.31954083e-06
Iter: 1105 loss: 5.31936394e-06
Iter: 1106 loss: 5.31678779e-06
Iter: 1107 loss: 5.33168941e-06
Iter: 1108 loss: 5.31660635e-06
Iter: 1109 loss: 5.31524893e-06
Iter: 1110 loss: 5.31258365e-06
Iter: 1111 loss: 5.35257095e-06
Iter: 1112 loss: 5.31242449e-06
Iter: 1113 loss: 5.30943544e-06
Iter: 1114 loss: 5.3133308e-06
Iter: 1115 loss: 5.30818534e-06
Iter: 1116 loss: 5.30448597e-06
Iter: 1117 loss: 5.31216847e-06
Iter: 1118 loss: 5.30324633e-06
Iter: 1119 loss: 5.29972885e-06
Iter: 1120 loss: 5.30645639e-06
Iter: 1121 loss: 5.2980331e-06
Iter: 1122 loss: 5.29525551e-06
Iter: 1123 loss: 5.31389105e-06
Iter: 1124 loss: 5.29477438e-06
Iter: 1125 loss: 5.29144791e-06
Iter: 1126 loss: 5.29186309e-06
Iter: 1127 loss: 5.28901637e-06
Iter: 1128 loss: 5.28524924e-06
Iter: 1129 loss: 5.2951e-06
Iter: 1130 loss: 5.28414421e-06
Iter: 1131 loss: 5.28229248e-06
Iter: 1132 loss: 5.28205783e-06
Iter: 1133 loss: 5.27976863e-06
Iter: 1134 loss: 5.27526618e-06
Iter: 1135 loss: 5.35024265e-06
Iter: 1136 loss: 5.27516113e-06
Iter: 1137 loss: 5.27251632e-06
Iter: 1138 loss: 5.27241627e-06
Iter: 1139 loss: 5.27105e-06
Iter: 1140 loss: 5.27088787e-06
Iter: 1141 loss: 5.2701389e-06
Iter: 1142 loss: 5.2682517e-06
Iter: 1143 loss: 5.27426255e-06
Iter: 1144 loss: 5.26708936e-06
Iter: 1145 loss: 5.26347094e-06
Iter: 1146 loss: 5.26397298e-06
Iter: 1147 loss: 5.26101803e-06
Iter: 1148 loss: 5.25688847e-06
Iter: 1149 loss: 5.29210183e-06
Iter: 1150 loss: 5.2567093e-06
Iter: 1151 loss: 5.25439509e-06
Iter: 1152 loss: 5.25656833e-06
Iter: 1153 loss: 5.25287214e-06
Iter: 1154 loss: 5.2499181e-06
Iter: 1155 loss: 5.25530368e-06
Iter: 1156 loss: 5.24851202e-06
Iter: 1157 loss: 5.24499774e-06
Iter: 1158 loss: 5.26417807e-06
Iter: 1159 loss: 5.24443e-06
Iter: 1160 loss: 5.24145526e-06
Iter: 1161 loss: 5.24707957e-06
Iter: 1162 loss: 5.24038705e-06
Iter: 1163 loss: 5.23759081e-06
Iter: 1164 loss: 5.24254619e-06
Iter: 1165 loss: 5.23616472e-06
Iter: 1166 loss: 5.2330065e-06
Iter: 1167 loss: 5.27867178e-06
Iter: 1168 loss: 5.23309973e-06
Iter: 1169 loss: 5.23158633e-06
Iter: 1170 loss: 5.22925e-06
Iter: 1171 loss: 5.22913888e-06
Iter: 1172 loss: 5.23115432e-06
Iter: 1173 loss: 5.22844311e-06
Iter: 1174 loss: 5.22760774e-06
Iter: 1175 loss: 5.22497521e-06
Iter: 1176 loss: 5.22754e-06
Iter: 1177 loss: 5.22288519e-06
Iter: 1178 loss: 5.21908e-06
Iter: 1179 loss: 5.24695815e-06
Iter: 1180 loss: 5.21877701e-06
Iter: 1181 loss: 5.21650236e-06
Iter: 1182 loss: 5.21805396e-06
Iter: 1183 loss: 5.21516813e-06
Iter: 1184 loss: 5.2121668e-06
Iter: 1185 loss: 5.21610582e-06
Iter: 1186 loss: 5.21046786e-06
Iter: 1187 loss: 5.20735421e-06
Iter: 1188 loss: 5.22322716e-06
Iter: 1189 loss: 5.2069081e-06
Iter: 1190 loss: 5.20407048e-06
Iter: 1191 loss: 5.20348931e-06
Iter: 1192 loss: 5.20174672e-06
Iter: 1193 loss: 5.19862897e-06
Iter: 1194 loss: 5.22915798e-06
Iter: 1195 loss: 5.19865489e-06
Iter: 1196 loss: 5.19629066e-06
Iter: 1197 loss: 5.2000687e-06
Iter: 1198 loss: 5.19487958e-06
Iter: 1199 loss: 5.19278728e-06
Iter: 1200 loss: 5.21522952e-06
Iter: 1201 loss: 5.19275909e-06
Iter: 1202 loss: 5.19058813e-06
Iter: 1203 loss: 5.18978868e-06
Iter: 1204 loss: 5.18873958e-06
Iter: 1205 loss: 5.18693923e-06
Iter: 1206 loss: 5.20395861e-06
Iter: 1207 loss: 5.18684192e-06
Iter: 1208 loss: 5.18450452e-06
Iter: 1209 loss: 5.19403284e-06
Iter: 1210 loss: 5.1840716e-06
Iter: 1211 loss: 5.18305114e-06
Iter: 1212 loss: 5.18056459e-06
Iter: 1213 loss: 5.19194145e-06
Iter: 1214 loss: 5.17941407e-06
Iter: 1215 loss: 5.17608169e-06
Iter: 1216 loss: 5.20170806e-06
Iter: 1217 loss: 5.17585431e-06
Iter: 1218 loss: 5.17350645e-06
Iter: 1219 loss: 5.18172146e-06
Iter: 1220 loss: 5.17275294e-06
Iter: 1221 loss: 5.1703978e-06
Iter: 1222 loss: 5.17105764e-06
Iter: 1223 loss: 5.16867112e-06
Iter: 1224 loss: 5.16602358e-06
Iter: 1225 loss: 5.18494e-06
Iter: 1226 loss: 5.165728e-06
Iter: 1227 loss: 5.16368709e-06
Iter: 1228 loss: 5.16374303e-06
Iter: 1229 loss: 5.16196087e-06
Iter: 1230 loss: 5.15837473e-06
Iter: 1231 loss: 5.16466844e-06
Iter: 1232 loss: 5.15696865e-06
Iter: 1233 loss: 5.15395323e-06
Iter: 1234 loss: 5.17886929e-06
Iter: 1235 loss: 5.15381407e-06
Iter: 1236 loss: 5.15095553e-06
Iter: 1237 loss: 5.15493139e-06
Iter: 1238 loss: 5.14963631e-06
Iter: 1239 loss: 5.14709973e-06
Iter: 1240 loss: 5.14703243e-06
Iter: 1241 loss: 5.14542808e-06
Iter: 1242 loss: 5.14158364e-06
Iter: 1243 loss: 5.19985269e-06
Iter: 1244 loss: 5.14126123e-06
Iter: 1245 loss: 5.14269414e-06
Iter: 1246 loss: 5.14018393e-06
Iter: 1247 loss: 5.13889245e-06
Iter: 1248 loss: 5.13601799e-06
Iter: 1249 loss: 5.19546029e-06
Iter: 1250 loss: 5.13600025e-06
Iter: 1251 loss: 5.13409486e-06
Iter: 1252 loss: 5.13324449e-06
Iter: 1253 loss: 5.1323168e-06
Iter: 1254 loss: 5.1292418e-06
Iter: 1255 loss: 5.13684336e-06
Iter: 1256 loss: 5.12806309e-06
Iter: 1257 loss: 5.12446786e-06
Iter: 1258 loss: 5.1271727e-06
Iter: 1259 loss: 5.12257793e-06
Iter: 1260 loss: 5.11898315e-06
Iter: 1261 loss: 5.13319219e-06
Iter: 1262 loss: 5.11835242e-06
Iter: 1263 loss: 5.11441795e-06
Iter: 1264 loss: 5.11678945e-06
Iter: 1265 loss: 5.11200597e-06
Iter: 1266 loss: 5.10803648e-06
Iter: 1267 loss: 5.14321937e-06
Iter: 1268 loss: 5.10761856e-06
Iter: 1269 loss: 5.10408699e-06
Iter: 1270 loss: 5.10409609e-06
Iter: 1271 loss: 5.10150039e-06
Iter: 1272 loss: 5.09660276e-06
Iter: 1273 loss: 5.11951293e-06
Iter: 1274 loss: 5.09582742e-06
Iter: 1275 loss: 5.09248093e-06
Iter: 1276 loss: 5.12027691e-06
Iter: 1277 loss: 5.09224174e-06
Iter: 1278 loss: 5.09008169e-06
Iter: 1279 loss: 5.10264e-06
Iter: 1280 loss: 5.08974517e-06
Iter: 1281 loss: 5.08755465e-06
Iter: 1282 loss: 5.10416248e-06
Iter: 1283 loss: 5.08739959e-06
Iter: 1284 loss: 5.08499943e-06
Iter: 1285 loss: 5.08730091e-06
Iter: 1286 loss: 5.0836943e-06
Iter: 1287 loss: 5.08252879e-06
Iter: 1288 loss: 5.0798817e-06
Iter: 1289 loss: 5.1146817e-06
Iter: 1290 loss: 5.0797762e-06
Iter: 1291 loss: 5.07637196e-06
Iter: 1292 loss: 5.08477478e-06
Iter: 1293 loss: 5.07553523e-06
Iter: 1294 loss: 5.07162076e-06
Iter: 1295 loss: 5.08777e-06
Iter: 1296 loss: 5.07087407e-06
Iter: 1297 loss: 5.06819288e-06
Iter: 1298 loss: 5.0671274e-06
Iter: 1299 loss: 5.06575952e-06
Iter: 1300 loss: 5.06152173e-06
Iter: 1301 loss: 5.07726e-06
Iter: 1302 loss: 5.06058e-06
Iter: 1303 loss: 5.05700064e-06
Iter: 1304 loss: 5.07486493e-06
Iter: 1305 loss: 5.0562694e-06
Iter: 1306 loss: 5.05327534e-06
Iter: 1307 loss: 5.06079232e-06
Iter: 1308 loss: 5.05226944e-06
Iter: 1309 loss: 5.04894706e-06
Iter: 1310 loss: 5.05706385e-06
Iter: 1311 loss: 5.04777154e-06
Iter: 1312 loss: 5.04457557e-06
Iter: 1313 loss: 5.05679691e-06
Iter: 1314 loss: 5.0441231e-06
Iter: 1315 loss: 5.04113495e-06
Iter: 1316 loss: 5.05399e-06
Iter: 1317 loss: 5.04070067e-06
Iter: 1318 loss: 5.03867523e-06
Iter: 1319 loss: 5.03876754e-06
Iter: 1320 loss: 5.03709134e-06
Iter: 1321 loss: 5.04329273e-06
Iter: 1322 loss: 5.03658521e-06
Iter: 1323 loss: 5.03535102e-06
Iter: 1324 loss: 5.0328631e-06
Iter: 1325 loss: 5.07508776e-06
Iter: 1326 loss: 5.03292267e-06
Iter: 1327 loss: 5.03068304e-06
Iter: 1328 loss: 5.02858711e-06
Iter: 1329 loss: 5.02799139e-06
Iter: 1330 loss: 5.02470402e-06
Iter: 1331 loss: 5.0678027e-06
Iter: 1332 loss: 5.02470266e-06
Iter: 1333 loss: 5.02249e-06
Iter: 1334 loss: 5.02213106e-06
Iter: 1335 loss: 5.02078819e-06
Iter: 1336 loss: 5.01747945e-06
Iter: 1337 loss: 5.0186045e-06
Iter: 1338 loss: 5.01527711e-06
Iter: 1339 loss: 5.01172281e-06
Iter: 1340 loss: 5.03935189e-06
Iter: 1341 loss: 5.01152e-06
Iter: 1342 loss: 5.00838814e-06
Iter: 1343 loss: 5.01266277e-06
Iter: 1344 loss: 5.00668057e-06
Iter: 1345 loss: 5.00401666e-06
Iter: 1346 loss: 5.02614421e-06
Iter: 1347 loss: 5.00371243e-06
Iter: 1348 loss: 5.00149508e-06
Iter: 1349 loss: 5.00434408e-06
Iter: 1350 loss: 5.00043689e-06
Iter: 1351 loss: 4.9984119e-06
Iter: 1352 loss: 5.02454077e-06
Iter: 1353 loss: 4.99833641e-06
Iter: 1354 loss: 4.99679709e-06
Iter: 1355 loss: 5.01741852e-06
Iter: 1356 loss: 4.99698763e-06
Iter: 1357 loss: 4.99574344e-06
Iter: 1358 loss: 4.99577254e-06
Iter: 1359 loss: 4.99495491e-06
Iter: 1360 loss: 4.99342059e-06
Iter: 1361 loss: 4.99127145e-06
Iter: 1362 loss: 4.99119278e-06
Iter: 1363 loss: 4.98887221e-06
Iter: 1364 loss: 4.98920599e-06
Iter: 1365 loss: 4.98702684e-06
Iter: 1366 loss: 4.98406416e-06
Iter: 1367 loss: 5.00202168e-06
Iter: 1368 loss: 4.98359e-06
Iter: 1369 loss: 4.98126974e-06
Iter: 1370 loss: 4.98903637e-06
Iter: 1371 loss: 4.98045029e-06
Iter: 1372 loss: 4.97833025e-06
Iter: 1373 loss: 4.97905876e-06
Iter: 1374 loss: 4.97673864e-06
Iter: 1375 loss: 4.97415931e-06
Iter: 1376 loss: 4.9766104e-06
Iter: 1377 loss: 4.97270412e-06
Iter: 1378 loss: 4.96875236e-06
Iter: 1379 loss: 4.97861674e-06
Iter: 1380 loss: 4.9675582e-06
Iter: 1381 loss: 4.96346502e-06
Iter: 1382 loss: 4.991678e-06
Iter: 1383 loss: 4.96323173e-06
Iter: 1384 loss: 4.96067332e-06
Iter: 1385 loss: 4.96686243e-06
Iter: 1386 loss: 4.9598566e-06
Iter: 1387 loss: 4.95737368e-06
Iter: 1388 loss: 4.97137626e-06
Iter: 1389 loss: 4.95699214e-06
Iter: 1390 loss: 4.95520362e-06
Iter: 1391 loss: 4.95521e-06
Iter: 1392 loss: 4.95376435e-06
Iter: 1393 loss: 4.95370796e-06
Iter: 1394 loss: 4.95284894e-06
Iter: 1395 loss: 4.95142513e-06
Iter: 1396 loss: 4.95126369e-06
Iter: 1397 loss: 4.95038512e-06
Iter: 1398 loss: 4.9484488e-06
Iter: 1399 loss: 4.94528877e-06
Iter: 1400 loss: 4.94513233e-06
Iter: 1401 loss: 4.94187907e-06
Iter: 1402 loss: 4.97270321e-06
Iter: 1403 loss: 4.94183814e-06
Iter: 1404 loss: 4.93903917e-06
Iter: 1405 loss: 4.94088727e-06
Iter: 1406 loss: 4.93718562e-06
Iter: 1407 loss: 4.93328298e-06
Iter: 1408 loss: 4.94259484e-06
Iter: 1409 loss: 4.93172865e-06
Iter: 1410 loss: 4.92888921e-06
Iter: 1411 loss: 4.93650259e-06
Iter: 1412 loss: 4.92785421e-06
Iter: 1413 loss: 4.92501e-06
Iter: 1414 loss: 4.92905929e-06
Iter: 1415 loss: 4.92348045e-06
Iter: 1416 loss: 4.92070922e-06
Iter: 1417 loss: 4.94270216e-06
Iter: 1418 loss: 4.92037907e-06
Iter: 1419 loss: 4.91799847e-06
Iter: 1420 loss: 4.92391609e-06
Iter: 1421 loss: 4.91693845e-06
Iter: 1422 loss: 4.91547826e-06
Iter: 1423 loss: 4.91533683e-06
Iter: 1424 loss: 4.91360061e-06
Iter: 1425 loss: 4.91765286e-06
Iter: 1426 loss: 4.91307355e-06
Iter: 1427 loss: 4.91169794e-06
Iter: 1428 loss: 4.91157061e-06
Iter: 1429 loss: 4.91041237e-06
Iter: 1430 loss: 4.90835782e-06
Iter: 1431 loss: 4.90787534e-06
Iter: 1432 loss: 4.90677576e-06
Iter: 1433 loss: 4.90400453e-06
Iter: 1434 loss: 4.90581533e-06
Iter: 1435 loss: 4.90230377e-06
Iter: 1436 loss: 4.89935519e-06
Iter: 1437 loss: 4.90744378e-06
Iter: 1438 loss: 4.89855029e-06
Iter: 1439 loss: 4.89576314e-06
Iter: 1440 loss: 4.90420598e-06
Iter: 1441 loss: 4.89479e-06
Iter: 1442 loss: 4.89105923e-06
Iter: 1443 loss: 4.89144622e-06
Iter: 1444 loss: 4.88806654e-06
Iter: 1445 loss: 4.88421847e-06
Iter: 1446 loss: 4.90548928e-06
Iter: 1447 loss: 4.88349542e-06
Iter: 1448 loss: 4.88023443e-06
Iter: 1449 loss: 4.87951638e-06
Iter: 1450 loss: 4.87731631e-06
Iter: 1451 loss: 4.87343641e-06
Iter: 1452 loss: 4.91215906e-06
Iter: 1453 loss: 4.87317493e-06
Iter: 1454 loss: 4.87044326e-06
Iter: 1455 loss: 4.87480793e-06
Iter: 1456 loss: 4.86872341e-06
Iter: 1457 loss: 4.86604358e-06
Iter: 1458 loss: 4.91171068e-06
Iter: 1459 loss: 4.86606905e-06
Iter: 1460 loss: 4.86377257e-06
Iter: 1461 loss: 4.89316199e-06
Iter: 1462 loss: 4.86381e-06
Iter: 1463 loss: 4.86300678e-06
Iter: 1464 loss: 4.8623092e-06
Iter: 1465 loss: 4.86196905e-06
Iter: 1466 loss: 4.8602e-06
Iter: 1467 loss: 4.85732289e-06
Iter: 1468 loss: 4.8572515e-06
Iter: 1469 loss: 4.85439796e-06
Iter: 1470 loss: 4.8711272e-06
Iter: 1471 loss: 4.85387e-06
Iter: 1472 loss: 4.85144574e-06
Iter: 1473 loss: 4.85072087e-06
Iter: 1474 loss: 4.8488946e-06
Iter: 1475 loss: 4.84575276e-06
Iter: 1476 loss: 4.85865758e-06
Iter: 1477 loss: 4.84491056e-06
Iter: 1478 loss: 4.84196789e-06
Iter: 1479 loss: 4.85726559e-06
Iter: 1480 loss: 4.84130805e-06
Iter: 1481 loss: 4.83879467e-06
Iter: 1482 loss: 4.83826898e-06
Iter: 1483 loss: 4.83654412e-06
Iter: 1484 loss: 4.83238182e-06
Iter: 1485 loss: 4.84314842e-06
Iter: 1486 loss: 4.83135136e-06
Iter: 1487 loss: 4.82837459e-06
Iter: 1488 loss: 4.83573331e-06
Iter: 1489 loss: 4.82702308e-06
Iter: 1490 loss: 4.82338146e-06
Iter: 1491 loss: 4.83207032e-06
Iter: 1492 loss: 4.82185897e-06
Iter: 1493 loss: 4.82099585e-06
Iter: 1494 loss: 4.81990173e-06
Iter: 1495 loss: 4.81798497e-06
Iter: 1496 loss: 4.82186624e-06
Iter: 1497 loss: 4.81695452e-06
Iter: 1498 loss: 4.81581e-06
Iter: 1499 loss: 4.81461666e-06
Iter: 1500 loss: 4.81432562e-06
Iter: 1501 loss: 4.81195912e-06
Iter: 1502 loss: 4.81821e-06
Iter: 1503 loss: 4.81135976e-06
Iter: 1504 loss: 4.80859126e-06
Iter: 1505 loss: 4.80513654e-06
Iter: 1506 loss: 4.80500876e-06
Iter: 1507 loss: 4.8009324e-06
Iter: 1508 loss: 4.83519398e-06
Iter: 1509 loss: 4.80074232e-06
Iter: 1510 loss: 4.79740265e-06
Iter: 1511 loss: 4.80059634e-06
Iter: 1512 loss: 4.79563369e-06
Iter: 1513 loss: 4.79221035e-06
Iter: 1514 loss: 4.81076768e-06
Iter: 1515 loss: 4.79183382e-06
Iter: 1516 loss: 4.78898528e-06
Iter: 1517 loss: 4.79467508e-06
Iter: 1518 loss: 4.7877993e-06
Iter: 1519 loss: 4.78429411e-06
Iter: 1520 loss: 4.78769562e-06
Iter: 1521 loss: 4.78252741e-06
Iter: 1522 loss: 4.77970752e-06
Iter: 1523 loss: 4.78727361e-06
Iter: 1524 loss: 4.77873073e-06
Iter: 1525 loss: 4.7756007e-06
Iter: 1526 loss: 4.78003676e-06
Iter: 1527 loss: 4.77391677e-06
Iter: 1528 loss: 4.78101265e-06
Iter: 1529 loss: 4.77303183e-06
Iter: 1530 loss: 4.77260573e-06
Iter: 1531 loss: 4.77114418e-06
Iter: 1532 loss: 4.77458525e-06
Iter: 1533 loss: 4.77015146e-06
Iter: 1534 loss: 4.76794139e-06
Iter: 1535 loss: 4.78502761e-06
Iter: 1536 loss: 4.767664e-06
Iter: 1537 loss: 4.76559762e-06
Iter: 1538 loss: 4.76636887e-06
Iter: 1539 loss: 4.76435571e-06
Iter: 1540 loss: 4.76276e-06
Iter: 1541 loss: 4.76860214e-06
Iter: 1542 loss: 4.76231526e-06
Iter: 1543 loss: 4.76042214e-06
Iter: 1544 loss: 4.7582962e-06
Iter: 1545 loss: 4.75806519e-06
Iter: 1546 loss: 4.75463912e-06
Iter: 1547 loss: 4.77333106e-06
Iter: 1548 loss: 4.75424486e-06
Iter: 1549 loss: 4.75215165e-06
Iter: 1550 loss: 4.75989873e-06
Iter: 1551 loss: 4.75185152e-06
Iter: 1552 loss: 4.74950957e-06
Iter: 1553 loss: 4.75362731e-06
Iter: 1554 loss: 4.74842273e-06
Iter: 1555 loss: 4.74621947e-06
Iter: 1556 loss: 4.7493354e-06
Iter: 1557 loss: 4.74517583e-06
Iter: 1558 loss: 4.74266199e-06
Iter: 1559 loss: 4.75291927e-06
Iter: 1560 loss: 4.74203625e-06
Iter: 1561 loss: 4.74257695e-06
Iter: 1562 loss: 4.74149329e-06
Iter: 1563 loss: 4.74045555e-06
Iter: 1564 loss: 4.73914633e-06
Iter: 1565 loss: 4.7392723e-06
Iter: 1566 loss: 4.73794626e-06
Iter: 1567 loss: 4.73740783e-06
Iter: 1568 loss: 4.73668752e-06
Iter: 1569 loss: 4.73435648e-06
Iter: 1570 loss: 4.74487933e-06
Iter: 1571 loss: 4.73398677e-06
Iter: 1572 loss: 4.73250111e-06
Iter: 1573 loss: 4.7309004e-06
Iter: 1574 loss: 4.73078762e-06
Iter: 1575 loss: 4.72856664e-06
Iter: 1576 loss: 4.73888394e-06
Iter: 1577 loss: 4.72796637e-06
Iter: 1578 loss: 4.72574084e-06
Iter: 1579 loss: 4.7362646e-06
Iter: 1580 loss: 4.72536794e-06
Iter: 1581 loss: 4.72364673e-06
Iter: 1582 loss: 4.72371e-06
Iter: 1583 loss: 4.722227e-06
Iter: 1584 loss: 4.72006286e-06
Iter: 1585 loss: 4.73287128e-06
Iter: 1586 loss: 4.71985777e-06
Iter: 1587 loss: 4.71781732e-06
Iter: 1588 loss: 4.71785188e-06
Iter: 1589 loss: 4.71627664e-06
Iter: 1590 loss: 4.7136e-06
Iter: 1591 loss: 4.71998101e-06
Iter: 1592 loss: 4.71271e-06
Iter: 1593 loss: 4.71102521e-06
Iter: 1594 loss: 4.71074145e-06
Iter: 1595 loss: 4.70877876e-06
Iter: 1596 loss: 4.71758858e-06
Iter: 1597 loss: 4.70826e-06
Iter: 1598 loss: 4.70747727e-06
Iter: 1599 loss: 4.70603618e-06
Iter: 1600 loss: 4.70602208e-06
Iter: 1601 loss: 4.70482337e-06
Iter: 1602 loss: 4.72028842e-06
Iter: 1603 loss: 4.70459918e-06
Iter: 1604 loss: 4.70345913e-06
Iter: 1605 loss: 4.70168925e-06
Iter: 1606 loss: 4.7017038e-06
Iter: 1607 loss: 4.69981433e-06
Iter: 1608 loss: 4.70337909e-06
Iter: 1609 loss: 4.69901215e-06
Iter: 1610 loss: 4.69684892e-06
Iter: 1611 loss: 4.70137456e-06
Iter: 1612 loss: 4.69557199e-06
Iter: 1613 loss: 4.69279e-06
Iter: 1614 loss: 4.70580153e-06
Iter: 1615 loss: 4.69203e-06
Iter: 1616 loss: 4.6902469e-06
Iter: 1617 loss: 4.6875366e-06
Iter: 1618 loss: 4.6873e-06
Iter: 1619 loss: 4.68350845e-06
Iter: 1620 loss: 4.70375653e-06
Iter: 1621 loss: 4.68292728e-06
Iter: 1622 loss: 4.68014514e-06
Iter: 1623 loss: 4.70163604e-06
Iter: 1624 loss: 4.6800119e-06
Iter: 1625 loss: 4.67744576e-06
Iter: 1626 loss: 4.68113512e-06
Iter: 1627 loss: 4.67614973e-06
Iter: 1628 loss: 4.6788432e-06
Iter: 1629 loss: 4.67518566e-06
Iter: 1630 loss: 4.6745381e-06
Iter: 1631 loss: 4.67306e-06
Iter: 1632 loss: 4.67517657e-06
Iter: 1633 loss: 4.67209156e-06
Iter: 1634 loss: 4.67066457e-06
Iter: 1635 loss: 4.67067093e-06
Iter: 1636 loss: 4.66886331e-06
Iter: 1637 loss: 4.66768e-06
Iter: 1638 loss: 4.66722213e-06
Iter: 1639 loss: 4.66506344e-06
Iter: 1640 loss: 4.66694109e-06
Iter: 1641 loss: 4.66392703e-06
Iter: 1642 loss: 4.66127403e-06
Iter: 1643 loss: 4.66762549e-06
Iter: 1644 loss: 4.66062374e-06
Iter: 1645 loss: 4.6579612e-06
Iter: 1646 loss: 4.67303062e-06
Iter: 1647 loss: 4.65752737e-06
Iter: 1648 loss: 4.65542871e-06
Iter: 1649 loss: 4.65436187e-06
Iter: 1650 loss: 4.65328594e-06
Iter: 1651 loss: 4.65067114e-06
Iter: 1652 loss: 4.67466725e-06
Iter: 1653 loss: 4.65040466e-06
Iter: 1654 loss: 4.64859113e-06
Iter: 1655 loss: 4.64768891e-06
Iter: 1656 loss: 4.64672667e-06
Iter: 1657 loss: 4.64406185e-06
Iter: 1658 loss: 4.65923767e-06
Iter: 1659 loss: 4.64388722e-06
Iter: 1660 loss: 4.64259392e-06
Iter: 1661 loss: 4.65173e-06
Iter: 1662 loss: 4.64256846e-06
Iter: 1663 loss: 4.641126e-06
Iter: 1664 loss: 4.64719142e-06
Iter: 1665 loss: 4.64084951e-06
Iter: 1666 loss: 4.63988454e-06
Iter: 1667 loss: 4.63773904e-06
Iter: 1668 loss: 4.65205494e-06
Iter: 1669 loss: 4.63734114e-06
Iter: 1670 loss: 4.63496781e-06
Iter: 1671 loss: 4.64953064e-06
Iter: 1672 loss: 4.63461583e-06
Iter: 1673 loss: 4.63261722e-06
Iter: 1674 loss: 4.65519497e-06
Iter: 1675 loss: 4.63268043e-06
Iter: 1676 loss: 4.63162723e-06
Iter: 1677 loss: 4.62913886e-06
Iter: 1678 loss: 4.64689219e-06
Iter: 1679 loss: 4.62860817e-06
Iter: 1680 loss: 4.62630578e-06
Iter: 1681 loss: 4.65633275e-06
Iter: 1682 loss: 4.626183e-06
Iter: 1683 loss: 4.62461685e-06
Iter: 1684 loss: 4.6282621e-06
Iter: 1685 loss: 4.62394837e-06
Iter: 1686 loss: 4.6223663e-06
Iter: 1687 loss: 4.62389698e-06
Iter: 1688 loss: 4.62147818e-06
Iter: 1689 loss: 4.61967147e-06
Iter: 1690 loss: 4.62047774e-06
Iter: 1691 loss: 4.61839136e-06
Iter: 1692 loss: 4.61677519e-06
Iter: 1693 loss: 4.63080369e-06
Iter: 1694 loss: 4.61648415e-06
Iter: 1695 loss: 4.6157179e-06
Iter: 1696 loss: 4.62741355e-06
Iter: 1697 loss: 4.61559057e-06
Iter: 1698 loss: 4.61505351e-06
Iter: 1699 loss: 4.61990339e-06
Iter: 1700 loss: 4.61475702e-06
Iter: 1701 loss: 4.61422678e-06
Iter: 1702 loss: 4.61256241e-06
Iter: 1703 loss: 4.61887976e-06
Iter: 1704 loss: 4.61187028e-06
Iter: 1705 loss: 4.60984211e-06
Iter: 1706 loss: 4.61662967e-06
Iter: 1707 loss: 4.60914634e-06
Iter: 1708 loss: 4.60811e-06
Iter: 1709 loss: 4.60791125e-06
Iter: 1710 loss: 4.60705269e-06
Iter: 1711 loss: 4.60524279e-06
Iter: 1712 loss: 4.63598917e-06
Iter: 1713 loss: 4.60519868e-06
Iter: 1714 loss: 4.60285355e-06
Iter: 1715 loss: 4.60732326e-06
Iter: 1716 loss: 4.60191723e-06
Iter: 1717 loss: 4.59956436e-06
Iter: 1718 loss: 4.603668e-06
Iter: 1719 loss: 4.59847615e-06
Iter: 1720 loss: 4.59639614e-06
Iter: 1721 loss: 4.60642786e-06
Iter: 1722 loss: 4.59570219e-06
Iter: 1723 loss: 4.59350804e-06
Iter: 1724 loss: 4.59921193e-06
Iter: 1725 loss: 4.59247894e-06
Iter: 1726 loss: 4.59091279e-06
Iter: 1727 loss: 4.60044521e-06
Iter: 1728 loss: 4.59055173e-06
Iter: 1729 loss: 4.58967497e-06
Iter: 1730 loss: 4.58958766e-06
Iter: 1731 loss: 4.58863769e-06
Iter: 1732 loss: 4.59159946e-06
Iter: 1733 loss: 4.58846898e-06
Iter: 1734 loss: 4.58741852e-06
Iter: 1735 loss: 4.58536306e-06
Iter: 1736 loss: 4.62580738e-06
Iter: 1737 loss: 4.58540489e-06
Iter: 1738 loss: 4.58353543e-06
Iter: 1739 loss: 4.58369595e-06
Iter: 1740 loss: 4.58200429e-06
Iter: 1741 loss: 4.58085651e-06
Iter: 1742 loss: 4.58073555e-06
Iter: 1743 loss: 4.57947908e-06
Iter: 1744 loss: 4.57930582e-06
Iter: 1745 loss: 4.57816077e-06
Iter: 1746 loss: 4.57671149e-06
Iter: 1747 loss: 4.57699889e-06
Iter: 1748 loss: 4.57573515e-06
Iter: 1749 loss: 4.57378701e-06
Iter: 1750 loss: 4.57509668e-06
Iter: 1751 loss: 4.57228134e-06
Iter: 1752 loss: 4.57012902e-06
Iter: 1753 loss: 4.59027251e-06
Iter: 1754 loss: 4.57007627e-06
Iter: 1755 loss: 4.56814132e-06
Iter: 1756 loss: 4.56720909e-06
Iter: 1757 loss: 4.566386e-06
Iter: 1758 loss: 4.56301495e-06
Iter: 1759 loss: 4.58068189e-06
Iter: 1760 loss: 4.56251155e-06
Iter: 1761 loss: 4.56094176e-06
Iter: 1762 loss: 4.57522765e-06
Iter: 1763 loss: 4.56081398e-06
Iter: 1764 loss: 4.55937652e-06
Iter: 1765 loss: 4.55941517e-06
Iter: 1766 loss: 4.55860936e-06
Iter: 1767 loss: 4.55721511e-06
Iter: 1768 loss: 4.58663817e-06
Iter: 1769 loss: 4.55712825e-06
Iter: 1770 loss: 4.55550071e-06
Iter: 1771 loss: 4.55498366e-06
Iter: 1772 loss: 4.55393456e-06
Iter: 1773 loss: 4.55220561e-06
Iter: 1774 loss: 4.56107318e-06
Iter: 1775 loss: 4.55172358e-06
Iter: 1776 loss: 4.54937026e-06
Iter: 1777 loss: 4.55515601e-06
Iter: 1778 loss: 4.54857218e-06
Iter: 1779 loss: 4.54665042e-06
Iter: 1780 loss: 4.54791643e-06
Iter: 1781 loss: 4.54542e-06
Iter: 1782 loss: 4.54357314e-06
Iter: 1783 loss: 4.54518749e-06
Iter: 1784 loss: 4.54228393e-06
Iter: 1785 loss: 4.53984967e-06
Iter: 1786 loss: 4.54928249e-06
Iter: 1787 loss: 4.53912435e-06
Iter: 1788 loss: 4.53700386e-06
Iter: 1789 loss: 4.54709289e-06
Iter: 1790 loss: 4.53659686e-06
Iter: 1791 loss: 4.53440816e-06
Iter: 1792 loss: 4.53549546e-06
Iter: 1793 loss: 4.53310531e-06
Iter: 1794 loss: 4.53065695e-06
Iter: 1795 loss: 4.5509687e-06
Iter: 1796 loss: 4.53055327e-06
Iter: 1797 loss: 4.53048597e-06
Iter: 1798 loss: 4.52992572e-06
Iter: 1799 loss: 4.52948188e-06
Iter: 1800 loss: 4.52821359e-06
Iter: 1801 loss: 4.53853454e-06
Iter: 1802 loss: 4.52807262e-06
Iter: 1803 loss: 4.52632958e-06
Iter: 1804 loss: 4.52534323e-06
Iter: 1805 loss: 4.52467066e-06
Iter: 1806 loss: 4.52270524e-06
Iter: 1807 loss: 4.53269649e-06
Iter: 1808 loss: 4.52226232e-06
Iter: 1809 loss: 4.52091035e-06
Iter: 1810 loss: 4.53978782e-06
Iter: 1811 loss: 4.52091081e-06
Iter: 1812 loss: 4.51946516e-06
Iter: 1813 loss: 4.51677261e-06
Iter: 1814 loss: 4.57198939e-06
Iter: 1815 loss: 4.51669757e-06
Iter: 1816 loss: 4.51466349e-06
Iter: 1817 loss: 4.52296717e-06
Iter: 1818 loss: 4.51397864e-06
Iter: 1819 loss: 4.51141113e-06
Iter: 1820 loss: 4.51444748e-06
Iter: 1821 loss: 4.51017786e-06
Iter: 1822 loss: 4.50692323e-06
Iter: 1823 loss: 4.52423228e-06
Iter: 1824 loss: 4.5066e-06
Iter: 1825 loss: 4.50454627e-06
Iter: 1826 loss: 4.51406504e-06
Iter: 1827 loss: 4.50419611e-06
Iter: 1828 loss: 4.50205516e-06
Iter: 1829 loss: 4.50161588e-06
Iter: 1830 loss: 4.50048128e-06
Iter: 1831 loss: 4.503333e-06
Iter: 1832 loss: 4.4996018e-06
Iter: 1833 loss: 4.4989074e-06
Iter: 1834 loss: 4.49758227e-06
Iter: 1835 loss: 4.52521726e-06
Iter: 1836 loss: 4.49751406e-06
Iter: 1837 loss: 4.49652725e-06
Iter: 1838 loss: 4.49570689e-06
Iter: 1839 loss: 4.49528852e-06
Iter: 1840 loss: 4.49324534e-06
Iter: 1841 loss: 4.49557683e-06
Iter: 1842 loss: 4.49222e-06
Iter: 1843 loss: 4.49071104e-06
Iter: 1844 loss: 4.49054869e-06
Iter: 1845 loss: 4.48898027e-06
Iter: 1846 loss: 4.48845822e-06
Iter: 1847 loss: 4.48773062e-06
Iter: 1848 loss: 4.48562287e-06
Iter: 1849 loss: 4.48587434e-06
Iter: 1850 loss: 4.48418541e-06
Iter: 1851 loss: 4.481838e-06
Iter: 1852 loss: 4.4856738e-06
Iter: 1853 loss: 4.48091487e-06
Iter: 1854 loss: 4.47845559e-06
Iter: 1855 loss: 4.4940316e-06
Iter: 1856 loss: 4.47798675e-06
Iter: 1857 loss: 4.47602179e-06
Iter: 1858 loss: 4.48190895e-06
Iter: 1859 loss: 4.47537695e-06
Iter: 1860 loss: 4.4735325e-06
Iter: 1861 loss: 4.47952289e-06
Iter: 1862 loss: 4.4731205e-06
Iter: 1863 loss: 4.47173306e-06
Iter: 1864 loss: 4.48834e-06
Iter: 1865 loss: 4.47167167e-06
Iter: 1866 loss: 4.46989679e-06
Iter: 1867 loss: 4.47244565e-06
Iter: 1868 loss: 4.46899958e-06
Iter: 1869 loss: 4.46820104e-06
Iter: 1870 loss: 4.4669564e-06
Iter: 1871 loss: 4.46676859e-06
Iter: 1872 loss: 4.46542526e-06
Iter: 1873 loss: 4.46800414e-06
Iter: 1874 loss: 4.46485774e-06
Iter: 1875 loss: 4.46320246e-06
Iter: 1876 loss: 4.46481272e-06
Iter: 1877 loss: 4.46228114e-06
Iter: 1878 loss: 4.46117701e-06
Iter: 1879 loss: 4.46107333e-06
Iter: 1880 loss: 4.46044669e-06
Iter: 1881 loss: 4.45870091e-06
Iter: 1882 loss: 4.47200682e-06
Iter: 1883 loss: 4.45859496e-06
Iter: 1884 loss: 4.45621845e-06
Iter: 1885 loss: 4.46884269e-06
Iter: 1886 loss: 4.45601427e-06
Iter: 1887 loss: 4.45427486e-06
Iter: 1888 loss: 4.45832757e-06
Iter: 1889 loss: 4.45363594e-06
Iter: 1890 loss: 4.45160777e-06
Iter: 1891 loss: 4.45713704e-06
Iter: 1892 loss: 4.45097885e-06
Iter: 1893 loss: 4.44959e-06
Iter: 1894 loss: 4.46028207e-06
Iter: 1895 loss: 4.44943e-06
Iter: 1896 loss: 4.44847274e-06
Iter: 1897 loss: 4.44991929e-06
Iter: 1898 loss: 4.44777197e-06
Iter: 1899 loss: 4.44726447e-06
Iter: 1900 loss: 4.44702846e-06
Iter: 1901 loss: 4.44654052e-06
Iter: 1902 loss: 4.44514626e-06
Iter: 1903 loss: 4.44918669e-06
Iter: 1904 loss: 4.44420311e-06
Iter: 1905 loss: 4.4423723e-06
Iter: 1906 loss: 4.44609032e-06
Iter: 1907 loss: 4.4417452e-06
Iter: 1908 loss: 4.43979116e-06
Iter: 1909 loss: 4.44835223e-06
Iter: 1910 loss: 4.43945737e-06
Iter: 1911 loss: 4.43779118e-06
Iter: 1912 loss: 4.45181831e-06
Iter: 1913 loss: 4.43772205e-06
Iter: 1914 loss: 4.43682893e-06
Iter: 1915 loss: 4.44547e-06
Iter: 1916 loss: 4.43696172e-06
Iter: 1917 loss: 4.43635872e-06
Iter: 1918 loss: 4.43495537e-06
Iter: 1919 loss: 4.4408589e-06
Iter: 1920 loss: 4.434115e-06
Iter: 1921 loss: 4.43230419e-06
Iter: 1922 loss: 4.45281876e-06
Iter: 1923 loss: 4.43231147e-06
Iter: 1924 loss: 4.43107638e-06
Iter: 1925 loss: 4.43387262e-06
Iter: 1926 loss: 4.43056115e-06
Iter: 1927 loss: 4.42894725e-06
Iter: 1928 loss: 4.43187173e-06
Iter: 1929 loss: 4.42799956e-06
Iter: 1930 loss: 4.42680175e-06
Iter: 1931 loss: 4.43894805e-06
Iter: 1932 loss: 4.4268736e-06
Iter: 1933 loss: 4.42637884e-06
Iter: 1934 loss: 4.42632836e-06
Iter: 1935 loss: 4.42559212e-06
Iter: 1936 loss: 4.42473356e-06
Iter: 1937 loss: 4.42454211e-06
Iter: 1938 loss: 4.42396322e-06
Iter: 1939 loss: 4.42319379e-06
Iter: 1940 loss: 4.42286773e-06
Iter: 1941 loss: 4.42166538e-06
Iter: 1942 loss: 4.42207966e-06
Iter: 1943 loss: 4.42082637e-06
Iter: 1944 loss: 4.41923748e-06
Iter: 1945 loss: 4.43139152e-06
Iter: 1946 loss: 4.41896373e-06
Iter: 1947 loss: 4.41757447e-06
Iter: 1948 loss: 4.42861437e-06
Iter: 1949 loss: 4.41743578e-06
Iter: 1950 loss: 4.41627253e-06
Iter: 1951 loss: 4.41839e-06
Iter: 1952 loss: 4.41576549e-06
Iter: 1953 loss: 4.41460497e-06
Iter: 1954 loss: 4.41300244e-06
Iter: 1955 loss: 4.41304383e-06
Iter: 1956 loss: 4.41114798e-06
Iter: 1957 loss: 4.42034252e-06
Iter: 1958 loss: 4.41101975e-06
Iter: 1959 loss: 4.40946633e-06
Iter: 1960 loss: 4.40940494e-06
Iter: 1961 loss: 4.40815347e-06
Iter: 1962 loss: 4.40637405e-06
Iter: 1963 loss: 4.43304179e-06
Iter: 1964 loss: 4.406279e-06
Iter: 1965 loss: 4.40465919e-06
Iter: 1966 loss: 4.40549502e-06
Iter: 1967 loss: 4.40380609e-06
Iter: 1968 loss: 4.40219856e-06
Iter: 1969 loss: 4.40726399e-06
Iter: 1970 loss: 4.4017288e-06
Iter: 1971 loss: 4.40108806e-06
Iter: 1972 loss: 4.40062331e-06
Iter: 1973 loss: 4.40029453e-06
Iter: 1974 loss: 4.39943278e-06
Iter: 1975 loss: 4.41026623e-06
Iter: 1976 loss: 4.39928453e-06
Iter: 1977 loss: 4.39845189e-06
Iter: 1978 loss: 4.39835912e-06
Iter: 1979 loss: 4.39775249e-06
Iter: 1980 loss: 4.39644691e-06
Iter: 1981 loss: 4.39676296e-06
Iter: 1982 loss: 4.39529686e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.8
+ date
Mon Oct 26 13:56:10 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.8/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.8_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 1 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.8_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420742158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420706598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420706d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04207f6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420698400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04206b9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042067b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04205fe0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420614488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420614bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04205a7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04205bb7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04205bb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420577d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420577ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420517a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04204d6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04204f5158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04204557b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042045a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042045a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04204467b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04203c19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420446840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04204367b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420361bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042038f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042031f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420301488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042031f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04202fa730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420294158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04202877b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0420259bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042026fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042020e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.06899703e-05
Iter: 2 loss: 5.22236733e-05
Iter: 3 loss: 5.10481477e-05
Iter: 4 loss: 4.76803325e-05
Iter: 5 loss: 4.73741602e-05
Iter: 6 loss: 4.56972484e-05
Iter: 7 loss: 4.08545675e-05
Iter: 8 loss: 6.11777505e-05
Iter: 9 loss: 3.89650522e-05
Iter: 10 loss: 3.45161206e-05
Iter: 11 loss: 8.67658382e-05
Iter: 12 loss: 3.44497093e-05
Iter: 13 loss: 3.22433298e-05
Iter: 14 loss: 2.99632266e-05
Iter: 15 loss: 2.95452446e-05
Iter: 16 loss: 2.57375832e-05
Iter: 17 loss: 6.49804e-05
Iter: 18 loss: 2.56240655e-05
Iter: 19 loss: 2.40617555e-05
Iter: 20 loss: 3.3144097e-05
Iter: 21 loss: 2.38557241e-05
Iter: 22 loss: 2.24457e-05
Iter: 23 loss: 2.34780418e-05
Iter: 24 loss: 2.15787841e-05
Iter: 25 loss: 2.05849246e-05
Iter: 26 loss: 3.01519503e-05
Iter: 27 loss: 2.05469096e-05
Iter: 28 loss: 1.98018315e-05
Iter: 29 loss: 1.87449441e-05
Iter: 30 loss: 1.87071219e-05
Iter: 31 loss: 1.77604816e-05
Iter: 32 loss: 2.58644977e-05
Iter: 33 loss: 1.77087568e-05
Iter: 34 loss: 1.67989128e-05
Iter: 35 loss: 1.71488027e-05
Iter: 36 loss: 1.6167e-05
Iter: 37 loss: 1.748455e-05
Iter: 38 loss: 1.59514348e-05
Iter: 39 loss: 1.57769828e-05
Iter: 40 loss: 1.54935278e-05
Iter: 41 loss: 1.54919e-05
Iter: 42 loss: 1.51203476e-05
Iter: 43 loss: 1.44922396e-05
Iter: 44 loss: 1.4491472e-05
Iter: 45 loss: 1.40176162e-05
Iter: 46 loss: 1.84803212e-05
Iter: 47 loss: 1.39986041e-05
Iter: 48 loss: 1.35912851e-05
Iter: 49 loss: 1.39919121e-05
Iter: 50 loss: 1.33613084e-05
Iter: 51 loss: 1.29382679e-05
Iter: 52 loss: 1.71593929e-05
Iter: 53 loss: 1.29242471e-05
Iter: 54 loss: 1.26845625e-05
Iter: 55 loss: 1.26695359e-05
Iter: 56 loss: 1.24881471e-05
Iter: 57 loss: 1.2085864e-05
Iter: 58 loss: 1.3869063e-05
Iter: 59 loss: 1.20063378e-05
Iter: 60 loss: 1.17816462e-05
Iter: 61 loss: 1.32719388e-05
Iter: 62 loss: 1.17587606e-05
Iter: 63 loss: 1.15529965e-05
Iter: 64 loss: 1.11967456e-05
Iter: 65 loss: 1.11964218e-05
Iter: 66 loss: 1.08736331e-05
Iter: 67 loss: 1.47078454e-05
Iter: 68 loss: 1.0869293e-05
Iter: 69 loss: 1.06340904e-05
Iter: 70 loss: 1.07476953e-05
Iter: 71 loss: 1.04761284e-05
Iter: 72 loss: 1.04209339e-05
Iter: 73 loss: 1.03426992e-05
Iter: 74 loss: 1.02246941e-05
Iter: 75 loss: 1.08668992e-05
Iter: 76 loss: 1.0206847e-05
Iter: 77 loss: 1.01471505e-05
Iter: 78 loss: 9.97478764e-06
Iter: 79 loss: 1.07208125e-05
Iter: 80 loss: 9.90901754e-06
Iter: 81 loss: 9.71745703e-06
Iter: 82 loss: 1.13825045e-05
Iter: 83 loss: 9.70699875e-06
Iter: 84 loss: 9.58130568e-06
Iter: 85 loss: 9.483344e-06
Iter: 86 loss: 9.44391468e-06
Iter: 87 loss: 9.28923509e-06
Iter: 88 loss: 9.28685768e-06
Iter: 89 loss: 9.20211642e-06
Iter: 90 loss: 9.21384799e-06
Iter: 91 loss: 9.13791155e-06
Iter: 92 loss: 9.00596751e-06
Iter: 93 loss: 9.57585326e-06
Iter: 94 loss: 8.97886e-06
Iter: 95 loss: 8.87737951e-06
Iter: 96 loss: 9.42983479e-06
Iter: 97 loss: 8.86250382e-06
Iter: 98 loss: 8.76265949e-06
Iter: 99 loss: 8.61474473e-06
Iter: 100 loss: 8.61096396e-06
Iter: 101 loss: 8.49865864e-06
Iter: 102 loss: 1.02017e-05
Iter: 103 loss: 8.49857e-06
Iter: 104 loss: 8.39957738e-06
Iter: 105 loss: 8.33345803e-06
Iter: 106 loss: 8.29627152e-06
Iter: 107 loss: 8.47074443e-06
Iter: 108 loss: 8.25210645e-06
Iter: 109 loss: 8.22247239e-06
Iter: 110 loss: 8.16713327e-06
Iter: 111 loss: 9.38373887e-06
Iter: 112 loss: 8.1667913e-06
Iter: 113 loss: 8.11252085e-06
Iter: 114 loss: 8.04573e-06
Iter: 115 loss: 8.03973853e-06
Iter: 116 loss: 7.95587857e-06
Iter: 117 loss: 8.55547387e-06
Iter: 118 loss: 7.94851076e-06
Iter: 119 loss: 7.88520811e-06
Iter: 120 loss: 7.89134901e-06
Iter: 121 loss: 7.83625455e-06
Iter: 122 loss: 7.74194632e-06
Iter: 123 loss: 8.38394772e-06
Iter: 124 loss: 7.73279407e-06
Iter: 125 loss: 7.66923404e-06
Iter: 126 loss: 7.61672482e-06
Iter: 127 loss: 7.59839531e-06
Iter: 128 loss: 7.50813069e-06
Iter: 129 loss: 8.42202098e-06
Iter: 130 loss: 7.50537265e-06
Iter: 131 loss: 7.45831585e-06
Iter: 132 loss: 7.93647087e-06
Iter: 133 loss: 7.45647458e-06
Iter: 134 loss: 7.4125237e-06
Iter: 135 loss: 7.33697107e-06
Iter: 136 loss: 7.33701427e-06
Iter: 137 loss: 7.26986445e-06
Iter: 138 loss: 7.26973076e-06
Iter: 139 loss: 7.22837831e-06
Iter: 140 loss: 7.2299058e-06
Iter: 141 loss: 7.19607669e-06
Iter: 142 loss: 7.15362557e-06
Iter: 143 loss: 7.14930411e-06
Iter: 144 loss: 7.12311612e-06
Iter: 145 loss: 7.06674e-06
Iter: 146 loss: 7.95407e-06
Iter: 147 loss: 7.0647875e-06
Iter: 148 loss: 7.0286078e-06
Iter: 149 loss: 6.99400471e-06
Iter: 150 loss: 6.98602298e-06
Iter: 151 loss: 6.91831792e-06
Iter: 152 loss: 7.33493e-06
Iter: 153 loss: 6.91008063e-06
Iter: 154 loss: 6.86862768e-06
Iter: 155 loss: 6.89380795e-06
Iter: 156 loss: 6.84225779e-06
Iter: 157 loss: 6.77669505e-06
Iter: 158 loss: 7.13133704e-06
Iter: 159 loss: 6.76711e-06
Iter: 160 loss: 6.72827264e-06
Iter: 161 loss: 6.8093741e-06
Iter: 162 loss: 6.71267344e-06
Iter: 163 loss: 6.66714459e-06
Iter: 164 loss: 6.82244945e-06
Iter: 165 loss: 6.65504285e-06
Iter: 166 loss: 6.62353295e-06
Iter: 167 loss: 6.86735257e-06
Iter: 168 loss: 6.6213197e-06
Iter: 169 loss: 6.59302896e-06
Iter: 170 loss: 6.56344628e-06
Iter: 171 loss: 6.5584918e-06
Iter: 172 loss: 6.51702157e-06
Iter: 173 loss: 7.01512545e-06
Iter: 174 loss: 6.51632763e-06
Iter: 175 loss: 6.49476942e-06
Iter: 176 loss: 6.75729552e-06
Iter: 177 loss: 6.49468e-06
Iter: 178 loss: 6.46908757e-06
Iter: 179 loss: 6.49452431e-06
Iter: 180 loss: 6.4544929e-06
Iter: 181 loss: 6.43438534e-06
Iter: 182 loss: 6.38938354e-06
Iter: 183 loss: 7.0291e-06
Iter: 184 loss: 6.38737129e-06
Iter: 185 loss: 6.35441847e-06
Iter: 186 loss: 6.46552689e-06
Iter: 187 loss: 6.34559046e-06
Iter: 188 loss: 6.30633031e-06
Iter: 189 loss: 6.46119815e-06
Iter: 190 loss: 6.29729766e-06
Iter: 191 loss: 6.2727695e-06
Iter: 192 loss: 6.35297329e-06
Iter: 193 loss: 6.26590099e-06
Iter: 194 loss: 6.23384358e-06
Iter: 195 loss: 6.22556763e-06
Iter: 196 loss: 6.20522223e-06
Iter: 197 loss: 6.17371279e-06
Iter: 198 loss: 6.27782083e-06
Iter: 199 loss: 6.16520583e-06
Iter: 200 loss: 6.13056909e-06
Iter: 201 loss: 6.31160037e-06
Iter: 202 loss: 6.12514941e-06
Iter: 203 loss: 6.1039209e-06
Iter: 204 loss: 6.25941448e-06
Iter: 205 loss: 6.10217967e-06
Iter: 206 loss: 6.07906668e-06
Iter: 207 loss: 6.05293189e-06
Iter: 208 loss: 6.04968591e-06
Iter: 209 loss: 6.03250373e-06
Iter: 210 loss: 6.03033459e-06
Iter: 211 loss: 6.02301589e-06
Iter: 212 loss: 6.02190266e-06
Iter: 213 loss: 6.01558349e-06
Iter: 214 loss: 5.99682335e-06
Iter: 215 loss: 6.05915648e-06
Iter: 216 loss: 5.98819588e-06
Iter: 217 loss: 5.9659892e-06
Iter: 218 loss: 6.0803e-06
Iter: 219 loss: 5.96256314e-06
Iter: 220 loss: 5.9461172e-06
Iter: 221 loss: 5.9654426e-06
Iter: 222 loss: 5.93732102e-06
Iter: 223 loss: 5.91273692e-06
Iter: 224 loss: 5.95258734e-06
Iter: 225 loss: 5.90152877e-06
Iter: 226 loss: 5.88496459e-06
Iter: 227 loss: 5.92499509e-06
Iter: 228 loss: 5.87909472e-06
Iter: 229 loss: 5.8561368e-06
Iter: 230 loss: 5.86997794e-06
Iter: 231 loss: 5.84129702e-06
Iter: 232 loss: 5.82101711e-06
Iter: 233 loss: 6.03282342e-06
Iter: 234 loss: 5.82032771e-06
Iter: 235 loss: 5.80153164e-06
Iter: 236 loss: 5.78365416e-06
Iter: 237 loss: 5.77913397e-06
Iter: 238 loss: 5.7490488e-06
Iter: 239 loss: 6.07318361e-06
Iter: 240 loss: 5.74852629e-06
Iter: 241 loss: 5.7338757e-06
Iter: 242 loss: 5.72065e-06
Iter: 243 loss: 5.71707642e-06
Iter: 244 loss: 5.71522651e-06
Iter: 245 loss: 5.70498514e-06
Iter: 246 loss: 5.69386702e-06
Iter: 247 loss: 5.71254168e-06
Iter: 248 loss: 5.68902396e-06
Iter: 249 loss: 5.68181667e-06
Iter: 250 loss: 5.67089319e-06
Iter: 251 loss: 5.67059669e-06
Iter: 252 loss: 5.65391201e-06
Iter: 253 loss: 5.65863365e-06
Iter: 254 loss: 5.64188122e-06
Iter: 255 loss: 5.62742071e-06
Iter: 256 loss: 5.73280749e-06
Iter: 257 loss: 5.62624336e-06
Iter: 258 loss: 5.61119759e-06
Iter: 259 loss: 5.59779619e-06
Iter: 260 loss: 5.59395812e-06
Iter: 261 loss: 5.57812473e-06
Iter: 262 loss: 5.75074955e-06
Iter: 263 loss: 5.57773592e-06
Iter: 264 loss: 5.56294071e-06
Iter: 265 loss: 5.56610757e-06
Iter: 266 loss: 5.55198039e-06
Iter: 267 loss: 5.53747759e-06
Iter: 268 loss: 5.73649868e-06
Iter: 269 loss: 5.53736299e-06
Iter: 270 loss: 5.52762049e-06
Iter: 271 loss: 5.50799314e-06
Iter: 272 loss: 5.86634042e-06
Iter: 273 loss: 5.50761797e-06
Iter: 274 loss: 5.49093e-06
Iter: 275 loss: 5.49087417e-06
Iter: 276 loss: 5.47738728e-06
Iter: 277 loss: 5.48655544e-06
Iter: 278 loss: 5.46906e-06
Iter: 279 loss: 5.47349828e-06
Iter: 280 loss: 5.4622069e-06
Iter: 281 loss: 5.45668263e-06
Iter: 282 loss: 5.44619797e-06
Iter: 283 loss: 5.66028348e-06
Iter: 284 loss: 5.44602199e-06
Iter: 285 loss: 5.43663373e-06
Iter: 286 loss: 5.42904036e-06
Iter: 287 loss: 5.42634871e-06
Iter: 288 loss: 5.40899146e-06
Iter: 289 loss: 5.46110732e-06
Iter: 290 loss: 5.40362817e-06
Iter: 291 loss: 5.39164375e-06
Iter: 292 loss: 5.51778157e-06
Iter: 293 loss: 5.3912845e-06
Iter: 294 loss: 5.38140557e-06
Iter: 295 loss: 5.36042808e-06
Iter: 296 loss: 5.69329313e-06
Iter: 297 loss: 5.35982e-06
Iter: 298 loss: 5.34865e-06
Iter: 299 loss: 5.34727133e-06
Iter: 300 loss: 5.33777848e-06
Iter: 301 loss: 5.32497597e-06
Iter: 302 loss: 5.32437116e-06
Iter: 303 loss: 5.30966827e-06
Iter: 304 loss: 5.51635821e-06
Iter: 305 loss: 5.30964553e-06
Iter: 306 loss: 5.29982208e-06
Iter: 307 loss: 5.28846294e-06
Iter: 308 loss: 5.28709279e-06
Iter: 309 loss: 5.27071188e-06
Iter: 310 loss: 5.49530796e-06
Iter: 311 loss: 5.2706564e-06
Iter: 312 loss: 5.26380381e-06
Iter: 313 loss: 5.28366218e-06
Iter: 314 loss: 5.26173426e-06
Iter: 315 loss: 5.25313772e-06
Iter: 316 loss: 5.34945457e-06
Iter: 317 loss: 5.25306223e-06
Iter: 318 loss: 5.24897723e-06
Iter: 319 loss: 5.23893868e-06
Iter: 320 loss: 5.34023684e-06
Iter: 321 loss: 5.23774088e-06
Iter: 322 loss: 5.22783876e-06
Iter: 323 loss: 5.229012e-06
Iter: 324 loss: 5.22020036e-06
Iter: 325 loss: 5.20565573e-06
Iter: 326 loss: 5.31993282e-06
Iter: 327 loss: 5.20453705e-06
Iter: 328 loss: 5.19647438e-06
Iter: 329 loss: 5.22798564e-06
Iter: 330 loss: 5.19466812e-06
Iter: 331 loss: 5.18685283e-06
Iter: 332 loss: 5.17714443e-06
Iter: 333 loss: 5.17629587e-06
Iter: 334 loss: 5.16477166e-06
Iter: 335 loss: 5.31385831e-06
Iter: 336 loss: 5.16471437e-06
Iter: 337 loss: 5.15736565e-06
Iter: 338 loss: 5.15613192e-06
Iter: 339 loss: 5.15103238e-06
Iter: 340 loss: 5.13674195e-06
Iter: 341 loss: 5.17096441e-06
Iter: 342 loss: 5.13165469e-06
Iter: 343 loss: 5.12254883e-06
Iter: 344 loss: 5.17242188e-06
Iter: 345 loss: 5.12131646e-06
Iter: 346 loss: 5.11043072e-06
Iter: 347 loss: 5.11082408e-06
Iter: 348 loss: 5.10188784e-06
Iter: 349 loss: 5.11201324e-06
Iter: 350 loss: 5.09856591e-06
Iter: 351 loss: 5.09545225e-06
Iter: 352 loss: 5.08735138e-06
Iter: 353 loss: 5.1707375e-06
Iter: 354 loss: 5.0863755e-06
Iter: 355 loss: 5.07673212e-06
Iter: 356 loss: 5.0647468e-06
Iter: 357 loss: 5.06389642e-06
Iter: 358 loss: 5.05384105e-06
Iter: 359 loss: 5.05385378e-06
Iter: 360 loss: 5.04575291e-06
Iter: 361 loss: 5.04770696e-06
Iter: 362 loss: 5.03952106e-06
Iter: 363 loss: 5.03249294e-06
Iter: 364 loss: 5.11147482e-06
Iter: 365 loss: 5.0322642e-06
Iter: 366 loss: 5.02591683e-06
Iter: 367 loss: 5.02048215e-06
Iter: 368 loss: 5.01889645e-06
Iter: 369 loss: 5.00855458e-06
Iter: 370 loss: 5.07567347e-06
Iter: 371 loss: 5.00748274e-06
Iter: 372 loss: 5.00134365e-06
Iter: 373 loss: 4.99496855e-06
Iter: 374 loss: 4.99386e-06
Iter: 375 loss: 4.97966539e-06
Iter: 376 loss: 5.02463945e-06
Iter: 377 loss: 4.97548353e-06
Iter: 378 loss: 4.96804341e-06
Iter: 379 loss: 5.04949912e-06
Iter: 380 loss: 4.9678838e-06
Iter: 381 loss: 4.96082248e-06
Iter: 382 loss: 4.96359098e-06
Iter: 383 loss: 4.95578479e-06
Iter: 384 loss: 4.95036693e-06
Iter: 385 loss: 4.94917322e-06
Iter: 386 loss: 4.94672076e-06
Iter: 387 loss: 4.94058531e-06
Iter: 388 loss: 4.99018824e-06
Iter: 389 loss: 4.93942844e-06
Iter: 390 loss: 4.93173138e-06
Iter: 391 loss: 4.92897e-06
Iter: 392 loss: 4.92463641e-06
Iter: 393 loss: 4.91557876e-06
Iter: 394 loss: 5.02516286e-06
Iter: 395 loss: 4.91552873e-06
Iter: 396 loss: 4.90956882e-06
Iter: 397 loss: 4.91175524e-06
Iter: 398 loss: 4.90535831e-06
Iter: 399 loss: 4.89536887e-06
Iter: 400 loss: 4.92232402e-06
Iter: 401 loss: 4.89207832e-06
Iter: 402 loss: 4.88496971e-06
Iter: 403 loss: 4.94370033e-06
Iter: 404 loss: 4.88439855e-06
Iter: 405 loss: 4.87908665e-06
Iter: 406 loss: 4.86875069e-06
Iter: 407 loss: 5.0559338e-06
Iter: 408 loss: 4.8685597e-06
Iter: 409 loss: 4.86384488e-06
Iter: 410 loss: 4.86240788e-06
Iter: 411 loss: 4.85765258e-06
Iter: 412 loss: 4.84868269e-06
Iter: 413 loss: 5.0392764e-06
Iter: 414 loss: 4.84867815e-06
Iter: 415 loss: 4.84045722e-06
Iter: 416 loss: 4.84041948e-06
Iter: 417 loss: 4.83836902e-06
Iter: 418 loss: 4.83806889e-06
Iter: 419 loss: 4.83511576e-06
Iter: 420 loss: 4.8285192e-06
Iter: 421 loss: 4.91377614e-06
Iter: 422 loss: 4.82790756e-06
Iter: 423 loss: 4.82154837e-06
Iter: 424 loss: 4.84423617e-06
Iter: 425 loss: 4.82004452e-06
Iter: 426 loss: 4.81544794e-06
Iter: 427 loss: 4.81052757e-06
Iter: 428 loss: 4.80988365e-06
Iter: 429 loss: 4.80124527e-06
Iter: 430 loss: 4.8481088e-06
Iter: 431 loss: 4.79987784e-06
Iter: 432 loss: 4.79434038e-06
Iter: 433 loss: 4.81879442e-06
Iter: 434 loss: 4.79334e-06
Iter: 435 loss: 4.78604488e-06
Iter: 436 loss: 4.78808261e-06
Iter: 437 loss: 4.78098264e-06
Iter: 438 loss: 4.77489e-06
Iter: 439 loss: 4.82131691e-06
Iter: 440 loss: 4.77444109e-06
Iter: 441 loss: 4.76890182e-06
Iter: 442 loss: 4.76702871e-06
Iter: 443 loss: 4.76407331e-06
Iter: 444 loss: 4.75819252e-06
Iter: 445 loss: 4.81655752e-06
Iter: 446 loss: 4.75798242e-06
Iter: 447 loss: 4.753133e-06
Iter: 448 loss: 4.74706576e-06
Iter: 449 loss: 4.74649823e-06
Iter: 450 loss: 4.74306398e-06
Iter: 451 loss: 4.74162607e-06
Iter: 452 loss: 4.73877299e-06
Iter: 453 loss: 4.7386693e-06
Iter: 454 loss: 4.73743148e-06
Iter: 455 loss: 4.73375167e-06
Iter: 456 loss: 4.74047602e-06
Iter: 457 loss: 4.73107457e-06
Iter: 458 loss: 4.72433476e-06
Iter: 459 loss: 4.75183106e-06
Iter: 460 loss: 4.72290958e-06
Iter: 461 loss: 4.71867497e-06
Iter: 462 loss: 4.75040497e-06
Iter: 463 loss: 4.71826934e-06
Iter: 464 loss: 4.71431804e-06
Iter: 465 loss: 4.70588111e-06
Iter: 466 loss: 4.84387783e-06
Iter: 467 loss: 4.70575e-06
Iter: 468 loss: 4.69961287e-06
Iter: 469 loss: 4.69924316e-06
Iter: 470 loss: 4.69471479e-06
Iter: 471 loss: 4.69834413e-06
Iter: 472 loss: 4.6920959e-06
Iter: 473 loss: 4.68560074e-06
Iter: 474 loss: 4.70112309e-06
Iter: 475 loss: 4.68319195e-06
Iter: 476 loss: 4.67885502e-06
Iter: 477 loss: 4.68030203e-06
Iter: 478 loss: 4.67562813e-06
Iter: 479 loss: 4.66804522e-06
Iter: 480 loss: 4.68948838e-06
Iter: 481 loss: 4.66592292e-06
Iter: 482 loss: 4.66064148e-06
Iter: 483 loss: 4.69710722e-06
Iter: 484 loss: 4.660164e-06
Iter: 485 loss: 4.65833546e-06
Iter: 486 loss: 4.65772609e-06
Iter: 487 loss: 4.65500852e-06
Iter: 488 loss: 4.64902587e-06
Iter: 489 loss: 4.74081662e-06
Iter: 490 loss: 4.6487844e-06
Iter: 491 loss: 4.64568893e-06
Iter: 492 loss: 4.65218909e-06
Iter: 493 loss: 4.64443201e-06
Iter: 494 loss: 4.63994274e-06
Iter: 495 loss: 4.63503602e-06
Iter: 496 loss: 4.63439437e-06
Iter: 497 loss: 4.62877779e-06
Iter: 498 loss: 4.70665964e-06
Iter: 499 loss: 4.62862317e-06
Iter: 500 loss: 4.62466278e-06
Iter: 501 loss: 4.62086928e-06
Iter: 502 loss: 4.62001481e-06
Iter: 503 loss: 4.61175387e-06
Iter: 504 loss: 4.65644462e-06
Iter: 505 loss: 4.61048148e-06
Iter: 506 loss: 4.60654428e-06
Iter: 507 loss: 4.62570824e-06
Iter: 508 loss: 4.605924e-06
Iter: 509 loss: 4.60116553e-06
Iter: 510 loss: 4.59749117e-06
Iter: 511 loss: 4.59585408e-06
Iter: 512 loss: 4.59120838e-06
Iter: 513 loss: 4.6192572e-06
Iter: 514 loss: 4.59068542e-06
Iter: 515 loss: 4.58566183e-06
Iter: 516 loss: 4.58346358e-06
Iter: 517 loss: 4.58099294e-06
Iter: 518 loss: 4.583183e-06
Iter: 519 loss: 4.57892293e-06
Iter: 520 loss: 4.57664828e-06
Iter: 521 loss: 4.57808801e-06
Iter: 522 loss: 4.57516398e-06
Iter: 523 loss: 4.57300212e-06
Iter: 524 loss: 4.56822954e-06
Iter: 525 loss: 4.64778532e-06
Iter: 526 loss: 4.56815542e-06
Iter: 527 loss: 4.56308044e-06
Iter: 528 loss: 4.5863917e-06
Iter: 529 loss: 4.56215275e-06
Iter: 530 loss: 4.55793361e-06
Iter: 531 loss: 4.55804729e-06
Iter: 532 loss: 4.55467125e-06
Iter: 533 loss: 4.54921519e-06
Iter: 534 loss: 4.59499552e-06
Iter: 535 loss: 4.54892324e-06
Iter: 536 loss: 4.54535939e-06
Iter: 537 loss: 4.55394502e-06
Iter: 538 loss: 4.54417e-06
Iter: 539 loss: 4.53965367e-06
Iter: 540 loss: 4.54211931e-06
Iter: 541 loss: 4.53670873e-06
Iter: 542 loss: 4.53306848e-06
Iter: 543 loss: 4.57418901e-06
Iter: 544 loss: 4.53297162e-06
Iter: 545 loss: 4.52933227e-06
Iter: 546 loss: 4.52581935e-06
Iter: 547 loss: 4.52511358e-06
Iter: 548 loss: 4.52029872e-06
Iter: 549 loss: 4.56048e-06
Iter: 550 loss: 4.51996266e-06
Iter: 551 loss: 4.51648611e-06
Iter: 552 loss: 4.51065443e-06
Iter: 553 loss: 4.51080268e-06
Iter: 554 loss: 4.51591768e-06
Iter: 555 loss: 4.50801735e-06
Iter: 556 loss: 4.50585412e-06
Iter: 557 loss: 4.50520201e-06
Iter: 558 loss: 4.50396328e-06
Iter: 559 loss: 4.50202515e-06
Iter: 560 loss: 4.49647e-06
Iter: 561 loss: 4.51336382e-06
Iter: 562 loss: 4.49355866e-06
Iter: 563 loss: 4.48945457e-06
Iter: 564 loss: 4.48899846e-06
Iter: 565 loss: 4.48485571e-06
Iter: 566 loss: 4.48264e-06
Iter: 567 loss: 4.48067112e-06
Iter: 568 loss: 4.47513139e-06
Iter: 569 loss: 4.52848963e-06
Iter: 570 loss: 4.47485763e-06
Iter: 571 loss: 4.47193815e-06
Iter: 572 loss: 4.46862668e-06
Iter: 573 loss: 4.46818103e-06
Iter: 574 loss: 4.46242848e-06
Iter: 575 loss: 4.48567607e-06
Iter: 576 loss: 4.4610706e-06
Iter: 577 loss: 4.45760088e-06
Iter: 578 loss: 4.4856024e-06
Iter: 579 loss: 4.45734895e-06
Iter: 580 loss: 4.45401292e-06
Iter: 581 loss: 4.45032128e-06
Iter: 582 loss: 4.44955822e-06
Iter: 583 loss: 4.44546185e-06
Iter: 584 loss: 4.49555819e-06
Iter: 585 loss: 4.44544457e-06
Iter: 586 loss: 4.44307e-06
Iter: 587 loss: 4.44853868e-06
Iter: 588 loss: 4.44233865e-06
Iter: 589 loss: 4.43925092e-06
Iter: 590 loss: 4.46945796e-06
Iter: 591 loss: 4.43910267e-06
Iter: 592 loss: 4.43750559e-06
Iter: 593 loss: 4.43424869e-06
Iter: 594 loss: 4.48449873e-06
Iter: 595 loss: 4.43415865e-06
Iter: 596 loss: 4.43156478e-06
Iter: 597 loss: 4.43307545e-06
Iter: 598 loss: 4.42993633e-06
Iter: 599 loss: 4.42490091e-06
Iter: 600 loss: 4.42795499e-06
Iter: 601 loss: 4.42184182e-06
Iter: 602 loss: 4.41780594e-06
Iter: 603 loss: 4.46247032e-06
Iter: 604 loss: 4.41774591e-06
Iter: 605 loss: 4.41449583e-06
Iter: 606 loss: 4.41103521e-06
Iter: 607 loss: 4.41046041e-06
Iter: 608 loss: 4.40481244e-06
Iter: 609 loss: 4.46660397e-06
Iter: 610 loss: 4.40466874e-06
Iter: 611 loss: 4.40189069e-06
Iter: 612 loss: 4.39824953e-06
Iter: 613 loss: 4.39815039e-06
Iter: 614 loss: 4.39286578e-06
Iter: 615 loss: 4.45164505e-06
Iter: 616 loss: 4.39283849e-06
Iter: 617 loss: 4.38977622e-06
Iter: 618 loss: 4.40310077e-06
Iter: 619 loss: 4.38916595e-06
Iter: 620 loss: 4.38589177e-06
Iter: 621 loss: 4.38150073e-06
Iter: 622 loss: 4.38138704e-06
Iter: 623 loss: 4.38322695e-06
Iter: 624 loss: 4.37955441e-06
Iter: 625 loss: 4.37793915e-06
Iter: 626 loss: 4.38258e-06
Iter: 627 loss: 4.37751441e-06
Iter: 628 loss: 4.37637618e-06
Iter: 629 loss: 4.37232802e-06
Iter: 630 loss: 4.37341714e-06
Iter: 631 loss: 4.36848131e-06
Iter: 632 loss: 4.36322171e-06
Iter: 633 loss: 4.36315258e-06
Iter: 634 loss: 4.36007122e-06
Iter: 635 loss: 4.3552e-06
Iter: 636 loss: 4.35524726e-06
Iter: 637 loss: 4.35250968e-06
Iter: 638 loss: 4.35217135e-06
Iter: 639 loss: 4.34920184e-06
Iter: 640 loss: 4.34872118e-06
Iter: 641 loss: 4.34674075e-06
Iter: 642 loss: 4.34338608e-06
Iter: 643 loss: 4.37470953e-06
Iter: 644 loss: 4.34323738e-06
Iter: 645 loss: 4.34064214e-06
Iter: 646 loss: 4.33759396e-06
Iter: 647 loss: 4.33718833e-06
Iter: 648 loss: 4.33203877e-06
Iter: 649 loss: 4.35906213e-06
Iter: 650 loss: 4.33130617e-06
Iter: 651 loss: 4.32835759e-06
Iter: 652 loss: 4.3449545e-06
Iter: 653 loss: 4.32779962e-06
Iter: 654 loss: 4.32491197e-06
Iter: 655 loss: 4.32676643e-06
Iter: 656 loss: 4.32308752e-06
Iter: 657 loss: 4.32309844e-06
Iter: 658 loss: 4.32156139e-06
Iter: 659 loss: 4.32020624e-06
Iter: 660 loss: 4.31773833e-06
Iter: 661 loss: 4.3176351e-06
Iter: 662 loss: 4.31516855e-06
Iter: 663 loss: 4.30996897e-06
Iter: 664 loss: 4.3878872e-06
Iter: 665 loss: 4.30968885e-06
Iter: 666 loss: 4.3052969e-06
Iter: 667 loss: 4.36074697e-06
Iter: 668 loss: 4.30536784e-06
Iter: 669 loss: 4.30229466e-06
Iter: 670 loss: 4.30190494e-06
Iter: 671 loss: 4.29974898e-06
Iter: 672 loss: 4.29655393e-06
Iter: 673 loss: 4.29656575e-06
Iter: 674 loss: 4.29427655e-06
Iter: 675 loss: 4.28994508e-06
Iter: 676 loss: 4.38984671e-06
Iter: 677 loss: 4.28999965e-06
Iter: 678 loss: 4.28464136e-06
Iter: 679 loss: 4.32271236e-06
Iter: 680 loss: 4.28408e-06
Iter: 681 loss: 4.28114072e-06
Iter: 682 loss: 4.28709609e-06
Iter: 683 loss: 4.27993245e-06
Iter: 684 loss: 4.27548457e-06
Iter: 685 loss: 4.27875739e-06
Iter: 686 loss: 4.27269106e-06
Iter: 687 loss: 4.26953784e-06
Iter: 688 loss: 4.29701822e-06
Iter: 689 loss: 4.26937186e-06
Iter: 690 loss: 4.26661609e-06
Iter: 691 loss: 4.2697111e-06
Iter: 692 loss: 4.2650513e-06
Iter: 693 loss: 4.26339784e-06
Iter: 694 loss: 4.26288943e-06
Iter: 695 loss: 4.26144743e-06
Iter: 696 loss: 4.25914914e-06
Iter: 697 loss: 4.31964963e-06
Iter: 698 loss: 4.25915459e-06
Iter: 699 loss: 4.25687176e-06
Iter: 700 loss: 4.25319831e-06
Iter: 701 loss: 4.25318149e-06
Iter: 702 loss: 4.24917152e-06
Iter: 703 loss: 4.28505518e-06
Iter: 704 loss: 4.24901e-06
Iter: 705 loss: 4.24589234e-06
Iter: 706 loss: 4.24479595e-06
Iter: 707 loss: 4.24304471e-06
Iter: 708 loss: 4.2386373e-06
Iter: 709 loss: 4.2751567e-06
Iter: 710 loss: 4.23836173e-06
Iter: 711 loss: 4.23476467e-06
Iter: 712 loss: 4.23608117e-06
Iter: 713 loss: 4.23219626e-06
Iter: 714 loss: 4.22777794e-06
Iter: 715 loss: 4.25160761e-06
Iter: 716 loss: 4.22711491e-06
Iter: 717 loss: 4.22420544e-06
Iter: 718 loss: 4.23286747e-06
Iter: 719 loss: 4.22323592e-06
Iter: 720 loss: 4.21932145e-06
Iter: 721 loss: 4.2228794e-06
Iter: 722 loss: 4.21695222e-06
Iter: 723 loss: 4.21385539e-06
Iter: 724 loss: 4.22583162e-06
Iter: 725 loss: 4.21310415e-06
Iter: 726 loss: 4.20963488e-06
Iter: 727 loss: 4.2186316e-06
Iter: 728 loss: 4.2085112e-06
Iter: 729 loss: 4.2081524e-06
Iter: 730 loss: 4.20717242e-06
Iter: 731 loss: 4.2058191e-06
Iter: 732 loss: 4.20728156e-06
Iter: 733 loss: 4.20517699e-06
Iter: 734 loss: 4.20428478e-06
Iter: 735 loss: 4.20125798e-06
Iter: 736 loss: 4.21486902e-06
Iter: 737 loss: 4.20025572e-06
Iter: 738 loss: 4.19758544e-06
Iter: 739 loss: 4.19747767e-06
Iter: 740 loss: 4.19569596e-06
Iter: 741 loss: 4.19143407e-06
Iter: 742 loss: 4.25438384e-06
Iter: 743 loss: 4.19130265e-06
Iter: 744 loss: 4.18728723e-06
Iter: 745 loss: 4.23633355e-06
Iter: 746 loss: 4.18715035e-06
Iter: 747 loss: 4.18455511e-06
Iter: 748 loss: 4.1826388e-06
Iter: 749 loss: 4.18170748e-06
Iter: 750 loss: 4.17789306e-06
Iter: 751 loss: 4.21999766e-06
Iter: 752 loss: 4.17787851e-06
Iter: 753 loss: 4.17528418e-06
Iter: 754 loss: 4.17646243e-06
Iter: 755 loss: 4.1736248e-06
Iter: 756 loss: 4.1701237e-06
Iter: 757 loss: 4.18147101e-06
Iter: 758 loss: 4.16896546e-06
Iter: 759 loss: 4.16641524e-06
Iter: 760 loss: 4.17514957e-06
Iter: 761 loss: 4.16567809e-06
Iter: 762 loss: 4.16280636e-06
Iter: 763 loss: 4.17066758e-06
Iter: 764 loss: 4.16185912e-06
Iter: 765 loss: 4.15955674e-06
Iter: 766 loss: 4.15941486e-06
Iter: 767 loss: 4.15838895e-06
Iter: 768 loss: 4.157474e-06
Iter: 769 loss: 4.15723798e-06
Iter: 770 loss: 4.15568502e-06
Iter: 771 loss: 4.1536432e-06
Iter: 772 loss: 4.15357863e-06
Iter: 773 loss: 4.15005e-06
Iter: 774 loss: 4.1547064e-06
Iter: 775 loss: 4.14831038e-06
Iter: 776 loss: 4.14597116e-06
Iter: 777 loss: 4.17262027e-06
Iter: 778 loss: 4.14592159e-06
Iter: 779 loss: 4.14351234e-06
Iter: 780 loss: 4.13849557e-06
Iter: 781 loss: 4.22395624e-06
Iter: 782 loss: 4.13842372e-06
Iter: 783 loss: 4.13560974e-06
Iter: 784 loss: 4.13569e-06
Iter: 785 loss: 4.13313728e-06
Iter: 786 loss: 4.13139423e-06
Iter: 787 loss: 4.13053294e-06
Iter: 788 loss: 4.12751024e-06
Iter: 789 loss: 4.15430122e-06
Iter: 790 loss: 4.12728423e-06
Iter: 791 loss: 4.12447025e-06
Iter: 792 loss: 4.12306326e-06
Iter: 793 loss: 4.12157806e-06
Iter: 794 loss: 4.12041709e-06
Iter: 795 loss: 4.11977362e-06
Iter: 796 loss: 4.11860674e-06
Iter: 797 loss: 4.11857218e-06
Iter: 798 loss: 4.11778365e-06
Iter: 799 loss: 4.11549945e-06
Iter: 800 loss: 4.14059559e-06
Iter: 801 loss: 4.11526253e-06
Iter: 802 loss: 4.11297697e-06
Iter: 803 loss: 4.12493182e-06
Iter: 804 loss: 4.1125777e-06
Iter: 805 loss: 4.11093879e-06
Iter: 806 loss: 4.11129759e-06
Iter: 807 loss: 4.10966095e-06
Iter: 808 loss: 4.10713938e-06
Iter: 809 loss: 4.11281144e-06
Iter: 810 loss: 4.10614257e-06
Iter: 811 loss: 4.10409075e-06
Iter: 812 loss: 4.11667224e-06
Iter: 813 loss: 4.10386292e-06
Iter: 814 loss: 4.10203575e-06
Iter: 815 loss: 4.09906079e-06
Iter: 816 loss: 4.09894074e-06
Iter: 817 loss: 4.09596487e-06
Iter: 818 loss: 4.14284e-06
Iter: 819 loss: 4.09601398e-06
Iter: 820 loss: 4.09408085e-06
Iter: 821 loss: 4.09488393e-06
Iter: 822 loss: 4.09278573e-06
Iter: 823 loss: 4.08988217e-06
Iter: 824 loss: 4.09522954e-06
Iter: 825 loss: 4.08853202e-06
Iter: 826 loss: 4.08651113e-06
Iter: 827 loss: 4.08769483e-06
Iter: 828 loss: 4.08507276e-06
Iter: 829 loss: 4.08562028e-06
Iter: 830 loss: 4.08391861e-06
Iter: 831 loss: 4.08294181e-06
Iter: 832 loss: 4.0817049e-06
Iter: 833 loss: 4.08157666e-06
Iter: 834 loss: 4.08025517e-06
Iter: 835 loss: 4.08051301e-06
Iter: 836 loss: 4.07914968e-06
Iter: 837 loss: 4.07712105e-06
Iter: 838 loss: 4.07699372e-06
Iter: 839 loss: 4.07525295e-06
Iter: 840 loss: 4.0730597e-06
Iter: 841 loss: 4.09689301e-06
Iter: 842 loss: 4.07299467e-06
Iter: 843 loss: 4.07159314e-06
Iter: 844 loss: 4.07077277e-06
Iter: 845 loss: 4.07008474e-06
Iter: 846 loss: 4.0671257e-06
Iter: 847 loss: 4.07252e-06
Iter: 848 loss: 4.06592562e-06
Iter: 849 loss: 4.06396703e-06
Iter: 850 loss: 4.07748803e-06
Iter: 851 loss: 4.06373556e-06
Iter: 852 loss: 4.06169238e-06
Iter: 853 loss: 4.06079198e-06
Iter: 854 loss: 4.05972878e-06
Iter: 855 loss: 4.05691571e-06
Iter: 856 loss: 4.07780499e-06
Iter: 857 loss: 4.05673836e-06
Iter: 858 loss: 4.05478931e-06
Iter: 859 loss: 4.05182618e-06
Iter: 860 loss: 4.05175842e-06
Iter: 861 loss: 4.05357514e-06
Iter: 862 loss: 4.05049923e-06
Iter: 863 loss: 4.04921957e-06
Iter: 864 loss: 4.05178798e-06
Iter: 865 loss: 4.0487671e-06
Iter: 866 loss: 4.04763341e-06
Iter: 867 loss: 4.04529465e-06
Iter: 868 loss: 4.07089101e-06
Iter: 869 loss: 4.04509501e-06
Iter: 870 loss: 4.041789e-06
Iter: 871 loss: 4.05938499e-06
Iter: 872 loss: 4.04144976e-06
Iter: 873 loss: 4.03954346e-06
Iter: 874 loss: 4.04553066e-06
Iter: 875 loss: 4.03890817e-06
Iter: 876 loss: 4.03683816e-06
Iter: 877 loss: 4.03695049e-06
Iter: 878 loss: 4.03518834e-06
Iter: 879 loss: 4.03318063e-06
Iter: 880 loss: 4.05444734e-06
Iter: 881 loss: 4.03306e-06
Iter: 882 loss: 4.03121112e-06
Iter: 883 loss: 4.02917203e-06
Iter: 884 loss: 4.0288096e-06
Iter: 885 loss: 4.02585101e-06
Iter: 886 loss: 4.04848106e-06
Iter: 887 loss: 4.02567184e-06
Iter: 888 loss: 4.02393653e-06
Iter: 889 loss: 4.02605565e-06
Iter: 890 loss: 4.02283695e-06
Iter: 891 loss: 4.02028218e-06
Iter: 892 loss: 4.02613387e-06
Iter: 893 loss: 4.019279e-06
Iter: 894 loss: 4.01749685e-06
Iter: 895 loss: 4.0366549e-06
Iter: 896 loss: 4.01746456e-06
Iter: 897 loss: 4.01668149e-06
Iter: 898 loss: 4.01663419e-06
Iter: 899 loss: 4.01565558e-06
Iter: 900 loss: 4.01318266e-06
Iter: 901 loss: 4.02348724e-06
Iter: 902 loss: 4.01225134e-06
Iter: 903 loss: 4.01014131e-06
Iter: 904 loss: 4.02781461e-06
Iter: 905 loss: 4.01007219e-06
Iter: 906 loss: 4.00865611e-06
Iter: 907 loss: 4.0099103e-06
Iter: 908 loss: 4.00770296e-06
Iter: 909 loss: 4.00549516e-06
Iter: 910 loss: 4.00998033e-06
Iter: 911 loss: 4.00448198e-06
Iter: 912 loss: 4.00281351e-06
Iter: 913 loss: 4.00561385e-06
Iter: 914 loss: 4.00202407e-06
Iter: 915 loss: 3.99955707e-06
Iter: 916 loss: 4.00705721e-06
Iter: 917 loss: 3.99890814e-06
Iter: 918 loss: 3.99693454e-06
Iter: 919 loss: 4.00155614e-06
Iter: 920 loss: 3.99613145e-06
Iter: 921 loss: 3.99386954e-06
Iter: 922 loss: 3.99306737e-06
Iter: 923 loss: 3.99173223e-06
Iter: 924 loss: 3.98932343e-06
Iter: 925 loss: 4.01178431e-06
Iter: 926 loss: 3.98921611e-06
Iter: 927 loss: 3.9871079e-06
Iter: 928 loss: 3.99054397e-06
Iter: 929 loss: 3.98613201e-06
Iter: 930 loss: 3.98596421e-06
Iter: 931 loss: 3.98510929e-06
Iter: 932 loss: 3.98428529e-06
Iter: 933 loss: 3.98332168e-06
Iter: 934 loss: 3.98308975e-06
Iter: 935 loss: 3.98206339e-06
Iter: 936 loss: 3.97983376e-06
Iter: 937 loss: 4.01825e-06
Iter: 938 loss: 3.97987878e-06
Iter: 939 loss: 3.97724216e-06
Iter: 940 loss: 3.99579403e-06
Iter: 941 loss: 3.97698341e-06
Iter: 942 loss: 3.97501481e-06
Iter: 943 loss: 3.98696375e-06
Iter: 944 loss: 3.97485474e-06
Iter: 945 loss: 3.97322037e-06
Iter: 946 loss: 3.97041367e-06
Iter: 947 loss: 3.97035183e-06
Iter: 948 loss: 3.96855921e-06
Iter: 949 loss: 3.96835821e-06
Iter: 950 loss: 3.96714495e-06
Iter: 951 loss: 3.9656743e-06
Iter: 952 loss: 3.96551695e-06
Iter: 953 loss: 3.96276801e-06
Iter: 954 loss: 3.97254644e-06
Iter: 955 loss: 3.96217911e-06
Iter: 956 loss: 3.96031783e-06
Iter: 957 loss: 3.97175882e-06
Iter: 958 loss: 3.96015e-06
Iter: 959 loss: 3.95857569e-06
Iter: 960 loss: 3.9562583e-06
Iter: 961 loss: 3.95624465e-06
Iter: 962 loss: 3.95564712e-06
Iter: 963 loss: 3.95497727e-06
Iter: 964 loss: 3.9536817e-06
Iter: 965 loss: 3.95843517e-06
Iter: 966 loss: 3.95329198e-06
Iter: 967 loss: 3.95255483e-06
Iter: 968 loss: 3.95053166e-06
Iter: 969 loss: 3.95869256e-06
Iter: 970 loss: 3.94960898e-06
Iter: 971 loss: 3.94742801e-06
Iter: 972 loss: 3.97312579e-06
Iter: 973 loss: 3.94744529e-06
Iter: 974 loss: 3.94534e-06
Iter: 975 loss: 3.94957351e-06
Iter: 976 loss: 3.94461267e-06
Iter: 977 loss: 3.94242215e-06
Iter: 978 loss: 3.94675862e-06
Iter: 979 loss: 3.94150265e-06
Iter: 980 loss: 3.9398351e-06
Iter: 981 loss: 3.95114057e-06
Iter: 982 loss: 3.93965456e-06
Iter: 983 loss: 3.93801338e-06
Iter: 984 loss: 3.93623668e-06
Iter: 985 loss: 3.93599657e-06
Iter: 986 loss: 3.93388973e-06
Iter: 987 loss: 3.95104325e-06
Iter: 988 loss: 3.93388746e-06
Iter: 989 loss: 3.93179289e-06
Iter: 990 loss: 3.93394293e-06
Iter: 991 loss: 3.93077e-06
Iter: 992 loss: 3.92889069e-06
Iter: 993 loss: 3.94331801e-06
Iter: 994 loss: 3.92879065e-06
Iter: 995 loss: 3.9274355e-06
Iter: 996 loss: 3.92542552e-06
Iter: 997 loss: 3.92533866e-06
Iter: 998 loss: 3.92822403e-06
Iter: 999 loss: 3.92451102e-06
Iter: 1000 loss: 3.92394122e-06
Iter: 1001 loss: 3.92266702e-06
Iter: 1002 loss: 3.93916253e-06
Iter: 1003 loss: 3.92256879e-06
Iter: 1004 loss: 3.92138327e-06
Iter: 1005 loss: 3.92009224e-06
Iter: 1006 loss: 3.91995718e-06
Iter: 1007 loss: 3.91791036e-06
Iter: 1008 loss: 3.93314076e-06
Iter: 1009 loss: 3.91757112e-06
Iter: 1010 loss: 3.91620688e-06
Iter: 1011 loss: 3.91797039e-06
Iter: 1012 loss: 3.91542471e-06
Iter: 1013 loss: 3.9132774e-06
Iter: 1014 loss: 3.9176839e-06
Iter: 1015 loss: 3.91241883e-06
Iter: 1016 loss: 3.91014055e-06
Iter: 1017 loss: 3.91893036e-06
Iter: 1018 loss: 3.90961532e-06
Iter: 1019 loss: 3.90797277e-06
Iter: 1020 loss: 3.90704554e-06
Iter: 1021 loss: 3.90627838e-06
Iter: 1022 loss: 3.90353398e-06
Iter: 1023 loss: 3.92594939e-06
Iter: 1024 loss: 3.9033207e-06
Iter: 1025 loss: 3.90189962e-06
Iter: 1026 loss: 3.90183141e-06
Iter: 1027 loss: 3.90068271e-06
Iter: 1028 loss: 3.89797287e-06
Iter: 1029 loss: 3.90396963e-06
Iter: 1030 loss: 3.89703791e-06
Iter: 1031 loss: 3.89744855e-06
Iter: 1032 loss: 3.89621437e-06
Iter: 1033 loss: 3.89533125e-06
Iter: 1034 loss: 3.89394972e-06
Iter: 1035 loss: 3.89397064e-06
Iter: 1036 loss: 3.8923281e-06
Iter: 1037 loss: 3.89160596e-06
Iter: 1038 loss: 3.89079651e-06
Iter: 1039 loss: 3.88927492e-06
Iter: 1040 loss: 3.89514798e-06
Iter: 1041 loss: 3.88884655e-06
Iter: 1042 loss: 3.88676244e-06
Iter: 1043 loss: 3.88585977e-06
Iter: 1044 loss: 3.88488388e-06
Iter: 1045 loss: 3.88265698e-06
Iter: 1046 loss: 3.91152025e-06
Iter: 1047 loss: 3.88260241e-06
Iter: 1048 loss: 3.88069566e-06
Iter: 1049 loss: 3.8830276e-06
Iter: 1050 loss: 3.87958426e-06
Iter: 1051 loss: 3.87722912e-06
Iter: 1052 loss: 3.88009357e-06
Iter: 1053 loss: 3.87606769e-06
Iter: 1054 loss: 3.87409546e-06
Iter: 1055 loss: 3.88347598e-06
Iter: 1056 loss: 3.8738317e-06
Iter: 1057 loss: 3.87167483e-06
Iter: 1058 loss: 3.87338241e-06
Iter: 1059 loss: 3.87030923e-06
Iter: 1060 loss: 3.86838246e-06
Iter: 1061 loss: 3.87909449e-06
Iter: 1062 loss: 3.86803822e-06
Iter: 1063 loss: 3.86604961e-06
Iter: 1064 loss: 3.86843703e-06
Iter: 1065 loss: 3.86496276e-06
Iter: 1066 loss: 3.8646358e-06
Iter: 1067 loss: 3.86371084e-06
Iter: 1068 loss: 3.86330203e-06
Iter: 1069 loss: 3.86205375e-06
Iter: 1070 loss: 3.87536602e-06
Iter: 1071 loss: 3.86186275e-06
Iter: 1072 loss: 3.86041802e-06
Iter: 1073 loss: 3.85790827e-06
Iter: 1074 loss: 3.85786188e-06
Iter: 1075 loss: 3.85625117e-06
Iter: 1076 loss: 3.85619205e-06
Iter: 1077 loss: 3.85473049e-06
Iter: 1078 loss: 3.8540129e-06
Iter: 1079 loss: 3.85325802e-06
Iter: 1080 loss: 3.85121211e-06
Iter: 1081 loss: 3.8693961e-06
Iter: 1082 loss: 3.85105341e-06
Iter: 1083 loss: 3.84962368e-06
Iter: 1084 loss: 3.84757914e-06
Iter: 1085 loss: 3.84750956e-06
Iter: 1086 loss: 3.84526038e-06
Iter: 1087 loss: 3.84528312e-06
Iter: 1088 loss: 3.84408668e-06
Iter: 1089 loss: 3.84644773e-06
Iter: 1090 loss: 3.84361056e-06
Iter: 1091 loss: 3.84193072e-06
Iter: 1092 loss: 3.84028453e-06
Iter: 1093 loss: 3.83990027e-06
Iter: 1094 loss: 3.83796487e-06
Iter: 1095 loss: 3.86661122e-06
Iter: 1096 loss: 3.83810584e-06
Iter: 1097 loss: 3.83733504e-06
Iter: 1098 loss: 3.83710903e-06
Iter: 1099 loss: 3.83612223e-06
Iter: 1100 loss: 3.8342896e-06
Iter: 1101 loss: 3.87656928e-06
Iter: 1102 loss: 3.83431552e-06
Iter: 1103 loss: 3.83279394e-06
Iter: 1104 loss: 3.83412817e-06
Iter: 1105 loss: 3.8316939e-06
Iter: 1106 loss: 3.83019778e-06
Iter: 1107 loss: 3.82909775e-06
Iter: 1108 loss: 3.82845064e-06
Iter: 1109 loss: 3.82575536e-06
Iter: 1110 loss: 3.84814439e-06
Iter: 1111 loss: 3.82562939e-06
Iter: 1112 loss: 3.82415737e-06
Iter: 1113 loss: 3.82451435e-06
Iter: 1114 loss: 3.82312282e-06
Iter: 1115 loss: 3.82024609e-06
Iter: 1116 loss: 3.82744156e-06
Iter: 1117 loss: 3.81910831e-06
Iter: 1118 loss: 3.81775862e-06
Iter: 1119 loss: 3.83487622e-06
Iter: 1120 loss: 3.81764221e-06
Iter: 1121 loss: 3.8165349e-06
Iter: 1122 loss: 3.81389418e-06
Iter: 1123 loss: 3.85152271e-06
Iter: 1124 loss: 3.81380164e-06
Iter: 1125 loss: 3.81105065e-06
Iter: 1126 loss: 3.84768737e-06
Iter: 1127 loss: 3.81111818e-06
Iter: 1128 loss: 3.80947313e-06
Iter: 1129 loss: 3.80856272e-06
Iter: 1130 loss: 3.8078615e-06
Iter: 1131 loss: 3.8074179e-06
Iter: 1132 loss: 3.80659139e-06
Iter: 1133 loss: 3.8055714e-06
Iter: 1134 loss: 3.80902907e-06
Iter: 1135 loss: 3.80518577e-06
Iter: 1136 loss: 3.80444635e-06
Iter: 1137 loss: 3.80246593e-06
Iter: 1138 loss: 3.8189296e-06
Iter: 1139 loss: 3.80223946e-06
Iter: 1140 loss: 3.79977791e-06
Iter: 1141 loss: 3.80755591e-06
Iter: 1142 loss: 3.79900712e-06
Iter: 1143 loss: 3.79713947e-06
Iter: 1144 loss: 3.79977018e-06
Iter: 1145 loss: 3.796177e-06
Iter: 1146 loss: 3.79392623e-06
Iter: 1147 loss: 3.80559504e-06
Iter: 1148 loss: 3.79357516e-06
Iter: 1149 loss: 3.79197627e-06
Iter: 1150 loss: 3.80122106e-06
Iter: 1151 loss: 3.79188759e-06
Iter: 1152 loss: 3.79014023e-06
Iter: 1153 loss: 3.78967934e-06
Iter: 1154 loss: 3.78853565e-06
Iter: 1155 loss: 3.78651225e-06
Iter: 1156 loss: 3.80565552e-06
Iter: 1157 loss: 3.78632694e-06
Iter: 1158 loss: 3.78512141e-06
Iter: 1159 loss: 3.78404411e-06
Iter: 1160 loss: 3.78367963e-06
Iter: 1161 loss: 3.78114419e-06
Iter: 1162 loss: 3.79176117e-06
Iter: 1163 loss: 3.78055756e-06
Iter: 1164 loss: 3.77939341e-06
Iter: 1165 loss: 3.78678487e-06
Iter: 1166 loss: 3.77921424e-06
Iter: 1167 loss: 3.77806373e-06
Iter: 1168 loss: 3.7779796e-06
Iter: 1169 loss: 3.77721653e-06
Iter: 1170 loss: 3.7752493e-06
Iter: 1171 loss: 3.79979269e-06
Iter: 1172 loss: 3.77506422e-06
Iter: 1173 loss: 3.77363085e-06
Iter: 1174 loss: 3.77727633e-06
Iter: 1175 loss: 3.77308538e-06
Iter: 1176 loss: 3.77135029e-06
Iter: 1177 loss: 3.7703885e-06
Iter: 1178 loss: 3.76970593e-06
Iter: 1179 loss: 3.76802382e-06
Iter: 1180 loss: 3.76797652e-06
Iter: 1181 loss: 3.76662547e-06
Iter: 1182 loss: 3.76442517e-06
Iter: 1183 loss: 3.81139125e-06
Iter: 1184 loss: 3.7643274e-06
Iter: 1185 loss: 3.76225171e-06
Iter: 1186 loss: 3.76217395e-06
Iter: 1187 loss: 3.76099797e-06
Iter: 1188 loss: 3.76160824e-06
Iter: 1189 loss: 3.76005869e-06
Iter: 1190 loss: 3.75759851e-06
Iter: 1191 loss: 3.75716581e-06
Iter: 1192 loss: 3.75535683e-06
Iter: 1193 loss: 3.75323452e-06
Iter: 1194 loss: 3.7610364e-06
Iter: 1195 loss: 3.75263903e-06
Iter: 1196 loss: 3.75014179e-06
Iter: 1197 loss: 3.75531044e-06
Iter: 1198 loss: 3.74919819e-06
Iter: 1199 loss: 3.74939464e-06
Iter: 1200 loss: 3.74845013e-06
Iter: 1201 loss: 3.74770798e-06
Iter: 1202 loss: 3.7476093e-06
Iter: 1203 loss: 3.74707747e-06
Iter: 1204 loss: 3.74617957e-06
Iter: 1205 loss: 3.74428237e-06
Iter: 1206 loss: 3.77574543e-06
Iter: 1207 loss: 3.74412548e-06
Iter: 1208 loss: 3.74217825e-06
Iter: 1209 loss: 3.75554919e-06
Iter: 1210 loss: 3.74208867e-06
Iter: 1211 loss: 3.74048045e-06
Iter: 1212 loss: 3.73897615e-06
Iter: 1213 loss: 3.73855755e-06
Iter: 1214 loss: 3.73650164e-06
Iter: 1215 loss: 3.7364473e-06
Iter: 1216 loss: 3.73504213e-06
Iter: 1217 loss: 3.73316243e-06
Iter: 1218 loss: 3.73306329e-06
Iter: 1219 loss: 3.73040666e-06
Iter: 1220 loss: 3.75199443e-06
Iter: 1221 loss: 3.73001285e-06
Iter: 1222 loss: 3.72820386e-06
Iter: 1223 loss: 3.73081366e-06
Iter: 1224 loss: 3.72744034e-06
Iter: 1225 loss: 3.72500153e-06
Iter: 1226 loss: 3.7318573e-06
Iter: 1227 loss: 3.72434761e-06
Iter: 1228 loss: 3.72273735e-06
Iter: 1229 loss: 3.7329728e-06
Iter: 1230 loss: 3.72259865e-06
Iter: 1231 loss: 3.72096611e-06
Iter: 1232 loss: 3.71920896e-06
Iter: 1233 loss: 3.7189352e-06
Iter: 1234 loss: 3.71781857e-06
Iter: 1235 loss: 3.71775059e-06
Iter: 1236 loss: 3.71689771e-06
Iter: 1237 loss: 3.73055263e-06
Iter: 1238 loss: 3.71694614e-06
Iter: 1239 loss: 3.71633178e-06
Iter: 1240 loss: 3.71437272e-06
Iter: 1241 loss: 3.71486658e-06
Iter: 1242 loss: 3.71258e-06
Iter: 1243 loss: 3.71047145e-06
Iter: 1244 loss: 3.73240141e-06
Iter: 1245 loss: 3.71055103e-06
Iter: 1246 loss: 3.70850103e-06
Iter: 1247 loss: 3.70886937e-06
Iter: 1248 loss: 3.70704083e-06
Iter: 1249 loss: 3.70569933e-06
Iter: 1250 loss: 3.70560565e-06
Iter: 1251 loss: 3.70433827e-06
Iter: 1252 loss: 3.70182761e-06
Iter: 1253 loss: 3.76204821e-06
Iter: 1254 loss: 3.70186035e-06
Iter: 1255 loss: 3.69998793e-06
Iter: 1256 loss: 3.69999771e-06
Iter: 1257 loss: 3.69853615e-06
Iter: 1258 loss: 3.69766258e-06
Iter: 1259 loss: 3.69728423e-06
Iter: 1260 loss: 3.69533745e-06
Iter: 1261 loss: 3.71979513e-06
Iter: 1262 loss: 3.69521354e-06
Iter: 1263 loss: 3.69386748e-06
Iter: 1264 loss: 3.69324789e-06
Iter: 1265 loss: 3.69255895e-06
Iter: 1266 loss: 3.6909646e-06
Iter: 1267 loss: 3.69590953e-06
Iter: 1268 loss: 3.69039208e-06
Iter: 1269 loss: 3.6895849e-06
Iter: 1270 loss: 3.70192879e-06
Iter: 1271 loss: 3.68967335e-06
Iter: 1272 loss: 3.68895326e-06
Iter: 1273 loss: 3.68755263e-06
Iter: 1274 loss: 3.71321676e-06
Iter: 1275 loss: 3.68748533e-06
Iter: 1276 loss: 3.68588053e-06
Iter: 1277 loss: 3.68717429e-06
Iter: 1278 loss: 3.68471842e-06
Iter: 1279 loss: 3.68303154e-06
Iter: 1280 loss: 3.69777e-06
Iter: 1281 loss: 3.68292285e-06
Iter: 1282 loss: 3.68149176e-06
Iter: 1283 loss: 3.6810743e-06
Iter: 1284 loss: 3.68022984e-06
Iter: 1285 loss: 3.67857319e-06
Iter: 1286 loss: 3.68550445e-06
Iter: 1287 loss: 3.67820417e-06
Iter: 1288 loss: 3.67620396e-06
Iter: 1289 loss: 3.67810071e-06
Iter: 1290 loss: 3.67519579e-06
Iter: 1291 loss: 3.67328676e-06
Iter: 1292 loss: 3.68554788e-06
Iter: 1293 loss: 3.67307098e-06
Iter: 1294 loss: 3.67159737e-06
Iter: 1295 loss: 3.67254347e-06
Iter: 1296 loss: 3.67061784e-06
Iter: 1297 loss: 3.66854488e-06
Iter: 1298 loss: 3.67959206e-06
Iter: 1299 loss: 3.66820473e-06
Iter: 1300 loss: 3.66707923e-06
Iter: 1301 loss: 3.67028906e-06
Iter: 1302 loss: 3.66676295e-06
Iter: 1303 loss: 3.665843e-06
Iter: 1304 loss: 3.66572476e-06
Iter: 1305 loss: 3.66497443e-06
Iter: 1306 loss: 3.66309564e-06
Iter: 1307 loss: 3.69495592e-06
Iter: 1308 loss: 3.66302697e-06
Iter: 1309 loss: 3.66189192e-06
Iter: 1310 loss: 3.66616723e-06
Iter: 1311 loss: 3.66143104e-06
Iter: 1312 loss: 3.6602396e-06
Iter: 1313 loss: 3.66055247e-06
Iter: 1314 loss: 3.65941128e-06
Iter: 1315 loss: 3.65761184e-06
Iter: 1316 loss: 3.6679578e-06
Iter: 1317 loss: 3.65742017e-06
Iter: 1318 loss: 3.65644701e-06
Iter: 1319 loss: 3.65746746e-06
Iter: 1320 loss: 3.65583264e-06
Iter: 1321 loss: 3.65412666e-06
Iter: 1322 loss: 3.65428014e-06
Iter: 1323 loss: 3.65270716e-06
Iter: 1324 loss: 3.65118376e-06
Iter: 1325 loss: 3.66420113e-06
Iter: 1326 loss: 3.65100641e-06
Iter: 1327 loss: 3.64964649e-06
Iter: 1328 loss: 3.65000369e-06
Iter: 1329 loss: 3.6487329e-06
Iter: 1330 loss: 3.64710695e-06
Iter: 1331 loss: 3.66336826e-06
Iter: 1332 loss: 3.64713333e-06
Iter: 1333 loss: 3.64570974e-06
Iter: 1334 loss: 3.64417565e-06
Iter: 1335 loss: 3.64409152e-06
Iter: 1336 loss: 3.64434936e-06
Iter: 1337 loss: 3.64330185e-06
Iter: 1338 loss: 3.64251605e-06
Iter: 1339 loss: 3.64376069e-06
Iter: 1340 loss: 3.64217908e-06
Iter: 1341 loss: 3.64155676e-06
Iter: 1342 loss: 3.64011657e-06
Iter: 1343 loss: 3.65537062e-06
Iter: 1344 loss: 3.63998743e-06
Iter: 1345 loss: 3.63869731e-06
Iter: 1346 loss: 3.6536253e-06
Iter: 1347 loss: 3.63862841e-06
Iter: 1348 loss: 3.63779691e-06
Iter: 1349 loss: 3.63739218e-06
Iter: 1350 loss: 3.63690015e-06
Iter: 1351 loss: 3.63511094e-06
Iter: 1352 loss: 3.63942581e-06
Iter: 1353 loss: 3.63451636e-06
Iter: 1354 loss: 3.63313347e-06
Iter: 1355 loss: 3.63730624e-06
Iter: 1356 loss: 3.63271693e-06
Iter: 1357 loss: 3.63106165e-06
Iter: 1358 loss: 3.63214485e-06
Iter: 1359 loss: 3.62989158e-06
Iter: 1360 loss: 3.62808942e-06
Iter: 1361 loss: 3.64009725e-06
Iter: 1362 loss: 3.62776109e-06
Iter: 1363 loss: 3.62658966e-06
Iter: 1364 loss: 3.62794458e-06
Iter: 1365 loss: 3.62592118e-06
Iter: 1366 loss: 3.62426431e-06
Iter: 1367 loss: 3.6323213e-06
Iter: 1368 loss: 3.62405808e-06
Iter: 1369 loss: 3.62296714e-06
Iter: 1370 loss: 3.62253104e-06
Iter: 1371 loss: 3.62195669e-06
Iter: 1372 loss: 3.62169567e-06
Iter: 1373 loss: 3.62096534e-06
Iter: 1374 loss: 3.62012452e-06
Iter: 1375 loss: 3.61894467e-06
Iter: 1376 loss: 3.61880643e-06
Iter: 1377 loss: 3.61793172e-06
Iter: 1378 loss: 3.6183319e-06
Iter: 1379 loss: 3.61721641e-06
Iter: 1380 loss: 3.61568846e-06
Iter: 1381 loss: 3.61535149e-06
Iter: 1382 loss: 3.61444927e-06
Iter: 1383 loss: 3.61296406e-06
Iter: 1384 loss: 3.63247e-06
Iter: 1385 loss: 3.61292223e-06
Iter: 1386 loss: 3.61170623e-06
Iter: 1387 loss: 3.61101684e-06
Iter: 1388 loss: 3.61051048e-06
Iter: 1389 loss: 3.60859667e-06
Iter: 1390 loss: 3.61619959e-06
Iter: 1391 loss: 3.60801278e-06
Iter: 1392 loss: 3.60644276e-06
Iter: 1393 loss: 3.60782383e-06
Iter: 1394 loss: 3.6055751e-06
Iter: 1395 loss: 3.6038773e-06
Iter: 1396 loss: 3.62305491e-06
Iter: 1397 loss: 3.60393324e-06
Iter: 1398 loss: 3.60290278e-06
Iter: 1399 loss: 3.60322065e-06
Iter: 1400 loss: 3.60219474e-06
Iter: 1401 loss: 3.60048034e-06
Iter: 1402 loss: 3.60125205e-06
Iter: 1403 loss: 3.59944625e-06
Iter: 1404 loss: 3.59778051e-06
Iter: 1405 loss: 3.6137792e-06
Iter: 1406 loss: 3.59779915e-06
Iter: 1407 loss: 3.59713431e-06
Iter: 1408 loss: 3.59702381e-06
Iter: 1409 loss: 3.59640808e-06
Iter: 1410 loss: 3.59489286e-06
Iter: 1411 loss: 3.60637432e-06
Iter: 1412 loss: 3.59446949e-06
Iter: 1413 loss: 3.59312844e-06
Iter: 1414 loss: 3.59817295e-06
Iter: 1415 loss: 3.5927751e-06
Iter: 1416 loss: 3.59136811e-06
Iter: 1417 loss: 3.59227033e-06
Iter: 1418 loss: 3.59058913e-06
Iter: 1419 loss: 3.58872421e-06
Iter: 1420 loss: 3.60104673e-06
Iter: 1421 loss: 3.58860143e-06
Iter: 1422 loss: 3.58751481e-06
Iter: 1423 loss: 3.5902176e-06
Iter: 1424 loss: 3.58719103e-06
Iter: 1425 loss: 3.58602256e-06
Iter: 1426 loss: 3.58696457e-06
Iter: 1427 loss: 3.58525904e-06
Iter: 1428 loss: 3.58385705e-06
Iter: 1429 loss: 3.58840543e-06
Iter: 1430 loss: 3.58341777e-06
Iter: 1431 loss: 3.58202396e-06
Iter: 1432 loss: 3.58157604e-06
Iter: 1433 loss: 3.58081297e-06
Iter: 1434 loss: 3.57930594e-06
Iter: 1435 loss: 3.60084937e-06
Iter: 1436 loss: 3.57932367e-06
Iter: 1437 loss: 3.57777458e-06
Iter: 1438 loss: 3.5765197e-06
Iter: 1439 loss: 3.57601471e-06
Iter: 1440 loss: 3.5753535e-06
Iter: 1441 loss: 3.57500176e-06
Iter: 1442 loss: 3.5742828e-06
Iter: 1443 loss: 3.58232955e-06
Iter: 1444 loss: 3.57428144e-06
Iter: 1445 loss: 3.57386443e-06
Iter: 1446 loss: 3.5726157e-06
Iter: 1447 loss: 3.57481213e-06
Iter: 1448 loss: 3.57180375e-06
Iter: 1449 loss: 3.57011959e-06
Iter: 1450 loss: 3.58185207e-06
Iter: 1451 loss: 3.56996361e-06
Iter: 1452 loss: 3.56852047e-06
Iter: 1453 loss: 3.57275621e-06
Iter: 1454 loss: 3.5680564e-06
Iter: 1455 loss: 3.56650344e-06
Iter: 1456 loss: 3.57034105e-06
Iter: 1457 loss: 3.56590726e-06
Iter: 1458 loss: 3.56480655e-06
Iter: 1459 loss: 3.56813712e-06
Iter: 1460 loss: 3.56444116e-06
Iter: 1461 loss: 3.56298597e-06
Iter: 1462 loss: 3.56476448e-06
Iter: 1463 loss: 3.5622943e-06
Iter: 1464 loss: 3.56070768e-06
Iter: 1465 loss: 3.56591022e-06
Iter: 1466 loss: 3.5603648e-06
Iter: 1467 loss: 3.55881525e-06
Iter: 1468 loss: 3.55803331e-06
Iter: 1469 loss: 3.55731368e-06
Iter: 1470 loss: 3.55597626e-06
Iter: 1471 loss: 3.55590191e-06
Iter: 1472 loss: 3.55476709e-06
Iter: 1473 loss: 3.55355451e-06
Iter: 1474 loss: 3.55341353e-06
Iter: 1475 loss: 3.55468524e-06
Iter: 1476 loss: 3.55289239e-06
Iter: 1477 loss: 3.55243628e-06
Iter: 1478 loss: 3.55193174e-06
Iter: 1479 loss: 3.5517694e-06
Iter: 1480 loss: 3.55108705e-06
Iter: 1481 loss: 3.54976737e-06
Iter: 1482 loss: 3.57090767e-06
Iter: 1483 loss: 3.54966028e-06
Iter: 1484 loss: 3.54792815e-06
Iter: 1485 loss: 3.5569235e-06
Iter: 1486 loss: 3.54767508e-06
Iter: 1487 loss: 3.54631447e-06
Iter: 1488 loss: 3.54536132e-06
Iter: 1489 loss: 3.54475424e-06
Iter: 1490 loss: 3.5427704e-06
Iter: 1491 loss: 3.55514067e-06
Iter: 1492 loss: 3.54248527e-06
Iter: 1493 loss: 3.54113808e-06
Iter: 1494 loss: 3.55018483e-06
Iter: 1495 loss: 3.54095755e-06
Iter: 1496 loss: 3.53970563e-06
Iter: 1497 loss: 3.54002304e-06
Iter: 1498 loss: 3.53886116e-06
Iter: 1499 loss: 3.53738119e-06
Iter: 1500 loss: 3.5436633e-06
Iter: 1501 loss: 3.5370665e-06
Iter: 1502 loss: 3.53556243e-06
Iter: 1503 loss: 3.53549285e-06
Iter: 1504 loss: 3.53441874e-06
Iter: 1505 loss: 3.53230052e-06
Iter: 1506 loss: 3.54106783e-06
Iter: 1507 loss: 3.5318526e-06
Iter: 1508 loss: 3.53013229e-06
Iter: 1509 loss: 3.53078281e-06
Iter: 1510 loss: 3.52900452e-06
Iter: 1511 loss: 3.52822826e-06
Iter: 1512 loss: 3.52787288e-06
Iter: 1513 loss: 3.52674147e-06
Iter: 1514 loss: 3.53058681e-06
Iter: 1515 loss: 3.52632424e-06
Iter: 1516 loss: 3.52591087e-06
Iter: 1517 loss: 3.52444249e-06
Iter: 1518 loss: 3.5335006e-06
Iter: 1519 loss: 3.52417e-06
Iter: 1520 loss: 3.52224106e-06
Iter: 1521 loss: 3.53466157e-06
Iter: 1522 loss: 3.52204233e-06
Iter: 1523 loss: 3.5207504e-06
Iter: 1524 loss: 3.52046095e-06
Iter: 1525 loss: 3.51975291e-06
Iter: 1526 loss: 3.51773497e-06
Iter: 1527 loss: 3.52699021e-06
Iter: 1528 loss: 3.51726612e-06
Iter: 1529 loss: 3.5160715e-06
Iter: 1530 loss: 3.52350344e-06
Iter: 1531 loss: 3.51594235e-06
Iter: 1532 loss: 3.51464382e-06
Iter: 1533 loss: 3.51421295e-06
Iter: 1534 loss: 3.51340395e-06
Iter: 1535 loss: 3.51205722e-06
Iter: 1536 loss: 3.53042151e-06
Iter: 1537 loss: 3.51200151e-06
Iter: 1538 loss: 3.51108702e-06
Iter: 1539 loss: 3.50889309e-06
Iter: 1540 loss: 3.5428543e-06
Iter: 1541 loss: 3.50890309e-06
Iter: 1542 loss: 3.50747268e-06
Iter: 1543 loss: 3.50739083e-06
Iter: 1544 loss: 3.50629875e-06
Iter: 1545 loss: 3.50644405e-06
Iter: 1546 loss: 3.50551727e-06
Iter: 1547 loss: 3.50522282e-06
Iter: 1548 loss: 3.50477148e-06
Iter: 1549 loss: 3.50411847e-06
Iter: 1550 loss: 3.50290111e-06
Iter: 1551 loss: 3.50291475e-06
Iter: 1552 loss: 3.50204573e-06
Iter: 1553 loss: 3.50105938e-06
Iter: 1554 loss: 3.50085793e-06
Iter: 1555 loss: 3.49965535e-06
Iter: 1556 loss: 3.51471681e-06
Iter: 1557 loss: 3.49967286e-06
Iter: 1558 loss: 3.49894685e-06
Iter: 1559 loss: 3.49844572e-06
Iter: 1560 loss: 3.49814263e-06
Iter: 1561 loss: 3.49686638e-06
Iter: 1562 loss: 3.49973493e-06
Iter: 1563 loss: 3.49632978e-06
Iter: 1564 loss: 3.49522634e-06
Iter: 1565 loss: 3.5055034e-06
Iter: 1566 loss: 3.49516836e-06
Iter: 1567 loss: 3.49412676e-06
Iter: 1568 loss: 3.49401171e-06
Iter: 1569 loss: 3.49316883e-06
Iter: 1570 loss: 3.49206175e-06
Iter: 1571 loss: 3.50043979e-06
Iter: 1572 loss: 3.49188213e-06
Iter: 1573 loss: 3.49090874e-06
Iter: 1574 loss: 3.49085462e-06
Iter: 1575 loss: 3.49019365e-06
Iter: 1576 loss: 3.4887662e-06
Iter: 1577 loss: 3.49538141e-06
Iter: 1578 loss: 3.48853587e-06
Iter: 1579 loss: 3.48755498e-06
Iter: 1580 loss: 3.4947102e-06
Iter: 1581 loss: 3.48762114e-06
Iter: 1582 loss: 3.48678623e-06
Iter: 1583 loss: 3.49258062e-06
Iter: 1584 loss: 3.48667913e-06
Iter: 1585 loss: 3.48599178e-06
Iter: 1586 loss: 3.48473554e-06
Iter: 1587 loss: 3.50514392e-06
Iter: 1588 loss: 3.4847285e-06
Iter: 1589 loss: 3.48364506e-06
Iter: 1590 loss: 3.48465232e-06
Iter: 1591 loss: 3.4830482e-06
Iter: 1592 loss: 3.48165077e-06
Iter: 1593 loss: 3.48423237e-06
Iter: 1594 loss: 3.48097728e-06
Iter: 1595 loss: 3.47977539e-06
Iter: 1596 loss: 3.48484446e-06
Iter: 1597 loss: 3.47951391e-06
Iter: 1598 loss: 3.47801824e-06
Iter: 1599 loss: 3.48141498e-06
Iter: 1600 loss: 3.47752075e-06
Iter: 1601 loss: 3.47647438e-06
Iter: 1602 loss: 3.48337767e-06
Iter: 1603 loss: 3.47631294e-06
Iter: 1604 loss: 3.47512e-06
Iter: 1605 loss: 3.47406967e-06
Iter: 1606 loss: 3.47374794e-06
Iter: 1607 loss: 3.47294508e-06
Iter: 1608 loss: 3.47286959e-06
Iter: 1609 loss: 3.4719917e-06
Iter: 1610 loss: 3.47063906e-06
Iter: 1611 loss: 3.47072728e-06
Iter: 1612 loss: 3.46967818e-06
Iter: 1613 loss: 3.46963679e-06
Iter: 1614 loss: 3.46908519e-06
Iter: 1615 loss: 3.46903653e-06
Iter: 1616 loss: 3.4685736e-06
Iter: 1617 loss: 3.46739444e-06
Iter: 1618 loss: 3.48784693e-06
Iter: 1619 loss: 3.46741103e-06
Iter: 1620 loss: 3.46604452e-06
Iter: 1621 loss: 3.46689876e-06
Iter: 1622 loss: 3.46526258e-06
Iter: 1623 loss: 3.46444858e-06
Iter: 1624 loss: 3.46463275e-06
Iter: 1625 loss: 3.46383104e-06
Iter: 1626 loss: 3.46240563e-06
Iter: 1627 loss: 3.46949901e-06
Iter: 1628 loss: 3.46217848e-06
Iter: 1629 loss: 3.46121897e-06
Iter: 1630 loss: 3.46400066e-06
Iter: 1631 loss: 3.46099364e-06
Iter: 1632 loss: 3.45972012e-06
Iter: 1633 loss: 3.45927651e-06
Iter: 1634 loss: 3.45858348e-06
Iter: 1635 loss: 3.45755348e-06
Iter: 1636 loss: 3.45758644e-06
Iter: 1637 loss: 3.45657418e-06
Iter: 1638 loss: 3.45587614e-06
Iter: 1639 loss: 3.45554054e-06
Iter: 1640 loss: 3.45416083e-06
Iter: 1641 loss: 3.46672505e-06
Iter: 1642 loss: 3.45402532e-06
Iter: 1643 loss: 3.45312e-06
Iter: 1644 loss: 3.45526314e-06
Iter: 1645 loss: 3.45298508e-06
Iter: 1646 loss: 3.45197714e-06
Iter: 1647 loss: 3.4539803e-06
Iter: 1648 loss: 3.45145327e-06
Iter: 1649 loss: 3.45045737e-06
Iter: 1650 loss: 3.45032163e-06
Iter: 1651 loss: 3.44998193e-06
Iter: 1652 loss: 3.44910563e-06
Iter: 1653 loss: 3.46435127e-06
Iter: 1654 loss: 3.44910404e-06
Iter: 1655 loss: 3.44825457e-06
Iter: 1656 loss: 3.44719956e-06
Iter: 1657 loss: 3.44701903e-06
Iter: 1658 loss: 3.44551745e-06
Iter: 1659 loss: 3.46096931e-06
Iter: 1660 loss: 3.44551972e-06
Iter: 1661 loss: 3.44466116e-06
Iter: 1662 loss: 3.44448063e-06
Iter: 1663 loss: 3.4439704e-06
Iter: 1664 loss: 3.44244381e-06
Iter: 1665 loss: 3.44524824e-06
Iter: 1666 loss: 3.44193131e-06
Iter: 1667 loss: 3.44050204e-06
Iter: 1668 loss: 3.44364776e-06
Iter: 1669 loss: 3.43996953e-06
Iter: 1670 loss: 3.43838838e-06
Iter: 1671 loss: 3.44962427e-06
Iter: 1672 loss: 3.43824649e-06
Iter: 1673 loss: 3.43735883e-06
Iter: 1674 loss: 3.43881106e-06
Iter: 1675 loss: 3.43684678e-06
Iter: 1676 loss: 3.43564284e-06
Iter: 1677 loss: 3.43815827e-06
Iter: 1678 loss: 3.43507622e-06
Iter: 1679 loss: 3.43431088e-06
Iter: 1680 loss: 3.44471505e-06
Iter: 1681 loss: 3.43426859e-06
Iter: 1682 loss: 3.43379293e-06
Iter: 1683 loss: 3.43578608e-06
Iter: 1684 loss: 3.43369811e-06
Iter: 1685 loss: 3.43273041e-06
Iter: 1686 loss: 3.43239753e-06
Iter: 1687 loss: 3.43203351e-06
Iter: 1688 loss: 3.4311538e-06
Iter: 1689 loss: 3.43268357e-06
Iter: 1690 loss: 3.43074407e-06
Iter: 1691 loss: 3.4300167e-06
Iter: 1692 loss: 3.428936e-06
Iter: 1693 loss: 3.45026911e-06
Iter: 1694 loss: 3.42878366e-06
Iter: 1695 loss: 3.4275381e-06
Iter: 1696 loss: 3.42754447e-06
Iter: 1697 loss: 3.42672547e-06
Iter: 1698 loss: 3.42677731e-06
Iter: 1699 loss: 3.42606018e-06
Iter: 1700 loss: 3.42479211e-06
Iter: 1701 loss: 3.42889598e-06
Iter: 1702 loss: 3.4244074e-06
Iter: 1703 loss: 3.42345811e-06
Iter: 1704 loss: 3.4247048e-06
Iter: 1705 loss: 3.42306339e-06
Iter: 1706 loss: 3.42168073e-06
Iter: 1707 loss: 3.4252962e-06
Iter: 1708 loss: 3.42124713e-06
Iter: 1709 loss: 3.42050816e-06
Iter: 1710 loss: 3.42805151e-06
Iter: 1711 loss: 3.42048907e-06
Iter: 1712 loss: 3.41971145e-06
Iter: 1713 loss: 3.41951477e-06
Iter: 1714 loss: 3.4189743e-06
Iter: 1715 loss: 3.4180639e-06
Iter: 1716 loss: 3.42874796e-06
Iter: 1717 loss: 3.41809573e-06
Iter: 1718 loss: 3.41786563e-06
Iter: 1719 loss: 3.41769532e-06
Iter: 1720 loss: 3.41746477e-06
Iter: 1721 loss: 3.41651298e-06
Iter: 1722 loss: 3.42058956e-06
Iter: 1723 loss: 3.4162158e-06
Iter: 1724 loss: 3.41503869e-06
Iter: 1725 loss: 3.41922259e-06
Iter: 1726 loss: 3.41463556e-06
Iter: 1727 loss: 3.41412056e-06
Iter: 1728 loss: 3.41510349e-06
Iter: 1729 loss: 3.41370151e-06
Iter: 1730 loss: 3.41279724e-06
Iter: 1731 loss: 3.41358532e-06
Iter: 1732 loss: 3.41212217e-06
Iter: 1733 loss: 3.41117175e-06
Iter: 1734 loss: 3.41638884e-06
Iter: 1735 loss: 3.41102395e-06
Iter: 1736 loss: 3.41016e-06
Iter: 1737 loss: 3.40997349e-06
Iter: 1738 loss: 3.40944052e-06
Iter: 1739 loss: 3.40821543e-06
Iter: 1740 loss: 3.41969667e-06
Iter: 1741 loss: 3.40814381e-06
Iter: 1742 loss: 3.4073712e-06
Iter: 1743 loss: 3.40674114e-06
Iter: 1744 loss: 3.40636052e-06
Iter: 1745 loss: 3.40515e-06
Iter: 1746 loss: 3.41371e-06
Iter: 1747 loss: 3.40489896e-06
Iter: 1748 loss: 3.40400152e-06
Iter: 1749 loss: 3.40690804e-06
Iter: 1750 loss: 3.40382599e-06
Iter: 1751 loss: 3.40266388e-06
Iter: 1752 loss: 3.4037887e-06
Iter: 1753 loss: 3.40186625e-06
Iter: 1754 loss: 3.40249881e-06
Iter: 1755 loss: 3.40130737e-06
Iter: 1756 loss: 3.40114229e-06
Iter: 1757 loss: 3.40049337e-06
Iter: 1758 loss: 3.4036093e-06
Iter: 1759 loss: 3.40031966e-06
Iter: 1760 loss: 3.39927556e-06
Iter: 1761 loss: 3.39834128e-06
Iter: 1762 loss: 3.39820394e-06
Iter: 1763 loss: 3.39717508e-06
Iter: 1764 loss: 3.39719e-06
Iter: 1765 loss: 3.39641406e-06
Iter: 1766 loss: 3.39541157e-06
Iter: 1767 loss: 3.39537746e-06
Iter: 1768 loss: 3.39365056e-06
Iter: 1769 loss: 3.40140423e-06
Iter: 1770 loss: 3.39349072e-06
Iter: 1771 loss: 3.39232702e-06
Iter: 1772 loss: 3.39487451e-06
Iter: 1773 loss: 3.39185726e-06
Iter: 1774 loss: 3.3903857e-06
Iter: 1775 loss: 3.39016879e-06
Iter: 1776 loss: 3.38913878e-06
Iter: 1777 loss: 3.38765631e-06
Iter: 1778 loss: 3.40346605e-06
Iter: 1779 loss: 3.3877011e-06
Iter: 1780 loss: 3.38653808e-06
Iter: 1781 loss: 3.38674363e-06
Iter: 1782 loss: 3.38571817e-06
Iter: 1783 loss: 3.38434847e-06
Iter: 1784 loss: 3.39753342e-06
Iter: 1785 loss: 3.38434575e-06
Iter: 1786 loss: 3.38342193e-06
Iter: 1787 loss: 3.38328846e-06
Iter: 1788 loss: 3.38267614e-06
Iter: 1789 loss: 3.38278869e-06
Iter: 1790 loss: 3.38210066e-06
Iter: 1791 loss: 3.38166569e-06
Iter: 1792 loss: 3.38071845e-06
Iter: 1793 loss: 3.3891165e-06
Iter: 1794 loss: 3.38064024e-06
Iter: 1795 loss: 3.37951951e-06
Iter: 1796 loss: 3.37965275e-06
Iter: 1797 loss: 3.37882261e-06
Iter: 1798 loss: 3.37736469e-06
Iter: 1799 loss: 3.38614518e-06
Iter: 1800 loss: 3.37722304e-06
Iter: 1801 loss: 3.37643928e-06
Iter: 1802 loss: 3.37893334e-06
Iter: 1803 loss: 3.37625852e-06
Iter: 1804 loss: 3.37522692e-06
Iter: 1805 loss: 3.37408756e-06
Iter: 1806 loss: 3.37388747e-06
Iter: 1807 loss: 3.37276219e-06
Iter: 1808 loss: 3.38691598e-06
Iter: 1809 loss: 3.37268648e-06
Iter: 1810 loss: 3.37166e-06
Iter: 1811 loss: 3.3704664e-06
Iter: 1812 loss: 3.37035317e-06
Iter: 1813 loss: 3.36894823e-06
Iter: 1814 loss: 3.36898165e-06
Iter: 1815 loss: 3.36811445e-06
Iter: 1816 loss: 3.36709172e-06
Iter: 1817 loss: 3.36694211e-06
Iter: 1818 loss: 3.36530888e-06
Iter: 1819 loss: 3.3777319e-06
Iter: 1820 loss: 3.36512471e-06
Iter: 1821 loss: 3.36466746e-06
Iter: 1822 loss: 3.36466974e-06
Iter: 1823 loss: 3.3639792e-06
Iter: 1824 loss: 3.36271501e-06
Iter: 1825 loss: 3.39102576e-06
Iter: 1826 loss: 3.3626809e-06
Iter: 1827 loss: 3.36173161e-06
Iter: 1828 loss: 3.36336507e-06
Iter: 1829 loss: 3.36129597e-06
Iter: 1830 loss: 3.36026392e-06
Iter: 1831 loss: 3.35988761e-06
Iter: 1832 loss: 3.35927507e-06
Iter: 1833 loss: 3.35772029e-06
Iter: 1834 loss: 3.36955782e-06
Iter: 1835 loss: 3.35756408e-06
Iter: 1836 loss: 3.35681671e-06
Iter: 1837 loss: 3.36154835e-06
Iter: 1838 loss: 3.35664913e-06
Iter: 1839 loss: 3.3558581e-06
Iter: 1840 loss: 3.35459299e-06
Iter: 1841 loss: 3.35449704e-06
Iter: 1842 loss: 3.3533297e-06
Iter: 1843 loss: 3.36425069e-06
Iter: 1844 loss: 3.35324921e-06
Iter: 1845 loss: 3.35232812e-06
Iter: 1846 loss: 3.35184586e-06
Iter: 1847 loss: 3.35141476e-06
Iter: 1848 loss: 3.34992501e-06
Iter: 1849 loss: 3.365461e-06
Iter: 1850 loss: 3.34996048e-06
Iter: 1851 loss: 3.34909214e-06
Iter: 1852 loss: 3.34958122e-06
Iter: 1853 loss: 3.34863e-06
Iter: 1854 loss: 3.34748961e-06
Iter: 1855 loss: 3.35346772e-06
Iter: 1856 loss: 3.34738456e-06
Iter: 1857 loss: 3.34670722e-06
Iter: 1858 loss: 3.34669539e-06
Iter: 1859 loss: 3.34638025e-06
Iter: 1860 loss: 3.34552328e-06
Iter: 1861 loss: 3.34802075e-06
Iter: 1862 loss: 3.34504875e-06
Iter: 1863 loss: 3.34352853e-06
Iter: 1864 loss: 3.34526612e-06
Iter: 1865 loss: 3.34273818e-06
Iter: 1866 loss: 3.34147535e-06
Iter: 1867 loss: 3.35035202e-06
Iter: 1868 loss: 3.34132574e-06
Iter: 1869 loss: 3.34006836e-06
Iter: 1870 loss: 3.3404956e-06
Iter: 1871 loss: 3.33920025e-06
Iter: 1872 loss: 3.3383717e-06
Iter: 1873 loss: 3.3383003e-06
Iter: 1874 loss: 3.33752268e-06
Iter: 1875 loss: 3.33589787e-06
Iter: 1876 loss: 3.361572e-06
Iter: 1877 loss: 3.33589446e-06
Iter: 1878 loss: 3.33463208e-06
Iter: 1879 loss: 3.35373124e-06
Iter: 1880 loss: 3.33463822e-06
Iter: 1881 loss: 3.33366324e-06
Iter: 1882 loss: 3.33250523e-06
Iter: 1883 loss: 3.33241405e-06
Iter: 1884 loss: 3.33106163e-06
Iter: 1885 loss: 3.34310471e-06
Iter: 1886 loss: 3.33097978e-06
Iter: 1887 loss: 3.33023058e-06
Iter: 1888 loss: 3.33316257e-06
Iter: 1889 loss: 3.33011781e-06
Iter: 1890 loss: 3.32940976e-06
Iter: 1891 loss: 3.33575349e-06
Iter: 1892 loss: 3.32931313e-06
Iter: 1893 loss: 3.32878039e-06
Iter: 1894 loss: 3.32799436e-06
Iter: 1895 loss: 3.32795071e-06
Iter: 1896 loss: 3.32720288e-06
Iter: 1897 loss: 3.32623722e-06
Iter: 1898 loss: 3.32608579e-06
Iter: 1899 loss: 3.32459877e-06
Iter: 1900 loss: 3.33382604e-06
Iter: 1901 loss: 3.32434684e-06
Iter: 1902 loss: 3.32312857e-06
Iter: 1903 loss: 3.3236247e-06
Iter: 1904 loss: 3.32213426e-06
Iter: 1905 loss: 3.32042191e-06
Iter: 1906 loss: 3.33086723e-06
Iter: 1907 loss: 3.32012314e-06
Iter: 1908 loss: 3.31884075e-06
Iter: 1909 loss: 3.32281252e-06
Iter: 1910 loss: 3.31853926e-06
Iter: 1911 loss: 3.31706951e-06
Iter: 1912 loss: 3.32151285e-06
Iter: 1913 loss: 3.31671708e-06
Iter: 1914 loss: 3.31571232e-06
Iter: 1915 loss: 3.32082595e-06
Iter: 1916 loss: 3.31553656e-06
Iter: 1917 loss: 3.31450155e-06
Iter: 1918 loss: 3.31335923e-06
Iter: 1919 loss: 3.31310866e-06
Iter: 1920 loss: 3.31176261e-06
Iter: 1921 loss: 3.32534091e-06
Iter: 1922 loss: 3.31179081e-06
Iter: 1923 loss: 3.31207275e-06
Iter: 1924 loss: 3.31148476e-06
Iter: 1925 loss: 3.31103638e-06
Iter: 1926 loss: 3.31025331e-06
Iter: 1927 loss: 3.31653609e-06
Iter: 1928 loss: 3.31001388e-06
Iter: 1929 loss: 3.30898501e-06
Iter: 1930 loss: 3.3104061e-06
Iter: 1931 loss: 3.30854186e-06
Iter: 1932 loss: 3.30731973e-06
Iter: 1933 loss: 3.30733383e-06
Iter: 1934 loss: 3.30641501e-06
Iter: 1935 loss: 3.30440912e-06
Iter: 1936 loss: 3.31193041e-06
Iter: 1937 loss: 3.30407283e-06
Iter: 1938 loss: 3.30281478e-06
Iter: 1939 loss: 3.30614853e-06
Iter: 1940 loss: 3.30240846e-06
Iter: 1941 loss: 3.30080729e-06
Iter: 1942 loss: 3.30117496e-06
Iter: 1943 loss: 3.29957288e-06
Iter: 1944 loss: 3.2982407e-06
Iter: 1945 loss: 3.29827594e-06
Iter: 1946 loss: 3.29723434e-06
Iter: 1947 loss: 3.29640579e-06
Iter: 1948 loss: 3.29602904e-06
Iter: 1949 loss: 3.29407612e-06
Iter: 1950 loss: 3.30626926e-06
Iter: 1951 loss: 3.29398927e-06
Iter: 1952 loss: 3.29286195e-06
Iter: 1953 loss: 3.2928408e-06
Iter: 1954 loss: 3.29191676e-06
Iter: 1955 loss: 3.29042814e-06
Iter: 1956 loss: 3.31061824e-06
Iter: 1957 loss: 3.29046861e-06
Iter: 1958 loss: 3.2890398e-06
Iter: 1959 loss: 3.29814748e-06
Iter: 1960 loss: 3.28896613e-06
Iter: 1961 loss: 3.28839087e-06
Iter: 1962 loss: 3.28732267e-06
Iter: 1963 loss: 3.30555713e-06
Iter: 1964 loss: 3.2873254e-06
Iter: 1965 loss: 3.2859034e-06
Iter: 1966 loss: 3.2853668e-06
Iter: 1967 loss: 3.28454462e-06
Iter: 1968 loss: 3.28324563e-06
Iter: 1969 loss: 3.29832687e-06
Iter: 1970 loss: 3.28331885e-06
Iter: 1971 loss: 3.2821381e-06
Iter: 1972 loss: 3.28146643e-06
Iter: 1973 loss: 3.28090641e-06
Iter: 1974 loss: 3.27931616e-06
Iter: 1975 loss: 3.2963485e-06
Iter: 1976 loss: 3.27929865e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi3
+ date
Mon Oct 26 14:08:30 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi3/300_300_300_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi3_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi3_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi3_8000/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi3/300_300_300_1 --optimizer lbfgs --function f1 --psi 1 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi3_8000/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fafb096b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf8c1f6ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf8c1f6158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf8c1f66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf8c124620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf8c092488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf8c0acbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf8c0acc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf8c028510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70781a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70751ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70765f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf707716a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70707bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70707950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70697048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70697730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70697158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf7067a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf706972f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70697d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70603a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf705a28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf705cc510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf705ccae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70524730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70554bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf704927b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf704a1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70496f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf704ef620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf7044d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf7044d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf70416b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf7042bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faf703c2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.0909e-05
Iter: 2 loss: 2.06337681e-05
Iter: 3 loss: 1.71856464e-05
Iter: 4 loss: 1.52043203e-05
Iter: 5 loss: 1.69530358e-05
Iter: 6 loss: 1.40430993e-05
Iter: 7 loss: 1.33478043e-05
Iter: 8 loss: 1.18581984e-05
Iter: 9 loss: 3.53696778e-05
Iter: 10 loss: 1.18075513e-05
Iter: 11 loss: 1.12265243e-05
Iter: 12 loss: 1.111232e-05
Iter: 13 loss: 1.06386979e-05
Iter: 14 loss: 1.0431695e-05
Iter: 15 loss: 1.01899786e-05
Iter: 16 loss: 9.37384175e-06
Iter: 17 loss: 1.04993878e-05
Iter: 18 loss: 8.9682926e-06
Iter: 19 loss: 8.37917287e-06
Iter: 20 loss: 1.01282858e-05
Iter: 21 loss: 8.19985871e-06
Iter: 22 loss: 7.60664534e-06
Iter: 23 loss: 1.15028979e-05
Iter: 24 loss: 7.54432858e-06
Iter: 25 loss: 7.22064669e-06
Iter: 26 loss: 9.92014702e-06
Iter: 27 loss: 7.20125172e-06
Iter: 28 loss: 7.03150181e-06
Iter: 29 loss: 6.60827254e-06
Iter: 30 loss: 1.06289544e-05
Iter: 31 loss: 6.54990481e-06
Iter: 32 loss: 6.25121265e-06
Iter: 33 loss: 6.24066888e-06
Iter: 34 loss: 6.0290331e-06
Iter: 35 loss: 5.92868219e-06
Iter: 36 loss: 5.82534176e-06
Iter: 37 loss: 5.878921e-06
Iter: 38 loss: 5.72853924e-06
Iter: 39 loss: 5.64526e-06
Iter: 40 loss: 5.96655536e-06
Iter: 41 loss: 5.62573314e-06
Iter: 42 loss: 5.56729674e-06
Iter: 43 loss: 5.3817439e-06
Iter: 44 loss: 5.59624732e-06
Iter: 45 loss: 5.23894596e-06
Iter: 46 loss: 5.0756189e-06
Iter: 47 loss: 5.06909055e-06
Iter: 48 loss: 4.92578874e-06
Iter: 49 loss: 5.33947696e-06
Iter: 50 loss: 4.88116893e-06
Iter: 51 loss: 4.75527258e-06
Iter: 52 loss: 5.34105584e-06
Iter: 53 loss: 4.73188948e-06
Iter: 54 loss: 4.65950416e-06
Iter: 55 loss: 4.60896217e-06
Iter: 56 loss: 4.58307386e-06
Iter: 57 loss: 4.46918148e-06
Iter: 58 loss: 6.06604499e-06
Iter: 59 loss: 4.46879767e-06
Iter: 60 loss: 4.40732947e-06
Iter: 61 loss: 4.50306288e-06
Iter: 62 loss: 4.378413e-06
Iter: 63 loss: 4.29086867e-06
Iter: 64 loss: 4.15865179e-06
Iter: 65 loss: 4.15603199e-06
Iter: 66 loss: 4.04028879e-06
Iter: 67 loss: 5.07763798e-06
Iter: 68 loss: 4.03476815e-06
Iter: 69 loss: 3.93766732e-06
Iter: 70 loss: 4.2094e-06
Iter: 71 loss: 3.90632204e-06
Iter: 72 loss: 3.95100778e-06
Iter: 73 loss: 3.8791195e-06
Iter: 74 loss: 3.85912335e-06
Iter: 75 loss: 3.80389292e-06
Iter: 76 loss: 4.11157771e-06
Iter: 77 loss: 3.78752475e-06
Iter: 78 loss: 3.71784881e-06
Iter: 79 loss: 3.9077695e-06
Iter: 80 loss: 3.69493591e-06
Iter: 81 loss: 3.64728498e-06
Iter: 82 loss: 3.69569852e-06
Iter: 83 loss: 3.62068613e-06
Iter: 84 loss: 3.5624189e-06
Iter: 85 loss: 4.11216661e-06
Iter: 86 loss: 3.56011105e-06
Iter: 87 loss: 3.52456755e-06
Iter: 88 loss: 3.61500679e-06
Iter: 89 loss: 3.51230938e-06
Iter: 90 loss: 3.46978322e-06
Iter: 91 loss: 3.41419241e-06
Iter: 92 loss: 3.41055647e-06
Iter: 93 loss: 3.37935535e-06
Iter: 94 loss: 3.37410688e-06
Iter: 95 loss: 3.33880689e-06
Iter: 96 loss: 3.31282672e-06
Iter: 97 loss: 3.30104876e-06
Iter: 98 loss: 3.24989401e-06
Iter: 99 loss: 3.48721051e-06
Iter: 100 loss: 3.24039047e-06
Iter: 101 loss: 3.20975369e-06
Iter: 102 loss: 3.18435468e-06
Iter: 103 loss: 3.17563627e-06
Iter: 104 loss: 3.14812337e-06
Iter: 105 loss: 3.14419617e-06
Iter: 106 loss: 3.12285397e-06
Iter: 107 loss: 3.44235355e-06
Iter: 108 loss: 3.12279053e-06
Iter: 109 loss: 3.11262602e-06
Iter: 110 loss: 3.08093172e-06
Iter: 111 loss: 3.13804549e-06
Iter: 112 loss: 3.06003017e-06
Iter: 113 loss: 3.02405806e-06
Iter: 114 loss: 3.5798264e-06
Iter: 115 loss: 3.02404283e-06
Iter: 116 loss: 3.00210013e-06
Iter: 117 loss: 2.95790528e-06
Iter: 118 loss: 3.79431503e-06
Iter: 119 loss: 2.95735435e-06
Iter: 120 loss: 2.93953531e-06
Iter: 121 loss: 2.92854429e-06
Iter: 122 loss: 2.91024935e-06
Iter: 123 loss: 2.89890204e-06
Iter: 124 loss: 2.89164791e-06
Iter: 125 loss: 2.85897931e-06
Iter: 126 loss: 2.95096334e-06
Iter: 127 loss: 2.84866746e-06
Iter: 128 loss: 2.82747214e-06
Iter: 129 loss: 2.83019085e-06
Iter: 130 loss: 2.81125313e-06
Iter: 131 loss: 2.77984122e-06
Iter: 132 loss: 3.15560806e-06
Iter: 133 loss: 2.77951835e-06
Iter: 134 loss: 2.76263404e-06
Iter: 135 loss: 2.83286454e-06
Iter: 136 loss: 2.75895263e-06
Iter: 137 loss: 2.74372678e-06
Iter: 138 loss: 2.71358431e-06
Iter: 139 loss: 3.30525518e-06
Iter: 140 loss: 2.71331169e-06
Iter: 141 loss: 2.71031649e-06
Iter: 142 loss: 2.69990255e-06
Iter: 143 loss: 2.68614895e-06
Iter: 144 loss: 2.73265323e-06
Iter: 145 loss: 2.68253621e-06
Iter: 146 loss: 2.67282371e-06
Iter: 147 loss: 2.64869891e-06
Iter: 148 loss: 2.87600369e-06
Iter: 149 loss: 2.64527944e-06
Iter: 150 loss: 2.62372441e-06
Iter: 151 loss: 2.6833784e-06
Iter: 152 loss: 2.61677e-06
Iter: 153 loss: 2.58979026e-06
Iter: 154 loss: 2.68091776e-06
Iter: 155 loss: 2.58257842e-06
Iter: 156 loss: 2.5644531e-06
Iter: 157 loss: 2.63706761e-06
Iter: 158 loss: 2.56044291e-06
Iter: 159 loss: 2.54114093e-06
Iter: 160 loss: 2.62243293e-06
Iter: 161 loss: 2.53698272e-06
Iter: 162 loss: 2.52125938e-06
Iter: 163 loss: 2.56397789e-06
Iter: 164 loss: 2.51600954e-06
Iter: 165 loss: 2.500253e-06
Iter: 166 loss: 2.48880679e-06
Iter: 167 loss: 2.48339984e-06
Iter: 168 loss: 2.47150092e-06
Iter: 169 loss: 2.65296876e-06
Iter: 170 loss: 2.47148591e-06
Iter: 171 loss: 2.45855426e-06
Iter: 172 loss: 2.47716207e-06
Iter: 173 loss: 2.45219371e-06
Iter: 174 loss: 2.43803561e-06
Iter: 175 loss: 2.49277673e-06
Iter: 176 loss: 2.43466275e-06
Iter: 177 loss: 2.42660303e-06
Iter: 178 loss: 2.44214561e-06
Iter: 179 loss: 2.42319857e-06
Iter: 180 loss: 2.40819372e-06
Iter: 181 loss: 2.44576745e-06
Iter: 182 loss: 2.40296231e-06
Iter: 183 loss: 2.39631026e-06
Iter: 184 loss: 2.39537712e-06
Iter: 185 loss: 2.3906241e-06
Iter: 186 loss: 2.38272082e-06
Iter: 187 loss: 2.36673486e-06
Iter: 188 loss: 2.66837151e-06
Iter: 189 loss: 2.36659753e-06
Iter: 190 loss: 2.35119887e-06
Iter: 191 loss: 2.58614477e-06
Iter: 192 loss: 2.35118659e-06
Iter: 193 loss: 2.33909509e-06
Iter: 194 loss: 2.32223647e-06
Iter: 195 loss: 2.32148022e-06
Iter: 196 loss: 2.31020431e-06
Iter: 197 loss: 2.3089342e-06
Iter: 198 loss: 2.29831721e-06
Iter: 199 loss: 2.29800662e-06
Iter: 200 loss: 2.28968588e-06
Iter: 201 loss: 2.2753959e-06
Iter: 202 loss: 2.38492771e-06
Iter: 203 loss: 2.27437977e-06
Iter: 204 loss: 2.26697648e-06
Iter: 205 loss: 2.26113389e-06
Iter: 206 loss: 2.25890358e-06
Iter: 207 loss: 2.24674454e-06
Iter: 208 loss: 2.32207594e-06
Iter: 209 loss: 2.24533869e-06
Iter: 210 loss: 2.23581674e-06
Iter: 211 loss: 2.23227926e-06
Iter: 212 loss: 2.22710423e-06
Iter: 213 loss: 2.22325662e-06
Iter: 214 loss: 2.22082144e-06
Iter: 215 loss: 2.21601726e-06
Iter: 216 loss: 2.24826044e-06
Iter: 217 loss: 2.21549908e-06
Iter: 218 loss: 2.21039681e-06
Iter: 219 loss: 2.2058839e-06
Iter: 220 loss: 2.20445645e-06
Iter: 221 loss: 2.19572257e-06
Iter: 222 loss: 2.18769333e-06
Iter: 223 loss: 2.18558307e-06
Iter: 224 loss: 2.17525439e-06
Iter: 225 loss: 2.17293655e-06
Iter: 226 loss: 2.16631679e-06
Iter: 227 loss: 2.15529508e-06
Iter: 228 loss: 2.32704679e-06
Iter: 229 loss: 2.15528485e-06
Iter: 230 loss: 2.14884858e-06
Iter: 231 loss: 2.14334682e-06
Iter: 232 loss: 2.14142915e-06
Iter: 233 loss: 2.13318117e-06
Iter: 234 loss: 2.21692812e-06
Iter: 235 loss: 2.13294743e-06
Iter: 236 loss: 2.12333316e-06
Iter: 237 loss: 2.11445922e-06
Iter: 238 loss: 2.11207453e-06
Iter: 239 loss: 2.1037572e-06
Iter: 240 loss: 2.1797714e-06
Iter: 241 loss: 2.10331336e-06
Iter: 242 loss: 2.09456311e-06
Iter: 243 loss: 2.13079238e-06
Iter: 244 loss: 2.09271684e-06
Iter: 245 loss: 2.08758797e-06
Iter: 246 loss: 2.11575161e-06
Iter: 247 loss: 2.08682741e-06
Iter: 248 loss: 2.08238134e-06
Iter: 249 loss: 2.07377593e-06
Iter: 250 loss: 2.25417671e-06
Iter: 251 loss: 2.07368794e-06
Iter: 252 loss: 2.07780317e-06
Iter: 253 loss: 2.06929735e-06
Iter: 254 loss: 2.06647746e-06
Iter: 255 loss: 2.06315235e-06
Iter: 256 loss: 2.06277059e-06
Iter: 257 loss: 2.05838637e-06
Iter: 258 loss: 2.05913102e-06
Iter: 259 loss: 2.05521246e-06
Iter: 260 loss: 2.04770413e-06
Iter: 261 loss: 2.05352717e-06
Iter: 262 loss: 2.04321145e-06
Iter: 263 loss: 2.03768695e-06
Iter: 264 loss: 2.04333833e-06
Iter: 265 loss: 2.03459103e-06
Iter: 266 loss: 2.02618867e-06
Iter: 267 loss: 2.04235835e-06
Iter: 268 loss: 2.02263118e-06
Iter: 269 loss: 2.01602484e-06
Iter: 270 loss: 2.01584226e-06
Iter: 271 loss: 2.01058583e-06
Iter: 272 loss: 2.00344084e-06
Iter: 273 loss: 2.00343698e-06
Iter: 274 loss: 1.99868032e-06
Iter: 275 loss: 1.99343322e-06
Iter: 276 loss: 1.99264514e-06
Iter: 277 loss: 1.98661428e-06
Iter: 278 loss: 1.98656608e-06
Iter: 279 loss: 1.98245652e-06
Iter: 280 loss: 1.99267834e-06
Iter: 281 loss: 1.98099133e-06
Iter: 282 loss: 1.97555164e-06
Iter: 283 loss: 1.98245289e-06
Iter: 284 loss: 1.97285544e-06
Iter: 285 loss: 1.97378085e-06
Iter: 286 loss: 1.97096597e-06
Iter: 287 loss: 1.96940073e-06
Iter: 288 loss: 1.96538167e-06
Iter: 289 loss: 1.99850842e-06
Iter: 290 loss: 1.96468227e-06
Iter: 291 loss: 1.95992652e-06
Iter: 292 loss: 1.97722352e-06
Iter: 293 loss: 1.95869416e-06
Iter: 294 loss: 1.95502662e-06
Iter: 295 loss: 1.9739573e-06
Iter: 296 loss: 1.95437678e-06
Iter: 297 loss: 1.95057146e-06
Iter: 298 loss: 1.94413428e-06
Iter: 299 loss: 1.94410222e-06
Iter: 300 loss: 1.93920982e-06
Iter: 301 loss: 1.96976953e-06
Iter: 302 loss: 1.93865299e-06
Iter: 303 loss: 1.93303549e-06
Iter: 304 loss: 1.94137056e-06
Iter: 305 loss: 1.9303825e-06
Iter: 306 loss: 1.9264919e-06
Iter: 307 loss: 1.93427559e-06
Iter: 308 loss: 1.92483344e-06
Iter: 309 loss: 1.91891263e-06
Iter: 310 loss: 1.9301358e-06
Iter: 311 loss: 1.91646404e-06
Iter: 312 loss: 1.91177674e-06
Iter: 313 loss: 1.91357253e-06
Iter: 314 loss: 1.90855144e-06
Iter: 315 loss: 1.9024252e-06
Iter: 316 loss: 1.97322925e-06
Iter: 317 loss: 1.90232424e-06
Iter: 318 loss: 1.89970797e-06
Iter: 319 loss: 1.93074447e-06
Iter: 320 loss: 1.89965203e-06
Iter: 321 loss: 1.89784487e-06
Iter: 322 loss: 1.91607865e-06
Iter: 323 loss: 1.8978219e-06
Iter: 324 loss: 1.89583568e-06
Iter: 325 loss: 1.89080379e-06
Iter: 326 loss: 1.93389906e-06
Iter: 327 loss: 1.88996489e-06
Iter: 328 loss: 1.88646334e-06
Iter: 329 loss: 1.90279138e-06
Iter: 330 loss: 1.88587899e-06
Iter: 331 loss: 1.88239892e-06
Iter: 332 loss: 1.89159346e-06
Iter: 333 loss: 1.8811952e-06
Iter: 334 loss: 1.87724743e-06
Iter: 335 loss: 1.89177649e-06
Iter: 336 loss: 1.87637443e-06
Iter: 337 loss: 1.87377964e-06
Iter: 338 loss: 1.87071623e-06
Iter: 339 loss: 1.87036039e-06
Iter: 340 loss: 1.86514342e-06
Iter: 341 loss: 1.8931654e-06
Iter: 342 loss: 1.86433226e-06
Iter: 343 loss: 1.86097873e-06
Iter: 344 loss: 1.85633758e-06
Iter: 345 loss: 1.8561343e-06
Iter: 346 loss: 1.85008128e-06
Iter: 347 loss: 1.91514459e-06
Iter: 348 loss: 1.84991859e-06
Iter: 349 loss: 1.84480609e-06
Iter: 350 loss: 1.85064664e-06
Iter: 351 loss: 1.84200621e-06
Iter: 352 loss: 1.83799443e-06
Iter: 353 loss: 1.86968646e-06
Iter: 354 loss: 1.83773841e-06
Iter: 355 loss: 1.83359157e-06
Iter: 356 loss: 1.83549878e-06
Iter: 357 loss: 1.83077771e-06
Iter: 358 loss: 1.83436873e-06
Iter: 359 loss: 1.82975793e-06
Iter: 360 loss: 1.82866756e-06
Iter: 361 loss: 1.82601866e-06
Iter: 362 loss: 1.84746773e-06
Iter: 363 loss: 1.82557096e-06
Iter: 364 loss: 1.82193912e-06
Iter: 365 loss: 1.82943609e-06
Iter: 366 loss: 1.82041504e-06
Iter: 367 loss: 1.81747248e-06
Iter: 368 loss: 1.83267321e-06
Iter: 369 loss: 1.81699886e-06
Iter: 370 loss: 1.81424912e-06
Iter: 371 loss: 1.81275061e-06
Iter: 372 loss: 1.81156884e-06
Iter: 373 loss: 1.80970278e-06
Iter: 374 loss: 1.80931e-06
Iter: 375 loss: 1.8081106e-06
Iter: 376 loss: 1.80451684e-06
Iter: 377 loss: 1.81689074e-06
Iter: 378 loss: 1.80298559e-06
Iter: 379 loss: 1.79916242e-06
Iter: 380 loss: 1.79916049e-06
Iter: 381 loss: 1.79617155e-06
Iter: 382 loss: 1.79441577e-06
Iter: 383 loss: 1.79322501e-06
Iter: 384 loss: 1.79053313e-06
Iter: 385 loss: 1.79050414e-06
Iter: 386 loss: 1.7881182e-06
Iter: 387 loss: 1.78479354e-06
Iter: 388 loss: 1.78475125e-06
Iter: 389 loss: 1.78537084e-06
Iter: 390 loss: 1.78313041e-06
Iter: 391 loss: 1.78139476e-06
Iter: 392 loss: 1.78673918e-06
Iter: 393 loss: 1.78084542e-06
Iter: 394 loss: 1.77951119e-06
Iter: 395 loss: 1.77649656e-06
Iter: 396 loss: 1.81616656e-06
Iter: 397 loss: 1.77625373e-06
Iter: 398 loss: 1.77424113e-06
Iter: 399 loss: 1.77425295e-06
Iter: 400 loss: 1.77248876e-06
Iter: 401 loss: 1.77097513e-06
Iter: 402 loss: 1.77051379e-06
Iter: 403 loss: 1.76795902e-06
Iter: 404 loss: 1.78557275e-06
Iter: 405 loss: 1.76774734e-06
Iter: 406 loss: 1.76591197e-06
Iter: 407 loss: 1.77076925e-06
Iter: 408 loss: 1.76523736e-06
Iter: 409 loss: 1.76241554e-06
Iter: 410 loss: 1.76072695e-06
Iter: 411 loss: 1.75952914e-06
Iter: 412 loss: 1.75672449e-06
Iter: 413 loss: 1.77774427e-06
Iter: 414 loss: 1.75649438e-06
Iter: 415 loss: 1.75400032e-06
Iter: 416 loss: 1.75147488e-06
Iter: 417 loss: 1.75099558e-06
Iter: 418 loss: 1.74781769e-06
Iter: 419 loss: 1.76120341e-06
Iter: 420 loss: 1.7471292e-06
Iter: 421 loss: 1.74365675e-06
Iter: 422 loss: 1.75992966e-06
Iter: 423 loss: 1.74308957e-06
Iter: 424 loss: 1.74194361e-06
Iter: 425 loss: 1.74186152e-06
Iter: 426 loss: 1.74021216e-06
Iter: 427 loss: 1.7379632e-06
Iter: 428 loss: 1.73787851e-06
Iter: 429 loss: 1.7360162e-06
Iter: 430 loss: 1.73817409e-06
Iter: 431 loss: 1.73509363e-06
Iter: 432 loss: 1.73301567e-06
Iter: 433 loss: 1.73125011e-06
Iter: 434 loss: 1.73062335e-06
Iter: 435 loss: 1.72924797e-06
Iter: 436 loss: 1.72891168e-06
Iter: 437 loss: 1.72749537e-06
Iter: 438 loss: 1.72434261e-06
Iter: 439 loss: 1.76421349e-06
Iter: 440 loss: 1.72396949e-06
Iter: 441 loss: 1.72205523e-06
Iter: 442 loss: 1.72176044e-06
Iter: 443 loss: 1.72030025e-06
Iter: 444 loss: 1.72201408e-06
Iter: 445 loss: 1.71955548e-06
Iter: 446 loss: 1.71767806e-06
Iter: 447 loss: 1.71591046e-06
Iter: 448 loss: 1.71544593e-06
Iter: 449 loss: 1.71333375e-06
Iter: 450 loss: 1.72936768e-06
Iter: 451 loss: 1.71315924e-06
Iter: 452 loss: 1.7107418e-06
Iter: 453 loss: 1.70878138e-06
Iter: 454 loss: 1.70808494e-06
Iter: 455 loss: 1.70521957e-06
Iter: 456 loss: 1.7221505e-06
Iter: 457 loss: 1.70487829e-06
Iter: 458 loss: 1.70304429e-06
Iter: 459 loss: 1.70300416e-06
Iter: 460 loss: 1.701055e-06
Iter: 461 loss: 1.704959e-06
Iter: 462 loss: 1.70020985e-06
Iter: 463 loss: 1.69949067e-06
Iter: 464 loss: 1.6979202e-06
Iter: 465 loss: 1.71828617e-06
Iter: 466 loss: 1.69779764e-06
Iter: 467 loss: 1.6953943e-06
Iter: 468 loss: 1.70033604e-06
Iter: 469 loss: 1.69436089e-06
Iter: 470 loss: 1.69258283e-06
Iter: 471 loss: 1.69397185e-06
Iter: 472 loss: 1.69146415e-06
Iter: 473 loss: 1.68858674e-06
Iter: 474 loss: 1.71013608e-06
Iter: 475 loss: 1.68837391e-06
Iter: 476 loss: 1.68685733e-06
Iter: 477 loss: 1.69087139e-06
Iter: 478 loss: 1.68635404e-06
Iter: 479 loss: 1.68463043e-06
Iter: 480 loss: 1.68574934e-06
Iter: 481 loss: 1.68356951e-06
Iter: 482 loss: 1.68094925e-06
Iter: 483 loss: 1.68962e-06
Iter: 484 loss: 1.68018801e-06
Iter: 485 loss: 1.67847315e-06
Iter: 486 loss: 1.67552503e-06
Iter: 487 loss: 1.67552082e-06
Iter: 488 loss: 1.67225653e-06
Iter: 489 loss: 1.7170978e-06
Iter: 490 loss: 1.67221697e-06
Iter: 491 loss: 1.67037922e-06
Iter: 492 loss: 1.66907e-06
Iter: 493 loss: 1.66835889e-06
Iter: 494 loss: 1.67052167e-06
Iter: 495 loss: 1.66770985e-06
Iter: 496 loss: 1.66701057e-06
Iter: 497 loss: 1.66643292e-06
Iter: 498 loss: 1.66621658e-06
Iter: 499 loss: 1.66503014e-06
Iter: 500 loss: 1.66263101e-06
Iter: 501 loss: 1.70541819e-06
Iter: 502 loss: 1.66257064e-06
Iter: 503 loss: 1.66057862e-06
Iter: 504 loss: 1.66387224e-06
Iter: 505 loss: 1.65954316e-06
Iter: 506 loss: 1.65763129e-06
Iter: 507 loss: 1.68163683e-06
Iter: 508 loss: 1.65763095e-06
Iter: 509 loss: 1.65628887e-06
Iter: 510 loss: 1.65510505e-06
Iter: 511 loss: 1.65479969e-06
Iter: 512 loss: 1.65340839e-06
Iter: 513 loss: 1.65327367e-06
Iter: 514 loss: 1.65222048e-06
Iter: 515 loss: 1.65226265e-06
Iter: 516 loss: 1.65132121e-06
Iter: 517 loss: 1.64949267e-06
Iter: 518 loss: 1.65141921e-06
Iter: 519 loss: 1.64853691e-06
Iter: 520 loss: 1.6471248e-06
Iter: 521 loss: 1.66222321e-06
Iter: 522 loss: 1.64711048e-06
Iter: 523 loss: 1.64582343e-06
Iter: 524 loss: 1.64401592e-06
Iter: 525 loss: 1.64399432e-06
Iter: 526 loss: 1.64223979e-06
Iter: 527 loss: 1.65832148e-06
Iter: 528 loss: 1.64210837e-06
Iter: 529 loss: 1.64076573e-06
Iter: 530 loss: 1.63822119e-06
Iter: 531 loss: 1.69446957e-06
Iter: 532 loss: 1.63819277e-06
Iter: 533 loss: 1.64220933e-06
Iter: 534 loss: 1.63754044e-06
Iter: 535 loss: 1.63684012e-06
Iter: 536 loss: 1.63631671e-06
Iter: 537 loss: 1.6360857e-06
Iter: 538 loss: 1.63525408e-06
Iter: 539 loss: 1.63338643e-06
Iter: 540 loss: 1.66750965e-06
Iter: 541 loss: 1.63340667e-06
Iter: 542 loss: 1.63186007e-06
Iter: 543 loss: 1.64022617e-06
Iter: 544 loss: 1.63166101e-06
Iter: 545 loss: 1.63003347e-06
Iter: 546 loss: 1.63081825e-06
Iter: 547 loss: 1.62891797e-06
Iter: 548 loss: 1.62758602e-06
Iter: 549 loss: 1.64708217e-06
Iter: 550 loss: 1.62758647e-06
Iter: 551 loss: 1.62621473e-06
Iter: 552 loss: 1.62520246e-06
Iter: 553 loss: 1.62472566e-06
Iter: 554 loss: 1.62302206e-06
Iter: 555 loss: 1.64065807e-06
Iter: 556 loss: 1.62300762e-06
Iter: 557 loss: 1.62176343e-06
Iter: 558 loss: 1.62349829e-06
Iter: 559 loss: 1.62116226e-06
Iter: 560 loss: 1.61951289e-06
Iter: 561 loss: 1.62174354e-06
Iter: 562 loss: 1.61865057e-06
Iter: 563 loss: 1.61696289e-06
Iter: 564 loss: 1.61709795e-06
Iter: 565 loss: 1.61559399e-06
Iter: 566 loss: 1.6140084e-06
Iter: 567 loss: 1.61402363e-06
Iter: 568 loss: 1.61316564e-06
Iter: 569 loss: 1.61314642e-06
Iter: 570 loss: 1.61272919e-06
Iter: 571 loss: 1.61145931e-06
Iter: 572 loss: 1.61254013e-06
Iter: 573 loss: 1.61039543e-06
Iter: 574 loss: 1.60867125e-06
Iter: 575 loss: 1.62719255e-06
Iter: 576 loss: 1.60861521e-06
Iter: 577 loss: 1.60721902e-06
Iter: 578 loss: 1.60526633e-06
Iter: 579 loss: 1.60520585e-06
Iter: 580 loss: 1.60380364e-06
Iter: 581 loss: 1.60368745e-06
Iter: 582 loss: 1.60248601e-06
Iter: 583 loss: 1.60271e-06
Iter: 584 loss: 1.60162756e-06
Iter: 585 loss: 1.599749e-06
Iter: 586 loss: 1.61150012e-06
Iter: 587 loss: 1.59954243e-06
Iter: 588 loss: 1.59849037e-06
Iter: 589 loss: 1.59937076e-06
Iter: 590 loss: 1.59795229e-06
Iter: 591 loss: 1.59657316e-06
Iter: 592 loss: 1.60051525e-06
Iter: 593 loss: 1.59609249e-06
Iter: 594 loss: 1.59483807e-06
Iter: 595 loss: 1.59968681e-06
Iter: 596 loss: 1.59460149e-06
Iter: 597 loss: 1.59361377e-06
Iter: 598 loss: 1.59200727e-06
Iter: 599 loss: 1.59199624e-06
Iter: 600 loss: 1.5931837e-06
Iter: 601 loss: 1.59131173e-06
Iter: 602 loss: 1.59070396e-06
Iter: 603 loss: 1.5898114e-06
Iter: 604 loss: 1.58982562e-06
Iter: 605 loss: 1.58874013e-06
Iter: 606 loss: 1.58714897e-06
Iter: 607 loss: 1.58714261e-06
Iter: 608 loss: 1.58541786e-06
Iter: 609 loss: 1.58971682e-06
Iter: 610 loss: 1.58476098e-06
Iter: 611 loss: 1.58295313e-06
Iter: 612 loss: 1.59929164e-06
Iter: 613 loss: 1.58291368e-06
Iter: 614 loss: 1.58171122e-06
Iter: 615 loss: 1.58036642e-06
Iter: 616 loss: 1.58019657e-06
Iter: 617 loss: 1.57913416e-06
Iter: 618 loss: 1.57888928e-06
Iter: 619 loss: 1.57817351e-06
Iter: 620 loss: 1.57765828e-06
Iter: 621 loss: 1.57737304e-06
Iter: 622 loss: 1.57595105e-06
Iter: 623 loss: 1.57750264e-06
Iter: 624 loss: 1.57515876e-06
Iter: 625 loss: 1.57402587e-06
Iter: 626 loss: 1.57404133e-06
Iter: 627 loss: 1.57338491e-06
Iter: 628 loss: 1.57168097e-06
Iter: 629 loss: 1.58788453e-06
Iter: 630 loss: 1.57144541e-06
Iter: 631 loss: 1.56984947e-06
Iter: 632 loss: 1.56982537e-06
Iter: 633 loss: 1.56942258e-06
Iter: 634 loss: 1.5692101e-06
Iter: 635 loss: 1.56882834e-06
Iter: 636 loss: 1.56763429e-06
Iter: 637 loss: 1.56968099e-06
Iter: 638 loss: 1.56684553e-06
Iter: 639 loss: 1.56574242e-06
Iter: 640 loss: 1.57762906e-06
Iter: 641 loss: 1.56576561e-06
Iter: 642 loss: 1.56463932e-06
Iter: 643 loss: 1.56320084e-06
Iter: 644 loss: 1.56312467e-06
Iter: 645 loss: 1.56163605e-06
Iter: 646 loss: 1.57448017e-06
Iter: 647 loss: 1.56150008e-06
Iter: 648 loss: 1.56025237e-06
Iter: 649 loss: 1.56318129e-06
Iter: 650 loss: 1.55972884e-06
Iter: 651 loss: 1.55879525e-06
Iter: 652 loss: 1.55882185e-06
Iter: 653 loss: 1.558196e-06
Iter: 654 loss: 1.55737791e-06
Iter: 655 loss: 1.55738007e-06
Iter: 656 loss: 1.55620205e-06
Iter: 657 loss: 1.56145165e-06
Iter: 658 loss: 1.55597024e-06
Iter: 659 loss: 1.55517796e-06
Iter: 660 loss: 1.55850523e-06
Iter: 661 loss: 1.55492546e-06
Iter: 662 loss: 1.55394207e-06
Iter: 663 loss: 1.55399289e-06
Iter: 664 loss: 1.55302189e-06
Iter: 665 loss: 1.55361363e-06
Iter: 666 loss: 1.55268879e-06
Iter: 667 loss: 1.55229623e-06
Iter: 668 loss: 1.55172097e-06
Iter: 669 loss: 1.55174052e-06
Iter: 670 loss: 1.55104431e-06
Iter: 671 loss: 1.5503598e-06
Iter: 672 loss: 1.55020132e-06
Iter: 673 loss: 1.54914846e-06
Iter: 674 loss: 1.54901261e-06
Iter: 675 loss: 1.54824954e-06
Iter: 676 loss: 1.546362e-06
Iter: 677 loss: 1.55861574e-06
Iter: 678 loss: 1.54619443e-06
Iter: 679 loss: 1.54505108e-06
Iter: 680 loss: 1.54464647e-06
Iter: 681 loss: 1.54397594e-06
Iter: 682 loss: 1.54279223e-06
Iter: 683 loss: 1.54277666e-06
Iter: 684 loss: 1.54207828e-06
Iter: 685 loss: 1.54495297e-06
Iter: 686 loss: 1.5418907e-06
Iter: 687 loss: 1.54108693e-06
Iter: 688 loss: 1.53983024e-06
Iter: 689 loss: 1.53976828e-06
Iter: 690 loss: 1.53839824e-06
Iter: 691 loss: 1.54485315e-06
Iter: 692 loss: 1.53816291e-06
Iter: 693 loss: 1.53674205e-06
Iter: 694 loss: 1.54250154e-06
Iter: 695 loss: 1.53634096e-06
Iter: 696 loss: 1.53530482e-06
Iter: 697 loss: 1.54112809e-06
Iter: 698 loss: 1.53518499e-06
Iter: 699 loss: 1.5345795e-06
Iter: 700 loss: 1.53458586e-06
Iter: 701 loss: 1.53408632e-06
Iter: 702 loss: 1.53259748e-06
Iter: 703 loss: 1.53674068e-06
Iter: 704 loss: 1.53182759e-06
Iter: 705 loss: 1.53057931e-06
Iter: 706 loss: 1.53798487e-06
Iter: 707 loss: 1.53051542e-06
Iter: 708 loss: 1.52911525e-06
Iter: 709 loss: 1.52971677e-06
Iter: 710 loss: 1.52825885e-06
Iter: 711 loss: 1.5270524e-06
Iter: 712 loss: 1.53242888e-06
Iter: 713 loss: 1.52684731e-06
Iter: 714 loss: 1.52550683e-06
Iter: 715 loss: 1.52882581e-06
Iter: 716 loss: 1.52516282e-06
Iter: 717 loss: 1.5242648e-06
Iter: 718 loss: 1.52743223e-06
Iter: 719 loss: 1.52401776e-06
Iter: 720 loss: 1.52284906e-06
Iter: 721 loss: 1.52565485e-06
Iter: 722 loss: 1.52237578e-06
Iter: 723 loss: 1.52151324e-06
Iter: 724 loss: 1.5249509e-06
Iter: 725 loss: 1.52132338e-06
Iter: 726 loss: 1.52050757e-06
Iter: 727 loss: 1.51921665e-06
Iter: 728 loss: 1.51919539e-06
Iter: 729 loss: 1.5179005e-06
Iter: 730 loss: 1.53727842e-06
Iter: 731 loss: 1.51787253e-06
Iter: 732 loss: 1.51722315e-06
Iter: 733 loss: 1.52697635e-06
Iter: 734 loss: 1.51722043e-06
Iter: 735 loss: 1.51654774e-06
Iter: 736 loss: 1.51827987e-06
Iter: 737 loss: 1.51628228e-06
Iter: 738 loss: 1.51578945e-06
Iter: 739 loss: 1.51521226e-06
Iter: 740 loss: 1.51511756e-06
Iter: 741 loss: 1.51436689e-06
Iter: 742 loss: 1.51281688e-06
Iter: 743 loss: 1.54143936e-06
Iter: 744 loss: 1.51278846e-06
Iter: 745 loss: 1.51158906e-06
Iter: 746 loss: 1.51153461e-06
Iter: 747 loss: 1.51077643e-06
Iter: 748 loss: 1.50948165e-06
Iter: 749 loss: 1.50946835e-06
Iter: 750 loss: 1.50834489e-06
Iter: 751 loss: 1.52599534e-06
Iter: 752 loss: 1.50832125e-06
Iter: 753 loss: 1.50728238e-06
Iter: 754 loss: 1.50860023e-06
Iter: 755 loss: 1.50674919e-06
Iter: 756 loss: 1.50596111e-06
Iter: 757 loss: 1.50596952e-06
Iter: 758 loss: 1.50540575e-06
Iter: 759 loss: 1.50421261e-06
Iter: 760 loss: 1.52305017e-06
Iter: 761 loss: 1.50417327e-06
Iter: 762 loss: 1.50300309e-06
Iter: 763 loss: 1.51142763e-06
Iter: 764 loss: 1.50288338e-06
Iter: 765 loss: 1.50223309e-06
Iter: 766 loss: 1.51090421e-06
Iter: 767 loss: 1.50226401e-06
Iter: 768 loss: 1.50161804e-06
Iter: 769 loss: 1.50296432e-06
Iter: 770 loss: 1.50129881e-06
Iter: 771 loss: 1.50061612e-06
Iter: 772 loss: 1.50153187e-06
Iter: 773 loss: 1.50027472e-06
Iter: 774 loss: 1.49971356e-06
Iter: 775 loss: 1.49828281e-06
Iter: 776 loss: 1.51648612e-06
Iter: 777 loss: 1.49819152e-06
Iter: 778 loss: 1.49680181e-06
Iter: 779 loss: 1.51625534e-06
Iter: 780 loss: 1.49674952e-06
Iter: 781 loss: 1.49606376e-06
Iter: 782 loss: 1.49510856e-06
Iter: 783 loss: 1.49498169e-06
Iter: 784 loss: 1.49391315e-06
Iter: 785 loss: 1.50816516e-06
Iter: 786 loss: 1.49387756e-06
Iter: 787 loss: 1.49296989e-06
Iter: 788 loss: 1.49225593e-06
Iter: 789 loss: 1.49187747e-06
Iter: 790 loss: 1.49133007e-06
Iter: 791 loss: 1.49117159e-06
Iter: 792 loss: 1.49060861e-06
Iter: 793 loss: 1.49013135e-06
Iter: 794 loss: 1.48994081e-06
Iter: 795 loss: 1.48927415e-06
Iter: 796 loss: 1.48964511e-06
Iter: 797 loss: 1.48880099e-06
Iter: 798 loss: 1.48817639e-06
Iter: 799 loss: 1.4870725e-06
Iter: 800 loss: 1.51271979e-06
Iter: 801 loss: 1.48706272e-06
Iter: 802 loss: 1.48574122e-06
Iter: 803 loss: 1.50230278e-06
Iter: 804 loss: 1.48573963e-06
Iter: 805 loss: 1.48519405e-06
Iter: 806 loss: 1.48986669e-06
Iter: 807 loss: 1.48521895e-06
Iter: 808 loss: 1.48441848e-06
Iter: 809 loss: 1.48443019e-06
Iter: 810 loss: 1.48382492e-06
Iter: 811 loss: 1.48329582e-06
Iter: 812 loss: 1.48466006e-06
Iter: 813 loss: 1.48308959e-06
Iter: 814 loss: 1.48254435e-06
Iter: 815 loss: 1.48202867e-06
Iter: 816 loss: 1.48187303e-06
Iter: 817 loss: 1.4808511e-06
Iter: 818 loss: 1.48652214e-06
Iter: 819 loss: 1.48062747e-06
Iter: 820 loss: 1.48005165e-06
Iter: 821 loss: 1.47962544e-06
Iter: 822 loss: 1.47944888e-06
Iter: 823 loss: 1.47836113e-06
Iter: 824 loss: 1.48616027e-06
Iter: 825 loss: 1.47828041e-06
Iter: 826 loss: 1.47754338e-06
Iter: 827 loss: 1.47825722e-06
Iter: 828 loss: 1.47719823e-06
Iter: 829 loss: 1.47617629e-06
Iter: 830 loss: 1.4823969e-06
Iter: 831 loss: 1.47606818e-06
Iter: 832 loss: 1.47544733e-06
Iter: 833 loss: 1.47732112e-06
Iter: 834 loss: 1.47529079e-06
Iter: 835 loss: 1.47470212e-06
Iter: 836 loss: 1.47449066e-06
Iter: 837 loss: 1.47416904e-06
Iter: 838 loss: 1.47352773e-06
Iter: 839 loss: 1.4735225e-06
Iter: 840 loss: 1.47300193e-06
Iter: 841 loss: 1.47529454e-06
Iter: 842 loss: 1.47296043e-06
Iter: 843 loss: 1.47263108e-06
Iter: 844 loss: 1.47199091e-06
Iter: 845 loss: 1.48036224e-06
Iter: 846 loss: 1.47184494e-06
Iter: 847 loss: 1.47079845e-06
Iter: 848 loss: 1.47422134e-06
Iter: 849 loss: 1.470499e-06
Iter: 850 loss: 1.4698478e-06
Iter: 851 loss: 1.47276978e-06
Iter: 852 loss: 1.46968478e-06
Iter: 853 loss: 1.46897332e-06
Iter: 854 loss: 1.46797106e-06
Iter: 855 loss: 1.46793082e-06
Iter: 856 loss: 1.46686205e-06
Iter: 857 loss: 1.48145682e-06
Iter: 858 loss: 1.46682976e-06
Iter: 859 loss: 1.4660809e-06
Iter: 860 loss: 1.46567311e-06
Iter: 861 loss: 1.46534296e-06
Iter: 862 loss: 1.46457069e-06
Iter: 863 loss: 1.46453192e-06
Iter: 864 loss: 1.46387765e-06
Iter: 865 loss: 1.46358911e-06
Iter: 866 loss: 1.46326011e-06
Iter: 867 loss: 1.4623788e-06
Iter: 868 loss: 1.46613093e-06
Iter: 869 loss: 1.4622301e-06
Iter: 870 loss: 1.46176353e-06
Iter: 871 loss: 1.4672105e-06
Iter: 872 loss: 1.46180651e-06
Iter: 873 loss: 1.46130969e-06
Iter: 874 loss: 1.46329785e-06
Iter: 875 loss: 1.46121988e-06
Iter: 876 loss: 1.46079242e-06
Iter: 877 loss: 1.46006028e-06
Iter: 878 loss: 1.46007324e-06
Iter: 879 loss: 1.45947763e-06
Iter: 880 loss: 1.46255888e-06
Iter: 881 loss: 1.45935792e-06
Iter: 882 loss: 1.4587356e-06
Iter: 883 loss: 1.45794297e-06
Iter: 884 loss: 1.45793149e-06
Iter: 885 loss: 1.45697493e-06
Iter: 886 loss: 1.46624461e-06
Iter: 887 loss: 1.45697436e-06
Iter: 888 loss: 1.45628189e-06
Iter: 889 loss: 1.45582726e-06
Iter: 890 loss: 1.45557601e-06
Iter: 891 loss: 1.45465492e-06
Iter: 892 loss: 1.46448451e-06
Iter: 893 loss: 1.4546647e-06
Iter: 894 loss: 1.45390391e-06
Iter: 895 loss: 1.45405488e-06
Iter: 896 loss: 1.45335935e-06
Iter: 897 loss: 1.45211902e-06
Iter: 898 loss: 1.45791273e-06
Iter: 899 loss: 1.45189119e-06
Iter: 900 loss: 1.45122499e-06
Iter: 901 loss: 1.45201295e-06
Iter: 902 loss: 1.45088666e-06
Iter: 903 loss: 1.45001684e-06
Iter: 904 loss: 1.45226704e-06
Iter: 905 loss: 1.4497291e-06
Iter: 906 loss: 1.44914611e-06
Iter: 907 loss: 1.44908961e-06
Iter: 908 loss: 1.44877254e-06
Iter: 909 loss: 1.44850083e-06
Iter: 910 loss: 1.44833837e-06
Iter: 911 loss: 1.44789465e-06
Iter: 912 loss: 1.44702744e-06
Iter: 913 loss: 1.44703563e-06
Iter: 914 loss: 1.44605428e-06
Iter: 915 loss: 1.4562529e-06
Iter: 916 loss: 1.44604019e-06
Iter: 917 loss: 1.445507e-06
Iter: 918 loss: 1.44549722e-06
Iter: 919 loss: 1.44509829e-06
Iter: 920 loss: 1.44418414e-06
Iter: 921 loss: 1.44613273e-06
Iter: 922 loss: 1.44391117e-06
Iter: 923 loss: 1.44327987e-06
Iter: 924 loss: 1.44395972e-06
Iter: 925 loss: 1.44285173e-06
Iter: 926 loss: 1.44191426e-06
Iter: 927 loss: 1.44513365e-06
Iter: 928 loss: 1.44163619e-06
Iter: 929 loss: 1.44103797e-06
Iter: 930 loss: 1.44949593e-06
Iter: 931 loss: 1.44105354e-06
Iter: 932 loss: 1.44040791e-06
Iter: 933 loss: 1.43970146e-06
Iter: 934 loss: 1.43959301e-06
Iter: 935 loss: 1.43895e-06
Iter: 936 loss: 1.43892669e-06
Iter: 937 loss: 1.43853106e-06
Iter: 938 loss: 1.4440617e-06
Iter: 939 loss: 1.43850752e-06
Iter: 940 loss: 1.43808779e-06
Iter: 941 loss: 1.43746263e-06
Iter: 942 loss: 1.43746308e-06
Iter: 943 loss: 1.43685452e-06
Iter: 944 loss: 1.43898387e-06
Iter: 945 loss: 1.43662226e-06
Iter: 946 loss: 1.43617649e-06
Iter: 947 loss: 1.43629302e-06
Iter: 948 loss: 1.43578427e-06
Iter: 949 loss: 1.43500552e-06
Iter: 950 loss: 1.43716704e-06
Iter: 951 loss: 1.43469583e-06
Iter: 952 loss: 1.43413763e-06
Iter: 953 loss: 1.43489819e-06
Iter: 954 loss: 1.43390548e-06
Iter: 955 loss: 1.43301509e-06
Iter: 956 loss: 1.4333383e-06
Iter: 957 loss: 1.43237844e-06
Iter: 958 loss: 1.4316015e-06
Iter: 959 loss: 1.435596e-06
Iter: 960 loss: 1.43145326e-06
Iter: 961 loss: 1.43059901e-06
Iter: 962 loss: 1.43508578e-06
Iter: 963 loss: 1.43047441e-06
Iter: 964 loss: 1.4299867e-06
Iter: 965 loss: 1.43536465e-06
Iter: 966 loss: 1.42994008e-06
Iter: 967 loss: 1.42960698e-06
Iter: 968 loss: 1.42883346e-06
Iter: 969 loss: 1.43871978e-06
Iter: 970 loss: 1.42879367e-06
Iter: 971 loss: 1.42886881e-06
Iter: 972 loss: 1.42834756e-06
Iter: 973 loss: 1.42802173e-06
Iter: 974 loss: 1.42874478e-06
Iter: 975 loss: 1.42792078e-06
Iter: 976 loss: 1.42762337e-06
Iter: 977 loss: 1.42696229e-06
Iter: 978 loss: 1.43341845e-06
Iter: 979 loss: 1.42691704e-06
Iter: 980 loss: 1.42607723e-06
Iter: 981 loss: 1.4325542e-06
Iter: 982 loss: 1.42604063e-06
Iter: 983 loss: 1.42555064e-06
Iter: 984 loss: 1.42735632e-06
Iter: 985 loss: 1.42543058e-06
Iter: 986 loss: 1.42484214e-06
Iter: 987 loss: 1.42389285e-06
Iter: 988 loss: 1.42391423e-06
Iter: 989 loss: 1.42305703e-06
Iter: 990 loss: 1.43190539e-06
Iter: 991 loss: 1.42304384e-06
Iter: 992 loss: 1.42231806e-06
Iter: 993 loss: 1.422575e-06
Iter: 994 loss: 1.42182751e-06
Iter: 995 loss: 1.42121758e-06
Iter: 996 loss: 1.42769159e-06
Iter: 997 loss: 1.42122826e-06
Iter: 998 loss: 1.42057843e-06
Iter: 999 loss: 1.42159604e-06
Iter: 1000 loss: 1.42036879e-06
Iter: 1001 loss: 1.41965745e-06
Iter: 1002 loss: 1.421447e-06
Iter: 1003 loss: 1.41939324e-06
Iter: 1004 loss: 1.41884152e-06
Iter: 1005 loss: 1.42012118e-06
Iter: 1006 loss: 1.41863586e-06
Iter: 1007 loss: 1.41804344e-06
Iter: 1008 loss: 1.41804469e-06
Iter: 1009 loss: 1.41779356e-06
Iter: 1010 loss: 1.41730766e-06
Iter: 1011 loss: 1.4173263e-06
Iter: 1012 loss: 1.41678902e-06
Iter: 1013 loss: 1.41648616e-06
Iter: 1014 loss: 1.41630369e-06
Iter: 1015 loss: 1.41545445e-06
Iter: 1016 loss: 1.42229055e-06
Iter: 1017 loss: 1.41546184e-06
Iter: 1018 loss: 1.41494684e-06
Iter: 1019 loss: 1.41567818e-06
Iter: 1020 loss: 1.41471446e-06
Iter: 1021 loss: 1.41403939e-06
Iter: 1022 loss: 1.41435248e-06
Iter: 1023 loss: 1.41351927e-06
Iter: 1024 loss: 1.41296118e-06
Iter: 1025 loss: 1.41580017e-06
Iter: 1026 loss: 1.41289013e-06
Iter: 1027 loss: 1.4122586e-06
Iter: 1028 loss: 1.4123857e-06
Iter: 1029 loss: 1.41172166e-06
Iter: 1030 loss: 1.41124906e-06
Iter: 1031 loss: 1.411261e-06
Iter: 1032 loss: 1.41068904e-06
Iter: 1033 loss: 1.41031069e-06
Iter: 1034 loss: 1.41012629e-06
Iter: 1035 loss: 1.40934549e-06
Iter: 1036 loss: 1.41377018e-06
Iter: 1037 loss: 1.40930558e-06
Iter: 1038 loss: 1.40919224e-06
Iter: 1039 loss: 1.40905854e-06
Iter: 1040 loss: 1.40882423e-06
Iter: 1041 loss: 1.4082359e-06
Iter: 1042 loss: 1.41181704e-06
Iter: 1043 loss: 1.40804366e-06
Iter: 1044 loss: 1.40755708e-06
Iter: 1045 loss: 1.41203873e-06
Iter: 1046 loss: 1.40748614e-06
Iter: 1047 loss: 1.4070547e-06
Iter: 1048 loss: 1.40715224e-06
Iter: 1049 loss: 1.40675388e-06
Iter: 1050 loss: 1.4059799e-06
Iter: 1051 loss: 1.40676116e-06
Iter: 1052 loss: 1.40552061e-06
Iter: 1053 loss: 1.40500867e-06
Iter: 1054 loss: 1.40753252e-06
Iter: 1055 loss: 1.40484e-06
Iter: 1056 loss: 1.40430052e-06
Iter: 1057 loss: 1.40395218e-06
Iter: 1058 loss: 1.40369866e-06
Iter: 1059 loss: 1.4032579e-06
Iter: 1060 loss: 1.41012288e-06
Iter: 1061 loss: 1.40324653e-06
Iter: 1062 loss: 1.40278667e-06
Iter: 1063 loss: 1.40247391e-06
Iter: 1064 loss: 1.40228519e-06
Iter: 1065 loss: 1.40182578e-06
Iter: 1066 loss: 1.40182215e-06
Iter: 1067 loss: 1.40141481e-06
Iter: 1068 loss: 1.40093221e-06
Iter: 1069 loss: 1.40085422e-06
Iter: 1070 loss: 1.40036286e-06
Iter: 1071 loss: 1.4058935e-06
Iter: 1072 loss: 1.40036036e-06
Iter: 1073 loss: 1.40019029e-06
Iter: 1074 loss: 1.4001655e-06
Iter: 1075 loss: 1.39999327e-06
Iter: 1076 loss: 1.39951237e-06
Iter: 1077 loss: 1.39941562e-06
Iter: 1078 loss: 1.3990217e-06
Iter: 1079 loss: 1.39833332e-06
Iter: 1080 loss: 1.40207385e-06
Iter: 1081 loss: 1.39827921e-06
Iter: 1082 loss: 1.39753615e-06
Iter: 1083 loss: 1.40122563e-06
Iter: 1084 loss: 1.39741724e-06
Iter: 1085 loss: 1.39699773e-06
Iter: 1086 loss: 1.39864301e-06
Iter: 1087 loss: 1.39683902e-06
Iter: 1088 loss: 1.39634335e-06
Iter: 1089 loss: 1.39588042e-06
Iter: 1090 loss: 1.39576412e-06
Iter: 1091 loss: 1.39528402e-06
Iter: 1092 loss: 1.40249324e-06
Iter: 1093 loss: 1.39528106e-06
Iter: 1094 loss: 1.39480062e-06
Iter: 1095 loss: 1.39385907e-06
Iter: 1096 loss: 1.41188764e-06
Iter: 1097 loss: 1.39383974e-06
Iter: 1098 loss: 1.39325084e-06
Iter: 1099 loss: 1.39325118e-06
Iter: 1100 loss: 1.39275051e-06
Iter: 1101 loss: 1.39452186e-06
Iter: 1102 loss: 1.39264216e-06
Iter: 1103 loss: 1.39221254e-06
Iter: 1104 loss: 1.39444455e-06
Iter: 1105 loss: 1.39218855e-06
Iter: 1106 loss: 1.39192457e-06
Iter: 1107 loss: 1.39134659e-06
Iter: 1108 loss: 1.39850863e-06
Iter: 1109 loss: 1.39135773e-06
Iter: 1110 loss: 1.39173039e-06
Iter: 1111 loss: 1.39107419e-06
Iter: 1112 loss: 1.39090798e-06
Iter: 1113 loss: 1.39080316e-06
Iter: 1114 loss: 1.39074257e-06
Iter: 1115 loss: 1.39048416e-06
Iter: 1116 loss: 1.39008648e-06
Iter: 1117 loss: 1.39011127e-06
Iter: 1118 loss: 1.38960525e-06
Iter: 1119 loss: 1.39567317e-06
Iter: 1120 loss: 1.38955897e-06
Iter: 1121 loss: 1.3893075e-06
Iter: 1122 loss: 1.38970086e-06
Iter: 1123 loss: 1.38915834e-06
Iter: 1124 loss: 1.38877454e-06
Iter: 1125 loss: 1.38841949e-06
Iter: 1126 loss: 1.38834025e-06
Iter: 1127 loss: 1.38782127e-06
Iter: 1128 loss: 1.38864289e-06
Iter: 1129 loss: 1.38758844e-06
Iter: 1130 loss: 1.38684391e-06
Iter: 1131 loss: 1.39038798e-06
Iter: 1132 loss: 1.38670543e-06
Iter: 1133 loss: 1.3861968e-06
Iter: 1134 loss: 1.3862757e-06
Iter: 1135 loss: 1.38585608e-06
Iter: 1136 loss: 1.38529288e-06
Iter: 1137 loss: 1.39378949e-06
Iter: 1138 loss: 1.38527344e-06
Iter: 1139 loss: 1.38497489e-06
Iter: 1140 loss: 1.38660016e-06
Iter: 1141 loss: 1.38490498e-06
Iter: 1142 loss: 1.38465452e-06
Iter: 1143 loss: 1.3851859e-06
Iter: 1144 loss: 1.38450491e-06
Iter: 1145 loss: 1.38415976e-06
Iter: 1146 loss: 1.38633595e-06
Iter: 1147 loss: 1.38411588e-06
Iter: 1148 loss: 1.38393261e-06
Iter: 1149 loss: 1.38366306e-06
Iter: 1150 loss: 1.38911901e-06
Iter: 1151 loss: 1.38364078e-06
Iter: 1152 loss: 1.38325549e-06
Iter: 1153 loss: 1.38313294e-06
Iter: 1154 loss: 1.3828959e-06
Iter: 1155 loss: 1.38244025e-06
Iter: 1156 loss: 1.38796941e-06
Iter: 1157 loss: 1.3824399e-06
Iter: 1158 loss: 1.38209134e-06
Iter: 1159 loss: 1.38222072e-06
Iter: 1160 loss: 1.38184009e-06
Iter: 1161 loss: 1.38127234e-06
Iter: 1162 loss: 1.38279916e-06
Iter: 1163 loss: 1.38114342e-06
Iter: 1164 loss: 1.38064047e-06
Iter: 1165 loss: 1.38055748e-06
Iter: 1166 loss: 1.38028236e-06
Iter: 1167 loss: 1.37974439e-06
Iter: 1168 loss: 1.38529276e-06
Iter: 1169 loss: 1.37970085e-06
Iter: 1170 loss: 1.37935081e-06
Iter: 1171 loss: 1.37908273e-06
Iter: 1172 loss: 1.37895722e-06
Iter: 1173 loss: 1.37854863e-06
Iter: 1174 loss: 1.37852578e-06
Iter: 1175 loss: 1.37830534e-06
Iter: 1176 loss: 1.37929328e-06
Iter: 1177 loss: 1.37822235e-06
Iter: 1178 loss: 1.37797508e-06
Iter: 1179 loss: 1.3797935e-06
Iter: 1180 loss: 1.3779694e-06
Iter: 1181 loss: 1.3777576e-06
Iter: 1182 loss: 1.37728841e-06
Iter: 1183 loss: 1.38458404e-06
Iter: 1184 loss: 1.37729148e-06
Iter: 1185 loss: 1.37685788e-06
Iter: 1186 loss: 1.37744132e-06
Iter: 1187 loss: 1.37671748e-06
Iter: 1188 loss: 1.376059e-06
Iter: 1189 loss: 1.3761412e-06
Iter: 1190 loss: 1.37561904e-06
Iter: 1191 loss: 1.37505072e-06
Iter: 1192 loss: 1.38159953e-06
Iter: 1193 loss: 1.37506754e-06
Iter: 1194 loss: 1.37457459e-06
Iter: 1195 loss: 1.37531515e-06
Iter: 1196 loss: 1.37432085e-06
Iter: 1197 loss: 1.37377015e-06
Iter: 1198 loss: 1.37487018e-06
Iter: 1199 loss: 1.3734799e-06
Iter: 1200 loss: 1.37298548e-06
Iter: 1201 loss: 1.37351844e-06
Iter: 1202 loss: 1.37277425e-06
Iter: 1203 loss: 1.37220081e-06
Iter: 1204 loss: 1.37516895e-06
Iter: 1205 loss: 1.37205825e-06
Iter: 1206 loss: 1.3716699e-06
Iter: 1207 loss: 1.37147299e-06
Iter: 1208 loss: 1.3712546e-06
Iter: 1209 loss: 1.37063944e-06
Iter: 1210 loss: 1.37065217e-06
Iter: 1211 loss: 1.37041593e-06
Iter: 1212 loss: 1.37040297e-06
Iter: 1213 loss: 1.37020629e-06
Iter: 1214 loss: 1.37004167e-06
Iter: 1215 loss: 1.36995629e-06
Iter: 1216 loss: 1.36961262e-06
Iter: 1217 loss: 1.36899803e-06
Iter: 1218 loss: 1.36899484e-06
Iter: 1219 loss: 1.36855783e-06
Iter: 1220 loss: 1.36996414e-06
Iter: 1221 loss: 1.36832887e-06
Iter: 1222 loss: 1.36775566e-06
Iter: 1223 loss: 1.36852714e-06
Iter: 1224 loss: 1.3674146e-06
Iter: 1225 loss: 1.36700214e-06
Iter: 1226 loss: 1.37292454e-06
Iter: 1227 loss: 1.36696212e-06
Iter: 1228 loss: 1.36652602e-06
Iter: 1229 loss: 1.36646167e-06
Iter: 1230 loss: 1.36620395e-06
Iter: 1231 loss: 1.3655349e-06
Iter: 1232 loss: 1.36740482e-06
Iter: 1233 loss: 1.36531594e-06
Iter: 1234 loss: 1.36483061e-06
Iter: 1235 loss: 1.36520555e-06
Iter: 1236 loss: 1.36452024e-06
Iter: 1237 loss: 1.36378185e-06
Iter: 1238 loss: 1.36632229e-06
Iter: 1239 loss: 1.36365679e-06
Iter: 1240 loss: 1.36314839e-06
Iter: 1241 loss: 1.36330118e-06
Iter: 1242 loss: 1.36277436e-06
Iter: 1243 loss: 1.36238259e-06
Iter: 1244 loss: 1.36230221e-06
Iter: 1245 loss: 1.36205358e-06
Iter: 1246 loss: 1.36207132e-06
Iter: 1247 loss: 1.3619142e-06
Iter: 1248 loss: 1.36156291e-06
Iter: 1249 loss: 1.36156143e-06
Iter: 1250 loss: 1.36119434e-06
Iter: 1251 loss: 1.36086453e-06
Iter: 1252 loss: 1.36072254e-06
Iter: 1253 loss: 1.36025574e-06
Iter: 1254 loss: 1.36162203e-06
Iter: 1255 loss: 1.36012454e-06
Iter: 1256 loss: 1.35953815e-06
Iter: 1257 loss: 1.36056155e-06
Iter: 1258 loss: 1.35925336e-06
Iter: 1259 loss: 1.35884352e-06
Iter: 1260 loss: 1.36303152e-06
Iter: 1261 loss: 1.35886944e-06
Iter: 1262 loss: 1.35842333e-06
Iter: 1263 loss: 1.35802088e-06
Iter: 1264 loss: 1.35789276e-06
Iter: 1265 loss: 1.35731216e-06
Iter: 1266 loss: 1.3622996e-06
Iter: 1267 loss: 1.3572444e-06
Iter: 1268 loss: 1.35677533e-06
Iter: 1269 loss: 1.35624805e-06
Iter: 1270 loss: 1.35611344e-06
Iter: 1271 loss: 1.35547225e-06
Iter: 1272 loss: 1.36145877e-06
Iter: 1273 loss: 1.35541472e-06
Iter: 1274 loss: 1.35481719e-06
Iter: 1275 loss: 1.35572282e-06
Iter: 1276 loss: 1.35450227e-06
Iter: 1277 loss: 1.35441746e-06
Iter: 1278 loss: 1.35416121e-06
Iter: 1279 loss: 1.35395362e-06
Iter: 1280 loss: 1.35443361e-06
Iter: 1281 loss: 1.3538754e-06
Iter: 1282 loss: 1.35365656e-06
Iter: 1283 loss: 1.35311222e-06
Iter: 1284 loss: 1.36187066e-06
Iter: 1285 loss: 1.35308937e-06
Iter: 1286 loss: 1.35247e-06
Iter: 1287 loss: 1.35502432e-06
Iter: 1288 loss: 1.35234473e-06
Iter: 1289 loss: 1.35199014e-06
Iter: 1290 loss: 1.35214214e-06
Iter: 1291 loss: 1.3517656e-06
Iter: 1292 loss: 1.35115e-06
Iter: 1293 loss: 1.35230175e-06
Iter: 1294 loss: 1.35095343e-06
Iter: 1295 loss: 1.35052733e-06
Iter: 1296 loss: 1.354961e-06
Iter: 1297 loss: 1.35049777e-06
Iter: 1298 loss: 1.35000414e-06
Iter: 1299 loss: 1.34937454e-06
Iter: 1300 loss: 1.34934271e-06
Iter: 1301 loss: 1.34865104e-06
Iter: 1302 loss: 1.35622702e-06
Iter: 1303 loss: 1.34867923e-06
Iter: 1304 loss: 1.34816969e-06
Iter: 1305 loss: 1.3474812e-06
Iter: 1306 loss: 1.34743323e-06
Iter: 1307 loss: 1.34682432e-06
Iter: 1308 loss: 1.34682341e-06
Iter: 1309 loss: 1.34639663e-06
Iter: 1310 loss: 1.34682909e-06
Iter: 1311 loss: 1.34619063e-06
Iter: 1312 loss: 1.34576317e-06
Iter: 1313 loss: 1.34574771e-06
Iter: 1314 loss: 1.3455757e-06
Iter: 1315 loss: 1.34552261e-06
Iter: 1316 loss: 1.34540232e-06
Iter: 1317 loss: 1.34515312e-06
Iter: 1318 loss: 1.3447717e-06
Iter: 1319 loss: 1.34474726e-06
Iter: 1320 loss: 1.3442218e-06
Iter: 1321 loss: 1.34635729e-06
Iter: 1322 loss: 1.34412142e-06
Iter: 1323 loss: 1.34368668e-06
Iter: 1324 loss: 1.3437525e-06
Iter: 1325 loss: 1.34342349e-06
Iter: 1326 loss: 1.34279537e-06
Iter: 1327 loss: 1.34630773e-06
Iter: 1328 loss: 1.34275137e-06
Iter: 1329 loss: 1.34237894e-06
Iter: 1330 loss: 1.34445963e-06
Iter: 1331 loss: 1.3423097e-06
Iter: 1332 loss: 1.34192169e-06
Iter: 1333 loss: 1.34205743e-06
Iter: 1334 loss: 1.34159654e-06
Iter: 1335 loss: 1.34123638e-06
Iter: 1336 loss: 1.34308493e-06
Iter: 1337 loss: 1.34119603e-06
Iter: 1338 loss: 1.34071229e-06
Iter: 1339 loss: 1.34066408e-06
Iter: 1340 loss: 1.34042705e-06
Iter: 1341 loss: 1.33993535e-06
Iter: 1342 loss: 1.34128663e-06
Iter: 1343 loss: 1.33977267e-06
Iter: 1344 loss: 1.33924527e-06
Iter: 1345 loss: 1.34327354e-06
Iter: 1346 loss: 1.33922072e-06
Iter: 1347 loss: 1.33876642e-06
Iter: 1348 loss: 1.33879018e-06
Iter: 1349 loss: 1.33864069e-06
Iter: 1350 loss: 1.33845265e-06
Iter: 1351 loss: 1.33844071e-06
Iter: 1352 loss: 1.33807885e-06
Iter: 1353 loss: 1.33812887e-06
Iter: 1354 loss: 1.33778576e-06
Iter: 1355 loss: 1.33739536e-06
Iter: 1356 loss: 1.33881804e-06
Iter: 1357 loss: 1.33730191e-06
Iter: 1358 loss: 1.33693618e-06
Iter: 1359 loss: 1.33641663e-06
Iter: 1360 loss: 1.33637388e-06
Iter: 1361 loss: 1.33592903e-06
Iter: 1362 loss: 1.33591857e-06
Iter: 1363 loss: 1.33555034e-06
Iter: 1364 loss: 1.33546087e-06
Iter: 1365 loss: 1.33527283e-06
Iter: 1366 loss: 1.33469973e-06
Iter: 1367 loss: 1.33964886e-06
Iter: 1368 loss: 1.33472554e-06
Iter: 1369 loss: 1.33443928e-06
Iter: 1370 loss: 1.33393496e-06
Iter: 1371 loss: 1.34541597e-06
Iter: 1372 loss: 1.33394303e-06
Iter: 1373 loss: 1.33326512e-06
Iter: 1374 loss: 1.33918752e-06
Iter: 1375 loss: 1.33327694e-06
Iter: 1376 loss: 1.33286892e-06
Iter: 1377 loss: 1.33278991e-06
Iter: 1378 loss: 1.33252217e-06
Iter: 1379 loss: 1.33217736e-06
Iter: 1380 loss: 1.33213143e-06
Iter: 1381 loss: 1.33182107e-06
Iter: 1382 loss: 1.33481967e-06
Iter: 1383 loss: 1.33177446e-06
Iter: 1384 loss: 1.33168044e-06
Iter: 1385 loss: 1.33121887e-06
Iter: 1386 loss: 1.33206925e-06
Iter: 1387 loss: 1.33102105e-06
Iter: 1388 loss: 1.33056471e-06
Iter: 1389 loss: 1.33753292e-06
Iter: 1390 loss: 1.3305621e-06
Iter: 1391 loss: 1.33011565e-06
Iter: 1392 loss: 1.33103163e-06
Iter: 1393 loss: 1.32993387e-06
Iter: 1394 loss: 1.32952573e-06
Iter: 1395 loss: 1.3302747e-06
Iter: 1396 loss: 1.32931632e-06
Iter: 1397 loss: 1.32896707e-06
Iter: 1398 loss: 1.32886385e-06
Iter: 1399 loss: 1.32863454e-06
Iter: 1400 loss: 1.3280187e-06
Iter: 1401 loss: 1.33150479e-06
Iter: 1402 loss: 1.3279514e-06
Iter: 1403 loss: 1.32749778e-06
Iter: 1404 loss: 1.32928449e-06
Iter: 1405 loss: 1.32736955e-06
Iter: 1406 loss: 1.32691468e-06
Iter: 1407 loss: 1.32696641e-06
Iter: 1408 loss: 1.32652667e-06
Iter: 1409 loss: 1.32607363e-06
Iter: 1410 loss: 1.32742321e-06
Iter: 1411 loss: 1.32601099e-06
Iter: 1412 loss: 1.32541777e-06
Iter: 1413 loss: 1.32546006e-06
Iter: 1414 loss: 1.3250542e-06
Iter: 1415 loss: 1.32446382e-06
Iter: 1416 loss: 1.32751416e-06
Iter: 1417 loss: 1.32442256e-06
Iter: 1418 loss: 1.32425112e-06
Iter: 1419 loss: 1.32421826e-06
Iter: 1420 loss: 1.32396917e-06
Iter: 1421 loss: 1.32356502e-06
Iter: 1422 loss: 1.32355717e-06
Iter: 1423 loss: 1.32327182e-06
Iter: 1424 loss: 1.32305013e-06
Iter: 1425 loss: 1.32296827e-06
Iter: 1426 loss: 1.32249454e-06
Iter: 1427 loss: 1.3256556e-06
Iter: 1428 loss: 1.32241962e-06
Iter: 1429 loss: 1.32197647e-06
Iter: 1430 loss: 1.32310129e-06
Iter: 1431 loss: 1.32186733e-06
Iter: 1432 loss: 1.32151104e-06
Iter: 1433 loss: 1.32079174e-06
Iter: 1434 loss: 1.32080083e-06
Iter: 1435 loss: 1.32036985e-06
Iter: 1436 loss: 1.32033256e-06
Iter: 1437 loss: 1.32000332e-06
Iter: 1438 loss: 1.31995023e-06
Iter: 1439 loss: 1.31960564e-06
Iter: 1440 loss: 1.31926231e-06
Iter: 1441 loss: 1.32527828e-06
Iter: 1442 loss: 1.31926959e-06
Iter: 1443 loss: 1.31900651e-06
Iter: 1444 loss: 1.31861839e-06
Iter: 1445 loss: 1.31860702e-06
Iter: 1446 loss: 1.31806678e-06
Iter: 1447 loss: 1.32103105e-06
Iter: 1448 loss: 1.3179714e-06
Iter: 1449 loss: 1.31759543e-06
Iter: 1450 loss: 1.31874845e-06
Iter: 1451 loss: 1.31748766e-06
Iter: 1452 loss: 1.31717081e-06
Iter: 1453 loss: 1.31717434e-06
Iter: 1454 loss: 1.31697811e-06
Iter: 1455 loss: 1.31657566e-06
Iter: 1456 loss: 1.32290188e-06
Iter: 1457 loss: 1.31658521e-06
Iter: 1458 loss: 1.31614149e-06
Iter: 1459 loss: 1.31534853e-06
Iter: 1460 loss: 1.31535796e-06
Iter: 1461 loss: 1.31502338e-06
Iter: 1462 loss: 1.31497382e-06
Iter: 1463 loss: 1.31459285e-06
Iter: 1464 loss: 1.31553395e-06
Iter: 1465 loss: 1.31444858e-06
Iter: 1466 loss: 1.31407046e-06
Iter: 1467 loss: 1.3147253e-06
Iter: 1468 loss: 1.31397337e-06
Iter: 1469 loss: 1.31368927e-06
Iter: 1470 loss: 1.31366278e-06
Iter: 1471 loss: 1.31340425e-06
Iter: 1472 loss: 1.31292086e-06
Iter: 1473 loss: 1.31463753e-06
Iter: 1474 loss: 1.31275795e-06
Iter: 1475 loss: 1.31235265e-06
Iter: 1476 loss: 1.31238244e-06
Iter: 1477 loss: 1.31203979e-06
Iter: 1478 loss: 1.31143656e-06
Iter: 1479 loss: 1.31834236e-06
Iter: 1480 loss: 1.31147374e-06
Iter: 1481 loss: 1.31120055e-06
Iter: 1482 loss: 1.311701e-06
Iter: 1483 loss: 1.3110498e-06
Iter: 1484 loss: 1.31075376e-06
Iter: 1485 loss: 1.31163347e-06
Iter: 1486 loss: 1.31064269e-06
Iter: 1487 loss: 1.31019056e-06
Iter: 1488 loss: 1.31268916e-06
Iter: 1489 loss: 1.31011495e-06
Iter: 1490 loss: 1.30997e-06
Iter: 1491 loss: 1.3095389e-06
Iter: 1492 loss: 1.31315664e-06
Iter: 1493 loss: 1.30937303e-06
Iter: 1494 loss: 1.30876901e-06
Iter: 1495 loss: 1.31045363e-06
Iter: 1496 loss: 1.30854619e-06
Iter: 1497 loss: 1.30804165e-06
Iter: 1498 loss: 1.30833348e-06
Iter: 1499 loss: 1.30772503e-06
Iter: 1500 loss: 1.30744047e-06
Iter: 1501 loss: 1.30737499e-06
Iter: 1502 loss: 1.30710532e-06
Iter: 1503 loss: 1.30699186e-06
Iter: 1504 loss: 1.30680735e-06
Iter: 1505 loss: 1.30632793e-06
Iter: 1506 loss: 1.30596163e-06
Iter: 1507 loss: 1.30582066e-06
Iter: 1508 loss: 1.30520777e-06
Iter: 1509 loss: 1.30641138e-06
Iter: 1510 loss: 1.30497915e-06
Iter: 1511 loss: 1.30446949e-06
Iter: 1512 loss: 1.31057709e-06
Iter: 1513 loss: 1.30444607e-06
Iter: 1514 loss: 1.30406033e-06
Iter: 1515 loss: 1.3043807e-06
Iter: 1516 loss: 1.30382796e-06
Iter: 1517 loss: 1.30329818e-06
Iter: 1518 loss: 1.30683225e-06
Iter: 1519 loss: 1.3032286e-06
Iter: 1520 loss: 1.30304602e-06
Iter: 1521 loss: 1.30302703e-06
Iter: 1522 loss: 1.30281546e-06
Iter: 1523 loss: 1.30239164e-06
Iter: 1524 loss: 1.30238686e-06
Iter: 1525 loss: 1.30189323e-06
Iter: 1526 loss: 1.30164676e-06
Iter: 1527 loss: 1.30148703e-06
Iter: 1528 loss: 1.30093781e-06
Iter: 1529 loss: 1.30074307e-06
Iter: 1530 loss: 1.30049307e-06
Iter: 1531 loss: 1.29979458e-06
Iter: 1532 loss: 1.30849992e-06
Iter: 1533 loss: 1.29980185e-06
Iter: 1534 loss: 1.29933562e-06
Iter: 1535 loss: 1.29890418e-06
Iter: 1536 loss: 1.29877867e-06
Iter: 1537 loss: 1.29824468e-06
Iter: 1538 loss: 1.29825366e-06
Iter: 1539 loss: 1.29787782e-06
Iter: 1540 loss: 1.29852879e-06
Iter: 1541 loss: 1.29771274e-06
Iter: 1542 loss: 1.29719353e-06
Iter: 1543 loss: 1.2988769e-06
Iter: 1544 loss: 1.29704983e-06
Iter: 1545 loss: 1.29672196e-06
Iter: 1546 loss: 1.29724378e-06
Iter: 1547 loss: 1.29651949e-06
Iter: 1548 loss: 1.2960204e-06
Iter: 1549 loss: 1.29558896e-06
Iter: 1550 loss: 1.29543344e-06
Iter: 1551 loss: 1.29532032e-06
Iter: 1552 loss: 1.29512011e-06
Iter: 1553 loss: 1.29492082e-06
Iter: 1554 loss: 1.2962214e-06
Iter: 1555 loss: 1.29492059e-06
Iter: 1556 loss: 1.29461284e-06
Iter: 1557 loss: 1.29405043e-06
Iter: 1558 loss: 1.30762578e-06
Iter: 1559 loss: 1.29406635e-06
Iter: 1560 loss: 1.29373211e-06
Iter: 1561 loss: 1.29479804e-06
Iter: 1562 loss: 1.29361763e-06
Iter: 1563 loss: 1.29327327e-06
Iter: 1564 loss: 1.29263469e-06
Iter: 1565 loss: 1.29262594e-06
Iter: 1566 loss: 1.2919802e-06
Iter: 1567 loss: 1.29543264e-06
Iter: 1568 loss: 1.2919071e-06
Iter: 1569 loss: 1.29125976e-06
Iter: 1570 loss: 1.29455304e-06
Iter: 1571 loss: 1.29117507e-06
Iter: 1572 loss: 1.2908572e-06
Iter: 1573 loss: 1.29106797e-06
Iter: 1574 loss: 1.29061368e-06
Iter: 1575 loss: 1.29019395e-06
Iter: 1576 loss: 1.29587829e-06
Iter: 1577 loss: 1.29022806e-06
Iter: 1578 loss: 1.28998704e-06
Iter: 1579 loss: 1.29046634e-06
Iter: 1580 loss: 1.28978809e-06
Iter: 1581 loss: 1.28947295e-06
Iter: 1582 loss: 1.28904094e-06
Iter: 1583 loss: 1.28900365e-06
Iter: 1584 loss: 1.28857209e-06
Iter: 1585 loss: 1.29412297e-06
Iter: 1586 loss: 1.28859347e-06
Iter: 1587 loss: 1.28827639e-06
Iter: 1588 loss: 1.29088e-06
Iter: 1589 loss: 1.28825377e-06
Iter: 1590 loss: 1.2879741e-06
Iter: 1591 loss: 1.28864849e-06
Iter: 1592 loss: 1.28779232e-06
Iter: 1593 loss: 1.28753538e-06
Iter: 1594 loss: 1.28751401e-06
Iter: 1595 loss: 1.28734064e-06
Iter: 1596 loss: 1.2870621e-06
Iter: 1597 loss: 1.28669171e-06
Iter: 1598 loss: 1.28666579e-06
Iter: 1599 loss: 1.28612623e-06
Iter: 1600 loss: 1.2895e-06
Iter: 1601 loss: 1.28610941e-06
Iter: 1602 loss: 1.28569889e-06
Iter: 1603 loss: 1.28492559e-06
Iter: 1604 loss: 1.30320234e-06
Iter: 1605 loss: 1.28493548e-06
Iter: 1606 loss: 1.28439763e-06
Iter: 1607 loss: 1.28438899e-06
Iter: 1608 loss: 1.28384704e-06
Iter: 1609 loss: 1.28351076e-06
Iter: 1610 loss: 1.2833118e-06
Iter: 1611 loss: 1.28302099e-06
Iter: 1612 loss: 1.28296836e-06
Iter: 1613 loss: 1.28265606e-06
Iter: 1614 loss: 1.2822693e-06
Iter: 1615 loss: 1.28226634e-06
Iter: 1616 loss: 1.28181489e-06
Iter: 1617 loss: 1.28318243e-06
Iter: 1618 loss: 1.28168836e-06
Iter: 1619 loss: 1.28136026e-06
Iter: 1620 loss: 1.28105216e-06
Iter: 1621 loss: 1.28103204e-06
Iter: 1622 loss: 1.28049976e-06
Iter: 1623 loss: 1.28501301e-06
Iter: 1624 loss: 1.28050101e-06
Iter: 1625 loss: 1.28030604e-06
Iter: 1626 loss: 1.28029762e-06
Iter: 1627 loss: 1.28004058e-06
Iter: 1628 loss: 1.27967201e-06
Iter: 1629 loss: 1.27965268e-06
Iter: 1630 loss: 1.27928956e-06
Iter: 1631 loss: 1.28015824e-06
Iter: 1632 loss: 1.27914245e-06
Iter: 1633 loss: 1.27878252e-06
Iter: 1634 loss: 1.27866485e-06
Iter: 1635 loss: 1.27845715e-06
Iter: 1636 loss: 1.27791805e-06
Iter: 1637 loss: 1.28041e-06
Iter: 1638 loss: 1.27782482e-06
Iter: 1639 loss: 1.27743374e-06
Iter: 1640 loss: 1.27702947e-06
Iter: 1641 loss: 1.27693829e-06
Iter: 1642 loss: 1.27640897e-06
Iter: 1643 loss: 1.28355612e-06
Iter: 1644 loss: 1.27641192e-06
Iter: 1645 loss: 1.27595786e-06
Iter: 1646 loss: 1.27585497e-06
Iter: 1647 loss: 1.27554017e-06
Iter: 1648 loss: 1.27520093e-06
Iter: 1649 loss: 1.2751218e-06
Iter: 1650 loss: 1.27488249e-06
Iter: 1651 loss: 1.27438545e-06
Iter: 1652 loss: 1.28150396e-06
Iter: 1653 loss: 1.27435078e-06
Iter: 1654 loss: 1.27382202e-06
Iter: 1655 loss: 1.27992917e-06
Iter: 1656 loss: 1.27377916e-06
Iter: 1657 loss: 1.2735436e-06
Iter: 1658 loss: 1.2746824e-06
Iter: 1659 loss: 1.27350722e-06
Iter: 1660 loss: 1.27325052e-06
Iter: 1661 loss: 1.27537214e-06
Iter: 1662 loss: 1.27320209e-06
Iter: 1663 loss: 1.27296551e-06
Iter: 1664 loss: 1.27242834e-06
Iter: 1665 loss: 1.28351439e-06
Iter: 1666 loss: 1.27242095e-06
Iter: 1667 loss: 1.27204032e-06
Iter: 1668 loss: 1.27374142e-06
Iter: 1669 loss: 1.27195199e-06
Iter: 1670 loss: 1.27161991e-06
Iter: 1671 loss: 1.27123474e-06
Iter: 1672 loss: 1.27120359e-06
Iter: 1673 loss: 1.27080557e-06
Iter: 1674 loss: 1.27582211e-06
Iter: 1675 loss: 1.27078488e-06
Iter: 1676 loss: 1.27046314e-06
Iter: 1677 loss: 1.26977682e-06
Iter: 1678 loss: 1.28508987e-06
Iter: 1679 loss: 1.26978705e-06
Iter: 1680 loss: 1.26921452e-06
Iter: 1681 loss: 1.27551834e-06
Iter: 1682 loss: 1.26924829e-06
Iter: 1683 loss: 1.26880082e-06
Iter: 1684 loss: 1.26926057e-06
Iter: 1685 loss: 1.26850284e-06
Iter: 1686 loss: 1.26819407e-06
Iter: 1687 loss: 1.26819339e-06
Iter: 1688 loss: 1.26787643e-06
Iter: 1689 loss: 1.26745238e-06
Iter: 1690 loss: 1.26740747e-06
Iter: 1691 loss: 1.26709176e-06
Iter: 1692 loss: 1.27077283e-06
Iter: 1693 loss: 1.26705095e-06
Iter: 1694 loss: 1.26683949e-06
Iter: 1695 loss: 1.2705575e-06
Iter: 1696 loss: 1.266857e-06
Iter: 1697 loss: 1.26658563e-06
Iter: 1698 loss: 1.26658983e-06
Iter: 1699 loss: 1.2663503e-06
Iter: 1700 loss: 1.26603345e-06
Iter: 1701 loss: 1.26612974e-06
Iter: 1702 loss: 1.26585314e-06
Iter: 1703 loss: 1.2655e-06
Iter: 1704 loss: 1.26567704e-06
Iter: 1705 loss: 1.26529358e-06
Iter: 1706 loss: 1.26482644e-06
Iter: 1707 loss: 1.26590635e-06
Iter: 1708 loss: 1.26464943e-06
Iter: 1709 loss: 1.26431166e-06
Iter: 1710 loss: 1.26498571e-06
Iter: 1711 loss: 1.2640902e-06
Iter: 1712 loss: 1.26353984e-06
Iter: 1713 loss: 1.26480427e-06
Iter: 1714 loss: 1.26336818e-06
Iter: 1715 loss: 1.2629398e-06
Iter: 1716 loss: 1.26345083e-06
Iter: 1717 loss: 1.26271186e-06
Iter: 1718 loss: 1.26211387e-06
Iter: 1719 loss: 1.26319492e-06
Iter: 1720 loss: 1.26183079e-06
Iter: 1721 loss: 1.26168857e-06
Iter: 1722 loss: 1.26156635e-06
Iter: 1723 loss: 1.26136911e-06
Iter: 1724 loss: 1.26080488e-06
Iter: 1725 loss: 1.26522696e-06
Iter: 1726 loss: 1.2607195e-06
Iter: 1727 loss: 1.26073712e-06
Iter: 1728 loss: 1.26043324e-06
Iter: 1729 loss: 1.26026293e-06
Iter: 1730 loss: 1.26152827e-06
Iter: 1731 loss: 1.26021109e-06
Iter: 1732 loss: 1.2600899e-06
Iter: 1733 loss: 1.25968427e-06
Iter: 1734 loss: 1.26367513e-06
Iter: 1735 loss: 1.25964107e-06
Iter: 1736 loss: 1.25923827e-06
Iter: 1737 loss: 1.26227599e-06
Iter: 1738 loss: 1.25919337e-06
Iter: 1739 loss: 1.25894178e-06
Iter: 1740 loss: 1.25861777e-06
Iter: 1741 loss: 1.25864892e-06
Iter: 1742 loss: 1.25812744e-06
Iter: 1743 loss: 1.26211762e-06
Iter: 1744 loss: 1.25813256e-06
Iter: 1745 loss: 1.25781185e-06
Iter: 1746 loss: 1.25779718e-06
Iter: 1747 loss: 1.25762381e-06
Iter: 1748 loss: 1.25716929e-06
Iter: 1749 loss: 1.25960116e-06
Iter: 1750 loss: 1.25708311e-06
Iter: 1751 loss: 1.25678639e-06
Iter: 1752 loss: 1.25684551e-06
Iter: 1753 loss: 1.25650763e-06
Iter: 1754 loss: 1.25606653e-06
Iter: 1755 loss: 1.25955535e-06
Iter: 1756 loss: 1.25600729e-06
Iter: 1757 loss: 1.25570148e-06
Iter: 1758 loss: 1.25869315e-06
Iter: 1759 loss: 1.25566885e-06
Iter: 1760 loss: 1.25540544e-06
Iter: 1761 loss: 1.25497718e-06
Iter: 1762 loss: 1.25496399e-06
Iter: 1763 loss: 1.25527e-06
Iter: 1764 loss: 1.25479528e-06
Iter: 1765 loss: 1.25468978e-06
Iter: 1766 loss: 1.25447616e-06
Iter: 1767 loss: 1.25807401e-06
Iter: 1768 loss: 1.25447036e-06
Iter: 1769 loss: 1.25420229e-06
Iter: 1770 loss: 1.25461918e-06
Iter: 1771 loss: 1.25411543e-06
Iter: 1772 loss: 1.25384361e-06
Iter: 1773 loss: 1.25600207e-06
Iter: 1774 loss: 1.25379529e-06
Iter: 1775 loss: 1.2536151e-06
Iter: 1776 loss: 1.25323254e-06
Iter: 1777 loss: 1.26104544e-06
Iter: 1778 loss: 1.25323413e-06
Iter: 1779 loss: 1.25285317e-06
Iter: 1780 loss: 1.25686904e-06
Iter: 1781 loss: 1.25286783e-06
Iter: 1782 loss: 1.25253428e-06
Iter: 1783 loss: 1.25223698e-06
Iter: 1784 loss: 1.25214e-06
Iter: 1785 loss: 1.25166196e-06
Iter: 1786 loss: 1.25703548e-06
Iter: 1787 loss: 1.25166628e-06
Iter: 1788 loss: 1.25121528e-06
Iter: 1789 loss: 1.25090264e-06
Iter: 1790 loss: 1.25080123e-06
Iter: 1791 loss: 1.25040629e-06
Iter: 1792 loss: 1.25559632e-06
Iter: 1793 loss: 1.25041561e-06
Iter: 1794 loss: 1.25008887e-06
Iter: 1795 loss: 1.25169618e-06
Iter: 1796 loss: 1.24998974e-06
Iter: 1797 loss: 1.24970029e-06
Iter: 1798 loss: 1.25206407e-06
Iter: 1799 loss: 1.24963049e-06
Iter: 1800 loss: 1.24947951e-06
Iter: 1801 loss: 1.25241854e-06
Iter: 1802 loss: 1.24948917e-06
Iter: 1803 loss: 1.24935127e-06
Iter: 1804 loss: 1.24906978e-06
Iter: 1805 loss: 1.24960707e-06
Iter: 1806 loss: 1.24877738e-06
Iter: 1807 loss: 1.24845394e-06
Iter: 1808 loss: 1.25343036e-06
Iter: 1809 loss: 1.2484619e-06
Iter: 1810 loss: 1.24817768e-06
Iter: 1811 loss: 1.24831195e-06
Iter: 1812 loss: 1.24802841e-06
Iter: 1813 loss: 1.24767462e-06
Iter: 1814 loss: 1.25131146e-06
Iter: 1815 loss: 1.2476703e-06
Iter: 1816 loss: 1.24750329e-06
Iter: 1817 loss: 1.24729172e-06
Iter: 1818 loss: 1.24726148e-06
Iter: 1819 loss: 1.24694134e-06
Iter: 1820 loss: 1.24778239e-06
Iter: 1821 loss: 1.24678957e-06
Iter: 1822 loss: 1.24642952e-06
Iter: 1823 loss: 1.24642952e-06
Iter: 1824 loss: 1.24618123e-06
Iter: 1825 loss: 1.2457931e-06
Iter: 1826 loss: 1.24858434e-06
Iter: 1827 loss: 1.24575695e-06
Iter: 1828 loss: 1.24542612e-06
Iter: 1829 loss: 1.24519352e-06
Iter: 1830 loss: 1.24513383e-06
Iter: 1831 loss: 1.24469716e-06
Iter: 1832 loss: 1.24670316e-06
Iter: 1833 loss: 1.24460769e-06
Iter: 1834 loss: 1.2446701e-06
Iter: 1835 loss: 1.24447433e-06
Iter: 1836 loss: 1.24436451e-06
Iter: 1837 loss: 1.24403437e-06
Iter: 1838 loss: 1.24542578e-06
Iter: 1839 loss: 1.24395433e-06
Iter: 1840 loss: 1.2435894e-06
Iter: 1841 loss: 1.24595158e-06
Iter: 1842 loss: 1.24352243e-06
Iter: 1843 loss: 1.24331586e-06
Iter: 1844 loss: 1.24305438e-06
Iter: 1845 loss: 1.2429482e-06
Iter: 1846 loss: 1.24249891e-06
Iter: 1847 loss: 1.24453641e-06
Iter: 1848 loss: 1.24239807e-06
Iter: 1849 loss: 1.24209748e-06
Iter: 1850 loss: 1.24258577e-06
Iter: 1851 loss: 1.24192684e-06
Iter: 1852 loss: 1.24157714e-06
Iter: 1853 loss: 1.24486473e-06
Iter: 1854 loss: 1.24155736e-06
Iter: 1855 loss: 1.241272e-06
Iter: 1856 loss: 1.24144412e-06
Iter: 1857 loss: 1.24115058e-06
Iter: 1858 loss: 1.24083112e-06
Iter: 1859 loss: 1.24047494e-06
Iter: 1860 loss: 1.24040776e-06
Iter: 1861 loss: 1.24001804e-06
Iter: 1862 loss: 1.24306985e-06
Iter: 1863 loss: 1.24001372e-06
Iter: 1864 loss: 1.23959194e-06
Iter: 1865 loss: 1.2405992e-06
Iter: 1866 loss: 1.23946961e-06
Iter: 1867 loss: 1.23928282e-06
Iter: 1868 loss: 1.23924417e-06
Iter: 1869 loss: 1.23907409e-06
Iter: 1870 loss: 1.2396215e-06
Iter: 1871 loss: 1.23898099e-06
Iter: 1872 loss: 1.23877157e-06
Iter: 1873 loss: 1.23845643e-06
Iter: 1874 loss: 1.23848304e-06
Iter: 1875 loss: 1.23815471e-06
Iter: 1876 loss: 1.23905625e-06
Iter: 1877 loss: 1.23806012e-06
Iter: 1878 loss: 1.23779841e-06
Iter: 1879 loss: 1.2382427e-06
Iter: 1880 loss: 1.23763698e-06
Iter: 1881 loss: 1.23731968e-06
Iter: 1882 loss: 1.23841676e-06
Iter: 1883 loss: 1.23721384e-06
Iter: 1884 loss: 1.23689631e-06
Iter: 1885 loss: 1.23698828e-06
Iter: 1886 loss: 1.23666257e-06
Iter: 1887 loss: 1.23630366e-06
Iter: 1888 loss: 1.23924281e-06
Iter: 1889 loss: 1.23630684e-06
Iter: 1890 loss: 1.23586983e-06
Iter: 1891 loss: 1.23584482e-06
Iter: 1892 loss: 1.23551581e-06
Iter: 1893 loss: 1.23522159e-06
Iter: 1894 loss: 1.23699397e-06
Iter: 1895 loss: 1.23523512e-06
Iter: 1896 loss: 1.23491532e-06
Iter: 1897 loss: 1.23459904e-06
Iter: 1898 loss: 1.23452674e-06
Iter: 1899 loss: 1.23416157e-06
Iter: 1900 loss: 1.23636278e-06
Iter: 1901 loss: 1.23411405e-06
Iter: 1902 loss: 1.23384007e-06
Iter: 1903 loss: 1.23384268e-06
Iter: 1904 loss: 1.23358109e-06
Iter: 1905 loss: 1.23427571e-06
Iter: 1906 loss: 1.23348184e-06
Iter: 1907 loss: 1.23328118e-06
Iter: 1908 loss: 1.23319296e-06
Iter: 1909 loss: 1.23315453e-06
Iter: 1910 loss: 1.23281677e-06
Iter: 1911 loss: 1.23252039e-06
Iter: 1912 loss: 1.2324283e-06
Iter: 1913 loss: 1.23208554e-06
Iter: 1914 loss: 1.23661448e-06
Iter: 1915 loss: 1.23206974e-06
Iter: 1916 loss: 1.23189238e-06
Iter: 1917 loss: 1.23148811e-06
Iter: 1918 loss: 1.23150448e-06
Iter: 1919 loss: 1.23112727e-06
Iter: 1920 loss: 1.23686277e-06
Iter: 1921 loss: 1.23113398e-06
Iter: 1922 loss: 1.23082827e-06
Iter: 1923 loss: 1.23086807e-06
Iter: 1924 loss: 1.23065524e-06
Iter: 1925 loss: 1.23028155e-06
Iter: 1926 loss: 1.23484347e-06
Iter: 1927 loss: 1.2302844e-06
Iter: 1928 loss: 1.23010943e-06
Iter: 1929 loss: 1.22968697e-06
Iter: 1930 loss: 1.2297055e-06
Iter: 1931 loss: 1.2293151e-06
Iter: 1932 loss: 1.23208918e-06
Iter: 1933 loss: 1.22928463e-06
Iter: 1934 loss: 1.22901747e-06
Iter: 1935 loss: 1.22922597e-06
Iter: 1936 loss: 1.22881988e-06
Iter: 1937 loss: 1.2286099e-06
Iter: 1938 loss: 1.22862537e-06
Iter: 1939 loss: 1.22841379e-06
Iter: 1940 loss: 1.22830306e-06
Iter: 1941 loss: 1.2282801e-06
Iter: 1942 loss: 1.22804454e-06
Iter: 1943 loss: 1.22826714e-06
Iter: 1944 loss: 1.22798269e-06
Iter: 1945 loss: 1.22768415e-06
Iter: 1946 loss: 1.2274628e-06
Iter: 1947 loss: 1.22738709e-06
Iter: 1948 loss: 1.22711526e-06
Iter: 1949 loss: 1.22805682e-06
Iter: 1950 loss: 1.22699885e-06
Iter: 1951 loss: 1.22663357e-06
Iter: 1952 loss: 1.22733672e-06
Iter: 1953 loss: 1.22645463e-06
Iter: 1954 loss: 1.22627011e-06
Iter: 1955 loss: 1.22796962e-06
Iter: 1956 loss: 1.22627603e-06
Iter: 1957 loss: 1.22594838e-06
Iter: 1958 loss: 1.22606241e-06
Iter: 1959 loss: 1.22580207e-06
Iter: 1960 loss: 1.22541621e-06
Iter: 1961 loss: 1.22837844e-06
Iter: 1962 loss: 1.22540075e-06
Iter: 1963 loss: 1.22510141e-06
Iter: 1964 loss: 1.22464428e-06
Iter: 1965 loss: 1.23642587e-06
Iter: 1966 loss: 1.2246486e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi3/300_300_300_1
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0
+ date
Mon Oct 26 14:20:22 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 1 --phi 0 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cf164f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cf18a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cf155598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cf0de1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cf205a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cf2050d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cf1f1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cf1f1048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cf0de488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cf03d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cf1f17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cf014bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cefaf268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cefaf620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cefce840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cefafd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cefced08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7ceee98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cef909d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7ceedaa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cee6c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cee6ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cee36598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cee06378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cee06510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cee1b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cee1b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cedd3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7ced69488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7ced69950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7ced69d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7ced12488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cecf1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7cecd46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7b27fd400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7b27fd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.73989224e-06
Iter: 2 loss: 2.70073815e-06
Iter: 3 loss: 2.49270215e-06
Iter: 4 loss: 2.1732335e-06
Iter: 5 loss: 2.28473709e-06
Iter: 6 loss: 1.94812083e-06
Iter: 7 loss: 1.81879091e-06
Iter: 8 loss: 2.57726606e-06
Iter: 9 loss: 1.80195138e-06
Iter: 10 loss: 1.65574806e-06
Iter: 11 loss: 1.91878644e-06
Iter: 12 loss: 1.59225328e-06
Iter: 13 loss: 1.53236385e-06
Iter: 14 loss: 1.72752266e-06
Iter: 15 loss: 1.5156578e-06
Iter: 16 loss: 1.49630864e-06
Iter: 17 loss: 1.64351172e-06
Iter: 18 loss: 1.49485254e-06
Iter: 19 loss: 1.46935452e-06
Iter: 20 loss: 1.46572859e-06
Iter: 21 loss: 1.44780427e-06
Iter: 22 loss: 1.42724184e-06
Iter: 23 loss: 1.38264488e-06
Iter: 24 loss: 2.0620023e-06
Iter: 25 loss: 1.38087137e-06
Iter: 26 loss: 1.32271771e-06
Iter: 27 loss: 1.86907562e-06
Iter: 28 loss: 1.32032699e-06
Iter: 29 loss: 1.27253691e-06
Iter: 30 loss: 1.85575368e-06
Iter: 31 loss: 1.27206545e-06
Iter: 32 loss: 1.26009581e-06
Iter: 33 loss: 1.22365691e-06
Iter: 34 loss: 1.31170714e-06
Iter: 35 loss: 1.20271989e-06
Iter: 36 loss: 1.18917967e-06
Iter: 37 loss: 1.1843897e-06
Iter: 38 loss: 1.17108016e-06
Iter: 39 loss: 1.16446506e-06
Iter: 40 loss: 1.15817147e-06
Iter: 41 loss: 1.15834359e-06
Iter: 42 loss: 1.14828026e-06
Iter: 43 loss: 1.14291083e-06
Iter: 44 loss: 1.12641078e-06
Iter: 45 loss: 1.16503816e-06
Iter: 46 loss: 1.11685085e-06
Iter: 47 loss: 1.12367081e-06
Iter: 48 loss: 1.10712472e-06
Iter: 49 loss: 1.10010137e-06
Iter: 50 loss: 1.10656379e-06
Iter: 51 loss: 1.09599239e-06
Iter: 52 loss: 1.08608015e-06
Iter: 53 loss: 1.08160702e-06
Iter: 54 loss: 1.07661799e-06
Iter: 55 loss: 1.05796948e-06
Iter: 56 loss: 1.16160822e-06
Iter: 57 loss: 1.05529796e-06
Iter: 58 loss: 1.04198841e-06
Iter: 59 loss: 1.17545403e-06
Iter: 60 loss: 1.04161177e-06
Iter: 61 loss: 1.0260851e-06
Iter: 62 loss: 1.0541147e-06
Iter: 63 loss: 1.01933938e-06
Iter: 64 loss: 1.00210218e-06
Iter: 65 loss: 9.72644671e-07
Iter: 66 loss: 9.72627731e-07
Iter: 67 loss: 9.54812094e-07
Iter: 68 loss: 9.53031304e-07
Iter: 69 loss: 9.32237924e-07
Iter: 70 loss: 9.2384488e-07
Iter: 71 loss: 9.12787073e-07
Iter: 72 loss: 9.01657359e-07
Iter: 73 loss: 8.91894899e-07
Iter: 74 loss: 8.8892557e-07
Iter: 75 loss: 8.89852572e-07
Iter: 76 loss: 8.81550932e-07
Iter: 77 loss: 8.75079536e-07
Iter: 78 loss: 8.59493923e-07
Iter: 79 loss: 1.03014474e-06
Iter: 80 loss: 8.57901796e-07
Iter: 81 loss: 8.43803207e-07
Iter: 82 loss: 8.69298105e-07
Iter: 83 loss: 8.37681227e-07
Iter: 84 loss: 8.27812755e-07
Iter: 85 loss: 8.27777171e-07
Iter: 86 loss: 8.1598165e-07
Iter: 87 loss: 8.15494104e-07
Iter: 88 loss: 8.06389949e-07
Iter: 89 loss: 7.98208589e-07
Iter: 90 loss: 7.94602613e-07
Iter: 91 loss: 7.90468619e-07
Iter: 92 loss: 7.81267829e-07
Iter: 93 loss: 8.2681197e-07
Iter: 94 loss: 7.79717084e-07
Iter: 95 loss: 7.71034e-07
Iter: 96 loss: 8.87325427e-07
Iter: 97 loss: 7.70976612e-07
Iter: 98 loss: 7.64580363e-07
Iter: 99 loss: 7.5203792e-07
Iter: 100 loss: 1.00571833e-06
Iter: 101 loss: 7.51934067e-07
Iter: 102 loss: 7.40451583e-07
Iter: 103 loss: 8.32346359e-07
Iter: 104 loss: 7.39696588e-07
Iter: 105 loss: 7.30701913e-07
Iter: 106 loss: 8.36570166e-07
Iter: 107 loss: 7.30571173e-07
Iter: 108 loss: 7.25937241e-07
Iter: 109 loss: 7.17270837e-07
Iter: 110 loss: 9.09263e-07
Iter: 111 loss: 7.17228431e-07
Iter: 112 loss: 7.0902081e-07
Iter: 113 loss: 7.21149831e-07
Iter: 114 loss: 7.05100661e-07
Iter: 115 loss: 7.05338721e-07
Iter: 116 loss: 7.01937438e-07
Iter: 117 loss: 6.98921099e-07
Iter: 118 loss: 6.93997094e-07
Iter: 119 loss: 6.93980837e-07
Iter: 120 loss: 6.89225544e-07
Iter: 121 loss: 6.88771252e-07
Iter: 122 loss: 6.85292775e-07
Iter: 123 loss: 6.8194629e-07
Iter: 124 loss: 6.81244217e-07
Iter: 125 loss: 6.7714177e-07
Iter: 126 loss: 6.6803625e-07
Iter: 127 loss: 7.96777613e-07
Iter: 128 loss: 6.67572181e-07
Iter: 129 loss: 6.60983e-07
Iter: 130 loss: 6.67062238e-07
Iter: 131 loss: 6.57209512e-07
Iter: 132 loss: 6.5334973e-07
Iter: 133 loss: 6.52808126e-07
Iter: 134 loss: 6.47890886e-07
Iter: 135 loss: 6.47668e-07
Iter: 136 loss: 6.43884675e-07
Iter: 137 loss: 6.38982385e-07
Iter: 138 loss: 6.33789796e-07
Iter: 139 loss: 6.32897581e-07
Iter: 140 loss: 6.23790811e-07
Iter: 141 loss: 6.57217527e-07
Iter: 142 loss: 6.21556069e-07
Iter: 143 loss: 6.17839078e-07
Iter: 144 loss: 6.16858529e-07
Iter: 145 loss: 6.13047064e-07
Iter: 146 loss: 6.0828728e-07
Iter: 147 loss: 6.07893185e-07
Iter: 148 loss: 6.03561205e-07
Iter: 149 loss: 6.07163656e-07
Iter: 150 loss: 6.00986255e-07
Iter: 151 loss: 6.00185786e-07
Iter: 152 loss: 5.99355928e-07
Iter: 153 loss: 5.97649318e-07
Iter: 154 loss: 5.96638927e-07
Iter: 155 loss: 5.95956749e-07
Iter: 156 loss: 5.94062328e-07
Iter: 157 loss: 5.91620392e-07
Iter: 158 loss: 5.91458104e-07
Iter: 159 loss: 5.88832563e-07
Iter: 160 loss: 6.30266697e-07
Iter: 161 loss: 5.888445e-07
Iter: 162 loss: 5.85644671e-07
Iter: 163 loss: 5.87112311e-07
Iter: 164 loss: 5.83503038e-07
Iter: 165 loss: 5.8051927e-07
Iter: 166 loss: 5.73127636e-07
Iter: 167 loss: 6.46371291e-07
Iter: 168 loss: 5.72191198e-07
Iter: 169 loss: 5.64317588e-07
Iter: 170 loss: 6.47892193e-07
Iter: 171 loss: 5.64123695e-07
Iter: 172 loss: 5.63849881e-07
Iter: 173 loss: 5.61920615e-07
Iter: 174 loss: 5.60318881e-07
Iter: 175 loss: 5.58000181e-07
Iter: 176 loss: 5.57926228e-07
Iter: 177 loss: 5.5556859e-07
Iter: 178 loss: 5.61606214e-07
Iter: 179 loss: 5.5479984e-07
Iter: 180 loss: 5.5296772e-07
Iter: 181 loss: 5.52951292e-07
Iter: 182 loss: 5.51777589e-07
Iter: 183 loss: 5.49384879e-07
Iter: 184 loss: 5.9283434e-07
Iter: 185 loss: 5.49329229e-07
Iter: 186 loss: 5.46696924e-07
Iter: 187 loss: 5.48061394e-07
Iter: 188 loss: 5.44937052e-07
Iter: 189 loss: 5.42819066e-07
Iter: 190 loss: 5.42509213e-07
Iter: 191 loss: 5.40884e-07
Iter: 192 loss: 5.36634957e-07
Iter: 193 loss: 5.68869496e-07
Iter: 194 loss: 5.35791969e-07
Iter: 195 loss: 5.34612354e-07
Iter: 196 loss: 5.3362885e-07
Iter: 197 loss: 5.31434125e-07
Iter: 198 loss: 5.28523742e-07
Iter: 199 loss: 5.2837936e-07
Iter: 200 loss: 5.2601689e-07
Iter: 201 loss: 5.25624046e-07
Iter: 202 loss: 5.24039706e-07
Iter: 203 loss: 5.20528658e-07
Iter: 204 loss: 5.27230441e-07
Iter: 205 loss: 5.19059029e-07
Iter: 206 loss: 5.17366288e-07
Iter: 207 loss: 5.16621185e-07
Iter: 208 loss: 5.15335898e-07
Iter: 209 loss: 5.12172278e-07
Iter: 210 loss: 5.43223223e-07
Iter: 211 loss: 5.11768576e-07
Iter: 212 loss: 5.09455845e-07
Iter: 213 loss: 5.44978e-07
Iter: 214 loss: 5.09437825e-07
Iter: 215 loss: 5.06962124e-07
Iter: 216 loss: 5.14366263e-07
Iter: 217 loss: 5.0617956e-07
Iter: 218 loss: 5.05160529e-07
Iter: 219 loss: 5.04369325e-07
Iter: 220 loss: 5.04044351e-07
Iter: 221 loss: 5.02806756e-07
Iter: 222 loss: 5.16073101e-07
Iter: 223 loss: 5.0277572e-07
Iter: 224 loss: 5.0134696e-07
Iter: 225 loss: 5.03641104e-07
Iter: 226 loss: 5.00706392e-07
Iter: 227 loss: 4.99459e-07
Iter: 228 loss: 4.9795625e-07
Iter: 229 loss: 4.97819201e-07
Iter: 230 loss: 4.96163466e-07
Iter: 231 loss: 4.96082293e-07
Iter: 232 loss: 4.9478308e-07
Iter: 233 loss: 4.91680908e-07
Iter: 234 loss: 5.2799794e-07
Iter: 235 loss: 4.91405444e-07
Iter: 236 loss: 4.88420255e-07
Iter: 237 loss: 4.92969093e-07
Iter: 238 loss: 4.86969839e-07
Iter: 239 loss: 4.85648684e-07
Iter: 240 loss: 4.85402268e-07
Iter: 241 loss: 4.8379809e-07
Iter: 242 loss: 4.86602403e-07
Iter: 243 loss: 4.83108693e-07
Iter: 244 loss: 4.82212329e-07
Iter: 245 loss: 4.81460688e-07
Iter: 246 loss: 4.81241329e-07
Iter: 247 loss: 4.81034363e-07
Iter: 248 loss: 4.8060997e-07
Iter: 249 loss: 4.80091103e-07
Iter: 250 loss: 4.78530296e-07
Iter: 251 loss: 4.81228824e-07
Iter: 252 loss: 4.7746687e-07
Iter: 253 loss: 4.74996682e-07
Iter: 254 loss: 4.85051089e-07
Iter: 255 loss: 4.74439616e-07
Iter: 256 loss: 4.73966395e-07
Iter: 257 loss: 4.7325841e-07
Iter: 258 loss: 4.72491934e-07
Iter: 259 loss: 4.70509519e-07
Iter: 260 loss: 4.86729164e-07
Iter: 261 loss: 4.70172978e-07
Iter: 262 loss: 4.68484615e-07
Iter: 263 loss: 4.68473559e-07
Iter: 264 loss: 4.66885723e-07
Iter: 265 loss: 4.69962742e-07
Iter: 266 loss: 4.66200646e-07
Iter: 267 loss: 4.65097884e-07
Iter: 268 loss: 4.63281765e-07
Iter: 269 loss: 4.63288188e-07
Iter: 270 loss: 4.61202092e-07
Iter: 271 loss: 4.68511359e-07
Iter: 272 loss: 4.60639e-07
Iter: 273 loss: 4.5929346e-07
Iter: 274 loss: 4.59162038e-07
Iter: 275 loss: 4.58179784e-07
Iter: 276 loss: 4.55727161e-07
Iter: 277 loss: 4.81317102e-07
Iter: 278 loss: 4.55462725e-07
Iter: 279 loss: 4.54446024e-07
Iter: 280 loss: 4.54362748e-07
Iter: 281 loss: 4.53168695e-07
Iter: 282 loss: 4.54986861e-07
Iter: 283 loss: 4.52603501e-07
Iter: 284 loss: 4.5198226e-07
Iter: 285 loss: 4.50926677e-07
Iter: 286 loss: 4.509223e-07
Iter: 287 loss: 4.50100401e-07
Iter: 288 loss: 4.50072321e-07
Iter: 289 loss: 4.49116897e-07
Iter: 290 loss: 4.49381048e-07
Iter: 291 loss: 4.48438072e-07
Iter: 292 loss: 4.47552225e-07
Iter: 293 loss: 4.4721881e-07
Iter: 294 loss: 4.46733679e-07
Iter: 295 loss: 4.45107503e-07
Iter: 296 loss: 4.58124646e-07
Iter: 297 loss: 4.44989411e-07
Iter: 298 loss: 4.4375281e-07
Iter: 299 loss: 4.41642271e-07
Iter: 300 loss: 4.41638662e-07
Iter: 301 loss: 4.396e-07
Iter: 302 loss: 4.49730805e-07
Iter: 303 loss: 4.39254904e-07
Iter: 304 loss: 4.38549932e-07
Iter: 305 loss: 4.38471403e-07
Iter: 306 loss: 4.37688897e-07
Iter: 307 loss: 4.36881351e-07
Iter: 308 loss: 4.36727703e-07
Iter: 309 loss: 4.359012e-07
Iter: 310 loss: 4.3576236e-07
Iter: 311 loss: 4.35193897e-07
Iter: 312 loss: 4.34828962e-07
Iter: 313 loss: 4.34510639e-07
Iter: 314 loss: 4.34189616e-07
Iter: 315 loss: 4.33240785e-07
Iter: 316 loss: 4.37562392e-07
Iter: 317 loss: 4.32901089e-07
Iter: 318 loss: 4.31633509e-07
Iter: 319 loss: 4.36108763e-07
Iter: 320 loss: 4.31339856e-07
Iter: 321 loss: 4.30483e-07
Iter: 322 loss: 4.3045668e-07
Iter: 323 loss: 4.29924967e-07
Iter: 324 loss: 4.28687088e-07
Iter: 325 loss: 4.42176372e-07
Iter: 326 loss: 4.28547708e-07
Iter: 327 loss: 4.28420037e-07
Iter: 328 loss: 4.27993115e-07
Iter: 329 loss: 4.27496332e-07
Iter: 330 loss: 4.26674035e-07
Iter: 331 loss: 4.26674433e-07
Iter: 332 loss: 4.25763687e-07
Iter: 333 loss: 4.25116212e-07
Iter: 334 loss: 4.24779785e-07
Iter: 335 loss: 4.23253141e-07
Iter: 336 loss: 4.32541071e-07
Iter: 337 loss: 4.23082611e-07
Iter: 338 loss: 4.21745369e-07
Iter: 339 loss: 4.39987389e-07
Iter: 340 loss: 4.21735649e-07
Iter: 341 loss: 4.21131659e-07
Iter: 342 loss: 4.19743657e-07
Iter: 343 loss: 4.3740647e-07
Iter: 344 loss: 4.19645573e-07
Iter: 345 loss: 4.18803268e-07
Iter: 346 loss: 4.187944e-07
Iter: 347 loss: 4.17910627e-07
Iter: 348 loss: 4.20517722e-07
Iter: 349 loss: 4.17653837e-07
Iter: 350 loss: 4.17244223e-07
Iter: 351 loss: 4.16614455e-07
Iter: 352 loss: 4.16613204e-07
Iter: 353 loss: 4.16152233e-07
Iter: 354 loss: 4.16147287e-07
Iter: 355 loss: 4.1554739e-07
Iter: 356 loss: 4.14663e-07
Iter: 357 loss: 4.14656142e-07
Iter: 358 loss: 4.13694551e-07
Iter: 359 loss: 4.15233018e-07
Iter: 360 loss: 4.1326706e-07
Iter: 361 loss: 4.11687381e-07
Iter: 362 loss: 4.17015087e-07
Iter: 363 loss: 4.11265859e-07
Iter: 364 loss: 4.10234634e-07
Iter: 365 loss: 4.08330266e-07
Iter: 366 loss: 4.50927473e-07
Iter: 367 loss: 4.08343453e-07
Iter: 368 loss: 4.06531626e-07
Iter: 369 loss: 4.20579681e-07
Iter: 370 loss: 4.06397362e-07
Iter: 371 loss: 4.05929654e-07
Iter: 372 loss: 4.05758669e-07
Iter: 373 loss: 4.05238779e-07
Iter: 374 loss: 4.05099911e-07
Iter: 375 loss: 4.04769594e-07
Iter: 376 loss: 4.04266927e-07
Iter: 377 loss: 4.03741581e-07
Iter: 378 loss: 4.03644719e-07
Iter: 379 loss: 4.03358342e-07
Iter: 380 loss: 4.03190484e-07
Iter: 381 loss: 4.02786782e-07
Iter: 382 loss: 4.01879902e-07
Iter: 383 loss: 4.15630666e-07
Iter: 384 loss: 4.01854322e-07
Iter: 385 loss: 4.01007981e-07
Iter: 386 loss: 4.04309674e-07
Iter: 387 loss: 4.00818919e-07
Iter: 388 loss: 3.99955496e-07
Iter: 389 loss: 4.09378856e-07
Iter: 390 loss: 3.99940177e-07
Iter: 391 loss: 3.99540681e-07
Iter: 392 loss: 3.98547684e-07
Iter: 393 loss: 4.08278026e-07
Iter: 394 loss: 3.98421207e-07
Iter: 395 loss: 3.9834498e-07
Iter: 396 loss: 3.97838249e-07
Iter: 397 loss: 3.97550878e-07
Iter: 398 loss: 3.96944444e-07
Iter: 399 loss: 4.06107461e-07
Iter: 400 loss: 3.96916562e-07
Iter: 401 loss: 3.96387463e-07
Iter: 402 loss: 3.96699562e-07
Iter: 403 loss: 3.96017384e-07
Iter: 404 loss: 3.95448041e-07
Iter: 405 loss: 3.96330819e-07
Iter: 406 loss: 3.952008e-07
Iter: 407 loss: 3.94449131e-07
Iter: 408 loss: 3.95377725e-07
Iter: 409 loss: 3.94061885e-07
Iter: 410 loss: 3.93130051e-07
Iter: 411 loss: 3.92564687e-07
Iter: 412 loss: 3.921744e-07
Iter: 413 loss: 3.91550287e-07
Iter: 414 loss: 3.91513481e-07
Iter: 415 loss: 3.90898379e-07
Iter: 416 loss: 3.9234186e-07
Iter: 417 loss: 3.90673449e-07
Iter: 418 loss: 3.90355353e-07
Iter: 419 loss: 3.90201564e-07
Iter: 420 loss: 3.90039503e-07
Iter: 421 loss: 3.89828301e-07
Iter: 422 loss: 3.8976188e-07
Iter: 423 loss: 3.8960431e-07
Iter: 424 loss: 3.89167099e-07
Iter: 425 loss: 3.93817515e-07
Iter: 426 loss: 3.89118014e-07
Iter: 427 loss: 3.88848633e-07
Iter: 428 loss: 3.88828795e-07
Iter: 429 loss: 3.88505725e-07
Iter: 430 loss: 3.87593332e-07
Iter: 431 loss: 3.92647877e-07
Iter: 432 loss: 3.87322302e-07
Iter: 433 loss: 3.86111935e-07
Iter: 434 loss: 3.92843731e-07
Iter: 435 loss: 3.85925205e-07
Iter: 436 loss: 3.85072212e-07
Iter: 437 loss: 3.95443578e-07
Iter: 438 loss: 3.85055841e-07
Iter: 439 loss: 3.84523048e-07
Iter: 440 loss: 3.84523418e-07
Iter: 441 loss: 3.84114117e-07
Iter: 442 loss: 3.83371031e-07
Iter: 443 loss: 3.84309402e-07
Iter: 444 loss: 3.8299649e-07
Iter: 445 loss: 3.82569851e-07
Iter: 446 loss: 3.83841439e-07
Iter: 447 loss: 3.82461167e-07
Iter: 448 loss: 3.82089098e-07
Iter: 449 loss: 3.82091287e-07
Iter: 450 loss: 3.81928771e-07
Iter: 451 loss: 3.81605304e-07
Iter: 452 loss: 3.87220496e-07
Iter: 453 loss: 3.81608629e-07
Iter: 454 loss: 3.81299543e-07
Iter: 455 loss: 3.85273978e-07
Iter: 456 loss: 3.81288061e-07
Iter: 457 loss: 3.80887514e-07
Iter: 458 loss: 3.79866094e-07
Iter: 459 loss: 3.89743718e-07
Iter: 460 loss: 3.79731375e-07
Iter: 461 loss: 3.79129062e-07
Iter: 462 loss: 3.79098054e-07
Iter: 463 loss: 3.78446231e-07
Iter: 464 loss: 3.7824509e-07
Iter: 465 loss: 3.77840252e-07
Iter: 466 loss: 3.77264485e-07
Iter: 467 loss: 3.77948254e-07
Iter: 468 loss: 3.7698382e-07
Iter: 469 loss: 3.7639316e-07
Iter: 470 loss: 3.8430079e-07
Iter: 471 loss: 3.76397622e-07
Iter: 472 loss: 3.75976839e-07
Iter: 473 loss: 3.76259038e-07
Iter: 474 loss: 3.75701234e-07
Iter: 475 loss: 3.75220452e-07
Iter: 476 loss: 3.75955977e-07
Iter: 477 loss: 3.74997626e-07
Iter: 478 loss: 3.74559363e-07
Iter: 479 loss: 3.74404465e-07
Iter: 480 loss: 3.74166405e-07
Iter: 481 loss: 3.73761395e-07
Iter: 482 loss: 3.7370512e-07
Iter: 483 loss: 3.73403537e-07
Iter: 484 loss: 3.72763e-07
Iter: 485 loss: 3.81819632e-07
Iter: 486 loss: 3.72709053e-07
Iter: 487 loss: 3.72305294e-07
Iter: 488 loss: 3.72300121e-07
Iter: 489 loss: 3.71905514e-07
Iter: 490 loss: 3.72660281e-07
Iter: 491 loss: 3.71726628e-07
Iter: 492 loss: 3.71557434e-07
Iter: 493 loss: 3.7128143e-07
Iter: 494 loss: 3.71277935e-07
Iter: 495 loss: 3.70767566e-07
Iter: 496 loss: 3.74569481e-07
Iter: 497 loss: 3.70729424e-07
Iter: 498 loss: 3.70407747e-07
Iter: 499 loss: 3.69719885e-07
Iter: 500 loss: 3.80463632e-07
Iter: 501 loss: 3.69679839e-07
Iter: 502 loss: 3.68975662e-07
Iter: 503 loss: 3.68979585e-07
Iter: 504 loss: 3.68489793e-07
Iter: 505 loss: 3.6864202e-07
Iter: 506 loss: 3.68162318e-07
Iter: 507 loss: 3.67552502e-07
Iter: 508 loss: 3.69233248e-07
Iter: 509 loss: 3.67353039e-07
Iter: 510 loss: 3.66792563e-07
Iter: 511 loss: 3.66666484e-07
Iter: 512 loss: 3.66292511e-07
Iter: 513 loss: 3.66584175e-07
Iter: 514 loss: 3.66041888e-07
Iter: 515 loss: 3.65935762e-07
Iter: 516 loss: 3.65705461e-07
Iter: 517 loss: 3.691502e-07
Iter: 518 loss: 3.65702078e-07
Iter: 519 loss: 3.65440826e-07
Iter: 520 loss: 3.65654159e-07
Iter: 521 loss: 3.6529454e-07
Iter: 522 loss: 3.64814412e-07
Iter: 523 loss: 3.66750271e-07
Iter: 524 loss: 3.64704704e-07
Iter: 525 loss: 3.64376604e-07
Iter: 526 loss: 3.63545666e-07
Iter: 527 loss: 3.70468683e-07
Iter: 528 loss: 3.6341396e-07
Iter: 529 loss: 3.64101453e-07
Iter: 530 loss: 3.63094415e-07
Iter: 531 loss: 3.62933406e-07
Iter: 532 loss: 3.62552385e-07
Iter: 533 loss: 3.66635959e-07
Iter: 534 loss: 3.62502703e-07
Iter: 535 loss: 3.61990658e-07
Iter: 536 loss: 3.66720315e-07
Iter: 537 loss: 3.6197423e-07
Iter: 538 loss: 3.61698682e-07
Iter: 539 loss: 3.64570099e-07
Iter: 540 loss: 3.6169746e-07
Iter: 541 loss: 3.61509507e-07
Iter: 542 loss: 3.61131839e-07
Iter: 543 loss: 3.69092163e-07
Iter: 544 loss: 3.61119135e-07
Iter: 545 loss: 3.60462337e-07
Iter: 546 loss: 3.62886055e-07
Iter: 547 loss: 3.60293313e-07
Iter: 548 loss: 3.59978e-07
Iter: 549 loss: 3.63846596e-07
Iter: 550 loss: 3.59972489e-07
Iter: 551 loss: 3.59715898e-07
Iter: 552 loss: 3.62045796e-07
Iter: 553 loss: 3.59701687e-07
Iter: 554 loss: 3.59571715e-07
Iter: 555 loss: 3.59192143e-07
Iter: 556 loss: 3.62521973e-07
Iter: 557 loss: 3.59136095e-07
Iter: 558 loss: 3.59072317e-07
Iter: 559 loss: 3.58937029e-07
Iter: 560 loss: 3.58764737e-07
Iter: 561 loss: 3.58352565e-07
Iter: 562 loss: 3.64351877e-07
Iter: 563 loss: 3.58339037e-07
Iter: 564 loss: 3.5807625e-07
Iter: 565 loss: 3.62005636e-07
Iter: 566 loss: 3.58073549e-07
Iter: 567 loss: 3.57731665e-07
Iter: 568 loss: 3.56905673e-07
Iter: 569 loss: 3.66120418e-07
Iter: 570 loss: 3.56819385e-07
Iter: 571 loss: 3.55688542e-07
Iter: 572 loss: 3.56191663e-07
Iter: 573 loss: 3.54915585e-07
Iter: 574 loss: 3.54469194e-07
Iter: 575 loss: 3.5430071e-07
Iter: 576 loss: 3.53789574e-07
Iter: 577 loss: 3.56062856e-07
Iter: 578 loss: 3.53712863e-07
Iter: 579 loss: 3.53455192e-07
Iter: 580 loss: 3.53009426e-07
Iter: 581 loss: 3.62743577e-07
Iter: 582 loss: 3.5298936e-07
Iter: 583 loss: 3.52558288e-07
Iter: 584 loss: 3.52666916e-07
Iter: 585 loss: 3.52248406e-07
Iter: 586 loss: 3.52288282e-07
Iter: 587 loss: 3.52019413e-07
Iter: 588 loss: 3.5177905e-07
Iter: 589 loss: 3.51678864e-07
Iter: 590 loss: 3.51551023e-07
Iter: 591 loss: 3.5120658e-07
Iter: 592 loss: 3.50579398e-07
Iter: 593 loss: 3.65620394e-07
Iter: 594 loss: 3.50571895e-07
Iter: 595 loss: 3.50575647e-07
Iter: 596 loss: 3.50299445e-07
Iter: 597 loss: 3.5007298e-07
Iter: 598 loss: 3.50162622e-07
Iter: 599 loss: 3.4991794e-07
Iter: 600 loss: 3.4967772e-07
Iter: 601 loss: 3.50359187e-07
Iter: 602 loss: 3.49611184e-07
Iter: 603 loss: 3.49346976e-07
Iter: 604 loss: 3.50250872e-07
Iter: 605 loss: 3.49271943e-07
Iter: 606 loss: 3.49124292e-07
Iter: 607 loss: 3.4892102e-07
Iter: 608 loss: 3.4889996e-07
Iter: 609 loss: 3.48667243e-07
Iter: 610 loss: 3.48660251e-07
Iter: 611 loss: 3.48444246e-07
Iter: 612 loss: 3.47834089e-07
Iter: 613 loss: 3.50982475e-07
Iter: 614 loss: 3.47643208e-07
Iter: 615 loss: 3.46978311e-07
Iter: 616 loss: 3.47490754e-07
Iter: 617 loss: 3.46570118e-07
Iter: 618 loss: 3.46185e-07
Iter: 619 loss: 3.4720577e-07
Iter: 620 loss: 3.46058954e-07
Iter: 621 loss: 3.45953481e-07
Iter: 622 loss: 3.45863697e-07
Iter: 623 loss: 3.45687283e-07
Iter: 624 loss: 3.45485176e-07
Iter: 625 loss: 3.45454851e-07
Iter: 626 loss: 3.45273264e-07
Iter: 627 loss: 3.4557786e-07
Iter: 628 loss: 3.45183651e-07
Iter: 629 loss: 3.44884313e-07
Iter: 630 loss: 3.45549921e-07
Iter: 631 loss: 3.44750276e-07
Iter: 632 loss: 3.44414701e-07
Iter: 633 loss: 3.43678607e-07
Iter: 634 loss: 3.54831172e-07
Iter: 635 loss: 3.43643563e-07
Iter: 636 loss: 3.43540421e-07
Iter: 637 loss: 3.43233523e-07
Iter: 638 loss: 3.42976534e-07
Iter: 639 loss: 3.42597e-07
Iter: 640 loss: 3.42586191e-07
Iter: 641 loss: 3.42244846e-07
Iter: 642 loss: 3.43217209e-07
Iter: 643 loss: 3.42129681e-07
Iter: 644 loss: 3.41906286e-07
Iter: 645 loss: 3.44720945e-07
Iter: 646 loss: 3.4189091e-07
Iter: 647 loss: 3.41651287e-07
Iter: 648 loss: 3.42281339e-07
Iter: 649 loss: 3.4158009e-07
Iter: 650 loss: 3.41364824e-07
Iter: 651 loss: 3.4095163e-07
Iter: 652 loss: 3.4822105e-07
Iter: 653 loss: 3.40933866e-07
Iter: 654 loss: 3.4063288e-07
Iter: 655 loss: 3.40631033e-07
Iter: 656 loss: 3.40292e-07
Iter: 657 loss: 3.41069722e-07
Iter: 658 loss: 3.40186944e-07
Iter: 659 loss: 3.3997631e-07
Iter: 660 loss: 3.39633402e-07
Iter: 661 loss: 3.39622574e-07
Iter: 662 loss: 3.39289812e-07
Iter: 663 loss: 3.39275601e-07
Iter: 664 loss: 3.39125165e-07
Iter: 665 loss: 3.38879971e-07
Iter: 666 loss: 3.38872098e-07
Iter: 667 loss: 3.38712368e-07
Iter: 668 loss: 3.38710521e-07
Iter: 669 loss: 3.38503298e-07
Iter: 670 loss: 3.38115655e-07
Iter: 671 loss: 3.4691891e-07
Iter: 672 loss: 3.38115569e-07
Iter: 673 loss: 3.37608412e-07
Iter: 674 loss: 3.37411393e-07
Iter: 675 loss: 3.37148549e-07
Iter: 676 loss: 3.3656309e-07
Iter: 677 loss: 3.41704464e-07
Iter: 678 loss: 3.36528217e-07
Iter: 679 loss: 3.36167744e-07
Iter: 680 loss: 3.36160355e-07
Iter: 681 loss: 3.35990478e-07
Iter: 682 loss: 3.35720557e-07
Iter: 683 loss: 3.35707341e-07
Iter: 684 loss: 3.35492331e-07
Iter: 685 loss: 3.37043815e-07
Iter: 686 loss: 3.35463824e-07
Iter: 687 loss: 3.35257027e-07
Iter: 688 loss: 3.37560039e-07
Iter: 689 loss: 3.35265497e-07
Iter: 690 loss: 3.35148968e-07
Iter: 691 loss: 3.34921e-07
Iter: 692 loss: 3.38664961e-07
Iter: 693 loss: 3.34922362e-07
Iter: 694 loss: 3.34797392e-07
Iter: 695 loss: 3.34771528e-07
Iter: 696 loss: 3.34649428e-07
Iter: 697 loss: 3.34411482e-07
Iter: 698 loss: 3.38371848e-07
Iter: 699 loss: 3.34413528e-07
Iter: 700 loss: 3.34148865e-07
Iter: 701 loss: 3.34588066e-07
Iter: 702 loss: 3.34033871e-07
Iter: 703 loss: 3.33675274e-07
Iter: 704 loss: 3.37395534e-07
Iter: 705 loss: 3.33661944e-07
Iter: 706 loss: 3.33523587e-07
Iter: 707 loss: 3.33276603e-07
Iter: 708 loss: 3.33281918e-07
Iter: 709 loss: 3.32936111e-07
Iter: 710 loss: 3.32431711e-07
Iter: 711 loss: 3.32410679e-07
Iter: 712 loss: 3.32903795e-07
Iter: 713 loss: 3.32199676e-07
Iter: 714 loss: 3.32012178e-07
Iter: 715 loss: 3.31606344e-07
Iter: 716 loss: 3.38105338e-07
Iter: 717 loss: 3.31598471e-07
Iter: 718 loss: 3.31055446e-07
Iter: 719 loss: 3.30722287e-07
Iter: 720 loss: 3.30495709e-07
Iter: 721 loss: 3.30859109e-07
Iter: 722 loss: 3.30373894e-07
Iter: 723 loss: 3.30238265e-07
Iter: 724 loss: 3.30175908e-07
Iter: 725 loss: 3.30111618e-07
Iter: 726 loss: 3.29981106e-07
Iter: 727 loss: 3.30236645e-07
Iter: 728 loss: 3.29949614e-07
Iter: 729 loss: 3.29719057e-07
Iter: 730 loss: 3.29789e-07
Iter: 731 loss: 3.29579905e-07
Iter: 732 loss: 3.2936282e-07
Iter: 733 loss: 3.29179954e-07
Iter: 734 loss: 3.29122713e-07
Iter: 735 loss: 3.2889983e-07
Iter: 736 loss: 3.28899034e-07
Iter: 737 loss: 3.28645399e-07
Iter: 738 loss: 3.28200031e-07
Iter: 739 loss: 3.2819986e-07
Iter: 740 loss: 3.27803036e-07
Iter: 741 loss: 3.27939972e-07
Iter: 742 loss: 3.27521263e-07
Iter: 743 loss: 3.2711182e-07
Iter: 744 loss: 3.29143688e-07
Iter: 745 loss: 3.27044518e-07
Iter: 746 loss: 3.2709039e-07
Iter: 747 loss: 3.26915313e-07
Iter: 748 loss: 3.26826694e-07
Iter: 749 loss: 3.26559928e-07
Iter: 750 loss: 3.27640521e-07
Iter: 751 loss: 3.26466704e-07
Iter: 752 loss: 3.26111547e-07
Iter: 753 loss: 3.28573549e-07
Iter: 754 loss: 3.26086706e-07
Iter: 755 loss: 3.25887413e-07
Iter: 756 loss: 3.25881217e-07
Iter: 757 loss: 3.25778927e-07
Iter: 758 loss: 3.25585148e-07
Iter: 759 loss: 3.28969e-07
Iter: 760 loss: 3.25576138e-07
Iter: 761 loss: 3.25455602e-07
Iter: 762 loss: 3.25431074e-07
Iter: 763 loss: 3.25334611e-07
Iter: 764 loss: 3.25117753e-07
Iter: 765 loss: 3.28784211e-07
Iter: 766 loss: 3.25118464e-07
Iter: 767 loss: 3.24925111e-07
Iter: 768 loss: 3.25719157e-07
Iter: 769 loss: 3.24897059e-07
Iter: 770 loss: 3.24756684e-07
Iter: 771 loss: 3.24754097e-07
Iter: 772 loss: 3.24689381e-07
Iter: 773 loss: 3.24458313e-07
Iter: 774 loss: 3.24955863e-07
Iter: 775 loss: 3.24306825e-07
Iter: 776 loss: 3.23879362e-07
Iter: 777 loss: 3.24140615e-07
Iter: 778 loss: 3.23607139e-07
Iter: 779 loss: 3.23421773e-07
Iter: 780 loss: 3.23332245e-07
Iter: 781 loss: 3.23045725e-07
Iter: 782 loss: 3.23083725e-07
Iter: 783 loss: 3.22819574e-07
Iter: 784 loss: 3.22618348e-07
Iter: 785 loss: 3.22692813e-07
Iter: 786 loss: 3.22474762e-07
Iter: 787 loss: 3.22380458e-07
Iter: 788 loss: 3.22363803e-07
Iter: 789 loss: 3.22235792e-07
Iter: 790 loss: 3.22046162e-07
Iter: 791 loss: 3.22032463e-07
Iter: 792 loss: 3.21896408e-07
Iter: 793 loss: 3.21895868e-07
Iter: 794 loss: 3.21771296e-07
Iter: 795 loss: 3.21666846e-07
Iter: 796 loss: 3.21615659e-07
Iter: 797 loss: 3.21401814e-07
Iter: 798 loss: 3.20974266e-07
Iter: 799 loss: 3.29354378e-07
Iter: 800 loss: 3.20970855e-07
Iter: 801 loss: 3.20916513e-07
Iter: 802 loss: 3.20729299e-07
Iter: 803 loss: 3.20521281e-07
Iter: 804 loss: 3.20306015e-07
Iter: 805 loss: 3.20271965e-07
Iter: 806 loss: 3.20033223e-07
Iter: 807 loss: 3.20058746e-07
Iter: 808 loss: 3.19850443e-07
Iter: 809 loss: 3.19558467e-07
Iter: 810 loss: 3.19407491e-07
Iter: 811 loss: 3.19262455e-07
Iter: 812 loss: 3.19470615e-07
Iter: 813 loss: 3.19092578e-07
Iter: 814 loss: 3.18989635e-07
Iter: 815 loss: 3.18766638e-07
Iter: 816 loss: 3.21682e-07
Iter: 817 loss: 3.18740376e-07
Iter: 818 loss: 3.18575815e-07
Iter: 819 loss: 3.18572148e-07
Iter: 820 loss: 3.18435809e-07
Iter: 821 loss: 3.184131e-07
Iter: 822 loss: 3.18305865e-07
Iter: 823 loss: 3.18160488e-07
Iter: 824 loss: 3.18663325e-07
Iter: 825 loss: 3.18134255e-07
Iter: 826 loss: 3.17987656e-07
Iter: 827 loss: 3.18830871e-07
Iter: 828 loss: 3.17968613e-07
Iter: 829 loss: 3.17871354e-07
Iter: 830 loss: 3.17678484e-07
Iter: 831 loss: 3.21855538e-07
Iter: 832 loss: 3.17677603e-07
Iter: 833 loss: 3.17449235e-07
Iter: 834 loss: 3.17578667e-07
Iter: 835 loss: 3.17310707e-07
Iter: 836 loss: 3.16888475e-07
Iter: 837 loss: 3.19563242e-07
Iter: 838 loss: 3.16840612e-07
Iter: 839 loss: 3.16665705e-07
Iter: 840 loss: 3.16563444e-07
Iter: 841 loss: 3.16504639e-07
Iter: 842 loss: 3.16229688e-07
Iter: 843 loss: 3.16076239e-07
Iter: 844 loss: 3.1595431e-07
Iter: 845 loss: 3.15874189e-07
Iter: 846 loss: 3.15822405e-07
Iter: 847 loss: 3.15708e-07
Iter: 848 loss: 3.16353294e-07
Iter: 849 loss: 3.15696639e-07
Iter: 850 loss: 3.1561683e-07
Iter: 851 loss: 3.15527416e-07
Iter: 852 loss: 3.15525142e-07
Iter: 853 loss: 3.15434306e-07
Iter: 854 loss: 3.15416969e-07
Iter: 855 loss: 3.15356715e-07
Iter: 856 loss: 3.1508597e-07
Iter: 857 loss: 3.15726083e-07
Iter: 858 loss: 3.14922318e-07
Iter: 859 loss: 3.14900433e-07
Iter: 860 loss: 3.14780493e-07
Iter: 861 loss: 3.14643785e-07
Iter: 862 loss: 3.14556587e-07
Iter: 863 loss: 3.14502472e-07
Iter: 864 loss: 3.14325462e-07
Iter: 865 loss: 3.14028881e-07
Iter: 866 loss: 3.1402476e-07
Iter: 867 loss: 3.1445e-07
Iter: 868 loss: 3.13952114e-07
Iter: 869 loss: 3.13905872e-07
Iter: 870 loss: 3.13791816e-07
Iter: 871 loss: 3.15432715e-07
Iter: 872 loss: 3.13792157e-07
Iter: 873 loss: 3.13641749e-07
Iter: 874 loss: 3.13562964e-07
Iter: 875 loss: 3.13504216e-07
Iter: 876 loss: 3.13360943e-07
Iter: 877 loss: 3.1316398e-07
Iter: 878 loss: 3.13155908e-07
Iter: 879 loss: 3.12751126e-07
Iter: 880 loss: 3.12407735e-07
Iter: 881 loss: 3.12311e-07
Iter: 882 loss: 3.12205032e-07
Iter: 883 loss: 3.12087792e-07
Iter: 884 loss: 3.12002044e-07
Iter: 885 loss: 3.11866927e-07
Iter: 886 loss: 3.11853142e-07
Iter: 887 loss: 3.11734539e-07
Iter: 888 loss: 3.11732265e-07
Iter: 889 loss: 3.11626764e-07
Iter: 890 loss: 3.11676416e-07
Iter: 891 loss: 3.11555965e-07
Iter: 892 loss: 3.11414084e-07
Iter: 893 loss: 3.11213199e-07
Iter: 894 loss: 3.11202029e-07
Iter: 895 loss: 3.11074558e-07
Iter: 896 loss: 3.11036587e-07
Iter: 897 loss: 3.10886435e-07
Iter: 898 loss: 3.10554128e-07
Iter: 899 loss: 3.15367799e-07
Iter: 900 loss: 3.10542731e-07
Iter: 901 loss: 3.10197294e-07
Iter: 902 loss: 3.12582301e-07
Iter: 903 loss: 3.10178223e-07
Iter: 904 loss: 3.09924559e-07
Iter: 905 loss: 3.12861289e-07
Iter: 906 loss: 3.09931238e-07
Iter: 907 loss: 3.09830625e-07
Iter: 908 loss: 3.09721685e-07
Iter: 909 loss: 3.09711083e-07
Iter: 910 loss: 3.09592792e-07
Iter: 911 loss: 3.09592963e-07
Iter: 912 loss: 3.09504827e-07
Iter: 913 loss: 3.09297434e-07
Iter: 914 loss: 3.11040537e-07
Iter: 915 loss: 3.09269154e-07
Iter: 916 loss: 3.09026859e-07
Iter: 917 loss: 3.09633975e-07
Iter: 918 loss: 3.08928634e-07
Iter: 919 loss: 3.08758388e-07
Iter: 920 loss: 3.10077837e-07
Iter: 921 loss: 3.08751225e-07
Iter: 922 loss: 3.08519361e-07
Iter: 923 loss: 3.09109282e-07
Iter: 924 loss: 3.08445834e-07
Iter: 925 loss: 3.08267289e-07
Iter: 926 loss: 3.08589563e-07
Iter: 927 loss: 3.08192227e-07
Iter: 928 loss: 3.08070412e-07
Iter: 929 loss: 3.08126545e-07
Iter: 930 loss: 3.079831e-07
Iter: 931 loss: 3.07864e-07
Iter: 932 loss: 3.07857135e-07
Iter: 933 loss: 3.07741232e-07
Iter: 934 loss: 3.07609099e-07
Iter: 935 loss: 3.07585822e-07
Iter: 936 loss: 3.07453433e-07
Iter: 937 loss: 3.07865861e-07
Iter: 938 loss: 3.07406708e-07
Iter: 939 loss: 3.07133178e-07
Iter: 940 loss: 3.06978876e-07
Iter: 941 loss: 3.0688409e-07
Iter: 942 loss: 3.06670074e-07
Iter: 943 loss: 3.08773451e-07
Iter: 944 loss: 3.06660837e-07
Iter: 945 loss: 3.06460777e-07
Iter: 946 loss: 3.06732886e-07
Iter: 947 loss: 3.06377729e-07
Iter: 948 loss: 3.06237155e-07
Iter: 949 loss: 3.06247443e-07
Iter: 950 loss: 3.06144671e-07
Iter: 951 loss: 3.06031e-07
Iter: 952 loss: 3.06139e-07
Iter: 953 loss: 3.0597181e-07
Iter: 954 loss: 3.05893366e-07
Iter: 955 loss: 3.05884e-07
Iter: 956 loss: 3.05778315e-07
Iter: 957 loss: 3.05918405e-07
Iter: 958 loss: 3.05731533e-07
Iter: 959 loss: 3.05658716e-07
Iter: 960 loss: 3.05475623e-07
Iter: 961 loss: 3.08057963e-07
Iter: 962 loss: 3.0546741e-07
Iter: 963 loss: 3.05290257e-07
Iter: 964 loss: 3.05290882e-07
Iter: 965 loss: 3.05144397e-07
Iter: 966 loss: 3.05679208e-07
Iter: 967 loss: 3.05106454e-07
Iter: 968 loss: 3.04944081e-07
Iter: 969 loss: 3.04748426e-07
Iter: 970 loss: 3.04732339e-07
Iter: 971 loss: 3.04693259e-07
Iter: 972 loss: 3.04625019e-07
Iter: 973 loss: 3.04561979e-07
Iter: 974 loss: 3.04387754e-07
Iter: 975 loss: 3.05834902e-07
Iter: 976 loss: 3.04357116e-07
Iter: 977 loss: 3.04227115e-07
Iter: 978 loss: 3.04222226e-07
Iter: 979 loss: 3.04122352e-07
Iter: 980 loss: 3.03957449e-07
Iter: 981 loss: 3.03943835e-07
Iter: 982 loss: 3.03754376e-07
Iter: 983 loss: 3.03546358e-07
Iter: 984 loss: 3.03504862e-07
Iter: 985 loss: 3.03316142e-07
Iter: 986 loss: 3.05320214e-07
Iter: 987 loss: 3.03291444e-07
Iter: 988 loss: 3.03159283e-07
Iter: 989 loss: 3.04986514e-07
Iter: 990 loss: 3.03145839e-07
Iter: 991 loss: 3.03055572e-07
Iter: 992 loss: 3.02878107e-07
Iter: 993 loss: 3.0665467e-07
Iter: 994 loss: 3.02878846e-07
Iter: 995 loss: 3.02753676e-07
Iter: 996 loss: 3.03432103e-07
Iter: 997 loss: 3.02740489e-07
Iter: 998 loss: 3.02608044e-07
Iter: 999 loss: 3.02978265e-07
Iter: 1000 loss: 3.02555918e-07
Iter: 1001 loss: 3.02399258e-07
Iter: 1002 loss: 3.02784144e-07
Iter: 1003 loss: 3.02351339e-07
Iter: 1004 loss: 3.02233587e-07
Iter: 1005 loss: 3.02387548e-07
Iter: 1006 loss: 3.02150141e-07
Iter: 1007 loss: 3.0195082e-07
Iter: 1008 loss: 3.02246406e-07
Iter: 1009 loss: 3.01843414e-07
Iter: 1010 loss: 3.01699657e-07
Iter: 1011 loss: 3.0192291e-07
Iter: 1012 loss: 3.01619593e-07
Iter: 1013 loss: 3.01414e-07
Iter: 1014 loss: 3.02499501e-07
Iter: 1015 loss: 3.01382954e-07
Iter: 1016 loss: 3.01251021e-07
Iter: 1017 loss: 3.01084299e-07
Iter: 1018 loss: 3.01076057e-07
Iter: 1019 loss: 3.00917918e-07
Iter: 1020 loss: 3.02123453e-07
Iter: 1021 loss: 3.00913314e-07
Iter: 1022 loss: 3.00826542e-07
Iter: 1023 loss: 3.00835211e-07
Iter: 1024 loss: 3.0073565e-07
Iter: 1025 loss: 3.00654335e-07
Iter: 1026 loss: 3.00657319e-07
Iter: 1027 loss: 3.00542212e-07
Iter: 1028 loss: 3.00515921e-07
Iter: 1029 loss: 3.00452768e-07
Iter: 1030 loss: 3.00328935e-07
Iter: 1031 loss: 3.00337717e-07
Iter: 1032 loss: 3.00254925e-07
Iter: 1033 loss: 3.00361023e-07
Iter: 1034 loss: 3.00216186e-07
Iter: 1035 loss: 3.0011438e-07
Iter: 1036 loss: 3.00129983e-07
Iter: 1037 loss: 3.00045372e-07
Iter: 1038 loss: 2.99917104e-07
Iter: 1039 loss: 3.01040785e-07
Iter: 1040 loss: 2.99915428e-07
Iter: 1041 loss: 2.99838376e-07
Iter: 1042 loss: 2.99773234e-07
Iter: 1043 loss: 2.99752685e-07
Iter: 1044 loss: 2.99653721e-07
Iter: 1045 loss: 3.00948045e-07
Iter: 1046 loss: 2.99641158e-07
Iter: 1047 loss: 2.99552767e-07
Iter: 1048 loss: 2.9932238e-07
Iter: 1049 loss: 3.01350525e-07
Iter: 1050 loss: 2.99294584e-07
Iter: 1051 loss: 2.99054136e-07
Iter: 1052 loss: 3.0055125e-07
Iter: 1053 loss: 2.99026055e-07
Iter: 1054 loss: 2.98920554e-07
Iter: 1055 loss: 2.98909697e-07
Iter: 1056 loss: 2.98815252e-07
Iter: 1057 loss: 2.98810079e-07
Iter: 1058 loss: 2.98740019e-07
Iter: 1059 loss: 2.98661774e-07
Iter: 1060 loss: 2.98708272e-07
Iter: 1061 loss: 2.98620648e-07
Iter: 1062 loss: 2.98558433e-07
Iter: 1063 loss: 2.9856335e-07
Iter: 1064 loss: 2.98510145e-07
Iter: 1065 loss: 2.985565e-07
Iter: 1066 loss: 2.98477261e-07
Iter: 1067 loss: 2.98415245e-07
Iter: 1068 loss: 2.98443638e-07
Iter: 1069 loss: 2.98369031e-07
Iter: 1070 loss: 2.98289308e-07
Iter: 1071 loss: 2.98929308e-07
Iter: 1072 loss: 2.98280952e-07
Iter: 1073 loss: 2.98223654e-07
Iter: 1074 loss: 2.98138673e-07
Iter: 1075 loss: 2.98134808e-07
Iter: 1076 loss: 2.98022826e-07
Iter: 1077 loss: 2.98986322e-07
Iter: 1078 loss: 2.98025867e-07
Iter: 1079 loss: 2.97907263e-07
Iter: 1080 loss: 2.97722977e-07
Iter: 1081 loss: 2.97719083e-07
Iter: 1082 loss: 2.97605936e-07
Iter: 1083 loss: 2.98210722e-07
Iter: 1084 loss: 2.97596102e-07
Iter: 1085 loss: 2.97526981e-07
Iter: 1086 loss: 2.98468194e-07
Iter: 1087 loss: 2.97524053e-07
Iter: 1088 loss: 2.97434951e-07
Iter: 1089 loss: 2.97506688e-07
Iter: 1090 loss: 2.97393683e-07
Iter: 1091 loss: 2.9735196e-07
Iter: 1092 loss: 2.97399538e-07
Iter: 1093 loss: 2.97324277e-07
Iter: 1094 loss: 2.9726894e-07
Iter: 1095 loss: 2.97273203e-07
Iter: 1096 loss: 2.97235033e-07
Iter: 1097 loss: 2.97240092e-07
Iter: 1098 loss: 2.97208089e-07
Iter: 1099 loss: 2.9715568e-07
Iter: 1100 loss: 2.97272607e-07
Iter: 1101 loss: 2.97134534e-07
Iter: 1102 loss: 2.97088775e-07
Iter: 1103 loss: 2.97447258e-07
Iter: 1104 loss: 2.97097245e-07
Iter: 1105 loss: 2.9703898e-07
Iter: 1106 loss: 2.97029544e-07
Iter: 1107 loss: 2.97000952e-07
Iter: 1108 loss: 2.96954681e-07
Iter: 1109 loss: 2.97350624e-07
Iter: 1110 loss: 2.9696497e-07
Iter: 1111 loss: 2.96908695e-07
Iter: 1112 loss: 2.96827466e-07
Iter: 1113 loss: 2.96821639e-07
Iter: 1114 loss: 2.96734584e-07
Iter: 1115 loss: 2.96861714e-07
Iter: 1116 loss: 2.96693656e-07
Iter: 1117 loss: 2.96609954e-07
Iter: 1118 loss: 2.96616463e-07
Iter: 1119 loss: 2.96562462e-07
Iter: 1120 loss: 2.96617344e-07
Iter: 1121 loss: 2.96540719e-07
Iter: 1122 loss: 2.96484956e-07
Iter: 1123 loss: 2.9648703e-07
Iter: 1124 loss: 2.96451475e-07
Iter: 1125 loss: 2.96402902e-07
Iter: 1126 loss: 2.96401879e-07
Iter: 1127 loss: 2.96372548e-07
Iter: 1128 loss: 2.96391e-07
Iter: 1129 loss: 2.96350265e-07
Iter: 1130 loss: 2.96317666e-07
Iter: 1131 loss: 2.96363822e-07
Iter: 1132 loss: 2.96299419e-07
Iter: 1133 loss: 2.96249425e-07
Iter: 1134 loss: 2.96543334e-07
Iter: 1135 loss: 2.96244167e-07
Iter: 1136 loss: 2.96214893e-07
Iter: 1137 loss: 2.96229075e-07
Iter: 1138 loss: 2.96195594e-07
Iter: 1139 loss: 2.96158532e-07
Iter: 1140 loss: 2.96205485e-07
Iter: 1141 loss: 2.96124625e-07
Iter: 1142 loss: 2.96043254e-07
Iter: 1143 loss: 2.95984364e-07
Iter: 1144 loss: 2.95947359e-07
Iter: 1145 loss: 2.95860445e-07
Iter: 1146 loss: 2.96145316e-07
Iter: 1147 loss: 2.95833786e-07
Iter: 1148 loss: 2.95767e-07
Iter: 1149 loss: 2.9576961e-07
Iter: 1150 loss: 2.95710151e-07
Iter: 1151 loss: 2.95827704e-07
Iter: 1152 loss: 2.9567795e-07
Iter: 1153 loss: 2.95646771e-07
Iter: 1154 loss: 2.95624545e-07
Iter: 1155 loss: 2.95610221e-07
Iter: 1156 loss: 2.95576967e-07
Iter: 1157 loss: 2.95567816e-07
Iter: 1158 loss: 2.95520238e-07
Iter: 1159 loss: 2.95536097e-07
Iter: 1160 loss: 2.95481e-07
Iter: 1161 loss: 2.95436791e-07
Iter: 1162 loss: 2.95567929e-07
Iter: 1163 loss: 2.95412e-07
Iter: 1164 loss: 2.95361247e-07
Iter: 1165 loss: 2.95674738e-07
Iter: 1166 loss: 2.95353402e-07
Iter: 1167 loss: 2.95292466e-07
Iter: 1168 loss: 2.95271605e-07
Iter: 1169 loss: 2.95238465e-07
Iter: 1170 loss: 2.95154e-07
Iter: 1171 loss: 2.95546215e-07
Iter: 1172 loss: 2.95128785e-07
Iter: 1173 loss: 2.95044572e-07
Iter: 1174 loss: 2.95123243e-07
Iter: 1175 loss: 2.94992958e-07
Iter: 1176 loss: 2.94929322e-07
Iter: 1177 loss: 2.94983124e-07
Iter: 1178 loss: 2.94903373e-07
Iter: 1179 loss: 2.94846274e-07
Iter: 1180 loss: 2.95542918e-07
Iter: 1181 loss: 2.94846018e-07
Iter: 1182 loss: 2.94789572e-07
Iter: 1183 loss: 2.94882852e-07
Iter: 1184 loss: 2.94777351e-07
Iter: 1185 loss: 2.94706524e-07
Iter: 1186 loss: 2.94640614e-07
Iter: 1187 loss: 2.94632855e-07
Iter: 1188 loss: 2.94555122e-07
Iter: 1189 loss: 2.94551626e-07
Iter: 1190 loss: 2.94495351e-07
Iter: 1191 loss: 2.9454705e-07
Iter: 1192 loss: 2.9445971e-07
Iter: 1193 loss: 2.94395704e-07
Iter: 1194 loss: 2.94519396e-07
Iter: 1195 loss: 2.94365037e-07
Iter: 1196 loss: 2.94280426e-07
Iter: 1197 loss: 2.94680575e-07
Iter: 1198 loss: 2.94278038e-07
Iter: 1199 loss: 2.94214033e-07
Iter: 1200 loss: 2.94279857e-07
Iter: 1201 loss: 2.94169382e-07
Iter: 1202 loss: 2.94109839e-07
Iter: 1203 loss: 2.94286622e-07
Iter: 1204 loss: 2.94088522e-07
Iter: 1205 loss: 2.94001495e-07
Iter: 1206 loss: 2.94040888e-07
Iter: 1207 loss: 2.93935841e-07
Iter: 1208 loss: 2.9387e-07
Iter: 1209 loss: 2.93885819e-07
Iter: 1210 loss: 2.93800383e-07
Iter: 1211 loss: 2.93729187e-07
Iter: 1212 loss: 2.94717978e-07
Iter: 1213 loss: 2.93728135e-07
Iter: 1214 loss: 2.9366123e-07
Iter: 1215 loss: 2.9389372e-07
Iter: 1216 loss: 2.93645343e-07
Iter: 1217 loss: 2.93607854e-07
Iter: 1218 loss: 2.93553569e-07
Iter: 1219 loss: 2.93543792e-07
Iter: 1220 loss: 2.93512102e-07
Iter: 1221 loss: 2.93506218e-07
Iter: 1222 loss: 2.93472453e-07
Iter: 1223 loss: 2.93460744e-07
Iter: 1224 loss: 2.93450825e-07
Iter: 1225 loss: 2.9337815e-07
Iter: 1226 loss: 2.93407112e-07
Iter: 1227 loss: 2.93341373e-07
Iter: 1228 loss: 2.93262246e-07
Iter: 1229 loss: 2.93884511e-07
Iter: 1230 loss: 2.93252185e-07
Iter: 1231 loss: 2.93191704e-07
Iter: 1232 loss: 2.93204209e-07
Iter: 1233 loss: 2.93130284e-07
Iter: 1234 loss: 2.93046128e-07
Iter: 1235 loss: 2.93109906e-07
Iter: 1236 loss: 2.92995537e-07
Iter: 1237 loss: 2.92824069e-07
Iter: 1238 loss: 2.9317539e-07
Iter: 1239 loss: 2.92762138e-07
Iter: 1240 loss: 2.92665106e-07
Iter: 1241 loss: 2.92690146e-07
Iter: 1242 loss: 2.92594706e-07
Iter: 1243 loss: 2.92513732e-07
Iter: 1244 loss: 2.93564767e-07
Iter: 1245 loss: 2.92511743e-07
Iter: 1246 loss: 2.92449528e-07
Iter: 1247 loss: 2.92774672e-07
Iter: 1248 loss: 2.92428467e-07
Iter: 1249 loss: 2.92387483e-07
Iter: 1250 loss: 2.92327229e-07
Iter: 1251 loss: 2.92326405e-07
Iter: 1252 loss: 2.92270386e-07
Iter: 1253 loss: 2.92721865e-07
Iter: 1254 loss: 2.92252594e-07
Iter: 1255 loss: 2.9217253e-07
Iter: 1256 loss: 2.92314496e-07
Iter: 1257 loss: 2.92114464e-07
Iter: 1258 loss: 2.9202937e-07
Iter: 1259 loss: 2.92074247e-07
Iter: 1260 loss: 2.91957e-07
Iter: 1261 loss: 2.91845424e-07
Iter: 1262 loss: 2.92909192e-07
Iter: 1263 loss: 2.91836614e-07
Iter: 1264 loss: 2.91750666e-07
Iter: 1265 loss: 2.91787899e-07
Iter: 1266 loss: 2.91674894e-07
Iter: 1267 loss: 2.91580818e-07
Iter: 1268 loss: 2.91858669e-07
Iter: 1269 loss: 2.91532615e-07
Iter: 1270 loss: 2.91419553e-07
Iter: 1271 loss: 2.91896868e-07
Iter: 1272 loss: 2.91387238e-07
Iter: 1273 loss: 2.91317093e-07
Iter: 1274 loss: 2.91264428e-07
Iter: 1275 loss: 2.9123612e-07
Iter: 1276 loss: 2.91163332e-07
Iter: 1277 loss: 2.92269931e-07
Iter: 1278 loss: 2.91166117e-07
Iter: 1279 loss: 2.91091453e-07
Iter: 1280 loss: 2.91265792e-07
Iter: 1281 loss: 2.91067238e-07
Iter: 1282 loss: 2.91018125e-07
Iter: 1283 loss: 2.90926408e-07
Iter: 1284 loss: 2.90932036e-07
Iter: 1285 loss: 2.90859759e-07
Iter: 1286 loss: 2.90857372e-07
Iter: 1287 loss: 2.9080104e-07
Iter: 1288 loss: 2.9094474e-07
Iter: 1289 loss: 2.90779099e-07
Iter: 1290 loss: 2.9072865e-07
Iter: 1291 loss: 2.90777734e-07
Iter: 1292 loss: 2.90700768e-07
Iter: 1293 loss: 2.9063483e-07
Iter: 1294 loss: 2.9118803e-07
Iter: 1295 loss: 2.90641879e-07
Iter: 1296 loss: 2.90597541e-07
Iter: 1297 loss: 2.90655976e-07
Iter: 1298 loss: 2.90564685e-07
Iter: 1299 loss: 2.90524724e-07
Iter: 1300 loss: 2.90503635e-07
Iter: 1301 loss: 2.90479136e-07
Iter: 1302 loss: 2.90402483e-07
Iter: 1303 loss: 2.90749881e-07
Iter: 1304 loss: 2.90379546e-07
Iter: 1305 loss: 2.90326568e-07
Iter: 1306 loss: 2.9027467e-07
Iter: 1307 loss: 2.90256793e-07
Iter: 1308 loss: 2.90197363e-07
Iter: 1309 loss: 2.90197704e-07
Iter: 1310 loss: 2.90152173e-07
Iter: 1311 loss: 2.90269298e-07
Iter: 1312 loss: 2.90143504e-07
Iter: 1313 loss: 2.90088451e-07
Iter: 1314 loss: 2.90068783e-07
Iter: 1315 loss: 2.90057017e-07
Iter: 1316 loss: 2.90016317e-07
Iter: 1317 loss: 2.90465039e-07
Iter: 1318 loss: 2.90022427e-07
Iter: 1319 loss: 2.89978061e-07
Iter: 1320 loss: 2.90064776e-07
Iter: 1321 loss: 2.89964646e-07
Iter: 1322 loss: 2.89907632e-07
Iter: 1323 loss: 2.89910815e-07
Iter: 1324 loss: 2.89879551e-07
Iter: 1325 loss: 2.89824129e-07
Iter: 1326 loss: 2.90351295e-07
Iter: 1327 loss: 2.89819354e-07
Iter: 1328 loss: 2.89766604e-07
Iter: 1329 loss: 2.89796873e-07
Iter: 1330 loss: 2.89732156e-07
Iter: 1331 loss: 2.89659852e-07
Iter: 1332 loss: 2.89702967e-07
Iter: 1333 loss: 2.89623102e-07
Iter: 1334 loss: 2.89539969e-07
Iter: 1335 loss: 2.90159392e-07
Iter: 1336 loss: 2.89531e-07
Iter: 1337 loss: 2.89478493e-07
Iter: 1338 loss: 2.8946306e-07
Iter: 1339 loss: 2.89424065e-07
Iter: 1340 loss: 2.8937913e-07
Iter: 1341 loss: 2.89922212e-07
Iter: 1342 loss: 2.89376601e-07
Iter: 1343 loss: 2.89342438e-07
Iter: 1344 loss: 2.89522234e-07
Iter: 1345 loss: 2.89326e-07
Iter: 1346 loss: 2.89304069e-07
Iter: 1347 loss: 2.89236766e-07
Iter: 1348 loss: 2.90126337e-07
Iter: 1349 loss: 2.89233441e-07
Iter: 1350 loss: 2.89175773e-07
Iter: 1351 loss: 2.89637228e-07
Iter: 1352 loss: 2.89160425e-07
Iter: 1353 loss: 2.89085222e-07
Iter: 1354 loss: 2.89438617e-07
Iter: 1355 loss: 2.89075729e-07
Iter: 1356 loss: 2.89007346e-07
Iter: 1357 loss: 2.89011382e-07
Iter: 1358 loss: 2.88940328e-07
Iter: 1359 loss: 2.88859894e-07
Iter: 1360 loss: 2.89535819e-07
Iter: 1361 loss: 2.88857336e-07
Iter: 1362 loss: 2.88798219e-07
Iter: 1363 loss: 2.88905085e-07
Iter: 1364 loss: 2.88756269e-07
Iter: 1365 loss: 2.88685e-07
Iter: 1366 loss: 2.88783497e-07
Iter: 1367 loss: 2.88657333e-07
Iter: 1368 loss: 2.8858426e-07
Iter: 1369 loss: 2.89147039e-07
Iter: 1370 loss: 2.88583863e-07
Iter: 1371 loss: 2.88552059e-07
Iter: 1372 loss: 2.88491663e-07
Iter: 1373 loss: 2.88493595e-07
Iter: 1374 loss: 2.88444539e-07
Iter: 1375 loss: 2.8914306e-07
Iter: 1376 loss: 2.88435217e-07
Iter: 1377 loss: 2.8838781e-07
Iter: 1378 loss: 2.88472592e-07
Iter: 1379 loss: 2.88370018e-07
Iter: 1380 loss: 2.88309337e-07
Iter: 1381 loss: 2.8825346e-07
Iter: 1382 loss: 2.88243029e-07
Iter: 1383 loss: 2.88184168e-07
Iter: 1384 loss: 2.88744474e-07
Iter: 1385 loss: 2.88169474e-07
Iter: 1386 loss: 2.88114961e-07
Iter: 1387 loss: 2.88626666e-07
Iter: 1388 loss: 2.88114904e-07
Iter: 1389 loss: 2.88076876e-07
Iter: 1390 loss: 2.88033334e-07
Iter: 1391 loss: 2.88030549e-07
Iter: 1392 loss: 2.87967254e-07
Iter: 1393 loss: 2.8844e-07
Iter: 1394 loss: 2.87962735e-07
Iter: 1395 loss: 2.87916976e-07
Iter: 1396 loss: 2.87987405e-07
Iter: 1397 loss: 2.87898331e-07
Iter: 1398 loss: 2.87833586e-07
Iter: 1399 loss: 2.87825969e-07
Iter: 1400 loss: 2.87792e-07
Iter: 1401 loss: 2.87704637e-07
Iter: 1402 loss: 2.88472279e-07
Iter: 1403 loss: 2.87689318e-07
Iter: 1404 loss: 2.87641626e-07
Iter: 1405 loss: 2.87573556e-07
Iter: 1406 loss: 2.87571339e-07
Iter: 1407 loss: 2.87515633e-07
Iter: 1408 loss: 2.87509295e-07
Iter: 1409 loss: 2.87467e-07
Iter: 1410 loss: 2.87564802e-07
Iter: 1411 loss: 2.87445118e-07
Iter: 1412 loss: 2.87410643e-07
Iter: 1413 loss: 2.87342914e-07
Iter: 1414 loss: 2.87342033e-07
Iter: 1415 loss: 2.87283285e-07
Iter: 1416 loss: 2.87774839e-07
Iter: 1417 loss: 2.8727672e-07
Iter: 1418 loss: 2.87223401e-07
Iter: 1419 loss: 2.87602489e-07
Iter: 1420 loss: 2.87224054e-07
Iter: 1421 loss: 2.87175681e-07
Iter: 1422 loss: 2.87148879e-07
Iter: 1423 loss: 2.87128074e-07
Iter: 1424 loss: 2.87051535e-07
Iter: 1425 loss: 2.87533368e-07
Iter: 1426 loss: 2.87052558e-07
Iter: 1427 loss: 2.86983408e-07
Iter: 1428 loss: 2.87124493e-07
Iter: 1429 loss: 2.86969396e-07
Iter: 1430 loss: 2.86890184e-07
Iter: 1431 loss: 2.86870659e-07
Iter: 1432 loss: 2.86829476e-07
Iter: 1433 loss: 2.86746342e-07
Iter: 1434 loss: 2.87818e-07
Iter: 1435 loss: 2.86754016e-07
Iter: 1436 loss: 2.86696462e-07
Iter: 1437 loss: 2.86628904e-07
Iter: 1438 loss: 2.86627397e-07
Iter: 1439 loss: 2.86555803e-07
Iter: 1440 loss: 2.87418601e-07
Iter: 1441 loss: 2.8654398e-07
Iter: 1442 loss: 2.86494696e-07
Iter: 1443 loss: 2.86549493e-07
Iter: 1444 loss: 2.86451041e-07
Iter: 1445 loss: 2.86364e-07
Iter: 1446 loss: 2.86283864e-07
Iter: 1447 loss: 2.86263855e-07
Iter: 1448 loss: 2.86150197e-07
Iter: 1449 loss: 2.8686722e-07
Iter: 1450 loss: 2.86135275e-07
Iter: 1451 loss: 2.86054785e-07
Iter: 1452 loss: 2.86050891e-07
Iter: 1453 loss: 2.86004337e-07
Iter: 1454 loss: 2.85952808e-07
Iter: 1455 loss: 2.85953746e-07
Iter: 1456 loss: 2.85870783e-07
Iter: 1457 loss: 2.86478468e-07
Iter: 1458 loss: 2.85873568e-07
Iter: 1459 loss: 2.85801491e-07
Iter: 1460 loss: 2.859056e-07
Iter: 1461 loss: 2.85764827e-07
Iter: 1462 loss: 2.85695421e-07
Iter: 1463 loss: 2.85706193e-07
Iter: 1464 loss: 2.85641534e-07
Iter: 1465 loss: 2.85554933e-07
Iter: 1466 loss: 2.86328913e-07
Iter: 1467 loss: 2.85546378e-07
Iter: 1468 loss: 2.85482855e-07
Iter: 1469 loss: 2.8539273e-07
Iter: 1470 loss: 2.85392076e-07
Iter: 1471 loss: 2.85274893e-07
Iter: 1472 loss: 2.86282358e-07
Iter: 1473 loss: 2.85270318e-07
Iter: 1474 loss: 2.85203271e-07
Iter: 1475 loss: 2.85460231e-07
Iter: 1476 loss: 2.85172746e-07
Iter: 1477 loss: 2.85118176e-07
Iter: 1478 loss: 2.8507668e-07
Iter: 1479 loss: 2.85046724e-07
Iter: 1480 loss: 2.8498107e-07
Iter: 1481 loss: 2.85434879e-07
Iter: 1482 loss: 2.84977943e-07
Iter: 1483 loss: 2.84934117e-07
Iter: 1484 loss: 2.84932753e-07
Iter: 1485 loss: 2.84909e-07
Iter: 1486 loss: 2.84866445e-07
Iter: 1487 loss: 2.84855162e-07
Iter: 1488 loss: 2.84806504e-07
Iter: 1489 loss: 2.85209097e-07
Iter: 1490 loss: 2.84785727e-07
Iter: 1491 loss: 2.84731129e-07
Iter: 1492 loss: 2.84839672e-07
Iter: 1493 loss: 2.84704413e-07
Iter: 1494 loss: 2.84645353e-07
Iter: 1495 loss: 2.84659961e-07
Iter: 1496 loss: 2.84607779e-07
Iter: 1497 loss: 2.84530415e-07
Iter: 1498 loss: 2.85261734e-07
Iter: 1499 loss: 2.84533598e-07
Iter: 1500 loss: 2.84480507e-07
Iter: 1501 loss: 2.84450778e-07
Iter: 1502 loss: 2.84439579e-07
Iter: 1503 loss: 2.84375801e-07
Iter: 1504 loss: 2.85125594e-07
Iter: 1505 loss: 2.84374323e-07
Iter: 1506 loss: 2.84348914e-07
Iter: 1507 loss: 2.84440944e-07
Iter: 1508 loss: 2.84344338e-07
Iter: 1509 loss: 2.84304946e-07
Iter: 1510 loss: 2.84271522e-07
Iter: 1511 loss: 2.8425805e-07
Iter: 1512 loss: 2.84211694e-07
Iter: 1513 loss: 2.84354655e-07
Iter: 1514 loss: 2.84200041e-07
Iter: 1515 loss: 2.84159739e-07
Iter: 1516 loss: 2.84155362e-07
Iter: 1517 loss: 2.84125036e-07
Iter: 1518 loss: 2.84072826e-07
Iter: 1519 loss: 2.85332732e-07
Iter: 1520 loss: 2.84070154e-07
Iter: 1521 loss: 2.84021382e-07
Iter: 1522 loss: 2.84683182e-07
Iter: 1523 loss: 2.84026243e-07
Iter: 1524 loss: 2.8398216e-07
Iter: 1525 loss: 2.84071263e-07
Iter: 1526 loss: 2.83959281e-07
Iter: 1527 loss: 2.83916904e-07
Iter: 1528 loss: 2.83933133e-07
Iter: 1529 loss: 2.83881093e-07
Iter: 1530 loss: 2.83822715e-07
Iter: 1531 loss: 2.84359743e-07
Iter: 1532 loss: 2.83820441e-07
Iter: 1533 loss: 2.83790484e-07
Iter: 1534 loss: 2.83715508e-07
Iter: 1535 loss: 2.83717782e-07
Iter: 1536 loss: 2.83667816e-07
Iter: 1537 loss: 2.84225223e-07
Iter: 1538 loss: 2.83651474e-07
Iter: 1539 loss: 2.83606312e-07
Iter: 1540 loss: 2.8374933e-07
Iter: 1541 loss: 2.8357573e-07
Iter: 1542 loss: 2.83514055e-07
Iter: 1543 loss: 2.83452096e-07
Iter: 1544 loss: 2.83428705e-07
Iter: 1545 loss: 2.83335908e-07
Iter: 1546 loss: 2.83766838e-07
Iter: 1547 loss: 2.83342274e-07
Iter: 1548 loss: 2.83300722e-07
Iter: 1549 loss: 2.83286965e-07
Iter: 1550 loss: 2.83247942e-07
Iter: 1551 loss: 2.83193401e-07
Iter: 1552 loss: 2.83189934e-07
Iter: 1553 loss: 2.83118e-07
Iter: 1554 loss: 2.83935861e-07
Iter: 1555 loss: 2.83116833e-07
Iter: 1556 loss: 2.83072495e-07
Iter: 1557 loss: 2.83156794e-07
Iter: 1558 loss: 2.83052685e-07
Iter: 1559 loss: 2.82983422e-07
Iter: 1560 loss: 2.82980608e-07
Iter: 1561 loss: 2.8292996e-07
Iter: 1562 loss: 2.82829291e-07
Iter: 1563 loss: 2.83580221e-07
Iter: 1564 loss: 2.82830683e-07
Iter: 1565 loss: 2.82760226e-07
Iter: 1566 loss: 2.82651399e-07
Iter: 1567 loss: 2.82640059e-07
Iter: 1568 loss: 2.82505482e-07
Iter: 1569 loss: 2.83830616e-07
Iter: 1570 loss: 2.82518215e-07
Iter: 1571 loss: 2.82418739e-07
Iter: 1572 loss: 2.82851033e-07
Iter: 1573 loss: 2.82385656e-07
Iter: 1574 loss: 2.82306189e-07
Iter: 1575 loss: 2.82258327e-07
Iter: 1576 loss: 2.82229962e-07
Iter: 1577 loss: 2.82117924e-07
Iter: 1578 loss: 2.8219722e-07
Iter: 1579 loss: 2.82054458e-07
Iter: 1580 loss: 2.81971921e-07
Iter: 1581 loss: 2.81957512e-07
Iter: 1582 loss: 2.81861446e-07
Iter: 1583 loss: 2.81729513e-07
Iter: 1584 loss: 2.81730536e-07
Iter: 1585 loss: 2.81588257e-07
Iter: 1586 loss: 2.83325846e-07
Iter: 1587 loss: 2.81579446e-07
Iter: 1588 loss: 2.81467038e-07
Iter: 1589 loss: 2.81639132e-07
Iter: 1590 loss: 2.81424832e-07
Iter: 1591 loss: 2.81287896e-07
Iter: 1592 loss: 2.81455129e-07
Iter: 1593 loss: 2.81226107e-07
Iter: 1594 loss: 2.81093378e-07
Iter: 1595 loss: 2.82285754e-07
Iter: 1596 loss: 2.81083089e-07
Iter: 1597 loss: 2.80996e-07
Iter: 1598 loss: 2.80859268e-07
Iter: 1599 loss: 2.80860689e-07
Iter: 1600 loss: 2.80712356e-07
Iter: 1601 loss: 2.82043175e-07
Iter: 1602 loss: 2.80715057e-07
Iter: 1603 loss: 2.80589745e-07
Iter: 1604 loss: 2.8108326e-07
Iter: 1605 loss: 2.80568713e-07
Iter: 1606 loss: 2.80468527e-07
Iter: 1607 loss: 2.803001e-07
Iter: 1608 loss: 2.80295922e-07
Iter: 1609 loss: 2.80069e-07
Iter: 1610 loss: 2.8031377e-07
Iter: 1611 loss: 2.79948665e-07
Iter: 1612 loss: 2.79861098e-07
Iter: 1613 loss: 2.79800474e-07
Iter: 1614 loss: 2.79699663e-07
Iter: 1615 loss: 2.7958788e-07
Iter: 1616 loss: 2.79558378e-07
Iter: 1617 loss: 2.79454326e-07
Iter: 1618 loss: 2.79452536e-07
Iter: 1619 loss: 2.79393419e-07
Iter: 1620 loss: 2.79493463e-07
Iter: 1621 loss: 2.7934891e-07
Iter: 1622 loss: 2.79262423e-07
Iter: 1623 loss: 2.79322819e-07
Iter: 1624 loss: 2.79200776e-07
Iter: 1625 loss: 2.79105876e-07
Iter: 1626 loss: 2.80168592e-07
Iter: 1627 loss: 2.79116193e-07
Iter: 1628 loss: 2.79031894e-07
Iter: 1629 loss: 2.78920197e-07
Iter: 1630 loss: 2.78927075e-07
Iter: 1631 loss: 2.78803839e-07
Iter: 1632 loss: 2.79915582e-07
Iter: 1633 loss: 2.78805715e-07
Iter: 1634 loss: 2.78718488e-07
Iter: 1635 loss: 2.7925114e-07
Iter: 1636 loss: 2.78708228e-07
Iter: 1637 loss: 2.78653772e-07
Iter: 1638 loss: 2.7859798e-07
Iter: 1639 loss: 2.78579535e-07
Iter: 1640 loss: 2.78505183e-07
Iter: 1641 loss: 2.78676623e-07
Iter: 1642 loss: 2.78480684e-07
Iter: 1643 loss: 2.78462039e-07
Iter: 1644 loss: 2.78443935e-07
Iter: 1645 loss: 2.78417758e-07
Iter: 1646 loss: 2.78392605e-07
Iter: 1647 loss: 2.78372113e-07
Iter: 1648 loss: 2.7833542e-07
Iter: 1649 loss: 2.78551539e-07
Iter: 1650 loss: 2.78332209e-07
Iter: 1651 loss: 2.78289633e-07
Iter: 1652 loss: 2.78338916e-07
Iter: 1653 loss: 2.78269226e-07
Iter: 1654 loss: 2.78202833e-07
Iter: 1655 loss: 2.78214429e-07
Iter: 1656 loss: 2.78163441e-07
Iter: 1657 loss: 2.78072548e-07
Iter: 1658 loss: 2.78630353e-07
Iter: 1659 loss: 2.78071184e-07
Iter: 1660 loss: 2.77999334e-07
Iter: 1661 loss: 2.77925267e-07
Iter: 1662 loss: 2.77908384e-07
Iter: 1663 loss: 2.77826672e-07
Iter: 1664 loss: 2.78610912e-07
Iter: 1665 loss: 2.77842275e-07
Iter: 1666 loss: 2.77786683e-07
Iter: 1667 loss: 2.78021503e-07
Iter: 1668 loss: 2.77783e-07
Iter: 1669 loss: 2.77725121e-07
Iter: 1670 loss: 2.77692493e-07
Iter: 1671 loss: 2.7768894e-07
Iter: 1672 loss: 2.77593358e-07
Iter: 1673 loss: 2.7756289e-07
Iter: 1674 loss: 2.77533445e-07
Iter: 1675 loss: 2.77500106e-07
Iter: 1676 loss: 2.77471514e-07
Iter: 1677 loss: 2.7742368e-07
Iter: 1678 loss: 2.77338472e-07
Iter: 1679 loss: 2.79415872e-07
Iter: 1680 loss: 2.77332958e-07
Iter: 1681 loss: 2.77219073e-07
Iter: 1682 loss: 2.78115323e-07
Iter: 1683 loss: 2.77205885e-07
Iter: 1684 loss: 2.77112377e-07
Iter: 1685 loss: 2.77251274e-07
Iter: 1686 loss: 2.77070342e-07
Iter: 1687 loss: 2.7694341e-07
Iter: 1688 loss: 2.76967086e-07
Iter: 1689 loss: 2.76857293e-07
Iter: 1690 loss: 2.76708704e-07
Iter: 1691 loss: 2.77650145e-07
Iter: 1692 loss: 2.76701769e-07
Iter: 1693 loss: 2.76542721e-07
Iter: 1694 loss: 2.76498042e-07
Iter: 1695 loss: 2.76416472e-07
Iter: 1696 loss: 2.76263677e-07
Iter: 1697 loss: 2.77003437e-07
Iter: 1698 loss: 2.76245771e-07
Iter: 1699 loss: 2.76124979e-07
Iter: 1700 loss: 2.76855985e-07
Iter: 1701 loss: 2.76107215e-07
Iter: 1702 loss: 2.76003078e-07
Iter: 1703 loss: 2.75938589e-07
Iter: 1704 loss: 2.7590022e-07
Iter: 1705 loss: 2.75772265e-07
Iter: 1706 loss: 2.7599188e-07
Iter: 1707 loss: 2.75702689e-07
Iter: 1708 loss: 2.75656078e-07
Iter: 1709 loss: 2.75634591e-07
Iter: 1710 loss: 2.75581357e-07
Iter: 1711 loss: 2.75519227e-07
Iter: 1712 loss: 2.75503112e-07
Iter: 1713 loss: 2.75420689e-07
Iter: 1714 loss: 2.76009928e-07
Iter: 1715 loss: 2.75412788e-07
Iter: 1716 loss: 2.75326784e-07
Iter: 1717 loss: 2.75356683e-07
Iter: 1718 loss: 2.75268462e-07
Iter: 1719 loss: 2.75141701e-07
Iter: 1720 loss: 2.75369814e-07
Iter: 1721 loss: 2.75096824e-07
Iter: 1722 loss: 2.74982881e-07
Iter: 1723 loss: 2.75853893e-07
Iter: 1724 loss: 2.7497498e-07
Iter: 1725 loss: 2.74894433e-07
Iter: 1726 loss: 2.74935047e-07
Iter: 1727 loss: 2.74849867e-07
Iter: 1728 loss: 2.74782877e-07
Iter: 1729 loss: 2.75090741e-07
Iter: 1730 loss: 2.7476483e-07
Iter: 1731 loss: 2.74699687e-07
Iter: 1732 loss: 2.75117031e-07
Iter: 1733 loss: 2.74706e-07
Iter: 1734 loss: 2.74656372e-07
Iter: 1735 loss: 2.74635e-07
Iter: 1736 loss: 2.74622124e-07
Iter: 1737 loss: 2.74565025e-07
Iter: 1738 loss: 2.7456872e-07
Iter: 1739 loss: 2.74525917e-07
Iter: 1740 loss: 2.74473223e-07
Iter: 1741 loss: 2.74465037e-07
Iter: 1742 loss: 2.74405835e-07
Iter: 1743 loss: 2.74366869e-07
Iter: 1744 loss: 2.74336912e-07
Iter: 1745 loss: 2.74265176e-07
Iter: 1746 loss: 2.74565707e-07
Iter: 1747 loss: 2.7424494e-07
Iter: 1748 loss: 2.7415993e-07
Iter: 1749 loss: 2.7436144e-07
Iter: 1750 loss: 2.74130343e-07
Iter: 1751 loss: 2.74051729e-07
Iter: 1752 loss: 2.74338248e-07
Iter: 1753 loss: 2.74040474e-07
Iter: 1754 loss: 2.73986132e-07
Iter: 1755 loss: 2.74176472e-07
Iter: 1756 loss: 2.73974621e-07
Iter: 1757 loss: 2.73915731e-07
Iter: 1758 loss: 2.73912406e-07
Iter: 1759 loss: 2.73868295e-07
Iter: 1760 loss: 2.73798491e-07
Iter: 1761 loss: 2.73943158e-07
Iter: 1762 loss: 2.73772628e-07
Iter: 1763 loss: 2.7370217e-07
Iter: 1764 loss: 2.74245394e-07
Iter: 1765 loss: 2.73676648e-07
Iter: 1766 loss: 2.73621055e-07
Iter: 1767 loss: 2.73542923e-07
Iter: 1768 loss: 2.73543e-07
Iter: 1769 loss: 2.733982e-07
Iter: 1770 loss: 2.73494663e-07
Iter: 1771 loss: 2.73320666e-07
Iter: 1772 loss: 2.73276612e-07
Iter: 1773 loss: 2.73251032e-07
Iter: 1774 loss: 2.73167018e-07
Iter: 1775 loss: 2.73194928e-07
Iter: 1776 loss: 2.73125863e-07
Iter: 1777 loss: 2.73049295e-07
Iter: 1778 loss: 2.73273059e-07
Iter: 1779 loss: 2.73034232e-07
Iter: 1780 loss: 2.72954253e-07
Iter: 1781 loss: 2.7319885e-07
Iter: 1782 loss: 2.72930833e-07
Iter: 1783 loss: 2.72866828e-07
Iter: 1784 loss: 2.73061289e-07
Iter: 1785 loss: 2.72855289e-07
Iter: 1786 loss: 2.72787e-07
Iter: 1787 loss: 2.72969032e-07
Iter: 1788 loss: 2.7277e-07
Iter: 1789 loss: 2.7269e-07
Iter: 1790 loss: 2.72681e-07
Iter: 1791 loss: 2.72620639e-07
Iter: 1792 loss: 2.72537648e-07
Iter: 1793 loss: 2.72868476e-07
Iter: 1794 loss: 2.72514882e-07
Iter: 1795 loss: 2.72442151e-07
Iter: 1796 loss: 2.7313888e-07
Iter: 1797 loss: 2.72448517e-07
Iter: 1798 loss: 2.72408784e-07
Iter: 1799 loss: 2.72365355e-07
Iter: 1800 loss: 2.72360239e-07
Iter: 1801 loss: 2.72302714e-07
Iter: 1802 loss: 2.72333352e-07
Iter: 1803 loss: 2.7226082e-07
Iter: 1804 loss: 2.72193745e-07
Iter: 1805 loss: 2.72804073e-07
Iter: 1806 loss: 2.72191556e-07
Iter: 1807 loss: 2.721155e-07
Iter: 1808 loss: 2.72189709e-07
Iter: 1809 loss: 2.72089324e-07
Iter: 1810 loss: 2.7200889e-07
Iter: 1811 loss: 2.72161572e-07
Iter: 1812 loss: 2.71990899e-07
Iter: 1813 loss: 2.71905378e-07
Iter: 1814 loss: 2.72190078e-07
Iter: 1815 loss: 2.71880282e-07
Iter: 1816 loss: 2.71827e-07
Iter: 1817 loss: 2.72073919e-07
Iter: 1818 loss: 2.71800445e-07
Iter: 1819 loss: 2.71757983e-07
Iter: 1820 loss: 2.71880594e-07
Iter: 1821 loss: 2.71740163e-07
Iter: 1822 loss: 2.71676868e-07
Iter: 1823 loss: 2.71713361e-07
Iter: 1824 loss: 2.7163108e-07
Iter: 1825 loss: 2.71560737e-07
Iter: 1826 loss: 2.71710604e-07
Iter: 1827 loss: 2.71548572e-07
Iter: 1828 loss: 2.71475358e-07
Iter: 1829 loss: 2.72021509e-07
Iter: 1830 loss: 2.7147064e-07
Iter: 1831 loss: 2.71436932e-07
Iter: 1832 loss: 2.71367242e-07
Iter: 1833 loss: 2.71357351e-07
Iter: 1834 loss: 2.7126373e-07
Iter: 1835 loss: 2.71201145e-07
Iter: 1836 loss: 2.71153255e-07
Iter: 1837 loss: 2.71042893e-07
Iter: 1838 loss: 2.72510249e-07
Iter: 1839 loss: 2.71040278e-07
Iter: 1840 loss: 2.70927671e-07
Iter: 1841 loss: 2.71341094e-07
Iter: 1842 loss: 2.70885891e-07
Iter: 1843 loss: 2.70833965e-07
Iter: 1844 loss: 2.70898681e-07
Iter: 1845 loss: 2.70783687e-07
Iter: 1846 loss: 2.70713855e-07
Iter: 1847 loss: 2.71148878e-07
Iter: 1848 loss: 2.70698592e-07
Iter: 1849 loss: 2.7063939e-07
Iter: 1850 loss: 2.70767401e-07
Iter: 1851 loss: 2.7061165e-07
Iter: 1852 loss: 2.70536759e-07
Iter: 1853 loss: 2.70610485e-07
Iter: 1854 loss: 2.70504017e-07
Iter: 1855 loss: 2.70397948e-07
Iter: 1856 loss: 2.70478068e-07
Iter: 1857 loss: 2.70331668e-07
Iter: 1858 loss: 2.70233386e-07
Iter: 1859 loss: 2.70502937e-07
Iter: 1860 loss: 2.70195528e-07
Iter: 1861 loss: 2.70101395e-07
Iter: 1862 loss: 2.71032548e-07
Iter: 1863 loss: 2.70107563e-07
Iter: 1864 loss: 2.70026817e-07
Iter: 1865 loss: 2.70000982e-07
Iter: 1866 loss: 2.69967359e-07
Iter: 1867 loss: 2.69878512e-07
Iter: 1868 loss: 2.69935725e-07
Iter: 1869 loss: 2.69824056e-07
Iter: 1870 loss: 2.69736717e-07
Iter: 1871 loss: 2.702686e-07
Iter: 1872 loss: 2.69735324e-07
Iter: 1873 loss: 2.69648069e-07
Iter: 1874 loss: 2.70049412e-07
Iter: 1875 loss: 2.69627321e-07
Iter: 1876 loss: 2.69579e-07
Iter: 1877 loss: 2.69607625e-07
Iter: 1878 loss: 2.69550014e-07
Iter: 1879 loss: 2.69463101e-07
Iter: 1880 loss: 2.69820248e-07
Iter: 1881 loss: 2.69449231e-07
Iter: 1882 loss: 2.69376358e-07
Iter: 1883 loss: 2.69533984e-07
Iter: 1884 loss: 2.69340603e-07
Iter: 1885 loss: 2.69293196e-07
Iter: 1886 loss: 2.69396168e-07
Iter: 1887 loss: 2.69261307e-07
Iter: 1888 loss: 2.69175501e-07
Iter: 1889 loss: 2.69431155e-07
Iter: 1890 loss: 2.69165156e-07
Iter: 1891 loss: 2.69098678e-07
Iter: 1892 loss: 2.69195709e-07
Iter: 1893 loss: 2.69071e-07
Iter: 1894 loss: 2.69029698e-07
Iter: 1895 loss: 2.69537e-07
Iter: 1896 loss: 2.69023985e-07
Iter: 1897 loss: 2.68982177e-07
Iter: 1898 loss: 2.68961742e-07
Iter: 1899 loss: 2.68929938e-07
Iter: 1900 loss: 2.68863062e-07
Iter: 1901 loss: 2.68834924e-07
Iter: 1902 loss: 2.68794622e-07
Iter: 1903 loss: 2.68703872e-07
Iter: 1904 loss: 2.69282793e-07
Iter: 1905 loss: 2.68681418e-07
Iter: 1906 loss: 2.68602605e-07
Iter: 1907 loss: 2.69560701e-07
Iter: 1908 loss: 2.68590952e-07
Iter: 1909 loss: 2.68550423e-07
Iter: 1910 loss: 2.68516601e-07
Iter: 1911 loss: 2.68510519e-07
Iter: 1912 loss: 2.68438896e-07
Iter: 1913 loss: 2.68851e-07
Iter: 1914 loss: 2.68418802e-07
Iter: 1915 loss: 2.68367387e-07
Iter: 1916 loss: 2.68486758e-07
Iter: 1917 loss: 2.68341637e-07
Iter: 1918 loss: 2.68288431e-07
Iter: 1919 loss: 2.6834212e-07
Iter: 1920 loss: 2.68245344e-07
Iter: 1921 loss: 2.6815809e-07
Iter: 1922 loss: 2.68339761e-07
Iter: 1923 loss: 2.68119777e-07
Iter: 1924 loss: 2.68030817e-07
Iter: 1925 loss: 2.68082346e-07
Iter: 1926 loss: 2.67982273e-07
Iter: 1927 loss: 2.67900305e-07
Iter: 1928 loss: 2.68625598e-07
Iter: 1929 loss: 2.67872338e-07
Iter: 1930 loss: 2.67774936e-07
Iter: 1931 loss: 2.67772776e-07
Iter: 1932 loss: 2.67698738e-07
Iter: 1933 loss: 2.67592668e-07
Iter: 1934 loss: 2.67626035e-07
Iter: 1935 loss: 2.67519113e-07
Iter: 1936 loss: 2.6741202e-07
Iter: 1937 loss: 2.68051053e-07
Iter: 1938 loss: 2.67404459e-07
Iter: 1939 loss: 2.67304245e-07
Iter: 1940 loss: 2.68329472e-07
Iter: 1941 loss: 2.67313538e-07
Iter: 1942 loss: 2.67267723e-07
Iter: 1943 loss: 2.672422e-07
Iter: 1944 loss: 2.67214205e-07
Iter: 1945 loss: 2.67135079e-07
Iter: 1946 loss: 2.67615746e-07
Iter: 1947 loss: 2.67129934e-07
Iter: 1948 loss: 2.67069595e-07
Iter: 1949 loss: 2.67102251e-07
Iter: 1950 loss: 2.67026621e-07
Iter: 1951 loss: 2.66928907e-07
Iter: 1952 loss: 2.67125927e-07
Iter: 1953 loss: 2.66891959e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.4
+ date
Mon Oct 26 14:46:07 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.4/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.4_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 1 --phi 0.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.4_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f74a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f855d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f855b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f855158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f793730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f793d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f7209d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f68c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f6c1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f651378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f68c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f6c1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f5cff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f600510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f5739d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f570c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f5a5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f5c5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd24f528598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2230716a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd223071510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd22303d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd22303dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd223010840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2230102f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd223016d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1fc044598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd222fa5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd222fa5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd222f91378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd222f91620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1e01ca158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1e01b4268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1e016b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1e0190598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1e0126f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.92650815e-06
Iter: 2 loss: 3.11766166e-06
Iter: 3 loss: 3.0091503e-06
Iter: 4 loss: 2.35904781e-06
Iter: 5 loss: 4.1558269e-06
Iter: 6 loss: 2.14797319e-06
Iter: 7 loss: 2.02678848e-06
Iter: 8 loss: 2.22040808e-06
Iter: 9 loss: 1.9706797e-06
Iter: 10 loss: 1.83213342e-06
Iter: 11 loss: 2.92551363e-06
Iter: 12 loss: 1.82258759e-06
Iter: 13 loss: 1.74180218e-06
Iter: 14 loss: 1.7383345e-06
Iter: 15 loss: 1.67612666e-06
Iter: 16 loss: 1.67581845e-06
Iter: 17 loss: 1.65765471e-06
Iter: 18 loss: 1.64354992e-06
Iter: 19 loss: 1.60264779e-06
Iter: 20 loss: 1.76954552e-06
Iter: 21 loss: 1.58623982e-06
Iter: 22 loss: 1.53548967e-06
Iter: 23 loss: 1.60239006e-06
Iter: 24 loss: 1.50971573e-06
Iter: 25 loss: 1.46613581e-06
Iter: 26 loss: 1.46534558e-06
Iter: 27 loss: 1.42415934e-06
Iter: 28 loss: 1.36181086e-06
Iter: 29 loss: 1.36064216e-06
Iter: 30 loss: 1.32200387e-06
Iter: 31 loss: 1.38588371e-06
Iter: 32 loss: 1.30433227e-06
Iter: 33 loss: 1.27702776e-06
Iter: 34 loss: 1.3162454e-06
Iter: 35 loss: 1.26370924e-06
Iter: 36 loss: 1.24471171e-06
Iter: 37 loss: 1.24362066e-06
Iter: 38 loss: 1.2349966e-06
Iter: 39 loss: 1.22750043e-06
Iter: 40 loss: 1.22523181e-06
Iter: 41 loss: 1.20950097e-06
Iter: 42 loss: 1.24132578e-06
Iter: 43 loss: 1.20310779e-06
Iter: 44 loss: 1.17936156e-06
Iter: 45 loss: 1.2646102e-06
Iter: 46 loss: 1.17331422e-06
Iter: 47 loss: 1.16139427e-06
Iter: 48 loss: 1.16265505e-06
Iter: 49 loss: 1.1521779e-06
Iter: 50 loss: 1.13946248e-06
Iter: 51 loss: 1.13945975e-06
Iter: 52 loss: 1.12844452e-06
Iter: 53 loss: 1.1065423e-06
Iter: 54 loss: 1.53141229e-06
Iter: 55 loss: 1.10632982e-06
Iter: 56 loss: 1.09384962e-06
Iter: 57 loss: 1.13793112e-06
Iter: 58 loss: 1.09066275e-06
Iter: 59 loss: 1.07679421e-06
Iter: 60 loss: 1.22062852e-06
Iter: 61 loss: 1.07643382e-06
Iter: 62 loss: 1.06515722e-06
Iter: 63 loss: 1.05576339e-06
Iter: 64 loss: 1.0525531e-06
Iter: 65 loss: 1.03681066e-06
Iter: 66 loss: 1.0817165e-06
Iter: 67 loss: 1.03188972e-06
Iter: 68 loss: 1.01445676e-06
Iter: 69 loss: 1.09723373e-06
Iter: 70 loss: 1.01129024e-06
Iter: 71 loss: 9.9113322e-07
Iter: 72 loss: 9.75532e-07
Iter: 73 loss: 9.69177336e-07
Iter: 74 loss: 9.54786174e-07
Iter: 75 loss: 1.14737918e-06
Iter: 76 loss: 9.5469e-07
Iter: 77 loss: 9.46518924e-07
Iter: 78 loss: 1.05017853e-06
Iter: 79 loss: 9.46427e-07
Iter: 80 loss: 9.39888e-07
Iter: 81 loss: 9.26590076e-07
Iter: 82 loss: 1.16726608e-06
Iter: 83 loss: 9.26329108e-07
Iter: 84 loss: 9.19911827e-07
Iter: 85 loss: 9.19344529e-07
Iter: 86 loss: 9.12556231e-07
Iter: 87 loss: 9.13357951e-07
Iter: 88 loss: 9.07431968e-07
Iter: 89 loss: 8.9844184e-07
Iter: 90 loss: 8.7974189e-07
Iter: 91 loss: 1.19682045e-06
Iter: 92 loss: 8.79287711e-07
Iter: 93 loss: 8.65799279e-07
Iter: 94 loss: 8.65315315e-07
Iter: 95 loss: 8.49572643e-07
Iter: 96 loss: 8.58591363e-07
Iter: 97 loss: 8.39306495e-07
Iter: 98 loss: 8.23454229e-07
Iter: 99 loss: 8.39202585e-07
Iter: 100 loss: 8.14510372e-07
Iter: 101 loss: 8.07740889e-07
Iter: 102 loss: 8.06380115e-07
Iter: 103 loss: 8.01014153e-07
Iter: 104 loss: 7.99798158e-07
Iter: 105 loss: 7.96347308e-07
Iter: 106 loss: 7.89199134e-07
Iter: 107 loss: 7.83808105e-07
Iter: 108 loss: 7.81501e-07
Iter: 109 loss: 7.78215281e-07
Iter: 110 loss: 7.76305455e-07
Iter: 111 loss: 7.71793111e-07
Iter: 112 loss: 7.68563609e-07
Iter: 113 loss: 7.66998937e-07
Iter: 114 loss: 7.59535681e-07
Iter: 115 loss: 7.51544519e-07
Iter: 116 loss: 7.50255538e-07
Iter: 117 loss: 7.39379857e-07
Iter: 118 loss: 7.39170218e-07
Iter: 119 loss: 7.33850698e-07
Iter: 120 loss: 7.2638494e-07
Iter: 121 loss: 7.26104417e-07
Iter: 122 loss: 7.16998557e-07
Iter: 123 loss: 7.26517783e-07
Iter: 124 loss: 7.11927441e-07
Iter: 125 loss: 7.12768951e-07
Iter: 126 loss: 7.07824313e-07
Iter: 127 loss: 7.05945354e-07
Iter: 128 loss: 7.01500596e-07
Iter: 129 loss: 7.51822313e-07
Iter: 130 loss: 7.01056e-07
Iter: 131 loss: 6.95600306e-07
Iter: 132 loss: 7.37652272e-07
Iter: 133 loss: 6.95224912e-07
Iter: 134 loss: 6.89692115e-07
Iter: 135 loss: 7.02835109e-07
Iter: 136 loss: 6.87668944e-07
Iter: 137 loss: 6.83605322e-07
Iter: 138 loss: 6.82174118e-07
Iter: 139 loss: 6.79873835e-07
Iter: 140 loss: 6.74348144e-07
Iter: 141 loss: 6.9812836e-07
Iter: 142 loss: 6.7319769e-07
Iter: 143 loss: 6.67370273e-07
Iter: 144 loss: 7.16215e-07
Iter: 145 loss: 6.67020117e-07
Iter: 146 loss: 6.64458696e-07
Iter: 147 loss: 6.62951209e-07
Iter: 148 loss: 6.61870899e-07
Iter: 149 loss: 6.58996498e-07
Iter: 150 loss: 7.0056592e-07
Iter: 151 loss: 6.58992519e-07
Iter: 152 loss: 6.56258635e-07
Iter: 153 loss: 6.53500933e-07
Iter: 154 loss: 6.52935455e-07
Iter: 155 loss: 6.49564868e-07
Iter: 156 loss: 6.49496656e-07
Iter: 157 loss: 6.46835758e-07
Iter: 158 loss: 6.41819327e-07
Iter: 159 loss: 6.80384801e-07
Iter: 160 loss: 6.41451948e-07
Iter: 161 loss: 6.35184676e-07
Iter: 162 loss: 6.35958884e-07
Iter: 163 loss: 6.30382942e-07
Iter: 164 loss: 6.26672204e-07
Iter: 165 loss: 6.42245936e-07
Iter: 166 loss: 6.25860821e-07
Iter: 167 loss: 6.21626498e-07
Iter: 168 loss: 6.42884231e-07
Iter: 169 loss: 6.20925277e-07
Iter: 170 loss: 6.17571231e-07
Iter: 171 loss: 6.14537896e-07
Iter: 172 loss: 6.13729298e-07
Iter: 173 loss: 6.10861e-07
Iter: 174 loss: 6.29821443e-07
Iter: 175 loss: 6.10553e-07
Iter: 176 loss: 6.08485948e-07
Iter: 177 loss: 6.37924529e-07
Iter: 178 loss: 6.08473783e-07
Iter: 179 loss: 6.06630465e-07
Iter: 180 loss: 6.02732e-07
Iter: 181 loss: 6.65960272e-07
Iter: 182 loss: 6.02620275e-07
Iter: 183 loss: 5.99150326e-07
Iter: 184 loss: 6.34664843e-07
Iter: 185 loss: 5.99036412e-07
Iter: 186 loss: 5.95131837e-07
Iter: 187 loss: 5.98146414e-07
Iter: 188 loss: 5.92714059e-07
Iter: 189 loss: 5.8816812e-07
Iter: 190 loss: 5.84436123e-07
Iter: 191 loss: 5.83101382e-07
Iter: 192 loss: 5.79199536e-07
Iter: 193 loss: 6.28979535e-07
Iter: 194 loss: 5.79148946e-07
Iter: 195 loss: 5.75671834e-07
Iter: 196 loss: 5.97183202e-07
Iter: 197 loss: 5.75239142e-07
Iter: 198 loss: 5.7293272e-07
Iter: 199 loss: 5.70494706e-07
Iter: 200 loss: 5.70041607e-07
Iter: 201 loss: 5.69121426e-07
Iter: 202 loss: 5.68516271e-07
Iter: 203 loss: 5.67247298e-07
Iter: 204 loss: 5.64242555e-07
Iter: 205 loss: 5.97031658e-07
Iter: 206 loss: 5.63936e-07
Iter: 207 loss: 5.5991984e-07
Iter: 208 loss: 5.64134552e-07
Iter: 209 loss: 5.57663498e-07
Iter: 210 loss: 5.55781639e-07
Iter: 211 loss: 5.55468e-07
Iter: 212 loss: 5.53081236e-07
Iter: 213 loss: 5.50553182e-07
Iter: 214 loss: 5.50136633e-07
Iter: 215 loss: 5.47383308e-07
Iter: 216 loss: 5.62371838e-07
Iter: 217 loss: 5.47017635e-07
Iter: 218 loss: 5.45579098e-07
Iter: 219 loss: 5.45563239e-07
Iter: 220 loss: 5.44586044e-07
Iter: 221 loss: 5.42029682e-07
Iter: 222 loss: 5.6095e-07
Iter: 223 loss: 5.41488191e-07
Iter: 224 loss: 5.38328e-07
Iter: 225 loss: 5.56275e-07
Iter: 226 loss: 5.3789438e-07
Iter: 227 loss: 5.36694074e-07
Iter: 228 loss: 5.36533776e-07
Iter: 229 loss: 5.35260199e-07
Iter: 230 loss: 5.31771832e-07
Iter: 231 loss: 5.53705434e-07
Iter: 232 loss: 5.30895647e-07
Iter: 233 loss: 5.28079909e-07
Iter: 234 loss: 5.28007718e-07
Iter: 235 loss: 5.25472103e-07
Iter: 236 loss: 5.32212084e-07
Iter: 237 loss: 5.24648726e-07
Iter: 238 loss: 5.23189897e-07
Iter: 239 loss: 5.21167806e-07
Iter: 240 loss: 5.21063953e-07
Iter: 241 loss: 5.19486321e-07
Iter: 242 loss: 5.1945608e-07
Iter: 243 loss: 5.18240654e-07
Iter: 244 loss: 5.27750899e-07
Iter: 245 loss: 5.18146237e-07
Iter: 246 loss: 5.1738914e-07
Iter: 247 loss: 5.15529791e-07
Iter: 248 loss: 5.35459e-07
Iter: 249 loss: 5.15328338e-07
Iter: 250 loss: 5.13966711e-07
Iter: 251 loss: 5.13823579e-07
Iter: 252 loss: 5.12597524e-07
Iter: 253 loss: 5.11662392e-07
Iter: 254 loss: 5.11292114e-07
Iter: 255 loss: 5.093151e-07
Iter: 256 loss: 5.05703099e-07
Iter: 257 loss: 5.91111188e-07
Iter: 258 loss: 5.05687467e-07
Iter: 259 loss: 5.04124841e-07
Iter: 260 loss: 5.03373712e-07
Iter: 261 loss: 5.0166193e-07
Iter: 262 loss: 5.07217237e-07
Iter: 263 loss: 5.01191721e-07
Iter: 264 loss: 5.00236183e-07
Iter: 265 loss: 4.99646603e-07
Iter: 266 loss: 4.99290479e-07
Iter: 267 loss: 4.97903898e-07
Iter: 268 loss: 5.19272419e-07
Iter: 269 loss: 4.97897702e-07
Iter: 270 loss: 4.97247811e-07
Iter: 271 loss: 4.96127541e-07
Iter: 272 loss: 5.23477e-07
Iter: 273 loss: 4.96130497e-07
Iter: 274 loss: 4.94682695e-07
Iter: 275 loss: 4.94599e-07
Iter: 276 loss: 4.93492735e-07
Iter: 277 loss: 4.93083064e-07
Iter: 278 loss: 4.92454149e-07
Iter: 279 loss: 4.91654646e-07
Iter: 280 loss: 4.89659e-07
Iter: 281 loss: 5.09260644e-07
Iter: 282 loss: 4.89445256e-07
Iter: 283 loss: 4.86929764e-07
Iter: 284 loss: 5.00176043e-07
Iter: 285 loss: 4.86541694e-07
Iter: 286 loss: 4.84025691e-07
Iter: 287 loss: 5.03158503e-07
Iter: 288 loss: 4.83826739e-07
Iter: 289 loss: 4.82932e-07
Iter: 290 loss: 4.82233645e-07
Iter: 291 loss: 4.81960626e-07
Iter: 292 loss: 4.80512256e-07
Iter: 293 loss: 4.83042356e-07
Iter: 294 loss: 4.79861683e-07
Iter: 295 loss: 4.78539732e-07
Iter: 296 loss: 4.7851546e-07
Iter: 297 loss: 4.77900812e-07
Iter: 298 loss: 4.769214e-07
Iter: 299 loss: 4.76913613e-07
Iter: 300 loss: 4.75781434e-07
Iter: 301 loss: 4.89198555e-07
Iter: 302 loss: 4.75741729e-07
Iter: 303 loss: 4.74581384e-07
Iter: 304 loss: 4.72751424e-07
Iter: 305 loss: 4.72715e-07
Iter: 306 loss: 4.71193289e-07
Iter: 307 loss: 4.72237843e-07
Iter: 308 loss: 4.70211035e-07
Iter: 309 loss: 4.69130327e-07
Iter: 310 loss: 4.69071324e-07
Iter: 311 loss: 4.68039076e-07
Iter: 312 loss: 4.71304702e-07
Iter: 313 loss: 4.67754035e-07
Iter: 314 loss: 4.67268677e-07
Iter: 315 loss: 4.66450217e-07
Iter: 316 loss: 4.66432084e-07
Iter: 317 loss: 4.65280863e-07
Iter: 318 loss: 4.78267793e-07
Iter: 319 loss: 4.65261223e-07
Iter: 320 loss: 4.64424375e-07
Iter: 321 loss: 4.63259568e-07
Iter: 322 loss: 4.63201445e-07
Iter: 323 loss: 4.61704587e-07
Iter: 324 loss: 4.61072659e-07
Iter: 325 loss: 4.60287822e-07
Iter: 326 loss: 4.58556599e-07
Iter: 327 loss: 4.58490177e-07
Iter: 328 loss: 4.57110787e-07
Iter: 329 loss: 4.57958265e-07
Iter: 330 loss: 4.5621141e-07
Iter: 331 loss: 4.55341706e-07
Iter: 332 loss: 4.59344847e-07
Iter: 333 loss: 4.55177542e-07
Iter: 334 loss: 4.54058721e-07
Iter: 335 loss: 4.55825159e-07
Iter: 336 loss: 4.53525217e-07
Iter: 337 loss: 4.52822803e-07
Iter: 338 loss: 4.53116115e-07
Iter: 339 loss: 4.52320762e-07
Iter: 340 loss: 4.51545446e-07
Iter: 341 loss: 4.51444578e-07
Iter: 342 loss: 4.5087495e-07
Iter: 343 loss: 4.49705482e-07
Iter: 344 loss: 4.68159726e-07
Iter: 345 loss: 4.49703549e-07
Iter: 346 loss: 4.49043029e-07
Iter: 347 loss: 4.47874498e-07
Iter: 348 loss: 4.75040707e-07
Iter: 349 loss: 4.47887032e-07
Iter: 350 loss: 4.46720662e-07
Iter: 351 loss: 4.61612956e-07
Iter: 352 loss: 4.467027e-07
Iter: 353 loss: 4.45456237e-07
Iter: 354 loss: 4.4456948e-07
Iter: 355 loss: 4.44139573e-07
Iter: 356 loss: 4.4298605e-07
Iter: 357 loss: 4.44894283e-07
Iter: 358 loss: 4.42481564e-07
Iter: 359 loss: 4.41448805e-07
Iter: 360 loss: 4.49737342e-07
Iter: 361 loss: 4.41374908e-07
Iter: 362 loss: 4.40415647e-07
Iter: 363 loss: 4.46351692e-07
Iter: 364 loss: 4.40308185e-07
Iter: 365 loss: 4.39868018e-07
Iter: 366 loss: 4.39423729e-07
Iter: 367 loss: 4.39334144e-07
Iter: 368 loss: 4.38478821e-07
Iter: 369 loss: 4.46597028e-07
Iter: 370 loss: 4.38455402e-07
Iter: 371 loss: 4.37747047e-07
Iter: 372 loss: 4.36441212e-07
Iter: 373 loss: 4.66870773e-07
Iter: 374 loss: 4.36457128e-07
Iter: 375 loss: 4.35498436e-07
Iter: 376 loss: 4.38976372e-07
Iter: 377 loss: 4.35286722e-07
Iter: 378 loss: 4.3468421e-07
Iter: 379 loss: 4.34653373e-07
Iter: 380 loss: 4.34047195e-07
Iter: 381 loss: 4.33612286e-07
Iter: 382 loss: 4.33394518e-07
Iter: 383 loss: 4.32859224e-07
Iter: 384 loss: 4.33855632e-07
Iter: 385 loss: 4.32646289e-07
Iter: 386 loss: 4.31796138e-07
Iter: 387 loss: 4.35002903e-07
Iter: 388 loss: 4.31590138e-07
Iter: 389 loss: 4.30950593e-07
Iter: 390 loss: 4.30179313e-07
Iter: 391 loss: 4.30107377e-07
Iter: 392 loss: 4.29066631e-07
Iter: 393 loss: 4.30384034e-07
Iter: 394 loss: 4.28516017e-07
Iter: 395 loss: 4.27135859e-07
Iter: 396 loss: 4.43940507e-07
Iter: 397 loss: 4.27137053e-07
Iter: 398 loss: 4.26203542e-07
Iter: 399 loss: 4.2600908e-07
Iter: 400 loss: 4.25406967e-07
Iter: 401 loss: 4.24912173e-07
Iter: 402 loss: 4.2491456e-07
Iter: 403 loss: 4.24372899e-07
Iter: 404 loss: 4.23851304e-07
Iter: 405 loss: 4.23721929e-07
Iter: 406 loss: 4.2308335e-07
Iter: 407 loss: 4.23557481e-07
Iter: 408 loss: 4.22676038e-07
Iter: 409 loss: 4.21996162e-07
Iter: 410 loss: 4.23240749e-07
Iter: 411 loss: 4.21676418e-07
Iter: 412 loss: 4.20807538e-07
Iter: 413 loss: 4.31285514e-07
Iter: 414 loss: 4.20805321e-07
Iter: 415 loss: 4.20263e-07
Iter: 416 loss: 4.19193327e-07
Iter: 417 loss: 4.38274924e-07
Iter: 418 loss: 4.19157658e-07
Iter: 419 loss: 4.18162983e-07
Iter: 420 loss: 4.33603759e-07
Iter: 421 loss: 4.1817168e-07
Iter: 422 loss: 4.17100921e-07
Iter: 423 loss: 4.16741813e-07
Iter: 424 loss: 4.16115199e-07
Iter: 425 loss: 4.15548357e-07
Iter: 426 loss: 4.16378384e-07
Iter: 427 loss: 4.15255982e-07
Iter: 428 loss: 4.14645172e-07
Iter: 429 loss: 4.19221578e-07
Iter: 430 loss: 4.14608564e-07
Iter: 431 loss: 4.13934401e-07
Iter: 432 loss: 4.1541341e-07
Iter: 433 loss: 4.13691339e-07
Iter: 434 loss: 4.13324358e-07
Iter: 435 loss: 4.13462033e-07
Iter: 436 loss: 4.13073423e-07
Iter: 437 loss: 4.12609893e-07
Iter: 438 loss: 4.17074602e-07
Iter: 439 loss: 4.12596307e-07
Iter: 440 loss: 4.12058824e-07
Iter: 441 loss: 4.10852095e-07
Iter: 442 loss: 4.27421071e-07
Iter: 443 loss: 4.10800112e-07
Iter: 444 loss: 4.0980666e-07
Iter: 445 loss: 4.13224e-07
Iter: 446 loss: 4.09580537e-07
Iter: 447 loss: 4.08937524e-07
Iter: 448 loss: 4.19110535e-07
Iter: 449 loss: 4.08936444e-07
Iter: 450 loss: 4.08332227e-07
Iter: 451 loss: 4.10288777e-07
Iter: 452 loss: 4.08168376e-07
Iter: 453 loss: 4.07876655e-07
Iter: 454 loss: 4.07426768e-07
Iter: 455 loss: 4.07422561e-07
Iter: 456 loss: 4.06840456e-07
Iter: 457 loss: 4.06842e-07
Iter: 458 loss: 4.06466256e-07
Iter: 459 loss: 4.05839273e-07
Iter: 460 loss: 4.05821197e-07
Iter: 461 loss: 4.0529693e-07
Iter: 462 loss: 4.05657801e-07
Iter: 463 loss: 4.04947571e-07
Iter: 464 loss: 4.04299044e-07
Iter: 465 loss: 4.04294809e-07
Iter: 466 loss: 4.03784554e-07
Iter: 467 loss: 4.0316516e-07
Iter: 468 loss: 4.03101183e-07
Iter: 469 loss: 4.02569356e-07
Iter: 470 loss: 4.03747663e-07
Iter: 471 loss: 4.02349883e-07
Iter: 472 loss: 4.02017804e-07
Iter: 473 loss: 4.01960165e-07
Iter: 474 loss: 4.01676232e-07
Iter: 475 loss: 4.01272501e-07
Iter: 476 loss: 4.01256329e-07
Iter: 477 loss: 4.00849899e-07
Iter: 478 loss: 4.00270721e-07
Iter: 479 loss: 4.00237269e-07
Iter: 480 loss: 3.99376e-07
Iter: 481 loss: 4.07895897e-07
Iter: 482 loss: 3.99359749e-07
Iter: 483 loss: 3.98531427e-07
Iter: 484 loss: 4.04147045e-07
Iter: 485 loss: 3.98465659e-07
Iter: 486 loss: 3.98089071e-07
Iter: 487 loss: 3.97256855e-07
Iter: 488 loss: 4.09938366e-07
Iter: 489 loss: 3.97225733e-07
Iter: 490 loss: 3.96947172e-07
Iter: 491 loss: 3.96720054e-07
Iter: 492 loss: 3.96394711e-07
Iter: 493 loss: 3.96113307e-07
Iter: 494 loss: 3.96032874e-07
Iter: 495 loss: 3.9568323e-07
Iter: 496 loss: 3.9523718e-07
Iter: 497 loss: 3.95212709e-07
Iter: 498 loss: 3.94590529e-07
Iter: 499 loss: 3.97910526e-07
Iter: 500 loss: 3.94492133e-07
Iter: 501 loss: 3.94012091e-07
Iter: 502 loss: 3.98363454e-07
Iter: 503 loss: 3.9400669e-07
Iter: 504 loss: 3.93366349e-07
Iter: 505 loss: 3.93225235e-07
Iter: 506 loss: 3.92806442e-07
Iter: 507 loss: 3.92293202e-07
Iter: 508 loss: 3.93051437e-07
Iter: 509 loss: 3.92061054e-07
Iter: 510 loss: 3.9166332e-07
Iter: 511 loss: 3.91662553e-07
Iter: 512 loss: 3.9126445e-07
Iter: 513 loss: 3.90893717e-07
Iter: 514 loss: 3.90802484e-07
Iter: 515 loss: 3.90487031e-07
Iter: 516 loss: 3.91024514e-07
Iter: 517 loss: 3.90330399e-07
Iter: 518 loss: 3.90031232e-07
Iter: 519 loss: 3.93434362e-07
Iter: 520 loss: 3.90035e-07
Iter: 521 loss: 3.89639638e-07
Iter: 522 loss: 3.89006345e-07
Iter: 523 loss: 3.89003873e-07
Iter: 524 loss: 3.88454339e-07
Iter: 525 loss: 3.90700905e-07
Iter: 526 loss: 3.88318256e-07
Iter: 527 loss: 3.87896023e-07
Iter: 528 loss: 3.87893948e-07
Iter: 529 loss: 3.87585573e-07
Iter: 530 loss: 3.87004491e-07
Iter: 531 loss: 3.97555e-07
Iter: 532 loss: 3.86973568e-07
Iter: 533 loss: 3.86527802e-07
Iter: 534 loss: 3.87809223e-07
Iter: 535 loss: 3.86362984e-07
Iter: 536 loss: 3.85941291e-07
Iter: 537 loss: 3.89938862e-07
Iter: 538 loss: 3.85935635e-07
Iter: 539 loss: 3.85451415e-07
Iter: 540 loss: 3.86672525e-07
Iter: 541 loss: 3.85305555e-07
Iter: 542 loss: 3.85042313e-07
Iter: 543 loss: 3.84524753e-07
Iter: 544 loss: 3.94087863e-07
Iter: 545 loss: 3.84509349e-07
Iter: 546 loss: 3.83973401e-07
Iter: 547 loss: 3.83967603e-07
Iter: 548 loss: 3.83398628e-07
Iter: 549 loss: 3.834312e-07
Iter: 550 loss: 3.82944734e-07
Iter: 551 loss: 3.82552884e-07
Iter: 552 loss: 3.82264318e-07
Iter: 553 loss: 3.82149466e-07
Iter: 554 loss: 3.82051e-07
Iter: 555 loss: 3.81907199e-07
Iter: 556 loss: 3.81707537e-07
Iter: 557 loss: 3.81915015e-07
Iter: 558 loss: 3.81612381e-07
Iter: 559 loss: 3.81430112e-07
Iter: 560 loss: 3.8108152e-07
Iter: 561 loss: 3.86797097e-07
Iter: 562 loss: 3.81053155e-07
Iter: 563 loss: 3.80654e-07
Iter: 564 loss: 3.80656388e-07
Iter: 565 loss: 3.80295603e-07
Iter: 566 loss: 3.79937035e-07
Iter: 567 loss: 3.7987752e-07
Iter: 568 loss: 3.79403275e-07
Iter: 569 loss: 3.78928235e-07
Iter: 570 loss: 3.78856441e-07
Iter: 571 loss: 3.78317708e-07
Iter: 572 loss: 3.78259642e-07
Iter: 573 loss: 3.77823596e-07
Iter: 574 loss: 3.80133656e-07
Iter: 575 loss: 3.77748052e-07
Iter: 576 loss: 3.77540175e-07
Iter: 577 loss: 3.77147387e-07
Iter: 578 loss: 3.83986304e-07
Iter: 579 loss: 3.77134711e-07
Iter: 580 loss: 3.77014516e-07
Iter: 581 loss: 3.76931297e-07
Iter: 582 loss: 3.76724586e-07
Iter: 583 loss: 3.76866211e-07
Iter: 584 loss: 3.76606e-07
Iter: 585 loss: 3.76413254e-07
Iter: 586 loss: 3.75922809e-07
Iter: 587 loss: 3.80406959e-07
Iter: 588 loss: 3.75858519e-07
Iter: 589 loss: 3.7568762e-07
Iter: 590 loss: 3.75563673e-07
Iter: 591 loss: 3.75212e-07
Iter: 592 loss: 3.75099546e-07
Iter: 593 loss: 3.74935155e-07
Iter: 594 loss: 3.74503031e-07
Iter: 595 loss: 3.74475661e-07
Iter: 596 loss: 3.74164244e-07
Iter: 597 loss: 3.73685026e-07
Iter: 598 loss: 3.73679086e-07
Iter: 599 loss: 3.73520606e-07
Iter: 600 loss: 3.73210185e-07
Iter: 601 loss: 3.73210383e-07
Iter: 602 loss: 3.7284417e-07
Iter: 603 loss: 3.73704893e-07
Iter: 604 loss: 3.7271343e-07
Iter: 605 loss: 3.72431714e-07
Iter: 606 loss: 3.76460719e-07
Iter: 607 loss: 3.72431856e-07
Iter: 608 loss: 3.72200731e-07
Iter: 609 loss: 3.71905401e-07
Iter: 610 loss: 3.71883374e-07
Iter: 611 loss: 3.71463273e-07
Iter: 612 loss: 3.71247069e-07
Iter: 613 loss: 3.71042916e-07
Iter: 614 loss: 3.71034162e-07
Iter: 615 loss: 3.70793373e-07
Iter: 616 loss: 3.70580551e-07
Iter: 617 loss: 3.70073565e-07
Iter: 618 loss: 3.74470574e-07
Iter: 619 loss: 3.70002141e-07
Iter: 620 loss: 3.69626719e-07
Iter: 621 loss: 3.73499404e-07
Iter: 622 loss: 3.69625269e-07
Iter: 623 loss: 3.69524059e-07
Iter: 624 loss: 3.69506154e-07
Iter: 625 loss: 3.69398407e-07
Iter: 626 loss: 3.69061354e-07
Iter: 627 loss: 3.69760301e-07
Iter: 628 loss: 3.68851431e-07
Iter: 629 loss: 3.68572046e-07
Iter: 630 loss: 3.68520318e-07
Iter: 631 loss: 3.6822405e-07
Iter: 632 loss: 3.67982665e-07
Iter: 633 loss: 3.67869518e-07
Iter: 634 loss: 3.67456835e-07
Iter: 635 loss: 3.6699e-07
Iter: 636 loss: 3.66938195e-07
Iter: 637 loss: 3.66655684e-07
Iter: 638 loss: 3.66518037e-07
Iter: 639 loss: 3.66206024e-07
Iter: 640 loss: 3.66834342e-07
Iter: 641 loss: 3.66082105e-07
Iter: 642 loss: 3.65851264e-07
Iter: 643 loss: 3.65650322e-07
Iter: 644 loss: 3.65586573e-07
Iter: 645 loss: 3.65336689e-07
Iter: 646 loss: 3.65326798e-07
Iter: 647 loss: 3.65113578e-07
Iter: 648 loss: 3.66016508e-07
Iter: 649 loss: 3.65065489e-07
Iter: 650 loss: 3.64938217e-07
Iter: 651 loss: 3.6452127e-07
Iter: 652 loss: 3.65699123e-07
Iter: 653 loss: 3.64337239e-07
Iter: 654 loss: 3.64136383e-07
Iter: 655 loss: 3.63993706e-07
Iter: 656 loss: 3.63685444e-07
Iter: 657 loss: 3.63924784e-07
Iter: 658 loss: 3.63487146e-07
Iter: 659 loss: 3.63218248e-07
Iter: 660 loss: 3.63148928e-07
Iter: 661 loss: 3.62995365e-07
Iter: 662 loss: 3.62551276e-07
Iter: 663 loss: 3.66981283e-07
Iter: 664 loss: 3.62533e-07
Iter: 665 loss: 3.6234826e-07
Iter: 666 loss: 3.62054863e-07
Iter: 667 loss: 3.62048212e-07
Iter: 668 loss: 3.61740121e-07
Iter: 669 loss: 3.63902558e-07
Iter: 670 loss: 3.61719e-07
Iter: 671 loss: 3.61465908e-07
Iter: 672 loss: 3.6299528e-07
Iter: 673 loss: 3.61422053e-07
Iter: 674 loss: 3.61259879e-07
Iter: 675 loss: 3.61016191e-07
Iter: 676 loss: 3.61005334e-07
Iter: 677 loss: 3.60585432e-07
Iter: 678 loss: 3.60899207e-07
Iter: 679 loss: 3.60321707e-07
Iter: 680 loss: 3.60026945e-07
Iter: 681 loss: 3.5997823e-07
Iter: 682 loss: 3.59802243e-07
Iter: 683 loss: 3.59415367e-07
Iter: 684 loss: 3.6506313e-07
Iter: 685 loss: 3.59404112e-07
Iter: 686 loss: 3.58990803e-07
Iter: 687 loss: 3.59969789e-07
Iter: 688 loss: 3.58830846e-07
Iter: 689 loss: 3.58680893e-07
Iter: 690 loss: 3.5859884e-07
Iter: 691 loss: 3.58495583e-07
Iter: 692 loss: 3.58283273e-07
Iter: 693 loss: 3.60964862e-07
Iter: 694 loss: 3.58278868e-07
Iter: 695 loss: 3.58027364e-07
Iter: 696 loss: 3.61050525e-07
Iter: 697 loss: 3.58027194e-07
Iter: 698 loss: 3.57778106e-07
Iter: 699 loss: 3.57394129e-07
Iter: 700 loss: 3.573995e-07
Iter: 701 loss: 3.57052528e-07
Iter: 702 loss: 3.58487313e-07
Iter: 703 loss: 3.56983833e-07
Iter: 704 loss: 3.56667215e-07
Iter: 705 loss: 3.60840147e-07
Iter: 706 loss: 3.56665481e-07
Iter: 707 loss: 3.56432281e-07
Iter: 708 loss: 3.5629489e-07
Iter: 709 loss: 3.56202804e-07
Iter: 710 loss: 3.55955649e-07
Iter: 711 loss: 3.56442882e-07
Iter: 712 loss: 3.55858049e-07
Iter: 713 loss: 3.55687234e-07
Iter: 714 loss: 3.55679163e-07
Iter: 715 loss: 3.55546149e-07
Iter: 716 loss: 3.55458e-07
Iter: 717 loss: 3.55402619e-07
Iter: 718 loss: 3.55250165e-07
Iter: 719 loss: 3.55070227e-07
Iter: 720 loss: 3.55032256e-07
Iter: 721 loss: 3.54883724e-07
Iter: 722 loss: 3.54863914e-07
Iter: 723 loss: 3.54690599e-07
Iter: 724 loss: 3.54366222e-07
Iter: 725 loss: 3.61601906e-07
Iter: 726 loss: 3.54377391e-07
Iter: 727 loss: 3.54055913e-07
Iter: 728 loss: 3.55600207e-07
Iter: 729 loss: 3.54002765e-07
Iter: 730 loss: 3.53562456e-07
Iter: 731 loss: 3.53849828e-07
Iter: 732 loss: 3.53291711e-07
Iter: 733 loss: 3.53033215e-07
Iter: 734 loss: 3.53323571e-07
Iter: 735 loss: 3.52874622e-07
Iter: 736 loss: 3.52678143e-07
Iter: 737 loss: 3.5268971e-07
Iter: 738 loss: 3.52499228e-07
Iter: 739 loss: 3.52486609e-07
Iter: 740 loss: 3.52356551e-07
Iter: 741 loss: 3.52167319e-07
Iter: 742 loss: 3.52090922e-07
Iter: 743 loss: 3.51993094e-07
Iter: 744 loss: 3.51735196e-07
Iter: 745 loss: 3.53577e-07
Iter: 746 loss: 3.51693188e-07
Iter: 747 loss: 3.51397119e-07
Iter: 748 loss: 3.52515542e-07
Iter: 749 loss: 3.51314014e-07
Iter: 750 loss: 3.51149367e-07
Iter: 751 loss: 3.50942e-07
Iter: 752 loss: 3.50898631e-07
Iter: 753 loss: 3.50591677e-07
Iter: 754 loss: 3.52260656e-07
Iter: 755 loss: 3.50554927e-07
Iter: 756 loss: 3.50256272e-07
Iter: 757 loss: 3.5266541e-07
Iter: 758 loss: 3.50225264e-07
Iter: 759 loss: 3.50124026e-07
Iter: 760 loss: 3.49987886e-07
Iter: 761 loss: 3.49986863e-07
Iter: 762 loss: 3.4976307e-07
Iter: 763 loss: 3.51501541e-07
Iter: 764 loss: 3.49740048e-07
Iter: 765 loss: 3.49565141e-07
Iter: 766 loss: 3.49266656e-07
Iter: 767 loss: 3.49255e-07
Iter: 768 loss: 3.48896549e-07
Iter: 769 loss: 3.49486413e-07
Iter: 770 loss: 3.48717663e-07
Iter: 771 loss: 3.48287017e-07
Iter: 772 loss: 3.53596874e-07
Iter: 773 loss: 3.48265189e-07
Iter: 774 loss: 3.48088406e-07
Iter: 775 loss: 3.47783725e-07
Iter: 776 loss: 3.47773152e-07
Iter: 777 loss: 3.47452215e-07
Iter: 778 loss: 3.49350159e-07
Iter: 779 loss: 3.47420723e-07
Iter: 780 loss: 3.47297913e-07
Iter: 781 loss: 3.47270031e-07
Iter: 782 loss: 3.47207532e-07
Iter: 783 loss: 3.47037656e-07
Iter: 784 loss: 3.48647802e-07
Iter: 785 loss: 3.46999428e-07
Iter: 786 loss: 3.46779444e-07
Iter: 787 loss: 3.47183402e-07
Iter: 788 loss: 3.46681617e-07
Iter: 789 loss: 3.46491333e-07
Iter: 790 loss: 3.46475503e-07
Iter: 791 loss: 3.46356615e-07
Iter: 792 loss: 3.4607433e-07
Iter: 793 loss: 3.49307555e-07
Iter: 794 loss: 3.46036956e-07
Iter: 795 loss: 3.4581177e-07
Iter: 796 loss: 3.45800345e-07
Iter: 797 loss: 3.45573113e-07
Iter: 798 loss: 3.45465736e-07
Iter: 799 loss: 3.45356966e-07
Iter: 800 loss: 3.45155513e-07
Iter: 801 loss: 3.4500826e-07
Iter: 802 loss: 3.4494542e-07
Iter: 803 loss: 3.44961279e-07
Iter: 804 loss: 3.44814964e-07
Iter: 805 loss: 3.44729585e-07
Iter: 806 loss: 3.44648271e-07
Iter: 807 loss: 3.44621071e-07
Iter: 808 loss: 3.44504798e-07
Iter: 809 loss: 3.44352827e-07
Iter: 810 loss: 3.44336229e-07
Iter: 811 loss: 3.44114824e-07
Iter: 812 loss: 3.46215643e-07
Iter: 813 loss: 3.44097089e-07
Iter: 814 loss: 3.43927411e-07
Iter: 815 loss: 3.43783682e-07
Iter: 816 loss: 3.43736303e-07
Iter: 817 loss: 3.4351433e-07
Iter: 818 loss: 3.43803094e-07
Iter: 819 loss: 3.43396209e-07
Iter: 820 loss: 3.43205187e-07
Iter: 821 loss: 3.43194955e-07
Iter: 822 loss: 3.43085929e-07
Iter: 823 loss: 3.43196405e-07
Iter: 824 loss: 3.4299822e-07
Iter: 825 loss: 3.42863245e-07
Iter: 826 loss: 3.42824649e-07
Iter: 827 loss: 3.42738844e-07
Iter: 828 loss: 3.42564363e-07
Iter: 829 loss: 3.42572264e-07
Iter: 830 loss: 3.4249598e-07
Iter: 831 loss: 3.42293106e-07
Iter: 832 loss: 3.43532434e-07
Iter: 833 loss: 3.42246693e-07
Iter: 834 loss: 3.42009116e-07
Iter: 835 loss: 3.45015565e-07
Iter: 836 loss: 3.42002664e-07
Iter: 837 loss: 3.41799137e-07
Iter: 838 loss: 3.42762519e-07
Iter: 839 loss: 3.41772079e-07
Iter: 840 loss: 3.41652907e-07
Iter: 841 loss: 3.41521115e-07
Iter: 842 loss: 3.41514863e-07
Iter: 843 loss: 3.41381764e-07
Iter: 844 loss: 3.41361897e-07
Iter: 845 loss: 3.41261057e-07
Iter: 846 loss: 3.41197e-07
Iter: 847 loss: 3.41144016e-07
Iter: 848 loss: 3.41024531e-07
Iter: 849 loss: 3.41242554e-07
Iter: 850 loss: 3.40962345e-07
Iter: 851 loss: 3.40851329e-07
Iter: 852 loss: 3.41166981e-07
Iter: 853 loss: 3.40805911e-07
Iter: 854 loss: 3.40652093e-07
Iter: 855 loss: 3.41016062e-07
Iter: 856 loss: 3.40579021e-07
Iter: 857 loss: 3.4042472e-07
Iter: 858 loss: 3.40263767e-07
Iter: 859 loss: 3.40230457e-07
Iter: 860 loss: 3.40012406e-07
Iter: 861 loss: 3.40019056e-07
Iter: 862 loss: 3.39825931e-07
Iter: 863 loss: 3.3955223e-07
Iter: 864 loss: 3.39539667e-07
Iter: 865 loss: 3.39352312e-07
Iter: 866 loss: 3.40235033e-07
Iter: 867 loss: 3.39278898e-07
Iter: 868 loss: 3.39138694e-07
Iter: 869 loss: 3.39139547e-07
Iter: 870 loss: 3.3905485e-07
Iter: 871 loss: 3.39007016e-07
Iter: 872 loss: 3.38973848e-07
Iter: 873 loss: 3.38891567e-07
Iter: 874 loss: 3.39442238e-07
Iter: 875 loss: 3.3888216e-07
Iter: 876 loss: 3.38756763e-07
Iter: 877 loss: 3.38771656e-07
Iter: 878 loss: 3.38672095e-07
Iter: 879 loss: 3.38551274e-07
Iter: 880 loss: 3.3877086e-07
Iter: 881 loss: 3.3849733e-07
Iter: 882 loss: 3.38376253e-07
Iter: 883 loss: 3.38513615e-07
Iter: 884 loss: 3.38298662e-07
Iter: 885 loss: 3.38092207e-07
Iter: 886 loss: 3.38473512e-07
Iter: 887 loss: 3.38004554e-07
Iter: 888 loss: 3.37786162e-07
Iter: 889 loss: 3.37821234e-07
Iter: 890 loss: 3.37618758e-07
Iter: 891 loss: 3.37424609e-07
Iter: 892 loss: 3.3741793e-07
Iter: 893 loss: 3.37241602e-07
Iter: 894 loss: 3.37091137e-07
Iter: 895 loss: 3.37040831e-07
Iter: 896 loss: 3.36875075e-07
Iter: 897 loss: 3.37259394e-07
Iter: 898 loss: 3.36810757e-07
Iter: 899 loss: 3.36735866e-07
Iter: 900 loss: 3.36732342e-07
Iter: 901 loss: 3.36636617e-07
Iter: 902 loss: 3.36466258e-07
Iter: 903 loss: 3.40113843e-07
Iter: 904 loss: 3.36456708e-07
Iter: 905 loss: 3.36294789e-07
Iter: 906 loss: 3.37281e-07
Iter: 907 loss: 3.36280578e-07
Iter: 908 loss: 3.36102602e-07
Iter: 909 loss: 3.3701491e-07
Iter: 910 loss: 3.36064318e-07
Iter: 911 loss: 3.35923744e-07
Iter: 912 loss: 3.35895606e-07
Iter: 913 loss: 3.35815088e-07
Iter: 914 loss: 3.35657376e-07
Iter: 915 loss: 3.36787309e-07
Iter: 916 loss: 3.35632365e-07
Iter: 917 loss: 3.35497646e-07
Iter: 918 loss: 3.36013443e-07
Iter: 919 loss: 3.35472663e-07
Iter: 920 loss: 3.35358493e-07
Iter: 921 loss: 3.3525248e-07
Iter: 922 loss: 3.35216669e-07
Iter: 923 loss: 3.35099344e-07
Iter: 924 loss: 3.36827497e-07
Iter: 925 loss: 3.35099287e-07
Iter: 926 loss: 3.34978637e-07
Iter: 927 loss: 3.35205584e-07
Iter: 928 loss: 3.3494058e-07
Iter: 929 loss: 3.3484082e-07
Iter: 930 loss: 3.34611343e-07
Iter: 931 loss: 3.37947284e-07
Iter: 932 loss: 3.34596535e-07
Iter: 933 loss: 3.34535741e-07
Iter: 934 loss: 3.34480973e-07
Iter: 935 loss: 3.343564e-07
Iter: 936 loss: 3.34189281e-07
Iter: 937 loss: 3.34182971e-07
Iter: 938 loss: 3.3398581e-07
Iter: 939 loss: 3.34698512e-07
Iter: 940 loss: 3.3394187e-07
Iter: 941 loss: 3.33856804e-07
Iter: 942 loss: 3.33846174e-07
Iter: 943 loss: 3.33786886e-07
Iter: 944 loss: 3.33700086e-07
Iter: 945 loss: 3.3369983e-07
Iter: 946 loss: 3.33588559e-07
Iter: 947 loss: 3.34221056e-07
Iter: 948 loss: 3.33573723e-07
Iter: 949 loss: 3.33488913e-07
Iter: 950 loss: 3.33760426e-07
Iter: 951 loss: 3.33474787e-07
Iter: 952 loss: 3.33381763e-07
Iter: 953 loss: 3.33302722e-07
Iter: 954 loss: 3.33290018e-07
Iter: 955 loss: 3.33121648e-07
Iter: 956 loss: 3.33404728e-07
Iter: 957 loss: 3.33049e-07
Iter: 958 loss: 3.32841466e-07
Iter: 959 loss: 3.34692231e-07
Iter: 960 loss: 3.32829302e-07
Iter: 961 loss: 3.3271175e-07
Iter: 962 loss: 3.32416903e-07
Iter: 963 loss: 3.35394702e-07
Iter: 964 loss: 3.32385554e-07
Iter: 965 loss: 3.32191291e-07
Iter: 966 loss: 3.32173613e-07
Iter: 967 loss: 3.32010046e-07
Iter: 968 loss: 3.33017283e-07
Iter: 969 loss: 3.31983983e-07
Iter: 970 loss: 3.31910542e-07
Iter: 971 loss: 3.31829938e-07
Iter: 972 loss: 3.31806575e-07
Iter: 973 loss: 3.31739443e-07
Iter: 974 loss: 3.31738221e-07
Iter: 975 loss: 3.31667025e-07
Iter: 976 loss: 3.31611488e-07
Iter: 977 loss: 3.31601456e-07
Iter: 978 loss: 3.31462616e-07
Iter: 979 loss: 3.31724181e-07
Iter: 980 loss: 3.31436297e-07
Iter: 981 loss: 3.31280148e-07
Iter: 982 loss: 3.31918102e-07
Iter: 983 loss: 3.31245587e-07
Iter: 984 loss: 3.31092679e-07
Iter: 985 loss: 3.31156286e-07
Iter: 986 loss: 3.30966657e-07
Iter: 987 loss: 3.30775322e-07
Iter: 988 loss: 3.30829977e-07
Iter: 989 loss: 3.30629348e-07
Iter: 990 loss: 3.30384239e-07
Iter: 991 loss: 3.304026e-07
Iter: 992 loss: 3.3025384e-07
Iter: 993 loss: 3.30089222e-07
Iter: 994 loss: 3.30081434e-07
Iter: 995 loss: 3.29926877e-07
Iter: 996 loss: 3.30702306e-07
Iter: 997 loss: 3.29903202e-07
Iter: 998 loss: 3.29768284e-07
Iter: 999 loss: 3.31053343e-07
Iter: 1000 loss: 3.29776242e-07
Iter: 1001 loss: 3.29714055e-07
Iter: 1002 loss: 3.29625522e-07
Iter: 1003 loss: 3.2962771e-07
Iter: 1004 loss: 3.29526756e-07
Iter: 1005 loss: 3.29527751e-07
Iter: 1006 loss: 3.2941756e-07
Iter: 1007 loss: 3.29282727e-07
Iter: 1008 loss: 3.2926846e-07
Iter: 1009 loss: 3.29125953e-07
Iter: 1010 loss: 3.3020649e-07
Iter: 1011 loss: 3.29112311e-07
Iter: 1012 loss: 3.29002603e-07
Iter: 1013 loss: 3.29727499e-07
Iter: 1014 loss: 3.28982452e-07
Iter: 1015 loss: 3.28877604e-07
Iter: 1016 loss: 3.28878173e-07
Iter: 1017 loss: 3.28788531e-07
Iter: 1018 loss: 3.28650344e-07
Iter: 1019 loss: 3.28954371e-07
Iter: 1020 loss: 3.28609417e-07
Iter: 1021 loss: 3.28520514e-07
Iter: 1022 loss: 3.28529666e-07
Iter: 1023 loss: 3.28439967e-07
Iter: 1024 loss: 3.2829962e-07
Iter: 1025 loss: 3.30909302e-07
Iter: 1026 loss: 3.28313e-07
Iter: 1027 loss: 3.28153192e-07
Iter: 1028 loss: 3.2858361e-07
Iter: 1029 loss: 3.28120848e-07
Iter: 1030 loss: 3.27977148e-07
Iter: 1031 loss: 3.29751231e-07
Iter: 1032 loss: 3.27981979e-07
Iter: 1033 loss: 3.27870964e-07
Iter: 1034 loss: 3.27677299e-07
Iter: 1035 loss: 3.27681704e-07
Iter: 1036 loss: 3.27593284e-07
Iter: 1037 loss: 3.27590243e-07
Iter: 1038 loss: 3.27492245e-07
Iter: 1039 loss: 3.2744893e-07
Iter: 1040 loss: 3.27393764e-07
Iter: 1041 loss: 3.27301478e-07
Iter: 1042 loss: 3.27535e-07
Iter: 1043 loss: 3.27256714e-07
Iter: 1044 loss: 3.27175428e-07
Iter: 1045 loss: 3.2820418e-07
Iter: 1046 loss: 3.27167356e-07
Iter: 1047 loss: 3.27130834e-07
Iter: 1048 loss: 3.27079192e-07
Iter: 1049 loss: 3.27051708e-07
Iter: 1050 loss: 3.26937936e-07
Iter: 1051 loss: 3.26825727e-07
Iter: 1052 loss: 3.26791849e-07
Iter: 1053 loss: 3.26681743e-07
Iter: 1054 loss: 3.26664519e-07
Iter: 1055 loss: 3.26552396e-07
Iter: 1056 loss: 3.26344406e-07
Iter: 1057 loss: 3.26342615e-07
Iter: 1058 loss: 3.26084205e-07
Iter: 1059 loss: 3.26313852e-07
Iter: 1060 loss: 3.25924589e-07
Iter: 1061 loss: 3.25865614e-07
Iter: 1062 loss: 3.25812294e-07
Iter: 1063 loss: 3.25707532e-07
Iter: 1064 loss: 3.25554709e-07
Iter: 1065 loss: 3.29435409e-07
Iter: 1066 loss: 3.25561871e-07
Iter: 1067 loss: 3.25456256e-07
Iter: 1068 loss: 3.25451879e-07
Iter: 1069 loss: 3.25381961e-07
Iter: 1070 loss: 3.25791831e-07
Iter: 1071 loss: 3.25359451e-07
Iter: 1072 loss: 3.25299084e-07
Iter: 1073 loss: 3.2520353e-07
Iter: 1074 loss: 3.26951294e-07
Iter: 1075 loss: 3.25194918e-07
Iter: 1076 loss: 3.25100814e-07
Iter: 1077 loss: 3.26756322e-07
Iter: 1078 loss: 3.25095954e-07
Iter: 1079 loss: 3.25014497e-07
Iter: 1080 loss: 3.25070488e-07
Iter: 1081 loss: 3.24963196e-07
Iter: 1082 loss: 3.24844393e-07
Iter: 1083 loss: 3.24734685e-07
Iter: 1084 loss: 3.24701801e-07
Iter: 1085 loss: 3.24552758e-07
Iter: 1086 loss: 3.26942427e-07
Iter: 1087 loss: 3.24544516e-07
Iter: 1088 loss: 3.24410792e-07
Iter: 1089 loss: 3.24449928e-07
Iter: 1090 loss: 3.24323167e-07
Iter: 1091 loss: 3.24171253e-07
Iter: 1092 loss: 3.2415295e-07
Iter: 1093 loss: 3.24064445e-07
Iter: 1094 loss: 3.24039149e-07
Iter: 1095 loss: 3.23992197e-07
Iter: 1096 loss: 3.23942913e-07
Iter: 1097 loss: 3.23856824e-07
Iter: 1098 loss: 3.23856881e-07
Iter: 1099 loss: 3.23760389e-07
Iter: 1100 loss: 3.23878055e-07
Iter: 1101 loss: 3.23717131e-07
Iter: 1102 loss: 3.23608674e-07
Iter: 1103 loss: 3.24995597e-07
Iter: 1104 loss: 3.23610664e-07
Iter: 1105 loss: 3.23538302e-07
Iter: 1106 loss: 3.23391703e-07
Iter: 1107 loss: 3.25068413e-07
Iter: 1108 loss: 3.23385564e-07
Iter: 1109 loss: 3.23234588e-07
Iter: 1110 loss: 3.25397025e-07
Iter: 1111 loss: 3.23243654e-07
Iter: 1112 loss: 3.23144e-07
Iter: 1113 loss: 3.23189624e-07
Iter: 1114 loss: 3.23080883e-07
Iter: 1115 loss: 3.22972681e-07
Iter: 1116 loss: 3.23152733e-07
Iter: 1117 loss: 3.22923938e-07
Iter: 1118 loss: 3.22824178e-07
Iter: 1119 loss: 3.23383802e-07
Iter: 1120 loss: 3.22813435e-07
Iter: 1121 loss: 3.2272024e-07
Iter: 1122 loss: 3.23021453e-07
Iter: 1123 loss: 3.22694348e-07
Iter: 1124 loss: 3.22621588e-07
Iter: 1125 loss: 3.22500256e-07
Iter: 1126 loss: 3.22499659e-07
Iter: 1127 loss: 3.22367583e-07
Iter: 1128 loss: 3.22368237e-07
Iter: 1129 loss: 3.22245569e-07
Iter: 1130 loss: 3.22300139e-07
Iter: 1131 loss: 3.22174316e-07
Iter: 1132 loss: 3.2203593e-07
Iter: 1133 loss: 3.21936227e-07
Iter: 1134 loss: 3.2190141e-07
Iter: 1135 loss: 3.21777236e-07
Iter: 1136 loss: 3.21757682e-07
Iter: 1137 loss: 3.21695637e-07
Iter: 1138 loss: 3.21618415e-07
Iter: 1139 loss: 3.21602e-07
Iter: 1140 loss: 3.21522606e-07
Iter: 1141 loss: 3.22401377e-07
Iter: 1142 loss: 3.21525135e-07
Iter: 1143 loss: 3.2146508e-07
Iter: 1144 loss: 3.21519138e-07
Iter: 1145 loss: 3.21423983e-07
Iter: 1146 loss: 3.21369669e-07
Iter: 1147 loss: 3.21438506e-07
Iter: 1148 loss: 3.21337296e-07
Iter: 1149 loss: 3.21259e-07
Iter: 1150 loss: 3.21229209e-07
Iter: 1151 loss: 3.21200531e-07
Iter: 1152 loss: 3.21050948e-07
Iter: 1153 loss: 3.21689299e-07
Iter: 1154 loss: 3.21019854e-07
Iter: 1155 loss: 3.20923419e-07
Iter: 1156 loss: 3.20796346e-07
Iter: 1157 loss: 3.20785915e-07
Iter: 1158 loss: 3.20639856e-07
Iter: 1159 loss: 3.22039284e-07
Iter: 1160 loss: 3.20622661e-07
Iter: 1161 loss: 3.20468928e-07
Iter: 1162 loss: 3.20901194e-07
Iter: 1163 loss: 3.20411516e-07
Iter: 1164 loss: 3.20310335e-07
Iter: 1165 loss: 3.20399778e-07
Iter: 1166 loss: 3.2027134e-07
Iter: 1167 loss: 3.20198637e-07
Iter: 1168 loss: 3.20202588e-07
Iter: 1169 loss: 3.2014313e-07
Iter: 1170 loss: 3.20054198e-07
Iter: 1171 loss: 3.20066903e-07
Iter: 1172 loss: 3.1999457e-07
Iter: 1173 loss: 3.21017495e-07
Iter: 1174 loss: 3.19996843e-07
Iter: 1175 loss: 3.19937726e-07
Iter: 1176 loss: 3.19905865e-07
Iter: 1177 loss: 3.1986616e-07
Iter: 1178 loss: 3.19775211e-07
Iter: 1179 loss: 3.19826228e-07
Iter: 1180 loss: 3.19697619e-07
Iter: 1181 loss: 3.19558126e-07
Iter: 1182 loss: 3.20117806e-07
Iter: 1183 loss: 3.1952041e-07
Iter: 1184 loss: 3.19390097e-07
Iter: 1185 loss: 3.2020813e-07
Iter: 1186 loss: 3.19363892e-07
Iter: 1187 loss: 3.19260266e-07
Iter: 1188 loss: 3.19096046e-07
Iter: 1189 loss: 3.19079277e-07
Iter: 1190 loss: 3.18962634e-07
Iter: 1191 loss: 3.20480126e-07
Iter: 1192 loss: 3.18948224e-07
Iter: 1193 loss: 3.18846219e-07
Iter: 1194 loss: 3.19631738e-07
Iter: 1195 loss: 3.18832974e-07
Iter: 1196 loss: 3.18743531e-07
Iter: 1197 loss: 3.18661762e-07
Iter: 1198 loss: 3.18652155e-07
Iter: 1199 loss: 3.18605544e-07
Iter: 1200 loss: 3.18587638e-07
Iter: 1201 loss: 3.18527384e-07
Iter: 1202 loss: 3.18405512e-07
Iter: 1203 loss: 3.20362e-07
Iter: 1204 loss: 3.1839798e-07
Iter: 1205 loss: 3.18272839e-07
Iter: 1206 loss: 3.18970592e-07
Iter: 1207 loss: 3.18260334e-07
Iter: 1208 loss: 3.18126808e-07
Iter: 1209 loss: 3.18355035e-07
Iter: 1210 loss: 3.18082471e-07
Iter: 1211 loss: 3.17977822e-07
Iter: 1212 loss: 3.1815614e-07
Iter: 1213 loss: 3.17932432e-07
Iter: 1214 loss: 3.17808599e-07
Iter: 1215 loss: 3.17885309e-07
Iter: 1216 loss: 3.17748459e-07
Iter: 1217 loss: 3.17662824e-07
Iter: 1218 loss: 3.18981137e-07
Iter: 1219 loss: 3.17654155e-07
Iter: 1220 loss: 3.17587563e-07
Iter: 1221 loss: 3.17526599e-07
Iter: 1222 loss: 3.17521256e-07
Iter: 1223 loss: 3.17409103e-07
Iter: 1224 loss: 3.17427435e-07
Iter: 1225 loss: 3.17323554e-07
Iter: 1226 loss: 3.17216944e-07
Iter: 1227 loss: 3.17211942e-07
Iter: 1228 loss: 3.17145464e-07
Iter: 1229 loss: 3.16965753e-07
Iter: 1230 loss: 3.18913749e-07
Iter: 1231 loss: 3.16947961e-07
Iter: 1232 loss: 3.1681239e-07
Iter: 1233 loss: 3.1682157e-07
Iter: 1234 loss: 3.16692422e-07
Iter: 1235 loss: 3.16726045e-07
Iter: 1236 loss: 3.16594196e-07
Iter: 1237 loss: 3.1653542e-07
Iter: 1238 loss: 3.17152598e-07
Iter: 1239 loss: 3.16524392e-07
Iter: 1240 loss: 3.16459591e-07
Iter: 1241 loss: 3.16453935e-07
Iter: 1242 loss: 3.16385e-07
Iter: 1243 loss: 3.16322371e-07
Iter: 1244 loss: 3.16683384e-07
Iter: 1245 loss: 3.16309809e-07
Iter: 1246 loss: 3.16252908e-07
Iter: 1247 loss: 3.16277692e-07
Iter: 1248 loss: 3.16221019e-07
Iter: 1249 loss: 3.16143741e-07
Iter: 1250 loss: 3.16113756e-07
Iter: 1251 loss: 3.16081383e-07
Iter: 1252 loss: 3.15898319e-07
Iter: 1253 loss: 3.16206268e-07
Iter: 1254 loss: 3.15823684e-07
Iter: 1255 loss: 3.15657473e-07
Iter: 1256 loss: 3.1555328e-07
Iter: 1257 loss: 3.15484954e-07
Iter: 1258 loss: 3.15477394e-07
Iter: 1259 loss: 3.15402076e-07
Iter: 1260 loss: 3.15340657e-07
Iter: 1261 loss: 3.15274406e-07
Iter: 1262 loss: 3.15247064e-07
Iter: 1263 loss: 3.15178454e-07
Iter: 1264 loss: 3.15525199e-07
Iter: 1265 loss: 3.15171121e-07
Iter: 1266 loss: 3.15075397e-07
Iter: 1267 loss: 3.15182689e-07
Iter: 1268 loss: 3.15030405e-07
Iter: 1269 loss: 3.14958584e-07
Iter: 1270 loss: 3.15181637e-07
Iter: 1271 loss: 3.14943378e-07
Iter: 1272 loss: 3.14854674e-07
Iter: 1273 loss: 3.14780095e-07
Iter: 1274 loss: 3.1476236e-07
Iter: 1275 loss: 3.14618546e-07
Iter: 1276 loss: 3.15218983e-07
Iter: 1277 loss: 3.14608599e-07
Iter: 1278 loss: 3.14497015e-07
Iter: 1279 loss: 3.15194427e-07
Iter: 1280 loss: 3.14478314e-07
Iter: 1281 loss: 3.14392082e-07
Iter: 1282 loss: 3.14323728e-07
Iter: 1283 loss: 3.14315685e-07
Iter: 1284 loss: 3.14198644e-07
Iter: 1285 loss: 3.15611942e-07
Iter: 1286 loss: 3.14206687e-07
Iter: 1287 loss: 3.14131341e-07
Iter: 1288 loss: 3.14091238e-07
Iter: 1289 loss: 3.14062333e-07
Iter: 1290 loss: 3.13995315e-07
Iter: 1291 loss: 3.13992103e-07
Iter: 1292 loss: 3.1393003e-07
Iter: 1293 loss: 3.13814212e-07
Iter: 1294 loss: 3.13815661e-07
Iter: 1295 loss: 3.13703481e-07
Iter: 1296 loss: 3.14267254e-07
Iter: 1297 loss: 3.13685433e-07
Iter: 1298 loss: 3.13559781e-07
Iter: 1299 loss: 3.1412344e-07
Iter: 1300 loss: 3.13535907e-07
Iter: 1301 loss: 3.13438676e-07
Iter: 1302 loss: 3.13401927e-07
Iter: 1303 loss: 3.13357418e-07
Iter: 1304 loss: 3.13229293e-07
Iter: 1305 loss: 3.14706142e-07
Iter: 1306 loss: 3.13224291e-07
Iter: 1307 loss: 3.13163213e-07
Iter: 1308 loss: 3.13065e-07
Iter: 1309 loss: 3.13063339e-07
Iter: 1310 loss: 3.12981399e-07
Iter: 1311 loss: 3.12988618e-07
Iter: 1312 loss: 3.12935725e-07
Iter: 1313 loss: 3.12947236e-07
Iter: 1314 loss: 3.12886641e-07
Iter: 1315 loss: 3.12821214e-07
Iter: 1316 loss: 3.12881127e-07
Iter: 1317 loss: 3.12785232e-07
Iter: 1318 loss: 3.12689622e-07
Iter: 1319 loss: 3.1278438e-07
Iter: 1320 loss: 3.1261996e-07
Iter: 1321 loss: 3.12513976e-07
Iter: 1322 loss: 3.13183193e-07
Iter: 1323 loss: 3.12512071e-07
Iter: 1324 loss: 3.12387215e-07
Iter: 1325 loss: 3.12378404e-07
Iter: 1326 loss: 3.12291206e-07
Iter: 1327 loss: 3.1214384e-07
Iter: 1328 loss: 3.12351062e-07
Iter: 1329 loss: 3.12077162e-07
Iter: 1330 loss: 3.11912743e-07
Iter: 1331 loss: 3.13294777e-07
Iter: 1332 loss: 3.11886481e-07
Iter: 1333 loss: 3.11798431e-07
Iter: 1334 loss: 3.11736272e-07
Iter: 1335 loss: 3.1169418e-07
Iter: 1336 loss: 3.1161747e-07
Iter: 1337 loss: 3.11620227e-07
Iter: 1338 loss: 3.11563213e-07
Iter: 1339 loss: 3.11475816e-07
Iter: 1340 loss: 3.11471638e-07
Iter: 1341 loss: 3.11381655e-07
Iter: 1342 loss: 3.12035155e-07
Iter: 1343 loss: 3.11370854e-07
Iter: 1344 loss: 3.11304518e-07
Iter: 1345 loss: 3.1142352e-07
Iter: 1346 loss: 3.11254382e-07
Iter: 1347 loss: 3.1119697e-07
Iter: 1348 loss: 3.11297839e-07
Iter: 1349 loss: 3.11129668e-07
Iter: 1350 loss: 3.11024536e-07
Iter: 1351 loss: 3.11046051e-07
Iter: 1352 loss: 3.1092975e-07
Iter: 1353 loss: 3.10797475e-07
Iter: 1354 loss: 3.12097427e-07
Iter: 1355 loss: 3.10795258e-07
Iter: 1356 loss: 3.10693963e-07
Iter: 1357 loss: 3.11229286e-07
Iter: 1358 loss: 3.10671908e-07
Iter: 1359 loss: 3.10608129e-07
Iter: 1360 loss: 3.10575615e-07
Iter: 1361 loss: 3.1054671e-07
Iter: 1362 loss: 3.10473581e-07
Iter: 1363 loss: 3.10465111e-07
Iter: 1364 loss: 3.10400907e-07
Iter: 1365 loss: 3.10318228e-07
Iter: 1366 loss: 3.10306888e-07
Iter: 1367 loss: 3.10235635e-07
Iter: 1368 loss: 3.1130088e-07
Iter: 1369 loss: 3.10236061e-07
Iter: 1370 loss: 3.10127973e-07
Iter: 1371 loss: 3.09999194e-07
Iter: 1372 loss: 3.0999098e-07
Iter: 1373 loss: 3.09862287e-07
Iter: 1374 loss: 3.10891494e-07
Iter: 1375 loss: 3.09862685e-07
Iter: 1376 loss: 3.09745559e-07
Iter: 1377 loss: 3.10040235e-07
Iter: 1378 loss: 3.096863e-07
Iter: 1379 loss: 3.09597311e-07
Iter: 1380 loss: 3.09790323e-07
Iter: 1381 loss: 3.09545328e-07
Iter: 1382 loss: 3.09461655e-07
Iter: 1383 loss: 3.09711481e-07
Iter: 1384 loss: 3.09404129e-07
Iter: 1385 loss: 3.09342227e-07
Iter: 1386 loss: 3.09797e-07
Iter: 1387 loss: 3.09334808e-07
Iter: 1388 loss: 3.09254858e-07
Iter: 1389 loss: 3.09674022e-07
Iter: 1390 loss: 3.09270433e-07
Iter: 1391 loss: 3.09206655e-07
Iter: 1392 loss: 3.09123948e-07
Iter: 1393 loss: 3.09125483e-07
Iter: 1394 loss: 3.09038228e-07
Iter: 1395 loss: 3.10284946e-07
Iter: 1396 loss: 3.09038569e-07
Iter: 1397 loss: 3.08937047e-07
Iter: 1398 loss: 3.0884263e-07
Iter: 1399 loss: 3.08846097e-07
Iter: 1400 loss: 3.08694894e-07
Iter: 1401 loss: 3.09317784e-07
Iter: 1402 loss: 3.08686026e-07
Iter: 1403 loss: 3.08513791e-07
Iter: 1404 loss: 3.08434892e-07
Iter: 1405 loss: 3.08350593e-07
Iter: 1406 loss: 3.08185065e-07
Iter: 1407 loss: 3.08876963e-07
Iter: 1408 loss: 3.08155961e-07
Iter: 1409 loss: 3.08005696e-07
Iter: 1410 loss: 3.09015149e-07
Iter: 1411 loss: 3.08004758e-07
Iter: 1412 loss: 3.07885841e-07
Iter: 1413 loss: 3.08044719e-07
Iter: 1414 loss: 3.07836785e-07
Iter: 1415 loss: 3.07726282e-07
Iter: 1416 loss: 3.08062795e-07
Iter: 1417 loss: 3.07710081e-07
Iter: 1418 loss: 3.0763124e-07
Iter: 1419 loss: 3.07837468e-07
Iter: 1420 loss: 3.07600232e-07
Iter: 1421 loss: 3.07506895e-07
Iter: 1422 loss: 3.08132826e-07
Iter: 1423 loss: 3.07505303e-07
Iter: 1424 loss: 3.07428081e-07
Iter: 1425 loss: 3.07320306e-07
Iter: 1426 loss: 3.07315418e-07
Iter: 1427 loss: 3.07204118e-07
Iter: 1428 loss: 3.08714903e-07
Iter: 1429 loss: 3.0719886e-07
Iter: 1430 loss: 3.07084861e-07
Iter: 1431 loss: 3.07083383e-07
Iter: 1432 loss: 3.06996327e-07
Iter: 1433 loss: 3.06892815e-07
Iter: 1434 loss: 3.07220887e-07
Iter: 1435 loss: 3.06857856e-07
Iter: 1436 loss: 3.06728907e-07
Iter: 1437 loss: 3.07360381e-07
Iter: 1438 loss: 3.06698183e-07
Iter: 1439 loss: 3.06625424e-07
Iter: 1440 loss: 3.06545616e-07
Iter: 1441 loss: 3.0654968e-07
Iter: 1442 loss: 3.06437073e-07
Iter: 1443 loss: 3.06438807e-07
Iter: 1444 loss: 3.06371334e-07
Iter: 1445 loss: 3.06371163e-07
Iter: 1446 loss: 3.06328673e-07
Iter: 1447 loss: 3.06223512e-07
Iter: 1448 loss: 3.06291639e-07
Iter: 1449 loss: 3.06163599e-07
Iter: 1450 loss: 3.0604275e-07
Iter: 1451 loss: 3.06414336e-07
Iter: 1452 loss: 3.06009269e-07
Iter: 1453 loss: 3.05892115e-07
Iter: 1454 loss: 3.06904781e-07
Iter: 1455 loss: 3.05887568e-07
Iter: 1456 loss: 3.05773824e-07
Iter: 1457 loss: 3.05752479e-07
Iter: 1458 loss: 3.05694755e-07
Iter: 1459 loss: 3.05617618e-07
Iter: 1460 loss: 3.0635681e-07
Iter: 1461 loss: 3.05611252e-07
Iter: 1462 loss: 3.05534201e-07
Iter: 1463 loss: 3.05900585e-07
Iter: 1464 loss: 3.05512771e-07
Iter: 1465 loss: 3.05471247e-07
Iter: 1466 loss: 3.05428387e-07
Iter: 1467 loss: 3.05416165e-07
Iter: 1468 loss: 3.05332662e-07
Iter: 1469 loss: 3.06196966e-07
Iter: 1470 loss: 3.05330076e-07
Iter: 1471 loss: 3.05287358e-07
Iter: 1472 loss: 3.05186688e-07
Iter: 1473 loss: 3.06254663e-07
Iter: 1474 loss: 3.05170488e-07
Iter: 1475 loss: 3.05016215e-07
Iter: 1476 loss: 3.06031268e-07
Iter: 1477 loss: 3.05010957e-07
Iter: 1478 loss: 3.04867e-07
Iter: 1479 loss: 3.05329365e-07
Iter: 1480 loss: 3.04828461e-07
Iter: 1481 loss: 3.04705566e-07
Iter: 1482 loss: 3.04716536e-07
Iter: 1483 loss: 3.04600746e-07
Iter: 1484 loss: 3.04410065e-07
Iter: 1485 loss: 3.04837e-07
Iter: 1486 loss: 3.04353961e-07
Iter: 1487 loss: 3.04284072e-07
Iter: 1488 loss: 3.04267502e-07
Iter: 1489 loss: 3.04227825e-07
Iter: 1490 loss: 3.0420739e-07
Iter: 1491 loss: 3.04173938e-07
Iter: 1492 loss: 3.04116099e-07
Iter: 1493 loss: 3.04214524e-07
Iter: 1494 loss: 3.04069658e-07
Iter: 1495 loss: 3.04015373e-07
Iter: 1496 loss: 3.04014577e-07
Iter: 1497 loss: 3.03982176e-07
Iter: 1498 loss: 3.03864567e-07
Iter: 1499 loss: 3.05006921e-07
Iter: 1500 loss: 3.03870252e-07
Iter: 1501 loss: 3.03790841e-07
Iter: 1502 loss: 3.03773106e-07
Iter: 1503 loss: 3.03705718e-07
Iter: 1504 loss: 3.03550223e-07
Iter: 1505 loss: 3.05981018e-07
Iter: 1506 loss: 3.03551019e-07
Iter: 1507 loss: 3.03372872e-07
Iter: 1508 loss: 3.03430966e-07
Iter: 1509 loss: 3.032319e-07
Iter: 1510 loss: 3.03108664e-07
Iter: 1511 loss: 3.03085812e-07
Iter: 1512 loss: 3.02993271e-07
Iter: 1513 loss: 3.03178524e-07
Iter: 1514 loss: 3.029607e-07
Iter: 1515 loss: 3.02864e-07
Iter: 1516 loss: 3.02928385e-07
Iter: 1517 loss: 3.02835588e-07
Iter: 1518 loss: 3.02741626e-07
Iter: 1519 loss: 3.03033175e-07
Iter: 1520 loss: 3.02717041e-07
Iter: 1521 loss: 3.02657156e-07
Iter: 1522 loss: 3.0266537e-07
Iter: 1523 loss: 3.02611141e-07
Iter: 1524 loss: 3.02521784e-07
Iter: 1525 loss: 3.04049763e-07
Iter: 1526 loss: 3.0252653e-07
Iter: 1527 loss: 3.02435467e-07
Iter: 1528 loss: 3.02864e-07
Iter: 1529 loss: 3.02413241e-07
Iter: 1530 loss: 3.02319023e-07
Iter: 1531 loss: 3.03275442e-07
Iter: 1532 loss: 3.02313367e-07
Iter: 1533 loss: 3.02262919e-07
Iter: 1534 loss: 3.02164239e-07
Iter: 1535 loss: 3.02152955e-07
Iter: 1536 loss: 3.02081077e-07
Iter: 1537 loss: 3.02089489e-07
Iter: 1538 loss: 3.02013405e-07
Iter: 1539 loss: 3.01925411e-07
Iter: 1540 loss: 3.01923393e-07
Iter: 1541 loss: 3.01818261e-07
Iter: 1542 loss: 3.01982482e-07
Iter: 1543 loss: 3.0175454e-07
Iter: 1544 loss: 3.01700311e-07
Iter: 1545 loss: 3.01691415e-07
Iter: 1546 loss: 3.01628461e-07
Iter: 1547 loss: 3.01548027e-07
Iter: 1548 loss: 3.01543935e-07
Iter: 1549 loss: 3.01400235e-07
Iter: 1550 loss: 3.01464922e-07
Iter: 1551 loss: 3.01323581e-07
Iter: 1552 loss: 3.01191506e-07
Iter: 1553 loss: 3.01996067e-07
Iter: 1554 loss: 3.0117269e-07
Iter: 1555 loss: 3.01116074e-07
Iter: 1556 loss: 3.01967333e-07
Iter: 1557 loss: 3.01104933e-07
Iter: 1558 loss: 3.01057725e-07
Iter: 1559 loss: 3.01268869e-07
Iter: 1560 loss: 3.01040728e-07
Iter: 1561 loss: 3.00994202e-07
Iter: 1562 loss: 3.0092923e-07
Iter: 1563 loss: 3.00919453e-07
Iter: 1564 loss: 3.00862894e-07
Iter: 1565 loss: 3.01638465e-07
Iter: 1566 loss: 3.0085431e-07
Iter: 1567 loss: 3.0079849e-07
Iter: 1568 loss: 3.00910045e-07
Iter: 1569 loss: 3.00778396e-07
Iter: 1570 loss: 3.0072e-07
Iter: 1571 loss: 3.00626539e-07
Iter: 1572 loss: 3.0270337e-07
Iter: 1573 loss: 3.00636458e-07
Iter: 1574 loss: 3.00506514e-07
Iter: 1575 loss: 3.01191932e-07
Iter: 1576 loss: 3.00489603e-07
Iter: 1577 loss: 3.00365571e-07
Iter: 1578 loss: 3.01313491e-07
Iter: 1579 loss: 3.00355651e-07
Iter: 1580 loss: 3.00289429e-07
Iter: 1581 loss: 3.00192966e-07
Iter: 1582 loss: 3.00176055e-07
Iter: 1583 loss: 3.00147974e-07
Iter: 1584 loss: 3.00131092e-07
Iter: 1585 loss: 3.00096247e-07
Iter: 1586 loss: 3.00044121e-07
Iter: 1587 loss: 3.0144605e-07
Iter: 1588 loss: 3.00041563e-07
Iter: 1589 loss: 2.99979348e-07
Iter: 1590 loss: 3.00079677e-07
Iter: 1591 loss: 2.99941235e-07
Iter: 1592 loss: 2.99882e-07
Iter: 1593 loss: 3.00526494e-07
Iter: 1594 loss: 2.99895305e-07
Iter: 1595 loss: 2.99849091e-07
Iter: 1596 loss: 2.99853355e-07
Iter: 1597 loss: 2.99820357e-07
Iter: 1598 loss: 2.99777213e-07
Iter: 1599 loss: 2.99802906e-07
Iter: 1600 loss: 2.99727446e-07
Iter: 1601 loss: 2.99660968e-07
Iter: 1602 loss: 3.00183842e-07
Iter: 1603 loss: 2.99654744e-07
Iter: 1604 loss: 2.99571639e-07
Iter: 1605 loss: 2.99466478e-07
Iter: 1606 loss: 2.99457469e-07
Iter: 1607 loss: 2.99346112e-07
Iter: 1608 loss: 2.99617341e-07
Iter: 1609 loss: 2.99308681e-07
Iter: 1610 loss: 2.99209148e-07
Iter: 1611 loss: 2.99119506e-07
Iter: 1612 loss: 2.99090516e-07
Iter: 1613 loss: 2.99048878e-07
Iter: 1614 loss: 2.98996952e-07
Iter: 1615 loss: 2.98938915e-07
Iter: 1616 loss: 2.9921415e-07
Iter: 1617 loss: 2.98907594e-07
Iter: 1618 loss: 2.98868201e-07
Iter: 1619 loss: 2.98808118e-07
Iter: 1620 loss: 2.98805247e-07
Iter: 1621 loss: 2.98708358e-07
Iter: 1622 loss: 2.99661707e-07
Iter: 1623 loss: 2.9871552e-07
Iter: 1624 loss: 2.98657142e-07
Iter: 1625 loss: 2.98533223e-07
Iter: 1626 loss: 2.99677879e-07
Iter: 1627 loss: 2.98500566e-07
Iter: 1628 loss: 2.98459156e-07
Iter: 1629 loss: 2.9842937e-07
Iter: 1630 loss: 2.98362238e-07
Iter: 1631 loss: 2.98542972e-07
Iter: 1632 loss: 2.9833e-07
Iter: 1633 loss: 2.98268105e-07
Iter: 1634 loss: 2.98307782e-07
Iter: 1635 loss: 2.98233715e-07
Iter: 1636 loss: 2.98184744e-07
Iter: 1637 loss: 2.98185512e-07
Iter: 1638 loss: 2.98148024e-07
Iter: 1639 loss: 2.98064293e-07
Iter: 1640 loss: 2.98828098e-07
Iter: 1641 loss: 2.98040931e-07
Iter: 1642 loss: 2.97911271e-07
Iter: 1643 loss: 2.98624059e-07
Iter: 1644 loss: 2.97901948e-07
Iter: 1645 loss: 2.97804831e-07
Iter: 1646 loss: 2.97944155e-07
Iter: 1647 loss: 2.97747675e-07
Iter: 1648 loss: 2.9764908e-07
Iter: 1649 loss: 2.98221863e-07
Iter: 1650 loss: 2.97623e-07
Iter: 1651 loss: 2.97536644e-07
Iter: 1652 loss: 2.97536587e-07
Iter: 1653 loss: 2.97513907e-07
Iter: 1654 loss: 2.97431654e-07
Iter: 1655 loss: 2.98691248e-07
Iter: 1656 loss: 2.97428357e-07
Iter: 1657 loss: 2.97401812e-07
Iter: 1658 loss: 2.97388283e-07
Iter: 1659 loss: 2.97343774e-07
Iter: 1660 loss: 2.97285055e-07
Iter: 1661 loss: 2.97289091e-07
Iter: 1662 loss: 2.97225824e-07
Iter: 1663 loss: 2.97709619e-07
Iter: 1664 loss: 2.97226109e-07
Iter: 1665 loss: 2.97170885e-07
Iter: 1666 loss: 2.97157868e-07
Iter: 1667 loss: 2.9712487e-07
Iter: 1668 loss: 2.97043442e-07
Iter: 1669 loss: 2.96971052e-07
Iter: 1670 loss: 2.96974321e-07
Iter: 1671 loss: 2.96875896e-07
Iter: 1672 loss: 2.96864272e-07
Iter: 1673 loss: 2.96780399e-07
Iter: 1674 loss: 2.96641304e-07
Iter: 1675 loss: 2.96625018e-07
Iter: 1676 loss: 2.9651045e-07
Iter: 1677 loss: 2.97122511e-07
Iter: 1678 loss: 2.96474781e-07
Iter: 1679 loss: 2.96389487e-07
Iter: 1680 loss: 2.96679872e-07
Iter: 1681 loss: 2.96353448e-07
Iter: 1682 loss: 2.96350947e-07
Iter: 1683 loss: 2.96337049e-07
Iter: 1684 loss: 2.96294672e-07
Iter: 1685 loss: 2.96253717e-07
Iter: 1686 loss: 2.96249709e-07
Iter: 1687 loss: 2.96187e-07
Iter: 1688 loss: 2.9619e-07
Iter: 1689 loss: 2.96135141e-07
Iter: 1690 loss: 2.96017902e-07
Iter: 1691 loss: 2.96348617e-07
Iter: 1692 loss: 2.9598e-07
Iter: 1693 loss: 2.95910638e-07
Iter: 1694 loss: 2.96051127e-07
Iter: 1695 loss: 2.95892391e-07
Iter: 1696 loss: 2.95820769e-07
Iter: 1697 loss: 2.96459092e-07
Iter: 1698 loss: 2.9580832e-07
Iter: 1699 loss: 2.95754631e-07
Iter: 1700 loss: 2.95617497e-07
Iter: 1701 loss: 2.9819995e-07
Iter: 1702 loss: 2.95611102e-07
Iter: 1703 loss: 2.95621078e-07
Iter: 1704 loss: 2.95569635e-07
Iter: 1705 loss: 2.95536495e-07
Iter: 1706 loss: 2.95505856e-07
Iter: 1707 loss: 2.954975e-07
Iter: 1708 loss: 2.95431249e-07
Iter: 1709 loss: 2.95370199e-07
Iter: 1710 loss: 2.95354795e-07
Iter: 1711 loss: 2.95235509e-07
Iter: 1712 loss: 2.95972143e-07
Iter: 1713 loss: 2.95229256e-07
Iter: 1714 loss: 2.95146123e-07
Iter: 1715 loss: 2.95042298e-07
Iter: 1716 loss: 2.95039399e-07
Iter: 1717 loss: 2.94901838e-07
Iter: 1718 loss: 2.94899053e-07
Iter: 1719 loss: 2.94811798e-07
Iter: 1720 loss: 2.96106492e-07
Iter: 1721 loss: 2.94811969e-07
Iter: 1722 loss: 2.94781273e-07
Iter: 1723 loss: 2.94727442e-07
Iter: 1724 loss: 2.95630457e-07
Iter: 1725 loss: 2.94714027e-07
Iter: 1726 loss: 2.94660168e-07
Iter: 1727 loss: 2.94667814e-07
Iter: 1728 loss: 2.94613869e-07
Iter: 1729 loss: 2.94550148e-07
Iter: 1730 loss: 2.94522835e-07
Iter: 1731 loss: 2.94432311e-07
Iter: 1732 loss: 2.94410796e-07
Iter: 1733 loss: 2.94336417e-07
Iter: 1734 loss: 2.94255045e-07
Iter: 1735 loss: 2.9516923e-07
Iter: 1736 loss: 2.94256978e-07
Iter: 1737 loss: 2.9416222e-07
Iter: 1738 loss: 2.94307426e-07
Iter: 1739 loss: 2.94132576e-07
Iter: 1740 loss: 2.94039239e-07
Iter: 1741 loss: 2.9401096e-07
Iter: 1742 loss: 2.9394667e-07
Iter: 1743 loss: 2.93920266e-07
Iter: 1744 loss: 2.93919754e-07
Iter: 1745 loss: 2.93881442e-07
Iter: 1746 loss: 2.93888348e-07
Iter: 1747 loss: 2.938514e-07
Iter: 1748 loss: 2.93814281e-07
Iter: 1749 loss: 2.9376119e-07
Iter: 1750 loss: 2.93754709e-07
Iter: 1751 loss: 2.93705682e-07
Iter: 1752 loss: 2.93954713e-07
Iter: 1753 loss: 2.93687492e-07
Iter: 1754 loss: 2.93622492e-07
Iter: 1755 loss: 2.93871921e-07
Iter: 1756 loss: 2.93610384e-07
Iter: 1757 loss: 2.93550556e-07
Iter: 1758 loss: 2.93462534e-07
Iter: 1759 loss: 2.93468815e-07
Iter: 1760 loss: 2.93382527e-07
Iter: 1761 loss: 2.94204654e-07
Iter: 1762 loss: 2.93367691e-07
Iter: 1763 loss: 2.93255766e-07
Iter: 1764 loss: 2.93454235e-07
Iter: 1765 loss: 2.93219671e-07
Iter: 1766 loss: 2.93160667e-07
Iter: 1767 loss: 2.93207933e-07
Iter: 1768 loss: 2.93133e-07
Iter: 1769 loss: 2.93093933e-07
Iter: 1770 loss: 2.93090238e-07
Iter: 1771 loss: 2.93049823e-07
Iter: 1772 loss: 2.93056473e-07
Iter: 1773 loss: 2.93016853e-07
Iter: 1774 loss: 2.93000795e-07
Iter: 1775 loss: 2.92936136e-07
Iter: 1776 loss: 2.93376729e-07
Iter: 1777 loss: 2.92920731e-07
Iter: 1778 loss: 2.9284837e-07
Iter: 1779 loss: 2.93253862e-07
Iter: 1780 loss: 2.92840298e-07
Iter: 1781 loss: 2.92760745e-07
Iter: 1782 loss: 2.9327262e-07
Iter: 1783 loss: 2.92762962e-07
Iter: 1784 loss: 2.92712912e-07
Iter: 1785 loss: 2.92603346e-07
Iter: 1786 loss: 2.93267306e-07
Iter: 1787 loss: 2.92572025e-07
Iter: 1788 loss: 2.92503955e-07
Iter: 1789 loss: 2.92488494e-07
Iter: 1790 loss: 2.92456207e-07
Iter: 1791 loss: 2.92463938e-07
Iter: 1792 loss: 2.92421589e-07
Iter: 1793 loss: 2.9234667e-07
Iter: 1794 loss: 2.92845982e-07
Iter: 1795 loss: 2.92317338e-07
Iter: 1796 loss: 2.92225707e-07
Iter: 1797 loss: 2.92631569e-07
Iter: 1798 loss: 2.92195239e-07
Iter: 1799 loss: 2.92135297e-07
Iter: 1800 loss: 2.92890547e-07
Iter: 1801 loss: 2.92125662e-07
Iter: 1802 loss: 2.92059042e-07
Iter: 1803 loss: 2.92048867e-07
Iter: 1804 loss: 2.9199424e-07
Iter: 1805 loss: 2.91924692e-07
Iter: 1806 loss: 2.91975908e-07
Iter: 1807 loss: 2.91883168e-07
Iter: 1808 loss: 2.9181885e-07
Iter: 1809 loss: 2.92088288e-07
Iter: 1810 loss: 2.91810693e-07
Iter: 1811 loss: 2.91773631e-07
Iter: 1812 loss: 2.91750752e-07
Iter: 1813 loss: 2.91740434e-07
Iter: 1814 loss: 2.91698882e-07
Iter: 1815 loss: 2.92250945e-07
Iter: 1816 loss: 2.91691293e-07
Iter: 1817 loss: 2.91663952e-07
Iter: 1818 loss: 2.92346442e-07
Iter: 1819 loss: 2.91653919e-07
Iter: 1820 loss: 2.91611173e-07
Iter: 1821 loss: 2.91509593e-07
Iter: 1822 loss: 2.93177834e-07
Iter: 1823 loss: 2.91507774e-07
Iter: 1824 loss: 2.91429785e-07
Iter: 1825 loss: 2.91571325e-07
Iter: 1826 loss: 2.91389568e-07
Iter: 1827 loss: 2.91362312e-07
Iter: 1828 loss: 2.91344975e-07
Iter: 1829 loss: 2.91290604e-07
Iter: 1830 loss: 2.91189451e-07
Iter: 1831 loss: 2.93215663e-07
Iter: 1832 loss: 2.91186723e-07
Iter: 1833 loss: 2.91111178e-07
Iter: 1834 loss: 2.91204628e-07
Iter: 1835 loss: 2.91090657e-07
Iter: 1836 loss: 2.91069455e-07
Iter: 1837 loss: 2.91043534e-07
Iter: 1838 loss: 2.91015965e-07
Iter: 1839 loss: 2.90985241e-07
Iter: 1840 loss: 2.90968728e-07
Iter: 1841 loss: 2.90917313e-07
Iter: 1842 loss: 2.90881701e-07
Iter: 1843 loss: 2.90861749e-07
Iter: 1844 loss: 2.90829036e-07
Iter: 1845 loss: 2.90823152e-07
Iter: 1846 loss: 2.90765826e-07
Iter: 1847 loss: 2.9079132e-07
Iter: 1848 loss: 2.90749114e-07
Iter: 1849 loss: 2.90696619e-07
Iter: 1850 loss: 2.90656629e-07
Iter: 1851 loss: 2.90642134e-07
Iter: 1852 loss: 2.90643754e-07
Iter: 1853 loss: 2.90613315e-07
Iter: 1854 loss: 2.90604447e-07
Iter: 1855 loss: 2.90533478e-07
Iter: 1856 loss: 2.90917029e-07
Iter: 1857 loss: 2.90508694e-07
Iter: 1858 loss: 2.90426556e-07
Iter: 1859 loss: 2.90603822e-07
Iter: 1860 loss: 2.90404671e-07
Iter: 1861 loss: 2.90337852e-07
Iter: 1862 loss: 2.90339869e-07
Iter: 1863 loss: 2.90308378e-07
Iter: 1864 loss: 2.90192645e-07
Iter: 1865 loss: 2.90338278e-07
Iter: 1866 loss: 2.90113633e-07
Iter: 1867 loss: 2.89946115e-07
Iter: 1868 loss: 2.91616175e-07
Iter: 1869 loss: 2.89952538e-07
Iter: 1870 loss: 2.89924287e-07
Iter: 1871 loss: 2.89912776e-07
Iter: 1872 loss: 2.89867216e-07
Iter: 1873 loss: 2.89839363e-07
Iter: 1874 loss: 2.89839875e-07
Iter: 1875 loss: 2.89779564e-07
Iter: 1876 loss: 2.89799857e-07
Iter: 1877 loss: 2.89756628e-07
Iter: 1878 loss: 2.89728433e-07
Iter: 1879 loss: 2.89724767e-07
Iter: 1880 loss: 2.89681367e-07
Iter: 1881 loss: 2.89626598e-07
Iter: 1882 loss: 2.89620516e-07
Iter: 1883 loss: 2.89567481e-07
Iter: 1884 loss: 2.89753586e-07
Iter: 1885 loss: 2.89552361e-07
Iter: 1886 loss: 2.89484319e-07
Iter: 1887 loss: 2.89866477e-07
Iter: 1888 loss: 2.8947926e-07
Iter: 1889 loss: 2.89420541e-07
Iter: 1890 loss: 2.8934312e-07
Iter: 1891 loss: 2.89346531e-07
Iter: 1892 loss: 2.89278432e-07
Iter: 1893 loss: 2.89288749e-07
Iter: 1894 loss: 2.89244724e-07
Iter: 1895 loss: 2.89537724e-07
Iter: 1896 loss: 2.89240404e-07
Iter: 1897 loss: 2.89200699e-07
Iter: 1898 loss: 2.89140075e-07
Iter: 1899 loss: 2.90229906e-07
Iter: 1900 loss: 2.8913729e-07
Iter: 1901 loss: 2.89082436e-07
Iter: 1902 loss: 2.89243502e-07
Iter: 1903 loss: 2.89068907e-07
Iter: 1904 loss: 2.8901735e-07
Iter: 1905 loss: 2.89018317e-07
Iter: 1906 loss: 2.88982335e-07
Iter: 1907 loss: 2.88917107e-07
Iter: 1908 loss: 2.88925662e-07
Iter: 1909 loss: 2.88846763e-07
Iter: 1910 loss: 2.88814192e-07
Iter: 1911 loss: 2.88782758e-07
Iter: 1912 loss: 2.88799811e-07
Iter: 1913 loss: 2.88731201e-07
Iter: 1914 loss: 2.88695958e-07
Iter: 1915 loss: 2.88679644e-07
Iter: 1916 loss: 2.88675892e-07
Iter: 1917 loss: 2.88640877e-07
Iter: 1918 loss: 2.88665206e-07
Iter: 1919 loss: 2.88620328e-07
Iter: 1920 loss: 2.88573403e-07
Iter: 1921 loss: 2.88971506e-07
Iter: 1922 loss: 2.88570845e-07
Iter: 1923 loss: 2.88544101e-07
Iter: 1924 loss: 2.88482426e-07
Iter: 1925 loss: 2.89759413e-07
Iter: 1926 loss: 2.88481516e-07
Iter: 1927 loss: 2.88421063e-07
Iter: 1928 loss: 2.8881e-07
Iter: 1929 loss: 2.884114e-07
Iter: 1930 loss: 2.8832909e-07
Iter: 1931 loss: 2.88755757e-07
Iter: 1932 loss: 2.88308968e-07
Iter: 1933 loss: 2.88283388e-07
Iter: 1934 loss: 2.88223561e-07
Iter: 1935 loss: 2.8821546e-07
Iter: 1936 loss: 2.88153046e-07
Iter: 1937 loss: 2.88232513e-07
Iter: 1938 loss: 2.88124113e-07
Iter: 1939 loss: 2.88152307e-07
Iter: 1940 loss: 2.88104104e-07
Iter: 1941 loss: 2.88079605e-07
Iter: 1942 loss: 2.8804763e-07
Iter: 1943 loss: 2.88417709e-07
Iter: 1944 loss: 2.88036404e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.8
+ date
Mon Oct 26 15:13:45 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.8/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.8_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 1 --phi 0.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi0.8_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cfde158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5d06ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5d06ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5d107e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5d0ef510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5d0ef840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cf17840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cf2b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cf172f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cf186a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cebe950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5ce556a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5ce55840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5ce61b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5ce428c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cdd6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cded620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cded1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cd4d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cd4e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cd6e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cd36bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5ccdebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cd0b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cd0b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5ccb5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc34fb46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cc8c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cc87620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc5cc87ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc34f7d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc34f5b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc34f5b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc34ef7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc34f162f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efc34ebb268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.03713329e-06
Iter: 2 loss: 2.95838117e-06
Iter: 3 loss: 2.85639635e-06
Iter: 4 loss: 2.48622473e-06
Iter: 5 loss: 2.66235247e-06
Iter: 6 loss: 2.23701545e-06
Iter: 7 loss: 2.09884615e-06
Iter: 8 loss: 2.09876e-06
Iter: 9 loss: 1.97777172e-06
Iter: 10 loss: 1.91083654e-06
Iter: 11 loss: 1.8574932e-06
Iter: 12 loss: 1.75477817e-06
Iter: 13 loss: 2.02873252e-06
Iter: 14 loss: 1.7204502e-06
Iter: 15 loss: 1.69618909e-06
Iter: 16 loss: 1.67952362e-06
Iter: 17 loss: 1.65405277e-06
Iter: 18 loss: 1.59079309e-06
Iter: 19 loss: 2.20460583e-06
Iter: 20 loss: 1.5824196e-06
Iter: 21 loss: 1.53368387e-06
Iter: 22 loss: 1.53252552e-06
Iter: 23 loss: 1.48896754e-06
Iter: 24 loss: 1.51503582e-06
Iter: 25 loss: 1.46087257e-06
Iter: 26 loss: 1.4230593e-06
Iter: 27 loss: 1.42949432e-06
Iter: 28 loss: 1.39466908e-06
Iter: 29 loss: 1.35022162e-06
Iter: 30 loss: 1.43307579e-06
Iter: 31 loss: 1.33126571e-06
Iter: 32 loss: 1.30390708e-06
Iter: 33 loss: 1.55208272e-06
Iter: 34 loss: 1.30265494e-06
Iter: 35 loss: 1.27717954e-06
Iter: 36 loss: 1.24842859e-06
Iter: 37 loss: 1.24459484e-06
Iter: 38 loss: 1.20398568e-06
Iter: 39 loss: 1.31073409e-06
Iter: 40 loss: 1.1902589e-06
Iter: 41 loss: 1.17193554e-06
Iter: 42 loss: 1.16583783e-06
Iter: 43 loss: 1.14125487e-06
Iter: 44 loss: 1.14977763e-06
Iter: 45 loss: 1.1239581e-06
Iter: 46 loss: 1.10769656e-06
Iter: 47 loss: 1.22063432e-06
Iter: 48 loss: 1.10613144e-06
Iter: 49 loss: 1.08784548e-06
Iter: 50 loss: 1.09332859e-06
Iter: 51 loss: 1.074843e-06
Iter: 52 loss: 1.05851029e-06
Iter: 53 loss: 1.07243704e-06
Iter: 54 loss: 1.04890705e-06
Iter: 55 loss: 1.04460401e-06
Iter: 56 loss: 1.04088451e-06
Iter: 57 loss: 1.03532057e-06
Iter: 58 loss: 1.02323281e-06
Iter: 59 loss: 1.2042093e-06
Iter: 60 loss: 1.02272236e-06
Iter: 61 loss: 1.01359205e-06
Iter: 62 loss: 1.11690031e-06
Iter: 63 loss: 1.0133889e-06
Iter: 64 loss: 1.00226634e-06
Iter: 65 loss: 9.98359e-07
Iter: 66 loss: 9.92079549e-07
Iter: 67 loss: 9.80099685e-07
Iter: 68 loss: 9.60846137e-07
Iter: 69 loss: 9.60725401e-07
Iter: 70 loss: 9.38883261e-07
Iter: 71 loss: 1.14169779e-06
Iter: 72 loss: 9.37949437e-07
Iter: 73 loss: 9.20914772e-07
Iter: 74 loss: 9.18348292e-07
Iter: 75 loss: 9.06453181e-07
Iter: 76 loss: 8.94915445e-07
Iter: 77 loss: 8.93494587e-07
Iter: 78 loss: 8.85462e-07
Iter: 79 loss: 9.80998266e-07
Iter: 80 loss: 8.85331417e-07
Iter: 81 loss: 8.7990577e-07
Iter: 82 loss: 8.69587e-07
Iter: 83 loss: 1.09429311e-06
Iter: 84 loss: 8.69522296e-07
Iter: 85 loss: 8.59261945e-07
Iter: 86 loss: 1.01902447e-06
Iter: 87 loss: 8.5927627e-07
Iter: 88 loss: 8.51056598e-07
Iter: 89 loss: 8.39456163e-07
Iter: 90 loss: 8.39018071e-07
Iter: 91 loss: 8.27176279e-07
Iter: 92 loss: 8.75646606e-07
Iter: 93 loss: 8.24577342e-07
Iter: 94 loss: 8.0869e-07
Iter: 95 loss: 8.68352402e-07
Iter: 96 loss: 8.0485853e-07
Iter: 97 loss: 7.98493033e-07
Iter: 98 loss: 7.93764e-07
Iter: 99 loss: 7.91668299e-07
Iter: 100 loss: 7.82138954e-07
Iter: 101 loss: 9.21060064e-07
Iter: 102 loss: 7.82130883e-07
Iter: 103 loss: 7.75822855e-07
Iter: 104 loss: 7.71972395e-07
Iter: 105 loss: 7.69410178e-07
Iter: 106 loss: 7.6309243e-07
Iter: 107 loss: 7.56057034e-07
Iter: 108 loss: 7.55081714e-07
Iter: 109 loss: 7.43237706e-07
Iter: 110 loss: 7.76687898e-07
Iter: 111 loss: 7.39453e-07
Iter: 112 loss: 7.28292434e-07
Iter: 113 loss: 7.95227493e-07
Iter: 114 loss: 7.26891074e-07
Iter: 115 loss: 7.21126298e-07
Iter: 116 loss: 7.20573553e-07
Iter: 117 loss: 7.15015858e-07
Iter: 118 loss: 7.06961544e-07
Iter: 119 loss: 7.06712228e-07
Iter: 120 loss: 6.99049451e-07
Iter: 121 loss: 7.56781617e-07
Iter: 122 loss: 6.98466692e-07
Iter: 123 loss: 6.89453202e-07
Iter: 124 loss: 6.90334e-07
Iter: 125 loss: 6.82514155e-07
Iter: 126 loss: 6.7602241e-07
Iter: 127 loss: 6.91812261e-07
Iter: 128 loss: 6.7369848e-07
Iter: 129 loss: 6.67024153e-07
Iter: 130 loss: 7.51830612e-07
Iter: 131 loss: 6.66972483e-07
Iter: 132 loss: 6.63706714e-07
Iter: 133 loss: 6.56260227e-07
Iter: 134 loss: 7.52520407e-07
Iter: 135 loss: 6.55733345e-07
Iter: 136 loss: 6.57055352e-07
Iter: 137 loss: 6.52896e-07
Iter: 138 loss: 6.51133405e-07
Iter: 139 loss: 6.48552145e-07
Iter: 140 loss: 6.48488822e-07
Iter: 141 loss: 6.44374211e-07
Iter: 142 loss: 6.39185657e-07
Iter: 143 loss: 6.38788833e-07
Iter: 144 loss: 6.33299805e-07
Iter: 145 loss: 6.76585671e-07
Iter: 146 loss: 6.32927822e-07
Iter: 147 loss: 6.27361317e-07
Iter: 148 loss: 6.2552192e-07
Iter: 149 loss: 6.22279117e-07
Iter: 150 loss: 6.18039735e-07
Iter: 151 loss: 6.17755518e-07
Iter: 152 loss: 6.127284e-07
Iter: 153 loss: 6.19311436e-07
Iter: 154 loss: 6.1017289e-07
Iter: 155 loss: 6.06110632e-07
Iter: 156 loss: 6.0393586e-07
Iter: 157 loss: 6.0211255e-07
Iter: 158 loss: 5.98597751e-07
Iter: 159 loss: 5.98149938e-07
Iter: 160 loss: 5.96491304e-07
Iter: 161 loss: 5.92715651e-07
Iter: 162 loss: 6.45524835e-07
Iter: 163 loss: 5.92540573e-07
Iter: 164 loss: 5.88772821e-07
Iter: 165 loss: 5.88775833e-07
Iter: 166 loss: 5.85376313e-07
Iter: 167 loss: 5.81279e-07
Iter: 168 loss: 5.80871415e-07
Iter: 169 loss: 5.76945524e-07
Iter: 170 loss: 5.76554555e-07
Iter: 171 loss: 5.737229e-07
Iter: 172 loss: 5.68473865e-07
Iter: 173 loss: 5.77733431e-07
Iter: 174 loss: 5.66175686e-07
Iter: 175 loss: 5.62324203e-07
Iter: 176 loss: 5.8380266e-07
Iter: 177 loss: 5.617635e-07
Iter: 178 loss: 5.57795715e-07
Iter: 179 loss: 5.7631371e-07
Iter: 180 loss: 5.57049248e-07
Iter: 181 loss: 5.55152326e-07
Iter: 182 loss: 5.5484918e-07
Iter: 183 loss: 5.53660755e-07
Iter: 184 loss: 5.51085179e-07
Iter: 185 loss: 5.88911689e-07
Iter: 186 loss: 5.5095893e-07
Iter: 187 loss: 5.47407126e-07
Iter: 188 loss: 5.43764486e-07
Iter: 189 loss: 5.4306463e-07
Iter: 190 loss: 5.3807338e-07
Iter: 191 loss: 6.12102269e-07
Iter: 192 loss: 5.38067297e-07
Iter: 193 loss: 5.3521353e-07
Iter: 194 loss: 5.70019779e-07
Iter: 195 loss: 5.35178515e-07
Iter: 196 loss: 5.32495847e-07
Iter: 197 loss: 5.35232573e-07
Iter: 198 loss: 5.30962e-07
Iter: 199 loss: 5.28988608e-07
Iter: 200 loss: 5.35744277e-07
Iter: 201 loss: 5.28447458e-07
Iter: 202 loss: 5.25451924e-07
Iter: 203 loss: 5.23974563e-07
Iter: 204 loss: 5.22537107e-07
Iter: 205 loss: 5.20021899e-07
Iter: 206 loss: 5.2355341e-07
Iter: 207 loss: 5.1879374e-07
Iter: 208 loss: 5.16519094e-07
Iter: 209 loss: 5.16470777e-07
Iter: 210 loss: 5.15118359e-07
Iter: 211 loss: 5.12165514e-07
Iter: 212 loss: 5.57789463e-07
Iter: 213 loss: 5.12073598e-07
Iter: 214 loss: 5.09240749e-07
Iter: 215 loss: 5.19775767e-07
Iter: 216 loss: 5.08536402e-07
Iter: 217 loss: 5.07567506e-07
Iter: 218 loss: 5.07438187e-07
Iter: 219 loss: 5.06232482e-07
Iter: 220 loss: 5.04485456e-07
Iter: 221 loss: 5.04428158e-07
Iter: 222 loss: 5.02830062e-07
Iter: 223 loss: 5.04576803e-07
Iter: 224 loss: 5.01981958e-07
Iter: 225 loss: 4.99923658e-07
Iter: 226 loss: 4.97339e-07
Iter: 227 loss: 4.97126734e-07
Iter: 228 loss: 4.9407322e-07
Iter: 229 loss: 4.93920766e-07
Iter: 230 loss: 4.91382252e-07
Iter: 231 loss: 4.98837039e-07
Iter: 232 loss: 4.90592072e-07
Iter: 233 loss: 4.89163938e-07
Iter: 234 loss: 4.91183e-07
Iter: 235 loss: 4.88456e-07
Iter: 236 loss: 4.86083195e-07
Iter: 237 loss: 4.89866579e-07
Iter: 238 loss: 4.85016642e-07
Iter: 239 loss: 4.83686222e-07
Iter: 240 loss: 4.85940916e-07
Iter: 241 loss: 4.83087547e-07
Iter: 242 loss: 4.81811412e-07
Iter: 243 loss: 4.95867141e-07
Iter: 244 loss: 4.81769916e-07
Iter: 245 loss: 4.8058007e-07
Iter: 246 loss: 4.80023857e-07
Iter: 247 loss: 4.79428138e-07
Iter: 248 loss: 4.78562924e-07
Iter: 249 loss: 4.77323624e-07
Iter: 250 loss: 4.77268202e-07
Iter: 251 loss: 4.75054094e-07
Iter: 252 loss: 4.79176492e-07
Iter: 253 loss: 4.74106344e-07
Iter: 254 loss: 4.72640295e-07
Iter: 255 loss: 4.7261625e-07
Iter: 256 loss: 4.71152816e-07
Iter: 257 loss: 4.7508945e-07
Iter: 258 loss: 4.707031e-07
Iter: 259 loss: 4.69511463e-07
Iter: 260 loss: 4.67296474e-07
Iter: 261 loss: 5.15513648e-07
Iter: 262 loss: 4.6727618e-07
Iter: 263 loss: 4.65147934e-07
Iter: 264 loss: 4.72220847e-07
Iter: 265 loss: 4.64587458e-07
Iter: 266 loss: 4.6399677e-07
Iter: 267 loss: 4.63632716e-07
Iter: 268 loss: 4.62693095e-07
Iter: 269 loss: 4.62252615e-07
Iter: 270 loss: 4.61774022e-07
Iter: 271 loss: 4.60637523e-07
Iter: 272 loss: 4.63414779e-07
Iter: 273 loss: 4.60267529e-07
Iter: 274 loss: 4.58689897e-07
Iter: 275 loss: 4.62543056e-07
Iter: 276 loss: 4.58121519e-07
Iter: 277 loss: 4.56585212e-07
Iter: 278 loss: 4.53946143e-07
Iter: 279 loss: 4.53918261e-07
Iter: 280 loss: 4.54124603e-07
Iter: 281 loss: 4.52678194e-07
Iter: 282 loss: 4.51811104e-07
Iter: 283 loss: 4.49399181e-07
Iter: 284 loss: 4.60053343e-07
Iter: 285 loss: 4.48494063e-07
Iter: 286 loss: 4.46713273e-07
Iter: 287 loss: 4.68668702e-07
Iter: 288 loss: 4.4667803e-07
Iter: 289 loss: 4.45702653e-07
Iter: 290 loss: 4.52427628e-07
Iter: 291 loss: 4.45596527e-07
Iter: 292 loss: 4.44471e-07
Iter: 293 loss: 4.49161121e-07
Iter: 294 loss: 4.44233535e-07
Iter: 295 loss: 4.43497015e-07
Iter: 296 loss: 4.43042694e-07
Iter: 297 loss: 4.42745687e-07
Iter: 298 loss: 4.41752775e-07
Iter: 299 loss: 4.41708096e-07
Iter: 300 loss: 4.40953329e-07
Iter: 301 loss: 4.39670913e-07
Iter: 302 loss: 4.39679411e-07
Iter: 303 loss: 4.38527877e-07
Iter: 304 loss: 4.39267041e-07
Iter: 305 loss: 4.37812304e-07
Iter: 306 loss: 4.36746973e-07
Iter: 307 loss: 4.37621281e-07
Iter: 308 loss: 4.36086111e-07
Iter: 309 loss: 4.34422532e-07
Iter: 310 loss: 4.42838257e-07
Iter: 311 loss: 4.34141668e-07
Iter: 312 loss: 4.33346116e-07
Iter: 313 loss: 4.32606782e-07
Iter: 314 loss: 4.32404534e-07
Iter: 315 loss: 4.31449337e-07
Iter: 316 loss: 4.31408182e-07
Iter: 317 loss: 4.30792227e-07
Iter: 318 loss: 4.29400188e-07
Iter: 319 loss: 4.4620657e-07
Iter: 320 loss: 4.29272063e-07
Iter: 321 loss: 4.27739224e-07
Iter: 322 loss: 4.33515766e-07
Iter: 323 loss: 4.27372356e-07
Iter: 324 loss: 4.26174e-07
Iter: 325 loss: 4.27128327e-07
Iter: 326 loss: 4.25455823e-07
Iter: 327 loss: 4.24549398e-07
Iter: 328 loss: 4.24420762e-07
Iter: 329 loss: 4.23685179e-07
Iter: 330 loss: 4.22391622e-07
Iter: 331 loss: 4.53273486e-07
Iter: 332 loss: 4.22390286e-07
Iter: 333 loss: 4.21430741e-07
Iter: 334 loss: 4.22711054e-07
Iter: 335 loss: 4.20948311e-07
Iter: 336 loss: 4.20523577e-07
Iter: 337 loss: 4.20435924e-07
Iter: 338 loss: 4.19995104e-07
Iter: 339 loss: 4.19907167e-07
Iter: 340 loss: 4.19591231e-07
Iter: 341 loss: 4.18953391e-07
Iter: 342 loss: 4.18805428e-07
Iter: 343 loss: 4.18401441e-07
Iter: 344 loss: 4.1791165e-07
Iter: 345 loss: 4.17866943e-07
Iter: 346 loss: 4.17482028e-07
Iter: 347 loss: 4.1675753e-07
Iter: 348 loss: 4.318685e-07
Iter: 349 loss: 4.16724788e-07
Iter: 350 loss: 4.15696093e-07
Iter: 351 loss: 4.20040209e-07
Iter: 352 loss: 4.15479e-07
Iter: 353 loss: 4.14131875e-07
Iter: 354 loss: 4.16439491e-07
Iter: 355 loss: 4.13545365e-07
Iter: 356 loss: 4.1269351e-07
Iter: 357 loss: 4.10801817e-07
Iter: 358 loss: 4.38733366e-07
Iter: 359 loss: 4.10721327e-07
Iter: 360 loss: 4.08546498e-07
Iter: 361 loss: 4.26200501e-07
Iter: 362 loss: 4.08408908e-07
Iter: 363 loss: 4.07529285e-07
Iter: 364 loss: 4.19387163e-07
Iter: 365 loss: 4.07521611e-07
Iter: 366 loss: 4.06738479e-07
Iter: 367 loss: 4.11146e-07
Iter: 368 loss: 4.06643977e-07
Iter: 369 loss: 4.06090322e-07
Iter: 370 loss: 4.05057079e-07
Iter: 371 loss: 4.26178929e-07
Iter: 372 loss: 4.05021325e-07
Iter: 373 loss: 4.04226739e-07
Iter: 374 loss: 4.07304356e-07
Iter: 375 loss: 4.04029066e-07
Iter: 376 loss: 4.03609818e-07
Iter: 377 loss: 4.03586739e-07
Iter: 378 loss: 4.03059744e-07
Iter: 379 loss: 4.03107208e-07
Iter: 380 loss: 4.02668491e-07
Iter: 381 loss: 4.02090791e-07
Iter: 382 loss: 4.0190551e-07
Iter: 383 loss: 4.01597958e-07
Iter: 384 loss: 4.00794931e-07
Iter: 385 loss: 4.114695e-07
Iter: 386 loss: 4.00787172e-07
Iter: 387 loss: 4.002151e-07
Iter: 388 loss: 3.99259079e-07
Iter: 389 loss: 3.99272295e-07
Iter: 390 loss: 3.98821044e-07
Iter: 391 loss: 3.98665748e-07
Iter: 392 loss: 3.98174507e-07
Iter: 393 loss: 3.96934439e-07
Iter: 394 loss: 4.0705072e-07
Iter: 395 loss: 3.96701068e-07
Iter: 396 loss: 3.95718587e-07
Iter: 397 loss: 4.05541385e-07
Iter: 398 loss: 3.95695167e-07
Iter: 399 loss: 3.95026632e-07
Iter: 400 loss: 3.95151432e-07
Iter: 401 loss: 3.94520299e-07
Iter: 402 loss: 3.93865605e-07
Iter: 403 loss: 3.93860546e-07
Iter: 404 loss: 3.93249422e-07
Iter: 405 loss: 3.94234689e-07
Iter: 406 loss: 3.92963386e-07
Iter: 407 loss: 3.92417462e-07
Iter: 408 loss: 3.91606221e-07
Iter: 409 loss: 3.91586099e-07
Iter: 410 loss: 3.90494506e-07
Iter: 411 loss: 3.95140859e-07
Iter: 412 loss: 3.90284413e-07
Iter: 413 loss: 3.90207816e-07
Iter: 414 loss: 3.89921979e-07
Iter: 415 loss: 3.89627132e-07
Iter: 416 loss: 3.88826606e-07
Iter: 417 loss: 3.94301907e-07
Iter: 418 loss: 3.88630326e-07
Iter: 419 loss: 3.88415117e-07
Iter: 420 loss: 3.8827136e-07
Iter: 421 loss: 3.87962757e-07
Iter: 422 loss: 3.87832586e-07
Iter: 423 loss: 3.87655064e-07
Iter: 424 loss: 3.87184571e-07
Iter: 425 loss: 3.87079695e-07
Iter: 426 loss: 3.86781636e-07
Iter: 427 loss: 3.8626942e-07
Iter: 428 loss: 3.86256716e-07
Iter: 429 loss: 3.85965279e-07
Iter: 430 loss: 3.85246381e-07
Iter: 431 loss: 3.92064123e-07
Iter: 432 loss: 3.85140424e-07
Iter: 433 loss: 3.84200263e-07
Iter: 434 loss: 3.84815166e-07
Iter: 435 loss: 3.83604174e-07
Iter: 436 loss: 3.83236909e-07
Iter: 437 loss: 3.82989299e-07
Iter: 438 loss: 3.82495443e-07
Iter: 439 loss: 3.82933138e-07
Iter: 440 loss: 3.82196077e-07
Iter: 441 loss: 3.81689176e-07
Iter: 442 loss: 3.81274e-07
Iter: 443 loss: 3.81142513e-07
Iter: 444 loss: 3.80364753e-07
Iter: 445 loss: 3.82985576e-07
Iter: 446 loss: 3.80163613e-07
Iter: 447 loss: 3.79913644e-07
Iter: 448 loss: 3.79842902e-07
Iter: 449 loss: 3.79537084e-07
Iter: 450 loss: 3.79061618e-07
Iter: 451 loss: 3.79033168e-07
Iter: 452 loss: 3.78470617e-07
Iter: 453 loss: 3.79007304e-07
Iter: 454 loss: 3.7811725e-07
Iter: 455 loss: 3.77812626e-07
Iter: 456 loss: 3.77754645e-07
Iter: 457 loss: 3.77439335e-07
Iter: 458 loss: 3.76684085e-07
Iter: 459 loss: 3.82991857e-07
Iter: 460 loss: 3.76555761e-07
Iter: 461 loss: 3.76655862e-07
Iter: 462 loss: 3.76225984e-07
Iter: 463 loss: 3.7602166e-07
Iter: 464 loss: 3.75464907e-07
Iter: 465 loss: 3.82457301e-07
Iter: 466 loss: 3.75440123e-07
Iter: 467 loss: 3.74860463e-07
Iter: 468 loss: 3.7589632e-07
Iter: 469 loss: 3.74583863e-07
Iter: 470 loss: 3.74013155e-07
Iter: 471 loss: 3.75446433e-07
Iter: 472 loss: 3.73821536e-07
Iter: 473 loss: 3.73312957e-07
Iter: 474 loss: 3.73290618e-07
Iter: 475 loss: 3.72946829e-07
Iter: 476 loss: 3.72067291e-07
Iter: 477 loss: 3.80031196e-07
Iter: 478 loss: 3.71937347e-07
Iter: 479 loss: 3.7118923e-07
Iter: 480 loss: 3.79080433e-07
Iter: 481 loss: 3.71182e-07
Iter: 482 loss: 3.7079252e-07
Iter: 483 loss: 3.70775808e-07
Iter: 484 loss: 3.70525555e-07
Iter: 485 loss: 3.70173439e-07
Iter: 486 loss: 3.70163036e-07
Iter: 487 loss: 3.69782015e-07
Iter: 488 loss: 3.70455751e-07
Iter: 489 loss: 3.69596876e-07
Iter: 490 loss: 3.69122e-07
Iter: 491 loss: 3.73878805e-07
Iter: 492 loss: 3.6912229e-07
Iter: 493 loss: 3.68840517e-07
Iter: 494 loss: 3.68412827e-07
Iter: 495 loss: 3.68391028e-07
Iter: 496 loss: 3.68018391e-07
Iter: 497 loss: 3.6801498e-07
Iter: 498 loss: 3.67760123e-07
Iter: 499 loss: 3.67141098e-07
Iter: 500 loss: 3.73266374e-07
Iter: 501 loss: 3.67043356e-07
Iter: 502 loss: 3.66322467e-07
Iter: 503 loss: 3.70166333e-07
Iter: 504 loss: 3.66212646e-07
Iter: 505 loss: 3.65834353e-07
Iter: 506 loss: 3.67869205e-07
Iter: 507 loss: 3.65744967e-07
Iter: 508 loss: 3.65272598e-07
Iter: 509 loss: 3.668506e-07
Iter: 510 loss: 3.65131768e-07
Iter: 511 loss: 3.64801821e-07
Iter: 512 loss: 3.65070747e-07
Iter: 513 loss: 3.64609633e-07
Iter: 514 loss: 3.6432516e-07
Iter: 515 loss: 3.65414735e-07
Iter: 516 loss: 3.64263713e-07
Iter: 517 loss: 3.63842105e-07
Iter: 518 loss: 3.64206301e-07
Iter: 519 loss: 3.63608876e-07
Iter: 520 loss: 3.63139861e-07
Iter: 521 loss: 3.62522229e-07
Iter: 522 loss: 3.62503783e-07
Iter: 523 loss: 3.62116566e-07
Iter: 524 loss: 3.62033632e-07
Iter: 525 loss: 3.61618333e-07
Iter: 526 loss: 3.61277245e-07
Iter: 527 loss: 3.61160858e-07
Iter: 528 loss: 3.60703609e-07
Iter: 529 loss: 3.64063709e-07
Iter: 530 loss: 3.60684254e-07
Iter: 531 loss: 3.60251818e-07
Iter: 532 loss: 3.6194939e-07
Iter: 533 loss: 3.60165188e-07
Iter: 534 loss: 3.59897882e-07
Iter: 535 loss: 3.59547926e-07
Iter: 536 loss: 3.5952911e-07
Iter: 537 loss: 3.59145417e-07
Iter: 538 loss: 3.60022028e-07
Iter: 539 loss: 3.59020476e-07
Iter: 540 loss: 3.58622742e-07
Iter: 541 loss: 3.61368905e-07
Iter: 542 loss: 3.5856749e-07
Iter: 543 loss: 3.58219722e-07
Iter: 544 loss: 3.6089881e-07
Iter: 545 loss: 3.58170837e-07
Iter: 546 loss: 3.57894351e-07
Iter: 547 loss: 3.5730335e-07
Iter: 548 loss: 3.65925757e-07
Iter: 549 loss: 3.57273905e-07
Iter: 550 loss: 3.56796733e-07
Iter: 551 loss: 3.56798637e-07
Iter: 552 loss: 3.56310522e-07
Iter: 553 loss: 3.57257534e-07
Iter: 554 loss: 3.56106511e-07
Iter: 555 loss: 3.55737086e-07
Iter: 556 loss: 3.55568602e-07
Iter: 557 loss: 3.55410464e-07
Iter: 558 loss: 3.55023758e-07
Iter: 559 loss: 3.60846286e-07
Iter: 560 loss: 3.55039219e-07
Iter: 561 loss: 3.54621278e-07
Iter: 562 loss: 3.54161301e-07
Iter: 563 loss: 3.54099456e-07
Iter: 564 loss: 3.53764307e-07
Iter: 565 loss: 3.59025563e-07
Iter: 566 loss: 3.53761607e-07
Iter: 567 loss: 3.53429499e-07
Iter: 568 loss: 3.53471e-07
Iter: 569 loss: 3.53177825e-07
Iter: 570 loss: 3.52787254e-07
Iter: 571 loss: 3.52321592e-07
Iter: 572 loss: 3.52275634e-07
Iter: 573 loss: 3.51658798e-07
Iter: 574 loss: 3.55032711e-07
Iter: 575 loss: 3.51572254e-07
Iter: 576 loss: 3.51190124e-07
Iter: 577 loss: 3.53220543e-07
Iter: 578 loss: 3.51121258e-07
Iter: 579 loss: 3.50712412e-07
Iter: 580 loss: 3.54380177e-07
Iter: 581 loss: 3.5067697e-07
Iter: 582 loss: 3.50508685e-07
Iter: 583 loss: 3.50236803e-07
Iter: 584 loss: 3.50225378e-07
Iter: 585 loss: 3.49891138e-07
Iter: 586 loss: 3.52190966e-07
Iter: 587 loss: 3.49842338e-07
Iter: 588 loss: 3.49591346e-07
Iter: 589 loss: 3.5321537e-07
Iter: 590 loss: 3.495893e-07
Iter: 591 loss: 3.49447419e-07
Iter: 592 loss: 3.48944866e-07
Iter: 593 loss: 3.51003052e-07
Iter: 594 loss: 3.48753531e-07
Iter: 595 loss: 3.48457263e-07
Iter: 596 loss: 3.48399794e-07
Iter: 597 loss: 3.48056858e-07
Iter: 598 loss: 3.48515755e-07
Iter: 599 loss: 3.47880928e-07
Iter: 600 loss: 3.47639059e-07
Iter: 601 loss: 3.48287671e-07
Iter: 602 loss: 3.47536599e-07
Iter: 603 loss: 3.4719622e-07
Iter: 604 loss: 3.48094545e-07
Iter: 605 loss: 3.47061331e-07
Iter: 606 loss: 3.46742809e-07
Iter: 607 loss: 3.46271065e-07
Iter: 608 loss: 3.46264443e-07
Iter: 609 loss: 3.45851618e-07
Iter: 610 loss: 3.48172307e-07
Iter: 611 loss: 3.4580529e-07
Iter: 612 loss: 3.45450019e-07
Iter: 613 loss: 3.45976844e-07
Iter: 614 loss: 3.45296485e-07
Iter: 615 loss: 3.44972761e-07
Iter: 616 loss: 3.44954344e-07
Iter: 617 loss: 3.4476335e-07
Iter: 618 loss: 3.44580883e-07
Iter: 619 loss: 3.44523329e-07
Iter: 620 loss: 3.4424221e-07
Iter: 621 loss: 3.43768136e-07
Iter: 622 loss: 3.43765919e-07
Iter: 623 loss: 3.433681e-07
Iter: 624 loss: 3.48735654e-07
Iter: 625 loss: 3.43349171e-07
Iter: 626 loss: 3.4308357e-07
Iter: 627 loss: 3.46227068e-07
Iter: 628 loss: 3.43085759e-07
Iter: 629 loss: 3.42798188e-07
Iter: 630 loss: 3.4353593e-07
Iter: 631 loss: 3.42699309e-07
Iter: 632 loss: 3.4251272e-07
Iter: 633 loss: 3.42164e-07
Iter: 634 loss: 3.50040239e-07
Iter: 635 loss: 3.42153783e-07
Iter: 636 loss: 3.41904922e-07
Iter: 637 loss: 3.41910322e-07
Iter: 638 loss: 3.4162548e-07
Iter: 639 loss: 3.41957e-07
Iter: 640 loss: 3.41481893e-07
Iter: 641 loss: 3.41208278e-07
Iter: 642 loss: 3.40979909e-07
Iter: 643 loss: 3.40933127e-07
Iter: 644 loss: 3.40501231e-07
Iter: 645 loss: 3.41259351e-07
Iter: 646 loss: 3.40316859e-07
Iter: 647 loss: 3.40040089e-07
Iter: 648 loss: 3.40042021e-07
Iter: 649 loss: 3.39758571e-07
Iter: 650 loss: 3.3985296e-07
Iter: 651 loss: 3.39556379e-07
Iter: 652 loss: 3.39281968e-07
Iter: 653 loss: 3.39250278e-07
Iter: 654 loss: 3.390806e-07
Iter: 655 loss: 3.38713306e-07
Iter: 656 loss: 3.39889368e-07
Iter: 657 loss: 3.38597829e-07
Iter: 658 loss: 3.38371876e-07
Iter: 659 loss: 3.38352748e-07
Iter: 660 loss: 3.38195662e-07
Iter: 661 loss: 3.37868158e-07
Iter: 662 loss: 3.43092552e-07
Iter: 663 loss: 3.3786398e-07
Iter: 664 loss: 3.37551455e-07
Iter: 665 loss: 3.37798696e-07
Iter: 666 loss: 3.37368078e-07
Iter: 667 loss: 3.37049812e-07
Iter: 668 loss: 3.37038585e-07
Iter: 669 loss: 3.36737031e-07
Iter: 670 loss: 3.37010107e-07
Iter: 671 loss: 3.36534e-07
Iter: 672 loss: 3.36326593e-07
Iter: 673 loss: 3.36141852e-07
Iter: 674 loss: 3.36058065e-07
Iter: 675 loss: 3.35693358e-07
Iter: 676 loss: 3.36404923e-07
Iter: 677 loss: 3.3554295e-07
Iter: 678 loss: 3.35432389e-07
Iter: 679 loss: 3.35340417e-07
Iter: 680 loss: 3.35203026e-07
Iter: 681 loss: 3.35033832e-07
Iter: 682 loss: 3.35012317e-07
Iter: 683 loss: 3.34797676e-07
Iter: 684 loss: 3.34704055e-07
Iter: 685 loss: 3.34583888e-07
Iter: 686 loss: 3.34235153e-07
Iter: 687 loss: 3.34931656e-07
Iter: 688 loss: 3.34086849e-07
Iter: 689 loss: 3.33895059e-07
Iter: 690 loss: 3.33853905e-07
Iter: 691 loss: 3.33645886e-07
Iter: 692 loss: 3.33321623e-07
Iter: 693 loss: 3.33332423e-07
Iter: 694 loss: 3.32926504e-07
Iter: 695 loss: 3.33534103e-07
Iter: 696 loss: 3.32740171e-07
Iter: 697 loss: 3.32498615e-07
Iter: 698 loss: 3.35119239e-07
Iter: 699 loss: 3.32498331e-07
Iter: 700 loss: 3.32219741e-07
Iter: 701 loss: 3.32937788e-07
Iter: 702 loss: 3.32130043e-07
Iter: 703 loss: 3.31960138e-07
Iter: 704 loss: 3.3190932e-07
Iter: 705 loss: 3.31798503e-07
Iter: 706 loss: 3.31671231e-07
Iter: 707 loss: 3.3167538e-07
Iter: 708 loss: 3.31518834e-07
Iter: 709 loss: 3.31253943e-07
Iter: 710 loss: 3.3631369e-07
Iter: 711 loss: 3.31250817e-07
Iter: 712 loss: 3.3095057e-07
Iter: 713 loss: 3.31494618e-07
Iter: 714 loss: 3.30823354e-07
Iter: 715 loss: 3.30630826e-07
Iter: 716 loss: 3.306167e-07
Iter: 717 loss: 3.30407516e-07
Iter: 718 loss: 3.29901042e-07
Iter: 719 loss: 3.33984644e-07
Iter: 720 loss: 3.29801736e-07
Iter: 721 loss: 3.29323314e-07
Iter: 722 loss: 3.32571688e-07
Iter: 723 loss: 3.2926863e-07
Iter: 724 loss: 3.28976114e-07
Iter: 725 loss: 3.31013183e-07
Iter: 726 loss: 3.2895943e-07
Iter: 727 loss: 3.28809705e-07
Iter: 728 loss: 3.28796773e-07
Iter: 729 loss: 3.28689566e-07
Iter: 730 loss: 3.28451335e-07
Iter: 731 loss: 3.32077946e-07
Iter: 732 loss: 3.28435931e-07
Iter: 733 loss: 3.28191391e-07
Iter: 734 loss: 3.28660718e-07
Iter: 735 loss: 3.28104562e-07
Iter: 736 loss: 3.27781095e-07
Iter: 737 loss: 3.28182296e-07
Iter: 738 loss: 3.27614885e-07
Iter: 739 loss: 3.27392399e-07
Iter: 740 loss: 3.27364461e-07
Iter: 741 loss: 3.27167811e-07
Iter: 742 loss: 3.2675e-07
Iter: 743 loss: 3.33869423e-07
Iter: 744 loss: 3.26741372e-07
Iter: 745 loss: 3.26547791e-07
Iter: 746 loss: 3.26525537e-07
Iter: 747 loss: 3.26327353e-07
Iter: 748 loss: 3.26466363e-07
Iter: 749 loss: 3.26184647e-07
Iter: 750 loss: 3.26041857e-07
Iter: 751 loss: 3.25985553e-07
Iter: 752 loss: 3.25895599e-07
Iter: 753 loss: 3.25705429e-07
Iter: 754 loss: 3.25707276e-07
Iter: 755 loss: 3.25593589e-07
Iter: 756 loss: 3.25403676e-07
Iter: 757 loss: 3.2540504e-07
Iter: 758 loss: 3.25169253e-07
Iter: 759 loss: 3.24862469e-07
Iter: 760 loss: 3.24827482e-07
Iter: 761 loss: 3.24394193e-07
Iter: 762 loss: 3.27849108e-07
Iter: 763 loss: 3.24362873e-07
Iter: 764 loss: 3.24095367e-07
Iter: 765 loss: 3.24092866e-07
Iter: 766 loss: 3.23869472e-07
Iter: 767 loss: 3.24060835e-07
Iter: 768 loss: 3.23738078e-07
Iter: 769 loss: 3.23521277e-07
Iter: 770 loss: 3.23353561e-07
Iter: 771 loss: 3.2327722e-07
Iter: 772 loss: 3.2301395e-07
Iter: 773 loss: 3.24555288e-07
Iter: 774 loss: 3.22975836e-07
Iter: 775 loss: 3.22812042e-07
Iter: 776 loss: 3.22823723e-07
Iter: 777 loss: 3.22691164e-07
Iter: 778 loss: 3.22582281e-07
Iter: 779 loss: 3.22532117e-07
Iter: 780 loss: 3.22342743e-07
Iter: 781 loss: 3.2225006e-07
Iter: 782 loss: 3.22178607e-07
Iter: 783 loss: 3.22025073e-07
Iter: 784 loss: 3.22020611e-07
Iter: 785 loss: 3.21816032e-07
Iter: 786 loss: 3.2158519e-07
Iter: 787 loss: 3.21578227e-07
Iter: 788 loss: 3.21350853e-07
Iter: 789 loss: 3.21702089e-07
Iter: 790 loss: 3.21227503e-07
Iter: 791 loss: 3.20969093e-07
Iter: 792 loss: 3.24610596e-07
Iter: 793 loss: 3.20962e-07
Iter: 794 loss: 3.2080473e-07
Iter: 795 loss: 3.20630477e-07
Iter: 796 loss: 3.20599582e-07
Iter: 797 loss: 3.20401227e-07
Iter: 798 loss: 3.20753315e-07
Iter: 799 loss: 3.20313291e-07
Iter: 800 loss: 3.20111269e-07
Iter: 801 loss: 3.20102856e-07
Iter: 802 loss: 3.19990193e-07
Iter: 803 loss: 3.19849022e-07
Iter: 804 loss: 3.19840581e-07
Iter: 805 loss: 3.19639838e-07
Iter: 806 loss: 3.19770891e-07
Iter: 807 loss: 3.19509951e-07
Iter: 808 loss: 3.19315689e-07
Iter: 809 loss: 3.1932538e-07
Iter: 810 loss: 3.19138024e-07
Iter: 811 loss: 3.19156214e-07
Iter: 812 loss: 3.18987503e-07
Iter: 813 loss: 3.1878659e-07
Iter: 814 loss: 3.18586132e-07
Iter: 815 loss: 3.18548132e-07
Iter: 816 loss: 3.18263631e-07
Iter: 817 loss: 3.20979041e-07
Iter: 818 loss: 3.18254138e-07
Iter: 819 loss: 3.18081391e-07
Iter: 820 loss: 3.18077e-07
Iter: 821 loss: 3.18008745e-07
Iter: 822 loss: 3.17818035e-07
Iter: 823 loss: 3.19662377e-07
Iter: 824 loss: 3.17794189e-07
Iter: 825 loss: 3.17665069e-07
Iter: 826 loss: 3.17676893e-07
Iter: 827 loss: 3.17529e-07
Iter: 828 loss: 3.17312015e-07
Iter: 829 loss: 3.17306501e-07
Iter: 830 loss: 3.17145236e-07
Iter: 831 loss: 3.17239596e-07
Iter: 832 loss: 3.17032516e-07
Iter: 833 loss: 3.16822508e-07
Iter: 834 loss: 3.18815637e-07
Iter: 835 loss: 3.16821399e-07
Iter: 836 loss: 3.16588626e-07
Iter: 837 loss: 3.16644616e-07
Iter: 838 loss: 3.16433614e-07
Iter: 839 loss: 3.16253079e-07
Iter: 840 loss: 3.16331e-07
Iter: 841 loss: 3.16125522e-07
Iter: 842 loss: 3.15901246e-07
Iter: 843 loss: 3.16413832e-07
Iter: 844 loss: 3.15795e-07
Iter: 845 loss: 3.15621605e-07
Iter: 846 loss: 3.15628256e-07
Iter: 847 loss: 3.15462756e-07
Iter: 848 loss: 3.15466195e-07
Iter: 849 loss: 3.15329942e-07
Iter: 850 loss: 3.15183115e-07
Iter: 851 loss: 3.15078751e-07
Iter: 852 loss: 3.15021225e-07
Iter: 853 loss: 3.14773615e-07
Iter: 854 loss: 3.15961614e-07
Iter: 855 loss: 3.14723479e-07
Iter: 856 loss: 3.14507076e-07
Iter: 857 loss: 3.1772484e-07
Iter: 858 loss: 3.14501278e-07
Iter: 859 loss: 3.14396e-07
Iter: 860 loss: 3.14213082e-07
Iter: 861 loss: 3.18137864e-07
Iter: 862 loss: 3.14197507e-07
Iter: 863 loss: 3.14059434e-07
Iter: 864 loss: 3.14062049e-07
Iter: 865 loss: 3.13887938e-07
Iter: 866 loss: 3.13807163e-07
Iter: 867 loss: 3.13739207e-07
Iter: 868 loss: 3.13562765e-07
Iter: 869 loss: 3.13548753e-07
Iter: 870 loss: 3.13415853e-07
Iter: 871 loss: 3.13257971e-07
Iter: 872 loss: 3.13239411e-07
Iter: 873 loss: 3.13143772e-07
Iter: 874 loss: 3.12944252e-07
Iter: 875 loss: 3.16971864e-07
Iter: 876 loss: 3.12947577e-07
Iter: 877 loss: 3.12722477e-07
Iter: 878 loss: 3.13284914e-07
Iter: 879 loss: 3.12644772e-07
Iter: 880 loss: 3.12494564e-07
Iter: 881 loss: 3.12485867e-07
Iter: 882 loss: 3.123765e-07
Iter: 883 loss: 3.12230043e-07
Iter: 884 loss: 3.12212421e-07
Iter: 885 loss: 3.11997496e-07
Iter: 886 loss: 3.12201109e-07
Iter: 887 loss: 3.11864454e-07
Iter: 888 loss: 3.11689575e-07
Iter: 889 loss: 3.1332695e-07
Iter: 890 loss: 3.11669254e-07
Iter: 891 loss: 3.11472036e-07
Iter: 892 loss: 3.12211398e-07
Iter: 893 loss: 3.114219e-07
Iter: 894 loss: 3.11299857e-07
Iter: 895 loss: 3.11069812e-07
Iter: 896 loss: 3.11074928e-07
Iter: 897 loss: 3.1100393e-07
Iter: 898 loss: 3.1095675e-07
Iter: 899 loss: 3.108687e-07
Iter: 900 loss: 3.10782497e-07
Iter: 901 loss: 3.1074768e-07
Iter: 902 loss: 3.1061353e-07
Iter: 903 loss: 3.10474519e-07
Iter: 904 loss: 3.10440356e-07
Iter: 905 loss: 3.10327e-07
Iter: 906 loss: 3.1027858e-07
Iter: 907 loss: 3.10219235e-07
Iter: 908 loss: 3.10038274e-07
Iter: 909 loss: 3.12090719e-07
Iter: 910 loss: 3.10022472e-07
Iter: 911 loss: 3.09823633e-07
Iter: 912 loss: 3.11239546e-07
Iter: 913 loss: 3.09805955e-07
Iter: 914 loss: 3.09571249e-07
Iter: 915 loss: 3.1041003e-07
Iter: 916 loss: 3.09523557e-07
Iter: 917 loss: 3.09367806e-07
Iter: 918 loss: 3.09333927e-07
Iter: 919 loss: 3.09263385e-07
Iter: 920 loss: 3.0902487e-07
Iter: 921 loss: 3.09091e-07
Iter: 922 loss: 3.0886693e-07
Iter: 923 loss: 3.08766857e-07
Iter: 924 loss: 3.08703761e-07
Iter: 925 loss: 3.08579615e-07
Iter: 926 loss: 3.08614432e-07
Iter: 927 loss: 3.08512767e-07
Iter: 928 loss: 3.0837424e-07
Iter: 929 loss: 3.08235883e-07
Iter: 930 loss: 3.08213771e-07
Iter: 931 loss: 3.08135725e-07
Iter: 932 loss: 3.08101875e-07
Iter: 933 loss: 3.07998562e-07
Iter: 934 loss: 3.07756665e-07
Iter: 935 loss: 3.09887753e-07
Iter: 936 loss: 3.07717812e-07
Iter: 937 loss: 3.07525e-07
Iter: 938 loss: 3.10180695e-07
Iter: 939 loss: 3.07507747e-07
Iter: 940 loss: 3.07312973e-07
Iter: 941 loss: 3.07810296e-07
Iter: 942 loss: 3.07255334e-07
Iter: 943 loss: 3.07120956e-07
Iter: 944 loss: 3.0722606e-07
Iter: 945 loss: 3.07056212e-07
Iter: 946 loss: 3.06955229e-07
Iter: 947 loss: 3.08152778e-07
Iter: 948 loss: 3.06959e-07
Iter: 949 loss: 3.06833584e-07
Iter: 950 loss: 3.06725155e-07
Iter: 951 loss: 3.06686673e-07
Iter: 952 loss: 3.06543797e-07
Iter: 953 loss: 3.06535327e-07
Iter: 954 loss: 3.06430451e-07
Iter: 955 loss: 3.06200235e-07
Iter: 956 loss: 3.06998629e-07
Iter: 957 loss: 3.06135803e-07
Iter: 958 loss: 3.05985623e-07
Iter: 959 loss: 3.05978688e-07
Iter: 960 loss: 3.05848118e-07
Iter: 961 loss: 3.05653089e-07
Iter: 962 loss: 3.05641436e-07
Iter: 963 loss: 3.05417842e-07
Iter: 964 loss: 3.05461953e-07
Iter: 965 loss: 3.05273829e-07
Iter: 966 loss: 3.05136496e-07
Iter: 967 loss: 3.05132971e-07
Iter: 968 loss: 3.05037645e-07
Iter: 969 loss: 3.05888477e-07
Iter: 970 loss: 3.05021985e-07
Iter: 971 loss: 3.04927028e-07
Iter: 972 loss: 3.04693941e-07
Iter: 973 loss: 3.06495707e-07
Iter: 974 loss: 3.04656339e-07
Iter: 975 loss: 3.04431921e-07
Iter: 976 loss: 3.05404171e-07
Iter: 977 loss: 3.0436604e-07
Iter: 978 loss: 3.04200569e-07
Iter: 979 loss: 3.04195908e-07
Iter: 980 loss: 3.0406369e-07
Iter: 981 loss: 3.03858116e-07
Iter: 982 loss: 3.038611e-07
Iter: 983 loss: 3.03641428e-07
Iter: 984 loss: 3.03768417e-07
Iter: 985 loss: 3.03500826e-07
Iter: 986 loss: 3.03346496e-07
Iter: 987 loss: 3.03335753e-07
Iter: 988 loss: 3.03216609e-07
Iter: 989 loss: 3.03633556e-07
Iter: 990 loss: 3.03165933e-07
Iter: 991 loss: 3.03076035e-07
Iter: 992 loss: 3.03002963e-07
Iter: 993 loss: 3.02985768e-07
Iter: 994 loss: 3.02821036e-07
Iter: 995 loss: 3.04646221e-07
Iter: 996 loss: 3.02827e-07
Iter: 997 loss: 3.02749697e-07
Iter: 998 loss: 3.02554213e-07
Iter: 999 loss: 3.04660375e-07
Iter: 1000 loss: 3.02549466e-07
Iter: 1001 loss: 3.02333092e-07
Iter: 1002 loss: 3.03970381e-07
Iter: 1003 loss: 3.02316153e-07
Iter: 1004 loss: 3.0213738e-07
Iter: 1005 loss: 3.03676188e-07
Iter: 1006 loss: 3.02132179e-07
Iter: 1007 loss: 3.02023523e-07
Iter: 1008 loss: 3.01848615e-07
Iter: 1009 loss: 3.01847535e-07
Iter: 1010 loss: 3.0168016e-07
Iter: 1011 loss: 3.01834518e-07
Iter: 1012 loss: 3.01561926e-07
Iter: 1013 loss: 3.01473051e-07
Iter: 1014 loss: 3.01455486e-07
Iter: 1015 loss: 3.01358114e-07
Iter: 1016 loss: 3.01496584e-07
Iter: 1017 loss: 3.01314287e-07
Iter: 1018 loss: 3.0123897e-07
Iter: 1019 loss: 3.01123492e-07
Iter: 1020 loss: 3.01113573e-07
Iter: 1021 loss: 3.0093787e-07
Iter: 1022 loss: 3.01287031e-07
Iter: 1023 loss: 3.00872017e-07
Iter: 1024 loss: 3.00681279e-07
Iter: 1025 loss: 3.01230273e-07
Iter: 1026 loss: 3.00624237e-07
Iter: 1027 loss: 3.00436199e-07
Iter: 1028 loss: 3.03443954e-07
Iter: 1029 loss: 3.00434806e-07
Iter: 1030 loss: 3.00325553e-07
Iter: 1031 loss: 3.00343146e-07
Iter: 1032 loss: 3.00262229e-07
Iter: 1033 loss: 3.00192369e-07
Iter: 1034 loss: 3.00188503e-07
Iter: 1035 loss: 3.00102158e-07
Iter: 1036 loss: 2.99894566e-07
Iter: 1037 loss: 3.01207479e-07
Iter: 1038 loss: 2.99832607e-07
Iter: 1039 loss: 2.9971e-07
Iter: 1040 loss: 2.99714145e-07
Iter: 1041 loss: 2.99583178e-07
Iter: 1042 loss: 3.00283261e-07
Iter: 1043 loss: 2.99563908e-07
Iter: 1044 loss: 2.99470571e-07
Iter: 1045 loss: 2.99241208e-07
Iter: 1046 loss: 3.01492463e-07
Iter: 1047 loss: 2.99219835e-07
Iter: 1048 loss: 2.9903839e-07
Iter: 1049 loss: 2.99042199e-07
Iter: 1050 loss: 2.98935021e-07
Iter: 1051 loss: 3.00384869e-07
Iter: 1052 loss: 2.98930956e-07
Iter: 1053 loss: 2.9884589e-07
Iter: 1054 loss: 2.9863449e-07
Iter: 1055 loss: 3.00393197e-07
Iter: 1056 loss: 2.98616442e-07
Iter: 1057 loss: 2.98421554e-07
Iter: 1058 loss: 2.99418446e-07
Iter: 1059 loss: 2.9839444e-07
Iter: 1060 loss: 2.98244316e-07
Iter: 1061 loss: 2.99244959e-07
Iter: 1062 loss: 2.98229679e-07
Iter: 1063 loss: 2.98078078e-07
Iter: 1064 loss: 2.99372431e-07
Iter: 1065 loss: 2.98079669e-07
Iter: 1066 loss: 2.98002391e-07
Iter: 1067 loss: 2.97851813e-07
Iter: 1068 loss: 2.97855706e-07
Iter: 1069 loss: 2.9777496e-07
Iter: 1070 loss: 2.97778513e-07
Iter: 1071 loss: 2.976914e-07
Iter: 1072 loss: 2.97539913e-07
Iter: 1073 loss: 3.00687674e-07
Iter: 1074 loss: 2.97533177e-07
Iter: 1075 loss: 2.97405904e-07
Iter: 1076 loss: 2.97467267e-07
Iter: 1077 loss: 2.97314699e-07
Iter: 1078 loss: 2.97229121e-07
Iter: 1079 loss: 2.97215308e-07
Iter: 1080 loss: 2.97110233e-07
Iter: 1081 loss: 2.97017806e-07
Iter: 1082 loss: 2.96982932e-07
Iter: 1083 loss: 2.9687385e-07
Iter: 1084 loss: 2.97015276e-07
Iter: 1085 loss: 2.96783185e-07
Iter: 1086 loss: 2.96651024e-07
Iter: 1087 loss: 2.98078106e-07
Iter: 1088 loss: 2.96651933e-07
Iter: 1089 loss: 2.9649874e-07
Iter: 1090 loss: 2.96400543e-07
Iter: 1091 loss: 2.96350635e-07
Iter: 1092 loss: 2.96190706e-07
Iter: 1093 loss: 2.96264886e-07
Iter: 1094 loss: 2.96079975e-07
Iter: 1095 loss: 2.95874202e-07
Iter: 1096 loss: 2.96831502e-07
Iter: 1097 loss: 2.95822076e-07
Iter: 1098 loss: 2.95771713e-07
Iter: 1099 loss: 2.95741586e-07
Iter: 1100 loss: 2.95673317e-07
Iter: 1101 loss: 2.95551047e-07
Iter: 1102 loss: 2.95552951e-07
Iter: 1103 loss: 2.95459358e-07
Iter: 1104 loss: 2.96673448e-07
Iter: 1105 loss: 2.95460666e-07
Iter: 1106 loss: 2.95362895e-07
Iter: 1107 loss: 2.95439293e-07
Iter: 1108 loss: 2.95287379e-07
Iter: 1109 loss: 2.95216864e-07
Iter: 1110 loss: 2.95199584e-07
Iter: 1111 loss: 2.95164369e-07
Iter: 1112 loss: 2.95045083e-07
Iter: 1113 loss: 2.96072244e-07
Iter: 1114 loss: 2.95037182e-07
Iter: 1115 loss: 2.94960131e-07
Iter: 1116 loss: 2.94846217e-07
Iter: 1117 loss: 2.94852384e-07
Iter: 1118 loss: 2.94754898e-07
Iter: 1119 loss: 2.95335752e-07
Iter: 1120 loss: 2.94742733e-07
Iter: 1121 loss: 2.94602302e-07
Iter: 1122 loss: 2.95010636e-07
Iter: 1123 loss: 2.94563e-07
Iter: 1124 loss: 2.94461472e-07
Iter: 1125 loss: 2.94371262e-07
Iter: 1126 loss: 2.94348268e-07
Iter: 1127 loss: 2.94202437e-07
Iter: 1128 loss: 2.94716187e-07
Iter: 1129 loss: 2.94157019e-07
Iter: 1130 loss: 2.94021277e-07
Iter: 1131 loss: 2.94345483e-07
Iter: 1132 loss: 2.93974267e-07
Iter: 1133 loss: 2.93891048e-07
Iter: 1134 loss: 2.93869164e-07
Iter: 1135 loss: 2.93821984e-07
Iter: 1136 loss: 2.93688515e-07
Iter: 1137 loss: 2.95476468e-07
Iter: 1138 loss: 2.93680245e-07
Iter: 1139 loss: 2.93635225e-07
Iter: 1140 loss: 2.93619678e-07
Iter: 1141 loss: 2.93555217e-07
Iter: 1142 loss: 2.93449546e-07
Iter: 1143 loss: 2.93463245e-07
Iter: 1144 loss: 2.93374541e-07
Iter: 1145 loss: 2.94181405e-07
Iter: 1146 loss: 2.93372523e-07
Iter: 1147 loss: 2.93298939e-07
Iter: 1148 loss: 2.93209837e-07
Iter: 1149 loss: 2.93193835e-07
Iter: 1150 loss: 2.93066535e-07
Iter: 1151 loss: 2.93338985e-07
Iter: 1152 loss: 2.92997413e-07
Iter: 1153 loss: 2.92914677e-07
Iter: 1154 loss: 2.93225725e-07
Iter: 1155 loss: 2.92876734e-07
Iter: 1156 loss: 2.92753697e-07
Iter: 1157 loss: 2.92694551e-07
Iter: 1158 loss: 2.92629551e-07
Iter: 1159 loss: 2.92460413e-07
Iter: 1160 loss: 2.92891116e-07
Iter: 1161 loss: 2.92398e-07
Iter: 1162 loss: 2.92256772e-07
Iter: 1163 loss: 2.92745057e-07
Iter: 1164 loss: 2.92217464e-07
Iter: 1165 loss: 2.92115203e-07
Iter: 1166 loss: 2.92899813e-07
Iter: 1167 loss: 2.92096104e-07
Iter: 1168 loss: 2.91986e-07
Iter: 1169 loss: 2.92712258e-07
Iter: 1170 loss: 2.91961101e-07
Iter: 1171 loss: 2.91906304e-07
Iter: 1172 loss: 2.91866314e-07
Iter: 1173 loss: 2.9183866e-07
Iter: 1174 loss: 2.91741173e-07
Iter: 1175 loss: 2.92510236e-07
Iter: 1176 loss: 2.91724746e-07
Iter: 1177 loss: 2.91633683e-07
Iter: 1178 loss: 2.91514084e-07
Iter: 1179 loss: 2.91512691e-07
Iter: 1180 loss: 2.91453375e-07
Iter: 1181 loss: 2.91437686e-07
Iter: 1182 loss: 2.91368536e-07
Iter: 1183 loss: 2.91234386e-07
Iter: 1184 loss: 2.92790162e-07
Iter: 1185 loss: 2.9122333e-07
Iter: 1186 loss: 2.91100548e-07
Iter: 1187 loss: 2.9109745e-07
Iter: 1188 loss: 2.91012157e-07
Iter: 1189 loss: 2.90983365e-07
Iter: 1190 loss: 2.9092422e-07
Iter: 1191 loss: 2.90787398e-07
Iter: 1192 loss: 2.91052231e-07
Iter: 1193 loss: 2.90750108e-07
Iter: 1194 loss: 2.90618175e-07
Iter: 1195 loss: 2.90603879e-07
Iter: 1196 loss: 2.90505113e-07
Iter: 1197 loss: 2.90424396e-07
Iter: 1198 loss: 2.90403733e-07
Iter: 1199 loss: 2.90350442e-07
Iter: 1200 loss: 2.90892757e-07
Iter: 1201 loss: 2.90348055e-07
Iter: 1202 loss: 2.90270464e-07
Iter: 1203 loss: 2.90135773e-07
Iter: 1204 loss: 2.9013836e-07
Iter: 1205 loss: 2.90082966e-07
Iter: 1206 loss: 2.90060086e-07
Iter: 1207 loss: 2.90000742e-07
Iter: 1208 loss: 2.89937418e-07
Iter: 1209 loss: 2.89925367e-07
Iter: 1210 loss: 2.89830155e-07
Iter: 1211 loss: 2.90203559e-07
Iter: 1212 loss: 2.89819809e-07
Iter: 1213 loss: 2.89701717e-07
Iter: 1214 loss: 2.89905415e-07
Iter: 1215 loss: 2.89656896e-07
Iter: 1216 loss: 2.89574047e-07
Iter: 1217 loss: 2.89707486e-07
Iter: 1218 loss: 2.89532e-07
Iter: 1219 loss: 2.89435604e-07
Iter: 1220 loss: 2.90004181e-07
Iter: 1221 loss: 2.89423213e-07
Iter: 1222 loss: 2.89346531e-07
Iter: 1223 loss: 2.89203086e-07
Iter: 1224 loss: 2.92766e-07
Iter: 1225 loss: 2.89201154e-07
Iter: 1226 loss: 2.89042e-07
Iter: 1227 loss: 2.89621426e-07
Iter: 1228 loss: 2.88985461e-07
Iter: 1229 loss: 2.88837867e-07
Iter: 1230 loss: 2.8913459e-07
Iter: 1231 loss: 2.88774345e-07
Iter: 1232 loss: 2.88711789e-07
Iter: 1233 loss: 2.88690359e-07
Iter: 1234 loss: 2.88626438e-07
Iter: 1235 loss: 2.88578661e-07
Iter: 1236 loss: 2.88557885e-07
Iter: 1237 loss: 2.88465287e-07
Iter: 1238 loss: 2.88592958e-07
Iter: 1239 loss: 2.88422427e-07
Iter: 1240 loss: 2.8832784e-07
Iter: 1241 loss: 2.8953886e-07
Iter: 1242 loss: 2.88321644e-07
Iter: 1243 loss: 2.88254284e-07
Iter: 1244 loss: 2.88148215e-07
Iter: 1245 loss: 2.88143781e-07
Iter: 1246 loss: 2.88034244e-07
Iter: 1247 loss: 2.88033561e-07
Iter: 1248 loss: 2.8794048e-07
Iter: 1249 loss: 2.8781642e-07
Iter: 1250 loss: 2.87819773e-07
Iter: 1251 loss: 2.8770944e-07
Iter: 1252 loss: 2.88233196e-07
Iter: 1253 loss: 2.87695258e-07
Iter: 1254 loss: 2.87585578e-07
Iter: 1255 loss: 2.88178569e-07
Iter: 1256 loss: 2.87562955e-07
Iter: 1257 loss: 2.87492355e-07
Iter: 1258 loss: 2.87371449e-07
Iter: 1259 loss: 2.9029394e-07
Iter: 1260 loss: 2.87368664e-07
Iter: 1261 loss: 2.87235e-07
Iter: 1262 loss: 2.87896427e-07
Iter: 1263 loss: 2.87234911e-07
Iter: 1264 loss: 2.8713896e-07
Iter: 1265 loss: 2.87936246e-07
Iter: 1266 loss: 2.87122162e-07
Iter: 1267 loss: 2.86988694e-07
Iter: 1268 loss: 2.87396233e-07
Iter: 1269 loss: 2.86967776e-07
Iter: 1270 loss: 2.86880379e-07
Iter: 1271 loss: 2.86850593e-07
Iter: 1272 loss: 2.86790083e-07
Iter: 1273 loss: 2.86701976e-07
Iter: 1274 loss: 2.88162596e-07
Iter: 1275 loss: 2.86694e-07
Iter: 1276 loss: 2.86574021e-07
Iter: 1277 loss: 2.86475426e-07
Iter: 1278 loss: 2.86449648e-07
Iter: 1279 loss: 2.86344289e-07
Iter: 1280 loss: 2.87876134e-07
Iter: 1281 loss: 2.86349973e-07
Iter: 1282 loss: 2.86266669e-07
Iter: 1283 loss: 2.86458203e-07
Iter: 1284 loss: 2.86217755e-07
Iter: 1285 loss: 2.86161e-07
Iter: 1286 loss: 2.86061379e-07
Iter: 1287 loss: 2.86063539e-07
Iter: 1288 loss: 2.86021731e-07
Iter: 1289 loss: 2.85982452e-07
Iter: 1290 loss: 2.85940757e-07
Iter: 1291 loss: 2.85847591e-07
Iter: 1292 loss: 2.8584634e-07
Iter: 1293 loss: 2.85770341e-07
Iter: 1294 loss: 2.85657961e-07
Iter: 1295 loss: 2.85649065e-07
Iter: 1296 loss: 2.85521025e-07
Iter: 1297 loss: 2.85407367e-07
Iter: 1298 loss: 2.85372096e-07
Iter: 1299 loss: 2.85227969e-07
Iter: 1300 loss: 2.85804106e-07
Iter: 1301 loss: 2.85207392e-07
Iter: 1302 loss: 2.85113401e-07
Iter: 1303 loss: 2.85108285e-07
Iter: 1304 loss: 2.85062924e-07
Iter: 1305 loss: 2.84963e-07
Iter: 1306 loss: 2.84971236e-07
Iter: 1307 loss: 2.84854593e-07
Iter: 1308 loss: 2.85372522e-07
Iter: 1309 loss: 2.84836403e-07
Iter: 1310 loss: 2.84756453e-07
Iter: 1311 loss: 2.85806635e-07
Iter: 1312 loss: 2.84756823e-07
Iter: 1313 loss: 2.84687445e-07
Iter: 1314 loss: 2.84649673e-07
Iter: 1315 loss: 2.84617585e-07
Iter: 1316 loss: 2.84561395e-07
Iter: 1317 loss: 2.85108513e-07
Iter: 1318 loss: 2.84553039e-07
Iter: 1319 loss: 2.8446749e-07
Iter: 1320 loss: 2.84410305e-07
Iter: 1321 loss: 2.84368355e-07
Iter: 1322 loss: 2.84277576e-07
Iter: 1323 loss: 2.84452938e-07
Iter: 1324 loss: 2.84237615e-07
Iter: 1325 loss: 2.84153828e-07
Iter: 1326 loss: 2.84311966e-07
Iter: 1327 loss: 2.84108296e-07
Iter: 1328 loss: 2.84011435e-07
Iter: 1329 loss: 2.84894384e-07
Iter: 1330 loss: 2.83995178e-07
Iter: 1331 loss: 2.83933616e-07
Iter: 1332 loss: 2.83904257e-07
Iter: 1333 loss: 2.8386475e-07
Iter: 1334 loss: 2.83758425e-07
Iter: 1335 loss: 2.84421816e-07
Iter: 1336 loss: 2.83741372e-07
Iter: 1337 loss: 2.83649257e-07
Iter: 1338 loss: 2.8387592e-07
Iter: 1339 loss: 2.83628822e-07
Iter: 1340 loss: 2.83531278e-07
Iter: 1341 loss: 2.83413442e-07
Iter: 1342 loss: 2.83404347e-07
Iter: 1343 loss: 2.83365154e-07
Iter: 1344 loss: 2.83351881e-07
Iter: 1345 loss: 2.83280826e-07
Iter: 1346 loss: 2.83289069e-07
Iter: 1347 loss: 2.83234e-07
Iter: 1348 loss: 2.83150484e-07
Iter: 1349 loss: 2.83199682e-07
Iter: 1350 loss: 2.83099496e-07
Iter: 1351 loss: 2.830364e-07
Iter: 1352 loss: 2.83024747e-07
Iter: 1353 loss: 2.82986292e-07
Iter: 1354 loss: 2.82915721e-07
Iter: 1355 loss: 2.82916091e-07
Iter: 1356 loss: 2.8281795e-07
Iter: 1357 loss: 2.82731037e-07
Iter: 1358 loss: 2.82726546e-07
Iter: 1359 loss: 2.8259646e-07
Iter: 1360 loss: 2.82601775e-07
Iter: 1361 loss: 2.82512588e-07
Iter: 1362 loss: 2.82856831e-07
Iter: 1363 loss: 2.82496842e-07
Iter: 1364 loss: 2.82435735e-07
Iter: 1365 loss: 2.82380427e-07
Iter: 1366 loss: 2.82366443e-07
Iter: 1367 loss: 2.82235817e-07
Iter: 1368 loss: 2.83344832e-07
Iter: 1369 loss: 2.82227688e-07
Iter: 1370 loss: 2.82149074e-07
Iter: 1371 loss: 2.82201825e-07
Iter: 1372 loss: 2.82114115e-07
Iter: 1373 loss: 2.82046017e-07
Iter: 1374 loss: 2.82032659e-07
Iter: 1375 loss: 2.81975844e-07
Iter: 1376 loss: 2.81924144e-07
Iter: 1377 loss: 2.81910133e-07
Iter: 1378 loss: 2.81859229e-07
Iter: 1379 loss: 2.81773339e-07
Iter: 1380 loss: 2.84062196e-07
Iter: 1381 loss: 2.81769616e-07
Iter: 1382 loss: 2.81697908e-07
Iter: 1383 loss: 2.827571e-07
Iter: 1384 loss: 2.8171138e-07
Iter: 1385 loss: 2.81628417e-07
Iter: 1386 loss: 2.81696458e-07
Iter: 1387 loss: 2.81586438e-07
Iter: 1388 loss: 2.81501059e-07
Iter: 1389 loss: 2.81343404e-07
Iter: 1390 loss: 2.81335474e-07
Iter: 1391 loss: 2.81194843e-07
Iter: 1392 loss: 2.81947393e-07
Iter: 1393 loss: 2.81174607e-07
Iter: 1394 loss: 2.81072971e-07
Iter: 1395 loss: 2.82580856e-07
Iter: 1396 loss: 2.81067e-07
Iter: 1397 loss: 2.80981823e-07
Iter: 1398 loss: 2.81074051e-07
Iter: 1399 loss: 2.80926713e-07
Iter: 1400 loss: 2.80873451e-07
Iter: 1401 loss: 2.81010927e-07
Iter: 1402 loss: 2.80842528e-07
Iter: 1403 loss: 2.80752062e-07
Iter: 1404 loss: 2.81227926e-07
Iter: 1405 loss: 2.80733076e-07
Iter: 1406 loss: 2.80679785e-07
Iter: 1407 loss: 2.80599494e-07
Iter: 1408 loss: 2.80604525e-07
Iter: 1409 loss: 2.80519714e-07
Iter: 1410 loss: 2.81249328e-07
Iter: 1411 loss: 2.80515394e-07
Iter: 1412 loss: 2.80438542e-07
Iter: 1413 loss: 2.80810241e-07
Iter: 1414 loss: 2.80430868e-07
Iter: 1415 loss: 2.80380021e-07
Iter: 1416 loss: 2.80257382e-07
Iter: 1417 loss: 2.82499514e-07
Iter: 1418 loss: 2.80248685e-07
Iter: 1419 loss: 2.80204716e-07
Iter: 1420 loss: 2.80174618e-07
Iter: 1421 loss: 2.80113511e-07
Iter: 1422 loss: 2.80054e-07
Iter: 1423 loss: 2.80038535e-07
Iter: 1424 loss: 2.79956453e-07
Iter: 1425 loss: 2.80030065e-07
Iter: 1426 loss: 2.7990319e-07
Iter: 1427 loss: 2.79809768e-07
Iter: 1428 loss: 2.79956311e-07
Iter: 1429 loss: 2.79750566e-07
Iter: 1430 loss: 2.7965217e-07
Iter: 1431 loss: 2.80237657e-07
Iter: 1432 loss: 2.79643643e-07
Iter: 1433 loss: 2.79542093e-07
Iter: 1434 loss: 2.79982601e-07
Iter: 1435 loss: 2.79503638e-07
Iter: 1436 loss: 2.79438183e-07
Iter: 1437 loss: 2.79417179e-07
Iter: 1438 loss: 2.79393049e-07
Iter: 1439 loss: 2.79297979e-07
Iter: 1440 loss: 2.8010129e-07
Iter: 1441 loss: 2.79298661e-07
Iter: 1442 loss: 2.79189237e-07
Iter: 1443 loss: 2.79224963e-07
Iter: 1444 loss: 2.79122702e-07
Iter: 1445 loss: 2.79064238e-07
Iter: 1446 loss: 2.79156723e-07
Iter: 1447 loss: 2.79023311e-07
Iter: 1448 loss: 2.78933641e-07
Iter: 1449 loss: 2.79480332e-07
Iter: 1450 loss: 2.78909198e-07
Iter: 1451 loss: 2.78842379e-07
Iter: 1452 loss: 2.78741766e-07
Iter: 1453 loss: 2.7873179e-07
Iter: 1454 loss: 2.78626146e-07
Iter: 1455 loss: 2.79527626e-07
Iter: 1456 loss: 2.78625237e-07
Iter: 1457 loss: 2.78558218e-07
Iter: 1458 loss: 2.7854577e-07
Iter: 1459 loss: 2.78508253e-07
Iter: 1460 loss: 2.78408038e-07
Iter: 1461 loss: 2.79097208e-07
Iter: 1462 loss: 2.78382515e-07
Iter: 1463 loss: 2.78267578e-07
Iter: 1464 loss: 2.79460068e-07
Iter: 1465 loss: 2.78279e-07
Iter: 1466 loss: 2.78208347e-07
Iter: 1467 loss: 2.78722894e-07
Iter: 1468 loss: 2.7819695e-07
Iter: 1469 loss: 2.78132347e-07
Iter: 1470 loss: 2.78174724e-07
Iter: 1471 loss: 2.78089828e-07
Iter: 1472 loss: 2.78015364e-07
Iter: 1473 loss: 2.77933509e-07
Iter: 1474 loss: 2.77921572e-07
Iter: 1475 loss: 2.77837785e-07
Iter: 1476 loss: 2.7784435e-07
Iter: 1477 loss: 2.77744846e-07
Iter: 1478 loss: 2.77789411e-07
Iter: 1479 loss: 2.77688031e-07
Iter: 1480 loss: 2.77610241e-07
Iter: 1481 loss: 2.77798165e-07
Iter: 1482 loss: 2.77582558e-07
Iter: 1483 loss: 2.77498174e-07
Iter: 1484 loss: 2.78083803e-07
Iter: 1485 loss: 2.77470122e-07
Iter: 1486 loss: 2.7740947e-07
Iter: 1487 loss: 2.77299648e-07
Iter: 1488 loss: 2.79131939e-07
Iter: 1489 loss: 2.7729925e-07
Iter: 1490 loss: 2.77247182e-07
Iter: 1491 loss: 2.77225553e-07
Iter: 1492 loss: 2.77167032e-07
Iter: 1493 loss: 2.77166947e-07
Iter: 1494 loss: 2.77105215e-07
Iter: 1495 loss: 2.77057609e-07
Iter: 1496 loss: 2.76975925e-07
Iter: 1497 loss: 2.76972372e-07
Iter: 1498 loss: 2.76880314e-07
Iter: 1499 loss: 2.77901677e-07
Iter: 1500 loss: 2.76874971e-07
Iter: 1501 loss: 2.76803e-07
Iter: 1502 loss: 2.7705309e-07
Iter: 1503 loss: 2.76795902e-07
Iter: 1504 loss: 2.76754065e-07
Iter: 1505 loss: 2.76709613e-07
Iter: 1506 loss: 2.76691452e-07
Iter: 1507 loss: 2.76594e-07
Iter: 1508 loss: 2.76907087e-07
Iter: 1509 loss: 2.76576515e-07
Iter: 1510 loss: 2.76501396e-07
Iter: 1511 loss: 2.76495655e-07
Iter: 1512 loss: 2.76460298e-07
Iter: 1513 loss: 2.76389557e-07
Iter: 1514 loss: 2.76396577e-07
Iter: 1515 loss: 2.76320975e-07
Iter: 1516 loss: 2.77352939e-07
Iter: 1517 loss: 2.76315177e-07
Iter: 1518 loss: 2.76251114e-07
Iter: 1519 loss: 2.76125832e-07
Iter: 1520 loss: 2.78713514e-07
Iter: 1521 loss: 2.76128731e-07
Iter: 1522 loss: 2.75996342e-07
Iter: 1523 loss: 2.76226075e-07
Iter: 1524 loss: 2.75959138e-07
Iter: 1525 loss: 2.75910907e-07
Iter: 1526 loss: 2.75879671e-07
Iter: 1527 loss: 2.75818849e-07
Iter: 1528 loss: 2.75697658e-07
Iter: 1529 loss: 2.77644176e-07
Iter: 1530 loss: 2.75684727e-07
Iter: 1531 loss: 2.75577349e-07
Iter: 1532 loss: 2.7588959e-07
Iter: 1533 loss: 2.7552349e-07
Iter: 1534 loss: 2.75450674e-07
Iter: 1535 loss: 2.76122506e-07
Iter: 1536 loss: 2.7544894e-07
Iter: 1537 loss: 2.75363504e-07
Iter: 1538 loss: 2.75597415e-07
Iter: 1539 loss: 2.75346054e-07
Iter: 1540 loss: 2.75282929e-07
Iter: 1541 loss: 2.75182686e-07
Iter: 1542 loss: 2.75177342e-07
Iter: 1543 loss: 2.7508429e-07
Iter: 1544 loss: 2.75088695e-07
Iter: 1545 loss: 2.75003288e-07
Iter: 1546 loss: 2.75112939e-07
Iter: 1547 loss: 2.7496327e-07
Iter: 1548 loss: 2.74908246e-07
Iter: 1549 loss: 2.751637e-07
Iter: 1550 loss: 2.74897701e-07
Iter: 1551 loss: 2.74825283e-07
Iter: 1552 loss: 2.74836111e-07
Iter: 1553 loss: 2.7477472e-07
Iter: 1554 loss: 2.74680502e-07
Iter: 1555 loss: 2.74658476e-07
Iter: 1556 loss: 2.74613399e-07
Iter: 1557 loss: 2.7450767e-07
Iter: 1558 loss: 2.75281025e-07
Iter: 1559 loss: 2.74519607e-07
Iter: 1560 loss: 2.74440794e-07
Iter: 1561 loss: 2.75381637e-07
Iter: 1562 loss: 2.74440822e-07
Iter: 1563 loss: 2.74394893e-07
Iter: 1564 loss: 2.74274612e-07
Iter: 1565 loss: 2.75172113e-07
Iter: 1566 loss: 2.74249743e-07
Iter: 1567 loss: 2.74100785e-07
Iter: 1568 loss: 2.74374344e-07
Iter: 1569 loss: 2.7402973e-07
Iter: 1570 loss: 2.73906e-07
Iter: 1571 loss: 2.75339573e-07
Iter: 1572 loss: 2.73890748e-07
Iter: 1573 loss: 2.73777061e-07
Iter: 1574 loss: 2.74480868e-07
Iter: 1575 loss: 2.73760776e-07
Iter: 1576 loss: 2.73691882e-07
Iter: 1577 loss: 2.73698845e-07
Iter: 1578 loss: 2.73638875e-07
Iter: 1579 loss: 2.73595504e-07
Iter: 1580 loss: 2.73595e-07
Iter: 1581 loss: 2.73527689e-07
Iter: 1582 loss: 2.73468657e-07
Iter: 1583 loss: 2.73466128e-07
Iter: 1584 loss: 2.73415424e-07
Iter: 1585 loss: 2.74176443e-07
Iter: 1586 loss: 2.73421477e-07
Iter: 1587 loss: 2.73347439e-07
Iter: 1588 loss: 2.73279454e-07
Iter: 1589 loss: 2.73285451e-07
Iter: 1590 loss: 2.73209196e-07
Iter: 1591 loss: 2.7318643e-07
Iter: 1592 loss: 2.73133651e-07
Iter: 1593 loss: 2.73043497e-07
Iter: 1594 loss: 2.74355614e-07
Iter: 1595 loss: 2.73045885e-07
Iter: 1596 loss: 2.72961245e-07
Iter: 1597 loss: 2.73405448e-07
Iter: 1598 loss: 2.72952718e-07
Iter: 1599 loss: 2.72941605e-07
Iter: 1600 loss: 2.72834768e-07
Iter: 1601 loss: 2.73951343e-07
Iter: 1602 loss: 2.72834427e-07
Iter: 1603 loss: 2.72748935e-07
Iter: 1604 loss: 2.73253306e-07
Iter: 1605 loss: 2.72734837e-07
Iter: 1606 loss: 2.7265574e-07
Iter: 1607 loss: 2.72655683e-07
Iter: 1608 loss: 2.72588863e-07
Iter: 1609 loss: 2.72511471e-07
Iter: 1610 loss: 2.72494532e-07
Iter: 1611 loss: 2.72396392e-07
Iter: 1612 loss: 2.72628938e-07
Iter: 1613 loss: 2.72360182e-07
Iter: 1614 loss: 2.72301122e-07
Iter: 1615 loss: 2.72289356e-07
Iter: 1616 loss: 2.72231944e-07
Iter: 1617 loss: 2.72129341e-07
Iter: 1618 loss: 2.74216717e-07
Iter: 1619 loss: 2.72122918e-07
Iter: 1620 loss: 2.7205823e-07
Iter: 1621 loss: 2.72055729e-07
Iter: 1622 loss: 2.72002751e-07
Iter: 1623 loss: 2.7195253e-07
Iter: 1624 loss: 2.71944401e-07
Iter: 1625 loss: 2.7187383e-07
Iter: 1626 loss: 2.71963074e-07
Iter: 1627 loss: 2.71827957e-07
Iter: 1628 loss: 2.71817157e-07
Iter: 1629 loss: 2.71797745e-07
Iter: 1630 loss: 2.71762104e-07
Iter: 1631 loss: 2.7168889e-07
Iter: 1632 loss: 2.72365071e-07
Iter: 1633 loss: 2.71673287e-07
Iter: 1634 loss: 2.71582905e-07
Iter: 1635 loss: 2.71670814e-07
Iter: 1636 loss: 2.71524073e-07
Iter: 1637 loss: 2.71458191e-07
Iter: 1638 loss: 2.72174901e-07
Iter: 1639 loss: 2.71427695e-07
Iter: 1640 loss: 2.71347631e-07
Iter: 1641 loss: 2.72252e-07
Iter: 1642 loss: 2.71360278e-07
Iter: 1643 loss: 2.71305453e-07
Iter: 1644 loss: 2.71217175e-07
Iter: 1645 loss: 2.71226327e-07
Iter: 1646 loss: 2.71169625e-07
Iter: 1647 loss: 2.72124e-07
Iter: 1648 loss: 2.71159934e-07
Iter: 1649 loss: 2.71086e-07
Iter: 1650 loss: 2.712992e-07
Iter: 1651 loss: 2.71069894e-07
Iter: 1652 loss: 2.71021975e-07
Iter: 1653 loss: 2.71100077e-07
Iter: 1654 loss: 2.70998896e-07
Iter: 1655 loss: 2.70936255e-07
Iter: 1656 loss: 2.71052727e-07
Iter: 1657 loss: 2.70903286e-07
Iter: 1658 loss: 2.7082308e-07
Iter: 1659 loss: 2.70775786e-07
Iter: 1660 loss: 2.70758392e-07
Iter: 1661 loss: 2.70652777e-07
Iter: 1662 loss: 2.7099054e-07
Iter: 1663 loss: 2.7062103e-07
Iter: 1664 loss: 2.70520218e-07
Iter: 1665 loss: 2.71482179e-07
Iter: 1666 loss: 2.70509219e-07
Iter: 1667 loss: 2.70466529e-07
Iter: 1668 loss: 2.7038385e-07
Iter: 1669 loss: 2.70381264e-07
Iter: 1670 loss: 2.7027636e-07
Iter: 1671 loss: 2.70408293e-07
Iter: 1672 loss: 2.70235034e-07
Iter: 1673 loss: 2.70151759e-07
Iter: 1674 loss: 2.70163639e-07
Iter: 1675 loss: 2.70090368e-07
Iter: 1676 loss: 2.70153464e-07
Iter: 1677 loss: 2.70072576e-07
Iter: 1678 loss: 2.70022866e-07
Iter: 1679 loss: 2.69932741e-07
Iter: 1680 loss: 2.69922651e-07
Iter: 1681 loss: 2.69861573e-07
Iter: 1682 loss: 2.69859328e-07
Iter: 1683 loss: 2.69797368e-07
Iter: 1684 loss: 2.69711819e-07
Iter: 1685 loss: 2.69712274e-07
Iter: 1686 loss: 2.69613423e-07
Iter: 1687 loss: 2.70807817e-07
Iter: 1688 loss: 2.69625332e-07
Iter: 1689 loss: 2.69560076e-07
Iter: 1690 loss: 2.69479017e-07
Iter: 1691 loss: 2.69463385e-07
Iter: 1692 loss: 2.69376585e-07
Iter: 1693 loss: 2.69866575e-07
Iter: 1694 loss: 2.6936948e-07
Iter: 1695 loss: 2.69323891e-07
Iter: 1696 loss: 2.69713553e-07
Iter: 1697 loss: 2.69316359e-07
Iter: 1698 loss: 2.6927728e-07
Iter: 1699 loss: 2.69193265e-07
Iter: 1700 loss: 2.71099168e-07
Iter: 1701 loss: 2.69188803e-07
Iter: 1702 loss: 2.6910476e-07
Iter: 1703 loss: 2.69330172e-07
Iter: 1704 loss: 2.69093391e-07
Iter: 1705 loss: 2.69020916e-07
Iter: 1706 loss: 2.69042e-07
Iter: 1707 loss: 2.68975839e-07
Iter: 1708 loss: 2.68903307e-07
Iter: 1709 loss: 2.68913681e-07
Iter: 1710 loss: 2.6886039e-07
Iter: 1711 loss: 2.68737722e-07
Iter: 1712 loss: 2.7007431e-07
Iter: 1713 loss: 2.68734368e-07
Iter: 1714 loss: 2.68641259e-07
Iter: 1715 loss: 2.68647625e-07
Iter: 1716 loss: 2.68564378e-07
Iter: 1717 loss: 2.68806616e-07
Iter: 1718 loss: 2.68550025e-07
Iter: 1719 loss: 2.6850563e-07
Iter: 1720 loss: 2.68563269e-07
Iter: 1721 loss: 2.68476356e-07
Iter: 1722 loss: 2.68427357e-07
Iter: 1723 loss: 2.68789961e-07
Iter: 1724 loss: 2.68419626e-07
Iter: 1725 loss: 2.68369064e-07
Iter: 1726 loss: 2.68327398e-07
Iter: 1727 loss: 2.68321571e-07
Iter: 1728 loss: 2.68290648e-07
Iter: 1729 loss: 2.68292808e-07
Iter: 1730 loss: 2.68260862e-07
Iter: 1731 loss: 2.68222607e-07
Iter: 1732 loss: 2.68225904e-07
Iter: 1733 loss: 2.68176308e-07
Iter: 1734 loss: 2.68173494e-07
Iter: 1735 loss: 2.68128161e-07
Iter: 1736 loss: 2.68059523e-07
Iter: 1737 loss: 2.68038747e-07
Iter: 1738 loss: 2.67986707e-07
Iter: 1739 loss: 2.67905136e-07
Iter: 1740 loss: 2.68849789e-07
Iter: 1741 loss: 2.67908547e-07
Iter: 1742 loss: 2.67808673e-07
Iter: 1743 loss: 2.67924662e-07
Iter: 1744 loss: 2.67760299e-07
Iter: 1745 loss: 2.67693508e-07
Iter: 1746 loss: 2.6785608e-07
Iter: 1747 loss: 2.67667417e-07
Iter: 1748 loss: 2.67622625e-07
Iter: 1749 loss: 2.67614922e-07
Iter: 1750 loss: 2.67572801e-07
Iter: 1751 loss: 2.67471904e-07
Iter: 1752 loss: 2.68548376e-07
Iter: 1753 loss: 2.67462383e-07
Iter: 1754 loss: 2.67409547e-07
Iter: 1755 loss: 2.67392068e-07
Iter: 1756 loss: 2.67348071e-07
Iter: 1757 loss: 2.67272384e-07
Iter: 1758 loss: 2.67281337e-07
Iter: 1759 loss: 2.67200761e-07
Iter: 1760 loss: 2.67294126e-07
Iter: 1761 loss: 2.67155315e-07
Iter: 1762 loss: 2.67064451e-07
Iter: 1763 loss: 2.67076189e-07
Iter: 1764 loss: 2.67038899e-07
Iter: 1765 loss: 2.66975121e-07
Iter: 1766 loss: 2.6698396e-07
Iter: 1767 loss: 2.66898809e-07
Iter: 1768 loss: 2.67045664e-07
Iter: 1769 loss: 2.66873428e-07
Iter: 1770 loss: 2.66797826e-07
Iter: 1771 loss: 2.66918931e-07
Iter: 1772 loss: 2.6676264e-07
Iter: 1773 loss: 2.66681e-07
Iter: 1774 loss: 2.6684549e-07
Iter: 1775 loss: 2.66641337e-07
Iter: 1776 loss: 2.66592934e-07
Iter: 1777 loss: 2.66587392e-07
Iter: 1778 loss: 2.66536716e-07
Iter: 1779 loss: 2.66475183e-07
Iter: 1780 loss: 2.66462308e-07
Iter: 1781 loss: 2.66377469e-07
Iter: 1782 loss: 2.66343363e-07
Iter: 1783 loss: 2.66306756e-07
Iter: 1784 loss: 2.66300674e-07
Iter: 1785 loss: 2.66248719e-07
Iter: 1786 loss: 2.66204239e-07
Iter: 1787 loss: 2.66160214e-07
Iter: 1788 loss: 2.66149499e-07
Iter: 1789 loss: 2.66081713e-07
Iter: 1790 loss: 2.660733e-07
Iter: 1791 loss: 2.66028479e-07
Iter: 1792 loss: 2.65970243e-07
Iter: 1793 loss: 2.65963621e-07
Iter: 1794 loss: 2.65928549e-07
Iter: 1795 loss: 2.65860251e-07
Iter: 1796 loss: 2.67472416e-07
Iter: 1797 loss: 2.65860194e-07
Iter: 1798 loss: 2.65799912e-07
Iter: 1799 loss: 2.65784081e-07
Iter: 1800 loss: 2.65730051e-07
Iter: 1801 loss: 2.65659935e-07
Iter: 1802 loss: 2.65662493e-07
Iter: 1803 loss: 2.65605706e-07
Iter: 1804 loss: 2.65940827e-07
Iter: 1805 loss: 2.65601102e-07
Iter: 1806 loss: 2.65576602e-07
Iter: 1807 loss: 2.65514643e-07
Iter: 1808 loss: 2.65513336e-07
Iter: 1809 loss: 2.65445777e-07
Iter: 1810 loss: 2.65965e-07
Iter: 1811 loss: 2.65443191e-07
Iter: 1812 loss: 2.65390327e-07
Iter: 1813 loss: 2.65526666e-07
Iter: 1814 loss: 2.65379043e-07
Iter: 1815 loss: 2.65324218e-07
Iter: 1816 loss: 2.65301367e-07
Iter: 1817 loss: 2.65294773e-07
Iter: 1818 loss: 2.65251629e-07
Iter: 1819 loss: 2.65239521e-07
Iter: 1820 loss: 2.65204505e-07
Iter: 1821 loss: 2.65142063e-07
Iter: 1822 loss: 2.66603138e-07
Iter: 1823 loss: 2.65152352e-07
Iter: 1824 loss: 2.65069446e-07
Iter: 1825 loss: 2.65075613e-07
Iter: 1826 loss: 2.64998107e-07
Iter: 1827 loss: 2.64927507e-07
Iter: 1828 loss: 2.64933135e-07
Iter: 1829 loss: 2.6490207e-07
Iter: 1830 loss: 2.64834171e-07
Iter: 1831 loss: 2.64828714e-07
Iter: 1832 loss: 2.64765845e-07
Iter: 1833 loss: 2.64828572e-07
Iter: 1834 loss: 2.64735831e-07
Iter: 1835 loss: 2.64651476e-07
Iter: 1836 loss: 2.65060976e-07
Iter: 1837 loss: 2.64655114e-07
Iter: 1838 loss: 2.64597475e-07
Iter: 1839 loss: 2.64791623e-07
Iter: 1840 loss: 2.64583775e-07
Iter: 1841 loss: 2.64559674e-07
Iter: 1842 loss: 2.64634906e-07
Iter: 1843 loss: 2.64526591e-07
Iter: 1844 loss: 2.64501693e-07
Iter: 1845 loss: 2.64452808e-07
Iter: 1846 loss: 2.64438796e-07
Iter: 1847 loss: 2.64345658e-07
Iter: 1848 loss: 2.64438825e-07
Iter: 1849 loss: 2.64302145e-07
Iter: 1850 loss: 2.64217704e-07
Iter: 1851 loss: 2.64224354e-07
Iter: 1852 loss: 2.64165635e-07
Iter: 1853 loss: 2.64164555e-07
Iter: 1854 loss: 2.64112543e-07
Iter: 1855 loss: 2.64037737e-07
Iter: 1856 loss: 2.64025402e-07
Iter: 1857 loss: 2.63977938e-07
Iter: 1858 loss: 2.63900802e-07
Iter: 1859 loss: 2.64742539e-07
Iter: 1860 loss: 2.639012e-07
Iter: 1861 loss: 2.63832703e-07
Iter: 1862 loss: 2.63871e-07
Iter: 1863 loss: 2.63768726e-07
Iter: 1864 loss: 2.63694744e-07
Iter: 1865 loss: 2.63838729e-07
Iter: 1866 loss: 2.63667289e-07
Iter: 1867 loss: 2.63632785e-07
Iter: 1868 loss: 2.64294442e-07
Iter: 1869 loss: 2.63625765e-07
Iter: 1870 loss: 2.63575942e-07
Iter: 1871 loss: 2.63592796e-07
Iter: 1872 loss: 2.63544564e-07
Iter: 1873 loss: 2.63488658e-07
Iter: 1874 loss: 2.63662827e-07
Iter: 1875 loss: 2.6345063e-07
Iter: 1876 loss: 2.63391485e-07
Iter: 1877 loss: 2.63492382e-07
Iter: 1878 loss: 2.63371703e-07
Iter: 1879 loss: 2.63315343e-07
Iter: 1880 loss: 2.63396089e-07
Iter: 1881 loss: 2.63292463e-07
Iter: 1882 loss: 2.63221239e-07
Iter: 1883 loss: 2.63325376e-07
Iter: 1884 loss: 2.6317926e-07
Iter: 1885 loss: 2.63141715e-07
Iter: 1886 loss: 2.63135234e-07
Iter: 1887 loss: 2.63118864e-07
Iter: 1888 loss: 2.63057814e-07
Iter: 1889 loss: 2.63791321e-07
Iter: 1890 loss: 2.63052357e-07
Iter: 1891 loss: 2.62975902e-07
Iter: 1892 loss: 2.63099537e-07
Iter: 1893 loss: 2.62929177e-07
Iter: 1894 loss: 2.62879666e-07
Iter: 1895 loss: 2.62870202e-07
Iter: 1896 loss: 2.62849e-07
Iter: 1897 loss: 2.628083e-07
Iter: 1898 loss: 2.62811e-07
Iter: 1899 loss: 2.62760182e-07
Iter: 1900 loss: 2.63093796e-07
Iter: 1901 loss: 2.62755236e-07
Iter: 1902 loss: 2.62705271e-07
Iter: 1903 loss: 2.62868866e-07
Iter: 1904 loss: 2.62688843e-07
Iter: 1905 loss: 2.62650985e-07
Iter: 1906 loss: 2.6276274e-07
Iter: 1907 loss: 2.62634757e-07
Iter: 1908 loss: 2.62587605e-07
Iter: 1909 loss: 2.62623132e-07
Iter: 1910 loss: 2.62567568e-07
Iter: 1911 loss: 2.62510866e-07
Iter: 1912 loss: 2.62576691e-07
Iter: 1913 loss: 2.62477e-07
Iter: 1914 loss: 2.62410282e-07
Iter: 1915 loss: 2.62581665e-07
Iter: 1916 loss: 2.62399112e-07
Iter: 1917 loss: 2.62357901e-07
Iter: 1918 loss: 2.62355059e-07
Iter: 1919 loss: 2.62321691e-07
Iter: 1920 loss: 2.62243475e-07
Iter: 1921 loss: 2.62235858e-07
Iter: 1922 loss: 2.62175178e-07
Iter: 1923 loss: 2.62369895e-07
Iter: 1924 loss: 2.62149882e-07
Iter: 1925 loss: 2.62122938e-07
Iter: 1926 loss: 2.62118192e-07
Iter: 1927 loss: 2.6208653e-07
Iter: 1928 loss: 2.62039634e-07
Iter: 1929 loss: 2.63107722e-07
Iter: 1930 loss: 2.62040686e-07
Iter: 1931 loss: 2.61978755e-07
Iter: 1932 loss: 2.62309442e-07
Iter: 1933 loss: 2.61969547e-07
Iter: 1934 loss: 2.61910714e-07
Iter: 1935 loss: 2.62045575e-07
Iter: 1936 loss: 2.61899515e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi0.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.2
+ date
Mon Oct 26 15:39:28 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.2/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.2_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 1 --phi 1.2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.2_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad43a4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad43a4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad438f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad442d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad445a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad445a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad42d68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad425b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad425b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad425fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad4206950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad420a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad420a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad41c4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad41c4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad4143b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad4183488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad4183840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad410d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad40ae620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad40a9378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad407aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad403f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad4055950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ad4055a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac07c6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac0789840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac0756620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac0761620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac0756598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac071eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac06f7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac06cf378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac0680840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac069fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7ac069f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.33458478e-06
Iter: 2 loss: 4.4267731e-06
Iter: 3 loss: 3.85194198e-06
Iter: 4 loss: 3.31621459e-06
Iter: 5 loss: 3.39840653e-06
Iter: 6 loss: 2.91164724e-06
Iter: 7 loss: 2.6322673e-06
Iter: 8 loss: 3.07520031e-06
Iter: 9 loss: 2.50233938e-06
Iter: 10 loss: 2.33903506e-06
Iter: 11 loss: 2.32860862e-06
Iter: 12 loss: 2.23264851e-06
Iter: 13 loss: 2.14322336e-06
Iter: 14 loss: 2.1205783e-06
Iter: 15 loss: 2.07956145e-06
Iter: 16 loss: 2.07195217e-06
Iter: 17 loss: 2.03916943e-06
Iter: 18 loss: 2.01718058e-06
Iter: 19 loss: 2.00499971e-06
Iter: 20 loss: 1.95911298e-06
Iter: 21 loss: 1.96933911e-06
Iter: 22 loss: 1.92531024e-06
Iter: 23 loss: 1.86861052e-06
Iter: 24 loss: 2.12210284e-06
Iter: 25 loss: 1.8574641e-06
Iter: 26 loss: 1.79798212e-06
Iter: 27 loss: 2.24442624e-06
Iter: 28 loss: 1.79330254e-06
Iter: 29 loss: 1.76347055e-06
Iter: 30 loss: 1.70971089e-06
Iter: 31 loss: 3.01896057e-06
Iter: 32 loss: 1.70971839e-06
Iter: 33 loss: 1.66116638e-06
Iter: 34 loss: 2.35994958e-06
Iter: 35 loss: 1.66106304e-06
Iter: 36 loss: 1.63131108e-06
Iter: 37 loss: 2.02828664e-06
Iter: 38 loss: 1.63121649e-06
Iter: 39 loss: 1.613275e-06
Iter: 40 loss: 1.57546583e-06
Iter: 41 loss: 2.2005147e-06
Iter: 42 loss: 1.57443083e-06
Iter: 43 loss: 1.53789279e-06
Iter: 44 loss: 1.85868385e-06
Iter: 45 loss: 1.53600627e-06
Iter: 46 loss: 1.50110247e-06
Iter: 47 loss: 1.65954987e-06
Iter: 48 loss: 1.49450568e-06
Iter: 49 loss: 1.46920752e-06
Iter: 50 loss: 1.45976742e-06
Iter: 51 loss: 1.44583532e-06
Iter: 52 loss: 1.42052386e-06
Iter: 53 loss: 1.75113678e-06
Iter: 54 loss: 1.4204104e-06
Iter: 55 loss: 1.39615315e-06
Iter: 56 loss: 1.35777566e-06
Iter: 57 loss: 1.35737241e-06
Iter: 58 loss: 1.31793081e-06
Iter: 59 loss: 1.4307476e-06
Iter: 60 loss: 1.30558442e-06
Iter: 61 loss: 1.27468797e-06
Iter: 62 loss: 1.63841798e-06
Iter: 63 loss: 1.27431542e-06
Iter: 64 loss: 1.25186943e-06
Iter: 65 loss: 1.3960863e-06
Iter: 66 loss: 1.2494379e-06
Iter: 67 loss: 1.23687437e-06
Iter: 68 loss: 1.21828839e-06
Iter: 69 loss: 1.21790674e-06
Iter: 70 loss: 1.19994957e-06
Iter: 71 loss: 1.27719636e-06
Iter: 72 loss: 1.19629931e-06
Iter: 73 loss: 1.18646392e-06
Iter: 74 loss: 1.18477578e-06
Iter: 75 loss: 1.17878358e-06
Iter: 76 loss: 1.16651017e-06
Iter: 77 loss: 1.38178871e-06
Iter: 78 loss: 1.16616388e-06
Iter: 79 loss: 1.14890895e-06
Iter: 80 loss: 1.14699765e-06
Iter: 81 loss: 1.13438068e-06
Iter: 82 loss: 1.11312909e-06
Iter: 83 loss: 1.11284362e-06
Iter: 84 loss: 1.10289125e-06
Iter: 85 loss: 1.08245308e-06
Iter: 86 loss: 1.44553542e-06
Iter: 87 loss: 1.08205677e-06
Iter: 88 loss: 1.05618255e-06
Iter: 89 loss: 1.2527646e-06
Iter: 90 loss: 1.05425158e-06
Iter: 91 loss: 1.03261152e-06
Iter: 92 loss: 1.09476537e-06
Iter: 93 loss: 1.0258459e-06
Iter: 94 loss: 1.01088369e-06
Iter: 95 loss: 9.96313815e-07
Iter: 96 loss: 9.93053163e-07
Iter: 97 loss: 9.76468073e-07
Iter: 98 loss: 1.23390987e-06
Iter: 99 loss: 9.76441697e-07
Iter: 100 loss: 9.6373e-07
Iter: 101 loss: 1.04794856e-06
Iter: 102 loss: 9.62433546e-07
Iter: 103 loss: 9.54243205e-07
Iter: 104 loss: 9.48559091e-07
Iter: 105 loss: 9.45629438e-07
Iter: 106 loss: 9.32945682e-07
Iter: 107 loss: 9.34682646e-07
Iter: 108 loss: 9.23330617e-07
Iter: 109 loss: 9.27754286e-07
Iter: 110 loss: 9.17185389e-07
Iter: 111 loss: 9.12732617e-07
Iter: 112 loss: 9.00263e-07
Iter: 113 loss: 9.65540494e-07
Iter: 114 loss: 8.96329311e-07
Iter: 115 loss: 8.868534e-07
Iter: 116 loss: 1.00251407e-06
Iter: 117 loss: 8.86723114e-07
Iter: 118 loss: 8.77132038e-07
Iter: 119 loss: 9.22763888e-07
Iter: 120 loss: 8.75419289e-07
Iter: 121 loss: 8.68290499e-07
Iter: 122 loss: 8.66703147e-07
Iter: 123 loss: 8.62095e-07
Iter: 124 loss: 8.56654879e-07
Iter: 125 loss: 9.13636313e-07
Iter: 126 loss: 8.5647946e-07
Iter: 127 loss: 8.49908815e-07
Iter: 128 loss: 8.38973165e-07
Iter: 129 loss: 8.38934056e-07
Iter: 130 loss: 8.27720612e-07
Iter: 131 loss: 8.51309949e-07
Iter: 132 loss: 8.23278356e-07
Iter: 133 loss: 8.15734893e-07
Iter: 134 loss: 8.15689361e-07
Iter: 135 loss: 8.07660854e-07
Iter: 136 loss: 8.00117448e-07
Iter: 137 loss: 7.9824116e-07
Iter: 138 loss: 7.84098688e-07
Iter: 139 loss: 7.91421712e-07
Iter: 140 loss: 7.74668479e-07
Iter: 141 loss: 7.65085304e-07
Iter: 142 loss: 8.72547503e-07
Iter: 143 loss: 7.64902666e-07
Iter: 144 loss: 7.5363937e-07
Iter: 145 loss: 7.84877386e-07
Iter: 146 loss: 7.4998303e-07
Iter: 147 loss: 7.45135594e-07
Iter: 148 loss: 7.40555549e-07
Iter: 149 loss: 7.39434e-07
Iter: 150 loss: 7.34428284e-07
Iter: 151 loss: 7.34345178e-07
Iter: 152 loss: 7.29923499e-07
Iter: 153 loss: 7.37535288e-07
Iter: 154 loss: 7.27987867e-07
Iter: 155 loss: 7.24606e-07
Iter: 156 loss: 7.19547927e-07
Iter: 157 loss: 7.19411162e-07
Iter: 158 loss: 7.15816e-07
Iter: 159 loss: 7.15264832e-07
Iter: 160 loss: 7.12368205e-07
Iter: 161 loss: 7.05162279e-07
Iter: 162 loss: 7.78923891e-07
Iter: 163 loss: 7.04326965e-07
Iter: 164 loss: 6.9592835e-07
Iter: 165 loss: 7.30917577e-07
Iter: 166 loss: 6.9412323e-07
Iter: 167 loss: 6.87387455e-07
Iter: 168 loss: 6.87354941e-07
Iter: 169 loss: 6.83658413e-07
Iter: 170 loss: 6.84523229e-07
Iter: 171 loss: 6.8094846e-07
Iter: 172 loss: 6.76390755e-07
Iter: 173 loss: 6.68302334e-07
Iter: 174 loss: 8.68137363e-07
Iter: 175 loss: 6.68301425e-07
Iter: 176 loss: 6.76674063e-07
Iter: 177 loss: 6.65046286e-07
Iter: 178 loss: 6.63616788e-07
Iter: 179 loss: 6.59312775e-07
Iter: 180 loss: 6.75221145e-07
Iter: 181 loss: 6.57437965e-07
Iter: 182 loss: 6.5147924e-07
Iter: 183 loss: 6.8943973e-07
Iter: 184 loss: 6.50771085e-07
Iter: 185 loss: 6.45995215e-07
Iter: 186 loss: 7.01188867e-07
Iter: 187 loss: 6.45904e-07
Iter: 188 loss: 6.42754344e-07
Iter: 189 loss: 6.38648e-07
Iter: 190 loss: 6.3835e-07
Iter: 191 loss: 6.34237381e-07
Iter: 192 loss: 6.6075205e-07
Iter: 193 loss: 6.33859258e-07
Iter: 194 loss: 6.29121814e-07
Iter: 195 loss: 6.38900474e-07
Iter: 196 loss: 6.27233192e-07
Iter: 197 loss: 6.23923029e-07
Iter: 198 loss: 6.24060931e-07
Iter: 199 loss: 6.21327672e-07
Iter: 200 loss: 6.18131935e-07
Iter: 201 loss: 6.56601742e-07
Iter: 202 loss: 6.18091747e-07
Iter: 203 loss: 6.15632302e-07
Iter: 204 loss: 6.28556393e-07
Iter: 205 loss: 6.15268732e-07
Iter: 206 loss: 6.12964357e-07
Iter: 207 loss: 6.07711627e-07
Iter: 208 loss: 6.77915295e-07
Iter: 209 loss: 6.07422692e-07
Iter: 210 loss: 6.04652314e-07
Iter: 211 loss: 6.04545733e-07
Iter: 212 loss: 6.01138368e-07
Iter: 213 loss: 6.05354444e-07
Iter: 214 loss: 5.99344276e-07
Iter: 215 loss: 5.95473409e-07
Iter: 216 loss: 5.89807087e-07
Iter: 217 loss: 5.89677711e-07
Iter: 218 loss: 5.84570557e-07
Iter: 219 loss: 6.37686469e-07
Iter: 220 loss: 5.84425322e-07
Iter: 221 loss: 5.79896e-07
Iter: 222 loss: 6.12102667e-07
Iter: 223 loss: 5.79486596e-07
Iter: 224 loss: 5.77117873e-07
Iter: 225 loss: 5.71028522e-07
Iter: 226 loss: 6.2338745e-07
Iter: 227 loss: 5.70042289e-07
Iter: 228 loss: 5.6854492e-07
Iter: 229 loss: 5.66507481e-07
Iter: 230 loss: 5.64219818e-07
Iter: 231 loss: 5.6391616e-07
Iter: 232 loss: 5.62312607e-07
Iter: 233 loss: 5.593281e-07
Iter: 234 loss: 5.56963926e-07
Iter: 235 loss: 5.56082341e-07
Iter: 236 loss: 5.531939e-07
Iter: 237 loss: 5.53046334e-07
Iter: 238 loss: 5.50362188e-07
Iter: 239 loss: 5.52009624e-07
Iter: 240 loss: 5.4863483e-07
Iter: 241 loss: 5.4679424e-07
Iter: 242 loss: 5.45331886e-07
Iter: 243 loss: 5.44816544e-07
Iter: 244 loss: 5.43877945e-07
Iter: 245 loss: 5.43169676e-07
Iter: 246 loss: 5.41879558e-07
Iter: 247 loss: 5.39736e-07
Iter: 248 loss: 5.39734856e-07
Iter: 249 loss: 5.37186793e-07
Iter: 250 loss: 5.37946448e-07
Iter: 251 loss: 5.35395031e-07
Iter: 252 loss: 5.34999913e-07
Iter: 253 loss: 5.3417881e-07
Iter: 254 loss: 5.33141815e-07
Iter: 255 loss: 5.30504963e-07
Iter: 256 loss: 5.53932182e-07
Iter: 257 loss: 5.30107059e-07
Iter: 258 loss: 5.27189513e-07
Iter: 259 loss: 5.40763608e-07
Iter: 260 loss: 5.2669327e-07
Iter: 261 loss: 5.23922381e-07
Iter: 262 loss: 5.38394715e-07
Iter: 263 loss: 5.2349219e-07
Iter: 264 loss: 5.20172932e-07
Iter: 265 loss: 5.19958292e-07
Iter: 266 loss: 5.17476678e-07
Iter: 267 loss: 5.14344322e-07
Iter: 268 loss: 5.18354398e-07
Iter: 269 loss: 5.1277317e-07
Iter: 270 loss: 5.1070748e-07
Iter: 271 loss: 5.1068e-07
Iter: 272 loss: 5.086182e-07
Iter: 273 loss: 5.075849e-07
Iter: 274 loss: 5.06604181e-07
Iter: 275 loss: 5.04243303e-07
Iter: 276 loss: 5.13766793e-07
Iter: 277 loss: 5.03703063e-07
Iter: 278 loss: 5.0318846e-07
Iter: 279 loss: 5.02915441e-07
Iter: 280 loss: 5.0220342e-07
Iter: 281 loss: 5.0015251e-07
Iter: 282 loss: 5.07277946e-07
Iter: 283 loss: 4.99185205e-07
Iter: 284 loss: 4.97140491e-07
Iter: 285 loss: 5.05146772e-07
Iter: 286 loss: 4.96678638e-07
Iter: 287 loss: 4.9447948e-07
Iter: 288 loss: 5.226334e-07
Iter: 289 loss: 4.94455435e-07
Iter: 290 loss: 4.93202e-07
Iter: 291 loss: 4.90607135e-07
Iter: 292 loss: 5.37136714e-07
Iter: 293 loss: 4.90556488e-07
Iter: 294 loss: 4.87875354e-07
Iter: 295 loss: 5.02469049e-07
Iter: 296 loss: 4.87477337e-07
Iter: 297 loss: 4.85523628e-07
Iter: 298 loss: 5.12670681e-07
Iter: 299 loss: 4.85542046e-07
Iter: 300 loss: 4.84134e-07
Iter: 301 loss: 4.82286339e-07
Iter: 302 loss: 4.82161681e-07
Iter: 303 loss: 4.80107417e-07
Iter: 304 loss: 4.83353347e-07
Iter: 305 loss: 4.79133064e-07
Iter: 306 loss: 4.77916444e-07
Iter: 307 loss: 4.77658e-07
Iter: 308 loss: 4.76914e-07
Iter: 309 loss: 4.75742809e-07
Iter: 310 loss: 4.7571433e-07
Iter: 311 loss: 4.74239187e-07
Iter: 312 loss: 4.83630231e-07
Iter: 313 loss: 4.74080196e-07
Iter: 314 loss: 4.72549061e-07
Iter: 315 loss: 4.800022e-07
Iter: 316 loss: 4.72288463e-07
Iter: 317 loss: 4.71479211e-07
Iter: 318 loss: 4.69556483e-07
Iter: 319 loss: 4.90946377e-07
Iter: 320 loss: 4.69368558e-07
Iter: 321 loss: 4.6791331e-07
Iter: 322 loss: 4.67891454e-07
Iter: 323 loss: 4.66559698e-07
Iter: 324 loss: 4.72113726e-07
Iter: 325 loss: 4.66281904e-07
Iter: 326 loss: 4.65138385e-07
Iter: 327 loss: 4.63238564e-07
Iter: 328 loss: 4.63229838e-07
Iter: 329 loss: 4.61317427e-07
Iter: 330 loss: 4.71021139e-07
Iter: 331 loss: 4.60997256e-07
Iter: 332 loss: 4.59731893e-07
Iter: 333 loss: 4.5973556e-07
Iter: 334 loss: 4.58846728e-07
Iter: 335 loss: 4.56900096e-07
Iter: 336 loss: 4.85263e-07
Iter: 337 loss: 4.56825092e-07
Iter: 338 loss: 4.54731946e-07
Iter: 339 loss: 4.68130139e-07
Iter: 340 loss: 4.54504288e-07
Iter: 341 loss: 4.53275277e-07
Iter: 342 loss: 4.70594273e-07
Iter: 343 loss: 4.53281757e-07
Iter: 344 loss: 4.5198675e-07
Iter: 345 loss: 4.49828292e-07
Iter: 346 loss: 4.49819424e-07
Iter: 347 loss: 4.49509656e-07
Iter: 348 loss: 4.49077021e-07
Iter: 349 loss: 4.48341609e-07
Iter: 350 loss: 4.47196726e-07
Iter: 351 loss: 4.47174443e-07
Iter: 352 loss: 4.46233685e-07
Iter: 353 loss: 4.45877447e-07
Iter: 354 loss: 4.45361025e-07
Iter: 355 loss: 4.44221939e-07
Iter: 356 loss: 4.60738505e-07
Iter: 357 loss: 4.44203806e-07
Iter: 358 loss: 4.43339559e-07
Iter: 359 loss: 4.50611196e-07
Iter: 360 loss: 4.43300877e-07
Iter: 361 loss: 4.42716271e-07
Iter: 362 loss: 4.41225836e-07
Iter: 363 loss: 4.54046329e-07
Iter: 364 loss: 4.40958814e-07
Iter: 365 loss: 4.39413e-07
Iter: 366 loss: 4.51637959e-07
Iter: 367 loss: 4.39284804e-07
Iter: 368 loss: 4.37968765e-07
Iter: 369 loss: 4.5053892e-07
Iter: 370 loss: 4.37925848e-07
Iter: 371 loss: 4.37028859e-07
Iter: 372 loss: 4.36578546e-07
Iter: 373 loss: 4.36125532e-07
Iter: 374 loss: 4.34752593e-07
Iter: 375 loss: 4.34887227e-07
Iter: 376 loss: 4.33689507e-07
Iter: 377 loss: 4.33143157e-07
Iter: 378 loss: 4.32893245e-07
Iter: 379 loss: 4.32212858e-07
Iter: 380 loss: 4.31862077e-07
Iter: 381 loss: 4.31515559e-07
Iter: 382 loss: 4.30596231e-07
Iter: 383 loss: 4.36053739e-07
Iter: 384 loss: 4.30488626e-07
Iter: 385 loss: 4.29406043e-07
Iter: 386 loss: 4.30749765e-07
Iter: 387 loss: 4.28861711e-07
Iter: 388 loss: 4.27922572e-07
Iter: 389 loss: 4.26777035e-07
Iter: 390 loss: 4.26703537e-07
Iter: 391 loss: 4.25532193e-07
Iter: 392 loss: 4.29965553e-07
Iter: 393 loss: 4.25243627e-07
Iter: 394 loss: 4.23429867e-07
Iter: 395 loss: 4.28815611e-07
Iter: 396 loss: 4.22904264e-07
Iter: 397 loss: 4.22075175e-07
Iter: 398 loss: 4.23158895e-07
Iter: 399 loss: 4.21640351e-07
Iter: 400 loss: 4.20875693e-07
Iter: 401 loss: 4.20815695e-07
Iter: 402 loss: 4.20235494e-07
Iter: 403 loss: 4.19466716e-07
Iter: 404 loss: 4.19400607e-07
Iter: 405 loss: 4.18962145e-07
Iter: 406 loss: 4.1800547e-07
Iter: 407 loss: 4.32282491e-07
Iter: 408 loss: 4.17958745e-07
Iter: 409 loss: 4.16712794e-07
Iter: 410 loss: 4.17753824e-07
Iter: 411 loss: 4.15986733e-07
Iter: 412 loss: 4.14553398e-07
Iter: 413 loss: 4.14548197e-07
Iter: 414 loss: 4.1386761e-07
Iter: 415 loss: 4.14019553e-07
Iter: 416 loss: 4.13341184e-07
Iter: 417 loss: 4.12413215e-07
Iter: 418 loss: 4.20179646e-07
Iter: 419 loss: 4.12350175e-07
Iter: 420 loss: 4.11514833e-07
Iter: 421 loss: 4.10029884e-07
Iter: 422 loss: 4.46917625e-07
Iter: 423 loss: 4.10029372e-07
Iter: 424 loss: 4.08541212e-07
Iter: 425 loss: 4.099048e-07
Iter: 426 loss: 4.07696604e-07
Iter: 427 loss: 4.0784775e-07
Iter: 428 loss: 4.07088976e-07
Iter: 429 loss: 4.067237e-07
Iter: 430 loss: 4.06480808e-07
Iter: 431 loss: 4.06358822e-07
Iter: 432 loss: 4.05717572e-07
Iter: 433 loss: 4.04435184e-07
Iter: 434 loss: 4.261139e-07
Iter: 435 loss: 4.04399771e-07
Iter: 436 loss: 4.03809111e-07
Iter: 437 loss: 4.03686499e-07
Iter: 438 loss: 4.03063609e-07
Iter: 439 loss: 4.05204815e-07
Iter: 440 loss: 4.02873354e-07
Iter: 441 loss: 4.02351134e-07
Iter: 442 loss: 4.01380362e-07
Iter: 443 loss: 4.25363879e-07
Iter: 444 loss: 4.01366833e-07
Iter: 445 loss: 4.00181023e-07
Iter: 446 loss: 4.07208603e-07
Iter: 447 loss: 4.00035447e-07
Iter: 448 loss: 3.99395674e-07
Iter: 449 loss: 3.99370094e-07
Iter: 450 loss: 3.98906195e-07
Iter: 451 loss: 3.98135342e-07
Iter: 452 loss: 3.98125508e-07
Iter: 453 loss: 3.97306451e-07
Iter: 454 loss: 3.97308298e-07
Iter: 455 loss: 3.97000292e-07
Iter: 456 loss: 3.96171288e-07
Iter: 457 loss: 4.02137459e-07
Iter: 458 loss: 3.96009114e-07
Iter: 459 loss: 3.94903708e-07
Iter: 460 loss: 4.00396743e-07
Iter: 461 loss: 3.94716e-07
Iter: 462 loss: 3.93951893e-07
Iter: 463 loss: 3.9394692e-07
Iter: 464 loss: 3.93365355e-07
Iter: 465 loss: 3.92126452e-07
Iter: 466 loss: 4.11989845e-07
Iter: 467 loss: 3.92094478e-07
Iter: 468 loss: 3.90999844e-07
Iter: 469 loss: 3.92049145e-07
Iter: 470 loss: 3.90375817e-07
Iter: 471 loss: 3.89427441e-07
Iter: 472 loss: 3.89413742e-07
Iter: 473 loss: 3.88555691e-07
Iter: 474 loss: 3.90090406e-07
Iter: 475 loss: 3.88208178e-07
Iter: 476 loss: 3.87631246e-07
Iter: 477 loss: 3.89294513e-07
Iter: 478 loss: 3.87456822e-07
Iter: 479 loss: 3.87013188e-07
Iter: 480 loss: 3.87300645e-07
Iter: 481 loss: 3.8673079e-07
Iter: 482 loss: 3.86146382e-07
Iter: 483 loss: 3.94110202e-07
Iter: 484 loss: 3.86136605e-07
Iter: 485 loss: 3.85893145e-07
Iter: 486 loss: 3.86290026e-07
Iter: 487 loss: 3.85788837e-07
Iter: 488 loss: 3.85411255e-07
Iter: 489 loss: 3.8537604e-07
Iter: 490 loss: 3.85081279e-07
Iter: 491 loss: 3.84514692e-07
Iter: 492 loss: 3.83901636e-07
Iter: 493 loss: 3.83816825e-07
Iter: 494 loss: 3.83091162e-07
Iter: 495 loss: 3.87211259e-07
Iter: 496 loss: 3.82974633e-07
Iter: 497 loss: 3.81977941e-07
Iter: 498 loss: 3.84522821e-07
Iter: 499 loss: 3.81641115e-07
Iter: 500 loss: 3.80822485e-07
Iter: 501 loss: 3.79081428e-07
Iter: 502 loss: 4.07364865e-07
Iter: 503 loss: 3.79027881e-07
Iter: 504 loss: 3.77951096e-07
Iter: 505 loss: 3.93020628e-07
Iter: 506 loss: 3.77932565e-07
Iter: 507 loss: 3.77235608e-07
Iter: 508 loss: 3.87288651e-07
Iter: 509 loss: 3.7722549e-07
Iter: 510 loss: 3.76731975e-07
Iter: 511 loss: 3.76598138e-07
Iter: 512 loss: 3.76273363e-07
Iter: 513 loss: 3.75598e-07
Iter: 514 loss: 3.76019585e-07
Iter: 515 loss: 3.75175432e-07
Iter: 516 loss: 3.75034347e-07
Iter: 517 loss: 3.74871263e-07
Iter: 518 loss: 3.74584033e-07
Iter: 519 loss: 3.73972227e-07
Iter: 520 loss: 3.83597126e-07
Iter: 521 loss: 3.73949632e-07
Iter: 522 loss: 3.73221042e-07
Iter: 523 loss: 3.8230155e-07
Iter: 524 loss: 3.73194638e-07
Iter: 525 loss: 3.72768e-07
Iter: 526 loss: 3.72821376e-07
Iter: 527 loss: 3.72373677e-07
Iter: 528 loss: 3.71947493e-07
Iter: 529 loss: 3.71562351e-07
Iter: 530 loss: 3.71430559e-07
Iter: 531 loss: 3.71114936e-07
Iter: 532 loss: 3.71021599e-07
Iter: 533 loss: 3.70695915e-07
Iter: 534 loss: 3.70103777e-07
Iter: 535 loss: 3.7011273e-07
Iter: 536 loss: 3.69629333e-07
Iter: 537 loss: 3.70489374e-07
Iter: 538 loss: 3.69401562e-07
Iter: 539 loss: 3.68859929e-07
Iter: 540 loss: 3.70264473e-07
Iter: 541 loss: 3.68681697e-07
Iter: 542 loss: 3.68137478e-07
Iter: 543 loss: 3.73897194e-07
Iter: 544 loss: 3.6811295e-07
Iter: 545 loss: 3.67739e-07
Iter: 546 loss: 3.668153e-07
Iter: 547 loss: 3.74651677e-07
Iter: 548 loss: 3.66684958e-07
Iter: 549 loss: 3.65710093e-07
Iter: 550 loss: 3.80615347e-07
Iter: 551 loss: 3.65717142e-07
Iter: 552 loss: 3.64880776e-07
Iter: 553 loss: 3.68361896e-07
Iter: 554 loss: 3.64684183e-07
Iter: 555 loss: 3.64165885e-07
Iter: 556 loss: 3.66511074e-07
Iter: 557 loss: 3.64046116e-07
Iter: 558 loss: 3.63569427e-07
Iter: 559 loss: 3.66493168e-07
Iter: 560 loss: 3.63521366e-07
Iter: 561 loss: 3.63295442e-07
Iter: 562 loss: 3.62741929e-07
Iter: 563 loss: 3.70969047e-07
Iter: 564 loss: 3.62757476e-07
Iter: 565 loss: 3.62464789e-07
Iter: 566 loss: 3.62447452e-07
Iter: 567 loss: 3.62155788e-07
Iter: 568 loss: 3.62658682e-07
Iter: 569 loss: 3.62009246e-07
Iter: 570 loss: 3.61746487e-07
Iter: 571 loss: 3.61369132e-07
Iter: 572 loss: 3.61330649e-07
Iter: 573 loss: 3.60750903e-07
Iter: 574 loss: 3.60918e-07
Iter: 575 loss: 3.60361639e-07
Iter: 576 loss: 3.59947677e-07
Iter: 577 loss: 3.59874093e-07
Iter: 578 loss: 3.59522e-07
Iter: 579 loss: 3.58941179e-07
Iter: 580 loss: 3.58923415e-07
Iter: 581 loss: 3.58265794e-07
Iter: 582 loss: 3.60239767e-07
Iter: 583 loss: 3.58074885e-07
Iter: 584 loss: 3.57590125e-07
Iter: 585 loss: 3.57603767e-07
Iter: 586 loss: 3.572e-07
Iter: 587 loss: 3.57213935e-07
Iter: 588 loss: 3.5688447e-07
Iter: 589 loss: 3.56511464e-07
Iter: 590 loss: 3.61039554e-07
Iter: 591 loss: 3.56515841e-07
Iter: 592 loss: 3.56170432e-07
Iter: 593 loss: 3.55440477e-07
Iter: 594 loss: 3.65395579e-07
Iter: 595 loss: 3.55380791e-07
Iter: 596 loss: 3.54827449e-07
Iter: 597 loss: 3.5917293e-07
Iter: 598 loss: 3.54768616e-07
Iter: 599 loss: 3.54311766e-07
Iter: 600 loss: 3.58880925e-07
Iter: 601 loss: 3.54284168e-07
Iter: 602 loss: 3.53823907e-07
Iter: 603 loss: 3.53380187e-07
Iter: 604 loss: 3.5327929e-07
Iter: 605 loss: 3.5274627e-07
Iter: 606 loss: 3.52702926e-07
Iter: 607 loss: 3.52299821e-07
Iter: 608 loss: 3.51891856e-07
Iter: 609 loss: 3.51849735e-07
Iter: 610 loss: 3.51569042e-07
Iter: 611 loss: 3.52786117e-07
Iter: 612 loss: 3.51495743e-07
Iter: 613 loss: 3.51244807e-07
Iter: 614 loss: 3.51003905e-07
Iter: 615 loss: 3.50973778e-07
Iter: 616 loss: 3.5062294e-07
Iter: 617 loss: 3.52877947e-07
Iter: 618 loss: 3.50597446e-07
Iter: 619 loss: 3.50246751e-07
Iter: 620 loss: 3.5305618e-07
Iter: 621 loss: 3.50214e-07
Iter: 622 loss: 3.50057775e-07
Iter: 623 loss: 3.50237599e-07
Iter: 624 loss: 3.49973391e-07
Iter: 625 loss: 3.49651486e-07
Iter: 626 loss: 3.49195147e-07
Iter: 627 loss: 3.49189378e-07
Iter: 628 loss: 3.48516153e-07
Iter: 629 loss: 3.48855878e-07
Iter: 630 loss: 3.48059189e-07
Iter: 631 loss: 3.47400203e-07
Iter: 632 loss: 3.53025456e-07
Iter: 633 loss: 3.47359219e-07
Iter: 634 loss: 3.46614343e-07
Iter: 635 loss: 3.4858661e-07
Iter: 636 loss: 3.46385917e-07
Iter: 637 loss: 3.45985711e-07
Iter: 638 loss: 3.45677876e-07
Iter: 639 loss: 3.45551712e-07
Iter: 640 loss: 3.45011642e-07
Iter: 641 loss: 3.48968626e-07
Iter: 642 loss: 3.44964491e-07
Iter: 643 loss: 3.44686441e-07
Iter: 644 loss: 3.4467871e-07
Iter: 645 loss: 3.44462393e-07
Iter: 646 loss: 3.4409959e-07
Iter: 647 loss: 3.44109139e-07
Iter: 648 loss: 3.4375023e-07
Iter: 649 loss: 3.44908699e-07
Iter: 650 loss: 3.43655188e-07
Iter: 651 loss: 3.4342861e-07
Iter: 652 loss: 3.43421505e-07
Iter: 653 loss: 3.4327482e-07
Iter: 654 loss: 3.43133422e-07
Iter: 655 loss: 3.43091131e-07
Iter: 656 loss: 3.42765503e-07
Iter: 657 loss: 3.44018019e-07
Iter: 658 loss: 3.42723496e-07
Iter: 659 loss: 3.42397357e-07
Iter: 660 loss: 3.42011163e-07
Iter: 661 loss: 3.41991921e-07
Iter: 662 loss: 3.4162187e-07
Iter: 663 loss: 3.42655426e-07
Iter: 664 loss: 3.41506052e-07
Iter: 665 loss: 3.41123041e-07
Iter: 666 loss: 3.45963031e-07
Iter: 667 loss: 3.41108773e-07
Iter: 668 loss: 3.40855479e-07
Iter: 669 loss: 3.40471303e-07
Iter: 670 loss: 3.40445524e-07
Iter: 671 loss: 3.4001e-07
Iter: 672 loss: 3.41188468e-07
Iter: 673 loss: 3.39825021e-07
Iter: 674 loss: 3.39445535e-07
Iter: 675 loss: 3.42885414e-07
Iter: 676 loss: 3.39402618e-07
Iter: 677 loss: 3.38969642e-07
Iter: 678 loss: 3.3935936e-07
Iter: 679 loss: 3.38707366e-07
Iter: 680 loss: 3.38367101e-07
Iter: 681 loss: 3.38524387e-07
Iter: 682 loss: 3.38135862e-07
Iter: 683 loss: 3.37763225e-07
Iter: 684 loss: 3.43019593e-07
Iter: 685 loss: 3.37769563e-07
Iter: 686 loss: 3.37442032e-07
Iter: 687 loss: 3.37689016e-07
Iter: 688 loss: 3.37219092e-07
Iter: 689 loss: 3.36875587e-07
Iter: 690 loss: 3.38614711e-07
Iter: 691 loss: 3.36821984e-07
Iter: 692 loss: 3.3649124e-07
Iter: 693 loss: 3.36693461e-07
Iter: 694 loss: 3.36272251e-07
Iter: 695 loss: 3.36023277e-07
Iter: 696 loss: 3.35678294e-07
Iter: 697 loss: 3.35683353e-07
Iter: 698 loss: 3.3549145e-07
Iter: 699 loss: 3.35416019e-07
Iter: 700 loss: 3.35217123e-07
Iter: 701 loss: 3.3515758e-07
Iter: 702 loss: 3.3504034e-07
Iter: 703 loss: 3.34768458e-07
Iter: 704 loss: 3.34690014e-07
Iter: 705 loss: 3.34554954e-07
Iter: 706 loss: 3.34216338e-07
Iter: 707 loss: 3.35322738e-07
Iter: 708 loss: 3.34128117e-07
Iter: 709 loss: 3.33755054e-07
Iter: 710 loss: 3.37189533e-07
Iter: 711 loss: 3.33742889e-07
Iter: 712 loss: 3.33515658e-07
Iter: 713 loss: 3.33337709e-07
Iter: 714 loss: 3.33275324e-07
Iter: 715 loss: 3.32897685e-07
Iter: 716 loss: 3.33115082e-07
Iter: 717 loss: 3.32631913e-07
Iter: 718 loss: 3.32286163e-07
Iter: 719 loss: 3.32249698e-07
Iter: 720 loss: 3.32036024e-07
Iter: 721 loss: 3.31847104e-07
Iter: 722 loss: 3.31797963e-07
Iter: 723 loss: 3.31467106e-07
Iter: 724 loss: 3.34715253e-07
Iter: 725 loss: 3.31465742e-07
Iter: 726 loss: 3.3130911e-07
Iter: 727 loss: 3.30758979e-07
Iter: 728 loss: 3.32751966e-07
Iter: 729 loss: 3.30546641e-07
Iter: 730 loss: 3.30019787e-07
Iter: 731 loss: 3.29996e-07
Iter: 732 loss: 3.29718262e-07
Iter: 733 loss: 3.33987828e-07
Iter: 734 loss: 3.29734746e-07
Iter: 735 loss: 3.29568508e-07
Iter: 736 loss: 3.2922631e-07
Iter: 737 loss: 3.3327288e-07
Iter: 738 loss: 3.29194e-07
Iter: 739 loss: 3.28903184e-07
Iter: 740 loss: 3.31480351e-07
Iter: 741 loss: 3.28872488e-07
Iter: 742 loss: 3.28667142e-07
Iter: 743 loss: 3.29930032e-07
Iter: 744 loss: 3.2864142e-07
Iter: 745 loss: 3.2843127e-07
Iter: 746 loss: 3.28747547e-07
Iter: 747 loss: 3.28337109e-07
Iter: 748 loss: 3.28138469e-07
Iter: 749 loss: 3.28009833e-07
Iter: 750 loss: 3.27921498e-07
Iter: 751 loss: 3.27773364e-07
Iter: 752 loss: 3.27774387e-07
Iter: 753 loss: 3.27568614e-07
Iter: 754 loss: 3.27206948e-07
Iter: 755 loss: 3.35479967e-07
Iter: 756 loss: 3.27223859e-07
Iter: 757 loss: 3.26864466e-07
Iter: 758 loss: 3.31091371e-07
Iter: 759 loss: 3.26875124e-07
Iter: 760 loss: 3.26559956e-07
Iter: 761 loss: 3.26010252e-07
Iter: 762 loss: 3.2603171e-07
Iter: 763 loss: 3.25454124e-07
Iter: 764 loss: 3.27161587e-07
Iter: 765 loss: 3.25274243e-07
Iter: 766 loss: 3.25012934e-07
Iter: 767 loss: 3.24970813e-07
Iter: 768 loss: 3.24676478e-07
Iter: 769 loss: 3.24203313e-07
Iter: 770 loss: 3.24190694e-07
Iter: 771 loss: 3.23811605e-07
Iter: 772 loss: 3.24383251e-07
Iter: 773 loss: 3.23638204e-07
Iter: 774 loss: 3.23395625e-07
Iter: 775 loss: 3.23392271e-07
Iter: 776 loss: 3.23209605e-07
Iter: 777 loss: 3.23495584e-07
Iter: 778 loss: 3.23084805e-07
Iter: 779 loss: 3.22854817e-07
Iter: 780 loss: 3.23127779e-07
Iter: 781 loss: 3.22729306e-07
Iter: 782 loss: 3.22524897e-07
Iter: 783 loss: 3.224919e-07
Iter: 784 loss: 3.22344874e-07
Iter: 785 loss: 3.21986e-07
Iter: 786 loss: 3.26031227e-07
Iter: 787 loss: 3.21968457e-07
Iter: 788 loss: 3.21764446e-07
Iter: 789 loss: 3.21809296e-07
Iter: 790 loss: 3.21610173e-07
Iter: 791 loss: 3.21339968e-07
Iter: 792 loss: 3.22978792e-07
Iter: 793 loss: 3.21316747e-07
Iter: 794 loss: 3.21109411e-07
Iter: 795 loss: 3.20830225e-07
Iter: 796 loss: 3.20826899e-07
Iter: 797 loss: 3.20490528e-07
Iter: 798 loss: 3.21942366e-07
Iter: 799 loss: 3.20415637e-07
Iter: 800 loss: 3.20179765e-07
Iter: 801 loss: 3.20177264e-07
Iter: 802 loss: 3.20044137e-07
Iter: 803 loss: 3.19675e-07
Iter: 804 loss: 3.21207267e-07
Iter: 805 loss: 3.19529903e-07
Iter: 806 loss: 3.19027038e-07
Iter: 807 loss: 3.2165272e-07
Iter: 808 loss: 3.18957746e-07
Iter: 809 loss: 3.18546455e-07
Iter: 810 loss: 3.22947471e-07
Iter: 811 loss: 3.18533864e-07
Iter: 812 loss: 3.18189279e-07
Iter: 813 loss: 3.18996229e-07
Iter: 814 loss: 3.1806735e-07
Iter: 815 loss: 3.17812294e-07
Iter: 816 loss: 3.17672402e-07
Iter: 817 loss: 3.17578838e-07
Iter: 818 loss: 3.17342852e-07
Iter: 819 loss: 3.17327419e-07
Iter: 820 loss: 3.17091349e-07
Iter: 821 loss: 3.17148448e-07
Iter: 822 loss: 3.16913145e-07
Iter: 823 loss: 3.16732951e-07
Iter: 824 loss: 3.18301261e-07
Iter: 825 loss: 3.167128e-07
Iter: 826 loss: 3.16595788e-07
Iter: 827 loss: 3.16859683e-07
Iter: 828 loss: 3.16532e-07
Iter: 829 loss: 3.1638271e-07
Iter: 830 loss: 3.16154399e-07
Iter: 831 loss: 3.1615042e-07
Iter: 832 loss: 3.16021755e-07
Iter: 833 loss: 3.16006037e-07
Iter: 834 loss: 3.15813054e-07
Iter: 835 loss: 3.15626068e-07
Iter: 836 loss: 3.15577637e-07
Iter: 837 loss: 3.15360808e-07
Iter: 838 loss: 3.15310245e-07
Iter: 839 loss: 3.15153471e-07
Iter: 840 loss: 3.14729846e-07
Iter: 841 loss: 3.15491064e-07
Iter: 842 loss: 3.14568638e-07
Iter: 843 loss: 3.14370652e-07
Iter: 844 loss: 3.14341492e-07
Iter: 845 loss: 3.14139186e-07
Iter: 846 loss: 3.1380614e-07
Iter: 847 loss: 3.13795823e-07
Iter: 848 loss: 3.1347605e-07
Iter: 849 loss: 3.15621605e-07
Iter: 850 loss: 3.13449107e-07
Iter: 851 loss: 3.13151361e-07
Iter: 852 loss: 3.15328919e-07
Iter: 853 loss: 3.13114015e-07
Iter: 854 loss: 3.12970201e-07
Iter: 855 loss: 3.13140504e-07
Iter: 856 loss: 3.12903211e-07
Iter: 857 loss: 3.12739644e-07
Iter: 858 loss: 3.13421026e-07
Iter: 859 loss: 3.12695363e-07
Iter: 860 loss: 3.12495104e-07
Iter: 861 loss: 3.12179452e-07
Iter: 862 loss: 3.12173256e-07
Iter: 863 loss: 3.11861271e-07
Iter: 864 loss: 3.13475027e-07
Iter: 865 loss: 3.11811448e-07
Iter: 866 loss: 3.1159189e-07
Iter: 867 loss: 3.11586746e-07
Iter: 868 loss: 3.11482069e-07
Iter: 869 loss: 3.11266291e-07
Iter: 870 loss: 3.15853185e-07
Iter: 871 loss: 3.11270071e-07
Iter: 872 loss: 3.11041617e-07
Iter: 873 loss: 3.11511229e-07
Iter: 874 loss: 3.109499e-07
Iter: 875 loss: 3.10748817e-07
Iter: 876 loss: 3.1171146e-07
Iter: 877 loss: 3.10704422e-07
Iter: 878 loss: 3.10466078e-07
Iter: 879 loss: 3.1187966e-07
Iter: 880 loss: 3.10428788e-07
Iter: 881 loss: 3.10284037e-07
Iter: 882 loss: 3.10080509e-07
Iter: 883 loss: 3.10070646e-07
Iter: 884 loss: 3.09965088e-07
Iter: 885 loss: 3.09930499e-07
Iter: 886 loss: 3.09813572e-07
Iter: 887 loss: 3.09585232e-07
Iter: 888 loss: 3.13722239e-07
Iter: 889 loss: 3.09595436e-07
Iter: 890 loss: 3.09329721e-07
Iter: 891 loss: 3.10722982e-07
Iter: 892 loss: 3.09279727e-07
Iter: 893 loss: 3.08996846e-07
Iter: 894 loss: 3.09257e-07
Iter: 895 loss: 3.08818926e-07
Iter: 896 loss: 3.08519617e-07
Iter: 897 loss: 3.085147e-07
Iter: 898 loss: 3.08267289e-07
Iter: 899 loss: 3.08151613e-07
Iter: 900 loss: 3.0811475e-07
Iter: 901 loss: 3.07944958e-07
Iter: 902 loss: 3.07637e-07
Iter: 903 loss: 3.14321682e-07
Iter: 904 loss: 3.07622656e-07
Iter: 905 loss: 3.07363422e-07
Iter: 906 loss: 3.08253163e-07
Iter: 907 loss: 3.07292453e-07
Iter: 908 loss: 3.07044928e-07
Iter: 909 loss: 3.07246637e-07
Iter: 910 loss: 3.06913705e-07
Iter: 911 loss: 3.06768754e-07
Iter: 912 loss: 3.06723024e-07
Iter: 913 loss: 3.06600583e-07
Iter: 914 loss: 3.06429286e-07
Iter: 915 loss: 3.064157e-07
Iter: 916 loss: 3.06259977e-07
Iter: 917 loss: 3.07087106e-07
Iter: 918 loss: 3.06251195e-07
Iter: 919 loss: 3.06059775e-07
Iter: 920 loss: 3.07014943e-07
Iter: 921 loss: 3.06022457e-07
Iter: 922 loss: 3.05887312e-07
Iter: 923 loss: 3.05858975e-07
Iter: 924 loss: 3.05787353e-07
Iter: 925 loss: 3.05625804e-07
Iter: 926 loss: 3.06994394e-07
Iter: 927 loss: 3.05603578e-07
Iter: 928 loss: 3.05457945e-07
Iter: 929 loss: 3.05208971e-07
Iter: 930 loss: 3.05208602e-07
Iter: 931 loss: 3.05021018e-07
Iter: 932 loss: 3.06070433e-07
Iter: 933 loss: 3.04984638e-07
Iter: 934 loss: 3.04736346e-07
Iter: 935 loss: 3.05732556e-07
Iter: 936 loss: 3.04669385e-07
Iter: 937 loss: 3.0445625e-07
Iter: 938 loss: 3.04345178e-07
Iter: 939 loss: 3.04230127e-07
Iter: 940 loss: 3.03981324e-07
Iter: 941 loss: 3.04122636e-07
Iter: 942 loss: 3.03792518e-07
Iter: 943 loss: 3.03421302e-07
Iter: 944 loss: 3.04491152e-07
Iter: 945 loss: 3.03292182e-07
Iter: 946 loss: 3.03091809e-07
Iter: 947 loss: 3.03042469e-07
Iter: 948 loss: 3.02945068e-07
Iter: 949 loss: 3.02776812e-07
Iter: 950 loss: 3.02781245e-07
Iter: 951 loss: 3.02592326e-07
Iter: 952 loss: 3.05237194e-07
Iter: 953 loss: 3.02598806e-07
Iter: 954 loss: 3.02421711e-07
Iter: 955 loss: 3.02396586e-07
Iter: 956 loss: 3.02293017e-07
Iter: 957 loss: 3.02173135e-07
Iter: 958 loss: 3.02823594e-07
Iter: 959 loss: 3.02152756e-07
Iter: 960 loss: 3.0204103e-07
Iter: 961 loss: 3.02312628e-07
Iter: 962 loss: 3.02000728e-07
Iter: 963 loss: 3.01878714e-07
Iter: 964 loss: 3.01657224e-07
Iter: 965 loss: 3.06502955e-07
Iter: 966 loss: 3.01657309e-07
Iter: 967 loss: 3.01559567e-07
Iter: 968 loss: 3.01513836e-07
Iter: 969 loss: 3.01384574e-07
Iter: 970 loss: 3.0138159e-07
Iter: 971 loss: 3.01303032e-07
Iter: 972 loss: 3.01162714e-07
Iter: 973 loss: 3.0095697e-07
Iter: 974 loss: 3.00953729e-07
Iter: 975 loss: 3.00662862e-07
Iter: 976 loss: 3.01190312e-07
Iter: 977 loss: 3.00550795e-07
Iter: 978 loss: 3.00337433e-07
Iter: 979 loss: 3.0035045e-07
Iter: 980 loss: 3.00164515e-07
Iter: 981 loss: 3.00805198e-07
Iter: 982 loss: 3.00133365e-07
Iter: 983 loss: 2.99955587e-07
Iter: 984 loss: 2.99829367e-07
Iter: 985 loss: 2.9978645e-07
Iter: 986 loss: 2.99579938e-07
Iter: 987 loss: 2.99578659e-07
Iter: 988 loss: 2.99479865e-07
Iter: 989 loss: 2.99252974e-07
Iter: 990 loss: 3.01141426e-07
Iter: 991 loss: 2.99194369e-07
Iter: 992 loss: 2.98975294e-07
Iter: 993 loss: 2.98975408e-07
Iter: 994 loss: 2.98810335e-07
Iter: 995 loss: 2.98684427e-07
Iter: 996 loss: 2.98619284e-07
Iter: 997 loss: 2.9843585e-07
Iter: 998 loss: 2.99273381e-07
Iter: 999 loss: 2.98417888e-07
Iter: 1000 loss: 2.98310596e-07
Iter: 1001 loss: 2.98306844e-07
Iter: 1002 loss: 2.98228542e-07
Iter: 1003 loss: 2.98030329e-07
Iter: 1004 loss: 2.99453802e-07
Iter: 1005 loss: 2.97956888e-07
Iter: 1006 loss: 2.97772374e-07
Iter: 1007 loss: 2.98338222e-07
Iter: 1008 loss: 2.97707203e-07
Iter: 1009 loss: 2.97504215e-07
Iter: 1010 loss: 2.99337785e-07
Iter: 1011 loss: 2.97494722e-07
Iter: 1012 loss: 2.97328938e-07
Iter: 1013 loss: 2.98422748e-07
Iter: 1014 loss: 2.97315182e-07
Iter: 1015 loss: 2.97170118e-07
Iter: 1016 loss: 2.9710057e-07
Iter: 1017 loss: 2.97012122e-07
Iter: 1018 loss: 2.96860492e-07
Iter: 1019 loss: 2.99212218e-07
Iter: 1020 loss: 2.96854239e-07
Iter: 1021 loss: 2.96692491e-07
Iter: 1022 loss: 2.96386361e-07
Iter: 1023 loss: 2.96403215e-07
Iter: 1024 loss: 2.96115786e-07
Iter: 1025 loss: 2.96651848e-07
Iter: 1026 loss: 2.96003776e-07
Iter: 1027 loss: 2.95831455e-07
Iter: 1028 loss: 2.95816278e-07
Iter: 1029 loss: 2.95708787e-07
Iter: 1030 loss: 2.95621845e-07
Iter: 1031 loss: 2.95578786e-07
Iter: 1032 loss: 2.95428208e-07
Iter: 1033 loss: 2.95719133e-07
Iter: 1034 loss: 2.95359825e-07
Iter: 1035 loss: 2.9517733e-07
Iter: 1036 loss: 2.96796543e-07
Iter: 1037 loss: 2.9515752e-07
Iter: 1038 loss: 2.95049e-07
Iter: 1039 loss: 2.94921705e-07
Iter: 1040 loss: 2.94921279e-07
Iter: 1041 loss: 2.94726107e-07
Iter: 1042 loss: 2.95193843e-07
Iter: 1043 loss: 2.94658321e-07
Iter: 1044 loss: 2.94513029e-07
Iter: 1045 loss: 2.95085158e-07
Iter: 1046 loss: 2.94471164e-07
Iter: 1047 loss: 2.94308222e-07
Iter: 1048 loss: 2.94317317e-07
Iter: 1049 loss: 2.94220882e-07
Iter: 1050 loss: 2.94034407e-07
Iter: 1051 loss: 2.9775066e-07
Iter: 1052 loss: 2.94039268e-07
Iter: 1053 loss: 2.93927656e-07
Iter: 1054 loss: 2.93903838e-07
Iter: 1055 loss: 2.93822296e-07
Iter: 1056 loss: 2.93706705e-07
Iter: 1057 loss: 2.93702044e-07
Iter: 1058 loss: 2.9354436e-07
Iter: 1059 loss: 2.93815958e-07
Iter: 1060 loss: 2.93485698e-07
Iter: 1061 loss: 2.93272336e-07
Iter: 1062 loss: 2.94100744e-07
Iter: 1063 loss: 2.93246387e-07
Iter: 1064 loss: 2.93085634e-07
Iter: 1065 loss: 2.92833647e-07
Iter: 1066 loss: 2.92817191e-07
Iter: 1067 loss: 2.92675338e-07
Iter: 1068 loss: 2.92657916e-07
Iter: 1069 loss: 2.92514898e-07
Iter: 1070 loss: 2.92269618e-07
Iter: 1071 loss: 2.9745388e-07
Iter: 1072 loss: 2.92276695e-07
Iter: 1073 loss: 2.92059156e-07
Iter: 1074 loss: 2.92483435e-07
Iter: 1075 loss: 2.91973663e-07
Iter: 1076 loss: 2.91741856e-07
Iter: 1077 loss: 2.91818424e-07
Iter: 1078 loss: 2.91617937e-07
Iter: 1079 loss: 2.91574565e-07
Iter: 1080 loss: 2.91514823e-07
Iter: 1081 loss: 2.91441097e-07
Iter: 1082 loss: 2.9134327e-07
Iter: 1083 loss: 2.91322692e-07
Iter: 1084 loss: 2.91202269e-07
Iter: 1085 loss: 2.91437402e-07
Iter: 1086 loss: 2.91131272e-07
Iter: 1087 loss: 2.91032336e-07
Iter: 1088 loss: 2.91020939e-07
Iter: 1089 loss: 2.90923111e-07
Iter: 1090 loss: 2.90809766e-07
Iter: 1091 loss: 2.90789785e-07
Iter: 1092 loss: 2.90676212e-07
Iter: 1093 loss: 2.90967165e-07
Iter: 1094 loss: 2.90634773e-07
Iter: 1095 loss: 2.90463447e-07
Iter: 1096 loss: 2.91383685e-07
Iter: 1097 loss: 2.90447758e-07
Iter: 1098 loss: 2.90343024e-07
Iter: 1099 loss: 2.9022172e-07
Iter: 1100 loss: 2.90214359e-07
Iter: 1101 loss: 2.90036894e-07
Iter: 1102 loss: 2.90044284e-07
Iter: 1103 loss: 2.89922582e-07
Iter: 1104 loss: 2.89773197e-07
Iter: 1105 loss: 2.89765694e-07
Iter: 1106 loss: 2.89580385e-07
Iter: 1107 loss: 2.89383536e-07
Iter: 1108 loss: 2.89347753e-07
Iter: 1109 loss: 2.89084198e-07
Iter: 1110 loss: 2.90137223e-07
Iter: 1111 loss: 2.89007772e-07
Iter: 1112 loss: 2.88747913e-07
Iter: 1113 loss: 2.90895116e-07
Iter: 1114 loss: 2.88738818e-07
Iter: 1115 loss: 2.88570163e-07
Iter: 1116 loss: 2.90728821e-07
Iter: 1117 loss: 2.88577326e-07
Iter: 1118 loss: 2.88456476e-07
Iter: 1119 loss: 2.88250476e-07
Iter: 1120 loss: 2.93466485e-07
Iter: 1121 loss: 2.88244308e-07
Iter: 1122 loss: 2.88129513e-07
Iter: 1123 loss: 2.88108652e-07
Iter: 1124 loss: 2.87963644e-07
Iter: 1125 loss: 2.87815936e-07
Iter: 1126 loss: 2.87798656e-07
Iter: 1127 loss: 2.87664363e-07
Iter: 1128 loss: 2.88257411e-07
Iter: 1129 loss: 2.87637761e-07
Iter: 1130 loss: 2.87547124e-07
Iter: 1131 loss: 2.88448121e-07
Iter: 1132 loss: 2.87543401e-07
Iter: 1133 loss: 2.87476183e-07
Iter: 1134 loss: 2.87351043e-07
Iter: 1135 loss: 2.87357466e-07
Iter: 1136 loss: 2.87306818e-07
Iter: 1137 loss: 2.87291641e-07
Iter: 1138 loss: 2.87219791e-07
Iter: 1139 loss: 2.87107667e-07
Iter: 1140 loss: 2.87117558e-07
Iter: 1141 loss: 2.86967065e-07
Iter: 1142 loss: 2.868031e-07
Iter: 1143 loss: 2.86778572e-07
Iter: 1144 loss: 2.86518627e-07
Iter: 1145 loss: 2.88568657e-07
Iter: 1146 loss: 2.86488699e-07
Iter: 1147 loss: 2.86309671e-07
Iter: 1148 loss: 2.86193369e-07
Iter: 1149 loss: 2.86119644e-07
Iter: 1150 loss: 2.8600229e-07
Iter: 1151 loss: 2.85952467e-07
Iter: 1152 loss: 2.8583878e-07
Iter: 1153 loss: 2.85788786e-07
Iter: 1154 loss: 2.85726e-07
Iter: 1155 loss: 2.85599754e-07
Iter: 1156 loss: 2.86531844e-07
Iter: 1157 loss: 2.85588925e-07
Iter: 1158 loss: 2.85471344e-07
Iter: 1159 loss: 2.86017581e-07
Iter: 1160 loss: 2.85443434e-07
Iter: 1161 loss: 2.85379656e-07
Iter: 1162 loss: 2.85237348e-07
Iter: 1163 loss: 2.88104303e-07
Iter: 1164 loss: 2.85247495e-07
Iter: 1165 loss: 2.85151742e-07
Iter: 1166 loss: 2.85165044e-07
Iter: 1167 loss: 2.85071508e-07
Iter: 1168 loss: 2.85050703e-07
Iter: 1169 loss: 2.85001249e-07
Iter: 1170 loss: 2.84909561e-07
Iter: 1171 loss: 2.85747944e-07
Iter: 1172 loss: 2.84897226e-07
Iter: 1173 loss: 2.84817247e-07
Iter: 1174 loss: 2.84666186e-07
Iter: 1175 loss: 2.84674144e-07
Iter: 1176 loss: 2.84526379e-07
Iter: 1177 loss: 2.84907458e-07
Iter: 1178 loss: 2.84465159e-07
Iter: 1179 loss: 2.84337318e-07
Iter: 1180 loss: 2.84101077e-07
Iter: 1181 loss: 2.89926447e-07
Iter: 1182 loss: 2.84094625e-07
Iter: 1183 loss: 2.83934099e-07
Iter: 1184 loss: 2.83922816e-07
Iter: 1185 loss: 2.83789774e-07
Iter: 1186 loss: 2.84802667e-07
Iter: 1187 loss: 2.83768173e-07
Iter: 1188 loss: 2.83647523e-07
Iter: 1189 loss: 2.83621887e-07
Iter: 1190 loss: 2.83537702e-07
Iter: 1191 loss: 2.83387436e-07
Iter: 1192 loss: 2.84829071e-07
Iter: 1193 loss: 2.83396787e-07
Iter: 1194 loss: 2.83236602e-07
Iter: 1195 loss: 2.83322493e-07
Iter: 1196 loss: 2.83151167e-07
Iter: 1197 loss: 2.83013719e-07
Iter: 1198 loss: 2.82887527e-07
Iter: 1199 loss: 2.8285956e-07
Iter: 1200 loss: 2.82780292e-07
Iter: 1201 loss: 2.82747123e-07
Iter: 1202 loss: 2.82667941e-07
Iter: 1203 loss: 2.82643214e-07
Iter: 1204 loss: 2.82589895e-07
Iter: 1205 loss: 2.82485559e-07
Iter: 1206 loss: 2.83013975e-07
Iter: 1207 loss: 2.82455233e-07
Iter: 1208 loss: 2.82350868e-07
Iter: 1209 loss: 2.82581652e-07
Iter: 1210 loss: 2.82298515e-07
Iter: 1211 loss: 2.82214643e-07
Iter: 1212 loss: 2.82066139e-07
Iter: 1213 loss: 2.82070118e-07
Iter: 1214 loss: 2.81868324e-07
Iter: 1215 loss: 2.82143418e-07
Iter: 1216 loss: 2.81815915e-07
Iter: 1217 loss: 2.81603462e-07
Iter: 1218 loss: 2.82764717e-07
Iter: 1219 loss: 2.81598801e-07
Iter: 1220 loss: 2.81458824e-07
Iter: 1221 loss: 2.81458369e-07
Iter: 1222 loss: 2.81361224e-07
Iter: 1223 loss: 2.81348463e-07
Iter: 1224 loss: 2.8128909e-07
Iter: 1225 loss: 2.81152097e-07
Iter: 1226 loss: 2.81867301e-07
Iter: 1227 loss: 2.81159174e-07
Iter: 1228 loss: 2.81017833e-07
Iter: 1229 loss: 2.80841192e-07
Iter: 1230 loss: 2.8080882e-07
Iter: 1231 loss: 2.80687345e-07
Iter: 1232 loss: 2.80592474e-07
Iter: 1233 loss: 2.8054e-07
Iter: 1234 loss: 2.8035987e-07
Iter: 1235 loss: 2.80351912e-07
Iter: 1236 loss: 2.80234161e-07
Iter: 1237 loss: 2.80875099e-07
Iter: 1238 loss: 2.80222821e-07
Iter: 1239 loss: 2.80136391e-07
Iter: 1240 loss: 2.8003052e-07
Iter: 1241 loss: 2.80021283e-07
Iter: 1242 loss: 2.79895261e-07
Iter: 1243 loss: 2.79883807e-07
Iter: 1244 loss: 2.79797376e-07
Iter: 1245 loss: 2.79634378e-07
Iter: 1246 loss: 2.79624118e-07
Iter: 1247 loss: 2.79489569e-07
Iter: 1248 loss: 2.80239163e-07
Iter: 1249 loss: 2.79451058e-07
Iter: 1250 loss: 2.7931884e-07
Iter: 1251 loss: 2.79242897e-07
Iter: 1252 loss: 2.79191482e-07
Iter: 1253 loss: 2.79139442e-07
Iter: 1254 loss: 2.79100618e-07
Iter: 1255 loss: 2.79018934e-07
Iter: 1256 loss: 2.78946715e-07
Iter: 1257 loss: 2.7891403e-07
Iter: 1258 loss: 2.78818789e-07
Iter: 1259 loss: 2.78823e-07
Iter: 1260 loss: 2.78747166e-07
Iter: 1261 loss: 2.78655648e-07
Iter: 1262 loss: 2.78643967e-07
Iter: 1263 loss: 2.78536788e-07
Iter: 1264 loss: 2.78636037e-07
Iter: 1265 loss: 2.78425603e-07
Iter: 1266 loss: 2.78336017e-07
Iter: 1267 loss: 2.78333715e-07
Iter: 1268 loss: 2.78232363e-07
Iter: 1269 loss: 2.78171882e-07
Iter: 1270 loss: 2.78139282e-07
Iter: 1271 loss: 2.7798842e-07
Iter: 1272 loss: 2.78221876e-07
Iter: 1273 loss: 2.7792538e-07
Iter: 1274 loss: 2.77827581e-07
Iter: 1275 loss: 2.77829031e-07
Iter: 1276 loss: 2.77744448e-07
Iter: 1277 loss: 2.77630704e-07
Iter: 1278 loss: 2.77621041e-07
Iter: 1279 loss: 2.77516051e-07
Iter: 1280 loss: 2.77583979e-07
Iter: 1281 loss: 2.77461169e-07
Iter: 1282 loss: 2.77306555e-07
Iter: 1283 loss: 2.77577271e-07
Iter: 1284 loss: 2.77213104e-07
Iter: 1285 loss: 2.77099559e-07
Iter: 1286 loss: 2.77098735e-07
Iter: 1287 loss: 2.76974902e-07
Iter: 1288 loss: 2.76968194e-07
Iter: 1289 loss: 2.76870026e-07
Iter: 1290 loss: 2.76763359e-07
Iter: 1291 loss: 2.76764041e-07
Iter: 1292 loss: 2.76685142e-07
Iter: 1293 loss: 2.76527032e-07
Iter: 1294 loss: 2.76519785e-07
Iter: 1295 loss: 2.76372816e-07
Iter: 1296 loss: 2.7684365e-07
Iter: 1297 loss: 2.76338824e-07
Iter: 1298 loss: 2.76210045e-07
Iter: 1299 loss: 2.78056092e-07
Iter: 1300 loss: 2.76203764e-07
Iter: 1301 loss: 2.76105368e-07
Iter: 1302 loss: 2.76076179e-07
Iter: 1303 loss: 2.76024196e-07
Iter: 1304 loss: 2.75902607e-07
Iter: 1305 loss: 2.75773175e-07
Iter: 1306 loss: 2.75762432e-07
Iter: 1307 loss: 2.75725199e-07
Iter: 1308 loss: 2.75671738e-07
Iter: 1309 loss: 2.7559642e-07
Iter: 1310 loss: 2.75486912e-07
Iter: 1311 loss: 2.75491061e-07
Iter: 1312 loss: 2.75318598e-07
Iter: 1313 loss: 2.75109727e-07
Iter: 1314 loss: 2.75097022e-07
Iter: 1315 loss: 2.74900941e-07
Iter: 1316 loss: 2.76708107e-07
Iter: 1317 loss: 2.74873e-07
Iter: 1318 loss: 2.74751386e-07
Iter: 1319 loss: 2.76251171e-07
Iter: 1320 loss: 2.74755905e-07
Iter: 1321 loss: 2.74638694e-07
Iter: 1322 loss: 2.75194338e-07
Iter: 1323 loss: 2.74597085e-07
Iter: 1324 loss: 2.74538081e-07
Iter: 1325 loss: 2.74721259e-07
Iter: 1326 loss: 2.74512161e-07
Iter: 1327 loss: 2.74405778e-07
Iter: 1328 loss: 2.74658504e-07
Iter: 1329 loss: 2.74353113e-07
Iter: 1330 loss: 2.7427842e-07
Iter: 1331 loss: 2.74132077e-07
Iter: 1332 loss: 2.76579385e-07
Iter: 1333 loss: 2.74105219e-07
Iter: 1334 loss: 2.7404991e-07
Iter: 1335 loss: 2.7401174e-07
Iter: 1336 loss: 2.73923263e-07
Iter: 1337 loss: 2.73781438e-07
Iter: 1338 loss: 2.76835692e-07
Iter: 1339 loss: 2.73765977e-07
Iter: 1340 loss: 2.73601529e-07
Iter: 1341 loss: 2.74086574e-07
Iter: 1342 loss: 2.73546334e-07
Iter: 1343 loss: 2.73412127e-07
Iter: 1344 loss: 2.75338095e-07
Iter: 1345 loss: 2.73418664e-07
Iter: 1346 loss: 2.73293182e-07
Iter: 1347 loss: 2.73170428e-07
Iter: 1348 loss: 2.7315923e-07
Iter: 1349 loss: 2.73032668e-07
Iter: 1350 loss: 2.73275134e-07
Iter: 1351 loss: 2.72987734e-07
Iter: 1352 loss: 2.72816237e-07
Iter: 1353 loss: 2.72908153e-07
Iter: 1354 loss: 2.72726197e-07
Iter: 1355 loss: 2.72611658e-07
Iter: 1356 loss: 2.73409881e-07
Iter: 1357 loss: 2.72593553e-07
Iter: 1358 loss: 2.72481628e-07
Iter: 1359 loss: 2.73649221e-07
Iter: 1360 loss: 2.7247026e-07
Iter: 1361 loss: 2.7240273e-07
Iter: 1362 loss: 2.72366606e-07
Iter: 1363 loss: 2.72323518e-07
Iter: 1364 loss: 2.72182319e-07
Iter: 1365 loss: 2.72538e-07
Iter: 1366 loss: 2.72127693e-07
Iter: 1367 loss: 2.72023385e-07
Iter: 1368 loss: 2.72101119e-07
Iter: 1369 loss: 2.71937779e-07
Iter: 1370 loss: 2.71837393e-07
Iter: 1371 loss: 2.72025915e-07
Iter: 1372 loss: 2.71794903e-07
Iter: 1373 loss: 2.71649981e-07
Iter: 1374 loss: 2.72855289e-07
Iter: 1375 loss: 2.71634434e-07
Iter: 1376 loss: 2.71578614e-07
Iter: 1377 loss: 2.71484197e-07
Iter: 1378 loss: 2.71490535e-07
Iter: 1379 loss: 2.71426586e-07
Iter: 1380 loss: 2.71405895e-07
Iter: 1381 loss: 2.71337399e-07
Iter: 1382 loss: 2.71263445e-07
Iter: 1383 loss: 2.71245085e-07
Iter: 1384 loss: 2.71144415e-07
Iter: 1385 loss: 2.71163628e-07
Iter: 1386 loss: 2.71045423e-07
Iter: 1387 loss: 2.70884982e-07
Iter: 1388 loss: 2.70794658e-07
Iter: 1389 loss: 2.70710075e-07
Iter: 1390 loss: 2.70512118e-07
Iter: 1391 loss: 2.72079859e-07
Iter: 1392 loss: 2.70496486e-07
Iter: 1393 loss: 2.70363216e-07
Iter: 1394 loss: 2.70351762e-07
Iter: 1395 loss: 2.70257573e-07
Iter: 1396 loss: 2.7022682e-07
Iter: 1397 loss: 2.70183904e-07
Iter: 1398 loss: 2.70088663e-07
Iter: 1399 loss: 2.71047e-07
Iter: 1400 loss: 2.70085877e-07
Iter: 1401 loss: 2.7000749e-07
Iter: 1402 loss: 2.69930382e-07
Iter: 1403 loss: 2.6991e-07
Iter: 1404 loss: 2.69825307e-07
Iter: 1405 loss: 2.69884168e-07
Iter: 1406 loss: 2.69755731e-07
Iter: 1407 loss: 2.69668192e-07
Iter: 1408 loss: 2.70278548e-07
Iter: 1409 loss: 2.69634029e-07
Iter: 1410 loss: 2.69571331e-07
Iter: 1411 loss: 2.70053363e-07
Iter: 1412 loss: 2.6955405e-07
Iter: 1413 loss: 2.69454688e-07
Iter: 1414 loss: 2.69682062e-07
Iter: 1415 loss: 2.69405319e-07
Iter: 1416 loss: 2.69332872e-07
Iter: 1417 loss: 2.69239763e-07
Iter: 1418 loss: 2.69219925e-07
Iter: 1419 loss: 2.69076793e-07
Iter: 1420 loss: 2.70632711e-07
Iter: 1421 loss: 2.69072672e-07
Iter: 1422 loss: 2.69000111e-07
Iter: 1423 loss: 2.68840637e-07
Iter: 1424 loss: 2.71346835e-07
Iter: 1425 loss: 2.68832963e-07
Iter: 1426 loss: 2.686985e-07
Iter: 1427 loss: 2.68703616e-07
Iter: 1428 loss: 2.686198e-07
Iter: 1429 loss: 2.69468217e-07
Iter: 1430 loss: 2.68621079e-07
Iter: 1431 loss: 2.68562587e-07
Iter: 1432 loss: 2.68455693e-07
Iter: 1433 loss: 2.70041284e-07
Iter: 1434 loss: 2.68461406e-07
Iter: 1435 loss: 2.68315603e-07
Iter: 1436 loss: 2.69990181e-07
Iter: 1437 loss: 2.68319553e-07
Iter: 1438 loss: 2.68250517e-07
Iter: 1439 loss: 2.68088286e-07
Iter: 1440 loss: 2.70215509e-07
Iter: 1441 loss: 2.68060404e-07
Iter: 1442 loss: 2.67913038e-07
Iter: 1443 loss: 2.68586973e-07
Iter: 1444 loss: 2.67863925e-07
Iter: 1445 loss: 2.67697942e-07
Iter: 1446 loss: 2.68198363e-07
Iter: 1447 loss: 2.67648375e-07
Iter: 1448 loss: 2.67597557e-07
Iter: 1449 loss: 2.67570329e-07
Iter: 1450 loss: 2.67519908e-07
Iter: 1451 loss: 2.67395421e-07
Iter: 1452 loss: 2.69920918e-07
Iter: 1453 loss: 2.6740247e-07
Iter: 1454 loss: 2.67302596e-07
Iter: 1455 loss: 2.67302823e-07
Iter: 1456 loss: 2.67245582e-07
Iter: 1457 loss: 2.6719627e-07
Iter: 1458 loss: 2.6718115e-07
Iter: 1459 loss: 2.67093526e-07
Iter: 1460 loss: 2.67133913e-07
Iter: 1461 loss: 2.67051121e-07
Iter: 1462 loss: 2.66951588e-07
Iter: 1463 loss: 2.66951844e-07
Iter: 1464 loss: 2.66877436e-07
Iter: 1465 loss: 2.66766051e-07
Iter: 1466 loss: 2.66758349e-07
Iter: 1467 loss: 2.66711595e-07
Iter: 1468 loss: 2.66654922e-07
Iter: 1469 loss: 2.66658e-07
Iter: 1470 loss: 2.66528389e-07
Iter: 1471 loss: 2.66949201e-07
Iter: 1472 loss: 2.66485074e-07
Iter: 1473 loss: 2.66381562e-07
Iter: 1474 loss: 2.66383438e-07
Iter: 1475 loss: 2.66323696e-07
Iter: 1476 loss: 2.66197446e-07
Iter: 1477 loss: 2.66307381e-07
Iter: 1478 loss: 2.66118889e-07
Iter: 1479 loss: 2.66035727e-07
Iter: 1480 loss: 2.66373888e-07
Iter: 1481 loss: 2.65986785e-07
Iter: 1482 loss: 2.65886769e-07
Iter: 1483 loss: 2.6720852e-07
Iter: 1484 loss: 2.65888104e-07
Iter: 1485 loss: 2.65817732e-07
Iter: 1486 loss: 2.65904589e-07
Iter: 1487 loss: 2.65778e-07
Iter: 1488 loss: 2.65694297e-07
Iter: 1489 loss: 2.65645724e-07
Iter: 1490 loss: 2.65599169e-07
Iter: 1491 loss: 2.65508788e-07
Iter: 1492 loss: 2.65503218e-07
Iter: 1493 loss: 2.65452e-07
Iter: 1494 loss: 2.65332801e-07
Iter: 1495 loss: 2.67489696e-07
Iter: 1496 loss: 2.65314327e-07
Iter: 1497 loss: 2.65243472e-07
Iter: 1498 loss: 2.66663477e-07
Iter: 1499 loss: 2.65237162e-07
Iter: 1500 loss: 2.65155e-07
Iter: 1501 loss: 2.6563049e-07
Iter: 1502 loss: 2.65163806e-07
Iter: 1503 loss: 2.65092297e-07
Iter: 1504 loss: 2.64984635e-07
Iter: 1505 loss: 2.64979917e-07
Iter: 1506 loss: 2.64926598e-07
Iter: 1507 loss: 2.64926712e-07
Iter: 1508 loss: 2.64866742e-07
Iter: 1509 loss: 2.64753965e-07
Iter: 1510 loss: 2.66929305e-07
Iter: 1511 loss: 2.64748223e-07
Iter: 1512 loss: 2.64649032e-07
Iter: 1513 loss: 2.65072e-07
Iter: 1514 loss: 2.64627772e-07
Iter: 1515 loss: 2.64494759e-07
Iter: 1516 loss: 2.64445703e-07
Iter: 1517 loss: 2.64388035e-07
Iter: 1518 loss: 2.64291e-07
Iter: 1519 loss: 2.64278611e-07
Iter: 1520 loss: 2.64202072e-07
Iter: 1521 loss: 2.64100549e-07
Iter: 1522 loss: 2.64088101e-07
Iter: 1523 loss: 2.64002e-07
Iter: 1524 loss: 2.65044491e-07
Iter: 1525 loss: 2.63983e-07
Iter: 1526 loss: 2.63902166e-07
Iter: 1527 loss: 2.63789758e-07
Iter: 1528 loss: 2.63769778e-07
Iter: 1529 loss: 2.63672348e-07
Iter: 1530 loss: 2.64089863e-07
Iter: 1531 loss: 2.6364205e-07
Iter: 1532 loss: 2.63613e-07
Iter: 1533 loss: 2.63601805e-07
Iter: 1534 loss: 2.63551158e-07
Iter: 1535 loss: 2.63562271e-07
Iter: 1536 loss: 2.63525266e-07
Iter: 1537 loss: 2.63464869e-07
Iter: 1538 loss: 2.63409135e-07
Iter: 1539 loss: 2.63395407e-07
Iter: 1540 loss: 2.6333629e-07
Iter: 1541 loss: 2.63349961e-07
Iter: 1542 loss: 2.63290929e-07
Iter: 1543 loss: 2.63227378e-07
Iter: 1544 loss: 2.63225587e-07
Iter: 1545 loss: 2.63103516e-07
Iter: 1546 loss: 2.63078022e-07
Iter: 1547 loss: 2.63010634e-07
Iter: 1548 loss: 2.62871282e-07
Iter: 1549 loss: 2.64040466e-07
Iter: 1550 loss: 2.62869975e-07
Iter: 1551 loss: 2.62746568e-07
Iter: 1552 loss: 2.63545928e-07
Iter: 1553 loss: 2.62730197e-07
Iter: 1554 loss: 2.62663093e-07
Iter: 1555 loss: 2.62625178e-07
Iter: 1556 loss: 2.62584734e-07
Iter: 1557 loss: 2.6250558e-07
Iter: 1558 loss: 2.63614112e-07
Iter: 1559 loss: 2.62502368e-07
Iter: 1560 loss: 2.62423e-07
Iter: 1561 loss: 2.62405081e-07
Iter: 1562 loss: 2.62350966e-07
Iter: 1563 loss: 2.62275023e-07
Iter: 1564 loss: 2.62149911e-07
Iter: 1565 loss: 2.62156e-07
Iter: 1566 loss: 2.62046171e-07
Iter: 1567 loss: 2.62047308e-07
Iter: 1568 loss: 2.61992255e-07
Iter: 1569 loss: 2.62926562e-07
Iter: 1570 loss: 2.61977959e-07
Iter: 1571 loss: 2.6192248e-07
Iter: 1572 loss: 2.61814932e-07
Iter: 1573 loss: 2.63843049e-07
Iter: 1574 loss: 2.61813511e-07
Iter: 1575 loss: 2.61683681e-07
Iter: 1576 loss: 2.6254844e-07
Iter: 1577 loss: 2.61676576e-07
Iter: 1578 loss: 2.61570449e-07
Iter: 1579 loss: 2.61875414e-07
Iter: 1580 loss: 2.61529465e-07
Iter: 1581 loss: 2.61466596e-07
Iter: 1582 loss: 2.61398895e-07
Iter: 1583 loss: 2.61396735e-07
Iter: 1584 loss: 2.61291063e-07
Iter: 1585 loss: 2.61823914e-07
Iter: 1586 loss: 2.61250023e-07
Iter: 1587 loss: 2.61230554e-07
Iter: 1588 loss: 2.61204292e-07
Iter: 1589 loss: 2.61176382e-07
Iter: 1590 loss: 2.61076337e-07
Iter: 1591 loss: 2.61495586e-07
Iter: 1592 loss: 2.61037087e-07
Iter: 1593 loss: 2.60931017e-07
Iter: 1594 loss: 2.62091731e-07
Iter: 1595 loss: 2.60923628e-07
Iter: 1596 loss: 2.60812044e-07
Iter: 1597 loss: 2.61547257e-07
Iter: 1598 loss: 2.60806416e-07
Iter: 1599 loss: 2.60727575e-07
Iter: 1600 loss: 2.60659618e-07
Iter: 1601 loss: 2.60643873e-07
Iter: 1602 loss: 2.60560455e-07
Iter: 1603 loss: 2.6074764e-07
Iter: 1604 loss: 2.60519272e-07
Iter: 1605 loss: 2.60470387e-07
Iter: 1606 loss: 2.60443954e-07
Iter: 1607 loss: 2.60412264e-07
Iter: 1608 loss: 2.6029e-07
Iter: 1609 loss: 2.60822617e-07
Iter: 1610 loss: 2.60264869e-07
Iter: 1611 loss: 2.60178183e-07
Iter: 1612 loss: 2.60168918e-07
Iter: 1613 loss: 2.6010423e-07
Iter: 1614 loss: 2.60111619e-07
Iter: 1615 loss: 2.60039599e-07
Iter: 1616 loss: 2.59969113e-07
Iter: 1617 loss: 2.60021835e-07
Iter: 1618 loss: 2.59907381e-07
Iter: 1619 loss: 2.5979449e-07
Iter: 1620 loss: 2.59827289e-07
Iter: 1621 loss: 2.59740858e-07
Iter: 1622 loss: 2.59648232e-07
Iter: 1623 loss: 2.59642036e-07
Iter: 1624 loss: 2.59588774e-07
Iter: 1625 loss: 2.59473296e-07
Iter: 1626 loss: 2.59474291e-07
Iter: 1627 loss: 2.59369529e-07
Iter: 1628 loss: 2.5991443e-07
Iter: 1629 loss: 2.59365237e-07
Iter: 1630 loss: 2.59263459e-07
Iter: 1631 loss: 2.59965418e-07
Iter: 1632 loss: 2.59259821e-07
Iter: 1633 loss: 2.59194394e-07
Iter: 1634 loss: 2.59126921e-07
Iter: 1635 loss: 2.59118195e-07
Iter: 1636 loss: 2.59016531e-07
Iter: 1637 loss: 2.59363304e-07
Iter: 1638 loss: 2.58993197e-07
Iter: 1639 loss: 2.58916202e-07
Iter: 1640 loss: 2.58915861e-07
Iter: 1641 loss: 2.58862286e-07
Iter: 1642 loss: 2.58762299e-07
Iter: 1643 loss: 2.58774804e-07
Iter: 1644 loss: 2.5865387e-07
Iter: 1645 loss: 2.58672515e-07
Iter: 1646 loss: 2.58565194e-07
Iter: 1647 loss: 2.58470038e-07
Iter: 1648 loss: 2.58461569e-07
Iter: 1649 loss: 2.58384375e-07
Iter: 1650 loss: 2.58204807e-07
Iter: 1651 loss: 2.60794764e-07
Iter: 1652 loss: 2.58206711e-07
Iter: 1653 loss: 2.58068582e-07
Iter: 1654 loss: 2.59923752e-07
Iter: 1655 loss: 2.58057412e-07
Iter: 1656 loss: 2.57983515e-07
Iter: 1657 loss: 2.59195872e-07
Iter: 1658 loss: 2.57987892e-07
Iter: 1659 loss: 2.57936534e-07
Iter: 1660 loss: 2.57882562e-07
Iter: 1661 loss: 2.57872102e-07
Iter: 1662 loss: 2.5783396e-07
Iter: 1663 loss: 2.58044508e-07
Iter: 1664 loss: 2.57817788e-07
Iter: 1665 loss: 2.57737554e-07
Iter: 1666 loss: 2.57766771e-07
Iter: 1667 loss: 2.5767514e-07
Iter: 1668 loss: 2.57617671e-07
Iter: 1669 loss: 2.57637765e-07
Iter: 1670 loss: 2.57567251e-07
Iter: 1671 loss: 2.57487727e-07
Iter: 1672 loss: 2.58008868e-07
Iter: 1673 loss: 2.57479087e-07
Iter: 1674 loss: 2.57385295e-07
Iter: 1675 loss: 2.57890946e-07
Iter: 1676 loss: 2.573735e-07
Iter: 1677 loss: 2.57311399e-07
Iter: 1678 loss: 2.57172303e-07
Iter: 1679 loss: 2.58648981e-07
Iter: 1680 loss: 2.57153488e-07
Iter: 1681 loss: 2.57052079e-07
Iter: 1682 loss: 2.57045713e-07
Iter: 1683 loss: 2.569505e-07
Iter: 1684 loss: 2.57106535e-07
Iter: 1685 loss: 2.56880526e-07
Iter: 1686 loss: 2.56809642e-07
Iter: 1687 loss: 2.56750326e-07
Iter: 1688 loss: 2.56725684e-07
Iter: 1689 loss: 2.5662456e-07
Iter: 1690 loss: 2.56619e-07
Iter: 1691 loss: 2.56528466e-07
Iter: 1692 loss: 2.56763371e-07
Iter: 1693 loss: 2.56509196e-07
Iter: 1694 loss: 2.56447e-07
Iter: 1695 loss: 2.56308908e-07
Iter: 1696 loss: 2.57810683e-07
Iter: 1697 loss: 2.56281965e-07
Iter: 1698 loss: 2.56316952e-07
Iter: 1699 loss: 2.56223586e-07
Iter: 1700 loss: 2.56180158e-07
Iter: 1701 loss: 2.56117715e-07
Iter: 1702 loss: 2.57885972e-07
Iter: 1703 loss: 2.56115499e-07
Iter: 1704 loss: 2.56009514e-07
Iter: 1705 loss: 2.56098247e-07
Iter: 1706 loss: 2.55952386e-07
Iter: 1707 loss: 2.55931184e-07
Iter: 1708 loss: 2.55918849e-07
Iter: 1709 loss: 2.55865359e-07
Iter: 1710 loss: 2.55803286e-07
Iter: 1711 loss: 2.57423608e-07
Iter: 1712 loss: 2.55805077e-07
Iter: 1713 loss: 2.55738115e-07
Iter: 1714 loss: 2.55683119e-07
Iter: 1715 loss: 2.55657881e-07
Iter: 1716 loss: 2.55616726e-07
Iter: 1717 loss: 2.55581313e-07
Iter: 1718 loss: 2.55548457e-07
Iter: 1719 loss: 2.55479733e-07
Iter: 1720 loss: 2.55465892e-07
Iter: 1721 loss: 2.55339614e-07
Iter: 1722 loss: 2.55425789e-07
Iter: 1723 loss: 2.55290928e-07
Iter: 1724 loss: 2.55178463e-07
Iter: 1725 loss: 2.55186364e-07
Iter: 1726 loss: 2.5513171e-07
Iter: 1727 loss: 2.55015152e-07
Iter: 1728 loss: 2.57036476e-07
Iter: 1729 loss: 2.55006739e-07
Iter: 1730 loss: 2.54907604e-07
Iter: 1731 loss: 2.55586144e-07
Iter: 1732 loss: 2.54890779e-07
Iter: 1733 loss: 2.54800881e-07
Iter: 1734 loss: 2.55447048e-07
Iter: 1735 loss: 2.5480324e-07
Iter: 1736 loss: 2.54734687e-07
Iter: 1737 loss: 2.54690917e-07
Iter: 1738 loss: 2.54663803e-07
Iter: 1739 loss: 2.54585359e-07
Iter: 1740 loss: 2.54766064e-07
Iter: 1741 loss: 2.54564327e-07
Iter: 1742 loss: 2.54505039e-07
Iter: 1743 loss: 2.54492363e-07
Iter: 1744 loss: 2.544636e-07
Iter: 1745 loss: 2.54359406e-07
Iter: 1746 loss: 2.55056392e-07
Iter: 1747 loss: 2.54346958e-07
Iter: 1748 loss: 2.54244924e-07
Iter: 1749 loss: 2.55338477e-07
Iter: 1750 loss: 2.54256236e-07
Iter: 1751 loss: 2.54192315e-07
Iter: 1752 loss: 2.5468313e-07
Iter: 1753 loss: 2.54180918e-07
Iter: 1754 loss: 2.54114866e-07
Iter: 1755 loss: 2.54040884e-07
Iter: 1756 loss: 2.54022581e-07
Iter: 1757 loss: 2.53984638e-07
Iter: 1758 loss: 2.53977703e-07
Iter: 1759 loss: 2.53906137e-07
Iter: 1760 loss: 2.53987309e-07
Iter: 1761 loss: 2.53878966e-07
Iter: 1762 loss: 2.53841932e-07
Iter: 1763 loss: 2.53777642e-07
Iter: 1764 loss: 2.53775795e-07
Iter: 1765 loss: 2.5369377e-07
Iter: 1766 loss: 2.54190979e-07
Iter: 1767 loss: 2.53674216e-07
Iter: 1768 loss: 2.53563371e-07
Iter: 1769 loss: 2.54074507e-07
Iter: 1770 loss: 2.53567e-07
Iter: 1771 loss: 2.53504908e-07
Iter: 1772 loss: 2.53374822e-07
Iter: 1773 loss: 2.55310312e-07
Iter: 1774 loss: 2.53361719e-07
Iter: 1775 loss: 2.53366238e-07
Iter: 1776 loss: 2.53304165e-07
Iter: 1777 loss: 2.53257838e-07
Iter: 1778 loss: 2.53156799e-07
Iter: 1779 loss: 2.53156884e-07
Iter: 1780 loss: 2.53054282e-07
Iter: 1781 loss: 2.53157e-07
Iter: 1782 loss: 2.52989167e-07
Iter: 1783 loss: 2.52870734e-07
Iter: 1784 loss: 2.53721055e-07
Iter: 1785 loss: 2.52858541e-07
Iter: 1786 loss: 2.52775351e-07
Iter: 1787 loss: 2.53583039e-07
Iter: 1788 loss: 2.52790727e-07
Iter: 1789 loss: 2.52744087e-07
Iter: 1790 loss: 2.52690086e-07
Iter: 1791 loss: 2.54216786e-07
Iter: 1792 loss: 2.52681673e-07
Iter: 1793 loss: 2.52650182e-07
Iter: 1794 loss: 2.52651688e-07
Iter: 1795 loss: 2.5260772e-07
Iter: 1796 loss: 2.52556333e-07
Iter: 1797 loss: 2.52553832e-07
Iter: 1798 loss: 2.52490793e-07
Iter: 1799 loss: 2.52700772e-07
Iter: 1800 loss: 2.52461e-07
Iter: 1801 loss: 2.52405698e-07
Iter: 1802 loss: 2.52680536e-07
Iter: 1803 loss: 2.52408739e-07
Iter: 1804 loss: 2.52341437e-07
Iter: 1805 loss: 2.52289084e-07
Iter: 1806 loss: 2.52289794e-07
Iter: 1807 loss: 2.52194752e-07
Iter: 1808 loss: 2.52499802e-07
Iter: 1809 loss: 2.52166e-07
Iter: 1810 loss: 2.52085897e-07
Iter: 1811 loss: 2.52083396e-07
Iter: 1812 loss: 2.5203849e-07
Iter: 1813 loss: 2.51970619e-07
Iter: 1814 loss: 2.53555953e-07
Iter: 1815 loss: 2.51962604e-07
Iter: 1816 loss: 2.51857671e-07
Iter: 1817 loss: 2.52036e-07
Iter: 1818 loss: 2.51817085e-07
Iter: 1819 loss: 2.51739664e-07
Iter: 1820 loss: 2.52906489e-07
Iter: 1821 loss: 2.51726675e-07
Iter: 1822 loss: 2.5166571e-07
Iter: 1823 loss: 2.51720792e-07
Iter: 1824 loss: 2.51632827e-07
Iter: 1825 loss: 2.51546339e-07
Iter: 1826 loss: 2.51664119e-07
Iter: 1827 loss: 2.51527695e-07
Iter: 1828 loss: 2.51430208e-07
Iter: 1829 loss: 2.52430226e-07
Iter: 1830 loss: 2.51440724e-07
Iter: 1831 loss: 2.51400422e-07
Iter: 1832 loss: 2.51276845e-07
Iter: 1833 loss: 2.5213788e-07
Iter: 1834 loss: 2.51253141e-07
Iter: 1835 loss: 2.51218978e-07
Iter: 1836 loss: 2.51199822e-07
Iter: 1837 loss: 2.51154916e-07
Iter: 1838 loss: 2.51214544e-07
Iter: 1839 loss: 2.5112891e-07
Iter: 1840 loss: 2.51083577e-07
Iter: 1841 loss: 2.5107e-07
Iter: 1842 loss: 2.51041484e-07
Iter: 1843 loss: 2.51012e-07
Iter: 1844 loss: 2.51009908e-07
Iter: 1845 loss: 2.50957385e-07
Iter: 1846 loss: 2.50936125e-07
Iter: 1847 loss: 2.50929816e-07
Iter: 1848 loss: 2.50864673e-07
Iter: 1849 loss: 2.50831732e-07
Iter: 1850 loss: 2.50829174e-07
Iter: 1851 loss: 2.50744847e-07
Iter: 1852 loss: 2.51216875e-07
Iter: 1853 loss: 2.5074695e-07
Iter: 1854 loss: 2.5068536e-07
Iter: 1855 loss: 2.50799275e-07
Iter: 1856 loss: 2.50651567e-07
Iter: 1857 loss: 2.50561499e-07
Iter: 1858 loss: 2.5056562e-07
Iter: 1859 loss: 2.5050133e-07
Iter: 1860 loss: 2.50429252e-07
Iter: 1861 loss: 2.50420442e-07
Iter: 1862 loss: 2.50355527e-07
Iter: 1863 loss: 2.50220268e-07
Iter: 1864 loss: 2.52815255e-07
Iter: 1865 loss: 2.50224048e-07
Iter: 1866 loss: 2.50113487e-07
Iter: 1867 loss: 2.50962614e-07
Iter: 1868 loss: 2.50108656e-07
Iter: 1869 loss: 2.50067728e-07
Iter: 1870 loss: 2.50666517e-07
Iter: 1871 loss: 2.50064204e-07
Iter: 1872 loss: 2.5001961e-07
Iter: 1873 loss: 2.49962966e-07
Iter: 1874 loss: 2.49946595e-07
Iter: 1875 loss: 2.49889467e-07
Iter: 1876 loss: 2.50167204e-07
Iter: 1877 loss: 2.49881168e-07
Iter: 1878 loss: 2.49817248e-07
Iter: 1879 loss: 2.5061496e-07
Iter: 1880 loss: 2.49822108e-07
Iter: 1881 loss: 2.49788059e-07
Iter: 1882 loss: 2.49744232e-07
Iter: 1883 loss: 2.50384801e-07
Iter: 1884 loss: 2.49733745e-07
Iter: 1885 loss: 2.49669313e-07
Iter: 1886 loss: 2.50156859e-07
Iter: 1887 loss: 2.49668517e-07
Iter: 1888 loss: 2.49608661e-07
Iter: 1889 loss: 2.49726241e-07
Iter: 1890 loss: 2.49584048e-07
Iter: 1891 loss: 2.49513619e-07
Iter: 1892 loss: 2.49575663e-07
Iter: 1893 loss: 2.49481559e-07
Iter: 1894 loss: 2.49434095e-07
Iter: 1895 loss: 2.50050277e-07
Iter: 1896 loss: 2.4943256e-07
Iter: 1897 loss: 2.49377081e-07
Iter: 1898 loss: 2.49415791e-07
Iter: 1899 loss: 2.49349966e-07
Iter: 1900 loss: 2.49296505e-07
Iter: 1901 loss: 2.49274336e-07
Iter: 1902 loss: 2.49247194e-07
Iter: 1903 loss: 2.4919359e-07
Iter: 1904 loss: 2.49414711e-07
Iter: 1905 loss: 2.49167385e-07
Iter: 1906 loss: 2.49094114e-07
Iter: 1907 loss: 2.49692221e-07
Iter: 1908 loss: 2.49094882e-07
Iter: 1909 loss: 2.49049691e-07
Iter: 1910 loss: 2.48963488e-07
Iter: 1911 loss: 2.50681694e-07
Iter: 1912 loss: 2.48951523e-07
Iter: 1913 loss: 2.48923698e-07
Iter: 1914 loss: 2.48906e-07
Iter: 1915 loss: 2.48858953e-07
Iter: 1916 loss: 2.48790712e-07
Iter: 1917 loss: 2.48780566e-07
Iter: 1918 loss: 2.48715082e-07
Iter: 1919 loss: 2.48747938e-07
Iter: 1920 loss: 2.48659347e-07
Iter: 1921 loss: 2.48610831e-07
Iter: 1922 loss: 2.48606881e-07
Iter: 1923 loss: 2.48570188e-07
Iter: 1924 loss: 2.48568824e-07
Iter: 1925 loss: 2.48520564e-07
Iter: 1926 loss: 2.48474464e-07
Iter: 1927 loss: 2.48622882e-07
Iter: 1928 loss: 2.48456047e-07
Iter: 1929 loss: 2.48411027e-07
Iter: 1930 loss: 2.48718379e-07
Iter: 1931 loss: 2.48425e-07
Iter: 1932 loss: 2.48378171e-07
Iter: 1933 loss: 2.48476482e-07
Iter: 1934 loss: 2.4836163e-07
Iter: 1935 loss: 2.48333833e-07
Iter: 1936 loss: 2.48277786e-07
Iter: 1937 loss: 2.49063419e-07
Iter: 1938 loss: 2.48268123e-07
Iter: 1939 loss: 2.48197864e-07
Iter: 1940 loss: 2.49003762e-07
Iter: 1941 loss: 2.48198575e-07
Iter: 1942 loss: 2.48153555e-07
Iter: 1943 loss: 2.48373595e-07
Iter: 1944 loss: 2.48127151e-07
Iter: 1945 loss: 2.48084746e-07
Iter: 1946 loss: 2.48109274e-07
Iter: 1947 loss: 2.48055187e-07
Iter: 1948 loss: 2.48024207e-07
Iter: 1949 loss: 2.48020285e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.6
+ date
Mon Oct 26 16:05:14 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.6/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.6_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.6_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.6_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.6/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 1 --phi 1.6 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi1.6_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4dec18a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4dec030268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4dec06de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4dec06da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4dec13a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4dd00b7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4dd00908c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4dd00249d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4dd0022598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c7d5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4dd0022bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4dd00226a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c793a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4dd00241e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c72b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c72b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c71e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c6daa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c69e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c646378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c6741e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c674ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c5ca9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c5f8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c5f88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c57f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c546840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c528510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c520598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c534b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c4f5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c45e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c4711e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c4957b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c496a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4d6c41b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.30907346e-06
Iter: 2 loss: 5.35129038e-06
Iter: 3 loss: 4.12011286e-06
Iter: 4 loss: 3.64997572e-06
Iter: 5 loss: 3.79687026e-06
Iter: 6 loss: 3.31508045e-06
Iter: 7 loss: 3.05507206e-06
Iter: 8 loss: 3.55474594e-06
Iter: 9 loss: 2.94638448e-06
Iter: 10 loss: 2.71857107e-06
Iter: 11 loss: 4.98319059e-06
Iter: 12 loss: 2.71103636e-06
Iter: 13 loss: 2.62399362e-06
Iter: 14 loss: 2.74586887e-06
Iter: 15 loss: 2.58097498e-06
Iter: 16 loss: 2.48551396e-06
Iter: 17 loss: 3.12281372e-06
Iter: 18 loss: 2.47590378e-06
Iter: 19 loss: 2.43318505e-06
Iter: 20 loss: 2.36265714e-06
Iter: 21 loss: 2.36239066e-06
Iter: 22 loss: 2.31056629e-06
Iter: 23 loss: 2.30902606e-06
Iter: 24 loss: 2.26553811e-06
Iter: 25 loss: 2.29234797e-06
Iter: 26 loss: 2.2376712e-06
Iter: 27 loss: 2.17460934e-06
Iter: 28 loss: 2.12002669e-06
Iter: 29 loss: 2.10306075e-06
Iter: 30 loss: 2.01612033e-06
Iter: 31 loss: 2.17453203e-06
Iter: 32 loss: 1.97872259e-06
Iter: 33 loss: 1.89949833e-06
Iter: 34 loss: 2.48612787e-06
Iter: 35 loss: 1.89306502e-06
Iter: 36 loss: 1.86838361e-06
Iter: 37 loss: 1.8585547e-06
Iter: 38 loss: 1.82859935e-06
Iter: 39 loss: 1.83456723e-06
Iter: 40 loss: 1.80629195e-06
Iter: 41 loss: 1.78514756e-06
Iter: 42 loss: 1.75566106e-06
Iter: 43 loss: 1.75449009e-06
Iter: 44 loss: 1.71552142e-06
Iter: 45 loss: 1.71555723e-06
Iter: 46 loss: 1.69828263e-06
Iter: 47 loss: 1.69108387e-06
Iter: 48 loss: 1.68208385e-06
Iter: 49 loss: 1.6478557e-06
Iter: 50 loss: 1.71623208e-06
Iter: 51 loss: 1.63382674e-06
Iter: 52 loss: 1.59499882e-06
Iter: 53 loss: 1.6317648e-06
Iter: 54 loss: 1.5727835e-06
Iter: 55 loss: 1.54005409e-06
Iter: 56 loss: 1.55098337e-06
Iter: 57 loss: 1.51695428e-06
Iter: 58 loss: 1.4856447e-06
Iter: 59 loss: 1.48461436e-06
Iter: 60 loss: 1.4691833e-06
Iter: 61 loss: 1.44694536e-06
Iter: 62 loss: 1.44619162e-06
Iter: 63 loss: 1.42102976e-06
Iter: 64 loss: 1.54564964e-06
Iter: 65 loss: 1.41665168e-06
Iter: 66 loss: 1.39110307e-06
Iter: 67 loss: 1.39148574e-06
Iter: 68 loss: 1.37078518e-06
Iter: 69 loss: 1.37558e-06
Iter: 70 loss: 1.36285587e-06
Iter: 71 loss: 1.35301048e-06
Iter: 72 loss: 1.33522849e-06
Iter: 73 loss: 1.76387107e-06
Iter: 74 loss: 1.33519438e-06
Iter: 75 loss: 1.31808179e-06
Iter: 76 loss: 1.32600917e-06
Iter: 77 loss: 1.30651256e-06
Iter: 78 loss: 1.28906686e-06
Iter: 79 loss: 1.51982226e-06
Iter: 80 loss: 1.28892543e-06
Iter: 81 loss: 1.27281328e-06
Iter: 82 loss: 1.28879253e-06
Iter: 83 loss: 1.26375517e-06
Iter: 84 loss: 1.24659505e-06
Iter: 85 loss: 1.25404154e-06
Iter: 86 loss: 1.2348695e-06
Iter: 87 loss: 1.20702327e-06
Iter: 88 loss: 1.32505238e-06
Iter: 89 loss: 1.20117136e-06
Iter: 90 loss: 1.18496087e-06
Iter: 91 loss: 1.15083617e-06
Iter: 92 loss: 1.71565523e-06
Iter: 93 loss: 1.14992736e-06
Iter: 94 loss: 1.15091757e-06
Iter: 95 loss: 1.13448937e-06
Iter: 96 loss: 1.12537737e-06
Iter: 97 loss: 1.12504324e-06
Iter: 98 loss: 1.11798101e-06
Iter: 99 loss: 1.10208066e-06
Iter: 100 loss: 1.08323263e-06
Iter: 101 loss: 1.08120594e-06
Iter: 102 loss: 1.0596217e-06
Iter: 103 loss: 1.15483567e-06
Iter: 104 loss: 1.05538902e-06
Iter: 105 loss: 1.05151e-06
Iter: 106 loss: 1.04784328e-06
Iter: 107 loss: 1.03959553e-06
Iter: 108 loss: 1.03967716e-06
Iter: 109 loss: 1.0330732e-06
Iter: 110 loss: 1.02319609e-06
Iter: 111 loss: 1.00883858e-06
Iter: 112 loss: 1.00838224e-06
Iter: 113 loss: 9.97610755e-07
Iter: 114 loss: 1.11221084e-06
Iter: 115 loss: 9.9736e-07
Iter: 116 loss: 9.83976179e-07
Iter: 117 loss: 9.9779993e-07
Iter: 118 loss: 9.76534238e-07
Iter: 119 loss: 9.68052518e-07
Iter: 120 loss: 1.00420777e-06
Iter: 121 loss: 9.66304924e-07
Iter: 122 loss: 9.56994427e-07
Iter: 123 loss: 9.74583e-07
Iter: 124 loss: 9.53089739e-07
Iter: 125 loss: 9.44206647e-07
Iter: 126 loss: 9.54425559e-07
Iter: 127 loss: 9.39327947e-07
Iter: 128 loss: 9.30740612e-07
Iter: 129 loss: 9.39674692e-07
Iter: 130 loss: 9.25918698e-07
Iter: 131 loss: 9.1699593e-07
Iter: 132 loss: 1.04215508e-06
Iter: 133 loss: 9.169691e-07
Iter: 134 loss: 9.09645109e-07
Iter: 135 loss: 8.97574523e-07
Iter: 136 loss: 8.97516315e-07
Iter: 137 loss: 8.84657652e-07
Iter: 138 loss: 9.2186076e-07
Iter: 139 loss: 8.80609718e-07
Iter: 140 loss: 8.66609e-07
Iter: 141 loss: 9.23611424e-07
Iter: 142 loss: 8.63521336e-07
Iter: 143 loss: 8.67959784e-07
Iter: 144 loss: 8.59757677e-07
Iter: 145 loss: 8.57421867e-07
Iter: 146 loss: 8.4958981e-07
Iter: 147 loss: 8.47411457e-07
Iter: 148 loss: 8.40775101e-07
Iter: 149 loss: 8.2929148e-07
Iter: 150 loss: 9.73379883e-07
Iter: 151 loss: 8.29201326e-07
Iter: 152 loss: 8.20529408e-07
Iter: 153 loss: 9.08042e-07
Iter: 154 loss: 8.20274295e-07
Iter: 155 loss: 8.1347531e-07
Iter: 156 loss: 8.17717364e-07
Iter: 157 loss: 8.09125595e-07
Iter: 158 loss: 8.03422495e-07
Iter: 159 loss: 8.10641836e-07
Iter: 160 loss: 8.00499492e-07
Iter: 161 loss: 7.92487072e-07
Iter: 162 loss: 8.38024619e-07
Iter: 163 loss: 7.9139852e-07
Iter: 164 loss: 7.87516285e-07
Iter: 165 loss: 7.7946828e-07
Iter: 166 loss: 9.21532205e-07
Iter: 167 loss: 7.79344532e-07
Iter: 168 loss: 7.74613341e-07
Iter: 169 loss: 7.73594763e-07
Iter: 170 loss: 7.70238785e-07
Iter: 171 loss: 7.76735192e-07
Iter: 172 loss: 7.68865675e-07
Iter: 173 loss: 7.6483758e-07
Iter: 174 loss: 7.58063493e-07
Iter: 175 loss: 7.58072e-07
Iter: 176 loss: 7.50300501e-07
Iter: 177 loss: 7.70076838e-07
Iter: 178 loss: 7.47602257e-07
Iter: 179 loss: 7.47738227e-07
Iter: 180 loss: 7.44261115e-07
Iter: 181 loss: 7.40816802e-07
Iter: 182 loss: 7.34504681e-07
Iter: 183 loss: 8.80303162e-07
Iter: 184 loss: 7.34494677e-07
Iter: 185 loss: 7.28955456e-07
Iter: 186 loss: 7.43378507e-07
Iter: 187 loss: 7.27093777e-07
Iter: 188 loss: 7.23026062e-07
Iter: 189 loss: 7.3183e-07
Iter: 190 loss: 7.2146463e-07
Iter: 191 loss: 7.15321335e-07
Iter: 192 loss: 7.43890837e-07
Iter: 193 loss: 7.1420493e-07
Iter: 194 loss: 7.10932397e-07
Iter: 195 loss: 7.06921128e-07
Iter: 196 loss: 7.06528169e-07
Iter: 197 loss: 7.01936528e-07
Iter: 198 loss: 7.01948579e-07
Iter: 199 loss: 6.98243184e-07
Iter: 200 loss: 6.96126676e-07
Iter: 201 loss: 6.94588152e-07
Iter: 202 loss: 6.88507271e-07
Iter: 203 loss: 6.94809273e-07
Iter: 204 loss: 6.8516664e-07
Iter: 205 loss: 6.81254505e-07
Iter: 206 loss: 7.32150738e-07
Iter: 207 loss: 6.81254505e-07
Iter: 208 loss: 6.77024843e-07
Iter: 209 loss: 6.7618754e-07
Iter: 210 loss: 6.73406248e-07
Iter: 211 loss: 6.69230076e-07
Iter: 212 loss: 6.73346676e-07
Iter: 213 loss: 6.66903759e-07
Iter: 214 loss: 6.60647856e-07
Iter: 215 loss: 6.70358077e-07
Iter: 216 loss: 6.57726503e-07
Iter: 217 loss: 6.62157788e-07
Iter: 218 loss: 6.56238512e-07
Iter: 219 loss: 6.55060717e-07
Iter: 220 loss: 6.5122515e-07
Iter: 221 loss: 6.54097676e-07
Iter: 222 loss: 6.47967568e-07
Iter: 223 loss: 6.43556348e-07
Iter: 224 loss: 7.04441902e-07
Iter: 225 loss: 6.43510873e-07
Iter: 226 loss: 6.40636699e-07
Iter: 227 loss: 6.42155555e-07
Iter: 228 loss: 6.38729375e-07
Iter: 229 loss: 6.35839342e-07
Iter: 230 loss: 6.35789604e-07
Iter: 231 loss: 6.33608352e-07
Iter: 232 loss: 6.27990062e-07
Iter: 233 loss: 6.76600962e-07
Iter: 234 loss: 6.27113536e-07
Iter: 235 loss: 6.22585048e-07
Iter: 236 loss: 6.56578663e-07
Iter: 237 loss: 6.22221137e-07
Iter: 238 loss: 6.17871422e-07
Iter: 239 loss: 6.2626782e-07
Iter: 240 loss: 6.16034527e-07
Iter: 241 loss: 6.14084342e-07
Iter: 242 loss: 6.13736177e-07
Iter: 243 loss: 6.12242161e-07
Iter: 244 loss: 6.09445408e-07
Iter: 245 loss: 6.72366468e-07
Iter: 246 loss: 6.09439553e-07
Iter: 247 loss: 6.05898379e-07
Iter: 248 loss: 6.02665068e-07
Iter: 249 loss: 6.01821057e-07
Iter: 250 loss: 5.9807553e-07
Iter: 251 loss: 5.98072916e-07
Iter: 252 loss: 5.95586243e-07
Iter: 253 loss: 5.96472091e-07
Iter: 254 loss: 5.93824609e-07
Iter: 255 loss: 5.93068137e-07
Iter: 256 loss: 5.92009883e-07
Iter: 257 loss: 5.91180424e-07
Iter: 258 loss: 5.88522539e-07
Iter: 259 loss: 5.93568075e-07
Iter: 260 loss: 5.86852821e-07
Iter: 261 loss: 5.83557721e-07
Iter: 262 loss: 5.83530777e-07
Iter: 263 loss: 5.81474467e-07
Iter: 264 loss: 5.99061309e-07
Iter: 265 loss: 5.81357938e-07
Iter: 266 loss: 5.80035e-07
Iter: 267 loss: 5.78032598e-07
Iter: 268 loss: 5.77964158e-07
Iter: 269 loss: 5.74468686e-07
Iter: 270 loss: 5.89217848e-07
Iter: 271 loss: 5.7375712e-07
Iter: 272 loss: 5.71539658e-07
Iter: 273 loss: 5.68734038e-07
Iter: 274 loss: 5.68503708e-07
Iter: 275 loss: 5.6519508e-07
Iter: 276 loss: 5.96257564e-07
Iter: 277 loss: 5.65073833e-07
Iter: 278 loss: 5.62847674e-07
Iter: 279 loss: 5.73791226e-07
Iter: 280 loss: 5.62474213e-07
Iter: 281 loss: 5.59678256e-07
Iter: 282 loss: 5.61984905e-07
Iter: 283 loss: 5.58055831e-07
Iter: 284 loss: 5.55787324e-07
Iter: 285 loss: 5.52497056e-07
Iter: 286 loss: 5.52377401e-07
Iter: 287 loss: 5.49672905e-07
Iter: 288 loss: 5.49646302e-07
Iter: 289 loss: 5.47909e-07
Iter: 290 loss: 5.47891148e-07
Iter: 291 loss: 5.46471597e-07
Iter: 292 loss: 5.43319118e-07
Iter: 293 loss: 5.90023205e-07
Iter: 294 loss: 5.43175133e-07
Iter: 295 loss: 5.41150712e-07
Iter: 296 loss: 5.47534569e-07
Iter: 297 loss: 5.40593192e-07
Iter: 298 loss: 5.3905444e-07
Iter: 299 loss: 5.39032158e-07
Iter: 300 loss: 5.37926269e-07
Iter: 301 loss: 5.36041057e-07
Iter: 302 loss: 5.36050607e-07
Iter: 303 loss: 5.34468086e-07
Iter: 304 loss: 5.3446206e-07
Iter: 305 loss: 5.33378739e-07
Iter: 306 loss: 5.32824174e-07
Iter: 307 loss: 5.32302465e-07
Iter: 308 loss: 5.30262923e-07
Iter: 309 loss: 5.28469855e-07
Iter: 310 loss: 5.27933821e-07
Iter: 311 loss: 5.2681338e-07
Iter: 312 loss: 5.26671954e-07
Iter: 313 loss: 5.25326072e-07
Iter: 314 loss: 5.25217e-07
Iter: 315 loss: 5.24239908e-07
Iter: 316 loss: 5.22351854e-07
Iter: 317 loss: 5.24454265e-07
Iter: 318 loss: 5.21323443e-07
Iter: 319 loss: 5.19284868e-07
Iter: 320 loss: 5.19438572e-07
Iter: 321 loss: 5.17714398e-07
Iter: 322 loss: 5.1904783e-07
Iter: 323 loss: 5.16904322e-07
Iter: 324 loss: 5.1603854e-07
Iter: 325 loss: 5.13695966e-07
Iter: 326 loss: 5.23037897e-07
Iter: 327 loss: 5.12709903e-07
Iter: 328 loss: 5.10429118e-07
Iter: 329 loss: 5.3032079e-07
Iter: 330 loss: 5.10290874e-07
Iter: 331 loss: 5.08768835e-07
Iter: 332 loss: 5.18517936e-07
Iter: 333 loss: 5.0857517e-07
Iter: 334 loss: 5.06788751e-07
Iter: 335 loss: 5.08720177e-07
Iter: 336 loss: 5.05817752e-07
Iter: 337 loss: 5.04565605e-07
Iter: 338 loss: 5.07821e-07
Iter: 339 loss: 5.04146783e-07
Iter: 340 loss: 5.02478201e-07
Iter: 341 loss: 5.04960099e-07
Iter: 342 loss: 5.01722e-07
Iter: 343 loss: 5.0064375e-07
Iter: 344 loss: 5.02021408e-07
Iter: 345 loss: 5.00099532e-07
Iter: 346 loss: 4.98700786e-07
Iter: 347 loss: 5.0323149e-07
Iter: 348 loss: 4.98318173e-07
Iter: 349 loss: 4.97008955e-07
Iter: 350 loss: 5.07962682e-07
Iter: 351 loss: 4.96920052e-07
Iter: 352 loss: 4.96121856e-07
Iter: 353 loss: 4.9453331e-07
Iter: 354 loss: 5.26154281e-07
Iter: 355 loss: 4.94514552e-07
Iter: 356 loss: 4.92429649e-07
Iter: 357 loss: 4.97401629e-07
Iter: 358 loss: 4.91745311e-07
Iter: 359 loss: 4.90197692e-07
Iter: 360 loss: 4.9000181e-07
Iter: 361 loss: 4.89313891e-07
Iter: 362 loss: 4.87984437e-07
Iter: 363 loss: 5.15825434e-07
Iter: 364 loss: 4.8799069e-07
Iter: 365 loss: 4.86500142e-07
Iter: 366 loss: 4.86233944e-07
Iter: 367 loss: 4.85230544e-07
Iter: 368 loss: 4.84253178e-07
Iter: 369 loss: 4.84035581e-07
Iter: 370 loss: 4.82952373e-07
Iter: 371 loss: 4.80836e-07
Iter: 372 loss: 5.21926836e-07
Iter: 373 loss: 4.80828305e-07
Iter: 374 loss: 4.79192408e-07
Iter: 375 loss: 5.03202102e-07
Iter: 376 loss: 4.79210769e-07
Iter: 377 loss: 4.77894503e-07
Iter: 378 loss: 4.7976755e-07
Iter: 379 loss: 4.7732118e-07
Iter: 380 loss: 4.7594682e-07
Iter: 381 loss: 4.77126889e-07
Iter: 382 loss: 4.75144617e-07
Iter: 383 loss: 4.73949626e-07
Iter: 384 loss: 4.79511527e-07
Iter: 385 loss: 4.73742375e-07
Iter: 386 loss: 4.72295596e-07
Iter: 387 loss: 4.76806321e-07
Iter: 388 loss: 4.71880043e-07
Iter: 389 loss: 4.71097792e-07
Iter: 390 loss: 4.69408548e-07
Iter: 391 loss: 4.94578103e-07
Iter: 392 loss: 4.69313e-07
Iter: 393 loss: 4.68535063e-07
Iter: 394 loss: 4.6816632e-07
Iter: 395 loss: 4.67088569e-07
Iter: 396 loss: 4.71052658e-07
Iter: 397 loss: 4.6684417e-07
Iter: 398 loss: 4.66183224e-07
Iter: 399 loss: 4.6436395e-07
Iter: 400 loss: 4.77094432e-07
Iter: 401 loss: 4.63951636e-07
Iter: 402 loss: 4.62306474e-07
Iter: 403 loss: 4.7655152e-07
Iter: 404 loss: 4.62196908e-07
Iter: 405 loss: 4.60508204e-07
Iter: 406 loss: 4.73594611e-07
Iter: 407 loss: 4.603869e-07
Iter: 408 loss: 4.59661123e-07
Iter: 409 loss: 4.59030161e-07
Iter: 410 loss: 4.58804777e-07
Iter: 411 loss: 4.5743954e-07
Iter: 412 loss: 4.64180118e-07
Iter: 413 loss: 4.57217283e-07
Iter: 414 loss: 4.56010923e-07
Iter: 415 loss: 4.58984204e-07
Iter: 416 loss: 4.55574593e-07
Iter: 417 loss: 4.54456398e-07
Iter: 418 loss: 4.52861713e-07
Iter: 419 loss: 4.52764141e-07
Iter: 420 loss: 4.52418249e-07
Iter: 421 loss: 4.51859961e-07
Iter: 422 loss: 4.51162236e-07
Iter: 423 loss: 4.49521309e-07
Iter: 424 loss: 4.70185228e-07
Iter: 425 loss: 4.49415836e-07
Iter: 426 loss: 4.47896923e-07
Iter: 427 loss: 4.65831874e-07
Iter: 428 loss: 4.47848151e-07
Iter: 429 loss: 4.47607533e-07
Iter: 430 loss: 4.47451413e-07
Iter: 431 loss: 4.47036967e-07
Iter: 432 loss: 4.45933836e-07
Iter: 433 loss: 4.50854799e-07
Iter: 434 loss: 4.45458568e-07
Iter: 435 loss: 4.44428565e-07
Iter: 436 loss: 4.48887477e-07
Iter: 437 loss: 4.44238481e-07
Iter: 438 loss: 4.43389979e-07
Iter: 439 loss: 4.51979929e-07
Iter: 440 loss: 4.43363291e-07
Iter: 441 loss: 4.42651299e-07
Iter: 442 loss: 4.46303233e-07
Iter: 443 loss: 4.42549037e-07
Iter: 444 loss: 4.4197941e-07
Iter: 445 loss: 4.40680083e-07
Iter: 446 loss: 4.56855958e-07
Iter: 447 loss: 4.40550082e-07
Iter: 448 loss: 4.39679297e-07
Iter: 449 loss: 4.39556629e-07
Iter: 450 loss: 4.38859388e-07
Iter: 451 loss: 4.3800668e-07
Iter: 452 loss: 4.37910614e-07
Iter: 453 loss: 4.36789435e-07
Iter: 454 loss: 4.42776525e-07
Iter: 455 loss: 4.36628198e-07
Iter: 456 loss: 4.35662599e-07
Iter: 457 loss: 4.39633538e-07
Iter: 458 loss: 4.3541263e-07
Iter: 459 loss: 4.34215821e-07
Iter: 460 loss: 4.33243486e-07
Iter: 461 loss: 4.32856723e-07
Iter: 462 loss: 4.31755922e-07
Iter: 463 loss: 4.39896269e-07
Iter: 464 loss: 4.31648687e-07
Iter: 465 loss: 4.305397e-07
Iter: 466 loss: 4.41731061e-07
Iter: 467 loss: 4.30502922e-07
Iter: 468 loss: 4.30037119e-07
Iter: 469 loss: 4.28918895e-07
Iter: 470 loss: 4.42538635e-07
Iter: 471 loss: 4.28815298e-07
Iter: 472 loss: 4.27755879e-07
Iter: 473 loss: 4.34094204e-07
Iter: 474 loss: 4.27620876e-07
Iter: 475 loss: 4.26915051e-07
Iter: 476 loss: 4.29991474e-07
Iter: 477 loss: 4.26776069e-07
Iter: 478 loss: 4.25748226e-07
Iter: 479 loss: 4.26921503e-07
Iter: 480 loss: 4.25186158e-07
Iter: 481 loss: 4.24610334e-07
Iter: 482 loss: 4.25272333e-07
Iter: 483 loss: 4.24314067e-07
Iter: 484 loss: 4.23540712e-07
Iter: 485 loss: 4.26735767e-07
Iter: 486 loss: 4.23369158e-07
Iter: 487 loss: 4.22676067e-07
Iter: 488 loss: 4.24403481e-07
Iter: 489 loss: 4.22435932e-07
Iter: 490 loss: 4.21837171e-07
Iter: 491 loss: 4.21314326e-07
Iter: 492 loss: 4.2115272e-07
Iter: 493 loss: 4.20449794e-07
Iter: 494 loss: 4.20431405e-07
Iter: 495 loss: 4.19976402e-07
Iter: 496 loss: 4.19270975e-07
Iter: 497 loss: 4.1925847e-07
Iter: 498 loss: 4.18471e-07
Iter: 499 loss: 4.29588169e-07
Iter: 500 loss: 4.18466442e-07
Iter: 501 loss: 4.17688625e-07
Iter: 502 loss: 4.19490789e-07
Iter: 503 loss: 4.17391107e-07
Iter: 504 loss: 4.16869796e-07
Iter: 505 loss: 4.15582974e-07
Iter: 506 loss: 4.27390631e-07
Iter: 507 loss: 4.15399228e-07
Iter: 508 loss: 4.13995366e-07
Iter: 509 loss: 4.17951043e-07
Iter: 510 loss: 4.13563441e-07
Iter: 511 loss: 4.12346e-07
Iter: 512 loss: 4.31340965e-07
Iter: 513 loss: 4.12352733e-07
Iter: 514 loss: 4.11616355e-07
Iter: 515 loss: 4.19871469e-07
Iter: 516 loss: 4.1162798e-07
Iter: 517 loss: 4.11147653e-07
Iter: 518 loss: 4.10035398e-07
Iter: 519 loss: 4.24391175e-07
Iter: 520 loss: 4.10022096e-07
Iter: 521 loss: 4.09367232e-07
Iter: 522 loss: 4.09303425e-07
Iter: 523 loss: 4.08726407e-07
Iter: 524 loss: 4.08642734e-07
Iter: 525 loss: 4.08211235e-07
Iter: 526 loss: 4.07584309e-07
Iter: 527 loss: 4.1042432e-07
Iter: 528 loss: 4.07484976e-07
Iter: 529 loss: 4.06920293e-07
Iter: 530 loss: 4.1042145e-07
Iter: 531 loss: 4.06887096e-07
Iter: 532 loss: 4.06447668e-07
Iter: 533 loss: 4.06563231e-07
Iter: 534 loss: 4.0612278e-07
Iter: 535 loss: 4.05740622e-07
Iter: 536 loss: 4.05725814e-07
Iter: 537 loss: 4.05319639e-07
Iter: 538 loss: 4.04798072e-07
Iter: 539 loss: 4.04739808e-07
Iter: 540 loss: 4.04331331e-07
Iter: 541 loss: 4.04188512e-07
Iter: 542 loss: 4.03941044e-07
Iter: 543 loss: 4.03053207e-07
Iter: 544 loss: 4.02714591e-07
Iter: 545 loss: 4.02284172e-07
Iter: 546 loss: 4.01577267e-07
Iter: 547 loss: 4.08977172e-07
Iter: 548 loss: 4.01584657e-07
Iter: 549 loss: 4.00778788e-07
Iter: 550 loss: 4.03916488e-07
Iter: 551 loss: 4.00580205e-07
Iter: 552 loss: 4.00003955e-07
Iter: 553 loss: 3.99951091e-07
Iter: 554 loss: 3.99496372e-07
Iter: 555 loss: 3.98781594e-07
Iter: 556 loss: 4.00299058e-07
Iter: 557 loss: 3.98506415e-07
Iter: 558 loss: 3.97920246e-07
Iter: 559 loss: 4.06130084e-07
Iter: 560 loss: 3.97912657e-07
Iter: 561 loss: 3.97361532e-07
Iter: 562 loss: 3.96040463e-07
Iter: 563 loss: 4.07946857e-07
Iter: 564 loss: 3.95829204e-07
Iter: 565 loss: 3.9596506e-07
Iter: 566 loss: 3.95345438e-07
Iter: 567 loss: 3.9494023e-07
Iter: 568 loss: 3.94599681e-07
Iter: 569 loss: 3.94498e-07
Iter: 570 loss: 3.93968463e-07
Iter: 571 loss: 4.02351361e-07
Iter: 572 loss: 3.93955588e-07
Iter: 573 loss: 3.93488193e-07
Iter: 574 loss: 3.93443116e-07
Iter: 575 loss: 3.93096798e-07
Iter: 576 loss: 3.92670813e-07
Iter: 577 loss: 3.91700382e-07
Iter: 578 loss: 4.04267041e-07
Iter: 579 loss: 3.91631829e-07
Iter: 580 loss: 3.9096e-07
Iter: 581 loss: 3.90968353e-07
Iter: 582 loss: 3.90430557e-07
Iter: 583 loss: 3.91786671e-07
Iter: 584 loss: 3.90257e-07
Iter: 585 loss: 3.89758554e-07
Iter: 586 loss: 3.97629094e-07
Iter: 587 loss: 3.89746106e-07
Iter: 588 loss: 3.89473229e-07
Iter: 589 loss: 3.89030816e-07
Iter: 590 loss: 3.8902607e-07
Iter: 591 loss: 3.8855984e-07
Iter: 592 loss: 3.89136346e-07
Iter: 593 loss: 3.88307427e-07
Iter: 594 loss: 3.87614534e-07
Iter: 595 loss: 3.91500549e-07
Iter: 596 loss: 3.87533873e-07
Iter: 597 loss: 3.86862922e-07
Iter: 598 loss: 3.88699334e-07
Iter: 599 loss: 3.86688555e-07
Iter: 600 loss: 3.86078739e-07
Iter: 601 loss: 3.8534e-07
Iter: 602 loss: 3.85256612e-07
Iter: 603 loss: 3.84748631e-07
Iter: 604 loss: 3.84668908e-07
Iter: 605 loss: 3.8414521e-07
Iter: 606 loss: 3.84309487e-07
Iter: 607 loss: 3.83784879e-07
Iter: 608 loss: 3.83428301e-07
Iter: 609 loss: 3.833606e-07
Iter: 610 loss: 3.83140502e-07
Iter: 611 loss: 3.82666769e-07
Iter: 612 loss: 3.82667e-07
Iter: 613 loss: 3.82182691e-07
Iter: 614 loss: 3.81935138e-07
Iter: 615 loss: 3.81690512e-07
Iter: 616 loss: 3.80969823e-07
Iter: 617 loss: 3.82107601e-07
Iter: 618 loss: 3.80658321e-07
Iter: 619 loss: 3.80486256e-07
Iter: 620 loss: 3.80278181e-07
Iter: 621 loss: 3.80057969e-07
Iter: 622 loss: 3.79585572e-07
Iter: 623 loss: 3.83560689e-07
Iter: 624 loss: 3.79512869e-07
Iter: 625 loss: 3.78812899e-07
Iter: 626 loss: 3.8057297e-07
Iter: 627 loss: 3.78552073e-07
Iter: 628 loss: 3.77942399e-07
Iter: 629 loss: 3.82059284e-07
Iter: 630 loss: 3.77891723e-07
Iter: 631 loss: 3.7737405e-07
Iter: 632 loss: 3.80541877e-07
Iter: 633 loss: 3.77275057e-07
Iter: 634 loss: 3.76944683e-07
Iter: 635 loss: 3.76586428e-07
Iter: 636 loss: 3.76533364e-07
Iter: 637 loss: 3.76129691e-07
Iter: 638 loss: 3.7615024e-07
Iter: 639 loss: 3.7588967e-07
Iter: 640 loss: 3.7739332e-07
Iter: 641 loss: 3.75855e-07
Iter: 642 loss: 3.75614036e-07
Iter: 643 loss: 3.76046e-07
Iter: 644 loss: 3.75529481e-07
Iter: 645 loss: 3.75206071e-07
Iter: 646 loss: 3.74802028e-07
Iter: 647 loss: 3.74765875e-07
Iter: 648 loss: 3.74373485e-07
Iter: 649 loss: 3.74307092e-07
Iter: 650 loss: 3.74058573e-07
Iter: 651 loss: 3.73474137e-07
Iter: 652 loss: 3.7637497e-07
Iter: 653 loss: 3.73372274e-07
Iter: 654 loss: 3.72929634e-07
Iter: 655 loss: 3.78679e-07
Iter: 656 loss: 3.72941543e-07
Iter: 657 loss: 3.72535453e-07
Iter: 658 loss: 3.71801093e-07
Iter: 659 loss: 3.71812661e-07
Iter: 660 loss: 3.71154272e-07
Iter: 661 loss: 3.70551135e-07
Iter: 662 loss: 3.70368781e-07
Iter: 663 loss: 3.70332884e-07
Iter: 664 loss: 3.69964141e-07
Iter: 665 loss: 3.6961319e-07
Iter: 666 loss: 3.69510303e-07
Iter: 667 loss: 3.69304672e-07
Iter: 668 loss: 3.68949031e-07
Iter: 669 loss: 3.70503443e-07
Iter: 670 loss: 3.68859276e-07
Iter: 671 loss: 3.68567271e-07
Iter: 672 loss: 3.70670676e-07
Iter: 673 loss: 3.6854118e-07
Iter: 674 loss: 3.6825071e-07
Iter: 675 loss: 3.68783219e-07
Iter: 676 loss: 3.68131538e-07
Iter: 677 loss: 3.6778215e-07
Iter: 678 loss: 3.68655094e-07
Iter: 679 loss: 3.67679661e-07
Iter: 680 loss: 3.6738146e-07
Iter: 681 loss: 3.6677497e-07
Iter: 682 loss: 3.78124298e-07
Iter: 683 loss: 3.66764425e-07
Iter: 684 loss: 3.66205484e-07
Iter: 685 loss: 3.67534341e-07
Iter: 686 loss: 3.65997039e-07
Iter: 687 loss: 3.65515263e-07
Iter: 688 loss: 3.65502274e-07
Iter: 689 loss: 3.6512489e-07
Iter: 690 loss: 3.66935097e-07
Iter: 691 loss: 3.65063926e-07
Iter: 692 loss: 3.64811058e-07
Iter: 693 loss: 3.64245693e-07
Iter: 694 loss: 3.70331719e-07
Iter: 695 loss: 3.64186974e-07
Iter: 696 loss: 3.63503631e-07
Iter: 697 loss: 3.66147219e-07
Iter: 698 loss: 3.63344355e-07
Iter: 699 loss: 3.62938806e-07
Iter: 700 loss: 3.62922378e-07
Iter: 701 loss: 3.62660842e-07
Iter: 702 loss: 3.62513049e-07
Iter: 703 loss: 3.62421019e-07
Iter: 704 loss: 3.62064299e-07
Iter: 705 loss: 3.64517717e-07
Iter: 706 loss: 3.62044801e-07
Iter: 707 loss: 3.61832917e-07
Iter: 708 loss: 3.65055e-07
Iter: 709 loss: 3.61836271e-07
Iter: 710 loss: 3.61692713e-07
Iter: 711 loss: 3.6157877e-07
Iter: 712 loss: 3.61540316e-07
Iter: 713 loss: 3.61196953e-07
Iter: 714 loss: 3.60718388e-07
Iter: 715 loss: 3.60723391e-07
Iter: 716 loss: 3.60274043e-07
Iter: 717 loss: 3.6047021e-07
Iter: 718 loss: 3.5995123e-07
Iter: 719 loss: 3.59422472e-07
Iter: 720 loss: 3.64407242e-07
Iter: 721 loss: 3.59411814e-07
Iter: 722 loss: 3.59037642e-07
Iter: 723 loss: 3.63832754e-07
Iter: 724 loss: 3.59060209e-07
Iter: 725 loss: 3.58777385e-07
Iter: 726 loss: 3.58391361e-07
Iter: 727 loss: 3.58391787e-07
Iter: 728 loss: 3.5796657e-07
Iter: 729 loss: 3.57954235e-07
Iter: 730 loss: 3.57627073e-07
Iter: 731 loss: 3.57377871e-07
Iter: 732 loss: 3.5734783e-07
Iter: 733 loss: 3.5709445e-07
Iter: 734 loss: 3.57508014e-07
Iter: 735 loss: 3.56993979e-07
Iter: 736 loss: 3.56696603e-07
Iter: 737 loss: 3.56830981e-07
Iter: 738 loss: 3.56502056e-07
Iter: 739 loss: 3.56308533e-07
Iter: 740 loss: 3.56315269e-07
Iter: 741 loss: 3.56052169e-07
Iter: 742 loss: 3.5564517e-07
Iter: 743 loss: 3.55626213e-07
Iter: 744 loss: 3.55222738e-07
Iter: 745 loss: 3.59515298e-07
Iter: 746 loss: 3.5522325e-07
Iter: 747 loss: 3.54961458e-07
Iter: 748 loss: 3.54426902e-07
Iter: 749 loss: 3.63808198e-07
Iter: 750 loss: 3.54413942e-07
Iter: 751 loss: 3.53963856e-07
Iter: 752 loss: 3.57067677e-07
Iter: 753 loss: 3.53934809e-07
Iter: 754 loss: 3.5353716e-07
Iter: 755 loss: 3.54596096e-07
Iter: 756 loss: 3.53440612e-07
Iter: 757 loss: 3.53133203e-07
Iter: 758 loss: 3.53124932e-07
Iter: 759 loss: 3.52995528e-07
Iter: 760 loss: 3.52584777e-07
Iter: 761 loss: 3.55431666e-07
Iter: 762 loss: 3.52471517e-07
Iter: 763 loss: 3.52036409e-07
Iter: 764 loss: 3.53290318e-07
Iter: 765 loss: 3.51910785e-07
Iter: 766 loss: 3.51491849e-07
Iter: 767 loss: 3.56274541e-07
Iter: 768 loss: 3.51467037e-07
Iter: 769 loss: 3.51121514e-07
Iter: 770 loss: 3.52990298e-07
Iter: 771 loss: 3.51075812e-07
Iter: 772 loss: 3.50803248e-07
Iter: 773 loss: 3.5088749e-07
Iter: 774 loss: 3.50662475e-07
Iter: 775 loss: 3.50437e-07
Iter: 776 loss: 3.50454854e-07
Iter: 777 loss: 3.50191669e-07
Iter: 778 loss: 3.49650321e-07
Iter: 779 loss: 3.5677752e-07
Iter: 780 loss: 3.4962946e-07
Iter: 781 loss: 3.49241532e-07
Iter: 782 loss: 3.49220102e-07
Iter: 783 loss: 3.49052584e-07
Iter: 784 loss: 3.48708738e-07
Iter: 785 loss: 3.53986451e-07
Iter: 786 loss: 3.48678157e-07
Iter: 787 loss: 3.48214826e-07
Iter: 788 loss: 3.50358704e-07
Iter: 789 loss: 3.48117737e-07
Iter: 790 loss: 3.47797595e-07
Iter: 791 loss: 3.50966729e-07
Iter: 792 loss: 3.47820304e-07
Iter: 793 loss: 3.4747967e-07
Iter: 794 loss: 3.48067118e-07
Iter: 795 loss: 3.47337277e-07
Iter: 796 loss: 3.47116838e-07
Iter: 797 loss: 3.46650523e-07
Iter: 798 loss: 3.5418725e-07
Iter: 799 loss: 3.46610534e-07
Iter: 800 loss: 3.46193616e-07
Iter: 801 loss: 3.51843369e-07
Iter: 802 loss: 3.46194554e-07
Iter: 803 loss: 3.45831438e-07
Iter: 804 loss: 3.47692605e-07
Iter: 805 loss: 3.45741682e-07
Iter: 806 loss: 3.45410086e-07
Iter: 807 loss: 3.467905e-07
Iter: 808 loss: 3.45332722e-07
Iter: 809 loss: 3.45168672e-07
Iter: 810 loss: 3.46620311e-07
Iter: 811 loss: 3.45137835e-07
Iter: 812 loss: 3.44954515e-07
Iter: 813 loss: 3.44928822e-07
Iter: 814 loss: 3.44772e-07
Iter: 815 loss: 3.44573152e-07
Iter: 816 loss: 3.45199794e-07
Iter: 817 loss: 3.44517787e-07
Iter: 818 loss: 3.44297632e-07
Iter: 819 loss: 3.44529894e-07
Iter: 820 loss: 3.44182979e-07
Iter: 821 loss: 3.43968338e-07
Iter: 822 loss: 3.44053404e-07
Iter: 823 loss: 3.43803094e-07
Iter: 824 loss: 3.43540705e-07
Iter: 825 loss: 3.43802867e-07
Iter: 826 loss: 3.43372022e-07
Iter: 827 loss: 3.43338684e-07
Iter: 828 loss: 3.43209194e-07
Iter: 829 loss: 3.43116739e-07
Iter: 830 loss: 3.42824109e-07
Iter: 831 loss: 3.44110276e-07
Iter: 832 loss: 3.42672763e-07
Iter: 833 loss: 3.42228248e-07
Iter: 834 loss: 3.4387e-07
Iter: 835 loss: 3.42061753e-07
Iter: 836 loss: 3.41638156e-07
Iter: 837 loss: 3.42372317e-07
Iter: 838 loss: 3.41406377e-07
Iter: 839 loss: 3.41208164e-07
Iter: 840 loss: 3.41193243e-07
Iter: 841 loss: 3.4098133e-07
Iter: 842 loss: 3.40703082e-07
Iter: 843 loss: 3.40697227e-07
Iter: 844 loss: 3.40558074e-07
Iter: 845 loss: 3.40493216e-07
Iter: 846 loss: 3.40355484e-07
Iter: 847 loss: 3.40151473e-07
Iter: 848 loss: 3.45004821e-07
Iter: 849 loss: 3.40145618e-07
Iter: 850 loss: 3.3990105e-07
Iter: 851 loss: 3.4092875e-07
Iter: 852 loss: 3.3982235e-07
Iter: 853 loss: 3.39595914e-07
Iter: 854 loss: 3.4025868e-07
Iter: 855 loss: 3.39508915e-07
Iter: 856 loss: 3.39284753e-07
Iter: 857 loss: 3.38798344e-07
Iter: 858 loss: 3.47017476e-07
Iter: 859 loss: 3.38787629e-07
Iter: 860 loss: 3.38390635e-07
Iter: 861 loss: 3.38387878e-07
Iter: 862 loss: 3.38105679e-07
Iter: 863 loss: 3.40604942e-07
Iter: 864 loss: 3.38117019e-07
Iter: 865 loss: 3.37902918e-07
Iter: 866 loss: 3.3780438e-07
Iter: 867 loss: 3.37708e-07
Iter: 868 loss: 3.37414235e-07
Iter: 869 loss: 3.3752363e-07
Iter: 870 loss: 3.37253084e-07
Iter: 871 loss: 3.36864275e-07
Iter: 872 loss: 3.3719482e-07
Iter: 873 loss: 3.3665907e-07
Iter: 874 loss: 3.36346488e-07
Iter: 875 loss: 3.36331141e-07
Iter: 876 loss: 3.36118887e-07
Iter: 877 loss: 3.3711143e-07
Iter: 878 loss: 3.36094701e-07
Iter: 879 loss: 3.35922493e-07
Iter: 880 loss: 3.36979667e-07
Iter: 881 loss: 3.35909192e-07
Iter: 882 loss: 3.3572681e-07
Iter: 883 loss: 3.35342605e-07
Iter: 884 loss: 3.41789985e-07
Iter: 885 loss: 3.35319328e-07
Iter: 886 loss: 3.35002937e-07
Iter: 887 loss: 3.35397431e-07
Iter: 888 loss: 3.34821095e-07
Iter: 889 loss: 3.34368792e-07
Iter: 890 loss: 3.37665654e-07
Iter: 891 loss: 3.34341394e-07
Iter: 892 loss: 3.34043193e-07
Iter: 893 loss: 3.34181777e-07
Iter: 894 loss: 3.33833157e-07
Iter: 895 loss: 3.33452e-07
Iter: 896 loss: 3.33705941e-07
Iter: 897 loss: 3.33233459e-07
Iter: 898 loss: 3.33032119e-07
Iter: 899 loss: 3.33000884e-07
Iter: 900 loss: 3.32762568e-07
Iter: 901 loss: 3.32188904e-07
Iter: 902 loss: 3.37917641e-07
Iter: 903 loss: 3.32122681e-07
Iter: 904 loss: 3.31859781e-07
Iter: 905 loss: 3.35312819e-07
Iter: 906 loss: 3.31850799e-07
Iter: 907 loss: 3.31590797e-07
Iter: 908 loss: 3.3146344e-07
Iter: 909 loss: 3.31325e-07
Iter: 910 loss: 3.3107807e-07
Iter: 911 loss: 3.3373e-07
Iter: 912 loss: 3.31077985e-07
Iter: 913 loss: 3.30820399e-07
Iter: 914 loss: 3.32188762e-07
Iter: 915 loss: 3.30785696e-07
Iter: 916 loss: 3.30636766e-07
Iter: 917 loss: 3.32146101e-07
Iter: 918 loss: 3.30614142e-07
Iter: 919 loss: 3.30494743e-07
Iter: 920 loss: 3.3015661e-07
Iter: 921 loss: 3.32822111e-07
Iter: 922 loss: 3.30079331e-07
Iter: 923 loss: 3.29737759e-07
Iter: 924 loss: 3.31907103e-07
Iter: 925 loss: 3.2968731e-07
Iter: 926 loss: 3.29385671e-07
Iter: 927 loss: 3.30625397e-07
Iter: 928 loss: 3.29312797e-07
Iter: 929 loss: 3.28950478e-07
Iter: 930 loss: 3.29239356e-07
Iter: 931 loss: 3.28756499e-07
Iter: 932 loss: 3.28439427e-07
Iter: 933 loss: 3.27831259e-07
Iter: 934 loss: 3.41980069e-07
Iter: 935 loss: 3.27840297e-07
Iter: 936 loss: 3.27531268e-07
Iter: 937 loss: 3.27499038e-07
Iter: 938 loss: 3.27261034e-07
Iter: 939 loss: 3.28447129e-07
Iter: 940 loss: 3.27197966e-07
Iter: 941 loss: 3.26903717e-07
Iter: 942 loss: 3.27879604e-07
Iter: 943 loss: 3.26839512e-07
Iter: 944 loss: 3.26597728e-07
Iter: 945 loss: 3.26382491e-07
Iter: 946 loss: 3.2635063e-07
Iter: 947 loss: 3.26074769e-07
Iter: 948 loss: 3.28174508e-07
Iter: 949 loss: 3.26049616e-07
Iter: 950 loss: 3.25824828e-07
Iter: 951 loss: 3.28256846e-07
Iter: 952 loss: 3.258221e-07
Iter: 953 loss: 3.25624455e-07
Iter: 954 loss: 3.2574971e-07
Iter: 955 loss: 3.25486269e-07
Iter: 956 loss: 3.25291381e-07
Iter: 957 loss: 3.25883377e-07
Iter: 958 loss: 3.25220896e-07
Iter: 959 loss: 3.2503948e-07
Iter: 960 loss: 3.24607271e-07
Iter: 961 loss: 3.30035078e-07
Iter: 962 loss: 3.24593486e-07
Iter: 963 loss: 3.24400929e-07
Iter: 964 loss: 3.24366624e-07
Iter: 965 loss: 3.2411873e-07
Iter: 966 loss: 3.24223492e-07
Iter: 967 loss: 3.23998648e-07
Iter: 968 loss: 3.2381007e-07
Iter: 969 loss: 3.24087154e-07
Iter: 970 loss: 3.23705933e-07
Iter: 971 loss: 3.23420068e-07
Iter: 972 loss: 3.23179819e-07
Iter: 973 loss: 3.23096714e-07
Iter: 974 loss: 3.22804169e-07
Iter: 975 loss: 3.22804283e-07
Iter: 976 loss: 3.22420647e-07
Iter: 977 loss: 3.22752783e-07
Iter: 978 loss: 3.22199952e-07
Iter: 979 loss: 3.21853861e-07
Iter: 980 loss: 3.22257904e-07
Iter: 981 loss: 3.21701293e-07
Iter: 982 loss: 3.21289065e-07
Iter: 983 loss: 3.21786814e-07
Iter: 984 loss: 3.21094035e-07
Iter: 985 loss: 3.2062627e-07
Iter: 986 loss: 3.27792293e-07
Iter: 987 loss: 3.20607597e-07
Iter: 988 loss: 3.2045665e-07
Iter: 989 loss: 3.20861147e-07
Iter: 990 loss: 3.20392076e-07
Iter: 991 loss: 3.20212308e-07
Iter: 992 loss: 3.1988327e-07
Iter: 993 loss: 3.26661791e-07
Iter: 994 loss: 3.19870196e-07
Iter: 995 loss: 3.19560712e-07
Iter: 996 loss: 3.21823222e-07
Iter: 997 loss: 3.19548178e-07
Iter: 998 loss: 3.19323249e-07
Iter: 999 loss: 3.20535605e-07
Iter: 1000 loss: 3.19310971e-07
Iter: 1001 loss: 3.19088883e-07
Iter: 1002 loss: 3.19737751e-07
Iter: 1003 loss: 3.19049775e-07
Iter: 1004 loss: 3.18896696e-07
Iter: 1005 loss: 3.18664632e-07
Iter: 1006 loss: 3.18652724e-07
Iter: 1007 loss: 3.18380842e-07
Iter: 1008 loss: 3.19577907e-07
Iter: 1009 loss: 3.18343837e-07
Iter: 1010 loss: 3.18078548e-07
Iter: 1011 loss: 3.20632296e-07
Iter: 1012 loss: 3.18084858e-07
Iter: 1013 loss: 3.17857968e-07
Iter: 1014 loss: 3.18913635e-07
Iter: 1015 loss: 3.17828068e-07
Iter: 1016 loss: 3.17694742e-07
Iter: 1017 loss: 3.17384035e-07
Iter: 1018 loss: 3.23187749e-07
Iter: 1019 loss: 3.17360957e-07
Iter: 1020 loss: 3.17135516e-07
Iter: 1021 loss: 3.20811523e-07
Iter: 1022 loss: 3.17141513e-07
Iter: 1023 loss: 3.1685812e-07
Iter: 1024 loss: 3.1757645e-07
Iter: 1025 loss: 3.16781581e-07
Iter: 1026 loss: 3.16625488e-07
Iter: 1027 loss: 3.1686406e-07
Iter: 1028 loss: 3.16562847e-07
Iter: 1029 loss: 3.16378475e-07
Iter: 1030 loss: 3.16262827e-07
Iter: 1031 loss: 3.16171679e-07
Iter: 1032 loss: 3.15879504e-07
Iter: 1033 loss: 3.16669912e-07
Iter: 1034 loss: 3.15782131e-07
Iter: 1035 loss: 3.15608872e-07
Iter: 1036 loss: 3.16762225e-07
Iter: 1037 loss: 3.15598299e-07
Iter: 1038 loss: 3.15401962e-07
Iter: 1039 loss: 3.15345062e-07
Iter: 1040 loss: 3.15183797e-07
Iter: 1041 loss: 3.14950341e-07
Iter: 1042 loss: 3.14704977e-07
Iter: 1043 loss: 3.1466152e-07
Iter: 1044 loss: 3.14329043e-07
Iter: 1045 loss: 3.17946274e-07
Iter: 1046 loss: 3.14318925e-07
Iter: 1047 loss: 3.14080864e-07
Iter: 1048 loss: 3.16399905e-07
Iter: 1049 loss: 3.14078193e-07
Iter: 1050 loss: 3.1386412e-07
Iter: 1051 loss: 3.14531889e-07
Iter: 1052 loss: 3.13796136e-07
Iter: 1053 loss: 3.13694784e-07
Iter: 1054 loss: 3.13433191e-07
Iter: 1055 loss: 3.17681867e-07
Iter: 1056 loss: 3.13418411e-07
Iter: 1057 loss: 3.13454279e-07
Iter: 1058 loss: 3.13297676e-07
Iter: 1059 loss: 3.13227758e-07
Iter: 1060 loss: 3.13099548e-07
Iter: 1061 loss: 3.1311572e-07
Iter: 1062 loss: 3.12956757e-07
Iter: 1063 loss: 3.13289036e-07
Iter: 1064 loss: 3.1291242e-07
Iter: 1065 loss: 3.12728e-07
Iter: 1066 loss: 3.1288846e-07
Iter: 1067 loss: 3.12604101e-07
Iter: 1068 loss: 3.12409355e-07
Iter: 1069 loss: 3.12267161e-07
Iter: 1070 loss: 3.12223335e-07
Iter: 1071 loss: 3.11994711e-07
Iter: 1072 loss: 3.119967e-07
Iter: 1073 loss: 3.11822475e-07
Iter: 1074 loss: 3.11591066e-07
Iter: 1075 loss: 3.1159e-07
Iter: 1076 loss: 3.11298209e-07
Iter: 1077 loss: 3.11525582e-07
Iter: 1078 loss: 3.1110693e-07
Iter: 1079 loss: 3.10839027e-07
Iter: 1080 loss: 3.10832803e-07
Iter: 1081 loss: 3.10640786e-07
Iter: 1082 loss: 3.1051971e-07
Iter: 1083 loss: 3.10443227e-07
Iter: 1084 loss: 3.10338123e-07
Iter: 1085 loss: 3.10103133e-07
Iter: 1086 loss: 3.15243938e-07
Iter: 1087 loss: 3.1009003e-07
Iter: 1088 loss: 3.1001511e-07
Iter: 1089 loss: 3.09955766e-07
Iter: 1090 loss: 3.09838811e-07
Iter: 1091 loss: 3.09777761e-07
Iter: 1092 loss: 3.09709975e-07
Iter: 1093 loss: 3.09569742e-07
Iter: 1094 loss: 3.09564854e-07
Iter: 1095 loss: 3.09453867e-07
Iter: 1096 loss: 3.09257985e-07
Iter: 1097 loss: 3.11417949e-07
Iter: 1098 loss: 3.09260713e-07
Iter: 1099 loss: 3.09086545e-07
Iter: 1100 loss: 3.08860024e-07
Iter: 1101 loss: 3.08855022e-07
Iter: 1102 loss: 3.0863913e-07
Iter: 1103 loss: 3.10421854e-07
Iter: 1104 loss: 3.08630376e-07
Iter: 1105 loss: 3.08365259e-07
Iter: 1106 loss: 3.08580809e-07
Iter: 1107 loss: 3.0817904e-07
Iter: 1108 loss: 3.07948028e-07
Iter: 1109 loss: 3.09166182e-07
Iter: 1110 loss: 3.07929156e-07
Iter: 1111 loss: 3.07733984e-07
Iter: 1112 loss: 3.07995776e-07
Iter: 1113 loss: 3.0764528e-07
Iter: 1114 loss: 3.0743621e-07
Iter: 1115 loss: 3.08646207e-07
Iter: 1116 loss: 3.07405401e-07
Iter: 1117 loss: 3.07222791e-07
Iter: 1118 loss: 3.07454172e-07
Iter: 1119 loss: 3.07130108e-07
Iter: 1120 loss: 3.06934055e-07
Iter: 1121 loss: 3.07527245e-07
Iter: 1122 loss: 3.06858595e-07
Iter: 1123 loss: 3.06722427e-07
Iter: 1124 loss: 3.06725951e-07
Iter: 1125 loss: 3.06646115e-07
Iter: 1126 loss: 3.06408566e-07
Iter: 1127 loss: 3.07314849e-07
Iter: 1128 loss: 3.06313751e-07
Iter: 1129 loss: 3.06063271e-07
Iter: 1130 loss: 3.09305847e-07
Iter: 1131 loss: 3.06065033e-07
Iter: 1132 loss: 3.05850278e-07
Iter: 1133 loss: 3.06246534e-07
Iter: 1134 loss: 3.05744919e-07
Iter: 1135 loss: 3.05484519e-07
Iter: 1136 loss: 3.05605312e-07
Iter: 1137 loss: 3.05325614e-07
Iter: 1138 loss: 3.05095909e-07
Iter: 1139 loss: 3.07503342e-07
Iter: 1140 loss: 3.05090339e-07
Iter: 1141 loss: 3.04887095e-07
Iter: 1142 loss: 3.04920263e-07
Iter: 1143 loss: 3.04720146e-07
Iter: 1144 loss: 3.04480125e-07
Iter: 1145 loss: 3.0457295e-07
Iter: 1146 loss: 3.04312124e-07
Iter: 1147 loss: 3.04154469e-07
Iter: 1148 loss: 3.04128491e-07
Iter: 1149 loss: 3.04007926e-07
Iter: 1150 loss: 3.04007358e-07
Iter: 1151 loss: 3.03890943e-07
Iter: 1152 loss: 3.0372766e-07
Iter: 1153 loss: 3.03673517e-07
Iter: 1154 loss: 3.03545619e-07
Iter: 1155 loss: 3.03612069e-07
Iter: 1156 loss: 3.03448871e-07
Iter: 1157 loss: 3.03389726e-07
Iter: 1158 loss: 3.03231673e-07
Iter: 1159 loss: 3.04941921e-07
Iter: 1160 loss: 3.03241052e-07
Iter: 1161 loss: 3.03072341e-07
Iter: 1162 loss: 3.03344422e-07
Iter: 1163 loss: 3.02990031e-07
Iter: 1164 loss: 3.02838203e-07
Iter: 1165 loss: 3.03903249e-07
Iter: 1166 loss: 3.02817341e-07
Iter: 1167 loss: 3.02637631e-07
Iter: 1168 loss: 3.02670685e-07
Iter: 1169 loss: 3.02511069e-07
Iter: 1170 loss: 3.02327749e-07
Iter: 1171 loss: 3.02524711e-07
Iter: 1172 loss: 3.02210708e-07
Iter: 1173 loss: 3.01977934e-07
Iter: 1174 loss: 3.0417533e-07
Iter: 1175 loss: 3.01941043e-07
Iter: 1176 loss: 3.01780091e-07
Iter: 1177 loss: 3.01608679e-07
Iter: 1178 loss: 3.01582588e-07
Iter: 1179 loss: 3.01288679e-07
Iter: 1180 loss: 3.02642746e-07
Iter: 1181 loss: 3.01225526e-07
Iter: 1182 loss: 3.01007674e-07
Iter: 1183 loss: 3.03579952e-07
Iter: 1184 loss: 3.01002245e-07
Iter: 1185 loss: 3.00827281e-07
Iter: 1186 loss: 3.00666017e-07
Iter: 1187 loss: 3.00640409e-07
Iter: 1188 loss: 3.00532236e-07
Iter: 1189 loss: 3.0052e-07
Iter: 1190 loss: 3.00385096e-07
Iter: 1191 loss: 3.00214651e-07
Iter: 1192 loss: 3.00189299e-07
Iter: 1193 loss: 3.00010299e-07
Iter: 1194 loss: 3.00123304e-07
Iter: 1195 loss: 2.99866826e-07
Iter: 1196 loss: 2.99651845e-07
Iter: 1197 loss: 2.99842242e-07
Iter: 1198 loss: 2.99561236e-07
Iter: 1199 loss: 2.99291088e-07
Iter: 1200 loss: 3.01785121e-07
Iter: 1201 loss: 2.99298506e-07
Iter: 1202 loss: 2.99151225e-07
Iter: 1203 loss: 2.99088612e-07
Iter: 1204 loss: 2.99045325e-07
Iter: 1205 loss: 2.9881511e-07
Iter: 1206 loss: 2.99789349e-07
Iter: 1207 loss: 2.9876955e-07
Iter: 1208 loss: 2.98622041e-07
Iter: 1209 loss: 2.99862393e-07
Iter: 1210 loss: 2.98588134e-07
Iter: 1211 loss: 2.98476834e-07
Iter: 1212 loss: 2.98355587e-07
Iter: 1213 loss: 2.98311136e-07
Iter: 1214 loss: 2.98206714e-07
Iter: 1215 loss: 2.9820913e-07
Iter: 1216 loss: 2.98080465e-07
Iter: 1217 loss: 2.97971638e-07
Iter: 1218 loss: 2.97921275e-07
Iter: 1219 loss: 2.97785618e-07
Iter: 1220 loss: 2.98590805e-07
Iter: 1221 loss: 2.97767031e-07
Iter: 1222 loss: 2.97618953e-07
Iter: 1223 loss: 2.99122235e-07
Iter: 1224 loss: 2.97624837e-07
Iter: 1225 loss: 2.9755347e-07
Iter: 1226 loss: 2.97354177e-07
Iter: 1227 loss: 2.99044871e-07
Iter: 1228 loss: 2.97331e-07
Iter: 1229 loss: 2.9709193e-07
Iter: 1230 loss: 2.97790962e-07
Iter: 1231 loss: 2.97035967e-07
Iter: 1232 loss: 2.96861629e-07
Iter: 1233 loss: 2.99124736e-07
Iter: 1234 loss: 2.96865153e-07
Iter: 1235 loss: 2.96714802e-07
Iter: 1236 loss: 2.96723897e-07
Iter: 1237 loss: 2.96612768e-07
Iter: 1238 loss: 2.9639051e-07
Iter: 1239 loss: 2.96516816e-07
Iter: 1240 loss: 2.9623709e-07
Iter: 1241 loss: 2.96091457e-07
Iter: 1242 loss: 2.96092736e-07
Iter: 1243 loss: 2.95978907e-07
Iter: 1244 loss: 2.95775209e-07
Iter: 1245 loss: 2.95751079e-07
Iter: 1246 loss: 2.95642906e-07
Iter: 1247 loss: 2.97308418e-07
Iter: 1248 loss: 2.95636397e-07
Iter: 1249 loss: 2.95500229e-07
Iter: 1250 loss: 2.95966458e-07
Iter: 1251 loss: 2.95460438e-07
Iter: 1252 loss: 2.95371336e-07
Iter: 1253 loss: 2.95343625e-07
Iter: 1254 loss: 2.95267625e-07
Iter: 1255 loss: 2.95179461e-07
Iter: 1256 loss: 2.95184833e-07
Iter: 1257 loss: 2.95097607e-07
Iter: 1258 loss: 2.94912411e-07
Iter: 1259 loss: 2.97592919e-07
Iter: 1260 loss: 2.94913775e-07
Iter: 1261 loss: 2.94758934e-07
Iter: 1262 loss: 2.94682337e-07
Iter: 1263 loss: 2.94616427e-07
Iter: 1264 loss: 2.94456299e-07
Iter: 1265 loss: 2.94424694e-07
Iter: 1266 loss: 2.94268773e-07
Iter: 1267 loss: 2.94844057e-07
Iter: 1268 loss: 2.94226197e-07
Iter: 1269 loss: 2.94085737e-07
Iter: 1270 loss: 2.93882721e-07
Iter: 1271 loss: 2.93887837e-07
Iter: 1272 loss: 2.93617603e-07
Iter: 1273 loss: 2.94337895e-07
Iter: 1274 loss: 2.93538932e-07
Iter: 1275 loss: 2.9332142e-07
Iter: 1276 loss: 2.9331693e-07
Iter: 1277 loss: 2.93209609e-07
Iter: 1278 loss: 2.93112492e-07
Iter: 1279 loss: 2.93072446e-07
Iter: 1280 loss: 2.9288492e-07
Iter: 1281 loss: 2.94483073e-07
Iter: 1282 loss: 2.92884806e-07
Iter: 1283 loss: 2.92753043e-07
Iter: 1284 loss: 2.93465206e-07
Iter: 1285 loss: 2.92705579e-07
Iter: 1286 loss: 2.92622673e-07
Iter: 1287 loss: 2.92656637e-07
Iter: 1288 loss: 2.92577852e-07
Iter: 1289 loss: 2.92386943e-07
Iter: 1290 loss: 2.92840468e-07
Iter: 1291 loss: 2.92335358e-07
Iter: 1292 loss: 2.9223628e-07
Iter: 1293 loss: 2.92108155e-07
Iter: 1294 loss: 2.9209059e-07
Iter: 1295 loss: 2.91901074e-07
Iter: 1296 loss: 2.92086156e-07
Iter: 1297 loss: 2.91779486e-07
Iter: 1298 loss: 2.91606341e-07
Iter: 1299 loss: 2.93469327e-07
Iter: 1300 loss: 2.91597502e-07
Iter: 1301 loss: 2.91425295e-07
Iter: 1302 loss: 2.91995093e-07
Iter: 1303 loss: 2.91378569e-07
Iter: 1304 loss: 2.91264598e-07
Iter: 1305 loss: 2.91148069e-07
Iter: 1306 loss: 2.91147728e-07
Iter: 1307 loss: 2.90935418e-07
Iter: 1308 loss: 2.92213457e-07
Iter: 1309 loss: 2.90912851e-07
Iter: 1310 loss: 2.90744765e-07
Iter: 1311 loss: 2.91773091e-07
Iter: 1312 loss: 2.9074198e-07
Iter: 1313 loss: 2.90578413e-07
Iter: 1314 loss: 2.90455091e-07
Iter: 1315 loss: 2.90399385e-07
Iter: 1316 loss: 2.90276432e-07
Iter: 1317 loss: 2.90267792e-07
Iter: 1318 loss: 2.90144044e-07
Iter: 1319 loss: 2.90012565e-07
Iter: 1320 loss: 2.89979965e-07
Iter: 1321 loss: 2.89945433e-07
Iter: 1322 loss: 2.89892e-07
Iter: 1323 loss: 2.89841239e-07
Iter: 1324 loss: 2.89689467e-07
Iter: 1325 loss: 2.91203179e-07
Iter: 1326 loss: 2.89646778e-07
Iter: 1327 loss: 2.89457546e-07
Iter: 1328 loss: 2.9022479e-07
Iter: 1329 loss: 2.89430432e-07
Iter: 1330 loss: 2.89315068e-07
Iter: 1331 loss: 2.89169179e-07
Iter: 1332 loss: 2.89144111e-07
Iter: 1333 loss: 2.8901232e-07
Iter: 1334 loss: 2.88983784e-07
Iter: 1335 loss: 2.88901191e-07
Iter: 1336 loss: 2.89178871e-07
Iter: 1337 loss: 2.88878198e-07
Iter: 1338 loss: 2.88761072e-07
Iter: 1339 loss: 2.88582754e-07
Iter: 1340 loss: 2.88574284e-07
Iter: 1341 loss: 2.88423678e-07
Iter: 1342 loss: 2.88902385e-07
Iter: 1343 loss: 2.88372405e-07
Iter: 1344 loss: 2.88231e-07
Iter: 1345 loss: 2.8822376e-07
Iter: 1346 loss: 2.88140427e-07
Iter: 1347 loss: 2.87989849e-07
Iter: 1348 loss: 2.87993601e-07
Iter: 1349 loss: 2.87864736e-07
Iter: 1350 loss: 2.87868204e-07
Iter: 1351 loss: 2.87753721e-07
Iter: 1352 loss: 2.87754801e-07
Iter: 1353 loss: 2.876574e-07
Iter: 1354 loss: 2.87572249e-07
Iter: 1355 loss: 2.88526252e-07
Iter: 1356 loss: 2.87549142e-07
Iter: 1357 loss: 2.87439917e-07
Iter: 1358 loss: 2.87436592e-07
Iter: 1359 loss: 2.87359342e-07
Iter: 1360 loss: 2.87271746e-07
Iter: 1361 loss: 2.87084e-07
Iter: 1362 loss: 2.87079274e-07
Iter: 1363 loss: 2.8691835e-07
Iter: 1364 loss: 2.88179422e-07
Iter: 1365 loss: 2.86910108e-07
Iter: 1366 loss: 2.86762941e-07
Iter: 1367 loss: 2.88057322e-07
Iter: 1368 loss: 2.867543e-07
Iter: 1369 loss: 2.86630097e-07
Iter: 1370 loss: 2.86840645e-07
Iter: 1371 loss: 2.86595537e-07
Iter: 1372 loss: 2.86463489e-07
Iter: 1373 loss: 2.86284433e-07
Iter: 1374 loss: 2.86274712e-07
Iter: 1375 loss: 2.86149145e-07
Iter: 1376 loss: 2.86144285e-07
Iter: 1377 loss: 2.86020622e-07
Iter: 1378 loss: 2.86164578e-07
Iter: 1379 loss: 2.85984413e-07
Iter: 1380 loss: 2.85832982e-07
Iter: 1381 loss: 2.85925523e-07
Iter: 1382 loss: 2.85733165e-07
Iter: 1383 loss: 2.85668818e-07
Iter: 1384 loss: 2.85639231e-07
Iter: 1385 loss: 2.85594922e-07
Iter: 1386 loss: 2.85513408e-07
Iter: 1387 loss: 2.85512328e-07
Iter: 1388 loss: 2.85465234e-07
Iter: 1389 loss: 2.85455883e-07
Iter: 1390 loss: 2.85421038e-07
Iter: 1391 loss: 2.85308715e-07
Iter: 1392 loss: 2.8583554e-07
Iter: 1393 loss: 2.85277764e-07
Iter: 1394 loss: 2.8511829e-07
Iter: 1395 loss: 2.8549e-07
Iter: 1396 loss: 2.85096803e-07
Iter: 1397 loss: 2.84979393e-07
Iter: 1398 loss: 2.84972714e-07
Iter: 1399 loss: 2.84893844e-07
Iter: 1400 loss: 2.84963164e-07
Iter: 1401 loss: 2.84820089e-07
Iter: 1402 loss: 2.84704697e-07
Iter: 1403 loss: 2.84724536e-07
Iter: 1404 loss: 2.84597547e-07
Iter: 1405 loss: 2.84466864e-07
Iter: 1406 loss: 2.85417855e-07
Iter: 1407 loss: 2.84476016e-07
Iter: 1408 loss: 2.84364575e-07
Iter: 1409 loss: 2.84934913e-07
Iter: 1410 loss: 2.84322596e-07
Iter: 1411 loss: 2.84231845e-07
Iter: 1412 loss: 2.84358237e-07
Iter: 1413 loss: 2.84162468e-07
Iter: 1414 loss: 2.84092039e-07
Iter: 1415 loss: 2.84737638e-07
Iter: 1416 loss: 2.84062821e-07
Iter: 1417 loss: 2.8395695e-07
Iter: 1418 loss: 2.83992108e-07
Iter: 1419 loss: 2.83890358e-07
Iter: 1420 loss: 2.83820839e-07
Iter: 1421 loss: 2.83812824e-07
Iter: 1422 loss: 2.83750865e-07
Iter: 1423 loss: 2.8362939e-07
Iter: 1424 loss: 2.86474261e-07
Iter: 1425 loss: 2.83636268e-07
Iter: 1426 loss: 2.83503482e-07
Iter: 1427 loss: 2.83528e-07
Iter: 1428 loss: 2.83402983e-07
Iter: 1429 loss: 2.83230179e-07
Iter: 1430 loss: 2.8372358e-07
Iter: 1431 loss: 2.83189763e-07
Iter: 1432 loss: 2.83074826e-07
Iter: 1433 loss: 2.83065219e-07
Iter: 1434 loss: 2.82995018e-07
Iter: 1435 loss: 2.82968813e-07
Iter: 1436 loss: 2.82928681e-07
Iter: 1437 loss: 2.82827614e-07
Iter: 1438 loss: 2.82931239e-07
Iter: 1439 loss: 2.82743315e-07
Iter: 1440 loss: 2.82644322e-07
Iter: 1441 loss: 2.83606369e-07
Iter: 1442 loss: 2.82644947e-07
Iter: 1443 loss: 2.82543482e-07
Iter: 1444 loss: 2.82777506e-07
Iter: 1445 loss: 2.82518215e-07
Iter: 1446 loss: 2.82436929e-07
Iter: 1447 loss: 2.82560961e-07
Iter: 1448 loss: 2.82408507e-07
Iter: 1449 loss: 2.8231031e-07
Iter: 1450 loss: 2.8316569e-07
Iter: 1451 loss: 2.82293229e-07
Iter: 1452 loss: 2.82234339e-07
Iter: 1453 loss: 2.82410184e-07
Iter: 1454 loss: 2.82231298e-07
Iter: 1455 loss: 2.82174256e-07
Iter: 1456 loss: 2.82169395e-07
Iter: 1457 loss: 2.82120709e-07
Iter: 1458 loss: 2.82052781e-07
Iter: 1459 loss: 2.81942761e-07
Iter: 1460 loss: 2.81955295e-07
Iter: 1461 loss: 2.81811367e-07
Iter: 1462 loss: 2.82017766e-07
Iter: 1463 loss: 2.81742359e-07
Iter: 1464 loss: 2.8161719e-07
Iter: 1465 loss: 2.83257691e-07
Iter: 1466 loss: 2.8161972e-07
Iter: 1467 loss: 2.81524819e-07
Iter: 1468 loss: 2.81648852e-07
Iter: 1469 loss: 2.81457602e-07
Iter: 1470 loss: 2.81335275e-07
Iter: 1471 loss: 2.81350623e-07
Iter: 1472 loss: 2.81252937e-07
Iter: 1473 loss: 2.81121061e-07
Iter: 1474 loss: 2.81892909e-07
Iter: 1475 loss: 2.81111369e-07
Iter: 1476 loss: 2.80951838e-07
Iter: 1477 loss: 2.81603207e-07
Iter: 1478 loss: 2.80934444e-07
Iter: 1479 loss: 2.80838265e-07
Iter: 1480 loss: 2.80897922e-07
Iter: 1481 loss: 2.80775339e-07
Iter: 1482 loss: 2.80689619e-07
Iter: 1483 loss: 2.80688596e-07
Iter: 1484 loss: 2.80624e-07
Iter: 1485 loss: 2.80606088e-07
Iter: 1486 loss: 2.80558027e-07
Iter: 1487 loss: 2.80481e-07
Iter: 1488 loss: 2.81308985e-07
Iter: 1489 loss: 2.80468328e-07
Iter: 1490 loss: 2.80434278e-07
Iter: 1491 loss: 2.8036078e-07
Iter: 1492 loss: 2.8035987e-07
Iter: 1493 loss: 2.80277391e-07
Iter: 1494 loss: 2.80231291e-07
Iter: 1495 loss: 2.80200169e-07
Iter: 1496 loss: 2.80109191e-07
Iter: 1497 loss: 2.81004475e-07
Iter: 1498 loss: 2.80096202e-07
Iter: 1499 loss: 2.80011619e-07
Iter: 1500 loss: 2.80767154e-07
Iter: 1501 loss: 2.80020487e-07
Iter: 1502 loss: 2.79948722e-07
Iter: 1503 loss: 2.7986286e-07
Iter: 1504 loss: 2.79846603e-07
Iter: 1505 loss: 2.79725754e-07
Iter: 1506 loss: 2.79850639e-07
Iter: 1507 loss: 2.79658593e-07
Iter: 1508 loss: 2.79553205e-07
Iter: 1509 loss: 2.79545702e-07
Iter: 1510 loss: 2.7946902e-07
Iter: 1511 loss: 2.79432982e-07
Iter: 1512 loss: 2.79382334e-07
Iter: 1513 loss: 2.79267027e-07
Iter: 1514 loss: 2.801292e-07
Iter: 1515 loss: 2.79262792e-07
Iter: 1516 loss: 2.79133161e-07
Iter: 1517 loss: 2.79324098e-07
Iter: 1518 loss: 2.79077909e-07
Iter: 1519 loss: 2.78998925e-07
Iter: 1520 loss: 2.80044787e-07
Iter: 1521 loss: 2.78997334e-07
Iter: 1522 loss: 2.7892969e-07
Iter: 1523 loss: 2.78809352e-07
Iter: 1524 loss: 2.7881353e-07
Iter: 1525 loss: 2.78679778e-07
Iter: 1526 loss: 2.78571889e-07
Iter: 1527 loss: 2.78529967e-07
Iter: 1528 loss: 2.78346903e-07
Iter: 1529 loss: 2.79184178e-07
Iter: 1530 loss: 2.78322233e-07
Iter: 1531 loss: 2.78158552e-07
Iter: 1532 loss: 2.79881078e-07
Iter: 1533 loss: 2.78165089e-07
Iter: 1534 loss: 2.78066295e-07
Iter: 1535 loss: 2.7898389e-07
Iter: 1536 loss: 2.7806081e-07
Iter: 1537 loss: 2.78023293e-07
Iter: 1538 loss: 2.77893605e-07
Iter: 1539 loss: 2.80328777e-07
Iter: 1540 loss: 2.77891161e-07
Iter: 1541 loss: 2.77761927e-07
Iter: 1542 loss: 2.78536788e-07
Iter: 1543 loss: 2.77777076e-07
Iter: 1544 loss: 2.77644631e-07
Iter: 1545 loss: 2.78344658e-07
Iter: 1546 loss: 2.77643153e-07
Iter: 1547 loss: 2.77558513e-07
Iter: 1548 loss: 2.77575651e-07
Iter: 1549 loss: 2.77495133e-07
Iter: 1550 loss: 2.77413903e-07
Iter: 1551 loss: 2.77424959e-07
Iter: 1552 loss: 2.77360613e-07
Iter: 1553 loss: 2.77321078e-07
Iter: 1554 loss: 2.77311017e-07
Iter: 1555 loss: 2.77207675e-07
Iter: 1556 loss: 2.777237e-07
Iter: 1557 loss: 2.77189201e-07
Iter: 1558 loss: 2.77104306e-07
Iter: 1559 loss: 2.76967029e-07
Iter: 1560 loss: 2.79691562e-07
Iter: 1561 loss: 2.76965295e-07
Iter: 1562 loss: 2.76829979e-07
Iter: 1563 loss: 2.77315337e-07
Iter: 1564 loss: 2.76787773e-07
Iter: 1565 loss: 2.76653935e-07
Iter: 1566 loss: 2.76861641e-07
Iter: 1567 loss: 2.76556761e-07
Iter: 1568 loss: 2.76421162e-07
Iter: 1569 loss: 2.77892269e-07
Iter: 1570 loss: 2.76422213e-07
Iter: 1571 loss: 2.76267286e-07
Iter: 1572 loss: 2.76395383e-07
Iter: 1573 loss: 2.76166702e-07
Iter: 1574 loss: 2.7604014e-07
Iter: 1575 loss: 2.76275529e-07
Iter: 1576 loss: 2.75968546e-07
Iter: 1577 loss: 2.7587177e-07
Iter: 1578 loss: 2.76766571e-07
Iter: 1579 loss: 2.75853836e-07
Iter: 1580 loss: 2.75729775e-07
Iter: 1581 loss: 2.7596613e-07
Iter: 1582 loss: 2.75679099e-07
Iter: 1583 loss: 2.75581272e-07
Iter: 1584 loss: 2.76141492e-07
Iter: 1585 loss: 2.75586245e-07
Iter: 1586 loss: 2.75467755e-07
Iter: 1587 loss: 2.75813534e-07
Iter: 1588 loss: 2.75441181e-07
Iter: 1589 loss: 2.75378397e-07
Iter: 1590 loss: 2.75803416e-07
Iter: 1591 loss: 2.7535603e-07
Iter: 1592 loss: 2.75306775e-07
Iter: 1593 loss: 2.7521827e-07
Iter: 1594 loss: 2.75209345e-07
Iter: 1595 loss: 2.75088752e-07
Iter: 1596 loss: 2.75067777e-07
Iter: 1597 loss: 2.75015537e-07
Iter: 1598 loss: 2.74842648e-07
Iter: 1599 loss: 2.75280229e-07
Iter: 1600 loss: 2.74806041e-07
Iter: 1601 loss: 2.74660465e-07
Iter: 1602 loss: 2.75176916e-07
Iter: 1603 loss: 2.74618969e-07
Iter: 1604 loss: 2.74479873e-07
Iter: 1605 loss: 2.75969228e-07
Iter: 1606 loss: 2.74469045e-07
Iter: 1607 loss: 2.74403817e-07
Iter: 1608 loss: 2.74247611e-07
Iter: 1609 loss: 2.74254774e-07
Iter: 1610 loss: 2.74078985e-07
Iter: 1611 loss: 2.74812948e-07
Iter: 1612 loss: 2.74060454e-07
Iter: 1613 loss: 2.73900469e-07
Iter: 1614 loss: 2.75947e-07
Iter: 1615 loss: 2.73901463e-07
Iter: 1616 loss: 2.73812248e-07
Iter: 1617 loss: 2.73722833e-07
Iter: 1618 loss: 2.73716637e-07
Iter: 1619 loss: 2.73588796e-07
Iter: 1620 loss: 2.73583737e-07
Iter: 1621 loss: 2.73518054e-07
Iter: 1622 loss: 2.73514786e-07
Iter: 1623 loss: 2.73452315e-07
Iter: 1624 loss: 2.73315777e-07
Iter: 1625 loss: 2.73652745e-07
Iter: 1626 loss: 2.73272292e-07
Iter: 1627 loss: 2.7317725e-07
Iter: 1628 loss: 2.73031333e-07
Iter: 1629 loss: 2.73032299e-07
Iter: 1630 loss: 2.7286211e-07
Iter: 1631 loss: 2.73234207e-07
Iter: 1632 loss: 2.72808052e-07
Iter: 1633 loss: 2.72677028e-07
Iter: 1634 loss: 2.73039774e-07
Iter: 1635 loss: 2.72603842e-07
Iter: 1636 loss: 2.72523295e-07
Iter: 1637 loss: 2.72529121e-07
Iter: 1638 loss: 2.72421e-07
Iter: 1639 loss: 2.72321842e-07
Iter: 1640 loss: 2.72308057e-07
Iter: 1641 loss: 2.72178966e-07
Iter: 1642 loss: 2.72392214e-07
Iter: 1643 loss: 2.72115813e-07
Iter: 1644 loss: 2.71975694e-07
Iter: 1645 loss: 2.73171878e-07
Iter: 1646 loss: 2.71977683e-07
Iter: 1647 loss: 2.71831965e-07
Iter: 1648 loss: 2.7185871e-07
Iter: 1649 loss: 2.71726122e-07
Iter: 1650 loss: 2.71622525e-07
Iter: 1651 loss: 2.71623236e-07
Iter: 1652 loss: 2.71543627e-07
Iter: 1653 loss: 2.71554711e-07
Iter: 1654 loss: 2.71483913e-07
Iter: 1655 loss: 2.71384408e-07
Iter: 1656 loss: 2.72069798e-07
Iter: 1657 loss: 2.71364172e-07
Iter: 1658 loss: 2.71270125e-07
Iter: 1659 loss: 2.71159422e-07
Iter: 1660 loss: 2.71161042e-07
Iter: 1661 loss: 2.71060856e-07
Iter: 1662 loss: 2.71172553e-07
Iter: 1663 loss: 2.70982838e-07
Iter: 1664 loss: 2.70840474e-07
Iter: 1665 loss: 2.71287547e-07
Iter: 1666 loss: 2.70832373e-07
Iter: 1667 loss: 2.70690123e-07
Iter: 1668 loss: 2.71223058e-07
Iter: 1669 loss: 2.70678242e-07
Iter: 1670 loss: 2.70561941e-07
Iter: 1671 loss: 2.71420731e-07
Iter: 1672 loss: 2.70557194e-07
Iter: 1673 loss: 2.70467581e-07
Iter: 1674 loss: 2.70425687e-07
Iter: 1675 loss: 2.70395788e-07
Iter: 1676 loss: 2.70271642e-07
Iter: 1677 loss: 2.70625037e-07
Iter: 1678 loss: 2.70230828e-07
Iter: 1679 loss: 2.701168e-07
Iter: 1680 loss: 2.71385886e-07
Iter: 1681 loss: 2.70109751e-07
Iter: 1682 loss: 2.70029318e-07
Iter: 1683 loss: 2.70158722e-07
Iter: 1684 loss: 2.69990323e-07
Iter: 1685 loss: 2.69902046e-07
Iter: 1686 loss: 2.70434498e-07
Iter: 1687 loss: 2.69906224e-07
Iter: 1688 loss: 2.69843724e-07
Iter: 1689 loss: 2.69973384e-07
Iter: 1690 loss: 2.69804616e-07
Iter: 1691 loss: 2.69759823e-07
Iter: 1692 loss: 2.69740099e-07
Iter: 1693 loss: 2.69690219e-07
Iter: 1694 loss: 2.69603817e-07
Iter: 1695 loss: 2.69554675e-07
Iter: 1696 loss: 2.69515454e-07
Iter: 1697 loss: 2.6942422e-07
Iter: 1698 loss: 2.69874022e-07
Iter: 1699 loss: 2.69376073e-07
Iter: 1700 loss: 2.69314853e-07
Iter: 1701 loss: 2.69455e-07
Iter: 1702 loss: 2.69270686e-07
Iter: 1703 loss: 2.69181044e-07
Iter: 1704 loss: 2.70574361e-07
Iter: 1705 loss: 2.69172688e-07
Iter: 1706 loss: 2.69084779e-07
Iter: 1707 loss: 2.69041692e-07
Iter: 1708 loss: 2.69027595e-07
Iter: 1709 loss: 2.68924282e-07
Iter: 1710 loss: 2.69053487e-07
Iter: 1711 loss: 2.68865278e-07
Iter: 1712 loss: 2.68748096e-07
Iter: 1713 loss: 2.70101964e-07
Iter: 1714 loss: 2.68764e-07
Iter: 1715 loss: 2.68657971e-07
Iter: 1716 loss: 2.68589758e-07
Iter: 1717 loss: 2.68577082e-07
Iter: 1718 loss: 2.68452879e-07
Iter: 1719 loss: 2.6845396e-07
Iter: 1720 loss: 2.68406751e-07
Iter: 1721 loss: 2.68379608e-07
Iter: 1722 loss: 2.68343541e-07
Iter: 1723 loss: 2.68243355e-07
Iter: 1724 loss: 2.68503129e-07
Iter: 1725 loss: 2.68215928e-07
Iter: 1726 loss: 2.68142e-07
Iter: 1727 loss: 2.68053725e-07
Iter: 1728 loss: 2.68069215e-07
Iter: 1729 loss: 2.67938304e-07
Iter: 1730 loss: 2.68151666e-07
Iter: 1731 loss: 2.67872053e-07
Iter: 1732 loss: 2.67768741e-07
Iter: 1733 loss: 2.68086126e-07
Iter: 1734 loss: 2.67751545e-07
Iter: 1735 loss: 2.67639621e-07
Iter: 1736 loss: 2.68391943e-07
Iter: 1737 loss: 2.67628224e-07
Iter: 1738 loss: 2.6750709e-07
Iter: 1739 loss: 2.67877851e-07
Iter: 1740 loss: 2.67466703e-07
Iter: 1741 loss: 2.67379562e-07
Iter: 1742 loss: 2.67288044e-07
Iter: 1743 loss: 2.67283667e-07
Iter: 1744 loss: 2.67146e-07
Iter: 1745 loss: 2.68541e-07
Iter: 1746 loss: 2.67150085e-07
Iter: 1747 loss: 2.67029634e-07
Iter: 1748 loss: 2.67384877e-07
Iter: 1749 loss: 2.67011899e-07
Iter: 1750 loss: 2.66937661e-07
Iter: 1751 loss: 2.67624557e-07
Iter: 1752 loss: 2.66910774e-07
Iter: 1753 loss: 2.6683432e-07
Iter: 1754 loss: 2.66908302e-07
Iter: 1755 loss: 2.66791517e-07
Iter: 1756 loss: 2.66706365e-07
Iter: 1757 loss: 2.6709759e-07
Iter: 1758 loss: 2.66697782e-07
Iter: 1759 loss: 2.66641678e-07
Iter: 1760 loss: 2.66597453e-07
Iter: 1761 loss: 2.66551268e-07
Iter: 1762 loss: 2.66469954e-07
Iter: 1763 loss: 2.66461313e-07
Iter: 1764 loss: 2.6637565e-07
Iter: 1765 loss: 2.66232149e-07
Iter: 1766 loss: 2.66553428e-07
Iter: 1767 loss: 2.66168229e-07
Iter: 1768 loss: 2.65984511e-07
Iter: 1769 loss: 2.66565252e-07
Iter: 1770 loss: 2.65943527e-07
Iter: 1771 loss: 2.65808069e-07
Iter: 1772 loss: 2.67899e-07
Iter: 1773 loss: 2.65817675e-07
Iter: 1774 loss: 2.65723514e-07
Iter: 1775 loss: 2.65774531e-07
Iter: 1776 loss: 2.65643394e-07
Iter: 1777 loss: 2.65543633e-07
Iter: 1778 loss: 2.65529195e-07
Iter: 1779 loss: 2.65443305e-07
Iter: 1780 loss: 2.65345165e-07
Iter: 1781 loss: 2.65322797e-07
Iter: 1782 loss: 2.65245575e-07
Iter: 1783 loss: 2.65492787e-07
Iter: 1784 loss: 2.6522423e-07
Iter: 1785 loss: 2.65146639e-07
Iter: 1786 loss: 2.65761571e-07
Iter: 1787 loss: 2.65146696e-07
Iter: 1788 loss: 2.6509548e-07
Iter: 1789 loss: 2.65085276e-07
Iter: 1790 loss: 2.65060919e-07
Iter: 1791 loss: 2.64968293e-07
Iter: 1792 loss: 2.65207433e-07
Iter: 1793 loss: 2.64969174e-07
Iter: 1794 loss: 2.64894851e-07
Iter: 1795 loss: 2.64799723e-07
Iter: 1796 loss: 2.64798047e-07
Iter: 1797 loss: 2.64694e-07
Iter: 1798 loss: 2.65165312e-07
Iter: 1799 loss: 2.64667165e-07
Iter: 1800 loss: 2.64554672e-07
Iter: 1801 loss: 2.64738617e-07
Iter: 1802 loss: 2.64503683e-07
Iter: 1803 loss: 2.6440074e-07
Iter: 1804 loss: 2.65076039e-07
Iter: 1805 loss: 2.6438903e-07
Iter: 1806 loss: 2.64275428e-07
Iter: 1807 loss: 2.64692744e-07
Iter: 1808 loss: 2.6423757e-07
Iter: 1809 loss: 2.64117887e-07
Iter: 1810 loss: 2.63989307e-07
Iter: 1811 loss: 2.63959947e-07
Iter: 1812 loss: 2.63813348e-07
Iter: 1813 loss: 2.64991513e-07
Iter: 1814 loss: 2.63814655e-07
Iter: 1815 loss: 2.63687298e-07
Iter: 1816 loss: 2.65168921e-07
Iter: 1817 loss: 2.63677094e-07
Iter: 1818 loss: 2.63607376e-07
Iter: 1819 loss: 2.63891906e-07
Iter: 1820 loss: 2.63585775e-07
Iter: 1821 loss: 2.63521e-07
Iter: 1822 loss: 2.63668852e-07
Iter: 1823 loss: 2.6348178e-07
Iter: 1824 loss: 2.63423829e-07
Iter: 1825 loss: 2.63569405e-07
Iter: 1826 loss: 2.63413654e-07
Iter: 1827 loss: 2.63330435e-07
Iter: 1828 loss: 2.63298602e-07
Iter: 1829 loss: 2.63258585e-07
Iter: 1830 loss: 2.63144273e-07
Iter: 1831 loss: 2.63233574e-07
Iter: 1832 loss: 2.63110934e-07
Iter: 1833 loss: 2.62982866e-07
Iter: 1834 loss: 2.63095e-07
Iter: 1835 loss: 2.62914853e-07
Iter: 1836 loss: 2.62801279e-07
Iter: 1837 loss: 2.63131511e-07
Iter: 1838 loss: 2.62723631e-07
Iter: 1839 loss: 2.62622791e-07
Iter: 1840 loss: 2.64312035e-07
Iter: 1841 loss: 2.62617846e-07
Iter: 1842 loss: 2.62518228e-07
Iter: 1843 loss: 2.62836608e-07
Iter: 1844 loss: 2.62493984e-07
Iter: 1845 loss: 2.624096e-07
Iter: 1846 loss: 2.62311914e-07
Iter: 1847 loss: 2.62298073e-07
Iter: 1848 loss: 2.62192259e-07
Iter: 1849 loss: 2.63642534e-07
Iter: 1850 loss: 2.62206584e-07
Iter: 1851 loss: 2.62104663e-07
Iter: 1852 loss: 2.62829445e-07
Iter: 1853 loss: 2.62103271e-07
Iter: 1854 loss: 2.62053618e-07
Iter: 1855 loss: 2.6221042e-07
Iter: 1856 loss: 2.62044324e-07
Iter: 1857 loss: 2.61982279e-07
Iter: 1858 loss: 2.61924981e-07
Iter: 1859 loss: 2.61904177e-07
Iter: 1860 loss: 2.61835055e-07
Iter: 1861 loss: 2.6228804e-07
Iter: 1862 loss: 2.61828632e-07
Iter: 1863 loss: 2.61756554e-07
Iter: 1864 loss: 2.61711165e-07
Iter: 1865 loss: 2.61688456e-07
Iter: 1866 loss: 2.61567777e-07
Iter: 1867 loss: 2.61648438e-07
Iter: 1868 loss: 2.6150181e-07
Iter: 1869 loss: 2.61365358e-07
Iter: 1870 loss: 2.61699057e-07
Iter: 1871 loss: 2.61335742e-07
Iter: 1872 loss: 2.61190507e-07
Iter: 1873 loss: 2.61586479e-07
Iter: 1874 loss: 2.61150717e-07
Iter: 1875 loss: 2.61044875e-07
Iter: 1876 loss: 2.6207988e-07
Iter: 1877 loss: 2.61025747e-07
Iter: 1878 loss: 2.60943978e-07
Iter: 1879 loss: 2.61343075e-07
Iter: 1880 loss: 2.60926953e-07
Iter: 1881 loss: 2.6086559e-07
Iter: 1882 loss: 2.60834611e-07
Iter: 1883 loss: 2.60791808e-07
Iter: 1884 loss: 2.60727745e-07
Iter: 1885 loss: 2.60728825e-07
Iter: 1886 loss: 2.60694435e-07
Iter: 1887 loss: 2.60827903e-07
Iter: 1888 loss: 2.60649813e-07
Iter: 1889 loss: 2.60608658e-07
Iter: 1890 loss: 2.60800618e-07
Iter: 1891 loss: 2.60595129e-07
Iter: 1892 loss: 2.60548461e-07
Iter: 1893 loss: 2.60488662e-07
Iter: 1894 loss: 2.60504379e-07
Iter: 1895 loss: 2.60418346e-07
Iter: 1896 loss: 2.60636455e-07
Iter: 1897 loss: 2.60381796e-07
Iter: 1898 loss: 2.60301761e-07
Iter: 1899 loss: 2.60573074e-07
Iter: 1900 loss: 2.60284708e-07
Iter: 1901 loss: 2.60199243e-07
Iter: 1902 loss: 2.60103462e-07
Iter: 1903 loss: 2.60101444e-07
Iter: 1904 loss: 2.59963656e-07
Iter: 1905 loss: 2.60335156e-07
Iter: 1906 loss: 2.59896353e-07
Iter: 1907 loss: 2.59745406e-07
Iter: 1908 loss: 2.60258673e-07
Iter: 1909 loss: 2.59703171e-07
Iter: 1910 loss: 2.59608868e-07
Iter: 1911 loss: 2.60727461e-07
Iter: 1912 loss: 2.59604235e-07
Iter: 1913 loss: 2.59521499e-07
Iter: 1914 loss: 2.59795911e-07
Iter: 1915 loss: 2.5948566e-07
Iter: 1916 loss: 2.59402697e-07
Iter: 1917 loss: 2.59336701e-07
Iter: 1918 loss: 2.59321212e-07
Iter: 1919 loss: 2.59279432e-07
Iter: 1920 loss: 2.5925965e-07
Iter: 1921 loss: 2.5919951e-07
Iter: 1922 loss: 2.59144599e-07
Iter: 1923 loss: 2.5912766e-07
Iter: 1924 loss: 2.59081759e-07
Iter: 1925 loss: 2.59794035e-07
Iter: 1926 loss: 2.59056151e-07
Iter: 1927 loss: 2.59019657e-07
Iter: 1928 loss: 2.58994419e-07
Iter: 1929 loss: 2.58952497e-07
Iter: 1930 loss: 2.58890253e-07
Iter: 1931 loss: 2.59224e-07
Iter: 1932 loss: 2.58883347e-07
Iter: 1933 loss: 2.58808853e-07
Iter: 1934 loss: 2.58811525e-07
Iter: 1935 loss: 2.58766164e-07
Iter: 1936 loss: 2.58674334e-07
Iter: 1937 loss: 2.58778357e-07
Iter: 1938 loss: 2.5863784e-07
Iter: 1939 loss: 2.5856275e-07
Iter: 1940 loss: 2.58698776e-07
Iter: 1941 loss: 2.58528871e-07
Iter: 1942 loss: 2.58441645e-07
Iter: 1943 loss: 2.58747974e-07
Iter: 1944 loss: 2.58424109e-07
Iter: 1945 loss: 2.58360842e-07
Iter: 1946 loss: 2.58361553e-07
Iter: 1947 loss: 2.58312696e-07
Iter: 1948 loss: 2.58225555e-07
Iter: 1949 loss: 2.58209951e-07
Iter: 1950 loss: 2.58167177e-07
Iter: 1951 loss: 2.58150806e-07
Iter: 1952 loss: 2.58104706e-07
Iter: 1953 loss: 2.58145576e-07
Iter: 1954 loss: 2.58084413e-07
Iter: 1955 loss: 2.58014836e-07
Iter: 1956 loss: 2.58132388e-07
Iter: 1957 loss: 2.57991985e-07
Iter: 1958 loss: 2.5790132e-07
Iter: 1959 loss: 2.58173316e-07
Iter: 1960 loss: 2.57885375e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi1.6/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2
+ date
Mon Oct 26 16:31:06 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 1 --phi 2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f01311e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f0104620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f0104e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f021bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f01ed510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f01ed0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f0098c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f0062620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f002a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f002ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f002a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9607ac8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd960781f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd960776c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9607767b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd96072a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd96072a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9606c2c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd96068b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9606772f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd960673268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd960673e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9605c2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9605e8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9605e86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9605a88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd960547840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9605107b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd96050c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd960512268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9604e4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9604a6268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9604a52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd96043f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd96045db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9603fc7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.31452643e-05
Iter: 2 loss: 1.68458755e-05
Iter: 3 loss: 1.06716579e-05
Iter: 4 loss: 9.36872675e-06
Iter: 5 loss: 1.05346626e-05
Iter: 6 loss: 8.60961e-06
Iter: 7 loss: 8.02658542e-06
Iter: 8 loss: 8.08306686e-06
Iter: 9 loss: 7.5766543e-06
Iter: 10 loss: 6.95966264e-06
Iter: 11 loss: 1.10314922e-05
Iter: 12 loss: 6.89552826e-06
Iter: 13 loss: 6.69621659e-06
Iter: 14 loss: 7.29766634e-06
Iter: 15 loss: 6.63650098e-06
Iter: 16 loss: 6.41554107e-06
Iter: 17 loss: 7.44989711e-06
Iter: 18 loss: 6.37486164e-06
Iter: 19 loss: 6.19797584e-06
Iter: 20 loss: 6.41285624e-06
Iter: 21 loss: 6.10544475e-06
Iter: 22 loss: 5.89833053e-06
Iter: 23 loss: 6.13717384e-06
Iter: 24 loss: 5.78761137e-06
Iter: 25 loss: 5.54933467e-06
Iter: 26 loss: 6.57959527e-06
Iter: 27 loss: 5.50047525e-06
Iter: 28 loss: 5.31080423e-06
Iter: 29 loss: 5.02814146e-06
Iter: 30 loss: 5.02216153e-06
Iter: 31 loss: 4.69206816e-06
Iter: 32 loss: 6.14199962e-06
Iter: 33 loss: 4.62606386e-06
Iter: 34 loss: 4.33497553e-06
Iter: 35 loss: 6.75525644e-06
Iter: 36 loss: 4.31735134e-06
Iter: 37 loss: 4.29519332e-06
Iter: 38 loss: 4.24497102e-06
Iter: 39 loss: 4.15943396e-06
Iter: 40 loss: 3.93498976e-06
Iter: 41 loss: 5.59788168e-06
Iter: 42 loss: 3.88812396e-06
Iter: 43 loss: 3.7598229e-06
Iter: 44 loss: 5.61078787e-06
Iter: 45 loss: 3.75945569e-06
Iter: 46 loss: 3.70118028e-06
Iter: 47 loss: 3.84545729e-06
Iter: 48 loss: 3.68037627e-06
Iter: 49 loss: 3.60347053e-06
Iter: 50 loss: 3.62277842e-06
Iter: 51 loss: 3.54743452e-06
Iter: 52 loss: 3.50460755e-06
Iter: 53 loss: 3.50412574e-06
Iter: 54 loss: 3.4575778e-06
Iter: 55 loss: 3.36126e-06
Iter: 56 loss: 5.02707553e-06
Iter: 57 loss: 3.35915774e-06
Iter: 58 loss: 3.25597648e-06
Iter: 59 loss: 4.254136e-06
Iter: 60 loss: 3.25226029e-06
Iter: 61 loss: 3.19204764e-06
Iter: 62 loss: 3.3162064e-06
Iter: 63 loss: 3.1679408e-06
Iter: 64 loss: 3.08704512e-06
Iter: 65 loss: 3.05564436e-06
Iter: 66 loss: 3.01194359e-06
Iter: 67 loss: 2.93309586e-06
Iter: 68 loss: 3.24461553e-06
Iter: 69 loss: 2.91528295e-06
Iter: 70 loss: 2.83997042e-06
Iter: 71 loss: 3.14237855e-06
Iter: 72 loss: 2.82318047e-06
Iter: 73 loss: 2.75486946e-06
Iter: 74 loss: 2.85350052e-06
Iter: 75 loss: 2.72162856e-06
Iter: 76 loss: 2.72209377e-06
Iter: 77 loss: 2.6821067e-06
Iter: 78 loss: 2.66879215e-06
Iter: 79 loss: 2.62763183e-06
Iter: 80 loss: 2.70386363e-06
Iter: 81 loss: 2.60070556e-06
Iter: 82 loss: 2.54711381e-06
Iter: 83 loss: 3.06583752e-06
Iter: 84 loss: 2.54505585e-06
Iter: 85 loss: 2.5095851e-06
Iter: 86 loss: 2.63040647e-06
Iter: 87 loss: 2.49995514e-06
Iter: 88 loss: 2.45454976e-06
Iter: 89 loss: 2.47053595e-06
Iter: 90 loss: 2.42262013e-06
Iter: 91 loss: 2.3951352e-06
Iter: 92 loss: 2.39395331e-06
Iter: 93 loss: 2.37042741e-06
Iter: 94 loss: 2.30882415e-06
Iter: 95 loss: 2.7877245e-06
Iter: 96 loss: 2.29696298e-06
Iter: 97 loss: 2.25025087e-06
Iter: 98 loss: 2.24776454e-06
Iter: 99 loss: 2.22305471e-06
Iter: 100 loss: 2.24877977e-06
Iter: 101 loss: 2.20920356e-06
Iter: 102 loss: 2.17205752e-06
Iter: 103 loss: 2.1741148e-06
Iter: 104 loss: 2.14290685e-06
Iter: 105 loss: 2.10893404e-06
Iter: 106 loss: 2.19757203e-06
Iter: 107 loss: 2.09741529e-06
Iter: 108 loss: 2.05577953e-06
Iter: 109 loss: 2.22214931e-06
Iter: 110 loss: 2.04660682e-06
Iter: 111 loss: 2.05692595e-06
Iter: 112 loss: 2.03271202e-06
Iter: 113 loss: 2.02628325e-06
Iter: 114 loss: 2.00496811e-06
Iter: 115 loss: 1.98296175e-06
Iter: 116 loss: 1.97458235e-06
Iter: 117 loss: 1.93704477e-06
Iter: 118 loss: 2.28025579e-06
Iter: 119 loss: 1.93549386e-06
Iter: 120 loss: 1.90677713e-06
Iter: 121 loss: 1.99374426e-06
Iter: 122 loss: 1.89830303e-06
Iter: 123 loss: 1.87852197e-06
Iter: 124 loss: 2.07091216e-06
Iter: 125 loss: 1.87769683e-06
Iter: 126 loss: 1.85648275e-06
Iter: 127 loss: 1.8915548e-06
Iter: 128 loss: 1.84690111e-06
Iter: 129 loss: 1.83180987e-06
Iter: 130 loss: 1.87882324e-06
Iter: 131 loss: 1.82730992e-06
Iter: 132 loss: 1.80858046e-06
Iter: 133 loss: 1.81342455e-06
Iter: 134 loss: 1.794779e-06
Iter: 135 loss: 1.77285153e-06
Iter: 136 loss: 1.85654915e-06
Iter: 137 loss: 1.76757487e-06
Iter: 138 loss: 1.7443665e-06
Iter: 139 loss: 1.70652231e-06
Iter: 140 loss: 1.7063478e-06
Iter: 141 loss: 1.69302348e-06
Iter: 142 loss: 1.68964323e-06
Iter: 143 loss: 1.67195662e-06
Iter: 144 loss: 1.67195253e-06
Iter: 145 loss: 1.65794359e-06
Iter: 146 loss: 1.64645985e-06
Iter: 147 loss: 1.64248013e-06
Iter: 148 loss: 1.63532513e-06
Iter: 149 loss: 1.63001175e-06
Iter: 150 loss: 1.62770687e-06
Iter: 151 loss: 1.62091737e-06
Iter: 152 loss: 1.60281024e-06
Iter: 153 loss: 1.72556065e-06
Iter: 154 loss: 1.59847741e-06
Iter: 155 loss: 1.5781402e-06
Iter: 156 loss: 1.87212424e-06
Iter: 157 loss: 1.57812201e-06
Iter: 158 loss: 1.56360625e-06
Iter: 159 loss: 1.58313105e-06
Iter: 160 loss: 1.55626594e-06
Iter: 161 loss: 1.53988935e-06
Iter: 162 loss: 1.75668856e-06
Iter: 163 loss: 1.53976748e-06
Iter: 164 loss: 1.53141491e-06
Iter: 165 loss: 1.51737777e-06
Iter: 166 loss: 1.51733786e-06
Iter: 167 loss: 1.50032258e-06
Iter: 168 loss: 1.66815153e-06
Iter: 169 loss: 1.49973209e-06
Iter: 170 loss: 1.48930781e-06
Iter: 171 loss: 1.52796088e-06
Iter: 172 loss: 1.48670188e-06
Iter: 173 loss: 1.47588548e-06
Iter: 174 loss: 1.46010166e-06
Iter: 175 loss: 1.4597656e-06
Iter: 176 loss: 1.43963121e-06
Iter: 177 loss: 1.50932385e-06
Iter: 178 loss: 1.43429372e-06
Iter: 179 loss: 1.42210956e-06
Iter: 180 loss: 1.61672858e-06
Iter: 181 loss: 1.4221057e-06
Iter: 182 loss: 1.41807436e-06
Iter: 183 loss: 1.41726889e-06
Iter: 184 loss: 1.41207693e-06
Iter: 185 loss: 1.39898543e-06
Iter: 186 loss: 1.51860934e-06
Iter: 187 loss: 1.39698477e-06
Iter: 188 loss: 1.38471228e-06
Iter: 189 loss: 1.48394531e-06
Iter: 190 loss: 1.38397149e-06
Iter: 191 loss: 1.37578036e-06
Iter: 192 loss: 1.35934829e-06
Iter: 193 loss: 1.66320729e-06
Iter: 194 loss: 1.35914706e-06
Iter: 195 loss: 1.34735967e-06
Iter: 196 loss: 1.34737195e-06
Iter: 197 loss: 1.33690435e-06
Iter: 198 loss: 1.31950878e-06
Iter: 199 loss: 1.3194624e-06
Iter: 200 loss: 1.31666184e-06
Iter: 201 loss: 1.31227398e-06
Iter: 202 loss: 1.30433068e-06
Iter: 203 loss: 1.30198259e-06
Iter: 204 loss: 1.29741738e-06
Iter: 205 loss: 1.28600584e-06
Iter: 206 loss: 1.31299316e-06
Iter: 207 loss: 1.2818839e-06
Iter: 208 loss: 1.2742812e-06
Iter: 209 loss: 1.27424e-06
Iter: 210 loss: 1.26828013e-06
Iter: 211 loss: 1.25676229e-06
Iter: 212 loss: 1.36774167e-06
Iter: 213 loss: 1.25639258e-06
Iter: 214 loss: 1.24969461e-06
Iter: 215 loss: 1.26060399e-06
Iter: 216 loss: 1.24666849e-06
Iter: 217 loss: 1.23835468e-06
Iter: 218 loss: 1.23927225e-06
Iter: 219 loss: 1.23194104e-06
Iter: 220 loss: 1.22555559e-06
Iter: 221 loss: 1.2930916e-06
Iter: 222 loss: 1.22536608e-06
Iter: 223 loss: 1.21772814e-06
Iter: 224 loss: 1.25367683e-06
Iter: 225 loss: 1.21629671e-06
Iter: 226 loss: 1.21180472e-06
Iter: 227 loss: 1.20372817e-06
Iter: 228 loss: 1.40450038e-06
Iter: 229 loss: 1.20365371e-06
Iter: 230 loss: 1.19449521e-06
Iter: 231 loss: 1.23422546e-06
Iter: 232 loss: 1.19253571e-06
Iter: 233 loss: 1.18505943e-06
Iter: 234 loss: 1.20337063e-06
Iter: 235 loss: 1.18247385e-06
Iter: 236 loss: 1.17499678e-06
Iter: 237 loss: 1.16419e-06
Iter: 238 loss: 1.16378624e-06
Iter: 239 loss: 1.16347849e-06
Iter: 240 loss: 1.15859689e-06
Iter: 241 loss: 1.15487296e-06
Iter: 242 loss: 1.16243905e-06
Iter: 243 loss: 1.15337048e-06
Iter: 244 loss: 1.14838144e-06
Iter: 245 loss: 1.13877354e-06
Iter: 246 loss: 1.34242862e-06
Iter: 247 loss: 1.13870135e-06
Iter: 248 loss: 1.12988596e-06
Iter: 249 loss: 1.15335581e-06
Iter: 250 loss: 1.12677264e-06
Iter: 251 loss: 1.11787301e-06
Iter: 252 loss: 1.23580412e-06
Iter: 253 loss: 1.11790268e-06
Iter: 254 loss: 1.11262602e-06
Iter: 255 loss: 1.11680697e-06
Iter: 256 loss: 1.10947963e-06
Iter: 257 loss: 1.10203769e-06
Iter: 258 loss: 1.11341342e-06
Iter: 259 loss: 1.09841847e-06
Iter: 260 loss: 1.09479561e-06
Iter: 261 loss: 1.09320206e-06
Iter: 262 loss: 1.09206167e-06
Iter: 263 loss: 1.08852055e-06
Iter: 264 loss: 1.08795632e-06
Iter: 265 loss: 1.08458539e-06
Iter: 266 loss: 1.07571623e-06
Iter: 267 loss: 1.11327881e-06
Iter: 268 loss: 1.07383926e-06
Iter: 269 loss: 1.06845846e-06
Iter: 270 loss: 1.10345718e-06
Iter: 271 loss: 1.06793937e-06
Iter: 272 loss: 1.06216885e-06
Iter: 273 loss: 1.06377274e-06
Iter: 274 loss: 1.05798881e-06
Iter: 275 loss: 1.05311346e-06
Iter: 276 loss: 1.08493873e-06
Iter: 277 loss: 1.05265769e-06
Iter: 278 loss: 1.04788251e-06
Iter: 279 loss: 1.07131177e-06
Iter: 280 loss: 1.0469945e-06
Iter: 281 loss: 1.04281446e-06
Iter: 282 loss: 1.04277046e-06
Iter: 283 loss: 1.03959974e-06
Iter: 284 loss: 1.03424645e-06
Iter: 285 loss: 1.02992726e-06
Iter: 286 loss: 1.02832837e-06
Iter: 287 loss: 1.02014485e-06
Iter: 288 loss: 1.06178152e-06
Iter: 289 loss: 1.01877345e-06
Iter: 290 loss: 1.01201545e-06
Iter: 291 loss: 1.09620771e-06
Iter: 292 loss: 1.01196883e-06
Iter: 293 loss: 1.00931504e-06
Iter: 294 loss: 1.04908781e-06
Iter: 295 loss: 1.00934631e-06
Iter: 296 loss: 1.00687e-06
Iter: 297 loss: 1.01215926e-06
Iter: 298 loss: 1.00585635e-06
Iter: 299 loss: 1.00370164e-06
Iter: 300 loss: 9.9835222e-07
Iter: 301 loss: 1.05262552e-06
Iter: 302 loss: 9.97680104e-07
Iter: 303 loss: 9.92747118e-07
Iter: 304 loss: 1.00049863e-06
Iter: 305 loss: 9.9048782e-07
Iter: 306 loss: 9.83486e-07
Iter: 307 loss: 1.01730586e-06
Iter: 308 loss: 9.82261781e-07
Iter: 309 loss: 9.78746e-07
Iter: 310 loss: 1.00986108e-06
Iter: 311 loss: 9.78569915e-07
Iter: 312 loss: 9.74825753e-07
Iter: 313 loss: 9.70654924e-07
Iter: 314 loss: 9.70050451e-07
Iter: 315 loss: 9.68346058e-07
Iter: 316 loss: 9.67360847e-07
Iter: 317 loss: 9.65326308e-07
Iter: 318 loss: 9.61402e-07
Iter: 319 loss: 1.04312335e-06
Iter: 320 loss: 9.61408432e-07
Iter: 321 loss: 9.55589485e-07
Iter: 322 loss: 9.65540835e-07
Iter: 323 loss: 9.53082292e-07
Iter: 324 loss: 9.486414e-07
Iter: 325 loss: 9.47115097e-07
Iter: 326 loss: 9.4456459e-07
Iter: 327 loss: 9.43026123e-07
Iter: 328 loss: 9.41327926e-07
Iter: 329 loss: 9.39361371e-07
Iter: 330 loss: 9.39368192e-07
Iter: 331 loss: 9.38058065e-07
Iter: 332 loss: 9.3539461e-07
Iter: 333 loss: 9.80322852e-07
Iter: 334 loss: 9.35336061e-07
Iter: 335 loss: 9.3235883e-07
Iter: 336 loss: 9.43658563e-07
Iter: 337 loss: 9.31657951e-07
Iter: 338 loss: 9.29314524e-07
Iter: 339 loss: 9.26386122e-07
Iter: 340 loss: 9.26195867e-07
Iter: 341 loss: 9.21942274e-07
Iter: 342 loss: 9.44633257e-07
Iter: 343 loss: 9.21340757e-07
Iter: 344 loss: 9.17548618e-07
Iter: 345 loss: 9.32032322e-07
Iter: 346 loss: 9.16688407e-07
Iter: 347 loss: 9.13977885e-07
Iter: 348 loss: 9.45925137e-07
Iter: 349 loss: 9.13896e-07
Iter: 350 loss: 9.12154348e-07
Iter: 351 loss: 9.12826636e-07
Iter: 352 loss: 9.10884182e-07
Iter: 353 loss: 9.07622621e-07
Iter: 354 loss: 9.11567781e-07
Iter: 355 loss: 9.05845525e-07
Iter: 356 loss: 9.0337619e-07
Iter: 357 loss: 9.05996671e-07
Iter: 358 loss: 9.02065608e-07
Iter: 359 loss: 8.98326107e-07
Iter: 360 loss: 8.98229132e-07
Iter: 361 loss: 8.95364792e-07
Iter: 362 loss: 8.96169809e-07
Iter: 363 loss: 8.94143e-07
Iter: 364 loss: 8.92564913e-07
Iter: 365 loss: 8.89405499e-07
Iter: 366 loss: 9.46639148e-07
Iter: 367 loss: 8.89337343e-07
Iter: 368 loss: 8.86136661e-07
Iter: 369 loss: 8.95837729e-07
Iter: 370 loss: 8.85192435e-07
Iter: 371 loss: 8.83132202e-07
Iter: 372 loss: 8.92911714e-07
Iter: 373 loss: 8.82779773e-07
Iter: 374 loss: 8.80412642e-07
Iter: 375 loss: 8.75157184e-07
Iter: 376 loss: 9.48603088e-07
Iter: 377 loss: 8.75007e-07
Iter: 378 loss: 8.70771089e-07
Iter: 379 loss: 9.20397724e-07
Iter: 380 loss: 8.7080349e-07
Iter: 381 loss: 8.67757421e-07
Iter: 382 loss: 8.84457734e-07
Iter: 383 loss: 8.67290055e-07
Iter: 384 loss: 8.64823733e-07
Iter: 385 loss: 8.7924235e-07
Iter: 386 loss: 8.64559354e-07
Iter: 387 loss: 8.6191028e-07
Iter: 388 loss: 8.62316824e-07
Iter: 389 loss: 8.59934175e-07
Iter: 390 loss: 8.57395321e-07
Iter: 391 loss: 8.82181212e-07
Iter: 392 loss: 8.57281861e-07
Iter: 393 loss: 8.55643236e-07
Iter: 394 loss: 8.52057269e-07
Iter: 395 loss: 9.11636562e-07
Iter: 396 loss: 8.52053176e-07
Iter: 397 loss: 8.48912919e-07
Iter: 398 loss: 8.84741894e-07
Iter: 399 loss: 8.48907348e-07
Iter: 400 loss: 8.47184083e-07
Iter: 401 loss: 8.47043736e-07
Iter: 402 loss: 8.45773229e-07
Iter: 403 loss: 8.42886323e-07
Iter: 404 loss: 8.84270094e-07
Iter: 405 loss: 8.42785823e-07
Iter: 406 loss: 8.40854113e-07
Iter: 407 loss: 8.45863781e-07
Iter: 408 loss: 8.40296821e-07
Iter: 409 loss: 8.37436801e-07
Iter: 410 loss: 8.39128916e-07
Iter: 411 loss: 8.35592914e-07
Iter: 412 loss: 8.33619e-07
Iter: 413 loss: 8.46010039e-07
Iter: 414 loss: 8.33424849e-07
Iter: 415 loss: 8.31620696e-07
Iter: 416 loss: 8.28667055e-07
Iter: 417 loss: 8.28582586e-07
Iter: 418 loss: 8.27711631e-07
Iter: 419 loss: 8.26912e-07
Iter: 420 loss: 8.25394579e-07
Iter: 421 loss: 8.2473e-07
Iter: 422 loss: 8.23960249e-07
Iter: 423 loss: 8.21653828e-07
Iter: 424 loss: 8.32859655e-07
Iter: 425 loss: 8.21170147e-07
Iter: 426 loss: 8.19448132e-07
Iter: 427 loss: 8.2127076e-07
Iter: 428 loss: 8.18474291e-07
Iter: 429 loss: 8.16065267e-07
Iter: 430 loss: 8.17463388e-07
Iter: 431 loss: 8.14537259e-07
Iter: 432 loss: 8.14012822e-07
Iter: 433 loss: 8.13511406e-07
Iter: 434 loss: 8.12233225e-07
Iter: 435 loss: 8.10136271e-07
Iter: 436 loss: 8.10143945e-07
Iter: 437 loss: 8.08394816e-07
Iter: 438 loss: 8.08478319e-07
Iter: 439 loss: 8.07060871e-07
Iter: 440 loss: 8.04251556e-07
Iter: 441 loss: 8.08270613e-07
Iter: 442 loss: 8.02846216e-07
Iter: 443 loss: 7.99607619e-07
Iter: 444 loss: 8.23179676e-07
Iter: 445 loss: 7.99315387e-07
Iter: 446 loss: 7.97743098e-07
Iter: 447 loss: 7.95037636e-07
Iter: 448 loss: 7.95031326e-07
Iter: 449 loss: 7.93326819e-07
Iter: 450 loss: 7.93169193e-07
Iter: 451 loss: 7.91841671e-07
Iter: 452 loss: 7.96217137e-07
Iter: 453 loss: 7.91463947e-07
Iter: 454 loss: 7.8967895e-07
Iter: 455 loss: 7.8985147e-07
Iter: 456 loss: 7.88254852e-07
Iter: 457 loss: 7.86562453e-07
Iter: 458 loss: 7.93382242e-07
Iter: 459 loss: 7.86212468e-07
Iter: 460 loss: 7.83978862e-07
Iter: 461 loss: 7.80901303e-07
Iter: 462 loss: 7.80719063e-07
Iter: 463 loss: 7.7983168e-07
Iter: 464 loss: 7.79226582e-07
Iter: 465 loss: 7.7806169e-07
Iter: 466 loss: 7.8703772e-07
Iter: 467 loss: 7.77972389e-07
Iter: 468 loss: 7.7729959e-07
Iter: 469 loss: 7.75282501e-07
Iter: 470 loss: 7.8007e-07
Iter: 471 loss: 7.74028933e-07
Iter: 472 loss: 7.71803741e-07
Iter: 473 loss: 8.02676368e-07
Iter: 474 loss: 7.71848931e-07
Iter: 475 loss: 7.7012146e-07
Iter: 476 loss: 7.76136176e-07
Iter: 477 loss: 7.69685187e-07
Iter: 478 loss: 7.68177074e-07
Iter: 479 loss: 7.70528118e-07
Iter: 480 loss: 7.67431914e-07
Iter: 481 loss: 7.65847403e-07
Iter: 482 loss: 7.65804202e-07
Iter: 483 loss: 7.64487197e-07
Iter: 484 loss: 7.6296908e-07
Iter: 485 loss: 7.80943139e-07
Iter: 486 loss: 7.62853176e-07
Iter: 487 loss: 7.61263e-07
Iter: 488 loss: 7.64857418e-07
Iter: 489 loss: 7.60664079e-07
Iter: 490 loss: 7.59097702e-07
Iter: 491 loss: 7.62842149e-07
Iter: 492 loss: 7.58566102e-07
Iter: 493 loss: 7.57032126e-07
Iter: 494 loss: 7.58251417e-07
Iter: 495 loss: 7.56111945e-07
Iter: 496 loss: 7.54040286e-07
Iter: 497 loss: 7.61578235e-07
Iter: 498 loss: 7.53494078e-07
Iter: 499 loss: 7.53607e-07
Iter: 500 loss: 7.53035351e-07
Iter: 501 loss: 7.52524954e-07
Iter: 502 loss: 7.5107846e-07
Iter: 503 loss: 7.52413825e-07
Iter: 504 loss: 7.49713422e-07
Iter: 505 loss: 7.47561785e-07
Iter: 506 loss: 7.64008178e-07
Iter: 507 loss: 7.47467766e-07
Iter: 508 loss: 7.45769398e-07
Iter: 509 loss: 7.44091381e-07
Iter: 510 loss: 7.43729572e-07
Iter: 511 loss: 7.41474651e-07
Iter: 512 loss: 7.4142423e-07
Iter: 513 loss: 7.40287703e-07
Iter: 514 loss: 7.38952167e-07
Iter: 515 loss: 7.38822109e-07
Iter: 516 loss: 7.3634385e-07
Iter: 517 loss: 7.42811039e-07
Iter: 518 loss: 7.35538e-07
Iter: 519 loss: 7.34762352e-07
Iter: 520 loss: 7.34521564e-07
Iter: 521 loss: 7.3356432e-07
Iter: 522 loss: 7.32944386e-07
Iter: 523 loss: 7.32588944e-07
Iter: 524 loss: 7.3111903e-07
Iter: 525 loss: 7.33971888e-07
Iter: 526 loss: 7.30490797e-07
Iter: 527 loss: 7.28756049e-07
Iter: 528 loss: 7.35600338e-07
Iter: 529 loss: 7.28406462e-07
Iter: 530 loss: 7.27041083e-07
Iter: 531 loss: 7.3782428e-07
Iter: 532 loss: 7.26974918e-07
Iter: 533 loss: 7.2568082e-07
Iter: 534 loss: 7.32201897e-07
Iter: 535 loss: 7.25426275e-07
Iter: 536 loss: 7.24765925e-07
Iter: 537 loss: 7.22636059e-07
Iter: 538 loss: 7.28591431e-07
Iter: 539 loss: 7.21558649e-07
Iter: 540 loss: 7.19465731e-07
Iter: 541 loss: 7.43224632e-07
Iter: 542 loss: 7.19450611e-07
Iter: 543 loss: 7.17519583e-07
Iter: 544 loss: 7.2358057e-07
Iter: 545 loss: 7.16944669e-07
Iter: 546 loss: 7.15624935e-07
Iter: 547 loss: 7.30503928e-07
Iter: 548 loss: 7.15577869e-07
Iter: 549 loss: 7.1466485e-07
Iter: 550 loss: 7.12761448e-07
Iter: 551 loss: 7.48371122e-07
Iter: 552 loss: 7.12730809e-07
Iter: 553 loss: 7.11898394e-07
Iter: 554 loss: 7.11741848e-07
Iter: 555 loss: 7.10762265e-07
Iter: 556 loss: 7.11826431e-07
Iter: 557 loss: 7.10231518e-07
Iter: 558 loss: 7.0866156e-07
Iter: 559 loss: 7.07051811e-07
Iter: 560 loss: 7.06731726e-07
Iter: 561 loss: 7.0525823e-07
Iter: 562 loss: 7.16394254e-07
Iter: 563 loss: 7.05146249e-07
Iter: 564 loss: 7.03667e-07
Iter: 565 loss: 7.08930429e-07
Iter: 566 loss: 7.03222952e-07
Iter: 567 loss: 7.0311e-07
Iter: 568 loss: 7.02651789e-07
Iter: 569 loss: 7.0235518e-07
Iter: 570 loss: 7.01326542e-07
Iter: 571 loss: 7.084966e-07
Iter: 572 loss: 7.01175395e-07
Iter: 573 loss: 6.99812745e-07
Iter: 574 loss: 6.99063378e-07
Iter: 575 loss: 6.9841991e-07
Iter: 576 loss: 6.96783445e-07
Iter: 577 loss: 6.99401937e-07
Iter: 578 loss: 6.9604755e-07
Iter: 579 loss: 6.93834181e-07
Iter: 580 loss: 7.11782036e-07
Iter: 581 loss: 6.93705033e-07
Iter: 582 loss: 6.92446292e-07
Iter: 583 loss: 6.98343e-07
Iter: 584 loss: 6.92186575e-07
Iter: 585 loss: 6.91379512e-07
Iter: 586 loss: 6.91400089e-07
Iter: 587 loss: 6.90651405e-07
Iter: 588 loss: 6.89630724e-07
Iter: 589 loss: 6.9622439e-07
Iter: 590 loss: 6.89540798e-07
Iter: 591 loss: 6.88264549e-07
Iter: 592 loss: 6.89218155e-07
Iter: 593 loss: 6.87430315e-07
Iter: 594 loss: 6.86245698e-07
Iter: 595 loss: 6.88236241e-07
Iter: 596 loss: 6.85766224e-07
Iter: 597 loss: 6.84405791e-07
Iter: 598 loss: 6.86898e-07
Iter: 599 loss: 6.83851681e-07
Iter: 600 loss: 6.83097426e-07
Iter: 601 loss: 6.82970153e-07
Iter: 602 loss: 6.82262794e-07
Iter: 603 loss: 6.83810867e-07
Iter: 604 loss: 6.81950667e-07
Iter: 605 loss: 6.81263032e-07
Iter: 606 loss: 6.79650498e-07
Iter: 607 loss: 6.94652101e-07
Iter: 608 loss: 6.79324671e-07
Iter: 609 loss: 6.78055244e-07
Iter: 610 loss: 6.86072e-07
Iter: 611 loss: 6.77946332e-07
Iter: 612 loss: 6.7636654e-07
Iter: 613 loss: 6.74813748e-07
Iter: 614 loss: 6.74552211e-07
Iter: 615 loss: 6.7402317e-07
Iter: 616 loss: 6.73638624e-07
Iter: 617 loss: 6.72823376e-07
Iter: 618 loss: 6.72083104e-07
Iter: 619 loss: 6.71839871e-07
Iter: 620 loss: 6.70726649e-07
Iter: 621 loss: 6.77838102e-07
Iter: 622 loss: 6.70575105e-07
Iter: 623 loss: 6.69824203e-07
Iter: 624 loss: 6.7005692e-07
Iter: 625 loss: 6.69301812e-07
Iter: 626 loss: 6.67922734e-07
Iter: 627 loss: 6.76657066e-07
Iter: 628 loss: 6.67801828e-07
Iter: 629 loss: 6.67214636e-07
Iter: 630 loss: 6.6666604e-07
Iter: 631 loss: 6.66492156e-07
Iter: 632 loss: 6.65061521e-07
Iter: 633 loss: 6.66464757e-07
Iter: 634 loss: 6.64290951e-07
Iter: 635 loss: 6.64196705e-07
Iter: 636 loss: 6.63689093e-07
Iter: 637 loss: 6.63171249e-07
Iter: 638 loss: 6.62963657e-07
Iter: 639 loss: 6.62670686e-07
Iter: 640 loss: 6.61986235e-07
Iter: 641 loss: 6.61669333e-07
Iter: 642 loss: 6.61351919e-07
Iter: 643 loss: 6.60420369e-07
Iter: 644 loss: 6.60915362e-07
Iter: 645 loss: 6.5988354e-07
Iter: 646 loss: 6.58468821e-07
Iter: 647 loss: 6.60907574e-07
Iter: 648 loss: 6.57882481e-07
Iter: 649 loss: 6.56627435e-07
Iter: 650 loss: 6.58459953e-07
Iter: 651 loss: 6.56022507e-07
Iter: 652 loss: 6.55053327e-07
Iter: 653 loss: 6.69389067e-07
Iter: 654 loss: 6.5505003e-07
Iter: 655 loss: 6.5428128e-07
Iter: 656 loss: 6.55487838e-07
Iter: 657 loss: 6.53941925e-07
Iter: 658 loss: 6.52924768e-07
Iter: 659 loss: 6.55146948e-07
Iter: 660 loss: 6.52567394e-07
Iter: 661 loss: 6.51759137e-07
Iter: 662 loss: 6.51407959e-07
Iter: 663 loss: 6.50980382e-07
Iter: 664 loss: 6.49881088e-07
Iter: 665 loss: 6.67333552e-07
Iter: 666 loss: 6.49845731e-07
Iter: 667 loss: 6.49141668e-07
Iter: 668 loss: 6.51061328e-07
Iter: 669 loss: 6.48919297e-07
Iter: 670 loss: 6.48312152e-07
Iter: 671 loss: 6.46996796e-07
Iter: 672 loss: 6.66692245e-07
Iter: 673 loss: 6.46924661e-07
Iter: 674 loss: 6.49753815e-07
Iter: 675 loss: 6.46588944e-07
Iter: 676 loss: 6.46385047e-07
Iter: 677 loss: 6.45831506e-07
Iter: 678 loss: 6.47483944e-07
Iter: 679 loss: 6.45508237e-07
Iter: 680 loss: 6.4441474e-07
Iter: 681 loss: 6.46376861e-07
Iter: 682 loss: 6.44037129e-07
Iter: 683 loss: 6.43013209e-07
Iter: 684 loss: 6.48771106e-07
Iter: 685 loss: 6.42919076e-07
Iter: 686 loss: 6.42083478e-07
Iter: 687 loss: 6.41073427e-07
Iter: 688 loss: 6.41003908e-07
Iter: 689 loss: 6.39798316e-07
Iter: 690 loss: 6.46819558e-07
Iter: 691 loss: 6.39609311e-07
Iter: 692 loss: 6.38602e-07
Iter: 693 loss: 6.47236902e-07
Iter: 694 loss: 6.38553615e-07
Iter: 695 loss: 6.37828748e-07
Iter: 696 loss: 6.38160543e-07
Iter: 697 loss: 6.37279413e-07
Iter: 698 loss: 6.36262939e-07
Iter: 699 loss: 6.39098971e-07
Iter: 700 loss: 6.35980768e-07
Iter: 701 loss: 6.35541e-07
Iter: 702 loss: 6.35534889e-07
Iter: 703 loss: 6.35037168e-07
Iter: 704 loss: 6.34149615e-07
Iter: 705 loss: 6.50084189e-07
Iter: 706 loss: 6.34063099e-07
Iter: 707 loss: 6.33016157e-07
Iter: 708 loss: 6.39115683e-07
Iter: 709 loss: 6.32890305e-07
Iter: 710 loss: 6.3284358e-07
Iter: 711 loss: 6.32509909e-07
Iter: 712 loss: 6.32255762e-07
Iter: 713 loss: 6.31315061e-07
Iter: 714 loss: 6.3320158e-07
Iter: 715 loss: 6.30778e-07
Iter: 716 loss: 6.29847932e-07
Iter: 717 loss: 6.37104222e-07
Iter: 718 loss: 6.29792169e-07
Iter: 719 loss: 6.28857663e-07
Iter: 720 loss: 6.29770057e-07
Iter: 721 loss: 6.28369321e-07
Iter: 722 loss: 6.27272e-07
Iter: 723 loss: 6.33031391e-07
Iter: 724 loss: 6.27099951e-07
Iter: 725 loss: 6.26350584e-07
Iter: 726 loss: 6.26384804e-07
Iter: 727 loss: 6.25745884e-07
Iter: 728 loss: 6.2483673e-07
Iter: 729 loss: 6.32624733e-07
Iter: 730 loss: 6.24803647e-07
Iter: 731 loss: 6.2389779e-07
Iter: 732 loss: 6.24074573e-07
Iter: 733 loss: 6.2327058e-07
Iter: 734 loss: 6.22467951e-07
Iter: 735 loss: 6.28088742e-07
Iter: 736 loss: 6.22341815e-07
Iter: 737 loss: 6.21773722e-07
Iter: 738 loss: 6.21447498e-07
Iter: 739 loss: 6.21145432e-07
Iter: 740 loss: 6.20576202e-07
Iter: 741 loss: 6.2050151e-07
Iter: 742 loss: 6.20133392e-07
Iter: 743 loss: 6.21174934e-07
Iter: 744 loss: 6.20047501e-07
Iter: 745 loss: 6.19439447e-07
Iter: 746 loss: 6.20167043e-07
Iter: 747 loss: 6.19183538e-07
Iter: 748 loss: 6.18752154e-07
Iter: 749 loss: 6.17991475e-07
Iter: 750 loss: 6.36134359e-07
Iter: 751 loss: 6.17960495e-07
Iter: 752 loss: 6.17066632e-07
Iter: 753 loss: 6.17711748e-07
Iter: 754 loss: 6.1649547e-07
Iter: 755 loss: 6.15733256e-07
Iter: 756 loss: 6.27797931e-07
Iter: 757 loss: 6.15715635e-07
Iter: 758 loss: 6.15114402e-07
Iter: 759 loss: 6.15824e-07
Iter: 760 loss: 6.14785279e-07
Iter: 761 loss: 6.1409e-07
Iter: 762 loss: 6.14491796e-07
Iter: 763 loss: 6.13669727e-07
Iter: 764 loss: 6.12627e-07
Iter: 765 loss: 6.1568403e-07
Iter: 766 loss: 6.12362555e-07
Iter: 767 loss: 6.11510529e-07
Iter: 768 loss: 6.20352694e-07
Iter: 769 loss: 6.11487621e-07
Iter: 770 loss: 6.10975235e-07
Iter: 771 loss: 6.10022425e-07
Iter: 772 loss: 6.09999233e-07
Iter: 773 loss: 6.09263168e-07
Iter: 774 loss: 6.09254243e-07
Iter: 775 loss: 6.08646587e-07
Iter: 776 loss: 6.11893711e-07
Iter: 777 loss: 6.08586788e-07
Iter: 778 loss: 6.08090886e-07
Iter: 779 loss: 6.11528435e-07
Iter: 780 loss: 6.0808037e-07
Iter: 781 loss: 6.0762585e-07
Iter: 782 loss: 6.07196853e-07
Iter: 783 loss: 6.07083507e-07
Iter: 784 loss: 6.06635808e-07
Iter: 785 loss: 6.05838295e-07
Iter: 786 loss: 6.05813909e-07
Iter: 787 loss: 6.04873094e-07
Iter: 788 loss: 6.10286861e-07
Iter: 789 loss: 6.04734964e-07
Iter: 790 loss: 6.03980141e-07
Iter: 791 loss: 6.06572485e-07
Iter: 792 loss: 6.03855938e-07
Iter: 793 loss: 6.03131582e-07
Iter: 794 loss: 6.03025285e-07
Iter: 795 loss: 6.02500222e-07
Iter: 796 loss: 6.01599538e-07
Iter: 797 loss: 6.11186465e-07
Iter: 798 loss: 6.01525471e-07
Iter: 799 loss: 6.00921965e-07
Iter: 800 loss: 6.05122409e-07
Iter: 801 loss: 6.00887461e-07
Iter: 802 loss: 6.00325279e-07
Iter: 803 loss: 5.99561531e-07
Iter: 804 loss: 5.99508837e-07
Iter: 805 loss: 5.98635836e-07
Iter: 806 loss: 6.04615366e-07
Iter: 807 loss: 5.98514305e-07
Iter: 808 loss: 5.97711221e-07
Iter: 809 loss: 6.04115087e-07
Iter: 810 loss: 5.97737198e-07
Iter: 811 loss: 5.97227313e-07
Iter: 812 loss: 6.03056378e-07
Iter: 813 loss: 5.97214353e-07
Iter: 814 loss: 5.96861412e-07
Iter: 815 loss: 5.97368285e-07
Iter: 816 loss: 5.96669224e-07
Iter: 817 loss: 5.96283371e-07
Iter: 818 loss: 5.95911956e-07
Iter: 819 loss: 5.95857841e-07
Iter: 820 loss: 5.95226197e-07
Iter: 821 loss: 5.9626916e-07
Iter: 822 loss: 5.94915832e-07
Iter: 823 loss: 5.9423121e-07
Iter: 824 loss: 5.94238543e-07
Iter: 825 loss: 5.93676134e-07
Iter: 826 loss: 5.92910624e-07
Iter: 827 loss: 5.96667064e-07
Iter: 828 loss: 5.92740093e-07
Iter: 829 loss: 5.91933485e-07
Iter: 830 loss: 5.92585366e-07
Iter: 831 loss: 5.91381877e-07
Iter: 832 loss: 5.90690945e-07
Iter: 833 loss: 5.95096367e-07
Iter: 834 loss: 5.90591583e-07
Iter: 835 loss: 5.899646e-07
Iter: 836 loss: 5.91066851e-07
Iter: 837 loss: 5.89649801e-07
Iter: 838 loss: 5.88930675e-07
Iter: 839 loss: 5.91299852e-07
Iter: 840 loss: 5.88706712e-07
Iter: 841 loss: 5.88150897e-07
Iter: 842 loss: 5.96328448e-07
Iter: 843 loss: 5.8814976e-07
Iter: 844 loss: 5.8775953e-07
Iter: 845 loss: 5.89406e-07
Iter: 846 loss: 5.87703425e-07
Iter: 847 loss: 5.87182e-07
Iter: 848 loss: 5.89891101e-07
Iter: 849 loss: 5.87135105e-07
Iter: 850 loss: 5.86872602e-07
Iter: 851 loss: 5.86867372e-07
Iter: 852 loss: 5.86657507e-07
Iter: 853 loss: 5.86296e-07
Iter: 854 loss: 5.86590033e-07
Iter: 855 loss: 5.86038e-07
Iter: 856 loss: 5.8558993e-07
Iter: 857 loss: 5.85205157e-07
Iter: 858 loss: 5.85126202e-07
Iter: 859 loss: 5.84593408e-07
Iter: 860 loss: 5.89428907e-07
Iter: 861 loss: 5.84572717e-07
Iter: 862 loss: 5.8404396e-07
Iter: 863 loss: 5.84116378e-07
Iter: 864 loss: 5.83651342e-07
Iter: 865 loss: 5.82839505e-07
Iter: 866 loss: 5.85167868e-07
Iter: 867 loss: 5.82583539e-07
Iter: 868 loss: 5.8190318e-07
Iter: 869 loss: 5.8287344e-07
Iter: 870 loss: 5.81543645e-07
Iter: 871 loss: 5.80897449e-07
Iter: 872 loss: 5.81767836e-07
Iter: 873 loss: 5.80513301e-07
Iter: 874 loss: 5.79878417e-07
Iter: 875 loss: 5.85014277e-07
Iter: 876 loss: 5.79839252e-07
Iter: 877 loss: 5.79305095e-07
Iter: 878 loss: 5.80998744e-07
Iter: 879 loss: 5.79195785e-07
Iter: 880 loss: 5.78935101e-07
Iter: 881 loss: 5.78849267e-07
Iter: 882 loss: 5.78689082e-07
Iter: 883 loss: 5.78515255e-07
Iter: 884 loss: 5.78452614e-07
Iter: 885 loss: 5.78056415e-07
Iter: 886 loss: 5.78179595e-07
Iter: 887 loss: 5.77746562e-07
Iter: 888 loss: 5.77313131e-07
Iter: 889 loss: 5.77737126e-07
Iter: 890 loss: 5.7705148e-07
Iter: 891 loss: 5.76657669e-07
Iter: 892 loss: 5.78508775e-07
Iter: 893 loss: 5.76627031e-07
Iter: 894 loss: 5.76236516e-07
Iter: 895 loss: 5.77083426e-07
Iter: 896 loss: 5.76087359e-07
Iter: 897 loss: 5.75632384e-07
Iter: 898 loss: 5.75555077e-07
Iter: 899 loss: 5.75300305e-07
Iter: 900 loss: 5.74732667e-07
Iter: 901 loss: 5.77427443e-07
Iter: 902 loss: 5.7459232e-07
Iter: 903 loss: 5.74019509e-07
Iter: 904 loss: 5.74663204e-07
Iter: 905 loss: 5.73806233e-07
Iter: 906 loss: 5.73049647e-07
Iter: 907 loss: 5.76995149e-07
Iter: 908 loss: 5.72943065e-07
Iter: 909 loss: 5.72521344e-07
Iter: 910 loss: 5.728308e-07
Iter: 911 loss: 5.7227362e-07
Iter: 912 loss: 5.71929718e-07
Iter: 913 loss: 5.71927671e-07
Iter: 914 loss: 5.71575811e-07
Iter: 915 loss: 5.73231205e-07
Iter: 916 loss: 5.71502824e-07
Iter: 917 loss: 5.71354519e-07
Iter: 918 loss: 5.71107194e-07
Iter: 919 loss: 5.71091846e-07
Iter: 920 loss: 5.70707471e-07
Iter: 921 loss: 5.7218972e-07
Iter: 922 loss: 5.70640509e-07
Iter: 923 loss: 5.70338443e-07
Iter: 924 loss: 5.6984635e-07
Iter: 925 loss: 5.6983265e-07
Iter: 926 loss: 5.69264444e-07
Iter: 927 loss: 5.72593422e-07
Iter: 928 loss: 5.69236192e-07
Iter: 929 loss: 5.68733071e-07
Iter: 930 loss: 5.69940312e-07
Iter: 931 loss: 5.68524115e-07
Iter: 932 loss: 5.68001553e-07
Iter: 933 loss: 5.71211785e-07
Iter: 934 loss: 5.6793084e-07
Iter: 935 loss: 5.675696e-07
Iter: 936 loss: 5.67445e-07
Iter: 937 loss: 5.67169877e-07
Iter: 938 loss: 5.66546589e-07
Iter: 939 loss: 5.67568691e-07
Iter: 940 loss: 5.66253107e-07
Iter: 941 loss: 5.65521532e-07
Iter: 942 loss: 5.6705818e-07
Iter: 943 loss: 5.65269147e-07
Iter: 944 loss: 5.64747154e-07
Iter: 945 loss: 5.70389091e-07
Iter: 946 loss: 5.64752e-07
Iter: 947 loss: 5.64374773e-07
Iter: 948 loss: 5.67989105e-07
Iter: 949 loss: 5.64371931e-07
Iter: 950 loss: 5.63957428e-07
Iter: 951 loss: 5.64738173e-07
Iter: 952 loss: 5.63781896e-07
Iter: 953 loss: 5.634746e-07
Iter: 954 loss: 5.63181231e-07
Iter: 955 loss: 5.63122171e-07
Iter: 956 loss: 5.62685955e-07
Iter: 957 loss: 5.66936365e-07
Iter: 958 loss: 5.62690559e-07
Iter: 959 loss: 5.62321702e-07
Iter: 960 loss: 5.62133721e-07
Iter: 961 loss: 5.61991783e-07
Iter: 962 loss: 5.61548802e-07
Iter: 963 loss: 5.6158467e-07
Iter: 964 loss: 5.61196202e-07
Iter: 965 loss: 5.60661135e-07
Iter: 966 loss: 5.64122047e-07
Iter: 967 loss: 5.60618673e-07
Iter: 968 loss: 5.60204114e-07
Iter: 969 loss: 5.61180855e-07
Iter: 970 loss: 5.60057345e-07
Iter: 971 loss: 5.59604246e-07
Iter: 972 loss: 5.6251065e-07
Iter: 973 loss: 5.59529099e-07
Iter: 974 loss: 5.59113232e-07
Iter: 975 loss: 5.58649276e-07
Iter: 976 loss: 5.58629267e-07
Iter: 977 loss: 5.57979718e-07
Iter: 978 loss: 5.6137219e-07
Iter: 979 loss: 5.578936e-07
Iter: 980 loss: 5.574189e-07
Iter: 981 loss: 5.58554348e-07
Iter: 982 loss: 5.57219096e-07
Iter: 983 loss: 5.5685706e-07
Iter: 984 loss: 5.56780094e-07
Iter: 985 loss: 5.56615475e-07
Iter: 986 loss: 5.5616988e-07
Iter: 987 loss: 5.61962e-07
Iter: 988 loss: 5.56145721e-07
Iter: 989 loss: 5.55671591e-07
Iter: 990 loss: 5.58180204e-07
Iter: 991 loss: 5.55586e-07
Iter: 992 loss: 5.55156589e-07
Iter: 993 loss: 5.56052726e-07
Iter: 994 loss: 5.55018346e-07
Iter: 995 loss: 5.54466055e-07
Iter: 996 loss: 5.55583483e-07
Iter: 997 loss: 5.54282394e-07
Iter: 998 loss: 5.53871132e-07
Iter: 999 loss: 5.55005954e-07
Iter: 1000 loss: 5.53687e-07
Iter: 1001 loss: 5.53377788e-07
Iter: 1002 loss: 5.53066229e-07
Iter: 1003 loss: 5.52973802e-07
Iter: 1004 loss: 5.52425774e-07
Iter: 1005 loss: 5.56117129e-07
Iter: 1006 loss: 5.52325673e-07
Iter: 1007 loss: 5.51883829e-07
Iter: 1008 loss: 5.53470727e-07
Iter: 1009 loss: 5.51686185e-07
Iter: 1010 loss: 5.51212509e-07
Iter: 1011 loss: 5.53278596e-07
Iter: 1012 loss: 5.51101095e-07
Iter: 1013 loss: 5.50632876e-07
Iter: 1014 loss: 5.5068756e-07
Iter: 1015 loss: 5.50321261e-07
Iter: 1016 loss: 5.50513732e-07
Iter: 1017 loss: 5.5013669e-07
Iter: 1018 loss: 5.49990204e-07
Iter: 1019 loss: 5.4952568e-07
Iter: 1020 loss: 5.53596692e-07
Iter: 1021 loss: 5.49480887e-07
Iter: 1022 loss: 5.49003971e-07
Iter: 1023 loss: 5.49858157e-07
Iter: 1024 loss: 5.48812466e-07
Iter: 1025 loss: 5.48418029e-07
Iter: 1026 loss: 5.53558209e-07
Iter: 1027 loss: 5.48367211e-07
Iter: 1028 loss: 5.48065202e-07
Iter: 1029 loss: 5.4815132e-07
Iter: 1030 loss: 5.47808e-07
Iter: 1031 loss: 5.47341074e-07
Iter: 1032 loss: 5.48321964e-07
Iter: 1033 loss: 5.47137e-07
Iter: 1034 loss: 5.46602337e-07
Iter: 1035 loss: 5.46810043e-07
Iter: 1036 loss: 5.46277079e-07
Iter: 1037 loss: 5.45784189e-07
Iter: 1038 loss: 5.49103106e-07
Iter: 1039 loss: 5.45742296e-07
Iter: 1040 loss: 5.4536207e-07
Iter: 1041 loss: 5.45158173e-07
Iter: 1042 loss: 5.44961154e-07
Iter: 1043 loss: 5.44595423e-07
Iter: 1044 loss: 5.44601676e-07
Iter: 1045 loss: 5.44258455e-07
Iter: 1046 loss: 5.4402e-07
Iter: 1047 loss: 5.43798649e-07
Iter: 1048 loss: 5.43511419e-07
Iter: 1049 loss: 5.48364142e-07
Iter: 1050 loss: 5.4350528e-07
Iter: 1051 loss: 5.43189117e-07
Iter: 1052 loss: 5.44406589e-07
Iter: 1053 loss: 5.43134092e-07
Iter: 1054 loss: 5.42916155e-07
Iter: 1055 loss: 5.42394446e-07
Iter: 1056 loss: 5.48688774e-07
Iter: 1057 loss: 5.42348744e-07
Iter: 1058 loss: 5.41910936e-07
Iter: 1059 loss: 5.4558484e-07
Iter: 1060 loss: 5.41924e-07
Iter: 1061 loss: 5.41544068e-07
Iter: 1062 loss: 5.4351716e-07
Iter: 1063 loss: 5.4146858e-07
Iter: 1064 loss: 5.41152758e-07
Iter: 1065 loss: 5.41432598e-07
Iter: 1066 loss: 5.40967562e-07
Iter: 1067 loss: 5.40531573e-07
Iter: 1068 loss: 5.40678286e-07
Iter: 1069 loss: 5.40163228e-07
Iter: 1070 loss: 5.39699e-07
Iter: 1071 loss: 5.42213797e-07
Iter: 1072 loss: 5.3960423e-07
Iter: 1073 loss: 5.39203199e-07
Iter: 1074 loss: 5.38818426e-07
Iter: 1075 loss: 5.38741574e-07
Iter: 1076 loss: 5.38166489e-07
Iter: 1077 loss: 5.41686802e-07
Iter: 1078 loss: 5.38066502e-07
Iter: 1079 loss: 5.37588903e-07
Iter: 1080 loss: 5.42743e-07
Iter: 1081 loss: 5.37596918e-07
Iter: 1082 loss: 5.3729309e-07
Iter: 1083 loss: 5.38290692e-07
Iter: 1084 loss: 5.37230676e-07
Iter: 1085 loss: 5.36961693e-07
Iter: 1086 loss: 5.37015353e-07
Iter: 1087 loss: 5.36828111e-07
Iter: 1088 loss: 5.36408493e-07
Iter: 1089 loss: 5.42177816e-07
Iter: 1090 loss: 5.36385926e-07
Iter: 1091 loss: 5.36004904e-07
Iter: 1092 loss: 5.35861261e-07
Iter: 1093 loss: 5.35617687e-07
Iter: 1094 loss: 5.35381503e-07
Iter: 1095 loss: 5.35358311e-07
Iter: 1096 loss: 5.35128e-07
Iter: 1097 loss: 5.35192612e-07
Iter: 1098 loss: 5.34991614e-07
Iter: 1099 loss: 5.34684887e-07
Iter: 1100 loss: 5.34812216e-07
Iter: 1101 loss: 5.34493722e-07
Iter: 1102 loss: 5.34086212e-07
Iter: 1103 loss: 5.35695222e-07
Iter: 1104 loss: 5.34018454e-07
Iter: 1105 loss: 5.33664e-07
Iter: 1106 loss: 5.33820526e-07
Iter: 1107 loss: 5.33471564e-07
Iter: 1108 loss: 5.33036655e-07
Iter: 1109 loss: 5.33747766e-07
Iter: 1110 loss: 5.32861634e-07
Iter: 1111 loss: 5.32415356e-07
Iter: 1112 loss: 5.34554147e-07
Iter: 1113 loss: 5.32308263e-07
Iter: 1114 loss: 5.32079355e-07
Iter: 1115 loss: 5.32073955e-07
Iter: 1116 loss: 5.31896831e-07
Iter: 1117 loss: 5.32458444e-07
Iter: 1118 loss: 5.31840783e-07
Iter: 1119 loss: 5.3157936e-07
Iter: 1120 loss: 5.32027229e-07
Iter: 1121 loss: 5.31472267e-07
Iter: 1122 loss: 5.31312878e-07
Iter: 1123 loss: 5.30897694e-07
Iter: 1124 loss: 5.36117113e-07
Iter: 1125 loss: 5.30897637e-07
Iter: 1126 loss: 5.30529064e-07
Iter: 1127 loss: 5.34704895e-07
Iter: 1128 loss: 5.30534408e-07
Iter: 1129 loss: 5.30255761e-07
Iter: 1130 loss: 5.31340334e-07
Iter: 1131 loss: 5.30199316e-07
Iter: 1132 loss: 5.29938461e-07
Iter: 1133 loss: 5.29675049e-07
Iter: 1134 loss: 5.29584611e-07
Iter: 1135 loss: 5.29264412e-07
Iter: 1136 loss: 5.32859758e-07
Iter: 1137 loss: 5.29252702e-07
Iter: 1138 loss: 5.28953933e-07
Iter: 1139 loss: 5.28682335e-07
Iter: 1140 loss: 5.28658347e-07
Iter: 1141 loss: 5.28256635e-07
Iter: 1142 loss: 5.30021e-07
Iter: 1143 loss: 5.28158807e-07
Iter: 1144 loss: 5.27726e-07
Iter: 1145 loss: 5.28080761e-07
Iter: 1146 loss: 5.27458155e-07
Iter: 1147 loss: 5.27037969e-07
Iter: 1148 loss: 5.30747172e-07
Iter: 1149 loss: 5.27006819e-07
Iter: 1150 loss: 5.26678264e-07
Iter: 1151 loss: 5.30010084e-07
Iter: 1152 loss: 5.2666752e-07
Iter: 1153 loss: 5.26485167e-07
Iter: 1154 loss: 5.29088538e-07
Iter: 1155 loss: 5.2650023e-07
Iter: 1156 loss: 5.2637256e-07
Iter: 1157 loss: 5.26063445e-07
Iter: 1158 loss: 5.27697239e-07
Iter: 1159 loss: 5.25946234e-07
Iter: 1160 loss: 5.25573569e-07
Iter: 1161 loss: 5.27958605e-07
Iter: 1162 loss: 5.25582152e-07
Iter: 1163 loss: 5.25266955e-07
Iter: 1164 loss: 5.26326744e-07
Iter: 1165 loss: 5.25177597e-07
Iter: 1166 loss: 5.24875645e-07
Iter: 1167 loss: 5.25107794e-07
Iter: 1168 loss: 5.24678e-07
Iter: 1169 loss: 5.24353595e-07
Iter: 1170 loss: 5.25079713e-07
Iter: 1171 loss: 5.24232291e-07
Iter: 1172 loss: 5.23782546e-07
Iter: 1173 loss: 5.24532084e-07
Iter: 1174 loss: 5.23575238e-07
Iter: 1175 loss: 5.23220706e-07
Iter: 1176 loss: 5.23882932e-07
Iter: 1177 loss: 5.23084168e-07
Iter: 1178 loss: 5.22624418e-07
Iter: 1179 loss: 5.231758e-07
Iter: 1180 loss: 5.22325195e-07
Iter: 1181 loss: 5.22010509e-07
Iter: 1182 loss: 5.23670849e-07
Iter: 1183 loss: 5.21923e-07
Iter: 1184 loss: 5.21663651e-07
Iter: 1185 loss: 5.21635172e-07
Iter: 1186 loss: 5.21518928e-07
Iter: 1187 loss: 5.23158292e-07
Iter: 1188 loss: 5.21480047e-07
Iter: 1189 loss: 5.21330946e-07
Iter: 1190 loss: 5.21037691e-07
Iter: 1191 loss: 5.25901214e-07
Iter: 1192 loss: 5.21052812e-07
Iter: 1193 loss: 5.20716355e-07
Iter: 1194 loss: 5.21185257e-07
Iter: 1195 loss: 5.20524509e-07
Iter: 1196 loss: 5.20261324e-07
Iter: 1197 loss: 5.21701168e-07
Iter: 1198 loss: 5.20254218e-07
Iter: 1199 loss: 5.19887692e-07
Iter: 1200 loss: 5.21377046e-07
Iter: 1201 loss: 5.19814705e-07
Iter: 1202 loss: 5.19597847e-07
Iter: 1203 loss: 5.1965867e-07
Iter: 1204 loss: 5.19430841e-07
Iter: 1205 loss: 5.19114963e-07
Iter: 1206 loss: 5.20166566e-07
Iter: 1207 loss: 5.19048569e-07
Iter: 1208 loss: 5.18771969e-07
Iter: 1209 loss: 5.19405774e-07
Iter: 1210 loss: 5.18641286e-07
Iter: 1211 loss: 5.18344109e-07
Iter: 1212 loss: 5.18177558e-07
Iter: 1213 loss: 5.18059153e-07
Iter: 1214 loss: 5.17633e-07
Iter: 1215 loss: 5.19502123e-07
Iter: 1216 loss: 5.17578542e-07
Iter: 1217 loss: 5.17278e-07
Iter: 1218 loss: 5.21475783e-07
Iter: 1219 loss: 5.17290914e-07
Iter: 1220 loss: 5.17101057e-07
Iter: 1221 loss: 5.17130275e-07
Iter: 1222 loss: 5.17034778e-07
Iter: 1223 loss: 5.16836167e-07
Iter: 1224 loss: 5.16826333e-07
Iter: 1225 loss: 5.16595037e-07
Iter: 1226 loss: 5.16438149e-07
Iter: 1227 loss: 5.16346063e-07
Iter: 1228 loss: 5.16011767e-07
Iter: 1229 loss: 5.16598277e-07
Iter: 1230 loss: 5.15903707e-07
Iter: 1231 loss: 5.15598572e-07
Iter: 1232 loss: 5.19602e-07
Iter: 1233 loss: 5.15575493e-07
Iter: 1234 loss: 5.15328225e-07
Iter: 1235 loss: 5.15259217e-07
Iter: 1236 loss: 5.15087493e-07
Iter: 1237 loss: 5.14732562e-07
Iter: 1238 loss: 5.15292413e-07
Iter: 1239 loss: 5.14552426e-07
Iter: 1240 loss: 5.14274632e-07
Iter: 1241 loss: 5.17273406e-07
Iter: 1242 loss: 5.14273779e-07
Iter: 1243 loss: 5.14067665e-07
Iter: 1244 loss: 5.13849784e-07
Iter: 1245 loss: 5.13844782e-07
Iter: 1246 loss: 5.13464215e-07
Iter: 1247 loss: 5.14158273e-07
Iter: 1248 loss: 5.13337625e-07
Iter: 1249 loss: 5.12954387e-07
Iter: 1250 loss: 5.15561851e-07
Iter: 1251 loss: 5.12935912e-07
Iter: 1252 loss: 5.12847e-07
Iter: 1253 loss: 5.12790791e-07
Iter: 1254 loss: 5.12658858e-07
Iter: 1255 loss: 5.12522263e-07
Iter: 1256 loss: 5.12477129e-07
Iter: 1257 loss: 5.12247652e-07
Iter: 1258 loss: 5.12165855e-07
Iter: 1259 loss: 5.12052509e-07
Iter: 1260 loss: 5.1177517e-07
Iter: 1261 loss: 5.11828148e-07
Iter: 1262 loss: 5.11561893e-07
Iter: 1263 loss: 5.11256928e-07
Iter: 1264 loss: 5.14772069e-07
Iter: 1265 loss: 5.11264773e-07
Iter: 1266 loss: 5.10937e-07
Iter: 1267 loss: 5.11720827e-07
Iter: 1268 loss: 5.10922519e-07
Iter: 1269 loss: 5.10608572e-07
Iter: 1270 loss: 5.10325947e-07
Iter: 1271 loss: 5.10258076e-07
Iter: 1272 loss: 5.09950382e-07
Iter: 1273 loss: 5.14088e-07
Iter: 1274 loss: 5.0995061e-07
Iter: 1275 loss: 5.09692427e-07
Iter: 1276 loss: 5.09571692e-07
Iter: 1277 loss: 5.09431061e-07
Iter: 1278 loss: 5.0905436e-07
Iter: 1279 loss: 5.0962251e-07
Iter: 1280 loss: 5.08883e-07
Iter: 1281 loss: 5.08518497e-07
Iter: 1282 loss: 5.12161421e-07
Iter: 1283 loss: 5.08492e-07
Iter: 1284 loss: 5.08378662e-07
Iter: 1285 loss: 5.08378093e-07
Iter: 1286 loss: 5.08168398e-07
Iter: 1287 loss: 5.08091659e-07
Iter: 1288 loss: 5.07964842e-07
Iter: 1289 loss: 5.07762593e-07
Iter: 1290 loss: 5.08024698e-07
Iter: 1291 loss: 5.07560458e-07
Iter: 1292 loss: 5.07362586e-07
Iter: 1293 loss: 5.07164941e-07
Iter: 1294 loss: 5.07119694e-07
Iter: 1295 loss: 5.06794208e-07
Iter: 1296 loss: 5.10368523e-07
Iter: 1297 loss: 5.0677113e-07
Iter: 1298 loss: 5.06530569e-07
Iter: 1299 loss: 5.07573759e-07
Iter: 1300 loss: 5.06510901e-07
Iter: 1301 loss: 5.06243737e-07
Iter: 1302 loss: 5.06003971e-07
Iter: 1303 loss: 5.05952244e-07
Iter: 1304 loss: 5.05622268e-07
Iter: 1305 loss: 5.07353263e-07
Iter: 1306 loss: 5.0555019e-07
Iter: 1307 loss: 5.05251478e-07
Iter: 1308 loss: 5.06038077e-07
Iter: 1309 loss: 5.05147568e-07
Iter: 1310 loss: 5.04881314e-07
Iter: 1311 loss: 5.0507208e-07
Iter: 1312 loss: 5.04703792e-07
Iter: 1313 loss: 5.04273771e-07
Iter: 1314 loss: 5.05076969e-07
Iter: 1315 loss: 5.04160141e-07
Iter: 1316 loss: 5.03901788e-07
Iter: 1317 loss: 5.07394816e-07
Iter: 1318 loss: 5.03897468e-07
Iter: 1319 loss: 5.03653439e-07
Iter: 1320 loss: 5.05239427e-07
Iter: 1321 loss: 5.03592048e-07
Iter: 1322 loss: 5.03446302e-07
Iter: 1323 loss: 5.03278e-07
Iter: 1324 loss: 5.03209435e-07
Iter: 1325 loss: 5.02956e-07
Iter: 1326 loss: 5.03269462e-07
Iter: 1327 loss: 5.02792318e-07
Iter: 1328 loss: 5.02550847e-07
Iter: 1329 loss: 5.03082447e-07
Iter: 1330 loss: 5.02404532e-07
Iter: 1331 loss: 5.02232183e-07
Iter: 1332 loss: 5.0423273e-07
Iter: 1333 loss: 5.02228374e-07
Iter: 1334 loss: 5.01956663e-07
Iter: 1335 loss: 5.01773059e-07
Iter: 1336 loss: 5.01704e-07
Iter: 1337 loss: 5.01478894e-07
Iter: 1338 loss: 5.02542207e-07
Iter: 1339 loss: 5.01357e-07
Iter: 1340 loss: 5.01110208e-07
Iter: 1341 loss: 5.01740203e-07
Iter: 1342 loss: 5.01040518e-07
Iter: 1343 loss: 5.00766191e-07
Iter: 1344 loss: 5.01646809e-07
Iter: 1345 loss: 5.0070696e-07
Iter: 1346 loss: 5.00421265e-07
Iter: 1347 loss: 5.00201e-07
Iter: 1348 loss: 5.00100214e-07
Iter: 1349 loss: 4.99732948e-07
Iter: 1350 loss: 5.02077853e-07
Iter: 1351 loss: 4.99694693e-07
Iter: 1352 loss: 4.99466921e-07
Iter: 1353 loss: 4.9947073e-07
Iter: 1354 loss: 4.99270072e-07
Iter: 1355 loss: 4.98981194e-07
Iter: 1356 loss: 4.98959821e-07
Iter: 1357 loss: 4.98618704e-07
Iter: 1358 loss: 4.99212035e-07
Iter: 1359 loss: 4.98474037e-07
Iter: 1360 loss: 4.98081363e-07
Iter: 1361 loss: 4.98655197e-07
Iter: 1362 loss: 4.97838414e-07
Iter: 1363 loss: 4.97583869e-07
Iter: 1364 loss: 4.99884663e-07
Iter: 1365 loss: 4.97577958e-07
Iter: 1366 loss: 4.97294934e-07
Iter: 1367 loss: 4.97608085e-07
Iter: 1368 loss: 4.97151348e-07
Iter: 1369 loss: 4.96849e-07
Iter: 1370 loss: 4.97033113e-07
Iter: 1371 loss: 4.96694838e-07
Iter: 1372 loss: 4.9633752e-07
Iter: 1373 loss: 4.97187045e-07
Iter: 1374 loss: 4.96187909e-07
Iter: 1375 loss: 4.95814561e-07
Iter: 1376 loss: 4.98269401e-07
Iter: 1377 loss: 4.95836332e-07
Iter: 1378 loss: 4.95532163e-07
Iter: 1379 loss: 4.95091683e-07
Iter: 1380 loss: 4.95048084e-07
Iter: 1381 loss: 4.94618234e-07
Iter: 1382 loss: 4.97081771e-07
Iter: 1383 loss: 4.94517053e-07
Iter: 1384 loss: 4.94481128e-07
Iter: 1385 loss: 4.94344e-07
Iter: 1386 loss: 4.94171218e-07
Iter: 1387 loss: 4.94040137e-07
Iter: 1388 loss: 4.94009839e-07
Iter: 1389 loss: 4.93796563e-07
Iter: 1390 loss: 4.93662469e-07
Iter: 1391 loss: 4.93567768e-07
Iter: 1392 loss: 4.93249388e-07
Iter: 1393 loss: 4.93824473e-07
Iter: 1394 loss: 4.93005587e-07
Iter: 1395 loss: 4.92772074e-07
Iter: 1396 loss: 4.93171967e-07
Iter: 1397 loss: 4.92605409e-07
Iter: 1398 loss: 4.92252127e-07
Iter: 1399 loss: 4.95667109e-07
Iter: 1400 loss: 4.92291804e-07
Iter: 1401 loss: 4.92047889e-07
Iter: 1402 loss: 4.92223307e-07
Iter: 1403 loss: 4.91909589e-07
Iter: 1404 loss: 4.91640719e-07
Iter: 1405 loss: 4.91468029e-07
Iter: 1406 loss: 4.91381e-07
Iter: 1407 loss: 4.91066544e-07
Iter: 1408 loss: 4.94574635e-07
Iter: 1409 loss: 4.91017772e-07
Iter: 1410 loss: 4.90698426e-07
Iter: 1411 loss: 4.90715308e-07
Iter: 1412 loss: 4.90461503e-07
Iter: 1413 loss: 4.90098728e-07
Iter: 1414 loss: 4.90894e-07
Iter: 1415 loss: 4.89933086e-07
Iter: 1416 loss: 4.8955485e-07
Iter: 1417 loss: 4.92355582e-07
Iter: 1418 loss: 4.89539957e-07
Iter: 1419 loss: 4.8924062e-07
Iter: 1420 loss: 4.89253409e-07
Iter: 1421 loss: 4.8914967e-07
Iter: 1422 loss: 4.88858745e-07
Iter: 1423 loss: 4.90985315e-07
Iter: 1424 loss: 4.88846524e-07
Iter: 1425 loss: 4.88519845e-07
Iter: 1426 loss: 4.91770493e-07
Iter: 1427 loss: 4.88476132e-07
Iter: 1428 loss: 4.88298042e-07
Iter: 1429 loss: 4.88083515e-07
Iter: 1430 loss: 4.88074932e-07
Iter: 1431 loss: 4.87698685e-07
Iter: 1432 loss: 4.89906881e-07
Iter: 1433 loss: 4.87635475e-07
Iter: 1434 loss: 4.87390651e-07
Iter: 1435 loss: 4.90185471e-07
Iter: 1436 loss: 4.8738741e-07
Iter: 1437 loss: 4.87188458e-07
Iter: 1438 loss: 4.86958925e-07
Iter: 1439 loss: 4.8691868e-07
Iter: 1440 loss: 4.86614795e-07
Iter: 1441 loss: 4.87789e-07
Iter: 1442 loss: 4.86593876e-07
Iter: 1443 loss: 4.86296699e-07
Iter: 1444 loss: 4.87115699e-07
Iter: 1445 loss: 4.86158797e-07
Iter: 1446 loss: 4.85914086e-07
Iter: 1447 loss: 4.86634804e-07
Iter: 1448 loss: 4.8578795e-07
Iter: 1449 loss: 4.8553693e-07
Iter: 1450 loss: 4.8568711e-07
Iter: 1451 loss: 4.85334056e-07
Iter: 1452 loss: 4.85435464e-07
Iter: 1453 loss: 4.85192118e-07
Iter: 1454 loss: 4.85104465e-07
Iter: 1455 loss: 4.84795237e-07
Iter: 1456 loss: 4.85509588e-07
Iter: 1457 loss: 4.84653469e-07
Iter: 1458 loss: 4.84342308e-07
Iter: 1459 loss: 4.88539513e-07
Iter: 1460 loss: 4.84316388e-07
Iter: 1461 loss: 4.8411e-07
Iter: 1462 loss: 4.84167572e-07
Iter: 1463 loss: 4.83833787e-07
Iter: 1464 loss: 4.83543317e-07
Iter: 1465 loss: 4.84514089e-07
Iter: 1466 loss: 4.83491817e-07
Iter: 1467 loss: 4.83231815e-07
Iter: 1468 loss: 4.86007195e-07
Iter: 1469 loss: 4.83230849e-07
Iter: 1470 loss: 4.82977498e-07
Iter: 1471 loss: 4.82752739e-07
Iter: 1472 loss: 4.82705104e-07
Iter: 1473 loss: 4.82425946e-07
Iter: 1474 loss: 4.8322562e-07
Iter: 1475 loss: 4.82276732e-07
Iter: 1476 loss: 4.81987115e-07
Iter: 1477 loss: 4.83217718e-07
Iter: 1478 loss: 4.81900429e-07
Iter: 1479 loss: 4.81662937e-07
Iter: 1480 loss: 4.83061513e-07
Iter: 1481 loss: 4.81628206e-07
Iter: 1482 loss: 4.81417885e-07
Iter: 1483 loss: 4.81436e-07
Iter: 1484 loss: 4.81261281e-07
Iter: 1485 loss: 4.81139864e-07
Iter: 1486 loss: 4.81126904e-07
Iter: 1487 loss: 4.80976325e-07
Iter: 1488 loss: 4.80801305e-07
Iter: 1489 loss: 4.80729796e-07
Iter: 1490 loss: 4.80594224e-07
Iter: 1491 loss: 4.80456094e-07
Iter: 1492 loss: 4.80405618e-07
Iter: 1493 loss: 4.80119922e-07
Iter: 1494 loss: 4.80852862e-07
Iter: 1495 loss: 4.80025506e-07
Iter: 1496 loss: 4.79680182e-07
Iter: 1497 loss: 4.80839049e-07
Iter: 1498 loss: 4.7959395e-07
Iter: 1499 loss: 4.79357709e-07
Iter: 1500 loss: 4.80082406e-07
Iter: 1501 loss: 4.79322296e-07
Iter: 1502 loss: 4.79016819e-07
Iter: 1503 loss: 4.79661708e-07
Iter: 1504 loss: 4.78927689e-07
Iter: 1505 loss: 4.78667175e-07
Iter: 1506 loss: 4.78469133e-07
Iter: 1507 loss: 4.78409106e-07
Iter: 1508 loss: 4.78112099e-07
Iter: 1509 loss: 4.81188522e-07
Iter: 1510 loss: 4.78103516e-07
Iter: 1511 loss: 4.77880235e-07
Iter: 1512 loss: 4.78264838e-07
Iter: 1513 loss: 4.77752224e-07
Iter: 1514 loss: 4.77493927e-07
Iter: 1515 loss: 4.77766093e-07
Iter: 1516 loss: 4.7733289e-07
Iter: 1517 loss: 4.77048332e-07
Iter: 1518 loss: 4.79211565e-07
Iter: 1519 loss: 4.77029573e-07
Iter: 1520 loss: 4.76816069e-07
Iter: 1521 loss: 4.80261406e-07
Iter: 1522 loss: 4.76814108e-07
Iter: 1523 loss: 4.76721482e-07
Iter: 1524 loss: 4.76442494e-07
Iter: 1525 loss: 4.79019263e-07
Iter: 1526 loss: 4.76431779e-07
Iter: 1527 loss: 4.76146255e-07
Iter: 1528 loss: 4.76357854e-07
Iter: 1529 loss: 4.75958188e-07
Iter: 1530 loss: 4.75729337e-07
Iter: 1531 loss: 4.75701825e-07
Iter: 1532 loss: 4.75465242e-07
Iter: 1533 loss: 4.75251341e-07
Iter: 1534 loss: 4.75237357e-07
Iter: 1535 loss: 4.74928186e-07
Iter: 1536 loss: 4.79062805e-07
Iter: 1537 loss: 4.74927901e-07
Iter: 1538 loss: 4.74675886e-07
Iter: 1539 loss: 4.7482925e-07
Iter: 1540 loss: 4.74507118e-07
Iter: 1541 loss: 4.74163244e-07
Iter: 1542 loss: 4.74225942e-07
Iter: 1543 loss: 4.73976513e-07
Iter: 1544 loss: 4.73603961e-07
Iter: 1545 loss: 4.74870063e-07
Iter: 1546 loss: 4.73536687e-07
Iter: 1547 loss: 4.73218336e-07
Iter: 1548 loss: 4.76033591e-07
Iter: 1549 loss: 4.73233854e-07
Iter: 1550 loss: 4.73005343e-07
Iter: 1551 loss: 4.72923546e-07
Iter: 1552 loss: 4.72818613e-07
Iter: 1553 loss: 4.72947107e-07
Iter: 1554 loss: 4.72701061e-07
Iter: 1555 loss: 4.72637623e-07
Iter: 1556 loss: 4.72391861e-07
Iter: 1557 loss: 4.74924946e-07
Iter: 1558 loss: 4.72375916e-07
Iter: 1559 loss: 4.72077431e-07
Iter: 1560 loss: 4.71958458e-07
Iter: 1561 loss: 4.71865633e-07
Iter: 1562 loss: 4.71565897e-07
Iter: 1563 loss: 4.74648544e-07
Iter: 1564 loss: 4.71559076e-07
Iter: 1565 loss: 4.71319765e-07
Iter: 1566 loss: 4.7257646e-07
Iter: 1567 loss: 4.7123865e-07
Iter: 1568 loss: 4.71042426e-07
Iter: 1569 loss: 4.71384055e-07
Iter: 1570 loss: 4.70964636e-07
Iter: 1571 loss: 4.70710575e-07
Iter: 1572 loss: 4.71561066e-07
Iter: 1573 loss: 4.70614509e-07
Iter: 1574 loss: 4.70343139e-07
Iter: 1575 loss: 4.7035698e-07
Iter: 1576 loss: 4.70110393e-07
Iter: 1577 loss: 4.69832457e-07
Iter: 1578 loss: 4.69911441e-07
Iter: 1579 loss: 4.69655731e-07
Iter: 1580 loss: 4.69386237e-07
Iter: 1581 loss: 4.73543309e-07
Iter: 1582 loss: 4.693797e-07
Iter: 1583 loss: 4.69146528e-07
Iter: 1584 loss: 4.69340534e-07
Iter: 1585 loss: 4.69026475e-07
Iter: 1586 loss: 4.68904176e-07
Iter: 1587 loss: 4.68863675e-07
Iter: 1588 loss: 4.68707896e-07
Iter: 1589 loss: 4.68939476e-07
Iter: 1590 loss: 4.68612598e-07
Iter: 1591 loss: 4.6850181e-07
Iter: 1592 loss: 4.68210033e-07
Iter: 1593 loss: 4.72013653e-07
Iter: 1594 loss: 4.68172573e-07
Iter: 1595 loss: 4.67841488e-07
Iter: 1596 loss: 4.68065195e-07
Iter: 1597 loss: 4.67620168e-07
Iter: 1598 loss: 4.67145782e-07
Iter: 1599 loss: 4.70916802e-07
Iter: 1600 loss: 4.67129809e-07
Iter: 1601 loss: 4.66866481e-07
Iter: 1602 loss: 4.70206203e-07
Iter: 1603 loss: 4.66876031e-07
Iter: 1604 loss: 4.66728494e-07
Iter: 1605 loss: 4.66549096e-07
Iter: 1606 loss: 4.66533692e-07
Iter: 1607 loss: 4.66280426e-07
Iter: 1608 loss: 4.69045091e-07
Iter: 1609 loss: 4.66286451e-07
Iter: 1610 loss: 4.66122344e-07
Iter: 1611 loss: 4.66096026e-07
Iter: 1612 loss: 4.65967616e-07
Iter: 1613 loss: 4.65705739e-07
Iter: 1614 loss: 4.6564827e-07
Iter: 1615 loss: 4.6551429e-07
Iter: 1616 loss: 4.65232688e-07
Iter: 1617 loss: 4.66767972e-07
Iter: 1618 loss: 4.65171922e-07
Iter: 1619 loss: 4.64950688e-07
Iter: 1620 loss: 4.67880511e-07
Iter: 1621 loss: 4.64924312e-07
Iter: 1622 loss: 4.64840923e-07
Iter: 1623 loss: 4.66581923e-07
Iter: 1624 loss: 4.64833533e-07
Iter: 1625 loss: 4.64735393e-07
Iter: 1626 loss: 4.6450117e-07
Iter: 1627 loss: 4.66400707e-07
Iter: 1628 loss: 4.6447775e-07
Iter: 1629 loss: 4.6418478e-07
Iter: 1630 loss: 4.64800252e-07
Iter: 1631 loss: 4.64054324e-07
Iter: 1632 loss: 4.63787217e-07
Iter: 1633 loss: 4.64023316e-07
Iter: 1634 loss: 4.63666851e-07
Iter: 1635 loss: 4.63392752e-07
Iter: 1636 loss: 4.65053574e-07
Iter: 1637 loss: 4.63340541e-07
Iter: 1638 loss: 4.63064566e-07
Iter: 1639 loss: 4.64859795e-07
Iter: 1640 loss: 4.63014203e-07
Iter: 1641 loss: 4.62851972e-07
Iter: 1642 loss: 4.63340371e-07
Iter: 1643 loss: 4.62833157e-07
Iter: 1644 loss: 4.62592084e-07
Iter: 1645 loss: 4.63132608e-07
Iter: 1646 loss: 4.62548684e-07
Iter: 1647 loss: 4.62390915e-07
Iter: 1648 loss: 4.62393871e-07
Iter: 1649 loss: 4.62244543e-07
Iter: 1650 loss: 4.62034023e-07
Iter: 1651 loss: 4.61972945e-07
Iter: 1652 loss: 4.61823731e-07
Iter: 1653 loss: 4.61578963e-07
Iter: 1654 loss: 4.61579759e-07
Iter: 1655 loss: 4.61421337e-07
Iter: 1656 loss: 4.6354e-07
Iter: 1657 loss: 4.61434411e-07
Iter: 1658 loss: 4.61313618e-07
Iter: 1659 loss: 4.61076638e-07
Iter: 1660 loss: 4.61102701e-07
Iter: 1661 loss: 4.60854807e-07
Iter: 1662 loss: 4.61137915e-07
Iter: 1663 loss: 4.6073842e-07
Iter: 1664 loss: 4.60539127e-07
Iter: 1665 loss: 4.606689e-07
Iter: 1666 loss: 4.60440504e-07
Iter: 1667 loss: 4.60208071e-07
Iter: 1668 loss: 4.60458239e-07
Iter: 1669 loss: 4.6008148e-07
Iter: 1670 loss: 4.59794137e-07
Iter: 1671 loss: 4.62614878e-07
Iter: 1672 loss: 4.59815595e-07
Iter: 1673 loss: 4.59592684e-07
Iter: 1674 loss: 4.60454544e-07
Iter: 1675 loss: 4.59563068e-07
Iter: 1676 loss: 4.59370142e-07
Iter: 1677 loss: 4.59272911e-07
Iter: 1678 loss: 4.59171645e-07
Iter: 1679 loss: 4.58843061e-07
Iter: 1680 loss: 4.61092611e-07
Iter: 1681 loss: 4.58813929e-07
Iter: 1682 loss: 4.58676595e-07
Iter: 1683 loss: 4.58456299e-07
Iter: 1684 loss: 4.584397e-07
Iter: 1685 loss: 4.58151419e-07
Iter: 1686 loss: 4.6008256e-07
Iter: 1687 loss: 4.58126493e-07
Iter: 1688 loss: 4.57969065e-07
Iter: 1689 loss: 4.5799257e-07
Iter: 1690 loss: 4.57852593e-07
Iter: 1691 loss: 4.58285427e-07
Iter: 1692 loss: 4.57807744e-07
Iter: 1693 loss: 4.5772191e-07
Iter: 1694 loss: 4.57722081e-07
Iter: 1695 loss: 4.57613e-07
Iter: 1696 loss: 4.57532394e-07
Iter: 1697 loss: 4.57241725e-07
Iter: 1698 loss: 4.62173489e-07
Iter: 1699 loss: 4.57264377e-07
Iter: 1700 loss: 4.56972231e-07
Iter: 1701 loss: 4.58871227e-07
Iter: 1702 loss: 4.56985788e-07
Iter: 1703 loss: 4.5668574e-07
Iter: 1704 loss: 4.57094302e-07
Iter: 1705 loss: 4.56587344e-07
Iter: 1706 loss: 4.56371595e-07
Iter: 1707 loss: 4.58700299e-07
Iter: 1708 loss: 4.5636591e-07
Iter: 1709 loss: 4.56179038e-07
Iter: 1710 loss: 4.56672296e-07
Iter: 1711 loss: 4.56148541e-07
Iter: 1712 loss: 4.55995348e-07
Iter: 1713 loss: 4.56343514e-07
Iter: 1714 loss: 4.5595192e-07
Iter: 1715 loss: 4.55771385e-07
Iter: 1716 loss: 4.55773886e-07
Iter: 1717 loss: 4.55641043e-07
Iter: 1718 loss: 4.55416e-07
Iter: 1719 loss: 4.56086298e-07
Iter: 1720 loss: 4.55357679e-07
Iter: 1721 loss: 4.55155146e-07
Iter: 1722 loss: 4.5555376e-07
Iter: 1723 loss: 4.55096085e-07
Iter: 1724 loss: 4.55072325e-07
Iter: 1725 loss: 4.55015197e-07
Iter: 1726 loss: 4.54960514e-07
Iter: 1727 loss: 4.54832673e-07
Iter: 1728 loss: 4.56263024e-07
Iter: 1729 loss: 4.5482841e-07
Iter: 1730 loss: 4.54629458e-07
Iter: 1731 loss: 4.54910662e-07
Iter: 1732 loss: 4.54583585e-07
Iter: 1733 loss: 4.54366841e-07
Iter: 1734 loss: 4.54689598e-07
Iter: 1735 loss: 4.54295446e-07
Iter: 1736 loss: 4.54132191e-07
Iter: 1737 loss: 4.54066083e-07
Iter: 1738 loss: 4.53933666e-07
Iter: 1739 loss: 4.53669941e-07
Iter: 1740 loss: 4.5409638e-07
Iter: 1741 loss: 4.53520045e-07
Iter: 1742 loss: 4.53338885e-07
Iter: 1743 loss: 4.533251e-07
Iter: 1744 loss: 4.53177705e-07
Iter: 1745 loss: 4.53713767e-07
Iter: 1746 loss: 4.53093151e-07
Iter: 1747 loss: 4.52998478e-07
Iter: 1748 loss: 4.52958517e-07
Iter: 1749 loss: 4.52855517e-07
Iter: 1750 loss: 4.52740039e-07
Iter: 1751 loss: 4.53936821e-07
Iter: 1752 loss: 4.52728784e-07
Iter: 1753 loss: 4.52545606e-07
Iter: 1754 loss: 4.52450109e-07
Iter: 1755 loss: 4.5242507e-07
Iter: 1756 loss: 4.52311667e-07
Iter: 1757 loss: 4.52339464e-07
Iter: 1758 loss: 4.52220092e-07
Iter: 1759 loss: 4.5265574e-07
Iter: 1760 loss: 4.52186526e-07
Iter: 1761 loss: 4.52067439e-07
Iter: 1762 loss: 4.51952133e-07
Iter: 1763 loss: 4.55120755e-07
Iter: 1764 loss: 4.51933488e-07
Iter: 1765 loss: 4.5176796e-07
Iter: 1766 loss: 4.52292369e-07
Iter: 1767 loss: 4.51768642e-07
Iter: 1768 loss: 4.51601608e-07
Iter: 1769 loss: 4.52004031e-07
Iter: 1770 loss: 4.51529502e-07
Iter: 1771 loss: 4.5138745e-07
Iter: 1772 loss: 4.51222547e-07
Iter: 1773 loss: 4.51224224e-07
Iter: 1774 loss: 4.5097616e-07
Iter: 1775 loss: 4.52413417e-07
Iter: 1776 loss: 4.50927814e-07
Iter: 1777 loss: 4.50759472e-07
Iter: 1778 loss: 4.51406976e-07
Iter: 1779 loss: 4.50685832e-07
Iter: 1780 loss: 4.5056251e-07
Iter: 1781 loss: 4.50567939e-07
Iter: 1782 loss: 4.50476023e-07
Iter: 1783 loss: 4.50261552e-07
Iter: 1784 loss: 4.50282357e-07
Iter: 1785 loss: 4.50131552e-07
Iter: 1786 loss: 4.51253982e-07
Iter: 1787 loss: 4.50136866e-07
Iter: 1788 loss: 4.50019854e-07
Iter: 1789 loss: 4.50201298e-07
Iter: 1790 loss: 4.49951585e-07
Iter: 1791 loss: 4.49857566e-07
Iter: 1792 loss: 4.49845828e-07
Iter: 1793 loss: 4.49762467e-07
Iter: 1794 loss: 4.49547258e-07
Iter: 1795 loss: 4.53613183e-07
Iter: 1796 loss: 4.4954885e-07
Iter: 1797 loss: 4.49413847e-07
Iter: 1798 loss: 4.49655772e-07
Iter: 1799 loss: 4.4935e-07
Iter: 1800 loss: 4.49224217e-07
Iter: 1801 loss: 4.49917422e-07
Iter: 1802 loss: 4.49197728e-07
Iter: 1803 loss: 4.49041892e-07
Iter: 1804 loss: 4.48937755e-07
Iter: 1805 loss: 4.48911237e-07
Iter: 1806 loss: 4.48703275e-07
Iter: 1807 loss: 4.4962502e-07
Iter: 1808 loss: 4.48710921e-07
Iter: 1809 loss: 4.48533e-07
Iter: 1810 loss: 4.48603231e-07
Iter: 1811 loss: 4.48391631e-07
Iter: 1812 loss: 4.48238e-07
Iter: 1813 loss: 4.50216277e-07
Iter: 1814 loss: 4.48232896e-07
Iter: 1815 loss: 4.48021694e-07
Iter: 1816 loss: 4.48509354e-07
Iter: 1817 loss: 4.47992534e-07
Iter: 1818 loss: 4.4782854e-07
Iter: 1819 loss: 4.47936031e-07
Iter: 1820 loss: 4.47745379e-07
Iter: 1821 loss: 4.47599518e-07
Iter: 1822 loss: 4.48004698e-07
Iter: 1823 loss: 4.47544437e-07
Iter: 1824 loss: 4.47409548e-07
Iter: 1825 loss: 4.48266292e-07
Iter: 1826 loss: 4.47404318e-07
Iter: 1827 loss: 4.47237426e-07
Iter: 1828 loss: 4.48732351e-07
Iter: 1829 loss: 4.47236687e-07
Iter: 1830 loss: 4.47129963e-07
Iter: 1831 loss: 4.46939765e-07
Iter: 1832 loss: 4.48193163e-07
Iter: 1833 loss: 4.46906597e-07
Iter: 1834 loss: 4.46698948e-07
Iter: 1835 loss: 4.49366325e-07
Iter: 1836 loss: 4.46690535e-07
Iter: 1837 loss: 4.46565764e-07
Iter: 1838 loss: 4.46582447e-07
Iter: 1839 loss: 4.46479959e-07
Iter: 1840 loss: 4.46246474e-07
Iter: 1841 loss: 4.465698e-07
Iter: 1842 loss: 4.46176671e-07
Iter: 1843 loss: 4.45987894e-07
Iter: 1844 loss: 4.46261595e-07
Iter: 1845 loss: 4.45938156e-07
Iter: 1846 loss: 4.45660675e-07
Iter: 1847 loss: 4.46155e-07
Iter: 1848 loss: 4.45577e-07
Iter: 1849 loss: 4.4534454e-07
Iter: 1850 loss: 4.45854312e-07
Iter: 1851 loss: 4.45287895e-07
Iter: 1852 loss: 4.45130183e-07
Iter: 1853 loss: 4.45126432e-07
Iter: 1854 loss: 4.45002712e-07
Iter: 1855 loss: 4.44858927e-07
Iter: 1856 loss: 4.49273159e-07
Iter: 1857 loss: 4.44873393e-07
Iter: 1858 loss: 4.44658212e-07
Iter: 1859 loss: 4.47109358e-07
Iter: 1860 loss: 4.4467555e-07
Iter: 1861 loss: 4.44574738e-07
Iter: 1862 loss: 4.45613921e-07
Iter: 1863 loss: 4.44534976e-07
Iter: 1864 loss: 4.44480833e-07
Iter: 1865 loss: 4.44325678e-07
Iter: 1866 loss: 4.47284066e-07
Iter: 1867 loss: 4.44293562e-07
Iter: 1868 loss: 4.44078523e-07
Iter: 1869 loss: 4.44570333e-07
Iter: 1870 loss: 4.44034924e-07
Iter: 1871 loss: 4.43843021e-07
Iter: 1872 loss: 4.44253885e-07
Iter: 1873 loss: 4.43797774e-07
Iter: 1874 loss: 4.43571736e-07
Iter: 1875 loss: 4.4399053e-07
Iter: 1876 loss: 4.43474903e-07
Iter: 1877 loss: 4.43353883e-07
Iter: 1878 loss: 4.43531e-07
Iter: 1879 loss: 4.43250258e-07
Iter: 1880 loss: 4.43081603e-07
Iter: 1881 loss: 4.43692642e-07
Iter: 1882 loss: 4.43015949e-07
Iter: 1883 loss: 4.42807902e-07
Iter: 1884 loss: 4.43619854e-07
Iter: 1885 loss: 4.42783175e-07
Iter: 1886 loss: 4.42612816e-07
Iter: 1887 loss: 4.42938585e-07
Iter: 1888 loss: 4.4253494e-07
Iter: 1889 loss: 4.42344373e-07
Iter: 1890 loss: 4.42314843e-07
Iter: 1891 loss: 4.42199962e-07
Iter: 1892 loss: 4.42090681e-07
Iter: 1893 loss: 4.42042051e-07
Iter: 1894 loss: 4.41919781e-07
Iter: 1895 loss: 4.42040744e-07
Iter: 1896 loss: 4.41883969e-07
Iter: 1897 loss: 4.4175e-07
Iter: 1898 loss: 4.43345954e-07
Iter: 1899 loss: 4.4174709e-07
Iter: 1900 loss: 4.41634484e-07
Iter: 1901 loss: 4.41453551e-07
Iter: 1902 loss: 4.45831489e-07
Iter: 1903 loss: 4.41447412e-07
Iter: 1904 loss: 4.41308288e-07
Iter: 1905 loss: 4.41356349e-07
Iter: 1906 loss: 4.41207249e-07
Iter: 1907 loss: 4.40990874e-07
Iter: 1908 loss: 4.42025481e-07
Iter: 1909 loss: 4.40997098e-07
Iter: 1910 loss: 4.40830377e-07
Iter: 1911 loss: 4.41325653e-07
Iter: 1912 loss: 4.40751876e-07
Iter: 1913 loss: 4.40619488e-07
Iter: 1914 loss: 4.41702838e-07
Iter: 1915 loss: 4.40592288e-07
Iter: 1916 loss: 4.40453732e-07
Iter: 1917 loss: 4.40393592e-07
Iter: 1918 loss: 4.40343399e-07
Iter: 1919 loss: 4.40173693e-07
Iter: 1920 loss: 4.41309311e-07
Iter: 1921 loss: 4.40152064e-07
Iter: 1922 loss: 4.39977555e-07
Iter: 1923 loss: 4.40238068e-07
Iter: 1924 loss: 4.39901783e-07
Iter: 1925 loss: 4.39661733e-07
Iter: 1926 loss: 4.39799152e-07
Iter: 1927 loss: 4.39508028e-07
Iter: 1928 loss: 4.39370865e-07
Iter: 1929 loss: 4.39374958e-07
Iter: 1930 loss: 4.39230888e-07
Iter: 1931 loss: 4.39485547e-07
Iter: 1932 loss: 4.39177967e-07
Iter: 1933 loss: 4.38964094e-07
Iter: 1934 loss: 4.40507392e-07
Iter: 1935 loss: 4.3895713e-07
Iter: 1936 loss: 4.3890239e-07
Iter: 1937 loss: 4.38798168e-07
Iter: 1938 loss: 4.38803085e-07
Iter: 1939 loss: 4.38636505e-07
Iter: 1940 loss: 4.38465577e-07
Iter: 1941 loss: 4.38420557e-07
Iter: 1942 loss: 4.38275777e-07
Iter: 1943 loss: 4.39999098e-07
Iter: 1944 loss: 4.3828814e-07
Iter: 1945 loss: 4.38107662e-07
Iter: 1946 loss: 4.38464355e-07
Iter: 1947 loss: 4.38099534e-07
Iter: 1948 loss: 4.37947733e-07
Iter: 1949 loss: 4.38873656e-07
Iter: 1950 loss: 4.37948785e-07
Iter: 1951 loss: 4.37822962e-07
Iter: 1952 loss: 4.37816709e-07
Iter: 1953 loss: 4.37739061e-07
Iter: 1954 loss: 4.37586237e-07
Iter: 1955 loss: 4.38036949e-07
Iter: 1956 loss: 4.3755054e-07
Iter: 1957 loss: 4.37445e-07
Iter: 1958 loss: 4.37837429e-07
Iter: 1959 loss: 4.37392885e-07
Iter: 1960 loss: 4.37220166e-07
Iter: 1961 loss: 4.3779761e-07
Iter: 1962 loss: 4.37219626e-07
Iter: 1963 loss: 4.37121e-07
Iter: 1964 loss: 4.37688328e-07
Iter: 1965 loss: 4.37098436e-07
Iter: 1966 loss: 4.36986539e-07
Iter: 1967 loss: 4.3735227e-07
Iter: 1968 loss: 4.36928133e-07
Iter: 1969 loss: 4.3683977e-07
Iter: 1970 loss: 4.3713996e-07
Iter: 1971 loss: 4.36763514e-07
Iter: 1972 loss: 4.36711417e-07
Iter: 1973 loss: 4.3658352e-07
Iter: 1974 loss: 4.38924985e-07
Iter: 1975 loss: 4.36558963e-07
Iter: 1976 loss: 4.36328207e-07
Iter: 1977 loss: 4.37447397e-07
Iter: 1978 loss: 4.36349353e-07
Iter: 1979 loss: 4.36212588e-07
Iter: 1980 loss: 4.36152e-07
Iter: 1981 loss: 4.3606417e-07
Iter: 1982 loss: 4.35949e-07
Iter: 1983 loss: 4.35968502e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.4
+ date
Mon Oct 26 16:57:32 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.4/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.4_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.4_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.4_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 1 --phi 2.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.4_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d65140158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d65128a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d65128d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d651d0e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d651fa510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d651fa840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d65071840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d65014840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d650712f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d650396a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d64fafae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d64fa16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d64f4f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d64f52510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d64eea400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d64ef1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d64ef12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d64f3a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d64ea5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d64e53400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d64e7f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d64e00ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d42968730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d42988950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d429887b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4294a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d428ea950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d428d0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d428ead08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d428e3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d4287e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d1c159510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d1c144268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d1c0e1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d1c10e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9d1c0c98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 19886.9648
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Traceback (most recent call last):
  File "biholoNN_train.py", line 169, in <module>
    results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=train_func, initial_position=init_params, max_iterations=max_epochs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 287, in minimize
    parallel_iterations=parallel_iterations)[0]
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 574, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2499, in while_loop_v2
    return_same_structure=True)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2735, in while_loop
    loop_vars = body(*loop_vars)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 260, in _body
    max_line_search_iterations)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 156, in line_search_step
    max_iterations=max_iterations)  # No search needed for these.
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 259, in hager_zhang
    threshold_use_approximate_wolfe_condition)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 609, in _prepare_args
    val_initial = value_and_gradients_function(initial_step_size)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 251, in _restricted_func
    objective_value, gradient = value_and_gradients_function(pt)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 814, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
	 [[Sum_1/_34]]
  (1) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_f_16876]

Function call stack:
f -> f

++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.8
+ date
Mon Oct 26 17:01:44 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.8/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.8_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.8_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.8_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 1 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi2.8_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef52ff158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef53887b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef53a4bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef52bae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef52af2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef522b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef52af840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef51ac598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef51d7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef51d7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef5147730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef51468c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef5116950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef511f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef50b58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef50c1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef5077840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef5097c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef505f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef4feb378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef4feb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef4fddae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef4f8a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef4f9f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef4f9fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faef4f5e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faed8aba840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faed8a54730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faed8a43598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faed8a43950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faed8a349d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faed89eb158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faed89f48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faed897f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faed899f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7faeb4250e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1644.77075
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Traceback (most recent call last):
  File "biholoNN_train.py", line 169, in <module>
    results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=train_func, initial_position=init_params, max_iterations=max_epochs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 287, in minimize
    parallel_iterations=parallel_iterations)[0]
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 574, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2499, in while_loop_v2
    return_same_structure=True)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2735, in while_loop
    loop_vars = body(*loop_vars)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 260, in _body
    max_line_search_iterations)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 156, in line_search_step
    max_iterations=max_iterations)  # No search needed for these.
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 259, in hager_zhang
    threshold_use_approximate_wolfe_condition)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 609, in _prepare_args
    val_initial = value_and_gradients_function(initial_step_size)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 251, in _restricted_func
    objective_value, gradient = value_and_gradients_function(pt)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 814, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
	 [[Sum_1/_34]]
  (1) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_f_16876]

Function call stack:
f -> f

++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi2.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi3
+ date
Mon Oct 26 17:05:28 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi3/500_500_500_500_1 ']'
+ for npairs in 8000
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi3_8000
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi3_8000
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi3_8000/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi3/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 1 --phi 3 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output69/f1_psi1_phi3_8000/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a758158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a812840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a76c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a76c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a837598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a837e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a6cee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a662840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a691510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a691bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a5f8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a6059d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a605b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a5ccbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a5ccae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a56ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a544378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a54eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a4ee598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a4ab620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a4d51e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a4d5d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a43a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a445730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a43aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a3b9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb7a408840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb50f01510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb50ef2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb50f231e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb50ecf400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb50e761e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb50ea61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb50e34c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb50e3f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdb50dfff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000196806854
Iter: 2 loss: 0.000225864889
Iter: 3 loss: 0.000129632564
Iter: 4 loss: 0.000120274926
Iter: 5 loss: 0.000202982221
Iter: 6 loss: 0.000119795768
Iter: 7 loss: 0.000111847883
Iter: 8 loss: 0.000128758198
Iter: 9 loss: 0.000108656481
Iter: 10 loss: 0.000103114478
Iter: 11 loss: 0.00012573901
Iter: 12 loss: 0.000101961909
Iter: 13 loss: 9.95987793e-05
Iter: 14 loss: 0.000102792415
Iter: 15 loss: 9.83918653e-05
Iter: 16 loss: 9.64373e-05
Iter: 17 loss: 9.27032379e-05
Iter: 18 loss: 0.000172057335
Iter: 19 loss: 9.2690505e-05
Iter: 20 loss: 9.11056632e-05
Iter: 21 loss: 9.07879075e-05
Iter: 22 loss: 8.94290642e-05
Iter: 23 loss: 9.19495651e-05
Iter: 24 loss: 8.88446e-05
Iter: 25 loss: 8.72174351e-05
Iter: 26 loss: 8.93705292e-05
Iter: 27 loss: 8.63883906e-05
Iter: 28 loss: 8.50151118e-05
Iter: 29 loss: 9.05143606e-05
Iter: 30 loss: 8.4702915e-05
Iter: 31 loss: 8.3613646e-05
Iter: 32 loss: 8.60409709e-05
Iter: 33 loss: 8.319474e-05
Iter: 34 loss: 8.23406444e-05
Iter: 35 loss: 8.53536185e-05
Iter: 36 loss: 8.21158683e-05
Iter: 37 loss: 8.13793595e-05
Iter: 38 loss: 8.61427397e-05
Iter: 39 loss: 8.1297454e-05
Iter: 40 loss: 8.0875885e-05
Iter: 41 loss: 8.08388286e-05
Iter: 42 loss: 8.05198724e-05
Iter: 43 loss: 8.00268317e-05
Iter: 44 loss: 8.0019614e-05
Iter: 45 loss: 7.9470643e-05
Iter: 46 loss: 8.47811607e-05
Iter: 47 loss: 7.94513e-05
Iter: 48 loss: 7.90679915e-05
Iter: 49 loss: 7.86403543e-05
Iter: 50 loss: 7.85805532e-05
Iter: 51 loss: 7.80420451e-05
Iter: 52 loss: 8.10371857e-05
Iter: 53 loss: 7.79673792e-05
Iter: 54 loss: 7.75181252e-05
Iter: 55 loss: 7.7409044e-05
Iter: 56 loss: 7.71236228e-05
Iter: 57 loss: 7.67717574e-05
Iter: 58 loss: 7.67483143e-05
Iter: 59 loss: 7.64517317e-05
Iter: 60 loss: 7.60274706e-05
Iter: 61 loss: 7.60074108e-05
Iter: 62 loss: 7.54956855e-05
Iter: 63 loss: 7.73190623e-05
Iter: 64 loss: 7.53604545e-05
Iter: 65 loss: 7.49341707e-05
Iter: 66 loss: 7.63606222e-05
Iter: 67 loss: 7.48145103e-05
Iter: 68 loss: 7.44258869e-05
Iter: 69 loss: 7.56612717e-05
Iter: 70 loss: 7.4312833e-05
Iter: 71 loss: 7.40644609e-05
Iter: 72 loss: 7.40645e-05
Iter: 73 loss: 7.38130475e-05
Iter: 74 loss: 7.41547192e-05
Iter: 75 loss: 7.3685078e-05
Iter: 76 loss: 7.34899659e-05
Iter: 77 loss: 7.374501e-05
Iter: 78 loss: 7.33886118e-05
Iter: 79 loss: 7.31136315e-05
Iter: 80 loss: 7.34965361e-05
Iter: 81 loss: 7.29779786e-05
Iter: 82 loss: 7.27017905e-05
Iter: 83 loss: 7.30212196e-05
Iter: 84 loss: 7.25546706e-05
Iter: 85 loss: 7.22541663e-05
Iter: 86 loss: 7.2553783e-05
Iter: 87 loss: 7.20862154e-05
Iter: 88 loss: 7.17858275e-05
Iter: 89 loss: 7.38264716e-05
Iter: 90 loss: 7.1756549e-05
Iter: 91 loss: 7.15040223e-05
Iter: 92 loss: 7.28050145e-05
Iter: 93 loss: 7.14618873e-05
Iter: 94 loss: 7.12603069e-05
Iter: 95 loss: 7.13205591e-05
Iter: 96 loss: 7.11154426e-05
Iter: 97 loss: 7.08518055e-05
Iter: 98 loss: 7.08275256e-05
Iter: 99 loss: 7.0628e-05
Iter: 100 loss: 7.03194601e-05
Iter: 101 loss: 7.25716818e-05
Iter: 102 loss: 7.0296388e-05
Iter: 103 loss: 7.00177188e-05
Iter: 104 loss: 7.11544417e-05
Iter: 105 loss: 6.99573357e-05
Iter: 106 loss: 6.97888e-05
Iter: 107 loss: 6.97789292e-05
Iter: 108 loss: 6.96787392e-05
Iter: 109 loss: 6.95350755e-05
Iter: 110 loss: 6.95309718e-05
Iter: 111 loss: 6.93210095e-05
Iter: 112 loss: 7.01821118e-05
Iter: 113 loss: 6.92746689e-05
Iter: 114 loss: 6.9108326e-05
Iter: 115 loss: 6.93370894e-05
Iter: 116 loss: 6.90236338e-05
Iter: 117 loss: 6.88449509e-05
Iter: 118 loss: 6.87951542e-05
Iter: 119 loss: 6.86857384e-05
Iter: 120 loss: 6.84486877e-05
Iter: 121 loss: 7.00665914e-05
Iter: 122 loss: 6.84271872e-05
Iter: 123 loss: 6.82425307e-05
Iter: 124 loss: 6.87578286e-05
Iter: 125 loss: 6.81853271e-05
Iter: 126 loss: 6.79742734e-05
Iter: 127 loss: 6.86787243e-05
Iter: 128 loss: 6.79133445e-05
Iter: 129 loss: 6.77511634e-05
Iter: 130 loss: 6.76599157e-05
Iter: 131 loss: 6.75900665e-05
Iter: 132 loss: 6.73572504e-05
Iter: 133 loss: 6.82855607e-05
Iter: 134 loss: 6.73038448e-05
Iter: 135 loss: 6.70893569e-05
Iter: 136 loss: 6.79170844e-05
Iter: 137 loss: 6.70426307e-05
Iter: 138 loss: 6.69628789e-05
Iter: 139 loss: 6.69424e-05
Iter: 140 loss: 6.68468056e-05
Iter: 141 loss: 6.67337372e-05
Iter: 142 loss: 6.67174754e-05
Iter: 143 loss: 6.65655098e-05
Iter: 144 loss: 6.70995796e-05
Iter: 145 loss: 6.6523e-05
Iter: 146 loss: 6.63785468e-05
Iter: 147 loss: 6.67932472e-05
Iter: 148 loss: 6.6333916e-05
Iter: 149 loss: 6.61923841e-05
Iter: 150 loss: 6.60512596e-05
Iter: 151 loss: 6.60214137e-05
Iter: 152 loss: 6.58507051e-05
Iter: 153 loss: 6.73058894e-05
Iter: 154 loss: 6.58389181e-05
Iter: 155 loss: 6.56921911e-05
Iter: 156 loss: 6.57231139e-05
Iter: 157 loss: 6.55788826e-05
Iter: 158 loss: 6.53981479e-05
Iter: 159 loss: 6.75257761e-05
Iter: 160 loss: 6.53982133e-05
Iter: 161 loss: 6.52659583e-05
Iter: 162 loss: 6.51729642e-05
Iter: 163 loss: 6.51298833e-05
Iter: 164 loss: 6.49290814e-05
Iter: 165 loss: 6.57275523e-05
Iter: 166 loss: 6.48847636e-05
Iter: 167 loss: 6.47326378e-05
Iter: 168 loss: 6.51325317e-05
Iter: 169 loss: 6.46783083e-05
Iter: 170 loss: 6.45112523e-05
Iter: 171 loss: 6.51400114e-05
Iter: 172 loss: 6.4468084e-05
Iter: 173 loss: 6.43988897e-05
Iter: 174 loss: 6.43795938e-05
Iter: 175 loss: 6.432386e-05
Iter: 176 loss: 6.42049345e-05
Iter: 177 loss: 6.59772486e-05
Iter: 178 loss: 6.42000959e-05
Iter: 179 loss: 6.40963e-05
Iter: 180 loss: 6.55572949e-05
Iter: 181 loss: 6.40962389e-05
Iter: 182 loss: 6.40195285e-05
Iter: 183 loss: 6.38766942e-05
Iter: 184 loss: 6.73093309e-05
Iter: 185 loss: 6.38740094e-05
Iter: 186 loss: 6.37037592e-05
Iter: 187 loss: 6.42192535e-05
Iter: 188 loss: 6.36507684e-05
Iter: 189 loss: 6.35025499e-05
Iter: 190 loss: 6.4083e-05
Iter: 191 loss: 6.34701864e-05
Iter: 192 loss: 6.33299205e-05
Iter: 193 loss: 6.40857761e-05
Iter: 194 loss: 6.33102754e-05
Iter: 195 loss: 6.31738876e-05
Iter: 196 loss: 6.33237505e-05
Iter: 197 loss: 6.30970171e-05
Iter: 198 loss: 6.29501e-05
Iter: 199 loss: 6.34933676e-05
Iter: 200 loss: 6.29085116e-05
Iter: 201 loss: 6.27891859e-05
Iter: 202 loss: 6.28137277e-05
Iter: 203 loss: 6.27038535e-05
Iter: 204 loss: 6.25263492e-05
Iter: 205 loss: 6.31815637e-05
Iter: 206 loss: 6.24860259e-05
Iter: 207 loss: 6.24176682e-05
Iter: 208 loss: 6.24097811e-05
Iter: 209 loss: 6.2323139e-05
Iter: 210 loss: 6.22415319e-05
Iter: 211 loss: 6.22231237e-05
Iter: 212 loss: 6.21385188e-05
Iter: 213 loss: 6.24574604e-05
Iter: 214 loss: 6.21178478e-05
Iter: 215 loss: 6.20187711e-05
Iter: 216 loss: 6.21232102e-05
Iter: 217 loss: 6.1961633e-05
Iter: 218 loss: 6.1849074e-05
Iter: 219 loss: 6.1904786e-05
Iter: 220 loss: 6.17712503e-05
Iter: 221 loss: 6.16347243e-05
Iter: 222 loss: 6.17142068e-05
Iter: 223 loss: 6.15475292e-05
Iter: 224 loss: 6.14143937e-05
Iter: 225 loss: 6.3206222e-05
Iter: 226 loss: 6.1415165e-05
Iter: 227 loss: 6.13222364e-05
Iter: 228 loss: 6.15367753e-05
Iter: 229 loss: 6.12890726e-05
Iter: 230 loss: 6.11814758e-05
Iter: 231 loss: 6.14583e-05
Iter: 232 loss: 6.11462747e-05
Iter: 233 loss: 6.10418792e-05
Iter: 234 loss: 6.10400166e-05
Iter: 235 loss: 6.09605922e-05
Iter: 236 loss: 6.08161863e-05
Iter: 237 loss: 6.12906297e-05
Iter: 238 loss: 6.07775364e-05
Iter: 239 loss: 6.06570975e-05
Iter: 240 loss: 6.1337254e-05
Iter: 241 loss: 6.06399153e-05
Iter: 242 loss: 6.0533861e-05
Iter: 243 loss: 6.20806677e-05
Iter: 244 loss: 6.05358291e-05
Iter: 245 loss: 6.04758643e-05
Iter: 246 loss: 6.03824e-05
Iter: 247 loss: 6.03804219e-05
Iter: 248 loss: 6.02632754e-05
Iter: 249 loss: 6.06824105e-05
Iter: 250 loss: 6.02356959e-05
Iter: 251 loss: 6.01249776e-05
Iter: 252 loss: 6.07064067e-05
Iter: 253 loss: 6.01077554e-05
Iter: 254 loss: 6.00326712e-05
Iter: 255 loss: 5.99122468e-05
Iter: 256 loss: 5.9910155e-05
Iter: 257 loss: 5.97936451e-05
Iter: 258 loss: 6.07156071e-05
Iter: 259 loss: 5.97866674e-05
Iter: 260 loss: 5.96728823e-05
Iter: 261 loss: 5.97429425e-05
Iter: 262 loss: 5.95994425e-05
Iter: 263 loss: 5.94852099e-05
Iter: 264 loss: 6.13018638e-05
Iter: 265 loss: 5.94859339e-05
Iter: 266 loss: 5.94136363e-05
Iter: 267 loss: 5.93439181e-05
Iter: 268 loss: 5.93283621e-05
Iter: 269 loss: 5.92037795e-05
Iter: 270 loss: 5.96158388e-05
Iter: 271 loss: 5.91693606e-05
Iter: 272 loss: 5.90513591e-05
Iter: 273 loss: 5.92602337e-05
Iter: 274 loss: 5.8996131e-05
Iter: 275 loss: 5.89530537e-05
Iter: 276 loss: 5.89266419e-05
Iter: 277 loss: 5.88678813e-05
Iter: 278 loss: 5.88383264e-05
Iter: 279 loss: 5.88124458e-05
Iter: 280 loss: 5.87355826e-05
Iter: 281 loss: 5.86852329e-05
Iter: 282 loss: 5.86575843e-05
Iter: 283 loss: 5.85514026e-05
Iter: 284 loss: 5.97191247e-05
Iter: 285 loss: 5.85488815e-05
Iter: 286 loss: 5.84747468e-05
Iter: 287 loss: 5.83920155e-05
Iter: 288 loss: 5.83802357e-05
Iter: 289 loss: 5.82683097e-05
Iter: 290 loss: 5.84648733e-05
Iter: 291 loss: 5.82201901e-05
Iter: 292 loss: 5.80815395e-05
Iter: 293 loss: 5.86350143e-05
Iter: 294 loss: 5.80493506e-05
Iter: 295 loss: 5.79396728e-05
Iter: 296 loss: 5.83518122e-05
Iter: 297 loss: 5.79151601e-05
Iter: 298 loss: 5.7798723e-05
Iter: 299 loss: 5.81996137e-05
Iter: 300 loss: 5.77642058e-05
Iter: 301 loss: 5.76734456e-05
Iter: 302 loss: 5.80856358e-05
Iter: 303 loss: 5.76516322e-05
Iter: 304 loss: 5.75756276e-05
Iter: 305 loss: 5.74666483e-05
Iter: 306 loss: 5.74625446e-05
Iter: 307 loss: 5.73886136e-05
Iter: 308 loss: 5.73795478e-05
Iter: 309 loss: 5.72925564e-05
Iter: 310 loss: 5.7578236e-05
Iter: 311 loss: 5.7262987e-05
Iter: 312 loss: 5.719404e-05
Iter: 313 loss: 5.71396085e-05
Iter: 314 loss: 5.71202727e-05
Iter: 315 loss: 5.70241391e-05
Iter: 316 loss: 5.7512756e-05
Iter: 317 loss: 5.70116426e-05
Iter: 318 loss: 5.69121403e-05
Iter: 319 loss: 5.70365883e-05
Iter: 320 loss: 5.68610703e-05
Iter: 321 loss: 5.6770943e-05
Iter: 322 loss: 5.69189433e-05
Iter: 323 loss: 5.67278184e-05
Iter: 324 loss: 5.66232375e-05
Iter: 325 loss: 5.6771969e-05
Iter: 326 loss: 5.65728915e-05
Iter: 327 loss: 5.64651782e-05
Iter: 328 loss: 5.67310344e-05
Iter: 329 loss: 5.64230941e-05
Iter: 330 loss: 5.63226058e-05
Iter: 331 loss: 5.70057236e-05
Iter: 332 loss: 5.63125541e-05
Iter: 333 loss: 5.62337809e-05
Iter: 334 loss: 5.66789058e-05
Iter: 335 loss: 5.62211535e-05
Iter: 336 loss: 5.61540437e-05
Iter: 337 loss: 5.61068591e-05
Iter: 338 loss: 5.60819317e-05
Iter: 339 loss: 5.59870205e-05
Iter: 340 loss: 5.62165915e-05
Iter: 341 loss: 5.59518412e-05
Iter: 342 loss: 5.59277833e-05
Iter: 343 loss: 5.59008222e-05
Iter: 344 loss: 5.58560787e-05
Iter: 345 loss: 5.57832354e-05
Iter: 346 loss: 5.57811873e-05
Iter: 347 loss: 5.56983796e-05
Iter: 348 loss: 5.57675303e-05
Iter: 349 loss: 5.56499836e-05
Iter: 350 loss: 5.55637889e-05
Iter: 351 loss: 5.65254741e-05
Iter: 352 loss: 5.5563909e-05
Iter: 353 loss: 5.54974758e-05
Iter: 354 loss: 5.54892504e-05
Iter: 355 loss: 5.54488288e-05
Iter: 356 loss: 5.53493119e-05
Iter: 357 loss: 5.54076178e-05
Iter: 358 loss: 5.52848214e-05
Iter: 359 loss: 5.51851772e-05
Iter: 360 loss: 5.54988364e-05
Iter: 361 loss: 5.51563135e-05
Iter: 362 loss: 5.50496734e-05
Iter: 363 loss: 5.5334749e-05
Iter: 364 loss: 5.50118e-05
Iter: 365 loss: 5.49193355e-05
Iter: 366 loss: 5.55892329e-05
Iter: 367 loss: 5.49132528e-05
Iter: 368 loss: 5.48288408e-05
Iter: 369 loss: 5.49667566e-05
Iter: 370 loss: 5.47916279e-05
Iter: 371 loss: 5.47063282e-05
Iter: 372 loss: 5.49352553e-05
Iter: 373 loss: 5.46753872e-05
Iter: 374 loss: 5.4593289e-05
Iter: 375 loss: 5.46635e-05
Iter: 376 loss: 5.4544158e-05
Iter: 377 loss: 5.45143594e-05
Iter: 378 loss: 5.44878785e-05
Iter: 379 loss: 5.4455093e-05
Iter: 380 loss: 5.43671e-05
Iter: 381 loss: 5.49937431e-05
Iter: 382 loss: 5.43491551e-05
Iter: 383 loss: 5.42390189e-05
Iter: 384 loss: 5.46967931e-05
Iter: 385 loss: 5.42171765e-05
Iter: 386 loss: 5.41304107e-05
Iter: 387 loss: 5.50523291e-05
Iter: 388 loss: 5.41261834e-05
Iter: 389 loss: 5.4067179e-05
Iter: 390 loss: 5.40611909e-05
Iter: 391 loss: 5.40175279e-05
Iter: 392 loss: 5.3943204e-05
Iter: 393 loss: 5.39123503e-05
Iter: 394 loss: 5.38731038e-05
Iter: 395 loss: 5.37777814e-05
Iter: 396 loss: 5.48064636e-05
Iter: 397 loss: 5.37746164e-05
Iter: 398 loss: 5.37032829e-05
Iter: 399 loss: 5.37425294e-05
Iter: 400 loss: 5.3660835e-05
Iter: 401 loss: 5.35590952e-05
Iter: 402 loss: 5.41146e-05
Iter: 403 loss: 5.35425352e-05
Iter: 404 loss: 5.34692372e-05
Iter: 405 loss: 5.37966334e-05
Iter: 406 loss: 5.34545907e-05
Iter: 407 loss: 5.33832535e-05
Iter: 408 loss: 5.33219682e-05
Iter: 409 loss: 5.33041384e-05
Iter: 410 loss: 5.32951526e-05
Iter: 411 loss: 5.32624181e-05
Iter: 412 loss: 5.32209706e-05
Iter: 413 loss: 5.31386104e-05
Iter: 414 loss: 5.48966782e-05
Iter: 415 loss: 5.31376027e-05
Iter: 416 loss: 5.30579e-05
Iter: 417 loss: 5.30802245e-05
Iter: 418 loss: 5.29984063e-05
Iter: 419 loss: 5.29094541e-05
Iter: 420 loss: 5.391593e-05
Iter: 421 loss: 5.29099707e-05
Iter: 422 loss: 5.28391101e-05
Iter: 423 loss: 5.29982281e-05
Iter: 424 loss: 5.28115852e-05
Iter: 425 loss: 5.27487937e-05
Iter: 426 loss: 5.27422635e-05
Iter: 427 loss: 5.26974181e-05
Iter: 428 loss: 5.26025106e-05
Iter: 429 loss: 5.27868324e-05
Iter: 430 loss: 5.2561154e-05
Iter: 431 loss: 5.24524621e-05
Iter: 432 loss: 5.25965806e-05
Iter: 433 loss: 5.2398027e-05
Iter: 434 loss: 5.231877e-05
Iter: 435 loss: 5.33134735e-05
Iter: 436 loss: 5.23175331e-05
Iter: 437 loss: 5.22507544e-05
Iter: 438 loss: 5.23842391e-05
Iter: 439 loss: 5.2221425e-05
Iter: 440 loss: 5.21480833e-05
Iter: 441 loss: 5.22910232e-05
Iter: 442 loss: 5.21134207e-05
Iter: 443 loss: 5.20389658e-05
Iter: 444 loss: 5.21905604e-05
Iter: 445 loss: 5.20085232e-05
Iter: 446 loss: 5.19390051e-05
Iter: 447 loss: 5.19384703e-05
Iter: 448 loss: 5.19000023e-05
Iter: 449 loss: 5.18453344e-05
Iter: 450 loss: 5.18426059e-05
Iter: 451 loss: 5.17739427e-05
Iter: 452 loss: 5.17381741e-05
Iter: 453 loss: 5.1703988e-05
Iter: 454 loss: 5.16203e-05
Iter: 455 loss: 5.2820702e-05
Iter: 456 loss: 5.16204746e-05
Iter: 457 loss: 5.15612919e-05
Iter: 458 loss: 5.17176668e-05
Iter: 459 loss: 5.15431384e-05
Iter: 460 loss: 5.14859e-05
Iter: 461 loss: 5.14416934e-05
Iter: 462 loss: 5.14232524e-05
Iter: 463 loss: 5.13380146e-05
Iter: 464 loss: 5.14990606e-05
Iter: 465 loss: 5.1300718e-05
Iter: 466 loss: 5.11995531e-05
Iter: 467 loss: 5.14848325e-05
Iter: 468 loss: 5.1166804e-05
Iter: 469 loss: 5.10781683e-05
Iter: 470 loss: 5.18860179e-05
Iter: 471 loss: 5.10747595e-05
Iter: 472 loss: 5.10019818e-05
Iter: 473 loss: 5.12441511e-05
Iter: 474 loss: 5.09847e-05
Iter: 475 loss: 5.09150195e-05
Iter: 476 loss: 5.08939847e-05
Iter: 477 loss: 5.08554731e-05
Iter: 478 loss: 5.08186858e-05
Iter: 479 loss: 5.08046614e-05
Iter: 480 loss: 5.07620134e-05
Iter: 481 loss: 5.07817895e-05
Iter: 482 loss: 5.0733528e-05
Iter: 483 loss: 5.06924407e-05
Iter: 484 loss: 5.0617e-05
Iter: 485 loss: 5.06197757e-05
Iter: 486 loss: 5.05432945e-05
Iter: 487 loss: 5.1095918e-05
Iter: 488 loss: 5.05404605e-05
Iter: 489 loss: 5.04673735e-05
Iter: 490 loss: 5.07787372e-05
Iter: 491 loss: 5.04536292e-05
Iter: 492 loss: 5.0395076e-05
Iter: 493 loss: 5.04643394e-05
Iter: 494 loss: 5.03607407e-05
Iter: 495 loss: 5.03083029e-05
Iter: 496 loss: 5.03360061e-05
Iter: 497 loss: 5.02743642e-05
Iter: 498 loss: 5.01949835e-05
Iter: 499 loss: 5.04474083e-05
Iter: 500 loss: 5.01723262e-05
Iter: 501 loss: 5.00998358e-05
Iter: 502 loss: 5.01756585e-05
Iter: 503 loss: 5.0058763e-05
Iter: 504 loss: 4.99858143e-05
Iter: 505 loss: 5.06935066e-05
Iter: 506 loss: 4.99813832e-05
Iter: 507 loss: 4.99148082e-05
Iter: 508 loss: 5.00421738e-05
Iter: 509 loss: 4.98885565e-05
Iter: 510 loss: 4.98262962e-05
Iter: 511 loss: 5.00391288e-05
Iter: 512 loss: 4.98084919e-05
Iter: 513 loss: 4.97503788e-05
Iter: 514 loss: 5.05371645e-05
Iter: 515 loss: 4.97509318e-05
Iter: 516 loss: 4.97172514e-05
Iter: 517 loss: 4.96529246e-05
Iter: 518 loss: 5.10937462e-05
Iter: 519 loss: 4.96535213e-05
Iter: 520 loss: 4.95981949e-05
Iter: 521 loss: 4.96316425e-05
Iter: 522 loss: 4.95611457e-05
Iter: 523 loss: 4.94904052e-05
Iter: 524 loss: 5.02464245e-05
Iter: 525 loss: 4.94880296e-05
Iter: 526 loss: 4.94310189e-05
Iter: 527 loss: 4.96097891e-05
Iter: 528 loss: 4.9418697e-05
Iter: 529 loss: 4.93705629e-05
Iter: 530 loss: 4.93363696e-05
Iter: 531 loss: 4.93184e-05
Iter: 532 loss: 4.92443651e-05
Iter: 533 loss: 4.9551898e-05
Iter: 534 loss: 4.92282416e-05
Iter: 535 loss: 4.91627652e-05
Iter: 536 loss: 4.9139584e-05
Iter: 537 loss: 4.9102131e-05
Iter: 538 loss: 4.90155289e-05
Iter: 539 loss: 4.96422144e-05
Iter: 540 loss: 4.90063467e-05
Iter: 541 loss: 4.89365848e-05
Iter: 542 loss: 4.9494025e-05
Iter: 543 loss: 4.8930815e-05
Iter: 544 loss: 4.88747755e-05
Iter: 545 loss: 4.8944381e-05
Iter: 546 loss: 4.88436417e-05
Iter: 547 loss: 4.87964135e-05
Iter: 548 loss: 4.87951947e-05
Iter: 549 loss: 4.87526922e-05
Iter: 550 loss: 4.87130201e-05
Iter: 551 loss: 4.87024372e-05
Iter: 552 loss: 4.86492499e-05
Iter: 553 loss: 4.86580466e-05
Iter: 554 loss: 4.86103199e-05
Iter: 555 loss: 4.85438395e-05
Iter: 556 loss: 4.87413417e-05
Iter: 557 loss: 4.85214e-05
Iter: 558 loss: 4.84664124e-05
Iter: 559 loss: 4.90847233e-05
Iter: 560 loss: 4.84639786e-05
Iter: 561 loss: 4.8422502e-05
Iter: 562 loss: 4.84270931e-05
Iter: 563 loss: 4.83848489e-05
Iter: 564 loss: 4.8326503e-05
Iter: 565 loss: 4.83341682e-05
Iter: 566 loss: 4.82813339e-05
Iter: 567 loss: 4.82007454e-05
Iter: 568 loss: 4.85373967e-05
Iter: 569 loss: 4.81846437e-05
Iter: 570 loss: 4.81203097e-05
Iter: 571 loss: 4.82480173e-05
Iter: 572 loss: 4.80960516e-05
Iter: 573 loss: 4.80152376e-05
Iter: 574 loss: 4.82833639e-05
Iter: 575 loss: 4.79929367e-05
Iter: 576 loss: 4.79332084e-05
Iter: 577 loss: 4.82888718e-05
Iter: 578 loss: 4.79279333e-05
Iter: 579 loss: 4.78747934e-05
Iter: 580 loss: 4.82167234e-05
Iter: 581 loss: 4.787125e-05
Iter: 582 loss: 4.78323054e-05
Iter: 583 loss: 4.81758543e-05
Iter: 584 loss: 4.78307702e-05
Iter: 585 loss: 4.78053771e-05
Iter: 586 loss: 4.77418289e-05
Iter: 587 loss: 4.84265584e-05
Iter: 588 loss: 4.77376925e-05
Iter: 589 loss: 4.7667083e-05
Iter: 590 loss: 4.78871298e-05
Iter: 591 loss: 4.7646754e-05
Iter: 592 loss: 4.75900597e-05
Iter: 593 loss: 4.77396279e-05
Iter: 594 loss: 4.7569145e-05
Iter: 595 loss: 4.75037086e-05
Iter: 596 loss: 4.7981659e-05
Iter: 597 loss: 4.74932676e-05
Iter: 598 loss: 4.74446642e-05
Iter: 599 loss: 4.74311746e-05
Iter: 600 loss: 4.74017434e-05
Iter: 601 loss: 4.73364635e-05
Iter: 602 loss: 4.74021945e-05
Iter: 603 loss: 4.72996435e-05
Iter: 604 loss: 4.72209067e-05
Iter: 605 loss: 4.76371497e-05
Iter: 606 loss: 4.72095344e-05
Iter: 607 loss: 4.71491949e-05
Iter: 608 loss: 4.73221444e-05
Iter: 609 loss: 4.71266467e-05
Iter: 610 loss: 4.70571722e-05
Iter: 611 loss: 4.71246458e-05
Iter: 612 loss: 4.70186351e-05
Iter: 613 loss: 4.69695879e-05
Iter: 614 loss: 4.69661281e-05
Iter: 615 loss: 4.69357037e-05
Iter: 616 loss: 4.7155092e-05
Iter: 617 loss: 4.69313418e-05
Iter: 618 loss: 4.68962462e-05
Iter: 619 loss: 4.68518883e-05
Iter: 620 loss: 4.68494763e-05
Iter: 621 loss: 4.67950595e-05
Iter: 622 loss: 4.68476719e-05
Iter: 623 loss: 4.6761983e-05
Iter: 624 loss: 4.66953497e-05
Iter: 625 loss: 4.67400168e-05
Iter: 626 loss: 4.66523743e-05
Iter: 627 loss: 4.6585028e-05
Iter: 628 loss: 4.73613181e-05
Iter: 629 loss: 4.65858684e-05
Iter: 630 loss: 4.65319099e-05
Iter: 631 loss: 4.67087484e-05
Iter: 632 loss: 4.65202902e-05
Iter: 633 loss: 4.64644872e-05
Iter: 634 loss: 4.64530385e-05
Iter: 635 loss: 4.64156728e-05
Iter: 636 loss: 4.63578544e-05
Iter: 637 loss: 4.65019e-05
Iter: 638 loss: 4.63360484e-05
Iter: 639 loss: 4.62721837e-05
Iter: 640 loss: 4.63969336e-05
Iter: 641 loss: 4.62414901e-05
Iter: 642 loss: 4.61838899e-05
Iter: 643 loss: 4.63239703e-05
Iter: 644 loss: 4.61618256e-05
Iter: 645 loss: 4.61034e-05
Iter: 646 loss: 4.64720724e-05
Iter: 647 loss: 4.60971823e-05
Iter: 648 loss: 4.60423471e-05
Iter: 649 loss: 4.65881458e-05
Iter: 650 loss: 4.60400843e-05
Iter: 651 loss: 4.59987787e-05
Iter: 652 loss: 4.61613381e-05
Iter: 653 loss: 4.59914445e-05
Iter: 654 loss: 4.59627408e-05
Iter: 655 loss: 4.59078292e-05
Iter: 656 loss: 4.7227084e-05
Iter: 657 loss: 4.59063958e-05
Iter: 658 loss: 4.5847486e-05
Iter: 659 loss: 4.60565352e-05
Iter: 660 loss: 4.58314935e-05
Iter: 661 loss: 4.57791502e-05
Iter: 662 loss: 4.5876357e-05
Iter: 663 loss: 4.57562e-05
Iter: 664 loss: 4.56885464e-05
Iter: 665 loss: 4.6066787e-05
Iter: 666 loss: 4.56797643e-05
Iter: 667 loss: 4.56296912e-05
Iter: 668 loss: 4.59022194e-05
Iter: 669 loss: 4.56225716e-05
Iter: 670 loss: 4.55809932e-05
Iter: 671 loss: 4.55162735e-05
Iter: 672 loss: 4.55157278e-05
Iter: 673 loss: 4.54441688e-05
Iter: 674 loss: 4.60266892e-05
Iter: 675 loss: 4.54403853e-05
Iter: 676 loss: 4.53825378e-05
Iter: 677 loss: 4.54128058e-05
Iter: 678 loss: 4.53449065e-05
Iter: 679 loss: 4.5286004e-05
Iter: 680 loss: 4.57687711e-05
Iter: 681 loss: 4.52831227e-05
Iter: 682 loss: 4.52377935e-05
Iter: 683 loss: 4.54081528e-05
Iter: 684 loss: 4.52266249e-05
Iter: 685 loss: 4.51873711e-05
Iter: 686 loss: 4.57468e-05
Iter: 687 loss: 4.51868837e-05
Iter: 688 loss: 4.51622182e-05
Iter: 689 loss: 4.51273809e-05
Iter: 690 loss: 4.51250598e-05
Iter: 691 loss: 4.50811021e-05
Iter: 692 loss: 4.52812201e-05
Iter: 693 loss: 4.50728912e-05
Iter: 694 loss: 4.50349326e-05
Iter: 695 loss: 4.50180669e-05
Iter: 696 loss: 4.50009829e-05
Iter: 697 loss: 4.49481595e-05
Iter: 698 loss: 4.50715233e-05
Iter: 699 loss: 4.49274812e-05
Iter: 700 loss: 4.48824758e-05
Iter: 701 loss: 4.53653192e-05
Iter: 702 loss: 4.48828541e-05
Iter: 703 loss: 4.4842709e-05
Iter: 704 loss: 4.49180079e-05
Iter: 705 loss: 4.48260544e-05
Iter: 706 loss: 4.47911407e-05
Iter: 707 loss: 4.47535785e-05
Iter: 708 loss: 4.47478888e-05
Iter: 709 loss: 4.46820814e-05
Iter: 710 loss: 4.49909276e-05
Iter: 711 loss: 4.467088e-05
Iter: 712 loss: 4.46213162e-05
Iter: 713 loss: 4.4741726e-05
Iter: 714 loss: 4.46047634e-05
Iter: 715 loss: 4.4553115e-05
Iter: 716 loss: 4.4725246e-05
Iter: 717 loss: 4.45382102e-05
Iter: 718 loss: 4.45166515e-05
Iter: 719 loss: 4.45110491e-05
Iter: 720 loss: 4.44847428e-05
Iter: 721 loss: 4.4445791e-05
Iter: 722 loss: 4.44445213e-05
Iter: 723 loss: 4.44020552e-05
Iter: 724 loss: 4.45518745e-05
Iter: 725 loss: 4.43920981e-05
Iter: 726 loss: 4.43553436e-05
Iter: 727 loss: 4.44010402e-05
Iter: 728 loss: 4.4336095e-05
Iter: 729 loss: 4.42874079e-05
Iter: 730 loss: 4.43188728e-05
Iter: 731 loss: 4.42611636e-05
Iter: 732 loss: 4.42118726e-05
Iter: 733 loss: 4.4347009e-05
Iter: 734 loss: 4.41987577e-05
Iter: 735 loss: 4.41539341e-05
Iter: 736 loss: 4.46979684e-05
Iter: 737 loss: 4.41518641e-05
Iter: 738 loss: 4.41223092e-05
Iter: 739 loss: 4.41348e-05
Iter: 740 loss: 4.41007942e-05
Iter: 741 loss: 4.40599324e-05
Iter: 742 loss: 4.41446464e-05
Iter: 743 loss: 4.40454496e-05
Iter: 744 loss: 4.40040858e-05
Iter: 745 loss: 4.40110671e-05
Iter: 746 loss: 4.39741561e-05
Iter: 747 loss: 4.39181749e-05
Iter: 748 loss: 4.41184457e-05
Iter: 749 loss: 4.39020077e-05
Iter: 750 loss: 4.38523675e-05
Iter: 751 loss: 4.40316617e-05
Iter: 752 loss: 4.38389179e-05
Iter: 753 loss: 4.38336756e-05
Iter: 754 loss: 4.38186798e-05
Iter: 755 loss: 4.37999079e-05
Iter: 756 loss: 4.37587587e-05
Iter: 757 loss: 4.40742442e-05
Iter: 758 loss: 4.37479612e-05
Iter: 759 loss: 4.36995251e-05
Iter: 760 loss: 4.40103177e-05
Iter: 761 loss: 4.36947594e-05
Iter: 762 loss: 4.36605078e-05
Iter: 763 loss: 4.38231691e-05
Iter: 764 loss: 4.36569644e-05
Iter: 765 loss: 4.36248738e-05
Iter: 766 loss: 4.36102346e-05
Iter: 767 loss: 4.35930051e-05
Iter: 768 loss: 4.35538095e-05
Iter: 769 loss: 4.37504204e-05
Iter: 770 loss: 4.35456604e-05
Iter: 771 loss: 4.35078146e-05
Iter: 772 loss: 4.36332084e-05
Iter: 773 loss: 4.3496766e-05
Iter: 774 loss: 4.34516551e-05
Iter: 775 loss: 4.35633847e-05
Iter: 776 loss: 4.34358444e-05
Iter: 777 loss: 4.33992464e-05
Iter: 778 loss: 4.33820314e-05
Iter: 779 loss: 4.33651076e-05
Iter: 780 loss: 4.33140267e-05
Iter: 781 loss: 4.34852045e-05
Iter: 782 loss: 4.32994275e-05
Iter: 783 loss: 4.3246233e-05
Iter: 784 loss: 4.33327259e-05
Iter: 785 loss: 4.32224697e-05
Iter: 786 loss: 4.31891604e-05
Iter: 787 loss: 4.3188058e-05
Iter: 788 loss: 4.3157379e-05
Iter: 789 loss: 4.32980378e-05
Iter: 790 loss: 4.31495973e-05
Iter: 791 loss: 4.31296357e-05
Iter: 792 loss: 4.30974615e-05
Iter: 793 loss: 4.30983673e-05
Iter: 794 loss: 4.30464715e-05
Iter: 795 loss: 4.31356202e-05
Iter: 796 loss: 4.30246691e-05
Iter: 797 loss: 4.29879547e-05
Iter: 798 loss: 4.34961767e-05
Iter: 799 loss: 4.29871434e-05
Iter: 800 loss: 4.29585234e-05
Iter: 801 loss: 4.29358915e-05
Iter: 802 loss: 4.2928732e-05
Iter: 803 loss: 4.28809799e-05
Iter: 804 loss: 4.30362634e-05
Iter: 805 loss: 4.28675921e-05
Iter: 806 loss: 4.28338608e-05
Iter: 807 loss: 4.31531116e-05
Iter: 808 loss: 4.28317944e-05
Iter: 809 loss: 4.27994892e-05
Iter: 810 loss: 4.27807972e-05
Iter: 811 loss: 4.27638479e-05
Iter: 812 loss: 4.27200175e-05
Iter: 813 loss: 4.29025313e-05
Iter: 814 loss: 4.27114428e-05
Iter: 815 loss: 4.26744591e-05
Iter: 816 loss: 4.27084378e-05
Iter: 817 loss: 4.26535735e-05
Iter: 818 loss: 4.26073602e-05
Iter: 819 loss: 4.2747346e-05
Iter: 820 loss: 4.25946382e-05
Iter: 821 loss: 4.25645776e-05
Iter: 822 loss: 4.25661965e-05
Iter: 823 loss: 4.25329235e-05
Iter: 824 loss: 4.25777544e-05
Iter: 825 loss: 4.25191029e-05
Iter: 826 loss: 4.25002181e-05
Iter: 827 loss: 4.24620448e-05
Iter: 828 loss: 4.33418099e-05
Iter: 829 loss: 4.24615318e-05
Iter: 830 loss: 4.241231e-05
Iter: 831 loss: 4.25347789e-05
Iter: 832 loss: 4.23963029e-05
Iter: 833 loss: 4.23493329e-05
Iter: 834 loss: 4.26748957e-05
Iter: 835 loss: 4.23441e-05
Iter: 836 loss: 4.23035381e-05
Iter: 837 loss: 4.24379468e-05
Iter: 838 loss: 4.22921075e-05
Iter: 839 loss: 4.22533376e-05
Iter: 840 loss: 4.22482117e-05
Iter: 841 loss: 4.22181911e-05
Iter: 842 loss: 4.21720397e-05
Iter: 843 loss: 4.25272083e-05
Iter: 844 loss: 4.21688092e-05
Iter: 845 loss: 4.21295372e-05
Iter: 846 loss: 4.22451594e-05
Iter: 847 loss: 4.21178047e-05
Iter: 848 loss: 4.20737e-05
Iter: 849 loss: 4.21491e-05
Iter: 850 loss: 4.20543874e-05
Iter: 851 loss: 4.20209471e-05
Iter: 852 loss: 4.20505676e-05
Iter: 853 loss: 4.2003674e-05
Iter: 854 loss: 4.19611351e-05
Iter: 855 loss: 4.2057e-05
Iter: 856 loss: 4.19450589e-05
Iter: 857 loss: 4.19234748e-05
Iter: 858 loss: 4.1922387e-05
Iter: 859 loss: 4.18954078e-05
Iter: 860 loss: 4.18762029e-05
Iter: 861 loss: 4.18691343e-05
Iter: 862 loss: 4.18361633e-05
Iter: 863 loss: 4.183294e-05
Iter: 864 loss: 4.18061318e-05
Iter: 865 loss: 4.17661395e-05
Iter: 866 loss: 4.17798256e-05
Iter: 867 loss: 4.17374467e-05
Iter: 868 loss: 4.16821e-05
Iter: 869 loss: 4.19688404e-05
Iter: 870 loss: 4.16724943e-05
Iter: 871 loss: 4.16278344e-05
Iter: 872 loss: 4.19240387e-05
Iter: 873 loss: 4.16231924e-05
Iter: 874 loss: 4.15807226e-05
Iter: 875 loss: 4.16586736e-05
Iter: 876 loss: 4.15622089e-05
Iter: 877 loss: 4.15219038e-05
Iter: 878 loss: 4.15693721e-05
Iter: 879 loss: 4.1502346e-05
Iter: 880 loss: 4.14648821e-05
Iter: 881 loss: 4.18139243e-05
Iter: 882 loss: 4.14639035e-05
Iter: 883 loss: 4.14288152e-05
Iter: 884 loss: 4.14594106e-05
Iter: 885 loss: 4.14096648e-05
Iter: 886 loss: 4.13715534e-05
Iter: 887 loss: 4.14315109e-05
Iter: 888 loss: 4.13588059e-05
Iter: 889 loss: 4.13177295e-05
Iter: 890 loss: 4.13975649e-05
Iter: 891 loss: 4.13009257e-05
Iter: 892 loss: 4.12854133e-05
Iter: 893 loss: 4.12778136e-05
Iter: 894 loss: 4.12600602e-05
Iter: 895 loss: 4.12235313e-05
Iter: 896 loss: 4.18392592e-05
Iter: 897 loss: 4.12224399e-05
Iter: 898 loss: 4.11811961e-05
Iter: 899 loss: 4.11905639e-05
Iter: 900 loss: 4.11511501e-05
Iter: 901 loss: 4.11103974e-05
Iter: 902 loss: 4.13215821e-05
Iter: 903 loss: 4.11036199e-05
Iter: 904 loss: 4.10587127e-05
Iter: 905 loss: 4.11735127e-05
Iter: 906 loss: 4.10439025e-05
Iter: 907 loss: 4.10073044e-05
Iter: 908 loss: 4.11553738e-05
Iter: 909 loss: 4.09981731e-05
Iter: 910 loss: 4.09563872e-05
Iter: 911 loss: 4.11225265e-05
Iter: 912 loss: 4.09497698e-05
Iter: 913 loss: 4.09171334e-05
Iter: 914 loss: 4.09749591e-05
Iter: 915 loss: 4.09059357e-05
Iter: 916 loss: 4.08704727e-05
Iter: 917 loss: 4.09509412e-05
Iter: 918 loss: 4.08572523e-05
Iter: 919 loss: 4.08195629e-05
Iter: 920 loss: 4.1038682e-05
Iter: 921 loss: 4.08135e-05
Iter: 922 loss: 4.07853731e-05
Iter: 923 loss: 4.07678162e-05
Iter: 924 loss: 4.07568295e-05
Iter: 925 loss: 4.07200205e-05
Iter: 926 loss: 4.10923349e-05
Iter: 927 loss: 4.07210173e-05
Iter: 928 loss: 4.06814652e-05
Iter: 929 loss: 4.0848834e-05
Iter: 930 loss: 4.06756371e-05
Iter: 931 loss: 4.0655781e-05
Iter: 932 loss: 4.06219915e-05
Iter: 933 loss: 4.06222207e-05
Iter: 934 loss: 4.05843384e-05
Iter: 935 loss: 4.06026e-05
Iter: 936 loss: 4.05572e-05
Iter: 937 loss: 4.05160827e-05
Iter: 938 loss: 4.09305467e-05
Iter: 939 loss: 4.05137325e-05
Iter: 940 loss: 4.04781167e-05
Iter: 941 loss: 4.04866951e-05
Iter: 942 loss: 4.04533e-05
Iter: 943 loss: 4.04190578e-05
Iter: 944 loss: 4.08376436e-05
Iter: 945 loss: 4.04198e-05
Iter: 946 loss: 4.03895028e-05
Iter: 947 loss: 4.04320526e-05
Iter: 948 loss: 4.03782615e-05
Iter: 949 loss: 4.03469749e-05
Iter: 950 loss: 4.0401359e-05
Iter: 951 loss: 4.03339072e-05
Iter: 952 loss: 4.03053127e-05
Iter: 953 loss: 4.05120882e-05
Iter: 954 loss: 4.03052109e-05
Iter: 955 loss: 4.02777587e-05
Iter: 956 loss: 4.02865844e-05
Iter: 957 loss: 4.0257597e-05
Iter: 958 loss: 4.02296282e-05
Iter: 959 loss: 4.02955193e-05
Iter: 960 loss: 4.02170481e-05
Iter: 961 loss: 4.0197745e-05
Iter: 962 loss: 4.01963152e-05
Iter: 963 loss: 4.01754442e-05
Iter: 964 loss: 4.01345678e-05
Iter: 965 loss: 4.0958359e-05
Iter: 966 loss: 4.01359648e-05
Iter: 967 loss: 4.00923091e-05
Iter: 968 loss: 4.01743382e-05
Iter: 969 loss: 4.00773679e-05
Iter: 970 loss: 4.003922e-05
Iter: 971 loss: 4.01295692e-05
Iter: 972 loss: 4.00247773e-05
Iter: 973 loss: 3.99858218e-05
Iter: 974 loss: 4.00590288e-05
Iter: 975 loss: 3.99640194e-05
Iter: 976 loss: 3.9924802e-05
Iter: 977 loss: 4.00168356e-05
Iter: 978 loss: 3.99100463e-05
Iter: 979 loss: 3.98723787e-05
Iter: 980 loss: 4.03608938e-05
Iter: 981 loss: 3.9872597e-05
Iter: 982 loss: 3.98491684e-05
Iter: 983 loss: 3.99002274e-05
Iter: 984 loss: 3.98387747e-05
Iter: 985 loss: 3.98122e-05
Iter: 986 loss: 3.98225347e-05
Iter: 987 loss: 3.97961194e-05
Iter: 988 loss: 3.97632975e-05
Iter: 989 loss: 4.00677163e-05
Iter: 990 loss: 3.9763414e-05
Iter: 991 loss: 3.9740451e-05
Iter: 992 loss: 3.97430922e-05
Iter: 993 loss: 3.9722996e-05
Iter: 994 loss: 3.96939868e-05
Iter: 995 loss: 3.98413104e-05
Iter: 996 loss: 3.96897303e-05
Iter: 997 loss: 3.966043e-05
Iter: 998 loss: 3.99469463e-05
Iter: 999 loss: 3.9658742e-05
Iter: 1000 loss: 3.96432079e-05
Iter: 1001 loss: 3.96099058e-05
Iter: 1002 loss: 3.9918119e-05
Iter: 1003 loss: 3.9600407e-05
Iter: 1004 loss: 3.95561729e-05
Iter: 1005 loss: 3.97259901e-05
Iter: 1006 loss: 3.95448296e-05
Iter: 1007 loss: 3.95064963e-05
Iter: 1008 loss: 3.9604718e-05
Iter: 1009 loss: 3.94934541e-05
Iter: 1010 loss: 3.94563831e-05
Iter: 1011 loss: 3.95365678e-05
Iter: 1012 loss: 3.94418821e-05
Iter: 1013 loss: 3.94061863e-05
Iter: 1014 loss: 3.96379328e-05
Iter: 1015 loss: 3.94022863e-05
Iter: 1016 loss: 3.9368977e-05
Iter: 1017 loss: 3.95090392e-05
Iter: 1018 loss: 3.93622213e-05
Iter: 1019 loss: 3.93348309e-05
Iter: 1020 loss: 3.93929877e-05
Iter: 1021 loss: 3.93258961e-05
Iter: 1022 loss: 3.9298815e-05
Iter: 1023 loss: 3.93753326e-05
Iter: 1024 loss: 3.92898583e-05
Iter: 1025 loss: 3.92621914e-05
Iter: 1026 loss: 3.93533628e-05
Iter: 1027 loss: 3.92509974e-05
Iter: 1028 loss: 3.92237052e-05
Iter: 1029 loss: 3.93003465e-05
Iter: 1030 loss: 3.92155707e-05
Iter: 1031 loss: 3.91936519e-05
Iter: 1032 loss: 3.95027782e-05
Iter: 1033 loss: 3.91953872e-05
Iter: 1034 loss: 3.91743124e-05
Iter: 1035 loss: 3.9136612e-05
Iter: 1036 loss: 4.00057143e-05
Iter: 1037 loss: 3.9136241e-05
Iter: 1038 loss: 3.91082e-05
Iter: 1039 loss: 3.91394933e-05
Iter: 1040 loss: 3.90914356e-05
Iter: 1041 loss: 3.90531859e-05
Iter: 1042 loss: 3.91085341e-05
Iter: 1043 loss: 3.90331697e-05
Iter: 1044 loss: 3.89917259e-05
Iter: 1045 loss: 3.91887188e-05
Iter: 1046 loss: 3.89832239e-05
Iter: 1047 loss: 3.89467968e-05
Iter: 1048 loss: 3.90762361e-05
Iter: 1049 loss: 3.89400448e-05
Iter: 1050 loss: 3.89122142e-05
Iter: 1051 loss: 3.89687884e-05
Iter: 1052 loss: 3.89015513e-05
Iter: 1053 loss: 3.88688131e-05
Iter: 1054 loss: 3.90843488e-05
Iter: 1055 loss: 3.88662738e-05
Iter: 1056 loss: 3.88387634e-05
Iter: 1057 loss: 3.88779117e-05
Iter: 1058 loss: 3.88245608e-05
Iter: 1059 loss: 3.87945038e-05
Iter: 1060 loss: 3.8837039e-05
Iter: 1061 loss: 3.87800646e-05
Iter: 1062 loss: 3.8753351e-05
Iter: 1063 loss: 3.90385467e-05
Iter: 1064 loss: 3.87494511e-05
Iter: 1065 loss: 3.87286927e-05
Iter: 1066 loss: 3.87965265e-05
Iter: 1067 loss: 3.87208347e-05
Iter: 1068 loss: 3.86986067e-05
Iter: 1069 loss: 3.88438239e-05
Iter: 1070 loss: 3.86932297e-05
Iter: 1071 loss: 3.86816828e-05
Iter: 1072 loss: 3.86500178e-05
Iter: 1073 loss: 3.90446403e-05
Iter: 1074 loss: 3.86461288e-05
Iter: 1075 loss: 3.86141874e-05
Iter: 1076 loss: 3.88191329e-05
Iter: 1077 loss: 3.86096217e-05
Iter: 1078 loss: 3.8581682e-05
Iter: 1079 loss: 3.85863859e-05
Iter: 1080 loss: 3.85602762e-05
Iter: 1081 loss: 3.85209423e-05
Iter: 1082 loss: 3.87141845e-05
Iter: 1083 loss: 3.8516213e-05
Iter: 1084 loss: 3.84827799e-05
Iter: 1085 loss: 3.8518072e-05
Iter: 1086 loss: 3.8465274e-05
Iter: 1087 loss: 3.84318919e-05
Iter: 1088 loss: 3.86040556e-05
Iter: 1089 loss: 3.84281957e-05
Iter: 1090 loss: 3.83990955e-05
Iter: 1091 loss: 3.86339525e-05
Iter: 1092 loss: 3.83962688e-05
Iter: 1093 loss: 3.83727456e-05
Iter: 1094 loss: 3.83908264e-05
Iter: 1095 loss: 3.83610095e-05
Iter: 1096 loss: 3.83357474e-05
Iter: 1097 loss: 3.84052546e-05
Iter: 1098 loss: 3.83271581e-05
Iter: 1099 loss: 3.82980288e-05
Iter: 1100 loss: 3.84592167e-05
Iter: 1101 loss: 3.82946819e-05
Iter: 1102 loss: 3.82754224e-05
Iter: 1103 loss: 3.84343293e-05
Iter: 1104 loss: 3.82737126e-05
Iter: 1105 loss: 3.82532598e-05
Iter: 1106 loss: 3.82379367e-05
Iter: 1107 loss: 3.82324652e-05
Iter: 1108 loss: 3.82058679e-05
Iter: 1109 loss: 3.82419712e-05
Iter: 1110 loss: 3.81936115e-05
Iter: 1111 loss: 3.81678037e-05
Iter: 1112 loss: 3.81981299e-05
Iter: 1113 loss: 3.81543141e-05
Iter: 1114 loss: 3.8127273e-05
Iter: 1115 loss: 3.81595237e-05
Iter: 1116 loss: 3.81114e-05
Iter: 1117 loss: 3.80735946e-05
Iter: 1118 loss: 3.82398503e-05
Iter: 1119 loss: 3.80684e-05
Iter: 1120 loss: 3.80388665e-05
Iter: 1121 loss: 3.80794881e-05
Iter: 1122 loss: 3.8023969e-05
Iter: 1123 loss: 3.79899648e-05
Iter: 1124 loss: 3.81964746e-05
Iter: 1125 loss: 3.79855192e-05
Iter: 1126 loss: 3.79645462e-05
Iter: 1127 loss: 3.81082937e-05
Iter: 1128 loss: 3.79598569e-05
Iter: 1129 loss: 3.79393023e-05
Iter: 1130 loss: 3.7967875e-05
Iter: 1131 loss: 3.79306512e-05
Iter: 1132 loss: 3.79026824e-05
Iter: 1133 loss: 3.79646735e-05
Iter: 1134 loss: 3.78946861e-05
Iter: 1135 loss: 3.78827754e-05
Iter: 1136 loss: 3.78829209e-05
Iter: 1137 loss: 3.78684854e-05
Iter: 1138 loss: 3.7857e-05
Iter: 1139 loss: 3.78542863e-05
Iter: 1140 loss: 3.78346231e-05
Iter: 1141 loss: 3.78969708e-05
Iter: 1142 loss: 3.78288387e-05
Iter: 1143 loss: 3.78097648e-05
Iter: 1144 loss: 3.77961915e-05
Iter: 1145 loss: 3.77903634e-05
Iter: 1146 loss: 3.77644428e-05
Iter: 1147 loss: 3.78563309e-05
Iter: 1148 loss: 3.77587567e-05
Iter: 1149 loss: 3.77352808e-05
Iter: 1150 loss: 3.78101286e-05
Iter: 1151 loss: 3.77266406e-05
Iter: 1152 loss: 3.77028882e-05
Iter: 1153 loss: 3.77132237e-05
Iter: 1154 loss: 3.76838e-05
Iter: 1155 loss: 3.76520366e-05
Iter: 1156 loss: 3.78125369e-05
Iter: 1157 loss: 3.76478019e-05
Iter: 1158 loss: 3.76192911e-05
Iter: 1159 loss: 3.76577736e-05
Iter: 1160 loss: 3.76084026e-05
Iter: 1161 loss: 3.75786258e-05
Iter: 1162 loss: 3.78155964e-05
Iter: 1163 loss: 3.75761374e-05
Iter: 1164 loss: 3.75550953e-05
Iter: 1165 loss: 3.76436146e-05
Iter: 1166 loss: 3.7548838e-05
Iter: 1167 loss: 3.752661e-05
Iter: 1168 loss: 3.75724267e-05
Iter: 1169 loss: 3.75153904e-05
Iter: 1170 loss: 3.74945e-05
Iter: 1171 loss: 3.77809847e-05
Iter: 1172 loss: 3.74921874e-05
Iter: 1173 loss: 3.74805386e-05
Iter: 1174 loss: 3.74747506e-05
Iter: 1175 loss: 3.74698102e-05
Iter: 1176 loss: 3.74511947e-05
Iter: 1177 loss: 3.74771371e-05
Iter: 1178 loss: 3.74433621e-05
Iter: 1179 loss: 3.7421094e-05
Iter: 1180 loss: 3.74447627e-05
Iter: 1181 loss: 3.74103256e-05
Iter: 1182 loss: 3.73871299e-05
Iter: 1183 loss: 3.74079136e-05
Iter: 1184 loss: 3.7374215e-05
Iter: 1185 loss: 3.73499606e-05
Iter: 1186 loss: 3.74106748e-05
Iter: 1187 loss: 3.73440598e-05
Iter: 1188 loss: 3.73198636e-05
Iter: 1189 loss: 3.73526345e-05
Iter: 1190 loss: 3.73053044e-05
Iter: 1191 loss: 3.72736176e-05
Iter: 1192 loss: 3.73561852e-05
Iter: 1193 loss: 3.72609e-05
Iter: 1194 loss: 3.723189e-05
Iter: 1195 loss: 3.74482042e-05
Iter: 1196 loss: 3.72326795e-05
Iter: 1197 loss: 3.72113063e-05
Iter: 1198 loss: 3.72507e-05
Iter: 1199 loss: 3.72021095e-05
Iter: 1200 loss: 3.71798451e-05
Iter: 1201 loss: 3.73936273e-05
Iter: 1202 loss: 3.71772185e-05
Iter: 1203 loss: 3.71650967e-05
Iter: 1204 loss: 3.72573231e-05
Iter: 1205 loss: 3.71635469e-05
Iter: 1206 loss: 3.71458118e-05
Iter: 1207 loss: 3.71427195e-05
Iter: 1208 loss: 3.71333954e-05
Iter: 1209 loss: 3.71155911e-05
Iter: 1210 loss: 3.71412571e-05
Iter: 1211 loss: 3.71077767e-05
Iter: 1212 loss: 3.70906091e-05
Iter: 1213 loss: 3.71509741e-05
Iter: 1214 loss: 3.70832204e-05
Iter: 1215 loss: 3.70645794e-05
Iter: 1216 loss: 3.70541238e-05
Iter: 1217 loss: 3.7044123e-05
Iter: 1218 loss: 3.70173148e-05
Iter: 1219 loss: 3.7116748e-05
Iter: 1220 loss: 3.70119305e-05
Iter: 1221 loss: 3.69874979e-05
Iter: 1222 loss: 3.7081656e-05
Iter: 1223 loss: 3.69833215e-05
Iter: 1224 loss: 3.6960555e-05
Iter: 1225 loss: 3.70006601e-05
Iter: 1226 loss: 3.69510453e-05
Iter: 1227 loss: 3.69273548e-05
Iter: 1228 loss: 3.69545232e-05
Iter: 1229 loss: 3.69129266e-05
Iter: 1230 loss: 3.68885048e-05
Iter: 1231 loss: 3.70109046e-05
Iter: 1232 loss: 3.68814726e-05
Iter: 1233 loss: 3.68662149e-05
Iter: 1234 loss: 3.71134738e-05
Iter: 1235 loss: 3.68674155e-05
Iter: 1236 loss: 3.68513e-05
Iter: 1237 loss: 3.68723777e-05
Iter: 1238 loss: 3.68445617e-05
Iter: 1239 loss: 3.68234869e-05
Iter: 1240 loss: 3.69184345e-05
Iter: 1241 loss: 3.68195579e-05
Iter: 1242 loss: 3.68058172e-05
Iter: 1243 loss: 3.68006367e-05
Iter: 1244 loss: 3.67921602e-05
Iter: 1245 loss: 3.67724133e-05
Iter: 1246 loss: 3.68133624e-05
Iter: 1247 loss: 3.67681641e-05
Iter: 1248 loss: 3.67425964e-05
Iter: 1249 loss: 3.68134934e-05
Iter: 1250 loss: 3.67398388e-05
Iter: 1251 loss: 3.67188477e-05
Iter: 1252 loss: 3.67127432e-05
Iter: 1253 loss: 3.6702193e-05
Iter: 1254 loss: 3.66758068e-05
Iter: 1255 loss: 3.68272486e-05
Iter: 1256 loss: 3.66749446e-05
Iter: 1257 loss: 3.66510103e-05
Iter: 1258 loss: 3.66724416e-05
Iter: 1259 loss: 3.66388922e-05
Iter: 1260 loss: 3.66098357e-05
Iter: 1261 loss: 3.66775e-05
Iter: 1262 loss: 3.66011227e-05
Iter: 1263 loss: 3.65790111e-05
Iter: 1264 loss: 3.66519271e-05
Iter: 1265 loss: 3.6566682e-05
Iter: 1266 loss: 3.65453452e-05
Iter: 1267 loss: 3.66843924e-05
Iter: 1268 loss: 3.65441e-05
Iter: 1269 loss: 3.65238739e-05
Iter: 1270 loss: 3.67821267e-05
Iter: 1271 loss: 3.65224332e-05
Iter: 1272 loss: 3.65141714e-05
Iter: 1273 loss: 3.65517226e-05
Iter: 1274 loss: 3.6508296e-05
Iter: 1275 loss: 3.64960288e-05
Iter: 1276 loss: 3.64803709e-05
Iter: 1277 loss: 3.64769148e-05
Iter: 1278 loss: 3.64554726e-05
Iter: 1279 loss: 3.65374544e-05
Iter: 1280 loss: 3.64512e-05
Iter: 1281 loss: 3.64350417e-05
Iter: 1282 loss: 3.65170745e-05
Iter: 1283 loss: 3.64303269e-05
Iter: 1284 loss: 3.64142e-05
Iter: 1285 loss: 3.64116204e-05
Iter: 1286 loss: 3.64009829e-05
Iter: 1287 loss: 3.63796862e-05
Iter: 1288 loss: 3.64229527e-05
Iter: 1289 loss: 3.63708e-05
Iter: 1290 loss: 3.63479194e-05
Iter: 1291 loss: 3.64441148e-05
Iter: 1292 loss: 3.63447944e-05
Iter: 1293 loss: 3.63257313e-05
Iter: 1294 loss: 3.6338788e-05
Iter: 1295 loss: 3.6314661e-05
Iter: 1296 loss: 3.6291276e-05
Iter: 1297 loss: 3.63717554e-05
Iter: 1298 loss: 3.62854917e-05
Iter: 1299 loss: 3.62613137e-05
Iter: 1300 loss: 3.63182371e-05
Iter: 1301 loss: 3.6252779e-05
Iter: 1302 loss: 3.62342107e-05
Iter: 1303 loss: 3.64842635e-05
Iter: 1304 loss: 3.62314313e-05
Iter: 1305 loss: 3.62161445e-05
Iter: 1306 loss: 3.63583895e-05
Iter: 1307 loss: 3.62155406e-05
Iter: 1308 loss: 3.62066639e-05
Iter: 1309 loss: 3.62031788e-05
Iter: 1310 loss: 3.61982347e-05
Iter: 1311 loss: 3.61815728e-05
Iter: 1312 loss: 3.61924613e-05
Iter: 1313 loss: 3.61700695e-05
Iter: 1314 loss: 3.61502243e-05
Iter: 1315 loss: 3.61629e-05
Iter: 1316 loss: 3.61386046e-05
Iter: 1317 loss: 3.61178681e-05
Iter: 1318 loss: 3.63095969e-05
Iter: 1319 loss: 3.61160492e-05
Iter: 1320 loss: 3.61020575e-05
Iter: 1321 loss: 3.61036946e-05
Iter: 1322 loss: 3.6089994e-05
Iter: 1323 loss: 3.60722406e-05
Iter: 1324 loss: 3.61206112e-05
Iter: 1325 loss: 3.60639679e-05
Iter: 1326 loss: 3.60474914e-05
Iter: 1327 loss: 3.60503691e-05
Iter: 1328 loss: 3.60324702e-05
Iter: 1329 loss: 3.60096346e-05
Iter: 1330 loss: 3.61194616e-05
Iter: 1331 loss: 3.60062e-05
Iter: 1332 loss: 3.59802616e-05
Iter: 1333 loss: 3.60178747e-05
Iter: 1334 loss: 3.59691221e-05
Iter: 1335 loss: 3.59465303e-05
Iter: 1336 loss: 3.61407e-05
Iter: 1337 loss: 3.59455735e-05
Iter: 1338 loss: 3.59322294e-05
Iter: 1339 loss: 3.59331243e-05
Iter: 1340 loss: 3.59211917e-05
Iter: 1341 loss: 3.59115256e-05
Iter: 1342 loss: 3.59081459e-05
Iter: 1343 loss: 3.58929683e-05
Iter: 1344 loss: 3.59860132e-05
Iter: 1345 loss: 3.58937104e-05
Iter: 1346 loss: 3.58805519e-05
Iter: 1347 loss: 3.58697071e-05
Iter: 1348 loss: 3.5867015e-05
Iter: 1349 loss: 3.58461e-05
Iter: 1350 loss: 3.59249098e-05
Iter: 1351 loss: 3.58429825e-05
Iter: 1352 loss: 3.5820929e-05
Iter: 1353 loss: 3.58972648e-05
Iter: 1354 loss: 3.58187535e-05
Iter: 1355 loss: 3.57978242e-05
Iter: 1356 loss: 3.58114485e-05
Iter: 1357 loss: 3.57899771e-05
Iter: 1358 loss: 3.57680037e-05
Iter: 1359 loss: 3.58346078e-05
Iter: 1360 loss: 3.57601893e-05
Iter: 1361 loss: 3.57446261e-05
Iter: 1362 loss: 3.57883255e-05
Iter: 1363 loss: 3.57393765e-05
Iter: 1364 loss: 3.57181416e-05
Iter: 1365 loss: 3.57417957e-05
Iter: 1366 loss: 3.57079043e-05
Iter: 1367 loss: 3.56868768e-05
Iter: 1368 loss: 3.57547397e-05
Iter: 1369 loss: 3.56801647e-05
Iter: 1370 loss: 3.56656346e-05
Iter: 1371 loss: 3.58589677e-05
Iter: 1372 loss: 3.56656747e-05
Iter: 1373 loss: 3.56495257e-05
Iter: 1374 loss: 3.57286808e-05
Iter: 1375 loss: 3.56475139e-05
Iter: 1376 loss: 3.5638448e-05
Iter: 1377 loss: 3.56334858e-05
Iter: 1378 loss: 3.56308446e-05
Iter: 1379 loss: 3.56129385e-05
Iter: 1380 loss: 3.56408127e-05
Iter: 1381 loss: 3.56055389e-05
Iter: 1382 loss: 3.55921184e-05
Iter: 1383 loss: 3.5618119e-05
Iter: 1384 loss: 3.55863594e-05
Iter: 1385 loss: 3.55711054e-05
Iter: 1386 loss: 3.561638e-05
Iter: 1387 loss: 3.55670745e-05
Iter: 1388 loss: 3.55515731e-05
Iter: 1389 loss: 3.55996308e-05
Iter: 1390 loss: 3.55460179e-05
Iter: 1391 loss: 3.55317898e-05
Iter: 1392 loss: 3.55348748e-05
Iter: 1393 loss: 3.55197699e-05
Iter: 1394 loss: 3.55013617e-05
Iter: 1395 loss: 3.55434568e-05
Iter: 1396 loss: 3.54889708e-05
Iter: 1397 loss: 3.54721196e-05
Iter: 1398 loss: 3.55456577e-05
Iter: 1399 loss: 3.54670919e-05
Iter: 1400 loss: 3.54470976e-05
Iter: 1401 loss: 3.54536387e-05
Iter: 1402 loss: 3.5436e-05
Iter: 1403 loss: 3.54159e-05
Iter: 1404 loss: 3.5652316e-05
Iter: 1405 loss: 3.54148178e-05
Iter: 1406 loss: 3.54043805e-05
Iter: 1407 loss: 3.54045951e-05
Iter: 1408 loss: 3.5395431e-05
Iter: 1409 loss: 3.53806972e-05
Iter: 1410 loss: 3.56225719e-05
Iter: 1411 loss: 3.53806245e-05
Iter: 1412 loss: 3.53617615e-05
Iter: 1413 loss: 3.5531426e-05
Iter: 1414 loss: 3.53623909e-05
Iter: 1415 loss: 3.53490905e-05
Iter: 1416 loss: 3.53410469e-05
Iter: 1417 loss: 3.53361247e-05
Iter: 1418 loss: 3.53185751e-05
Iter: 1419 loss: 3.54028598e-05
Iter: 1420 loss: 3.5314566e-05
Iter: 1421 loss: 3.5298679e-05
Iter: 1422 loss: 3.53539726e-05
Iter: 1423 loss: 3.52962452e-05
Iter: 1424 loss: 3.52772549e-05
Iter: 1425 loss: 3.52971474e-05
Iter: 1426 loss: 3.52700881e-05
Iter: 1427 loss: 3.52514544e-05
Iter: 1428 loss: 3.52909519e-05
Iter: 1429 loss: 3.5244866e-05
Iter: 1430 loss: 3.52260759e-05
Iter: 1431 loss: 3.52723073e-05
Iter: 1432 loss: 3.52197312e-05
Iter: 1433 loss: 3.52024836e-05
Iter: 1434 loss: 3.52257339e-05
Iter: 1435 loss: 3.51936615e-05
Iter: 1436 loss: 3.51738963e-05
Iter: 1437 loss: 3.52546303e-05
Iter: 1438 loss: 3.51697745e-05
Iter: 1439 loss: 3.51553608e-05
Iter: 1440 loss: 3.53023497e-05
Iter: 1441 loss: 3.51506969e-05
Iter: 1442 loss: 3.5138517e-05
Iter: 1443 loss: 3.52526476e-05
Iter: 1444 loss: 3.51356866e-05
Iter: 1445 loss: 3.51286981e-05
Iter: 1446 loss: 3.51169947e-05
Iter: 1447 loss: 3.53771939e-05
Iter: 1448 loss: 3.51156486e-05
Iter: 1449 loss: 3.51010385e-05
Iter: 1450 loss: 3.5214638e-05
Iter: 1451 loss: 3.51004019e-05
Iter: 1452 loss: 3.50883383e-05
Iter: 1453 loss: 3.5092191e-05
Iter: 1454 loss: 3.50768751e-05
Iter: 1455 loss: 3.50611263e-05
Iter: 1456 loss: 3.50577793e-05
Iter: 1457 loss: 3.50448136e-05
Iter: 1458 loss: 3.50293703e-05
Iter: 1459 loss: 3.50276605e-05
Iter: 1460 loss: 3.50154514e-05
Iter: 1461 loss: 3.50040355e-05
Iter: 1462 loss: 3.50012451e-05
Iter: 1463 loss: 3.49781039e-05
Iter: 1464 loss: 3.50489681e-05
Iter: 1465 loss: 3.49717156e-05
Iter: 1466 loss: 3.49521797e-05
Iter: 1467 loss: 3.49885631e-05
Iter: 1468 loss: 3.49439e-05
Iter: 1469 loss: 3.49215697e-05
Iter: 1470 loss: 3.49890943e-05
Iter: 1471 loss: 3.49149159e-05
Iter: 1472 loss: 3.48946414e-05
Iter: 1473 loss: 3.50227201e-05
Iter: 1474 loss: 3.48924441e-05
Iter: 1475 loss: 3.48851754e-05
Iter: 1476 loss: 3.48836547e-05
Iter: 1477 loss: 3.48747271e-05
Iter: 1478 loss: 3.48573667e-05
Iter: 1479 loss: 3.51355629e-05
Iter: 1480 loss: 3.48575122e-05
Iter: 1481 loss: 3.48393296e-05
Iter: 1482 loss: 3.49320289e-05
Iter: 1483 loss: 3.4838522e-05
Iter: 1484 loss: 3.4819328e-05
Iter: 1485 loss: 3.48590693e-05
Iter: 1486 loss: 3.48144531e-05
Iter: 1487 loss: 3.4801451e-05
Iter: 1488 loss: 3.48099e-05
Iter: 1489 loss: 3.4791803e-05
Iter: 1490 loss: 3.47766399e-05
Iter: 1491 loss: 3.4849636e-05
Iter: 1492 loss: 3.47728419e-05
Iter: 1493 loss: 3.47554e-05
Iter: 1494 loss: 3.48009416e-05
Iter: 1495 loss: 3.47492642e-05
Iter: 1496 loss: 3.47333626e-05
Iter: 1497 loss: 3.4765606e-05
Iter: 1498 loss: 3.47291571e-05
Iter: 1499 loss: 3.47087698e-05
Iter: 1500 loss: 3.47101086e-05
Iter: 1501 loss: 3.46924644e-05
Iter: 1502 loss: 3.46739471e-05
Iter: 1503 loss: 3.47603491e-05
Iter: 1504 loss: 3.46672023e-05
Iter: 1505 loss: 3.46485613e-05
Iter: 1506 loss: 3.46814268e-05
Iter: 1507 loss: 3.46363195e-05
Iter: 1508 loss: 3.46251254e-05
Iter: 1509 loss: 3.46238448e-05
Iter: 1510 loss: 3.46125053e-05
Iter: 1511 loss: 3.46333254e-05
Iter: 1512 loss: 3.46065062e-05
Iter: 1513 loss: 3.45949484e-05
Iter: 1514 loss: 3.45826484e-05
Iter: 1515 loss: 3.45810295e-05
Iter: 1516 loss: 3.45637964e-05
Iter: 1517 loss: 3.46973757e-05
Iter: 1518 loss: 3.45634253e-05
Iter: 1519 loss: 3.45479748e-05
Iter: 1520 loss: 3.45715889e-05
Iter: 1521 loss: 3.45422195e-05
Iter: 1522 loss: 3.45261506e-05
Iter: 1523 loss: 3.4549128e-05
Iter: 1524 loss: 3.45215885e-05
Iter: 1525 loss: 3.45058397e-05
Iter: 1526 loss: 3.45537228e-05
Iter: 1527 loss: 3.45018052e-05
Iter: 1528 loss: 3.44866894e-05
Iter: 1529 loss: 3.45282388e-05
Iter: 1530 loss: 3.44815635e-05
Iter: 1531 loss: 3.44636755e-05
Iter: 1532 loss: 3.44932378e-05
Iter: 1533 loss: 3.44560613e-05
Iter: 1534 loss: 3.44388209e-05
Iter: 1535 loss: 3.44431865e-05
Iter: 1536 loss: 3.44273976e-05
Iter: 1537 loss: 3.44083892e-05
Iter: 1538 loss: 3.45290064e-05
Iter: 1539 loss: 3.44056789e-05
Iter: 1540 loss: 3.43892971e-05
Iter: 1541 loss: 3.44090695e-05
Iter: 1542 loss: 3.43802822e-05
Iter: 1543 loss: 3.43734027e-05
Iter: 1544 loss: 3.43686334e-05
Iter: 1545 loss: 3.43645879e-05
Iter: 1546 loss: 3.43489555e-05
Iter: 1547 loss: 3.4482011e-05
Iter: 1548 loss: 3.43454376e-05
Iter: 1549 loss: 3.43283064e-05
Iter: 1550 loss: 3.44313812e-05
Iter: 1551 loss: 3.4324541e-05
Iter: 1552 loss: 3.43107313e-05
Iter: 1553 loss: 3.4464636e-05
Iter: 1554 loss: 3.43113716e-05
Iter: 1555 loss: 3.43021384e-05
Iter: 1556 loss: 3.42903404e-05
Iter: 1557 loss: 3.42892417e-05
Iter: 1558 loss: 3.42722778e-05
Iter: 1559 loss: 3.43402025e-05
Iter: 1560 loss: 3.42688363e-05
Iter: 1561 loss: 3.42517633e-05
Iter: 1562 loss: 3.43424e-05
Iter: 1563 loss: 3.42517596e-05
Iter: 1564 loss: 3.42360872e-05
Iter: 1565 loss: 3.42531275e-05
Iter: 1566 loss: 3.42278508e-05
Iter: 1567 loss: 3.42096537e-05
Iter: 1568 loss: 3.42162457e-05
Iter: 1569 loss: 3.41973682e-05
Iter: 1570 loss: 3.41785308e-05
Iter: 1571 loss: 3.42509775e-05
Iter: 1572 loss: 3.41722916e-05
Iter: 1573 loss: 3.41540908e-05
Iter: 1574 loss: 3.42066451e-05
Iter: 1575 loss: 3.41477971e-05
Iter: 1576 loss: 3.41420237e-05
Iter: 1577 loss: 3.41378691e-05
Iter: 1578 loss: 3.41326886e-05
Iter: 1579 loss: 3.41181149e-05
Iter: 1580 loss: 3.4118144e-05
Iter: 1581 loss: 3.41074665e-05
Iter: 1582 loss: 3.41176419e-05
Iter: 1583 loss: 3.41003179e-05
Iter: 1584 loss: 3.40851911e-05
Iter: 1585 loss: 3.41135819e-05
Iter: 1586 loss: 3.4077686e-05
Iter: 1587 loss: 3.40625156e-05
Iter: 1588 loss: 3.41971754e-05
Iter: 1589 loss: 3.40631595e-05
Iter: 1590 loss: 3.40517981e-05
Iter: 1591 loss: 3.405389e-05
Iter: 1592 loss: 3.40452716e-05
Iter: 1593 loss: 3.40276529e-05
Iter: 1594 loss: 3.40260231e-05
Iter: 1595 loss: 3.40142833e-05
Iter: 1596 loss: 3.39990365e-05
Iter: 1597 loss: 3.41994237e-05
Iter: 1598 loss: 3.39979779e-05
Iter: 1599 loss: 3.39863327e-05
Iter: 1600 loss: 3.40171537e-05
Iter: 1601 loss: 3.39826292e-05
Iter: 1602 loss: 3.39664402e-05
Iter: 1603 loss: 3.39792641e-05
Iter: 1604 loss: 3.39571779e-05
Iter: 1605 loss: 3.39407488e-05
Iter: 1606 loss: 3.3993012e-05
Iter: 1607 loss: 3.39376347e-05
Iter: 1608 loss: 3.39245089e-05
Iter: 1609 loss: 3.39366161e-05
Iter: 1610 loss: 3.39163125e-05
Iter: 1611 loss: 3.39105318e-05
Iter: 1612 loss: 3.39081853e-05
Iter: 1613 loss: 3.38982827e-05
Iter: 1614 loss: 3.38872705e-05
Iter: 1615 loss: 3.38882382e-05
Iter: 1616 loss: 3.38759419e-05
Iter: 1617 loss: 3.38641257e-05
Iter: 1618 loss: 3.38596583e-05
Iter: 1619 loss: 3.38441932e-05
Iter: 1620 loss: 3.40417828e-05
Iter: 1621 loss: 3.38463869e-05
Iter: 1622 loss: 3.38307473e-05
Iter: 1623 loss: 3.38835634e-05
Iter: 1624 loss: 3.38289574e-05
Iter: 1625 loss: 3.38163482e-05
Iter: 1626 loss: 3.38165228e-05
Iter: 1627 loss: 3.38047757e-05
Iter: 1628 loss: 3.3787579e-05
Iter: 1629 loss: 3.3804441e-05
Iter: 1630 loss: 3.37788e-05
Iter: 1631 loss: 3.37622041e-05
Iter: 1632 loss: 3.38956525e-05
Iter: 1633 loss: 3.37604761e-05
Iter: 1634 loss: 3.37477759e-05
Iter: 1635 loss: 3.37874371e-05
Iter: 1636 loss: 3.37429883e-05
Iter: 1637 loss: 3.37293131e-05
Iter: 1638 loss: 3.37667479e-05
Iter: 1639 loss: 3.3724551e-05
Iter: 1640 loss: 3.37125384e-05
Iter: 1641 loss: 3.3731485e-05
Iter: 1642 loss: 3.37087658e-05
Iter: 1643 loss: 3.36949815e-05
Iter: 1644 loss: 3.37486781e-05
Iter: 1645 loss: 3.3694243e-05
Iter: 1646 loss: 3.36814774e-05
Iter: 1647 loss: 3.38376121e-05
Iter: 1648 loss: 3.36819467e-05
Iter: 1649 loss: 3.36759513e-05
Iter: 1650 loss: 3.36599223e-05
Iter: 1651 loss: 3.38320897e-05
Iter: 1652 loss: 3.3655102e-05
Iter: 1653 loss: 3.36434896e-05
Iter: 1654 loss: 3.37068777e-05
Iter: 1655 loss: 3.36388694e-05
Iter: 1656 loss: 3.36240482e-05
Iter: 1657 loss: 3.36577068e-05
Iter: 1658 loss: 3.3617489e-05
Iter: 1659 loss: 3.3604807e-05
Iter: 1660 loss: 3.37476231e-05
Iter: 1661 loss: 3.36029552e-05
Iter: 1662 loss: 3.3595883e-05
Iter: 1663 loss: 3.35868463e-05
Iter: 1664 loss: 3.35850636e-05
Iter: 1665 loss: 3.35699442e-05
Iter: 1666 loss: 3.35863151e-05
Iter: 1667 loss: 3.35643963e-05
Iter: 1668 loss: 3.35442455e-05
Iter: 1669 loss: 3.35796212e-05
Iter: 1670 loss: 3.35343575e-05
Iter: 1671 loss: 3.35225777e-05
Iter: 1672 loss: 3.35216573e-05
Iter: 1673 loss: 3.3511722e-05
Iter: 1674 loss: 3.35043405e-05
Iter: 1675 loss: 3.3501834e-05
Iter: 1676 loss: 3.34889373e-05
Iter: 1677 loss: 3.35645382e-05
Iter: 1678 loss: 3.34882425e-05
Iter: 1679 loss: 3.34777505e-05
Iter: 1680 loss: 3.34766446e-05
Iter: 1681 loss: 3.34704382e-05
Iter: 1682 loss: 3.34547658e-05
Iter: 1683 loss: 3.37870188e-05
Iter: 1684 loss: 3.34549404e-05
Iter: 1685 loss: 3.34400975e-05
Iter: 1686 loss: 3.34775832e-05
Iter: 1687 loss: 3.34371398e-05
Iter: 1688 loss: 3.34244032e-05
Iter: 1689 loss: 3.34493161e-05
Iter: 1690 loss: 3.3416778e-05
Iter: 1691 loss: 3.34027718e-05
Iter: 1692 loss: 3.34662254e-05
Iter: 1693 loss: 3.34001234e-05
Iter: 1694 loss: 3.33858698e-05
Iter: 1695 loss: 3.34517608e-05
Iter: 1696 loss: 3.33823846e-05
Iter: 1697 loss: 3.33736898e-05
Iter: 1698 loss: 3.33647913e-05
Iter: 1699 loss: 3.33601674e-05
Iter: 1700 loss: 3.33462449e-05
Iter: 1701 loss: 3.34063807e-05
Iter: 1702 loss: 3.33423231e-05
Iter: 1703 loss: 3.33269272e-05
Iter: 1704 loss: 3.33774587e-05
Iter: 1705 loss: 3.3325392e-05
Iter: 1706 loss: 3.33082098e-05
Iter: 1707 loss: 3.33871576e-05
Iter: 1708 loss: 3.33059288e-05
Iter: 1709 loss: 3.32964119e-05
Iter: 1710 loss: 3.33333883e-05
Iter: 1711 loss: 3.32926938e-05
Iter: 1712 loss: 3.32811069e-05
Iter: 1713 loss: 3.33010576e-05
Iter: 1714 loss: 3.32792333e-05
Iter: 1715 loss: 3.32635609e-05
Iter: 1716 loss: 3.33816643e-05
Iter: 1717 loss: 3.32630079e-05
Iter: 1718 loss: 3.32559e-05
Iter: 1719 loss: 3.32426e-05
Iter: 1720 loss: 3.3434917e-05
Iter: 1721 loss: 3.3239914e-05
Iter: 1722 loss: 3.32234267e-05
Iter: 1723 loss: 3.32915733e-05
Iter: 1724 loss: 3.32210075e-05
Iter: 1725 loss: 3.3207838e-05
Iter: 1726 loss: 3.32497839e-05
Iter: 1727 loss: 3.3202854e-05
Iter: 1728 loss: 3.31885713e-05
Iter: 1729 loss: 3.32552e-05
Iter: 1730 loss: 3.31855117e-05
Iter: 1731 loss: 3.31734263e-05
Iter: 1732 loss: 3.32088021e-05
Iter: 1733 loss: 3.31705305e-05
Iter: 1734 loss: 3.31590563e-05
Iter: 1735 loss: 3.31676711e-05
Iter: 1736 loss: 3.31532428e-05
Iter: 1737 loss: 3.31389456e-05
Iter: 1738 loss: 3.31572155e-05
Iter: 1739 loss: 3.313251e-05
Iter: 1740 loss: 3.31183073e-05
Iter: 1741 loss: 3.31839765e-05
Iter: 1742 loss: 3.31140036e-05
Iter: 1743 loss: 3.31018091e-05
Iter: 1744 loss: 3.32014642e-05
Iter: 1745 loss: 3.31001429e-05
Iter: 1746 loss: 3.30892108e-05
Iter: 1747 loss: 3.30977164e-05
Iter: 1748 loss: 3.30819275e-05
Iter: 1749 loss: 3.30769908e-05
Iter: 1750 loss: 3.30753137e-05
Iter: 1751 loss: 3.30720432e-05
Iter: 1752 loss: 3.30571675e-05
Iter: 1753 loss: 3.31160772e-05
Iter: 1754 loss: 3.30547482e-05
Iter: 1755 loss: 3.30414041e-05
Iter: 1756 loss: 3.30959592e-05
Iter: 1757 loss: 3.30390285e-05
Iter: 1758 loss: 3.30247349e-05
Iter: 1759 loss: 3.30466501e-05
Iter: 1760 loss: 3.3016091e-05
Iter: 1761 loss: 3.30045659e-05
Iter: 1762 loss: 3.3163582e-05
Iter: 1763 loss: 3.30049661e-05
Iter: 1764 loss: 3.29941031e-05
Iter: 1765 loss: 3.30021285e-05
Iter: 1766 loss: 3.29901231e-05
Iter: 1767 loss: 3.29745053e-05
Iter: 1768 loss: 3.29902869e-05
Iter: 1769 loss: 3.29656395e-05
Iter: 1770 loss: 3.29589129e-05
Iter: 1771 loss: 3.2981894e-05
Iter: 1772 loss: 3.29521099e-05
Iter: 1773 loss: 3.29382419e-05
Iter: 1774 loss: 3.29596805e-05
Iter: 1775 loss: 3.2930926e-05
Iter: 1776 loss: 3.29184404e-05
Iter: 1777 loss: 3.30501207e-05
Iter: 1778 loss: 3.29186369e-05
Iter: 1779 loss: 3.2908e-05
Iter: 1780 loss: 3.29262766e-05
Iter: 1781 loss: 3.29017457e-05
Iter: 1782 loss: 3.28927053e-05
Iter: 1783 loss: 3.29865288e-05
Iter: 1784 loss: 3.28924834e-05
Iter: 1785 loss: 3.28833e-05
Iter: 1786 loss: 3.28871938e-05
Iter: 1787 loss: 3.28748902e-05
Iter: 1788 loss: 3.28657698e-05
Iter: 1789 loss: 3.28612587e-05
Iter: 1790 loss: 3.28577735e-05
Iter: 1791 loss: 3.28445021e-05
Iter: 1792 loss: 3.28548704e-05
Iter: 1793 loss: 3.28365277e-05
Iter: 1794 loss: 3.2823431e-05
Iter: 1795 loss: 3.28937822e-05
Iter: 1796 loss: 3.28214519e-05
Iter: 1797 loss: 3.28108435e-05
Iter: 1798 loss: 3.28987298e-05
Iter: 1799 loss: 3.28090755e-05
Iter: 1800 loss: 3.27985726e-05
Iter: 1801 loss: 3.28058813e-05
Iter: 1802 loss: 3.27910639e-05
Iter: 1803 loss: 3.27803209e-05
Iter: 1804 loss: 3.27741291e-05
Iter: 1805 loss: 3.27676025e-05
Iter: 1806 loss: 3.27510061e-05
Iter: 1807 loss: 3.28953138e-05
Iter: 1808 loss: 3.27507914e-05
Iter: 1809 loss: 3.27390226e-05
Iter: 1810 loss: 3.27629095e-05
Iter: 1811 loss: 3.27356756e-05
Iter: 1812 loss: 3.27217494e-05
Iter: 1813 loss: 3.27932867e-05
Iter: 1814 loss: 3.27195521e-05
Iter: 1815 loss: 3.27094167e-05
Iter: 1816 loss: 3.27473936e-05
Iter: 1817 loss: 3.27074449e-05
Iter: 1818 loss: 3.26992449e-05
Iter: 1819 loss: 3.28002461e-05
Iter: 1820 loss: 3.26982408e-05
Iter: 1821 loss: 3.26907175e-05
Iter: 1822 loss: 3.26757472e-05
Iter: 1823 loss: 3.26747686e-05
Iter: 1824 loss: 3.26652516e-05
Iter: 1825 loss: 3.26733862e-05
Iter: 1826 loss: 3.26562913e-05
Iter: 1827 loss: 3.26422e-05
Iter: 1828 loss: 3.26680383e-05
Iter: 1829 loss: 3.2635453e-05
Iter: 1830 loss: 3.26236477e-05
Iter: 1831 loss: 3.27702655e-05
Iter: 1832 loss: 3.26242662e-05
Iter: 1833 loss: 3.26118134e-05
Iter: 1834 loss: 3.26528752e-05
Iter: 1835 loss: 3.26119116e-05
Iter: 1836 loss: 3.2603366e-05
Iter: 1837 loss: 3.25902947e-05
Iter: 1838 loss: 3.25886e-05
Iter: 1839 loss: 3.25732399e-05
Iter: 1840 loss: 3.26304798e-05
Iter: 1841 loss: 3.25706242e-05
Iter: 1842 loss: 3.25539768e-05
Iter: 1843 loss: 3.26048757e-05
Iter: 1844 loss: 3.25501023e-05
Iter: 1845 loss: 3.25415749e-05
Iter: 1846 loss: 3.2625e-05
Iter: 1847 loss: 3.25402507e-05
Iter: 1848 loss: 3.25305627e-05
Iter: 1849 loss: 3.25644942e-05
Iter: 1850 loss: 3.25274e-05
Iter: 1851 loss: 3.25174187e-05
Iter: 1852 loss: 3.25979709e-05
Iter: 1853 loss: 3.25178917e-05
Iter: 1854 loss: 3.25103611e-05
Iter: 1855 loss: 3.2517717e-05
Iter: 1856 loss: 3.25031215e-05
Iter: 1857 loss: 3.24978864e-05
Iter: 1858 loss: 3.2483018e-05
Iter: 1859 loss: 3.27946691e-05
Iter: 1860 loss: 3.24839275e-05
Iter: 1861 loss: 3.24685971e-05
Iter: 1862 loss: 3.25381625e-05
Iter: 1863 loss: 3.24652538e-05
Iter: 1864 loss: 3.24526627e-05
Iter: 1865 loss: 3.24955763e-05
Iter: 1866 loss: 3.24460634e-05
Iter: 1867 loss: 3.24348403e-05
Iter: 1868 loss: 3.25460715e-05
Iter: 1869 loss: 3.24359862e-05
Iter: 1870 loss: 3.24255598e-05
Iter: 1871 loss: 3.24347857e-05
Iter: 1872 loss: 3.24176144e-05
Iter: 1873 loss: 3.24089706e-05
Iter: 1874 loss: 3.24142893e-05
Iter: 1875 loss: 3.24008761e-05
Iter: 1876 loss: 3.23883214e-05
Iter: 1877 loss: 3.24360735e-05
Iter: 1878 loss: 3.23846398e-05
Iter: 1879 loss: 3.23710374e-05
Iter: 1880 loss: 3.23872264e-05
Iter: 1881 loss: 3.23667409e-05
Iter: 1882 loss: 3.23545682e-05
Iter: 1883 loss: 3.25228066e-05
Iter: 1884 loss: 3.23543791e-05
Iter: 1885 loss: 3.23454224e-05
Iter: 1886 loss: 3.23765744e-05
Iter: 1887 loss: 3.23440654e-05
Iter: 1888 loss: 3.23348722e-05
Iter: 1889 loss: 3.23862041e-05
Iter: 1890 loss: 3.23336353e-05
Iter: 1891 loss: 3.23267777e-05
Iter: 1892 loss: 3.23193635e-05
Iter: 1893 loss: 3.23203894e-05
Iter: 1894 loss: 3.23102e-05
Iter: 1895 loss: 3.23137065e-05
Iter: 1896 loss: 3.23041022e-05
Iter: 1897 loss: 3.22916e-05
Iter: 1898 loss: 3.23426866e-05
Iter: 1899 loss: 3.22899505e-05
Iter: 1900 loss: 3.22800406e-05
Iter: 1901 loss: 3.22789856e-05
Iter: 1902 loss: 3.22700726e-05
Iter: 1903 loss: 3.22593623e-05
Iter: 1904 loss: 3.22591659e-05
Iter: 1905 loss: 3.22519372e-05
Iter: 1906 loss: 3.22684136e-05
Iter: 1907 loss: 3.22473425e-05
Iter: 1908 loss: 3.22390224e-05
Iter: 1909 loss: 3.22279484e-05
Iter: 1910 loss: 3.22267733e-05
Iter: 1911 loss: 3.22136839e-05
Iter: 1912 loss: 3.22732303e-05
Iter: 1913 loss: 3.2210075e-05
Iter: 1914 loss: 3.21982407e-05
Iter: 1915 loss: 3.22303167e-05
Iter: 1916 loss: 3.21960542e-05
Iter: 1917 loss: 3.21853113e-05
Iter: 1918 loss: 3.23123968e-05
Iter: 1919 loss: 3.21838779e-05
Iter: 1920 loss: 3.21773332e-05
Iter: 1921 loss: 3.22344968e-05
Iter: 1922 loss: 3.21774241e-05
Iter: 1923 loss: 3.21717162e-05
Iter: 1924 loss: 3.2169919e-05
Iter: 1925 loss: 3.21656407e-05
Iter: 1926 loss: 3.21558837e-05
Iter: 1927 loss: 3.21535917e-05
Iter: 1928 loss: 3.2148655e-05
Iter: 1929 loss: 3.21377374e-05
Iter: 1930 loss: 3.21565822e-05
Iter: 1931 loss: 3.21327825e-05
Iter: 1932 loss: 3.21204498e-05
Iter: 1933 loss: 3.21328407e-05
Iter: 1934 loss: 3.21134576e-05
Iter: 1935 loss: 3.21000553e-05
Iter: 1936 loss: 3.21550251e-05
Iter: 1937 loss: 3.21000189e-05
Iter: 1938 loss: 3.20873733e-05
Iter: 1939 loss: 3.21938205e-05
Iter: 1940 loss: 3.20859181e-05
Iter: 1941 loss: 3.20764302e-05
Iter: 1942 loss: 3.20998042e-05
Iter: 1943 loss: 3.2073418e-05
Iter: 1944 loss: 3.20649415e-05
Iter: 1945 loss: 3.20594845e-05
Iter: 1946 loss: 3.20566251e-05
Iter: 1947 loss: 3.20415493e-05
Iter: 1948 loss: 3.21360494e-05
Iter: 1949 loss: 3.20396757e-05
Iter: 1950 loss: 3.20288964e-05
Iter: 1951 loss: 3.20347826e-05
Iter: 1952 loss: 3.20241452e-05
Iter: 1953 loss: 3.20169e-05
Iter: 1954 loss: 3.20172694e-05
Iter: 1955 loss: 3.20080653e-05
Iter: 1956 loss: 3.2021002e-05
Iter: 1957 loss: 3.20054241e-05
Iter: 1958 loss: 3.19970422e-05
Iter: 1959 loss: 3.19951141e-05
Iter: 1960 loss: 3.19921237e-05
Iter: 1961 loss: 3.19813516e-05
Iter: 1962 loss: 3.20096879e-05
Iter: 1963 loss: 3.19759092e-05
Iter: 1964 loss: 3.19678438e-05
Iter: 1965 loss: 3.1970354e-05
Iter: 1966 loss: 3.19612045e-05
Iter: 1967 loss: 3.19478786e-05
Iter: 1968 loss: 3.19739556e-05
Iter: 1969 loss: 3.19421306e-05
Iter: 1970 loss: 3.19321043e-05
Iter: 1971 loss: 3.19883475e-05
Iter: 1972 loss: 3.1931886e-05
Iter: 1973 loss: 3.19188148e-05
Iter: 1974 loss: 3.19649189e-05
Iter: 1975 loss: 3.19141836e-05
Iter: 1976 loss: 3.19077299e-05
Iter: 1977 loss: 3.19532264e-05
Iter: 1978 loss: 3.19042883e-05
Iter: 1979 loss: 3.18954917e-05
Iter: 1980 loss: 3.18948296e-05
Iter: 1981 loss: 3.18897874e-05
Iter: 1982 loss: 3.1877149e-05
Iter: 1983 loss: 3.18906168e-05
Iter: 1984 loss: 3.18666935e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script69
+ '[' -r STOP.script69 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi1_phi3/500_500_500_500_1
