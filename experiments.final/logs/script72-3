+ RUN=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ LAYERS=300_300_300_1
+ case $RUN in
+ PSI=2
+ OPTIONS='			 --optimizer adam 				 --n_pairs 50000 				 --batch_size 5000 				 --max_epochs 30 				 --learning_rate 0.001 				 --decay_rate 0.8 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output72
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output73
+ for fn in f1
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0 /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi0
+ date
Sat Oct 31 13:56:34 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --function f1 --psi 2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 30 --learning_rate 0.001 --decay_rate 0.8 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd0354c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd02568c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd02566a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd03546a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd03a50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd03c16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd02de2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd02de9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd02de840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd0030400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd01318c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd03548c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd01830d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd01831e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd016fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd016f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd0053730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c805f9d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd00e98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd00e9ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c805171e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c80527d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd00a18c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c80430f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c80413730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd02218c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd0221510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5cd0212a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c804ea488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c804dc400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c804eaf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c8045cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c8045cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c8030e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c8030eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5c80255950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.3568527
test_loss: 0.35916302
train_loss: 0.36127922
test_loss: 0.3587995
train_loss: 0.3587767
test_loss: 0.3586277
train_loss: 0.3693859
test_loss: 0.35855785
train_loss: 0.35855657
test_loss: 0.35851
train_loss: 0.35658962
test_loss: 0.3585333
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi0/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62d1532620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62d14f5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62d14e2d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62d14e2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62d13e42f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62d13e4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62d1427ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62d1415d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62d1415ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c9164f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62d1415bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c9198ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c9198b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c90b7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c90b72f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c9198c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c9146620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c9033a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c9146400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8fae950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8fae9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c90d5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8faeea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8f6c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8f7e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8f058c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8f05950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8ec6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8ec6bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8e529d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8e52ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8e37c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8e376a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8db4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8db4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f62c8db4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.224626765
Iter: 2 loss: 0.078667596
Iter: 3 loss: 0.0649956912
Iter: 4 loss: 0.0279183388
Iter: 5 loss: 0.0251646806
Iter: 6 loss: 0.0202013217
Iter: 7 loss: 0.02029147
Iter: 8 loss: 0.016316399
Iter: 9 loss: 0.0140033783
Iter: 10 loss: 0.0136971939
Iter: 11 loss: 0.012745929
Iter: 12 loss: 0.0122135784
Iter: 13 loss: 0.0118308021
Iter: 14 loss: 0.010942705
Iter: 15 loss: 0.0116394749
Iter: 16 loss: 0.010438526
Iter: 17 loss: 0.00984963
Iter: 18 loss: 0.0150704198
Iter: 19 loss: 0.00982519425
Iter: 20 loss: 0.0094419606
Iter: 21 loss: 0.00913576595
Iter: 22 loss: 0.0090341568
Iter: 23 loss: 0.00857923552
Iter: 24 loss: 0.00854757614
Iter: 25 loss: 0.00822465215
Iter: 26 loss: 0.00788792
Iter: 27 loss: 0.00781477615
Iter: 28 loss: 0.0076124547
Iter: 29 loss: 0.00729966955
Iter: 30 loss: 0.00969567057
Iter: 31 loss: 0.00728336349
Iter: 32 loss: 0.0070966538
Iter: 33 loss: 0.00722081028
Iter: 34 loss: 0.00698732445
Iter: 35 loss: 0.00684645027
Iter: 36 loss: 0.00677213818
Iter: 37 loss: 0.00671543041
Iter: 38 loss: 0.00661994051
Iter: 39 loss: 0.00661361776
Iter: 40 loss: 0.00654471759
Iter: 41 loss: 0.00646667276
Iter: 42 loss: 0.00643277
Iter: 43 loss: 0.00639634207
Iter: 44 loss: 0.0063335
Iter: 45 loss: 0.00674523972
Iter: 46 loss: 0.00632836297
Iter: 47 loss: 0.00628410187
Iter: 48 loss: 0.00632264838
Iter: 49 loss: 0.00625925884
Iter: 50 loss: 0.00622302759
Iter: 51 loss: 0.00618598657
Iter: 52 loss: 0.00617973041
Iter: 53 loss: 0.00612760615
Iter: 54 loss: 0.00614976883
Iter: 55 loss: 0.00609429646
Iter: 56 loss: 0.00601002
Iter: 57 loss: 0.00614314573
Iter: 58 loss: 0.00597665319
Iter: 59 loss: 0.00590509642
Iter: 60 loss: 0.00582943065
Iter: 61 loss: 0.00581833394
Iter: 62 loss: 0.00571492966
Iter: 63 loss: 0.00572290411
Iter: 64 loss: 0.00563752372
Iter: 65 loss: 0.00549259782
Iter: 66 loss: 0.00561147463
Iter: 67 loss: 0.00540390285
Iter: 68 loss: 0.00524736149
Iter: 69 loss: 0.00524607953
Iter: 70 loss: 0.00513308728
Iter: 71 loss: 0.00511809
Iter: 72 loss: 0.00505583454
Iter: 73 loss: 0.0049377745
Iter: 74 loss: 0.00501812343
Iter: 75 loss: 0.00485707
Iter: 76 loss: 0.00473351637
Iter: 77 loss: 0.00479190797
Iter: 78 loss: 0.00466785673
Iter: 79 loss: 0.00451891869
Iter: 80 loss: 0.00450140703
Iter: 81 loss: 0.00442294776
Iter: 82 loss: 0.00469288696
Iter: 83 loss: 0.00440818816
Iter: 84 loss: 0.00434853928
Iter: 85 loss: 0.0049764053
Iter: 86 loss: 0.00434654113
Iter: 87 loss: 0.00428431341
Iter: 88 loss: 0.00440905709
Iter: 89 loss: 0.00426084362
Iter: 90 loss: 0.00422056485
Iter: 91 loss: 0.00435134117
Iter: 92 loss: 0.00421040412
Iter: 93 loss: 0.00418103393
Iter: 94 loss: 0.00426166598
Iter: 95 loss: 0.0041715
Iter: 96 loss: 0.00413627923
Iter: 97 loss: 0.00408436218
Iter: 98 loss: 0.00408363808
Iter: 99 loss: 0.00404065382
Iter: 100 loss: 0.00416681264
Iter: 101 loss: 0.00402829144
Iter: 102 loss: 0.00399367837
Iter: 103 loss: 0.00441965275
Iter: 104 loss: 0.0039919652
Iter: 105 loss: 0.00395143125
Iter: 106 loss: 0.00415419694
Iter: 107 loss: 0.0039442433
Iter: 108 loss: 0.0038877239
Iter: 109 loss: 0.00400392618
Iter: 110 loss: 0.00386576238
Iter: 111 loss: 0.00382679305
Iter: 112 loss: 0.00385099044
Iter: 113 loss: 0.00380082568
Iter: 114 loss: 0.00373500446
Iter: 115 loss: 0.00385900587
Iter: 116 loss: 0.0037045849
Iter: 117 loss: 0.00363543374
Iter: 118 loss: 0.00363500696
Iter: 119 loss: 0.00356203085
Iter: 120 loss: 0.00354560185
Iter: 121 loss: 0.00354564609
Iter: 122 loss: 0.00349638727
Iter: 123 loss: 0.00339498
Iter: 124 loss: 0.0104859602
Iter: 125 loss: 0.00339454692
Iter: 126 loss: 0.00328690535
Iter: 127 loss: 0.00426501082
Iter: 128 loss: 0.0032780217
Iter: 129 loss: 0.00321838958
Iter: 130 loss: 0.00596652366
Iter: 131 loss: 0.0032179635
Iter: 132 loss: 0.00315269036
Iter: 133 loss: 0.00352228712
Iter: 134 loss: 0.0031457406
Iter: 135 loss: 0.00308041694
Iter: 136 loss: 0.00309522077
Iter: 137 loss: 0.00302315224
Iter: 138 loss: 0.00306901732
Iter: 139 loss: 0.00298325811
Iter: 140 loss: 0.00296353432
Iter: 141 loss: 0.00336310943
Iter: 142 loss: 0.00296333805
Iter: 143 loss: 0.00294785877
Iter: 144 loss: 0.0029559359
Iter: 145 loss: 0.0029360312
Iter: 146 loss: 0.00288294302
Iter: 147 loss: 0.002930969
Iter: 148 loss: 0.0028541619
Iter: 149 loss: 0.00278486032
Iter: 150 loss: 0.00370013225
Iter: 151 loss: 0.00277978228
Iter: 152 loss: 0.00275237905
Iter: 153 loss: 0.0027523376
Iter: 154 loss: 0.00273679802
Iter: 155 loss: 0.0027807178
Iter: 156 loss: 0.00273032486
Iter: 157 loss: 0.00270848675
Iter: 158 loss: 0.0027012818
Iter: 159 loss: 0.00268911803
Iter: 160 loss: 0.00269273249
Iter: 161 loss: 0.00267100171
Iter: 162 loss: 0.00264607882
Iter: 163 loss: 0.00276556378
Iter: 164 loss: 0.00264205434
Iter: 165 loss: 0.0026300007
Iter: 166 loss: 0.00286756223
Iter: 167 loss: 0.00262965099
Iter: 168 loss: 0.00261269975
Iter: 169 loss: 0.0026111817
Iter: 170 loss: 0.00259847404
Iter: 171 loss: 0.00257577468
Iter: 172 loss: 0.00275293877
Iter: 173 loss: 0.00257408479
Iter: 174 loss: 0.00256220391
Iter: 175 loss: 0.0026130951
Iter: 176 loss: 0.00255973358
Iter: 177 loss: 0.00254747621
Iter: 178 loss: 0.00257460703
Iter: 179 loss: 0.00254212529
Iter: 180 loss: 0.00253512128
Iter: 181 loss: 0.00253459206
Iter: 182 loss: 0.0025288309
Iter: 183 loss: 0.00252468046
Iter: 184 loss: 0.00252264645
Iter: 185 loss: 0.00250900257
Iter: 186 loss: 0.00248544104
Iter: 187 loss: 0.00248542195
Iter: 188 loss: 0.00247041602
Iter: 189 loss: 0.00258080428
Iter: 190 loss: 0.00246892544
Iter: 191 loss: 0.00245789438
Iter: 192 loss: 0.0024307156
Iter: 193 loss: 0.00266786152
Iter: 194 loss: 0.00242698984
Iter: 195 loss: 0.00240719109
Iter: 196 loss: 0.00293808803
Iter: 197 loss: 0.00240718573
Iter: 198 loss: 0.00239969604
Iter: 199 loss: 0.00239871978
Iter: 200 loss: 0.00239464315
Iter: 201 loss: 0.00239205686
Iter: 202 loss: 0.00239044055
Iter: 203 loss: 0.00238507194
Iter: 204 loss: 0.00237568887
Iter: 205 loss: 0.00237568328
Iter: 206 loss: 0.00235922355
Iter: 207 loss: 0.00253024092
Iter: 208 loss: 0.00235881144
Iter: 209 loss: 0.00235323375
Iter: 210 loss: 0.00237083901
Iter: 211 loss: 0.00235168054
Iter: 212 loss: 0.00234240945
Iter: 213 loss: 0.00232217181
Iter: 214 loss: 0.00276146689
Iter: 215 loss: 0.00232159812
Iter: 216 loss: 0.00231025601
Iter: 217 loss: 0.00234869798
Iter: 218 loss: 0.002307249
Iter: 219 loss: 0.00230270345
Iter: 220 loss: 0.00229607103
Iter: 221 loss: 0.00229576649
Iter: 222 loss: 0.00227793958
Iter: 223 loss: 0.00235120719
Iter: 224 loss: 0.00227392884
Iter: 225 loss: 0.0022471936
Iter: 226 loss: 0.00238824356
Iter: 227 loss: 0.00224229135
Iter: 228 loss: 0.00224049343
Iter: 229 loss: 0.00223578396
Iter: 230 loss: 0.00223266985
Iter: 231 loss: 0.00224407692
Iter: 232 loss: 0.00223196344
Iter: 233 loss: 0.00222740369
Iter: 234 loss: 0.00222725305
Iter: 235 loss: 0.00222021458
Iter: 236 loss: 0.00220902078
Iter: 237 loss: 0.00220891
Iter: 238 loss: 0.00219624769
Iter: 239 loss: 0.00228958437
Iter: 240 loss: 0.00219522347
Iter: 241 loss: 0.00218728208
Iter: 242 loss: 0.00222100527
Iter: 243 loss: 0.00218551909
Iter: 244 loss: 0.00218183827
Iter: 245 loss: 0.00222664024
Iter: 246 loss: 0.00218173582
Iter: 247 loss: 0.00217919238
Iter: 248 loss: 0.00218249205
Iter: 249 loss: 0.0021779642
Iter: 250 loss: 0.00217530481
Iter: 251 loss: 0.00217776443
Iter: 252 loss: 0.00217376184
Iter: 253 loss: 0.00217127986
Iter: 254 loss: 0.00218452141
Iter: 255 loss: 0.00217091944
Iter: 256 loss: 0.00216691499
Iter: 257 loss: 0.00217754883
Iter: 258 loss: 0.00216560718
Iter: 259 loss: 0.00216032146
Iter: 260 loss: 0.00217986386
Iter: 261 loss: 0.00215898245
Iter: 262 loss: 0.00215093605
Iter: 263 loss: 0.00216084253
Iter: 264 loss: 0.00214661332
Iter: 265 loss: 0.00214813557
Iter: 266 loss: 0.0021448466
Iter: 267 loss: 0.00214246567
Iter: 268 loss: 0.00215544528
Iter: 269 loss: 0.00214212737
Iter: 270 loss: 0.00214090059
Iter: 271 loss: 0.00213842979
Iter: 272 loss: 0.00218886719
Iter: 273 loss: 0.00213839789
Iter: 274 loss: 0.00213568541
Iter: 275 loss: 0.00213440927
Iter: 276 loss: 0.00213310542
Iter: 277 loss: 0.00213063601
Iter: 278 loss: 0.00213237526
Iter: 279 loss: 0.00212903554
Iter: 280 loss: 0.00212818524
Iter: 281 loss: 0.002130426
Iter: 282 loss: 0.00212790165
Iter: 283 loss: 0.00212634262
Iter: 284 loss: 0.00214701262
Iter: 285 loss: 0.002126334
Iter: 286 loss: 0.00212491211
Iter: 287 loss: 0.00213017687
Iter: 288 loss: 0.00212455774
Iter: 289 loss: 0.00212337542
Iter: 290 loss: 0.00212393934
Iter: 291 loss: 0.00212258333
Iter: 292 loss: 0.00212113
Iter: 293 loss: 0.00211858936
Iter: 294 loss: 0.00211858843
Iter: 295 loss: 0.00211613509
Iter: 296 loss: 0.00212543039
Iter: 297 loss: 0.00211553
Iter: 298 loss: 0.00211299956
Iter: 299 loss: 0.00211671088
Iter: 300 loss: 0.0021117921
Iter: 301 loss: 0.00210967544
Iter: 302 loss: 0.00210963748
Iter: 303 loss: 0.00210821722
Iter: 304 loss: 0.00210958463
Iter: 305 loss: 0.00210741581
Iter: 306 loss: 0.00210657716
Iter: 307 loss: 0.00210800627
Iter: 308 loss: 0.00210617902
Iter: 309 loss: 0.00210530963
Iter: 310 loss: 0.00210461207
Iter: 311 loss: 0.00210435269
Iter: 312 loss: 0.00210165023
Iter: 313 loss: 0.00210862141
Iter: 314 loss: 0.0021007224
Iter: 315 loss: 0.00209836964
Iter: 316 loss: 0.0021038549
Iter: 317 loss: 0.00209755264
Iter: 318 loss: 0.00209448067
Iter: 319 loss: 0.00209857943
Iter: 320 loss: 0.00209290278
Iter: 321 loss: 0.00208841357
Iter: 322 loss: 0.00209612213
Iter: 323 loss: 0.00208629761
Iter: 324 loss: 0.00207794
Iter: 325 loss: 0.00208586222
Iter: 326 loss: 0.00207287609
Iter: 327 loss: 0.00207073637
Iter: 328 loss: 0.00209544343
Iter: 329 loss: 0.00207070913
Iter: 330 loss: 0.00206989422
Iter: 331 loss: 0.00207864819
Iter: 332 loss: 0.00206985651
Iter: 333 loss: 0.0020693636
Iter: 334 loss: 0.00206817663
Iter: 335 loss: 0.00208107312
Iter: 336 loss: 0.00206805486
Iter: 337 loss: 0.0020645177
Iter: 338 loss: 0.00206448254
Iter: 339 loss: 0.00205764035
Iter: 340 loss: 0.00208591577
Iter: 341 loss: 0.0020560876
Iter: 342 loss: 0.0020496042
Iter: 343 loss: 0.00206147181
Iter: 344 loss: 0.00204680488
Iter: 345 loss: 0.0020428542
Iter: 346 loss: 0.00204339018
Iter: 347 loss: 0.0020396621
Iter: 348 loss: 0.00203707395
Iter: 349 loss: 0.00205261074
Iter: 350 loss: 0.00203679455
Iter: 351 loss: 0.00203468604
Iter: 352 loss: 0.00204544375
Iter: 353 loss: 0.00203433726
Iter: 354 loss: 0.00203251
Iter: 355 loss: 0.00202944758
Iter: 356 loss: 0.00202943897
Iter: 357 loss: 0.00202377024
Iter: 358 loss: 0.00202041096
Iter: 359 loss: 0.00201803679
Iter: 360 loss: 0.0020135832
Iter: 361 loss: 0.00203525
Iter: 362 loss: 0.00201279321
Iter: 363 loss: 0.00202015555
Iter: 364 loss: 0.00201219274
Iter: 365 loss: 0.00201093778
Iter: 366 loss: 0.00201817974
Iter: 367 loss: 0.00201077573
Iter: 368 loss: 0.00200886512
Iter: 369 loss: 0.0020088274
Iter: 370 loss: 0.00200698501
Iter: 371 loss: 0.0020104975
Iter: 372 loss: 0.0020062027
Iter: 373 loss: 0.00200510956
Iter: 374 loss: 0.00201625307
Iter: 375 loss: 0.00200507836
Iter: 376 loss: 0.00200464763
Iter: 377 loss: 0.00200398127
Iter: 378 loss: 0.00200397335
Iter: 379 loss: 0.00200327486
Iter: 380 loss: 0.00200167065
Iter: 381 loss: 0.00202203519
Iter: 382 loss: 0.00200155471
Iter: 383 loss: 0.00199923618
Iter: 384 loss: 0.0020058509
Iter: 385 loss: 0.00199848483
Iter: 386 loss: 0.00199784432
Iter: 387 loss: 0.00199698494
Iter: 388 loss: 0.0019967705
Iter: 389 loss: 0.00199792138
Iter: 390 loss: 0.00199673697
Iter: 391 loss: 0.0019965726
Iter: 392 loss: 0.00199623592
Iter: 393 loss: 0.00200269464
Iter: 394 loss: 0.00199623196
Iter: 395 loss: 0.00203110185
Iter: 396 loss: 0.00199577282
Iter: 397 loss: 0.00199519587
Iter: 398 loss: 0.00199648459
Iter: 399 loss: 0.00199498306
Iter: 400 loss: 0.00199451251
Iter: 401 loss: 0.00199373858
Iter: 402 loss: 0.00199373392
Iter: 403 loss: 0.00199302193
Iter: 404 loss: 0.00199140189
Iter: 405 loss: 0.00201220205
Iter: 406 loss: 0.00199129502
Iter: 407 loss: 0.00199057534
Iter: 408 loss: 0.0019937614
Iter: 409 loss: 0.00199041981
Iter: 410 loss: 0.00199019047
Iter: 411 loss: 0.00198973832
Iter: 412 loss: 0.00199841661
Iter: 413 loss: 0.00198973436
Iter: 414 loss: 0.00198835507
Iter: 415 loss: 0.00198740326
Iter: 416 loss: 0.0019869064
Iter: 417 loss: 0.0019861632
Iter: 418 loss: 0.00198570965
Iter: 419 loss: 0.00198513828
Iter: 420 loss: 0.00198396761
Iter: 421 loss: 0.00200435077
Iter: 422 loss: 0.00198394363
Iter: 423 loss: 0.00198155548
Iter: 424 loss: 0.00197631
Iter: 425 loss: 0.00205258839
Iter: 426 loss: 0.00197607465
Iter: 427 loss: 0.00197541155
Iter: 428 loss: 0.0019752658
Iter: 429 loss: 0.00197453238
Iter: 430 loss: 0.00197934313
Iter: 431 loss: 0.00197445136
Iter: 432 loss: 0.00197502715
Iter: 433 loss: 0.00197385298
Iter: 434 loss: 0.00197340967
Iter: 435 loss: 0.00197321363
Iter: 436 loss: 0.00197298522
Iter: 437 loss: 0.00197211816
Iter: 438 loss: 0.00197127741
Iter: 439 loss: 0.00197108928
Iter: 440 loss: 0.00196912
Iter: 441 loss: 0.00197083433
Iter: 442 loss: 0.00196788437
Iter: 443 loss: 0.00196719728
Iter: 444 loss: 0.0019715298
Iter: 445 loss: 0.00196710881
Iter: 446 loss: 0.00196626689
Iter: 447 loss: 0.00196441216
Iter: 448 loss: 0.00199052365
Iter: 449 loss: 0.00196432206
Iter: 450 loss: 0.00196321215
Iter: 451 loss: 0.00196754048
Iter: 452 loss: 0.00196289667
Iter: 453 loss: 0.00196205825
Iter: 454 loss: 0.0019614012
Iter: 455 loss: 0.00196114671
Iter: 456 loss: 0.00198022695
Iter: 457 loss: 0.00195978885
Iter: 458 loss: 0.00195716275
Iter: 459 loss: 0.0019773664
Iter: 460 loss: 0.00195696764
Iter: 461 loss: 0.00195646798
Iter: 462 loss: 0.00195637462
Iter: 463 loss: 0.00195602048
Iter: 464 loss: 0.00195793761
Iter: 465 loss: 0.0019559667
Iter: 466 loss: 0.0019555348
Iter: 467 loss: 0.00195820676
Iter: 468 loss: 0.00195548404
Iter: 469 loss: 0.00195492851
Iter: 470 loss: 0.00195473433
Iter: 471 loss: 0.00195442303
Iter: 472 loss: 0.00195986312
Iter: 473 loss: 0.00195356342
Iter: 474 loss: 0.00195327168
Iter: 475 loss: 0.00195504865
Iter: 476 loss: 0.00195324607
Iter: 477 loss: 0.00195315573
Iter: 478 loss: 0.00195285992
Iter: 479 loss: 0.00195309124
Iter: 480 loss: 0.00195259589
Iter: 481 loss: 0.0019525073
Iter: 482 loss: 0.00195210264
Iter: 483 loss: 0.00195146701
Iter: 484 loss: 0.00195265457
Iter: 485 loss: 0.00195119972
Iter: 486 loss: 0.0019508017
Iter: 487 loss: 0.00195073208
Iter: 488 loss: 0.00195046281
Iter: 489 loss: 0.00195054105
Iter: 490 loss: 0.0019500585
Iter: 491 loss: 0.00194992987
Iter: 492 loss: 0.00195017713
Iter: 493 loss: 0.00194987282
Iter: 494 loss: 0.00194974872
Iter: 495 loss: 0.00194933382
Iter: 496 loss: 0.0019493734
Iter: 497 loss: 0.00194891228
Iter: 498 loss: 0.00194897701
Iter: 499 loss: 0.0019486167
Iter: 500 loss: 0.00194849842
Iter: 501 loss: 0.00194943324
Iter: 502 loss: 0.00194849307
Iter: 503 loss: 0.00194847
Iter: 504 loss: 0.00194843614
Iter: 505 loss: 0.00194843695
Iter: 506 loss: 0.00194836548
Iter: 507 loss: 0.00194837409
Iter: 508 loss: 0.00194831402
Iter: 509 loss: 0.00194819365
Iter: 510 loss: 0.0019479508
Iter: 511 loss: 0.00195231452
Iter: 512 loss: 0.00194794638
Iter: 513 loss: 0.00194726977
Iter: 514 loss: 0.00194620225
Iter: 515 loss: 0.00194619014
Iter: 516 loss: 0.00194784405
Iter: 517 loss: 0.00194582541
Iter: 518 loss: 0.00194561051
Iter: 519 loss: 0.00194563542
Iter: 520 loss: 0.00194544892
Iter: 521 loss: 0.00194520527
Iter: 522 loss: 0.00194801029
Iter: 523 loss: 0.00194520515
Iter: 524 loss: 0.00194555486
Iter: 525 loss: 0.00194513367
Iter: 526 loss: 0.00194508582
Iter: 527 loss: 0.00194504822
Iter: 528 loss: 0.00194503739
Iter: 529 loss: 0.00194499118
Iter: 530 loss: 0.00194497639
Iter: 531 loss: 0.00194495346
Iter: 532 loss: 0.0019449282
Iter: 533 loss: 0.00194483297
Iter: 534 loss: 0.00194458163
Iter: 535 loss: 0.00194912252
Iter: 536 loss: 0.00194457697
Iter: 537 loss: 0.00194397639
Iter: 538 loss: 0.00194397697
Iter: 539 loss: 0.00194370409
Iter: 540 loss: 0.00194459572
Iter: 541 loss: 0.00194363133
Iter: 542 loss: 0.00194352097
Iter: 543 loss: 0.00194366113
Iter: 544 loss: 0.0019434694
Iter: 545 loss: 0.00194331142
Iter: 546 loss: 0.00194276473
Iter: 547 loss: 0.00194165029
Iter: 548 loss: 0.00194164203
Iter: 549 loss: 0.00194107823
Iter: 550 loss: 0.00194277172
Iter: 551 loss: 0.00194091361
Iter: 552 loss: 0.00194060279
Iter: 553 loss: 0.00194092793
Iter: 554 loss: 0.00194043305
Iter: 555 loss: 0.00193970744
Iter: 556 loss: 0.00194039103
Iter: 557 loss: 0.00193929067
Iter: 558 loss: 0.00194025482
Iter: 559 loss: 0.00193908368
Iter: 560 loss: 0.00193897
Iter: 561 loss: 0.00193967368
Iter: 562 loss: 0.00193895551
Iter: 563 loss: 0.00193888182
Iter: 564 loss: 0.00193945621
Iter: 565 loss: 0.00193887611
Iter: 566 loss: 0.00193880952
Iter: 567 loss: 0.00193857553
Iter: 568 loss: 0.00193822477
Iter: 569 loss: 0.00193818379
Iter: 570 loss: 0.00193719578
Iter: 571 loss: 0.00193822477
Iter: 572 loss: 0.00193661312
Iter: 573 loss: 0.00193599658
Iter: 574 loss: 0.0019389363
Iter: 575 loss: 0.00193589972
Iter: 576 loss: 0.001935657
Iter: 577 loss: 0.00193585688
Iter: 578 loss: 0.0019355095
Iter: 579 loss: 0.00193527678
Iter: 580 loss: 0.0019367988
Iter: 581 loss: 0.00193524756
Iter: 582 loss: 0.00193511555
Iter: 583 loss: 0.00193480204
Iter: 584 loss: 0.0019384108
Iter: 585 loss: 0.00193477259
Iter: 586 loss: 0.00193415862
Iter: 587 loss: 0.00193390436
Iter: 588 loss: 0.00193358201
Iter: 589 loss: 0.00193329668
Iter: 590 loss: 0.00193327409
Iter: 591 loss: 0.00193317875
Iter: 592 loss: 0.00193295465
Iter: 593 loss: 0.00193550088
Iter: 594 loss: 0.00193293428
Iter: 595 loss: 0.00193261215
Iter: 596 loss: 0.00193268107
Iter: 597 loss: 0.00193237199
Iter: 598 loss: 0.00193227082
Iter: 599 loss: 0.0019322678
Iter: 600 loss: 0.00193216407
Iter: 601 loss: 0.00193195138
Iter: 602 loss: 0.001935862
Iter: 603 loss: 0.00193194277
Iter: 604 loss: 0.00193158421
Iter: 605 loss: 0.0019347897
Iter: 606 loss: 0.00193156954
Iter: 607 loss: 0.00193137256
Iter: 608 loss: 0.00193134556
Iter: 609 loss: 0.00193127221
Iter: 610 loss: 0.0019309998
Iter: 611 loss: 0.0019303048
Iter: 612 loss: 0.00194351445
Iter: 613 loss: 0.00193029502
Iter: 614 loss: 0.00192963087
Iter: 615 loss: 0.00193967484
Iter: 616 loss: 0.00192963087
Iter: 617 loss: 0.00192930363
Iter: 618 loss: 0.00193118432
Iter: 619 loss: 0.00192926254
Iter: 620 loss: 0.00192917523
Iter: 621 loss: 0.00192921702
Iter: 622 loss: 0.00192911469
Iter: 623 loss: 0.00192908454
Iter: 624 loss: 0.00192902086
Iter: 625 loss: 0.00193019013
Iter: 626 loss: 0.00192902226
Iter: 627 loss: 0.0019289942
Iter: 628 loss: 0.00192896242
Iter: 629 loss: 0.00192888675
Iter: 630 loss: 0.00192887872
Iter: 631 loss: 0.00192880817
Iter: 632 loss: 0.00192952203
Iter: 633 loss: 0.00192880887
Iter: 634 loss: 0.00192860945
Iter: 635 loss: 0.00192847359
Iter: 636 loss: 0.00192840153
Iter: 637 loss: 0.00192809012
Iter: 638 loss: 0.00192740245
Iter: 639 loss: 0.0019375172
Iter: 640 loss: 0.00192737195
Iter: 641 loss: 0.00192692783
Iter: 642 loss: 0.00192688243
Iter: 643 loss: 0.00192655833
Iter: 644 loss: 0.00192643143
Iter: 645 loss: 0.00192607916
Iter: 646 loss: 0.00192787894
Iter: 647 loss: 0.00192596
Iter: 648 loss: 0.00192631152
Iter: 649 loss: 0.00192568055
Iter: 650 loss: 0.00192564726
Iter: 651 loss: 0.00192549499
Iter: 652 loss: 0.00192536553
Iter: 653 loss: 0.00192536821
Iter: 654 loss: 0.00192530884
Iter: 655 loss: 0.00192517915
Iter: 656 loss: 0.00192676752
Iter: 657 loss: 0.0019251639
Iter: 658 loss: 0.00192508567
Iter: 659 loss: 0.00192550535
Iter: 660 loss: 0.0019250745
Iter: 661 loss: 0.00192499417
Iter: 662 loss: 0.00192518916
Iter: 663 loss: 0.00192496693
Iter: 664 loss: 0.00192484353
Iter: 665 loss: 0.00192594167
Iter: 666 loss: 0.0019248398
Iter: 667 loss: 0.00192462583
Iter: 668 loss: 0.00192449742
Iter: 669 loss: 0.0019244086
Iter: 670 loss: 0.00192417728
Iter: 671 loss: 0.00192656077
Iter: 672 loss: 0.00192417344
Iter: 673 loss: 0.00192410429
Iter: 674 loss: 0.00192390126
Iter: 675 loss: 0.00192470674
Iter: 676 loss: 0.00192381791
Iter: 677 loss: 0.0019235902
Iter: 678 loss: 0.00192373316
Iter: 679 loss: 0.00192344445
Iter: 680 loss: 0.00192330778
Iter: 681 loss: 0.00192339823
Iter: 682 loss: 0.00192322396
Iter: 683 loss: 0.00192312489
Iter: 684 loss: 0.00192312524
Iter: 685 loss: 0.00192304235
Iter: 686 loss: 0.00192291266
Iter: 687 loss: 0.0019229122
Iter: 688 loss: 0.00192252337
Iter: 689 loss: 0.00192223792
Iter: 690 loss: 0.00192211452
Iter: 691 loss: 0.00192200951
Iter: 692 loss: 0.00192184804
Iter: 693 loss: 0.00192184583
Iter: 694 loss: 0.00192168436
Iter: 695 loss: 0.00192214944
Iter: 696 loss: 0.00192163
Iter: 697 loss: 0.00192157645
Iter: 698 loss: 0.00192157459
Iter: 699 loss: 0.00192152872
Iter: 700 loss: 0.00192137517
Iter: 701 loss: 0.00192308193
Iter: 702 loss: 0.00192137458
Iter: 703 loss: 0.00192128029
Iter: 704 loss: 0.00192121416
Iter: 705 loss: 0.00192118017
Iter: 706 loss: 0.00192104571
Iter: 707 loss: 0.00192094734
Iter: 708 loss: 0.00192090147
Iter: 709 loss: 0.00192069856
Iter: 710 loss: 0.00192056503
Iter: 711 loss: 0.00192048587
Iter: 712 loss: 0.00191998447
Iter: 713 loss: 0.00191985583
Iter: 714 loss: 0.0019193599
Iter: 715 loss: 0.00191883976
Iter: 716 loss: 0.00191875477
Iter: 717 loss: 0.00191830308
Iter: 718 loss: 0.00192316587
Iter: 719 loss: 0.00191828771
Iter: 720 loss: 0.00191821682
Iter: 721 loss: 0.00191837
Iter: 722 loss: 0.00191818888
Iter: 723 loss: 0.00191809679
Iter: 724 loss: 0.00191825745
Iter: 725 loss: 0.00191805908
Iter: 726 loss: 0.00191788469
Iter: 727 loss: 0.001917382
Iter: 728 loss: 0.00191924046
Iter: 729 loss: 0.00191716338
Iter: 730 loss: 0.00191651424
Iter: 731 loss: 0.00191626535
Iter: 732 loss: 0.00191618805
Iter: 733 loss: 0.00191607978
Iter: 734 loss: 0.00191604905
Iter: 735 loss: 0.00191604416
Iter: 736 loss: 0.00191600993
Iter: 737 loss: 0.00191623298
Iter: 738 loss: 0.00191600551
Iter: 739 loss: 0.00191597792
Iter: 740 loss: 0.00191589165
Iter: 741 loss: 0.00191631238
Iter: 742 loss: 0.00191586162
Iter: 743 loss: 0.00191576965
Iter: 744 loss: 0.00191617059
Iter: 745 loss: 0.00191575161
Iter: 746 loss: 0.00191557826
Iter: 747 loss: 0.00191547466
Iter: 748 loss: 0.0019154062
Iter: 749 loss: 0.00191498967
Iter: 750 loss: 0.00191719108
Iter: 751 loss: 0.00191492494
Iter: 752 loss: 0.00191483647
Iter: 753 loss: 0.001914597
Iter: 754 loss: 0.00191598572
Iter: 755 loss: 0.00191452703
Iter: 756 loss: 0.00191423774
Iter: 757 loss: 0.00191433076
Iter: 758 loss: 0.00191403367
Iter: 759 loss: 0.00191395544
Iter: 760 loss: 0.00191408757
Iter: 761 loss: 0.00191392412
Iter: 762 loss: 0.0019138779
Iter: 763 loss: 0.00191379478
Iter: 764 loss: 0.00191558502
Iter: 765 loss: 0.00191379362
Iter: 766 loss: 0.00191373902
Iter: 767 loss: 0.00191357557
Iter: 768 loss: 0.00191431621
Iter: 769 loss: 0.00191351492
Iter: 770 loss: 0.00191353378
Iter: 771 loss: 0.00191337825
Iter: 772 loss: 0.00191324879
Iter: 773 loss: 0.00191375602
Iter: 774 loss: 0.00191321922
Iter: 775 loss: 0.00191314262
Iter: 776 loss: 0.00191345089
Iter: 777 loss: 0.00191312563
Iter: 778 loss: 0.00191306416
Iter: 779 loss: 0.00191327615
Iter: 780 loss: 0.00191305159
Iter: 781 loss: 0.00191295228
Iter: 782 loss: 0.00191282341
Iter: 783 loss: 0.00191281305
Iter: 784 loss: 0.00191383203
Iter: 785 loss: 0.00191275519
Iter: 786 loss: 0.00191273424
Iter: 787 loss: 0.00191269233
Iter: 788 loss: 0.00191269349
Iter: 789 loss: 0.00191260246
Iter: 790 loss: 0.00191235449
Iter: 791 loss: 0.00191393879
Iter: 792 loss: 0.00191228476
Iter: 793 loss: 0.00191319548
Iter: 794 loss: 0.00191206
Iter: 795 loss: 0.00191185786
Iter: 796 loss: 0.00191282923
Iter: 797 loss: 0.00191182317
Iter: 798 loss: 0.00191175938
Iter: 799 loss: 0.00191156939
Iter: 800 loss: 0.00191186892
Iter: 801 loss: 0.00191143551
Iter: 802 loss: 0.00191183225
Iter: 803 loss: 0.00191134668
Iter: 804 loss: 0.00191119383
Iter: 805 loss: 0.00191287627
Iter: 806 loss: 0.00191119092
Iter: 807 loss: 0.00191116333
Iter: 808 loss: 0.00191119546
Iter: 809 loss: 0.00191114785
Iter: 810 loss: 0.00191109697
Iter: 811 loss: 0.00191100023
Iter: 812 loss: 0.0019127822
Iter: 813 loss: 0.0019109986
Iter: 814 loss: 0.00191078463
Iter: 815 loss: 0.00191050256
Iter: 816 loss: 0.00191048626
Iter: 817 loss: 0.00191028393
Iter: 818 loss: 0.00191111374
Iter: 819 loss: 0.00191024353
Iter: 820 loss: 0.00190994551
Iter: 821 loss: 0.00191194122
Iter: 822 loss: 0.00190991652
Iter: 823 loss: 0.00190970802
Iter: 824 loss: 0.00191007089
Iter: 825 loss: 0.00190961559
Iter: 826 loss: 0.00191015122
Iter: 827 loss: 0.00190959009
Iter: 828 loss: 0.00190956902
Iter: 829 loss: 0.00190963922
Iter: 830 loss: 0.001909566
Iter: 831 loss: 0.00190951116
Iter: 832 loss: 0.0019096192
Iter: 833 loss: 0.00190948648
Iter: 834 loss: 0.00190941873
Iter: 835 loss: 0.00190968346
Iter: 836 loss: 0.0019094008
Iter: 837 loss: 0.00190929801
Iter: 838 loss: 0.00190900988
Iter: 839 loss: 0.00191088
Iter: 840 loss: 0.00190893689
Iter: 841 loss: 0.00190881197
Iter: 842 loss: 0.00190942409
Iter: 843 loss: 0.00190878776
Iter: 844 loss: 0.00190857926
Iter: 845 loss: 0.00190959813
Iter: 846 loss: 0.00190854573
Iter: 847 loss: 0.00190848717
Iter: 848 loss: 0.00190844538
Iter: 849 loss: 0.00190842687
Iter: 850 loss: 0.00190829369
Iter: 851 loss: 0.00190807681
Iter: 852 loss: 0.00190807739
Iter: 853 loss: 0.00190807844
Iter: 854 loss: 0.00190764596
Iter: 855 loss: 0.00190729776
Iter: 856 loss: 0.00190824852
Iter: 857 loss: 0.00190718891
Iter: 858 loss: 0.00190707669
Iter: 859 loss: 0.00190721126
Iter: 860 loss: 0.00190701766
Iter: 861 loss: 0.00190695224
Iter: 862 loss: 0.00190674991
Iter: 863 loss: 0.00190733769
Iter: 864 loss: 0.00190664071
Iter: 865 loss: 0.00190669578
Iter: 866 loss: 0.00190655631
Iter: 867 loss: 0.00190638017
Iter: 868 loss: 0.00190655526
Iter: 869 loss: 0.00190628041
Iter: 870 loss: 0.00190621242
Iter: 871 loss: 0.00190639636
Iter: 872 loss: 0.00190619566
Iter: 873 loss: 0.00190611556
Iter: 874 loss: 0.00190605526
Iter: 875 loss: 0.00190602848
Iter: 876 loss: 0.00190596469
Iter: 877 loss: 0.00190575351
Iter: 878 loss: 0.00190553372
Iter: 879 loss: 0.00190545211
Iter: 880 loss: 0.00190533139
Iter: 881 loss: 0.00190531067
Iter: 882 loss: 0.00190499343
Iter: 883 loss: 0.00190628448
Iter: 884 loss: 0.00190492731
Iter: 885 loss: 0.00190472766
Iter: 886 loss: 0.00190470868
Iter: 887 loss: 0.00190456701
Iter: 888 loss: 0.00190421008
Iter: 889 loss: 0.00190446281
Iter: 890 loss: 0.0019039925
Iter: 891 loss: 0.0019036656
Iter: 892 loss: 0.00190365023
Iter: 893 loss: 0.00190360937
Iter: 894 loss: 0.0019035961
Iter: 895 loss: 0.00190357305
Iter: 896 loss: 0.00190357969
Iter: 897 loss: 0.00190355664
Iter: 898 loss: 0.0019035039
Iter: 899 loss: 0.00190360192
Iter: 900 loss: 0.00190347701
Iter: 901 loss: 0.00190341845
Iter: 902 loss: 0.00190332043
Iter: 903 loss: 0.00190331927
Iter: 904 loss: 0.00190316688
Iter: 905 loss: 0.00190279167
Iter: 906 loss: 0.00190653163
Iter: 907 loss: 0.0019027479
Iter: 908 loss: 0.00190243474
Iter: 909 loss: 0.00190713489
Iter: 910 loss: 0.00190243509
Iter: 911 loss: 0.00190217607
Iter: 912 loss: 0.00190294744
Iter: 913 loss: 0.00190210133
Iter: 914 loss: 0.00190192705
Iter: 915 loss: 0.00190177979
Iter: 916 loss: 0.00190173299
Iter: 917 loss: 0.00190175453
Iter: 918 loss: 0.00190158549
Iter: 919 loss: 0.00190153299
Iter: 920 loss: 0.00190150132
Iter: 921 loss: 0.0019014834
Iter: 922 loss: 0.00190135813
Iter: 923 loss: 0.00190254743
Iter: 924 loss: 0.00190135161
Iter: 925 loss: 0.00190128188
Iter: 926 loss: 0.00190126442
Iter: 927 loss: 0.00190124544
Iter: 928 loss: 0.00190123834
Iter: 929 loss: 0.00190121017
Iter: 930 loss: 0.00190114102
Iter: 931 loss: 0.00190153066
Iter: 932 loss: 0.00190112286
Iter: 933 loss: 0.00190103636
Iter: 934 loss: 0.00190139771
Iter: 935 loss: 0.00190101762
Iter: 936 loss: 0.00190099049
Iter: 937 loss: 0.00190087978
Iter: 938 loss: 0.00190060982
Iter: 939 loss: 0.00190594024
Iter: 940 loss: 0.00190060865
Iter: 941 loss: 0.00190023752
Iter: 942 loss: 0.00190318108
Iter: 943 loss: 0.00190021144
Iter: 944 loss: 0.00190008385
Iter: 945 loss: 0.00190008292
Iter: 946 loss: 0.0018999764
Iter: 947 loss: 0.00189977244
Iter: 948 loss: 0.00190367
Iter: 949 loss: 0.00189977034
Iter: 950 loss: 0.00189949537
Iter: 951 loss: 0.00189926242
Iter: 952 loss: 0.00189918559
Iter: 953 loss: 0.0018990793
Iter: 954 loss: 0.00189919095
Iter: 955 loss: 0.0018990197
Iter: 956 loss: 0.00189897604
Iter: 957 loss: 0.00189892517
Iter: 958 loss: 0.0018989197
Iter: 959 loss: 0.00189888221
Iter: 960 loss: 0.00189896405
Iter: 961 loss: 0.0018988736
Iter: 962 loss: 0.00189895101
Iter: 963 loss: 0.00189885695
Iter: 964 loss: 0.00189884449
Iter: 965 loss: 0.00189883588
Iter: 966 loss: 0.00189883099
Iter: 967 loss: 0.0018987034
Iter: 968 loss: 0.0018986084
Iter: 969 loss: 0.00189856405
Iter: 970 loss: 0.0018983169
Iter: 971 loss: 0.00189876521
Iter: 972 loss: 0.00189820642
Iter: 973 loss: 0.00189800235
Iter: 974 loss: 0.00189865695
Iter: 975 loss: 0.00189794146
Iter: 976 loss: 0.00189774169
Iter: 977 loss: 0.00189767405
Iter: 978 loss: 0.00189755519
Iter: 979 loss: 0.00189760327
Iter: 980 loss: 0.00189746323
Iter: 981 loss: 0.0018974198
Iter: 982 loss: 0.00189738744
Iter: 983 loss: 0.00189737126
Iter: 984 loss: 0.00189730246
Iter: 985 loss: 0.0018972971
Iter: 986 loss: 0.00189724565
Iter: 987 loss: 0.00189736742
Iter: 988 loss: 0.00189722516
Iter: 989 loss: 0.00189715775
Iter: 990 loss: 0.00189693901
Iter: 991 loss: 0.00189710676
Iter: 992 loss: 0.00189674797
Iter: 993 loss: 0.00189706264
Iter: 994 loss: 0.00189663086
Iter: 995 loss: 0.0018965326
Iter: 996 loss: 0.00189646101
Iter: 997 loss: 0.00189642515
Iter: 998 loss: 0.0018962553
Iter: 999 loss: 0.00189624296
Iter: 1000 loss: 0.00189619721
Iter: 1001 loss: 0.00189612783
Iter: 1002 loss: 0.00189612794
Iter: 1003 loss: 0.00189597346
Iter: 1004 loss: 0.00189563725
Iter: 1005 loss: 0.00190076442
Iter: 1006 loss: 0.00189562305
Iter: 1007 loss: 0.0018954355
Iter: 1008 loss: 0.00189788162
Iter: 1009 loss: 0.00189543341
Iter: 1010 loss: 0.00189531851
Iter: 1011 loss: 0.00189574459
Iter: 1012 loss: 0.00189528964
Iter: 1013 loss: 0.0018954135
Iter: 1014 loss: 0.00189517066
Iter: 1015 loss: 0.0018950433
Iter: 1016 loss: 0.00189582945
Iter: 1017 loss: 0.00189502712
Iter: 1018 loss: 0.00189500512
Iter: 1019 loss: 0.00189496856
Iter: 1020 loss: 0.00189496833
Iter: 1021 loss: 0.00189491967
Iter: 1022 loss: 0.00189524074
Iter: 1023 loss: 0.00189491617
Iter: 1024 loss: 0.00189489964
Iter: 1025 loss: 0.00189490127
Iter: 1026 loss: 0.00189489126
Iter: 1027 loss: 0.00189486006
Iter: 1028 loss: 0.0018950121
Iter: 1029 loss: 0.00189484865
Iter: 1030 loss: 0.00189466646
Iter: 1031 loss: 0.0018950135
Iter: 1032 loss: 0.00189459289
Iter: 1033 loss: 0.00189422513
Iter: 1034 loss: 0.00189594761
Iter: 1035 loss: 0.00189415552
Iter: 1036 loss: 0.00189411314
Iter: 1037 loss: 0.00189402746
Iter: 1038 loss: 0.00189559651
Iter: 1039 loss: 0.00189402618
Iter: 1040 loss: 0.00189395121
Iter: 1041 loss: 0.00189395039
Iter: 1042 loss: 0.00189389172
Iter: 1043 loss: 0.00189386227
Iter: 1044 loss: 0.00189386087
Iter: 1045 loss: 0.00189384026
Iter: 1046 loss: 0.00189382804
Iter: 1047 loss: 0.00189380045
Iter: 1048 loss: 0.00189436658
Iter: 1049 loss: 0.00189380185
Iter: 1050 loss: 0.00189376646
Iter: 1051 loss: 0.00189370406
Iter: 1052 loss: 0.00189517462
Iter: 1053 loss: 0.00189370243
Iter: 1054 loss: 0.0018937327
Iter: 1055 loss: 0.00189367007
Iter: 1056 loss: 0.00189361686
Iter: 1057 loss: 0.00189379067
Iter: 1058 loss: 0.00189360604
Iter: 1059 loss: 0.00189357903
Iter: 1060 loss: 0.00189357949
Iter: 1061 loss: 0.00189355249
Iter: 1062 loss: 0.00189353095
Iter: 1063 loss: 0.00189352164
Iter: 1064 loss: 0.00189349987
Iter: 1065 loss: 0.00189345377
Iter: 1066 loss: 0.00189427752
Iter: 1067 loss: 0.00189345307
Iter: 1068 loss: 0.00189334119
Iter: 1069 loss: 0.00189325167
Iter: 1070 loss: 0.00189321674
Iter: 1071 loss: 0.00189313758
Iter: 1072 loss: 0.00189312745
Iter: 1073 loss: 0.00189306086
Iter: 1074 loss: 0.00189304445
Iter: 1075 loss: 0.0018930044
Iter: 1076 loss: 0.00189289916
Iter: 1077 loss: 0.00189392117
Iter: 1078 loss: 0.00189289707
Iter: 1079 loss: 0.00189287914
Iter: 1080 loss: 0.0018928647
Iter: 1081 loss: 0.00189286214
Iter: 1082 loss: 0.00189279113
Iter: 1083 loss: 0.00189257239
Iter: 1084 loss: 0.00189302664
Iter: 1085 loss: 0.00189243723
Iter: 1086 loss: 0.00189207366
Iter: 1087 loss: 0.00189207273
Iter: 1088 loss: 0.00189200474
Iter: 1089 loss: 0.00189200067
Iter: 1090 loss: 0.00189198076
Iter: 1091 loss: 0.00189196644
Iter: 1092 loss: 0.00189194269
Iter: 1093 loss: 0.00189189147
Iter: 1094 loss: 0.00189260952
Iter: 1095 loss: 0.00189188879
Iter: 1096 loss: 0.0018918456
Iter: 1097 loss: 0.00189180125
Iter: 1098 loss: 0.0018917924
Iter: 1099 loss: 0.00189171883
Iter: 1100 loss: 0.00189152465
Iter: 1101 loss: 0.00189293711
Iter: 1102 loss: 0.0018914846
Iter: 1103 loss: 0.00189132802
Iter: 1104 loss: 0.00189114804
Iter: 1105 loss: 0.00189112918
Iter: 1106 loss: 0.00189099601
Iter: 1107 loss: 0.00189102627
Iter: 1108 loss: 0.001890896
Iter: 1109 loss: 0.00189087342
Iter: 1110 loss: 0.00189080404
Iter: 1111 loss: 0.00189150218
Iter: 1112 loss: 0.00189079531
Iter: 1113 loss: 0.00189061137
Iter: 1114 loss: 0.0018903428
Iter: 1115 loss: 0.00189033663
Iter: 1116 loss: 0.0018901038
Iter: 1117 loss: 0.00189066236
Iter: 1118 loss: 0.00189002312
Iter: 1119 loss: 0.00188988564
Iter: 1120 loss: 0.0018901421
Iter: 1121 loss: 0.00188982766
Iter: 1122 loss: 0.00188974896
Iter: 1123 loss: 0.00188957143
Iter: 1124 loss: 0.00189201511
Iter: 1125 loss: 0.00188955944
Iter: 1126 loss: 0.00188946864
Iter: 1127 loss: 0.00188958063
Iter: 1128 loss: 0.00188942
Iter: 1129 loss: 0.00188939529
Iter: 1130 loss: 0.00188939518
Iter: 1131 loss: 0.00188936247
Iter: 1132 loss: 0.00188929273
Iter: 1133 loss: 0.00189054408
Iter: 1134 loss: 0.00188929401
Iter: 1135 loss: 0.00188931567
Iter: 1136 loss: 0.00188923825
Iter: 1137 loss: 0.0018891748
Iter: 1138 loss: 0.00188928307
Iter: 1139 loss: 0.00188914745
Iter: 1140 loss: 0.00188908167
Iter: 1141 loss: 0.00188935653
Iter: 1142 loss: 0.00188906584
Iter: 1143 loss: 0.00188900274
Iter: 1144 loss: 0.00188897748
Iter: 1145 loss: 0.00188894325
Iter: 1146 loss: 0.00188888295
Iter: 1147 loss: 0.00188872009
Iter: 1148 loss: 0.00188975804
Iter: 1149 loss: 0.0018886769
Iter: 1150 loss: 0.00188851857
Iter: 1151 loss: 0.00188851182
Iter: 1152 loss: 0.0018884025
Iter: 1153 loss: 0.00188838854
Iter: 1154 loss: 0.00188829436
Iter: 1155 loss: 0.00188951206
Iter: 1156 loss: 0.0018882947
Iter: 1157 loss: 0.00188823929
Iter: 1158 loss: 0.00188828155
Iter: 1159 loss: 0.001888206
Iter: 1160 loss: 0.00188816804
Iter: 1161 loss: 0.00188806769
Iter: 1162 loss: 0.00188864442
Iter: 1163 loss: 0.00188803766
Iter: 1164 loss: 0.00188782869
Iter: 1165 loss: 0.00188745908
Iter: 1166 loss: 0.00188745861
Iter: 1167 loss: 0.00188736687
Iter: 1168 loss: 0.00188748399
Iter: 1169 loss: 0.00188731507
Iter: 1170 loss: 0.00188720087
Iter: 1171 loss: 0.00188750122
Iter: 1172 loss: 0.00188716059
Iter: 1173 loss: 0.00188712415
Iter: 1174 loss: 0.00188709178
Iter: 1175 loss: 0.00188708468
Iter: 1176 loss: 0.00188727374
Iter: 1177 loss: 0.00188705546
Iter: 1178 loss: 0.00188703125
Iter: 1179 loss: 0.00188709586
Iter: 1180 loss: 0.00188702287
Iter: 1181 loss: 0.00188700121
Iter: 1182 loss: 0.00188691937
Iter: 1183 loss: 0.00188685395
Iter: 1184 loss: 0.00188681285
Iter: 1185 loss: 0.00188663381
Iter: 1186 loss: 0.0018869522
Iter: 1187 loss: 0.00188655499
Iter: 1188 loss: 0.00188649434
Iter: 1189 loss: 0.0018864763
Iter: 1190 loss: 0.00188642577
Iter: 1191 loss: 0.00188671402
Iter: 1192 loss: 0.00188641914
Iter: 1193 loss: 0.00188639725
Iter: 1194 loss: 0.00188633415
Iter: 1195 loss: 0.00188627688
Iter: 1196 loss: 0.00188624673
Iter: 1197 loss: 0.00188615429
Iter: 1198 loss: 0.00188606139
Iter: 1199 loss: 0.00188604183
Iter: 1200 loss: 0.00188590935
Iter: 1201 loss: 0.00188568525
Iter: 1202 loss: 0.00188568537
Iter: 1203 loss: 0.00188560726
Iter: 1204 loss: 0.00188553403
Iter: 1205 loss: 0.00188551482
Iter: 1206 loss: 0.00188540155
Iter: 1207 loss: 0.00188641343
Iter: 1208 loss: 0.0018853941
Iter: 1209 loss: 0.00188536057
Iter: 1210 loss: 0.00188535475
Iter: 1211 loss: 0.0018853338
Iter: 1212 loss: 0.00188532157
Iter: 1213 loss: 0.00188530982
Iter: 1214 loss: 0.0018852544
Iter: 1215 loss: 0.00188513671
Iter: 1216 loss: 0.00188694499
Iter: 1217 loss: 0.00188512821
Iter: 1218 loss: 0.00188502669
Iter: 1219 loss: 0.00188524
Iter: 1220 loss: 0.00188498641
Iter: 1221 loss: 0.00188507349
Iter: 1222 loss: 0.00188493822
Iter: 1223 loss: 0.00188489177
Iter: 1224 loss: 0.00188504579
Iter: 1225 loss: 0.00188487885
Iter: 1226 loss: 0.00188483763
Iter: 1227 loss: 0.00188487
Iter: 1228 loss: 0.00188481284
Iter: 1229 loss: 0.00188475405
Iter: 1230 loss: 0.00188501494
Iter: 1231 loss: 0.00188474287
Iter: 1232 loss: 0.00188467675
Iter: 1233 loss: 0.00188469572
Iter: 1234 loss: 0.00188463368
Iter: 1235 loss: 0.00188458269
Iter: 1236 loss: 0.00188463612
Iter: 1237 loss: 0.00188455521
Iter: 1238 loss: 0.00188446394
Iter: 1239 loss: 0.00188427442
Iter: 1240 loss: 0.00188757339
Iter: 1241 loss: 0.00188427162
Iter: 1242 loss: 0.00188416406
Iter: 1243 loss: 0.00188412878
Iter: 1244 loss: 0.00188407989
Iter: 1245 loss: 0.00188403344
Iter: 1246 loss: 0.00188402226
Iter: 1247 loss: 0.00188410352
Iter: 1248 loss: 0.00188393891
Iter: 1249 loss: 0.0018838828
Iter: 1250 loss: 0.00188414229
Iter: 1251 loss: 0.00188387767
Iter: 1252 loss: 0.00188386044
Iter: 1253 loss: 0.00188380794
Iter: 1254 loss: 0.0018839261
Iter: 1255 loss: 0.00188377837
Iter: 1256 loss: 0.0018836481
Iter: 1257 loss: 0.00188364438
Iter: 1258 loss: 0.001883385
Iter: 1259 loss: 0.00188546663
Iter: 1260 loss: 0.0018833715
Iter: 1261 loss: 0.00188319676
Iter: 1262 loss: 0.00188426161
Iter: 1263 loss: 0.00188317569
Iter: 1264 loss: 0.00188305497
Iter: 1265 loss: 0.0018834736
Iter: 1266 loss: 0.00188302284
Iter: 1267 loss: 0.00188294589
Iter: 1268 loss: 0.00188276975
Iter: 1269 loss: 0.00188506418
Iter: 1270 loss: 0.00188275904
Iter: 1271 loss: 0.00188253925
Iter: 1272 loss: 0.00188246602
Iter: 1273 loss: 0.00188229908
Iter: 1274 loss: 0.00188410748
Iter: 1275 loss: 0.00188229722
Iter: 1276 loss: 0.00188223796
Iter: 1277 loss: 0.00188211841
Iter: 1278 loss: 0.0018843665
Iter: 1279 loss: 0.00188211654
Iter: 1280 loss: 0.0018819758
Iter: 1281 loss: 0.00188224169
Iter: 1282 loss: 0.0018819148
Iter: 1283 loss: 0.00188188627
Iter: 1284 loss: 0.00188194658
Iter: 1285 loss: 0.00188187312
Iter: 1286 loss: 0.00188182713
Iter: 1287 loss: 0.00188178557
Iter: 1288 loss: 0.00188171957
Iter: 1289 loss: 0.00188172143
Iter: 1290 loss: 0.00188166671
Iter: 1291 loss: 0.00188149838
Iter: 1292 loss: 0.00188159919
Iter: 1293 loss: 0.00188134238
Iter: 1294 loss: 0.00188090978
Iter: 1295 loss: 0.00188180176
Iter: 1296 loss: 0.00188073597
Iter: 1297 loss: 0.00188047951
Iter: 1298 loss: 0.00188227324
Iter: 1299 loss: 0.00188045716
Iter: 1300 loss: 0.00188032188
Iter: 1301 loss: 0.0018805177
Iter: 1302 loss: 0.0018802546
Iter: 1303 loss: 0.00188020791
Iter: 1304 loss: 0.00188017031
Iter: 1305 loss: 0.00188015238
Iter: 1306 loss: 0.00188020477
Iter: 1307 loss: 0.00188008579
Iter: 1308 loss: 0.00188003865
Iter: 1309 loss: 0.00188007986
Iter: 1310 loss: 0.0018800057
Iter: 1311 loss: 0.00187993713
Iter: 1312 loss: 0.0018798915
Iter: 1313 loss: 0.00187986391
Iter: 1314 loss: 0.00187969347
Iter: 1315 loss: 0.00187973119
Iter: 1316 loss: 0.00187956705
Iter: 1317 loss: 0.00187934423
Iter: 1318 loss: 0.0018814113
Iter: 1319 loss: 0.00187933375
Iter: 1320 loss: 0.00187920057
Iter: 1321 loss: 0.00187919964
Iter: 1322 loss: 0.00187908905
Iter: 1323 loss: 0.00188077113
Iter: 1324 loss: 0.00187908928
Iter: 1325 loss: 0.00187898544
Iter: 1326 loss: 0.001879049
Iter: 1327 loss: 0.00187892187
Iter: 1328 loss: 0.00187880686
Iter: 1329 loss: 0.00187877915
Iter: 1330 loss: 0.00187870779
Iter: 1331 loss: 0.00187856203
Iter: 1332 loss: 0.00188009744
Iter: 1333 loss: 0.00187855877
Iter: 1334 loss: 0.00187849661
Iter: 1335 loss: 0.00187867519
Iter: 1336 loss: 0.00187847926
Iter: 1337 loss: 0.00187845167
Iter: 1338 loss: 0.00187843456
Iter: 1339 loss: 0.00187842292
Iter: 1340 loss: 0.0018783561
Iter: 1341 loss: 0.00187828066
Iter: 1342 loss: 0.00187827274
Iter: 1343 loss: 0.00187818496
Iter: 1344 loss: 0.00187890104
Iter: 1345 loss: 0.00187817973
Iter: 1346 loss: 0.00187812396
Iter: 1347 loss: 0.00187801546
Iter: 1348 loss: 0.00188028323
Iter: 1349 loss: 0.0018780157
Iter: 1350 loss: 0.00187788322
Iter: 1351 loss: 0.00187756238
Iter: 1352 loss: 0.00188139721
Iter: 1353 loss: 0.00187753607
Iter: 1354 loss: 0.00187739905
Iter: 1355 loss: 0.00187811512
Iter: 1356 loss: 0.00187737891
Iter: 1357 loss: 0.001877337
Iter: 1358 loss: 0.00187731604
Iter: 1359 loss: 0.00187725562
Iter: 1360 loss: 0.00187710638
Iter: 1361 loss: 0.00187862781
Iter: 1362 loss: 0.00187708857
Iter: 1363 loss: 0.00187675096
Iter: 1364 loss: 0.00187809975
Iter: 1365 loss: 0.00187667611
Iter: 1366 loss: 0.00187652919
Iter: 1367 loss: 0.00187765644
Iter: 1368 loss: 0.00187651697
Iter: 1369 loss: 0.00187645259
Iter: 1370 loss: 0.0018765314
Iter: 1371 loss: 0.0018764179
Iter: 1372 loss: 0.00187640241
Iter: 1373 loss: 0.00187640171
Iter: 1374 loss: 0.00187638332
Iter: 1375 loss: 0.00187661825
Iter: 1376 loss: 0.00187638006
Iter: 1377 loss: 0.0018763605
Iter: 1378 loss: 0.00187632791
Iter: 1379 loss: 0.00187632674
Iter: 1380 loss: 0.00187625922
Iter: 1381 loss: 0.00187640369
Iter: 1382 loss: 0.00187623315
Iter: 1383 loss: 0.00187617063
Iter: 1384 loss: 0.00187626225
Iter: 1385 loss: 0.00187614153
Iter: 1386 loss: 0.00187600846
Iter: 1387 loss: 0.00187566807
Iter: 1388 loss: 0.00187886809
Iter: 1389 loss: 0.00187561801
Iter: 1390 loss: 0.00187573431
Iter: 1391 loss: 0.00187551067
Iter: 1392 loss: 0.00187539868
Iter: 1393 loss: 0.00187523104
Iter: 1394 loss: 0.00187522895
Iter: 1395 loss: 0.0018751428
Iter: 1396 loss: 0.00187509053
Iter: 1397 loss: 0.00187506666
Iter: 1398 loss: 0.00187506189
Iter: 1399 loss: 0.00187504257
Iter: 1400 loss: 0.0018750194
Iter: 1401 loss: 0.00187501789
Iter: 1402 loss: 0.00187496806
Iter: 1403 loss: 0.00187494303
Iter: 1404 loss: 0.0018749201
Iter: 1405 loss: 0.00187481777
Iter: 1406 loss: 0.00187543244
Iter: 1407 loss: 0.00187480426
Iter: 1408 loss: 0.00187468913
Iter: 1409 loss: 0.00187492371
Iter: 1410 loss: 0.00187464606
Iter: 1411 loss: 0.0018745556
Iter: 1412 loss: 0.00187455525
Iter: 1413 loss: 0.00187452172
Iter: 1414 loss: 0.00187456782
Iter: 1415 loss: 0.00187450624
Iter: 1416 loss: 0.00187445898
Iter: 1417 loss: 0.00187433278
Iter: 1418 loss: 0.00187518029
Iter: 1419 loss: 0.0018743024
Iter: 1420 loss: 0.00187416817
Iter: 1421 loss: 0.00187551812
Iter: 1422 loss: 0.00187416258
Iter: 1423 loss: 0.00187410333
Iter: 1424 loss: 0.00187410053
Iter: 1425 loss: 0.00187405944
Iter: 1426 loss: 0.00187392591
Iter: 1427 loss: 0.00187378144
Iter: 1428 loss: 0.00187373231
Iter: 1429 loss: 0.00187679823
Iter: 1430 loss: 0.00187370565
Iter: 1431 loss: 0.00187366863
Iter: 1432 loss: 0.00187379424
Iter: 1433 loss: 0.00187365781
Iter: 1434 loss: 0.00187364733
Iter: 1435 loss: 0.00187366153
Iter: 1436 loss: 0.00187364069
Iter: 1437 loss: 0.00187362148
Iter: 1438 loss: 0.00187357387
Iter: 1439 loss: 0.00187436049
Iter: 1440 loss: 0.00187357422
Iter: 1441 loss: 0.00187345513
Iter: 1442 loss: 0.001873628
Iter: 1443 loss: 0.0018733982
Iter: 1444 loss: 0.00187362276
Iter: 1445 loss: 0.0018732592
Iter: 1446 loss: 0.00187311904
Iter: 1447 loss: 0.00187334488
Iter: 1448 loss: 0.00187305338
Iter: 1449 loss: 0.00187299028
Iter: 1450 loss: 0.00187296944
Iter: 1451 loss: 0.00187288807
Iter: 1452 loss: 0.00187322102
Iter: 1453 loss: 0.00187286886
Iter: 1454 loss: 0.00187277119
Iter: 1455 loss: 0.00187258224
Iter: 1456 loss: 0.00187688274
Iter: 1457 loss: 0.00187258096
Iter: 1458 loss: 0.00187244941
Iter: 1459 loss: 0.00187310111
Iter: 1460 loss: 0.00187242602
Iter: 1461 loss: 0.00187231507
Iter: 1462 loss: 0.00187192089
Iter: 1463 loss: 0.00187138608
Iter: 1464 loss: 0.00187129271
Iter: 1465 loss: 0.00187543
Iter: 1466 loss: 0.00187126664
Iter: 1467 loss: 0.00187121518
Iter: 1468 loss: 0.00187154696
Iter: 1469 loss: 0.00187121099
Iter: 1470 loss: 0.00187115127
Iter: 1471 loss: 0.00187117653
Iter: 1472 loss: 0.00187111087
Iter: 1473 loss: 0.00187090435
Iter: 1474 loss: 0.00187027524
Iter: 1475 loss: 0.0018723004
Iter: 1476 loss: 0.00186995149
Iter: 1477 loss: 0.00186957477
Iter: 1478 loss: 0.0018743067
Iter: 1479 loss: 0.00186957128
Iter: 1480 loss: 0.00186951295
Iter: 1481 loss: 0.00186952786
Iter: 1482 loss: 0.0018694693
Iter: 1483 loss: 0.00186940399
Iter: 1484 loss: 0.00186933752
Iter: 1485 loss: 0.00186932739
Iter: 1486 loss: 0.00186922308
Iter: 1487 loss: 0.00186909828
Iter: 1488 loss: 0.00186908513
Iter: 1489 loss: 0.00187005079
Iter: 1490 loss: 0.00186889933
Iter: 1491 loss: 0.00186847325
Iter: 1492 loss: 0.00186918303
Iter: 1493 loss: 0.00186827895
Iter: 1494 loss: 0.00186804519
Iter: 1495 loss: 0.00186851795
Iter: 1496 loss: 0.00186795788
Iter: 1497 loss: 0.00186780014
Iter: 1498 loss: 0.00187035813
Iter: 1499 loss: 0.00186779955
Iter: 1500 loss: 0.00186752761
Iter: 1501 loss: 0.00186895032
Iter: 1502 loss: 0.0018674864
Iter: 1503 loss: 0.00186740269
Iter: 1504 loss: 0.001867285
Iter: 1505 loss: 0.00186728069
Iter: 1506 loss: 0.00186709058
Iter: 1507 loss: 0.00186703517
Iter: 1508 loss: 0.00186691957
Iter: 1509 loss: 0.00186682539
Iter: 1510 loss: 0.00186737056
Iter: 1511 loss: 0.00186681561
Iter: 1512 loss: 0.00186674274
Iter: 1513 loss: 0.00186670595
Iter: 1514 loss: 0.00186667335
Iter: 1515 loss: 0.00186651689
Iter: 1516 loss: 0.00186675438
Iter: 1517 loss: 0.00186643947
Iter: 1518 loss: 0.00186619628
Iter: 1519 loss: 0.00186835218
Iter: 1520 loss: 0.00186618906
Iter: 1521 loss: 0.00186610804
Iter: 1522 loss: 0.00186590781
Iter: 1523 loss: 0.00186828594
Iter: 1524 loss: 0.00186589372
Iter: 1525 loss: 0.00186580734
Iter: 1526 loss: 0.001866073
Iter: 1527 loss: 0.00186578277
Iter: 1528 loss: 0.00186570408
Iter: 1529 loss: 0.00186562166
Iter: 1530 loss: 0.00186560652
Iter: 1531 loss: 0.00186539139
Iter: 1532 loss: 0.00186516251
Iter: 1533 loss: 0.00186512584
Iter: 1534 loss: 0.00186946662
Iter: 1535 loss: 0.00186510419
Iter: 1536 loss: 0.00186508405
Iter: 1537 loss: 0.00186504843
Iter: 1538 loss: 0.00186505029
Iter: 1539 loss: 0.00186494808
Iter: 1540 loss: 0.00186521513
Iter: 1541 loss: 0.00186491222
Iter: 1542 loss: 0.00186486822
Iter: 1543 loss: 0.00186484028
Iter: 1544 loss: 0.00186478
Iter: 1545 loss: 0.00186477718
Iter: 1546 loss: 0.00186472945
Iter: 1547 loss: 0.00186469243
Iter: 1548 loss: 0.00186462793
Iter: 1549 loss: 0.00186462607
Iter: 1550 loss: 0.00186454249
Iter: 1551 loss: 0.00186430686
Iter: 1552 loss: 0.00186562585
Iter: 1553 loss: 0.00186423678
Iter: 1554 loss: 0.00186408707
Iter: 1555 loss: 0.00186608464
Iter: 1556 loss: 0.00186408695
Iter: 1557 loss: 0.00186402991
Iter: 1558 loss: 0.00186405261
Iter: 1559 loss: 0.00186399044
Iter: 1560 loss: 0.00186392991
Iter: 1561 loss: 0.00186389964
Iter: 1562 loss: 0.0018638738
Iter: 1563 loss: 0.00186383678
Iter: 1564 loss: 0.00186372525
Iter: 1565 loss: 0.00186417624
Iter: 1566 loss: 0.00186367892
Iter: 1567 loss: 0.00186361629
Iter: 1568 loss: 0.00186456635
Iter: 1569 loss: 0.00186361535
Iter: 1570 loss: 0.00186357298
Iter: 1571 loss: 0.00186354946
Iter: 1572 loss: 0.00186345982
Iter: 1573 loss: 0.00186327088
Iter: 1574 loss: 0.00186616299
Iter: 1575 loss: 0.00186326331
Iter: 1576 loss: 0.00186315179
Iter: 1577 loss: 0.00186313782
Iter: 1578 loss: 0.00186307018
Iter: 1579 loss: 0.00186300604
Iter: 1580 loss: 0.00186298683
Iter: 1581 loss: 0.00186291791
Iter: 1582 loss: 0.00186401349
Iter: 1583 loss: 0.0018629164
Iter: 1584 loss: 0.00186279719
Iter: 1585 loss: 0.00186325819
Iter: 1586 loss: 0.00186277053
Iter: 1587 loss: 0.00186267542
Iter: 1588 loss: 0.00186267623
Iter: 1589 loss: 0.00186260184
Iter: 1590 loss: 0.00186252128
Iter: 1591 loss: 0.00186301291
Iter: 1592 loss: 0.00186251104
Iter: 1593 loss: 0.00186248403
Iter: 1594 loss: 0.0018628235
Iter: 1595 loss: 0.00186248496
Iter: 1596 loss: 0.00186246308
Iter: 1597 loss: 0.0018624342
Iter: 1598 loss: 0.00186243327
Iter: 1599 loss: 0.00186241185
Iter: 1600 loss: 0.0018623611
Iter: 1601 loss: 0.00186287006
Iter: 1602 loss: 0.00186235644
Iter: 1603 loss: 0.00186225388
Iter: 1604 loss: 0.0018619739
Iter: 1605 loss: 0.00186396972
Iter: 1606 loss: 0.00186190591
Iter: 1607 loss: 0.00186179299
Iter: 1608 loss: 0.00186178694
Iter: 1609 loss: 0.0018616973
Iter: 1610 loss: 0.00186229567
Iter: 1611 loss: 0.00186168938
Iter: 1612 loss: 0.0018616264
Iter: 1613 loss: 0.00186156551
Iter: 1614 loss: 0.00186155248
Iter: 1615 loss: 0.001861508
Iter: 1616 loss: 0.00186136388
Iter: 1617 loss: 0.00186156598
Iter: 1618 loss: 0.00186125794
Iter: 1619 loss: 0.00186109752
Iter: 1620 loss: 0.00186156482
Iter: 1621 loss: 0.00186104607
Iter: 1622 loss: 0.00186086656
Iter: 1623 loss: 0.0018619867
Iter: 1624 loss: 0.00186084502
Iter: 1625 loss: 0.00186072709
Iter: 1626 loss: 0.00186068425
Iter: 1627 loss: 0.00186061836
Iter: 1628 loss: 0.00186061964
Iter: 1629 loss: 0.00186058145
Iter: 1630 loss: 0.00186056411
Iter: 1631 loss: 0.00186053955
Iter: 1632 loss: 0.0018605506
Iter: 1633 loss: 0.0018605229
Iter: 1634 loss: 0.00186048483
Iter: 1635 loss: 0.00186036038
Iter: 1636 loss: 0.00186022976
Iter: 1637 loss: 0.00186017901
Iter: 1638 loss: 0.00185996178
Iter: 1639 loss: 0.00186046527
Iter: 1640 loss: 0.00185987714
Iter: 1641 loss: 0.00185976608
Iter: 1642 loss: 0.00185994152
Iter: 1643 loss: 0.00185971544
Iter: 1644 loss: 0.00186825404
Iter: 1645 loss: 0.00185963581
Iter: 1646 loss: 0.00185948214
Iter: 1647 loss: 0.0018592804
Iter: 1648 loss: 0.00185926678
Iter: 1649 loss: 0.00185867283
Iter: 1650 loss: 0.00186027773
Iter: 1651 loss: 0.00185847469
Iter: 1652 loss: 0.00185809145
Iter: 1653 loss: 0.00185808947
Iter: 1654 loss: 0.00185778691
Iter: 1655 loss: 0.00185777771
Iter: 1656 loss: 0.00185757689
Iter: 1657 loss: 0.00185745535
Iter: 1658 loss: 0.00185744907
Iter: 1659 loss: 0.00185733126
Iter: 1660 loss: 0.00185847504
Iter: 1661 loss: 0.00185732776
Iter: 1662 loss: 0.00185737794
Iter: 1663 loss: 0.00185729994
Iter: 1664 loss: 0.0018572869
Iter: 1665 loss: 0.00185725489
Iter: 1666 loss: 0.00185741531
Iter: 1667 loss: 0.00185724441
Iter: 1668 loss: 0.00185720972
Iter: 1669 loss: 0.00185721251
Iter: 1670 loss: 0.00185718609
Iter: 1671 loss: 0.00185715349
Iter: 1672 loss: 0.00185704767
Iter: 1673 loss: 0.00185694627
Iter: 1674 loss: 0.00185689772
Iter: 1675 loss: 0.00185669318
Iter: 1676 loss: 0.00185850123
Iter: 1677 loss: 0.00185668387
Iter: 1678 loss: 0.00185659504
Iter: 1679 loss: 0.00185749354
Iter: 1680 loss: 0.00185659342
Iter: 1681 loss: 0.00185647083
Iter: 1682 loss: 0.00185641204
Iter: 1683 loss: 0.00185635302
Iter: 1684 loss: 0.00185606198
Iter: 1685 loss: 0.00185585755
Iter: 1686 loss: 0.00185574056
Iter: 1687 loss: 0.00185541098
Iter: 1688 loss: 0.00185616594
Iter: 1689 loss: 0.00185528467
Iter: 1690 loss: 0.00185507
Iter: 1691 loss: 0.00185561902
Iter: 1692 loss: 0.00185499794
Iter: 1693 loss: 0.00185501715
Iter: 1694 loss: 0.00185492798
Iter: 1695 loss: 0.00185489166
Iter: 1696 loss: 0.00185492262
Iter: 1697 loss: 0.00185487361
Iter: 1698 loss: 0.00185478455
Iter: 1699 loss: 0.00185496476
Iter: 1700 loss: 0.00185474916
Iter: 1701 loss: 0.00185459829
Iter: 1702 loss: 0.00185467652
Iter: 1703 loss: 0.00185449515
Iter: 1704 loss: 0.00185439654
Iter: 1705 loss: 0.00185563776
Iter: 1706 loss: 0.00185439538
Iter: 1707 loss: 0.0018543601
Iter: 1708 loss: 0.00185424066
Iter: 1709 loss: 0.00185421831
Iter: 1710 loss: 0.0018541062
Iter: 1711 loss: 0.00185394392
Iter: 1712 loss: 0.00185435067
Iter: 1713 loss: 0.00185388443
Iter: 1714 loss: 0.00185381435
Iter: 1715 loss: 0.00185365346
Iter: 1716 loss: 0.00185558666
Iter: 1717 loss: 0.00185364042
Iter: 1718 loss: 0.00185345369
Iter: 1719 loss: 0.00185362832
Iter: 1720 loss: 0.00185334939
Iter: 1721 loss: 0.00185329258
Iter: 1722 loss: 0.00185329083
Iter: 1723 loss: 0.00185324275
Iter: 1724 loss: 0.0018531594
Iter: 1725 loss: 0.00185315928
Iter: 1726 loss: 0.00185312051
Iter: 1727 loss: 0.00185304624
Iter: 1728 loss: 0.00185304577
Iter: 1729 loss: 0.00185288442
Iter: 1730 loss: 0.00185340736
Iter: 1731 loss: 0.00185284112
Iter: 1732 loss: 0.00185294577
Iter: 1733 loss: 0.00185273541
Iter: 1734 loss: 0.00185265031
Iter: 1735 loss: 0.00185401808
Iter: 1736 loss: 0.00185264787
Iter: 1737 loss: 0.00185261061
Iter: 1738 loss: 0.00185274833
Iter: 1739 loss: 0.00185259944
Iter: 1740 loss: 0.00185255171
Iter: 1741 loss: 0.00185240526
Iter: 1742 loss: 0.00185275241
Iter: 1743 loss: 0.00185231946
Iter: 1744 loss: 0.00185220526
Iter: 1745 loss: 0.00185393402
Iter: 1746 loss: 0.00185220502
Iter: 1747 loss: 0.00185206765
Iter: 1748 loss: 0.00185190362
Iter: 1749 loss: 0.00185188651
Iter: 1750 loss: 0.00185169175
Iter: 1751 loss: 0.00185169023
Iter: 1752 loss: 0.00185158546
Iter: 1753 loss: 0.00185260235
Iter: 1754 loss: 0.00185158022
Iter: 1755 loss: 0.00185146322
Iter: 1756 loss: 0.00185264123
Iter: 1757 loss: 0.00185146008
Iter: 1758 loss: 0.0018513184
Iter: 1759 loss: 0.00185104948
Iter: 1760 loss: 0.00185831077
Iter: 1761 loss: 0.00185104879
Iter: 1762 loss: 0.00185052084
Iter: 1763 loss: 0.00185239606
Iter: 1764 loss: 0.00185038964
Iter: 1765 loss: 0.00185012049
Iter: 1766 loss: 0.00185059884
Iter: 1767 loss: 0.00185000233
Iter: 1768 loss: 0.00184991607
Iter: 1769 loss: 0.00184991409
Iter: 1770 loss: 0.0018498681
Iter: 1771 loss: 0.00184993423
Iter: 1772 loss: 0.00184984913
Iter: 1773 loss: 0.00184974913
Iter: 1774 loss: 0.00185006973
Iter: 1775 loss: 0.00184971781
Iter: 1776 loss: 0.00184958102
Iter: 1777 loss: 0.00184934167
Iter: 1778 loss: 0.00184934144
Iter: 1779 loss: 0.00184903434
Iter: 1780 loss: 0.00184896821
Iter: 1781 loss: 0.00184876611
Iter: 1782 loss: 0.00184854213
Iter: 1783 loss: 0.00185216055
Iter: 1784 loss: 0.00184854085
Iter: 1785 loss: 0.00184841466
Iter: 1786 loss: 0.00184835074
Iter: 1787 loss: 0.00184828951
Iter: 1788 loss: 0.00184817892
Iter: 1789 loss: 0.00184797659
Iter: 1790 loss: 0.00185284764
Iter: 1791 loss: 0.00184797414
Iter: 1792 loss: 0.00184780953
Iter: 1793 loss: 0.00184930151
Iter: 1794 loss: 0.00184779707
Iter: 1795 loss: 0.00184766925
Iter: 1796 loss: 0.00184767006
Iter: 1797 loss: 0.0018476313
Iter: 1798 loss: 0.00184756017
Iter: 1799 loss: 0.00184903061
Iter: 1800 loss: 0.00184755621
Iter: 1801 loss: 0.00184751139
Iter: 1802 loss: 0.00184739474
Iter: 1803 loss: 0.00184864528
Iter: 1804 loss: 0.00184738147
Iter: 1805 loss: 0.00184718217
Iter: 1806 loss: 0.00184717146
Iter: 1807 loss: 0.00184692722
Iter: 1808 loss: 0.00184883527
Iter: 1809 loss: 0.00184690917
Iter: 1810 loss: 0.00184677052
Iter: 1811 loss: 0.00184785994
Iter: 1812 loss: 0.00184676156
Iter: 1813 loss: 0.00184669
Iter: 1814 loss: 0.00184699404
Iter: 1815 loss: 0.00184667169
Iter: 1816 loss: 0.00184653699
Iter: 1817 loss: 0.00184659846
Iter: 1818 loss: 0.00184644619
Iter: 1819 loss: 0.00184624479
Iter: 1820 loss: 0.00184574933
Iter: 1821 loss: 0.00185183901
Iter: 1822 loss: 0.00184569461
Iter: 1823 loss: 0.00184537924
Iter: 1824 loss: 0.00184782234
Iter: 1825 loss: 0.0018453541
Iter: 1826 loss: 0.0018452697
Iter: 1827 loss: 0.00184587
Iter: 1828 loss: 0.00184526492
Iter: 1829 loss: 0.00184515386
Iter: 1830 loss: 0.00184508064
Iter: 1831 loss: 0.00184504269
Iter: 1832 loss: 0.00184502464
Iter: 1833 loss: 0.00184494676
Iter: 1834 loss: 0.00184490066
Iter: 1835 loss: 0.00184486399
Iter: 1836 loss: 0.0018448506
Iter: 1837 loss: 0.00184474466
Iter: 1838 loss: 0.00184504641
Iter: 1839 loss: 0.00184470986
Iter: 1840 loss: 0.00184466341
Iter: 1841 loss: 0.0018451307
Iter: 1842 loss: 0.00184466271
Iter: 1843 loss: 0.00184462755
Iter: 1844 loss: 0.0018445095
Iter: 1845 loss: 0.00184447924
Iter: 1846 loss: 0.001844379
Iter: 1847 loss: 0.00184462045
Iter: 1848 loss: 0.0018443052
Iter: 1849 loss: 0.00184427528
Iter: 1850 loss: 0.00184431334
Iter: 1851 loss: 0.00184425595
Iter: 1852 loss: 0.00184423162
Iter: 1853 loss: 0.00184419227
Iter: 1854 loss: 0.00184419274
Iter: 1855 loss: 0.00184412696
Iter: 1856 loss: 0.00184403465
Iter: 1857 loss: 0.0018440315
Iter: 1858 loss: 0.00184391136
Iter: 1859 loss: 0.00184384512
Iter: 1860 loss: 0.00184379297
Iter: 1861 loss: 0.00184373057
Iter: 1862 loss: 0.00184376666
Iter: 1863 loss: 0.00184368948
Iter: 1864 loss: 0.00184368039
Iter: 1865 loss: 0.00184366736
Iter: 1866 loss: 0.00184364524
Iter: 1867 loss: 0.00184357318
Iter: 1868 loss: 0.00184347713
Iter: 1869 loss: 0.00184345548
Iter: 1870 loss: 0.00184327213
Iter: 1871 loss: 0.00184324849
Iter: 1872 loss: 0.00184365548
Iter: 1873 loss: 0.00184315396
Iter: 1874 loss: 0.00184308295
Iter: 1875 loss: 0.00184320961
Iter: 1876 loss: 0.00184305338
Iter: 1877 loss: 0.00184300402
Iter: 1878 loss: 0.00184294721
Iter: 1879 loss: 0.00184293813
Iter: 1880 loss: 0.00184283801
Iter: 1881 loss: 0.00184304442
Iter: 1882 loss: 0.00184279727
Iter: 1883 loss: 0.001842723
Iter: 1884 loss: 0.00184272206
Iter: 1885 loss: 0.00184267794
Iter: 1886 loss: 0.00184261845
Iter: 1887 loss: 0.0018426138
Iter: 1888 loss: 0.00184248667
Iter: 1889 loss: 0.00184251531
Iter: 1890 loss: 0.00184239354
Iter: 1891 loss: 0.00184225128
Iter: 1892 loss: 0.00184352626
Iter: 1893 loss: 0.00184224453
Iter: 1894 loss: 0.00184211601
Iter: 1895 loss: 0.00184218655
Iter: 1896 loss: 0.00184203184
Iter: 1897 loss: 0.00184291555
Iter: 1898 loss: 0.00184200727
Iter: 1899 loss: 0.00184199587
Iter: 1900 loss: 0.00184196467
Iter: 1901 loss: 0.00184210029
Iter: 1902 loss: 0.00184195233
Iter: 1903 loss: 0.00184188108
Iter: 1904 loss: 0.00184185943
Iter: 1905 loss: 0.00184181472
Iter: 1906 loss: 0.0018416841
Iter: 1907 loss: 0.00184278539
Iter: 1908 loss: 0.00184168061
Iter: 1909 loss: 0.00184150354
Iter: 1910 loss: 0.00184121926
Iter: 1911 loss: 0.00184121751
Iter: 1912 loss: 0.00184101285
Iter: 1913 loss: 0.00184116268
Iter: 1914 loss: 0.00184088899
Iter: 1915 loss: 0.00184074836
Iter: 1916 loss: 0.0018407352
Iter: 1917 loss: 0.00184063427
Iter: 1918 loss: 0.00184048363
Iter: 1919 loss: 0.00184048084
Iter: 1920 loss: 0.0018403749
Iter: 1921 loss: 0.00184090179
Iter: 1922 loss: 0.00184035499
Iter: 1923 loss: 0.00184027397
Iter: 1924 loss: 0.00184013904
Iter: 1925 loss: 0.00184013566
Iter: 1926 loss: 0.00183994556
Iter: 1927 loss: 0.00184093299
Iter: 1928 loss: 0.00183991343
Iter: 1929 loss: 0.00183986407
Iter: 1930 loss: 0.00183980563
Iter: 1931 loss: 0.00183960621
Iter: 1932 loss: 0.00183908886
Iter: 1933 loss: 0.00184482615
Iter: 1934 loss: 0.00183901424
Iter: 1935 loss: 0.00183892529
Iter: 1936 loss: 0.00183869083
Iter: 1937 loss: 0.00183861016
Iter: 1938 loss: 0.0018393246
Iter: 1939 loss: 0.00183860702
Iter: 1940 loss: 0.00183852948
Iter: 1941 loss: 0.00183849037
Iter: 1942 loss: 0.00183845684
Iter: 1943 loss: 0.00183823309
Iter: 1944 loss: 0.00183820201
Iter: 1945 loss: 0.00183803891
Iter: 1946 loss: 0.0018378594
Iter: 1947 loss: 0.00183784636
Iter: 1948 loss: 0.00183806405
Iter: 1949 loss: 0.0018376573
Iter: 1950 loss: 0.0018376
Iter: 1951 loss: 0.00183799828
Iter: 1952 loss: 0.00183759583
Iter: 1953 loss: 0.0018375311
Iter: 1954 loss: 0.00183741609
Iter: 1955 loss: 0.00184030272
Iter: 1956 loss: 0.00183741422
Iter: 1957 loss: 0.0018372772
Iter: 1958 loss: 0.0018369949
Iter: 1959 loss: 0.00184254174
Iter: 1960 loss: 0.0018369907
Iter: 1961 loss: 0.00183732156
Iter: 1962 loss: 0.00183691771
Iter: 1963 loss: 0.00183683459
Iter: 1964 loss: 0.00183684332
Iter: 1965 loss: 0.00183677045
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script72
+ '[' -r STOP.script72 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0/300_300_300_1 '
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi0.4
+ date
Sat Oct 31 14:25:45 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0/300_300_300_1 --function f1 --psi 2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 30 --learning_rate 0.001 --decay_rate 0.8 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec105252f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec104f2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec1065f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec10666ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec10571400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec10571950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec104728c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec104889d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec10488400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec10472048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec10259158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec103cf8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec103cf6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec104cbe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec104ba2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec1017dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec1018f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec10185268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec101ac8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec101ac840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec101d11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec101d1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec1020d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec1021a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec1021a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec1003a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec1003a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec10347620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec10347510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec1035a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec100de400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febc41631e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febc4149048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec102e2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec102c7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fec10099e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.02928257
test_loss: 0.029483255
train_loss: 0.022313833
test_loss: 0.022226341
train_loss: 0.017465606
test_loss: 0.01823455
train_loss: 0.015703304
test_loss: 0.016827974
train_loss: 0.015795445
test_loss: 0.016395656
train_loss: 0.015765866
test_loss: 0.01625196
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi0.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd152f3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd15216ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd152166a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd15216a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd15232400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd15232158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e79a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e7551e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e755730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e76a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e83a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e6e7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e6e7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e7168c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e6a69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e6a7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e6a7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e6747b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e5e9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e5e9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e5a4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e5a4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e5a5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e520950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e520840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e4d1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e49b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e48f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e48f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e56d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e416ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e3c2488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e3db488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e3c2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e37f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd0e3038c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.00041841148
Iter: 2 loss: 0.000422820449
Iter: 3 loss: 0.000415511546
Iter: 4 loss: 0.000414211245
Iter: 5 loss: 0.00041146687
Iter: 6 loss: 0.000457889924
Iter: 7 loss: 0.000411391782
Iter: 8 loss: 0.000409276923
Iter: 9 loss: 0.000409121567
Iter: 10 loss: 0.000407095591
Iter: 11 loss: 0.000404340215
Iter: 12 loss: 0.000404202263
Iter: 13 loss: 0.000401243946
Iter: 14 loss: 0.000412865484
Iter: 15 loss: 0.000400529883
Iter: 16 loss: 0.000397490279
Iter: 17 loss: 0.000409991189
Iter: 18 loss: 0.000396791555
Iter: 19 loss: 0.000394325296
Iter: 20 loss: 0.000395987823
Iter: 21 loss: 0.000392772286
Iter: 22 loss: 0.000390250061
Iter: 23 loss: 0.00039616006
Iter: 24 loss: 0.000389313383
Iter: 25 loss: 0.000386523636
Iter: 26 loss: 0.000405037688
Iter: 27 loss: 0.000386224769
Iter: 28 loss: 0.000384438608
Iter: 29 loss: 0.000381097954
Iter: 30 loss: 0.000454105786
Iter: 31 loss: 0.000381090154
Iter: 32 loss: 0.00037615074
Iter: 33 loss: 0.00039744348
Iter: 34 loss: 0.000375124393
Iter: 35 loss: 0.000371068
Iter: 36 loss: 0.000371022412
Iter: 37 loss: 0.00036903104
Iter: 38 loss: 0.000365005835
Iter: 39 loss: 0.000437360635
Iter: 40 loss: 0.000364942563
Iter: 41 loss: 0.000361293351
Iter: 42 loss: 0.000416716503
Iter: 43 loss: 0.000361290848
Iter: 44 loss: 0.000357755867
Iter: 45 loss: 0.000388305343
Iter: 46 loss: 0.000357552984
Iter: 47 loss: 0.000355527445
Iter: 48 loss: 0.000352207513
Iter: 49 loss: 0.000352190313
Iter: 50 loss: 0.000348281901
Iter: 51 loss: 0.00034828193
Iter: 52 loss: 0.00034533054
Iter: 53 loss: 0.000347933557
Iter: 54 loss: 0.0003435986
Iter: 55 loss: 0.000339299499
Iter: 56 loss: 0.000350161281
Iter: 57 loss: 0.000337760313
Iter: 58 loss: 0.000334645476
Iter: 59 loss: 0.000354628661
Iter: 60 loss: 0.00033429137
Iter: 61 loss: 0.000331488613
Iter: 62 loss: 0.000343913533
Iter: 63 loss: 0.000330927549
Iter: 64 loss: 0.000328685128
Iter: 65 loss: 0.000327428657
Iter: 66 loss: 0.000326433568
Iter: 67 loss: 0.000323194428
Iter: 68 loss: 0.000328391907
Iter: 69 loss: 0.000321679661
Iter: 70 loss: 0.000319489569
Iter: 71 loss: 0.000319364306
Iter: 72 loss: 0.000317064667
Iter: 73 loss: 0.000313206168
Iter: 74 loss: 0.000313195662
Iter: 75 loss: 0.000309489667
Iter: 76 loss: 0.000315571378
Iter: 77 loss: 0.000307789742
Iter: 78 loss: 0.00030564965
Iter: 79 loss: 0.000305210968
Iter: 80 loss: 0.000303227338
Iter: 81 loss: 0.000300027663
Iter: 82 loss: 0.000300004263
Iter: 83 loss: 0.000297635619
Iter: 84 loss: 0.000307387
Iter: 85 loss: 0.00029711629
Iter: 86 loss: 0.000294671685
Iter: 87 loss: 0.000315162149
Iter: 88 loss: 0.000294527563
Iter: 89 loss: 0.000292786164
Iter: 90 loss: 0.000290089112
Iter: 91 loss: 0.000290051976
Iter: 92 loss: 0.00028683088
Iter: 93 loss: 0.000333812379
Iter: 94 loss: 0.000286820286
Iter: 95 loss: 0.000284717273
Iter: 96 loss: 0.000281352113
Iter: 97 loss: 0.000281322747
Iter: 98 loss: 0.000279341533
Iter: 99 loss: 0.000279037224
Iter: 100 loss: 0.000277118204
Iter: 101 loss: 0.000274854392
Iter: 102 loss: 0.000274602586
Iter: 103 loss: 0.000271942117
Iter: 104 loss: 0.000277393527
Iter: 105 loss: 0.000270856486
Iter: 106 loss: 0.000268562755
Iter: 107 loss: 0.000268529286
Iter: 108 loss: 0.000267053256
Iter: 109 loss: 0.000264202827
Iter: 110 loss: 0.00032415794
Iter: 111 loss: 0.000264186587
Iter: 112 loss: 0.000261203793
Iter: 113 loss: 0.000271320605
Iter: 114 loss: 0.00026039037
Iter: 115 loss: 0.000257473352
Iter: 116 loss: 0.000292330253
Iter: 117 loss: 0.000257432344
Iter: 118 loss: 0.00025581973
Iter: 119 loss: 0.000253265956
Iter: 120 loss: 0.000253241393
Iter: 121 loss: 0.000250318961
Iter: 122 loss: 0.000255878462
Iter: 123 loss: 0.000249065692
Iter: 124 loss: 0.000247019
Iter: 125 loss: 0.000273378449
Iter: 126 loss: 0.000247004093
Iter: 127 loss: 0.000245367613
Iter: 128 loss: 0.000244776835
Iter: 129 loss: 0.000243868722
Iter: 130 loss: 0.000241323898
Iter: 131 loss: 0.000255261199
Iter: 132 loss: 0.000240935449
Iter: 133 loss: 0.000238680208
Iter: 134 loss: 0.000238130771
Iter: 135 loss: 0.000236692358
Iter: 136 loss: 0.000234933628
Iter: 137 loss: 0.00023482104
Iter: 138 loss: 0.000233028986
Iter: 139 loss: 0.000233625862
Iter: 140 loss: 0.000231760641
Iter: 141 loss: 0.000229634519
Iter: 142 loss: 0.000233185972
Iter: 143 loss: 0.00022866315
Iter: 144 loss: 0.000225660013
Iter: 145 loss: 0.000246377575
Iter: 146 loss: 0.00022536896
Iter: 147 loss: 0.000223893
Iter: 148 loss: 0.000224087547
Iter: 149 loss: 0.00022276552
Iter: 150 loss: 0.000221012146
Iter: 151 loss: 0.000241761809
Iter: 152 loss: 0.00022098748
Iter: 153 loss: 0.000219738169
Iter: 154 loss: 0.000217956287
Iter: 155 loss: 0.000217892069
Iter: 156 loss: 0.000215618027
Iter: 157 loss: 0.000217561144
Iter: 158 loss: 0.000214274187
Iter: 159 loss: 0.000212551371
Iter: 160 loss: 0.000230087404
Iter: 161 loss: 0.000212499537
Iter: 162 loss: 0.000210945451
Iter: 163 loss: 0.000213733176
Iter: 164 loss: 0.000210245766
Iter: 165 loss: 0.000208294543
Iter: 166 loss: 0.000209237711
Iter: 167 loss: 0.00020698685
Iter: 168 loss: 0.000204670214
Iter: 169 loss: 0.000204786164
Iter: 170 loss: 0.00020284018
Iter: 171 loss: 0.000200274255
Iter: 172 loss: 0.000217039429
Iter: 173 loss: 0.000200002774
Iter: 174 loss: 0.000198033886
Iter: 175 loss: 0.000222178263
Iter: 176 loss: 0.000198012727
Iter: 177 loss: 0.000196795139
Iter: 178 loss: 0.000194888125
Iter: 179 loss: 0.000194865424
Iter: 180 loss: 0.000193451444
Iter: 181 loss: 0.000193425221
Iter: 182 loss: 0.000192115345
Iter: 183 loss: 0.000190643885
Iter: 184 loss: 0.000190447317
Iter: 185 loss: 0.000188602411
Iter: 186 loss: 0.000190375533
Iter: 187 loss: 0.000187523037
Iter: 188 loss: 0.000186119636
Iter: 189 loss: 0.000208498575
Iter: 190 loss: 0.000186119651
Iter: 191 loss: 0.000184906123
Iter: 192 loss: 0.000186114688
Iter: 193 loss: 0.000184225646
Iter: 194 loss: 0.000182719421
Iter: 195 loss: 0.000183341384
Iter: 196 loss: 0.000181673298
Iter: 197 loss: 0.000179736395
Iter: 198 loss: 0.0001835117
Iter: 199 loss: 0.000178928487
Iter: 200 loss: 0.000177585433
Iter: 201 loss: 0.000177576818
Iter: 202 loss: 0.000176570175
Iter: 203 loss: 0.00017465993
Iter: 204 loss: 0.000217146458
Iter: 205 loss: 0.000174653513
Iter: 206 loss: 0.000172540545
Iter: 207 loss: 0.000181759839
Iter: 208 loss: 0.000172112414
Iter: 209 loss: 0.000170322659
Iter: 210 loss: 0.000175932015
Iter: 211 loss: 0.000169793639
Iter: 212 loss: 0.000168134284
Iter: 213 loss: 0.000173380642
Iter: 214 loss: 0.000167654
Iter: 215 loss: 0.000165961072
Iter: 216 loss: 0.000185688637
Iter: 217 loss: 0.00016593645
Iter: 218 loss: 0.000164931989
Iter: 219 loss: 0.000164217447
Iter: 220 loss: 0.000163855977
Iter: 221 loss: 0.000162309749
Iter: 222 loss: 0.000169025428
Iter: 223 loss: 0.000161996431
Iter: 224 loss: 0.000160612297
Iter: 225 loss: 0.000166419835
Iter: 226 loss: 0.000160318334
Iter: 227 loss: 0.000158960727
Iter: 228 loss: 0.000161646603
Iter: 229 loss: 0.000158393232
Iter: 230 loss: 0.000157188086
Iter: 231 loss: 0.000159293486
Iter: 232 loss: 0.000156656723
Iter: 233 loss: 0.000155398695
Iter: 234 loss: 0.000163680757
Iter: 235 loss: 0.000155266607
Iter: 236 loss: 0.000154192676
Iter: 237 loss: 0.000153383706
Iter: 238 loss: 0.000153029658
Iter: 239 loss: 0.000151670072
Iter: 240 loss: 0.000154608919
Iter: 241 loss: 0.000151144064
Iter: 242 loss: 0.000150126143
Iter: 243 loss: 0.000162668948
Iter: 244 loss: 0.000150115156
Iter: 245 loss: 0.000149274987
Iter: 246 loss: 0.000148766878
Iter: 247 loss: 0.00014841964
Iter: 248 loss: 0.000147239043
Iter: 249 loss: 0.000150032982
Iter: 250 loss: 0.000146808612
Iter: 251 loss: 0.000145429891
Iter: 252 loss: 0.000155334434
Iter: 253 loss: 0.000145310594
Iter: 254 loss: 0.000144268241
Iter: 255 loss: 0.000144613456
Iter: 256 loss: 0.000143524623
Iter: 257 loss: 0.000142449091
Iter: 258 loss: 0.000148209598
Iter: 259 loss: 0.000142286488
Iter: 260 loss: 0.000141465105
Iter: 261 loss: 0.000146178587
Iter: 262 loss: 0.000141354452
Iter: 263 loss: 0.000140536067
Iter: 264 loss: 0.000139933778
Iter: 265 loss: 0.00013965604
Iter: 266 loss: 0.000138691379
Iter: 267 loss: 0.000145856175
Iter: 268 loss: 0.000138613934
Iter: 269 loss: 0.000137727184
Iter: 270 loss: 0.000139630953
Iter: 271 loss: 0.000137380877
Iter: 272 loss: 0.000136652088
Iter: 273 loss: 0.000136644405
Iter: 274 loss: 0.000136064715
Iter: 275 loss: 0.000135134585
Iter: 276 loss: 0.000137852621
Iter: 277 loss: 0.000134847796
Iter: 278 loss: 0.000133714522
Iter: 279 loss: 0.000139595621
Iter: 280 loss: 0.000133532929
Iter: 281 loss: 0.000132690737
Iter: 282 loss: 0.000132861634
Iter: 283 loss: 0.000132061861
Iter: 284 loss: 0.000131091394
Iter: 285 loss: 0.000136148388
Iter: 286 loss: 0.000130937246
Iter: 287 loss: 0.000129967375
Iter: 288 loss: 0.000133628258
Iter: 289 loss: 0.000129734748
Iter: 290 loss: 0.000128943
Iter: 291 loss: 0.000130957298
Iter: 292 loss: 0.000128663582
Iter: 293 loss: 0.000127945212
Iter: 294 loss: 0.000129586362
Iter: 295 loss: 0.000127675768
Iter: 296 loss: 0.000126980216
Iter: 297 loss: 0.000131384499
Iter: 298 loss: 0.000126901825
Iter: 299 loss: 0.000126239553
Iter: 300 loss: 0.000125994935
Iter: 301 loss: 0.000125625214
Iter: 302 loss: 0.000124902261
Iter: 303 loss: 0.000130491739
Iter: 304 loss: 0.000124849437
Iter: 305 loss: 0.000124129598
Iter: 306 loss: 0.000123924721
Iter: 307 loss: 0.000123487131
Iter: 308 loss: 0.000122577127
Iter: 309 loss: 0.000123964943
Iter: 310 loss: 0.000122140773
Iter: 311 loss: 0.000121341429
Iter: 312 loss: 0.000128193904
Iter: 313 loss: 0.000121297824
Iter: 314 loss: 0.000120511708
Iter: 315 loss: 0.000121418227
Iter: 316 loss: 0.000120090903
Iter: 317 loss: 0.000119308759
Iter: 318 loss: 0.000119874559
Iter: 319 loss: 0.000118822543
Iter: 320 loss: 0.000118117154
Iter: 321 loss: 0.000126812403
Iter: 322 loss: 0.000118109565
Iter: 323 loss: 0.000117509582
Iter: 324 loss: 0.000118306838
Iter: 325 loss: 0.000117207099
Iter: 326 loss: 0.000116420364
Iter: 327 loss: 0.000117347408
Iter: 328 loss: 0.000115998351
Iter: 329 loss: 0.000115267052
Iter: 330 loss: 0.000118302996
Iter: 331 loss: 0.000115108298
Iter: 332 loss: 0.000114446775
Iter: 333 loss: 0.000118637421
Iter: 334 loss: 0.000114372844
Iter: 335 loss: 0.000113876464
Iter: 336 loss: 0.000113675589
Iter: 337 loss: 0.000113410097
Iter: 338 loss: 0.000112837064
Iter: 339 loss: 0.000117771378
Iter: 340 loss: 0.000112805712
Iter: 341 loss: 0.000112277325
Iter: 342 loss: 0.000111962756
Iter: 343 loss: 0.000111743226
Iter: 344 loss: 0.000110970701
Iter: 345 loss: 0.000111957423
Iter: 346 loss: 0.000110569221
Iter: 347 loss: 0.000109753135
Iter: 348 loss: 0.000117081639
Iter: 349 loss: 0.000109714209
Iter: 350 loss: 0.000109017623
Iter: 351 loss: 0.000109117325
Iter: 352 loss: 0.000108488486
Iter: 353 loss: 0.000107694279
Iter: 354 loss: 0.000108566004
Iter: 355 loss: 0.000107259344
Iter: 356 loss: 0.000106582323
Iter: 357 loss: 0.000106582062
Iter: 358 loss: 0.000106078682
Iter: 359 loss: 0.000106660664
Iter: 360 loss: 0.000105810075
Iter: 361 loss: 0.000105183863
Iter: 362 loss: 0.000106320935
Iter: 363 loss: 0.000104909763
Iter: 364 loss: 0.000104370672
Iter: 365 loss: 0.000105609332
Iter: 366 loss: 0.000104168335
Iter: 367 loss: 0.000103500664
Iter: 368 loss: 0.000106797132
Iter: 369 loss: 0.000103386992
Iter: 370 loss: 0.000102830658
Iter: 371 loss: 0.000102719088
Iter: 372 loss: 0.000102347738
Iter: 373 loss: 0.000101737292
Iter: 374 loss: 0.000106471569
Iter: 375 loss: 0.000101692633
Iter: 376 loss: 0.000101110476
Iter: 377 loss: 0.000101332029
Iter: 378 loss: 0.00010070538
Iter: 379 loss: 0.000100027275
Iter: 380 loss: 0.000101206126
Iter: 381 loss: 9.97230236e-05
Iter: 382 loss: 9.91163251e-05
Iter: 383 loss: 0.000104230916
Iter: 384 loss: 9.90811604e-05
Iter: 385 loss: 9.85437364e-05
Iter: 386 loss: 9.85114e-05
Iter: 387 loss: 9.81036719e-05
Iter: 388 loss: 9.74038121e-05
Iter: 389 loss: 9.79891411e-05
Iter: 390 loss: 9.69841494e-05
Iter: 391 loss: 9.62748964e-05
Iter: 392 loss: 9.89923e-05
Iter: 393 loss: 9.61070618e-05
Iter: 394 loss: 9.5491625e-05
Iter: 395 loss: 0.00010029455
Iter: 396 loss: 9.54473435e-05
Iter: 397 loss: 9.49876849e-05
Iter: 398 loss: 9.55670403e-05
Iter: 399 loss: 9.47484223e-05
Iter: 400 loss: 9.42424886e-05
Iter: 401 loss: 9.91706547e-05
Iter: 402 loss: 9.42249e-05
Iter: 403 loss: 9.38769299e-05
Iter: 404 loss: 9.36246361e-05
Iter: 405 loss: 9.35043499e-05
Iter: 406 loss: 9.29925154e-05
Iter: 407 loss: 9.46771e-05
Iter: 408 loss: 9.28503796e-05
Iter: 409 loss: 9.2272443e-05
Iter: 410 loss: 9.38060402e-05
Iter: 411 loss: 9.20793391e-05
Iter: 412 loss: 9.15547425e-05
Iter: 413 loss: 9.19952727e-05
Iter: 414 loss: 9.12402611e-05
Iter: 415 loss: 9.06872447e-05
Iter: 416 loss: 9.35414282e-05
Iter: 417 loss: 9.05984125e-05
Iter: 418 loss: 9.00396699e-05
Iter: 419 loss: 9.09779556e-05
Iter: 420 loss: 8.97876889e-05
Iter: 421 loss: 8.92211392e-05
Iter: 422 loss: 8.94888799e-05
Iter: 423 loss: 8.88375725e-05
Iter: 424 loss: 8.83185567e-05
Iter: 425 loss: 9.20892489e-05
Iter: 426 loss: 8.82745371e-05
Iter: 427 loss: 8.77641796e-05
Iter: 428 loss: 8.90327792e-05
Iter: 429 loss: 8.75845944e-05
Iter: 430 loss: 8.71236e-05
Iter: 431 loss: 8.74316756e-05
Iter: 432 loss: 8.68313218e-05
Iter: 433 loss: 8.63868e-05
Iter: 434 loss: 9.15672863e-05
Iter: 435 loss: 8.63803e-05
Iter: 436 loss: 8.60301516e-05
Iter: 437 loss: 8.62614761e-05
Iter: 438 loss: 8.58078129e-05
Iter: 439 loss: 8.53507154e-05
Iter: 440 loss: 8.65880211e-05
Iter: 441 loss: 8.52000958e-05
Iter: 442 loss: 8.47367337e-05
Iter: 443 loss: 8.50237484e-05
Iter: 444 loss: 8.44397509e-05
Iter: 445 loss: 8.39217828e-05
Iter: 446 loss: 8.4666186e-05
Iter: 447 loss: 8.36661347e-05
Iter: 448 loss: 8.31497382e-05
Iter: 449 loss: 8.70648219e-05
Iter: 450 loss: 8.31103534e-05
Iter: 451 loss: 8.26443138e-05
Iter: 452 loss: 8.39426066e-05
Iter: 453 loss: 8.24938e-05
Iter: 454 loss: 8.21004069e-05
Iter: 455 loss: 8.22891743e-05
Iter: 456 loss: 8.18351909e-05
Iter: 457 loss: 8.13748e-05
Iter: 458 loss: 8.34698658e-05
Iter: 459 loss: 8.12857324e-05
Iter: 460 loss: 8.07839097e-05
Iter: 461 loss: 8.21607173e-05
Iter: 462 loss: 8.06206735e-05
Iter: 463 loss: 8.02173818e-05
Iter: 464 loss: 8.05772361e-05
Iter: 465 loss: 7.99798e-05
Iter: 466 loss: 7.95958185e-05
Iter: 467 loss: 8.38742562e-05
Iter: 468 loss: 7.95885644e-05
Iter: 469 loss: 7.92687642e-05
Iter: 470 loss: 7.94966545e-05
Iter: 471 loss: 7.90689228e-05
Iter: 472 loss: 7.86988094e-05
Iter: 473 loss: 7.99480476e-05
Iter: 474 loss: 7.85987359e-05
Iter: 475 loss: 7.82547068e-05
Iter: 476 loss: 7.85665688e-05
Iter: 477 loss: 7.80547489e-05
Iter: 478 loss: 7.76733068e-05
Iter: 479 loss: 7.80704286e-05
Iter: 480 loss: 7.74602668e-05
Iter: 481 loss: 7.70539336e-05
Iter: 482 loss: 7.95419255e-05
Iter: 483 loss: 7.70051774e-05
Iter: 484 loss: 7.6642551e-05
Iter: 485 loss: 7.8223522e-05
Iter: 486 loss: 7.65687582e-05
Iter: 487 loss: 7.62598065e-05
Iter: 488 loss: 7.62148411e-05
Iter: 489 loss: 7.59983086e-05
Iter: 490 loss: 7.56156296e-05
Iter: 491 loss: 7.7965793e-05
Iter: 492 loss: 7.5569289e-05
Iter: 493 loss: 7.52219275e-05
Iter: 494 loss: 7.6570177e-05
Iter: 495 loss: 7.51410553e-05
Iter: 496 loss: 7.48516468e-05
Iter: 497 loss: 7.49010505e-05
Iter: 498 loss: 7.46330916e-05
Iter: 499 loss: 7.43251367e-05
Iter: 500 loss: 7.73911743e-05
Iter: 501 loss: 7.43151759e-05
Iter: 502 loss: 7.40429532e-05
Iter: 503 loss: 7.42172415e-05
Iter: 504 loss: 7.38683593e-05
Iter: 505 loss: 7.35420617e-05
Iter: 506 loss: 7.52315682e-05
Iter: 507 loss: 7.3489995e-05
Iter: 508 loss: 7.32136905e-05
Iter: 509 loss: 7.29583553e-05
Iter: 510 loss: 7.28917221e-05
Iter: 511 loss: 7.24514612e-05
Iter: 512 loss: 7.31298715e-05
Iter: 513 loss: 7.22421901e-05
Iter: 514 loss: 7.18284355e-05
Iter: 515 loss: 7.57311573e-05
Iter: 516 loss: 7.18115916e-05
Iter: 517 loss: 7.14770285e-05
Iter: 518 loss: 7.24121783e-05
Iter: 519 loss: 7.13694462e-05
Iter: 520 loss: 7.10686218e-05
Iter: 521 loss: 7.13662957e-05
Iter: 522 loss: 7.0897855e-05
Iter: 523 loss: 7.05756247e-05
Iter: 524 loss: 7.18773226e-05
Iter: 525 loss: 7.05033599e-05
Iter: 526 loss: 7.01841418e-05
Iter: 527 loss: 7.20862e-05
Iter: 528 loss: 7.01439e-05
Iter: 529 loss: 6.98835502e-05
Iter: 530 loss: 7.00771416e-05
Iter: 531 loss: 6.97223e-05
Iter: 532 loss: 6.94515e-05
Iter: 533 loss: 7.15641072e-05
Iter: 534 loss: 6.9432208e-05
Iter: 535 loss: 6.92186295e-05
Iter: 536 loss: 6.89869776e-05
Iter: 537 loss: 6.89508452e-05
Iter: 538 loss: 6.86004641e-05
Iter: 539 loss: 7.26284634e-05
Iter: 540 loss: 6.85948689e-05
Iter: 541 loss: 6.83662583e-05
Iter: 542 loss: 6.79262e-05
Iter: 543 loss: 7.72879866e-05
Iter: 544 loss: 6.7924e-05
Iter: 545 loss: 6.75894698e-05
Iter: 546 loss: 7.20668395e-05
Iter: 547 loss: 6.75878618e-05
Iter: 548 loss: 6.72341e-05
Iter: 549 loss: 6.71436355e-05
Iter: 550 loss: 6.69217916e-05
Iter: 551 loss: 6.65973057e-05
Iter: 552 loss: 6.8604917e-05
Iter: 553 loss: 6.65589469e-05
Iter: 554 loss: 6.62889433e-05
Iter: 555 loss: 6.72653696e-05
Iter: 556 loss: 6.62211678e-05
Iter: 557 loss: 6.59928337e-05
Iter: 558 loss: 6.57150231e-05
Iter: 559 loss: 6.56887933e-05
Iter: 560 loss: 6.53718e-05
Iter: 561 loss: 6.78319702e-05
Iter: 562 loss: 6.53490715e-05
Iter: 563 loss: 6.51108421e-05
Iter: 564 loss: 6.70893787e-05
Iter: 565 loss: 6.50960719e-05
Iter: 566 loss: 6.489105e-05
Iter: 567 loss: 6.50721631e-05
Iter: 568 loss: 6.47712e-05
Iter: 569 loss: 6.45275359e-05
Iter: 570 loss: 6.54750474e-05
Iter: 571 loss: 6.44699e-05
Iter: 572 loss: 6.42084487e-05
Iter: 573 loss: 6.51935552e-05
Iter: 574 loss: 6.41455408e-05
Iter: 575 loss: 6.39458449e-05
Iter: 576 loss: 6.4227e-05
Iter: 577 loss: 6.38471247e-05
Iter: 578 loss: 6.36168406e-05
Iter: 579 loss: 6.3389758e-05
Iter: 580 loss: 6.33411255e-05
Iter: 581 loss: 6.30600771e-05
Iter: 582 loss: 6.5784574e-05
Iter: 583 loss: 6.30497743e-05
Iter: 584 loss: 6.27759582e-05
Iter: 585 loss: 6.3307496e-05
Iter: 586 loss: 6.26616238e-05
Iter: 587 loss: 6.24477543e-05
Iter: 588 loss: 6.22165462e-05
Iter: 589 loss: 6.21804793e-05
Iter: 590 loss: 6.19360508e-05
Iter: 591 loss: 6.19237398e-05
Iter: 592 loss: 6.17345577e-05
Iter: 593 loss: 6.1565268e-05
Iter: 594 loss: 6.15169629e-05
Iter: 595 loss: 6.1253224e-05
Iter: 596 loss: 6.1679e-05
Iter: 597 loss: 6.11307914e-05
Iter: 598 loss: 6.09346171e-05
Iter: 599 loss: 6.09166163e-05
Iter: 600 loss: 6.07826514e-05
Iter: 601 loss: 6.08243936e-05
Iter: 602 loss: 6.0686496e-05
Iter: 603 loss: 6.05027708e-05
Iter: 604 loss: 6.11418291e-05
Iter: 605 loss: 6.04549859e-05
Iter: 606 loss: 6.02565342e-05
Iter: 607 loss: 6.04435845e-05
Iter: 608 loss: 6.01423744e-05
Iter: 609 loss: 5.99089544e-05
Iter: 610 loss: 5.98785809e-05
Iter: 611 loss: 5.97127291e-05
Iter: 612 loss: 5.94651501e-05
Iter: 613 loss: 6.02885193e-05
Iter: 614 loss: 5.93971054e-05
Iter: 615 loss: 5.91377175e-05
Iter: 616 loss: 6.07073453e-05
Iter: 617 loss: 5.91055e-05
Iter: 618 loss: 5.89041556e-05
Iter: 619 loss: 5.87165414e-05
Iter: 620 loss: 5.86688766e-05
Iter: 621 loss: 5.84286026e-05
Iter: 622 loss: 6.06375506e-05
Iter: 623 loss: 5.84178633e-05
Iter: 624 loss: 5.81813438e-05
Iter: 625 loss: 5.8763726e-05
Iter: 626 loss: 5.80972192e-05
Iter: 627 loss: 5.78870531e-05
Iter: 628 loss: 5.76237944e-05
Iter: 629 loss: 5.76023303e-05
Iter: 630 loss: 5.75621962e-05
Iter: 631 loss: 5.74592777e-05
Iter: 632 loss: 5.73065627e-05
Iter: 633 loss: 5.74052392e-05
Iter: 634 loss: 5.7208712e-05
Iter: 635 loss: 5.70292104e-05
Iter: 636 loss: 5.7688394e-05
Iter: 637 loss: 5.69848162e-05
Iter: 638 loss: 5.68284158e-05
Iter: 639 loss: 5.718381e-05
Iter: 640 loss: 5.67692659e-05
Iter: 641 loss: 5.661061e-05
Iter: 642 loss: 5.65509508e-05
Iter: 643 loss: 5.64637267e-05
Iter: 644 loss: 5.62661953e-05
Iter: 645 loss: 5.64496695e-05
Iter: 646 loss: 5.61520501e-05
Iter: 647 loss: 5.59543e-05
Iter: 648 loss: 5.84514382e-05
Iter: 649 loss: 5.59524269e-05
Iter: 650 loss: 5.57899621e-05
Iter: 651 loss: 5.57131243e-05
Iter: 652 loss: 5.56336126e-05
Iter: 653 loss: 5.54194921e-05
Iter: 654 loss: 5.56096275e-05
Iter: 655 loss: 5.52940764e-05
Iter: 656 loss: 5.50776349e-05
Iter: 657 loss: 5.81038476e-05
Iter: 658 loss: 5.50769473e-05
Iter: 659 loss: 5.49287579e-05
Iter: 660 loss: 5.47644231e-05
Iter: 661 loss: 5.47411073e-05
Iter: 662 loss: 5.45017465e-05
Iter: 663 loss: 5.47920754e-05
Iter: 664 loss: 5.43762362e-05
Iter: 665 loss: 5.44307695e-05
Iter: 666 loss: 5.42584858e-05
Iter: 667 loss: 5.41750851e-05
Iter: 668 loss: 5.41508489e-05
Iter: 669 loss: 5.4100612e-05
Iter: 670 loss: 5.39605244e-05
Iter: 671 loss: 5.4113214e-05
Iter: 672 loss: 5.38836466e-05
Iter: 673 loss: 5.37391606e-05
Iter: 674 loss: 5.40199762e-05
Iter: 675 loss: 5.36789521e-05
Iter: 676 loss: 5.35016952e-05
Iter: 677 loss: 5.33989405e-05
Iter: 678 loss: 5.33233833e-05
Iter: 679 loss: 5.31270925e-05
Iter: 680 loss: 5.37388951e-05
Iter: 681 loss: 5.30695324e-05
Iter: 682 loss: 5.2833173e-05
Iter: 683 loss: 5.4260363e-05
Iter: 684 loss: 5.28037926e-05
Iter: 685 loss: 5.26516269e-05
Iter: 686 loss: 5.2516727e-05
Iter: 687 loss: 5.24773131e-05
Iter: 688 loss: 5.22840019e-05
Iter: 689 loss: 5.47161326e-05
Iter: 690 loss: 5.22822447e-05
Iter: 691 loss: 5.21011461e-05
Iter: 692 loss: 5.2226369e-05
Iter: 693 loss: 5.19876121e-05
Iter: 694 loss: 5.17929111e-05
Iter: 695 loss: 5.18660527e-05
Iter: 696 loss: 5.16569344e-05
Iter: 697 loss: 5.14595122e-05
Iter: 698 loss: 5.26412477e-05
Iter: 699 loss: 5.14338753e-05
Iter: 700 loss: 5.13076884e-05
Iter: 701 loss: 5.12970219e-05
Iter: 702 loss: 5.12338738e-05
Iter: 703 loss: 5.11654362e-05
Iter: 704 loss: 5.11547878e-05
Iter: 705 loss: 5.10161772e-05
Iter: 706 loss: 5.09629936e-05
Iter: 707 loss: 5.0887651e-05
Iter: 708 loss: 5.0714887e-05
Iter: 709 loss: 5.20137291e-05
Iter: 710 loss: 5.07013392e-05
Iter: 711 loss: 5.05588505e-05
Iter: 712 loss: 5.03637129e-05
Iter: 713 loss: 5.03545307e-05
Iter: 714 loss: 5.0146391e-05
Iter: 715 loss: 5.07962759e-05
Iter: 716 loss: 5.00856331e-05
Iter: 717 loss: 4.98812224e-05
Iter: 718 loss: 5.24027419e-05
Iter: 719 loss: 4.98791342e-05
Iter: 720 loss: 4.97433393e-05
Iter: 721 loss: 4.9541035e-05
Iter: 722 loss: 4.95364366e-05
Iter: 723 loss: 4.93350599e-05
Iter: 724 loss: 5.08193625e-05
Iter: 725 loss: 4.93183034e-05
Iter: 726 loss: 4.91095234e-05
Iter: 727 loss: 4.98821319e-05
Iter: 728 loss: 4.90576422e-05
Iter: 729 loss: 4.88897495e-05
Iter: 730 loss: 4.87062862e-05
Iter: 731 loss: 4.86787103e-05
Iter: 732 loss: 4.84932534e-05
Iter: 733 loss: 5.06812539e-05
Iter: 734 loss: 4.84901138e-05
Iter: 735 loss: 4.83203767e-05
Iter: 736 loss: 5.03586198e-05
Iter: 737 loss: 4.8318172e-05
Iter: 738 loss: 4.8263697e-05
Iter: 739 loss: 4.81699753e-05
Iter: 740 loss: 4.81698735e-05
Iter: 741 loss: 4.80132367e-05
Iter: 742 loss: 4.82617615e-05
Iter: 743 loss: 4.79402515e-05
Iter: 744 loss: 4.78057664e-05
Iter: 745 loss: 4.78939801e-05
Iter: 746 loss: 4.77206704e-05
Iter: 747 loss: 4.75612542e-05
Iter: 748 loss: 4.96408029e-05
Iter: 749 loss: 4.75601955e-05
Iter: 750 loss: 4.74533e-05
Iter: 751 loss: 4.73224791e-05
Iter: 752 loss: 4.731015e-05
Iter: 753 loss: 4.7037538e-05
Iter: 754 loss: 4.77524918e-05
Iter: 755 loss: 4.69451625e-05
Iter: 756 loss: 4.6834728e-05
Iter: 757 loss: 4.67901627e-05
Iter: 758 loss: 4.66748897e-05
Iter: 759 loss: 4.66119418e-05
Iter: 760 loss: 4.65601333e-05
Iter: 761 loss: 4.640712e-05
Iter: 762 loss: 4.63231481e-05
Iter: 763 loss: 4.62555581e-05
Iter: 764 loss: 4.60921801e-05
Iter: 765 loss: 4.64003169e-05
Iter: 766 loss: 4.6020541e-05
Iter: 767 loss: 4.61449526e-05
Iter: 768 loss: 4.59560797e-05
Iter: 769 loss: 4.59108778e-05
Iter: 770 loss: 4.58040377e-05
Iter: 771 loss: 4.70598789e-05
Iter: 772 loss: 4.5794e-05
Iter: 773 loss: 4.5644374e-05
Iter: 774 loss: 4.59343282e-05
Iter: 775 loss: 4.55819354e-05
Iter: 776 loss: 4.54132896e-05
Iter: 777 loss: 4.53003813e-05
Iter: 778 loss: 4.52373715e-05
Iter: 779 loss: 4.50176121e-05
Iter: 780 loss: 4.64729601e-05
Iter: 781 loss: 4.49944309e-05
Iter: 782 loss: 4.47872153e-05
Iter: 783 loss: 4.59647636e-05
Iter: 784 loss: 4.47583516e-05
Iter: 785 loss: 4.46126796e-05
Iter: 786 loss: 4.46433769e-05
Iter: 787 loss: 4.45057849e-05
Iter: 788 loss: 4.43567333e-05
Iter: 789 loss: 4.51291162e-05
Iter: 790 loss: 4.43326207e-05
Iter: 791 loss: 4.4192202e-05
Iter: 792 loss: 4.47311395e-05
Iter: 793 loss: 4.41583834e-05
Iter: 794 loss: 4.40400254e-05
Iter: 795 loss: 4.38470379e-05
Iter: 796 loss: 4.38460411e-05
Iter: 797 loss: 4.37011622e-05
Iter: 798 loss: 4.38653624e-05
Iter: 799 loss: 4.36227274e-05
Iter: 800 loss: 4.34902868e-05
Iter: 801 loss: 4.3486576e-05
Iter: 802 loss: 4.33830101e-05
Iter: 803 loss: 4.3252141e-05
Iter: 804 loss: 4.41261946e-05
Iter: 805 loss: 4.3239037e-05
Iter: 806 loss: 4.31438311e-05
Iter: 807 loss: 4.45807964e-05
Iter: 808 loss: 4.31439039e-05
Iter: 809 loss: 4.30853761e-05
Iter: 810 loss: 4.29530555e-05
Iter: 811 loss: 4.47511266e-05
Iter: 812 loss: 4.29450738e-05
Iter: 813 loss: 4.28046842e-05
Iter: 814 loss: 4.39929208e-05
Iter: 815 loss: 4.27964806e-05
Iter: 816 loss: 4.26769038e-05
Iter: 817 loss: 4.27096784e-05
Iter: 818 loss: 4.2590309e-05
Iter: 819 loss: 4.24581158e-05
Iter: 820 loss: 4.23505262e-05
Iter: 821 loss: 4.23112651e-05
Iter: 822 loss: 4.21672885e-05
Iter: 823 loss: 4.23753299e-05
Iter: 824 loss: 4.20968936e-05
Iter: 825 loss: 4.19733478e-05
Iter: 826 loss: 4.23283e-05
Iter: 827 loss: 4.19333701e-05
Iter: 828 loss: 4.18409218e-05
Iter: 829 loss: 4.18388045e-05
Iter: 830 loss: 4.17729607e-05
Iter: 831 loss: 4.17055635e-05
Iter: 832 loss: 4.16927069e-05
Iter: 833 loss: 4.15739232e-05
Iter: 834 loss: 4.20292781e-05
Iter: 835 loss: 4.15458126e-05
Iter: 836 loss: 4.14315655e-05
Iter: 837 loss: 4.14351598e-05
Iter: 838 loss: 4.1341209e-05
Iter: 839 loss: 4.12591398e-05
Iter: 840 loss: 4.12588379e-05
Iter: 841 loss: 4.11599976e-05
Iter: 842 loss: 4.11365909e-05
Iter: 843 loss: 4.1073341e-05
Iter: 844 loss: 4.09836139e-05
Iter: 845 loss: 4.09462664e-05
Iter: 846 loss: 4.08994929e-05
Iter: 847 loss: 4.07849475e-05
Iter: 848 loss: 4.14287206e-05
Iter: 849 loss: 4.07689186e-05
Iter: 850 loss: 4.06574909e-05
Iter: 851 loss: 4.08363703e-05
Iter: 852 loss: 4.06061336e-05
Iter: 853 loss: 4.04946477e-05
Iter: 854 loss: 4.06181134e-05
Iter: 855 loss: 4.04340317e-05
Iter: 856 loss: 4.03213e-05
Iter: 857 loss: 4.17208e-05
Iter: 858 loss: 4.03200684e-05
Iter: 859 loss: 4.02368823e-05
Iter: 860 loss: 4.00883146e-05
Iter: 861 loss: 4.38060233e-05
Iter: 862 loss: 4.00882709e-05
Iter: 863 loss: 4.00441349e-05
Iter: 864 loss: 4.00007557e-05
Iter: 865 loss: 3.99387718e-05
Iter: 866 loss: 3.98326e-05
Iter: 867 loss: 3.98325828e-05
Iter: 868 loss: 3.97440199e-05
Iter: 869 loss: 3.97438416e-05
Iter: 870 loss: 3.96930336e-05
Iter: 871 loss: 3.96877367e-05
Iter: 872 loss: 3.96507603e-05
Iter: 873 loss: 3.95807583e-05
Iter: 874 loss: 4.06573672e-05
Iter: 875 loss: 3.95806564e-05
Iter: 876 loss: 3.95510287e-05
Iter: 877 loss: 3.9475326e-05
Iter: 878 loss: 4.01355283e-05
Iter: 879 loss: 3.94631461e-05
Iter: 880 loss: 3.93741138e-05
Iter: 881 loss: 3.94693234e-05
Iter: 882 loss: 3.93251357e-05
Iter: 883 loss: 3.92145375e-05
Iter: 884 loss: 4.00581521e-05
Iter: 885 loss: 3.92060792e-05
Iter: 886 loss: 3.90863679e-05
Iter: 887 loss: 3.8982238e-05
Iter: 888 loss: 3.89502165e-05
Iter: 889 loss: 3.8809696e-05
Iter: 890 loss: 3.98845332e-05
Iter: 891 loss: 3.87989567e-05
Iter: 892 loss: 3.86775e-05
Iter: 893 loss: 3.93726841e-05
Iter: 894 loss: 3.86610627e-05
Iter: 895 loss: 3.85955609e-05
Iter: 896 loss: 3.87086475e-05
Iter: 897 loss: 3.85664498e-05
Iter: 898 loss: 3.84747618e-05
Iter: 899 loss: 3.88821463e-05
Iter: 900 loss: 3.84567174e-05
Iter: 901 loss: 3.84085542e-05
Iter: 902 loss: 3.84639425e-05
Iter: 903 loss: 3.83827719e-05
Iter: 904 loss: 3.8316095e-05
Iter: 905 loss: 3.84844316e-05
Iter: 906 loss: 3.8293023e-05
Iter: 907 loss: 3.82577964e-05
Iter: 908 loss: 3.82564613e-05
Iter: 909 loss: 3.82261787e-05
Iter: 910 loss: 3.81485515e-05
Iter: 911 loss: 3.88038898e-05
Iter: 912 loss: 3.81355203e-05
Iter: 913 loss: 3.80558595e-05
Iter: 914 loss: 3.81376303e-05
Iter: 915 loss: 3.80114e-05
Iter: 916 loss: 3.79137e-05
Iter: 917 loss: 3.83902698e-05
Iter: 918 loss: 3.7896607e-05
Iter: 919 loss: 3.78049226e-05
Iter: 920 loss: 3.82698709e-05
Iter: 921 loss: 3.77897231e-05
Iter: 922 loss: 3.77036922e-05
Iter: 923 loss: 3.76817115e-05
Iter: 924 loss: 3.76276294e-05
Iter: 925 loss: 3.75364252e-05
Iter: 926 loss: 3.85768144e-05
Iter: 927 loss: 3.75348682e-05
Iter: 928 loss: 3.74553783e-05
Iter: 929 loss: 3.76611133e-05
Iter: 930 loss: 3.74283773e-05
Iter: 931 loss: 3.73744697e-05
Iter: 932 loss: 3.80324564e-05
Iter: 933 loss: 3.73737494e-05
Iter: 934 loss: 3.73335643e-05
Iter: 935 loss: 3.72784161e-05
Iter: 936 loss: 3.72758877e-05
Iter: 937 loss: 3.72151189e-05
Iter: 938 loss: 3.80289639e-05
Iter: 939 loss: 3.7214777e-05
Iter: 940 loss: 3.71816714e-05
Iter: 941 loss: 3.74846932e-05
Iter: 942 loss: 3.7180198e-05
Iter: 943 loss: 3.71443857e-05
Iter: 944 loss: 3.70791313e-05
Iter: 945 loss: 3.86400716e-05
Iter: 946 loss: 3.70791313e-05
Iter: 947 loss: 3.70234411e-05
Iter: 948 loss: 3.7012469e-05
Iter: 949 loss: 3.69753907e-05
Iter: 950 loss: 3.68994661e-05
Iter: 951 loss: 3.72801078e-05
Iter: 952 loss: 3.68867e-05
Iter: 953 loss: 3.68177389e-05
Iter: 954 loss: 3.71633359e-05
Iter: 955 loss: 3.68062465e-05
Iter: 956 loss: 3.67366847e-05
Iter: 957 loss: 3.67691755e-05
Iter: 958 loss: 3.66895802e-05
Iter: 959 loss: 3.66151653e-05
Iter: 960 loss: 3.70624766e-05
Iter: 961 loss: 3.66058593e-05
Iter: 962 loss: 3.65391861e-05
Iter: 963 loss: 3.68912479e-05
Iter: 964 loss: 3.65286942e-05
Iter: 965 loss: 3.64850057e-05
Iter: 966 loss: 3.68400433e-05
Iter: 967 loss: 3.6482219e-05
Iter: 968 loss: 3.64434236e-05
Iter: 969 loss: 3.63890213e-05
Iter: 970 loss: 3.63868676e-05
Iter: 971 loss: 3.63397194e-05
Iter: 972 loss: 3.70341841e-05
Iter: 973 loss: 3.63397558e-05
Iter: 974 loss: 3.63057516e-05
Iter: 975 loss: 3.63700638e-05
Iter: 976 loss: 3.62913052e-05
Iter: 977 loss: 3.6240468e-05
Iter: 978 loss: 3.6354897e-05
Iter: 979 loss: 3.62212159e-05
Iter: 980 loss: 3.61938764e-05
Iter: 981 loss: 3.61292587e-05
Iter: 982 loss: 3.68667825e-05
Iter: 983 loss: 3.6123176e-05
Iter: 984 loss: 3.60487284e-05
Iter: 985 loss: 3.63995314e-05
Iter: 986 loss: 3.60350314e-05
Iter: 987 loss: 3.59699916e-05
Iter: 988 loss: 3.64143198e-05
Iter: 989 loss: 3.59635815e-05
Iter: 990 loss: 3.5904e-05
Iter: 991 loss: 3.60261911e-05
Iter: 992 loss: 3.58802245e-05
Iter: 993 loss: 3.58215257e-05
Iter: 994 loss: 3.58636025e-05
Iter: 995 loss: 3.57851095e-05
Iter: 996 loss: 3.57222671e-05
Iter: 997 loss: 3.65948945e-05
Iter: 998 loss: 3.57221361e-05
Iter: 999 loss: 3.56854653e-05
Iter: 1000 loss: 3.57866375e-05
Iter: 1001 loss: 3.56736855e-05
Iter: 1002 loss: 3.56286291e-05
Iter: 1003 loss: 3.56826e-05
Iter: 1004 loss: 3.56050368e-05
Iter: 1005 loss: 3.55715601e-05
Iter: 1006 loss: 3.56737073e-05
Iter: 1007 loss: 3.55616285e-05
Iter: 1008 loss: 3.55239681e-05
Iter: 1009 loss: 3.5667e-05
Iter: 1010 loss: 3.55150514e-05
Iter: 1011 loss: 3.54807853e-05
Iter: 1012 loss: 3.57384633e-05
Iter: 1013 loss: 3.54780932e-05
Iter: 1014 loss: 3.54588192e-05
Iter: 1015 loss: 3.54076547e-05
Iter: 1016 loss: 3.57778954e-05
Iter: 1017 loss: 3.53966716e-05
Iter: 1018 loss: 3.53408905e-05
Iter: 1019 loss: 3.55833035e-05
Iter: 1020 loss: 3.53295327e-05
Iter: 1021 loss: 3.52717689e-05
Iter: 1022 loss: 3.54077347e-05
Iter: 1023 loss: 3.5250534e-05
Iter: 1024 loss: 3.51819835e-05
Iter: 1025 loss: 3.55311495e-05
Iter: 1026 loss: 3.51708259e-05
Iter: 1027 loss: 3.5115e-05
Iter: 1028 loss: 3.51617819e-05
Iter: 1029 loss: 3.50817827e-05
Iter: 1030 loss: 3.50371265e-05
Iter: 1031 loss: 3.50372e-05
Iter: 1032 loss: 3.50024056e-05
Iter: 1033 loss: 3.50969858e-05
Iter: 1034 loss: 3.49910079e-05
Iter: 1035 loss: 3.49554721e-05
Iter: 1036 loss: 3.50545815e-05
Iter: 1037 loss: 3.49440888e-05
Iter: 1038 loss: 3.49167967e-05
Iter: 1039 loss: 3.49375841e-05
Iter: 1040 loss: 3.49002366e-05
Iter: 1041 loss: 3.48631511e-05
Iter: 1042 loss: 3.50633309e-05
Iter: 1043 loss: 3.48575777e-05
Iter: 1044 loss: 3.48278e-05
Iter: 1045 loss: 3.51159979e-05
Iter: 1046 loss: 3.48267495e-05
Iter: 1047 loss: 3.48066242e-05
Iter: 1048 loss: 3.47505265e-05
Iter: 1049 loss: 3.50627961e-05
Iter: 1050 loss: 3.47338573e-05
Iter: 1051 loss: 3.46682427e-05
Iter: 1052 loss: 3.48567773e-05
Iter: 1053 loss: 3.4647579e-05
Iter: 1054 loss: 3.4581426e-05
Iter: 1055 loss: 3.49525762e-05
Iter: 1056 loss: 3.45721564e-05
Iter: 1057 loss: 3.45217632e-05
Iter: 1058 loss: 3.48856556e-05
Iter: 1059 loss: 3.45173685e-05
Iter: 1060 loss: 3.44742548e-05
Iter: 1061 loss: 3.44670443e-05
Iter: 1062 loss: 3.44374457e-05
Iter: 1063 loss: 3.43873435e-05
Iter: 1064 loss: 3.48610629e-05
Iter: 1065 loss: 3.43854881e-05
Iter: 1066 loss: 3.43409702e-05
Iter: 1067 loss: 3.45213e-05
Iter: 1068 loss: 3.43309621e-05
Iter: 1069 loss: 3.42933708e-05
Iter: 1070 loss: 3.44830696e-05
Iter: 1071 loss: 3.42871572e-05
Iter: 1072 loss: 3.42600179e-05
Iter: 1073 loss: 3.4228804e-05
Iter: 1074 loss: 3.42250387e-05
Iter: 1075 loss: 3.41796622e-05
Iter: 1076 loss: 3.4646393e-05
Iter: 1077 loss: 3.41783307e-05
Iter: 1078 loss: 3.41500781e-05
Iter: 1079 loss: 3.43461579e-05
Iter: 1080 loss: 3.41473497e-05
Iter: 1081 loss: 3.41207e-05
Iter: 1082 loss: 3.40620645e-05
Iter: 1083 loss: 3.49290858e-05
Iter: 1084 loss: 3.40594743e-05
Iter: 1085 loss: 3.40108818e-05
Iter: 1086 loss: 3.4058794e-05
Iter: 1087 loss: 3.39835133e-05
Iter: 1088 loss: 3.39317339e-05
Iter: 1089 loss: 3.42401036e-05
Iter: 1090 loss: 3.39252038e-05
Iter: 1091 loss: 3.38791469e-05
Iter: 1092 loss: 3.4062432e-05
Iter: 1093 loss: 3.3868906e-05
Iter: 1094 loss: 3.38163372e-05
Iter: 1095 loss: 3.38645295e-05
Iter: 1096 loss: 3.37859237e-05
Iter: 1097 loss: 3.37302e-05
Iter: 1098 loss: 3.39176549e-05
Iter: 1099 loss: 3.37149249e-05
Iter: 1100 loss: 3.36667326e-05
Iter: 1101 loss: 3.43344364e-05
Iter: 1102 loss: 3.36665689e-05
Iter: 1103 loss: 3.36387275e-05
Iter: 1104 loss: 3.37324236e-05
Iter: 1105 loss: 3.36311386e-05
Iter: 1106 loss: 3.36040321e-05
Iter: 1107 loss: 3.35721597e-05
Iter: 1108 loss: 3.35686e-05
Iter: 1109 loss: 3.35348341e-05
Iter: 1110 loss: 3.35349396e-05
Iter: 1111 loss: 3.3512144e-05
Iter: 1112 loss: 3.36051962e-05
Iter: 1113 loss: 3.35071381e-05
Iter: 1114 loss: 3.34809229e-05
Iter: 1115 loss: 3.34446195e-05
Iter: 1116 loss: 3.34429897e-05
Iter: 1117 loss: 3.34058313e-05
Iter: 1118 loss: 3.33675816e-05
Iter: 1119 loss: 3.33606877e-05
Iter: 1120 loss: 3.33034259e-05
Iter: 1121 loss: 3.36994344e-05
Iter: 1122 loss: 3.32979034e-05
Iter: 1123 loss: 3.3247321e-05
Iter: 1124 loss: 3.34086726e-05
Iter: 1125 loss: 3.32327327e-05
Iter: 1126 loss: 3.31774936e-05
Iter: 1127 loss: 3.33625649e-05
Iter: 1128 loss: 3.31623123e-05
Iter: 1129 loss: 3.31147457e-05
Iter: 1130 loss: 3.31836709e-05
Iter: 1131 loss: 3.309155e-05
Iter: 1132 loss: 3.30542862e-05
Iter: 1133 loss: 3.30535549e-05
Iter: 1134 loss: 3.30268776e-05
Iter: 1135 loss: 3.31048359e-05
Iter: 1136 loss: 3.3018594e-05
Iter: 1137 loss: 3.29917457e-05
Iter: 1138 loss: 3.29649338e-05
Iter: 1139 loss: 3.29594259e-05
Iter: 1140 loss: 3.29290197e-05
Iter: 1141 loss: 3.33251592e-05
Iter: 1142 loss: 3.29289323e-05
Iter: 1143 loss: 3.29030736e-05
Iter: 1144 loss: 3.29248869e-05
Iter: 1145 loss: 3.28877941e-05
Iter: 1146 loss: 3.28450624e-05
Iter: 1147 loss: 3.28821152e-05
Iter: 1148 loss: 3.28198657e-05
Iter: 1149 loss: 3.27867674e-05
Iter: 1150 loss: 3.27267335e-05
Iter: 1151 loss: 3.41698906e-05
Iter: 1152 loss: 3.27267517e-05
Iter: 1153 loss: 3.26634181e-05
Iter: 1154 loss: 3.30523e-05
Iter: 1155 loss: 3.2655822e-05
Iter: 1156 loss: 3.26052796e-05
Iter: 1157 loss: 3.29347095e-05
Iter: 1158 loss: 3.25998481e-05
Iter: 1159 loss: 3.25582441e-05
Iter: 1160 loss: 3.27127891e-05
Iter: 1161 loss: 3.2548e-05
Iter: 1162 loss: 3.25048313e-05
Iter: 1163 loss: 3.24889625e-05
Iter: 1164 loss: 3.24648936e-05
Iter: 1165 loss: 3.24306748e-05
Iter: 1166 loss: 3.24271e-05
Iter: 1167 loss: 3.23955828e-05
Iter: 1168 loss: 3.24463836e-05
Iter: 1169 loss: 3.23810309e-05
Iter: 1170 loss: 3.23430577e-05
Iter: 1171 loss: 3.23576678e-05
Iter: 1172 loss: 3.23167842e-05
Iter: 1173 loss: 3.22837623e-05
Iter: 1174 loss: 3.24533285e-05
Iter: 1175 loss: 3.22783817e-05
Iter: 1176 loss: 3.22436535e-05
Iter: 1177 loss: 3.23174318e-05
Iter: 1178 loss: 3.22299093e-05
Iter: 1179 loss: 3.21922525e-05
Iter: 1180 loss: 3.23476197e-05
Iter: 1181 loss: 3.21840926e-05
Iter: 1182 loss: 3.2160191e-05
Iter: 1183 loss: 3.21058833e-05
Iter: 1184 loss: 3.28200149e-05
Iter: 1185 loss: 3.21022926e-05
Iter: 1186 loss: 3.20456129e-05
Iter: 1187 loss: 3.22601e-05
Iter: 1188 loss: 3.20320614e-05
Iter: 1189 loss: 3.19802784e-05
Iter: 1190 loss: 3.22485794e-05
Iter: 1191 loss: 3.19720348e-05
Iter: 1192 loss: 3.1923104e-05
Iter: 1193 loss: 3.21063126e-05
Iter: 1194 loss: 3.19111859e-05
Iter: 1195 loss: 3.18589846e-05
Iter: 1196 loss: 3.18968741e-05
Iter: 1197 loss: 3.18267921e-05
Iter: 1198 loss: 3.17777776e-05
Iter: 1199 loss: 3.22385167e-05
Iter: 1200 loss: 3.17756021e-05
Iter: 1201 loss: 3.17296654e-05
Iter: 1202 loss: 3.20332911e-05
Iter: 1203 loss: 3.17249069e-05
Iter: 1204 loss: 3.16934747e-05
Iter: 1205 loss: 3.17174345e-05
Iter: 1206 loss: 3.1674077e-05
Iter: 1207 loss: 3.16436126e-05
Iter: 1208 loss: 3.17177764e-05
Iter: 1209 loss: 3.16326841e-05
Iter: 1210 loss: 3.15969446e-05
Iter: 1211 loss: 3.17272061e-05
Iter: 1212 loss: 3.15878642e-05
Iter: 1213 loss: 3.15576399e-05
Iter: 1214 loss: 3.17309605e-05
Iter: 1215 loss: 3.15536199e-05
Iter: 1216 loss: 3.153133e-05
Iter: 1217 loss: 3.1472875e-05
Iter: 1218 loss: 3.19171377e-05
Iter: 1219 loss: 3.14609679e-05
Iter: 1220 loss: 3.13912242e-05
Iter: 1221 loss: 3.15942707e-05
Iter: 1222 loss: 3.13696146e-05
Iter: 1223 loss: 3.13005294e-05
Iter: 1224 loss: 3.17012891e-05
Iter: 1225 loss: 3.12914344e-05
Iter: 1226 loss: 3.12328048e-05
Iter: 1227 loss: 3.15192556e-05
Iter: 1228 loss: 3.12225238e-05
Iter: 1229 loss: 3.11679905e-05
Iter: 1230 loss: 3.12702177e-05
Iter: 1231 loss: 3.11446674e-05
Iter: 1232 loss: 3.10983451e-05
Iter: 1233 loss: 3.13495366e-05
Iter: 1234 loss: 3.10914475e-05
Iter: 1235 loss: 3.10527357e-05
Iter: 1236 loss: 3.15763536e-05
Iter: 1237 loss: 3.10525e-05
Iter: 1238 loss: 3.10262112e-05
Iter: 1239 loss: 3.10361793e-05
Iter: 1240 loss: 3.10077521e-05
Iter: 1241 loss: 3.09773823e-05
Iter: 1242 loss: 3.09571187e-05
Iter: 1243 loss: 3.09456e-05
Iter: 1244 loss: 3.08979943e-05
Iter: 1245 loss: 3.13577912e-05
Iter: 1246 loss: 3.0896208e-05
Iter: 1247 loss: 3.08675444e-05
Iter: 1248 loss: 3.09092502e-05
Iter: 1249 loss: 3.08534654e-05
Iter: 1250 loss: 3.08113886e-05
Iter: 1251 loss: 3.07447081e-05
Iter: 1252 loss: 3.0744035e-05
Iter: 1253 loss: 3.06857546e-05
Iter: 1254 loss: 3.07260416e-05
Iter: 1255 loss: 3.06492111e-05
Iter: 1256 loss: 3.05830436e-05
Iter: 1257 loss: 3.10422765e-05
Iter: 1258 loss: 3.05768772e-05
Iter: 1259 loss: 3.05276772e-05
Iter: 1260 loss: 3.08578528e-05
Iter: 1261 loss: 3.05228423e-05
Iter: 1262 loss: 3.04821988e-05
Iter: 1263 loss: 3.05753711e-05
Iter: 1264 loss: 3.04669229e-05
Iter: 1265 loss: 3.04205914e-05
Iter: 1266 loss: 3.04208916e-05
Iter: 1267 loss: 3.0383344e-05
Iter: 1268 loss: 3.03632787e-05
Iter: 1269 loss: 3.0349589e-05
Iter: 1270 loss: 3.03259712e-05
Iter: 1271 loss: 3.03100496e-05
Iter: 1272 loss: 3.03012057e-05
Iter: 1273 loss: 3.02607987e-05
Iter: 1274 loss: 3.02971057e-05
Iter: 1275 loss: 3.02373373e-05
Iter: 1276 loss: 3.01985874e-05
Iter: 1277 loss: 3.07621776e-05
Iter: 1278 loss: 3.01985183e-05
Iter: 1279 loss: 3.01723485e-05
Iter: 1280 loss: 3.01700093e-05
Iter: 1281 loss: 3.01507116e-05
Iter: 1282 loss: 3.01072905e-05
Iter: 1283 loss: 3.0140136e-05
Iter: 1284 loss: 3.00807842e-05
Iter: 1285 loss: 3.00466781e-05
Iter: 1286 loss: 3.00055235e-05
Iter: 1287 loss: 3.00015708e-05
Iter: 1288 loss: 2.99449748e-05
Iter: 1289 loss: 3.02120861e-05
Iter: 1290 loss: 2.99344974e-05
Iter: 1291 loss: 2.98794839e-05
Iter: 1292 loss: 3.01369582e-05
Iter: 1293 loss: 2.98694013e-05
Iter: 1294 loss: 2.98182495e-05
Iter: 1295 loss: 3.00006704e-05
Iter: 1296 loss: 2.98050545e-05
Iter: 1297 loss: 2.97534079e-05
Iter: 1298 loss: 2.98126179e-05
Iter: 1299 loss: 2.97258084e-05
Iter: 1300 loss: 2.97000952e-05
Iter: 1301 loss: 2.96948019e-05
Iter: 1302 loss: 2.96639228e-05
Iter: 1303 loss: 2.96599355e-05
Iter: 1304 loss: 2.96379203e-05
Iter: 1305 loss: 2.96014296e-05
Iter: 1306 loss: 2.96075668e-05
Iter: 1307 loss: 2.95735917e-05
Iter: 1308 loss: 2.95343598e-05
Iter: 1309 loss: 3.01019099e-05
Iter: 1310 loss: 2.95342361e-05
Iter: 1311 loss: 2.95056725e-05
Iter: 1312 loss: 2.94594356e-05
Iter: 1313 loss: 2.94591209e-05
Iter: 1314 loss: 2.94075289e-05
Iter: 1315 loss: 2.99035637e-05
Iter: 1316 loss: 2.94056063e-05
Iter: 1317 loss: 2.93719877e-05
Iter: 1318 loss: 2.92926416e-05
Iter: 1319 loss: 3.0243853e-05
Iter: 1320 loss: 2.92856948e-05
Iter: 1321 loss: 2.92043223e-05
Iter: 1322 loss: 2.96296166e-05
Iter: 1323 loss: 2.91913148e-05
Iter: 1324 loss: 2.91277793e-05
Iter: 1325 loss: 2.99075182e-05
Iter: 1326 loss: 2.91271281e-05
Iter: 1327 loss: 2.90826138e-05
Iter: 1328 loss: 2.91789038e-05
Iter: 1329 loss: 2.90653552e-05
Iter: 1330 loss: 2.90106382e-05
Iter: 1331 loss: 2.91490396e-05
Iter: 1332 loss: 2.89915315e-05
Iter: 1333 loss: 2.89570489e-05
Iter: 1334 loss: 2.94570964e-05
Iter: 1335 loss: 2.89569234e-05
Iter: 1336 loss: 2.89208365e-05
Iter: 1337 loss: 2.90439257e-05
Iter: 1338 loss: 2.89106338e-05
Iter: 1339 loss: 2.88869323e-05
Iter: 1340 loss: 2.88844676e-05
Iter: 1341 loss: 2.88672054e-05
Iter: 1342 loss: 2.883868e-05
Iter: 1343 loss: 2.89673517e-05
Iter: 1344 loss: 2.88329811e-05
Iter: 1345 loss: 2.88013871e-05
Iter: 1346 loss: 2.87985549e-05
Iter: 1347 loss: 2.8775321e-05
Iter: 1348 loss: 2.87424627e-05
Iter: 1349 loss: 2.87894491e-05
Iter: 1350 loss: 2.87265666e-05
Iter: 1351 loss: 2.86791146e-05
Iter: 1352 loss: 2.88286719e-05
Iter: 1353 loss: 2.86655031e-05
Iter: 1354 loss: 2.86196446e-05
Iter: 1355 loss: 2.86072172e-05
Iter: 1356 loss: 2.85786846e-05
Iter: 1357 loss: 2.85308106e-05
Iter: 1358 loss: 2.85412334e-05
Iter: 1359 loss: 2.84955713e-05
Iter: 1360 loss: 2.84382877e-05
Iter: 1361 loss: 2.84264534e-05
Iter: 1362 loss: 2.83885311e-05
Iter: 1363 loss: 2.83019726e-05
Iter: 1364 loss: 2.9101162e-05
Iter: 1365 loss: 2.829816e-05
Iter: 1366 loss: 2.82445326e-05
Iter: 1367 loss: 2.82997826e-05
Iter: 1368 loss: 2.82145302e-05
Iter: 1369 loss: 2.83612408e-05
Iter: 1370 loss: 2.81905541e-05
Iter: 1371 loss: 2.81678549e-05
Iter: 1372 loss: 2.81602061e-05
Iter: 1373 loss: 2.81471803e-05
Iter: 1374 loss: 2.81173507e-05
Iter: 1375 loss: 2.80700788e-05
Iter: 1376 loss: 2.80695494e-05
Iter: 1377 loss: 2.79870164e-05
Iter: 1378 loss: 2.80929416e-05
Iter: 1379 loss: 2.79447449e-05
Iter: 1380 loss: 2.78458938e-05
Iter: 1381 loss: 2.84323178e-05
Iter: 1382 loss: 2.78332482e-05
Iter: 1383 loss: 2.7742266e-05
Iter: 1384 loss: 2.8168346e-05
Iter: 1385 loss: 2.77251984e-05
Iter: 1386 loss: 2.76515784e-05
Iter: 1387 loss: 2.79525957e-05
Iter: 1388 loss: 2.76353549e-05
Iter: 1389 loss: 2.75643233e-05
Iter: 1390 loss: 2.78853568e-05
Iter: 1391 loss: 2.7550479e-05
Iter: 1392 loss: 2.74993326e-05
Iter: 1393 loss: 2.75977181e-05
Iter: 1394 loss: 2.74779286e-05
Iter: 1395 loss: 2.74293197e-05
Iter: 1396 loss: 2.75672646e-05
Iter: 1397 loss: 2.74140475e-05
Iter: 1398 loss: 2.73697879e-05
Iter: 1399 loss: 2.73779897e-05
Iter: 1400 loss: 2.73367114e-05
Iter: 1401 loss: 2.73727492e-05
Iter: 1402 loss: 2.73206479e-05
Iter: 1403 loss: 2.73022542e-05
Iter: 1404 loss: 2.73350706e-05
Iter: 1405 loss: 2.72940888e-05
Iter: 1406 loss: 2.72801626e-05
Iter: 1407 loss: 2.72402613e-05
Iter: 1408 loss: 2.74359518e-05
Iter: 1409 loss: 2.72268189e-05
Iter: 1410 loss: 2.71779863e-05
Iter: 1411 loss: 2.71713943e-05
Iter: 1412 loss: 2.71368281e-05
Iter: 1413 loss: 2.70778655e-05
Iter: 1414 loss: 2.76926185e-05
Iter: 1415 loss: 2.70764122e-05
Iter: 1416 loss: 2.70159289e-05
Iter: 1417 loss: 2.70905839e-05
Iter: 1418 loss: 2.69841021e-05
Iter: 1419 loss: 2.69190386e-05
Iter: 1420 loss: 2.70172022e-05
Iter: 1421 loss: 2.68878903e-05
Iter: 1422 loss: 2.68214826e-05
Iter: 1423 loss: 2.73231908e-05
Iter: 1424 loss: 2.68162476e-05
Iter: 1425 loss: 2.67628966e-05
Iter: 1426 loss: 2.67983596e-05
Iter: 1427 loss: 2.67292326e-05
Iter: 1428 loss: 2.66743464e-05
Iter: 1429 loss: 2.71339959e-05
Iter: 1430 loss: 2.66711322e-05
Iter: 1431 loss: 2.66241041e-05
Iter: 1432 loss: 2.68029617e-05
Iter: 1433 loss: 2.661279e-05
Iter: 1434 loss: 2.65840372e-05
Iter: 1435 loss: 2.65841336e-05
Iter: 1436 loss: 2.65555318e-05
Iter: 1437 loss: 2.68469103e-05
Iter: 1438 loss: 2.65546551e-05
Iter: 1439 loss: 2.6540898e-05
Iter: 1440 loss: 2.65104791e-05
Iter: 1441 loss: 2.69756565e-05
Iter: 1442 loss: 2.6509093e-05
Iter: 1443 loss: 2.64631963e-05
Iter: 1444 loss: 2.64341252e-05
Iter: 1445 loss: 2.64161172e-05
Iter: 1446 loss: 2.63258735e-05
Iter: 1447 loss: 2.63258225e-05
Iter: 1448 loss: 2.62437843e-05
Iter: 1449 loss: 2.69256379e-05
Iter: 1450 loss: 2.62383219e-05
Iter: 1451 loss: 2.61887799e-05
Iter: 1452 loss: 2.62268914e-05
Iter: 1453 loss: 2.61585064e-05
Iter: 1454 loss: 2.61192581e-05
Iter: 1455 loss: 2.6215e-05
Iter: 1456 loss: 2.61049245e-05
Iter: 1457 loss: 2.60774723e-05
Iter: 1458 loss: 2.60456472e-05
Iter: 1459 loss: 2.60420802e-05
Iter: 1460 loss: 2.59926364e-05
Iter: 1461 loss: 2.62144677e-05
Iter: 1462 loss: 2.59828703e-05
Iter: 1463 loss: 2.59345779e-05
Iter: 1464 loss: 2.60960878e-05
Iter: 1465 loss: 2.59209664e-05
Iter: 1466 loss: 2.5892743e-05
Iter: 1467 loss: 2.63097445e-05
Iter: 1468 loss: 2.58928067e-05
Iter: 1469 loss: 2.58662512e-05
Iter: 1470 loss: 2.61054847e-05
Iter: 1471 loss: 2.58648161e-05
Iter: 1472 loss: 2.58495238e-05
Iter: 1473 loss: 2.5820973e-05
Iter: 1474 loss: 2.64777518e-05
Iter: 1475 loss: 2.58209e-05
Iter: 1476 loss: 2.57836327e-05
Iter: 1477 loss: 2.57333413e-05
Iter: 1478 loss: 2.5730631e-05
Iter: 1479 loss: 2.56626063e-05
Iter: 1480 loss: 2.57943248e-05
Iter: 1481 loss: 2.56344229e-05
Iter: 1482 loss: 2.55687883e-05
Iter: 1483 loss: 2.6221529e-05
Iter: 1484 loss: 2.55667383e-05
Iter: 1485 loss: 2.55098148e-05
Iter: 1486 loss: 2.56760723e-05
Iter: 1487 loss: 2.54919287e-05
Iter: 1488 loss: 2.54420083e-05
Iter: 1489 loss: 2.56919229e-05
Iter: 1490 loss: 2.54334955e-05
Iter: 1491 loss: 2.53921025e-05
Iter: 1492 loss: 2.56284875e-05
Iter: 1493 loss: 2.53861799e-05
Iter: 1494 loss: 2.53487724e-05
Iter: 1495 loss: 2.55223731e-05
Iter: 1496 loss: 2.53418075e-05
Iter: 1497 loss: 2.53205289e-05
Iter: 1498 loss: 2.5286281e-05
Iter: 1499 loss: 2.52859027e-05
Iter: 1500 loss: 2.52685022e-05
Iter: 1501 loss: 2.5268384e-05
Iter: 1502 loss: 2.5258154e-05
Iter: 1503 loss: 2.52818554e-05
Iter: 1504 loss: 2.52544341e-05
Iter: 1505 loss: 2.52391819e-05
Iter: 1506 loss: 2.52231e-05
Iter: 1507 loss: 2.52204973e-05
Iter: 1508 loss: 2.51887795e-05
Iter: 1509 loss: 2.54285133e-05
Iter: 1510 loss: 2.51862821e-05
Iter: 1511 loss: 2.51506535e-05
Iter: 1512 loss: 2.51130477e-05
Iter: 1513 loss: 2.51066576e-05
Iter: 1514 loss: 2.50573557e-05
Iter: 1515 loss: 2.5156216e-05
Iter: 1516 loss: 2.50371249e-05
Iter: 1517 loss: 2.49847017e-05
Iter: 1518 loss: 2.52175978e-05
Iter: 1519 loss: 2.49742261e-05
Iter: 1520 loss: 2.49370605e-05
Iter: 1521 loss: 2.51154779e-05
Iter: 1522 loss: 2.49305449e-05
Iter: 1523 loss: 2.48956949e-05
Iter: 1524 loss: 2.48797041e-05
Iter: 1525 loss: 2.48624674e-05
Iter: 1526 loss: 2.48234464e-05
Iter: 1527 loss: 2.49509612e-05
Iter: 1528 loss: 2.48126489e-05
Iter: 1529 loss: 2.47790558e-05
Iter: 1530 loss: 2.51056681e-05
Iter: 1531 loss: 2.47778407e-05
Iter: 1532 loss: 2.47602748e-05
Iter: 1533 loss: 2.47598909e-05
Iter: 1534 loss: 2.47464013e-05
Iter: 1535 loss: 2.47666067e-05
Iter: 1536 loss: 2.47398657e-05
Iter: 1537 loss: 2.4726236e-05
Iter: 1538 loss: 2.47002608e-05
Iter: 1539 loss: 2.52587506e-05
Iter: 1540 loss: 2.47000498e-05
Iter: 1541 loss: 2.46671552e-05
Iter: 1542 loss: 2.4666253e-05
Iter: 1543 loss: 2.46403652e-05
Iter: 1544 loss: 2.45992633e-05
Iter: 1545 loss: 2.51248312e-05
Iter: 1546 loss: 2.45988522e-05
Iter: 1547 loss: 2.45744704e-05
Iter: 1548 loss: 2.4575882e-05
Iter: 1549 loss: 2.45554038e-05
Iter: 1550 loss: 2.45211922e-05
Iter: 1551 loss: 2.46211348e-05
Iter: 1552 loss: 2.45105839e-05
Iter: 1553 loss: 2.44710245e-05
Iter: 1554 loss: 2.45293104e-05
Iter: 1555 loss: 2.44518524e-05
Iter: 1556 loss: 2.44109433e-05
Iter: 1557 loss: 2.50068297e-05
Iter: 1558 loss: 2.4410836e-05
Iter: 1559 loss: 2.43828836e-05
Iter: 1560 loss: 2.4400897e-05
Iter: 1561 loss: 2.43651557e-05
Iter: 1562 loss: 2.43358809e-05
Iter: 1563 loss: 2.43842806e-05
Iter: 1564 loss: 2.43218401e-05
Iter: 1565 loss: 2.42976457e-05
Iter: 1566 loss: 2.44319526e-05
Iter: 1567 loss: 2.42941223e-05
Iter: 1568 loss: 2.42740171e-05
Iter: 1569 loss: 2.43700124e-05
Iter: 1570 loss: 2.42703227e-05
Iter: 1571 loss: 2.42537571e-05
Iter: 1572 loss: 2.42500828e-05
Iter: 1573 loss: 2.42394635e-05
Iter: 1574 loss: 2.4216075e-05
Iter: 1575 loss: 2.42063397e-05
Iter: 1576 loss: 2.4194007e-05
Iter: 1577 loss: 2.41616399e-05
Iter: 1578 loss: 2.45054835e-05
Iter: 1579 loss: 2.41609196e-05
Iter: 1580 loss: 2.41323105e-05
Iter: 1581 loss: 2.41521648e-05
Iter: 1582 loss: 2.41144735e-05
Iter: 1583 loss: 2.40817953e-05
Iter: 1584 loss: 2.40393747e-05
Iter: 1585 loss: 2.40366098e-05
Iter: 1586 loss: 2.39879482e-05
Iter: 1587 loss: 2.41433918e-05
Iter: 1588 loss: 2.39739293e-05
Iter: 1589 loss: 2.39341534e-05
Iter: 1590 loss: 2.43019422e-05
Iter: 1591 loss: 2.39321562e-05
Iter: 1592 loss: 2.38985704e-05
Iter: 1593 loss: 2.39611691e-05
Iter: 1594 loss: 2.38842877e-05
Iter: 1595 loss: 2.38504144e-05
Iter: 1596 loss: 2.43838622e-05
Iter: 1597 loss: 2.38503708e-05
Iter: 1598 loss: 2.38353314e-05
Iter: 1599 loss: 2.38256598e-05
Iter: 1600 loss: 2.38198e-05
Iter: 1601 loss: 2.37972636e-05
Iter: 1602 loss: 2.40264035e-05
Iter: 1603 loss: 2.37965687e-05
Iter: 1604 loss: 2.37851164e-05
Iter: 1605 loss: 2.37556815e-05
Iter: 1606 loss: 2.40074387e-05
Iter: 1607 loss: 2.37506756e-05
Iter: 1608 loss: 2.37042459e-05
Iter: 1609 loss: 2.43187715e-05
Iter: 1610 loss: 2.37038585e-05
Iter: 1611 loss: 2.36678188e-05
Iter: 1612 loss: 2.3859031e-05
Iter: 1613 loss: 2.36623346e-05
Iter: 1614 loss: 2.36336382e-05
Iter: 1615 loss: 2.36867709e-05
Iter: 1616 loss: 2.36215092e-05
Iter: 1617 loss: 2.35935622e-05
Iter: 1618 loss: 2.36324959e-05
Iter: 1619 loss: 2.35798325e-05
Iter: 1620 loss: 2.35432453e-05
Iter: 1621 loss: 2.35908537e-05
Iter: 1622 loss: 2.35243024e-05
Iter: 1623 loss: 2.34947856e-05
Iter: 1624 loss: 2.34918934e-05
Iter: 1625 loss: 2.34706567e-05
Iter: 1626 loss: 2.35282569e-05
Iter: 1627 loss: 2.34636645e-05
Iter: 1628 loss: 2.34402469e-05
Iter: 1629 loss: 2.36609685e-05
Iter: 1630 loss: 2.34394174e-05
Iter: 1631 loss: 2.3426599e-05
Iter: 1632 loss: 2.34466679e-05
Iter: 1633 loss: 2.34206309e-05
Iter: 1634 loss: 2.34092095e-05
Iter: 1635 loss: 2.35112129e-05
Iter: 1636 loss: 2.3408611e-05
Iter: 1637 loss: 2.33995434e-05
Iter: 1638 loss: 2.33736464e-05
Iter: 1639 loss: 2.34849103e-05
Iter: 1640 loss: 2.33635583e-05
Iter: 1641 loss: 2.33370556e-05
Iter: 1642 loss: 2.35349944e-05
Iter: 1643 loss: 2.33349347e-05
Iter: 1644 loss: 2.33124301e-05
Iter: 1645 loss: 2.33640312e-05
Iter: 1646 loss: 2.33041e-05
Iter: 1647 loss: 2.32820512e-05
Iter: 1648 loss: 2.33708215e-05
Iter: 1649 loss: 2.32770872e-05
Iter: 1650 loss: 2.32535676e-05
Iter: 1651 loss: 2.32431812e-05
Iter: 1652 loss: 2.32310413e-05
Iter: 1653 loss: 2.32014027e-05
Iter: 1654 loss: 2.32059228e-05
Iter: 1655 loss: 2.31788108e-05
Iter: 1656 loss: 2.31420072e-05
Iter: 1657 loss: 2.33132414e-05
Iter: 1658 loss: 2.31350477e-05
Iter: 1659 loss: 2.31072172e-05
Iter: 1660 loss: 2.32748462e-05
Iter: 1661 loss: 2.31036356e-05
Iter: 1662 loss: 2.31107661e-05
Iter: 1663 loss: 2.30907972e-05
Iter: 1664 loss: 2.308078e-05
Iter: 1665 loss: 2.30772348e-05
Iter: 1666 loss: 2.30713958e-05
Iter: 1667 loss: 2.30558726e-05
Iter: 1668 loss: 2.30810292e-05
Iter: 1669 loss: 2.30486185e-05
Iter: 1670 loss: 2.30265778e-05
Iter: 1671 loss: 2.30284131e-05
Iter: 1672 loss: 2.30094465e-05
Iter: 1673 loss: 2.29900543e-05
Iter: 1674 loss: 2.29831676e-05
Iter: 1675 loss: 2.29722136e-05
Iter: 1676 loss: 2.29415928e-05
Iter: 1677 loss: 2.31483209e-05
Iter: 1678 loss: 2.2938475e-05
Iter: 1679 loss: 2.29062371e-05
Iter: 1680 loss: 2.29555e-05
Iter: 1681 loss: 2.28908684e-05
Iter: 1682 loss: 2.28493391e-05
Iter: 1683 loss: 2.29930847e-05
Iter: 1684 loss: 2.28382796e-05
Iter: 1685 loss: 2.28114077e-05
Iter: 1686 loss: 2.27667133e-05
Iter: 1687 loss: 2.27664386e-05
Iter: 1688 loss: 2.2723867e-05
Iter: 1689 loss: 2.29640464e-05
Iter: 1690 loss: 2.27179735e-05
Iter: 1691 loss: 2.26936354e-05
Iter: 1692 loss: 2.26997035e-05
Iter: 1693 loss: 2.26758311e-05
Iter: 1694 loss: 2.26654847e-05
Iter: 1695 loss: 2.26608172e-05
Iter: 1696 loss: 2.26433367e-05
Iter: 1697 loss: 2.28390545e-05
Iter: 1698 loss: 2.26430529e-05
Iter: 1699 loss: 2.26345364e-05
Iter: 1700 loss: 2.26242628e-05
Iter: 1701 loss: 2.26230277e-05
Iter: 1702 loss: 2.26029697e-05
Iter: 1703 loss: 2.2674918e-05
Iter: 1704 loss: 2.25976946e-05
Iter: 1705 loss: 2.25810782e-05
Iter: 1706 loss: 2.25593449e-05
Iter: 1707 loss: 2.2557886e-05
Iter: 1708 loss: 2.25208732e-05
Iter: 1709 loss: 2.25794101e-05
Iter: 1710 loss: 2.25032545e-05
Iter: 1711 loss: 2.24635296e-05
Iter: 1712 loss: 2.2858756e-05
Iter: 1713 loss: 2.24621926e-05
Iter: 1714 loss: 2.24306386e-05
Iter: 1715 loss: 2.27125856e-05
Iter: 1716 loss: 2.24290761e-05
Iter: 1717 loss: 2.241529e-05
Iter: 1718 loss: 2.24177238e-05
Iter: 1719 loss: 2.24050509e-05
Iter: 1720 loss: 2.23901952e-05
Iter: 1721 loss: 2.237843e-05
Iter: 1722 loss: 2.23739189e-05
Iter: 1723 loss: 2.23523348e-05
Iter: 1724 loss: 2.23776624e-05
Iter: 1725 loss: 2.23408606e-05
Iter: 1726 loss: 2.23189272e-05
Iter: 1727 loss: 2.23892912e-05
Iter: 1728 loss: 2.23127172e-05
Iter: 1729 loss: 2.23208444e-05
Iter: 1730 loss: 2.23066145e-05
Iter: 1731 loss: 2.22999115e-05
Iter: 1732 loss: 2.22949311e-05
Iter: 1733 loss: 2.22928575e-05
Iter: 1734 loss: 2.22745584e-05
Iter: 1735 loss: 2.22775889e-05
Iter: 1736 loss: 2.22606941e-05
Iter: 1737 loss: 2.22438539e-05
Iter: 1738 loss: 2.22437138e-05
Iter: 1739 loss: 2.22315666e-05
Iter: 1740 loss: 2.22135077e-05
Iter: 1741 loss: 2.22131475e-05
Iter: 1742 loss: 2.2184031e-05
Iter: 1743 loss: 2.2459757e-05
Iter: 1744 loss: 2.21828304e-05
Iter: 1745 loss: 2.2164415e-05
Iter: 1746 loss: 2.2154567e-05
Iter: 1747 loss: 2.21462378e-05
Iter: 1748 loss: 2.21166265e-05
Iter: 1749 loss: 2.22237104e-05
Iter: 1750 loss: 2.21091323e-05
Iter: 1751 loss: 2.20838429e-05
Iter: 1752 loss: 2.21298833e-05
Iter: 1753 loss: 2.20727943e-05
Iter: 1754 loss: 2.20557849e-05
Iter: 1755 loss: 2.2186774e-05
Iter: 1756 loss: 2.20546026e-05
Iter: 1757 loss: 2.20388683e-05
Iter: 1758 loss: 2.20449019e-05
Iter: 1759 loss: 2.20277398e-05
Iter: 1760 loss: 2.20019974e-05
Iter: 1761 loss: 2.21727823e-05
Iter: 1762 loss: 2.19991125e-05
Iter: 1763 loss: 2.1983964e-05
Iter: 1764 loss: 2.19832018e-05
Iter: 1765 loss: 2.19782123e-05
Iter: 1766 loss: 2.1966549e-05
Iter: 1767 loss: 2.21288283e-05
Iter: 1768 loss: 2.19657722e-05
Iter: 1769 loss: 2.19492176e-05
Iter: 1770 loss: 2.20135753e-05
Iter: 1771 loss: 2.19452413e-05
Iter: 1772 loss: 2.19259164e-05
Iter: 1773 loss: 2.19809299e-05
Iter: 1774 loss: 2.19198409e-05
Iter: 1775 loss: 2.19048125e-05
Iter: 1776 loss: 2.19100184e-05
Iter: 1777 loss: 2.1894175e-05
Iter: 1778 loss: 2.18809655e-05
Iter: 1779 loss: 2.19262693e-05
Iter: 1780 loss: 2.18774931e-05
Iter: 1781 loss: 2.18612258e-05
Iter: 1782 loss: 2.19095273e-05
Iter: 1783 loss: 2.18564346e-05
Iter: 1784 loss: 2.18457426e-05
Iter: 1785 loss: 2.18441055e-05
Iter: 1786 loss: 2.1836735e-05
Iter: 1787 loss: 2.18265686e-05
Iter: 1788 loss: 2.18374771e-05
Iter: 1789 loss: 2.18210698e-05
Iter: 1790 loss: 2.18134555e-05
Iter: 1791 loss: 2.18091227e-05
Iter: 1792 loss: 2.1805914e-05
Iter: 1793 loss: 2.17868e-05
Iter: 1794 loss: 2.17996021e-05
Iter: 1795 loss: 2.1775033e-05
Iter: 1796 loss: 2.17555953e-05
Iter: 1797 loss: 2.1968277e-05
Iter: 1798 loss: 2.17552079e-05
Iter: 1799 loss: 2.17426314e-05
Iter: 1800 loss: 2.18212917e-05
Iter: 1801 loss: 2.17412125e-05
Iter: 1802 loss: 2.1729229e-05
Iter: 1803 loss: 2.17369416e-05
Iter: 1804 loss: 2.17215711e-05
Iter: 1805 loss: 2.17097822e-05
Iter: 1806 loss: 2.17098059e-05
Iter: 1807 loss: 2.17024299e-05
Iter: 1808 loss: 2.1686079e-05
Iter: 1809 loss: 2.1916092e-05
Iter: 1810 loss: 2.16851859e-05
Iter: 1811 loss: 2.1665066e-05
Iter: 1812 loss: 2.1799091e-05
Iter: 1813 loss: 2.16629342e-05
Iter: 1814 loss: 2.16447152e-05
Iter: 1815 loss: 2.1696942e-05
Iter: 1816 loss: 2.16388689e-05
Iter: 1817 loss: 2.16449571e-05
Iter: 1818 loss: 2.16321878e-05
Iter: 1819 loss: 2.16248245e-05
Iter: 1820 loss: 2.16380067e-05
Iter: 1821 loss: 2.16216686e-05
Iter: 1822 loss: 2.1615584e-05
Iter: 1823 loss: 2.16022927e-05
Iter: 1824 loss: 2.17728848e-05
Iter: 1825 loss: 2.16013032e-05
Iter: 1826 loss: 2.15820492e-05
Iter: 1827 loss: 2.162501e-05
Iter: 1828 loss: 2.15744512e-05
Iter: 1829 loss: 2.15566306e-05
Iter: 1830 loss: 2.16740809e-05
Iter: 1831 loss: 2.15546825e-05
Iter: 1832 loss: 2.15425098e-05
Iter: 1833 loss: 2.16780827e-05
Iter: 1834 loss: 2.15423825e-05
Iter: 1835 loss: 2.15314285e-05
Iter: 1836 loss: 2.16935059e-05
Iter: 1837 loss: 2.15313157e-05
Iter: 1838 loss: 2.15227792e-05
Iter: 1839 loss: 2.1628899e-05
Iter: 1840 loss: 2.15225755e-05
Iter: 1841 loss: 2.15157597e-05
Iter: 1842 loss: 2.15029122e-05
Iter: 1843 loss: 2.17942779e-05
Iter: 1844 loss: 2.15029067e-05
Iter: 1845 loss: 2.14868396e-05
Iter: 1846 loss: 2.14703668e-05
Iter: 1847 loss: 2.14671636e-05
Iter: 1848 loss: 2.14406864e-05
Iter: 1849 loss: 2.15485561e-05
Iter: 1850 loss: 2.14347529e-05
Iter: 1851 loss: 2.14116426e-05
Iter: 1852 loss: 2.15377731e-05
Iter: 1853 loss: 2.14082029e-05
Iter: 1854 loss: 2.14352476e-05
Iter: 1855 loss: 2.14046231e-05
Iter: 1856 loss: 2.14003485e-05
Iter: 1857 loss: 2.13876901e-05
Iter: 1858 loss: 2.14288557e-05
Iter: 1859 loss: 2.13817384e-05
Iter: 1860 loss: 2.13623498e-05
Iter: 1861 loss: 2.14762767e-05
Iter: 1862 loss: 2.1359665e-05
Iter: 1863 loss: 2.13468265e-05
Iter: 1864 loss: 2.13595013e-05
Iter: 1865 loss: 2.1339456e-05
Iter: 1866 loss: 2.1328804e-05
Iter: 1867 loss: 2.13232015e-05
Iter: 1868 loss: 2.13181793e-05
Iter: 1869 loss: 2.13127023e-05
Iter: 1870 loss: 2.1302465e-05
Iter: 1871 loss: 2.15319869e-05
Iter: 1872 loss: 2.13024141e-05
Iter: 1873 loss: 2.12884988e-05
Iter: 1874 loss: 2.13057247e-05
Iter: 1875 loss: 2.12808663e-05
Iter: 1876 loss: 2.13017793e-05
Iter: 1877 loss: 2.12673294e-05
Iter: 1878 loss: 2.1260892e-05
Iter: 1879 loss: 2.12485647e-05
Iter: 1880 loss: 2.14984939e-05
Iter: 1881 loss: 2.12485265e-05
Iter: 1882 loss: 2.12368141e-05
Iter: 1883 loss: 2.12245905e-05
Iter: 1884 loss: 2.12224732e-05
Iter: 1885 loss: 2.1206155e-05
Iter: 1886 loss: 2.11939641e-05
Iter: 1887 loss: 2.11885708e-05
Iter: 1888 loss: 2.11665283e-05
Iter: 1889 loss: 2.12254708e-05
Iter: 1890 loss: 2.11589431e-05
Iter: 1891 loss: 2.11401366e-05
Iter: 1892 loss: 2.14255633e-05
Iter: 1893 loss: 2.11402257e-05
Iter: 1894 loss: 2.11290644e-05
Iter: 1895 loss: 2.11218703e-05
Iter: 1896 loss: 2.11175811e-05
Iter: 1897 loss: 2.10921153e-05
Iter: 1898 loss: 2.11436545e-05
Iter: 1899 loss: 2.10813978e-05
Iter: 1900 loss: 2.10604067e-05
Iter: 1901 loss: 2.11159931e-05
Iter: 1902 loss: 2.10533672e-05
Iter: 1903 loss: 2.10401577e-05
Iter: 1904 loss: 2.10399648e-05
Iter: 1905 loss: 2.10347534e-05
Iter: 1906 loss: 2.10259659e-05
Iter: 1907 loss: 2.10259277e-05
Iter: 1908 loss: 2.10105554e-05
Iter: 1909 loss: 2.10408525e-05
Iter: 1910 loss: 2.10041544e-05
Iter: 1911 loss: 2.09942173e-05
Iter: 1912 loss: 2.10273392e-05
Iter: 1913 loss: 2.09915142e-05
Iter: 1914 loss: 2.09806913e-05
Iter: 1915 loss: 2.0982272e-05
Iter: 1916 loss: 2.0972504e-05
Iter: 1917 loss: 2.09513055e-05
Iter: 1918 loss: 2.10103972e-05
Iter: 1919 loss: 2.09447062e-05
Iter: 1920 loss: 2.09254504e-05
Iter: 1921 loss: 2.1001304e-05
Iter: 1922 loss: 2.09212012e-05
Iter: 1923 loss: 2.09034697e-05
Iter: 1924 loss: 2.09966929e-05
Iter: 1925 loss: 2.09005429e-05
Iter: 1926 loss: 2.08886922e-05
Iter: 1927 loss: 2.08994679e-05
Iter: 1928 loss: 2.08820111e-05
Iter: 1929 loss: 2.08705715e-05
Iter: 1930 loss: 2.09103164e-05
Iter: 1931 loss: 2.08673828e-05
Iter: 1932 loss: 2.0858166e-05
Iter: 1933 loss: 2.09154932e-05
Iter: 1934 loss: 2.08569982e-05
Iter: 1935 loss: 2.0851352e-05
Iter: 1936 loss: 2.08511065e-05
Iter: 1937 loss: 2.08470119e-05
Iter: 1938 loss: 2.08372112e-05
Iter: 1939 loss: 2.09543632e-05
Iter: 1940 loss: 2.0836178e-05
Iter: 1941 loss: 2.08270812e-05
Iter: 1942 loss: 2.08968522e-05
Iter: 1943 loss: 2.08262427e-05
Iter: 1944 loss: 2.08198016e-05
Iter: 1945 loss: 2.08506754e-05
Iter: 1946 loss: 2.08185247e-05
Iter: 1947 loss: 2.08131642e-05
Iter: 1948 loss: 2.07970406e-05
Iter: 1949 loss: 2.08494021e-05
Iter: 1950 loss: 2.07894409e-05
Iter: 1951 loss: 2.07688172e-05
Iter: 1952 loss: 2.08725942e-05
Iter: 1953 loss: 2.07653138e-05
Iter: 1954 loss: 2.07477497e-05
Iter: 1955 loss: 2.07846606e-05
Iter: 1956 loss: 2.07404864e-05
Iter: 1957 loss: 2.07216945e-05
Iter: 1958 loss: 2.07648445e-05
Iter: 1959 loss: 2.07146259e-05
Iter: 1960 loss: 2.0699219e-05
Iter: 1961 loss: 2.08268702e-05
Iter: 1962 loss: 2.06983495e-05
Iter: 1963 loss: 2.06872064e-05
Iter: 1964 loss: 2.07559024e-05
Iter: 1965 loss: 2.06857676e-05
Iter: 1966 loss: 2.06767363e-05
Iter: 1967 loss: 2.07822577e-05
Iter: 1968 loss: 2.06766217e-05
Iter: 1969 loss: 2.06719797e-05
Iter: 1970 loss: 2.06973255e-05
Iter: 1971 loss: 2.06712084e-05
Iter: 1972 loss: 2.06662717e-05
Iter: 1973 loss: 2.06674122e-05
Iter: 1974 loss: 2.0662319e-05
Iter: 1975 loss: 2.06588556e-05
Iter: 1976 loss: 2.06652e-05
Iter: 1977 loss: 2.06570585e-05
Iter: 1978 loss: 2.06524692e-05
Iter: 1979 loss: 2.06759614e-05
Iter: 1980 loss: 2.06516052e-05
Iter: 1981 loss: 2.06471559e-05
Iter: 1982 loss: 2.06355799e-05
Iter: 1983 loss: 2.07354351e-05
Iter: 1984 loss: 2.06336535e-05
Iter: 1985 loss: 2.06205714e-05
Iter: 1986 loss: 2.06316799e-05
Iter: 1987 loss: 2.06126897e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script72
+ '[' -r STOP.script72 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.4/300_300_300_1 '
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi0.8
+ date
Sat Oct 31 14:54:19 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.4/300_300_300_1 --function f1 --psi 2 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 30 --learning_rate 0.001 --decay_rate 0.8 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73601aa620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73601aa8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7360182d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f736011f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73601c9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73601cc268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7310092bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73601ccf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f730876f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f730876f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f730876ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73087cbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73087cbea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73600559d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73087f60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73087f69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73085758c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308567598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308718f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73085aa6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73085878c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308587b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73084ef730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f730844fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f730844f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308465620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308420ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73085d4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73085d4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73085cc400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f730862e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73083c7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73083d70d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308501c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f730852c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308303f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.011677802
test_loss: 0.011983382
train_loss: 0.008103805
test_loss: 0.008676057
train_loss: 0.0070990077
test_loss: 0.007611732
train_loss: 0.0067444695
test_loss: 0.0073153805
train_loss: 0.006480963
test_loss: 0.007221946
train_loss: 0.0065664183
test_loss: 0.0071950033
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi0.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa06db3a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa06daad950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa06daad8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa06daad730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0670bf0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0670bcae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa06707fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa06704c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066fb0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066fa1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066fb0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066efc7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066efc400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066f3b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066f859d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066f7bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066f78400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066f78598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066f78620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066e19ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066e93bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066ea5c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066e89730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066d9d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066d9d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066d2d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066d7b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066d54840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066cdb730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066cd3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066c1c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066c99510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066cc90d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066c92730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066ba99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa066b9e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 7.61186093e-05
Iter: 2 loss: 7.78026151e-05
Iter: 3 loss: 7.53437926e-05
Iter: 4 loss: 7.51143525e-05
Iter: 5 loss: 7.46789156e-05
Iter: 6 loss: 8.41276633e-05
Iter: 7 loss: 7.46774458e-05
Iter: 8 loss: 7.43910859e-05
Iter: 9 loss: 7.43739365e-05
Iter: 10 loss: 7.40743926e-05
Iter: 11 loss: 7.3636591e-05
Iter: 12 loss: 7.36245129e-05
Iter: 13 loss: 7.32307672e-05
Iter: 14 loss: 7.46260266e-05
Iter: 15 loss: 7.31286927e-05
Iter: 16 loss: 7.28190207e-05
Iter: 17 loss: 7.28190062e-05
Iter: 18 loss: 7.26473663e-05
Iter: 19 loss: 7.23952908e-05
Iter: 20 loss: 7.23886187e-05
Iter: 21 loss: 7.21492615e-05
Iter: 22 loss: 7.28165833e-05
Iter: 23 loss: 7.20719836e-05
Iter: 24 loss: 7.18648153e-05
Iter: 25 loss: 7.34698697e-05
Iter: 26 loss: 7.18496303e-05
Iter: 27 loss: 7.16889626e-05
Iter: 28 loss: 7.15730421e-05
Iter: 29 loss: 7.15178612e-05
Iter: 30 loss: 7.12904439e-05
Iter: 31 loss: 7.12902693e-05
Iter: 32 loss: 7.11084649e-05
Iter: 33 loss: 7.09585e-05
Iter: 34 loss: 7.09343731e-05
Iter: 35 loss: 7.08064181e-05
Iter: 36 loss: 7.06295323e-05
Iter: 37 loss: 7.06217616e-05
Iter: 38 loss: 7.04500126e-05
Iter: 39 loss: 7.09160522e-05
Iter: 40 loss: 7.03934056e-05
Iter: 41 loss: 7.02786347e-05
Iter: 42 loss: 7.02707548e-05
Iter: 43 loss: 7.01652607e-05
Iter: 44 loss: 6.99567754e-05
Iter: 45 loss: 7.40505056e-05
Iter: 46 loss: 6.99547236e-05
Iter: 47 loss: 6.97630749e-05
Iter: 48 loss: 7.12888868e-05
Iter: 49 loss: 6.97500654e-05
Iter: 50 loss: 6.95461422e-05
Iter: 51 loss: 7.0264723e-05
Iter: 52 loss: 6.94933697e-05
Iter: 53 loss: 6.93550537e-05
Iter: 54 loss: 6.92131871e-05
Iter: 55 loss: 6.91865862e-05
Iter: 56 loss: 6.90297238e-05
Iter: 57 loss: 6.90281158e-05
Iter: 58 loss: 6.89118096e-05
Iter: 59 loss: 6.89779845e-05
Iter: 60 loss: 6.88360451e-05
Iter: 61 loss: 6.86959756e-05
Iter: 62 loss: 6.94955525e-05
Iter: 63 loss: 6.8676658e-05
Iter: 64 loss: 6.85722334e-05
Iter: 65 loss: 6.84348e-05
Iter: 66 loss: 6.84263e-05
Iter: 67 loss: 6.83080434e-05
Iter: 68 loss: 6.83029939e-05
Iter: 69 loss: 6.81895253e-05
Iter: 70 loss: 6.80888043e-05
Iter: 71 loss: 6.80593221e-05
Iter: 72 loss: 6.78979268e-05
Iter: 73 loss: 6.79211516e-05
Iter: 74 loss: 6.77753414e-05
Iter: 75 loss: 6.76067866e-05
Iter: 76 loss: 6.89972439e-05
Iter: 77 loss: 6.75964911e-05
Iter: 78 loss: 6.7439425e-05
Iter: 79 loss: 6.86583808e-05
Iter: 80 loss: 6.74282492e-05
Iter: 81 loss: 6.73462273e-05
Iter: 82 loss: 6.71982125e-05
Iter: 83 loss: 7.07620493e-05
Iter: 84 loss: 6.71982125e-05
Iter: 85 loss: 6.70693407e-05
Iter: 86 loss: 6.88378786e-05
Iter: 87 loss: 6.70689478e-05
Iter: 88 loss: 6.6935987e-05
Iter: 89 loss: 6.72479073e-05
Iter: 90 loss: 6.68872e-05
Iter: 91 loss: 6.67831191e-05
Iter: 92 loss: 6.66212873e-05
Iter: 93 loss: 6.66191554e-05
Iter: 94 loss: 6.64511535e-05
Iter: 95 loss: 6.81825e-05
Iter: 96 loss: 6.64465042e-05
Iter: 97 loss: 6.63044557e-05
Iter: 98 loss: 6.70517838e-05
Iter: 99 loss: 6.62820894e-05
Iter: 100 loss: 6.61755766e-05
Iter: 101 loss: 6.60806691e-05
Iter: 102 loss: 6.60533187e-05
Iter: 103 loss: 6.59198195e-05
Iter: 104 loss: 6.59168654e-05
Iter: 105 loss: 6.582879e-05
Iter: 106 loss: 6.56626289e-05
Iter: 107 loss: 6.93098118e-05
Iter: 108 loss: 6.56621705e-05
Iter: 109 loss: 6.55300537e-05
Iter: 110 loss: 6.751193e-05
Iter: 111 loss: 6.5530061e-05
Iter: 112 loss: 6.53984607e-05
Iter: 113 loss: 6.54711839e-05
Iter: 114 loss: 6.53121388e-05
Iter: 115 loss: 6.51697628e-05
Iter: 116 loss: 6.50530128e-05
Iter: 117 loss: 6.50112197e-05
Iter: 118 loss: 6.48622663e-05
Iter: 119 loss: 6.4861837e-05
Iter: 120 loss: 6.46990229e-05
Iter: 121 loss: 6.48399437e-05
Iter: 122 loss: 6.46031476e-05
Iter: 123 loss: 6.4472486e-05
Iter: 124 loss: 6.44230167e-05
Iter: 125 loss: 6.43514213e-05
Iter: 126 loss: 6.42049417e-05
Iter: 127 loss: 6.42001833e-05
Iter: 128 loss: 6.41101869e-05
Iter: 129 loss: 6.38932397e-05
Iter: 130 loss: 6.62520179e-05
Iter: 131 loss: 6.38703787e-05
Iter: 132 loss: 6.36222539e-05
Iter: 133 loss: 6.43780513e-05
Iter: 134 loss: 6.35484321e-05
Iter: 135 loss: 6.3391446e-05
Iter: 136 loss: 6.33892414e-05
Iter: 137 loss: 6.3246378e-05
Iter: 138 loss: 6.32307638e-05
Iter: 139 loss: 6.31272269e-05
Iter: 140 loss: 6.29515489e-05
Iter: 141 loss: 6.32827214e-05
Iter: 142 loss: 6.28772323e-05
Iter: 143 loss: 6.26744295e-05
Iter: 144 loss: 6.47209e-05
Iter: 145 loss: 6.26680267e-05
Iter: 146 loss: 6.25460452e-05
Iter: 147 loss: 6.23155647e-05
Iter: 148 loss: 6.73596e-05
Iter: 149 loss: 6.23148226e-05
Iter: 150 loss: 6.20907231e-05
Iter: 151 loss: 6.27515037e-05
Iter: 152 loss: 6.20218925e-05
Iter: 153 loss: 6.18986232e-05
Iter: 154 loss: 6.18788908e-05
Iter: 155 loss: 6.17649057e-05
Iter: 156 loss: 6.1588973e-05
Iter: 157 loss: 6.15864265e-05
Iter: 158 loss: 6.14090604e-05
Iter: 159 loss: 6.25591274e-05
Iter: 160 loss: 6.13898155e-05
Iter: 161 loss: 6.11864598e-05
Iter: 162 loss: 6.16473044e-05
Iter: 163 loss: 6.11098149e-05
Iter: 164 loss: 6.09531489e-05
Iter: 165 loss: 6.07429392e-05
Iter: 166 loss: 6.07314178e-05
Iter: 167 loss: 6.0568218e-05
Iter: 168 loss: 6.05413734e-05
Iter: 169 loss: 6.04338493e-05
Iter: 170 loss: 6.02046566e-05
Iter: 171 loss: 6.38962156e-05
Iter: 172 loss: 6.01974389e-05
Iter: 173 loss: 5.99405103e-05
Iter: 174 loss: 6.014061e-05
Iter: 175 loss: 5.97845792e-05
Iter: 176 loss: 5.95267629e-05
Iter: 177 loss: 6.20889623e-05
Iter: 178 loss: 5.95183046e-05
Iter: 179 loss: 5.93018121e-05
Iter: 180 loss: 6.15111e-05
Iter: 181 loss: 5.92952529e-05
Iter: 182 loss: 5.91655844e-05
Iter: 183 loss: 5.89334741e-05
Iter: 184 loss: 6.47664e-05
Iter: 185 loss: 5.8933525e-05
Iter: 186 loss: 5.87510876e-05
Iter: 187 loss: 5.87439936e-05
Iter: 188 loss: 5.8598147e-05
Iter: 189 loss: 5.8384896e-05
Iter: 190 loss: 5.83791189e-05
Iter: 191 loss: 5.81638451e-05
Iter: 192 loss: 5.98413972e-05
Iter: 193 loss: 5.81484237e-05
Iter: 194 loss: 5.79313055e-05
Iter: 195 loss: 5.9201182e-05
Iter: 196 loss: 5.79027728e-05
Iter: 197 loss: 5.77713072e-05
Iter: 198 loss: 5.75831691e-05
Iter: 199 loss: 5.75768245e-05
Iter: 200 loss: 5.74110891e-05
Iter: 201 loss: 5.74030455e-05
Iter: 202 loss: 5.72761783e-05
Iter: 203 loss: 5.70482334e-05
Iter: 204 loss: 6.26144756e-05
Iter: 205 loss: 5.70483244e-05
Iter: 206 loss: 5.68064315e-05
Iter: 207 loss: 5.73197613e-05
Iter: 208 loss: 5.67109928e-05
Iter: 209 loss: 5.65072951e-05
Iter: 210 loss: 5.65050395e-05
Iter: 211 loss: 5.63618451e-05
Iter: 212 loss: 5.61334818e-05
Iter: 213 loss: 5.61314955e-05
Iter: 214 loss: 5.59197506e-05
Iter: 215 loss: 5.71321762e-05
Iter: 216 loss: 5.58903776e-05
Iter: 217 loss: 5.56239647e-05
Iter: 218 loss: 5.64192233e-05
Iter: 219 loss: 5.55428524e-05
Iter: 220 loss: 5.53600476e-05
Iter: 221 loss: 5.57564417e-05
Iter: 222 loss: 5.52893689e-05
Iter: 223 loss: 5.50553086e-05
Iter: 224 loss: 5.5628072e-05
Iter: 225 loss: 5.497034e-05
Iter: 226 loss: 5.48037e-05
Iter: 227 loss: 5.48102398e-05
Iter: 228 loss: 5.46722877e-05
Iter: 229 loss: 5.44398208e-05
Iter: 230 loss: 5.77086103e-05
Iter: 231 loss: 5.44392606e-05
Iter: 232 loss: 5.43195783e-05
Iter: 233 loss: 5.40959445e-05
Iter: 234 loss: 5.91576463e-05
Iter: 235 loss: 5.40954643e-05
Iter: 236 loss: 5.39407956e-05
Iter: 237 loss: 5.39226894e-05
Iter: 238 loss: 5.37384622e-05
Iter: 239 loss: 5.39425055e-05
Iter: 240 loss: 5.36384614e-05
Iter: 241 loss: 5.34341489e-05
Iter: 242 loss: 5.51788617e-05
Iter: 243 loss: 5.34224164e-05
Iter: 244 loss: 5.31945952e-05
Iter: 245 loss: 5.41991249e-05
Iter: 246 loss: 5.31491096e-05
Iter: 247 loss: 5.29711106e-05
Iter: 248 loss: 5.28169912e-05
Iter: 249 loss: 5.27691627e-05
Iter: 250 loss: 5.25368087e-05
Iter: 251 loss: 5.25359064e-05
Iter: 252 loss: 5.23066155e-05
Iter: 253 loss: 5.3326592e-05
Iter: 254 loss: 5.22611081e-05
Iter: 255 loss: 5.21179245e-05
Iter: 256 loss: 5.23304625e-05
Iter: 257 loss: 5.20492868e-05
Iter: 258 loss: 5.18483248e-05
Iter: 259 loss: 5.24000789e-05
Iter: 260 loss: 5.17809858e-05
Iter: 261 loss: 5.16135115e-05
Iter: 262 loss: 5.16494911e-05
Iter: 263 loss: 5.14896383e-05
Iter: 264 loss: 5.12987099e-05
Iter: 265 loss: 5.12976476e-05
Iter: 266 loss: 5.11981561e-05
Iter: 267 loss: 5.09560414e-05
Iter: 268 loss: 5.35160689e-05
Iter: 269 loss: 5.09285528e-05
Iter: 270 loss: 5.07315053e-05
Iter: 271 loss: 5.07309887e-05
Iter: 272 loss: 5.05301068e-05
Iter: 273 loss: 5.10422033e-05
Iter: 274 loss: 5.04607706e-05
Iter: 275 loss: 5.02983494e-05
Iter: 276 loss: 5.01362e-05
Iter: 277 loss: 5.01026807e-05
Iter: 278 loss: 4.99471025e-05
Iter: 279 loss: 4.99416274e-05
Iter: 280 loss: 4.97687033e-05
Iter: 281 loss: 4.98421286e-05
Iter: 282 loss: 4.96501743e-05
Iter: 283 loss: 4.94537453e-05
Iter: 284 loss: 4.9606977e-05
Iter: 285 loss: 4.93342486e-05
Iter: 286 loss: 4.91847495e-05
Iter: 287 loss: 4.91807405e-05
Iter: 288 loss: 4.90321981e-05
Iter: 289 loss: 4.8832273e-05
Iter: 290 loss: 4.88217411e-05
Iter: 291 loss: 4.86190183e-05
Iter: 292 loss: 5.09361489e-05
Iter: 293 loss: 4.86145873e-05
Iter: 294 loss: 4.84323318e-05
Iter: 295 loss: 4.87216457e-05
Iter: 296 loss: 4.83478361e-05
Iter: 297 loss: 4.82138712e-05
Iter: 298 loss: 4.85899145e-05
Iter: 299 loss: 4.81709103e-05
Iter: 300 loss: 4.79937589e-05
Iter: 301 loss: 4.85399e-05
Iter: 302 loss: 4.79411392e-05
Iter: 303 loss: 4.78028451e-05
Iter: 304 loss: 4.75963607e-05
Iter: 305 loss: 4.75917732e-05
Iter: 306 loss: 4.73498221e-05
Iter: 307 loss: 4.87596917e-05
Iter: 308 loss: 4.73179789e-05
Iter: 309 loss: 4.71246494e-05
Iter: 310 loss: 5.0053517e-05
Iter: 311 loss: 4.71245039e-05
Iter: 312 loss: 4.70027e-05
Iter: 313 loss: 4.68078179e-05
Iter: 314 loss: 4.68061335e-05
Iter: 315 loss: 4.66142228e-05
Iter: 316 loss: 4.72389183e-05
Iter: 317 loss: 4.65594567e-05
Iter: 318 loss: 4.63941e-05
Iter: 319 loss: 4.84451957e-05
Iter: 320 loss: 4.63924807e-05
Iter: 321 loss: 4.62828248e-05
Iter: 322 loss: 4.61450472e-05
Iter: 323 loss: 4.6134126e-05
Iter: 324 loss: 4.59528019e-05
Iter: 325 loss: 4.77945177e-05
Iter: 326 loss: 4.59469447e-05
Iter: 327 loss: 4.58082177e-05
Iter: 328 loss: 4.58171708e-05
Iter: 329 loss: 4.56997404e-05
Iter: 330 loss: 4.55272238e-05
Iter: 331 loss: 4.63473698e-05
Iter: 332 loss: 4.54961337e-05
Iter: 333 loss: 4.53162284e-05
Iter: 334 loss: 4.58749164e-05
Iter: 335 loss: 4.52628155e-05
Iter: 336 loss: 4.51369742e-05
Iter: 337 loss: 4.51288943e-05
Iter: 338 loss: 4.50338266e-05
Iter: 339 loss: 4.49070649e-05
Iter: 340 loss: 4.49063518e-05
Iter: 341 loss: 4.47928542e-05
Iter: 342 loss: 4.46250742e-05
Iter: 343 loss: 4.46208651e-05
Iter: 344 loss: 4.44448669e-05
Iter: 345 loss: 4.46573467e-05
Iter: 346 loss: 4.4352455e-05
Iter: 347 loss: 4.42301571e-05
Iter: 348 loss: 4.4216e-05
Iter: 349 loss: 4.41090087e-05
Iter: 350 loss: 4.39223113e-05
Iter: 351 loss: 4.39223222e-05
Iter: 352 loss: 4.37705166e-05
Iter: 353 loss: 4.37702911e-05
Iter: 354 loss: 4.36437e-05
Iter: 355 loss: 4.37185336e-05
Iter: 356 loss: 4.35617403e-05
Iter: 357 loss: 4.34281246e-05
Iter: 358 loss: 4.40685981e-05
Iter: 359 loss: 4.3404114e-05
Iter: 360 loss: 4.32650631e-05
Iter: 361 loss: 4.38289935e-05
Iter: 362 loss: 4.32343586e-05
Iter: 363 loss: 4.31300687e-05
Iter: 364 loss: 4.30420732e-05
Iter: 365 loss: 4.30128857e-05
Iter: 366 loss: 4.28951826e-05
Iter: 367 loss: 4.28920175e-05
Iter: 368 loss: 4.27985942e-05
Iter: 369 loss: 4.26419938e-05
Iter: 370 loss: 4.26416082e-05
Iter: 371 loss: 4.25176186e-05
Iter: 372 loss: 4.36738483e-05
Iter: 373 loss: 4.25124e-05
Iter: 374 loss: 4.23875463e-05
Iter: 375 loss: 4.28777203e-05
Iter: 376 loss: 4.23586462e-05
Iter: 377 loss: 4.22597514e-05
Iter: 378 loss: 4.2096468e-05
Iter: 379 loss: 4.20958131e-05
Iter: 380 loss: 4.19908101e-05
Iter: 381 loss: 4.19790122e-05
Iter: 382 loss: 4.18754644e-05
Iter: 383 loss: 4.19037606e-05
Iter: 384 loss: 4.18004e-05
Iter: 385 loss: 4.16930852e-05
Iter: 386 loss: 4.22608136e-05
Iter: 387 loss: 4.16766416e-05
Iter: 388 loss: 4.15578797e-05
Iter: 389 loss: 4.15397189e-05
Iter: 390 loss: 4.14571223e-05
Iter: 391 loss: 4.13409216e-05
Iter: 392 loss: 4.23808597e-05
Iter: 393 loss: 4.13352755e-05
Iter: 394 loss: 4.12214649e-05
Iter: 395 loss: 4.12007721e-05
Iter: 396 loss: 4.11239234e-05
Iter: 397 loss: 4.09921704e-05
Iter: 398 loss: 4.11067558e-05
Iter: 399 loss: 4.09147469e-05
Iter: 400 loss: 4.08392443e-05
Iter: 401 loss: 4.08242049e-05
Iter: 402 loss: 4.07602529e-05
Iter: 403 loss: 4.0652707e-05
Iter: 404 loss: 4.06523832e-05
Iter: 405 loss: 4.05192877e-05
Iter: 406 loss: 4.05470309e-05
Iter: 407 loss: 4.04204329e-05
Iter: 408 loss: 4.025218e-05
Iter: 409 loss: 4.09244167e-05
Iter: 410 loss: 4.02141704e-05
Iter: 411 loss: 4.01171928e-05
Iter: 412 loss: 4.0115443e-05
Iter: 413 loss: 4.00132121e-05
Iter: 414 loss: 3.99349447e-05
Iter: 415 loss: 3.99020646e-05
Iter: 416 loss: 3.97958574e-05
Iter: 417 loss: 4.06182589e-05
Iter: 418 loss: 3.97881086e-05
Iter: 419 loss: 3.96763826e-05
Iter: 420 loss: 3.99130222e-05
Iter: 421 loss: 3.96321848e-05
Iter: 422 loss: 3.95409297e-05
Iter: 423 loss: 3.94027375e-05
Iter: 424 loss: 3.94001181e-05
Iter: 425 loss: 3.93247137e-05
Iter: 426 loss: 3.93125702e-05
Iter: 427 loss: 3.92174552e-05
Iter: 428 loss: 3.92382208e-05
Iter: 429 loss: 3.9146893e-05
Iter: 430 loss: 3.90580244e-05
Iter: 431 loss: 3.9111721e-05
Iter: 432 loss: 3.90008427e-05
Iter: 433 loss: 3.89020206e-05
Iter: 434 loss: 3.89016677e-05
Iter: 435 loss: 3.88541666e-05
Iter: 436 loss: 3.87364926e-05
Iter: 437 loss: 3.98789198e-05
Iter: 438 loss: 3.87209111e-05
Iter: 439 loss: 3.86346946e-05
Iter: 440 loss: 3.86277534e-05
Iter: 441 loss: 3.85508429e-05
Iter: 442 loss: 3.84804152e-05
Iter: 443 loss: 3.84614978e-05
Iter: 444 loss: 3.83578445e-05
Iter: 445 loss: 3.84248851e-05
Iter: 446 loss: 3.8292008e-05
Iter: 447 loss: 3.82150138e-05
Iter: 448 loss: 3.82141661e-05
Iter: 449 loss: 3.81467362e-05
Iter: 450 loss: 3.81848658e-05
Iter: 451 loss: 3.81026694e-05
Iter: 452 loss: 3.80196725e-05
Iter: 453 loss: 3.83627412e-05
Iter: 454 loss: 3.80016645e-05
Iter: 455 loss: 3.79134945e-05
Iter: 456 loss: 3.81539976e-05
Iter: 457 loss: 3.78847326e-05
Iter: 458 loss: 3.78196528e-05
Iter: 459 loss: 3.79530902e-05
Iter: 460 loss: 3.77932884e-05
Iter: 461 loss: 3.76962926e-05
Iter: 462 loss: 3.79478661e-05
Iter: 463 loss: 3.76634052e-05
Iter: 464 loss: 3.75933196e-05
Iter: 465 loss: 3.74834744e-05
Iter: 466 loss: 3.74821611e-05
Iter: 467 loss: 3.73527e-05
Iter: 468 loss: 3.80652709e-05
Iter: 469 loss: 3.73340372e-05
Iter: 470 loss: 3.72823633e-05
Iter: 471 loss: 3.72670111e-05
Iter: 472 loss: 3.72221293e-05
Iter: 473 loss: 3.71154092e-05
Iter: 474 loss: 3.8332535e-05
Iter: 475 loss: 3.71054302e-05
Iter: 476 loss: 3.69968184e-05
Iter: 477 loss: 3.74394258e-05
Iter: 478 loss: 3.69729387e-05
Iter: 479 loss: 3.68793226e-05
Iter: 480 loss: 3.79459052e-05
Iter: 481 loss: 3.68777255e-05
Iter: 482 loss: 3.67933899e-05
Iter: 483 loss: 3.67531975e-05
Iter: 484 loss: 3.67120338e-05
Iter: 485 loss: 3.66245731e-05
Iter: 486 loss: 3.75893214e-05
Iter: 487 loss: 3.6622725e-05
Iter: 488 loss: 3.65482047e-05
Iter: 489 loss: 3.66754321e-05
Iter: 490 loss: 3.65147171e-05
Iter: 491 loss: 3.64351945e-05
Iter: 492 loss: 3.64834596e-05
Iter: 493 loss: 3.63838335e-05
Iter: 494 loss: 3.63149593e-05
Iter: 495 loss: 3.63140134e-05
Iter: 496 loss: 3.62617648e-05
Iter: 497 loss: 3.61491657e-05
Iter: 498 loss: 3.79859666e-05
Iter: 499 loss: 3.61453021e-05
Iter: 500 loss: 3.60538252e-05
Iter: 501 loss: 3.66621098e-05
Iter: 502 loss: 3.60445e-05
Iter: 503 loss: 3.59470214e-05
Iter: 504 loss: 3.62491301e-05
Iter: 505 loss: 3.59183541e-05
Iter: 506 loss: 3.58524849e-05
Iter: 507 loss: 3.58273683e-05
Iter: 508 loss: 3.57909958e-05
Iter: 509 loss: 3.57566678e-05
Iter: 510 loss: 3.57341378e-05
Iter: 511 loss: 3.569416e-05
Iter: 512 loss: 3.56235687e-05
Iter: 513 loss: 3.5623576e-05
Iter: 514 loss: 3.55175725e-05
Iter: 515 loss: 3.62490973e-05
Iter: 516 loss: 3.5507448e-05
Iter: 517 loss: 3.54333279e-05
Iter: 518 loss: 3.65194064e-05
Iter: 519 loss: 3.54331751e-05
Iter: 520 loss: 3.53827927e-05
Iter: 521 loss: 3.52887364e-05
Iter: 522 loss: 3.74071242e-05
Iter: 523 loss: 3.52885982e-05
Iter: 524 loss: 3.51922936e-05
Iter: 525 loss: 3.5482095e-05
Iter: 526 loss: 3.51631243e-05
Iter: 527 loss: 3.51006129e-05
Iter: 528 loss: 3.50984301e-05
Iter: 529 loss: 3.50442933e-05
Iter: 530 loss: 3.49600959e-05
Iter: 531 loss: 3.49589609e-05
Iter: 532 loss: 3.48916328e-05
Iter: 533 loss: 3.48915419e-05
Iter: 534 loss: 3.48262729e-05
Iter: 535 loss: 3.48453686e-05
Iter: 536 loss: 3.47794485e-05
Iter: 537 loss: 3.47193527e-05
Iter: 538 loss: 3.49994298e-05
Iter: 539 loss: 3.47081e-05
Iter: 540 loss: 3.4631561e-05
Iter: 541 loss: 3.46561574e-05
Iter: 542 loss: 3.4577246e-05
Iter: 543 loss: 3.45152366e-05
Iter: 544 loss: 3.51206327e-05
Iter: 545 loss: 3.45128865e-05
Iter: 546 loss: 3.44483051e-05
Iter: 547 loss: 3.44882355e-05
Iter: 548 loss: 3.44068721e-05
Iter: 549 loss: 3.4351473e-05
Iter: 550 loss: 3.42875028e-05
Iter: 551 loss: 3.42796666e-05
Iter: 552 loss: 3.41870218e-05
Iter: 553 loss: 3.43419961e-05
Iter: 554 loss: 3.4145145e-05
Iter: 555 loss: 3.40656152e-05
Iter: 556 loss: 3.40654442e-05
Iter: 557 loss: 3.39913095e-05
Iter: 558 loss: 3.40366132e-05
Iter: 559 loss: 3.39437574e-05
Iter: 560 loss: 3.38691971e-05
Iter: 561 loss: 3.39462058e-05
Iter: 562 loss: 3.38276368e-05
Iter: 563 loss: 3.3765049e-05
Iter: 564 loss: 3.46643137e-05
Iter: 565 loss: 3.37649035e-05
Iter: 566 loss: 3.37086676e-05
Iter: 567 loss: 3.37130914e-05
Iter: 568 loss: 3.36650774e-05
Iter: 569 loss: 3.36084304e-05
Iter: 570 loss: 3.37925412e-05
Iter: 571 loss: 3.35924706e-05
Iter: 572 loss: 3.35201476e-05
Iter: 573 loss: 3.38482241e-05
Iter: 574 loss: 3.35063487e-05
Iter: 575 loss: 3.34608303e-05
Iter: 576 loss: 3.353586e-05
Iter: 577 loss: 3.3439821e-05
Iter: 578 loss: 3.33821372e-05
Iter: 579 loss: 3.36915946e-05
Iter: 580 loss: 3.33734824e-05
Iter: 581 loss: 3.33328207e-05
Iter: 582 loss: 3.32546e-05
Iter: 583 loss: 3.49172333e-05
Iter: 584 loss: 3.3254204e-05
Iter: 585 loss: 3.31913252e-05
Iter: 586 loss: 3.31912233e-05
Iter: 587 loss: 3.3131997e-05
Iter: 588 loss: 3.31333285e-05
Iter: 589 loss: 3.30848925e-05
Iter: 590 loss: 3.30226794e-05
Iter: 591 loss: 3.31100127e-05
Iter: 592 loss: 3.29919349e-05
Iter: 593 loss: 3.29289287e-05
Iter: 594 loss: 3.37625243e-05
Iter: 595 loss: 3.29285504e-05
Iter: 596 loss: 3.28864844e-05
Iter: 597 loss: 3.28206079e-05
Iter: 598 loss: 3.28198439e-05
Iter: 599 loss: 3.27558228e-05
Iter: 600 loss: 3.36332778e-05
Iter: 601 loss: 3.27555645e-05
Iter: 602 loss: 3.26987647e-05
Iter: 603 loss: 3.27227899e-05
Iter: 604 loss: 3.26596964e-05
Iter: 605 loss: 3.2605436e-05
Iter: 606 loss: 3.25355795e-05
Iter: 607 loss: 3.25307265e-05
Iter: 608 loss: 3.24647553e-05
Iter: 609 loss: 3.34996366e-05
Iter: 610 loss: 3.24646389e-05
Iter: 611 loss: 3.24169523e-05
Iter: 612 loss: 3.25691581e-05
Iter: 613 loss: 3.2403339e-05
Iter: 614 loss: 3.2354641e-05
Iter: 615 loss: 3.23206841e-05
Iter: 616 loss: 3.23032109e-05
Iter: 617 loss: 3.22446358e-05
Iter: 618 loss: 3.27489324e-05
Iter: 619 loss: 3.22414853e-05
Iter: 620 loss: 3.21905318e-05
Iter: 621 loss: 3.21763073e-05
Iter: 622 loss: 3.21451516e-05
Iter: 623 loss: 3.20741528e-05
Iter: 624 loss: 3.20863765e-05
Iter: 625 loss: 3.2020831e-05
Iter: 626 loss: 3.19766768e-05
Iter: 627 loss: 3.19687606e-05
Iter: 628 loss: 3.1923566e-05
Iter: 629 loss: 3.1881671e-05
Iter: 630 loss: 3.18708226e-05
Iter: 631 loss: 3.18152743e-05
Iter: 632 loss: 3.22727574e-05
Iter: 633 loss: 3.18118473e-05
Iter: 634 loss: 3.17600025e-05
Iter: 635 loss: 3.18860148e-05
Iter: 636 loss: 3.17414706e-05
Iter: 637 loss: 3.16975347e-05
Iter: 638 loss: 3.16716396e-05
Iter: 639 loss: 3.16532532e-05
Iter: 640 loss: 3.16037112e-05
Iter: 641 loss: 3.23814311e-05
Iter: 642 loss: 3.16036858e-05
Iter: 643 loss: 3.15675934e-05
Iter: 644 loss: 3.15987309e-05
Iter: 645 loss: 3.15463112e-05
Iter: 646 loss: 3.15023899e-05
Iter: 647 loss: 3.17250961e-05
Iter: 648 loss: 3.14951103e-05
Iter: 649 loss: 3.14545505e-05
Iter: 650 loss: 3.14591707e-05
Iter: 651 loss: 3.14233184e-05
Iter: 652 loss: 3.13677265e-05
Iter: 653 loss: 3.14825738e-05
Iter: 654 loss: 3.13454075e-05
Iter: 655 loss: 3.12837437e-05
Iter: 656 loss: 3.1583244e-05
Iter: 657 loss: 3.12730117e-05
Iter: 658 loss: 3.12246739e-05
Iter: 659 loss: 3.11708136e-05
Iter: 660 loss: 3.11633376e-05
Iter: 661 loss: 3.11078402e-05
Iter: 662 loss: 3.18349448e-05
Iter: 663 loss: 3.11074837e-05
Iter: 664 loss: 3.10560499e-05
Iter: 665 loss: 3.118725e-05
Iter: 666 loss: 3.10381947e-05
Iter: 667 loss: 3.09969328e-05
Iter: 668 loss: 3.10056566e-05
Iter: 669 loss: 3.09662173e-05
Iter: 670 loss: 3.09137795e-05
Iter: 671 loss: 3.14520403e-05
Iter: 672 loss: 3.09123061e-05
Iter: 673 loss: 3.08713934e-05
Iter: 674 loss: 3.08271519e-05
Iter: 675 loss: 3.08202725e-05
Iter: 676 loss: 3.07590853e-05
Iter: 677 loss: 3.11598778e-05
Iter: 678 loss: 3.07524933e-05
Iter: 679 loss: 3.06897455e-05
Iter: 680 loss: 3.09435381e-05
Iter: 681 loss: 3.06757756e-05
Iter: 682 loss: 3.0631476e-05
Iter: 683 loss: 3.07717128e-05
Iter: 684 loss: 3.06187139e-05
Iter: 685 loss: 3.05693284e-05
Iter: 686 loss: 3.05934554e-05
Iter: 687 loss: 3.053619e-05
Iter: 688 loss: 3.04861842e-05
Iter: 689 loss: 3.06530674e-05
Iter: 690 loss: 3.04724817e-05
Iter: 691 loss: 3.04220703e-05
Iter: 692 loss: 3.06201291e-05
Iter: 693 loss: 3.04105e-05
Iter: 694 loss: 3.03675297e-05
Iter: 695 loss: 3.03397755e-05
Iter: 696 loss: 3.03232027e-05
Iter: 697 loss: 3.0263187e-05
Iter: 698 loss: 3.05373396e-05
Iter: 699 loss: 3.02517292e-05
Iter: 700 loss: 3.01838445e-05
Iter: 701 loss: 3.04765308e-05
Iter: 702 loss: 3.01699147e-05
Iter: 703 loss: 3.01243017e-05
Iter: 704 loss: 3.01284981e-05
Iter: 705 loss: 3.00890279e-05
Iter: 706 loss: 3.00326483e-05
Iter: 707 loss: 3.06853472e-05
Iter: 708 loss: 3.00317461e-05
Iter: 709 loss: 2.99919739e-05
Iter: 710 loss: 2.99512776e-05
Iter: 711 loss: 2.9943596e-05
Iter: 712 loss: 2.990131e-05
Iter: 713 loss: 2.99010699e-05
Iter: 714 loss: 2.98624873e-05
Iter: 715 loss: 2.9873152e-05
Iter: 716 loss: 2.98346458e-05
Iter: 717 loss: 2.98016457e-05
Iter: 718 loss: 3.00384527e-05
Iter: 719 loss: 2.97987535e-05
Iter: 720 loss: 2.97668048e-05
Iter: 721 loss: 2.9744886e-05
Iter: 722 loss: 2.97331917e-05
Iter: 723 loss: 2.96926446e-05
Iter: 724 loss: 2.99390449e-05
Iter: 725 loss: 2.96876533e-05
Iter: 726 loss: 2.96447815e-05
Iter: 727 loss: 2.96766666e-05
Iter: 728 loss: 2.96185226e-05
Iter: 729 loss: 2.95687678e-05
Iter: 730 loss: 2.95800273e-05
Iter: 731 loss: 2.9532026e-05
Iter: 732 loss: 2.94829097e-05
Iter: 733 loss: 2.99797866e-05
Iter: 734 loss: 2.94814563e-05
Iter: 735 loss: 2.94350666e-05
Iter: 736 loss: 2.95303071e-05
Iter: 737 loss: 2.94164474e-05
Iter: 738 loss: 2.93774938e-05
Iter: 739 loss: 2.94157908e-05
Iter: 740 loss: 2.93553821e-05
Iter: 741 loss: 2.93042976e-05
Iter: 742 loss: 2.96566104e-05
Iter: 743 loss: 2.92995082e-05
Iter: 744 loss: 2.92655932e-05
Iter: 745 loss: 2.92412715e-05
Iter: 746 loss: 2.92295135e-05
Iter: 747 loss: 2.91925498e-05
Iter: 748 loss: 2.9191242e-05
Iter: 749 loss: 2.91626293e-05
Iter: 750 loss: 2.91136312e-05
Iter: 751 loss: 2.91136457e-05
Iter: 752 loss: 2.9072904e-05
Iter: 753 loss: 2.97085662e-05
Iter: 754 loss: 2.9072864e-05
Iter: 755 loss: 2.90398166e-05
Iter: 756 loss: 2.90086209e-05
Iter: 757 loss: 2.90010776e-05
Iter: 758 loss: 2.89611817e-05
Iter: 759 loss: 2.93929588e-05
Iter: 760 loss: 2.89602795e-05
Iter: 761 loss: 2.89239761e-05
Iter: 762 loss: 2.89144446e-05
Iter: 763 loss: 2.88918545e-05
Iter: 764 loss: 2.88433657e-05
Iter: 765 loss: 2.88841948e-05
Iter: 766 loss: 2.8814602e-05
Iter: 767 loss: 2.87694929e-05
Iter: 768 loss: 2.93572884e-05
Iter: 769 loss: 2.87692492e-05
Iter: 770 loss: 2.8729839e-05
Iter: 771 loss: 2.87594812e-05
Iter: 772 loss: 2.87058974e-05
Iter: 773 loss: 2.86655741e-05
Iter: 774 loss: 2.87961411e-05
Iter: 775 loss: 2.8654209e-05
Iter: 776 loss: 2.86051691e-05
Iter: 777 loss: 2.87150742e-05
Iter: 778 loss: 2.85865099e-05
Iter: 779 loss: 2.85507631e-05
Iter: 780 loss: 2.86224549e-05
Iter: 781 loss: 2.85361566e-05
Iter: 782 loss: 2.84981561e-05
Iter: 783 loss: 2.89684649e-05
Iter: 784 loss: 2.84976777e-05
Iter: 785 loss: 2.84769958e-05
Iter: 786 loss: 2.84365651e-05
Iter: 787 loss: 2.92538389e-05
Iter: 788 loss: 2.84362559e-05
Iter: 789 loss: 2.83959835e-05
Iter: 790 loss: 2.89551172e-05
Iter: 791 loss: 2.83958252e-05
Iter: 792 loss: 2.8362756e-05
Iter: 793 loss: 2.83208228e-05
Iter: 794 loss: 2.83175832e-05
Iter: 795 loss: 2.82757373e-05
Iter: 796 loss: 2.89241398e-05
Iter: 797 loss: 2.82757428e-05
Iter: 798 loss: 2.82421715e-05
Iter: 799 loss: 2.82231322e-05
Iter: 800 loss: 2.82085039e-05
Iter: 801 loss: 2.81656339e-05
Iter: 802 loss: 2.82251167e-05
Iter: 803 loss: 2.81442935e-05
Iter: 804 loss: 2.81076937e-05
Iter: 805 loss: 2.86871982e-05
Iter: 806 loss: 2.81077064e-05
Iter: 807 loss: 2.80759232e-05
Iter: 808 loss: 2.80913664e-05
Iter: 809 loss: 2.80546046e-05
Iter: 810 loss: 2.8018887e-05
Iter: 811 loss: 2.81815664e-05
Iter: 812 loss: 2.80121512e-05
Iter: 813 loss: 2.79745727e-05
Iter: 814 loss: 2.80468848e-05
Iter: 815 loss: 2.7958813e-05
Iter: 816 loss: 2.79282303e-05
Iter: 817 loss: 2.79168307e-05
Iter: 818 loss: 2.78999869e-05
Iter: 819 loss: 2.78823554e-05
Iter: 820 loss: 2.78740117e-05
Iter: 821 loss: 2.78572952e-05
Iter: 822 loss: 2.7819262e-05
Iter: 823 loss: 2.83142363e-05
Iter: 824 loss: 2.78166772e-05
Iter: 825 loss: 2.77868694e-05
Iter: 826 loss: 2.82439451e-05
Iter: 827 loss: 2.77868676e-05
Iter: 828 loss: 2.77593335e-05
Iter: 829 loss: 2.77472645e-05
Iter: 830 loss: 2.77332365e-05
Iter: 831 loss: 2.77034469e-05
Iter: 832 loss: 2.78692878e-05
Iter: 833 loss: 2.76992578e-05
Iter: 834 loss: 2.76670689e-05
Iter: 835 loss: 2.77233303e-05
Iter: 836 loss: 2.76528463e-05
Iter: 837 loss: 2.76225219e-05
Iter: 838 loss: 2.7588314e-05
Iter: 839 loss: 2.75838465e-05
Iter: 840 loss: 2.75465718e-05
Iter: 841 loss: 2.75466e-05
Iter: 842 loss: 2.75116254e-05
Iter: 843 loss: 2.75778912e-05
Iter: 844 loss: 2.74968152e-05
Iter: 845 loss: 2.74671984e-05
Iter: 846 loss: 2.75571219e-05
Iter: 847 loss: 2.7458289e-05
Iter: 848 loss: 2.74254362e-05
Iter: 849 loss: 2.75596576e-05
Iter: 850 loss: 2.74182057e-05
Iter: 851 loss: 2.73903679e-05
Iter: 852 loss: 2.73451278e-05
Iter: 853 loss: 2.73448404e-05
Iter: 854 loss: 2.73129554e-05
Iter: 855 loss: 2.73121477e-05
Iter: 856 loss: 2.72786838e-05
Iter: 857 loss: 2.73966634e-05
Iter: 858 loss: 2.72699872e-05
Iter: 859 loss: 2.72513353e-05
Iter: 860 loss: 2.72205471e-05
Iter: 861 loss: 2.72204197e-05
Iter: 862 loss: 2.71842018e-05
Iter: 863 loss: 2.7617647e-05
Iter: 864 loss: 2.71837234e-05
Iter: 865 loss: 2.71548288e-05
Iter: 866 loss: 2.71271601e-05
Iter: 867 loss: 2.712075e-05
Iter: 868 loss: 2.70821874e-05
Iter: 869 loss: 2.71880053e-05
Iter: 870 loss: 2.70695746e-05
Iter: 871 loss: 2.70340151e-05
Iter: 872 loss: 2.75379989e-05
Iter: 873 loss: 2.70339442e-05
Iter: 874 loss: 2.70101882e-05
Iter: 875 loss: 2.69747925e-05
Iter: 876 loss: 2.69740412e-05
Iter: 877 loss: 2.6938249e-05
Iter: 878 loss: 2.70887067e-05
Iter: 879 loss: 2.69306329e-05
Iter: 880 loss: 2.68970452e-05
Iter: 881 loss: 2.73224068e-05
Iter: 882 loss: 2.68968288e-05
Iter: 883 loss: 2.68766398e-05
Iter: 884 loss: 2.68462027e-05
Iter: 885 loss: 2.68456097e-05
Iter: 886 loss: 2.68184722e-05
Iter: 887 loss: 2.71843965e-05
Iter: 888 loss: 2.68183067e-05
Iter: 889 loss: 2.67914456e-05
Iter: 890 loss: 2.68809708e-05
Iter: 891 loss: 2.67839896e-05
Iter: 892 loss: 2.67585838e-05
Iter: 893 loss: 2.68480071e-05
Iter: 894 loss: 2.67520263e-05
Iter: 895 loss: 2.67326413e-05
Iter: 896 loss: 2.66967654e-05
Iter: 897 loss: 2.75249149e-05
Iter: 898 loss: 2.66967691e-05
Iter: 899 loss: 2.66711177e-05
Iter: 900 loss: 2.66708339e-05
Iter: 901 loss: 2.66462375e-05
Iter: 902 loss: 2.66268e-05
Iter: 903 loss: 2.66192183e-05
Iter: 904 loss: 2.65866329e-05
Iter: 905 loss: 2.66074967e-05
Iter: 906 loss: 2.65659255e-05
Iter: 907 loss: 2.65399358e-05
Iter: 908 loss: 2.65390263e-05
Iter: 909 loss: 2.65147537e-05
Iter: 910 loss: 2.6479458e-05
Iter: 911 loss: 2.64784558e-05
Iter: 912 loss: 2.64366972e-05
Iter: 913 loss: 2.64980299e-05
Iter: 914 loss: 2.64165064e-05
Iter: 915 loss: 2.63953516e-05
Iter: 916 loss: 2.6392072e-05
Iter: 917 loss: 2.63694674e-05
Iter: 918 loss: 2.63472375e-05
Iter: 919 loss: 2.6342459e-05
Iter: 920 loss: 2.63198472e-05
Iter: 921 loss: 2.66774168e-05
Iter: 922 loss: 2.63198017e-05
Iter: 923 loss: 2.62986578e-05
Iter: 924 loss: 2.63332422e-05
Iter: 925 loss: 2.62889043e-05
Iter: 926 loss: 2.62651301e-05
Iter: 927 loss: 2.63098445e-05
Iter: 928 loss: 2.62549838e-05
Iter: 929 loss: 2.62316044e-05
Iter: 930 loss: 2.61948699e-05
Iter: 931 loss: 2.61944151e-05
Iter: 932 loss: 2.61665737e-05
Iter: 933 loss: 2.61663517e-05
Iter: 934 loss: 2.61396726e-05
Iter: 935 loss: 2.6130534e-05
Iter: 936 loss: 2.61152381e-05
Iter: 937 loss: 2.60844281e-05
Iter: 938 loss: 2.6077123e-05
Iter: 939 loss: 2.60574107e-05
Iter: 940 loss: 2.60285815e-05
Iter: 941 loss: 2.60273373e-05
Iter: 942 loss: 2.60003908e-05
Iter: 943 loss: 2.59779681e-05
Iter: 944 loss: 2.59703083e-05
Iter: 945 loss: 2.59372027e-05
Iter: 946 loss: 2.60120942e-05
Iter: 947 loss: 2.5924719e-05
Iter: 948 loss: 2.58966575e-05
Iter: 949 loss: 2.58965174e-05
Iter: 950 loss: 2.58781365e-05
Iter: 951 loss: 2.58598429e-05
Iter: 952 loss: 2.58560322e-05
Iter: 953 loss: 2.58371329e-05
Iter: 954 loss: 2.58359705e-05
Iter: 955 loss: 2.58219643e-05
Iter: 956 loss: 2.58135915e-05
Iter: 957 loss: 2.58078981e-05
Iter: 958 loss: 2.57824831e-05
Iter: 959 loss: 2.58068776e-05
Iter: 960 loss: 2.57680404e-05
Iter: 961 loss: 2.57435986e-05
Iter: 962 loss: 2.57300744e-05
Iter: 963 loss: 2.57192914e-05
Iter: 964 loss: 2.56913409e-05
Iter: 965 loss: 2.56912626e-05
Iter: 966 loss: 2.56686e-05
Iter: 967 loss: 2.56355543e-05
Iter: 968 loss: 2.56345375e-05
Iter: 969 loss: 2.55965442e-05
Iter: 970 loss: 2.56686108e-05
Iter: 971 loss: 2.55804116e-05
Iter: 972 loss: 2.55571795e-05
Iter: 973 loss: 2.55543709e-05
Iter: 974 loss: 2.55373034e-05
Iter: 975 loss: 2.55083905e-05
Iter: 976 loss: 2.55083214e-05
Iter: 977 loss: 2.54865517e-05
Iter: 978 loss: 2.54864462e-05
Iter: 979 loss: 2.54659699e-05
Iter: 980 loss: 2.5481866e-05
Iter: 981 loss: 2.54535535e-05
Iter: 982 loss: 2.54371189e-05
Iter: 983 loss: 2.55751402e-05
Iter: 984 loss: 2.5436173e-05
Iter: 985 loss: 2.54178358e-05
Iter: 986 loss: 2.54180031e-05
Iter: 987 loss: 2.54031638e-05
Iter: 988 loss: 2.53825638e-05
Iter: 989 loss: 2.54749648e-05
Iter: 990 loss: 2.5378491e-05
Iter: 991 loss: 2.53597591e-05
Iter: 992 loss: 2.53378676e-05
Iter: 993 loss: 2.53352737e-05
Iter: 994 loss: 2.53098551e-05
Iter: 995 loss: 2.54575061e-05
Iter: 996 loss: 2.53064682e-05
Iter: 997 loss: 2.5279116e-05
Iter: 998 loss: 2.53843828e-05
Iter: 999 loss: 2.52726823e-05
Iter: 1000 loss: 2.52528862e-05
Iter: 1001 loss: 2.52304799e-05
Iter: 1002 loss: 2.52275058e-05
Iter: 1003 loss: 2.5200221e-05
Iter: 1004 loss: 2.54737424e-05
Iter: 1005 loss: 2.51993915e-05
Iter: 1006 loss: 2.51721231e-05
Iter: 1007 loss: 2.52662285e-05
Iter: 1008 loss: 2.51649326e-05
Iter: 1009 loss: 2.51474339e-05
Iter: 1010 loss: 2.51562888e-05
Iter: 1011 loss: 2.51358451e-05
Iter: 1012 loss: 2.5113859e-05
Iter: 1013 loss: 2.53724e-05
Iter: 1014 loss: 2.51136153e-05
Iter: 1015 loss: 2.5101097e-05
Iter: 1016 loss: 2.51044039e-05
Iter: 1017 loss: 2.50920202e-05
Iter: 1018 loss: 2.50725989e-05
Iter: 1019 loss: 2.52008522e-05
Iter: 1020 loss: 2.5070618e-05
Iter: 1021 loss: 2.50590019e-05
Iter: 1022 loss: 2.50579797e-05
Iter: 1023 loss: 2.50493867e-05
Iter: 1024 loss: 2.50302546e-05
Iter: 1025 loss: 2.5053625e-05
Iter: 1026 loss: 2.50201701e-05
Iter: 1027 loss: 2.50021585e-05
Iter: 1028 loss: 2.49986915e-05
Iter: 1029 loss: 2.49867153e-05
Iter: 1030 loss: 2.49712757e-05
Iter: 1031 loss: 2.4970539e-05
Iter: 1032 loss: 2.49581153e-05
Iter: 1033 loss: 2.49360983e-05
Iter: 1034 loss: 2.49360382e-05
Iter: 1035 loss: 2.49103105e-05
Iter: 1036 loss: 2.49589466e-05
Iter: 1037 loss: 2.48994402e-05
Iter: 1038 loss: 2.48867036e-05
Iter: 1039 loss: 2.48836695e-05
Iter: 1040 loss: 2.48711985e-05
Iter: 1041 loss: 2.4847257e-05
Iter: 1042 loss: 2.53495637e-05
Iter: 1043 loss: 2.48471461e-05
Iter: 1044 loss: 2.48357628e-05
Iter: 1045 loss: 2.48329088e-05
Iter: 1046 loss: 2.48212054e-05
Iter: 1047 loss: 2.48124852e-05
Iter: 1048 loss: 2.48086326e-05
Iter: 1049 loss: 2.47977696e-05
Iter: 1050 loss: 2.47972384e-05
Iter: 1051 loss: 2.47896151e-05
Iter: 1052 loss: 2.47759745e-05
Iter: 1053 loss: 2.47759926e-05
Iter: 1054 loss: 2.47599855e-05
Iter: 1055 loss: 2.48623364e-05
Iter: 1056 loss: 2.47581702e-05
Iter: 1057 loss: 2.47458047e-05
Iter: 1058 loss: 2.47299395e-05
Iter: 1059 loss: 2.47287589e-05
Iter: 1060 loss: 2.47125645e-05
Iter: 1061 loss: 2.49144e-05
Iter: 1062 loss: 2.47124644e-05
Iter: 1063 loss: 2.46963773e-05
Iter: 1064 loss: 2.47046664e-05
Iter: 1065 loss: 2.46857016e-05
Iter: 1066 loss: 2.46690506e-05
Iter: 1067 loss: 2.46594063e-05
Iter: 1068 loss: 2.46522432e-05
Iter: 1069 loss: 2.46326581e-05
Iter: 1070 loss: 2.48722517e-05
Iter: 1071 loss: 2.46324544e-05
Iter: 1072 loss: 2.461228e-05
Iter: 1073 loss: 2.46520231e-05
Iter: 1074 loss: 2.46038908e-05
Iter: 1075 loss: 2.45876709e-05
Iter: 1076 loss: 2.46224e-05
Iter: 1077 loss: 2.45813026e-05
Iter: 1078 loss: 2.45610754e-05
Iter: 1079 loss: 2.46992677e-05
Iter: 1080 loss: 2.45591546e-05
Iter: 1081 loss: 2.45506271e-05
Iter: 1082 loss: 2.46069285e-05
Iter: 1083 loss: 2.45497213e-05
Iter: 1084 loss: 2.45403226e-05
Iter: 1085 loss: 2.45300125e-05
Iter: 1086 loss: 2.45285519e-05
Iter: 1087 loss: 2.45171468e-05
Iter: 1088 loss: 2.45906267e-05
Iter: 1089 loss: 2.45159281e-05
Iter: 1090 loss: 2.45049487e-05
Iter: 1091 loss: 2.44963448e-05
Iter: 1092 loss: 2.4492987e-05
Iter: 1093 loss: 2.4477984e-05
Iter: 1094 loss: 2.45095944e-05
Iter: 1095 loss: 2.4472105e-05
Iter: 1096 loss: 2.44552575e-05
Iter: 1097 loss: 2.46149666e-05
Iter: 1098 loss: 2.44546136e-05
Iter: 1099 loss: 2.44428e-05
Iter: 1100 loss: 2.442185e-05
Iter: 1101 loss: 2.44218936e-05
Iter: 1102 loss: 2.439944e-05
Iter: 1103 loss: 2.44983548e-05
Iter: 1104 loss: 2.43950235e-05
Iter: 1105 loss: 2.43805571e-05
Iter: 1106 loss: 2.43801423e-05
Iter: 1107 loss: 2.43708328e-05
Iter: 1108 loss: 2.43632276e-05
Iter: 1109 loss: 2.43604591e-05
Iter: 1110 loss: 2.43505783e-05
Iter: 1111 loss: 2.43502909e-05
Iter: 1112 loss: 2.4343306e-05
Iter: 1113 loss: 2.43432733e-05
Iter: 1114 loss: 2.43377908e-05
Iter: 1115 loss: 2.43258837e-05
Iter: 1116 loss: 2.4369976e-05
Iter: 1117 loss: 2.43229333e-05
Iter: 1118 loss: 2.43160393e-05
Iter: 1119 loss: 2.43126779e-05
Iter: 1120 loss: 2.43094037e-05
Iter: 1121 loss: 2.42962851e-05
Iter: 1122 loss: 2.4328474e-05
Iter: 1123 loss: 2.42916358e-05
Iter: 1124 loss: 2.42802398e-05
Iter: 1125 loss: 2.42717451e-05
Iter: 1126 loss: 2.42680308e-05
Iter: 1127 loss: 2.42560091e-05
Iter: 1128 loss: 2.42558162e-05
Iter: 1129 loss: 2.42448186e-05
Iter: 1130 loss: 2.42302776e-05
Iter: 1131 loss: 2.42294154e-05
Iter: 1132 loss: 2.42126189e-05
Iter: 1133 loss: 2.42389906e-05
Iter: 1134 loss: 2.42047608e-05
Iter: 1135 loss: 2.41937196e-05
Iter: 1136 loss: 2.41934576e-05
Iter: 1137 loss: 2.41821799e-05
Iter: 1138 loss: 2.41767084e-05
Iter: 1139 loss: 2.41712714e-05
Iter: 1140 loss: 2.4162171e-05
Iter: 1141 loss: 2.41621347e-05
Iter: 1142 loss: 2.4153569e-05
Iter: 1143 loss: 2.41523194e-05
Iter: 1144 loss: 2.41462203e-05
Iter: 1145 loss: 2.41359085e-05
Iter: 1146 loss: 2.42471979e-05
Iter: 1147 loss: 2.41356702e-05
Iter: 1148 loss: 2.4129231e-05
Iter: 1149 loss: 2.41170947e-05
Iter: 1150 loss: 2.438517e-05
Iter: 1151 loss: 2.41171201e-05
Iter: 1152 loss: 2.41053513e-05
Iter: 1153 loss: 2.42385704e-05
Iter: 1154 loss: 2.4105113e-05
Iter: 1155 loss: 2.40956942e-05
Iter: 1156 loss: 2.40808913e-05
Iter: 1157 loss: 2.40807458e-05
Iter: 1158 loss: 2.40674453e-05
Iter: 1159 loss: 2.42041478e-05
Iter: 1160 loss: 2.40670597e-05
Iter: 1161 loss: 2.4053521e-05
Iter: 1162 loss: 2.40842201e-05
Iter: 1163 loss: 2.40484114e-05
Iter: 1164 loss: 2.40375139e-05
Iter: 1165 loss: 2.40260451e-05
Iter: 1166 loss: 2.40241461e-05
Iter: 1167 loss: 2.40073205e-05
Iter: 1168 loss: 2.41014786e-05
Iter: 1169 loss: 2.40050358e-05
Iter: 1170 loss: 2.39877518e-05
Iter: 1171 loss: 2.41177459e-05
Iter: 1172 loss: 2.39864257e-05
Iter: 1173 loss: 2.39756664e-05
Iter: 1174 loss: 2.39814035e-05
Iter: 1175 loss: 2.39686069e-05
Iter: 1176 loss: 2.39541623e-05
Iter: 1177 loss: 2.41040689e-05
Iter: 1178 loss: 2.3953753e-05
Iter: 1179 loss: 2.39468791e-05
Iter: 1180 loss: 2.3974033e-05
Iter: 1181 loss: 2.39452911e-05
Iter: 1182 loss: 2.39373421e-05
Iter: 1183 loss: 2.3923978e-05
Iter: 1184 loss: 2.39239707e-05
Iter: 1185 loss: 2.39121655e-05
Iter: 1186 loss: 2.39924248e-05
Iter: 1187 loss: 2.39110195e-05
Iter: 1188 loss: 2.3898534e-05
Iter: 1189 loss: 2.38991706e-05
Iter: 1190 loss: 2.38887e-05
Iter: 1191 loss: 2.38743123e-05
Iter: 1192 loss: 2.38815301e-05
Iter: 1193 loss: 2.3864668e-05
Iter: 1194 loss: 2.38498797e-05
Iter: 1195 loss: 2.38498051e-05
Iter: 1196 loss: 2.38390712e-05
Iter: 1197 loss: 2.38205612e-05
Iter: 1198 loss: 2.38206085e-05
Iter: 1199 loss: 2.38024804e-05
Iter: 1200 loss: 2.38496959e-05
Iter: 1201 loss: 2.37963031e-05
Iter: 1202 loss: 2.37836248e-05
Iter: 1203 loss: 2.37831646e-05
Iter: 1204 loss: 2.37720778e-05
Iter: 1205 loss: 2.37636559e-05
Iter: 1206 loss: 2.37601344e-05
Iter: 1207 loss: 2.37513377e-05
Iter: 1208 loss: 2.37503755e-05
Iter: 1209 loss: 2.37429031e-05
Iter: 1210 loss: 2.37386594e-05
Iter: 1211 loss: 2.3735447e-05
Iter: 1212 loss: 2.37215827e-05
Iter: 1213 loss: 2.37604072e-05
Iter: 1214 loss: 2.37171735e-05
Iter: 1215 loss: 2.37081549e-05
Iter: 1216 loss: 2.36974793e-05
Iter: 1217 loss: 2.36963315e-05
Iter: 1218 loss: 2.36797569e-05
Iter: 1219 loss: 2.38291577e-05
Iter: 1220 loss: 2.36788765e-05
Iter: 1221 loss: 2.36675041e-05
Iter: 1222 loss: 2.36499618e-05
Iter: 1223 loss: 2.36497217e-05
Iter: 1224 loss: 2.36339911e-05
Iter: 1225 loss: 2.3633982e-05
Iter: 1226 loss: 2.36186097e-05
Iter: 1227 loss: 2.36245542e-05
Iter: 1228 loss: 2.36080214e-05
Iter: 1229 loss: 2.35925727e-05
Iter: 1230 loss: 2.35835141e-05
Iter: 1231 loss: 2.35770276e-05
Iter: 1232 loss: 2.35597072e-05
Iter: 1233 loss: 2.37674e-05
Iter: 1234 loss: 2.35594762e-05
Iter: 1235 loss: 2.35420375e-05
Iter: 1236 loss: 2.36041706e-05
Iter: 1237 loss: 2.35375283e-05
Iter: 1238 loss: 2.35265907e-05
Iter: 1239 loss: 2.35893513e-05
Iter: 1240 loss: 2.35251573e-05
Iter: 1241 loss: 2.35118769e-05
Iter: 1242 loss: 2.35267435e-05
Iter: 1243 loss: 2.35047719e-05
Iter: 1244 loss: 2.34929248e-05
Iter: 1245 loss: 2.35705629e-05
Iter: 1246 loss: 2.34916843e-05
Iter: 1247 loss: 2.34823146e-05
Iter: 1248 loss: 2.34640465e-05
Iter: 1249 loss: 2.38351331e-05
Iter: 1250 loss: 2.34639356e-05
Iter: 1251 loss: 2.34481704e-05
Iter: 1252 loss: 2.36234901e-05
Iter: 1253 loss: 2.34478684e-05
Iter: 1254 loss: 2.34328545e-05
Iter: 1255 loss: 2.34291037e-05
Iter: 1256 loss: 2.34196486e-05
Iter: 1257 loss: 2.34022846e-05
Iter: 1258 loss: 2.34255094e-05
Iter: 1259 loss: 2.33935079e-05
Iter: 1260 loss: 2.33749888e-05
Iter: 1261 loss: 2.36367177e-05
Iter: 1262 loss: 2.33749161e-05
Iter: 1263 loss: 2.33631472e-05
Iter: 1264 loss: 2.33422925e-05
Iter: 1265 loss: 2.3342287e-05
Iter: 1266 loss: 2.33191404e-05
Iter: 1267 loss: 2.33769824e-05
Iter: 1268 loss: 2.33110295e-05
Iter: 1269 loss: 2.32963539e-05
Iter: 1270 loss: 2.32949878e-05
Iter: 1271 loss: 2.32824932e-05
Iter: 1272 loss: 2.32702296e-05
Iter: 1273 loss: 2.32675011e-05
Iter: 1274 loss: 2.32546863e-05
Iter: 1275 loss: 2.32533712e-05
Iter: 1276 loss: 2.32454968e-05
Iter: 1277 loss: 2.32383682e-05
Iter: 1278 loss: 2.32364619e-05
Iter: 1279 loss: 2.32192269e-05
Iter: 1280 loss: 2.32298e-05
Iter: 1281 loss: 2.32080511e-05
Iter: 1282 loss: 2.31927552e-05
Iter: 1283 loss: 2.31899467e-05
Iter: 1284 loss: 2.31795093e-05
Iter: 1285 loss: 2.31578324e-05
Iter: 1286 loss: 2.33954688e-05
Iter: 1287 loss: 2.31573395e-05
Iter: 1288 loss: 2.31440627e-05
Iter: 1289 loss: 2.31241938e-05
Iter: 1290 loss: 2.31237791e-05
Iter: 1291 loss: 2.31032645e-05
Iter: 1292 loss: 2.3363209e-05
Iter: 1293 loss: 2.31030826e-05
Iter: 1294 loss: 2.30828045e-05
Iter: 1295 loss: 2.30965779e-05
Iter: 1296 loss: 2.30700753e-05
Iter: 1297 loss: 2.30500445e-05
Iter: 1298 loss: 2.30303849e-05
Iter: 1299 loss: 2.30261758e-05
Iter: 1300 loss: 2.30052829e-05
Iter: 1301 loss: 2.30051155e-05
Iter: 1302 loss: 2.29854413e-05
Iter: 1303 loss: 2.30611e-05
Iter: 1304 loss: 2.29808738e-05
Iter: 1305 loss: 2.29696689e-05
Iter: 1306 loss: 2.30770529e-05
Iter: 1307 loss: 2.29692869e-05
Iter: 1308 loss: 2.29575016e-05
Iter: 1309 loss: 2.29492289e-05
Iter: 1310 loss: 2.29449652e-05
Iter: 1311 loss: 2.29310281e-05
Iter: 1312 loss: 2.30404985e-05
Iter: 1313 loss: 2.29300658e-05
Iter: 1314 loss: 2.29194193e-05
Iter: 1315 loss: 2.28956887e-05
Iter: 1316 loss: 2.32400744e-05
Iter: 1317 loss: 2.28946301e-05
Iter: 1318 loss: 2.28717709e-05
Iter: 1319 loss: 2.31006434e-05
Iter: 1320 loss: 2.28709505e-05
Iter: 1321 loss: 2.28474546e-05
Iter: 1322 loss: 2.28703902e-05
Iter: 1323 loss: 2.28341742e-05
Iter: 1324 loss: 2.28142871e-05
Iter: 1325 loss: 2.28171757e-05
Iter: 1326 loss: 2.27991804e-05
Iter: 1327 loss: 2.27786695e-05
Iter: 1328 loss: 2.27784149e-05
Iter: 1329 loss: 2.276331e-05
Iter: 1330 loss: 2.27349683e-05
Iter: 1331 loss: 2.33564697e-05
Iter: 1332 loss: 2.2734861e-05
Iter: 1333 loss: 2.27048222e-05
Iter: 1334 loss: 2.27781347e-05
Iter: 1335 loss: 2.2694061e-05
Iter: 1336 loss: 2.2677079e-05
Iter: 1337 loss: 2.2675842e-05
Iter: 1338 loss: 2.26589327e-05
Iter: 1339 loss: 2.26649299e-05
Iter: 1340 loss: 2.26470256e-05
Iter: 1341 loss: 2.26266384e-05
Iter: 1342 loss: 2.28727258e-05
Iter: 1343 loss: 2.26263564e-05
Iter: 1344 loss: 2.26172306e-05
Iter: 1345 loss: 2.26088032e-05
Iter: 1346 loss: 2.26066095e-05
Iter: 1347 loss: 2.2585542e-05
Iter: 1348 loss: 2.25913082e-05
Iter: 1349 loss: 2.25702424e-05
Iter: 1350 loss: 2.2549495e-05
Iter: 1351 loss: 2.25508375e-05
Iter: 1352 loss: 2.25332769e-05
Iter: 1353 loss: 2.25069707e-05
Iter: 1354 loss: 2.28118806e-05
Iter: 1355 loss: 2.25065924e-05
Iter: 1356 loss: 2.24854521e-05
Iter: 1357 loss: 2.24587347e-05
Iter: 1358 loss: 2.24566793e-05
Iter: 1359 loss: 2.24281412e-05
Iter: 1360 loss: 2.2635928e-05
Iter: 1361 loss: 2.24257237e-05
Iter: 1362 loss: 2.23976494e-05
Iter: 1363 loss: 2.25307231e-05
Iter: 1364 loss: 2.23925108e-05
Iter: 1365 loss: 2.23734423e-05
Iter: 1366 loss: 2.23456409e-05
Iter: 1367 loss: 2.23448915e-05
Iter: 1368 loss: 2.23166317e-05
Iter: 1369 loss: 2.25548865e-05
Iter: 1370 loss: 2.23149873e-05
Iter: 1371 loss: 2.22913404e-05
Iter: 1372 loss: 2.25637086e-05
Iter: 1373 loss: 2.22909257e-05
Iter: 1374 loss: 2.22771159e-05
Iter: 1375 loss: 2.23565694e-05
Iter: 1376 loss: 2.22752387e-05
Iter: 1377 loss: 2.22604322e-05
Iter: 1378 loss: 2.2236738e-05
Iter: 1379 loss: 2.22365015e-05
Iter: 1380 loss: 2.22159433e-05
Iter: 1381 loss: 2.24139549e-05
Iter: 1382 loss: 2.22151757e-05
Iter: 1383 loss: 2.21960472e-05
Iter: 1384 loss: 2.21642731e-05
Iter: 1385 loss: 2.21641712e-05
Iter: 1386 loss: 2.21359442e-05
Iter: 1387 loss: 2.22431408e-05
Iter: 1388 loss: 2.21291666e-05
Iter: 1389 loss: 2.2097518e-05
Iter: 1390 loss: 2.23016232e-05
Iter: 1391 loss: 2.20940547e-05
Iter: 1392 loss: 2.20734946e-05
Iter: 1393 loss: 2.20410329e-05
Iter: 1394 loss: 2.20406546e-05
Iter: 1395 loss: 2.20140064e-05
Iter: 1396 loss: 2.20129332e-05
Iter: 1397 loss: 2.19902431e-05
Iter: 1398 loss: 2.19843751e-05
Iter: 1399 loss: 2.19702306e-05
Iter: 1400 loss: 2.19404719e-05
Iter: 1401 loss: 2.19323592e-05
Iter: 1402 loss: 2.1914022e-05
Iter: 1403 loss: 2.19086032e-05
Iter: 1404 loss: 2.18948899e-05
Iter: 1405 loss: 2.18768673e-05
Iter: 1406 loss: 2.18936802e-05
Iter: 1407 loss: 2.18665155e-05
Iter: 1408 loss: 2.18429923e-05
Iter: 1409 loss: 2.19282101e-05
Iter: 1410 loss: 2.18370733e-05
Iter: 1411 loss: 2.18221903e-05
Iter: 1412 loss: 2.18029709e-05
Iter: 1413 loss: 2.18016794e-05
Iter: 1414 loss: 2.1771044e-05
Iter: 1415 loss: 2.19760914e-05
Iter: 1416 loss: 2.17678662e-05
Iter: 1417 loss: 2.17482739e-05
Iter: 1418 loss: 2.1713342e-05
Iter: 1419 loss: 2.25834392e-05
Iter: 1420 loss: 2.17133711e-05
Iter: 1421 loss: 2.16809458e-05
Iter: 1422 loss: 2.20901802e-05
Iter: 1423 loss: 2.16806475e-05
Iter: 1424 loss: 2.16478038e-05
Iter: 1425 loss: 2.16850858e-05
Iter: 1426 loss: 2.16299923e-05
Iter: 1427 loss: 2.15993477e-05
Iter: 1428 loss: 2.15907185e-05
Iter: 1429 loss: 2.15720174e-05
Iter: 1430 loss: 2.15430264e-05
Iter: 1431 loss: 2.15408309e-05
Iter: 1432 loss: 2.15218934e-05
Iter: 1433 loss: 2.14866886e-05
Iter: 1434 loss: 2.22777871e-05
Iter: 1435 loss: 2.14865559e-05
Iter: 1436 loss: 2.14489919e-05
Iter: 1437 loss: 2.15771543e-05
Iter: 1438 loss: 2.14389274e-05
Iter: 1439 loss: 2.14330976e-05
Iter: 1440 loss: 2.14191168e-05
Iter: 1441 loss: 2.14063075e-05
Iter: 1442 loss: 2.13979347e-05
Iter: 1443 loss: 2.13930161e-05
Iter: 1444 loss: 2.13686217e-05
Iter: 1445 loss: 2.1364458e-05
Iter: 1446 loss: 2.13478052e-05
Iter: 1447 loss: 2.13250205e-05
Iter: 1448 loss: 2.13943076e-05
Iter: 1449 loss: 2.13183157e-05
Iter: 1450 loss: 2.12870018e-05
Iter: 1451 loss: 2.13311723e-05
Iter: 1452 loss: 2.12716623e-05
Iter: 1453 loss: 2.12461891e-05
Iter: 1454 loss: 2.12283558e-05
Iter: 1455 loss: 2.12193e-05
Iter: 1456 loss: 2.11905917e-05
Iter: 1457 loss: 2.16432818e-05
Iter: 1458 loss: 2.11906e-05
Iter: 1459 loss: 2.1162321e-05
Iter: 1460 loss: 2.11872102e-05
Iter: 1461 loss: 2.11457154e-05
Iter: 1462 loss: 2.11221595e-05
Iter: 1463 loss: 2.11361676e-05
Iter: 1464 loss: 2.11071238e-05
Iter: 1465 loss: 2.10800972e-05
Iter: 1466 loss: 2.14867759e-05
Iter: 1467 loss: 2.10800936e-05
Iter: 1468 loss: 2.10631224e-05
Iter: 1469 loss: 2.10293474e-05
Iter: 1470 loss: 2.1684551e-05
Iter: 1471 loss: 2.10289181e-05
Iter: 1472 loss: 2.09958e-05
Iter: 1473 loss: 2.1241518e-05
Iter: 1474 loss: 2.09930149e-05
Iter: 1475 loss: 2.09783284e-05
Iter: 1476 loss: 2.09731807e-05
Iter: 1477 loss: 2.09643404e-05
Iter: 1478 loss: 2.09527934e-05
Iter: 1479 loss: 2.09520222e-05
Iter: 1480 loss: 2.09319e-05
Iter: 1481 loss: 2.09237078e-05
Iter: 1482 loss: 2.09130467e-05
Iter: 1483 loss: 2.08915517e-05
Iter: 1484 loss: 2.09632599e-05
Iter: 1485 loss: 2.08856545e-05
Iter: 1486 loss: 2.08575329e-05
Iter: 1487 loss: 2.09490991e-05
Iter: 1488 loss: 2.08496149e-05
Iter: 1489 loss: 2.08301208e-05
Iter: 1490 loss: 2.07988196e-05
Iter: 1491 loss: 2.07985322e-05
Iter: 1492 loss: 2.07639769e-05
Iter: 1493 loss: 2.10021462e-05
Iter: 1494 loss: 2.07608082e-05
Iter: 1495 loss: 2.0727146e-05
Iter: 1496 loss: 2.09984246e-05
Iter: 1497 loss: 2.07249632e-05
Iter: 1498 loss: 2.07047015e-05
Iter: 1499 loss: 2.06830809e-05
Iter: 1500 loss: 2.0679563e-05
Iter: 1501 loss: 2.06569803e-05
Iter: 1502 loss: 2.06567202e-05
Iter: 1503 loss: 2.0637508e-05
Iter: 1504 loss: 2.06099248e-05
Iter: 1505 loss: 2.06090554e-05
Iter: 1506 loss: 2.05781271e-05
Iter: 1507 loss: 2.06595578e-05
Iter: 1508 loss: 2.05676079e-05
Iter: 1509 loss: 2.05824181e-05
Iter: 1510 loss: 2.05552496e-05
Iter: 1511 loss: 2.0548503e-05
Iter: 1512 loss: 2.0527872e-05
Iter: 1513 loss: 2.05737506e-05
Iter: 1514 loss: 2.05155393e-05
Iter: 1515 loss: 2.04933276e-05
Iter: 1516 loss: 2.0493284e-05
Iter: 1517 loss: 2.04752905e-05
Iter: 1518 loss: 2.04482567e-05
Iter: 1519 loss: 2.04476746e-05
Iter: 1520 loss: 2.04130592e-05
Iter: 1521 loss: 2.044346e-05
Iter: 1522 loss: 2.03926356e-05
Iter: 1523 loss: 2.03762065e-05
Iter: 1524 loss: 2.03696545e-05
Iter: 1525 loss: 2.03504551e-05
Iter: 1526 loss: 2.03133131e-05
Iter: 1527 loss: 2.10771286e-05
Iter: 1528 loss: 2.03130548e-05
Iter: 1529 loss: 2.02813426e-05
Iter: 1530 loss: 2.03802156e-05
Iter: 1531 loss: 2.02720257e-05
Iter: 1532 loss: 2.02398551e-05
Iter: 1533 loss: 2.05025899e-05
Iter: 1534 loss: 2.02376832e-05
Iter: 1535 loss: 2.02161864e-05
Iter: 1536 loss: 2.02071278e-05
Iter: 1537 loss: 2.01959119e-05
Iter: 1538 loss: 2.01753792e-05
Iter: 1539 loss: 2.02259362e-05
Iter: 1540 loss: 2.01680923e-05
Iter: 1541 loss: 2.01495877e-05
Iter: 1542 loss: 2.01554285e-05
Iter: 1543 loss: 2.01363546e-05
Iter: 1544 loss: 2.01155235e-05
Iter: 1545 loss: 2.01155362e-05
Iter: 1546 loss: 2.01036564e-05
Iter: 1547 loss: 2.00783543e-05
Iter: 1548 loss: 2.04936769e-05
Iter: 1549 loss: 2.0077583e-05
Iter: 1550 loss: 2.00523245e-05
Iter: 1551 loss: 2.02071096e-05
Iter: 1552 loss: 2.00492977e-05
Iter: 1553 loss: 2.00226295e-05
Iter: 1554 loss: 2.00743434e-05
Iter: 1555 loss: 2.00114937e-05
Iter: 1556 loss: 1.99868737e-05
Iter: 1557 loss: 1.9971947e-05
Iter: 1558 loss: 1.99618898e-05
Iter: 1559 loss: 1.9940655e-05
Iter: 1560 loss: 1.99390925e-05
Iter: 1561 loss: 1.9916688e-05
Iter: 1562 loss: 1.99082206e-05
Iter: 1563 loss: 1.98958678e-05
Iter: 1564 loss: 1.98725447e-05
Iter: 1565 loss: 2.01572439e-05
Iter: 1566 loss: 1.98722519e-05
Iter: 1567 loss: 1.98514972e-05
Iter: 1568 loss: 1.98287216e-05
Iter: 1569 loss: 1.98253401e-05
Iter: 1570 loss: 1.98026901e-05
Iter: 1571 loss: 1.98009511e-05
Iter: 1572 loss: 1.97763202e-05
Iter: 1573 loss: 1.97602058e-05
Iter: 1574 loss: 1.97507561e-05
Iter: 1575 loss: 1.97361042e-05
Iter: 1576 loss: 1.97360641e-05
Iter: 1577 loss: 1.97230838e-05
Iter: 1578 loss: 1.96883029e-05
Iter: 1579 loss: 1.9913492e-05
Iter: 1580 loss: 1.96796173e-05
Iter: 1581 loss: 1.96444471e-05
Iter: 1582 loss: 1.98214e-05
Iter: 1583 loss: 1.96386609e-05
Iter: 1584 loss: 1.96085857e-05
Iter: 1585 loss: 1.97758382e-05
Iter: 1586 loss: 1.96042893e-05
Iter: 1587 loss: 1.95790763e-05
Iter: 1588 loss: 1.96418478e-05
Iter: 1589 loss: 1.95701923e-05
Iter: 1590 loss: 1.95489629e-05
Iter: 1591 loss: 1.95497378e-05
Iter: 1592 loss: 1.95321827e-05
Iter: 1593 loss: 1.95093835e-05
Iter: 1594 loss: 1.95093598e-05
Iter: 1595 loss: 1.94914937e-05
Iter: 1596 loss: 1.94639251e-05
Iter: 1597 loss: 1.94634504e-05
Iter: 1598 loss: 1.94413224e-05
Iter: 1599 loss: 1.94411477e-05
Iter: 1600 loss: 1.94211261e-05
Iter: 1601 loss: 1.9432915e-05
Iter: 1602 loss: 1.94081404e-05
Iter: 1603 loss: 1.9396688e-05
Iter: 1604 loss: 1.93948108e-05
Iter: 1605 loss: 1.93873784e-05
Iter: 1606 loss: 1.93830238e-05
Iter: 1607 loss: 1.93799133e-05
Iter: 1608 loss: 1.93628675e-05
Iter: 1609 loss: 1.93711912e-05
Iter: 1610 loss: 1.93514225e-05
Iter: 1611 loss: 1.93370179e-05
Iter: 1612 loss: 1.93110536e-05
Iter: 1613 loss: 1.99488495e-05
Iter: 1614 loss: 1.93111391e-05
Iter: 1615 loss: 1.92852676e-05
Iter: 1616 loss: 1.96318288e-05
Iter: 1617 loss: 1.92851076e-05
Iter: 1618 loss: 1.9263096e-05
Iter: 1619 loss: 1.93057422e-05
Iter: 1620 loss: 1.92539683e-05
Iter: 1621 loss: 1.92331754e-05
Iter: 1622 loss: 1.92608204e-05
Iter: 1623 loss: 1.92227326e-05
Iter: 1624 loss: 1.920063e-05
Iter: 1625 loss: 1.93242849e-05
Iter: 1626 loss: 1.91974723e-05
Iter: 1627 loss: 1.91762465e-05
Iter: 1628 loss: 1.92323059e-05
Iter: 1629 loss: 1.91691943e-05
Iter: 1630 loss: 1.91502295e-05
Iter: 1631 loss: 1.91513191e-05
Iter: 1632 loss: 1.91354302e-05
Iter: 1633 loss: 1.91172912e-05
Iter: 1634 loss: 1.91172e-05
Iter: 1635 loss: 1.91036252e-05
Iter: 1636 loss: 1.91282234e-05
Iter: 1637 loss: 1.90976389e-05
Iter: 1638 loss: 1.90811e-05
Iter: 1639 loss: 1.91868367e-05
Iter: 1640 loss: 1.90792125e-05
Iter: 1641 loss: 1.90686842e-05
Iter: 1642 loss: 1.90951196e-05
Iter: 1643 loss: 1.90650753e-05
Iter: 1644 loss: 1.90536339e-05
Iter: 1645 loss: 1.90326937e-05
Iter: 1646 loss: 1.95213179e-05
Iter: 1647 loss: 1.90326737e-05
Iter: 1648 loss: 1.90120627e-05
Iter: 1649 loss: 1.90176434e-05
Iter: 1650 loss: 1.89971688e-05
Iter: 1651 loss: 1.89772145e-05
Iter: 1652 loss: 1.89768944e-05
Iter: 1653 loss: 1.89596794e-05
Iter: 1654 loss: 1.89425737e-05
Iter: 1655 loss: 1.89390594e-05
Iter: 1656 loss: 1.89183829e-05
Iter: 1657 loss: 1.90681058e-05
Iter: 1658 loss: 1.89165512e-05
Iter: 1659 loss: 1.88943486e-05
Iter: 1660 loss: 1.89496568e-05
Iter: 1661 loss: 1.88864506e-05
Iter: 1662 loss: 1.88670947e-05
Iter: 1663 loss: 1.88896156e-05
Iter: 1664 loss: 1.88568065e-05
Iter: 1665 loss: 1.88367394e-05
Iter: 1666 loss: 1.89868988e-05
Iter: 1667 loss: 1.88351369e-05
Iter: 1668 loss: 1.88193299e-05
Iter: 1669 loss: 1.88915874e-05
Iter: 1670 loss: 1.88163176e-05
Iter: 1671 loss: 1.88067715e-05
Iter: 1672 loss: 1.89488164e-05
Iter: 1673 loss: 1.88066733e-05
Iter: 1674 loss: 1.87999503e-05
Iter: 1675 loss: 1.87914357e-05
Iter: 1676 loss: 1.8790819e-05
Iter: 1677 loss: 1.87768637e-05
Iter: 1678 loss: 1.88151935e-05
Iter: 1679 loss: 1.87723435e-05
Iter: 1680 loss: 1.87607275e-05
Iter: 1681 loss: 1.87418591e-05
Iter: 1682 loss: 1.87417609e-05
Iter: 1683 loss: 1.8720737e-05
Iter: 1684 loss: 1.88170197e-05
Iter: 1685 loss: 1.87166697e-05
Iter: 1686 loss: 1.8696006e-05
Iter: 1687 loss: 1.88706235e-05
Iter: 1688 loss: 1.86948782e-05
Iter: 1689 loss: 1.86814868e-05
Iter: 1690 loss: 1.86676571e-05
Iter: 1691 loss: 1.86651723e-05
Iter: 1692 loss: 1.86502875e-05
Iter: 1693 loss: 1.8650122e-05
Iter: 1694 loss: 1.86360467e-05
Iter: 1695 loss: 1.86246834e-05
Iter: 1696 loss: 1.86204888e-05
Iter: 1697 loss: 1.86031903e-05
Iter: 1698 loss: 1.8668703e-05
Iter: 1699 loss: 1.8599012e-05
Iter: 1700 loss: 1.85841091e-05
Iter: 1701 loss: 1.85840909e-05
Iter: 1702 loss: 1.85774297e-05
Iter: 1703 loss: 1.86154357e-05
Iter: 1704 loss: 1.8576502e-05
Iter: 1705 loss: 1.85695953e-05
Iter: 1706 loss: 1.85652607e-05
Iter: 1707 loss: 1.85625577e-05
Iter: 1708 loss: 1.85526878e-05
Iter: 1709 loss: 1.85842182e-05
Iter: 1710 loss: 1.85498684e-05
Iter: 1711 loss: 1.85393383e-05
Iter: 1712 loss: 1.85227054e-05
Iter: 1713 loss: 1.85225399e-05
Iter: 1714 loss: 1.85020144e-05
Iter: 1715 loss: 1.85337e-05
Iter: 1716 loss: 1.84923447e-05
Iter: 1717 loss: 1.8477971e-05
Iter: 1718 loss: 1.84774963e-05
Iter: 1719 loss: 1.8466184e-05
Iter: 1720 loss: 1.84518067e-05
Iter: 1721 loss: 1.84507589e-05
Iter: 1722 loss: 1.84344135e-05
Iter: 1723 loss: 1.8562605e-05
Iter: 1724 loss: 1.84332712e-05
Iter: 1725 loss: 1.84162982e-05
Iter: 1726 loss: 1.84605e-05
Iter: 1727 loss: 1.84105083e-05
Iter: 1728 loss: 1.83976881e-05
Iter: 1729 loss: 1.83925149e-05
Iter: 1730 loss: 1.83857283e-05
Iter: 1731 loss: 1.83779321e-05
Iter: 1732 loss: 1.83747725e-05
Iter: 1733 loss: 1.83687771e-05
Iter: 1734 loss: 1.83828688e-05
Iter: 1735 loss: 1.83666707e-05
Iter: 1736 loss: 1.83589127e-05
Iter: 1737 loss: 1.83595639e-05
Iter: 1738 loss: 1.83528973e-05
Iter: 1739 loss: 1.83440898e-05
Iter: 1740 loss: 1.83644133e-05
Iter: 1741 loss: 1.83406974e-05
Iter: 1742 loss: 1.83295706e-05
Iter: 1743 loss: 1.83172597e-05
Iter: 1744 loss: 1.83154989e-05
Iter: 1745 loss: 1.82982621e-05
Iter: 1746 loss: 1.83167613e-05
Iter: 1747 loss: 1.82887889e-05
Iter: 1748 loss: 1.82747353e-05
Iter: 1749 loss: 1.82747517e-05
Iter: 1750 loss: 1.82618769e-05
Iter: 1751 loss: 1.82570257e-05
Iter: 1752 loss: 1.82500135e-05
Iter: 1753 loss: 1.82353251e-05
Iter: 1754 loss: 1.82945514e-05
Iter: 1755 loss: 1.82321091e-05
Iter: 1756 loss: 1.8217379e-05
Iter: 1757 loss: 1.83240409e-05
Iter: 1758 loss: 1.82161129e-05
Iter: 1759 loss: 1.82064905e-05
Iter: 1760 loss: 1.81896175e-05
Iter: 1761 loss: 1.81896066e-05
Iter: 1762 loss: 1.81892901e-05
Iter: 1763 loss: 1.81812175e-05
Iter: 1764 loss: 1.81747819e-05
Iter: 1765 loss: 1.81799878e-05
Iter: 1766 loss: 1.81708892e-05
Iter: 1767 loss: 1.81611504e-05
Iter: 1768 loss: 1.81765354e-05
Iter: 1769 loss: 1.81566065e-05
Iter: 1770 loss: 1.81482046e-05
Iter: 1771 loss: 1.81578871e-05
Iter: 1772 loss: 1.81436571e-05
Iter: 1773 loss: 1.81319519e-05
Iter: 1774 loss: 1.8129871e-05
Iter: 1775 loss: 1.81218511e-05
Iter: 1776 loss: 1.81071518e-05
Iter: 1777 loss: 1.8107512e-05
Iter: 1778 loss: 1.80955722e-05
Iter: 1779 loss: 1.80804454e-05
Iter: 1780 loss: 1.82639269e-05
Iter: 1781 loss: 1.80804018e-05
Iter: 1782 loss: 1.80665083e-05
Iter: 1783 loss: 1.80888565e-05
Iter: 1784 loss: 1.80600982e-05
Iter: 1785 loss: 1.80485222e-05
Iter: 1786 loss: 1.80630632e-05
Iter: 1787 loss: 1.80425159e-05
Iter: 1788 loss: 1.80295319e-05
Iter: 1789 loss: 1.81724681e-05
Iter: 1790 loss: 1.80293355e-05
Iter: 1791 loss: 1.80198731e-05
Iter: 1792 loss: 1.80038987e-05
Iter: 1793 loss: 1.80038151e-05
Iter: 1794 loss: 1.79989911e-05
Iter: 1795 loss: 1.79958442e-05
Iter: 1796 loss: 1.79880099e-05
Iter: 1797 loss: 1.79989565e-05
Iter: 1798 loss: 1.79842627e-05
Iter: 1799 loss: 1.79746548e-05
Iter: 1800 loss: 1.80002316e-05
Iter: 1801 loss: 1.79715171e-05
Iter: 1802 loss: 1.79638373e-05
Iter: 1803 loss: 1.79654162e-05
Iter: 1804 loss: 1.79581693e-05
Iter: 1805 loss: 1.79467897e-05
Iter: 1806 loss: 1.79654126e-05
Iter: 1807 loss: 1.79415729e-05
Iter: 1808 loss: 1.79314156e-05
Iter: 1809 loss: 1.79222188e-05
Iter: 1810 loss: 1.79197214e-05
Iter: 1811 loss: 1.79057934e-05
Iter: 1812 loss: 1.80281131e-05
Iter: 1813 loss: 1.7905013e-05
Iter: 1814 loss: 1.78920163e-05
Iter: 1815 loss: 1.79409744e-05
Iter: 1816 loss: 1.78888586e-05
Iter: 1817 loss: 1.78786504e-05
Iter: 1818 loss: 1.78765858e-05
Iter: 1819 loss: 1.78698829e-05
Iter: 1820 loss: 1.78589598e-05
Iter: 1821 loss: 1.78589125e-05
Iter: 1822 loss: 1.78504342e-05
Iter: 1823 loss: 1.78361042e-05
Iter: 1824 loss: 1.78361024e-05
Iter: 1825 loss: 1.78248702e-05
Iter: 1826 loss: 1.80003335e-05
Iter: 1827 loss: 1.78248156e-05
Iter: 1828 loss: 1.78135488e-05
Iter: 1829 loss: 1.78674127e-05
Iter: 1830 loss: 1.78114969e-05
Iter: 1831 loss: 1.78030568e-05
Iter: 1832 loss: 1.78358969e-05
Iter: 1833 loss: 1.78010887e-05
Iter: 1834 loss: 1.77947313e-05
Iter: 1835 loss: 1.77896836e-05
Iter: 1836 loss: 1.778769e-05
Iter: 1837 loss: 1.77760194e-05
Iter: 1838 loss: 1.78045411e-05
Iter: 1839 loss: 1.77718193e-05
Iter: 1840 loss: 1.77615148e-05
Iter: 1841 loss: 1.7754428e-05
Iter: 1842 loss: 1.77506863e-05
Iter: 1843 loss: 1.77374677e-05
Iter: 1844 loss: 1.7807708e-05
Iter: 1845 loss: 1.77354887e-05
Iter: 1846 loss: 1.77216607e-05
Iter: 1847 loss: 1.77898255e-05
Iter: 1848 loss: 1.77193433e-05
Iter: 1849 loss: 1.77083675e-05
Iter: 1850 loss: 1.77060938e-05
Iter: 1851 loss: 1.76988178e-05
Iter: 1852 loss: 1.76889134e-05
Iter: 1853 loss: 1.76888134e-05
Iter: 1854 loss: 1.76803842e-05
Iter: 1855 loss: 1.76689937e-05
Iter: 1856 loss: 1.76683534e-05
Iter: 1857 loss: 1.76563535e-05
Iter: 1858 loss: 1.77243674e-05
Iter: 1859 loss: 1.7654751e-05
Iter: 1860 loss: 1.76435169e-05
Iter: 1861 loss: 1.77968668e-05
Iter: 1862 loss: 1.76434442e-05
Iter: 1863 loss: 1.76373578e-05
Iter: 1864 loss: 1.76512276e-05
Iter: 1865 loss: 1.76351678e-05
Iter: 1866 loss: 1.76289377e-05
Iter: 1867 loss: 1.76209614e-05
Iter: 1868 loss: 1.76203812e-05
Iter: 1869 loss: 1.76090343e-05
Iter: 1870 loss: 1.76719368e-05
Iter: 1871 loss: 1.760745e-05
Iter: 1872 loss: 1.75995337e-05
Iter: 1873 loss: 1.75907044e-05
Iter: 1874 loss: 1.75895093e-05
Iter: 1875 loss: 1.75770438e-05
Iter: 1876 loss: 1.76183894e-05
Iter: 1877 loss: 1.75736386e-05
Iter: 1878 loss: 1.75615405e-05
Iter: 1879 loss: 1.76806861e-05
Iter: 1880 loss: 1.75611531e-05
Iter: 1881 loss: 1.75524492e-05
Iter: 1882 loss: 1.75461264e-05
Iter: 1883 loss: 1.7543156e-05
Iter: 1884 loss: 1.75336772e-05
Iter: 1885 loss: 1.76837966e-05
Iter: 1886 loss: 1.75336791e-05
Iter: 1887 loss: 1.75249588e-05
Iter: 1888 loss: 1.75198365e-05
Iter: 1889 loss: 1.75162422e-05
Iter: 1890 loss: 1.75054683e-05
Iter: 1891 loss: 1.75163477e-05
Iter: 1892 loss: 1.7499362e-05
Iter: 1893 loss: 1.749281e-05
Iter: 1894 loss: 1.74905836e-05
Iter: 1895 loss: 1.74858469e-05
Iter: 1896 loss: 1.74887864e-05
Iter: 1897 loss: 1.74829183e-05
Iter: 1898 loss: 1.74761699e-05
Iter: 1899 loss: 1.74675224e-05
Iter: 1900 loss: 1.74668858e-05
Iter: 1901 loss: 1.74567867e-05
Iter: 1902 loss: 1.75442292e-05
Iter: 1903 loss: 1.7456312e-05
Iter: 1904 loss: 1.74492525e-05
Iter: 1905 loss: 1.74399847e-05
Iter: 1906 loss: 1.74394336e-05
Iter: 1907 loss: 1.74276793e-05
Iter: 1908 loss: 1.74567303e-05
Iter: 1909 loss: 1.7423532e-05
Iter: 1910 loss: 1.74133202e-05
Iter: 1911 loss: 1.754607e-05
Iter: 1912 loss: 1.74132565e-05
Iter: 1913 loss: 1.74048546e-05
Iter: 1914 loss: 1.73971421e-05
Iter: 1915 loss: 1.73950939e-05
Iter: 1916 loss: 1.73856097e-05
Iter: 1917 loss: 1.75133955e-05
Iter: 1918 loss: 1.73855369e-05
Iter: 1919 loss: 1.7376864e-05
Iter: 1920 loss: 1.73836579e-05
Iter: 1921 loss: 1.73716726e-05
Iter: 1922 loss: 1.73630979e-05
Iter: 1923 loss: 1.7355278e-05
Iter: 1924 loss: 1.73531735e-05
Iter: 1925 loss: 1.73592271e-05
Iter: 1926 loss: 1.73475819e-05
Iter: 1927 loss: 1.73439221e-05
Iter: 1928 loss: 1.73419267e-05
Iter: 1929 loss: 1.73402841e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script72
+ '[' -r STOP.script72 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.8/300_300_300_1 '
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi1.2
+ date
Sat Oct 31 15:27:00 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi0.8/300_300_300_1 --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 30 --learning_rate 0.001 --decay_rate 0.8 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a453c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a4640d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a46406a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a4640950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97f0016a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97f0016d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a42f39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a43d5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a430d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a42b1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a43d52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a4407d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a44078c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a4366730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a4360400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a4200e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a420e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a41f4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a43336a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a4333d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a431f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a431f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a4109730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a409a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a409aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a40a5ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a408c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a4049840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a4049620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a402e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f979c7b97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f979c70c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f979c72d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a443dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f97a4448a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f979c7667b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.008840483
test_loss: 0.008850923
train_loss: 0.0063875895
test_loss: 0.006738455
train_loss: 0.005668355
test_loss: 0.0060776696
train_loss: 0.0053821
test_loss: 0.0058994265
train_loss: 0.0050680526
test_loss: 0.005834117
train_loss: 0.005262814
test_loss: 0.0058201137
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45d20fc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45d2094d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45d2094400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45d20947b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45d1fae0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45d1fb4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45d1f629d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45d1ed1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45d1ed1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45d1f06378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45d1f30d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7b94730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7b94378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7b1b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7ba89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7bc6d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7bbd400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7bbd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7aec730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7a97d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7a1b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7a1b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c79fd730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7a79950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7a79158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7955400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7922840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c79426a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7917950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c78e2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c78958c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c78c91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c799c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c79a9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c78258c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45c7852f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.8354239e-05
Iter: 2 loss: 4.89494123e-05
Iter: 3 loss: 4.77471622e-05
Iter: 4 loss: 4.75434572e-05
Iter: 5 loss: 4.7453872e-05
Iter: 6 loss: 4.735e-05
Iter: 7 loss: 4.7052712e-05
Iter: 8 loss: 4.99183e-05
Iter: 9 loss: 4.70417217e-05
Iter: 10 loss: 4.68352264e-05
Iter: 11 loss: 4.65528683e-05
Iter: 12 loss: 4.65393969e-05
Iter: 13 loss: 4.62475837e-05
Iter: 14 loss: 4.85987912e-05
Iter: 15 loss: 4.62282478e-05
Iter: 16 loss: 4.59972471e-05
Iter: 17 loss: 4.80198796e-05
Iter: 18 loss: 4.59852854e-05
Iter: 19 loss: 4.58597751e-05
Iter: 20 loss: 4.57391579e-05
Iter: 21 loss: 4.57112328e-05
Iter: 22 loss: 4.5589084e-05
Iter: 23 loss: 4.5580342e-05
Iter: 24 loss: 4.55004e-05
Iter: 25 loss: 4.53306857e-05
Iter: 26 loss: 4.80896415e-05
Iter: 27 loss: 4.53255852e-05
Iter: 28 loss: 4.51603773e-05
Iter: 29 loss: 4.62756871e-05
Iter: 30 loss: 4.51439628e-05
Iter: 31 loss: 4.49803229e-05
Iter: 32 loss: 4.57645874e-05
Iter: 33 loss: 4.49512881e-05
Iter: 34 loss: 4.48387764e-05
Iter: 35 loss: 4.47541606e-05
Iter: 36 loss: 4.47171478e-05
Iter: 37 loss: 4.45855367e-05
Iter: 38 loss: 4.59074654e-05
Iter: 39 loss: 4.45814403e-05
Iter: 40 loss: 4.44939105e-05
Iter: 41 loss: 4.47688035e-05
Iter: 42 loss: 4.44686048e-05
Iter: 43 loss: 4.43851459e-05
Iter: 44 loss: 4.49749787e-05
Iter: 45 loss: 4.43775789e-05
Iter: 46 loss: 4.43176141e-05
Iter: 47 loss: 4.42096716e-05
Iter: 48 loss: 4.68333674e-05
Iter: 49 loss: 4.42096716e-05
Iter: 50 loss: 4.41014163e-05
Iter: 51 loss: 4.50962325e-05
Iter: 52 loss: 4.40965414e-05
Iter: 53 loss: 4.39945397e-05
Iter: 54 loss: 4.44527068e-05
Iter: 55 loss: 4.39746691e-05
Iter: 56 loss: 4.39040596e-05
Iter: 57 loss: 4.38529241e-05
Iter: 58 loss: 4.38285279e-05
Iter: 59 loss: 4.3748365e-05
Iter: 60 loss: 4.37469862e-05
Iter: 61 loss: 4.36962873e-05
Iter: 62 loss: 4.35991933e-05
Iter: 63 loss: 4.56516864e-05
Iter: 64 loss: 4.35987713e-05
Iter: 65 loss: 4.35109978e-05
Iter: 66 loss: 4.41328302e-05
Iter: 67 loss: 4.3503027e-05
Iter: 68 loss: 4.34382455e-05
Iter: 69 loss: 4.42029523e-05
Iter: 70 loss: 4.34373214e-05
Iter: 71 loss: 4.33883397e-05
Iter: 72 loss: 4.33378882e-05
Iter: 73 loss: 4.3328575e-05
Iter: 74 loss: 4.3254895e-05
Iter: 75 loss: 4.35955044e-05
Iter: 76 loss: 4.32411434e-05
Iter: 77 loss: 4.31656808e-05
Iter: 78 loss: 4.33531823e-05
Iter: 79 loss: 4.31390035e-05
Iter: 80 loss: 4.30733344e-05
Iter: 81 loss: 4.36625633e-05
Iter: 82 loss: 4.30700675e-05
Iter: 83 loss: 4.30210421e-05
Iter: 84 loss: 4.29668798e-05
Iter: 85 loss: 4.2959e-05
Iter: 86 loss: 4.2897067e-05
Iter: 87 loss: 4.32760389e-05
Iter: 88 loss: 4.28896237e-05
Iter: 89 loss: 4.28317908e-05
Iter: 90 loss: 4.32263732e-05
Iter: 91 loss: 4.28261956e-05
Iter: 92 loss: 4.27819832e-05
Iter: 93 loss: 4.2720887e-05
Iter: 94 loss: 4.27182e-05
Iter: 95 loss: 4.26610968e-05
Iter: 96 loss: 4.26605329e-05
Iter: 97 loss: 4.26111292e-05
Iter: 98 loss: 4.25317958e-05
Iter: 99 loss: 4.25312028e-05
Iter: 100 loss: 4.24616337e-05
Iter: 101 loss: 4.26221959e-05
Iter: 102 loss: 4.24358113e-05
Iter: 103 loss: 4.23881283e-05
Iter: 104 loss: 4.23875244e-05
Iter: 105 loss: 4.23416241e-05
Iter: 106 loss: 4.23427664e-05
Iter: 107 loss: 4.23052552e-05
Iter: 108 loss: 4.22542471e-05
Iter: 109 loss: 4.2289983e-05
Iter: 110 loss: 4.22224657e-05
Iter: 111 loss: 4.2158641e-05
Iter: 112 loss: 4.26167389e-05
Iter: 113 loss: 4.21529912e-05
Iter: 114 loss: 4.21006262e-05
Iter: 115 loss: 4.22477242e-05
Iter: 116 loss: 4.20840224e-05
Iter: 117 loss: 4.2024396e-05
Iter: 118 loss: 4.21452569e-05
Iter: 119 loss: 4.20002907e-05
Iter: 120 loss: 4.19458956e-05
Iter: 121 loss: 4.19533681e-05
Iter: 122 loss: 4.19044409e-05
Iter: 123 loss: 4.18569034e-05
Iter: 124 loss: 4.25158032e-05
Iter: 125 loss: 4.18567033e-05
Iter: 126 loss: 4.18079144e-05
Iter: 127 loss: 4.1809144e-05
Iter: 128 loss: 4.17692281e-05
Iter: 129 loss: 4.17196643e-05
Iter: 130 loss: 4.18256532e-05
Iter: 131 loss: 4.17003e-05
Iter: 132 loss: 4.16365074e-05
Iter: 133 loss: 4.19810385e-05
Iter: 134 loss: 4.16269395e-05
Iter: 135 loss: 4.15853865e-05
Iter: 136 loss: 4.15156464e-05
Iter: 137 loss: 4.15154718e-05
Iter: 138 loss: 4.14312599e-05
Iter: 139 loss: 4.16617186e-05
Iter: 140 loss: 4.14037168e-05
Iter: 141 loss: 4.13390226e-05
Iter: 142 loss: 4.21004843e-05
Iter: 143 loss: 4.1337993e-05
Iter: 144 loss: 4.12746485e-05
Iter: 145 loss: 4.14939168e-05
Iter: 146 loss: 4.1257983e-05
Iter: 147 loss: 4.12144436e-05
Iter: 148 loss: 4.11503788e-05
Iter: 149 loss: 4.11487126e-05
Iter: 150 loss: 4.10768771e-05
Iter: 151 loss: 4.19512799e-05
Iter: 152 loss: 4.10759458e-05
Iter: 153 loss: 4.10181747e-05
Iter: 154 loss: 4.107106e-05
Iter: 155 loss: 4.09846543e-05
Iter: 156 loss: 4.09334789e-05
Iter: 157 loss: 4.15473769e-05
Iter: 158 loss: 4.09328932e-05
Iter: 159 loss: 4.08852e-05
Iter: 160 loss: 4.08806372e-05
Iter: 161 loss: 4.08456071e-05
Iter: 162 loss: 4.07884945e-05
Iter: 163 loss: 4.07948464e-05
Iter: 164 loss: 4.07445696e-05
Iter: 165 loss: 4.0700641e-05
Iter: 166 loss: 4.06982217e-05
Iter: 167 loss: 4.06537365e-05
Iter: 168 loss: 4.05753672e-05
Iter: 169 loss: 4.05754145e-05
Iter: 170 loss: 4.05021929e-05
Iter: 171 loss: 4.06364125e-05
Iter: 172 loss: 4.04706661e-05
Iter: 173 loss: 4.04036546e-05
Iter: 174 loss: 4.04033708e-05
Iter: 175 loss: 4.03582671e-05
Iter: 176 loss: 4.02900514e-05
Iter: 177 loss: 4.02887272e-05
Iter: 178 loss: 4.02123806e-05
Iter: 179 loss: 4.0361876e-05
Iter: 180 loss: 4.01807047e-05
Iter: 181 loss: 4.01261277e-05
Iter: 182 loss: 4.0124778e-05
Iter: 183 loss: 4.00723839e-05
Iter: 184 loss: 4.00235294e-05
Iter: 185 loss: 4.00111421e-05
Iter: 186 loss: 3.99405289e-05
Iter: 187 loss: 4.009642e-05
Iter: 188 loss: 3.99133423e-05
Iter: 189 loss: 3.98311713e-05
Iter: 190 loss: 4.02169171e-05
Iter: 191 loss: 3.98160628e-05
Iter: 192 loss: 3.97518561e-05
Iter: 193 loss: 3.98055345e-05
Iter: 194 loss: 3.97136027e-05
Iter: 195 loss: 3.96614123e-05
Iter: 196 loss: 3.96605246e-05
Iter: 197 loss: 3.96183023e-05
Iter: 198 loss: 3.95444767e-05
Iter: 199 loss: 3.95444731e-05
Iter: 200 loss: 3.94700473e-05
Iter: 201 loss: 3.97055628e-05
Iter: 202 loss: 3.94487324e-05
Iter: 203 loss: 3.93767514e-05
Iter: 204 loss: 4.03128724e-05
Iter: 205 loss: 3.93762784e-05
Iter: 206 loss: 3.93313167e-05
Iter: 207 loss: 3.92495931e-05
Iter: 208 loss: 4.12036388e-05
Iter: 209 loss: 3.92496549e-05
Iter: 210 loss: 3.91613576e-05
Iter: 211 loss: 3.93083974e-05
Iter: 212 loss: 3.91213252e-05
Iter: 213 loss: 3.90333807e-05
Iter: 214 loss: 3.95490315e-05
Iter: 215 loss: 3.90218665e-05
Iter: 216 loss: 3.8935661e-05
Iter: 217 loss: 3.96782234e-05
Iter: 218 loss: 3.89310153e-05
Iter: 219 loss: 3.88812114e-05
Iter: 220 loss: 3.8794693e-05
Iter: 221 loss: 3.87946347e-05
Iter: 222 loss: 3.87094187e-05
Iter: 223 loss: 3.96194228e-05
Iter: 224 loss: 3.87073487e-05
Iter: 225 loss: 3.86324464e-05
Iter: 226 loss: 3.89762281e-05
Iter: 227 loss: 3.86182437e-05
Iter: 228 loss: 3.85606618e-05
Iter: 229 loss: 3.86502106e-05
Iter: 230 loss: 3.85336098e-05
Iter: 231 loss: 3.84547748e-05
Iter: 232 loss: 3.85006679e-05
Iter: 233 loss: 3.84035848e-05
Iter: 234 loss: 3.83170736e-05
Iter: 235 loss: 3.83474471e-05
Iter: 236 loss: 3.82562139e-05
Iter: 237 loss: 3.82001126e-05
Iter: 238 loss: 3.81920472e-05
Iter: 239 loss: 3.81351856e-05
Iter: 240 loss: 3.80743368e-05
Iter: 241 loss: 3.80644888e-05
Iter: 242 loss: 3.7986727e-05
Iter: 243 loss: 3.79898556e-05
Iter: 244 loss: 3.79254852e-05
Iter: 245 loss: 3.78354671e-05
Iter: 246 loss: 3.84183841e-05
Iter: 247 loss: 3.78256445e-05
Iter: 248 loss: 3.7728365e-05
Iter: 249 loss: 3.8343318e-05
Iter: 250 loss: 3.77173201e-05
Iter: 251 loss: 3.76585376e-05
Iter: 252 loss: 3.75515629e-05
Iter: 253 loss: 4.00962417e-05
Iter: 254 loss: 3.75514428e-05
Iter: 255 loss: 3.74586452e-05
Iter: 256 loss: 3.88148255e-05
Iter: 257 loss: 3.7458587e-05
Iter: 258 loss: 3.7381702e-05
Iter: 259 loss: 3.77404504e-05
Iter: 260 loss: 3.73674266e-05
Iter: 261 loss: 3.73107214e-05
Iter: 262 loss: 3.72352151e-05
Iter: 263 loss: 3.72308568e-05
Iter: 264 loss: 3.71416609e-05
Iter: 265 loss: 3.79766789e-05
Iter: 266 loss: 3.71380156e-05
Iter: 267 loss: 3.70536072e-05
Iter: 268 loss: 3.71555725e-05
Iter: 269 loss: 3.70092312e-05
Iter: 270 loss: 3.69367554e-05
Iter: 271 loss: 3.72440663e-05
Iter: 272 loss: 3.69216141e-05
Iter: 273 loss: 3.68349793e-05
Iter: 274 loss: 3.71414935e-05
Iter: 275 loss: 3.68127075e-05
Iter: 276 loss: 3.67517205e-05
Iter: 277 loss: 3.66664644e-05
Iter: 278 loss: 3.66629538e-05
Iter: 279 loss: 3.66087843e-05
Iter: 280 loss: 3.65957603e-05
Iter: 281 loss: 3.65456362e-05
Iter: 282 loss: 3.64650805e-05
Iter: 283 loss: 3.6464422e-05
Iter: 284 loss: 3.63755898e-05
Iter: 285 loss: 3.64328116e-05
Iter: 286 loss: 3.6319143e-05
Iter: 287 loss: 3.62629798e-05
Iter: 288 loss: 3.62529536e-05
Iter: 289 loss: 3.61948769e-05
Iter: 290 loss: 3.60882623e-05
Iter: 291 loss: 3.85669191e-05
Iter: 292 loss: 3.60881058e-05
Iter: 293 loss: 3.59818805e-05
Iter: 294 loss: 3.61250786e-05
Iter: 295 loss: 3.59286387e-05
Iter: 296 loss: 3.58640464e-05
Iter: 297 loss: 3.58584803e-05
Iter: 298 loss: 3.57935205e-05
Iter: 299 loss: 3.57611134e-05
Iter: 300 loss: 3.57302852e-05
Iter: 301 loss: 3.56472483e-05
Iter: 302 loss: 3.56414894e-05
Iter: 303 loss: 3.5579149e-05
Iter: 304 loss: 3.54910444e-05
Iter: 305 loss: 3.54910444e-05
Iter: 306 loss: 3.54148724e-05
Iter: 307 loss: 3.54350959e-05
Iter: 308 loss: 3.5359426e-05
Iter: 309 loss: 3.52828501e-05
Iter: 310 loss: 3.62275823e-05
Iter: 311 loss: 3.52821153e-05
Iter: 312 loss: 3.52208226e-05
Iter: 313 loss: 3.51630806e-05
Iter: 314 loss: 3.51489143e-05
Iter: 315 loss: 3.50678893e-05
Iter: 316 loss: 3.54740623e-05
Iter: 317 loss: 3.50542177e-05
Iter: 318 loss: 3.4964527e-05
Iter: 319 loss: 3.53385622e-05
Iter: 320 loss: 3.49452093e-05
Iter: 321 loss: 3.48828908e-05
Iter: 322 loss: 3.47814057e-05
Iter: 323 loss: 3.47808054e-05
Iter: 324 loss: 3.47054374e-05
Iter: 325 loss: 3.47047026e-05
Iter: 326 loss: 3.46271263e-05
Iter: 327 loss: 3.47328569e-05
Iter: 328 loss: 3.45882581e-05
Iter: 329 loss: 3.45251465e-05
Iter: 330 loss: 3.44376458e-05
Iter: 331 loss: 3.44339278e-05
Iter: 332 loss: 3.43296051e-05
Iter: 333 loss: 3.49341935e-05
Iter: 334 loss: 3.43156498e-05
Iter: 335 loss: 3.4231598e-05
Iter: 336 loss: 3.54737531e-05
Iter: 337 loss: 3.42315034e-05
Iter: 338 loss: 3.41763698e-05
Iter: 339 loss: 3.40711777e-05
Iter: 340 loss: 3.63186264e-05
Iter: 341 loss: 3.40707702e-05
Iter: 342 loss: 3.39728431e-05
Iter: 343 loss: 3.4626828e-05
Iter: 344 loss: 3.3962926e-05
Iter: 345 loss: 3.38897516e-05
Iter: 346 loss: 3.49581023e-05
Iter: 347 loss: 3.3889628e-05
Iter: 348 loss: 3.38380705e-05
Iter: 349 loss: 3.37457313e-05
Iter: 350 loss: 3.60144295e-05
Iter: 351 loss: 3.37457786e-05
Iter: 352 loss: 3.36804551e-05
Iter: 353 loss: 3.36801313e-05
Iter: 354 loss: 3.36134253e-05
Iter: 355 loss: 3.36089979e-05
Iter: 356 loss: 3.35586956e-05
Iter: 357 loss: 3.34904835e-05
Iter: 358 loss: 3.36339872e-05
Iter: 359 loss: 3.34633951e-05
Iter: 360 loss: 3.33775606e-05
Iter: 361 loss: 3.37430247e-05
Iter: 362 loss: 3.33597272e-05
Iter: 363 loss: 3.32908e-05
Iter: 364 loss: 3.33487114e-05
Iter: 365 loss: 3.32499549e-05
Iter: 366 loss: 3.31635383e-05
Iter: 367 loss: 3.39333928e-05
Iter: 368 loss: 3.31592455e-05
Iter: 369 loss: 3.31065748e-05
Iter: 370 loss: 3.30271869e-05
Iter: 371 loss: 3.30256044e-05
Iter: 372 loss: 3.29391405e-05
Iter: 373 loss: 3.31472547e-05
Iter: 374 loss: 3.29079e-05
Iter: 375 loss: 3.28468195e-05
Iter: 376 loss: 3.28461465e-05
Iter: 377 loss: 3.2785847e-05
Iter: 378 loss: 3.27784219e-05
Iter: 379 loss: 3.27354101e-05
Iter: 380 loss: 3.2669428e-05
Iter: 381 loss: 3.26954e-05
Iter: 382 loss: 3.26237277e-05
Iter: 383 loss: 3.25621149e-05
Iter: 384 loss: 3.25599249e-05
Iter: 385 loss: 3.25143264e-05
Iter: 386 loss: 3.243221e-05
Iter: 387 loss: 3.44303735e-05
Iter: 388 loss: 3.24321882e-05
Iter: 389 loss: 3.2369906e-05
Iter: 390 loss: 3.30237744e-05
Iter: 391 loss: 3.23682652e-05
Iter: 392 loss: 3.23024506e-05
Iter: 393 loss: 3.2454358e-05
Iter: 394 loss: 3.22779233e-05
Iter: 395 loss: 3.22274136e-05
Iter: 396 loss: 3.21660627e-05
Iter: 397 loss: 3.21602638e-05
Iter: 398 loss: 3.20948529e-05
Iter: 399 loss: 3.20941399e-05
Iter: 400 loss: 3.20351755e-05
Iter: 401 loss: 3.20315048e-05
Iter: 402 loss: 3.19869287e-05
Iter: 403 loss: 3.19256069e-05
Iter: 404 loss: 3.24707871e-05
Iter: 405 loss: 3.19225146e-05
Iter: 406 loss: 3.18676393e-05
Iter: 407 loss: 3.18322927e-05
Iter: 408 loss: 3.18109451e-05
Iter: 409 loss: 3.17428785e-05
Iter: 410 loss: 3.17964732e-05
Iter: 411 loss: 3.17017766e-05
Iter: 412 loss: 3.16436635e-05
Iter: 413 loss: 3.16435471e-05
Iter: 414 loss: 3.15848811e-05
Iter: 415 loss: 3.1600146e-05
Iter: 416 loss: 3.15421748e-05
Iter: 417 loss: 3.14824829e-05
Iter: 418 loss: 3.15203324e-05
Iter: 419 loss: 3.14444042e-05
Iter: 420 loss: 3.13737182e-05
Iter: 421 loss: 3.24701759e-05
Iter: 422 loss: 3.13736746e-05
Iter: 423 loss: 3.13395649e-05
Iter: 424 loss: 3.12667034e-05
Iter: 425 loss: 3.24272405e-05
Iter: 426 loss: 3.12642442e-05
Iter: 427 loss: 3.12261618e-05
Iter: 428 loss: 3.12200173e-05
Iter: 429 loss: 3.1175e-05
Iter: 430 loss: 3.11089534e-05
Iter: 431 loss: 3.11072072e-05
Iter: 432 loss: 3.10374671e-05
Iter: 433 loss: 3.11027e-05
Iter: 434 loss: 3.09973475e-05
Iter: 435 loss: 3.09203388e-05
Iter: 436 loss: 3.11883196e-05
Iter: 437 loss: 3.09000498e-05
Iter: 438 loss: 3.08431554e-05
Iter: 439 loss: 3.08430936e-05
Iter: 440 loss: 3.07911832e-05
Iter: 441 loss: 3.07634873e-05
Iter: 442 loss: 3.07400114e-05
Iter: 443 loss: 3.06838447e-05
Iter: 444 loss: 3.11696094e-05
Iter: 445 loss: 3.06808251e-05
Iter: 446 loss: 3.06283764e-05
Iter: 447 loss: 3.06508773e-05
Iter: 448 loss: 3.05925241e-05
Iter: 449 loss: 3.05370777e-05
Iter: 450 loss: 3.05340727e-05
Iter: 451 loss: 3.04918121e-05
Iter: 452 loss: 3.04428213e-05
Iter: 453 loss: 3.04419846e-05
Iter: 454 loss: 3.03900651e-05
Iter: 455 loss: 3.03564739e-05
Iter: 456 loss: 3.03364504e-05
Iter: 457 loss: 3.02809367e-05
Iter: 458 loss: 3.05759895e-05
Iter: 459 loss: 3.02725311e-05
Iter: 460 loss: 3.02087519e-05
Iter: 461 loss: 3.03550569e-05
Iter: 462 loss: 3.01849541e-05
Iter: 463 loss: 3.01407072e-05
Iter: 464 loss: 3.01095715e-05
Iter: 465 loss: 3.00939646e-05
Iter: 466 loss: 3.00485626e-05
Iter: 467 loss: 3.00475858e-05
Iter: 468 loss: 3.0003348e-05
Iter: 469 loss: 2.99770545e-05
Iter: 470 loss: 2.99585918e-05
Iter: 471 loss: 2.9908093e-05
Iter: 472 loss: 3.01170458e-05
Iter: 473 loss: 2.989723e-05
Iter: 474 loss: 2.98351224e-05
Iter: 475 loss: 3.00122701e-05
Iter: 476 loss: 2.98155901e-05
Iter: 477 loss: 2.97724619e-05
Iter: 478 loss: 2.98218183e-05
Iter: 479 loss: 2.9749277e-05
Iter: 480 loss: 2.96882936e-05
Iter: 481 loss: 2.99209278e-05
Iter: 482 loss: 2.96737671e-05
Iter: 483 loss: 2.96300022e-05
Iter: 484 loss: 2.95756581e-05
Iter: 485 loss: 2.95710506e-05
Iter: 486 loss: 2.95040154e-05
Iter: 487 loss: 2.99805233e-05
Iter: 488 loss: 2.94979945e-05
Iter: 489 loss: 2.94399961e-05
Iter: 490 loss: 3.00817301e-05
Iter: 491 loss: 2.94389538e-05
Iter: 492 loss: 2.94064812e-05
Iter: 493 loss: 2.93556986e-05
Iter: 494 loss: 2.93551493e-05
Iter: 495 loss: 2.93049379e-05
Iter: 496 loss: 2.9304807e-05
Iter: 497 loss: 2.92654076e-05
Iter: 498 loss: 2.92067834e-05
Iter: 499 loss: 2.92055083e-05
Iter: 500 loss: 2.91418582e-05
Iter: 501 loss: 2.9471219e-05
Iter: 502 loss: 2.91316155e-05
Iter: 503 loss: 2.90805656e-05
Iter: 504 loss: 2.97038714e-05
Iter: 505 loss: 2.90799762e-05
Iter: 506 loss: 2.90469579e-05
Iter: 507 loss: 2.89859618e-05
Iter: 508 loss: 3.03915422e-05
Iter: 509 loss: 2.89859381e-05
Iter: 510 loss: 2.89508353e-05
Iter: 511 loss: 2.89465461e-05
Iter: 512 loss: 2.89104264e-05
Iter: 513 loss: 2.88526426e-05
Iter: 514 loss: 2.88522424e-05
Iter: 515 loss: 2.87973489e-05
Iter: 516 loss: 2.94194033e-05
Iter: 517 loss: 2.87964194e-05
Iter: 518 loss: 2.87478069e-05
Iter: 519 loss: 2.87695475e-05
Iter: 520 loss: 2.87147559e-05
Iter: 521 loss: 2.86635041e-05
Iter: 522 loss: 2.8666389e-05
Iter: 523 loss: 2.86233953e-05
Iter: 524 loss: 2.85813312e-05
Iter: 525 loss: 2.85809292e-05
Iter: 526 loss: 2.85369988e-05
Iter: 527 loss: 2.85568112e-05
Iter: 528 loss: 2.85072365e-05
Iter: 529 loss: 2.84652961e-05
Iter: 530 loss: 2.84751968e-05
Iter: 531 loss: 2.8434526e-05
Iter: 532 loss: 2.8373448e-05
Iter: 533 loss: 2.8976845e-05
Iter: 534 loss: 2.83714198e-05
Iter: 535 loss: 2.8336206e-05
Iter: 536 loss: 2.82804976e-05
Iter: 537 loss: 2.82799192e-05
Iter: 538 loss: 2.82226647e-05
Iter: 539 loss: 2.85560636e-05
Iter: 540 loss: 2.82151195e-05
Iter: 541 loss: 2.81604407e-05
Iter: 542 loss: 2.86530121e-05
Iter: 543 loss: 2.81578486e-05
Iter: 544 loss: 2.81220327e-05
Iter: 545 loss: 2.80765926e-05
Iter: 546 loss: 2.80731037e-05
Iter: 547 loss: 2.80319655e-05
Iter: 548 loss: 2.80315289e-05
Iter: 549 loss: 2.79945398e-05
Iter: 550 loss: 2.79591222e-05
Iter: 551 loss: 2.79508167e-05
Iter: 552 loss: 2.79016804e-05
Iter: 553 loss: 2.80869353e-05
Iter: 554 loss: 2.78897751e-05
Iter: 555 loss: 2.78378629e-05
Iter: 556 loss: 2.81411849e-05
Iter: 557 loss: 2.78310763e-05
Iter: 558 loss: 2.77961171e-05
Iter: 559 loss: 2.77416912e-05
Iter: 560 loss: 2.77409345e-05
Iter: 561 loss: 2.76806586e-05
Iter: 562 loss: 2.81033717e-05
Iter: 563 loss: 2.76751089e-05
Iter: 564 loss: 2.76217743e-05
Iter: 565 loss: 2.81643079e-05
Iter: 566 loss: 2.76201608e-05
Iter: 567 loss: 2.75896891e-05
Iter: 568 loss: 2.75403218e-05
Iter: 569 loss: 2.75399543e-05
Iter: 570 loss: 2.75008715e-05
Iter: 571 loss: 2.74997401e-05
Iter: 572 loss: 2.74650301e-05
Iter: 573 loss: 2.74116137e-05
Iter: 574 loss: 2.74108024e-05
Iter: 575 loss: 2.73563892e-05
Iter: 576 loss: 2.74619451e-05
Iter: 577 loss: 2.73337246e-05
Iter: 578 loss: 2.72940269e-05
Iter: 579 loss: 2.72920533e-05
Iter: 580 loss: 2.72604557e-05
Iter: 581 loss: 2.72153411e-05
Iter: 582 loss: 2.72138313e-05
Iter: 583 loss: 2.71705721e-05
Iter: 584 loss: 2.75008879e-05
Iter: 585 loss: 2.71673653e-05
Iter: 586 loss: 2.71245044e-05
Iter: 587 loss: 2.72731922e-05
Iter: 588 loss: 2.71132722e-05
Iter: 589 loss: 2.70807723e-05
Iter: 590 loss: 2.70748387e-05
Iter: 591 loss: 2.70529854e-05
Iter: 592 loss: 2.70042074e-05
Iter: 593 loss: 2.73834339e-05
Iter: 594 loss: 2.70005694e-05
Iter: 595 loss: 2.69630564e-05
Iter: 596 loss: 2.69415905e-05
Iter: 597 loss: 2.69254488e-05
Iter: 598 loss: 2.68723779e-05
Iter: 599 loss: 2.69228785e-05
Iter: 600 loss: 2.68420154e-05
Iter: 601 loss: 2.68184976e-05
Iter: 602 loss: 2.68089643e-05
Iter: 603 loss: 2.6783262e-05
Iter: 604 loss: 2.67308369e-05
Iter: 605 loss: 2.76756091e-05
Iter: 606 loss: 2.67299856e-05
Iter: 607 loss: 2.66903699e-05
Iter: 608 loss: 2.72898978e-05
Iter: 609 loss: 2.66903371e-05
Iter: 610 loss: 2.66505012e-05
Iter: 611 loss: 2.66548668e-05
Iter: 612 loss: 2.66198986e-05
Iter: 613 loss: 2.65769304e-05
Iter: 614 loss: 2.65471972e-05
Iter: 615 loss: 2.65316685e-05
Iter: 616 loss: 2.64811461e-05
Iter: 617 loss: 2.70607234e-05
Iter: 618 loss: 2.64803384e-05
Iter: 619 loss: 2.64416012e-05
Iter: 620 loss: 2.67703308e-05
Iter: 621 loss: 2.64393821e-05
Iter: 622 loss: 2.64095088e-05
Iter: 623 loss: 2.63810653e-05
Iter: 624 loss: 2.63742531e-05
Iter: 625 loss: 2.63358852e-05
Iter: 626 loss: 2.67499581e-05
Iter: 627 loss: 2.63350157e-05
Iter: 628 loss: 2.62981739e-05
Iter: 629 loss: 2.63004e-05
Iter: 630 loss: 2.62694011e-05
Iter: 631 loss: 2.62271024e-05
Iter: 632 loss: 2.63051807e-05
Iter: 633 loss: 2.62089179e-05
Iter: 634 loss: 2.61700152e-05
Iter: 635 loss: 2.67238829e-05
Iter: 636 loss: 2.61699388e-05
Iter: 637 loss: 2.61412024e-05
Iter: 638 loss: 2.60987963e-05
Iter: 639 loss: 2.60978013e-05
Iter: 640 loss: 2.60533634e-05
Iter: 641 loss: 2.6141508e-05
Iter: 642 loss: 2.60351135e-05
Iter: 643 loss: 2.59884873e-05
Iter: 644 loss: 2.67335035e-05
Iter: 645 loss: 2.59885092e-05
Iter: 646 loss: 2.59617736e-05
Iter: 647 loss: 2.59187836e-05
Iter: 648 loss: 2.59185126e-05
Iter: 649 loss: 2.58856016e-05
Iter: 650 loss: 2.58852215e-05
Iter: 651 loss: 2.58557066e-05
Iter: 652 loss: 2.58396703e-05
Iter: 653 loss: 2.58264317e-05
Iter: 654 loss: 2.57870925e-05
Iter: 655 loss: 2.57845477e-05
Iter: 656 loss: 2.5754769e-05
Iter: 657 loss: 2.57108841e-05
Iter: 658 loss: 2.61991718e-05
Iter: 659 loss: 2.57099964e-05
Iter: 660 loss: 2.56745334e-05
Iter: 661 loss: 2.57282572e-05
Iter: 662 loss: 2.56576932e-05
Iter: 663 loss: 2.56190251e-05
Iter: 664 loss: 2.56624953e-05
Iter: 665 loss: 2.55981395e-05
Iter: 666 loss: 2.55590021e-05
Iter: 667 loss: 2.60239885e-05
Iter: 668 loss: 2.55584382e-05
Iter: 669 loss: 2.5530966e-05
Iter: 670 loss: 2.55025989e-05
Iter: 671 loss: 2.54973493e-05
Iter: 672 loss: 2.5454261e-05
Iter: 673 loss: 2.55421546e-05
Iter: 674 loss: 2.54368788e-05
Iter: 675 loss: 2.53992584e-05
Iter: 676 loss: 2.53990802e-05
Iter: 677 loss: 2.53745857e-05
Iter: 678 loss: 2.53282633e-05
Iter: 679 loss: 2.63345501e-05
Iter: 680 loss: 2.53280723e-05
Iter: 681 loss: 2.52933824e-05
Iter: 682 loss: 2.57318952e-05
Iter: 683 loss: 2.52930804e-05
Iter: 684 loss: 2.52602185e-05
Iter: 685 loss: 2.53746493e-05
Iter: 686 loss: 2.52516438e-05
Iter: 687 loss: 2.5223082e-05
Iter: 688 loss: 2.5210591e-05
Iter: 689 loss: 2.51961628e-05
Iter: 690 loss: 2.51574784e-05
Iter: 691 loss: 2.55792911e-05
Iter: 692 loss: 2.51566707e-05
Iter: 693 loss: 2.5128229e-05
Iter: 694 loss: 2.51155216e-05
Iter: 695 loss: 2.51011388e-05
Iter: 696 loss: 2.5063342e-05
Iter: 697 loss: 2.53394392e-05
Iter: 698 loss: 2.50601988e-05
Iter: 699 loss: 2.50265748e-05
Iter: 700 loss: 2.50440426e-05
Iter: 701 loss: 2.50042722e-05
Iter: 702 loss: 2.49681907e-05
Iter: 703 loss: 2.50624435e-05
Iter: 704 loss: 2.49559e-05
Iter: 705 loss: 2.49142195e-05
Iter: 706 loss: 2.51392157e-05
Iter: 707 loss: 2.49078948e-05
Iter: 708 loss: 2.48787401e-05
Iter: 709 loss: 2.486007e-05
Iter: 710 loss: 2.48486595e-05
Iter: 711 loss: 2.48129581e-05
Iter: 712 loss: 2.49796285e-05
Iter: 713 loss: 2.48063188e-05
Iter: 714 loss: 2.47683402e-05
Iter: 715 loss: 2.50326248e-05
Iter: 716 loss: 2.47646967e-05
Iter: 717 loss: 2.47383832e-05
Iter: 718 loss: 2.46945201e-05
Iter: 719 loss: 2.469436e-05
Iter: 720 loss: 2.46563686e-05
Iter: 721 loss: 2.51759702e-05
Iter: 722 loss: 2.46561867e-05
Iter: 723 loss: 2.46193467e-05
Iter: 724 loss: 2.46678792e-05
Iter: 725 loss: 2.4600613e-05
Iter: 726 loss: 2.45673073e-05
Iter: 727 loss: 2.46624986e-05
Iter: 728 loss: 2.45568081e-05
Iter: 729 loss: 2.45218725e-05
Iter: 730 loss: 2.47069402e-05
Iter: 731 loss: 2.45165156e-05
Iter: 732 loss: 2.44895236e-05
Iter: 733 loss: 2.4495137e-05
Iter: 734 loss: 2.44695566e-05
Iter: 735 loss: 2.44338935e-05
Iter: 736 loss: 2.46532691e-05
Iter: 737 loss: 2.44295807e-05
Iter: 738 loss: 2.43999311e-05
Iter: 739 loss: 2.43888462e-05
Iter: 740 loss: 2.43724717e-05
Iter: 741 loss: 2.43377945e-05
Iter: 742 loss: 2.46823274e-05
Iter: 743 loss: 2.43366612e-05
Iter: 744 loss: 2.43047871e-05
Iter: 745 loss: 2.43243485e-05
Iter: 746 loss: 2.42843053e-05
Iter: 747 loss: 2.42506394e-05
Iter: 748 loss: 2.42486058e-05
Iter: 749 loss: 2.42229726e-05
Iter: 750 loss: 2.41914167e-05
Iter: 751 loss: 2.41913604e-05
Iter: 752 loss: 2.41611015e-05
Iter: 753 loss: 2.41579764e-05
Iter: 754 loss: 2.41359194e-05
Iter: 755 loss: 2.41055859e-05
Iter: 756 loss: 2.41272119e-05
Iter: 757 loss: 2.40867157e-05
Iter: 758 loss: 2.40530644e-05
Iter: 759 loss: 2.45041192e-05
Iter: 760 loss: 2.40529243e-05
Iter: 761 loss: 2.40293048e-05
Iter: 762 loss: 2.40189602e-05
Iter: 763 loss: 2.40068621e-05
Iter: 764 loss: 2.39762521e-05
Iter: 765 loss: 2.42656824e-05
Iter: 766 loss: 2.3975088e-05
Iter: 767 loss: 2.39487708e-05
Iter: 768 loss: 2.39459041e-05
Iter: 769 loss: 2.39267792e-05
Iter: 770 loss: 2.38939938e-05
Iter: 771 loss: 2.4078945e-05
Iter: 772 loss: 2.38895136e-05
Iter: 773 loss: 2.38583e-05
Iter: 774 loss: 2.38844877e-05
Iter: 775 loss: 2.38397479e-05
Iter: 776 loss: 2.38078901e-05
Iter: 777 loss: 2.38825887e-05
Iter: 778 loss: 2.37962086e-05
Iter: 779 loss: 2.37606582e-05
Iter: 780 loss: 2.3984594e-05
Iter: 781 loss: 2.3756611e-05
Iter: 782 loss: 2.37308632e-05
Iter: 783 loss: 2.37038657e-05
Iter: 784 loss: 2.36992782e-05
Iter: 785 loss: 2.3662571e-05
Iter: 786 loss: 2.38674875e-05
Iter: 787 loss: 2.36573942e-05
Iter: 788 loss: 2.36224714e-05
Iter: 789 loss: 2.38968714e-05
Iter: 790 loss: 2.36199885e-05
Iter: 791 loss: 2.35971856e-05
Iter: 792 loss: 2.35646548e-05
Iter: 793 loss: 2.35635507e-05
Iter: 794 loss: 2.35336865e-05
Iter: 795 loss: 2.39249148e-05
Iter: 796 loss: 2.35334956e-05
Iter: 797 loss: 2.35044427e-05
Iter: 798 loss: 2.35266707e-05
Iter: 799 loss: 2.34867839e-05
Iter: 800 loss: 2.34580784e-05
Iter: 801 loss: 2.35956086e-05
Iter: 802 loss: 2.34529325e-05
Iter: 803 loss: 2.34233266e-05
Iter: 804 loss: 2.34848958e-05
Iter: 805 loss: 2.34115323e-05
Iter: 806 loss: 2.33846094e-05
Iter: 807 loss: 2.34414038e-05
Iter: 808 loss: 2.33740429e-05
Iter: 809 loss: 2.33445553e-05
Iter: 810 loss: 2.34759627e-05
Iter: 811 loss: 2.33388273e-05
Iter: 812 loss: 2.33151295e-05
Iter: 813 loss: 2.3299719e-05
Iter: 814 loss: 2.32906023e-05
Iter: 815 loss: 2.32598795e-05
Iter: 816 loss: 2.36403939e-05
Iter: 817 loss: 2.32595939e-05
Iter: 818 loss: 2.32334569e-05
Iter: 819 loss: 2.32302082e-05
Iter: 820 loss: 2.32115272e-05
Iter: 821 loss: 2.31807517e-05
Iter: 822 loss: 2.31846025e-05
Iter: 823 loss: 2.31572794e-05
Iter: 824 loss: 2.31258928e-05
Iter: 825 loss: 2.34773688e-05
Iter: 826 loss: 2.31253762e-05
Iter: 827 loss: 2.3094788e-05
Iter: 828 loss: 2.31949598e-05
Iter: 829 loss: 2.30863188e-05
Iter: 830 loss: 2.30634632e-05
Iter: 831 loss: 2.30380901e-05
Iter: 832 loss: 2.30345395e-05
Iter: 833 loss: 2.30047081e-05
Iter: 834 loss: 2.34473118e-05
Iter: 835 loss: 2.30047444e-05
Iter: 836 loss: 2.29785073e-05
Iter: 837 loss: 2.29809848e-05
Iter: 838 loss: 2.29582256e-05
Iter: 839 loss: 2.29276411e-05
Iter: 840 loss: 2.30580699e-05
Iter: 841 loss: 2.29211873e-05
Iter: 842 loss: 2.28925783e-05
Iter: 843 loss: 2.30251862e-05
Iter: 844 loss: 2.28872595e-05
Iter: 845 loss: 2.28625722e-05
Iter: 846 loss: 2.28643385e-05
Iter: 847 loss: 2.28433128e-05
Iter: 848 loss: 2.28139434e-05
Iter: 849 loss: 2.31041959e-05
Iter: 850 loss: 2.28128793e-05
Iter: 851 loss: 2.27882265e-05
Iter: 852 loss: 2.27704331e-05
Iter: 853 loss: 2.27619294e-05
Iter: 854 loss: 2.27295168e-05
Iter: 855 loss: 2.28308927e-05
Iter: 856 loss: 2.27201745e-05
Iter: 857 loss: 2.26874472e-05
Iter: 858 loss: 2.29831239e-05
Iter: 859 loss: 2.26859265e-05
Iter: 860 loss: 2.26631055e-05
Iter: 861 loss: 2.26312113e-05
Iter: 862 loss: 2.26299435e-05
Iter: 863 loss: 2.25937329e-05
Iter: 864 loss: 2.26942575e-05
Iter: 865 loss: 2.25820877e-05
Iter: 866 loss: 2.25575059e-05
Iter: 867 loss: 2.25574786e-05
Iter: 868 loss: 2.25342883e-05
Iter: 869 loss: 2.25274689e-05
Iter: 870 loss: 2.25135555e-05
Iter: 871 loss: 2.24849e-05
Iter: 872 loss: 2.26365592e-05
Iter: 873 loss: 2.24804826e-05
Iter: 874 loss: 2.2453547e-05
Iter: 875 loss: 2.25258536e-05
Iter: 876 loss: 2.24445612e-05
Iter: 877 loss: 2.24195937e-05
Iter: 878 loss: 2.24338437e-05
Iter: 879 loss: 2.24033429e-05
Iter: 880 loss: 2.2376229e-05
Iter: 881 loss: 2.2624441e-05
Iter: 882 loss: 2.23750649e-05
Iter: 883 loss: 2.23521401e-05
Iter: 884 loss: 2.23461302e-05
Iter: 885 loss: 2.23319e-05
Iter: 886 loss: 2.23018742e-05
Iter: 887 loss: 2.23513198e-05
Iter: 888 loss: 2.22881663e-05
Iter: 889 loss: 2.22594281e-05
Iter: 890 loss: 2.26799202e-05
Iter: 891 loss: 2.22594845e-05
Iter: 892 loss: 2.22386861e-05
Iter: 893 loss: 2.22145281e-05
Iter: 894 loss: 2.2211716e-05
Iter: 895 loss: 2.21854098e-05
Iter: 896 loss: 2.24801315e-05
Iter: 897 loss: 2.2184935e-05
Iter: 898 loss: 2.21593982e-05
Iter: 899 loss: 2.21686678e-05
Iter: 900 loss: 2.21415539e-05
Iter: 901 loss: 2.21153023e-05
Iter: 902 loss: 2.20997681e-05
Iter: 903 loss: 2.20888724e-05
Iter: 904 loss: 2.20550537e-05
Iter: 905 loss: 2.23988663e-05
Iter: 906 loss: 2.20540351e-05
Iter: 907 loss: 2.20256261e-05
Iter: 908 loss: 2.21535483e-05
Iter: 909 loss: 2.20201255e-05
Iter: 910 loss: 2.19963549e-05
Iter: 911 loss: 2.20120564e-05
Iter: 912 loss: 2.19813264e-05
Iter: 913 loss: 2.19536814e-05
Iter: 914 loss: 2.212459e-05
Iter: 915 loss: 2.1950309e-05
Iter: 916 loss: 2.19266331e-05
Iter: 917 loss: 2.19254216e-05
Iter: 918 loss: 2.19073117e-05
Iter: 919 loss: 2.1877484e-05
Iter: 920 loss: 2.1923297e-05
Iter: 921 loss: 2.18634632e-05
Iter: 922 loss: 2.18358873e-05
Iter: 923 loss: 2.18359091e-05
Iter: 924 loss: 2.18148216e-05
Iter: 925 loss: 2.17945417e-05
Iter: 926 loss: 2.17898232e-05
Iter: 927 loss: 2.17657616e-05
Iter: 928 loss: 2.20344918e-05
Iter: 929 loss: 2.17653032e-05
Iter: 930 loss: 2.17422203e-05
Iter: 931 loss: 2.17527795e-05
Iter: 932 loss: 2.1726617e-05
Iter: 933 loss: 2.17026027e-05
Iter: 934 loss: 2.16868866e-05
Iter: 935 loss: 2.16777844e-05
Iter: 936 loss: 2.16467215e-05
Iter: 937 loss: 2.19778358e-05
Iter: 938 loss: 2.16459248e-05
Iter: 939 loss: 2.16196386e-05
Iter: 940 loss: 2.17305842e-05
Iter: 941 loss: 2.16140506e-05
Iter: 942 loss: 2.1592663e-05
Iter: 943 loss: 2.1598762e-05
Iter: 944 loss: 2.15771652e-05
Iter: 945 loss: 2.15521832e-05
Iter: 946 loss: 2.17436736e-05
Iter: 947 loss: 2.15502405e-05
Iter: 948 loss: 2.15279815e-05
Iter: 949 loss: 2.15282089e-05
Iter: 950 loss: 2.15102282e-05
Iter: 951 loss: 2.14825086e-05
Iter: 952 loss: 2.15781438e-05
Iter: 953 loss: 2.14752108e-05
Iter: 954 loss: 2.14460251e-05
Iter: 955 loss: 2.15949331e-05
Iter: 956 loss: 2.14412248e-05
Iter: 957 loss: 2.14203465e-05
Iter: 958 loss: 2.1414502e-05
Iter: 959 loss: 2.14016782e-05
Iter: 960 loss: 2.13807471e-05
Iter: 961 loss: 2.1691174e-05
Iter: 962 loss: 2.13807416e-05
Iter: 963 loss: 2.13612711e-05
Iter: 964 loss: 2.13576932e-05
Iter: 965 loss: 2.13445273e-05
Iter: 966 loss: 2.13204112e-05
Iter: 967 loss: 2.13184721e-05
Iter: 968 loss: 2.1300486e-05
Iter: 969 loss: 2.12751729e-05
Iter: 970 loss: 2.15611472e-05
Iter: 971 loss: 2.12746781e-05
Iter: 972 loss: 2.12529649e-05
Iter: 973 loss: 2.13313615e-05
Iter: 974 loss: 2.12474406e-05
Iter: 975 loss: 2.12277828e-05
Iter: 976 loss: 2.12262385e-05
Iter: 977 loss: 2.12115738e-05
Iter: 978 loss: 2.11877013e-05
Iter: 979 loss: 2.13720014e-05
Iter: 980 loss: 2.11859879e-05
Iter: 981 loss: 2.11641818e-05
Iter: 982 loss: 2.11783336e-05
Iter: 983 loss: 2.11503066e-05
Iter: 984 loss: 2.1126416e-05
Iter: 985 loss: 2.11917177e-05
Iter: 986 loss: 2.11185834e-05
Iter: 987 loss: 2.10936487e-05
Iter: 988 loss: 2.12575851e-05
Iter: 989 loss: 2.10910839e-05
Iter: 990 loss: 2.10727885e-05
Iter: 991 loss: 2.10618709e-05
Iter: 992 loss: 2.10543476e-05
Iter: 993 loss: 2.10327926e-05
Iter: 994 loss: 2.12835039e-05
Iter: 995 loss: 2.10325234e-05
Iter: 996 loss: 2.10123271e-05
Iter: 997 loss: 2.10223188e-05
Iter: 998 loss: 2.09988539e-05
Iter: 999 loss: 2.09774626e-05
Iter: 1000 loss: 2.0998501e-05
Iter: 1001 loss: 2.09653426e-05
Iter: 1002 loss: 2.09412265e-05
Iter: 1003 loss: 2.11734987e-05
Iter: 1004 loss: 2.09403806e-05
Iter: 1005 loss: 2.09217433e-05
Iter: 1006 loss: 2.08982783e-05
Iter: 1007 loss: 2.08964193e-05
Iter: 1008 loss: 2.08677229e-05
Iter: 1009 loss: 2.11169972e-05
Iter: 1010 loss: 2.08662277e-05
Iter: 1011 loss: 2.08410747e-05
Iter: 1012 loss: 2.09371865e-05
Iter: 1013 loss: 2.08350066e-05
Iter: 1014 loss: 2.08152269e-05
Iter: 1015 loss: 2.08167949e-05
Iter: 1016 loss: 2.07998855e-05
Iter: 1017 loss: 2.07732028e-05
Iter: 1018 loss: 2.09602294e-05
Iter: 1019 loss: 2.07707271e-05
Iter: 1020 loss: 2.07484482e-05
Iter: 1021 loss: 2.07422199e-05
Iter: 1022 loss: 2.07285284e-05
Iter: 1023 loss: 2.07037701e-05
Iter: 1024 loss: 2.09159298e-05
Iter: 1025 loss: 2.07024e-05
Iter: 1026 loss: 2.0678739e-05
Iter: 1027 loss: 2.07098055e-05
Iter: 1028 loss: 2.06667628e-05
Iter: 1029 loss: 2.06440236e-05
Iter: 1030 loss: 2.06877157e-05
Iter: 1031 loss: 2.06344412e-05
Iter: 1032 loss: 2.06064451e-05
Iter: 1033 loss: 2.07605481e-05
Iter: 1034 loss: 2.06023615e-05
Iter: 1035 loss: 2.05823344e-05
Iter: 1036 loss: 2.05653232e-05
Iter: 1037 loss: 2.05597535e-05
Iter: 1038 loss: 2.05330398e-05
Iter: 1039 loss: 2.08542315e-05
Iter: 1040 loss: 2.05327324e-05
Iter: 1041 loss: 2.05096567e-05
Iter: 1042 loss: 2.05069264e-05
Iter: 1043 loss: 2.04903627e-05
Iter: 1044 loss: 2.04672906e-05
Iter: 1045 loss: 2.05986435e-05
Iter: 1046 loss: 2.04641619e-05
Iter: 1047 loss: 2.04406278e-05
Iter: 1048 loss: 2.05246433e-05
Iter: 1049 loss: 2.04346288e-05
Iter: 1050 loss: 2.04144071e-05
Iter: 1051 loss: 2.0413645e-05
Iter: 1052 loss: 2.0397978e-05
Iter: 1053 loss: 2.03715863e-05
Iter: 1054 loss: 2.05801553e-05
Iter: 1055 loss: 2.03697709e-05
Iter: 1056 loss: 2.03492491e-05
Iter: 1057 loss: 2.03436484e-05
Iter: 1058 loss: 2.03310246e-05
Iter: 1059 loss: 2.03083073e-05
Iter: 1060 loss: 2.05191391e-05
Iter: 1061 loss: 2.0307356e-05
Iter: 1062 loss: 2.0286474e-05
Iter: 1063 loss: 2.03088857e-05
Iter: 1064 loss: 2.02750634e-05
Iter: 1065 loss: 2.02547199e-05
Iter: 1066 loss: 2.03221989e-05
Iter: 1067 loss: 2.02491537e-05
Iter: 1068 loss: 2.02255142e-05
Iter: 1069 loss: 2.03002201e-05
Iter: 1070 loss: 2.02188057e-05
Iter: 1071 loss: 2.02011834e-05
Iter: 1072 loss: 2.01967086e-05
Iter: 1073 loss: 2.01857074e-05
Iter: 1074 loss: 2.01631519e-05
Iter: 1075 loss: 2.0413303e-05
Iter: 1076 loss: 2.01626917e-05
Iter: 1077 loss: 2.01443727e-05
Iter: 1078 loss: 2.01361709e-05
Iter: 1079 loss: 2.01269377e-05
Iter: 1080 loss: 2.01071816e-05
Iter: 1081 loss: 2.02774499e-05
Iter: 1082 loss: 2.01060866e-05
Iter: 1083 loss: 2.00861359e-05
Iter: 1084 loss: 2.01106886e-05
Iter: 1085 loss: 2.00756476e-05
Iter: 1086 loss: 2.00562263e-05
Iter: 1087 loss: 2.00874747e-05
Iter: 1088 loss: 2.00471259e-05
Iter: 1089 loss: 2.00245613e-05
Iter: 1090 loss: 2.01384737e-05
Iter: 1091 loss: 2.00207887e-05
Iter: 1092 loss: 2.00026e-05
Iter: 1093 loss: 1.99980641e-05
Iter: 1094 loss: 1.9986579e-05
Iter: 1095 loss: 1.99656897e-05
Iter: 1096 loss: 2.02100582e-05
Iter: 1097 loss: 1.99653568e-05
Iter: 1098 loss: 1.99475217e-05
Iter: 1099 loss: 1.99504611e-05
Iter: 1100 loss: 1.99341612e-05
Iter: 1101 loss: 1.99160058e-05
Iter: 1102 loss: 2.0066258e-05
Iter: 1103 loss: 1.99149108e-05
Iter: 1104 loss: 1.98979651e-05
Iter: 1105 loss: 1.99055557e-05
Iter: 1106 loss: 1.98864873e-05
Iter: 1107 loss: 1.9869196e-05
Iter: 1108 loss: 1.98878461e-05
Iter: 1109 loss: 1.985977e-05
Iter: 1110 loss: 1.98384951e-05
Iter: 1111 loss: 2.00094655e-05
Iter: 1112 loss: 1.98370835e-05
Iter: 1113 loss: 1.98207053e-05
Iter: 1114 loss: 1.98099187e-05
Iter: 1115 loss: 1.98036214e-05
Iter: 1116 loss: 1.97855898e-05
Iter: 1117 loss: 2.00690738e-05
Iter: 1118 loss: 1.97855661e-05
Iter: 1119 loss: 1.97701756e-05
Iter: 1120 loss: 1.97645713e-05
Iter: 1121 loss: 1.97559966e-05
Iter: 1122 loss: 1.97371883e-05
Iter: 1123 loss: 1.98261077e-05
Iter: 1124 loss: 1.97337868e-05
Iter: 1125 loss: 1.9714922e-05
Iter: 1126 loss: 1.97533409e-05
Iter: 1127 loss: 1.9707375e-05
Iter: 1128 loss: 1.96906221e-05
Iter: 1129 loss: 1.97084373e-05
Iter: 1130 loss: 1.96814581e-05
Iter: 1131 loss: 1.96630517e-05
Iter: 1132 loss: 1.98366542e-05
Iter: 1133 loss: 1.96623e-05
Iter: 1134 loss: 1.96480632e-05
Iter: 1135 loss: 1.96451474e-05
Iter: 1136 loss: 1.96357087e-05
Iter: 1137 loss: 1.96192195e-05
Iter: 1138 loss: 1.98104608e-05
Iter: 1139 loss: 1.96189685e-05
Iter: 1140 loss: 1.9606312e-05
Iter: 1141 loss: 1.95955e-05
Iter: 1142 loss: 1.95919692e-05
Iter: 1143 loss: 1.9575873e-05
Iter: 1144 loss: 1.96590918e-05
Iter: 1145 loss: 1.95733446e-05
Iter: 1146 loss: 1.95562316e-05
Iter: 1147 loss: 1.96113688e-05
Iter: 1148 loss: 1.95513894e-05
Iter: 1149 loss: 1.95368484e-05
Iter: 1150 loss: 1.9542962e-05
Iter: 1151 loss: 1.95267439e-05
Iter: 1152 loss: 1.95098582e-05
Iter: 1153 loss: 1.9706591e-05
Iter: 1154 loss: 1.95095836e-05
Iter: 1155 loss: 1.94979093e-05
Iter: 1156 loss: 1.94892964e-05
Iter: 1157 loss: 1.94854183e-05
Iter: 1158 loss: 1.94691038e-05
Iter: 1159 loss: 1.96007331e-05
Iter: 1160 loss: 1.94680433e-05
Iter: 1161 loss: 1.94543754e-05
Iter: 1162 loss: 1.94555832e-05
Iter: 1163 loss: 1.94437489e-05
Iter: 1164 loss: 1.94280365e-05
Iter: 1165 loss: 1.94871118e-05
Iter: 1166 loss: 1.94242493e-05
Iter: 1167 loss: 1.94067616e-05
Iter: 1168 loss: 1.94751647e-05
Iter: 1169 loss: 1.94026161e-05
Iter: 1170 loss: 1.93887245e-05
Iter: 1171 loss: 1.94076019e-05
Iter: 1172 loss: 1.93817832e-05
Iter: 1173 loss: 1.93650776e-05
Iter: 1174 loss: 1.94645436e-05
Iter: 1175 loss: 1.93629858e-05
Iter: 1176 loss: 1.93509986e-05
Iter: 1177 loss: 1.93393407e-05
Iter: 1178 loss: 1.93367523e-05
Iter: 1179 loss: 1.93214219e-05
Iter: 1180 loss: 1.95003904e-05
Iter: 1181 loss: 1.93211818e-05
Iter: 1182 loss: 1.93069627e-05
Iter: 1183 loss: 1.93125379e-05
Iter: 1184 loss: 1.92970474e-05
Iter: 1185 loss: 1.92824373e-05
Iter: 1186 loss: 1.93546493e-05
Iter: 1187 loss: 1.92800053e-05
Iter: 1188 loss: 1.92646876e-05
Iter: 1189 loss: 1.93090036e-05
Iter: 1190 loss: 1.92599255e-05
Iter: 1191 loss: 1.9247851e-05
Iter: 1192 loss: 1.92556636e-05
Iter: 1193 loss: 1.92401531e-05
Iter: 1194 loss: 1.92235948e-05
Iter: 1195 loss: 1.93050746e-05
Iter: 1196 loss: 1.92207353e-05
Iter: 1197 loss: 1.92073312e-05
Iter: 1198 loss: 1.9206831e-05
Iter: 1199 loss: 1.91963809e-05
Iter: 1200 loss: 1.91817835e-05
Iter: 1201 loss: 1.93245342e-05
Iter: 1202 loss: 1.91812505e-05
Iter: 1203 loss: 1.91675117e-05
Iter: 1204 loss: 1.91756571e-05
Iter: 1205 loss: 1.91585877e-05
Iter: 1206 loss: 1.91455401e-05
Iter: 1207 loss: 1.92346379e-05
Iter: 1208 loss: 1.91443251e-05
Iter: 1209 loss: 1.91319596e-05
Iter: 1210 loss: 1.91463e-05
Iter: 1211 loss: 1.91254e-05
Iter: 1212 loss: 1.91140134e-05
Iter: 1213 loss: 1.91170202e-05
Iter: 1214 loss: 1.91058116e-05
Iter: 1215 loss: 1.90929313e-05
Iter: 1216 loss: 1.92512653e-05
Iter: 1217 loss: 1.90927567e-05
Iter: 1218 loss: 1.90823812e-05
Iter: 1219 loss: 1.9075138e-05
Iter: 1220 loss: 1.90714127e-05
Iter: 1221 loss: 1.90584651e-05
Iter: 1222 loss: 1.92176794e-05
Iter: 1223 loss: 1.90583651e-05
Iter: 1224 loss: 1.90472401e-05
Iter: 1225 loss: 1.90430019e-05
Iter: 1226 loss: 1.90369356e-05
Iter: 1227 loss: 1.90242699e-05
Iter: 1228 loss: 1.90958708e-05
Iter: 1229 loss: 1.90225801e-05
Iter: 1230 loss: 1.90095288e-05
Iter: 1231 loss: 1.90241535e-05
Iter: 1232 loss: 1.90024421e-05
Iter: 1233 loss: 1.8989751e-05
Iter: 1234 loss: 1.90103565e-05
Iter: 1235 loss: 1.89838102e-05
Iter: 1236 loss: 1.89716739e-05
Iter: 1237 loss: 1.91027102e-05
Iter: 1238 loss: 1.89713846e-05
Iter: 1239 loss: 1.8961473e-05
Iter: 1240 loss: 1.89560124e-05
Iter: 1241 loss: 1.89516031e-05
Iter: 1242 loss: 1.89398506e-05
Iter: 1243 loss: 1.9089568e-05
Iter: 1244 loss: 1.8939747e-05
Iter: 1245 loss: 1.89308e-05
Iter: 1246 loss: 1.89231978e-05
Iter: 1247 loss: 1.89206839e-05
Iter: 1248 loss: 1.89085367e-05
Iter: 1249 loss: 1.89514976e-05
Iter: 1250 loss: 1.89054335e-05
Iter: 1251 loss: 1.88924751e-05
Iter: 1252 loss: 1.89662351e-05
Iter: 1253 loss: 1.88907507e-05
Iter: 1254 loss: 1.8880899e-05
Iter: 1255 loss: 1.88826198e-05
Iter: 1256 loss: 1.88734721e-05
Iter: 1257 loss: 1.88598406e-05
Iter: 1258 loss: 1.89714083e-05
Iter: 1259 loss: 1.88589474e-05
Iter: 1260 loss: 1.88492e-05
Iter: 1261 loss: 1.88427166e-05
Iter: 1262 loss: 1.88391223e-05
Iter: 1263 loss: 1.88268568e-05
Iter: 1264 loss: 1.89588645e-05
Iter: 1265 loss: 1.88266167e-05
Iter: 1266 loss: 1.88166232e-05
Iter: 1267 loss: 1.88104168e-05
Iter: 1268 loss: 1.88064168e-05
Iter: 1269 loss: 1.87940605e-05
Iter: 1270 loss: 1.886889e-05
Iter: 1271 loss: 1.87924707e-05
Iter: 1272 loss: 1.87810365e-05
Iter: 1273 loss: 1.88341237e-05
Iter: 1274 loss: 1.87788009e-05
Iter: 1275 loss: 1.8769646e-05
Iter: 1276 loss: 1.87737387e-05
Iter: 1277 loss: 1.87633559e-05
Iter: 1278 loss: 1.87506448e-05
Iter: 1279 loss: 1.88305894e-05
Iter: 1280 loss: 1.87491678e-05
Iter: 1281 loss: 1.87405258e-05
Iter: 1282 loss: 1.87319747e-05
Iter: 1283 loss: 1.87301885e-05
Iter: 1284 loss: 1.87183941e-05
Iter: 1285 loss: 1.88355516e-05
Iter: 1286 loss: 1.8718014e-05
Iter: 1287 loss: 1.87068781e-05
Iter: 1288 loss: 1.87205715e-05
Iter: 1289 loss: 1.87010573e-05
Iter: 1290 loss: 1.86912403e-05
Iter: 1291 loss: 1.87403894e-05
Iter: 1292 loss: 1.86895886e-05
Iter: 1293 loss: 1.8678762e-05
Iter: 1294 loss: 1.86967027e-05
Iter: 1295 loss: 1.86738653e-05
Iter: 1296 loss: 1.86644393e-05
Iter: 1297 loss: 1.86735815e-05
Iter: 1298 loss: 1.86590551e-05
Iter: 1299 loss: 1.86468606e-05
Iter: 1300 loss: 1.87165242e-05
Iter: 1301 loss: 1.86451289e-05
Iter: 1302 loss: 1.86356629e-05
Iter: 1303 loss: 1.86291454e-05
Iter: 1304 loss: 1.86256511e-05
Iter: 1305 loss: 1.86144462e-05
Iter: 1306 loss: 1.87680416e-05
Iter: 1307 loss: 1.86143698e-05
Iter: 1308 loss: 1.86048746e-05
Iter: 1309 loss: 1.86122779e-05
Iter: 1310 loss: 1.85990575e-05
Iter: 1311 loss: 1.85895333e-05
Iter: 1312 loss: 1.86339457e-05
Iter: 1313 loss: 1.85877507e-05
Iter: 1314 loss: 1.85771514e-05
Iter: 1315 loss: 1.85852077e-05
Iter: 1316 loss: 1.85706886e-05
Iter: 1317 loss: 1.85602257e-05
Iter: 1318 loss: 1.85644676e-05
Iter: 1319 loss: 1.85529516e-05
Iter: 1320 loss: 1.85421668e-05
Iter: 1321 loss: 1.86968846e-05
Iter: 1322 loss: 1.85421904e-05
Iter: 1323 loss: 1.85336103e-05
Iter: 1324 loss: 1.85309873e-05
Iter: 1325 loss: 1.85259378e-05
Iter: 1326 loss: 1.85165045e-05
Iter: 1327 loss: 1.8631843e-05
Iter: 1328 loss: 1.85162862e-05
Iter: 1329 loss: 1.85080808e-05
Iter: 1330 loss: 1.8499748e-05
Iter: 1331 loss: 1.84980236e-05
Iter: 1332 loss: 1.84862747e-05
Iter: 1333 loss: 1.85563549e-05
Iter: 1334 loss: 1.84848177e-05
Iter: 1335 loss: 1.84727232e-05
Iter: 1336 loss: 1.84889141e-05
Iter: 1337 loss: 1.84666169e-05
Iter: 1338 loss: 1.84553246e-05
Iter: 1339 loss: 1.84676192e-05
Iter: 1340 loss: 1.84491255e-05
Iter: 1341 loss: 1.8437353e-05
Iter: 1342 loss: 1.85777426e-05
Iter: 1343 loss: 1.84372147e-05
Iter: 1344 loss: 1.84285855e-05
Iter: 1345 loss: 1.84268938e-05
Iter: 1346 loss: 1.84210876e-05
Iter: 1347 loss: 1.84104138e-05
Iter: 1348 loss: 1.85023891e-05
Iter: 1349 loss: 1.84098717e-05
Iter: 1350 loss: 1.84008695e-05
Iter: 1351 loss: 1.83940138e-05
Iter: 1352 loss: 1.83911852e-05
Iter: 1353 loss: 1.83797565e-05
Iter: 1354 loss: 1.84196069e-05
Iter: 1355 loss: 1.83767206e-05
Iter: 1356 loss: 1.83654247e-05
Iter: 1357 loss: 1.84522887e-05
Iter: 1358 loss: 1.8364557e-05
Iter: 1359 loss: 1.83562261e-05
Iter: 1360 loss: 1.83572156e-05
Iter: 1361 loss: 1.8349896e-05
Iter: 1362 loss: 1.83392167e-05
Iter: 1363 loss: 1.84435703e-05
Iter: 1364 loss: 1.83388638e-05
Iter: 1365 loss: 1.83315751e-05
Iter: 1366 loss: 1.83226257e-05
Iter: 1367 loss: 1.83217617e-05
Iter: 1368 loss: 1.83105258e-05
Iter: 1369 loss: 1.84325399e-05
Iter: 1370 loss: 1.83102238e-05
Iter: 1371 loss: 1.83006523e-05
Iter: 1372 loss: 1.82942458e-05
Iter: 1373 loss: 1.82906842e-05
Iter: 1374 loss: 1.82788208e-05
Iter: 1375 loss: 1.83467218e-05
Iter: 1376 loss: 1.82772601e-05
Iter: 1377 loss: 1.8265484e-05
Iter: 1378 loss: 1.83169614e-05
Iter: 1379 loss: 1.82631229e-05
Iter: 1380 loss: 1.82544209e-05
Iter: 1381 loss: 1.82648982e-05
Iter: 1382 loss: 1.8249817e-05
Iter: 1383 loss: 1.82388139e-05
Iter: 1384 loss: 1.82890763e-05
Iter: 1385 loss: 1.82366748e-05
Iter: 1386 loss: 1.82275635e-05
Iter: 1387 loss: 1.8217781e-05
Iter: 1388 loss: 1.82162166e-05
Iter: 1389 loss: 1.82039657e-05
Iter: 1390 loss: 1.83180855e-05
Iter: 1391 loss: 1.82035019e-05
Iter: 1392 loss: 1.81919622e-05
Iter: 1393 loss: 1.82220429e-05
Iter: 1394 loss: 1.81881041e-05
Iter: 1395 loss: 1.81787636e-05
Iter: 1396 loss: 1.82075182e-05
Iter: 1397 loss: 1.8175946e-05
Iter: 1398 loss: 1.81648138e-05
Iter: 1399 loss: 1.8195371e-05
Iter: 1400 loss: 1.81611795e-05
Iter: 1401 loss: 1.81524883e-05
Iter: 1402 loss: 1.8152914e-05
Iter: 1403 loss: 1.81456453e-05
Iter: 1404 loss: 1.81331488e-05
Iter: 1405 loss: 1.82271906e-05
Iter: 1406 loss: 1.81320829e-05
Iter: 1407 loss: 1.81227388e-05
Iter: 1408 loss: 1.81142623e-05
Iter: 1409 loss: 1.81119e-05
Iter: 1410 loss: 1.81007381e-05
Iter: 1411 loss: 1.82503973e-05
Iter: 1412 loss: 1.81007053e-05
Iter: 1413 loss: 1.80903371e-05
Iter: 1414 loss: 1.8096609e-05
Iter: 1415 loss: 1.8083645e-05
Iter: 1416 loss: 1.80732386e-05
Iter: 1417 loss: 1.81248251e-05
Iter: 1418 loss: 1.80714924e-05
Iter: 1419 loss: 1.80606949e-05
Iter: 1420 loss: 1.80737679e-05
Iter: 1421 loss: 1.80551069e-05
Iter: 1422 loss: 1.80442403e-05
Iter: 1423 loss: 1.8041379e-05
Iter: 1424 loss: 1.80346215e-05
Iter: 1425 loss: 1.80232273e-05
Iter: 1426 loss: 1.81902815e-05
Iter: 1427 loss: 1.80232146e-05
Iter: 1428 loss: 1.80129318e-05
Iter: 1429 loss: 1.80167899e-05
Iter: 1430 loss: 1.80057887e-05
Iter: 1431 loss: 1.79958297e-05
Iter: 1432 loss: 1.8084218e-05
Iter: 1433 loss: 1.79953531e-05
Iter: 1434 loss: 1.79858434e-05
Iter: 1435 loss: 1.79851795e-05
Iter: 1436 loss: 1.79780327e-05
Iter: 1437 loss: 1.79676063e-05
Iter: 1438 loss: 1.7993043e-05
Iter: 1439 loss: 1.79637937e-05
Iter: 1440 loss: 1.79506205e-05
Iter: 1441 loss: 1.79984218e-05
Iter: 1442 loss: 1.79471845e-05
Iter: 1443 loss: 1.79365452e-05
Iter: 1444 loss: 1.79312337e-05
Iter: 1445 loss: 1.79261806e-05
Iter: 1446 loss: 1.79146336e-05
Iter: 1447 loss: 1.79145736e-05
Iter: 1448 loss: 1.7905e-05
Iter: 1449 loss: 1.79019898e-05
Iter: 1450 loss: 1.78963091e-05
Iter: 1451 loss: 1.78851e-05
Iter: 1452 loss: 1.79765193e-05
Iter: 1453 loss: 1.78843839e-05
Iter: 1454 loss: 1.78742666e-05
Iter: 1455 loss: 1.78706214e-05
Iter: 1456 loss: 1.7865048e-05
Iter: 1457 loss: 1.7851693e-05
Iter: 1458 loss: 1.78644677e-05
Iter: 1459 loss: 1.78440569e-05
Iter: 1460 loss: 1.78318223e-05
Iter: 1461 loss: 1.80161496e-05
Iter: 1462 loss: 1.78318805e-05
Iter: 1463 loss: 1.78215087e-05
Iter: 1464 loss: 1.78186128e-05
Iter: 1465 loss: 1.78123082e-05
Iter: 1466 loss: 1.78014379e-05
Iter: 1467 loss: 1.79516392e-05
Iter: 1468 loss: 1.78013725e-05
Iter: 1469 loss: 1.77925158e-05
Iter: 1470 loss: 1.77821858e-05
Iter: 1471 loss: 1.77810325e-05
Iter: 1472 loss: 1.7768245e-05
Iter: 1473 loss: 1.78453192e-05
Iter: 1474 loss: 1.77666188e-05
Iter: 1475 loss: 1.77528927e-05
Iter: 1476 loss: 1.77748789e-05
Iter: 1477 loss: 1.77465736e-05
Iter: 1478 loss: 1.77347265e-05
Iter: 1479 loss: 1.77406364e-05
Iter: 1480 loss: 1.77269449e-05
Iter: 1481 loss: 1.77146758e-05
Iter: 1482 loss: 1.79079e-05
Iter: 1483 loss: 1.77146831e-05
Iter: 1484 loss: 1.77056536e-05
Iter: 1485 loss: 1.77010879e-05
Iter: 1486 loss: 1.7696857e-05
Iter: 1487 loss: 1.768527e-05
Iter: 1488 loss: 1.77908969e-05
Iter: 1489 loss: 1.76846588e-05
Iter: 1490 loss: 1.76753238e-05
Iter: 1491 loss: 1.76667199e-05
Iter: 1492 loss: 1.76645117e-05
Iter: 1493 loss: 1.76502308e-05
Iter: 1494 loss: 1.7682476e-05
Iter: 1495 loss: 1.76448593e-05
Iter: 1496 loss: 1.76317481e-05
Iter: 1497 loss: 1.77872462e-05
Iter: 1498 loss: 1.76315843e-05
Iter: 1499 loss: 1.76212707e-05
Iter: 1500 loss: 1.76199683e-05
Iter: 1501 loss: 1.76126196e-05
Iter: 1502 loss: 1.76008762e-05
Iter: 1503 loss: 1.77507682e-05
Iter: 1504 loss: 1.76007779e-05
Iter: 1505 loss: 1.75924706e-05
Iter: 1506 loss: 1.75801433e-05
Iter: 1507 loss: 1.75798577e-05
Iter: 1508 loss: 1.75668356e-05
Iter: 1509 loss: 1.76967606e-05
Iter: 1510 loss: 1.75664118e-05
Iter: 1511 loss: 1.75536352e-05
Iter: 1512 loss: 1.75555033e-05
Iter: 1513 loss: 1.75439964e-05
Iter: 1514 loss: 1.75314235e-05
Iter: 1515 loss: 1.75618661e-05
Iter: 1516 loss: 1.75268397e-05
Iter: 1517 loss: 1.75143614e-05
Iter: 1518 loss: 1.76485119e-05
Iter: 1519 loss: 1.75140394e-05
Iter: 1520 loss: 1.75052774e-05
Iter: 1521 loss: 1.75003388e-05
Iter: 1522 loss: 1.74964516e-05
Iter: 1523 loss: 1.74828347e-05
Iter: 1524 loss: 1.75766509e-05
Iter: 1525 loss: 1.74814923e-05
Iter: 1526 loss: 1.7470742e-05
Iter: 1527 loss: 1.74599954e-05
Iter: 1528 loss: 1.74578563e-05
Iter: 1529 loss: 1.74415291e-05
Iter: 1530 loss: 1.75102214e-05
Iter: 1531 loss: 1.74381257e-05
Iter: 1532 loss: 1.74241195e-05
Iter: 1533 loss: 1.7554572e-05
Iter: 1534 loss: 1.74235192e-05
Iter: 1535 loss: 1.74133784e-05
Iter: 1536 loss: 1.74182242e-05
Iter: 1537 loss: 1.74065972e-05
Iter: 1538 loss: 1.73935987e-05
Iter: 1539 loss: 1.7489936e-05
Iter: 1540 loss: 1.73925255e-05
Iter: 1541 loss: 1.73833396e-05
Iter: 1542 loss: 1.73708286e-05
Iter: 1543 loss: 1.73702738e-05
Iter: 1544 loss: 1.7356957e-05
Iter: 1545 loss: 1.75455425e-05
Iter: 1546 loss: 1.73568933e-05
Iter: 1547 loss: 1.73450935e-05
Iter: 1548 loss: 1.73360222e-05
Iter: 1549 loss: 1.7332286e-05
Iter: 1550 loss: 1.73183616e-05
Iter: 1551 loss: 1.73832541e-05
Iter: 1552 loss: 1.73158551e-05
Iter: 1553 loss: 1.73020271e-05
Iter: 1554 loss: 1.73988592e-05
Iter: 1555 loss: 1.73007284e-05
Iter: 1556 loss: 1.72911423e-05
Iter: 1557 loss: 1.72911532e-05
Iter: 1558 loss: 1.72834443e-05
Iter: 1559 loss: 1.72694163e-05
Iter: 1560 loss: 1.73511053e-05
Iter: 1561 loss: 1.72675809e-05
Iter: 1562 loss: 1.7257229e-05
Iter: 1563 loss: 1.72482723e-05
Iter: 1564 loss: 1.72455038e-05
Iter: 1565 loss: 1.72305481e-05
Iter: 1566 loss: 1.73124536e-05
Iter: 1567 loss: 1.72283744e-05
Iter: 1568 loss: 1.72153304e-05
Iter: 1569 loss: 1.73066328e-05
Iter: 1570 loss: 1.72141299e-05
Iter: 1571 loss: 1.72045584e-05
Iter: 1572 loss: 1.72153159e-05
Iter: 1573 loss: 1.71994179e-05
Iter: 1574 loss: 1.71867832e-05
Iter: 1575 loss: 1.7244025e-05
Iter: 1576 loss: 1.7184313e-05
Iter: 1577 loss: 1.71749016e-05
Iter: 1578 loss: 1.716513e-05
Iter: 1579 loss: 1.71633692e-05
Iter: 1580 loss: 1.71503671e-05
Iter: 1581 loss: 1.73361695e-05
Iter: 1582 loss: 1.7150338e-05
Iter: 1583 loss: 1.71393294e-05
Iter: 1584 loss: 1.71266074e-05
Iter: 1585 loss: 1.71250977e-05
Iter: 1586 loss: 1.71111897e-05
Iter: 1587 loss: 1.7202552e-05
Iter: 1588 loss: 1.71097399e-05
Iter: 1589 loss: 1.70961157e-05
Iter: 1590 loss: 1.71712218e-05
Iter: 1591 loss: 1.70941184e-05
Iter: 1592 loss: 1.70843687e-05
Iter: 1593 loss: 1.70875028e-05
Iter: 1594 loss: 1.70774401e-05
Iter: 1595 loss: 1.70633757e-05
Iter: 1596 loss: 1.71304564e-05
Iter: 1597 loss: 1.70608237e-05
Iter: 1598 loss: 1.70498297e-05
Iter: 1599 loss: 1.7039416e-05
Iter: 1600 loss: 1.70369312e-05
Iter: 1601 loss: 1.70202802e-05
Iter: 1602 loss: 1.71271495e-05
Iter: 1603 loss: 1.70184157e-05
Iter: 1604 loss: 1.70043622e-05
Iter: 1605 loss: 1.7092736e-05
Iter: 1606 loss: 1.70027943e-05
Iter: 1607 loss: 1.6992557e-05
Iter: 1608 loss: 1.70084968e-05
Iter: 1609 loss: 1.69877203e-05
Iter: 1610 loss: 1.69741434e-05
Iter: 1611 loss: 1.70171352e-05
Iter: 1612 loss: 1.69702289e-05
Iter: 1613 loss: 1.69597e-05
Iter: 1614 loss: 1.6953516e-05
Iter: 1615 loss: 1.6949085e-05
Iter: 1616 loss: 1.69354789e-05
Iter: 1617 loss: 1.71167594e-05
Iter: 1618 loss: 1.69354062e-05
Iter: 1619 loss: 1.69242649e-05
Iter: 1620 loss: 1.69100422e-05
Iter: 1621 loss: 1.69090199e-05
Iter: 1622 loss: 1.68951701e-05
Iter: 1623 loss: 1.70104449e-05
Iter: 1624 loss: 1.6894388e-05
Iter: 1625 loss: 1.68809711e-05
Iter: 1626 loss: 1.69431351e-05
Iter: 1627 loss: 1.68784663e-05
Iter: 1628 loss: 1.68687166e-05
Iter: 1629 loss: 1.68732222e-05
Iter: 1630 loss: 1.68620991e-05
Iter: 1631 loss: 1.68477127e-05
Iter: 1632 loss: 1.69052146e-05
Iter: 1633 loss: 1.68444494e-05
Iter: 1634 loss: 1.68328e-05
Iter: 1635 loss: 1.68235656e-05
Iter: 1636 loss: 1.68199731e-05
Iter: 1637 loss: 1.68035713e-05
Iter: 1638 loss: 1.69299747e-05
Iter: 1639 loss: 1.68023653e-05
Iter: 1640 loss: 1.67892576e-05
Iter: 1641 loss: 1.68637744e-05
Iter: 1642 loss: 1.67874568e-05
Iter: 1643 loss: 1.67776816e-05
Iter: 1644 loss: 1.67961553e-05
Iter: 1645 loss: 1.67735125e-05
Iter: 1646 loss: 1.67604339e-05
Iter: 1647 loss: 1.67861544e-05
Iter: 1648 loss: 1.67549697e-05
Iter: 1649 loss: 1.67436974e-05
Iter: 1650 loss: 1.67423459e-05
Iter: 1651 loss: 1.67342841e-05
Iter: 1652 loss: 1.67203416e-05
Iter: 1653 loss: 1.68920815e-05
Iter: 1654 loss: 1.67202052e-05
Iter: 1655 loss: 1.67093021e-05
Iter: 1656 loss: 1.66944174e-05
Iter: 1657 loss: 1.66936152e-05
Iter: 1658 loss: 1.66796453e-05
Iter: 1659 loss: 1.68051538e-05
Iter: 1660 loss: 1.66789432e-05
Iter: 1661 loss: 1.66648715e-05
Iter: 1662 loss: 1.6720207e-05
Iter: 1663 loss: 1.6661681e-05
Iter: 1664 loss: 1.66513255e-05
Iter: 1665 loss: 1.66609116e-05
Iter: 1666 loss: 1.66454029e-05
Iter: 1667 loss: 1.66315149e-05
Iter: 1668 loss: 1.66878017e-05
Iter: 1669 loss: 1.66284681e-05
Iter: 1670 loss: 1.66178688e-05
Iter: 1671 loss: 1.66110622e-05
Iter: 1672 loss: 1.66069804e-05
Iter: 1673 loss: 1.65920464e-05
Iter: 1674 loss: 1.66993323e-05
Iter: 1675 loss: 1.65908023e-05
Iter: 1676 loss: 1.65783331e-05
Iter: 1677 loss: 1.66370501e-05
Iter: 1678 loss: 1.65760157e-05
Iter: 1679 loss: 1.65658821e-05
Iter: 1680 loss: 1.65905294e-05
Iter: 1681 loss: 1.65622514e-05
Iter: 1682 loss: 1.6549784e-05
Iter: 1683 loss: 1.65719339e-05
Iter: 1684 loss: 1.65443817e-05
Iter: 1685 loss: 1.6534088e-05
Iter: 1686 loss: 1.65369474e-05
Iter: 1687 loss: 1.65266e-05
Iter: 1688 loss: 1.65133788e-05
Iter: 1689 loss: 1.66392274e-05
Iter: 1690 loss: 1.65128422e-05
Iter: 1691 loss: 1.65023139e-05
Iter: 1692 loss: 1.64887333e-05
Iter: 1693 loss: 1.6487822e-05
Iter: 1694 loss: 1.64751691e-05
Iter: 1695 loss: 1.66077225e-05
Iter: 1696 loss: 1.64747853e-05
Iter: 1697 loss: 1.64620906e-05
Iter: 1698 loss: 1.65036727e-05
Iter: 1699 loss: 1.64585726e-05
Iter: 1700 loss: 1.6449203e-05
Iter: 1701 loss: 1.64614121e-05
Iter: 1702 loss: 1.64444136e-05
Iter: 1703 loss: 1.64317717e-05
Iter: 1704 loss: 1.64699231e-05
Iter: 1705 loss: 1.64280336e-05
Iter: 1706 loss: 1.64174598e-05
Iter: 1707 loss: 1.64127687e-05
Iter: 1708 loss: 1.64073972e-05
Iter: 1709 loss: 1.63933855e-05
Iter: 1710 loss: 1.65067177e-05
Iter: 1711 loss: 1.63924433e-05
Iter: 1712 loss: 1.63810037e-05
Iter: 1713 loss: 1.64275662e-05
Iter: 1714 loss: 1.63784407e-05
Iter: 1715 loss: 1.63684963e-05
Iter: 1716 loss: 1.63961249e-05
Iter: 1717 loss: 1.63652985e-05
Iter: 1718 loss: 1.63535151e-05
Iter: 1719 loss: 1.63680852e-05
Iter: 1720 loss: 1.63474142e-05
Iter: 1721 loss: 1.63370987e-05
Iter: 1722 loss: 1.63454206e-05
Iter: 1723 loss: 1.63308923e-05
Iter: 1724 loss: 1.63177137e-05
Iter: 1725 loss: 1.64221237e-05
Iter: 1726 loss: 1.63167952e-05
Iter: 1727 loss: 1.63067543e-05
Iter: 1728 loss: 1.62945726e-05
Iter: 1729 loss: 1.62934411e-05
Iter: 1730 loss: 1.62814504e-05
Iter: 1731 loss: 1.64144585e-05
Iter: 1732 loss: 1.62812066e-05
Iter: 1733 loss: 1.62687247e-05
Iter: 1734 loss: 1.62988345e-05
Iter: 1735 loss: 1.62642518e-05
Iter: 1736 loss: 1.62545621e-05
Iter: 1737 loss: 1.62711658e-05
Iter: 1738 loss: 1.62502056e-05
Iter: 1739 loss: 1.62375218e-05
Iter: 1740 loss: 1.62709512e-05
Iter: 1741 loss: 1.62332472e-05
Iter: 1742 loss: 1.62223951e-05
Iter: 1743 loss: 1.62188844e-05
Iter: 1744 loss: 1.62125507e-05
Iter: 1745 loss: 1.61979897e-05
Iter: 1746 loss: 1.63092e-05
Iter: 1747 loss: 1.61968946e-05
Iter: 1748 loss: 1.61846328e-05
Iter: 1749 loss: 1.62325923e-05
Iter: 1750 loss: 1.61818571e-05
Iter: 1751 loss: 1.61711578e-05
Iter: 1752 loss: 1.62043834e-05
Iter: 1753 loss: 1.61680618e-05
Iter: 1754 loss: 1.61561293e-05
Iter: 1755 loss: 1.61696753e-05
Iter: 1756 loss: 1.61496573e-05
Iter: 1757 loss: 1.61391454e-05
Iter: 1758 loss: 1.61515381e-05
Iter: 1759 loss: 1.61335338e-05
Iter: 1760 loss: 1.61201478e-05
Iter: 1761 loss: 1.62103461e-05
Iter: 1762 loss: 1.61187527e-05
Iter: 1763 loss: 1.61085409e-05
Iter: 1764 loss: 1.60957534e-05
Iter: 1765 loss: 1.60947129e-05
Iter: 1766 loss: 1.6082442e-05
Iter: 1767 loss: 1.62361466e-05
Iter: 1768 loss: 1.60823074e-05
Iter: 1769 loss: 1.60700492e-05
Iter: 1770 loss: 1.61030621e-05
Iter: 1771 loss: 1.60659529e-05
Iter: 1772 loss: 1.60570671e-05
Iter: 1773 loss: 1.60719264e-05
Iter: 1774 loss: 1.60529798e-05
Iter: 1775 loss: 1.60413583e-05
Iter: 1776 loss: 1.60729905e-05
Iter: 1777 loss: 1.60375348e-05
Iter: 1778 loss: 1.60277377e-05
Iter: 1779 loss: 1.60242653e-05
Iter: 1780 loss: 1.60187446e-05
Iter: 1781 loss: 1.60053805e-05
Iter: 1782 loss: 1.61065473e-05
Iter: 1783 loss: 1.60043146e-05
Iter: 1784 loss: 1.59932843e-05
Iter: 1785 loss: 1.60383388e-05
Iter: 1786 loss: 1.59908741e-05
Iter: 1787 loss: 1.59811589e-05
Iter: 1788 loss: 1.60107593e-05
Iter: 1789 loss: 1.59783158e-05
Iter: 1790 loss: 1.59676256e-05
Iter: 1791 loss: 1.59789088e-05
Iter: 1792 loss: 1.59617739e-05
Iter: 1793 loss: 1.59516458e-05
Iter: 1794 loss: 1.59621e-05
Iter: 1795 loss: 1.59460433e-05
Iter: 1796 loss: 1.59327701e-05
Iter: 1797 loss: 1.60183317e-05
Iter: 1798 loss: 1.59313277e-05
Iter: 1799 loss: 1.59210649e-05
Iter: 1800 loss: 1.59086248e-05
Iter: 1801 loss: 1.59074334e-05
Iter: 1802 loss: 1.58948969e-05
Iter: 1803 loss: 1.60286527e-05
Iter: 1804 loss: 1.58945768e-05
Iter: 1805 loss: 1.58818475e-05
Iter: 1806 loss: 1.5927104e-05
Iter: 1807 loss: 1.58786424e-05
Iter: 1808 loss: 1.58697203e-05
Iter: 1809 loss: 1.58764415e-05
Iter: 1810 loss: 1.58642215e-05
Iter: 1811 loss: 1.58511812e-05
Iter: 1812 loss: 1.58974199e-05
Iter: 1813 loss: 1.58479052e-05
Iter: 1814 loss: 1.58374969e-05
Iter: 1815 loss: 1.58326293e-05
Iter: 1816 loss: 1.58274943e-05
Iter: 1817 loss: 1.58137154e-05
Iter: 1818 loss: 1.59261526e-05
Iter: 1819 loss: 1.58127732e-05
Iter: 1820 loss: 1.58018411e-05
Iter: 1821 loss: 1.58463845e-05
Iter: 1822 loss: 1.57994164e-05
Iter: 1823 loss: 1.57892209e-05
Iter: 1824 loss: 1.58164e-05
Iter: 1825 loss: 1.57859358e-05
Iter: 1826 loss: 1.57743671e-05
Iter: 1827 loss: 1.57903924e-05
Iter: 1828 loss: 1.57686445e-05
Iter: 1829 loss: 1.57578543e-05
Iter: 1830 loss: 1.57646027e-05
Iter: 1831 loss: 1.57509749e-05
Iter: 1832 loss: 1.57375907e-05
Iter: 1833 loss: 1.58562852e-05
Iter: 1834 loss: 1.57368631e-05
Iter: 1835 loss: 1.57269824e-05
Iter: 1836 loss: 1.57147588e-05
Iter: 1837 loss: 1.5713671e-05
Iter: 1838 loss: 1.56998103e-05
Iter: 1839 loss: 1.57735012e-05
Iter: 1840 loss: 1.56976548e-05
Iter: 1841 loss: 1.56837e-05
Iter: 1842 loss: 1.58017283e-05
Iter: 1843 loss: 1.56829265e-05
Iter: 1844 loss: 1.56743699e-05
Iter: 1845 loss: 1.56699043e-05
Iter: 1846 loss: 1.56659262e-05
Iter: 1847 loss: 1.56523747e-05
Iter: 1848 loss: 1.57450449e-05
Iter: 1849 loss: 1.56510359e-05
Iter: 1850 loss: 1.56413626e-05
Iter: 1851 loss: 1.56325859e-05
Iter: 1852 loss: 1.56301485e-05
Iter: 1853 loss: 1.56159767e-05
Iter: 1854 loss: 1.57115301e-05
Iter: 1855 loss: 1.56145434e-05
Iter: 1856 loss: 1.56023943e-05
Iter: 1857 loss: 1.56544393e-05
Iter: 1858 loss: 1.55999223e-05
Iter: 1859 loss: 1.55888556e-05
Iter: 1860 loss: 1.56193473e-05
Iter: 1861 loss: 1.55853268e-05
Iter: 1862 loss: 1.55735052e-05
Iter: 1863 loss: 1.56056904e-05
Iter: 1864 loss: 1.55696089e-05
Iter: 1865 loss: 1.55597572e-05
Iter: 1866 loss: 1.55573744e-05
Iter: 1867 loss: 1.55511025e-05
Iter: 1868 loss: 1.5539048e-05
Iter: 1869 loss: 1.56794176e-05
Iter: 1870 loss: 1.55388352e-05
Iter: 1871 loss: 1.55287016e-05
Iter: 1872 loss: 1.5517322e-05
Iter: 1873 loss: 1.55158014e-05
Iter: 1874 loss: 1.55013367e-05
Iter: 1875 loss: 1.5532396e-05
Iter: 1876 loss: 1.54956542e-05
Iter: 1877 loss: 1.5485366e-05
Iter: 1878 loss: 1.54848258e-05
Iter: 1879 loss: 1.54769768e-05
Iter: 1880 loss: 1.54661357e-05
Iter: 1881 loss: 1.54657155e-05
Iter: 1882 loss: 1.54531335e-05
Iter: 1883 loss: 1.5587435e-05
Iter: 1884 loss: 1.54527952e-05
Iter: 1885 loss: 1.54429363e-05
Iter: 1886 loss: 1.54333375e-05
Iter: 1887 loss: 1.54312129e-05
Iter: 1888 loss: 1.54167792e-05
Iter: 1889 loss: 1.54843565e-05
Iter: 1890 loss: 1.54141653e-05
Iter: 1891 loss: 1.54014979e-05
Iter: 1892 loss: 1.54727077e-05
Iter: 1893 loss: 1.53997098e-05
Iter: 1894 loss: 1.53889723e-05
Iter: 1895 loss: 1.54152422e-05
Iter: 1896 loss: 1.53851652e-05
Iter: 1897 loss: 1.53739929e-05
Iter: 1898 loss: 1.5426227e-05
Iter: 1899 loss: 1.53719739e-05
Iter: 1900 loss: 1.53625842e-05
Iter: 1901 loss: 1.5354226e-05
Iter: 1902 loss: 1.53518467e-05
Iter: 1903 loss: 1.53387518e-05
Iter: 1904 loss: 1.54365916e-05
Iter: 1905 loss: 1.53377659e-05
Iter: 1906 loss: 1.53244018e-05
Iter: 1907 loss: 1.53393776e-05
Iter: 1908 loss: 1.53171768e-05
Iter: 1909 loss: 1.53052551e-05
Iter: 1910 loss: 1.53017936e-05
Iter: 1911 loss: 1.52946013e-05
Iter: 1912 loss: 1.52838329e-05
Iter: 1913 loss: 1.52835273e-05
Iter: 1914 loss: 1.52725079e-05
Iter: 1915 loss: 1.52691828e-05
Iter: 1916 loss: 1.52626e-05
Iter: 1917 loss: 1.52507182e-05
Iter: 1918 loss: 1.53317524e-05
Iter: 1919 loss: 1.52495668e-05
Iter: 1920 loss: 1.52381381e-05
Iter: 1921 loss: 1.52407592e-05
Iter: 1922 loss: 1.52297143e-05
Iter: 1923 loss: 1.52170842e-05
Iter: 1924 loss: 1.52391167e-05
Iter: 1925 loss: 1.52115135e-05
Iter: 1926 loss: 1.51973318e-05
Iter: 1927 loss: 1.52725079e-05
Iter: 1928 loss: 1.51951817e-05
Iter: 1929 loss: 1.51825861e-05
Iter: 1930 loss: 1.52174307e-05
Iter: 1931 loss: 1.51785443e-05
Iter: 1932 loss: 1.5166961e-05
Iter: 1933 loss: 1.52385155e-05
Iter: 1934 loss: 1.51655449e-05
Iter: 1935 loss: 1.51552013e-05
Iter: 1936 loss: 1.51500662e-05
Iter: 1937 loss: 1.5145175e-05
Iter: 1938 loss: 1.51312634e-05
Iter: 1939 loss: 1.51522245e-05
Iter: 1940 loss: 1.51246732e-05
Iter: 1941 loss: 1.51092827e-05
Iter: 1942 loss: 1.52682987e-05
Iter: 1943 loss: 1.51088243e-05
Iter: 1944 loss: 1.50990645e-05
Iter: 1945 loss: 1.50849919e-05
Iter: 1946 loss: 1.50845617e-05
Iter: 1947 loss: 1.50680235e-05
Iter: 1948 loss: 1.51234417e-05
Iter: 1949 loss: 1.50635442e-05
Iter: 1950 loss: 1.50503101e-05
Iter: 1951 loss: 1.50502456e-05
Iter: 1952 loss: 1.50416672e-05
Iter: 1953 loss: 1.50367177e-05
Iter: 1954 loss: 1.50330707e-05
Iter: 1955 loss: 1.5020335e-05
Iter: 1956 loss: 1.5113681e-05
Iter: 1957 loss: 1.50192554e-05
Iter: 1958 loss: 1.50095129e-05
Iter: 1959 loss: 1.50000469e-05
Iter: 1960 loss: 1.49979687e-05
Iter: 1961 loss: 1.49815787e-05
Iter: 1962 loss: 1.50507603e-05
Iter: 1963 loss: 1.49781135e-05
Iter: 1964 loss: 1.496409e-05
Iter: 1965 loss: 1.5041016e-05
Iter: 1966 loss: 1.49620018e-05
Iter: 1967 loss: 1.4950645e-05
Iter: 1968 loss: 1.4971024e-05
Iter: 1969 loss: 1.49456882e-05
Iter: 1970 loss: 1.49339194e-05
Iter: 1971 loss: 1.50268097e-05
Iter: 1972 loss: 1.49330817e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script72
+ '[' -r STOP.script72 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.2/300_300_300_1 '
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi1.6
+ date
Sat Oct 31 16:01:14 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.2/300_300_300_1 --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 30 --learning_rate 0.001 --decay_rate 0.8 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe810501510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe810513e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe810513d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8105417b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8104aa488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe81022e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8104aa0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe81033c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe810287400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8102a99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe81019b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8101a7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8101a7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8103e4c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8102f19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8102f1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8103148c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8101b2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8101b9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8102d26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8102e68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe81006cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe810383730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7c46d5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7c46d51e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7c46dc620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe810084ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7c467f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7c467f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7c4681488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7c47438c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7c456d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7c45a2158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe81022bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe8101eeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe7c465bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.007947572
test_loss: 0.008432029
train_loss: 0.006201946
test_loss: 0.0064156423
train_loss: 0.005053264
test_loss: 0.0058013345
train_loss: 0.0048436853
test_loss: 0.0056131813
train_loss: 0.0047514467
test_loss: 0.0055505456
train_loss: 0.00473022
test_loss: 0.005537608
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb600a23d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb600a23bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb600a0cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb600938510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb600910378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb600910f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6008dd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6008ddc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb600804378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb600804840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6007ac730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5dc48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5dc4048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb600851c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5da8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5d8dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5da2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5da2e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5d11620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5d11b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5c816a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5c81ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5cca7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5bf7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5bf7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5bbe620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5bbe8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5c41510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5b918c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5b9d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5ac08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5aec6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5b2f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5b1fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5a9a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb5f5a75f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.13480338e-05
Iter: 2 loss: 4.15293798e-05
Iter: 3 loss: 4.07753723e-05
Iter: 4 loss: 4.05235332e-05
Iter: 5 loss: 4.16629264e-05
Iter: 6 loss: 4.04748789e-05
Iter: 7 loss: 4.02243859e-05
Iter: 8 loss: 4.04033199e-05
Iter: 9 loss: 4.00690042e-05
Iter: 10 loss: 3.97882759e-05
Iter: 11 loss: 4.01874e-05
Iter: 12 loss: 3.96503056e-05
Iter: 13 loss: 3.94092494e-05
Iter: 14 loss: 4.15905088e-05
Iter: 15 loss: 3.93979e-05
Iter: 16 loss: 3.91883441e-05
Iter: 17 loss: 3.97780132e-05
Iter: 18 loss: 3.91214562e-05
Iter: 19 loss: 3.89806592e-05
Iter: 20 loss: 3.89148845e-05
Iter: 21 loss: 3.88454573e-05
Iter: 22 loss: 3.87101572e-05
Iter: 23 loss: 3.87079817e-05
Iter: 24 loss: 3.8597158e-05
Iter: 25 loss: 3.85183885e-05
Iter: 26 loss: 3.84796294e-05
Iter: 27 loss: 3.83438673e-05
Iter: 28 loss: 3.85377643e-05
Iter: 29 loss: 3.82775252e-05
Iter: 30 loss: 3.81387072e-05
Iter: 31 loss: 3.9593062e-05
Iter: 32 loss: 3.81349528e-05
Iter: 33 loss: 3.80060446e-05
Iter: 34 loss: 3.80372148e-05
Iter: 35 loss: 3.79117373e-05
Iter: 36 loss: 3.7784208e-05
Iter: 37 loss: 3.78956101e-05
Iter: 38 loss: 3.77094148e-05
Iter: 39 loss: 3.75922937e-05
Iter: 40 loss: 3.80348756e-05
Iter: 41 loss: 3.75640302e-05
Iter: 42 loss: 3.74520059e-05
Iter: 43 loss: 3.77399119e-05
Iter: 44 loss: 3.74134943e-05
Iter: 45 loss: 3.73424264e-05
Iter: 46 loss: 3.73403673e-05
Iter: 47 loss: 3.72689392e-05
Iter: 48 loss: 3.72242685e-05
Iter: 49 loss: 3.71958668e-05
Iter: 50 loss: 3.71104106e-05
Iter: 51 loss: 3.71983551e-05
Iter: 52 loss: 3.70627531e-05
Iter: 53 loss: 3.70114285e-05
Iter: 54 loss: 3.70050693e-05
Iter: 55 loss: 3.69619229e-05
Iter: 56 loss: 3.6895246e-05
Iter: 57 loss: 3.68942819e-05
Iter: 58 loss: 3.68236761e-05
Iter: 59 loss: 3.70280577e-05
Iter: 60 loss: 3.6801659e-05
Iter: 61 loss: 3.67318135e-05
Iter: 62 loss: 3.74394585e-05
Iter: 63 loss: 3.67296525e-05
Iter: 64 loss: 3.66775348e-05
Iter: 65 loss: 3.6635658e-05
Iter: 66 loss: 3.66200147e-05
Iter: 67 loss: 3.65527085e-05
Iter: 68 loss: 3.70200796e-05
Iter: 69 loss: 3.65463675e-05
Iter: 70 loss: 3.64924126e-05
Iter: 71 loss: 3.65693886e-05
Iter: 72 loss: 3.64659172e-05
Iter: 73 loss: 3.64131411e-05
Iter: 74 loss: 3.68481851e-05
Iter: 75 loss: 3.64098451e-05
Iter: 76 loss: 3.63602412e-05
Iter: 77 loss: 3.63232903e-05
Iter: 78 loss: 3.63067811e-05
Iter: 79 loss: 3.62424107e-05
Iter: 80 loss: 3.6319303e-05
Iter: 81 loss: 3.62083556e-05
Iter: 82 loss: 3.61309649e-05
Iter: 83 loss: 3.63733452e-05
Iter: 84 loss: 3.61085295e-05
Iter: 85 loss: 3.60704216e-05
Iter: 86 loss: 3.60649065e-05
Iter: 87 loss: 3.60288614e-05
Iter: 88 loss: 3.59782134e-05
Iter: 89 loss: 3.59763144e-05
Iter: 90 loss: 3.59213955e-05
Iter: 91 loss: 3.60293343e-05
Iter: 92 loss: 3.58987199e-05
Iter: 93 loss: 3.58518955e-05
Iter: 94 loss: 3.58514844e-05
Iter: 95 loss: 3.58213365e-05
Iter: 96 loss: 3.5762172e-05
Iter: 97 loss: 3.69373156e-05
Iter: 98 loss: 3.57616082e-05
Iter: 99 loss: 3.56981545e-05
Iter: 100 loss: 3.59427213e-05
Iter: 101 loss: 3.56831551e-05
Iter: 102 loss: 3.56389e-05
Iter: 103 loss: 3.56388337e-05
Iter: 104 loss: 3.56009659e-05
Iter: 105 loss: 3.55491138e-05
Iter: 106 loss: 3.55466473e-05
Iter: 107 loss: 3.54861477e-05
Iter: 108 loss: 3.56159035e-05
Iter: 109 loss: 3.54624863e-05
Iter: 110 loss: 3.54099066e-05
Iter: 111 loss: 3.58909056e-05
Iter: 112 loss: 3.54075055e-05
Iter: 113 loss: 3.53556461e-05
Iter: 114 loss: 3.54747099e-05
Iter: 115 loss: 3.53362775e-05
Iter: 116 loss: 3.52971e-05
Iter: 117 loss: 3.52693532e-05
Iter: 118 loss: 3.52556744e-05
Iter: 119 loss: 3.519643e-05
Iter: 120 loss: 3.54027143e-05
Iter: 121 loss: 3.51809722e-05
Iter: 122 loss: 3.51346462e-05
Iter: 123 loss: 3.57719691e-05
Iter: 124 loss: 3.51345079e-05
Iter: 125 loss: 3.50926421e-05
Iter: 126 loss: 3.51823692e-05
Iter: 127 loss: 3.50762966e-05
Iter: 128 loss: 3.50402552e-05
Iter: 129 loss: 3.50220143e-05
Iter: 130 loss: 3.50052251e-05
Iter: 131 loss: 3.49556576e-05
Iter: 132 loss: 3.53994074e-05
Iter: 133 loss: 3.49532747e-05
Iter: 134 loss: 3.49137736e-05
Iter: 135 loss: 3.50475893e-05
Iter: 136 loss: 3.49031252e-05
Iter: 137 loss: 3.48636386e-05
Iter: 138 loss: 3.48868671e-05
Iter: 139 loss: 3.48380781e-05
Iter: 140 loss: 3.47963651e-05
Iter: 141 loss: 3.47878158e-05
Iter: 142 loss: 3.47602399e-05
Iter: 143 loss: 3.47173118e-05
Iter: 144 loss: 3.53497671e-05
Iter: 145 loss: 3.47172609e-05
Iter: 146 loss: 3.4673023e-05
Iter: 147 loss: 3.470089e-05
Iter: 148 loss: 3.46447923e-05
Iter: 149 loss: 3.46074157e-05
Iter: 150 loss: 3.45782537e-05
Iter: 151 loss: 3.45664339e-05
Iter: 152 loss: 3.45145e-05
Iter: 153 loss: 3.4876226e-05
Iter: 154 loss: 3.45095978e-05
Iter: 155 loss: 3.44597e-05
Iter: 156 loss: 3.48387912e-05
Iter: 157 loss: 3.44558284e-05
Iter: 158 loss: 3.44234177e-05
Iter: 159 loss: 3.43619649e-05
Iter: 160 loss: 3.57055469e-05
Iter: 161 loss: 3.43618558e-05
Iter: 162 loss: 3.4291792e-05
Iter: 163 loss: 3.4703553e-05
Iter: 164 loss: 3.42826424e-05
Iter: 165 loss: 3.42501444e-05
Iter: 166 loss: 3.42473431e-05
Iter: 167 loss: 3.42195162e-05
Iter: 168 loss: 3.41855412e-05
Iter: 169 loss: 3.41823579e-05
Iter: 170 loss: 3.41407504e-05
Iter: 171 loss: 3.41911655e-05
Iter: 172 loss: 3.41188352e-05
Iter: 173 loss: 3.40770275e-05
Iter: 174 loss: 3.47411915e-05
Iter: 175 loss: 3.40770166e-05
Iter: 176 loss: 3.404167e-05
Iter: 177 loss: 3.39829785e-05
Iter: 178 loss: 3.39827093e-05
Iter: 179 loss: 3.39228209e-05
Iter: 180 loss: 3.41161431e-05
Iter: 181 loss: 3.3905977e-05
Iter: 182 loss: 3.38461505e-05
Iter: 183 loss: 3.4234723e-05
Iter: 184 loss: 3.38397513e-05
Iter: 185 loss: 3.38002e-05
Iter: 186 loss: 3.40078477e-05
Iter: 187 loss: 3.37939782e-05
Iter: 188 loss: 3.37551537e-05
Iter: 189 loss: 3.38041245e-05
Iter: 190 loss: 3.37351e-05
Iter: 191 loss: 3.36964658e-05
Iter: 192 loss: 3.36517078e-05
Iter: 193 loss: 3.36464836e-05
Iter: 194 loss: 3.35807781e-05
Iter: 195 loss: 3.39973813e-05
Iter: 196 loss: 3.35732911e-05
Iter: 197 loss: 3.35196019e-05
Iter: 198 loss: 3.39613471e-05
Iter: 199 loss: 3.35163277e-05
Iter: 200 loss: 3.34694e-05
Iter: 201 loss: 3.34982797e-05
Iter: 202 loss: 3.34393553e-05
Iter: 203 loss: 3.33947282e-05
Iter: 204 loss: 3.33956123e-05
Iter: 205 loss: 3.3359458e-05
Iter: 206 loss: 3.33329226e-05
Iter: 207 loss: 3.33257049e-05
Iter: 208 loss: 3.32961354e-05
Iter: 209 loss: 3.3243974e-05
Iter: 210 loss: 3.32440432e-05
Iter: 211 loss: 3.31864176e-05
Iter: 212 loss: 3.33195967e-05
Iter: 213 loss: 3.3164979e-05
Iter: 214 loss: 3.31206029e-05
Iter: 215 loss: 3.31205083e-05
Iter: 216 loss: 3.30803887e-05
Iter: 217 loss: 3.30254843e-05
Iter: 218 loss: 3.30228359e-05
Iter: 219 loss: 3.29667419e-05
Iter: 220 loss: 3.29477916e-05
Iter: 221 loss: 3.29156101e-05
Iter: 222 loss: 3.28773458e-05
Iter: 223 loss: 3.28701935e-05
Iter: 224 loss: 3.28255701e-05
Iter: 225 loss: 3.28073438e-05
Iter: 226 loss: 3.27838e-05
Iter: 227 loss: 3.27247035e-05
Iter: 228 loss: 3.2889071e-05
Iter: 229 loss: 3.27056769e-05
Iter: 230 loss: 3.26497757e-05
Iter: 231 loss: 3.27905e-05
Iter: 232 loss: 3.26302106e-05
Iter: 233 loss: 3.25721921e-05
Iter: 234 loss: 3.26966401e-05
Iter: 235 loss: 3.25495603e-05
Iter: 236 loss: 3.24977118e-05
Iter: 237 loss: 3.29155664e-05
Iter: 238 loss: 3.24943176e-05
Iter: 239 loss: 3.24424545e-05
Iter: 240 loss: 3.24474022e-05
Iter: 241 loss: 3.24024222e-05
Iter: 242 loss: 3.23507847e-05
Iter: 243 loss: 3.24066e-05
Iter: 244 loss: 3.23224522e-05
Iter: 245 loss: 3.22718588e-05
Iter: 246 loss: 3.22713604e-05
Iter: 247 loss: 3.22399501e-05
Iter: 248 loss: 3.2185606e-05
Iter: 249 loss: 3.21855077e-05
Iter: 250 loss: 3.21255138e-05
Iter: 251 loss: 3.23274508e-05
Iter: 252 loss: 3.21092084e-05
Iter: 253 loss: 3.2064283e-05
Iter: 254 loss: 3.20641193e-05
Iter: 255 loss: 3.2032e-05
Iter: 256 loss: 3.19694809e-05
Iter: 257 loss: 3.32455384e-05
Iter: 258 loss: 3.19689861e-05
Iter: 259 loss: 3.18944512e-05
Iter: 260 loss: 3.19636965e-05
Iter: 261 loss: 3.18515667e-05
Iter: 262 loss: 3.1792064e-05
Iter: 263 loss: 3.17921294e-05
Iter: 264 loss: 3.17348458e-05
Iter: 265 loss: 3.18831444e-05
Iter: 266 loss: 3.17153099e-05
Iter: 267 loss: 3.16686164e-05
Iter: 268 loss: 3.16245714e-05
Iter: 269 loss: 3.16137812e-05
Iter: 270 loss: 3.15416764e-05
Iter: 271 loss: 3.17396334e-05
Iter: 272 loss: 3.15182624e-05
Iter: 273 loss: 3.14612189e-05
Iter: 274 loss: 3.22872438e-05
Iter: 275 loss: 3.14611971e-05
Iter: 276 loss: 3.14114368e-05
Iter: 277 loss: 3.14392091e-05
Iter: 278 loss: 3.13789315e-05
Iter: 279 loss: 3.13220698e-05
Iter: 280 loss: 3.16199803e-05
Iter: 281 loss: 3.13131241e-05
Iter: 282 loss: 3.12639604e-05
Iter: 283 loss: 3.13126366e-05
Iter: 284 loss: 3.12362426e-05
Iter: 285 loss: 3.11826e-05
Iter: 286 loss: 3.17879785e-05
Iter: 287 loss: 3.11815638e-05
Iter: 288 loss: 3.11496078e-05
Iter: 289 loss: 3.10858595e-05
Iter: 290 loss: 3.23124368e-05
Iter: 291 loss: 3.10851319e-05
Iter: 292 loss: 3.10170108e-05
Iter: 293 loss: 3.15703437e-05
Iter: 294 loss: 3.10125979e-05
Iter: 295 loss: 3.09609859e-05
Iter: 296 loss: 3.14284953e-05
Iter: 297 loss: 3.09586394e-05
Iter: 298 loss: 3.09199386e-05
Iter: 299 loss: 3.08633353e-05
Iter: 300 loss: 3.0861851e-05
Iter: 301 loss: 3.08003764e-05
Iter: 302 loss: 3.08264753e-05
Iter: 303 loss: 3.07582304e-05
Iter: 304 loss: 3.06831353e-05
Iter: 305 loss: 3.12514712e-05
Iter: 306 loss: 3.06774164e-05
Iter: 307 loss: 3.06146394e-05
Iter: 308 loss: 3.12730626e-05
Iter: 309 loss: 3.06129477e-05
Iter: 310 loss: 3.05737631e-05
Iter: 311 loss: 3.05152516e-05
Iter: 312 loss: 3.05140275e-05
Iter: 313 loss: 3.04461919e-05
Iter: 314 loss: 3.0630883e-05
Iter: 315 loss: 3.04239111e-05
Iter: 316 loss: 3.03593733e-05
Iter: 317 loss: 3.0687097e-05
Iter: 318 loss: 3.03487213e-05
Iter: 319 loss: 3.02960725e-05
Iter: 320 loss: 3.08272065e-05
Iter: 321 loss: 3.02944754e-05
Iter: 322 loss: 3.02505541e-05
Iter: 323 loss: 3.0204641e-05
Iter: 324 loss: 3.01966102e-05
Iter: 325 loss: 3.01368273e-05
Iter: 326 loss: 3.07032205e-05
Iter: 327 loss: 3.01344644e-05
Iter: 328 loss: 3.00839438e-05
Iter: 329 loss: 3.02685567e-05
Iter: 330 loss: 3.00713891e-05
Iter: 331 loss: 3.0025556e-05
Iter: 332 loss: 3.01252367e-05
Iter: 333 loss: 3.00078427e-05
Iter: 334 loss: 2.99626554e-05
Iter: 335 loss: 2.99283129e-05
Iter: 336 loss: 2.99137319e-05
Iter: 337 loss: 2.98613159e-05
Iter: 338 loss: 3.0582687e-05
Iter: 339 loss: 2.98611048e-05
Iter: 340 loss: 2.98068262e-05
Iter: 341 loss: 2.98333071e-05
Iter: 342 loss: 2.97704592e-05
Iter: 343 loss: 2.97235674e-05
Iter: 344 loss: 2.96833241e-05
Iter: 345 loss: 2.96705621e-05
Iter: 346 loss: 2.96023718e-05
Iter: 347 loss: 2.97985807e-05
Iter: 348 loss: 2.95809332e-05
Iter: 349 loss: 2.95371829e-05
Iter: 350 loss: 2.95339669e-05
Iter: 351 loss: 2.94961756e-05
Iter: 352 loss: 2.94670936e-05
Iter: 353 loss: 2.94550791e-05
Iter: 354 loss: 2.93999801e-05
Iter: 355 loss: 2.94363472e-05
Iter: 356 loss: 2.93652411e-05
Iter: 357 loss: 2.93015655e-05
Iter: 358 loss: 2.95035243e-05
Iter: 359 loss: 2.92835157e-05
Iter: 360 loss: 2.92343557e-05
Iter: 361 loss: 2.99245312e-05
Iter: 362 loss: 2.92343011e-05
Iter: 363 loss: 2.91909819e-05
Iter: 364 loss: 2.91992692e-05
Iter: 365 loss: 2.9158824e-05
Iter: 366 loss: 2.91086653e-05
Iter: 367 loss: 2.92053373e-05
Iter: 368 loss: 2.90876796e-05
Iter: 369 loss: 2.90344069e-05
Iter: 370 loss: 2.9293944e-05
Iter: 371 loss: 2.90250919e-05
Iter: 372 loss: 2.89811214e-05
Iter: 373 loss: 2.92495988e-05
Iter: 374 loss: 2.89757445e-05
Iter: 375 loss: 2.89344e-05
Iter: 376 loss: 2.89947511e-05
Iter: 377 loss: 2.89144264e-05
Iter: 378 loss: 2.8873299e-05
Iter: 379 loss: 2.88142255e-05
Iter: 380 loss: 2.88123083e-05
Iter: 381 loss: 2.87590792e-05
Iter: 382 loss: 2.8759041e-05
Iter: 383 loss: 2.87117036e-05
Iter: 384 loss: 2.8903869e-05
Iter: 385 loss: 2.87012044e-05
Iter: 386 loss: 2.86694376e-05
Iter: 387 loss: 2.86188897e-05
Iter: 388 loss: 2.86184404e-05
Iter: 389 loss: 2.8558854e-05
Iter: 390 loss: 2.88098672e-05
Iter: 391 loss: 2.85462884e-05
Iter: 392 loss: 2.84965681e-05
Iter: 393 loss: 2.92069381e-05
Iter: 394 loss: 2.84963462e-05
Iter: 395 loss: 2.84609087e-05
Iter: 396 loss: 2.84339621e-05
Iter: 397 loss: 2.84224116e-05
Iter: 398 loss: 2.83705704e-05
Iter: 399 loss: 2.83711888e-05
Iter: 400 loss: 2.83292156e-05
Iter: 401 loss: 2.82712936e-05
Iter: 402 loss: 2.87708481e-05
Iter: 403 loss: 2.82682195e-05
Iter: 404 loss: 2.82150759e-05
Iter: 405 loss: 2.85233764e-05
Iter: 406 loss: 2.82079945e-05
Iter: 407 loss: 2.81648463e-05
Iter: 408 loss: 2.81658286e-05
Iter: 409 loss: 2.81305402e-05
Iter: 410 loss: 2.80831446e-05
Iter: 411 loss: 2.83122499e-05
Iter: 412 loss: 2.80747226e-05
Iter: 413 loss: 2.80303357e-05
Iter: 414 loss: 2.82860037e-05
Iter: 415 loss: 2.80243949e-05
Iter: 416 loss: 2.79890937e-05
Iter: 417 loss: 2.81324956e-05
Iter: 418 loss: 2.7981263e-05
Iter: 419 loss: 2.79479555e-05
Iter: 420 loss: 2.78965217e-05
Iter: 421 loss: 2.78958087e-05
Iter: 422 loss: 2.7837681e-05
Iter: 423 loss: 2.79615924e-05
Iter: 424 loss: 2.78147636e-05
Iter: 425 loss: 2.7769107e-05
Iter: 426 loss: 2.77690669e-05
Iter: 427 loss: 2.77277559e-05
Iter: 428 loss: 2.7830265e-05
Iter: 429 loss: 2.77131912e-05
Iter: 430 loss: 2.76800383e-05
Iter: 431 loss: 2.76276241e-05
Iter: 432 loss: 2.76271276e-05
Iter: 433 loss: 2.75724778e-05
Iter: 434 loss: 2.79574433e-05
Iter: 435 loss: 2.7567532e-05
Iter: 436 loss: 2.75210805e-05
Iter: 437 loss: 2.79838023e-05
Iter: 438 loss: 2.75195835e-05
Iter: 439 loss: 2.74847152e-05
Iter: 440 loss: 2.74493468e-05
Iter: 441 loss: 2.74424638e-05
Iter: 442 loss: 2.73939022e-05
Iter: 443 loss: 2.74723916e-05
Iter: 444 loss: 2.73714923e-05
Iter: 445 loss: 2.73271435e-05
Iter: 446 loss: 2.78787193e-05
Iter: 447 loss: 2.73267196e-05
Iter: 448 loss: 2.7287495e-05
Iter: 449 loss: 2.73618971e-05
Iter: 450 loss: 2.72708603e-05
Iter: 451 loss: 2.72335565e-05
Iter: 452 loss: 2.73002952e-05
Iter: 453 loss: 2.72173056e-05
Iter: 454 loss: 2.71793124e-05
Iter: 455 loss: 2.74981812e-05
Iter: 456 loss: 2.71771678e-05
Iter: 457 loss: 2.71461959e-05
Iter: 458 loss: 2.7154194e-05
Iter: 459 loss: 2.71237222e-05
Iter: 460 loss: 2.70825039e-05
Iter: 461 loss: 2.71669924e-05
Iter: 462 loss: 2.70659184e-05
Iter: 463 loss: 2.70244873e-05
Iter: 464 loss: 2.6986414e-05
Iter: 465 loss: 2.69762895e-05
Iter: 466 loss: 2.69193697e-05
Iter: 467 loss: 2.72909183e-05
Iter: 468 loss: 2.69132834e-05
Iter: 469 loss: 2.68839431e-05
Iter: 470 loss: 2.68821823e-05
Iter: 471 loss: 2.68580716e-05
Iter: 472 loss: 2.68220065e-05
Iter: 473 loss: 2.68212098e-05
Iter: 474 loss: 2.67761807e-05
Iter: 475 loss: 2.68297554e-05
Iter: 476 loss: 2.67523719e-05
Iter: 477 loss: 2.67035084e-05
Iter: 478 loss: 2.68654367e-05
Iter: 479 loss: 2.66901043e-05
Iter: 480 loss: 2.6645941e-05
Iter: 481 loss: 2.72817615e-05
Iter: 482 loss: 2.66458392e-05
Iter: 483 loss: 2.66181123e-05
Iter: 484 loss: 2.65641138e-05
Iter: 485 loss: 2.76634673e-05
Iter: 486 loss: 2.656373e-05
Iter: 487 loss: 2.65156232e-05
Iter: 488 loss: 2.68991953e-05
Iter: 489 loss: 2.65122817e-05
Iter: 490 loss: 2.64668743e-05
Iter: 491 loss: 2.67641626e-05
Iter: 492 loss: 2.64621067e-05
Iter: 493 loss: 2.64319351e-05
Iter: 494 loss: 2.64729206e-05
Iter: 495 loss: 2.6416863e-05
Iter: 496 loss: 2.63813836e-05
Iter: 497 loss: 2.65767267e-05
Iter: 498 loss: 2.63762304e-05
Iter: 499 loss: 2.634872e-05
Iter: 500 loss: 2.63878283e-05
Iter: 501 loss: 2.63352431e-05
Iter: 502 loss: 2.63011025e-05
Iter: 503 loss: 2.63176225e-05
Iter: 504 loss: 2.62782014e-05
Iter: 505 loss: 2.62365065e-05
Iter: 506 loss: 2.62352696e-05
Iter: 507 loss: 2.62028752e-05
Iter: 508 loss: 2.61526457e-05
Iter: 509 loss: 2.64701521e-05
Iter: 510 loss: 2.61469431e-05
Iter: 511 loss: 2.61073055e-05
Iter: 512 loss: 2.66532425e-05
Iter: 513 loss: 2.61071473e-05
Iter: 514 loss: 2.6082962e-05
Iter: 515 loss: 2.60391025e-05
Iter: 516 loss: 2.70791843e-05
Iter: 517 loss: 2.60390807e-05
Iter: 518 loss: 2.59922035e-05
Iter: 519 loss: 2.61720597e-05
Iter: 520 loss: 2.59810658e-05
Iter: 521 loss: 2.59464305e-05
Iter: 522 loss: 2.62966932e-05
Iter: 523 loss: 2.59453973e-05
Iter: 524 loss: 2.59118406e-05
Iter: 525 loss: 2.59777808e-05
Iter: 526 loss: 2.58980126e-05
Iter: 527 loss: 2.5868394e-05
Iter: 528 loss: 2.5830388e-05
Iter: 529 loss: 2.58276541e-05
Iter: 530 loss: 2.57912689e-05
Iter: 531 loss: 2.57912543e-05
Iter: 532 loss: 2.57566389e-05
Iter: 533 loss: 2.58042692e-05
Iter: 534 loss: 2.57393731e-05
Iter: 535 loss: 2.57036281e-05
Iter: 536 loss: 2.57817428e-05
Iter: 537 loss: 2.56899439e-05
Iter: 538 loss: 2.56525163e-05
Iter: 539 loss: 2.59204207e-05
Iter: 540 loss: 2.5649264e-05
Iter: 541 loss: 2.56219155e-05
Iter: 542 loss: 2.56458407e-05
Iter: 543 loss: 2.56058174e-05
Iter: 544 loss: 2.55708728e-05
Iter: 545 loss: 2.56081803e-05
Iter: 546 loss: 2.5551697e-05
Iter: 547 loss: 2.551336e-05
Iter: 548 loss: 2.55180421e-05
Iter: 549 loss: 2.54840706e-05
Iter: 550 loss: 2.54377e-05
Iter: 551 loss: 2.55810701e-05
Iter: 552 loss: 2.54240804e-05
Iter: 553 loss: 2.53874787e-05
Iter: 554 loss: 2.59266781e-05
Iter: 555 loss: 2.5387435e-05
Iter: 556 loss: 2.5354675e-05
Iter: 557 loss: 2.54211e-05
Iter: 558 loss: 2.534126e-05
Iter: 559 loss: 2.53131675e-05
Iter: 560 loss: 2.52763239e-05
Iter: 561 loss: 2.52740065e-05
Iter: 562 loss: 2.52320388e-05
Iter: 563 loss: 2.55858922e-05
Iter: 564 loss: 2.52295922e-05
Iter: 565 loss: 2.51929305e-05
Iter: 566 loss: 2.54689694e-05
Iter: 567 loss: 2.51900146e-05
Iter: 568 loss: 2.51661259e-05
Iter: 569 loss: 2.51269848e-05
Iter: 570 loss: 2.51267702e-05
Iter: 571 loss: 2.5087953e-05
Iter: 572 loss: 2.54053393e-05
Iter: 573 loss: 2.50854864e-05
Iter: 574 loss: 2.50493431e-05
Iter: 575 loss: 2.52414957e-05
Iter: 576 loss: 2.50438461e-05
Iter: 577 loss: 2.50148532e-05
Iter: 578 loss: 2.49889672e-05
Iter: 579 loss: 2.49815639e-05
Iter: 580 loss: 2.4945326e-05
Iter: 581 loss: 2.54032293e-05
Iter: 582 loss: 2.49449877e-05
Iter: 583 loss: 2.49162513e-05
Iter: 584 loss: 2.49163386e-05
Iter: 585 loss: 2.48931865e-05
Iter: 586 loss: 2.4857145e-05
Iter: 587 loss: 2.5350073e-05
Iter: 588 loss: 2.48570232e-05
Iter: 589 loss: 2.48357337e-05
Iter: 590 loss: 2.47968565e-05
Iter: 591 loss: 2.57073334e-05
Iter: 592 loss: 2.47967619e-05
Iter: 593 loss: 2.47547905e-05
Iter: 594 loss: 2.48320084e-05
Iter: 595 loss: 2.47367134e-05
Iter: 596 loss: 2.46918316e-05
Iter: 597 loss: 2.49682125e-05
Iter: 598 loss: 2.46864402e-05
Iter: 599 loss: 2.46508389e-05
Iter: 600 loss: 2.50894154e-05
Iter: 601 loss: 2.46504133e-05
Iter: 602 loss: 2.4626981e-05
Iter: 603 loss: 2.45862211e-05
Iter: 604 loss: 2.45862066e-05
Iter: 605 loss: 2.45481133e-05
Iter: 606 loss: 2.48088836e-05
Iter: 607 loss: 2.45444244e-05
Iter: 608 loss: 2.45131687e-05
Iter: 609 loss: 2.48174583e-05
Iter: 610 loss: 2.45120682e-05
Iter: 611 loss: 2.44930743e-05
Iter: 612 loss: 2.44580187e-05
Iter: 613 loss: 2.52649e-05
Iter: 614 loss: 2.44580151e-05
Iter: 615 loss: 2.44213479e-05
Iter: 616 loss: 2.47336393e-05
Iter: 617 loss: 2.44193652e-05
Iter: 618 loss: 2.43855084e-05
Iter: 619 loss: 2.45199808e-05
Iter: 620 loss: 2.4377845e-05
Iter: 621 loss: 2.43467584e-05
Iter: 622 loss: 2.43338527e-05
Iter: 623 loss: 2.43175855e-05
Iter: 624 loss: 2.4277906e-05
Iter: 625 loss: 2.45588199e-05
Iter: 626 loss: 2.42743481e-05
Iter: 627 loss: 2.42404676e-05
Iter: 628 loss: 2.42757596e-05
Iter: 629 loss: 2.42217175e-05
Iter: 630 loss: 2.41886883e-05
Iter: 631 loss: 2.4268691e-05
Iter: 632 loss: 2.41768357e-05
Iter: 633 loss: 2.41524067e-05
Iter: 634 loss: 2.4151821e-05
Iter: 635 loss: 2.41343805e-05
Iter: 636 loss: 2.40953814e-05
Iter: 637 loss: 2.46355594e-05
Iter: 638 loss: 2.40932641e-05
Iter: 639 loss: 2.40510235e-05
Iter: 640 loss: 2.4218818e-05
Iter: 641 loss: 2.40413683e-05
Iter: 642 loss: 2.40127119e-05
Iter: 643 loss: 2.40124973e-05
Iter: 644 loss: 2.39894762e-05
Iter: 645 loss: 2.39645087e-05
Iter: 646 loss: 2.39606634e-05
Iter: 647 loss: 2.39286201e-05
Iter: 648 loss: 2.40233167e-05
Iter: 649 loss: 2.39188303e-05
Iter: 650 loss: 2.38884822e-05
Iter: 651 loss: 2.42113347e-05
Iter: 652 loss: 2.38877219e-05
Iter: 653 loss: 2.38663342e-05
Iter: 654 loss: 2.38349385e-05
Iter: 655 loss: 2.38341163e-05
Iter: 656 loss: 2.37964668e-05
Iter: 657 loss: 2.39392357e-05
Iter: 658 loss: 2.3787441e-05
Iter: 659 loss: 2.37529021e-05
Iter: 660 loss: 2.41169128e-05
Iter: 661 loss: 2.37519907e-05
Iter: 662 loss: 2.37287386e-05
Iter: 663 loss: 2.37031254e-05
Iter: 664 loss: 2.3699411e-05
Iter: 665 loss: 2.36648557e-05
Iter: 666 loss: 2.39300043e-05
Iter: 667 loss: 2.36623255e-05
Iter: 668 loss: 2.36336546e-05
Iter: 669 loss: 2.36806209e-05
Iter: 670 loss: 2.36204687e-05
Iter: 671 loss: 2.35900116e-05
Iter: 672 loss: 2.35929219e-05
Iter: 673 loss: 2.35665248e-05
Iter: 674 loss: 2.35297139e-05
Iter: 675 loss: 2.38229259e-05
Iter: 676 loss: 2.35272055e-05
Iter: 677 loss: 2.34921972e-05
Iter: 678 loss: 2.37241948e-05
Iter: 679 loss: 2.34885138e-05
Iter: 680 loss: 2.34661766e-05
Iter: 681 loss: 2.3432729e-05
Iter: 682 loss: 2.34319814e-05
Iter: 683 loss: 2.34003528e-05
Iter: 684 loss: 2.36615524e-05
Iter: 685 loss: 2.33983665e-05
Iter: 686 loss: 2.33691135e-05
Iter: 687 loss: 2.35474927e-05
Iter: 688 loss: 2.33655501e-05
Iter: 689 loss: 2.33445207e-05
Iter: 690 loss: 2.33130959e-05
Iter: 691 loss: 2.33123974e-05
Iter: 692 loss: 2.32851253e-05
Iter: 693 loss: 2.32842594e-05
Iter: 694 loss: 2.32618022e-05
Iter: 695 loss: 2.32298589e-05
Iter: 696 loss: 2.32287239e-05
Iter: 697 loss: 2.31945232e-05
Iter: 698 loss: 2.33121318e-05
Iter: 699 loss: 2.31855047e-05
Iter: 700 loss: 2.31516387e-05
Iter: 701 loss: 2.34719737e-05
Iter: 702 loss: 2.31502054e-05
Iter: 703 loss: 2.31295853e-05
Iter: 704 loss: 2.31107697e-05
Iter: 705 loss: 2.3105662e-05
Iter: 706 loss: 2.30696369e-05
Iter: 707 loss: 2.32460206e-05
Iter: 708 loss: 2.3063325e-05
Iter: 709 loss: 2.30334808e-05
Iter: 710 loss: 2.3116294e-05
Iter: 711 loss: 2.30237783e-05
Iter: 712 loss: 2.29935831e-05
Iter: 713 loss: 2.29736324e-05
Iter: 714 loss: 2.29621182e-05
Iter: 715 loss: 2.29447942e-05
Iter: 716 loss: 2.29375983e-05
Iter: 717 loss: 2.29162288e-05
Iter: 718 loss: 2.28887729e-05
Iter: 719 loss: 2.28868794e-05
Iter: 720 loss: 2.28559147e-05
Iter: 721 loss: 2.29051075e-05
Iter: 722 loss: 2.28414901e-05
Iter: 723 loss: 2.28146018e-05
Iter: 724 loss: 2.32113125e-05
Iter: 725 loss: 2.28145218e-05
Iter: 726 loss: 2.27900655e-05
Iter: 727 loss: 2.27831897e-05
Iter: 728 loss: 2.27682394e-05
Iter: 729 loss: 2.27397777e-05
Iter: 730 loss: 2.2787799e-05
Iter: 731 loss: 2.27269502e-05
Iter: 732 loss: 2.26868597e-05
Iter: 733 loss: 2.29064808e-05
Iter: 734 loss: 2.26809443e-05
Iter: 735 loss: 2.26598913e-05
Iter: 736 loss: 2.26348311e-05
Iter: 737 loss: 2.26321754e-05
Iter: 738 loss: 2.25979547e-05
Iter: 739 loss: 2.29420693e-05
Iter: 740 loss: 2.25968524e-05
Iter: 741 loss: 2.25670701e-05
Iter: 742 loss: 2.26414413e-05
Iter: 743 loss: 2.25566837e-05
Iter: 744 loss: 2.25328731e-05
Iter: 745 loss: 2.25382119e-05
Iter: 746 loss: 2.25153635e-05
Iter: 747 loss: 2.24842333e-05
Iter: 748 loss: 2.26049578e-05
Iter: 749 loss: 2.24769556e-05
Iter: 750 loss: 2.24435207e-05
Iter: 751 loss: 2.25312087e-05
Iter: 752 loss: 2.2432283e-05
Iter: 753 loss: 2.24028736e-05
Iter: 754 loss: 2.23962961e-05
Iter: 755 loss: 2.23772076e-05
Iter: 756 loss: 2.23470033e-05
Iter: 757 loss: 2.2346956e-05
Iter: 758 loss: 2.231494e-05
Iter: 759 loss: 2.23305033e-05
Iter: 760 loss: 2.22935269e-05
Iter: 761 loss: 2.22678063e-05
Iter: 762 loss: 2.22665985e-05
Iter: 763 loss: 2.22469807e-05
Iter: 764 loss: 2.2221273e-05
Iter: 765 loss: 2.22209146e-05
Iter: 766 loss: 2.22013987e-05
Iter: 767 loss: 2.21882437e-05
Iter: 768 loss: 2.21809969e-05
Iter: 769 loss: 2.21553273e-05
Iter: 770 loss: 2.22516937e-05
Iter: 771 loss: 2.21490845e-05
Iter: 772 loss: 2.21145219e-05
Iter: 773 loss: 2.21744049e-05
Iter: 774 loss: 2.20992406e-05
Iter: 775 loss: 2.20734692e-05
Iter: 776 loss: 2.2075028e-05
Iter: 777 loss: 2.20532911e-05
Iter: 778 loss: 2.20226375e-05
Iter: 779 loss: 2.23872503e-05
Iter: 780 loss: 2.20221882e-05
Iter: 781 loss: 2.19971334e-05
Iter: 782 loss: 2.20000111e-05
Iter: 783 loss: 2.19778849e-05
Iter: 784 loss: 2.19475387e-05
Iter: 785 loss: 2.19725334e-05
Iter: 786 loss: 2.1929518e-05
Iter: 787 loss: 2.18942896e-05
Iter: 788 loss: 2.21921036e-05
Iter: 789 loss: 2.18922542e-05
Iter: 790 loss: 2.18656733e-05
Iter: 791 loss: 2.18511705e-05
Iter: 792 loss: 2.18392524e-05
Iter: 793 loss: 2.18062632e-05
Iter: 794 loss: 2.20393085e-05
Iter: 795 loss: 2.1803271e-05
Iter: 796 loss: 2.1777194e-05
Iter: 797 loss: 2.20727707e-05
Iter: 798 loss: 2.17767738e-05
Iter: 799 loss: 2.17586385e-05
Iter: 800 loss: 2.17326742e-05
Iter: 801 loss: 2.17319121e-05
Iter: 802 loss: 2.17036977e-05
Iter: 803 loss: 2.18237219e-05
Iter: 804 loss: 2.1697866e-05
Iter: 805 loss: 2.16642748e-05
Iter: 806 loss: 2.18101195e-05
Iter: 807 loss: 2.1657459e-05
Iter: 808 loss: 2.16323133e-05
Iter: 809 loss: 2.16149238e-05
Iter: 810 loss: 2.16058215e-05
Iter: 811 loss: 2.15777436e-05
Iter: 812 loss: 2.15775271e-05
Iter: 813 loss: 2.1556667e-05
Iter: 814 loss: 2.15436667e-05
Iter: 815 loss: 2.15352757e-05
Iter: 816 loss: 2.15106156e-05
Iter: 817 loss: 2.15505916e-05
Iter: 818 loss: 2.14992579e-05
Iter: 819 loss: 2.14665051e-05
Iter: 820 loss: 2.16827448e-05
Iter: 821 loss: 2.14630272e-05
Iter: 822 loss: 2.14377451e-05
Iter: 823 loss: 2.14079228e-05
Iter: 824 loss: 2.14047122e-05
Iter: 825 loss: 2.13666644e-05
Iter: 826 loss: 2.16308053e-05
Iter: 827 loss: 2.13630592e-05
Iter: 828 loss: 2.13358981e-05
Iter: 829 loss: 2.14971697e-05
Iter: 830 loss: 2.13324183e-05
Iter: 831 loss: 2.13087042e-05
Iter: 832 loss: 2.13164913e-05
Iter: 833 loss: 2.12919113e-05
Iter: 834 loss: 2.12699379e-05
Iter: 835 loss: 2.15840646e-05
Iter: 836 loss: 2.12699288e-05
Iter: 837 loss: 2.12472878e-05
Iter: 838 loss: 2.1229831e-05
Iter: 839 loss: 2.12226769e-05
Iter: 840 loss: 2.11963616e-05
Iter: 841 loss: 2.12185532e-05
Iter: 842 loss: 2.11807546e-05
Iter: 843 loss: 2.11563565e-05
Iter: 844 loss: 2.11562692e-05
Iter: 845 loss: 2.11345905e-05
Iter: 846 loss: 2.11069892e-05
Iter: 847 loss: 2.11049919e-05
Iter: 848 loss: 2.10812177e-05
Iter: 849 loss: 2.13529311e-05
Iter: 850 loss: 2.10808939e-05
Iter: 851 loss: 2.10557973e-05
Iter: 852 loss: 2.10644e-05
Iter: 853 loss: 2.10380767e-05
Iter: 854 loss: 2.10111866e-05
Iter: 855 loss: 2.10227954e-05
Iter: 856 loss: 2.09928221e-05
Iter: 857 loss: 2.09666305e-05
Iter: 858 loss: 2.12471241e-05
Iter: 859 loss: 2.09659738e-05
Iter: 860 loss: 2.09404279e-05
Iter: 861 loss: 2.09631289e-05
Iter: 862 loss: 2.09255395e-05
Iter: 863 loss: 2.09005775e-05
Iter: 864 loss: 2.08998536e-05
Iter: 865 loss: 2.08804558e-05
Iter: 866 loss: 2.08470738e-05
Iter: 867 loss: 2.09527807e-05
Iter: 868 loss: 2.08375059e-05
Iter: 869 loss: 2.08091933e-05
Iter: 870 loss: 2.11215356e-05
Iter: 871 loss: 2.08086058e-05
Iter: 872 loss: 2.07856865e-05
Iter: 873 loss: 2.08045021e-05
Iter: 874 loss: 2.0771944e-05
Iter: 875 loss: 2.07457888e-05
Iter: 876 loss: 2.1003576e-05
Iter: 877 loss: 2.07448138e-05
Iter: 878 loss: 2.07273624e-05
Iter: 879 loss: 2.06982641e-05
Iter: 880 loss: 2.06981094e-05
Iter: 881 loss: 2.06678014e-05
Iter: 882 loss: 2.08674228e-05
Iter: 883 loss: 2.06646218e-05
Iter: 884 loss: 2.06382392e-05
Iter: 885 loss: 2.0839947e-05
Iter: 886 loss: 2.0636302e-05
Iter: 887 loss: 2.06173063e-05
Iter: 888 loss: 2.05996021e-05
Iter: 889 loss: 2.05950782e-05
Iter: 890 loss: 2.05729648e-05
Iter: 891 loss: 2.0900432e-05
Iter: 892 loss: 2.05728884e-05
Iter: 893 loss: 2.05523793e-05
Iter: 894 loss: 2.05388915e-05
Iter: 895 loss: 2.05310971e-05
Iter: 896 loss: 2.05039341e-05
Iter: 897 loss: 2.05201577e-05
Iter: 898 loss: 2.04863991e-05
Iter: 899 loss: 2.04591634e-05
Iter: 900 loss: 2.08789788e-05
Iter: 901 loss: 2.04591561e-05
Iter: 902 loss: 2.04362968e-05
Iter: 903 loss: 2.04328062e-05
Iter: 904 loss: 2.04168464e-05
Iter: 905 loss: 2.03892487e-05
Iter: 906 loss: 2.03889758e-05
Iter: 907 loss: 2.03671734e-05
Iter: 908 loss: 2.03362033e-05
Iter: 909 loss: 2.06251207e-05
Iter: 910 loss: 2.03349136e-05
Iter: 911 loss: 2.0307536e-05
Iter: 912 loss: 2.04740518e-05
Iter: 913 loss: 2.03041891e-05
Iter: 914 loss: 2.02850333e-05
Iter: 915 loss: 2.03448253e-05
Iter: 916 loss: 2.02794563e-05
Iter: 917 loss: 2.02573046e-05
Iter: 918 loss: 2.02469346e-05
Iter: 919 loss: 2.0236168e-05
Iter: 920 loss: 2.02107149e-05
Iter: 921 loss: 2.02838837e-05
Iter: 922 loss: 2.02026513e-05
Iter: 923 loss: 2.01802e-05
Iter: 924 loss: 2.04653679e-05
Iter: 925 loss: 2.01800867e-05
Iter: 926 loss: 2.01632083e-05
Iter: 927 loss: 2.01446092e-05
Iter: 928 loss: 2.01418734e-05
Iter: 929 loss: 2.0116102e-05
Iter: 930 loss: 2.02689462e-05
Iter: 931 loss: 2.0112826e-05
Iter: 932 loss: 2.00837203e-05
Iter: 933 loss: 2.01308867e-05
Iter: 934 loss: 2.00703143e-05
Iter: 935 loss: 2.00478698e-05
Iter: 936 loss: 2.00448158e-05
Iter: 937 loss: 2.00289578e-05
Iter: 938 loss: 2.00073409e-05
Iter: 939 loss: 2.03267555e-05
Iter: 940 loss: 2.00073e-05
Iter: 941 loss: 1.99869937e-05
Iter: 942 loss: 2.00012255e-05
Iter: 943 loss: 1.99744882e-05
Iter: 944 loss: 1.99527713e-05
Iter: 945 loss: 1.99461865e-05
Iter: 946 loss: 1.99332771e-05
Iter: 947 loss: 1.99009392e-05
Iter: 948 loss: 1.99699462e-05
Iter: 949 loss: 1.98881644e-05
Iter: 950 loss: 1.98692906e-05
Iter: 951 loss: 1.98663674e-05
Iter: 952 loss: 1.98521975e-05
Iter: 953 loss: 1.98419748e-05
Iter: 954 loss: 1.98370071e-05
Iter: 955 loss: 1.9810901e-05
Iter: 956 loss: 1.9888e-05
Iter: 957 loss: 1.98029429e-05
Iter: 958 loss: 1.97812587e-05
Iter: 959 loss: 1.97837853e-05
Iter: 960 loss: 1.97646459e-05
Iter: 961 loss: 1.97435293e-05
Iter: 962 loss: 1.97435438e-05
Iter: 963 loss: 1.97249847e-05
Iter: 964 loss: 1.97096597e-05
Iter: 965 loss: 1.97042209e-05
Iter: 966 loss: 1.96786223e-05
Iter: 967 loss: 1.97589179e-05
Iter: 968 loss: 1.96711408e-05
Iter: 969 loss: 1.96464953e-05
Iter: 970 loss: 1.98810303e-05
Iter: 971 loss: 1.96454821e-05
Iter: 972 loss: 1.96304154e-05
Iter: 973 loss: 1.9603136e-05
Iter: 974 loss: 2.02586325e-05
Iter: 975 loss: 1.96031324e-05
Iter: 976 loss: 1.95780667e-05
Iter: 977 loss: 1.98141497e-05
Iter: 978 loss: 1.9576999e-05
Iter: 979 loss: 1.95533794e-05
Iter: 980 loss: 1.96550864e-05
Iter: 981 loss: 1.95485445e-05
Iter: 982 loss: 1.95289049e-05
Iter: 983 loss: 1.95141365e-05
Iter: 984 loss: 1.95076427e-05
Iter: 985 loss: 1.94768782e-05
Iter: 986 loss: 1.95368593e-05
Iter: 987 loss: 1.94640561e-05
Iter: 988 loss: 1.94355252e-05
Iter: 989 loss: 1.95358298e-05
Iter: 990 loss: 1.94281256e-05
Iter: 991 loss: 1.94073546e-05
Iter: 992 loss: 1.94073637e-05
Iter: 993 loss: 1.93896285e-05
Iter: 994 loss: 1.93930864e-05
Iter: 995 loss: 1.93763917e-05
Iter: 996 loss: 1.93525921e-05
Iter: 997 loss: 1.95153e-05
Iter: 998 loss: 1.93503438e-05
Iter: 999 loss: 1.93341293e-05
Iter: 1000 loss: 1.93127944e-05
Iter: 1001 loss: 1.93115211e-05
Iter: 1002 loss: 1.92867e-05
Iter: 1003 loss: 1.95889334e-05
Iter: 1004 loss: 1.92864645e-05
Iter: 1005 loss: 1.92652697e-05
Iter: 1006 loss: 1.93047417e-05
Iter: 1007 loss: 1.92562638e-05
Iter: 1008 loss: 1.92390398e-05
Iter: 1009 loss: 1.92470434e-05
Iter: 1010 loss: 1.92273583e-05
Iter: 1011 loss: 1.92052248e-05
Iter: 1012 loss: 1.94281038e-05
Iter: 1013 loss: 1.92045245e-05
Iter: 1014 loss: 1.91887502e-05
Iter: 1015 loss: 1.91652889e-05
Iter: 1016 loss: 1.9164685e-05
Iter: 1017 loss: 1.91362233e-05
Iter: 1018 loss: 1.92216066e-05
Iter: 1019 loss: 1.91276358e-05
Iter: 1020 loss: 1.91035251e-05
Iter: 1021 loss: 1.94664153e-05
Iter: 1022 loss: 1.91035906e-05
Iter: 1023 loss: 1.90878072e-05
Iter: 1024 loss: 1.90684477e-05
Iter: 1025 loss: 1.90666979e-05
Iter: 1026 loss: 1.90415121e-05
Iter: 1027 loss: 1.90642531e-05
Iter: 1028 loss: 1.90268729e-05
Iter: 1029 loss: 1.89974708e-05
Iter: 1030 loss: 1.91296676e-05
Iter: 1031 loss: 1.89917882e-05
Iter: 1032 loss: 1.89687253e-05
Iter: 1033 loss: 1.92483312e-05
Iter: 1034 loss: 1.8968447e-05
Iter: 1035 loss: 1.89495258e-05
Iter: 1036 loss: 1.89886068e-05
Iter: 1037 loss: 1.8941977e-05
Iter: 1038 loss: 1.89241036e-05
Iter: 1039 loss: 1.90218616e-05
Iter: 1040 loss: 1.89215098e-05
Iter: 1041 loss: 1.89053135e-05
Iter: 1042 loss: 1.88908816e-05
Iter: 1043 loss: 1.88867361e-05
Iter: 1044 loss: 1.8867202e-05
Iter: 1045 loss: 1.90571573e-05
Iter: 1046 loss: 1.88664671e-05
Iter: 1047 loss: 1.88473659e-05
Iter: 1048 loss: 1.88727281e-05
Iter: 1049 loss: 1.88377562e-05
Iter: 1050 loss: 1.88184968e-05
Iter: 1051 loss: 1.88266022e-05
Iter: 1052 loss: 1.88052909e-05
Iter: 1053 loss: 1.87843198e-05
Iter: 1054 loss: 1.90777282e-05
Iter: 1055 loss: 1.87842925e-05
Iter: 1056 loss: 1.87696223e-05
Iter: 1057 loss: 1.8748533e-05
Iter: 1058 loss: 1.87478818e-05
Iter: 1059 loss: 1.87248697e-05
Iter: 1060 loss: 1.88005361e-05
Iter: 1061 loss: 1.87185105e-05
Iter: 1062 loss: 1.86956167e-05
Iter: 1063 loss: 1.89295024e-05
Iter: 1064 loss: 1.86949765e-05
Iter: 1065 loss: 1.86797715e-05
Iter: 1066 loss: 1.86581819e-05
Iter: 1067 loss: 1.86574489e-05
Iter: 1068 loss: 1.8627552e-05
Iter: 1069 loss: 1.86955058e-05
Iter: 1070 loss: 1.86163597e-05
Iter: 1071 loss: 1.85906338e-05
Iter: 1072 loss: 1.86929101e-05
Iter: 1073 loss: 1.85847657e-05
Iter: 1074 loss: 1.85662539e-05
Iter: 1075 loss: 1.88551439e-05
Iter: 1076 loss: 1.85662029e-05
Iter: 1077 loss: 1.85511562e-05
Iter: 1078 loss: 1.85627941e-05
Iter: 1079 loss: 1.85419412e-05
Iter: 1080 loss: 1.85236113e-05
Iter: 1081 loss: 1.86202469e-05
Iter: 1082 loss: 1.85207318e-05
Iter: 1083 loss: 1.8504199e-05
Iter: 1084 loss: 1.84886485e-05
Iter: 1085 loss: 1.84848395e-05
Iter: 1086 loss: 1.84618348e-05
Iter: 1087 loss: 1.86752204e-05
Iter: 1088 loss: 1.84608307e-05
Iter: 1089 loss: 1.84401433e-05
Iter: 1090 loss: 1.84852433e-05
Iter: 1091 loss: 1.84322489e-05
Iter: 1092 loss: 1.84180426e-05
Iter: 1093 loss: 1.84283126e-05
Iter: 1094 loss: 1.84092132e-05
Iter: 1095 loss: 1.83891607e-05
Iter: 1096 loss: 1.853699e-05
Iter: 1097 loss: 1.83875127e-05
Iter: 1098 loss: 1.83741358e-05
Iter: 1099 loss: 1.83504708e-05
Iter: 1100 loss: 1.83504453e-05
Iter: 1101 loss: 1.83270477e-05
Iter: 1102 loss: 1.85346107e-05
Iter: 1103 loss: 1.83259726e-05
Iter: 1104 loss: 1.83025968e-05
Iter: 1105 loss: 1.83823176e-05
Iter: 1106 loss: 1.8296374e-05
Iter: 1107 loss: 1.82785298e-05
Iter: 1108 loss: 1.82672266e-05
Iter: 1109 loss: 1.82602471e-05
Iter: 1110 loss: 1.82330896e-05
Iter: 1111 loss: 1.83008124e-05
Iter: 1112 loss: 1.82236145e-05
Iter: 1113 loss: 1.82015683e-05
Iter: 1114 loss: 1.82743879e-05
Iter: 1115 loss: 1.81954929e-05
Iter: 1116 loss: 1.81766754e-05
Iter: 1117 loss: 1.84581877e-05
Iter: 1118 loss: 1.81766663e-05
Iter: 1119 loss: 1.81615651e-05
Iter: 1120 loss: 1.81745836e-05
Iter: 1121 loss: 1.81526648e-05
Iter: 1122 loss: 1.81351024e-05
Iter: 1123 loss: 1.8230432e-05
Iter: 1124 loss: 1.81325267e-05
Iter: 1125 loss: 1.81182495e-05
Iter: 1126 loss: 1.80984243e-05
Iter: 1127 loss: 1.80975112e-05
Iter: 1128 loss: 1.80762436e-05
Iter: 1129 loss: 1.82833046e-05
Iter: 1130 loss: 1.80755051e-05
Iter: 1131 loss: 1.80581501e-05
Iter: 1132 loss: 1.81695468e-05
Iter: 1133 loss: 1.80561947e-05
Iter: 1134 loss: 1.80432344e-05
Iter: 1135 loss: 1.80216557e-05
Iter: 1136 loss: 1.80216048e-05
Iter: 1137 loss: 1.80060069e-05
Iter: 1138 loss: 1.80055467e-05
Iter: 1139 loss: 1.79908493e-05
Iter: 1140 loss: 1.79780909e-05
Iter: 1141 loss: 1.79740964e-05
Iter: 1142 loss: 1.79533599e-05
Iter: 1143 loss: 1.79819363e-05
Iter: 1144 loss: 1.79430899e-05
Iter: 1145 loss: 1.79229337e-05
Iter: 1146 loss: 1.82018193e-05
Iter: 1147 loss: 1.79228628e-05
Iter: 1148 loss: 1.7907354e-05
Iter: 1149 loss: 1.79093167e-05
Iter: 1150 loss: 1.78956143e-05
Iter: 1151 loss: 1.78775135e-05
Iter: 1152 loss: 1.78638147e-05
Iter: 1153 loss: 1.78578593e-05
Iter: 1154 loss: 1.78330447e-05
Iter: 1155 loss: 1.80372772e-05
Iter: 1156 loss: 1.78315095e-05
Iter: 1157 loss: 1.78146e-05
Iter: 1158 loss: 1.80501756e-05
Iter: 1159 loss: 1.78145274e-05
Iter: 1160 loss: 1.78003e-05
Iter: 1161 loss: 1.77946695e-05
Iter: 1162 loss: 1.77869642e-05
Iter: 1163 loss: 1.77697239e-05
Iter: 1164 loss: 1.78829578e-05
Iter: 1165 loss: 1.7767934e-05
Iter: 1166 loss: 1.77525835e-05
Iter: 1167 loss: 1.77421462e-05
Iter: 1168 loss: 1.7736571e-05
Iter: 1169 loss: 1.77162765e-05
Iter: 1170 loss: 1.77900056e-05
Iter: 1171 loss: 1.77111342e-05
Iter: 1172 loss: 1.76888443e-05
Iter: 1173 loss: 1.7868324e-05
Iter: 1174 loss: 1.76873127e-05
Iter: 1175 loss: 1.76736794e-05
Iter: 1176 loss: 1.76572885e-05
Iter: 1177 loss: 1.76556587e-05
Iter: 1178 loss: 1.76335343e-05
Iter: 1179 loss: 1.76921476e-05
Iter: 1180 loss: 1.76261055e-05
Iter: 1181 loss: 1.76086869e-05
Iter: 1182 loss: 1.76086705e-05
Iter: 1183 loss: 1.75953137e-05
Iter: 1184 loss: 1.75770383e-05
Iter: 1185 loss: 1.75761761e-05
Iter: 1186 loss: 1.75516325e-05
Iter: 1187 loss: 1.75851728e-05
Iter: 1188 loss: 1.75394161e-05
Iter: 1189 loss: 1.75216828e-05
Iter: 1190 loss: 1.77949896e-05
Iter: 1191 loss: 1.75216792e-05
Iter: 1192 loss: 1.75036512e-05
Iter: 1193 loss: 1.75328441e-05
Iter: 1194 loss: 1.7495342e-05
Iter: 1195 loss: 1.74809211e-05
Iter: 1196 loss: 1.75082623e-05
Iter: 1197 loss: 1.74748748e-05
Iter: 1198 loss: 1.74567e-05
Iter: 1199 loss: 1.75597506e-05
Iter: 1200 loss: 1.7454111e-05
Iter: 1201 loss: 1.74409e-05
Iter: 1202 loss: 1.7430244e-05
Iter: 1203 loss: 1.7426235e-05
Iter: 1204 loss: 1.74075012e-05
Iter: 1205 loss: 1.76025205e-05
Iter: 1206 loss: 1.74069701e-05
Iter: 1207 loss: 1.73939843e-05
Iter: 1208 loss: 1.73798071e-05
Iter: 1209 loss: 1.73776534e-05
Iter: 1210 loss: 1.73598783e-05
Iter: 1211 loss: 1.75407258e-05
Iter: 1212 loss: 1.73592653e-05
Iter: 1213 loss: 1.73416665e-05
Iter: 1214 loss: 1.73926492e-05
Iter: 1215 loss: 1.73362223e-05
Iter: 1216 loss: 1.73226072e-05
Iter: 1217 loss: 1.73081571e-05
Iter: 1218 loss: 1.73057615e-05
Iter: 1219 loss: 1.7282091e-05
Iter: 1220 loss: 1.73052395e-05
Iter: 1221 loss: 1.72687051e-05
Iter: 1222 loss: 1.72471809e-05
Iter: 1223 loss: 1.75746609e-05
Iter: 1224 loss: 1.72471664e-05
Iter: 1225 loss: 1.72286072e-05
Iter: 1226 loss: 1.72861346e-05
Iter: 1227 loss: 1.72231557e-05
Iter: 1228 loss: 1.7206763e-05
Iter: 1229 loss: 1.72013897e-05
Iter: 1230 loss: 1.71919273e-05
Iter: 1231 loss: 1.71750726e-05
Iter: 1232 loss: 1.73295921e-05
Iter: 1233 loss: 1.71742649e-05
Iter: 1234 loss: 1.7155302e-05
Iter: 1235 loss: 1.71956872e-05
Iter: 1236 loss: 1.71479187e-05
Iter: 1237 loss: 1.71344163e-05
Iter: 1238 loss: 1.71257489e-05
Iter: 1239 loss: 1.7120492e-05
Iter: 1240 loss: 1.70998537e-05
Iter: 1241 loss: 1.73084263e-05
Iter: 1242 loss: 1.70991771e-05
Iter: 1243 loss: 1.70852709e-05
Iter: 1244 loss: 1.70865169e-05
Iter: 1245 loss: 1.70745134e-05
Iter: 1246 loss: 1.70579642e-05
Iter: 1247 loss: 1.71998727e-05
Iter: 1248 loss: 1.70570838e-05
Iter: 1249 loss: 1.70420226e-05
Iter: 1250 loss: 1.70587264e-05
Iter: 1251 loss: 1.70339699e-05
Iter: 1252 loss: 1.70203039e-05
Iter: 1253 loss: 1.70418789e-05
Iter: 1254 loss: 1.70139065e-05
Iter: 1255 loss: 1.69969262e-05
Iter: 1256 loss: 1.71106476e-05
Iter: 1257 loss: 1.69952255e-05
Iter: 1258 loss: 1.69830928e-05
Iter: 1259 loss: 1.69623127e-05
Iter: 1260 loss: 1.69622945e-05
Iter: 1261 loss: 1.69372106e-05
Iter: 1262 loss: 1.69925406e-05
Iter: 1263 loss: 1.69275518e-05
Iter: 1264 loss: 1.69034447e-05
Iter: 1265 loss: 1.70014573e-05
Iter: 1266 loss: 1.68981405e-05
Iter: 1267 loss: 1.68863498e-05
Iter: 1268 loss: 1.68855368e-05
Iter: 1269 loss: 1.68734823e-05
Iter: 1270 loss: 1.68638562e-05
Iter: 1271 loss: 1.68602583e-05
Iter: 1272 loss: 1.68458209e-05
Iter: 1273 loss: 1.69822561e-05
Iter: 1274 loss: 1.68452825e-05
Iter: 1275 loss: 1.68313836e-05
Iter: 1276 loss: 1.68210536e-05
Iter: 1277 loss: 1.68164897e-05
Iter: 1278 loss: 1.6798569e-05
Iter: 1279 loss: 1.68396637e-05
Iter: 1280 loss: 1.67918188e-05
Iter: 1281 loss: 1.67754151e-05
Iter: 1282 loss: 1.69386767e-05
Iter: 1283 loss: 1.6774884e-05
Iter: 1284 loss: 1.67619837e-05
Iter: 1285 loss: 1.67610215e-05
Iter: 1286 loss: 1.67513517e-05
Iter: 1287 loss: 1.67366816e-05
Iter: 1288 loss: 1.68836486e-05
Iter: 1289 loss: 1.67362414e-05
Iter: 1290 loss: 1.67214639e-05
Iter: 1291 loss: 1.67100716e-05
Iter: 1292 loss: 1.67054131e-05
Iter: 1293 loss: 1.66878308e-05
Iter: 1294 loss: 1.66970822e-05
Iter: 1295 loss: 1.66762748e-05
Iter: 1296 loss: 1.66567588e-05
Iter: 1297 loss: 1.68068727e-05
Iter: 1298 loss: 1.66553691e-05
Iter: 1299 loss: 1.6640015e-05
Iter: 1300 loss: 1.67821017e-05
Iter: 1301 loss: 1.66394148e-05
Iter: 1302 loss: 1.66285772e-05
Iter: 1303 loss: 1.66132977e-05
Iter: 1304 loss: 1.66127575e-05
Iter: 1305 loss: 1.65939036e-05
Iter: 1306 loss: 1.66560858e-05
Iter: 1307 loss: 1.65887577e-05
Iter: 1308 loss: 1.65723013e-05
Iter: 1309 loss: 1.65722831e-05
Iter: 1310 loss: 1.65622751e-05
Iter: 1311 loss: 1.65474339e-05
Iter: 1312 loss: 1.6547101e-05
Iter: 1313 loss: 1.6531465e-05
Iter: 1314 loss: 1.66067039e-05
Iter: 1315 loss: 1.65286765e-05
Iter: 1316 loss: 1.65141282e-05
Iter: 1317 loss: 1.66097507e-05
Iter: 1318 loss: 1.65125894e-05
Iter: 1319 loss: 1.65011115e-05
Iter: 1320 loss: 1.64892954e-05
Iter: 1321 loss: 1.64871035e-05
Iter: 1322 loss: 1.64707781e-05
Iter: 1323 loss: 1.67078942e-05
Iter: 1324 loss: 1.64707453e-05
Iter: 1325 loss: 1.64581033e-05
Iter: 1326 loss: 1.64593821e-05
Iter: 1327 loss: 1.64483936e-05
Iter: 1328 loss: 1.6434893e-05
Iter: 1329 loss: 1.65483689e-05
Iter: 1330 loss: 1.64341436e-05
Iter: 1331 loss: 1.64217417e-05
Iter: 1332 loss: 1.64153134e-05
Iter: 1333 loss: 1.64095509e-05
Iter: 1334 loss: 1.63936893e-05
Iter: 1335 loss: 1.6402455e-05
Iter: 1336 loss: 1.63832592e-05
Iter: 1337 loss: 1.63634268e-05
Iter: 1338 loss: 1.64023277e-05
Iter: 1339 loss: 1.63551722e-05
Iter: 1340 loss: 1.63389741e-05
Iter: 1341 loss: 1.65554411e-05
Iter: 1342 loss: 1.63388831e-05
Iter: 1343 loss: 1.63231743e-05
Iter: 1344 loss: 1.63773475e-05
Iter: 1345 loss: 1.63189907e-05
Iter: 1346 loss: 1.63078148e-05
Iter: 1347 loss: 1.63069253e-05
Iter: 1348 loss: 1.62985634e-05
Iter: 1349 loss: 1.62821234e-05
Iter: 1350 loss: 1.64151425e-05
Iter: 1351 loss: 1.6281032e-05
Iter: 1352 loss: 1.62689503e-05
Iter: 1353 loss: 1.62554734e-05
Iter: 1354 loss: 1.6253729e-05
Iter: 1355 loss: 1.62360393e-05
Iter: 1356 loss: 1.62947872e-05
Iter: 1357 loss: 1.62312226e-05
Iter: 1358 loss: 1.62144825e-05
Iter: 1359 loss: 1.63796904e-05
Iter: 1360 loss: 1.6213884e-05
Iter: 1361 loss: 1.62022061e-05
Iter: 1362 loss: 1.62014e-05
Iter: 1363 loss: 1.61925345e-05
Iter: 1364 loss: 1.6180813e-05
Iter: 1365 loss: 1.61808312e-05
Iter: 1366 loss: 1.61716889e-05
Iter: 1367 loss: 1.6158263e-05
Iter: 1368 loss: 1.61578864e-05
Iter: 1369 loss: 1.61432545e-05
Iter: 1370 loss: 1.62566248e-05
Iter: 1371 loss: 1.61422358e-05
Iter: 1372 loss: 1.6127422e-05
Iter: 1373 loss: 1.61447169e-05
Iter: 1374 loss: 1.61194475e-05
Iter: 1375 loss: 1.61041571e-05
Iter: 1376 loss: 1.6096632e-05
Iter: 1377 loss: 1.60892851e-05
Iter: 1378 loss: 1.60702439e-05
Iter: 1379 loss: 1.61665284e-05
Iter: 1380 loss: 1.60670661e-05
Iter: 1381 loss: 1.60561976e-05
Iter: 1382 loss: 1.60555937e-05
Iter: 1383 loss: 1.60466316e-05
Iter: 1384 loss: 1.60308664e-05
Iter: 1385 loss: 1.60309028e-05
Iter: 1386 loss: 1.60136e-05
Iter: 1387 loss: 1.60329328e-05
Iter: 1388 loss: 1.60042928e-05
Iter: 1389 loss: 1.59876217e-05
Iter: 1390 loss: 1.61411335e-05
Iter: 1391 loss: 1.59868396e-05
Iter: 1392 loss: 1.59723459e-05
Iter: 1393 loss: 1.60439158e-05
Iter: 1394 loss: 1.59698466e-05
Iter: 1395 loss: 1.5957894e-05
Iter: 1396 loss: 1.59616648e-05
Iter: 1397 loss: 1.59493939e-05
Iter: 1398 loss: 1.59354531e-05
Iter: 1399 loss: 1.60728196e-05
Iter: 1400 loss: 1.5934962e-05
Iter: 1401 loss: 1.59237752e-05
Iter: 1402 loss: 1.59097726e-05
Iter: 1403 loss: 1.5908543e-05
Iter: 1404 loss: 1.58944349e-05
Iter: 1405 loss: 1.58943367e-05
Iter: 1406 loss: 1.58827352e-05
Iter: 1407 loss: 1.58688272e-05
Iter: 1408 loss: 1.5867352e-05
Iter: 1409 loss: 1.58508446e-05
Iter: 1410 loss: 1.58853491e-05
Iter: 1411 loss: 1.58442163e-05
Iter: 1412 loss: 1.58289258e-05
Iter: 1413 loss: 1.6059992e-05
Iter: 1414 loss: 1.58289258e-05
Iter: 1415 loss: 1.58179428e-05
Iter: 1416 loss: 1.58065795e-05
Iter: 1417 loss: 1.58044913e-05
Iter: 1418 loss: 1.57901486e-05
Iter: 1419 loss: 1.59410447e-05
Iter: 1420 loss: 1.57897703e-05
Iter: 1421 loss: 1.5776146e-05
Iter: 1422 loss: 1.58082803e-05
Iter: 1423 loss: 1.57711293e-05
Iter: 1424 loss: 1.57601935e-05
Iter: 1425 loss: 1.57441118e-05
Iter: 1426 loss: 1.57436934e-05
Iter: 1427 loss: 1.57250724e-05
Iter: 1428 loss: 1.57909126e-05
Iter: 1429 loss: 1.57202248e-05
Iter: 1430 loss: 1.57052455e-05
Iter: 1431 loss: 1.59218835e-05
Iter: 1432 loss: 1.57052491e-05
Iter: 1433 loss: 1.56912301e-05
Iter: 1434 loss: 1.5703481e-05
Iter: 1435 loss: 1.56830247e-05
Iter: 1436 loss: 1.56701572e-05
Iter: 1437 loss: 1.56790265e-05
Iter: 1438 loss: 1.56620536e-05
Iter: 1439 loss: 1.564575e-05
Iter: 1440 loss: 1.58120201e-05
Iter: 1441 loss: 1.56452861e-05
Iter: 1442 loss: 1.56354799e-05
Iter: 1443 loss: 1.56179376e-05
Iter: 1444 loss: 1.60452255e-05
Iter: 1445 loss: 1.56179176e-05
Iter: 1446 loss: 1.56013612e-05
Iter: 1447 loss: 1.57496834e-05
Iter: 1448 loss: 1.56005663e-05
Iter: 1449 loss: 1.55855196e-05
Iter: 1450 loss: 1.5669e-05
Iter: 1451 loss: 1.55833677e-05
Iter: 1452 loss: 1.55724792e-05
Iter: 1453 loss: 1.55672387e-05
Iter: 1454 loss: 1.55620401e-05
Iter: 1455 loss: 1.55510534e-05
Iter: 1456 loss: 1.55510061e-05
Iter: 1457 loss: 1.55413327e-05
Iter: 1458 loss: 1.5528216e-05
Iter: 1459 loss: 1.55276248e-05
Iter: 1460 loss: 1.55119269e-05
Iter: 1461 loss: 1.55439539e-05
Iter: 1462 loss: 1.55055986e-05
Iter: 1463 loss: 1.54910795e-05
Iter: 1464 loss: 1.56593887e-05
Iter: 1465 loss: 1.54908194e-05
Iter: 1466 loss: 1.54800127e-05
Iter: 1467 loss: 1.54663121e-05
Iter: 1468 loss: 1.54652153e-05
Iter: 1469 loss: 1.54498975e-05
Iter: 1470 loss: 1.56086408e-05
Iter: 1471 loss: 1.54494792e-05
Iter: 1472 loss: 1.54371864e-05
Iter: 1473 loss: 1.55159869e-05
Iter: 1474 loss: 1.54358058e-05
Iter: 1475 loss: 1.54271256e-05
Iter: 1476 loss: 1.54127501e-05
Iter: 1477 loss: 1.54126719e-05
Iter: 1478 loss: 1.53989586e-05
Iter: 1479 loss: 1.55697016e-05
Iter: 1480 loss: 1.53988258e-05
Iter: 1481 loss: 1.53843757e-05
Iter: 1482 loss: 1.53877736e-05
Iter: 1483 loss: 1.53738492e-05
Iter: 1484 loss: 1.5359512e-05
Iter: 1485 loss: 1.53623769e-05
Iter: 1486 loss: 1.53488472e-05
Iter: 1487 loss: 1.53349974e-05
Iter: 1488 loss: 1.5553549e-05
Iter: 1489 loss: 1.5335012e-05
Iter: 1490 loss: 1.53222973e-05
Iter: 1491 loss: 1.53447891e-05
Iter: 1492 loss: 1.53167221e-05
Iter: 1493 loss: 1.53052151e-05
Iter: 1494 loss: 1.53256078e-05
Iter: 1495 loss: 1.53001456e-05
Iter: 1496 loss: 1.52854973e-05
Iter: 1497 loss: 1.53417695e-05
Iter: 1498 loss: 1.52820612e-05
Iter: 1499 loss: 1.52715693e-05
Iter: 1500 loss: 1.52591274e-05
Iter: 1501 loss: 1.52578332e-05
Iter: 1502 loss: 1.52398534e-05
Iter: 1503 loss: 1.52827033e-05
Iter: 1504 loss: 1.52333232e-05
Iter: 1505 loss: 1.5221307e-05
Iter: 1506 loss: 1.52210023e-05
Iter: 1507 loss: 1.52088696e-05
Iter: 1508 loss: 1.5205218e-05
Iter: 1509 loss: 1.51980948e-05
Iter: 1510 loss: 1.51855565e-05
Iter: 1511 loss: 1.52252514e-05
Iter: 1512 loss: 1.51819977e-05
Iter: 1513 loss: 1.51680151e-05
Iter: 1514 loss: 1.52339835e-05
Iter: 1515 loss: 1.51654904e-05
Iter: 1516 loss: 1.51560198e-05
Iter: 1517 loss: 1.51548902e-05
Iter: 1518 loss: 1.51481381e-05
Iter: 1519 loss: 1.51353743e-05
Iter: 1520 loss: 1.52436223e-05
Iter: 1521 loss: 1.51345994e-05
Iter: 1522 loss: 1.51228705e-05
Iter: 1523 loss: 1.51220283e-05
Iter: 1524 loss: 1.51131844e-05
Iter: 1525 loss: 1.50995675e-05
Iter: 1526 loss: 1.51365566e-05
Iter: 1527 loss: 1.50950154e-05
Iter: 1528 loss: 1.50792257e-05
Iter: 1529 loss: 1.52063467e-05
Iter: 1530 loss: 1.50781898e-05
Iter: 1531 loss: 1.50692149e-05
Iter: 1532 loss: 1.50530377e-05
Iter: 1533 loss: 1.54505506e-05
Iter: 1534 loss: 1.50530723e-05
Iter: 1535 loss: 1.50397336e-05
Iter: 1536 loss: 1.50396299e-05
Iter: 1537 loss: 1.50285487e-05
Iter: 1538 loss: 1.50313836e-05
Iter: 1539 loss: 1.50204824e-05
Iter: 1540 loss: 1.50074502e-05
Iter: 1541 loss: 1.5008538e-05
Iter: 1542 loss: 1.49974639e-05
Iter: 1543 loss: 1.49847338e-05
Iter: 1544 loss: 1.51656295e-05
Iter: 1545 loss: 1.49846837e-05
Iter: 1546 loss: 1.49711268e-05
Iter: 1547 loss: 1.49771295e-05
Iter: 1548 loss: 1.49618736e-05
Iter: 1549 loss: 1.49497064e-05
Iter: 1550 loss: 1.49497737e-05
Iter: 1551 loss: 1.49399484e-05
Iter: 1552 loss: 1.49252464e-05
Iter: 1553 loss: 1.50661162e-05
Iter: 1554 loss: 1.49246935e-05
Iter: 1555 loss: 1.49118778e-05
Iter: 1556 loss: 1.49540228e-05
Iter: 1557 loss: 1.49083635e-05
Iter: 1558 loss: 1.48972831e-05
Iter: 1559 loss: 1.4885456e-05
Iter: 1560 loss: 1.48834715e-05
Iter: 1561 loss: 1.48739355e-05
Iter: 1562 loss: 1.48724812e-05
Iter: 1563 loss: 1.48645222e-05
Iter: 1564 loss: 1.48553618e-05
Iter: 1565 loss: 1.4854244e-05
Iter: 1566 loss: 1.48409263e-05
Iter: 1567 loss: 1.49599e-05
Iter: 1568 loss: 1.48402569e-05
Iter: 1569 loss: 1.48304061e-05
Iter: 1570 loss: 1.48229619e-05
Iter: 1571 loss: 1.48197341e-05
Iter: 1572 loss: 1.4806059e-05
Iter: 1573 loss: 1.4825604e-05
Iter: 1574 loss: 1.47992978e-05
Iter: 1575 loss: 1.47873225e-05
Iter: 1576 loss: 1.47873689e-05
Iter: 1577 loss: 1.47774872e-05
Iter: 1578 loss: 1.47641604e-05
Iter: 1579 loss: 1.47635074e-05
Iter: 1580 loss: 1.47507017e-05
Iter: 1581 loss: 1.48791314e-05
Iter: 1582 loss: 1.47502969e-05
Iter: 1583 loss: 1.47379678e-05
Iter: 1584 loss: 1.47809205e-05
Iter: 1585 loss: 1.47347446e-05
Iter: 1586 loss: 1.47261517e-05
Iter: 1587 loss: 1.4713738e-05
Iter: 1588 loss: 1.47133869e-05
Iter: 1589 loss: 1.4696614e-05
Iter: 1590 loss: 1.47604569e-05
Iter: 1591 loss: 1.46926113e-05
Iter: 1592 loss: 1.46807797e-05
Iter: 1593 loss: 1.46807579e-05
Iter: 1594 loss: 1.4671381e-05
Iter: 1595 loss: 1.46616539e-05
Iter: 1596 loss: 1.46599341e-05
Iter: 1597 loss: 1.4649685e-05
Iter: 1598 loss: 1.48080462e-05
Iter: 1599 loss: 1.4649735e-05
Iter: 1600 loss: 1.46398388e-05
Iter: 1601 loss: 1.46343682e-05
Iter: 1602 loss: 1.46300317e-05
Iter: 1603 loss: 1.46172879e-05
Iter: 1604 loss: 1.46111761e-05
Iter: 1605 loss: 1.46050033e-05
Iter: 1606 loss: 1.45912209e-05
Iter: 1607 loss: 1.45911554e-05
Iter: 1608 loss: 1.45796766e-05
Iter: 1609 loss: 1.45871218e-05
Iter: 1610 loss: 1.45724498e-05
Iter: 1611 loss: 1.45611139e-05
Iter: 1612 loss: 1.45723989e-05
Iter: 1613 loss: 1.45546837e-05
Iter: 1614 loss: 1.45423455e-05
Iter: 1615 loss: 1.47043411e-05
Iter: 1616 loss: 1.45422709e-05
Iter: 1617 loss: 1.4532965e-05
Iter: 1618 loss: 1.4524484e-05
Iter: 1619 loss: 1.4522213e-05
Iter: 1620 loss: 1.45100475e-05
Iter: 1621 loss: 1.46509892e-05
Iter: 1622 loss: 1.45098347e-05
Iter: 1623 loss: 1.44995747e-05
Iter: 1624 loss: 1.44917485e-05
Iter: 1625 loss: 1.44884034e-05
Iter: 1626 loss: 1.44732458e-05
Iter: 1627 loss: 1.4480449e-05
Iter: 1628 loss: 1.44630249e-05
Iter: 1629 loss: 1.44485284e-05
Iter: 1630 loss: 1.46045986e-05
Iter: 1631 loss: 1.44481983e-05
Iter: 1632 loss: 1.44345795e-05
Iter: 1633 loss: 1.45075373e-05
Iter: 1634 loss: 1.44324877e-05
Iter: 1635 loss: 1.4423179e-05
Iter: 1636 loss: 1.44188e-05
Iter: 1637 loss: 1.44141986e-05
Iter: 1638 loss: 1.44027945e-05
Iter: 1639 loss: 1.45376071e-05
Iter: 1640 loss: 1.44025762e-05
Iter: 1641 loss: 1.43929283e-05
Iter: 1642 loss: 1.43808829e-05
Iter: 1643 loss: 1.43798698e-05
Iter: 1644 loss: 1.43648595e-05
Iter: 1645 loss: 1.43801808e-05
Iter: 1646 loss: 1.43565321e-05
Iter: 1647 loss: 1.43407615e-05
Iter: 1648 loss: 1.44335208e-05
Iter: 1649 loss: 1.43387715e-05
Iter: 1650 loss: 1.43246798e-05
Iter: 1651 loss: 1.44720507e-05
Iter: 1652 loss: 1.43242651e-05
Iter: 1653 loss: 1.43150874e-05
Iter: 1654 loss: 1.43099287e-05
Iter: 1655 loss: 1.43059387e-05
Iter: 1656 loss: 1.42946883e-05
Iter: 1657 loss: 1.44229334e-05
Iter: 1658 loss: 1.42944773e-05
Iter: 1659 loss: 1.42852696e-05
Iter: 1660 loss: 1.42725476e-05
Iter: 1661 loss: 1.42719819e-05
Iter: 1662 loss: 1.42575127e-05
Iter: 1663 loss: 1.4315302e-05
Iter: 1664 loss: 1.42541521e-05
Iter: 1665 loss: 1.42414619e-05
Iter: 1666 loss: 1.43849347e-05
Iter: 1667 loss: 1.42411909e-05
Iter: 1668 loss: 1.42319623e-05
Iter: 1669 loss: 1.42266017e-05
Iter: 1670 loss: 1.422268e-05
Iter: 1671 loss: 1.42112276e-05
Iter: 1672 loss: 1.43699144e-05
Iter: 1673 loss: 1.42112149e-05
Iter: 1674 loss: 1.42020526e-05
Iter: 1675 loss: 1.41911842e-05
Iter: 1676 loss: 1.41900928e-05
Iter: 1677 loss: 1.41764231e-05
Iter: 1678 loss: 1.41858018e-05
Iter: 1679 loss: 1.41679357e-05
Iter: 1680 loss: 1.4157331e-05
Iter: 1681 loss: 1.41567944e-05
Iter: 1682 loss: 1.41476512e-05
Iter: 1683 loss: 1.41357505e-05
Iter: 1684 loss: 1.41349656e-05
Iter: 1685 loss: 1.41200435e-05
Iter: 1686 loss: 1.41445635e-05
Iter: 1687 loss: 1.41132841e-05
Iter: 1688 loss: 1.41012169e-05
Iter: 1689 loss: 1.41011969e-05
Iter: 1690 loss: 1.40894244e-05
Iter: 1691 loss: 1.4089992e-05
Iter: 1692 loss: 1.40802267e-05
Iter: 1693 loss: 1.40680677e-05
Iter: 1694 loss: 1.40617849e-05
Iter: 1695 loss: 1.40561915e-05
Iter: 1696 loss: 1.40407628e-05
Iter: 1697 loss: 1.41785858e-05
Iter: 1698 loss: 1.40400189e-05
Iter: 1699 loss: 1.4026934e-05
Iter: 1700 loss: 1.41134024e-05
Iter: 1701 loss: 1.40255615e-05
Iter: 1702 loss: 1.40159791e-05
Iter: 1703 loss: 1.40199591e-05
Iter: 1704 loss: 1.40093616e-05
Iter: 1705 loss: 1.39978238e-05
Iter: 1706 loss: 1.41074688e-05
Iter: 1707 loss: 1.39974436e-05
Iter: 1708 loss: 1.39890308e-05
Iter: 1709 loss: 1.39748236e-05
Iter: 1710 loss: 1.39748372e-05
Iter: 1711 loss: 1.39607328e-05
Iter: 1712 loss: 1.40753536e-05
Iter: 1713 loss: 1.39598815e-05
Iter: 1714 loss: 1.39469339e-05
Iter: 1715 loss: 1.40013517e-05
Iter: 1716 loss: 1.39442436e-05
Iter: 1717 loss: 1.39350068e-05
Iter: 1718 loss: 1.39254989e-05
Iter: 1719 loss: 1.39236945e-05
Iter: 1720 loss: 1.39110143e-05
Iter: 1721 loss: 1.40803122e-05
Iter: 1722 loss: 1.39109634e-05
Iter: 1723 loss: 1.38997084e-05
Iter: 1724 loss: 1.39185304e-05
Iter: 1725 loss: 1.38946125e-05
Iter: 1726 loss: 1.38845944e-05
Iter: 1727 loss: 1.38891419e-05
Iter: 1728 loss: 1.38777232e-05
Iter: 1729 loss: 1.38649166e-05
Iter: 1730 loss: 1.40054099e-05
Iter: 1731 loss: 1.38646574e-05
Iter: 1732 loss: 1.3855868e-05
Iter: 1733 loss: 1.38444102e-05
Iter: 1734 loss: 1.38436562e-05
Iter: 1735 loss: 1.38302021e-05
Iter: 1736 loss: 1.38404539e-05
Iter: 1737 loss: 1.38219621e-05
Iter: 1738 loss: 1.3812788e-05
Iter: 1739 loss: 1.38117875e-05
Iter: 1740 loss: 1.38022397e-05
Iter: 1741 loss: 1.38053683e-05
Iter: 1742 loss: 1.37954758e-05
Iter: 1743 loss: 1.37850602e-05
Iter: 1744 loss: 1.37749676e-05
Iter: 1745 loss: 1.37726884e-05
Iter: 1746 loss: 1.37633251e-05
Iter: 1747 loss: 1.37626284e-05
Iter: 1748 loss: 1.37535135e-05
Iter: 1749 loss: 1.37444858e-05
Iter: 1750 loss: 1.37425523e-05
Iter: 1751 loss: 1.37316492e-05
Iter: 1752 loss: 1.37651177e-05
Iter: 1753 loss: 1.37284624e-05
Iter: 1754 loss: 1.37173456e-05
Iter: 1755 loss: 1.38145133e-05
Iter: 1756 loss: 1.37167626e-05
Iter: 1757 loss: 1.37073766e-05
Iter: 1758 loss: 1.37043053e-05
Iter: 1759 loss: 1.36988392e-05
Iter: 1760 loss: 1.36872841e-05
Iter: 1761 loss: 1.37607858e-05
Iter: 1762 loss: 1.36860144e-05
Iter: 1763 loss: 1.36736835e-05
Iter: 1764 loss: 1.36953604e-05
Iter: 1765 loss: 1.36683047e-05
Iter: 1766 loss: 1.36591898e-05
Iter: 1767 loss: 1.36586968e-05
Iter: 1768 loss: 1.36517074e-05
Iter: 1769 loss: 1.36405924e-05
Iter: 1770 loss: 1.37886182e-05
Iter: 1771 loss: 1.3640547e-05
Iter: 1772 loss: 1.36322324e-05
Iter: 1773 loss: 1.36264098e-05
Iter: 1774 loss: 1.36234685e-05
Iter: 1775 loss: 1.36134968e-05
Iter: 1776 loss: 1.36846393e-05
Iter: 1777 loss: 1.36126346e-05
Iter: 1778 loss: 1.36015169e-05
Iter: 1779 loss: 1.36286126e-05
Iter: 1780 loss: 1.35975079e-05
Iter: 1781 loss: 1.35890123e-05
Iter: 1782 loss: 1.35780165e-05
Iter: 1783 loss: 1.35773298e-05
Iter: 1784 loss: 1.35627542e-05
Iter: 1785 loss: 1.36204944e-05
Iter: 1786 loss: 1.35594692e-05
Iter: 1787 loss: 1.35491864e-05
Iter: 1788 loss: 1.36973485e-05
Iter: 1789 loss: 1.35491891e-05
Iter: 1790 loss: 1.35397604e-05
Iter: 1791 loss: 1.35427163e-05
Iter: 1792 loss: 1.35331311e-05
Iter: 1793 loss: 1.35229666e-05
Iter: 1794 loss: 1.35281389e-05
Iter: 1795 loss: 1.35161827e-05
Iter: 1796 loss: 1.35050614e-05
Iter: 1797 loss: 1.35051159e-05
Iter: 1798 loss: 1.34979509e-05
Iter: 1799 loss: 1.34897246e-05
Iter: 1800 loss: 1.34887523e-05
Iter: 1801 loss: 1.34791044e-05
Iter: 1802 loss: 1.35579467e-05
Iter: 1803 loss: 1.34784586e-05
Iter: 1804 loss: 1.34689762e-05
Iter: 1805 loss: 1.34842303e-05
Iter: 1806 loss: 1.34645734e-05
Iter: 1807 loss: 1.34560432e-05
Iter: 1808 loss: 1.34503771e-05
Iter: 1809 loss: 1.34471693e-05
Iter: 1810 loss: 1.34401616e-05
Iter: 1811 loss: 1.34392412e-05
Iter: 1812 loss: 1.34320499e-05
Iter: 1813 loss: 1.34202955e-05
Iter: 1814 loss: 1.34202328e-05
Iter: 1815 loss: 1.34099555e-05
Iter: 1816 loss: 1.3472787e-05
Iter: 1817 loss: 1.34087604e-05
Iter: 1818 loss: 1.33980429e-05
Iter: 1819 loss: 1.34316188e-05
Iter: 1820 loss: 1.33950043e-05
Iter: 1821 loss: 1.33865506e-05
Iter: 1822 loss: 1.33825451e-05
Iter: 1823 loss: 1.33785106e-05
Iter: 1824 loss: 1.33655958e-05
Iter: 1825 loss: 1.33966132e-05
Iter: 1826 loss: 1.33609083e-05
Iter: 1827 loss: 1.33518952e-05
Iter: 1828 loss: 1.3351606e-05
Iter: 1829 loss: 1.33450121e-05
Iter: 1830 loss: 1.33343874e-05
Iter: 1831 loss: 1.33343246e-05
Iter: 1832 loss: 1.33245485e-05
Iter: 1833 loss: 1.34675302e-05
Iter: 1834 loss: 1.33245558e-05
Iter: 1835 loss: 1.33156072e-05
Iter: 1836 loss: 1.3315308e-05
Iter: 1837 loss: 1.33083231e-05
Iter: 1838 loss: 1.32984942e-05
Iter: 1839 loss: 1.33052827e-05
Iter: 1840 loss: 1.32923469e-05
Iter: 1841 loss: 1.32832738e-05
Iter: 1842 loss: 1.33853155e-05
Iter: 1843 loss: 1.32831228e-05
Iter: 1844 loss: 1.32737814e-05
Iter: 1845 loss: 1.32843325e-05
Iter: 1846 loss: 1.32686982e-05
Iter: 1847 loss: 1.3260742e-05
Iter: 1848 loss: 1.32931673e-05
Iter: 1849 loss: 1.32589703e-05
Iter: 1850 loss: 1.32489695e-05
Iter: 1851 loss: 1.32565419e-05
Iter: 1852 loss: 1.3242895e-05
Iter: 1853 loss: 1.32334953e-05
Iter: 1854 loss: 1.32330897e-05
Iter: 1855 loss: 1.3225892e-05
Iter: 1856 loss: 1.32131709e-05
Iter: 1857 loss: 1.32450659e-05
Iter: 1858 loss: 1.32087353e-05
Iter: 1859 loss: 1.31987936e-05
Iter: 1860 loss: 1.33138583e-05
Iter: 1861 loss: 1.31986199e-05
Iter: 1862 loss: 1.31883189e-05
Iter: 1863 loss: 1.31938095e-05
Iter: 1864 loss: 1.31814722e-05
Iter: 1865 loss: 1.31721354e-05
Iter: 1866 loss: 1.31998968e-05
Iter: 1867 loss: 1.31692359e-05
Iter: 1868 loss: 1.31580491e-05
Iter: 1869 loss: 1.32162968e-05
Iter: 1870 loss: 1.31561756e-05
Iter: 1871 loss: 1.31485795e-05
Iter: 1872 loss: 1.31404158e-05
Iter: 1873 loss: 1.31390425e-05
Iter: 1874 loss: 1.31294928e-05
Iter: 1875 loss: 1.31295046e-05
Iter: 1876 loss: 1.31214456e-05
Iter: 1877 loss: 1.31235538e-05
Iter: 1878 loss: 1.31156012e-05
Iter: 1879 loss: 1.31069401e-05
Iter: 1880 loss: 1.31174465e-05
Iter: 1881 loss: 1.31023662e-05
Iter: 1882 loss: 1.30928283e-05
Iter: 1883 loss: 1.32245405e-05
Iter: 1884 loss: 1.30928638e-05
Iter: 1885 loss: 1.30867329e-05
Iter: 1886 loss: 1.30745748e-05
Iter: 1887 loss: 1.33096264e-05
Iter: 1888 loss: 1.30744356e-05
Iter: 1889 loss: 1.30627395e-05
Iter: 1890 loss: 1.31660381e-05
Iter: 1891 loss: 1.30621938e-05
Iter: 1892 loss: 1.30515091e-05
Iter: 1893 loss: 1.3091254e-05
Iter: 1894 loss: 1.30488843e-05
Iter: 1895 loss: 1.30401677e-05
Iter: 1896 loss: 1.30303e-05
Iter: 1897 loss: 1.30290637e-05
Iter: 1898 loss: 1.30165045e-05
Iter: 1899 loss: 1.30765102e-05
Iter: 1900 loss: 1.30142726e-05
Iter: 1901 loss: 1.30062363e-05
Iter: 1902 loss: 1.30061253e-05
Iter: 1903 loss: 1.29992513e-05
Iter: 1904 loss: 1.29892887e-05
Iter: 1905 loss: 1.29889995e-05
Iter: 1906 loss: 1.29795944e-05
Iter: 1907 loss: 1.30642684e-05
Iter: 1908 loss: 1.29791661e-05
Iter: 1909 loss: 1.29698556e-05
Iter: 1910 loss: 1.29860382e-05
Iter: 1911 loss: 1.29657792e-05
Iter: 1912 loss: 1.29573473e-05
Iter: 1913 loss: 1.29543123e-05
Iter: 1914 loss: 1.29495675e-05
Iter: 1915 loss: 1.29415566e-05
Iter: 1916 loss: 1.29413511e-05
Iter: 1917 loss: 1.29342006e-05
Iter: 1918 loss: 1.2927092e-05
Iter: 1919 loss: 1.29255823e-05
Iter: 1920 loss: 1.29159671e-05
Iter: 1921 loss: 1.3005304e-05
Iter: 1922 loss: 1.29155e-05
Iter: 1923 loss: 1.29071686e-05
Iter: 1924 loss: 1.29123537e-05
Iter: 1925 loss: 1.29018663e-05
Iter: 1926 loss: 1.28927841e-05
Iter: 1927 loss: 1.2888042e-05
Iter: 1928 loss: 1.28839156e-05
Iter: 1929 loss: 1.2872285e-05
Iter: 1930 loss: 1.29354758e-05
Iter: 1931 loss: 1.28705751e-05
Iter: 1932 loss: 1.28606061e-05
Iter: 1933 loss: 1.2970937e-05
Iter: 1934 loss: 1.28604161e-05
Iter: 1935 loss: 1.28536412e-05
Iter: 1936 loss: 1.28443153e-05
Iter: 1937 loss: 1.28439005e-05
Iter: 1938 loss: 1.28338452e-05
Iter: 1939 loss: 1.29454429e-05
Iter: 1940 loss: 1.28336933e-05
Iter: 1941 loss: 1.28244201e-05
Iter: 1942 loss: 1.28533757e-05
Iter: 1943 loss: 1.28217407e-05
Iter: 1944 loss: 1.28148349e-05
Iter: 1945 loss: 1.28042157e-05
Iter: 1946 loss: 1.28040519e-05
Iter: 1947 loss: 1.27941985e-05
Iter: 1948 loss: 1.29338678e-05
Iter: 1949 loss: 1.27942203e-05
Iter: 1950 loss: 1.27851272e-05
Iter: 1951 loss: 1.28142101e-05
Iter: 1952 loss: 1.2782557e-05
Iter: 1953 loss: 1.27750345e-05
Iter: 1954 loss: 1.27802687e-05
Iter: 1955 loss: 1.27703961e-05
Iter: 1956 loss: 1.27611365e-05
Iter: 1957 loss: 1.28374504e-05
Iter: 1958 loss: 1.27605745e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script72
+ '[' -r STOP.script72 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.6/300_300_300_1 '
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi2 /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi2
+ date
Sat Oct 31 16:29:22 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi1.6/300_300_300_1 --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 30 --learning_rate 0.001 --decay_rate 0.8 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f05e22f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f05e26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f047bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f0552d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f04de268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f04dc7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f04de400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f043c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f043c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f02519d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f020f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f03278c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f0662ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f0349378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f03e9ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f03e9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f0396620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f0396d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f018c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f00d4268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f00d4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f01db9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f02ef6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f02e5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f02e52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f00b6158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f00cdae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3a47ae840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3a47ae2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3a47a8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f0053378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f014a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f014a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3f0141488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3a46e9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3a47d1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.010482186
test_loss: 0.010754059
train_loss: 0.006734551
test_loss: 0.0075703836
train_loss: 0.0061386395
test_loss: 0.006593128
train_loss: 0.0050239326
test_loss: 0.0063327546
train_loss: 0.005340576
test_loss: 0.0062644086
train_loss: 0.0048798835
test_loss: 0.00624637
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0241c51e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc02420a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc02420af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc024144a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc024144d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbffb403400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbffb39e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbffb38c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbffb35f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbffb3836a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbffb42a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd4274730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd4274bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd4222620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd421dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd421df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbffb336510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd41aabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd41f4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd41331e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd41336a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd41378c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd40f08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd415b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd4178488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd4178620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd401e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd4039730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd4039268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfd40a6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfc0798268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfc078a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfc078a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfc0758378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfc07cdd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fbfc06c6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.75100896e-05
Iter: 2 loss: 4.98311274e-05
Iter: 3 loss: 4.70477826e-05
Iter: 4 loss: 4.67115169e-05
Iter: 5 loss: 4.76430796e-05
Iter: 6 loss: 4.66021738e-05
Iter: 7 loss: 4.6357145e-05
Iter: 8 loss: 4.65387784e-05
Iter: 9 loss: 4.62068674e-05
Iter: 10 loss: 4.58892355e-05
Iter: 11 loss: 4.5758592e-05
Iter: 12 loss: 4.55909394e-05
Iter: 13 loss: 4.52806344e-05
Iter: 14 loss: 4.52764434e-05
Iter: 15 loss: 4.50615953e-05
Iter: 16 loss: 4.51463e-05
Iter: 17 loss: 4.49127692e-05
Iter: 18 loss: 4.46413069e-05
Iter: 19 loss: 4.58041613e-05
Iter: 20 loss: 4.45850674e-05
Iter: 21 loss: 4.43643839e-05
Iter: 22 loss: 4.51237502e-05
Iter: 23 loss: 4.43058743e-05
Iter: 24 loss: 4.4102635e-05
Iter: 25 loss: 4.43343888e-05
Iter: 26 loss: 4.39936e-05
Iter: 27 loss: 4.37947601e-05
Iter: 28 loss: 4.43890094e-05
Iter: 29 loss: 4.37346607e-05
Iter: 30 loss: 4.35382535e-05
Iter: 31 loss: 4.45031874e-05
Iter: 32 loss: 4.35044785e-05
Iter: 33 loss: 4.3348e-05
Iter: 34 loss: 4.33388632e-05
Iter: 35 loss: 4.32200977e-05
Iter: 36 loss: 4.30293803e-05
Iter: 37 loss: 4.35357142e-05
Iter: 38 loss: 4.29656502e-05
Iter: 39 loss: 4.28366839e-05
Iter: 40 loss: 4.28361709e-05
Iter: 41 loss: 4.27047562e-05
Iter: 42 loss: 4.28832791e-05
Iter: 43 loss: 4.26392216e-05
Iter: 44 loss: 4.25425969e-05
Iter: 45 loss: 4.25608559e-05
Iter: 46 loss: 4.24706668e-05
Iter: 47 loss: 4.23431738e-05
Iter: 48 loss: 4.25811741e-05
Iter: 49 loss: 4.2288746e-05
Iter: 50 loss: 4.21754958e-05
Iter: 51 loss: 4.36265545e-05
Iter: 52 loss: 4.21745099e-05
Iter: 53 loss: 4.20824144e-05
Iter: 54 loss: 4.20600081e-05
Iter: 55 loss: 4.20014258e-05
Iter: 56 loss: 4.18813615e-05
Iter: 57 loss: 4.25308717e-05
Iter: 58 loss: 4.18633863e-05
Iter: 59 loss: 4.17494884e-05
Iter: 60 loss: 4.18887248e-05
Iter: 61 loss: 4.16900693e-05
Iter: 62 loss: 4.15886389e-05
Iter: 63 loss: 4.21188197e-05
Iter: 64 loss: 4.15726718e-05
Iter: 65 loss: 4.14848109e-05
Iter: 66 loss: 4.15197719e-05
Iter: 67 loss: 4.14242786e-05
Iter: 68 loss: 4.13263188e-05
Iter: 69 loss: 4.18570198e-05
Iter: 70 loss: 4.13117959e-05
Iter: 71 loss: 4.12155714e-05
Iter: 72 loss: 4.13716916e-05
Iter: 73 loss: 4.11712972e-05
Iter: 74 loss: 4.10795183e-05
Iter: 75 loss: 4.12141162e-05
Iter: 76 loss: 4.10349894e-05
Iter: 77 loss: 4.09567801e-05
Iter: 78 loss: 4.209092e-05
Iter: 79 loss: 4.09567e-05
Iter: 80 loss: 4.08841661e-05
Iter: 81 loss: 4.103217e-05
Iter: 82 loss: 4.08550768e-05
Iter: 83 loss: 4.0791816e-05
Iter: 84 loss: 4.08037631e-05
Iter: 85 loss: 4.07446278e-05
Iter: 86 loss: 4.06693434e-05
Iter: 87 loss: 4.07031112e-05
Iter: 88 loss: 4.06182116e-05
Iter: 89 loss: 4.05301e-05
Iter: 90 loss: 4.12379886e-05
Iter: 91 loss: 4.05244064e-05
Iter: 92 loss: 4.0453986e-05
Iter: 93 loss: 4.08933629e-05
Iter: 94 loss: 4.0445786e-05
Iter: 95 loss: 4.03864251e-05
Iter: 96 loss: 4.03467857e-05
Iter: 97 loss: 4.03243757e-05
Iter: 98 loss: 4.0242634e-05
Iter: 99 loss: 4.06551262e-05
Iter: 100 loss: 4.02290934e-05
Iter: 101 loss: 4.01460493e-05
Iter: 102 loss: 4.04102284e-05
Iter: 103 loss: 4.01222569e-05
Iter: 104 loss: 4.00572499e-05
Iter: 105 loss: 4.00135104e-05
Iter: 106 loss: 3.99893397e-05
Iter: 107 loss: 3.99174132e-05
Iter: 108 loss: 4.08183259e-05
Iter: 109 loss: 3.99168784e-05
Iter: 110 loss: 3.98502634e-05
Iter: 111 loss: 3.98948687e-05
Iter: 112 loss: 3.9808252e-05
Iter: 113 loss: 3.97393924e-05
Iter: 114 loss: 3.99643141e-05
Iter: 115 loss: 3.97201511e-05
Iter: 116 loss: 3.96535033e-05
Iter: 117 loss: 4.03017038e-05
Iter: 118 loss: 3.96511896e-05
Iter: 119 loss: 3.9594539e-05
Iter: 120 loss: 3.96367977e-05
Iter: 121 loss: 3.95596799e-05
Iter: 122 loss: 3.95047464e-05
Iter: 123 loss: 3.94949639e-05
Iter: 124 loss: 3.94575727e-05
Iter: 125 loss: 3.93780792e-05
Iter: 126 loss: 3.96321484e-05
Iter: 127 loss: 3.93555456e-05
Iter: 128 loss: 3.92894581e-05
Iter: 129 loss: 3.95511597e-05
Iter: 130 loss: 3.92745715e-05
Iter: 131 loss: 3.92143847e-05
Iter: 132 loss: 3.9633258e-05
Iter: 133 loss: 3.92088114e-05
Iter: 134 loss: 3.91556496e-05
Iter: 135 loss: 3.92279981e-05
Iter: 136 loss: 3.91289577e-05
Iter: 137 loss: 3.90755522e-05
Iter: 138 loss: 3.90550049e-05
Iter: 139 loss: 3.90258865e-05
Iter: 140 loss: 3.89621709e-05
Iter: 141 loss: 3.97720432e-05
Iter: 142 loss: 3.89618e-05
Iter: 143 loss: 3.89028e-05
Iter: 144 loss: 3.88700792e-05
Iter: 145 loss: 3.8844104e-05
Iter: 146 loss: 3.8776885e-05
Iter: 147 loss: 3.88131957e-05
Iter: 148 loss: 3.87325781e-05
Iter: 149 loss: 3.8674094e-05
Iter: 150 loss: 3.86733082e-05
Iter: 151 loss: 3.86245447e-05
Iter: 152 loss: 3.86378342e-05
Iter: 153 loss: 3.85891544e-05
Iter: 154 loss: 3.85339445e-05
Iter: 155 loss: 3.93518858e-05
Iter: 156 loss: 3.85338026e-05
Iter: 157 loss: 3.8500064e-05
Iter: 158 loss: 3.84544037e-05
Iter: 159 loss: 3.84520972e-05
Iter: 160 loss: 3.83823317e-05
Iter: 161 loss: 3.85233652e-05
Iter: 162 loss: 3.83539737e-05
Iter: 163 loss: 3.82966173e-05
Iter: 164 loss: 3.83612423e-05
Iter: 165 loss: 3.82659e-05
Iter: 166 loss: 3.82037542e-05
Iter: 167 loss: 3.87576147e-05
Iter: 168 loss: 3.82007711e-05
Iter: 169 loss: 3.81495e-05
Iter: 170 loss: 3.83616571e-05
Iter: 171 loss: 3.81383506e-05
Iter: 172 loss: 3.8086022e-05
Iter: 173 loss: 3.8081067e-05
Iter: 174 loss: 3.80427155e-05
Iter: 175 loss: 3.79837766e-05
Iter: 176 loss: 3.81904865e-05
Iter: 177 loss: 3.79684025e-05
Iter: 178 loss: 3.79068406e-05
Iter: 179 loss: 3.80745769e-05
Iter: 180 loss: 3.78866862e-05
Iter: 181 loss: 3.78256664e-05
Iter: 182 loss: 3.80883575e-05
Iter: 183 loss: 3.78130499e-05
Iter: 184 loss: 3.77620636e-05
Iter: 185 loss: 3.78071418e-05
Iter: 186 loss: 3.77322867e-05
Iter: 187 loss: 3.76711178e-05
Iter: 188 loss: 3.78077966e-05
Iter: 189 loss: 3.76478893e-05
Iter: 190 loss: 3.76028147e-05
Iter: 191 loss: 3.82234866e-05
Iter: 192 loss: 3.76026474e-05
Iter: 193 loss: 3.75542622e-05
Iter: 194 loss: 3.75486925e-05
Iter: 195 loss: 3.7513968e-05
Iter: 196 loss: 3.74667688e-05
Iter: 197 loss: 3.75404961e-05
Iter: 198 loss: 3.74446026e-05
Iter: 199 loss: 3.73753282e-05
Iter: 200 loss: 3.74393858e-05
Iter: 201 loss: 3.73354487e-05
Iter: 202 loss: 3.72701033e-05
Iter: 203 loss: 3.7387359e-05
Iter: 204 loss: 3.72419163e-05
Iter: 205 loss: 3.71618225e-05
Iter: 206 loss: 3.73335934e-05
Iter: 207 loss: 3.71306305e-05
Iter: 208 loss: 3.70812886e-05
Iter: 209 loss: 3.70804046e-05
Iter: 210 loss: 3.70347625e-05
Iter: 211 loss: 3.6971931e-05
Iter: 212 loss: 3.69691334e-05
Iter: 213 loss: 3.69069203e-05
Iter: 214 loss: 3.69954214e-05
Iter: 215 loss: 3.68765104e-05
Iter: 216 loss: 3.68097317e-05
Iter: 217 loss: 3.76072348e-05
Iter: 218 loss: 3.68088367e-05
Iter: 219 loss: 3.67621687e-05
Iter: 220 loss: 3.68106339e-05
Iter: 221 loss: 3.67361245e-05
Iter: 222 loss: 3.66733875e-05
Iter: 223 loss: 3.67551693e-05
Iter: 224 loss: 3.66413369e-05
Iter: 225 loss: 3.65849846e-05
Iter: 226 loss: 3.68301044e-05
Iter: 227 loss: 3.65736778e-05
Iter: 228 loss: 3.6514677e-05
Iter: 229 loss: 3.69633563e-05
Iter: 230 loss: 3.65100859e-05
Iter: 231 loss: 3.64695443e-05
Iter: 232 loss: 3.64733351e-05
Iter: 233 loss: 3.64381667e-05
Iter: 234 loss: 3.63820218e-05
Iter: 235 loss: 3.63715517e-05
Iter: 236 loss: 3.63336294e-05
Iter: 237 loss: 3.62670944e-05
Iter: 238 loss: 3.6622383e-05
Iter: 239 loss: 3.6256788e-05
Iter: 240 loss: 3.61902494e-05
Iter: 241 loss: 3.64387415e-05
Iter: 242 loss: 3.6173944e-05
Iter: 243 loss: 3.61248749e-05
Iter: 244 loss: 3.61211059e-05
Iter: 245 loss: 3.6084537e-05
Iter: 246 loss: 3.60343183e-05
Iter: 247 loss: 3.60338381e-05
Iter: 248 loss: 3.59900914e-05
Iter: 249 loss: 3.59313781e-05
Iter: 250 loss: 3.5928213e-05
Iter: 251 loss: 3.58551697e-05
Iter: 252 loss: 3.59002443e-05
Iter: 253 loss: 3.58083525e-05
Iter: 254 loss: 3.57432582e-05
Iter: 255 loss: 3.67115026e-05
Iter: 256 loss: 3.57431563e-05
Iter: 257 loss: 3.56814598e-05
Iter: 258 loss: 3.57458848e-05
Iter: 259 loss: 3.56471901e-05
Iter: 260 loss: 3.55931625e-05
Iter: 261 loss: 3.58369725e-05
Iter: 262 loss: 3.55827324e-05
Iter: 263 loss: 3.55375669e-05
Iter: 264 loss: 3.58127145e-05
Iter: 265 loss: 3.55320917e-05
Iter: 266 loss: 3.54839867e-05
Iter: 267 loss: 3.55566517e-05
Iter: 268 loss: 3.54614058e-05
Iter: 269 loss: 3.54181939e-05
Iter: 270 loss: 3.53888245e-05
Iter: 271 loss: 3.53730466e-05
Iter: 272 loss: 3.53049036e-05
Iter: 273 loss: 3.54772928e-05
Iter: 274 loss: 3.52815259e-05
Iter: 275 loss: 3.52152347e-05
Iter: 276 loss: 3.55603916e-05
Iter: 277 loss: 3.52048155e-05
Iter: 278 loss: 3.51396957e-05
Iter: 279 loss: 3.52456045e-05
Iter: 280 loss: 3.51096023e-05
Iter: 281 loss: 3.50457558e-05
Iter: 282 loss: 3.52907664e-05
Iter: 283 loss: 3.50307164e-05
Iter: 284 loss: 3.49712391e-05
Iter: 285 loss: 3.52962306e-05
Iter: 286 loss: 3.49623151e-05
Iter: 287 loss: 3.49088659e-05
Iter: 288 loss: 3.49767943e-05
Iter: 289 loss: 3.48813737e-05
Iter: 290 loss: 3.48266694e-05
Iter: 291 loss: 3.49152324e-05
Iter: 292 loss: 3.48014182e-05
Iter: 293 loss: 3.47408968e-05
Iter: 294 loss: 3.48103495e-05
Iter: 295 loss: 3.47085e-05
Iter: 296 loss: 3.46397719e-05
Iter: 297 loss: 3.48679896e-05
Iter: 298 loss: 3.4620989e-05
Iter: 299 loss: 3.45652152e-05
Iter: 300 loss: 3.52660791e-05
Iter: 301 loss: 3.45647859e-05
Iter: 302 loss: 3.45211702e-05
Iter: 303 loss: 3.4544526e-05
Iter: 304 loss: 3.44926048e-05
Iter: 305 loss: 3.44313667e-05
Iter: 306 loss: 3.47774621e-05
Iter: 307 loss: 3.44228538e-05
Iter: 308 loss: 3.43855936e-05
Iter: 309 loss: 3.43371466e-05
Iter: 310 loss: 3.43339343e-05
Iter: 311 loss: 3.42636959e-05
Iter: 312 loss: 3.44656146e-05
Iter: 313 loss: 3.4241657e-05
Iter: 314 loss: 3.41706509e-05
Iter: 315 loss: 3.42286839e-05
Iter: 316 loss: 3.41280684e-05
Iter: 317 loss: 3.40611368e-05
Iter: 318 loss: 3.48551039e-05
Iter: 319 loss: 3.40602128e-05
Iter: 320 loss: 3.39988619e-05
Iter: 321 loss: 3.41106279e-05
Iter: 322 loss: 3.39724356e-05
Iter: 323 loss: 3.39164399e-05
Iter: 324 loss: 3.42289641e-05
Iter: 325 loss: 3.39084872e-05
Iter: 326 loss: 3.38550308e-05
Iter: 327 loss: 3.39054895e-05
Iter: 328 loss: 3.38244718e-05
Iter: 329 loss: 3.37662714e-05
Iter: 330 loss: 3.37875681e-05
Iter: 331 loss: 3.37257079e-05
Iter: 332 loss: 3.36547928e-05
Iter: 333 loss: 3.39283106e-05
Iter: 334 loss: 3.36380399e-05
Iter: 335 loss: 3.35756777e-05
Iter: 336 loss: 3.38948921e-05
Iter: 337 loss: 3.35656587e-05
Iter: 338 loss: 3.35163641e-05
Iter: 339 loss: 3.39437647e-05
Iter: 340 loss: 3.35137738e-05
Iter: 341 loss: 3.34728829e-05
Iter: 342 loss: 3.35301811e-05
Iter: 343 loss: 3.34526339e-05
Iter: 344 loss: 3.34066026e-05
Iter: 345 loss: 3.34786528e-05
Iter: 346 loss: 3.33850658e-05
Iter: 347 loss: 3.33327553e-05
Iter: 348 loss: 3.33716962e-05
Iter: 349 loss: 3.33005155e-05
Iter: 350 loss: 3.32370837e-05
Iter: 351 loss: 3.33650096e-05
Iter: 352 loss: 3.32113705e-05
Iter: 353 loss: 3.31472402e-05
Iter: 354 loss: 3.31816336e-05
Iter: 355 loss: 3.3104945e-05
Iter: 356 loss: 3.30404437e-05
Iter: 357 loss: 3.3476088e-05
Iter: 358 loss: 3.30340845e-05
Iter: 359 loss: 3.29635732e-05
Iter: 360 loss: 3.32989439e-05
Iter: 361 loss: 3.29508475e-05
Iter: 362 loss: 3.29099566e-05
Iter: 363 loss: 3.29901377e-05
Iter: 364 loss: 3.28931e-05
Iter: 365 loss: 3.28359674e-05
Iter: 366 loss: 3.2873817e-05
Iter: 367 loss: 3.27998714e-05
Iter: 368 loss: 3.27463422e-05
Iter: 369 loss: 3.27180751e-05
Iter: 370 loss: 3.26936752e-05
Iter: 371 loss: 3.26303e-05
Iter: 372 loss: 3.26302616e-05
Iter: 373 loss: 3.25833971e-05
Iter: 374 loss: 3.27844573e-05
Iter: 375 loss: 3.25737565e-05
Iter: 376 loss: 3.25181718e-05
Iter: 377 loss: 3.25742076e-05
Iter: 378 loss: 3.24869543e-05
Iter: 379 loss: 3.24396096e-05
Iter: 380 loss: 3.26112895e-05
Iter: 381 loss: 3.24277062e-05
Iter: 382 loss: 3.23778149e-05
Iter: 383 loss: 3.23973218e-05
Iter: 384 loss: 3.23431159e-05
Iter: 385 loss: 3.22838023e-05
Iter: 386 loss: 3.23824497e-05
Iter: 387 loss: 3.22570449e-05
Iter: 388 loss: 3.21962507e-05
Iter: 389 loss: 3.23724271e-05
Iter: 390 loss: 3.21773841e-05
Iter: 391 loss: 3.21130938e-05
Iter: 392 loss: 3.22851593e-05
Iter: 393 loss: 3.20917425e-05
Iter: 394 loss: 3.20437321e-05
Iter: 395 loss: 3.25002475e-05
Iter: 396 loss: 3.20418258e-05
Iter: 397 loss: 3.19942628e-05
Iter: 398 loss: 3.20220752e-05
Iter: 399 loss: 3.19634419e-05
Iter: 400 loss: 3.19082683e-05
Iter: 401 loss: 3.20647923e-05
Iter: 402 loss: 3.18908569e-05
Iter: 403 loss: 3.18379025e-05
Iter: 404 loss: 3.18756793e-05
Iter: 405 loss: 3.18050879e-05
Iter: 406 loss: 3.17372178e-05
Iter: 407 loss: 3.21238913e-05
Iter: 408 loss: 3.17277882e-05
Iter: 409 loss: 3.16787191e-05
Iter: 410 loss: 3.17263111e-05
Iter: 411 loss: 3.16509286e-05
Iter: 412 loss: 3.1606065e-05
Iter: 413 loss: 3.16058795e-05
Iter: 414 loss: 3.1570984e-05
Iter: 415 loss: 3.15249308e-05
Iter: 416 loss: 3.15221332e-05
Iter: 417 loss: 3.14663848e-05
Iter: 418 loss: 3.16452861e-05
Iter: 419 loss: 3.14505778e-05
Iter: 420 loss: 3.13984601e-05
Iter: 421 loss: 3.16589212e-05
Iter: 422 loss: 3.13896817e-05
Iter: 423 loss: 3.13355049e-05
Iter: 424 loss: 3.13030396e-05
Iter: 425 loss: 3.12806078e-05
Iter: 426 loss: 3.12195552e-05
Iter: 427 loss: 3.1362295e-05
Iter: 428 loss: 3.11969306e-05
Iter: 429 loss: 3.11339245e-05
Iter: 430 loss: 3.14850186e-05
Iter: 431 loss: 3.11249241e-05
Iter: 432 loss: 3.10737523e-05
Iter: 433 loss: 3.15847683e-05
Iter: 434 loss: 3.10721298e-05
Iter: 435 loss: 3.10338837e-05
Iter: 436 loss: 3.10050855e-05
Iter: 437 loss: 3.09925454e-05
Iter: 438 loss: 3.09348688e-05
Iter: 439 loss: 3.11921503e-05
Iter: 440 loss: 3.0923482e-05
Iter: 441 loss: 3.08706876e-05
Iter: 442 loss: 3.09380775e-05
Iter: 443 loss: 3.08434319e-05
Iter: 444 loss: 3.07902446e-05
Iter: 445 loss: 3.11458934e-05
Iter: 446 loss: 3.0784955e-05
Iter: 447 loss: 3.07404807e-05
Iter: 448 loss: 3.0920266e-05
Iter: 449 loss: 3.07305199e-05
Iter: 450 loss: 3.06808215e-05
Iter: 451 loss: 3.07886403e-05
Iter: 452 loss: 3.06616457e-05
Iter: 453 loss: 3.06245274e-05
Iter: 454 loss: 3.06403817e-05
Iter: 455 loss: 3.05991e-05
Iter: 456 loss: 3.05458852e-05
Iter: 457 loss: 3.05833237e-05
Iter: 458 loss: 3.05128542e-05
Iter: 459 loss: 3.04654568e-05
Iter: 460 loss: 3.10899341e-05
Iter: 461 loss: 3.04652367e-05
Iter: 462 loss: 3.04217847e-05
Iter: 463 loss: 3.03683701e-05
Iter: 464 loss: 3.03635861e-05
Iter: 465 loss: 3.03073539e-05
Iter: 466 loss: 3.05675385e-05
Iter: 467 loss: 3.02967273e-05
Iter: 468 loss: 3.02373737e-05
Iter: 469 loss: 3.04005716e-05
Iter: 470 loss: 3.02180542e-05
Iter: 471 loss: 3.01667096e-05
Iter: 472 loss: 3.06579641e-05
Iter: 473 loss: 3.01646542e-05
Iter: 474 loss: 3.01206055e-05
Iter: 475 loss: 3.0129715e-05
Iter: 476 loss: 3.00878037e-05
Iter: 477 loss: 3.00371976e-05
Iter: 478 loss: 3.00780594e-05
Iter: 479 loss: 3.0006544e-05
Iter: 480 loss: 2.99536805e-05
Iter: 481 loss: 3.01712898e-05
Iter: 482 loss: 2.99420535e-05
Iter: 483 loss: 2.99026269e-05
Iter: 484 loss: 2.99026e-05
Iter: 485 loss: 2.9875051e-05
Iter: 486 loss: 2.9905259e-05
Iter: 487 loss: 2.98602245e-05
Iter: 488 loss: 2.98200102e-05
Iter: 489 loss: 2.98045306e-05
Iter: 490 loss: 2.97828246e-05
Iter: 491 loss: 2.97339338e-05
Iter: 492 loss: 2.97555271e-05
Iter: 493 loss: 2.97006118e-05
Iter: 494 loss: 2.96395738e-05
Iter: 495 loss: 3.01997061e-05
Iter: 496 loss: 2.96369344e-05
Iter: 497 loss: 2.95931859e-05
Iter: 498 loss: 2.97118568e-05
Iter: 499 loss: 2.95787977e-05
Iter: 500 loss: 2.95332757e-05
Iter: 501 loss: 2.95735063e-05
Iter: 502 loss: 2.95067257e-05
Iter: 503 loss: 2.94591482e-05
Iter: 504 loss: 2.9562736e-05
Iter: 505 loss: 2.94408856e-05
Iter: 506 loss: 2.93941775e-05
Iter: 507 loss: 2.97399238e-05
Iter: 508 loss: 2.93903631e-05
Iter: 509 loss: 2.93476587e-05
Iter: 510 loss: 2.9439565e-05
Iter: 511 loss: 2.93309422e-05
Iter: 512 loss: 2.92839286e-05
Iter: 513 loss: 2.93778612e-05
Iter: 514 loss: 2.92648219e-05
Iter: 515 loss: 2.92234108e-05
Iter: 516 loss: 2.91853394e-05
Iter: 517 loss: 2.91752913e-05
Iter: 518 loss: 2.91454926e-05
Iter: 519 loss: 2.91415927e-05
Iter: 520 loss: 2.91065753e-05
Iter: 521 loss: 2.91475662e-05
Iter: 522 loss: 2.90879707e-05
Iter: 523 loss: 2.90497628e-05
Iter: 524 loss: 2.90687149e-05
Iter: 525 loss: 2.90241587e-05
Iter: 526 loss: 2.89804957e-05
Iter: 527 loss: 2.91624347e-05
Iter: 528 loss: 2.89710988e-05
Iter: 529 loss: 2.89294258e-05
Iter: 530 loss: 2.89423224e-05
Iter: 531 loss: 2.88996089e-05
Iter: 532 loss: 2.88524898e-05
Iter: 533 loss: 2.91162651e-05
Iter: 534 loss: 2.88458559e-05
Iter: 535 loss: 2.88049723e-05
Iter: 536 loss: 2.89863237e-05
Iter: 537 loss: 2.87968687e-05
Iter: 538 loss: 2.87601797e-05
Iter: 539 loss: 2.87683852e-05
Iter: 540 loss: 2.87330477e-05
Iter: 541 loss: 2.86892027e-05
Iter: 542 loss: 2.88591764e-05
Iter: 543 loss: 2.86789909e-05
Iter: 544 loss: 2.86381473e-05
Iter: 545 loss: 2.88538158e-05
Iter: 546 loss: 2.86318882e-05
Iter: 547 loss: 2.85950191e-05
Iter: 548 loss: 2.8734019e-05
Iter: 549 loss: 2.85862407e-05
Iter: 550 loss: 2.85544029e-05
Iter: 551 loss: 2.8502438e-05
Iter: 552 loss: 2.85021815e-05
Iter: 553 loss: 2.84539128e-05
Iter: 554 loss: 2.89130876e-05
Iter: 555 loss: 2.84519774e-05
Iter: 556 loss: 2.840659e-05
Iter: 557 loss: 2.87583789e-05
Iter: 558 loss: 2.84034104e-05
Iter: 559 loss: 2.8372855e-05
Iter: 560 loss: 2.83846657e-05
Iter: 561 loss: 2.83516274e-05
Iter: 562 loss: 2.83156569e-05
Iter: 563 loss: 2.83288791e-05
Iter: 564 loss: 2.82905949e-05
Iter: 565 loss: 2.82453402e-05
Iter: 566 loss: 2.84619309e-05
Iter: 567 loss: 2.82372257e-05
Iter: 568 loss: 2.81935572e-05
Iter: 569 loss: 2.82656692e-05
Iter: 570 loss: 2.81736611e-05
Iter: 571 loss: 2.8132552e-05
Iter: 572 loss: 2.83711342e-05
Iter: 573 loss: 2.81270186e-05
Iter: 574 loss: 2.80916302e-05
Iter: 575 loss: 2.81060929e-05
Iter: 576 loss: 2.80672648e-05
Iter: 577 loss: 2.80228378e-05
Iter: 578 loss: 2.83594345e-05
Iter: 579 loss: 2.80193781e-05
Iter: 580 loss: 2.79870656e-05
Iter: 581 loss: 2.79498527e-05
Iter: 582 loss: 2.79453525e-05
Iter: 583 loss: 2.791523e-05
Iter: 584 loss: 2.79113701e-05
Iter: 585 loss: 2.78844709e-05
Iter: 586 loss: 2.78330335e-05
Iter: 587 loss: 2.89200816e-05
Iter: 588 loss: 2.78327916e-05
Iter: 589 loss: 2.77891504e-05
Iter: 590 loss: 2.81248776e-05
Iter: 591 loss: 2.77858817e-05
Iter: 592 loss: 2.77520539e-05
Iter: 593 loss: 2.81201719e-05
Iter: 594 loss: 2.77512809e-05
Iter: 595 loss: 2.77211839e-05
Iter: 596 loss: 2.77063882e-05
Iter: 597 loss: 2.76921037e-05
Iter: 598 loss: 2.76560495e-05
Iter: 599 loss: 2.7653241e-05
Iter: 600 loss: 2.76264836e-05
Iter: 601 loss: 2.75839266e-05
Iter: 602 loss: 2.79640553e-05
Iter: 603 loss: 2.75820021e-05
Iter: 604 loss: 2.75466919e-05
Iter: 605 loss: 2.76310493e-05
Iter: 606 loss: 2.75339407e-05
Iter: 607 loss: 2.74980775e-05
Iter: 608 loss: 2.75508e-05
Iter: 609 loss: 2.74807917e-05
Iter: 610 loss: 2.74413142e-05
Iter: 611 loss: 2.76283645e-05
Iter: 612 loss: 2.7434171e-05
Iter: 613 loss: 2.73966725e-05
Iter: 614 loss: 2.74427402e-05
Iter: 615 loss: 2.73771311e-05
Iter: 616 loss: 2.73371952e-05
Iter: 617 loss: 2.75179664e-05
Iter: 618 loss: 2.73293426e-05
Iter: 619 loss: 2.72959114e-05
Iter: 620 loss: 2.74023696e-05
Iter: 621 loss: 2.7286349e-05
Iter: 622 loss: 2.72512862e-05
Iter: 623 loss: 2.73869555e-05
Iter: 624 loss: 2.72431043e-05
Iter: 625 loss: 2.72139e-05
Iter: 626 loss: 2.71944064e-05
Iter: 627 loss: 2.71834906e-05
Iter: 628 loss: 2.71470271e-05
Iter: 629 loss: 2.7627113e-05
Iter: 630 loss: 2.71468416e-05
Iter: 631 loss: 2.71173831e-05
Iter: 632 loss: 2.72415691e-05
Iter: 633 loss: 2.71112222e-05
Iter: 634 loss: 2.70877e-05
Iter: 635 loss: 2.70548044e-05
Iter: 636 loss: 2.70534438e-05
Iter: 637 loss: 2.70107666e-05
Iter: 638 loss: 2.71542558e-05
Iter: 639 loss: 2.69992306e-05
Iter: 640 loss: 2.69565044e-05
Iter: 641 loss: 2.70115852e-05
Iter: 642 loss: 2.69347238e-05
Iter: 643 loss: 2.68932345e-05
Iter: 644 loss: 2.71902027e-05
Iter: 645 loss: 2.68896e-05
Iter: 646 loss: 2.68520453e-05
Iter: 647 loss: 2.69962184e-05
Iter: 648 loss: 2.68432068e-05
Iter: 649 loss: 2.68099429e-05
Iter: 650 loss: 2.68054464e-05
Iter: 651 loss: 2.67819851e-05
Iter: 652 loss: 2.67427458e-05
Iter: 653 loss: 2.71313529e-05
Iter: 654 loss: 2.6741458e-05
Iter: 655 loss: 2.6708025e-05
Iter: 656 loss: 2.67417836e-05
Iter: 657 loss: 2.66892912e-05
Iter: 658 loss: 2.66602292e-05
Iter: 659 loss: 2.69140255e-05
Iter: 660 loss: 2.66587231e-05
Iter: 661 loss: 2.66327806e-05
Iter: 662 loss: 2.66317456e-05
Iter: 663 loss: 2.66117841e-05
Iter: 664 loss: 2.65818944e-05
Iter: 665 loss: 2.66817697e-05
Iter: 666 loss: 2.65737472e-05
Iter: 667 loss: 2.6539321e-05
Iter: 668 loss: 2.67498617e-05
Iter: 669 loss: 2.65352173e-05
Iter: 670 loss: 2.65072085e-05
Iter: 671 loss: 2.64752925e-05
Iter: 672 loss: 2.64712216e-05
Iter: 673 loss: 2.64271e-05
Iter: 674 loss: 2.64457321e-05
Iter: 675 loss: 2.63968486e-05
Iter: 676 loss: 2.63508664e-05
Iter: 677 loss: 2.67342384e-05
Iter: 678 loss: 2.63480943e-05
Iter: 679 loss: 2.63081183e-05
Iter: 680 loss: 2.64051159e-05
Iter: 681 loss: 2.62936701e-05
Iter: 682 loss: 2.626e-05
Iter: 683 loss: 2.65007402e-05
Iter: 684 loss: 2.6257012e-05
Iter: 685 loss: 2.62279573e-05
Iter: 686 loss: 2.62639951e-05
Iter: 687 loss: 2.62128506e-05
Iter: 688 loss: 2.617684e-05
Iter: 689 loss: 2.62188223e-05
Iter: 690 loss: 2.61576424e-05
Iter: 691 loss: 2.61235728e-05
Iter: 692 loss: 2.64440769e-05
Iter: 693 loss: 2.6122194e-05
Iter: 694 loss: 2.60919769e-05
Iter: 695 loss: 2.60945017e-05
Iter: 696 loss: 2.60685047e-05
Iter: 697 loss: 2.60297275e-05
Iter: 698 loss: 2.62868216e-05
Iter: 699 loss: 2.60256929e-05
Iter: 700 loss: 2.599855e-05
Iter: 701 loss: 2.60286561e-05
Iter: 702 loss: 2.59839526e-05
Iter: 703 loss: 2.59518383e-05
Iter: 704 loss: 2.6207952e-05
Iter: 705 loss: 2.59496301e-05
Iter: 706 loss: 2.59256885e-05
Iter: 707 loss: 2.59475692e-05
Iter: 708 loss: 2.59116532e-05
Iter: 709 loss: 2.58827768e-05
Iter: 710 loss: 2.58736291e-05
Iter: 711 loss: 2.58565e-05
Iter: 712 loss: 2.5814501e-05
Iter: 713 loss: 2.58149448e-05
Iter: 714 loss: 2.57809e-05
Iter: 715 loss: 2.57319334e-05
Iter: 716 loss: 2.59263397e-05
Iter: 717 loss: 2.57207757e-05
Iter: 718 loss: 2.56779822e-05
Iter: 719 loss: 2.60849592e-05
Iter: 720 loss: 2.56762851e-05
Iter: 721 loss: 2.56460607e-05
Iter: 722 loss: 2.5781e-05
Iter: 723 loss: 2.56400272e-05
Iter: 724 loss: 2.56085114e-05
Iter: 725 loss: 2.56373951e-05
Iter: 726 loss: 2.55903033e-05
Iter: 727 loss: 2.55610557e-05
Iter: 728 loss: 2.5642028e-05
Iter: 729 loss: 2.55517152e-05
Iter: 730 loss: 2.55123559e-05
Iter: 731 loss: 2.5629819e-05
Iter: 732 loss: 2.55004506e-05
Iter: 733 loss: 2.54722727e-05
Iter: 734 loss: 2.55643536e-05
Iter: 735 loss: 2.54644583e-05
Iter: 736 loss: 2.54358802e-05
Iter: 737 loss: 2.55130835e-05
Iter: 738 loss: 2.54264196e-05
Iter: 739 loss: 2.53990911e-05
Iter: 740 loss: 2.55769464e-05
Iter: 741 loss: 2.53961371e-05
Iter: 742 loss: 2.53743492e-05
Iter: 743 loss: 2.53896014e-05
Iter: 744 loss: 2.53608468e-05
Iter: 745 loss: 2.53338658e-05
Iter: 746 loss: 2.53303951e-05
Iter: 747 loss: 2.53112576e-05
Iter: 748 loss: 2.52757673e-05
Iter: 749 loss: 2.53325306e-05
Iter: 750 loss: 2.52593745e-05
Iter: 751 loss: 2.52211248e-05
Iter: 752 loss: 2.54064817e-05
Iter: 753 loss: 2.52144091e-05
Iter: 754 loss: 2.51773636e-05
Iter: 755 loss: 2.51941747e-05
Iter: 756 loss: 2.51522542e-05
Iter: 757 loss: 2.51154106e-05
Iter: 758 loss: 2.54991428e-05
Iter: 759 loss: 2.51144393e-05
Iter: 760 loss: 2.50864268e-05
Iter: 761 loss: 2.51983565e-05
Iter: 762 loss: 2.50800731e-05
Iter: 763 loss: 2.50548765e-05
Iter: 764 loss: 2.51009114e-05
Iter: 765 loss: 2.50440462e-05
Iter: 766 loss: 2.50150442e-05
Iter: 767 loss: 2.50798294e-05
Iter: 768 loss: 2.50040284e-05
Iter: 769 loss: 2.49740624e-05
Iter: 770 loss: 2.51280671e-05
Iter: 771 loss: 2.49692512e-05
Iter: 772 loss: 2.49425066e-05
Iter: 773 loss: 2.49705172e-05
Iter: 774 loss: 2.49274635e-05
Iter: 775 loss: 2.49022669e-05
Iter: 776 loss: 2.51529855e-05
Iter: 777 loss: 2.49015084e-05
Iter: 778 loss: 2.48776832e-05
Iter: 779 loss: 2.48502474e-05
Iter: 780 loss: 2.48468332e-05
Iter: 781 loss: 2.48093274e-05
Iter: 782 loss: 2.49584264e-05
Iter: 783 loss: 2.48009273e-05
Iter: 784 loss: 2.47710959e-05
Iter: 785 loss: 2.48690012e-05
Iter: 786 loss: 2.4762845e-05
Iter: 787 loss: 2.47332282e-05
Iter: 788 loss: 2.47707339e-05
Iter: 789 loss: 2.47180615e-05
Iter: 790 loss: 2.46828731e-05
Iter: 791 loss: 2.46708041e-05
Iter: 792 loss: 2.46507352e-05
Iter: 793 loss: 2.46070304e-05
Iter: 794 loss: 2.47804e-05
Iter: 795 loss: 2.45970332e-05
Iter: 796 loss: 2.45575957e-05
Iter: 797 loss: 2.48645501e-05
Iter: 798 loss: 2.45547017e-05
Iter: 799 loss: 2.45253013e-05
Iter: 800 loss: 2.47623248e-05
Iter: 801 loss: 2.45234078e-05
Iter: 802 loss: 2.45006231e-05
Iter: 803 loss: 2.44837793e-05
Iter: 804 loss: 2.44761341e-05
Iter: 805 loss: 2.44431503e-05
Iter: 806 loss: 2.48072465e-05
Iter: 807 loss: 2.44425501e-05
Iter: 808 loss: 2.4419227e-05
Iter: 809 loss: 2.4447545e-05
Iter: 810 loss: 2.44071016e-05
Iter: 811 loss: 2.43807663e-05
Iter: 812 loss: 2.4527455e-05
Iter: 813 loss: 2.43769919e-05
Iter: 814 loss: 2.43557661e-05
Iter: 815 loss: 2.44140501e-05
Iter: 816 loss: 2.43488576e-05
Iter: 817 loss: 2.43290924e-05
Iter: 818 loss: 2.4287885e-05
Iter: 819 loss: 2.49961449e-05
Iter: 820 loss: 2.42868882e-05
Iter: 821 loss: 2.42571841e-05
Iter: 822 loss: 2.42564965e-05
Iter: 823 loss: 2.42306105e-05
Iter: 824 loss: 2.42114802e-05
Iter: 825 loss: 2.42028309e-05
Iter: 826 loss: 2.41668804e-05
Iter: 827 loss: 2.43137e-05
Iter: 828 loss: 2.41590478e-05
Iter: 829 loss: 2.41212019e-05
Iter: 830 loss: 2.41287744e-05
Iter: 831 loss: 2.40931568e-05
Iter: 832 loss: 2.40577501e-05
Iter: 833 loss: 2.45143528e-05
Iter: 834 loss: 2.40574591e-05
Iter: 835 loss: 2.40305017e-05
Iter: 836 loss: 2.40941044e-05
Iter: 837 loss: 2.40206791e-05
Iter: 838 loss: 2.39931269e-05
Iter: 839 loss: 2.41424495e-05
Iter: 840 loss: 2.39889323e-05
Iter: 841 loss: 2.39637011e-05
Iter: 842 loss: 2.39863857e-05
Iter: 843 loss: 2.39489418e-05
Iter: 844 loss: 2.39239525e-05
Iter: 845 loss: 2.41221187e-05
Iter: 846 loss: 2.39222863e-05
Iter: 847 loss: 2.38990178e-05
Iter: 848 loss: 2.3912844e-05
Iter: 849 loss: 2.38839621e-05
Iter: 850 loss: 2.38572411e-05
Iter: 851 loss: 2.39902638e-05
Iter: 852 loss: 2.38528028e-05
Iter: 853 loss: 2.38286884e-05
Iter: 854 loss: 2.38315661e-05
Iter: 855 loss: 2.38102512e-05
Iter: 856 loss: 2.37832792e-05
Iter: 857 loss: 2.37899221e-05
Iter: 858 loss: 2.37637032e-05
Iter: 859 loss: 2.37289714e-05
Iter: 860 loss: 2.39315086e-05
Iter: 861 loss: 2.3724433e-05
Iter: 862 loss: 2.36958149e-05
Iter: 863 loss: 2.3791e-05
Iter: 864 loss: 2.36879387e-05
Iter: 865 loss: 2.36573105e-05
Iter: 866 loss: 2.37165586e-05
Iter: 867 loss: 2.36445412e-05
Iter: 868 loss: 2.36136384e-05
Iter: 869 loss: 2.36245105e-05
Iter: 870 loss: 2.35918415e-05
Iter: 871 loss: 2.35549051e-05
Iter: 872 loss: 2.37953755e-05
Iter: 873 loss: 2.35509488e-05
Iter: 874 loss: 2.35191874e-05
Iter: 875 loss: 2.3601573e-05
Iter: 876 loss: 2.35084153e-05
Iter: 877 loss: 2.34758809e-05
Iter: 878 loss: 2.37514942e-05
Iter: 879 loss: 2.34739528e-05
Iter: 880 loss: 2.34551662e-05
Iter: 881 loss: 2.34658328e-05
Iter: 882 loss: 2.3443019e-05
Iter: 883 loss: 2.34155377e-05
Iter: 884 loss: 2.35724983e-05
Iter: 885 loss: 2.34118888e-05
Iter: 886 loss: 2.33924038e-05
Iter: 887 loss: 2.33962674e-05
Iter: 888 loss: 2.33780083e-05
Iter: 889 loss: 2.33494538e-05
Iter: 890 loss: 2.34407307e-05
Iter: 891 loss: 2.33412848e-05
Iter: 892 loss: 2.33140272e-05
Iter: 893 loss: 2.33071296e-05
Iter: 894 loss: 2.32899e-05
Iter: 895 loss: 2.32560469e-05
Iter: 896 loss: 2.33906903e-05
Iter: 897 loss: 2.32483435e-05
Iter: 898 loss: 2.32178772e-05
Iter: 899 loss: 2.32619514e-05
Iter: 900 loss: 2.32032726e-05
Iter: 901 loss: 2.31722915e-05
Iter: 902 loss: 2.34240433e-05
Iter: 903 loss: 2.31702306e-05
Iter: 904 loss: 2.31400627e-05
Iter: 905 loss: 2.31351842e-05
Iter: 906 loss: 2.31143022e-05
Iter: 907 loss: 2.30841142e-05
Iter: 908 loss: 2.32058119e-05
Iter: 909 loss: 2.3077293e-05
Iter: 910 loss: 2.30471269e-05
Iter: 911 loss: 2.32190796e-05
Iter: 912 loss: 2.30430396e-05
Iter: 913 loss: 2.30223704e-05
Iter: 914 loss: 2.3150471e-05
Iter: 915 loss: 2.30200058e-05
Iter: 916 loss: 2.29981742e-05
Iter: 917 loss: 2.30005317e-05
Iter: 918 loss: 2.2981445e-05
Iter: 919 loss: 2.29573361e-05
Iter: 920 loss: 2.31738704e-05
Iter: 921 loss: 2.29560901e-05
Iter: 922 loss: 2.29336401e-05
Iter: 923 loss: 2.2912729e-05
Iter: 924 loss: 2.29073376e-05
Iter: 925 loss: 2.28803e-05
Iter: 926 loss: 2.30828264e-05
Iter: 927 loss: 2.28781828e-05
Iter: 928 loss: 2.28559038e-05
Iter: 929 loss: 2.28873723e-05
Iter: 930 loss: 2.28449408e-05
Iter: 931 loss: 2.28190329e-05
Iter: 932 loss: 2.2844215e-05
Iter: 933 loss: 2.28044155e-05
Iter: 934 loss: 2.27770252e-05
Iter: 935 loss: 2.28550416e-05
Iter: 936 loss: 2.2768385e-05
Iter: 937 loss: 2.27394521e-05
Iter: 938 loss: 2.27612127e-05
Iter: 939 loss: 2.2721737e-05
Iter: 940 loss: 2.2689037e-05
Iter: 941 loss: 2.28426325e-05
Iter: 942 loss: 2.26829907e-05
Iter: 943 loss: 2.26548618e-05
Iter: 944 loss: 2.28445606e-05
Iter: 945 loss: 2.26520551e-05
Iter: 946 loss: 2.26273751e-05
Iter: 947 loss: 2.26360098e-05
Iter: 948 loss: 2.2610162e-05
Iter: 949 loss: 2.25814e-05
Iter: 950 loss: 2.27795426e-05
Iter: 951 loss: 2.25786262e-05
Iter: 952 loss: 2.25544827e-05
Iter: 953 loss: 2.26995217e-05
Iter: 954 loss: 2.25515359e-05
Iter: 955 loss: 2.25322219e-05
Iter: 956 loss: 2.25290096e-05
Iter: 957 loss: 2.25157819e-05
Iter: 958 loss: 2.24873438e-05
Iter: 959 loss: 2.26981247e-05
Iter: 960 loss: 2.24849227e-05
Iter: 961 loss: 2.24697105e-05
Iter: 962 loss: 2.24438172e-05
Iter: 963 loss: 2.24438554e-05
Iter: 964 loss: 2.2416014e-05
Iter: 965 loss: 2.27717901e-05
Iter: 966 loss: 2.24158812e-05
Iter: 967 loss: 2.23939023e-05
Iter: 968 loss: 2.23961961e-05
Iter: 969 loss: 2.23770821e-05
Iter: 970 loss: 2.23509815e-05
Iter: 971 loss: 2.24115865e-05
Iter: 972 loss: 2.23413299e-05
Iter: 973 loss: 2.23142124e-05
Iter: 974 loss: 2.23648422e-05
Iter: 975 loss: 2.230268e-05
Iter: 976 loss: 2.22743292e-05
Iter: 977 loss: 2.23738789e-05
Iter: 978 loss: 2.22668677e-05
Iter: 979 loss: 2.22382059e-05
Iter: 980 loss: 2.23142233e-05
Iter: 981 loss: 2.22285071e-05
Iter: 982 loss: 2.22033268e-05
Iter: 983 loss: 2.23122552e-05
Iter: 984 loss: 2.21981209e-05
Iter: 985 loss: 2.21724076e-05
Iter: 986 loss: 2.22757444e-05
Iter: 987 loss: 2.21666069e-05
Iter: 988 loss: 2.21479568e-05
Iter: 989 loss: 2.23009629e-05
Iter: 990 loss: 2.21467744e-05
Iter: 991 loss: 2.21304817e-05
Iter: 992 loss: 2.21268856e-05
Iter: 993 loss: 2.21162663e-05
Iter: 994 loss: 2.20930015e-05
Iter: 995 loss: 2.22025301e-05
Iter: 996 loss: 2.20887487e-05
Iter: 997 loss: 2.20688053e-05
Iter: 998 loss: 2.2092976e-05
Iter: 999 loss: 2.20584297e-05
Iter: 1000 loss: 2.20335969e-05
Iter: 1001 loss: 2.20567599e-05
Iter: 1002 loss: 2.20193615e-05
Iter: 1003 loss: 2.19933863e-05
Iter: 1004 loss: 2.20290494e-05
Iter: 1005 loss: 2.1980597e-05
Iter: 1006 loss: 2.19537906e-05
Iter: 1007 loss: 2.22444232e-05
Iter: 1008 loss: 2.19531648e-05
Iter: 1009 loss: 2.19335416e-05
Iter: 1010 loss: 2.19260601e-05
Iter: 1011 loss: 2.19153935e-05
Iter: 1012 loss: 2.18867572e-05
Iter: 1013 loss: 2.18981986e-05
Iter: 1014 loss: 2.18669193e-05
Iter: 1015 loss: 2.18351324e-05
Iter: 1016 loss: 2.21272912e-05
Iter: 1017 loss: 2.18336972e-05
Iter: 1018 loss: 2.18100795e-05
Iter: 1019 loss: 2.17946836e-05
Iter: 1020 loss: 2.17856832e-05
Iter: 1021 loss: 2.17706383e-05
Iter: 1022 loss: 2.17661709e-05
Iter: 1023 loss: 2.17491797e-05
Iter: 1024 loss: 2.1756694e-05
Iter: 1025 loss: 2.17376291e-05
Iter: 1026 loss: 2.17162269e-05
Iter: 1027 loss: 2.18114692e-05
Iter: 1028 loss: 2.17120105e-05
Iter: 1029 loss: 2.16934131e-05
Iter: 1030 loss: 2.17110246e-05
Iter: 1031 loss: 2.16826593e-05
Iter: 1032 loss: 2.16613444e-05
Iter: 1033 loss: 2.17476372e-05
Iter: 1034 loss: 2.16566332e-05
Iter: 1035 loss: 2.16361041e-05
Iter: 1036 loss: 2.16323861e-05
Iter: 1037 loss: 2.16184981e-05
Iter: 1038 loss: 2.15930158e-05
Iter: 1039 loss: 2.16721437e-05
Iter: 1040 loss: 2.15856562e-05
Iter: 1041 loss: 2.15599266e-05
Iter: 1042 loss: 2.16828266e-05
Iter: 1043 loss: 2.15553173e-05
Iter: 1044 loss: 2.1534046e-05
Iter: 1045 loss: 2.16039243e-05
Iter: 1046 loss: 2.15281689e-05
Iter: 1047 loss: 2.15059881e-05
Iter: 1048 loss: 2.15006366e-05
Iter: 1049 loss: 2.14865522e-05
Iter: 1050 loss: 2.1458407e-05
Iter: 1051 loss: 2.1523103e-05
Iter: 1052 loss: 2.14479369e-05
Iter: 1053 loss: 2.14171378e-05
Iter: 1054 loss: 2.15399341e-05
Iter: 1055 loss: 2.14103093e-05
Iter: 1056 loss: 2.13823332e-05
Iter: 1057 loss: 2.1467069e-05
Iter: 1058 loss: 2.13739531e-05
Iter: 1059 loss: 2.1352962e-05
Iter: 1060 loss: 2.1352882e-05
Iter: 1061 loss: 2.13386647e-05
Iter: 1062 loss: 2.13433505e-05
Iter: 1063 loss: 2.13285166e-05
Iter: 1064 loss: 2.13093917e-05
Iter: 1065 loss: 2.13326821e-05
Iter: 1066 loss: 2.12993655e-05
Iter: 1067 loss: 2.12777104e-05
Iter: 1068 loss: 2.14060747e-05
Iter: 1069 loss: 2.12748673e-05
Iter: 1070 loss: 2.12575887e-05
Iter: 1071 loss: 2.12413397e-05
Iter: 1072 loss: 2.12372852e-05
Iter: 1073 loss: 2.12130253e-05
Iter: 1074 loss: 2.13969179e-05
Iter: 1075 loss: 2.12111372e-05
Iter: 1076 loss: 2.1187625e-05
Iter: 1077 loss: 2.11969455e-05
Iter: 1078 loss: 2.1171325e-05
Iter: 1079 loss: 2.11458646e-05
Iter: 1080 loss: 2.12517698e-05
Iter: 1081 loss: 2.11403312e-05
Iter: 1082 loss: 2.11215665e-05
Iter: 1083 loss: 2.12631512e-05
Iter: 1084 loss: 2.11202459e-05
Iter: 1085 loss: 2.11011957e-05
Iter: 1086 loss: 2.10743201e-05
Iter: 1087 loss: 2.10733524e-05
Iter: 1088 loss: 2.10441922e-05
Iter: 1089 loss: 2.11181377e-05
Iter: 1090 loss: 2.10340513e-05
Iter: 1091 loss: 2.10029484e-05
Iter: 1092 loss: 2.12286905e-05
Iter: 1093 loss: 2.10001381e-05
Iter: 1094 loss: 2.09814425e-05
Iter: 1095 loss: 2.11494735e-05
Iter: 1096 loss: 2.09805148e-05
Iter: 1097 loss: 2.09600075e-05
Iter: 1098 loss: 2.09724949e-05
Iter: 1099 loss: 2.09468417e-05
Iter: 1100 loss: 2.09301979e-05
Iter: 1101 loss: 2.09543141e-05
Iter: 1102 loss: 2.09221e-05
Iter: 1103 loss: 2.08995807e-05
Iter: 1104 loss: 2.09966529e-05
Iter: 1105 loss: 2.0894935e-05
Iter: 1106 loss: 2.08760757e-05
Iter: 1107 loss: 2.0879992e-05
Iter: 1108 loss: 2.08620113e-05
Iter: 1109 loss: 2.08384117e-05
Iter: 1110 loss: 2.08818674e-05
Iter: 1111 loss: 2.08282509e-05
Iter: 1112 loss: 2.08053461e-05
Iter: 1113 loss: 2.09890677e-05
Iter: 1114 loss: 2.08038473e-05
Iter: 1115 loss: 2.07852027e-05
Iter: 1116 loss: 2.078541e-05
Iter: 1117 loss: 2.07703288e-05
Iter: 1118 loss: 2.07469129e-05
Iter: 1119 loss: 2.08838628e-05
Iter: 1120 loss: 2.07438534e-05
Iter: 1121 loss: 2.0723106e-05
Iter: 1122 loss: 2.07630637e-05
Iter: 1123 loss: 2.07144221e-05
Iter: 1124 loss: 2.06942041e-05
Iter: 1125 loss: 2.07581652e-05
Iter: 1126 loss: 2.06883906e-05
Iter: 1127 loss: 2.06672521e-05
Iter: 1128 loss: 2.06933764e-05
Iter: 1129 loss: 2.06563091e-05
Iter: 1130 loss: 2.06333661e-05
Iter: 1131 loss: 2.06981385e-05
Iter: 1132 loss: 2.06260847e-05
Iter: 1133 loss: 2.06075965e-05
Iter: 1134 loss: 2.06075147e-05
Iter: 1135 loss: 2.05941415e-05
Iter: 1136 loss: 2.0576721e-05
Iter: 1137 loss: 2.05756423e-05
Iter: 1138 loss: 2.05533834e-05
Iter: 1139 loss: 2.06110508e-05
Iter: 1140 loss: 2.05458309e-05
Iter: 1141 loss: 2.05265387e-05
Iter: 1142 loss: 2.07472949e-05
Iter: 1143 loss: 2.05262731e-05
Iter: 1144 loss: 2.05109754e-05
Iter: 1145 loss: 2.048641e-05
Iter: 1146 loss: 2.04862154e-05
Iter: 1147 loss: 2.04600947e-05
Iter: 1148 loss: 2.05588767e-05
Iter: 1149 loss: 2.04538192e-05
Iter: 1150 loss: 2.04284479e-05
Iter: 1151 loss: 2.05367905e-05
Iter: 1152 loss: 2.04230564e-05
Iter: 1153 loss: 2.04020635e-05
Iter: 1154 loss: 2.05179967e-05
Iter: 1155 loss: 2.03989857e-05
Iter: 1156 loss: 2.0380041e-05
Iter: 1157 loss: 2.04031949e-05
Iter: 1158 loss: 2.03700547e-05
Iter: 1159 loss: 2.03494892e-05
Iter: 1160 loss: 2.03694326e-05
Iter: 1161 loss: 2.03377913e-05
Iter: 1162 loss: 2.031232e-05
Iter: 1163 loss: 2.04771568e-05
Iter: 1164 loss: 2.03096461e-05
Iter: 1165 loss: 2.02908886e-05
Iter: 1166 loss: 2.03303352e-05
Iter: 1167 loss: 2.02834926e-05
Iter: 1168 loss: 2.02653573e-05
Iter: 1169 loss: 2.04015705e-05
Iter: 1170 loss: 2.02638585e-05
Iter: 1171 loss: 2.0245734e-05
Iter: 1172 loss: 2.02650826e-05
Iter: 1173 loss: 2.02357332e-05
Iter: 1174 loss: 2.02199444e-05
Iter: 1175 loss: 2.02179981e-05
Iter: 1176 loss: 2.02066203e-05
Iter: 1177 loss: 2.01849653e-05
Iter: 1178 loss: 2.02851552e-05
Iter: 1179 loss: 2.01807889e-05
Iter: 1180 loss: 2.01575749e-05
Iter: 1181 loss: 2.02540832e-05
Iter: 1182 loss: 2.01525618e-05
Iter: 1183 loss: 2.01351013e-05
Iter: 1184 loss: 2.01314451e-05
Iter: 1185 loss: 2.01200128e-05
Iter: 1186 loss: 2.00949307e-05
Iter: 1187 loss: 2.01790062e-05
Iter: 1188 loss: 2.00881841e-05
Iter: 1189 loss: 2.0063515e-05
Iter: 1190 loss: 2.01034454e-05
Iter: 1191 loss: 2.00522245e-05
Iter: 1192 loss: 2.0028765e-05
Iter: 1193 loss: 2.00928353e-05
Iter: 1194 loss: 2.00211962e-05
Iter: 1195 loss: 1.99975184e-05
Iter: 1196 loss: 2.0237967e-05
Iter: 1197 loss: 1.99969545e-05
Iter: 1198 loss: 1.99823335e-05
Iter: 1199 loss: 1.99592087e-05
Iter: 1200 loss: 1.99589376e-05
Iter: 1201 loss: 1.99386341e-05
Iter: 1202 loss: 1.99385795e-05
Iter: 1203 loss: 1.99205388e-05
Iter: 1204 loss: 1.99758e-05
Iter: 1205 loss: 1.99151618e-05
Iter: 1206 loss: 1.98944763e-05
Iter: 1207 loss: 1.99265232e-05
Iter: 1208 loss: 1.98848029e-05
Iter: 1209 loss: 1.98682901e-05
Iter: 1210 loss: 1.99108836e-05
Iter: 1211 loss: 1.98626694e-05
Iter: 1212 loss: 1.98444122e-05
Iter: 1213 loss: 1.98472189e-05
Iter: 1214 loss: 1.98305843e-05
Iter: 1215 loss: 1.98120797e-05
Iter: 1216 loss: 1.99955757e-05
Iter: 1217 loss: 1.9811434e-05
Iter: 1218 loss: 1.97933405e-05
Iter: 1219 loss: 1.98060297e-05
Iter: 1220 loss: 1.97820573e-05
Iter: 1221 loss: 1.97629561e-05
Iter: 1222 loss: 1.979025e-05
Iter: 1223 loss: 1.97536392e-05
Iter: 1224 loss: 1.97306326e-05
Iter: 1225 loss: 1.97289664e-05
Iter: 1226 loss: 1.97116315e-05
Iter: 1227 loss: 1.96848523e-05
Iter: 1228 loss: 1.98370817e-05
Iter: 1229 loss: 1.96812107e-05
Iter: 1230 loss: 1.96596557e-05
Iter: 1231 loss: 1.98446305e-05
Iter: 1232 loss: 1.96584733e-05
Iter: 1233 loss: 1.96403016e-05
Iter: 1234 loss: 1.96535893e-05
Iter: 1235 loss: 1.96290475e-05
Iter: 1236 loss: 1.96071906e-05
Iter: 1237 loss: 1.96889487e-05
Iter: 1238 loss: 1.96018336e-05
Iter: 1239 loss: 1.95841967e-05
Iter: 1240 loss: 1.97091304e-05
Iter: 1241 loss: 1.95825687e-05
Iter: 1242 loss: 1.95664434e-05
Iter: 1243 loss: 1.96150304e-05
Iter: 1244 loss: 1.95616176e-05
Iter: 1245 loss: 1.95477696e-05
Iter: 1246 loss: 1.95312823e-05
Iter: 1247 loss: 1.95295597e-05
Iter: 1248 loss: 1.9508796e-05
Iter: 1249 loss: 1.96261353e-05
Iter: 1250 loss: 1.95058892e-05
Iter: 1251 loss: 1.94861386e-05
Iter: 1252 loss: 1.95723223e-05
Iter: 1253 loss: 1.9482206e-05
Iter: 1254 loss: 1.94644854e-05
Iter: 1255 loss: 1.94974673e-05
Iter: 1256 loss: 1.94569966e-05
Iter: 1257 loss: 1.94376e-05
Iter: 1258 loss: 1.94948643e-05
Iter: 1259 loss: 1.94315435e-05
Iter: 1260 loss: 1.94122476e-05
Iter: 1261 loss: 1.94236745e-05
Iter: 1262 loss: 1.93997912e-05
Iter: 1263 loss: 1.93778324e-05
Iter: 1264 loss: 1.94062195e-05
Iter: 1265 loss: 1.93666147e-05
Iter: 1266 loss: 1.9344e-05
Iter: 1267 loss: 1.9473533e-05
Iter: 1268 loss: 1.93409833e-05
Iter: 1269 loss: 1.93206361e-05
Iter: 1270 loss: 1.93361921e-05
Iter: 1271 loss: 1.93084197e-05
Iter: 1272 loss: 1.92882253e-05
Iter: 1273 loss: 1.9555333e-05
Iter: 1274 loss: 1.92881635e-05
Iter: 1275 loss: 1.9271034e-05
Iter: 1276 loss: 1.92866501e-05
Iter: 1277 loss: 1.92611151e-05
Iter: 1278 loss: 1.92422958e-05
Iter: 1279 loss: 1.94054e-05
Iter: 1280 loss: 1.92413572e-05
Iter: 1281 loss: 1.92264415e-05
Iter: 1282 loss: 1.92133793e-05
Iter: 1283 loss: 1.92095376e-05
Iter: 1284 loss: 1.91897961e-05
Iter: 1285 loss: 1.92129846e-05
Iter: 1286 loss: 1.91794734e-05
Iter: 1287 loss: 1.91593354e-05
Iter: 1288 loss: 1.93241522e-05
Iter: 1289 loss: 1.91581803e-05
Iter: 1290 loss: 1.91418112e-05
Iter: 1291 loss: 1.9209012e-05
Iter: 1292 loss: 1.91382678e-05
Iter: 1293 loss: 1.91206746e-05
Iter: 1294 loss: 1.91170748e-05
Iter: 1295 loss: 1.91054351e-05
Iter: 1296 loss: 1.90850769e-05
Iter: 1297 loss: 1.91893796e-05
Iter: 1298 loss: 1.90817136e-05
Iter: 1299 loss: 1.9061792e-05
Iter: 1300 loss: 1.9074243e-05
Iter: 1301 loss: 1.90490009e-05
Iter: 1302 loss: 1.902853e-05
Iter: 1303 loss: 1.9149973e-05
Iter: 1304 loss: 1.90259161e-05
Iter: 1305 loss: 1.90065257e-05
Iter: 1306 loss: 1.90212613e-05
Iter: 1307 loss: 1.89947659e-05
Iter: 1308 loss: 1.89769362e-05
Iter: 1309 loss: 1.90185729e-05
Iter: 1310 loss: 1.89704351e-05
Iter: 1311 loss: 1.89526836e-05
Iter: 1312 loss: 1.91461768e-05
Iter: 1313 loss: 1.8952238e-05
Iter: 1314 loss: 1.89354305e-05
Iter: 1315 loss: 1.89615e-05
Iter: 1316 loss: 1.89273596e-05
Iter: 1317 loss: 1.89125021e-05
Iter: 1318 loss: 1.89482344e-05
Iter: 1319 loss: 1.89071252e-05
Iter: 1320 loss: 1.88891081e-05
Iter: 1321 loss: 1.8895e-05
Iter: 1322 loss: 1.88763152e-05
Iter: 1323 loss: 1.88570611e-05
Iter: 1324 loss: 1.89129823e-05
Iter: 1325 loss: 1.88510385e-05
Iter: 1326 loss: 1.88318118e-05
Iter: 1327 loss: 1.88773811e-05
Iter: 1328 loss: 1.8824896e-05
Iter: 1329 loss: 1.88094455e-05
Iter: 1330 loss: 1.90416431e-05
Iter: 1331 loss: 1.880942e-05
Iter: 1332 loss: 1.87989681e-05
Iter: 1333 loss: 1.87815276e-05
Iter: 1334 loss: 1.87814348e-05
Iter: 1335 loss: 1.87594087e-05
Iter: 1336 loss: 1.88440863e-05
Iter: 1337 loss: 1.875417e-05
Iter: 1338 loss: 1.87346886e-05
Iter: 1339 loss: 1.87759961e-05
Iter: 1340 loss: 1.87270707e-05
Iter: 1341 loss: 1.87065925e-05
Iter: 1342 loss: 1.88360573e-05
Iter: 1343 loss: 1.87043115e-05
Iter: 1344 loss: 1.86857651e-05
Iter: 1345 loss: 1.86907691e-05
Iter: 1346 loss: 1.86723082e-05
Iter: 1347 loss: 1.86555153e-05
Iter: 1348 loss: 1.88011509e-05
Iter: 1349 loss: 1.86547059e-05
Iter: 1350 loss: 1.86356101e-05
Iter: 1351 loss: 1.86763391e-05
Iter: 1352 loss: 1.86281177e-05
Iter: 1353 loss: 1.86142533e-05
Iter: 1354 loss: 1.86440684e-05
Iter: 1355 loss: 1.86087946e-05
Iter: 1356 loss: 1.85928457e-05
Iter: 1357 loss: 1.85947247e-05
Iter: 1358 loss: 1.85806348e-05
Iter: 1359 loss: 1.85633107e-05
Iter: 1360 loss: 1.87316764e-05
Iter: 1361 loss: 1.85626905e-05
Iter: 1362 loss: 1.8549028e-05
Iter: 1363 loss: 1.85402823e-05
Iter: 1364 loss: 1.85349199e-05
Iter: 1365 loss: 1.85171484e-05
Iter: 1366 loss: 1.86522e-05
Iter: 1367 loss: 1.85158024e-05
Iter: 1368 loss: 1.84968412e-05
Iter: 1369 loss: 1.85283279e-05
Iter: 1370 loss: 1.84882447e-05
Iter: 1371 loss: 1.84717283e-05
Iter: 1372 loss: 1.84861383e-05
Iter: 1373 loss: 1.84620512e-05
Iter: 1374 loss: 1.84413184e-05
Iter: 1375 loss: 1.84754226e-05
Iter: 1376 loss: 1.84318451e-05
Iter: 1377 loss: 1.84121272e-05
Iter: 1378 loss: 1.84610035e-05
Iter: 1379 loss: 1.8405095e-05
Iter: 1380 loss: 1.83837019e-05
Iter: 1381 loss: 1.84951441e-05
Iter: 1382 loss: 1.83802385e-05
Iter: 1383 loss: 1.83637676e-05
Iter: 1384 loss: 1.84838427e-05
Iter: 1385 loss: 1.83623615e-05
Iter: 1386 loss: 1.83473894e-05
Iter: 1387 loss: 1.83885822e-05
Iter: 1388 loss: 1.83424745e-05
Iter: 1389 loss: 1.83268385e-05
Iter: 1390 loss: 1.83484408e-05
Iter: 1391 loss: 1.83190677e-05
Iter: 1392 loss: 1.83033808e-05
Iter: 1393 loss: 1.82927542e-05
Iter: 1394 loss: 1.82869553e-05
Iter: 1395 loss: 1.82657586e-05
Iter: 1396 loss: 1.84349519e-05
Iter: 1397 loss: 1.82643234e-05
Iter: 1398 loss: 1.82461554e-05
Iter: 1399 loss: 1.83187658e-05
Iter: 1400 loss: 1.82421281e-05
Iter: 1401 loss: 1.82274143e-05
Iter: 1402 loss: 1.82261756e-05
Iter: 1403 loss: 1.82154199e-05
Iter: 1404 loss: 1.81965861e-05
Iter: 1405 loss: 1.84141336e-05
Iter: 1406 loss: 1.81963842e-05
Iter: 1407 loss: 1.81825708e-05
Iter: 1408 loss: 1.81794931e-05
Iter: 1409 loss: 1.81705036e-05
Iter: 1410 loss: 1.81516698e-05
Iter: 1411 loss: 1.81950745e-05
Iter: 1412 loss: 1.81446048e-05
Iter: 1413 loss: 1.81266878e-05
Iter: 1414 loss: 1.81643081e-05
Iter: 1415 loss: 1.81195646e-05
Iter: 1416 loss: 1.80986863e-05
Iter: 1417 loss: 1.81511405e-05
Iter: 1418 loss: 1.80913612e-05
Iter: 1419 loss: 1.80743846e-05
Iter: 1420 loss: 1.8152532e-05
Iter: 1421 loss: 1.80712341e-05
Iter: 1422 loss: 1.80519073e-05
Iter: 1423 loss: 1.8167173e-05
Iter: 1424 loss: 1.80494644e-05
Iter: 1425 loss: 1.80354e-05
Iter: 1426 loss: 1.80297466e-05
Iter: 1427 loss: 1.80223469e-05
Iter: 1428 loss: 1.80035022e-05
Iter: 1429 loss: 1.80730731e-05
Iter: 1430 loss: 1.79989729e-05
Iter: 1431 loss: 1.79817653e-05
Iter: 1432 loss: 1.80305287e-05
Iter: 1433 loss: 1.79764138e-05
Iter: 1434 loss: 1.79599483e-05
Iter: 1435 loss: 1.79784147e-05
Iter: 1436 loss: 1.79509989e-05
Iter: 1437 loss: 1.79323906e-05
Iter: 1438 loss: 1.80599582e-05
Iter: 1439 loss: 1.79305789e-05
Iter: 1440 loss: 1.79152867e-05
Iter: 1441 loss: 1.79469735e-05
Iter: 1442 loss: 1.79092367e-05
Iter: 1443 loss: 1.78939e-05
Iter: 1444 loss: 1.79311701e-05
Iter: 1445 loss: 1.78884438e-05
Iter: 1446 loss: 1.78725859e-05
Iter: 1447 loss: 1.79408489e-05
Iter: 1448 loss: 1.78693044e-05
Iter: 1449 loss: 1.78565097e-05
Iter: 1450 loss: 1.784501e-05
Iter: 1451 loss: 1.78417213e-05
Iter: 1452 loss: 1.78212395e-05
Iter: 1453 loss: 1.79138915e-05
Iter: 1454 loss: 1.78173541e-05
Iter: 1455 loss: 1.77989932e-05
Iter: 1456 loss: 1.78343835e-05
Iter: 1457 loss: 1.77912734e-05
Iter: 1458 loss: 1.7782153e-05
Iter: 1459 loss: 1.77800175e-05
Iter: 1460 loss: 1.77713464e-05
Iter: 1461 loss: 1.77514612e-05
Iter: 1462 loss: 1.80060233e-05
Iter: 1463 loss: 1.77500624e-05
Iter: 1464 loss: 1.7733375e-05
Iter: 1465 loss: 1.78609098e-05
Iter: 1466 loss: 1.77321053e-05
Iter: 1467 loss: 1.77149e-05
Iter: 1468 loss: 1.77290894e-05
Iter: 1469 loss: 1.77045931e-05
Iter: 1470 loss: 1.76894846e-05
Iter: 1471 loss: 1.78163136e-05
Iter: 1472 loss: 1.76886115e-05
Iter: 1473 loss: 1.76740723e-05
Iter: 1474 loss: 1.76690592e-05
Iter: 1475 loss: 1.76608082e-05
Iter: 1476 loss: 1.76436e-05
Iter: 1477 loss: 1.78463943e-05
Iter: 1478 loss: 1.76433641e-05
Iter: 1479 loss: 1.76307094e-05
Iter: 1480 loss: 1.76342146e-05
Iter: 1481 loss: 1.76215908e-05
Iter: 1482 loss: 1.76048961e-05
Iter: 1483 loss: 1.76522844e-05
Iter: 1484 loss: 1.75995629e-05
Iter: 1485 loss: 1.75834157e-05
Iter: 1486 loss: 1.76366666e-05
Iter: 1487 loss: 1.75789246e-05
Iter: 1488 loss: 1.75635541e-05
Iter: 1489 loss: 1.75775949e-05
Iter: 1490 loss: 1.75546011e-05
Iter: 1491 loss: 1.75371279e-05
Iter: 1492 loss: 1.75820769e-05
Iter: 1493 loss: 1.75312198e-05
Iter: 1494 loss: 1.75174737e-05
Iter: 1495 loss: 1.76735102e-05
Iter: 1496 loss: 1.75172318e-05
Iter: 1497 loss: 1.75031437e-05
Iter: 1498 loss: 1.75185814e-05
Iter: 1499 loss: 1.74954039e-05
Iter: 1500 loss: 1.7482007e-05
Iter: 1501 loss: 1.74767738e-05
Iter: 1502 loss: 1.74695779e-05
Iter: 1503 loss: 1.74501565e-05
Iter: 1504 loss: 1.74866746e-05
Iter: 1505 loss: 1.74419856e-05
Iter: 1506 loss: 1.74224697e-05
Iter: 1507 loss: 1.74793731e-05
Iter: 1508 loss: 1.74164015e-05
Iter: 1509 loss: 1.73965673e-05
Iter: 1510 loss: 1.76022968e-05
Iter: 1511 loss: 1.73960543e-05
Iter: 1512 loss: 1.73839398e-05
Iter: 1513 loss: 1.73860208e-05
Iter: 1514 loss: 1.73747976e-05
Iter: 1515 loss: 1.73590852e-05
Iter: 1516 loss: 1.7475506e-05
Iter: 1517 loss: 1.73578956e-05
Iter: 1518 loss: 1.7345079e-05
Iter: 1519 loss: 1.73391745e-05
Iter: 1520 loss: 1.73329208e-05
Iter: 1521 loss: 1.73170702e-05
Iter: 1522 loss: 1.74180113e-05
Iter: 1523 loss: 1.73153348e-05
Iter: 1524 loss: 1.72999025e-05
Iter: 1525 loss: 1.73056e-05
Iter: 1526 loss: 1.72891941e-05
Iter: 1527 loss: 1.72711716e-05
Iter: 1528 loss: 1.73575663e-05
Iter: 1529 loss: 1.72679e-05
Iter: 1530 loss: 1.72521613e-05
Iter: 1531 loss: 1.73202643e-05
Iter: 1532 loss: 1.72490181e-05
Iter: 1533 loss: 1.72349028e-05
Iter: 1534 loss: 1.73426033e-05
Iter: 1535 loss: 1.72338023e-05
Iter: 1536 loss: 1.72244236e-05
Iter: 1537 loss: 1.7211878e-05
Iter: 1538 loss: 1.72112377e-05
Iter: 1539 loss: 1.71938627e-05
Iter: 1540 loss: 1.72349919e-05
Iter: 1541 loss: 1.71874563e-05
Iter: 1542 loss: 1.71691463e-05
Iter: 1543 loss: 1.72021973e-05
Iter: 1544 loss: 1.71612046e-05
Iter: 1545 loss: 1.7142338e-05
Iter: 1546 loss: 1.71906395e-05
Iter: 1547 loss: 1.7135786e-05
Iter: 1548 loss: 1.71196316e-05
Iter: 1549 loss: 1.7349661e-05
Iter: 1550 loss: 1.71194697e-05
Iter: 1551 loss: 1.71078136e-05
Iter: 1552 loss: 1.7117909e-05
Iter: 1553 loss: 1.71009e-05
Iter: 1554 loss: 1.70869098e-05
Iter: 1555 loss: 1.71020729e-05
Iter: 1556 loss: 1.70791718e-05
Iter: 1557 loss: 1.70644053e-05
Iter: 1558 loss: 1.71769752e-05
Iter: 1559 loss: 1.70633339e-05
Iter: 1560 loss: 1.70504427e-05
Iter: 1561 loss: 1.70393469e-05
Iter: 1562 loss: 1.70359126e-05
Iter: 1563 loss: 1.70160893e-05
Iter: 1564 loss: 1.70802305e-05
Iter: 1565 loss: 1.7010414e-05
Iter: 1566 loss: 1.6997843e-05
Iter: 1567 loss: 1.69977156e-05
Iter: 1568 loss: 1.69869709e-05
Iter: 1569 loss: 1.6994527e-05
Iter: 1570 loss: 1.69802843e-05
Iter: 1571 loss: 1.69670529e-05
Iter: 1572 loss: 1.69895939e-05
Iter: 1573 loss: 1.69610757e-05
Iter: 1574 loss: 1.6947055e-05
Iter: 1575 loss: 1.69518717e-05
Iter: 1576 loss: 1.69371706e-05
Iter: 1577 loss: 1.6919761e-05
Iter: 1578 loss: 1.69423183e-05
Iter: 1579 loss: 1.6910828e-05
Iter: 1580 loss: 1.68898659e-05
Iter: 1581 loss: 1.6989361e-05
Iter: 1582 loss: 1.68860533e-05
Iter: 1583 loss: 1.68708e-05
Iter: 1584 loss: 1.6938553e-05
Iter: 1585 loss: 1.68677907e-05
Iter: 1586 loss: 1.68537299e-05
Iter: 1587 loss: 1.69175546e-05
Iter: 1588 loss: 1.68510196e-05
Iter: 1589 loss: 1.68365477e-05
Iter: 1590 loss: 1.68607949e-05
Iter: 1591 loss: 1.68300539e-05
Iter: 1592 loss: 1.68149527e-05
Iter: 1593 loss: 1.68206116e-05
Iter: 1594 loss: 1.68044371e-05
Iter: 1595 loss: 1.67904745e-05
Iter: 1596 loss: 1.69103023e-05
Iter: 1597 loss: 1.67896396e-05
Iter: 1598 loss: 1.67754806e-05
Iter: 1599 loss: 1.67804174e-05
Iter: 1600 loss: 1.67655089e-05
Iter: 1601 loss: 1.6751037e-05
Iter: 1602 loss: 1.68495244e-05
Iter: 1603 loss: 1.67496728e-05
Iter: 1604 loss: 1.67382623e-05
Iter: 1605 loss: 1.68093502e-05
Iter: 1606 loss: 1.67369381e-05
Iter: 1607 loss: 1.67252638e-05
Iter: 1608 loss: 1.67176968e-05
Iter: 1609 loss: 1.67132384e-05
Iter: 1610 loss: 1.66990139e-05
Iter: 1611 loss: 1.66909485e-05
Iter: 1612 loss: 1.66847676e-05
Iter: 1613 loss: 1.6665881e-05
Iter: 1614 loss: 1.6884318e-05
Iter: 1615 loss: 1.66656428e-05
Iter: 1616 loss: 1.66514437e-05
Iter: 1617 loss: 1.6678272e-05
Iter: 1618 loss: 1.66454374e-05
Iter: 1619 loss: 1.66286882e-05
Iter: 1620 loss: 1.66569152e-05
Iter: 1621 loss: 1.66211e-05
Iter: 1622 loss: 1.66066e-05
Iter: 1623 loss: 1.66430145e-05
Iter: 1624 loss: 1.66016143e-05
Iter: 1625 loss: 1.6586986e-05
Iter: 1626 loss: 1.67584913e-05
Iter: 1627 loss: 1.65867641e-05
Iter: 1628 loss: 1.65780912e-05
Iter: 1629 loss: 1.65643942e-05
Iter: 1630 loss: 1.65641977e-05
Iter: 1631 loss: 1.65463844e-05
Iter: 1632 loss: 1.66604623e-05
Iter: 1633 loss: 1.65443962e-05
Iter: 1634 loss: 1.65308811e-05
Iter: 1635 loss: 1.65590336e-05
Iter: 1636 loss: 1.65256242e-05
Iter: 1637 loss: 1.65116526e-05
Iter: 1638 loss: 1.66080517e-05
Iter: 1639 loss: 1.6510312e-05
Iter: 1640 loss: 1.64981029e-05
Iter: 1641 loss: 1.65269321e-05
Iter: 1642 loss: 1.64935154e-05
Iter: 1643 loss: 1.64801295e-05
Iter: 1644 loss: 1.65175188e-05
Iter: 1645 loss: 1.64757839e-05
Iter: 1646 loss: 1.64642734e-05
Iter: 1647 loss: 1.64541361e-05
Iter: 1648 loss: 1.64510457e-05
Iter: 1649 loss: 1.64327776e-05
Iter: 1650 loss: 1.65133315e-05
Iter: 1651 loss: 1.6429145e-05
Iter: 1652 loss: 1.64139383e-05
Iter: 1653 loss: 1.64330486e-05
Iter: 1654 loss: 1.6406113e-05
Iter: 1655 loss: 1.63905697e-05
Iter: 1656 loss: 1.64541489e-05
Iter: 1657 loss: 1.63871682e-05
Iter: 1658 loss: 1.63707882e-05
Iter: 1659 loss: 1.64470803e-05
Iter: 1660 loss: 1.6367736e-05
Iter: 1661 loss: 1.63531804e-05
Iter: 1662 loss: 1.63956174e-05
Iter: 1663 loss: 1.63486657e-05
Iter: 1664 loss: 1.633395e-05
Iter: 1665 loss: 1.63746336e-05
Iter: 1666 loss: 1.6329248e-05
Iter: 1667 loss: 1.63137556e-05
Iter: 1668 loss: 1.63397326e-05
Iter: 1669 loss: 1.63067816e-05
Iter: 1670 loss: 1.62916185e-05
Iter: 1671 loss: 1.62968427e-05
Iter: 1672 loss: 1.62810029e-05
Iter: 1673 loss: 1.62669421e-05
Iter: 1674 loss: 1.64283283e-05
Iter: 1675 loss: 1.62666674e-05
Iter: 1676 loss: 1.62537763e-05
Iter: 1677 loss: 1.63108034e-05
Iter: 1678 loss: 1.62511642e-05
Iter: 1679 loss: 1.62406177e-05
Iter: 1680 loss: 1.62404394e-05
Iter: 1681 loss: 1.62321539e-05
Iter: 1682 loss: 1.62173492e-05
Iter: 1683 loss: 1.62785145e-05
Iter: 1684 loss: 1.62141696e-05
Iter: 1685 loss: 1.62011675e-05
Iter: 1686 loss: 1.61923126e-05
Iter: 1687 loss: 1.61875523e-05
Iter: 1688 loss: 1.61705211e-05
Iter: 1689 loss: 1.63033037e-05
Iter: 1690 loss: 1.61693188e-05
Iter: 1691 loss: 1.61548323e-05
Iter: 1692 loss: 1.61510452e-05
Iter: 1693 loss: 1.61420376e-05
Iter: 1694 loss: 1.61254884e-05
Iter: 1695 loss: 1.63006971e-05
Iter: 1696 loss: 1.61249955e-05
Iter: 1697 loss: 1.61137141e-05
Iter: 1698 loss: 1.61336648e-05
Iter: 1699 loss: 1.61087264e-05
Iter: 1700 loss: 1.60941017e-05
Iter: 1701 loss: 1.61770149e-05
Iter: 1702 loss: 1.6092119e-05
Iter: 1703 loss: 1.60817108e-05
Iter: 1704 loss: 1.60719501e-05
Iter: 1705 loss: 1.60694653e-05
Iter: 1706 loss: 1.60523286e-05
Iter: 1707 loss: 1.61609478e-05
Iter: 1708 loss: 1.60504496e-05
Iter: 1709 loss: 1.60364198e-05
Iter: 1710 loss: 1.60760155e-05
Iter: 1711 loss: 1.60319069e-05
Iter: 1712 loss: 1.60193067e-05
Iter: 1713 loss: 1.61275657e-05
Iter: 1714 loss: 1.60186319e-05
Iter: 1715 loss: 1.60082127e-05
Iter: 1716 loss: 1.60191303e-05
Iter: 1717 loss: 1.60026102e-05
Iter: 1718 loss: 1.59908304e-05
Iter: 1719 loss: 1.59878637e-05
Iter: 1720 loss: 1.59805422e-05
Iter: 1721 loss: 1.59658375e-05
Iter: 1722 loss: 1.60155287e-05
Iter: 1723 loss: 1.59619012e-05
Iter: 1724 loss: 1.59455121e-05
Iter: 1725 loss: 1.60206801e-05
Iter: 1726 loss: 1.59423562e-05
Iter: 1727 loss: 1.59300962e-05
Iter: 1728 loss: 1.59222873e-05
Iter: 1729 loss: 1.59175052e-05
Iter: 1730 loss: 1.58995772e-05
Iter: 1731 loss: 1.5960708e-05
Iter: 1732 loss: 1.58948242e-05
Iter: 1733 loss: 1.58796065e-05
Iter: 1734 loss: 1.59683223e-05
Iter: 1735 loss: 1.58776074e-05
Iter: 1736 loss: 1.58636158e-05
Iter: 1737 loss: 1.59528372e-05
Iter: 1738 loss: 1.5862086e-05
Iter: 1739 loss: 1.58503935e-05
Iter: 1740 loss: 1.58578914e-05
Iter: 1741 loss: 1.58429393e-05
Iter: 1742 loss: 1.58290168e-05
Iter: 1743 loss: 1.58541334e-05
Iter: 1744 loss: 1.58230032e-05
Iter: 1745 loss: 1.58097464e-05
Iter: 1746 loss: 1.58677394e-05
Iter: 1747 loss: 1.58071452e-05
Iter: 1748 loss: 1.57963113e-05
Iter: 1749 loss: 1.59130086e-05
Iter: 1750 loss: 1.57960221e-05
Iter: 1751 loss: 1.57872983e-05
Iter: 1752 loss: 1.57815084e-05
Iter: 1753 loss: 1.57781433e-05
Iter: 1754 loss: 1.57664472e-05
Iter: 1755 loss: 1.58011135e-05
Iter: 1756 loss: 1.57628747e-05
Iter: 1757 loss: 1.57495779e-05
Iter: 1758 loss: 1.57666036e-05
Iter: 1759 loss: 1.57426875e-05
Iter: 1760 loss: 1.5729609e-05
Iter: 1761 loss: 1.57718641e-05
Iter: 1762 loss: 1.57257764e-05
Iter: 1763 loss: 1.57126669e-05
Iter: 1764 loss: 1.57591603e-05
Iter: 1765 loss: 1.57092745e-05
Iter: 1766 loss: 1.56982296e-05
Iter: 1767 loss: 1.57414725e-05
Iter: 1768 loss: 1.56956503e-05
Iter: 1769 loss: 1.56844944e-05
Iter: 1770 loss: 1.56818132e-05
Iter: 1771 loss: 1.567463e-05
Iter: 1772 loss: 1.5660311e-05
Iter: 1773 loss: 1.569085e-05
Iter: 1774 loss: 1.56547394e-05
Iter: 1775 loss: 1.56417882e-05
Iter: 1776 loss: 1.564179e-05
Iter: 1777 loss: 1.56317146e-05
Iter: 1778 loss: 1.56283422e-05
Iter: 1779 loss: 1.56225869e-05
Iter: 1780 loss: 1.56083479e-05
Iter: 1781 loss: 1.56254864e-05
Iter: 1782 loss: 1.56007445e-05
Iter: 1783 loss: 1.55927664e-05
Iter: 1784 loss: 1.55920534e-05
Iter: 1785 loss: 1.55833768e-05
Iter: 1786 loss: 1.55703619e-05
Iter: 1787 loss: 1.55700618e-05
Iter: 1788 loss: 1.55565285e-05
Iter: 1789 loss: 1.55943708e-05
Iter: 1790 loss: 1.55523121e-05
Iter: 1791 loss: 1.55397011e-05
Iter: 1792 loss: 1.56000751e-05
Iter: 1793 loss: 1.5537491e-05
Iter: 1794 loss: 1.55258313e-05
Iter: 1795 loss: 1.55422094e-05
Iter: 1796 loss: 1.55201233e-05
Iter: 1797 loss: 1.55062844e-05
Iter: 1798 loss: 1.55509842e-05
Iter: 1799 loss: 1.55023808e-05
Iter: 1800 loss: 1.54884856e-05
Iter: 1801 loss: 1.55013e-05
Iter: 1802 loss: 1.5480482e-05
Iter: 1803 loss: 1.54657246e-05
Iter: 1804 loss: 1.55588987e-05
Iter: 1805 loss: 1.54640657e-05
Iter: 1806 loss: 1.5450827e-05
Iter: 1807 loss: 1.54574591e-05
Iter: 1808 loss: 1.5442045e-05
Iter: 1809 loss: 1.54265781e-05
Iter: 1810 loss: 1.55316538e-05
Iter: 1811 loss: 1.5425092e-05
Iter: 1812 loss: 1.54153822e-05
Iter: 1813 loss: 1.54632198e-05
Iter: 1814 loss: 1.54136669e-05
Iter: 1815 loss: 1.54029385e-05
Iter: 1816 loss: 1.54092177e-05
Iter: 1817 loss: 1.53960791e-05
Iter: 1818 loss: 1.53857836e-05
Iter: 1819 loss: 1.54140362e-05
Iter: 1820 loss: 1.53823657e-05
Iter: 1821 loss: 1.53712426e-05
Iter: 1822 loss: 1.54632144e-05
Iter: 1823 loss: 1.53706151e-05
Iter: 1824 loss: 1.53616893e-05
Iter: 1825 loss: 1.53541496e-05
Iter: 1826 loss: 1.53517e-05
Iter: 1827 loss: 1.53388191e-05
Iter: 1828 loss: 1.53422679e-05
Iter: 1829 loss: 1.53295478e-05
Iter: 1830 loss: 1.53125911e-05
Iter: 1831 loss: 1.53767069e-05
Iter: 1832 loss: 1.53085894e-05
Iter: 1833 loss: 1.52943e-05
Iter: 1834 loss: 1.53475794e-05
Iter: 1835 loss: 1.52907523e-05
Iter: 1836 loss: 1.52748689e-05
Iter: 1837 loss: 1.53580258e-05
Iter: 1838 loss: 1.52723806e-05
Iter: 1839 loss: 1.52620123e-05
Iter: 1840 loss: 1.52524944e-05
Iter: 1841 loss: 1.525e-05
Iter: 1842 loss: 1.52348493e-05
Iter: 1843 loss: 1.53996825e-05
Iter: 1844 loss: 1.5234451e-05
Iter: 1845 loss: 1.52223229e-05
Iter: 1846 loss: 1.52366683e-05
Iter: 1847 loss: 1.521583e-05
Iter: 1848 loss: 1.52034754e-05
Iter: 1849 loss: 1.53106921e-05
Iter: 1850 loss: 1.52027478e-05
Iter: 1851 loss: 1.51937047e-05
Iter: 1852 loss: 1.52011253e-05
Iter: 1853 loss: 1.51882596e-05
Iter: 1854 loss: 1.5176568e-05
Iter: 1855 loss: 1.52148023e-05
Iter: 1856 loss: 1.51733848e-05
Iter: 1857 loss: 1.51633267e-05
Iter: 1858 loss: 1.52274806e-05
Iter: 1859 loss: 1.51621753e-05
Iter: 1860 loss: 1.51522026e-05
Iter: 1861 loss: 1.51489503e-05
Iter: 1862 loss: 1.51431532e-05
Iter: 1863 loss: 1.5129961e-05
Iter: 1864 loss: 1.5154732e-05
Iter: 1865 loss: 1.51244067e-05
Iter: 1866 loss: 1.51102277e-05
Iter: 1867 loss: 1.51131881e-05
Iter: 1868 loss: 1.50996857e-05
Iter: 1869 loss: 1.50861842e-05
Iter: 1870 loss: 1.5208816e-05
Iter: 1871 loss: 1.50856213e-05
Iter: 1872 loss: 1.50737214e-05
Iter: 1873 loss: 1.50733686e-05
Iter: 1874 loss: 1.50642e-05
Iter: 1875 loss: 1.50515034e-05
Iter: 1876 loss: 1.52090461e-05
Iter: 1877 loss: 1.50514297e-05
Iter: 1878 loss: 1.50401229e-05
Iter: 1879 loss: 1.50524156e-05
Iter: 1880 loss: 1.50339547e-05
Iter: 1881 loss: 1.50221094e-05
Iter: 1882 loss: 1.50225624e-05
Iter: 1883 loss: 1.50128144e-05
Iter: 1884 loss: 1.50004362e-05
Iter: 1885 loss: 1.50004698e-05
Iter: 1886 loss: 1.49896414e-05
Iter: 1887 loss: 1.49975212e-05
Iter: 1888 loss: 1.49829684e-05
Iter: 1889 loss: 1.49717025e-05
Iter: 1890 loss: 1.5005463e-05
Iter: 1891 loss: 1.4968241e-05
Iter: 1892 loss: 1.49560146e-05
Iter: 1893 loss: 1.5028947e-05
Iter: 1894 loss: 1.49543894e-05
Iter: 1895 loss: 1.4944847e-05
Iter: 1896 loss: 1.49448115e-05
Iter: 1897 loss: 1.49372445e-05
Iter: 1898 loss: 1.49254392e-05
Iter: 1899 loss: 1.49695406e-05
Iter: 1900 loss: 1.49225825e-05
Iter: 1901 loss: 1.49120879e-05
Iter: 1902 loss: 1.49161215e-05
Iter: 1903 loss: 1.49048374e-05
Iter: 1904 loss: 1.4891596e-05
Iter: 1905 loss: 1.49320995e-05
Iter: 1906 loss: 1.48877079e-05
Iter: 1907 loss: 1.48732815e-05
Iter: 1908 loss: 1.48976396e-05
Iter: 1909 loss: 1.48668114e-05
Iter: 1910 loss: 1.48523468e-05
Iter: 1911 loss: 1.49057032e-05
Iter: 1912 loss: 1.48487425e-05
Iter: 1913 loss: 1.48361569e-05
Iter: 1914 loss: 1.48773952e-05
Iter: 1915 loss: 1.48326926e-05
Iter: 1916 loss: 1.48201989e-05
Iter: 1917 loss: 1.4885738e-05
Iter: 1918 loss: 1.48182153e-05
Iter: 1919 loss: 1.48077706e-05
Iter: 1920 loss: 1.48307563e-05
Iter: 1921 loss: 1.48037561e-05
Iter: 1922 loss: 1.47929304e-05
Iter: 1923 loss: 1.48124263e-05
Iter: 1924 loss: 1.47882993e-05
Iter: 1925 loss: 1.47765204e-05
Iter: 1926 loss: 1.48719664e-05
Iter: 1927 loss: 1.47757273e-05
Iter: 1928 loss: 1.47675601e-05
Iter: 1929 loss: 1.47711626e-05
Iter: 1930 loss: 1.47619257e-05
Iter: 1931 loss: 1.47502033e-05
Iter: 1932 loss: 1.48148893e-05
Iter: 1933 loss: 1.47484552e-05
Iter: 1934 loss: 1.47395795e-05
Iter: 1935 loss: 1.47299952e-05
Iter: 1936 loss: 1.47285282e-05
Iter: 1937 loss: 1.4713507e-05
Iter: 1938 loss: 1.47479941e-05
Iter: 1939 loss: 1.47079882e-05
Iter: 1940 loss: 1.46952952e-05
Iter: 1941 loss: 1.47466635e-05
Iter: 1942 loss: 1.46925458e-05
Iter: 1943 loss: 1.46810853e-05
Iter: 1944 loss: 1.47537439e-05
Iter: 1945 loss: 1.46798102e-05
Iter: 1946 loss: 1.46695666e-05
Iter: 1947 loss: 1.46685743e-05
Iter: 1948 loss: 1.46609582e-05
Iter: 1949 loss: 1.46478797e-05
Iter: 1950 loss: 1.46710654e-05
Iter: 1951 loss: 1.46421826e-05
Iter: 1952 loss: 1.462826e-05
Iter: 1953 loss: 1.47284109e-05
Iter: 1954 loss: 1.46270286e-05
Iter: 1955 loss: 1.46151242e-05
Iter: 1956 loss: 1.46471521e-05
Iter: 1957 loss: 1.46110833e-05
Iter: 1958 loss: 1.46005768e-05
Iter: 1959 loss: 1.46698721e-05
Iter: 1960 loss: 1.45994163e-05
Iter: 1961 loss: 1.45910417e-05
Iter: 1962 loss: 1.4600977e-05
Iter: 1963 loss: 1.45865561e-05
Iter: 1964 loss: 1.45752965e-05
Iter: 1965 loss: 1.46225466e-05
Iter: 1966 loss: 1.45729718e-05
Iter: 1967 loss: 1.45643753e-05
Iter: 1968 loss: 1.45886906e-05
Iter: 1969 loss: 1.45617269e-05
Iter: 1970 loss: 1.45530494e-05
Iter: 1971 loss: 1.45669947e-05
Iter: 1972 loss: 1.4549074e-05
Iter: 1973 loss: 1.45393024e-05
Iter: 1974 loss: 1.45407394e-05
Iter: 1975 loss: 1.45318263e-05
Iter: 1976 loss: 1.45192644e-05
Iter: 1977 loss: 1.45421891e-05
Iter: 1978 loss: 1.45137674e-05
Iter: 1979 loss: 1.45003523e-05
Iter: 1980 loss: 1.45306949e-05
Iter: 1981 loss: 1.44953474e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script72
+ '[' -r STOP.script72 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi2/300_300_300_1 '
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi2.4 /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi2.4
+ date
Sat Oct 31 16:57:49 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi2/300_300_300_1 --function f1 --psi 2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 30 --learning_rate 0.001 --decay_rate 0.8 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f912451b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f912451b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9170027d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91245cc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91245ccd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9124309d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9124447d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91244539d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9124476378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91244767b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9124476f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91244d1d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91244c0048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91243b6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91244c0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9124346b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9124343378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9124352d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9124187b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91241877b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91241b7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91241c6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91240c39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91241b7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9124395598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91243959d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91240878c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9124408840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91244087b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9124404598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91047a0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9104746488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91047461e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91047f7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f91047ca8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f912401d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.037340906
test_loss: 0.040124588
train_loss: 0.02311163
test_loss: 0.025784498
train_loss: 0.019575078
test_loss: 0.02193844
train_loss: 0.017485935
test_loss: 0.020612871
train_loss: 0.016018491
test_loss: 0.020273088
train_loss: 0.016718436
test_loss: 0.020168558
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output72/f1_psi2_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output73/f1_psi2_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bc413a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bc403f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bc403fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bc403f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bc40050d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bc3fffea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b9baa5ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b9bad3598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b9bad3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b9baf7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b9baf71e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b9ba14730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b74020d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b607e8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b607e8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b9ba83268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b9ba83620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b607bb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b6073fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b6073fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b606fe2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b606fec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b606a2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b6068f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b6068f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b606767b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b60607840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b60607598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b6062b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b6062b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b6056aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b60531158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b60528b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b6050f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b604a8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b604e1158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000499126036
Iter: 2 loss: 0.000728563173
Iter: 3 loss: 0.000495780201
Iter: 4 loss: 0.000493064639
Iter: 5 loss: 0.000510403654
Iter: 6 loss: 0.000492770108
Iter: 7 loss: 0.000491132843
Iter: 8 loss: 0.000490406528
Iter: 9 loss: 0.000489577127
Iter: 10 loss: 0.000487077457
Iter: 11 loss: 0.00049653108
Iter: 12 loss: 0.000486478384
Iter: 13 loss: 0.000484432385
Iter: 14 loss: 0.000482672127
Iter: 15 loss: 0.000482114556
Iter: 16 loss: 0.000478435366
Iter: 17 loss: 0.000494658
Iter: 18 loss: 0.000477699388
Iter: 19 loss: 0.000474547502
Iter: 20 loss: 0.000473026623
Iter: 21 loss: 0.000471505336
Iter: 22 loss: 0.00046676991
Iter: 23 loss: 0.00048352743
Iter: 24 loss: 0.000465558667
Iter: 25 loss: 0.00046162511
Iter: 26 loss: 0.000481335563
Iter: 27 loss: 0.000460962503
Iter: 28 loss: 0.000457024667
Iter: 29 loss: 0.000459983217
Iter: 30 loss: 0.000454606226
Iter: 31 loss: 0.000450442545
Iter: 32 loss: 0.000477129623
Iter: 33 loss: 0.000449975923
Iter: 34 loss: 0.000446366554
Iter: 35 loss: 0.000453390065
Iter: 36 loss: 0.000444858277
Iter: 37 loss: 0.000441217067
Iter: 38 loss: 0.000453418383
Iter: 39 loss: 0.00044022064
Iter: 40 loss: 0.000438758289
Iter: 41 loss: 0.000438449439
Iter: 42 loss: 0.000436507195
Iter: 43 loss: 0.000434394024
Iter: 44 loss: 0.000434073387
Iter: 45 loss: 0.000430752756
Iter: 46 loss: 0.000431641645
Iter: 47 loss: 0.000428342493
Iter: 48 loss: 0.000426079
Iter: 49 loss: 0.000425830542
Iter: 50 loss: 0.000423521153
Iter: 51 loss: 0.000420324301
Iter: 52 loss: 0.000420176191
Iter: 53 loss: 0.000415316143
Iter: 54 loss: 0.000434959133
Iter: 55 loss: 0.000414247334
Iter: 56 loss: 0.000410092442
Iter: 57 loss: 0.000450645894
Iter: 58 loss: 0.000409938511
Iter: 59 loss: 0.000406532432
Iter: 60 loss: 0.000409528497
Iter: 61 loss: 0.000404515275
Iter: 62 loss: 0.000400617078
Iter: 63 loss: 0.000411324785
Iter: 64 loss: 0.000399337121
Iter: 65 loss: 0.0003957265
Iter: 66 loss: 0.000403214217
Iter: 67 loss: 0.000394290168
Iter: 68 loss: 0.000390351255
Iter: 69 loss: 0.000397990691
Iter: 70 loss: 0.000388699875
Iter: 71 loss: 0.000384743907
Iter: 72 loss: 0.000420162803
Iter: 73 loss: 0.00038454571
Iter: 74 loss: 0.000381502
Iter: 75 loss: 0.000430886983
Iter: 76 loss: 0.000381501508
Iter: 77 loss: 0.000379611796
Iter: 78 loss: 0.000376269949
Iter: 79 loss: 0.000376269512
Iter: 80 loss: 0.000372092
Iter: 81 loss: 0.000385243155
Iter: 82 loss: 0.000370887166
Iter: 83 loss: 0.000367542903
Iter: 84 loss: 0.000420899101
Iter: 85 loss: 0.000367542438
Iter: 86 loss: 0.000364358857
Iter: 87 loss: 0.000370344671
Iter: 88 loss: 0.000362993393
Iter: 89 loss: 0.000360099511
Iter: 90 loss: 0.000359327
Iter: 91 loss: 0.000357532816
Iter: 92 loss: 0.00035326372
Iter: 93 loss: 0.000371017639
Iter: 94 loss: 0.000352344534
Iter: 95 loss: 0.000349076756
Iter: 96 loss: 0.000353754294
Iter: 97 loss: 0.000347474765
Iter: 98 loss: 0.000343999971
Iter: 99 loss: 0.0003459859
Iter: 100 loss: 0.000341732812
Iter: 101 loss: 0.000337489706
Iter: 102 loss: 0.000358025689
Iter: 103 loss: 0.000336745579
Iter: 104 loss: 0.000332790427
Iter: 105 loss: 0.000338696875
Iter: 106 loss: 0.000330855517
Iter: 107 loss: 0.000327060639
Iter: 108 loss: 0.000354471646
Iter: 109 loss: 0.000326728332
Iter: 110 loss: 0.000324500434
Iter: 111 loss: 0.000324445951
Iter: 112 loss: 0.000322426087
Iter: 113 loss: 0.000322926149
Iter: 114 loss: 0.000320951571
Iter: 115 loss: 0.000318534556
Iter: 116 loss: 0.000321955129
Iter: 117 loss: 0.000317345402
Iter: 118 loss: 0.000314784207
Iter: 119 loss: 0.00032185606
Iter: 120 loss: 0.000313948898
Iter: 121 loss: 0.000311714335
Iter: 122 loss: 0.000328316324
Iter: 123 loss: 0.000311532262
Iter: 124 loss: 0.000309278548
Iter: 125 loss: 0.000311893644
Iter: 126 loss: 0.000308077346
Iter: 127 loss: 0.000305548194
Iter: 128 loss: 0.000307954149
Iter: 129 loss: 0.000304101821
Iter: 130 loss: 0.000301370339
Iter: 131 loss: 0.000307299313
Iter: 132 loss: 0.000300308748
Iter: 133 loss: 0.000297105522
Iter: 134 loss: 0.000302784494
Iter: 135 loss: 0.000295694335
Iter: 136 loss: 0.000292794866
Iter: 137 loss: 0.000306311151
Iter: 138 loss: 0.000292259676
Iter: 139 loss: 0.00028926658
Iter: 140 loss: 0.000293244055
Iter: 141 loss: 0.00028775126
Iter: 142 loss: 0.000285386224
Iter: 143 loss: 0.000305813766
Iter: 144 loss: 0.000285250775
Iter: 145 loss: 0.000283689849
Iter: 146 loss: 0.000283686823
Iter: 147 loss: 0.00028250803
Iter: 148 loss: 0.000280514243
Iter: 149 loss: 0.000280510838
Iter: 150 loss: 0.000278161257
Iter: 151 loss: 0.000283236703
Iter: 152 loss: 0.000277239276
Iter: 153 loss: 0.000275110884
Iter: 154 loss: 0.000287282106
Iter: 155 loss: 0.000274822058
Iter: 156 loss: 0.000272873469
Iter: 157 loss: 0.000279782049
Iter: 158 loss: 0.000272367615
Iter: 159 loss: 0.000270184
Iter: 160 loss: 0.000275715429
Iter: 161 loss: 0.000269422075
Iter: 162 loss: 0.00026755876
Iter: 163 loss: 0.00026868729
Iter: 164 loss: 0.000266357354
Iter: 165 loss: 0.000264362985
Iter: 166 loss: 0.000271760917
Iter: 167 loss: 0.00026387308
Iter: 168 loss: 0.000261803565
Iter: 169 loss: 0.000266259478
Iter: 170 loss: 0.000260993955
Iter: 171 loss: 0.000259030319
Iter: 172 loss: 0.000261359935
Iter: 173 loss: 0.000257988344
Iter: 174 loss: 0.000256123662
Iter: 175 loss: 0.000275051629
Iter: 176 loss: 0.000256063358
Iter: 177 loss: 0.000254882732
Iter: 178 loss: 0.000262809132
Iter: 179 loss: 0.000254762708
Iter: 180 loss: 0.000253528386
Iter: 181 loss: 0.000256018015
Iter: 182 loss: 0.000253024977
Iter: 183 loss: 0.000251870544
Iter: 184 loss: 0.000250900455
Iter: 185 loss: 0.000250575482
Iter: 186 loss: 0.000248656288
Iter: 187 loss: 0.000257277046
Iter: 188 loss: 0.000248283497
Iter: 189 loss: 0.000246766198
Iter: 190 loss: 0.00026077198
Iter: 191 loss: 0.000246698619
Iter: 192 loss: 0.000245594099
Iter: 193 loss: 0.000248833414
Iter: 194 loss: 0.000245254254
Iter: 195 loss: 0.00024393131
Iter: 196 loss: 0.000243768882
Iter: 197 loss: 0.000242822047
Iter: 198 loss: 0.000241172529
Iter: 199 loss: 0.000243651957
Iter: 200 loss: 0.000240382797
Iter: 201 loss: 0.000238798093
Iter: 202 loss: 0.000246595562
Iter: 203 loss: 0.000238525914
Iter: 204 loss: 0.000236960477
Iter: 205 loss: 0.00023900767
Iter: 206 loss: 0.000236162974
Iter: 207 loss: 0.000234775565
Iter: 208 loss: 0.000245919568
Iter: 209 loss: 0.00023468188
Iter: 210 loss: 0.000233668921
Iter: 211 loss: 0.000238996916
Iter: 212 loss: 0.000233511819
Iter: 213 loss: 0.000232512859
Iter: 214 loss: 0.000236411171
Iter: 215 loss: 0.00023228128
Iter: 216 loss: 0.000231364014
Iter: 217 loss: 0.000231208134
Iter: 218 loss: 0.000230579783
Iter: 219 loss: 0.000229399651
Iter: 220 loss: 0.00023086062
Iter: 221 loss: 0.000228785182
Iter: 222 loss: 0.000227386714
Iter: 223 loss: 0.000233048602
Iter: 224 loss: 0.000227073222
Iter: 225 loss: 0.000225668977
Iter: 226 loss: 0.000233043669
Iter: 227 loss: 0.000225450523
Iter: 228 loss: 0.000224302523
Iter: 229 loss: 0.000226874836
Iter: 230 loss: 0.000223866911
Iter: 231 loss: 0.000222584291
Iter: 232 loss: 0.000222609262
Iter: 233 loss: 0.000221565686
Iter: 234 loss: 0.000219972018
Iter: 235 loss: 0.000223761264
Iter: 236 loss: 0.000219389854
Iter: 237 loss: 0.000217748981
Iter: 238 loss: 0.000222302828
Iter: 239 loss: 0.000217216613
Iter: 240 loss: 0.000215837485
Iter: 241 loss: 0.000223535011
Iter: 242 loss: 0.000215639564
Iter: 243 loss: 0.000214365762
Iter: 244 loss: 0.000220606831
Iter: 245 loss: 0.000214146625
Iter: 246 loss: 0.000213150692
Iter: 247 loss: 0.000222261719
Iter: 248 loss: 0.000213106323
Iter: 249 loss: 0.000212269326
Iter: 250 loss: 0.000211991486
Iter: 251 loss: 0.000211507489
Iter: 252 loss: 0.000210432481
Iter: 253 loss: 0.000211813225
Iter: 254 loss: 0.000209880323
Iter: 255 loss: 0.000208675105
Iter: 256 loss: 0.00021154937
Iter: 257 loss: 0.000208237834
Iter: 258 loss: 0.000207074685
Iter: 259 loss: 0.00021497134
Iter: 260 loss: 0.000206960627
Iter: 261 loss: 0.000205871314
Iter: 262 loss: 0.000208342142
Iter: 263 loss: 0.000205459015
Iter: 264 loss: 0.000204572149
Iter: 265 loss: 0.000207600882
Iter: 266 loss: 0.000204333977
Iter: 267 loss: 0.000203472329
Iter: 268 loss: 0.000203306467
Iter: 269 loss: 0.000202730225
Iter: 270 loss: 0.000201445713
Iter: 271 loss: 0.000205441451
Iter: 272 loss: 0.000201070565
Iter: 273 loss: 0.000199913964
Iter: 274 loss: 0.000200920535
Iter: 275 loss: 0.000199235335
Iter: 276 loss: 0.000198156093
Iter: 277 loss: 0.000212817176
Iter: 278 loss: 0.000198151392
Iter: 279 loss: 0.000197317277
Iter: 280 loss: 0.000201750707
Iter: 281 loss: 0.000197187561
Iter: 282 loss: 0.000196351568
Iter: 283 loss: 0.000197577901
Iter: 284 loss: 0.000195947141
Iter: 285 loss: 0.000195098546
Iter: 286 loss: 0.000195390036
Iter: 287 loss: 0.00019449761
Iter: 288 loss: 0.000193442233
Iter: 289 loss: 0.000193513697
Iter: 290 loss: 0.000192619715
Iter: 291 loss: 0.000191412459
Iter: 292 loss: 0.000206621437
Iter: 293 loss: 0.000191401719
Iter: 294 loss: 0.000190607228
Iter: 295 loss: 0.00019474572
Iter: 296 loss: 0.000190480467
Iter: 297 loss: 0.000189746032
Iter: 298 loss: 0.000189826373
Iter: 299 loss: 0.000189180864
Iter: 300 loss: 0.000188088074
Iter: 301 loss: 0.000190202874
Iter: 302 loss: 0.000187633254
Iter: 303 loss: 0.000186700257
Iter: 304 loss: 0.000189830811
Iter: 305 loss: 0.000186447534
Iter: 306 loss: 0.000185497207
Iter: 307 loss: 0.000186345686
Iter: 308 loss: 0.00018494403
Iter: 309 loss: 0.000183865486
Iter: 310 loss: 0.000186852441
Iter: 311 loss: 0.000183516968
Iter: 312 loss: 0.00018288265
Iter: 313 loss: 0.000182858363
Iter: 314 loss: 0.000182280972
Iter: 315 loss: 0.000183870463
Iter: 316 loss: 0.000182092597
Iter: 317 loss: 0.000181504351
Iter: 318 loss: 0.000180891526
Iter: 319 loss: 0.000180782328
Iter: 320 loss: 0.000179764465
Iter: 321 loss: 0.000182889402
Iter: 322 loss: 0.000179462586
Iter: 323 loss: 0.000178581846
Iter: 324 loss: 0.000179824448
Iter: 325 loss: 0.000178148024
Iter: 326 loss: 0.000177249909
Iter: 327 loss: 0.000186356541
Iter: 328 loss: 0.000177221926
Iter: 329 loss: 0.000176493864
Iter: 330 loss: 0.000178289964
Iter: 331 loss: 0.000176236848
Iter: 332 loss: 0.000175597233
Iter: 333 loss: 0.000176212459
Iter: 334 loss: 0.000175232402
Iter: 335 loss: 0.000174335088
Iter: 336 loss: 0.000175630674
Iter: 337 loss: 0.000173898705
Iter: 338 loss: 0.000173040375
Iter: 339 loss: 0.000174118366
Iter: 340 loss: 0.00017259593
Iter: 341 loss: 0.000171569656
Iter: 342 loss: 0.000175068009
Iter: 343 loss: 0.000171294305
Iter: 344 loss: 0.000170478452
Iter: 345 loss: 0.000174212953
Iter: 346 loss: 0.000170321757
Iter: 347 loss: 0.000169759296
Iter: 348 loss: 0.000178223781
Iter: 349 loss: 0.000169758903
Iter: 350 loss: 0.000169220046
Iter: 351 loss: 0.000169678198
Iter: 352 loss: 0.00016890085
Iter: 353 loss: 0.000168443978
Iter: 354 loss: 0.000168016879
Iter: 355 loss: 0.000167909806
Iter: 356 loss: 0.000167039805
Iter: 357 loss: 0.00016981599
Iter: 358 loss: 0.000166790909
Iter: 359 loss: 0.000165985955
Iter: 360 loss: 0.000169092586
Iter: 361 loss: 0.000165796548
Iter: 362 loss: 0.000165093821
Iter: 363 loss: 0.000168926606
Iter: 364 loss: 0.000164989819
Iter: 365 loss: 0.000164309211
Iter: 366 loss: 0.000165974954
Iter: 367 loss: 0.000164065626
Iter: 368 loss: 0.000163501682
Iter: 369 loss: 0.000163747332
Iter: 370 loss: 0.000163116725
Iter: 371 loss: 0.000162352211
Iter: 372 loss: 0.000165449848
Iter: 373 loss: 0.000162181008
Iter: 374 loss: 0.000161568343
Iter: 375 loss: 0.000162786338
Iter: 376 loss: 0.000161317512
Iter: 377 loss: 0.000160635973
Iter: 378 loss: 0.000161124219
Iter: 379 loss: 0.000160213807
Iter: 380 loss: 0.000159518648
Iter: 381 loss: 0.000164242869
Iter: 382 loss: 0.000159450239
Iter: 383 loss: 0.000158860319
Iter: 384 loss: 0.00016624514
Iter: 385 loss: 0.000158855008
Iter: 386 loss: 0.000158471463
Iter: 387 loss: 0.000158051203
Iter: 388 loss: 0.000157989241
Iter: 389 loss: 0.000157419679
Iter: 390 loss: 0.000158132301
Iter: 391 loss: 0.000157125047
Iter: 392 loss: 0.000156390262
Iter: 393 loss: 0.000158080569
Iter: 394 loss: 0.000156116934
Iter: 395 loss: 0.000155451067
Iter: 396 loss: 0.000159990523
Iter: 397 loss: 0.000155386268
Iter: 398 loss: 0.00015485246
Iter: 399 loss: 0.000157148766
Iter: 400 loss: 0.000154742695
Iter: 401 loss: 0.000154213049
Iter: 402 loss: 0.000154586494
Iter: 403 loss: 0.000153884655
Iter: 404 loss: 0.000153328176
Iter: 405 loss: 0.000154976166
Iter: 406 loss: 0.00015315885
Iter: 407 loss: 0.000152562745
Iter: 408 loss: 0.000152751032
Iter: 409 loss: 0.000152137538
Iter: 410 loss: 0.00015145725
Iter: 411 loss: 0.000156087306
Iter: 412 loss: 0.000151390792
Iter: 413 loss: 0.000150826731
Iter: 414 loss: 0.000150983935
Iter: 415 loss: 0.000150419946
Iter: 416 loss: 0.000149897358
Iter: 417 loss: 0.000157888455
Iter: 418 loss: 0.000149897343
Iter: 419 loss: 0.000149386411
Iter: 420 loss: 0.0001508391
Iter: 421 loss: 0.00014922554
Iter: 422 loss: 0.00014884441
Iter: 423 loss: 0.000148224848
Iter: 424 loss: 0.000148221166
Iter: 425 loss: 0.000147438579
Iter: 426 loss: 0.000149967644
Iter: 427 loss: 0.000147217026
Iter: 428 loss: 0.000146619044
Iter: 429 loss: 0.000151328117
Iter: 430 loss: 0.000146577979
Iter: 431 loss: 0.000146081496
Iter: 432 loss: 0.000147808532
Iter: 433 loss: 0.000145951723
Iter: 434 loss: 0.000145481041
Iter: 435 loss: 0.000147572224
Iter: 436 loss: 0.000145388185
Iter: 437 loss: 0.000144963793
Iter: 438 loss: 0.000145231854
Iter: 439 loss: 0.000144693549
Iter: 440 loss: 0.000144234073
Iter: 441 loss: 0.00014524888
Iter: 442 loss: 0.000144058053
Iter: 443 loss: 0.000143535901
Iter: 444 loss: 0.000144560472
Iter: 445 loss: 0.000143319136
Iter: 446 loss: 0.000142778794
Iter: 447 loss: 0.000144718317
Iter: 448 loss: 0.000142641264
Iter: 449 loss: 0.000142202378
Iter: 450 loss: 0.000143187935
Iter: 451 loss: 0.000142034987
Iter: 452 loss: 0.000141637167
Iter: 453 loss: 0.000141636541
Iter: 454 loss: 0.000141301542
Iter: 455 loss: 0.000140973192
Iter: 456 loss: 0.000140901946
Iter: 457 loss: 0.000140471908
Iter: 458 loss: 0.000140850549
Iter: 459 loss: 0.000140220916
Iter: 460 loss: 0.000139642027
Iter: 461 loss: 0.000140455377
Iter: 462 loss: 0.000139356373
Iter: 463 loss: 0.000138868
Iter: 464 loss: 0.000143099
Iter: 465 loss: 0.000138841482
Iter: 466 loss: 0.000138409698
Iter: 467 loss: 0.00014047799
Iter: 468 loss: 0.000138332834
Iter: 469 loss: 0.000137922936
Iter: 470 loss: 0.000138507021
Iter: 471 loss: 0.00013772129
Iter: 472 loss: 0.000137335184
Iter: 473 loss: 0.000138520161
Iter: 474 loss: 0.000137221359
Iter: 475 loss: 0.0001368011
Iter: 476 loss: 0.000136724382
Iter: 477 loss: 0.00013643979
Iter: 478 loss: 0.000135954557
Iter: 479 loss: 0.000139142197
Iter: 480 loss: 0.000135903188
Iter: 481 loss: 0.000135480281
Iter: 482 loss: 0.00013633944
Iter: 483 loss: 0.000135308946
Iter: 484 loss: 0.000134874543
Iter: 485 loss: 0.000136192961
Iter: 486 loss: 0.000134745496
Iter: 487 loss: 0.000134374743
Iter: 488 loss: 0.000134374379
Iter: 489 loss: 0.000134148955
Iter: 490 loss: 0.000133684953
Iter: 491 loss: 0.000141870871
Iter: 492 loss: 0.000133675538
Iter: 493 loss: 0.000133168374
Iter: 494 loss: 0.000134580958
Iter: 495 loss: 0.000133004229
Iter: 496 loss: 0.000132494315
Iter: 497 loss: 0.000134011992
Iter: 498 loss: 0.00013233909
Iter: 499 loss: 0.000131905646
Iter: 500 loss: 0.000134846297
Iter: 501 loss: 0.000131862704
Iter: 502 loss: 0.000131460329
Iter: 503 loss: 0.000132811925
Iter: 504 loss: 0.000131351233
Iter: 505 loss: 0.000130985369
Iter: 506 loss: 0.000132224348
Iter: 507 loss: 0.00013088691
Iter: 508 loss: 0.000130582543
Iter: 509 loss: 0.000130496512
Iter: 510 loss: 0.000130311033
Iter: 511 loss: 0.000129822118
Iter: 512 loss: 0.000131721201
Iter: 513 loss: 0.000129708365
Iter: 514 loss: 0.000129303327
Iter: 515 loss: 0.000130487228
Iter: 516 loss: 0.000129178428
Iter: 517 loss: 0.000128762884
Iter: 518 loss: 0.000129013526
Iter: 519 loss: 0.000128495449
Iter: 520 loss: 0.000128132975
Iter: 521 loss: 0.000128131331
Iter: 522 loss: 0.000127837164
Iter: 523 loss: 0.000129141772
Iter: 524 loss: 0.00012777868
Iter: 525 loss: 0.000127541629
Iter: 526 loss: 0.000127247782
Iter: 527 loss: 0.000127222636
Iter: 528 loss: 0.000126827115
Iter: 529 loss: 0.000127525331
Iter: 530 loss: 0.00012665338
Iter: 531 loss: 0.000126184619
Iter: 532 loss: 0.000126531551
Iter: 533 loss: 0.000125896317
Iter: 534 loss: 0.000125485807
Iter: 535 loss: 0.000131420529
Iter: 536 loss: 0.000125484788
Iter: 537 loss: 0.000125140054
Iter: 538 loss: 0.000126867992
Iter: 539 loss: 0.000125082675
Iter: 540 loss: 0.000124810555
Iter: 541 loss: 0.0001249005
Iter: 542 loss: 0.000124618455
Iter: 543 loss: 0.000124282466
Iter: 544 loss: 0.000125381237
Iter: 545 loss: 0.000124188358
Iter: 546 loss: 0.000123844919
Iter: 547 loss: 0.000124087033
Iter: 548 loss: 0.000123631558
Iter: 549 loss: 0.000123223072
Iter: 550 loss: 0.000124988146
Iter: 551 loss: 0.000123139398
Iter: 552 loss: 0.000122793572
Iter: 553 loss: 0.000123843987
Iter: 554 loss: 0.000122690428
Iter: 555 loss: 0.000122443016
Iter: 556 loss: 0.000122443147
Iter: 557 loss: 0.000122210811
Iter: 558 loss: 0.000122181329
Iter: 559 loss: 0.000122015386
Iter: 560 loss: 0.000121717014
Iter: 561 loss: 0.000121506106
Iter: 562 loss: 0.000121401245
Iter: 563 loss: 0.000120935263
Iter: 564 loss: 0.000122669677
Iter: 565 loss: 0.000120821554
Iter: 566 loss: 0.000120378558
Iter: 567 loss: 0.000121651188
Iter: 568 loss: 0.000120238168
Iter: 569 loss: 0.000119860299
Iter: 570 loss: 0.000122300553
Iter: 571 loss: 0.000119819015
Iter: 572 loss: 0.000119514472
Iter: 573 loss: 0.000121512458
Iter: 574 loss: 0.000119482022
Iter: 575 loss: 0.000119204022
Iter: 576 loss: 0.000118986878
Iter: 577 loss: 0.00011889969
Iter: 578 loss: 0.000118523851
Iter: 579 loss: 0.000119805351
Iter: 580 loss: 0.000118423217
Iter: 581 loss: 0.000118076365
Iter: 582 loss: 0.00011971093
Iter: 583 loss: 0.00011801294
Iter: 584 loss: 0.000117678836
Iter: 585 loss: 0.000117999465
Iter: 586 loss: 0.000117488249
Iter: 587 loss: 0.000117174379
Iter: 588 loss: 0.000118742013
Iter: 589 loss: 0.000117121272
Iter: 590 loss: 0.000116858944
Iter: 591 loss: 0.000119980374
Iter: 592 loss: 0.00011685535
Iter: 593 loss: 0.000116643896
Iter: 594 loss: 0.000116497293
Iter: 595 loss: 0.000116421666
Iter: 596 loss: 0.000116121977
Iter: 597 loss: 0.000116438488
Iter: 598 loss: 0.000115956267
Iter: 599 loss: 0.00011560526
Iter: 600 loss: 0.000116089795
Iter: 601 loss: 0.00011543103
Iter: 602 loss: 0.000115061543
Iter: 603 loss: 0.000115711511
Iter: 604 loss: 0.000114898576
Iter: 605 loss: 0.000114516748
Iter: 606 loss: 0.000117960612
Iter: 607 loss: 0.000114499329
Iter: 608 loss: 0.000114178125
Iter: 609 loss: 0.000115598363
Iter: 610 loss: 0.000114114286
Iter: 611 loss: 0.000113840317
Iter: 612 loss: 0.000114107461
Iter: 613 loss: 0.000113684808
Iter: 614 loss: 0.000113390088
Iter: 615 loss: 0.00011362214
Iter: 616 loss: 0.000113211405
Iter: 617 loss: 0.00011286814
Iter: 618 loss: 0.000114090501
Iter: 619 loss: 0.000112779846
Iter: 620 loss: 0.000112420232
Iter: 621 loss: 0.000113762777
Iter: 622 loss: 0.000112332797
Iter: 623 loss: 0.000112050082
Iter: 624 loss: 0.000113094007
Iter: 625 loss: 0.000111980509
Iter: 626 loss: 0.000111713816
Iter: 627 loss: 0.000114297
Iter: 628 loss: 0.00011170427
Iter: 629 loss: 0.000111496411
Iter: 630 loss: 0.000111301983
Iter: 631 loss: 0.000111252964
Iter: 632 loss: 0.000110986861
Iter: 633 loss: 0.000111150948
Iter: 634 loss: 0.000110816181
Iter: 635 loss: 0.000110449066
Iter: 636 loss: 0.000112024616
Iter: 637 loss: 0.000110373017
Iter: 638 loss: 0.000110078945
Iter: 639 loss: 0.000110893503
Iter: 640 loss: 0.000109983761
Iter: 641 loss: 0.000109696724
Iter: 642 loss: 0.000110194895
Iter: 643 loss: 0.000109569468
Iter: 644 loss: 0.000109297129
Iter: 645 loss: 0.000113394781
Iter: 646 loss: 0.000109296721
Iter: 647 loss: 0.00010909819
Iter: 648 loss: 0.000108813023
Iter: 649 loss: 0.000108803535
Iter: 650 loss: 0.00010847376
Iter: 651 loss: 0.000110060901
Iter: 652 loss: 0.000108415057
Iter: 653 loss: 0.000108144159
Iter: 654 loss: 0.000108804205
Iter: 655 loss: 0.000108047025
Iter: 656 loss: 0.000107738117
Iter: 657 loss: 0.000108926746
Iter: 658 loss: 0.000107665561
Iter: 659 loss: 0.000107458734
Iter: 660 loss: 0.000109237546
Iter: 661 loss: 0.000107447398
Iter: 662 loss: 0.000107261229
Iter: 663 loss: 0.000107940272
Iter: 664 loss: 0.00010721432
Iter: 665 loss: 0.000107024222
Iter: 666 loss: 0.00010690264
Iter: 667 loss: 0.000106828433
Iter: 668 loss: 0.000106566345
Iter: 669 loss: 0.000106784311
Iter: 670 loss: 0.000106410109
Iter: 671 loss: 0.00010611485
Iter: 672 loss: 0.000107404216
Iter: 673 loss: 0.000106055086
Iter: 674 loss: 0.000105809006
Iter: 675 loss: 0.000106584455
Iter: 676 loss: 0.00010573797
Iter: 677 loss: 0.000105455547
Iter: 678 loss: 0.000105860803
Iter: 679 loss: 0.000105318104
Iter: 680 loss: 0.000105106876
Iter: 681 loss: 0.000105106032
Iter: 682 loss: 0.000104930921
Iter: 683 loss: 0.000104706829
Iter: 684 loss: 0.000104690538
Iter: 685 loss: 0.000104391307
Iter: 686 loss: 0.000105021507
Iter: 687 loss: 0.000104273
Iter: 688 loss: 0.000103980601
Iter: 689 loss: 0.000104897699
Iter: 690 loss: 0.000103895596
Iter: 691 loss: 0.00010362723
Iter: 692 loss: 0.000105660692
Iter: 693 loss: 0.00010360709
Iter: 694 loss: 0.00010340786
Iter: 695 loss: 0.000104249077
Iter: 696 loss: 0.000103365877
Iter: 697 loss: 0.000103167382
Iter: 698 loss: 0.000104218838
Iter: 699 loss: 0.00010313683
Iter: 700 loss: 0.000102968741
Iter: 701 loss: 0.000102824284
Iter: 702 loss: 0.000102778315
Iter: 703 loss: 0.000102529186
Iter: 704 loss: 0.000102742488
Iter: 705 loss: 0.000102382073
Iter: 706 loss: 0.000102066049
Iter: 707 loss: 0.000103614162
Iter: 708 loss: 0.00010201124
Iter: 709 loss: 0.000101781581
Iter: 710 loss: 0.000102118538
Iter: 711 loss: 0.000101670237
Iter: 712 loss: 0.000101397251
Iter: 713 loss: 0.000102461985
Iter: 714 loss: 0.000101333921
Iter: 715 loss: 0.000101082376
Iter: 716 loss: 0.000102789287
Iter: 717 loss: 0.000101057485
Iter: 718 loss: 0.000100848847
Iter: 719 loss: 0.00010126338
Iter: 720 loss: 0.000100763216
Iter: 721 loss: 0.000100540099
Iter: 722 loss: 0.000100599507
Iter: 723 loss: 0.000100377685
Iter: 724 loss: 0.000100137069
Iter: 725 loss: 0.000100332189
Iter: 726 loss: 9.99932e-05
Iter: 727 loss: 9.97048483e-05
Iter: 728 loss: 0.000101840269
Iter: 729 loss: 9.96811868e-05
Iter: 730 loss: 9.94729344e-05
Iter: 731 loss: 0.000101645353
Iter: 732 loss: 9.94670118e-05
Iter: 733 loss: 9.93032e-05
Iter: 734 loss: 9.96948074e-05
Iter: 735 loss: 9.92441e-05
Iter: 736 loss: 9.90591579e-05
Iter: 737 loss: 9.8991768e-05
Iter: 738 loss: 9.8888675e-05
Iter: 739 loss: 9.86666128e-05
Iter: 740 loss: 9.88197862e-05
Iter: 741 loss: 9.85279621e-05
Iter: 742 loss: 9.82663842e-05
Iter: 743 loss: 9.97142488e-05
Iter: 744 loss: 9.82288329e-05
Iter: 745 loss: 9.80163677e-05
Iter: 746 loss: 9.85353108e-05
Iter: 747 loss: 9.79410834e-05
Iter: 748 loss: 9.77048621e-05
Iter: 749 loss: 9.80155164e-05
Iter: 750 loss: 9.75849543e-05
Iter: 751 loss: 9.73765564e-05
Iter: 752 loss: 0.000100438105
Iter: 753 loss: 9.73763817e-05
Iter: 754 loss: 9.72063426e-05
Iter: 755 loss: 9.72153503e-05
Iter: 756 loss: 9.70726251e-05
Iter: 757 loss: 9.68473469e-05
Iter: 758 loss: 9.73101e-05
Iter: 759 loss: 9.67570522e-05
Iter: 760 loss: 9.65563813e-05
Iter: 761 loss: 9.68985696e-05
Iter: 762 loss: 9.64664068e-05
Iter: 763 loss: 9.62429476e-05
Iter: 764 loss: 9.73252245e-05
Iter: 765 loss: 9.62035047e-05
Iter: 766 loss: 9.60617181e-05
Iter: 767 loss: 9.60619509e-05
Iter: 768 loss: 9.5936688e-05
Iter: 769 loss: 9.58263117e-05
Iter: 770 loss: 9.579373e-05
Iter: 771 loss: 9.55845171e-05
Iter: 772 loss: 9.62855047e-05
Iter: 773 loss: 9.55280921e-05
Iter: 774 loss: 9.536375e-05
Iter: 775 loss: 9.53816489e-05
Iter: 776 loss: 9.52377304e-05
Iter: 777 loss: 9.50024405e-05
Iter: 778 loss: 9.5486379e-05
Iter: 779 loss: 9.49080568e-05
Iter: 780 loss: 9.46958753e-05
Iter: 781 loss: 9.55016658e-05
Iter: 782 loss: 9.46449218e-05
Iter: 783 loss: 9.44388e-05
Iter: 784 loss: 9.54770367e-05
Iter: 785 loss: 9.44043277e-05
Iter: 786 loss: 9.42230909e-05
Iter: 787 loss: 9.52563132e-05
Iter: 788 loss: 9.41986145e-05
Iter: 789 loss: 9.40379396e-05
Iter: 790 loss: 9.42938495e-05
Iter: 791 loss: 9.39634192e-05
Iter: 792 loss: 9.37742152e-05
Iter: 793 loss: 9.37897421e-05
Iter: 794 loss: 9.36274e-05
Iter: 795 loss: 9.3412993e-05
Iter: 796 loss: 9.39185e-05
Iter: 797 loss: 9.33346892e-05
Iter: 798 loss: 9.31514369e-05
Iter: 799 loss: 9.49334353e-05
Iter: 800 loss: 9.31448303e-05
Iter: 801 loss: 9.29752e-05
Iter: 802 loss: 9.37536242e-05
Iter: 803 loss: 9.29427624e-05
Iter: 804 loss: 9.27985602e-05
Iter: 805 loss: 9.29748639e-05
Iter: 806 loss: 9.27227811e-05
Iter: 807 loss: 9.25690401e-05
Iter: 808 loss: 9.26396679e-05
Iter: 809 loss: 9.24646229e-05
Iter: 810 loss: 9.22609906e-05
Iter: 811 loss: 9.26650609e-05
Iter: 812 loss: 9.21767787e-05
Iter: 813 loss: 9.19637896e-05
Iter: 814 loss: 9.23295156e-05
Iter: 815 loss: 9.18687e-05
Iter: 816 loss: 9.16531426e-05
Iter: 817 loss: 9.23379921e-05
Iter: 818 loss: 9.15908313e-05
Iter: 819 loss: 9.13664262e-05
Iter: 820 loss: 9.1864e-05
Iter: 821 loss: 9.12805117e-05
Iter: 822 loss: 9.11061652e-05
Iter: 823 loss: 9.1106e-05
Iter: 824 loss: 9.09839509e-05
Iter: 825 loss: 9.09587834e-05
Iter: 826 loss: 9.08783113e-05
Iter: 827 loss: 9.06913192e-05
Iter: 828 loss: 9.08208458e-05
Iter: 829 loss: 9.05745692e-05
Iter: 830 loss: 9.03631953e-05
Iter: 831 loss: 9.14981356e-05
Iter: 832 loss: 9.03310574e-05
Iter: 833 loss: 9.01632084e-05
Iter: 834 loss: 9.05993438e-05
Iter: 835 loss: 9.01058593e-05
Iter: 836 loss: 8.99106235e-05
Iter: 837 loss: 9.16589197e-05
Iter: 838 loss: 8.99010338e-05
Iter: 839 loss: 8.979e-05
Iter: 840 loss: 8.96491692e-05
Iter: 841 loss: 8.96383208e-05
Iter: 842 loss: 8.94315599e-05
Iter: 843 loss: 9.04938e-05
Iter: 844 loss: 8.93981342e-05
Iter: 845 loss: 8.92330499e-05
Iter: 846 loss: 8.92961834e-05
Iter: 847 loss: 8.91183226e-05
Iter: 848 loss: 8.89025832e-05
Iter: 849 loss: 8.91731615e-05
Iter: 850 loss: 8.87914794e-05
Iter: 851 loss: 8.85652e-05
Iter: 852 loss: 9.02014581e-05
Iter: 853 loss: 8.85461632e-05
Iter: 854 loss: 8.83662942e-05
Iter: 855 loss: 8.86398193e-05
Iter: 856 loss: 8.82811291e-05
Iter: 857 loss: 8.80922889e-05
Iter: 858 loss: 8.99156585e-05
Iter: 859 loss: 8.80851439e-05
Iter: 860 loss: 8.79272266e-05
Iter: 861 loss: 8.81094529e-05
Iter: 862 loss: 8.78428182e-05
Iter: 863 loss: 8.76834e-05
Iter: 864 loss: 8.7672066e-05
Iter: 865 loss: 8.75522237e-05
Iter: 866 loss: 8.73478857e-05
Iter: 867 loss: 8.86951166e-05
Iter: 868 loss: 8.73265381e-05
Iter: 869 loss: 8.71718657e-05
Iter: 870 loss: 8.81051747e-05
Iter: 871 loss: 8.71524608e-05
Iter: 872 loss: 8.69976429e-05
Iter: 873 loss: 8.75533442e-05
Iter: 874 loss: 8.69584474e-05
Iter: 875 loss: 8.6844193e-05
Iter: 876 loss: 8.67836352e-05
Iter: 877 loss: 8.67316558e-05
Iter: 878 loss: 8.65675174e-05
Iter: 879 loss: 8.70531658e-05
Iter: 880 loss: 8.65174152e-05
Iter: 881 loss: 8.63311143e-05
Iter: 882 loss: 8.66891351e-05
Iter: 883 loss: 8.62533052e-05
Iter: 884 loss: 8.6078493e-05
Iter: 885 loss: 8.63133173e-05
Iter: 886 loss: 8.59907814e-05
Iter: 887 loss: 8.57931445e-05
Iter: 888 loss: 8.63607129e-05
Iter: 889 loss: 8.57310806e-05
Iter: 890 loss: 8.55527906e-05
Iter: 891 loss: 8.60344298e-05
Iter: 892 loss: 8.54940517e-05
Iter: 893 loss: 8.53168749e-05
Iter: 894 loss: 8.74429315e-05
Iter: 895 loss: 8.53146339e-05
Iter: 896 loss: 8.5206826e-05
Iter: 897 loss: 8.53012e-05
Iter: 898 loss: 8.51434306e-05
Iter: 899 loss: 8.50032229e-05
Iter: 900 loss: 8.49729186e-05
Iter: 901 loss: 8.48812051e-05
Iter: 902 loss: 8.47067422e-05
Iter: 903 loss: 8.60101e-05
Iter: 904 loss: 8.46926123e-05
Iter: 905 loss: 8.45935501e-05
Iter: 906 loss: 8.61701337e-05
Iter: 907 loss: 8.4593441e-05
Iter: 908 loss: 8.45073664e-05
Iter: 909 loss: 8.4391555e-05
Iter: 910 loss: 8.4385254e-05
Iter: 911 loss: 8.42213631e-05
Iter: 912 loss: 8.45899413e-05
Iter: 913 loss: 8.415919e-05
Iter: 914 loss: 8.39947679e-05
Iter: 915 loss: 8.41895089e-05
Iter: 916 loss: 8.39074928e-05
Iter: 917 loss: 8.37254629e-05
Iter: 918 loss: 8.4816e-05
Iter: 919 loss: 8.37027e-05
Iter: 920 loss: 8.35488609e-05
Iter: 921 loss: 8.36670442e-05
Iter: 922 loss: 8.34547536e-05
Iter: 923 loss: 8.3264109e-05
Iter: 924 loss: 8.35286337e-05
Iter: 925 loss: 8.31695434e-05
Iter: 926 loss: 8.29770433e-05
Iter: 927 loss: 8.34811144e-05
Iter: 928 loss: 8.29118944e-05
Iter: 929 loss: 8.27648619e-05
Iter: 930 loss: 8.2762861e-05
Iter: 931 loss: 8.26558535e-05
Iter: 932 loss: 8.26474934e-05
Iter: 933 loss: 8.25682437e-05
Iter: 934 loss: 8.24291783e-05
Iter: 935 loss: 8.24892195e-05
Iter: 936 loss: 8.23344526e-05
Iter: 937 loss: 8.21993526e-05
Iter: 938 loss: 8.41684232e-05
Iter: 939 loss: 8.21990761e-05
Iter: 940 loss: 8.20874702e-05
Iter: 941 loss: 8.25531897e-05
Iter: 942 loss: 8.20637069e-05
Iter: 943 loss: 8.19539e-05
Iter: 944 loss: 8.18412518e-05
Iter: 945 loss: 8.18205153e-05
Iter: 946 loss: 8.16609172e-05
Iter: 947 loss: 8.2357772e-05
Iter: 948 loss: 8.16287e-05
Iter: 949 loss: 8.14951927e-05
Iter: 950 loss: 8.17145919e-05
Iter: 951 loss: 8.14344894e-05
Iter: 952 loss: 8.12699509e-05
Iter: 953 loss: 8.18776607e-05
Iter: 954 loss: 8.12292201e-05
Iter: 955 loss: 8.1098231e-05
Iter: 956 loss: 8.12801e-05
Iter: 957 loss: 8.10330093e-05
Iter: 958 loss: 8.08828554e-05
Iter: 959 loss: 8.13783845e-05
Iter: 960 loss: 8.08411e-05
Iter: 961 loss: 8.06990865e-05
Iter: 962 loss: 8.09894336e-05
Iter: 963 loss: 8.06415264e-05
Iter: 964 loss: 8.04924639e-05
Iter: 965 loss: 8.15197127e-05
Iter: 966 loss: 8.04781885e-05
Iter: 967 loss: 8.03339499e-05
Iter: 968 loss: 8.06537355e-05
Iter: 969 loss: 8.02788272e-05
Iter: 970 loss: 8.0169717e-05
Iter: 971 loss: 8.00414637e-05
Iter: 972 loss: 8.00273046e-05
Iter: 973 loss: 7.98823894e-05
Iter: 974 loss: 7.98825204e-05
Iter: 975 loss: 7.97581306e-05
Iter: 976 loss: 8.05592e-05
Iter: 977 loss: 7.97444518e-05
Iter: 978 loss: 7.96655295e-05
Iter: 979 loss: 7.95236556e-05
Iter: 980 loss: 8.29703495e-05
Iter: 981 loss: 7.95234082e-05
Iter: 982 loss: 7.93573e-05
Iter: 983 loss: 8.01342394e-05
Iter: 984 loss: 7.93262734e-05
Iter: 985 loss: 7.91641214e-05
Iter: 986 loss: 7.9508005e-05
Iter: 987 loss: 7.91004e-05
Iter: 988 loss: 7.89529149e-05
Iter: 989 loss: 7.95526139e-05
Iter: 990 loss: 7.89206315e-05
Iter: 991 loss: 7.87938334e-05
Iter: 992 loss: 7.91057682e-05
Iter: 993 loss: 7.87491663e-05
Iter: 994 loss: 7.86076707e-05
Iter: 995 loss: 7.88679463e-05
Iter: 996 loss: 7.85467128e-05
Iter: 997 loss: 7.84018121e-05
Iter: 998 loss: 7.85296e-05
Iter: 999 loss: 7.83168944e-05
Iter: 1000 loss: 7.81812123e-05
Iter: 1001 loss: 7.81814815e-05
Iter: 1002 loss: 7.80698465e-05
Iter: 1003 loss: 7.82320858e-05
Iter: 1004 loss: 7.80160335e-05
Iter: 1005 loss: 7.7913719e-05
Iter: 1006 loss: 7.78026297e-05
Iter: 1007 loss: 7.77855676e-05
Iter: 1008 loss: 7.77093956e-05
Iter: 1009 loss: 7.76912493e-05
Iter: 1010 loss: 7.75952067e-05
Iter: 1011 loss: 7.75674125e-05
Iter: 1012 loss: 7.75092631e-05
Iter: 1013 loss: 7.73774518e-05
Iter: 1014 loss: 7.72787607e-05
Iter: 1015 loss: 7.72353596e-05
Iter: 1016 loss: 7.70827901e-05
Iter: 1017 loss: 7.85611483e-05
Iter: 1018 loss: 7.70773768e-05
Iter: 1019 loss: 7.69526523e-05
Iter: 1020 loss: 7.71863e-05
Iter: 1021 loss: 7.68995233e-05
Iter: 1022 loss: 7.6774857e-05
Iter: 1023 loss: 7.70860352e-05
Iter: 1024 loss: 7.67309684e-05
Iter: 1025 loss: 7.65944569e-05
Iter: 1026 loss: 7.68474274e-05
Iter: 1027 loss: 7.65359655e-05
Iter: 1028 loss: 7.63850257e-05
Iter: 1029 loss: 7.68817263e-05
Iter: 1030 loss: 7.63431e-05
Iter: 1031 loss: 7.62192576e-05
Iter: 1032 loss: 7.64435754e-05
Iter: 1033 loss: 7.61652191e-05
Iter: 1034 loss: 7.60489565e-05
Iter: 1035 loss: 7.75314256e-05
Iter: 1036 loss: 7.60484108e-05
Iter: 1037 loss: 7.59443501e-05
Iter: 1038 loss: 7.58779206e-05
Iter: 1039 loss: 7.58370152e-05
Iter: 1040 loss: 7.56772206e-05
Iter: 1041 loss: 7.58620445e-05
Iter: 1042 loss: 7.55918154e-05
Iter: 1043 loss: 7.54922221e-05
Iter: 1044 loss: 7.54865468e-05
Iter: 1045 loss: 7.53927e-05
Iter: 1046 loss: 7.5475531e-05
Iter: 1047 loss: 7.53380591e-05
Iter: 1048 loss: 7.52363849e-05
Iter: 1049 loss: 7.51933912e-05
Iter: 1050 loss: 7.51403859e-05
Iter: 1051 loss: 7.50126128e-05
Iter: 1052 loss: 7.50781401e-05
Iter: 1053 loss: 7.49276078e-05
Iter: 1054 loss: 7.47726081e-05
Iter: 1055 loss: 7.60884141e-05
Iter: 1056 loss: 7.4763826e-05
Iter: 1057 loss: 7.46536825e-05
Iter: 1058 loss: 7.51302141e-05
Iter: 1059 loss: 7.46310398e-05
Iter: 1060 loss: 7.45184589e-05
Iter: 1061 loss: 7.44889e-05
Iter: 1062 loss: 7.44185963e-05
Iter: 1063 loss: 7.42743141e-05
Iter: 1064 loss: 7.48402526e-05
Iter: 1065 loss: 7.42413395e-05
Iter: 1066 loss: 7.40994437e-05
Iter: 1067 loss: 7.45942816e-05
Iter: 1068 loss: 7.40624091e-05
Iter: 1069 loss: 7.39469542e-05
Iter: 1070 loss: 7.4944488e-05
Iter: 1071 loss: 7.39404786e-05
Iter: 1072 loss: 7.38467061e-05
Iter: 1073 loss: 7.3992429e-05
Iter: 1074 loss: 7.38023809e-05
Iter: 1075 loss: 7.36892252e-05
Iter: 1076 loss: 7.37442606e-05
Iter: 1077 loss: 7.36131478e-05
Iter: 1078 loss: 7.35239591e-05
Iter: 1079 loss: 7.352393e-05
Iter: 1080 loss: 7.34364221e-05
Iter: 1081 loss: 7.34372e-05
Iter: 1082 loss: 7.33664274e-05
Iter: 1083 loss: 7.32701155e-05
Iter: 1084 loss: 7.31829496e-05
Iter: 1085 loss: 7.31591426e-05
Iter: 1086 loss: 7.30142056e-05
Iter: 1087 loss: 7.34890855e-05
Iter: 1088 loss: 7.29740568e-05
Iter: 1089 loss: 7.28374434e-05
Iter: 1090 loss: 7.37993396e-05
Iter: 1091 loss: 7.28247396e-05
Iter: 1092 loss: 7.27107908e-05
Iter: 1093 loss: 7.29079256e-05
Iter: 1094 loss: 7.26595463e-05
Iter: 1095 loss: 7.25248101e-05
Iter: 1096 loss: 7.28218147e-05
Iter: 1097 loss: 7.24729543e-05
Iter: 1098 loss: 7.23444828e-05
Iter: 1099 loss: 7.2763e-05
Iter: 1100 loss: 7.23087942e-05
Iter: 1101 loss: 7.2192146e-05
Iter: 1102 loss: 7.24671409e-05
Iter: 1103 loss: 7.21496399e-05
Iter: 1104 loss: 7.20483949e-05
Iter: 1105 loss: 7.31815235e-05
Iter: 1106 loss: 7.20466051e-05
Iter: 1107 loss: 7.19597883e-05
Iter: 1108 loss: 7.19365635e-05
Iter: 1109 loss: 7.18829688e-05
Iter: 1110 loss: 7.17699295e-05
Iter: 1111 loss: 7.22931873e-05
Iter: 1112 loss: 7.17489e-05
Iter: 1113 loss: 7.1669725e-05
Iter: 1114 loss: 7.26078579e-05
Iter: 1115 loss: 7.16690847e-05
Iter: 1116 loss: 7.16015784e-05
Iter: 1117 loss: 7.15276e-05
Iter: 1118 loss: 7.15166316e-05
Iter: 1119 loss: 7.14007911e-05
Iter: 1120 loss: 7.1472663e-05
Iter: 1121 loss: 7.13263871e-05
Iter: 1122 loss: 7.11955072e-05
Iter: 1123 loss: 7.13472837e-05
Iter: 1124 loss: 7.11258472e-05
Iter: 1125 loss: 7.0979062e-05
Iter: 1126 loss: 7.17021685e-05
Iter: 1127 loss: 7.09536835e-05
Iter: 1128 loss: 7.08393491e-05
Iter: 1129 loss: 7.12880064e-05
Iter: 1130 loss: 7.08132429e-05
Iter: 1131 loss: 7.06929422e-05
Iter: 1132 loss: 7.12104375e-05
Iter: 1133 loss: 7.06681894e-05
Iter: 1134 loss: 7.05731654e-05
Iter: 1135 loss: 7.06116552e-05
Iter: 1136 loss: 7.05078e-05
Iter: 1137 loss: 7.03848054e-05
Iter: 1138 loss: 7.07786603e-05
Iter: 1139 loss: 7.03496335e-05
Iter: 1140 loss: 7.02449e-05
Iter: 1141 loss: 7.15466e-05
Iter: 1142 loss: 7.02441175e-05
Iter: 1143 loss: 7.01668177e-05
Iter: 1144 loss: 7.01079771e-05
Iter: 1145 loss: 7.0083217e-05
Iter: 1146 loss: 6.99724478e-05
Iter: 1147 loss: 7.08999e-05
Iter: 1148 loss: 6.99661832e-05
Iter: 1149 loss: 6.98775e-05
Iter: 1150 loss: 7.04108825e-05
Iter: 1151 loss: 6.98663353e-05
Iter: 1152 loss: 6.97968571e-05
Iter: 1153 loss: 6.97418363e-05
Iter: 1154 loss: 6.97207433e-05
Iter: 1155 loss: 6.96233037e-05
Iter: 1156 loss: 6.97002106e-05
Iter: 1157 loss: 6.95648e-05
Iter: 1158 loss: 6.94532937e-05
Iter: 1159 loss: 6.97932555e-05
Iter: 1160 loss: 6.94203191e-05
Iter: 1161 loss: 6.93028123e-05
Iter: 1162 loss: 6.95722629e-05
Iter: 1163 loss: 6.9259273e-05
Iter: 1164 loss: 6.91465175e-05
Iter: 1165 loss: 6.97574433e-05
Iter: 1166 loss: 6.91296955e-05
Iter: 1167 loss: 6.90325251e-05
Iter: 1168 loss: 6.92817266e-05
Iter: 1169 loss: 6.89993904e-05
Iter: 1170 loss: 6.88958535e-05
Iter: 1171 loss: 6.92086105e-05
Iter: 1172 loss: 6.88645086e-05
Iter: 1173 loss: 6.8771973e-05
Iter: 1174 loss: 6.89479348e-05
Iter: 1175 loss: 6.87325155e-05
Iter: 1176 loss: 6.86430722e-05
Iter: 1177 loss: 6.94501068e-05
Iter: 1178 loss: 6.86388e-05
Iter: 1179 loss: 6.85581326e-05
Iter: 1180 loss: 6.86173589e-05
Iter: 1181 loss: 6.85082778e-05
Iter: 1182 loss: 6.84310071e-05
Iter: 1183 loss: 6.89076e-05
Iter: 1184 loss: 6.84218758e-05
Iter: 1185 loss: 6.83347607e-05
Iter: 1186 loss: 6.84672123e-05
Iter: 1187 loss: 6.82930258e-05
Iter: 1188 loss: 6.82250247e-05
Iter: 1189 loss: 6.81751117e-05
Iter: 1190 loss: 6.81519814e-05
Iter: 1191 loss: 6.80364319e-05
Iter: 1192 loss: 6.84297629e-05
Iter: 1193 loss: 6.80056255e-05
Iter: 1194 loss: 6.78969955e-05
Iter: 1195 loss: 6.80995799e-05
Iter: 1196 loss: 6.78510696e-05
Iter: 1197 loss: 6.77373537e-05
Iter: 1198 loss: 6.78988581e-05
Iter: 1199 loss: 6.7681729e-05
Iter: 1200 loss: 6.75643823e-05
Iter: 1201 loss: 6.80005105e-05
Iter: 1202 loss: 6.7535686e-05
Iter: 1203 loss: 6.74205512e-05
Iter: 1204 loss: 6.81889142e-05
Iter: 1205 loss: 6.74087e-05
Iter: 1206 loss: 6.73230534e-05
Iter: 1207 loss: 6.75368e-05
Iter: 1208 loss: 6.72928654e-05
Iter: 1209 loss: 6.72073147e-05
Iter: 1210 loss: 6.72695169e-05
Iter: 1211 loss: 6.71544476e-05
Iter: 1212 loss: 6.70592126e-05
Iter: 1213 loss: 6.83523904e-05
Iter: 1214 loss: 6.70587906e-05
Iter: 1215 loss: 6.69991859e-05
Iter: 1216 loss: 6.70018489e-05
Iter: 1217 loss: 6.69520159e-05
Iter: 1218 loss: 6.68849098e-05
Iter: 1219 loss: 6.76827622e-05
Iter: 1220 loss: 6.68839639e-05
Iter: 1221 loss: 6.68235443e-05
Iter: 1222 loss: 6.68008e-05
Iter: 1223 loss: 6.67678469e-05
Iter: 1224 loss: 6.66991e-05
Iter: 1225 loss: 6.67182831e-05
Iter: 1226 loss: 6.66490814e-05
Iter: 1227 loss: 6.65449e-05
Iter: 1228 loss: 6.67147178e-05
Iter: 1229 loss: 6.64968684e-05
Iter: 1230 loss: 6.63969e-05
Iter: 1231 loss: 6.66720225e-05
Iter: 1232 loss: 6.63641695e-05
Iter: 1233 loss: 6.62589591e-05
Iter: 1234 loss: 6.67435233e-05
Iter: 1235 loss: 6.62388629e-05
Iter: 1236 loss: 6.61577506e-05
Iter: 1237 loss: 6.63467508e-05
Iter: 1238 loss: 6.61279264e-05
Iter: 1239 loss: 6.60377118e-05
Iter: 1240 loss: 6.61623053e-05
Iter: 1241 loss: 6.59928555e-05
Iter: 1242 loss: 6.59021607e-05
Iter: 1243 loss: 6.67681743e-05
Iter: 1244 loss: 6.58988356e-05
Iter: 1245 loss: 6.58251665e-05
Iter: 1246 loss: 6.58519e-05
Iter: 1247 loss: 6.57737692e-05
Iter: 1248 loss: 6.56940392e-05
Iter: 1249 loss: 6.65617335e-05
Iter: 1250 loss: 6.56922057e-05
Iter: 1251 loss: 6.56252814e-05
Iter: 1252 loss: 6.56725751e-05
Iter: 1253 loss: 6.55835756e-05
Iter: 1254 loss: 6.55256736e-05
Iter: 1255 loss: 6.62782331e-05
Iter: 1256 loss: 6.55250042e-05
Iter: 1257 loss: 6.54760879e-05
Iter: 1258 loss: 6.54114e-05
Iter: 1259 loss: 6.54073374e-05
Iter: 1260 loss: 6.53257885e-05
Iter: 1261 loss: 6.52769231e-05
Iter: 1262 loss: 6.52434101e-05
Iter: 1263 loss: 6.51282899e-05
Iter: 1264 loss: 6.60905498e-05
Iter: 1265 loss: 6.51215159e-05
Iter: 1266 loss: 6.5038621e-05
Iter: 1267 loss: 6.52296731e-05
Iter: 1268 loss: 6.50075235e-05
Iter: 1269 loss: 6.49179055e-05
Iter: 1270 loss: 6.52010203e-05
Iter: 1271 loss: 6.48924179e-05
Iter: 1272 loss: 6.48052519e-05
Iter: 1273 loss: 6.48445348e-05
Iter: 1274 loss: 6.47463166e-05
Iter: 1275 loss: 6.46496701e-05
Iter: 1276 loss: 6.56043e-05
Iter: 1277 loss: 6.46464832e-05
Iter: 1278 loss: 6.45805267e-05
Iter: 1279 loss: 6.47841443e-05
Iter: 1280 loss: 6.45609543e-05
Iter: 1281 loss: 6.44885295e-05
Iter: 1282 loss: 6.46025728e-05
Iter: 1283 loss: 6.44542306e-05
Iter: 1284 loss: 6.43828098e-05
Iter: 1285 loss: 6.51549635e-05
Iter: 1286 loss: 6.43812382e-05
Iter: 1287 loss: 6.43343301e-05
Iter: 1288 loss: 6.43969543e-05
Iter: 1289 loss: 6.43108069e-05
Iter: 1290 loss: 6.4252672e-05
Iter: 1291 loss: 6.44733591e-05
Iter: 1292 loss: 6.42387095e-05
Iter: 1293 loss: 6.41808874e-05
Iter: 1294 loss: 6.41682418e-05
Iter: 1295 loss: 6.41304941e-05
Iter: 1296 loss: 6.40569197e-05
Iter: 1297 loss: 6.40751678e-05
Iter: 1298 loss: 6.40028229e-05
Iter: 1299 loss: 6.3906351e-05
Iter: 1300 loss: 6.41545485e-05
Iter: 1301 loss: 6.38735219e-05
Iter: 1302 loss: 6.37856137e-05
Iter: 1303 loss: 6.39847422e-05
Iter: 1304 loss: 6.37525809e-05
Iter: 1305 loss: 6.36599725e-05
Iter: 1306 loss: 6.41416409e-05
Iter: 1307 loss: 6.36454351e-05
Iter: 1308 loss: 6.35672768e-05
Iter: 1309 loss: 6.38309284e-05
Iter: 1310 loss: 6.35462784e-05
Iter: 1311 loss: 6.34639e-05
Iter: 1312 loss: 6.35875476e-05
Iter: 1313 loss: 6.34244134e-05
Iter: 1314 loss: 6.33395903e-05
Iter: 1315 loss: 6.37983758e-05
Iter: 1316 loss: 6.33272e-05
Iter: 1317 loss: 6.32618612e-05
Iter: 1318 loss: 6.37375924e-05
Iter: 1319 loss: 6.32563169e-05
Iter: 1320 loss: 6.320393e-05
Iter: 1321 loss: 6.33347663e-05
Iter: 1322 loss: 6.31856819e-05
Iter: 1323 loss: 6.31213334e-05
Iter: 1324 loss: 6.32004521e-05
Iter: 1325 loss: 6.3087733e-05
Iter: 1326 loss: 6.30138093e-05
Iter: 1327 loss: 6.33533273e-05
Iter: 1328 loss: 6.29995484e-05
Iter: 1329 loss: 6.29395727e-05
Iter: 1330 loss: 6.28960188e-05
Iter: 1331 loss: 6.28755515e-05
Iter: 1332 loss: 6.27889e-05
Iter: 1333 loss: 6.2880019e-05
Iter: 1334 loss: 6.27408444e-05
Iter: 1335 loss: 6.26485926e-05
Iter: 1336 loss: 6.29944625e-05
Iter: 1337 loss: 6.26262699e-05
Iter: 1338 loss: 6.25429384e-05
Iter: 1339 loss: 6.28098569e-05
Iter: 1340 loss: 6.25193497e-05
Iter: 1341 loss: 6.24351e-05
Iter: 1342 loss: 6.27240806e-05
Iter: 1343 loss: 6.24122622e-05
Iter: 1344 loss: 6.23317246e-05
Iter: 1345 loss: 6.24499662e-05
Iter: 1346 loss: 6.22928201e-05
Iter: 1347 loss: 6.21943618e-05
Iter: 1348 loss: 6.26194669e-05
Iter: 1349 loss: 6.21738873e-05
Iter: 1350 loss: 6.20977e-05
Iter: 1351 loss: 6.25867688e-05
Iter: 1352 loss: 6.20892533e-05
Iter: 1353 loss: 6.2034e-05
Iter: 1354 loss: 6.22972584e-05
Iter: 1355 loss: 6.20237552e-05
Iter: 1356 loss: 6.19642233e-05
Iter: 1357 loss: 6.20839e-05
Iter: 1358 loss: 6.1939907e-05
Iter: 1359 loss: 6.18788763e-05
Iter: 1360 loss: 6.21070649e-05
Iter: 1361 loss: 6.18638514e-05
Iter: 1362 loss: 6.18075792e-05
Iter: 1363 loss: 6.18634076e-05
Iter: 1364 loss: 6.1775565e-05
Iter: 1365 loss: 6.17124169e-05
Iter: 1366 loss: 6.17384649e-05
Iter: 1367 loss: 6.16691395e-05
Iter: 1368 loss: 6.15949684e-05
Iter: 1369 loss: 6.18624472e-05
Iter: 1370 loss: 6.15761383e-05
Iter: 1371 loss: 6.15071185e-05
Iter: 1372 loss: 6.14901364e-05
Iter: 1373 loss: 6.1446015e-05
Iter: 1374 loss: 6.13512602e-05
Iter: 1375 loss: 6.1814746e-05
Iter: 1376 loss: 6.1334591e-05
Iter: 1377 loss: 6.12466902e-05
Iter: 1378 loss: 6.13544616e-05
Iter: 1379 loss: 6.12005097e-05
Iter: 1380 loss: 6.11143842e-05
Iter: 1381 loss: 6.16527541e-05
Iter: 1382 loss: 6.1104e-05
Iter: 1383 loss: 6.10281313e-05
Iter: 1384 loss: 6.14580495e-05
Iter: 1385 loss: 6.10177885e-05
Iter: 1386 loss: 6.0956816e-05
Iter: 1387 loss: 6.1136765e-05
Iter: 1388 loss: 6.09380513e-05
Iter: 1389 loss: 6.08826704e-05
Iter: 1390 loss: 6.12942677e-05
Iter: 1391 loss: 6.08783957e-05
Iter: 1392 loss: 6.0826198e-05
Iter: 1393 loss: 6.0850638e-05
Iter: 1394 loss: 6.07909751e-05
Iter: 1395 loss: 6.07244292e-05
Iter: 1396 loss: 6.10328279e-05
Iter: 1397 loss: 6.07120091e-05
Iter: 1398 loss: 6.06607318e-05
Iter: 1399 loss: 6.06593712e-05
Iter: 1400 loss: 6.06191243e-05
Iter: 1401 loss: 6.05463756e-05
Iter: 1402 loss: 6.06579779e-05
Iter: 1403 loss: 6.0511753e-05
Iter: 1404 loss: 6.04332672e-05
Iter: 1405 loss: 6.05874302e-05
Iter: 1406 loss: 6.04011293e-05
Iter: 1407 loss: 6.03268199e-05
Iter: 1408 loss: 6.05089554e-05
Iter: 1409 loss: 6.03002554e-05
Iter: 1410 loss: 6.02228392e-05
Iter: 1411 loss: 6.02644723e-05
Iter: 1412 loss: 6.01717111e-05
Iter: 1413 loss: 6.00706699e-05
Iter: 1414 loss: 6.06177346e-05
Iter: 1415 loss: 6.0055725e-05
Iter: 1416 loss: 5.99788364e-05
Iter: 1417 loss: 6.02224536e-05
Iter: 1418 loss: 5.99566411e-05
Iter: 1419 loss: 5.98866172e-05
Iter: 1420 loss: 6.05363311e-05
Iter: 1421 loss: 5.98835941e-05
Iter: 1422 loss: 5.98360239e-05
Iter: 1423 loss: 5.99655141e-05
Iter: 1424 loss: 5.98200968e-05
Iter: 1425 loss: 5.97615726e-05
Iter: 1426 loss: 5.99754349e-05
Iter: 1427 loss: 5.97469916e-05
Iter: 1428 loss: 5.9691436e-05
Iter: 1429 loss: 5.98116021e-05
Iter: 1430 loss: 5.96702521e-05
Iter: 1431 loss: 5.96156824e-05
Iter: 1432 loss: 5.97700382e-05
Iter: 1433 loss: 5.95983511e-05
Iter: 1434 loss: 5.95483943e-05
Iter: 1435 loss: 5.95335223e-05
Iter: 1436 loss: 5.95033234e-05
Iter: 1437 loss: 5.94301637e-05
Iter: 1438 loss: 5.96519967e-05
Iter: 1439 loss: 5.94084922e-05
Iter: 1440 loss: 5.93425139e-05
Iter: 1441 loss: 5.94332305e-05
Iter: 1442 loss: 5.93096775e-05
Iter: 1443 loss: 5.9236354e-05
Iter: 1444 loss: 5.94270241e-05
Iter: 1445 loss: 5.92113502e-05
Iter: 1446 loss: 5.91365642e-05
Iter: 1447 loss: 5.93444856e-05
Iter: 1448 loss: 5.91125572e-05
Iter: 1449 loss: 5.90324053e-05
Iter: 1450 loss: 5.92039141e-05
Iter: 1451 loss: 5.90014242e-05
Iter: 1452 loss: 5.89253723e-05
Iter: 1453 loss: 5.920904e-05
Iter: 1454 loss: 5.89067931e-05
Iter: 1455 loss: 5.88419389e-05
Iter: 1456 loss: 5.91322423e-05
Iter: 1457 loss: 5.88295152e-05
Iter: 1458 loss: 5.87745199e-05
Iter: 1459 loss: 5.93864752e-05
Iter: 1460 loss: 5.87734321e-05
Iter: 1461 loss: 5.87326103e-05
Iter: 1462 loss: 5.88164403e-05
Iter: 1463 loss: 5.87165741e-05
Iter: 1464 loss: 5.8667727e-05
Iter: 1465 loss: 5.87188515e-05
Iter: 1466 loss: 5.86405658e-05
Iter: 1467 loss: 5.85887174e-05
Iter: 1468 loss: 5.87282775e-05
Iter: 1469 loss: 5.85717062e-05
Iter: 1470 loss: 5.85179223e-05
Iter: 1471 loss: 5.85684647e-05
Iter: 1472 loss: 5.8487225e-05
Iter: 1473 loss: 5.84262889e-05
Iter: 1474 loss: 5.84212576e-05
Iter: 1475 loss: 5.8376143e-05
Iter: 1476 loss: 5.82976791e-05
Iter: 1477 loss: 5.86765454e-05
Iter: 1478 loss: 5.82834764e-05
Iter: 1479 loss: 5.82120847e-05
Iter: 1480 loss: 5.8361562e-05
Iter: 1481 loss: 5.81836357e-05
Iter: 1482 loss: 5.81027052e-05
Iter: 1483 loss: 5.83819856e-05
Iter: 1484 loss: 5.8081343e-05
Iter: 1485 loss: 5.80101405e-05
Iter: 1486 loss: 5.81315908e-05
Iter: 1487 loss: 5.79786174e-05
Iter: 1488 loss: 5.79111147e-05
Iter: 1489 loss: 5.81025379e-05
Iter: 1490 loss: 5.78891777e-05
Iter: 1491 loss: 5.78238541e-05
Iter: 1492 loss: 5.82087414e-05
Iter: 1493 loss: 5.78151703e-05
Iter: 1494 loss: 5.77695682e-05
Iter: 1495 loss: 5.84943664e-05
Iter: 1496 loss: 5.77694482e-05
Iter: 1497 loss: 5.77346655e-05
Iter: 1498 loss: 5.77305764e-05
Iter: 1499 loss: 5.77055107e-05
Iter: 1500 loss: 5.76494458e-05
Iter: 1501 loss: 5.77758256e-05
Iter: 1502 loss: 5.76288658e-05
Iter: 1503 loss: 5.7579753e-05
Iter: 1504 loss: 5.77035971e-05
Iter: 1505 loss: 5.75626837e-05
Iter: 1506 loss: 5.75126214e-05
Iter: 1507 loss: 5.75483973e-05
Iter: 1508 loss: 5.74815786e-05
Iter: 1509 loss: 5.74158548e-05
Iter: 1510 loss: 5.75598388e-05
Iter: 1511 loss: 5.73899415e-05
Iter: 1512 loss: 5.73288053e-05
Iter: 1513 loss: 5.74239457e-05
Iter: 1514 loss: 5.73003454e-05
Iter: 1515 loss: 5.72303616e-05
Iter: 1516 loss: 5.73764555e-05
Iter: 1517 loss: 5.7202662e-05
Iter: 1518 loss: 5.71347118e-05
Iter: 1519 loss: 5.72784775e-05
Iter: 1520 loss: 5.71081473e-05
Iter: 1521 loss: 5.70339544e-05
Iter: 1522 loss: 5.73467842e-05
Iter: 1523 loss: 5.70184748e-05
Iter: 1524 loss: 5.69595359e-05
Iter: 1525 loss: 5.7164325e-05
Iter: 1526 loss: 5.69439362e-05
Iter: 1527 loss: 5.68803771e-05
Iter: 1528 loss: 5.70845659e-05
Iter: 1529 loss: 5.68625765e-05
Iter: 1530 loss: 5.68271062e-05
Iter: 1531 loss: 5.68262112e-05
Iter: 1532 loss: 5.67930911e-05
Iter: 1533 loss: 5.67697898e-05
Iter: 1534 loss: 5.67583156e-05
Iter: 1535 loss: 5.67105817e-05
Iter: 1536 loss: 5.68873438e-05
Iter: 1537 loss: 5.66991803e-05
Iter: 1538 loss: 5.66502094e-05
Iter: 1539 loss: 5.66938652e-05
Iter: 1540 loss: 5.66217823e-05
Iter: 1541 loss: 5.65687e-05
Iter: 1542 loss: 5.65918454e-05
Iter: 1543 loss: 5.65324735e-05
Iter: 1544 loss: 5.64693946e-05
Iter: 1545 loss: 5.69636031e-05
Iter: 1546 loss: 5.64648617e-05
Iter: 1547 loss: 5.64136826e-05
Iter: 1548 loss: 5.64023539e-05
Iter: 1549 loss: 5.63692884e-05
Iter: 1550 loss: 5.62966052e-05
Iter: 1551 loss: 5.64547954e-05
Iter: 1552 loss: 5.62688438e-05
Iter: 1553 loss: 5.61939451e-05
Iter: 1554 loss: 5.64404e-05
Iter: 1555 loss: 5.61732704e-05
Iter: 1556 loss: 5.6110377e-05
Iter: 1557 loss: 5.63067297e-05
Iter: 1558 loss: 5.60924673e-05
Iter: 1559 loss: 5.60251e-05
Iter: 1560 loss: 5.61107627e-05
Iter: 1561 loss: 5.59906184e-05
Iter: 1562 loss: 5.59225118e-05
Iter: 1563 loss: 5.63146605e-05
Iter: 1564 loss: 5.59134314e-05
Iter: 1565 loss: 5.58760876e-05
Iter: 1566 loss: 5.58749598e-05
Iter: 1567 loss: 5.58373758e-05
Iter: 1568 loss: 5.57887834e-05
Iter: 1569 loss: 5.57857857e-05
Iter: 1570 loss: 5.5730663e-05
Iter: 1571 loss: 5.61175912e-05
Iter: 1572 loss: 5.57253443e-05
Iter: 1573 loss: 5.56770829e-05
Iter: 1574 loss: 5.56767664e-05
Iter: 1575 loss: 5.56384475e-05
Iter: 1576 loss: 5.5577555e-05
Iter: 1577 loss: 5.57842541e-05
Iter: 1578 loss: 5.55614351e-05
Iter: 1579 loss: 5.55083461e-05
Iter: 1580 loss: 5.56271916e-05
Iter: 1581 loss: 5.54881517e-05
Iter: 1582 loss: 5.54291764e-05
Iter: 1583 loss: 5.55250954e-05
Iter: 1584 loss: 5.54017097e-05
Iter: 1585 loss: 5.53405334e-05
Iter: 1586 loss: 5.56265659e-05
Iter: 1587 loss: 5.53294303e-05
Iter: 1588 loss: 5.52763231e-05
Iter: 1589 loss: 5.53046448e-05
Iter: 1590 loss: 5.52416059e-05
Iter: 1591 loss: 5.51696103e-05
Iter: 1592 loss: 5.52425699e-05
Iter: 1593 loss: 5.51292687e-05
Iter: 1594 loss: 5.50519107e-05
Iter: 1595 loss: 5.54133221e-05
Iter: 1596 loss: 5.5037839e-05
Iter: 1597 loss: 5.49642282e-05
Iter: 1598 loss: 5.50999757e-05
Iter: 1599 loss: 5.49323777e-05
Iter: 1600 loss: 5.49015713e-05
Iter: 1601 loss: 5.48925091e-05
Iter: 1602 loss: 5.4853037e-05
Iter: 1603 loss: 5.48301214e-05
Iter: 1604 loss: 5.48133976e-05
Iter: 1605 loss: 5.47625023e-05
Iter: 1606 loss: 5.48503194e-05
Iter: 1607 loss: 5.47397649e-05
Iter: 1608 loss: 5.46816664e-05
Iter: 1609 loss: 5.49446777e-05
Iter: 1610 loss: 5.46706e-05
Iter: 1611 loss: 5.46222254e-05
Iter: 1612 loss: 5.46291667e-05
Iter: 1613 loss: 5.45858748e-05
Iter: 1614 loss: 5.45275143e-05
Iter: 1615 loss: 5.46836527e-05
Iter: 1616 loss: 5.45082294e-05
Iter: 1617 loss: 5.44530958e-05
Iter: 1618 loss: 5.47166383e-05
Iter: 1619 loss: 5.4443306e-05
Iter: 1620 loss: 5.43867973e-05
Iter: 1621 loss: 5.44469658e-05
Iter: 1622 loss: 5.43559363e-05
Iter: 1623 loss: 5.4297132e-05
Iter: 1624 loss: 5.43955975e-05
Iter: 1625 loss: 5.42704547e-05
Iter: 1626 loss: 5.42091548e-05
Iter: 1627 loss: 5.44021495e-05
Iter: 1628 loss: 5.41913178e-05
Iter: 1629 loss: 5.41316949e-05
Iter: 1630 loss: 5.43062633e-05
Iter: 1631 loss: 5.41133959e-05
Iter: 1632 loss: 5.40473702e-05
Iter: 1633 loss: 5.41915797e-05
Iter: 1634 loss: 5.40219808e-05
Iter: 1635 loss: 5.39596731e-05
Iter: 1636 loss: 5.40841647e-05
Iter: 1637 loss: 5.39339671e-05
Iter: 1638 loss: 5.39021421e-05
Iter: 1639 loss: 5.38963359e-05
Iter: 1640 loss: 5.38604363e-05
Iter: 1641 loss: 5.38390377e-05
Iter: 1642 loss: 5.3824042e-05
Iter: 1643 loss: 5.37803789e-05
Iter: 1644 loss: 5.37634623e-05
Iter: 1645 loss: 5.3739881e-05
Iter: 1646 loss: 5.36927764e-05
Iter: 1647 loss: 5.36925581e-05
Iter: 1648 loss: 5.36593e-05
Iter: 1649 loss: 5.36387233e-05
Iter: 1650 loss: 5.36250191e-05
Iter: 1651 loss: 5.35698418e-05
Iter: 1652 loss: 5.35884101e-05
Iter: 1653 loss: 5.35315339e-05
Iter: 1654 loss: 5.34711216e-05
Iter: 1655 loss: 5.38477434e-05
Iter: 1656 loss: 5.34640385e-05
Iter: 1657 loss: 5.34113497e-05
Iter: 1658 loss: 5.35384206e-05
Iter: 1659 loss: 5.33924249e-05
Iter: 1660 loss: 5.33459897e-05
Iter: 1661 loss: 5.37089509e-05
Iter: 1662 loss: 5.334257e-05
Iter: 1663 loss: 5.33039856e-05
Iter: 1664 loss: 5.32717386e-05
Iter: 1665 loss: 5.32606755e-05
Iter: 1666 loss: 5.31941478e-05
Iter: 1667 loss: 5.3370215e-05
Iter: 1668 loss: 5.31714904e-05
Iter: 1669 loss: 5.31088044e-05
Iter: 1670 loss: 5.32527556e-05
Iter: 1671 loss: 5.30854886e-05
Iter: 1672 loss: 5.30261059e-05
Iter: 1673 loss: 5.35991712e-05
Iter: 1674 loss: 5.30239231e-05
Iter: 1675 loss: 5.29805e-05
Iter: 1676 loss: 5.34807405e-05
Iter: 1677 loss: 5.29794779e-05
Iter: 1678 loss: 5.29497884e-05
Iter: 1679 loss: 5.29098033e-05
Iter: 1680 loss: 5.29074932e-05
Iter: 1681 loss: 5.28588e-05
Iter: 1682 loss: 5.29333192e-05
Iter: 1683 loss: 5.28358578e-05
Iter: 1684 loss: 5.27839329e-05
Iter: 1685 loss: 5.28940363e-05
Iter: 1686 loss: 5.27633092e-05
Iter: 1687 loss: 5.27097e-05
Iter: 1688 loss: 5.33263483e-05
Iter: 1689 loss: 5.27088632e-05
Iter: 1690 loss: 5.26732038e-05
Iter: 1691 loss: 5.26360236e-05
Iter: 1692 loss: 5.26293916e-05
Iter: 1693 loss: 5.25711948e-05
Iter: 1694 loss: 5.27429511e-05
Iter: 1695 loss: 5.25534379e-05
Iter: 1696 loss: 5.249893e-05
Iter: 1697 loss: 5.25584837e-05
Iter: 1698 loss: 5.24693169e-05
Iter: 1699 loss: 5.24156203e-05
Iter: 1700 loss: 5.30566613e-05
Iter: 1701 loss: 5.24151546e-05
Iter: 1702 loss: 5.23734525e-05
Iter: 1703 loss: 5.24232892e-05
Iter: 1704 loss: 5.23513518e-05
Iter: 1705 loss: 5.22989722e-05
Iter: 1706 loss: 5.23911222e-05
Iter: 1707 loss: 5.22762093e-05
Iter: 1708 loss: 5.22251576e-05
Iter: 1709 loss: 5.23353738e-05
Iter: 1710 loss: 5.22054252e-05
Iter: 1711 loss: 5.21651e-05
Iter: 1712 loss: 5.2765743e-05
Iter: 1713 loss: 5.21651345e-05
Iter: 1714 loss: 5.21215952e-05
Iter: 1715 loss: 5.2159332e-05
Iter: 1716 loss: 5.2096002e-05
Iter: 1717 loss: 5.20632275e-05
Iter: 1718 loss: 5.202174e-05
Iter: 1719 loss: 5.20183057e-05
Iter: 1720 loss: 5.19546847e-05
Iter: 1721 loss: 5.21862e-05
Iter: 1722 loss: 5.19389723e-05
Iter: 1723 loss: 5.18952438e-05
Iter: 1724 loss: 5.248261e-05
Iter: 1725 loss: 5.18950692e-05
Iter: 1726 loss: 5.18583183e-05
Iter: 1727 loss: 5.18617708e-05
Iter: 1728 loss: 5.18298184e-05
Iter: 1729 loss: 5.17861154e-05
Iter: 1730 loss: 5.19590394e-05
Iter: 1731 loss: 5.17761728e-05
Iter: 1732 loss: 5.17361259e-05
Iter: 1733 loss: 5.17100489e-05
Iter: 1734 loss: 5.16946857e-05
Iter: 1735 loss: 5.16302243e-05
Iter: 1736 loss: 5.19276327e-05
Iter: 1737 loss: 5.16180444e-05
Iter: 1738 loss: 5.15657521e-05
Iter: 1739 loss: 5.174541e-05
Iter: 1740 loss: 5.15517313e-05
Iter: 1741 loss: 5.15047941e-05
Iter: 1742 loss: 5.18341367e-05
Iter: 1743 loss: 5.15003667e-05
Iter: 1744 loss: 5.1458097e-05
Iter: 1745 loss: 5.14320345e-05
Iter: 1746 loss: 5.14151106e-05
Iter: 1747 loss: 5.13678751e-05
Iter: 1748 loss: 5.19923778e-05
Iter: 1749 loss: 5.13677951e-05
Iter: 1750 loss: 5.13240229e-05
Iter: 1751 loss: 5.15571e-05
Iter: 1752 loss: 5.13171763e-05
Iter: 1753 loss: 5.12893348e-05
Iter: 1754 loss: 5.12345796e-05
Iter: 1755 loss: 5.23149502e-05
Iter: 1756 loss: 5.12340921e-05
Iter: 1757 loss: 5.11818798e-05
Iter: 1758 loss: 5.15111751e-05
Iter: 1759 loss: 5.11757098e-05
Iter: 1760 loss: 5.113308e-05
Iter: 1761 loss: 5.1281444e-05
Iter: 1762 loss: 5.11216786e-05
Iter: 1763 loss: 5.10741665e-05
Iter: 1764 loss: 5.12885526e-05
Iter: 1765 loss: 5.1064897e-05
Iter: 1766 loss: 5.10287464e-05
Iter: 1767 loss: 5.1021314e-05
Iter: 1768 loss: 5.09973834e-05
Iter: 1769 loss: 5.09412857e-05
Iter: 1770 loss: 5.1015e-05
Iter: 1771 loss: 5.09129386e-05
Iter: 1772 loss: 5.08590711e-05
Iter: 1773 loss: 5.11906255e-05
Iter: 1774 loss: 5.08525409e-05
Iter: 1775 loss: 5.08035155e-05
Iter: 1776 loss: 5.08705671e-05
Iter: 1777 loss: 5.07791701e-05
Iter: 1778 loss: 5.07282966e-05
Iter: 1779 loss: 5.10003701e-05
Iter: 1780 loss: 5.07206278e-05
Iter: 1781 loss: 5.06776778e-05
Iter: 1782 loss: 5.08396042e-05
Iter: 1783 loss: 5.06669239e-05
Iter: 1784 loss: 5.06306314e-05
Iter: 1785 loss: 5.08071607e-05
Iter: 1786 loss: 5.06242141e-05
Iter: 1787 loss: 5.0588631e-05
Iter: 1788 loss: 5.08328085e-05
Iter: 1789 loss: 5.05853532e-05
Iter: 1790 loss: 5.05601711e-05
Iter: 1791 loss: 5.05042262e-05
Iter: 1792 loss: 5.13034283e-05
Iter: 1793 loss: 5.05015632e-05
Iter: 1794 loss: 5.04405252e-05
Iter: 1795 loss: 5.06442339e-05
Iter: 1796 loss: 5.04240888e-05
Iter: 1797 loss: 5.03737356e-05
Iter: 1798 loss: 5.06216165e-05
Iter: 1799 loss: 5.03650226e-05
Iter: 1800 loss: 5.03214214e-05
Iter: 1801 loss: 5.07285949e-05
Iter: 1802 loss: 5.03195115e-05
Iter: 1803 loss: 5.0285802e-05
Iter: 1804 loss: 5.0243194e-05
Iter: 1805 loss: 5.02397488e-05
Iter: 1806 loss: 5.01848444e-05
Iter: 1807 loss: 5.03070696e-05
Iter: 1808 loss: 5.01637587e-05
Iter: 1809 loss: 5.01097202e-05
Iter: 1810 loss: 5.04079762e-05
Iter: 1811 loss: 5.01017348e-05
Iter: 1812 loss: 5.0056904e-05
Iter: 1813 loss: 5.02217663e-05
Iter: 1814 loss: 5.00458918e-05
Iter: 1815 loss: 5.00029455e-05
Iter: 1816 loss: 5.0092287e-05
Iter: 1817 loss: 4.9986149e-05
Iter: 1818 loss: 4.99430535e-05
Iter: 1819 loss: 5.02020193e-05
Iter: 1820 loss: 4.99378075e-05
Iter: 1821 loss: 4.99060407e-05
Iter: 1822 loss: 5.01107315e-05
Iter: 1823 loss: 4.99026246e-05
Iter: 1824 loss: 4.98683803e-05
Iter: 1825 loss: 4.99303787e-05
Iter: 1826 loss: 4.98537411e-05
Iter: 1827 loss: 4.98241934e-05
