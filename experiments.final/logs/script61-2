+ RUN=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='0 1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 4000 				 --batch_size 5000 				 --max_epochs 1000 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 0 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 4000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d032e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d0347048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d03f8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d0404510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d044af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d034f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d036f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d02d3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d0303950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d02ad400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d02c1e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d02738c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d028a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d0226d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d01ce840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d01fe2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d01fad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d01be730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d015e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d0177bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d01376a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d0148158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d00cfb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d0108620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d00c10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d00bfae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d0078598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d002f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3d0029a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3c8e72510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3c8e8dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3c8e359d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3c8e6c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3c8e6bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3c8e1b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3c8dbe400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.01094107
test_loss: 0.011803914
train_loss: 0.007948561
test_loss: 0.008138282
train_loss: 0.005591386
test_loss: 0.0051663215
train_loss: 0.0042072516
test_loss: 0.0032996712
train_loss: 0.0032494618
test_loss: 0.003143694
train_loss: 0.0029546795
test_loss: 0.00258143
train_loss: 0.0022783277
test_loss: 0.002400327
train_loss: 0.0021792997
test_loss: 0.0019738614
train_loss: 0.0017923851
test_loss: 0.0019370833
train_loss: 0.0017853363
test_loss: 0.0017885844
train_loss: 0.0017389171
test_loss: 0.0018192439
train_loss: 0.0016906837
test_loss: 0.0018916948
train_loss: 0.0017638201
test_loss: 0.0017442494
train_loss: 0.0016617501
test_loss: 0.0017247678
train_loss: 0.0015686193
test_loss: 0.0015206734
train_loss: 0.0015346048
test_loss: 0.0014447293
train_loss: 0.0012651094
test_loss: 0.001583
train_loss: 0.0014469007
test_loss: 0.0012583781
train_loss: 0.0013579414
test_loss: 0.0014396934
train_loss: 0.0013065422
test_loss: 0.0013643114
train_loss: 0.0014550632
test_loss: 0.0013998416
train_loss: 0.0012478414
test_loss: 0.0015351038
train_loss: 0.0012413948
test_loss: 0.0013872272
train_loss: 0.0013991327
test_loss: 0.0013321406
train_loss: 0.001254576
test_loss: 0.0014114823
train_loss: 0.0011711544
test_loss: 0.0014507422
train_loss: 0.0012631525
test_loss: 0.0012861369
train_loss: 0.0011340016
test_loss: 0.0013242976
train_loss: 0.0012119265
test_loss: 0.0014874141
train_loss: 0.0012405617
test_loss: 0.0014559644
train_loss: 0.0013094869
test_loss: 0.0014624101
train_loss: 0.0011854349
test_loss: 0.0014974476
train_loss: 0.0010849345
test_loss: 0.0013818042
train_loss: 0.0010796713
test_loss: 0.0012665838
train_loss: 0.0011650061
test_loss: 0.0013487984
train_loss: 0.0013063025
test_loss: 0.0014096667
train_loss: 0.0012816675
test_loss: 0.0013521669
train_loss: 0.0015283087
test_loss: 0.0014295393
train_loss: 0.0013851023
test_loss: 0.0013576879
train_loss: 0.0011563187
test_loss: 0.0014102781
train_loss: 0.0012422655
test_loss: 0.0014047958
train_loss: 0.0012464154
test_loss: 0.0013458295
train_loss: 0.0013625398
test_loss: 0.0016476202
train_loss: 0.0011464875
test_loss: 0.0016855868
train_loss: 0.001528285
test_loss: 0.0013173899
train_loss: 0.0014156313
test_loss: 0.0014022925
train_loss: 0.0010146073
test_loss: 0.0017160508
train_loss: 0.0012877958
test_loss: 0.0015364806
train_loss: 0.0015028077
test_loss: 0.0014044319
train_loss: 0.0014199524
test_loss: 0.0013636163
train_loss: 0.0012585633
test_loss: 0.0014340649
train_loss: 0.001294399
test_loss: 0.0013994786
train_loss: 0.0012778413
test_loss: 0.0012285277
train_loss: 0.0013854716
test_loss: 0.0012863765
train_loss: 0.0012881687
test_loss: 0.0013154129
train_loss: 0.001117126
test_loss: 0.0013889627
train_loss: 0.0011753541
test_loss: 0.0012962508
train_loss: 0.0013337235
test_loss: 0.0012925777
train_loss: 0.0012365414
test_loss: 0.0015426788
train_loss: 0.0013579909
test_loss: 0.0015554577
train_loss: 0.0011772771
test_loss: 0.0014706635
train_loss: 0.0014569676
test_loss: 0.0014608405
train_loss: 0.0015320398
test_loss: 0.0013274914
train_loss: 0.0011475713
test_loss: 0.0014671962
train_loss: 0.0012750655
test_loss: 0.0016416946
train_loss: 0.0013256996
test_loss: 0.0013818976
train_loss: 0.0011020493
test_loss: 0.0014340156
train_loss: 0.0010442102
test_loss: 0.0016067604
train_loss: 0.0011988063
test_loss: 0.001605857
train_loss: 0.0012362347
test_loss: 0.00141263
train_loss: 0.0013304516
test_loss: 0.0013808247
train_loss: 0.0012325618
test_loss: 0.001310263
train_loss: 0.0010803082
test_loss: 0.0014532433
train_loss: 0.0011835457
test_loss: 0.0017254204
train_loss: 0.0013654768
test_loss: 0.00137885
train_loss: 0.0011753298
test_loss: 0.0014273183
train_loss: 0.0013031523
test_loss: 0.0013177798
train_loss: 0.0013140468
test_loss: 0.0014252223
train_loss: 0.0011684666
test_loss: 0.0014177279
train_loss: 0.0014312477
test_loss: 0.0014069285
train_loss: 0.0012374036
test_loss: 0.0014206825
train_loss: 0.0013247962
test_loss: 0.0013877124
train_loss: 0.0012467889
test_loss: 0.0012653184
train_loss: 0.0010856831
test_loss: 0.0014973487
train_loss: 0.0013012243
test_loss: 0.0016260507
train_loss: 0.0014491405
test_loss: 0.0014147711
train_loss: 0.0010934796
test_loss: 0.001448408
train_loss: 0.0011251096
test_loss: 0.0012508573
train_loss: 0.0011940202
test_loss: 0.0013806543
train_loss: 0.0012699995
test_loss: 0.0014861295
train_loss: 0.0012534237
test_loss: 0.0013938171
train_loss: 0.0010961302
test_loss: 0.0012953841
train_loss: 0.001152013
test_loss: 0.0012936258
train_loss: 0.0012166655
test_loss: 0.0013782667
train_loss: 0.0011343957
test_loss: 0.0012903628
train_loss: 0.0012092586
test_loss: 0.0013456442
train_loss: 0.0011784807
test_loss: 0.0013329095
train_loss: 0.0013227484
test_loss: 0.0013302879
train_loss: 0.0013738613
test_loss: 0.0013891921
train_loss: 0.0011146464
test_loss: 0.0013725342
train_loss: 0.0011105423
test_loss: 0.0012731657
train_loss: 0.0013477887
test_loss: 0.0013307729
train_loss: 0.0011969565
test_loss: 0.0012417355
train_loss: 0.0011162387
test_loss: 0.0012604519
train_loss: 0.0010507694
test_loss: 0.0012985488
train_loss: 0.0012471541
test_loss: 0.0015696547
train_loss: 0.0014312072
test_loss: 0.0013906292
train_loss: 0.0014577884
test_loss: 0.0016963151
train_loss: 0.0015756987
test_loss: 0.0017518407
train_loss: 0.0013952479
test_loss: 0.0014773534
train_loss: 0.002067605
test_loss: 0.0015987098
train_loss: 0.0021345457
test_loss: 0.0014885676
train_loss: 0.001593668
test_loss: 0.0018245856
train_loss: 0.0014818977
test_loss: 0.0021334398
train_loss: 0.001373423
test_loss: 0.0018810533
train_loss: 0.0016391989
test_loss: 0.0013516946
train_loss: 0.0015115582
test_loss: 0.0015715152
train_loss: 0.0012359446
test_loss: 0.0019774605
train_loss: 0.0016029575
test_loss: 0.0020512573
train_loss: 0.0015248797
test_loss: 0.0021893198
train_loss: 0.0017043932
test_loss: 0.0020458072
train_loss: 0.0017423781
test_loss: 0.0017568242
train_loss: 0.001578042
test_loss: 0.0019594966
train_loss: 0.0016831778
test_loss: 0.0018799709
train_loss: 0.0014613221
test_loss: 0.001958852
train_loss: 0.0016310287
test_loss: 0.0016686636
train_loss: 0.0012061492
test_loss: 0.001891721
train_loss: 0.0016623817
test_loss: 0.0018065647
train_loss: 0.0014941656
test_loss: 0.0017476464
train_loss: 0.0013335382
test_loss: 0.0018597067
train_loss: 0.0016045806
test_loss: 0.001759858
train_loss: 0.0015291908
test_loss: 0.0017247597
train_loss: 0.001561279
test_loss: 0.0015575186
train_loss: 0.001297257
test_loss: 0.0017726091
train_loss: 0.0015839748
test_loss: 0.0016994067
train_loss: 0.0014300916
test_loss: 0.0015392229
train_loss: 0.0011259282
test_loss: 0.0017898863
train_loss: 0.0015211238
test_loss: 0.0016864269
train_loss: 0.001415416
test_loss: 0.0016084115
train_loss: 0.0013177838
test_loss: 0.0015707006
train_loss: 0.0012604785
test_loss: 0.0017710547
train_loss: 0.0014143937
test_loss: 0.001675897
train_loss: 0.0013741802
test_loss: 0.0015549178
train_loss: 0.0011750995
test_loss: 0.0017408156
train_loss: 0.001490536
test_loss: 0.0017044466
train_loss: 0.0014632536
test_loss: 0.0015505123
train_loss: 0.0011865497
test_loss: 0.0016710554
train_loss: 0.0014537349
test_loss: 0.0015458189
train_loss: 0.0013057154
test_loss: 0.0016396522
train_loss: 0.0013784354
test_loss: 0.0016084193
train_loss: 0.001281197
test_loss: 0.0016170505
train_loss: 0.0013470924
test_loss: 0.0016243363
train_loss: 0.0013358416
test_loss: 0.001501505
train_loss: 0.0011695577
test_loss: 0.0015382404
train_loss: 0.0011831927
test_loss: 0.0015836763
train_loss: 0.0012688116
test_loss: 0.0014687259
train_loss: 0.0013622644
test_loss: 0.0013817911
train_loss: 0.0012755403
test_loss: 0.001387342
train_loss: 0.001083524
test_loss: 0.0014372016
train_loss: 0.0015113207
test_loss: 0.0014767378
train_loss: 0.0014818789
test_loss: 0.0015216104
train_loss: 0.0012575431
test_loss: 0.001309736
train_loss: 0.0013110986
test_loss: 0.0013125073
train_loss: 0.0011776418
test_loss: 0.0012857127
train_loss: 0.0010799863
test_loss: 0.0013459388
train_loss: 0.0013788134
test_loss: 0.0015653588
train_loss: 0.0012540985
test_loss: 0.0013643248
train_loss: 0.0010834412
test_loss: 0.0014457499
train_loss: 0.0012130188
test_loss: 0.0013307282
train_loss: 0.0013879908
test_loss: 0.0013778809
train_loss: 0.0012436291
test_loss: 0.0015025173
train_loss: 0.0011687689
test_loss: 0.0016585717
train_loss: 0.0013545434
test_loss: 0.0013537516
train_loss: 0.0011879691
test_loss: 0.001465149
train_loss: 0.0011178999
test_loss: 0.0014227132
train_loss: 0.0011324182
test_loss: 0.0013488912
train_loss: 0.0011995488
test_loss: 0.0012135275
train_loss: 0.001083104
test_loss: 0.0012017989
train_loss: 0.0010649494
test_loss: 0.0012407758
train_loss: 0.0010898216
test_loss: 0.0014675629
train_loss: 0.0011024518
test_loss: 0.0013952304
train_loss: 0.0010139638
test_loss: 0.0013943393
train_loss: 0.0011315309
test_loss: 0.0014046411
train_loss: 0.0011352868
test_loss: 0.001332545
train_loss: 0.0009815248
test_loss: 0.0014118544
train_loss: 0.0011341283
test_loss: 0.0013103759
train_loss: 0.00096839236
test_loss: 0.0012709409
train_loss: 0.0011779143
test_loss: 0.001255965
train_loss: 0.0011561112
test_loss: 0.001372962
train_loss: 0.001159269
test_loss: 0.0012346742
train_loss: 0.0011741143
test_loss: 0.0012860859
train_loss: 0.0011785913
test_loss: 0.0013099844
train_loss: 0.0011205908
test_loss: 0.0013152602
train_loss: 0.0011331006
test_loss: 0.0011863628
train_loss: 0.001246686
test_loss: 0.0013195004
train_loss: 0.0011753519
test_loss: 0.0012694758
train_loss: 0.0011880386
test_loss: 0.0014639581
train_loss: 0.0011737542
test_loss: 0.0015489798
train_loss: 0.0012639393
test_loss: 0.0013367925
train_loss: 0.0010096177
test_loss: 0.0013508208
train_loss: 0.00096107693
test_loss: 0.001318167
train_loss: 0.001025769
test_loss: 0.0013085669
train_loss: 0.0008862567
test_loss: 0.0013767254
train_loss: 0.0010511489
test_loss: 0.0012326294
train_loss: 0.0010024554
test_loss: 0.0013082025
train_loss: 0.0009990842
test_loss: 0.0015409568
train_loss: 0.0010252107
test_loss: 0.0015686289
train_loss: 0.0017114239
test_loss: 0.0013614671
train_loss: 0.0011683599
test_loss: 0.0018845489
train_loss: 0.0015409417
test_loss: 0.00205167
train_loss: 0.0012740841
test_loss: 0.0018350468
train_loss: 0.0012845511
test_loss: 0.0014644867
train_loss: 0.0011059969
test_loss: 0.0014144867
train_loss: 0.0015008321
test_loss: 0.0013886373
train_loss: 0.0012572983
test_loss: 0.0015122875
train_loss: 0.0010775877
test_loss: 0.0014263086
train_loss: 0.0012706774
test_loss: 0.0012085015
train_loss: 0.00149574
test_loss: 0.0014631831
train_loss: 0.0011525737
test_loss: 0.0015447359
train_loss: 0.0011926966
test_loss: 0.0014358816
train_loss: 0.0012800206
test_loss: 0.0012447339
train_loss: 0.0012254952
test_loss: 0.0014086438
train_loss: 0.0010167046
test_loss: 0.0015403671
train_loss: 0.0012942903
test_loss: 0.0013355433
train_loss: 0.0013962183
test_loss: 0.0015556598
train_loss: 0.0011849379
test_loss: 0.0014192698
train_loss: 0.001201279
test_loss: 0.0013659383
train_loss: 0.0011944539
test_loss: 0.001279757
train_loss: 0.0010302045
test_loss: 0.0014393716
train_loss: 0.0011424002
test_loss: 0.0013494883
train_loss: 0.0011699344
test_loss: 0.0016236029
train_loss: 0.0011906965
test_loss: 0.0015660462
train_loss: 0.0013610644
test_loss: 0.0013871779
train_loss: 0.0011684913
test_loss: 0.0015449177
train_loss: 0.0010451436
test_loss: 0.0016135861
train_loss: 0.0013045863
test_loss: 0.0014924222
train_loss: 0.0012154254
test_loss: 0.0014451846
train_loss: 0.0011320596
test_loss: 0.0014968163
train_loss: 0.0013557883
test_loss: 0.0014278904
train_loss: 0.0012699033
test_loss: 0.001325265
train_loss: 0.001087559
test_loss: 0.0017337314
train_loss: 0.0016140138
test_loss: 0.0014582969
train_loss: 0.0012433889
test_loss: 0.0013974508
train_loss: 0.0012831365
test_loss: 0.0016707964
train_loss: 0.001280709
test_loss: 0.001526437
train_loss: 0.00113969
test_loss: 0.0014693561
train_loss: 0.0014583739
test_loss: 0.0014588899
train_loss: 0.001490013
test_loss: 0.0015165852
train_loss: 0.0013711028
test_loss: 0.0013374251
train_loss: 0.001378236
test_loss: 0.0014389686
train_loss: 0.0011015311
test_loss: 0.0016230188
train_loss: 0.0011705707
test_loss: 0.001407553
train_loss: 0.0011336379
test_loss: 0.0013275256
train_loss: 0.0010014541
test_loss: 0.0014133357
train_loss: 0.0009121164
test_loss: 0.0014698945
train_loss: 0.0010636611
test_loss: 0.0015113428
train_loss: 0.0012639137
test_loss: 0.0013688569
train_loss: 0.0012481832
test_loss: 0.0014016586
train_loss: 0.0013198061
test_loss: 0.0012276844
train_loss: 0.0010843766
test_loss: 0.0014612569
train_loss: 0.0010813039
test_loss: 0.0013507286
train_loss: 0.0010023755
test_loss: 0.0016584774
train_loss: 0.0012656306
test_loss: 0.0014195675
train_loss: 0.0011705627
test_loss: 0.0014745328
train_loss: 0.0011125049
test_loss: 0.0014099645
train_loss: 0.0012005193
test_loss: 0.0015482872
train_loss: 0.0016170103
test_loss: 0.0014538935
train_loss: 0.0015918701
test_loss: 0.0016058347
train_loss: 0.0011863362
test_loss: 0.0019852112
train_loss: 0.0015447363
test_loss: 0.001688864
train_loss: 0.0015088783
test_loss: 0.0016547493
train_loss: 0.0013179765
test_loss: 0.0017664668
train_loss: 0.001569552
test_loss: 0.001403245
train_loss: 0.0012554629
test_loss: 0.0016445382
train_loss: 0.0011547866
test_loss: 0.001485904
train_loss: 0.0011387938
test_loss: 0.0012680218
train_loss: 0.0011618775
test_loss: 0.0012772185
train_loss: 0.0011341847
test_loss: 0.0013171386
train_loss: 0.00091215805
test_loss: 0.0013610227
train_loss: 0.0009541949
test_loss: 0.0014695791
train_loss: 0.0009979212
test_loss: 0.0014734534
train_loss: 0.0011973518
test_loss: 0.0013809064
train_loss: 0.0010923924
test_loss: 0.0013049243
train_loss: 0.0011413015
test_loss: 0.0012174225
train_loss: 0.0011742858
test_loss: 0.0012205002
train_loss: 0.0010810383
test_loss: 0.0013026182
train_loss: 0.0011421507
test_loss: 0.0012458008
train_loss: 0.0011252852
test_loss: 0.0012276339
train_loss: 0.0011082304
test_loss: 0.0012662434
train_loss: 0.0011135293
test_loss: 0.0012838211
train_loss: 0.0010968199
test_loss: 0.0013105082
train_loss: 0.0011693418
test_loss: 0.0012220555
train_loss: 0.0012763112
test_loss: 0.0013401576
train_loss: 0.0012908267
test_loss: 0.0015799658
train_loss: 0.0011454439
test_loss: 0.0014308143
train_loss: 0.0011676403
test_loss: 0.0013719844
train_loss: 0.0013617604
test_loss: 0.0014617962
train_loss: 0.0012508186
test_loss: 0.0012877474
train_loss: 0.0010629456
test_loss: 0.0013060209
train_loss: 0.0010849547
test_loss: 0.0015491126
train_loss: 0.0016459586
test_loss: 0.0012356183
train_loss: 0.0013688192
test_loss: 0.0017332423
train_loss: 0.0012746236
test_loss: 0.002116775
train_loss: 0.0014218732
test_loss: 0.0022186032
train_loss: 0.0018580869
test_loss: 0.0022789738
train_loss: 0.0016990404
test_loss: 0.0018574143
train_loss: 0.001415762
test_loss: 0.0019761224
train_loss: 0.0014799784
test_loss: 0.0021592532
train_loss: 0.0017606532
test_loss: 0.002052485
train_loss: 0.0017664605
test_loss: 0.0017862766
train_loss: 0.0018626128
test_loss: 0.0015815245
train_loss: 0.002152174
test_loss: 0.0018275805
train_loss: 0.0018213496
test_loss: 0.0018277946
train_loss: 0.0017149503
test_loss: 0.0015816708
train_loss: 0.0017100238
test_loss: 0.0017266317
train_loss: 0.0012497086
test_loss: 0.0016060174
train_loss: 0.001784292
test_loss: 0.0013802138
train_loss: 0.0019174998
test_loss: 0.0018343839
train_loss: 0.0017772552
test_loss: 0.0018497371
train_loss: 0.001558397
test_loss: 0.0016240864
train_loss: 0.0013596951
test_loss: 0.0015858018
train_loss: 0.0011019093
test_loss: 0.0014494023
train_loss: 0.0010472409
test_loss: 0.00151377
train_loss: 0.001016486
test_loss: 0.0013954645
train_loss: 0.001031884
test_loss: 0.0013109101
train_loss: 0.0009350863
test_loss: 0.001397023
train_loss: 0.00091152545
test_loss: 0.0017022545
train_loss: 0.001306717
test_loss: 0.0013801414
train_loss: 0.0011740257
test_loss: 0.0013718092
train_loss: 0.0012346361
test_loss: 0.0013185593
train_loss: 0.0011071801
test_loss: 0.0016436642
train_loss: 0.0011615747
test_loss: 0.0013780715
train_loss: 0.0010512117
test_loss: 0.0014172449
train_loss: 0.0010947965
test_loss: 0.0015821272
train_loss: 0.0014126878
test_loss: 0.0013203024
train_loss: 0.0011410252
test_loss: 0.0016650922
train_loss: 0.0010819121
test_loss: 0.0014761648
train_loss: 0.0010069404
test_loss: 0.0014604682
train_loss: 0.0010659355
test_loss: 0.0016960263
train_loss: 0.0013696363
test_loss: 0.0013003771
train_loss: 0.001090588
test_loss: 0.0013211981
train_loss: 0.0011525752
test_loss: 0.0013834796
train_loss: 0.0010985213
test_loss: 0.0012945764
train_loss: 0.001000879
test_loss: 0.0014396012
train_loss: 0.0011369612
test_loss: 0.0014030212
train_loss: 0.0012744577
test_loss: 0.0015410982
train_loss: 0.001166927
test_loss: 0.0015694993
train_loss: 0.0011496276
test_loss: 0.0015383335
train_loss: 0.0012288184
test_loss: 0.0014641422
train_loss: 0.0015130364
test_loss: 0.0016263246
train_loss: 0.0014161101
test_loss: 0.0013574756
train_loss: 0.0010827829
test_loss: 0.0015376959
train_loss: 0.0011092353
test_loss: 0.0015459929
train_loss: 0.0012977157
test_loss: 0.0013946203
train_loss: 0.0015445219
test_loss: 0.0016277734
train_loss: 0.001438744
test_loss: 0.001462526
train_loss: 0.0016250417
test_loss: 0.0014182685
train_loss: 0.0015425596
test_loss: 0.0017919089
train_loss: 0.001380377
test_loss: 0.0015374896
train_loss: 0.0014764037
test_loss: 0.0015114496
train_loss: 0.0011348157
test_loss: 0.0015438781
train_loss: 0.0010345516
test_loss: 0.0015883059
train_loss: 0.0012662573
test_loss: 0.001566104
train_loss: 0.0013847504
test_loss: 0.0015263223
train_loss: 0.0009765161
test_loss: 0.0015567625
train_loss: 0.0010387189
test_loss: 0.0014377091
train_loss: 0.001119468
test_loss: 0.0013911284
train_loss: 0.0010419316
test_loss: 0.0014980256
train_loss: 0.0011721037
test_loss: 0.001392015
train_loss: 0.001239745
test_loss: 0.0015778922
train_loss: 0.0010201113
test_loss: 0.0015980481
train_loss: 0.0011642509
test_loss: 0.0014963937
train_loss: 0.0011639023
test_loss: 0.0013212305
train_loss: 0.0011557677
test_loss: 0.0014044575
train_loss: 0.00083456817
test_loss: 0.0014857346
train_loss: 0.0010134452
test_loss: 0.0013634241
train_loss: 0.0010349144
test_loss: 0.00149431
train_loss: 0.0011910853
test_loss: 0.0014592698
train_loss: 0.0012195233
test_loss: 0.0014948195
train_loss: 0.0011979812
test_loss: 0.0013244096
train_loss: 0.001101445
test_loss: 0.0012842453
train_loss: 0.0012226232
test_loss: 0.0014502538
train_loss: 0.0010982534
test_loss: 0.0013264882
train_loss: 0.0009914095
test_loss: 0.0013262018
train_loss: 0.0009623745
test_loss: 0.0012450103
train_loss: 0.0010940101
test_loss: 0.0011929141
train_loss: 0.0011831508
test_loss: 0.0012629745
train_loss: 0.001160145
test_loss: 0.0012683934
train_loss: 0.0010393693
test_loss: 0.0012811436
train_loss: 0.0011953423
test_loss: 0.0014125992
train_loss: 0.0009768005
test_loss: 0.0014125234
train_loss: 0.0011219686
test_loss: 0.0013676902
train_loss: 0.0012586717
test_loss: 0.0012027161
train_loss: 0.0010693407
test_loss: 0.0014091578
train_loss: 0.0009986098
test_loss: 0.0014615337
train_loss: 0.0009037992
test_loss: 0.0014708743
train_loss: 0.001078649
test_loss: 0.001402504
train_loss: 0.0011962581
test_loss: 0.0012754756
train_loss: 0.0010228478
test_loss: 0.0015069136
train_loss: 0.0014486022
test_loss: 0.0014344784
train_loss: 0.0010141515
test_loss: 0.00175898
train_loss: 0.0014108887
test_loss: 0.0014101177
train_loss: 0.001476102
test_loss: 0.001558057
train_loss: 0.0012203506
test_loss: 0.0017165288
train_loss: 0.0011535209
test_loss: 0.0015357604
train_loss: 0.0012046882
test_loss: 0.0013675196
train_loss: 0.0010896166
test_loss: 0.0014532255
train_loss: 0.0014189539
test_loss: 0.0013782737
train_loss: 0.0013534261
test_loss: 0.0014431366
train_loss: 0.0009498712
test_loss: 0.0015356991
train_loss: 0.001296957
test_loss: 0.0016855116
train_loss: 0.0011582916
test_loss: 0.0014252353
train_loss: 0.0011273138
test_loss: 0.0012201074
train_loss: 0.0010187957
test_loss: 0.0012819726
train_loss: 0.0010548729
test_loss: 0.0012831885
train_loss: 0.0009117922
test_loss: 0.00157428
train_loss: 0.0012734374
test_loss: 0.0013692523
train_loss: 0.0013230194
test_loss: 0.0013137145
train_loss: 0.0011443029
test_loss: 0.001424972
train_loss: 0.0010682298
test_loss: 0.0014156558
train_loss: 0.0011333502
test_loss: 0.0015377129
train_loss: 0.0014128654
test_loss: 0.0013574942
train_loss: 0.0011362776
test_loss: 0.001639199
train_loss: 0.0010802407
test_loss: 0.0015474696
train_loss: 0.0012880337
test_loss: 0.0013889319
train_loss: 0.0013223435
test_loss: 0.0013787344
train_loss: 0.0010651328
test_loss: 0.0016606152
train_loss: 0.0011396388
test_loss: 0.001617549
train_loss: 0.0013379563
test_loss: 0.0014670623
train_loss: 0.0011060942
test_loss: 0.0013344975
train_loss: 0.0010793422
test_loss: 0.0013898091
train_loss: 0.0010296567
test_loss: 0.0016803283
train_loss: 0.0011733208
test_loss: 0.001403226
train_loss: 0.0010736256
test_loss: 0.0015281388
train_loss: 0.0013309859
test_loss: 0.0013916272
train_loss: 0.0010707834
test_loss: 0.0012835817
train_loss: 0.0011177822
test_loss: 0.0012918455
train_loss: 0.0010328139
test_loss: 0.0012669661
train_loss: 0.0010349017
test_loss: 0.0016049006
train_loss: 0.0012731322
test_loss: 0.0014480497
train_loss: 0.0010789181
test_loss: 0.0013785907
train_loss: 0.001047821
test_loss: 0.0012836496
train_loss: 0.0009775354
test_loss: 0.0013233823
train_loss: 0.0011709193
test_loss: 0.0014139004
train_loss: 0.0014442801
test_loss: 0.001348926
train_loss: 0.0010177864
test_loss: 0.0019657325
train_loss: 0.001529832
test_loss: 0.002132527
train_loss: 0.001733734
test_loss: 0.0022929043
train_loss: 0.0018486759
test_loss: 0.0017950598
train_loss: 0.0014686941
test_loss: 0.0017873481
train_loss: 0.0014782657
test_loss: 0.0015465637
train_loss: 0.0015056035
test_loss: 0.0015116449
train_loss: 0.0011070219
test_loss: 0.001634608
train_loss: 0.001151312
test_loss: 0.0016841388
train_loss: 0.0012016208
test_loss: 0.0015500173
train_loss: 0.001860481
test_loss: 0.0018064453
train_loss: 0.0015160339
test_loss: 0.0014603424
train_loss: 0.001710316
test_loss: 0.0015717231
train_loss: 0.0017666114
test_loss: 0.001601557
train_loss: 0.0016807311
test_loss: 0.0018926199
train_loss: 0.0013339496
test_loss: 0.0017803838
train_loss: 0.0013230368
test_loss: 0.0014825622
train_loss: 0.0019705354
test_loss: 0.0018087892
train_loss: 0.0017481548
test_loss: 0.0016083857
train_loss: 0.0019147638
test_loss: 0.0019161401
train_loss: 0.0014807904
test_loss: 0.0017175424
train_loss: 0.0011155433
test_loss: 0.0019209951
train_loss: 0.0011912338
test_loss: 0.0020596348
train_loss: 0.0018272414
test_loss: 0.002065105
train_loss: 0.0014989969
test_loss: 0.0019213409
train_loss: 0.0016416812
test_loss: 0.0019189464
train_loss: 0.0014475102
test_loss: 0.0019852472
train_loss: 0.0016177876
test_loss: 0.0018700009
train_loss: 0.0014905892
test_loss: 0.0018033154
train_loss: 0.0014335216
test_loss: 0.0019504235
train_loss: 0.001723046
test_loss: 0.0017317162
train_loss: 0.0013686721
test_loss: 0.0016970272
train_loss: 0.001220162
test_loss: 0.0017070201
train_loss: 0.0013417094
test_loss: 0.0016813123
train_loss: 0.0013740202
test_loss: 0.0017168232
train_loss: 0.0012822457
test_loss: 0.0016420613
train_loss: 0.0012414803
test_loss: 0.0016178676
train_loss: 0.0013237749
test_loss: 0.0016168366
train_loss: 0.001207074
test_loss: 0.0016496293
train_loss: 0.0011827175
test_loss: 0.001626621
train_loss: 0.0011446843
test_loss: 0.0016533459
train_loss: 0.0012663583
test_loss: 0.0017438879
train_loss: 0.0013413617
test_loss: 0.0015727449
train_loss: 0.0012025421
test_loss: 0.0015704512
train_loss: 0.0011931275
test_loss: 0.0015961521
train_loss: 0.0013295158
test_loss: 0.0015417971
train_loss: 0.0015436776
test_loss: 0.0015009259
train_loss: 0.0013220418
test_loss: 0.0016100887
train_loss: 0.0011283187
test_loss: 0.0020289873
train_loss: 0.0015546997
test_loss: 0.0020487828
train_loss: 0.0015641972
test_loss: 0.0021131937
train_loss: 0.0014568805
test_loss: 0.0018057006
train_loss: 0.0013507608
test_loss: 0.0020409871
train_loss: 0.0016402528
test_loss: 0.0019388399
train_loss: 0.001531548
test_loss: 0.0018820069
train_loss: 0.0013854015
test_loss: 0.0019702232
train_loss: 0.0015139654
test_loss: 0.0018751416
train_loss: 0.001479308
test_loss: 0.0019359497
train_loss: 0.0014351574
test_loss: 0.0017977403
train_loss: 0.0011934008
test_loss: 0.0020099871
train_loss: 0.0016965808
test_loss: 0.0016835554
train_loss: 0.0012578163
test_loss: 0.0018933349
train_loss: 0.0015104718
test_loss: 0.0018253434
train_loss: 0.0013687443
test_loss: 0.0017500418
train_loss: 0.001467271
test_loss: 0.0018446689
train_loss: 0.001348317
test_loss: 0.0017293283
train_loss: 0.0014033769
test_loss: 0.0018443853
train_loss: 0.0013894293
test_loss: 0.0015867405
train_loss: 0.001177001
test_loss: 0.0018412616
train_loss: 0.0012026766
test_loss: 0.0016611359
train_loss: 0.0011329409
test_loss: 0.0017623577
train_loss: 0.0013235551
test_loss: 0.0017430022
train_loss: 0.001313406
test_loss: 0.0015277148
train_loss: 0.0010166259
test_loss: 0.0016848589
train_loss: 0.0012466682
test_loss: 0.0017902864
train_loss: 0.0014537905
test_loss: 0.0016038223
train_loss: 0.0012993055
test_loss: 0.0016062203
train_loss: 0.0010749615
test_loss: 0.0016878505
train_loss: 0.0013132269
test_loss: 0.0015287234
train_loss: 0.0013662076
test_loss: 0.001538949
train_loss: 0.0011934901
test_loss: 0.001532951
train_loss: 0.001103156
test_loss: 0.0014746055
train_loss: 0.0014119516
test_loss: 0.0013280583
train_loss: 0.0013013833
test_loss: 0.0015041931
train_loss: 0.0011175302
test_loss: 0.0014784563
train_loss: 0.0011605205
test_loss: 0.001434739
train_loss: 0.001193668
test_loss: 0.0013683205
train_loss: 0.0012611683
test_loss: 0.001322266
train_loss: 0.001264615
test_loss: 0.0014001613
train_loss: 0.001008289
test_loss: 0.0015877084
train_loss: 0.001094219
test_loss: 0.0015537621
train_loss: 0.0013030145
test_loss: 0.0013570709
train_loss: 0.001040174
test_loss: 0.0014241551
train_loss: 0.001372633
test_loss: 0.0015011171
train_loss: 0.00096776686
test_loss: 0.0013499836
train_loss: 0.0009815202
test_loss: 0.0013641636
train_loss: 0.0010026884
test_loss: 0.0015490243
train_loss: 0.001286306
test_loss: 0.0013956765
train_loss: 0.0010965881
test_loss: 0.0016151882
train_loss: 0.001248312
test_loss: 0.0014325767
train_loss: 0.0011765528
test_loss: 0.0015004261
train_loss: 0.0010507028
test_loss: 0.0014060566
train_loss: 0.0010099553
test_loss: 0.0015178556
train_loss: 0.0011041722
test_loss: 0.0014241862
train_loss: 0.0010648943
test_loss: 0.0014677442
train_loss: 0.0009752666
test_loss: 0.0014470418
train_loss: 0.0010552052
test_loss: 0.001562978
train_loss: 0.0011191863
test_loss: 0.0014751476
train_loss: 0.001103028
test_loss: 0.0014085472
train_loss: 0.001045737
test_loss: 0.0014429502
train_loss: 0.0009498799
test_loss: 0.0014267922
train_loss: 0.000976343
test_loss: 0.0014262669
train_loss: 0.0010169097
test_loss: 0.0015904959
train_loss: 0.0011972712
test_loss: 0.0015462468
train_loss: 0.0011674605
test_loss: 0.0012810831
train_loss: 0.00105954
test_loss: 0.0013091516
train_loss: 0.0009726622
test_loss: 0.0012607627
train_loss: 0.0009609771
test_loss: 0.0014466634
train_loss: 0.0008407744
test_loss: 0.0015334031
train_loss: 0.0012517881
test_loss: 0.001363576
train_loss: 0.0011207954
test_loss: 0.0014509737
train_loss: 0.0011095565
test_loss: 0.0013486784
train_loss: 0.0009971311
test_loss: 0.0014858163
train_loss: 0.0012291316
test_loss: 0.001445115
train_loss: 0.0011714639
test_loss: 0.0014177068
train_loss: 0.0013859321
test_loss: 0.0016124746
train_loss: 0.0012534212
test_loss: 0.0017008869
train_loss: 0.0012301658
test_loss: 0.0016397096
train_loss: 0.0012061024
test_loss: 0.0014458437
train_loss: 0.0011626756
test_loss: 0.0013957794
train_loss: 0.0013444583
test_loss: 0.0014651576
train_loss: 0.0009124622
test_loss: 0.0018369963
train_loss: 0.0015504311
test_loss: 0.0014926146
train_loss: 0.0016027564
test_loss: 0.0016446838
train_loss: 0.0012198255
test_loss: 0.0017682122
train_loss: 0.0013546161
test_loss: 0.0017182403
train_loss: 0.0016778781
test_loss: 0.0016023025
train_loss: 0.0016130528
test_loss: 0.0016152412
train_loss: 0.0016228633
test_loss: 0.0014204436
train_loss: 0.001255398
test_loss: 0.0016703332
train_loss: 0.0013025822
test_loss: 0.0016917832
train_loss: 0.0011941767
test_loss: 0.0016944777
train_loss: 0.001414741
test_loss: 0.0013579944
train_loss: 0.0012012984
test_loss: 0.0014364768
train_loss: 0.0010620805
test_loss: 0.0016735271
train_loss: 0.0012011596
test_loss: 0.001724152
train_loss: 0.0013673152
test_loss: 0.0015883476
train_loss: 0.0010812226
test_loss: 0.001532276
train_loss: 0.0009434672
test_loss: 0.0016185837
train_loss: 0.0011624633
test_loss: 0.0017224823
train_loss: 0.0013554656
test_loss: 0.0015778767
train_loss: 0.0011484377
test_loss: 0.0015394373
train_loss: 0.0010936736
test_loss: 0.0014881474
train_loss: 0.0011473184
test_loss: 0.0013851554
train_loss: 0.001403916
test_loss: 0.0014238238
train_loss: 0.0010872111
test_loss: 0.0016446864
train_loss: 0.001239342
test_loss: 0.0015737017
train_loss: 0.0011216463
test_loss: 0.0015203181
train_loss: 0.0012354079
test_loss: 0.0014559936
train_loss: 0.0011124008
test_loss: 0.0014013737
train_loss: 0.000993592
test_loss: 0.00147742
train_loss: 0.0013320718
test_loss: 0.0014579961
train_loss: 0.0010171195
test_loss: 0.0015469268
train_loss: 0.00090772676
test_loss: 0.00155095
train_loss: 0.0009735251
test_loss: 0.001637892
train_loss: 0.0012179496
test_loss: 0.0013828996
train_loss: 0.0013017246
test_loss: 0.0014122039
train_loss: 0.0012543215
test_loss: 0.0016657064
train_loss: 0.0011792128
test_loss: 0.0015199819
train_loss: 0.001110976
test_loss: 0.001593759
train_loss: 0.0012022057
test_loss: 0.0014137117
train_loss: 0.0009489234
test_loss: 0.0014886234
train_loss: 0.00092123705
test_loss: 0.0014603535
train_loss: 0.0009378041
test_loss: 0.0015927692
train_loss: 0.001231039
test_loss: 0.0012809463
train_loss: 0.0010762046
test_loss: 0.0013619022
train_loss: 0.0010429635
test_loss: 0.0015824172
train_loss: 0.0010852949
test_loss: 0.001452353
train_loss: 0.0010534415
test_loss: 0.0013655395
train_loss: 0.0010893301
test_loss: 0.0013870471
train_loss: 0.0012807795
test_loss: 0.0014552461
train_loss: 0.00091430417
test_loss: 0.0013040235
train_loss: 0.00090951816
test_loss: 0.0012581324
train_loss: 0.0010628151
test_loss: 0.0014970341
train_loss: 0.0010953849
test_loss: 0.0014488677
train_loss: 0.001165196
test_loss: 0.0013598072
train_loss: 0.0010687286
test_loss: 0.0013735336
train_loss: 0.0010024123
test_loss: 0.00142287
train_loss: 0.0011639103
test_loss: 0.0014303031
train_loss: 0.00125635
test_loss: 0.00136784
train_loss: 0.00097399595
test_loss: 0.0014627437
train_loss: 0.001049933
test_loss: 0.0015046783
train_loss: 0.0011093477
test_loss: 0.0013659329
train_loss: 0.00093499664
test_loss: 0.0014798293
train_loss: 0.001021171
test_loss: 0.0014213254
train_loss: 0.0011090835
test_loss: 0.0013111399
train_loss: 0.0010653068
test_loss: 0.0013064123
train_loss: 0.0009743967
test_loss: 0.001396779
train_loss: 0.0010031116
test_loss: 0.0013283077
train_loss: 0.0009097741
test_loss: 0.0014268041
train_loss: 0.0011901238
test_loss: 0.001449936
train_loss: 0.001081916
test_loss: 0.0013966198
train_loss: 0.0009969629
test_loss: 0.0012572303
train_loss: 0.000982547
test_loss: 0.0013761575
train_loss: 0.0009985099
test_loss: 0.0014249304
train_loss: 0.0011888243
test_loss: 0.0013793748
train_loss: 0.0011286107
test_loss: 0.0013392088
train_loss: 0.0010189139
test_loss: 0.0012530702
train_loss: 0.0009901342
test_loss: 0.001209065
train_loss: 0.0010579894
test_loss: 0.0013815586
train_loss: 0.0010987738
test_loss: 0.0013866285
train_loss: 0.0011422373
test_loss: 0.0013330944
train_loss: 0.0012003087
test_loss: 0.0014684809
train_loss: 0.0009417572
test_loss: 0.0012938321
train_loss: 0.0009591995
test_loss: 0.001497458
train_loss: 0.0011425124
test_loss: 0.0014169659
train_loss: 0.0012522364
test_loss: 0.0013860563
train_loss: 0.0010619722
test_loss: 0.0013599615
train_loss: 0.0011063534
test_loss: 0.0014696743
train_loss: 0.0010795891
test_loss: 0.0014164757
train_loss: 0.001105448
test_loss: 0.0015215535
train_loss: 0.0012214726
test_loss: 0.001343924
train_loss: 0.0010568905
test_loss: 0.0014208131
train_loss: 0.0009490215
test_loss: 0.0013236912
train_loss: 0.0013466168
test_loss: 0.001477343
train_loss: 0.001006866
test_loss: 0.0015197556
train_loss: 0.001060009
test_loss: 0.0017113695
train_loss: 0.0014752902
test_loss: 0.0014267309
train_loss: 0.0012316268
test_loss: 0.0017028155
train_loss: 0.0012139835
test_loss: 0.0015834228
train_loss: 0.0015099287
test_loss: 0.0015311188
train_loss: 0.0010466966
test_loss: 0.0017175707
train_loss: 0.001080539
test_loss: 0.0015809159
train_loss: 0.0014724534
test_loss: 0.0014185575
train_loss: 0.0014896588
test_loss: 0.0015544631
train_loss: 0.001243366
test_loss: 0.0014672512
train_loss: 0.00096409535
test_loss: 0.0017484522
train_loss: 0.0011955511
test_loss: 0.0019543348
train_loss: 0.001363643
test_loss: 0.001710536
train_loss: 0.0012939833
test_loss: 0.0018383046
train_loss: 0.0014218669
test_loss: 0.0018241165
train_loss: 0.001381785
test_loss: 0.0016391616
train_loss: 0.0012751976
test_loss: 0.0018084737
train_loss: 0.001519752
test_loss: 0.0017093075
train_loss: 0.0011927504
test_loss: 0.0018697398
train_loss: 0.0016132047
test_loss: 0.0017685308
train_loss: 0.0014519816
test_loss: 0.0015246379
train_loss: 0.0011044913
test_loss: 0.0018401259
train_loss: 0.0014673143
test_loss: 0.0017710812
train_loss: 0.0012504373
test_loss: 0.0019307748
train_loss: 0.0014568437
test_loss: 0.0015379644
train_loss: 0.0010967229
test_loss: 0.0020495534
train_loss: 0.001545847
test_loss: 0.0016823882
train_loss: 0.00112701
test_loss: 0.001770795
train_loss: 0.0012641755
test_loss: 0.0017152114
train_loss: 0.0012600339
test_loss: 0.0016055682
train_loss: 0.0010532353
test_loss: 0.00165962
train_loss: 0.0011991585
test_loss: 0.0015534199
train_loss: 0.00127102
test_loss: 0.0013772238
train_loss: 0.0014280706
test_loss: 0.0014586847
train_loss: 0.0013414426
test_loss: 0.0015922752
train_loss: 0.0012958643
test_loss: 0.0014513042
train_loss: 0.0012596037
test_loss: 0.0013944812
train_loss: 0.0011264051
test_loss: 0.0014370026
train_loss: 0.0011559069
test_loss: 0.0013879259
train_loss: 0.0010685486
test_loss: 0.0012970181
train_loss: 0.0010723128
test_loss: 0.0013437592
train_loss: 0.0010062618
test_loss: 0.0013651395
train_loss: 0.0010274069
test_loss: 0.0012682558
train_loss: 0.0011303064
test_loss: 0.0013749854
train_loss: 0.0013026611
test_loss: 0.0014760754
train_loss: 0.0009004262
test_loss: 0.0014931494
train_loss: 0.0010982262
test_loss: 0.0015578668
train_loss: 0.0012325977
test_loss: 0.0013213606
train_loss: 0.0011803192
test_loss: 0.0015325706
train_loss: 0.0011670798
test_loss: 0.0013954646
train_loss: 0.0009832433
test_loss: 0.00138903
train_loss: 0.0009357007
test_loss: 0.001439658
train_loss: 0.0009929315
test_loss: 0.0014140948
train_loss: 0.0009742287
test_loss: 0.0014051641
train_loss: 0.0009583647
test_loss: 0.001409333
train_loss: 0.0009736222
test_loss: 0.0013404669
train_loss: 0.00086465525
test_loss: 0.0014273698
train_loss: 0.0010465689
test_loss: 0.0013480058
train_loss: 0.001213776
test_loss: 0.0014077971
train_loss: 0.0010247563
test_loss: 0.0014464715
train_loss: 0.0009976459
test_loss: 0.0014567296
train_loss: 0.0009413922
test_loss: 0.0015634414
train_loss: 0.0011244172
test_loss: 0.0013851791
train_loss: 0.0010352294
test_loss: 0.0013243399
train_loss: 0.00091937097
test_loss: 0.0012671166
train_loss: 0.00085305184
test_loss: 0.0012779945
train_loss: 0.0010329734
test_loss: 0.0013806758
train_loss: 0.0009050259
test_loss: 0.0014818817
train_loss: 0.0010146894
test_loss: 0.0015153937
train_loss: 0.0010300664
test_loss: 0.0014288139
train_loss: 0.0009683058
test_loss: 0.0014483769
train_loss: 0.0009460663
test_loss: 0.0014346109
train_loss: 0.0009218998
test_loss: 0.0014772887
train_loss: 0.0011407639
test_loss: 0.0013261621
train_loss: 0.0009813799
test_loss: 0.0014126579
train_loss: 0.0011223243
test_loss: 0.0015177419
train_loss: 0.0012229523
test_loss: 0.0014930865
train_loss: 0.00095517904
test_loss: 0.0014980986
train_loss: 0.0010142736
test_loss: 0.0013686714
train_loss: 0.0009906937
test_loss: 0.0013783715
train_loss: 0.0010029969
test_loss: 0.0015174058
train_loss: 0.0010601958
test_loss: 0.0014463216
train_loss: 0.0009500472
test_loss: 0.001512643
train_loss: 0.0010367557
test_loss: 0.0015293789
train_loss: 0.00112793
test_loss: 0.0013628926
train_loss: 0.0010155978
test_loss: 0.0014509695
train_loss: 0.0011411719
test_loss: 0.0014220823
train_loss: 0.0012866613
test_loss: 0.0014817568
train_loss: 0.0009803631
test_loss: 0.001394319
train_loss: 0.00088023965
test_loss: 0.0015327319
train_loss: 0.001069485
test_loss: 0.0013050823
train_loss: 0.001080667
test_loss: 0.0014483808
train_loss: 0.001312527
test_loss: 0.0013037289
train_loss: 0.0010960599
test_loss: 0.0014792334
train_loss: 0.0010617012
test_loss: 0.0015326361
train_loss: 0.0010516271
test_loss: 0.0014586262
train_loss: 0.000996534
test_loss: 0.0014049993
train_loss: 0.0011016964
test_loss: 0.0013231183
train_loss: 0.0009872109
test_loss: 0.001474503
train_loss: 0.0011693933
test_loss: 0.0013253568
train_loss: 0.0010715681
test_loss: 0.0012844268
train_loss: 0.0010971096
test_loss: 0.001260421
train_loss: 0.0010271271
test_loss: 0.0014949839
train_loss: 0.0013654191
test_loss: 0.0013323728
train_loss: 0.0010873283
test_loss: 0.0016160138
train_loss: 0.0011170583
test_loss: 0.0013967068
train_loss: 0.0010377881
test_loss: 0.0013918725
train_loss: 0.00088926416
test_loss: 0.0015974725
train_loss: 0.0010332435
test_loss: 0.0015429639
train_loss: 0.0011964759
test_loss: 0.0013494827
train_loss: 0.0010992705
test_loss: 0.0014948413
train_loss: 0.0009144107
test_loss: 0.0014645896
train_loss: 0.0011943143
test_loss: 0.0014580775
train_loss: 0.0011568104
test_loss: 0.0013555284
train_loss: 0.0009533342
test_loss: 0.0014390772
train_loss: 0.0010932178
test_loss: 0.0014754668
train_loss: 0.0010556597
test_loss: 0.0014012259
train_loss: 0.0010788437
test_loss: 0.001295748
train_loss: 0.0010099818
test_loss: 0.001434816
train_loss: 0.00079702836
test_loss: 0.001514613
train_loss: 0.00102136
test_loss: 0.0013526607
train_loss: 0.0010450889
test_loss: 0.0014349985
train_loss: 0.0011131269
test_loss: 0.0013061581
train_loss: 0.0010862835
test_loss: 0.0013900002
train_loss: 0.0010556255
test_loss: 0.0013963224
train_loss: 0.00125912
test_loss: 0.0014129977
train_loss: 0.00091518305
test_loss: 0.0014509478
train_loss: 0.00092448614
test_loss: 0.0014284869
train_loss: 0.0010225954
test_loss: 0.0014738856
train_loss: 0.0012265374
test_loss: 0.001419172
train_loss: 0.0009874665
test_loss: 0.0013772028
train_loss: 0.0010669837
test_loss: 0.0014840779
train_loss: 0.0009746283
test_loss: 0.0013747667
train_loss: 0.0010037763
test_loss: 0.0013876639
train_loss: 0.0009772535
test_loss: 0.0014214024
train_loss: 0.0010168068
test_loss: 0.0014482975
train_loss: 0.0010124968
test_loss: 0.0014531139
train_loss: 0.0010047584
test_loss: 0.0014643401
train_loss: 0.0009129507
test_loss: 0.0016543274
train_loss: 0.0014085837
test_loss: 0.0014107216
train_loss: 0.0011557324
test_loss: 0.0015291913
train_loss: 0.0009270335
test_loss: 0.0017577662
train_loss: 0.0013301144
test_loss: 0.0013931006
train_loss: 0.0012795532
test_loss: 0.0014150474
train_loss: 0.0011562917
test_loss: 0.0015419838
train_loss: 0.0010918811
test_loss: 0.0016007911
train_loss: 0.0011258782
test_loss: 0.0014174562
train_loss: 0.0011639881
test_loss: 0.0013894498
train_loss: 0.0011963515
test_loss: 0.0014730206
train_loss: 0.0009964786
test_loss: 0.0015191529
train_loss: 0.0009940872
test_loss: 0.0016090828
train_loss: 0.0011120819
test_loss: 0.0014585939
train_loss: 0.0013799199
test_loss: 0.0014443899
train_loss: 0.0012550103
test_loss: 0.0014926597
train_loss: 0.001106186
test_loss: 0.0014838461
train_loss: 0.0010540544
test_loss: 0.0015481054
train_loss: 0.0011095988
test_loss: 0.0014283446
train_loss: 0.0013694388
test_loss: 0.0013497947
train_loss: 0.0012669227
test_loss: 0.0015077431
train_loss: 0.0012103156
test_loss: 0.0014535827
train_loss: 0.0012878093
test_loss: 0.0015590959
train_loss: 0.0011595597
test_loss: 0.0016048508
train_loss: 0.0012752005
test_loss: 0.0014496696
train_loss: 0.0011103199
test_loss: 0.0015349032
train_loss: 0.0012583527
test_loss: 0.0015090746
train_loss: 0.0010432648
test_loss: 0.0014610508
train_loss: 0.0011790176
test_loss: 0.0013957899
train_loss: 0.0010742382
test_loss: 0.0012755948
train_loss: 0.00084451225
test_loss: 0.0013721295
train_loss: 0.00085188606
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0016978018
train_loss: 0.0010476012
test_loss: 0.001440733
train_loss: 0.0010783119
test_loss: 0.001406664
train_loss: 0.001273673
test_loss: 0.0013213063
train_loss: 0.0010920763
test_loss: 0.0012433586
train_loss: 0.00093704276
test_loss: 0.0013654303
train_loss: 0.0009974735
test_loss: 0.0014159573
train_loss: 0.0010470188
test_loss: 0.0014006249
train_loss: 0.0012310569
test_loss: 0.0014048733
train_loss: 0.00092004775
test_loss: 0.0015270093
train_loss: 0.0010433017
test_loss: 0.0015184542
train_loss: 0.0012392104
test_loss: 0.0014283224
train_loss: 0.0011488142
test_loss: 0.0013664853
train_loss: 0.0010226275
test_loss: 0.0015514371
train_loss: 0.0012362659
test_loss: 0.0014386236
train_loss: 0.0013002113
test_loss: 0.0013776172
train_loss: 0.00092004146
test_loss: 0.0013479594
train_loss: 0.0009165781
test_loss: 0.0012412994
train_loss: 0.00092469243
test_loss: 0.0012550241
train_loss: 0.0009845044
test_loss: 0.0013864554
train_loss: 0.0010109993
test_loss: 0.0014175432
train_loss: 0.0010315578
test_loss: 0.0013832462
train_loss: 0.0010370124
test_loss: 0.0013680126
train_loss: 0.001043355
test_loss: 0.0014905696
train_loss: 0.0011164473
test_loss: 0.0014860036
train_loss: 0.0009050821
test_loss: 0.0015032396
train_loss: 0.00081305945
test_loss: 0.0016006406
train_loss: 0.0012954769
test_loss: 0.0013716829
train_loss: 0.0009947375
test_loss: 0.0014704698
train_loss: 0.0011118208
test_loss: 0.0013900297
train_loss: 0.0010173834
test_loss: 0.0014002301
train_loss: 0.0010501343
test_loss: 0.0014353545
train_loss: 0.0011416713
test_loss: 0.0016078965
train_loss: 0.0011155237
test_loss: 0.0013630468
train_loss: 0.0011286173
test_loss: 0.0013182655
train_loss: 0.0010302053
test_loss: 0.0013426966
train_loss: 0.00097040867
test_loss: 0.0014003451
train_loss: 0.0012586077
test_loss: 0.0013543881
train_loss: 0.0010496478
test_loss: 0.001368667
train_loss: 0.0008863853
test_loss: 0.0013546859
train_loss: 0.0011377382
test_loss: 0.001386665
train_loss: 0.0014581636
test_loss: 0.0013459129
train_loss: 0.0010930407
test_loss: 0.0016842998
train_loss: 0.0010554653
test_loss: 0.0014944214
train_loss: 0.0009653383
test_loss: 0.001413243
train_loss: 0.0008806918
test_loss: 0.0014270107
train_loss: 0.00081859546
test_loss: 0.0014045037
train_loss: 0.0010323256
test_loss: 0.0014464029
train_loss: 0.0011267681
test_loss: 0.0014203871
train_loss: 0.0010986569
test_loss: 0.0013152801
train_loss: 0.00081852905
test_loss: 0.0015072295
train_loss: 0.0009866663
test_loss: 0.0015479487
train_loss: 0.0012339728
test_loss: 0.0013775037
train_loss: 0.001067372
test_loss: 0.0013711608
train_loss: 0.0009932689
test_loss: 0.0014614383
train_loss: 0.0012343959
test_loss: 0.001353186
train_loss: 0.0010257231
test_loss: 0.0012823862
train_loss: 0.0010048843
test_loss: 0.0013218445
train_loss: 0.0011873594
test_loss: 0.0014899146
train_loss: 0.0009761811
test_loss: 0.001361348
train_loss: 0.0009736605
test_loss: 0.0014900073
train_loss: 0.0011614009
test_loss: 0.001419178
train_loss: 0.0010323274
test_loss: 0.001432203
train_loss: 0.0010686633
test_loss: 0.0013603702
train_loss: 0.0009830107
test_loss: 0.0013706117
train_loss: 0.0009866134
test_loss: 0.0016695098
train_loss: 0.0012360186
test_loss: 0.0012859913
train_loss: 0.0010131877
test_loss: 0.0014670648
train_loss: 0.0013387055
test_loss: 0.0013639965
train_loss: 0.0008387553
test_loss: 0.001595786
train_loss: 0.0014023416
test_loss: 0.0013991281
train_loss: 0.0012379963
test_loss: 0.0015384705
train_loss: 0.0011060685
test_loss: 0.0016451688
train_loss: 0.0012770218
test_loss: 0.0013764275
train_loss: 0.0013261369
test_loss: 0.0015246336
train_loss: 0.0010194604
test_loss: 0.0019336506
train_loss: 0.0014366261
test_loss: 0.0017569136
train_loss: 0.0010573172
test_loss: 0.002219899
train_loss: 0.0016436273
test_loss: 0.0019161989
train_loss: 0.0012732551
test_loss: 0.0017639416
train_loss: 0.0014458429
test_loss: 0.0018529913
train_loss: 0.0012822424
test_loss: 0.0019084465
train_loss: 0.0015494244
test_loss: 0.0017564584
train_loss: 0.001218814
test_loss: 0.0018346171
train_loss: 0.0016747478
test_loss: 0.0017159138
train_loss: 0.0011701854
test_loss: 0.0017558571
train_loss: 0.0014099762
test_loss: 0.0018432505
train_loss: 0.0012250589
test_loss: 0.001913447
train_loss: 0.0016410494
test_loss: 0.0015818512
train_loss: 0.0013733701
test_loss: 0.0016411069
train_loss: 0.0010825791
test_loss: 0.0019657349
train_loss: 0.0015389815
test_loss: 0.0017568673
train_loss: 0.0013991194
test_loss: 0.0018113737
train_loss: 0.0015713265
test_loss: 0.0014690171
train_loss: 0.0016786838
test_loss: 0.0016383408
train_loss: 0.0015917371
test_loss: 0.0015233601
train_loss: 0.0019819024
test_loss: 0.0019301906
train_loss: 0.0016996053
test_loss: 0.0017419548
train_loss: 0.0016316178
test_loss: 0.0017813168
train_loss: 0.001662782
test_loss: 0.001693753
train_loss: 0.0015794954
test_loss: 0.0016879993
train_loss: 0.0016381579
test_loss: 0.0017560072
train_loss: 0.001527087
test_loss: 0.0017516436
train_loss: 0.0013687365
test_loss: 0.0016067631
train_loss: 0.0015547047
test_loss: 0.0015932898
train_loss: 0.0014683069
test_loss: 0.0016532748
train_loss: 0.0015929695
test_loss: 0.0016846991
train_loss: 0.0014804981
test_loss: 0.0016676463
train_loss: 0.0014860211
test_loss: 0.0016025781
train_loss: 0.0014804071
test_loss: 0.0015989592
train_loss: 0.0015362087
test_loss: 0.0016830163
train_loss: 0.0014691928
test_loss: 0.0016061101
train_loss: 0.0015239711
test_loss: 0.0016259474
train_loss: 0.0014853273
test_loss: 0.0016392479
train_loss: 0.0014153797
test_loss: 0.0015572744
train_loss: 0.0014128796
test_loss: 0.0016367603
train_loss: 0.0012889594
test_loss: 0.0015883901
train_loss: 0.00129058
test_loss: 0.001501954
train_loss: 0.001283067
test_loss: 0.0014344708
train_loss: 0.0013327885
test_loss: 0.001520634
train_loss: 0.0012426109
test_loss: 0.001461612
train_loss: 0.0014059574
test_loss: 0.0014540866
train_loss: 0.0012790505
test_loss: 0.0014975806
train_loss: 0.0013376318
test_loss: 0.0014540215
train_loss: 0.001364419
test_loss: 0.0014569184
train_loss: 0.0012250678
test_loss: 0.001473449
train_loss: 0.0013446865
test_loss: 0.0015067215
train_loss: 0.001443843
test_loss: 0.0016778143
train_loss: 0.0011729539
test_loss: 0.0015901194
train_loss: 0.0016431399
test_loss: 0.0016027597
train_loss: 0.0013761433
test_loss: 0.0017420517
train_loss: 0.0013185866
test_loss: 0.001767627
train_loss: 0.0014301937
test_loss: 0.0015210828
train_loss: 0.0014401738
test_loss: 0.0017160489
train_loss: 0.0012089192
test_loss: 0.00177823
train_loss: 0.0010621563
test_loss: 0.0017440693
train_loss: 0.0015183032
test_loss: 0.0013982166
train_loss: 0.0017469323
test_loss: 0.0018695049
train_loss: 0.0016366119
test_loss: 0.0014049071
train_loss: 0.0012020731
test_loss: 0.0015841243
train_loss: 0.00096730364
test_loss: 0.0014998808
train_loss: 0.000983313
test_loss: 0.001494066
train_loss: 0.0012516303
test_loss: 0.0012770037
train_loss: 0.0010605791
test_loss: 0.0013484288
train_loss: 0.0011007796
test_loss: 0.0014855332
train_loss: 0.0009994641
test_loss: 0.0014797675
train_loss: 0.0010647803
test_loss: 0.0013963701
train_loss: 0.0010156163
test_loss: 0.00157884
train_loss: 0.0010537405
test_loss: 0.0014357985
train_loss: 0.0010198195
test_loss: 0.00137186
train_loss: 0.0009546556
test_loss: 0.0013517741
train_loss: 0.000844147
test_loss: 0.0013468183
train_loss: 0.0007766844
test_loss: 0.0014403136
train_loss: 0.00090854056
test_loss: 0.0014812534
train_loss: 0.0010068612
test_loss: 0.0013867768
train_loss: 0.0010020499
test_loss: 0.0014113183
train_loss: 0.0010377361
test_loss: 0.0014082959
train_loss: 0.0010292082
test_loss: 0.0014329178
train_loss: 0.0009297001
test_loss: 0.0014678813
train_loss: 0.00086787436
test_loss: 0.0015536757
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0/300_300_300_1 --function f1 --psi 0 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 4000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd194558d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd194588730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1945c28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd19464d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1944fbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1944fb620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1944a7158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1944728c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1944722f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd19443bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd19443b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1943fed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd19443b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd19444d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1943b5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd19436d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1943926a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd194360d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd19444da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1942a3d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1942c7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd194392c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1942c7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd194299840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd19425c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1941e7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1941e7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd194211268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1942127b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd194167488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd194194158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd19413af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1940f2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1941417b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd1940a1ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd19406dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.10361846
test_loss: 0.098968945
train_loss: 0.086700685
test_loss: 0.08453299
train_loss: 0.072773665
test_loss: 0.07124123
train_loss: 0.060775243
test_loss: 0.0604051
train_loss: 0.05377131
test_loss: 0.053109754
train_loss: 0.048930347
test_loss: 0.04884309
train_loss: 0.043722805
test_loss: 0.04241313
train_loss: 0.03553763
test_loss: 0.033820827
train_loss: 0.027007665
test_loss: 0.026830312
train_loss: 0.021986745
test_loss: 0.021551013
train_loss: 0.016870763
test_loss: 0.017483516
train_loss: 0.015106806
test_loss: 0.015290813
train_loss: 0.012911462
test_loss: 0.013358361
train_loss: 0.011373342
test_loss: 0.011880302
train_loss: 0.00985606
test_loss: 0.010652491
train_loss: 0.009054826
test_loss: 0.009357964
train_loss: 0.007914844
test_loss: 0.008511952
train_loss: 0.0070281466
test_loss: 0.007724798
train_loss: 0.006451457
test_loss: 0.0070757605
train_loss: 0.0061527113
test_loss: 0.0067583597
train_loss: 0.005530745
test_loss: 0.006206119
train_loss: 0.0052098325
test_loss: 0.005781212
train_loss: 0.004654571
test_loss: 0.005492038
train_loss: 0.0046407026
test_loss: 0.0053731957
train_loss: 0.004337728
test_loss: 0.0051075593
train_loss: 0.0041471366
test_loss: 0.0048555788
train_loss: 0.0039211744
test_loss: 0.0045181923
train_loss: 0.0036371774
test_loss: 0.0045928564
train_loss: 0.0036124066
test_loss: 0.0041342895
train_loss: 0.0036008114
test_loss: 0.0042255227
train_loss: 0.0036103213
test_loss: 0.0039782226
train_loss: 0.003679433
test_loss: 0.0043473137
train_loss: 0.0036999544
test_loss: 0.0041198926
train_loss: 0.0032810525
test_loss: 0.003972294
train_loss: 0.0031783641
test_loss: 0.0038273134
train_loss: 0.003170412
test_loss: 0.003834678
train_loss: 0.0031674209
test_loss: 0.003512451
train_loss: 0.0029775272
test_loss: 0.003414227
train_loss: 0.0027169632
test_loss: 0.003379109
train_loss: 0.0027466589
test_loss: 0.0030943828
train_loss: 0.0025547778
test_loss: 0.003128109
train_loss: 0.0028763795
test_loss: 0.0032226408
train_loss: 0.0025094347
test_loss: 0.0032024267
train_loss: 0.0024199109
test_loss: 0.0029609834
train_loss: 0.002503906
test_loss: 0.0028655012
train_loss: 0.002527038
test_loss: 0.0029594232
train_loss: 0.002493811
test_loss: 0.003019284
train_loss: 0.0024373718
test_loss: 0.002860519
train_loss: 0.0024632185
test_loss: 0.0032460403
train_loss: 0.002761157
test_loss: 0.002775991
train_loss: 0.0025252234
test_loss: 0.0030096006
train_loss: 0.0022443037
test_loss: 0.0029246884
train_loss: 0.0025415553
test_loss: 0.0029665837
train_loss: 0.0024001133
test_loss: 0.0029117512
train_loss: 0.0024922756
test_loss: 0.002701541
train_loss: 0.0024063506
test_loss: 0.0026623595
train_loss: 0.0022641078
test_loss: 0.0025826711
train_loss: 0.002071057
test_loss: 0.0025541363
train_loss: 0.002331633
test_loss: 0.0025042633
train_loss: 0.0020660427
test_loss: 0.0025214392
train_loss: 0.002282111
test_loss: 0.0025018097
train_loss: 0.002282321
test_loss: 0.0025189251
train_loss: 0.0019507651
test_loss: 0.002426562
train_loss: 0.002177566
test_loss: 0.002408537
train_loss: 0.0019179035
test_loss: 0.0026860177
train_loss: 0.0020645126
test_loss: 0.0024608658
train_loss: 0.0021848108
test_loss: 0.0024454647
train_loss: 0.0021796324
test_loss: 0.0024216198
train_loss: 0.0020606131
test_loss: 0.0023893805
train_loss: 0.0020629924
test_loss: 0.0024488599
train_loss: 0.0025041683
test_loss: 0.0027503802
train_loss: 0.002108173
test_loss: 0.002617929
train_loss: 0.0019886089
test_loss: 0.0024088146
train_loss: 0.0020391764
test_loss: 0.0023705098
train_loss: 0.0019487897
test_loss: 0.0023568694
train_loss: 0.0019683142
test_loss: 0.002333051
train_loss: 0.0019595572
test_loss: 0.0021954915
train_loss: 0.002112848
test_loss: 0.0023210973
train_loss: 0.0019501304
test_loss: 0.002376995
train_loss: 0.0017520301
test_loss: 0.0022093575
train_loss: 0.00190834
test_loss: 0.0025994056
train_loss: 0.0024128957
test_loss: 0.0021933955
train_loss: 0.0017485747
test_loss: 0.0023433666
train_loss: 0.0019458616
test_loss: 0.0023572638
train_loss: 0.002042436
test_loss: 0.0023008734
train_loss: 0.0019092656
test_loss: 0.0024212485
train_loss: 0.0018793503
test_loss: 0.0023727906
train_loss: 0.0019394616
test_loss: 0.002184064
train_loss: 0.0018040047
test_loss: 0.0021224853
train_loss: 0.0017428
test_loss: 0.0021063178
train_loss: 0.0017035562
test_loss: 0.0023175585
train_loss: 0.0019163148
test_loss: 0.0022643446
train_loss: 0.0016905491
test_loss: 0.0021888907
train_loss: 0.0016932143
test_loss: 0.0021678025
train_loss: 0.001802702
test_loss: 0.0021095409
train_loss: 0.0019171014
test_loss: 0.002114718
train_loss: 0.0018588762
test_loss: 0.0022762907
train_loss: 0.0019191819
test_loss: 0.002367046
train_loss: 0.0018034194
test_loss: 0.0021000083
train_loss: 0.0020763846
test_loss: 0.0023105533
train_loss: 0.0017314868
test_loss: 0.0022404967
train_loss: 0.0018408141
test_loss: 0.0021038866
train_loss: 0.0016757827
test_loss: 0.0021784077
train_loss: 0.0016326238
test_loss: 0.0021766566
train_loss: 0.0017352712
test_loss: 0.002153164
train_loss: 0.0017292013
test_loss: 0.002142542
train_loss: 0.0018581456
test_loss: 0.002089355
train_loss: 0.0018904265
test_loss: 0.0020799907
train_loss: 0.002070141
test_loss: 0.0021142112
train_loss: 0.0017975213
test_loss: 0.002209856
train_loss: 0.001988661
test_loss: 0.0024939303
train_loss: 0.0017192082
test_loss: 0.002392495
train_loss: 0.0020945775
test_loss: 0.002230523
train_loss: 0.0020188992
test_loss: 0.0021858648
train_loss: 0.0019205005
test_loss: 0.002046563
train_loss: 0.0020558597
test_loss: 0.0022135195
train_loss: 0.0015585423
test_loss: 0.0022959018
train_loss: 0.0019682576
test_loss: 0.002023117
train_loss: 0.0017518358
test_loss: 0.00217446
train_loss: 0.0020589202
test_loss: 0.0019845585
train_loss: 0.0016905557
test_loss: 0.0022373798
train_loss: 0.0019784584
test_loss: 0.0019716297
train_loss: 0.002048602
test_loss: 0.0019180967
train_loss: 0.0019641288
test_loss: 0.0019076867
train_loss: 0.0016998895
test_loss: 0.001981319
train_loss: 0.0015863514
test_loss: 0.0020118828
train_loss: 0.0015596645
test_loss: 0.0020060102
train_loss: 0.0016357494
test_loss: 0.0022620982
train_loss: 0.0016350177
test_loss: 0.001983911
train_loss: 0.0017666842
test_loss: 0.0018730318
train_loss: 0.0015119859
test_loss: 0.0021831351
train_loss: 0.001685019
test_loss: 0.0022813806
train_loss: 0.0018003883
test_loss: 0.0020321237
train_loss: 0.0017384819
test_loss: 0.0021432382
train_loss: 0.0018036487
test_loss: 0.0020424188
train_loss: 0.0019883073
test_loss: 0.0021634253
train_loss: 0.0018185534
test_loss: 0.0019539068
train_loss: 0.0017976728
test_loss: 0.0019504413
train_loss: 0.0015678778
test_loss: 0.0021180965
train_loss: 0.0018760182
test_loss: 0.0021782084
train_loss: 0.0019126554
test_loss: 0.0018055876
train_loss: 0.0017524235
test_loss: 0.001946923
train_loss: 0.0014744685
test_loss: 0.0019094492
train_loss: 0.001575932
test_loss: 0.0018828883
train_loss: 0.0016680213
test_loss: 0.0023456605
train_loss: 0.0017514566
test_loss: 0.0021106368
train_loss: 0.0017306143
test_loss: 0.002042894
train_loss: 0.0016202629
test_loss: 0.0020633054
train_loss: 0.0017619052
test_loss: 0.0018417307
train_loss: 0.0013903211
test_loss: 0.0018643383
train_loss: 0.0014668272
test_loss: 0.0020513134
train_loss: 0.0016883209
test_loss: 0.0019014254
train_loss: 0.0017355395
test_loss: 0.0018457356
train_loss: 0.0015006142
test_loss: 0.001825255
train_loss: 0.0015819344
test_loss: 0.0019491912
train_loss: 0.0014063332
test_loss: 0.0019914566
train_loss: 0.0014699078
test_loss: 0.0018919455
train_loss: 0.0015527557
test_loss: 0.0018221638
train_loss: 0.0012803182
test_loss: 0.0017678239
train_loss: 0.0016344896
test_loss: 0.0018894396
train_loss: 0.00157357
test_loss: 0.0019161344
train_loss: 0.0015896563
test_loss: 0.0019283489
train_loss: 0.0016404093
test_loss: 0.0018216196
train_loss: 0.0017695702
test_loss: 0.001808674
train_loss: 0.0016108388
test_loss: 0.0018332357
train_loss: 0.0016656007
test_loss: 0.001952725
train_loss: 0.0014877652
test_loss: 0.0019526758
train_loss: 0.0017030254
test_loss: 0.0018776928
train_loss: 0.0014908066
test_loss: 0.0019195153
train_loss: 0.0015950084
test_loss: 0.0017867179
train_loss: 0.0016273633
test_loss: 0.0018603607
train_loss: 0.0016625382
test_loss: 0.001854256
train_loss: 0.001564362
test_loss: 0.0018827682
train_loss: 0.0016403955
test_loss: 0.0017637819
train_loss: 0.0019173253
test_loss: 0.001750383
train_loss: 0.0016734668
test_loss: 0.0019550705
train_loss: 0.0016310683
test_loss: 0.0019401826
train_loss: 0.0015440175
test_loss: 0.0019947086
train_loss: 0.001528197
test_loss: 0.0018769491
train_loss: 0.0014564955
test_loss: 0.0021185488
train_loss: 0.0015308333
test_loss: 0.0018417195
train_loss: 0.0017825338
test_loss: 0.0017253769
train_loss: 0.00153784
test_loss: 0.0020037778
train_loss: 0.001593172
test_loss: 0.0020086016
train_loss: 0.0017006536
test_loss: 0.0019207239
train_loss: 0.0014743438
test_loss: 0.001949833
train_loss: 0.0014419182
test_loss: 0.002038023
train_loss: 0.0018118727
test_loss: 0.0018241657
train_loss: 0.0017746465
test_loss: 0.0019341614
train_loss: 0.001699283
test_loss: 0.0020661065
train_loss: 0.0016619891
test_loss: 0.0021578157
train_loss: 0.001730266
test_loss: 0.0022248302
train_loss: 0.0016977275
test_loss: 0.0020462507
train_loss: 0.0016264683
test_loss: 0.0019225514
train_loss: 0.0017865924
test_loss: 0.0019020281
train_loss: 0.0016931661
test_loss: 0.0019461401
train_loss: 0.0013945043
test_loss: 0.0019065983
train_loss: 0.0015090093
test_loss: 0.0017028189
train_loss: 0.0014229541
test_loss: 0.0017487059
train_loss: 0.0013017806
test_loss: 0.0017264357
train_loss: 0.0012484539
test_loss: 0.0016920628
train_loss: 0.0013056247
test_loss: 0.0017764654
train_loss: 0.0012826512
test_loss: 0.0018107964
train_loss: 0.0013046121
test_loss: 0.0018122011
train_loss: 0.0013930408
test_loss: 0.0021001268
train_loss: 0.0014700299
test_loss: 0.002047598
train_loss: 0.0015559834
test_loss: 0.0021847347
train_loss: 0.001911373
test_loss: 0.0017505264
train_loss: 0.0013907595
test_loss: 0.002287451
train_loss: 0.0018030491
test_loss: 0.002129495
train_loss: 0.0015302859
test_loss: 0.0022106762
train_loss: 0.0019699517
test_loss: 0.001932778
train_loss: 0.0019003081
test_loss: 0.0018327176
train_loss: 0.0016549214
test_loss: 0.0021196408
train_loss: 0.0017051871
test_loss: 0.0022935069
train_loss: 0.0017029103
test_loss: 0.0023400441
train_loss: 0.0019778349
test_loss: 0.0022007022
train_loss: 0.0016316881
test_loss: 0.0024182184
train_loss: 0.0019332711
test_loss: 0.002006834
train_loss: 0.002464205
test_loss: 0.002120031
train_loss: 0.0023236722
test_loss: 0.002091616
train_loss: 0.0019306301
test_loss: 0.002165123
train_loss: 0.0019324252
test_loss: 0.0017865474
train_loss: 0.0019933654
test_loss: 0.0019723608
train_loss: 0.0016426069
test_loss: 0.001975483
train_loss: 0.0016135614
test_loss: 0.0019114694
train_loss: 0.0015742263
test_loss: 0.0018223274
train_loss: 0.0015955173
test_loss: 0.0018849842
train_loss: 0.0014408308
test_loss: 0.001959873
train_loss: 0.0015190933
test_loss: 0.0016958743
train_loss: 0.0014925663
test_loss: 0.0018649315
train_loss: 0.0015612133
test_loss: 0.0017913983
train_loss: 0.0015671226
test_loss: 0.0017011676
train_loss: 0.0013543664
test_loss: 0.0018154816
train_loss: 0.0013636086
test_loss: 0.0018577796
train_loss: 0.0014979908
test_loss: 0.00164186
train_loss: 0.0014083246
test_loss: 0.0018691838
train_loss: 0.0015520166
test_loss: 0.0016776093
train_loss: 0.0016246041
test_loss: 0.0017060232
train_loss: 0.0014675079
test_loss: 0.001817616
train_loss: 0.0013693739
test_loss: 0.0017432607
train_loss: 0.0013817865
test_loss: 0.0018469444
train_loss: 0.0013654674
test_loss: 0.0017706264
train_loss: 0.0016898082
test_loss: 0.0017221357
train_loss: 0.0013801988
test_loss: 0.0017301531
train_loss: 0.0014969625
test_loss: 0.001730066
train_loss: 0.0015356283
test_loss: 0.001650921
train_loss: 0.0013361077
test_loss: 0.0016616088
train_loss: 0.0014410522
test_loss: 0.0017693018
train_loss: 0.0011893505
test_loss: 0.0016395003
train_loss: 0.001337423
test_loss: 0.0016861182
train_loss: 0.0012270538
test_loss: 0.0015995676
train_loss: 0.0013630071
test_loss: 0.0017161855
train_loss: 0.0013690451
test_loss: 0.0018722677
train_loss: 0.0015265956
test_loss: 0.0018843429
train_loss: 0.0016126132
test_loss: 0.0016716019
train_loss: 0.0014951143
test_loss: 0.0020580846
train_loss: 0.0014456367
test_loss: 0.0020640017
train_loss: 0.001609964
test_loss: 0.00184302
train_loss: 0.001608938
test_loss: 0.0018210985
train_loss: 0.0015773792
test_loss: 0.00164828
train_loss: 0.0015028028
test_loss: 0.0018406793
train_loss: 0.0014905034
test_loss: 0.0017801698
train_loss: 0.0014427914
test_loss: 0.0016849592
train_loss: 0.0014207681
test_loss: 0.0017472354
train_loss: 0.0014357265
test_loss: 0.0017270739
train_loss: 0.0014724181
test_loss: 0.0019592028
train_loss: 0.0015374467
test_loss: 0.0017221135
train_loss: 0.0014400147
test_loss: 0.0017192721
train_loss: 0.0016118777
test_loss: 0.0018266817
train_loss: 0.0015588981
test_loss: 0.0017855837
train_loss: 0.0015199665
test_loss: 0.0018912717
train_loss: 0.0015020071
test_loss: 0.0020459762
train_loss: 0.0016223958
test_loss: 0.0016760512
train_loss: 0.0014632269
test_loss: 0.0016680629
train_loss: 0.0013943495
test_loss: 0.001712545
train_loss: 0.0013268677
test_loss: 0.0016197711
train_loss: 0.0014463988
test_loss: 0.0017276262
train_loss: 0.00123175
test_loss: 0.0017192566
train_loss: 0.0014440888
test_loss: 0.0016315407
train_loss: 0.0017416854
test_loss: 0.0016988703
train_loss: 0.0013090697
test_loss: 0.0017351343
train_loss: 0.0013685146
test_loss: 0.0020696777
train_loss: 0.0015594454
test_loss: 0.0019095347
train_loss: 0.0016116811
test_loss: 0.0020163285
train_loss: 0.001829312
test_loss: 0.001572724
train_loss: 0.0013007809
test_loss: 0.0021178424
train_loss: 0.0015319819
test_loss: 0.0019324928
train_loss: 0.001952974
test_loss: 0.0017673438
train_loss: 0.0012689405
test_loss: 0.0021034703
train_loss: 0.001661658
test_loss: 0.0018733892
train_loss: 0.0016640924
test_loss: 0.0016427205
train_loss: 0.0013757749
test_loss: 0.0015804552
train_loss: 0.0013193068
test_loss: 0.0016815449
train_loss: 0.0013841889
test_loss: 0.0018298314
train_loss: 0.0014175619
test_loss: 0.0018132878
train_loss: 0.0012225875
test_loss: 0.0018160037
train_loss: 0.0016648583
test_loss: 0.0017764831
train_loss: 0.0014830327
test_loss: 0.001893286
train_loss: 0.0013940978
test_loss: 0.0019795527
train_loss: 0.0015488376
test_loss: 0.0020129164
train_loss: 0.0016777407
test_loss: 0.0017811133
train_loss: 0.0015683724
test_loss: 0.001753714
train_loss: 0.0017907663
test_loss: 0.001832952
train_loss: 0.0015399972
test_loss: 0.0019139935
train_loss: 0.0014360817
test_loss: 0.002111888
train_loss: 0.0014751233
test_loss: 0.0016371396
train_loss: 0.001651603
test_loss: 0.002004002
train_loss: 0.0016342967
test_loss: 0.0019263419
train_loss: 0.0013957229
test_loss: 0.0019079323
train_loss: 0.001465279
test_loss: 0.0018823468
train_loss: 0.0012699967
test_loss: 0.0016096716
train_loss: 0.0012637015
test_loss: 0.0016929958
train_loss: 0.0013743528
test_loss: 0.0016223697
train_loss: 0.0014718936
test_loss: 0.001625471
train_loss: 0.0012808223
test_loss: 0.0016457279
train_loss: 0.0012985351
test_loss: 0.0016201234
train_loss: 0.0012909254
test_loss: 0.0016327114
train_loss: 0.0012862514
test_loss: 0.0018477265
train_loss: 0.0015291818
test_loss: 0.001644648
train_loss: 0.0013495814
test_loss: 0.0016084682
train_loss: 0.0013388726
test_loss: 0.0016728222
train_loss: 0.0012745946
test_loss: 0.0016094189
train_loss: 0.0013786972
test_loss: 0.0016380601
train_loss: 0.0012407625
test_loss: 0.0017463085
train_loss: 0.0011946324
test_loss: 0.0017928928
train_loss: 0.0014175657
test_loss: 0.0017967778
train_loss: 0.0014649527
test_loss: 0.0016545202
train_loss: 0.0015540852
test_loss: 0.0018525099
train_loss: 0.0013659516
test_loss: 0.0017160665
train_loss: 0.0014528463
test_loss: 0.0017819263
train_loss: 0.0015926074
test_loss: 0.0017699891
train_loss: 0.0013605215
test_loss: 0.0018108547
train_loss: 0.001469794
test_loss: 0.0018425514
train_loss: 0.0016052225
test_loss: 0.0016547578
train_loss: 0.0015745875
test_loss: 0.0017806995
train_loss: 0.0016429513
test_loss: 0.001721566
train_loss: 0.0014345634
test_loss: 0.0017211422
train_loss: 0.0014804584
test_loss: 0.0017190066
train_loss: 0.0015504112
test_loss: 0.0019183292
train_loss: 0.0015807649
test_loss: 0.0017052203
train_loss: 0.0015104964
test_loss: 0.0016498399
train_loss: 0.0014292412
test_loss: 0.0017611048
train_loss: 0.0015176174
test_loss: 0.001769566
train_loss: 0.001493492
test_loss: 0.0018094832
train_loss: 0.0013264777
test_loss: 0.0016652994
train_loss: 0.0012248278
test_loss: 0.001662644
train_loss: 0.0011992744
test_loss: 0.0017952182
train_loss: 0.0014318295
test_loss: 0.0016155905
train_loss: 0.001292947
test_loss: 0.0017124161
train_loss: 0.0013860884
test_loss: 0.001771169
train_loss: 0.0015468611
test_loss: 0.001564058
train_loss: 0.0012833737
test_loss: 0.0016735197
train_loss: 0.001249346
test_loss: 0.0016417807
train_loss: 0.0012719017
test_loss: 0.0017653907
train_loss: 0.0013450212
test_loss: 0.0017730558
train_loss: 0.0013036391
test_loss: 0.0016400628
train_loss: 0.0013608172
test_loss: 0.0016099336
train_loss: 0.001338256
test_loss: 0.0015818316
train_loss: 0.0012608434
test_loss: 0.0015187671
train_loss: 0.0013538281
test_loss: 0.0017698905
train_loss: 0.0012027873
test_loss: 0.0016569482
train_loss: 0.0011701384
test_loss: 0.0016144258
train_loss: 0.0011266948
test_loss: 0.0015069649
train_loss: 0.0013613807
test_loss: 0.0015705924
train_loss: 0.0012962022
test_loss: 0.0016157164
train_loss: 0.0014224915
test_loss: 0.0017322191
train_loss: 0.001334884
test_loss: 0.0018085117
train_loss: 0.0014809313
test_loss: 0.0016716354
train_loss: 0.0012612792
test_loss: 0.0015894404
train_loss: 0.0011994526
test_loss: 0.0015717353
train_loss: 0.0011448169
test_loss: 0.0018176833
train_loss: 0.0013240587
test_loss: 0.0017745349
train_loss: 0.0015423767
test_loss: 0.0015905283
train_loss: 0.0012858671
test_loss: 0.0017793671
train_loss: 0.0014609267
test_loss: 0.0018356573
train_loss: 0.0014034718
test_loss: 0.0019328315
train_loss: 0.0014884432
test_loss: 0.0018886118
train_loss: 0.001845521
test_loss: 0.001757616
train_loss: 0.0015124533
test_loss: 0.0020598664
train_loss: 0.0015339112
test_loss: 0.002158494
train_loss: 0.0016814251
test_loss: 0.0019002415
train_loss: 0.0018607606
test_loss: 0.0018149684
train_loss: 0.0017788596
test_loss: 0.0020487201
train_loss: 0.0016037158
test_loss: 0.0019790332
train_loss: 0.0015115798
test_loss: 0.001979761
train_loss: 0.0014981818
test_loss: 0.0020029177
train_loss: 0.0016629479
test_loss: 0.0019033003
train_loss: 0.0016699851
test_loss: 0.0016915414
train_loss: 0.0014799874
test_loss: 0.0017118158
train_loss: 0.0014230887
test_loss: 0.0019232642
train_loss: 0.0015576928
test_loss: 0.0016570084
train_loss: 0.0013341117
test_loss: 0.0017811862
train_loss: 0.0013750901
test_loss: 0.0016938134
train_loss: 0.0013704078
test_loss: 0.0017903418
train_loss: 0.0011618936
test_loss: 0.0017999202
train_loss: 0.0013015604
test_loss: 0.0016110444
train_loss: 0.0012300699
test_loss: 0.0015686217
train_loss: 0.0011587563
test_loss: 0.0016223869
train_loss: 0.0013155611
test_loss: 0.0016009015
train_loss: 0.0013555437
test_loss: 0.0016439718
train_loss: 0.0012292879
test_loss: 0.0016348752
train_loss: 0.0013266582
test_loss: 0.0017535298
train_loss: 0.0012387205
test_loss: 0.0016878474
train_loss: 0.0012704177
test_loss: 0.0015831093
train_loss: 0.0012382115
test_loss: 0.0016770236
train_loss: 0.0013680056
test_loss: 0.0016617296
train_loss: 0.0011825362
test_loss: 0.0015212456
train_loss: 0.0011815464
test_loss: 0.0016638149
train_loss: 0.0011877264
test_loss: 0.0018551588
train_loss: 0.0013091974
test_loss: 0.0018996904
train_loss: 0.0016118111
test_loss: 0.0016374666
train_loss: 0.0014063613
test_loss: 0.0016527413
train_loss: 0.0013059416
test_loss: 0.0015286565
train_loss: 0.001239936
test_loss: 0.001581996
train_loss: 0.0012505143
test_loss: 0.001659547
train_loss: 0.0012780419
test_loss: 0.0016458777
train_loss: 0.0011479398
test_loss: 0.0017755385
train_loss: 0.0015513835
test_loss: 0.0015743399
train_loss: 0.0018455914
test_loss: 0.0019536521
train_loss: 0.0018037888
test_loss: 0.0016191178
train_loss: 0.0012992166
test_loss: 0.0020530906
train_loss: 0.001421238
test_loss: 0.0019114249
train_loss: 0.0014522597
test_loss: 0.0019109044
train_loss: 0.0014404991
test_loss: 0.0016719128
train_loss: 0.0016989016
test_loss: 0.0016467582
train_loss: 0.0016169631
test_loss: 0.0018040991
train_loss: 0.0014933886
test_loss: 0.0018514141
train_loss: 0.0014611245
test_loss: 0.0017336608
train_loss: 0.001276459
test_loss: 0.0017824546
train_loss: 0.0012060058
test_loss: 0.0017885527
train_loss: 0.0015258903
test_loss: 0.001793029
train_loss: 0.0017121192
test_loss: 0.0017227464
train_loss: 0.0014571964
test_loss: 0.0016185006
train_loss: 0.0012467966
test_loss: 0.0017613802
train_loss: 0.0013802401
test_loss: 0.0016883896
train_loss: 0.0014118125
test_loss: 0.0015684047
train_loss: 0.0013413709
test_loss: 0.0015994249
train_loss: 0.0013283289
test_loss: 0.0015373447
train_loss: 0.0013890541
test_loss: 0.0016525848
train_loss: 0.0013235665
test_loss: 0.0016996062
train_loss: 0.0012236754
test_loss: 0.0017798281
train_loss: 0.0012752851
test_loss: 0.0016429229
train_loss: 0.0013867794
test_loss: 0.0015809224
train_loss: 0.0013566456
test_loss: 0.0017309497
train_loss: 0.0013605861
test_loss: 0.0013933256
train_loss: 0.0013154893
test_loss: 0.0015354683
train_loss: 0.0013375019
test_loss: 0.0017521944
train_loss: 0.0015173606
test_loss: 0.0015925618
train_loss: 0.0013140489
test_loss: 0.0016162777
train_loss: 0.0012404184
test_loss: 0.001674895
train_loss: 0.0015338381
test_loss: 0.0016989724
train_loss: 0.0013311428
test_loss: 0.0016807279
train_loss: 0.0012834888
test_loss: 0.0016561705
train_loss: 0.0015367254
test_loss: 0.0014850689
train_loss: 0.0014506449
test_loss: 0.0015802446
train_loss: 0.0014308348
test_loss: 0.0016568983
train_loss: 0.0012843071
test_loss: 0.0017935191
train_loss: 0.0014788056
test_loss: 0.0017896515
train_loss: 0.0014368594
test_loss: 0.0018344382
train_loss: 0.0016756735
test_loss: 0.0015926693
train_loss: 0.001388411
test_loss: 0.0017191804
train_loss: 0.0014824223
test_loss: 0.0018310203
train_loss: 0.0014772539
test_loss: 0.0017686002
train_loss: 0.0014243897
test_loss: 0.0015927764
train_loss: 0.0014586021
test_loss: 0.001643383
train_loss: 0.0012205365
test_loss: 0.0018420431
train_loss: 0.0015134757
test_loss: 0.0015938755
train_loss: 0.0013613773
test_loss: 0.0017171126
train_loss: 0.0012085079
test_loss: 0.0017724882
train_loss: 0.0013500394
test_loss: 0.001649912
train_loss: 0.0013350889
test_loss: 0.0017301621
train_loss: 0.001271711
test_loss: 0.001625218
train_loss: 0.0013602781
test_loss: 0.0017176508
train_loss: 0.0011436392
test_loss: 0.0017206499
train_loss: 0.001289227
test_loss: 0.0017844072
train_loss: 0.0015127638
test_loss: 0.0017534975
train_loss: 0.0013948468
test_loss: 0.0015924022
train_loss: 0.0012217449
test_loss: 0.0015127667
train_loss: 0.0012008895
test_loss: 0.0014705777
train_loss: 0.0011970373
test_loss: 0.0017119693
train_loss: 0.0013377052
test_loss: 0.0015895312
train_loss: 0.0013225562
test_loss: 0.0015100946
train_loss: 0.0011824704
test_loss: 0.0015942635
train_loss: 0.0013278667
test_loss: 0.0015548462
train_loss: 0.001305699
test_loss: 0.0015214571
train_loss: 0.0011738125
test_loss: 0.0015326916
train_loss: 0.001495618
test_loss: 0.0015517755
train_loss: 0.0013713855
test_loss: 0.0016818936
train_loss: 0.0013608665
test_loss: 0.0017116675
train_loss: 0.001229517
test_loss: 0.0017021409
train_loss: 0.0010525404
test_loss: 0.0014957067
train_loss: 0.0012217303
test_loss: 0.0015710308
train_loss: 0.0011896762
test_loss: 0.0017045452
train_loss: 0.001141462
test_loss: 0.0015727036
train_loss: 0.001265721
test_loss: 0.001661959
train_loss: 0.0012488042
test_loss: 0.0017059592
train_loss: 0.0012733004
test_loss: 0.0016436732
train_loss: 0.0014097837
test_loss: 0.0016407875
train_loss: 0.0013135893
test_loss: 0.0016612887
train_loss: 0.0012853311
test_loss: 0.0015686563
train_loss: 0.0012975925
test_loss: 0.0015726237
train_loss: 0.0013677916
test_loss: 0.0016734722
train_loss: 0.0014858395
test_loss: 0.001506934
train_loss: 0.0012520422
test_loss: 0.0014881693
train_loss: 0.0011576983
test_loss: 0.0017619515
train_loss: 0.0014134008
test_loss: 0.0016627073
train_loss: 0.0013823054
test_loss: 0.001552731
train_loss: 0.0014922153
test_loss: 0.0016320529
train_loss: 0.0011983063
test_loss: 0.0015244619
train_loss: 0.0012646695
test_loss: 0.0016325522
train_loss: 0.0012563661
test_loss: 0.0017177745
train_loss: 0.0014696801
test_loss: 0.0018646703
train_loss: 0.0013217975
test_loss: 0.0019887686
train_loss: 0.0014822131
test_loss: 0.0016364313
train_loss: 0.0016114702
test_loss: 0.0017384049
train_loss: 0.0018910493
test_loss: 0.0018151911
train_loss: 0.0017504893
test_loss: 0.0019578177
train_loss: 0.0016653775
test_loss: 0.0016436063
train_loss: 0.0014861901
test_loss: 0.0020040488
train_loss: 0.0015789396
test_loss: 0.0018278697
train_loss: 0.0014231853
test_loss: 0.0016822339
train_loss: 0.0016221682
test_loss: 0.0018569898
train_loss: 0.0013331551
test_loss: 0.001904307
train_loss: 0.0015725147
test_loss: 0.0015789499
train_loss: 0.0014362468
test_loss: 0.0016097963
train_loss: 0.0012536609
test_loss: 0.0017633825
train_loss: 0.0011759944
test_loss: 0.0015811292
train_loss: 0.0011904981
test_loss: 0.0016107416
train_loss: 0.0012457502
test_loss: 0.0015086214
train_loss: 0.001258515
test_loss: 0.0015058193
train_loss: 0.0013510395
test_loss: 0.0015964822
train_loss: 0.0011857114
test_loss: 0.0015090677
train_loss: 0.0012756307
test_loss: 0.0014680522
train_loss: 0.0012313786
test_loss: 0.0015434842
train_loss: 0.0010957363
test_loss: 0.0016086578
train_loss: 0.0012017117
test_loss: 0.0015671742
train_loss: 0.001295855
test_loss: 0.0016976679
train_loss: 0.0012949079
test_loss: 0.0017233071
train_loss: 0.0013098515
test_loss: 0.0016436202
train_loss: 0.0011565861
test_loss: 0.0015819867
train_loss: 0.0013326264
test_loss: 0.0015571639
train_loss: 0.0013411304
test_loss: 0.0017244314
train_loss: 0.0016108962
test_loss: 0.0017265535
train_loss: 0.0013445935
test_loss: 0.0021629978
train_loss: 0.0015210549
test_loss: 0.001920091
train_loss: 0.0014359677
test_loss: 0.0020510133
train_loss: 0.0015292712
test_loss: 0.001988805
train_loss: 0.0015175545
test_loss: 0.002111409
train_loss: 0.001668671
test_loss: 0.0020535623
train_loss: 0.0016477776
test_loss: 0.0019844703
train_loss: 0.0017619391
test_loss: 0.0017706519
train_loss: 0.0013902894
test_loss: 0.0017752207
train_loss: 0.0012047158
test_loss: 0.0018883061
train_loss: 0.0014030484
test_loss: 0.0018975525
train_loss: 0.0014693735
test_loss: 0.0016304726
train_loss: 0.0013553682
test_loss: 0.0015681006
train_loss: 0.0013606623
test_loss: 0.0016984289
train_loss: 0.0012457798
test_loss: 0.0017428119
train_loss: 0.0010982229
test_loss: 0.0015933757
train_loss: 0.0010837977
test_loss: 0.0017694931
train_loss: 0.0012454783
test_loss: 0.001575482
train_loss: 0.0012779258
test_loss: 0.0016212388
train_loss: 0.001156735
test_loss: 0.0016109372
train_loss: 0.0012534128
test_loss: 0.0016765437
train_loss: 0.0014745932
test_loss: 0.0016064182
train_loss: 0.0011501216
test_loss: 0.0017532404
train_loss: 0.0011790495
test_loss: 0.0015808226
train_loss: 0.0013860618
test_loss: 0.0015831599
train_loss: 0.0015277577
test_loss: 0.0015447133
train_loss: 0.0012110682
test_loss: 0.0016745399
train_loss: 0.0014253627
test_loss: 0.0017204721
train_loss: 0.0013468487
test_loss: 0.0016979892
train_loss: 0.0012479423
test_loss: 0.0016549692
train_loss: 0.0012548924
test_loss: 0.0017136091
train_loss: 0.0010796878
test_loss: 0.0015892747
train_loss: 0.0012421607
test_loss: 0.0015814289
train_loss: 0.0012214239
test_loss: 0.0017350264
train_loss: 0.0012950109
test_loss: 0.0017536725
train_loss: 0.0012757969
test_loss: 0.0016765062
train_loss: 0.0013090397
test_loss: 0.0014879097
train_loss: 0.0013639161
test_loss: 0.001593009
train_loss: 0.0011841333
test_loss: 0.0017511294
train_loss: 0.0016294831
test_loss: 0.0017609048
train_loss: 0.0013291867
test_loss: 0.0017990173
train_loss: 0.0013195688
test_loss: 0.0017431612
train_loss: 0.0017922574
test_loss: 0.0017595552
train_loss: 0.0013468802
test_loss: 0.0018079266
train_loss: 0.0014612831
test_loss: 0.0016418725
train_loss: 0.001412143
test_loss: 0.001819659
train_loss: 0.0013371522
test_loss: 0.0019035311
train_loss: 0.0015420294
test_loss: 0.0017416088
train_loss: 0.0015345588
test_loss: 0.0018644739
train_loss: 0.001789006
test_loss: 0.0016493913
train_loss: 0.0013635003
test_loss: 0.0019764865
train_loss: 0.0015376045
test_loss: 0.0016824963
train_loss: 0.0017287659
test_loss: 0.0015909608
train_loss: 0.0013820609
test_loss: 0.0020042055
train_loss: 0.0015361044
test_loss: 0.0018998226
train_loss: 0.0014827361
test_loss: 0.0019611628
train_loss: 0.0017027596
test_loss: 0.0018361444
train_loss: 0.0015311099
test_loss: 0.0016699126
train_loss: 0.001515785
test_loss: 0.0017305673
train_loss: 0.0013753201
test_loss: 0.0018893682
train_loss: 0.0014388424
test_loss: 0.001620547
train_loss: 0.0014651555
test_loss: 0.0017014861
train_loss: 0.0012102362
test_loss: 0.0018050718
train_loss: 0.0013396288
test_loss: 0.0018890147
train_loss: 0.0013878649
test_loss: 0.0016693587
train_loss: 0.0012997359
test_loss: 0.0016294433
train_loss: 0.0013475758
test_loss: 0.0017077375
train_loss: 0.0014477149
test_loss: 0.0017581141
train_loss: 0.0011479612
test_loss: 0.0016632419
train_loss: 0.0013114569
test_loss: 0.0018704804
train_loss: 0.0013673246
test_loss: 0.0015827644
train_loss: 0.001220352
test_loss: 0.0016443689
train_loss: 0.0012759827
test_loss: 0.0018355027
train_loss: 0.0014287536
test_loss: 0.0016476461
train_loss: 0.0017333878
test_loss: 0.0016350353
train_loss: 0.0013268569
test_loss: 0.0018283348
train_loss: 0.0016049517
test_loss: 0.0015710449
train_loss: 0.001280713
test_loss: 0.0015886625
train_loss: 0.0012459287
test_loss: 0.0019828177
train_loss: 0.0015242598
test_loss: 0.0015598338
train_loss: 0.0014771046
test_loss: 0.001788603
train_loss: 0.0012997518
test_loss: 0.0016393705
train_loss: 0.0013140342
test_loss: 0.0014651435
train_loss: 0.0011426731
test_loss: 0.0015654868
train_loss: 0.0012100742
test_loss: 0.0015355942
train_loss: 0.0013984768
test_loss: 0.001440843
train_loss: 0.0010626889
test_loss: 0.0017200904
train_loss: 0.0011583014
test_loss: 0.0015554407
train_loss: 0.0013101171
test_loss: 0.0017300873
train_loss: 0.0011654352
test_loss: 0.0017175039
train_loss: 0.0013600356
test_loss: 0.0015267725
train_loss: 0.0012159834
test_loss: 0.0016653005
train_loss: 0.0012736219
test_loss: 0.0015354716
train_loss: 0.0013943232
test_loss: 0.0015488847
train_loss: 0.0012240057
test_loss: 0.0016452356
train_loss: 0.0012086538
test_loss: 0.0017414484
train_loss: 0.0014016905
test_loss: 0.0016827274
train_loss: 0.0014135755
test_loss: 0.0016623398
train_loss: 0.0013058473
test_loss: 0.0017451519
train_loss: 0.0013576398
test_loss: 0.0016779684
train_loss: 0.0014576705
test_loss: 0.0016759055
train_loss: 0.0013684573
test_loss: 0.0016833154
train_loss: 0.0012870051
test_loss: 0.0018248276
train_loss: 0.0013494139
test_loss: 0.0016318742
train_loss: 0.001345057
test_loss: 0.0016093616
train_loss: 0.001257407
test_loss: 0.0016722842
train_loss: 0.0013872737
test_loss: 0.0014666182
train_loss: 0.0012608514
test_loss: 0.001494371
train_loss: 0.0014465008
test_loss: 0.0014940456
train_loss: 0.0014426968
test_loss: 0.0016755689
train_loss: 0.0013687467
test_loss: 0.0015877608
train_loss: 0.0014326427
test_loss: 0.0016375333
train_loss: 0.0014090827
test_loss: 0.0016553833
train_loss: 0.001284522
test_loss: 0.001561475
train_loss: 0.0012122055
test_loss: 0.0015180896
train_loss: 0.0015392429
test_loss: 0.0015408124
train_loss: 0.0012179328
test_loss: 0.0017406419
train_loss: 0.0011495013
test_loss: 0.0017076986
train_loss: 0.0013539882
test_loss: 0.0017371925
train_loss: 0.0013513877
test_loss: 0.0015666627
train_loss: 0.0012626117
test_loss: 0.0015527759
train_loss: 0.0012761236
test_loss: 0.0015462926
train_loss: 0.0011484193
test_loss: 0.0015047857
train_loss: 0.0012123158
test_loss: 0.0015360201
train_loss: 0.0011602329
test_loss: 0.0017224695
train_loss: 0.0012364062
test_loss: 0.0018590555
train_loss: 0.0014824182
test_loss: 0.0015975991
train_loss: 0.0015058944
test_loss: 0.0014775613
train_loss: 0.0015014207
test_loss: 0.001514199
train_loss: 0.0012591275
test_loss: 0.0015793943
train_loss: 0.001270567
test_loss: 0.0015976216
train_loss: 0.0011826853
test_loss: 0.0015997834
train_loss: 0.0011475158
test_loss: 0.0016989007
train_loss: 0.0012792915
test_loss: 0.0017135815
train_loss: 0.0015615283
test_loss: 0.001725334
train_loss: 0.0012519597
test_loss: 0.0015602828
train_loss: 0.0014720013
test_loss: 0.001687509
train_loss: 0.0014850919
test_loss: 0.0016557928
train_loss: 0.0015338985
test_loss: 0.0015620576
train_loss: 0.0013306391
test_loss: 0.0016590996
train_loss: 0.0012634384
test_loss: 0.0017006834
train_loss: 0.0014974889
test_loss: 0.0016250456
train_loss: 0.0012647554
test_loss: 0.0016672311
train_loss: 0.0013942877
test_loss: 0.001612664
train_loss: 0.0012876169
test_loss: 0.0015712678
train_loss: 0.0012081248
test_loss: 0.0016537077
train_loss: 0.0012873174
test_loss: 0.0016414925
train_loss: 0.0014612207
test_loss: 0.0016550948
train_loss: 0.0013937694
test_loss: 0.0017127355
train_loss: 0.0012589492
test_loss: 0.0019226837
train_loss: 0.0014675737
test_loss: 0.0017478227
train_loss: 0.0016307449
test_loss: 0.0017555102
train_loss: 0.0018187874
test_loss: 0.001889799
train_loss: 0.0014819042
test_loss: 0.0018944597
train_loss: 0.0014290505
test_loss: 0.0017799302
train_loss: 0.0013351673
test_loss: 0.0016902862
train_loss: 0.0013307977
test_loss: 0.0016206963
train_loss: 0.0011908772
test_loss: 0.001577104
train_loss: 0.0011877393
test_loss: 0.0015249292
train_loss: 0.0012031231
test_loss: 0.001549605
train_loss: 0.0010613571
test_loss: 0.0015671696
train_loss: 0.0012829078
test_loss: 0.0017610805
train_loss: 0.0015102078
test_loss: 0.001672892
train_loss: 0.001209334
test_loss: 0.0017251688
train_loss: 0.0013515219
test_loss: 0.0016751682
train_loss: 0.0012195015
test_loss: 0.0018035503
train_loss: 0.0013704145
test_loss: 0.0016958136
train_loss: 0.0013266782
test_loss: 0.0016657878
train_loss: 0.0012810588
test_loss: 0.0015294615
train_loss: 0.0012260926
test_loss: 0.001515752
train_loss: 0.0011948401
test_loss: 0.0017445541
train_loss: 0.0013779148
test_loss: 0.0017085113
train_loss: 0.0010718906
test_loss: 0.0016074069
train_loss: 0.0010670313
test_loss: 0.0015124427
train_loss: 0.00113894
test_loss: 0.0015132806
train_loss: 0.0010366818
test_loss: 0.0014762138
train_loss: 0.0011424598
test_loss: 0.0014590345
train_loss: 0.0012882217
test_loss: 0.0015578417
train_loss: 0.0011520417
test_loss: 0.0014313443
train_loss: 0.0011325213
test_loss: 0.0013742587
train_loss: 0.0013580438
test_loss: 0.001520465
train_loss: 0.0011202113
test_loss: 0.0014106838
train_loss: 0.0012786576
test_loss: 0.0014857018
train_loss: 0.0011482352
test_loss: 0.001584729
train_loss: 0.00115974
test_loss: 0.0014902937
train_loss: 0.0011235947
test_loss: 0.0016349334
train_loss: 0.0012406133
test_loss: 0.0016430542
train_loss: 0.0010794294
test_loss: 0.0016996495
train_loss: 0.001234025
test_loss: 0.0018005507
train_loss: 0.0013786533
test_loss: 0.0016905619
train_loss: 0.0013307545
test_loss: 0.0016968126
train_loss: 0.0014673191
test_loss: 0.0016265146
train_loss: 0.0014062092
test_loss: 0.0016970111
train_loss: 0.0013054952
test_loss: 0.0016354639
train_loss: 0.0011453069
test_loss: 0.0015462766
train_loss: 0.0012072285
test_loss: 0.0015298033
train_loss: 0.0012609782
test_loss: 0.0016202746
train_loss: 0.0012712738
test_loss: 0.0017495016
train_loss: 0.0014199073
test_loss: 0.0016831311
train_loss: 0.0014618998
test_loss: 0.001596839
train_loss: 0.0012391658
test_loss: 0.0015275738
train_loss: 0.0011578484
test_loss: 0.0016444133
train_loss: 0.0012451223
test_loss: 0.0016580995
train_loss: 0.001291842
test_loss: 0.001607878
train_loss: 0.0010869462
test_loss: 0.0015949642
train_loss: 0.0011319696
test_loss: 0.0016822894
train_loss: 0.0014275421
test_loss: 0.0016086331
train_loss: 0.0014160448
test_loss: 0.0016574109
train_loss: 0.0011801599
test_loss: 0.0017715229
train_loss: 0.0014051085
test_loss: 0.0017025188
train_loss: 0.0012452377
test_loss: 0.0016132623
train_loss: 0.0011649847
test_loss: 0.0014313147
train_loss: 0.0011039932
test_loss: 0.0015086265
train_loss: 0.0013008752
test_loss: 0.0018257737
train_loss: 0.0013850003
test_loss: 0.0016111869
train_loss: 0.0013397101
test_loss: 0.0017262902
train_loss: 0.0013035841
test_loss: 0.0015692712
train_loss: 0.001080984
test_loss: 0.0016809632
train_loss: 0.0012265645
test_loss: 0.0016297491
train_loss: 0.001055513
test_loss: 0.0017066415
train_loss: 0.0013617052
test_loss: 0.0015225356
train_loss: 0.0015372236
test_loss: 0.0015074005
train_loss: 0.001282041
test_loss: 0.0016688398
train_loss: 0.0012654401
test_loss: 0.001613257
train_loss: 0.001232491
test_loss: 0.001678023
train_loss: 0.0012448898
test_loss: 0.0015630284
train_loss: 0.0014956001
test_loss: 0.001763091
train_loss: 0.0012348052
test_loss: 0.0018044102
train_loss: 0.0014395997
test_loss: 0.0018972495
train_loss: 0.0014594212
test_loss: 0.0016938543
train_loss: 0.001760104
test_loss: 0.001634737
train_loss: 0.0015948177
test_loss: 0.0017975272
train_loss: 0.0014822981
test_loss: 0.001687882
train_loss: 0.0012662355
test_loss: 0.0018491215
train_loss: 0.0014222973
test_loss: 0.001714878
train_loss: 0.0015602865
test_loss: 0.0015139345
train_loss: 0.0013316624
test_loss: 0.0015845533
train_loss: 0.001185343
test_loss: 0.0015815627
train_loss: 0.0012133892
test_loss: 0.001613295
train_loss: 0.0012932738
test_loss: 0.0017592559
train_loss: 0.0013037967
test_loss: 0.0017110824
train_loss: 0.0014115024
test_loss: 0.001554026
train_loss: 0.0012402702
test_loss: 0.0017305559
train_loss: 0.0012674723
test_loss: 0.0015552425
train_loss: 0.0014603002
test_loss: 0.0016828104
train_loss: 0.001330008
test_loss: 0.0016922714
train_loss: 0.0012254128
test_loss: 0.0016805417
train_loss: 0.0013588746
test_loss: 0.001667549
train_loss: 0.001323706
test_loss: 0.0015991565
train_loss: 0.0013725783
test_loss: 0.0014940269
train_loss: 0.0011729525
test_loss: 0.001769063
train_loss: 0.0013403577
test_loss: 0.0016685756
train_loss: 0.0012470088
test_loss: 0.0017236082
train_loss: 0.0012522311
test_loss: 0.0016579087
train_loss: 0.0012855369
test_loss: 0.0015591934
train_loss: 0.0012208999
test_loss: 0.001615001
train_loss: 0.0011919634
test_loss: 0.0017417461
train_loss: 0.0012307822
test_loss: 0.0016134965
train_loss: 0.0011640391
test_loss: 0.0015457253
train_loss: 0.0012061086
test_loss: 0.0016109437
train_loss: 0.0011378494
test_loss: 0.0017586566
train_loss: 0.0013753381
test_loss: 0.0016103593
train_loss: 0.0013562716
test_loss: 0.0015023061
train_loss: 0.0011016845
test_loss: 0.0017244468
train_loss: 0.0013290818
test_loss: 0.0017351555
train_loss: 0.0013883124
test_loss: 0.0014770289
train_loss: 0.0011763718
test_loss: 0.0015557222
train_loss: 0.0013727401
test_loss: 0.0017254578
train_loss: 0.0012116309
test_loss: 0.001487175
train_loss: 0.0011005086
test_loss: 0.0016362083
train_loss: 0.0011391155
test_loss: 0.0017255114
train_loss: 0.0013696674
test_loss: 0.0015806683
train_loss: 0.0012088061
test_loss: 0.0015917558
train_loss: 0.0010810674
test_loss: 0.0017767285
train_loss: 0.0013913012
test_loss: 0.0016523907
train_loss: 0.001363737
test_loss: 0.0017573272
train_loss: 0.0015108326
test_loss: 0.001599631
train_loss: 0.0014445172
test_loss: 0.0015941273
train_loss: 0.0010682344
test_loss: 0.0018871172
train_loss: 0.0016077581
test_loss: 0.0016649296
train_loss: 0.0013700936
test_loss: 0.0016744591
train_loss: 0.0013136138
test_loss: 0.0016461852
train_loss: 0.0015285469
test_loss: 0.0015750448
train_loss: 0.0012739883
test_loss: 0.0015690952
train_loss: 0.0013087147
test_loss: 0.0015804322
train_loss: 0.0011438659
test_loss: 0.0017533674
train_loss: 0.0014254224
test_loss: 0.0015481765
train_loss: 0.0011643112
test_loss: 0.0014765571
train_loss: 0.0011302668
test_loss: 0.0015469872
train_loss: 0.0011316371
test_loss: 0.0015780283
train_loss: 0.0012463375
test_loss: 0.0016750828
train_loss: 0.0012868873
test_loss: 0.0016277467
train_loss: 0.0014895179
test_loss: 0.0015579124
train_loss: 0.0013340674
test_loss: 0.0016726409
train_loss: 0.0014056289
test_loss: 0.0015489776
train_loss: 0.001267246
test_loss: 0.0017169694
train_loss: 0.001407821
test_loss: 0.0016100771
train_loss: 0.0011964443
test_loss: 0.0017230383/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

train_loss: 0.0013050323
test_loss: 0.001659015
train_loss: 0.0014166239
test_loss: 0.0015881478
train_loss: 0.001453615
test_loss: 0.0015423483
train_loss: 0.0012489553
test_loss: 0.0014977353
train_loss: 0.0011244632
test_loss: 0.0015481948
train_loss: 0.0012666249
test_loss: 0.0014615402
train_loss: 0.0011608133
test_loss: 0.001487624
train_loss: 0.0011299625
test_loss: 0.0015099552
train_loss: 0.0012183798
test_loss: 0.0015633955
train_loss: 0.0010312559
test_loss: 0.0015545468
train_loss: 0.0011530756
test_loss: 0.0015724556
train_loss: 0.0012832906
test_loss: 0.0015792864
train_loss: 0.0012531871
test_loss: 0.0016160513
train_loss: 0.0012128137
test_loss: 0.0016787902
train_loss: 0.0013180599
test_loss: 0.0015831914
train_loss: 0.0012717913
test_loss: 0.0015757084
train_loss: 0.0012777835
test_loss: 0.0016491401
train_loss: 0.001419872
test_loss: 0.0015412463
train_loss: 0.0012819202
test_loss: 0.0015528132
train_loss: 0.0012483083
test_loss: 0.0015004869
train_loss: 0.001179807
test_loss: 0.001987252
train_loss: 0.0014818329
test_loss: 0.0017967294
train_loss: 0.001384751
test_loss: 0.0016120923
train_loss: 0.0014980076
test_loss: 0.0017109466
train_loss: 0.0014064965
test_loss: 0.0015741152
train_loss: 0.001185873
test_loss: 0.0014879132
train_loss: 0.0012779533
test_loss: 0.0015291616
train_loss: 0.0013349293
test_loss: 0.0016326326
train_loss: 0.001264369
test_loss: 0.0015411404
train_loss: 0.0013085952
test_loss: 0.0015218706
train_loss: 0.0011189162
test_loss: 0.001646199
train_loss: 0.0012116772
test_loss: 0.0014970975
train_loss: 0.0011528614
test_loss: 0.0015541847
train_loss: 0.00133659
test_loss: 0.0014507596
train_loss: 0.0015268861
test_loss: 0.0016844163
train_loss: 0.0013218529
test_loss: 0.0017302418
train_loss: 0.001238978
test_loss: 0.0017793317
train_loss: 0.0013508941
test_loss: 0.0017742157
train_loss: 0.0012713585
test_loss: 0.0016323684
train_loss: 0.001283881
test_loss: 0.00165133
train_loss: 0.0011556097
test_loss: 0.0016552865
train_loss: 0.0011685286
test_loss: 0.0016120923
train_loss: 0.0012054036
test_loss: 0.0015650037
train_loss: 0.0012008758
test_loss: 0.0017937364
train_loss: 0.0016318855
test_loss: 0.0017383689
train_loss: 0.0014943015
test_loss: 0.0017066229
train_loss: 0.0015747072
test_loss: 0.0018026484
train_loss: 0.0013916218
test_loss: 0.0017942057
train_loss: 0.001289958
test_loss: 0.0017293815
train_loss: 0.0013357408
test_loss: 0.0016866053
train_loss: 0.0013630602
test_loss: 0.0018338051
train_loss: 0.0011378377
test_loss: 0.0016305175
train_loss: 0.0014192705
test_loss: 0.0017593536
train_loss: 0.0013373673
test_loss: 0.0017064505
train_loss: 0.0014082226
test_loss: 0.0016079826
train_loss: 0.0012787454
test_loss: 0.0016188995
train_loss: 0.0011736129
test_loss: 0.0020534366
train_loss: 0.0014936249
test_loss: 0.0016951787
train_loss: 0.0014355653
test_loss: 0.0015152752
train_loss: 0.0013373194
test_loss: 0.0016717352
train_loss: 0.001166286
test_loss: 0.0017115348
train_loss: 0.0013435384
test_loss: 0.0016239976
train_loss: 0.0014108245
test_loss: 0.0015945542
train_loss: 0.0014182884
test_loss: 0.0016574719
train_loss: 0.0011753741
test_loss: 0.0017430095
train_loss: 0.0012481484
test_loss: 0.001625294
train_loss: 0.0011361231
test_loss: 0.0015350054
train_loss: 0.0011463593
test_loss: 0.0014932184
train_loss: 0.0013959704
test_loss: 0.001497186
train_loss: 0.001431394
test_loss: 0.0016183893
train_loss: 0.0011991342
test_loss: 0.00158702
train_loss: 0.001202191
test_loss: 0.0015280832
train_loss: 0.0011568571
test_loss: 0.0017290986
train_loss: 0.0013926881
test_loss: 0.0016711813
train_loss: 0.0013510704
test_loss: 0.0015932765
train_loss: 0.001195285
test_loss: 0.0015450226
train_loss: 0.0012890692
test_loss: 0.0015075664
train_loss: 0.0011581441
test_loss: 0.0018324658
train_loss: 0.0013128209
test_loss: 0.0016327167
train_loss: 0.0013456893
test_loss: 0.0015488883
train_loss: 0.0011321061
test_loss: 0.0016857779
train_loss: 0.0011297103
test_loss: 0.0019069166
train_loss: 0.0012051732
test_loss: 0.0017735674
train_loss: 0.0013920587
test_loss: 0.0016657024
train_loss: 0.001224067
test_loss: 0.00152944
train_loss: 0.001125215
test_loss: 0.001507033
train_loss: 0.0011270797
test_loss: 0.001543421
train_loss: 0.0011127505
test_loss: 0.0015203841
train_loss: 0.0011389361
test_loss: 0.0014895877
train_loss: 0.0010399709
test_loss: 0.0014433275
train_loss: 0.0012105445
test_loss: 0.0017550306
train_loss: 0.0013416234
test_loss: 0.0016407159
train_loss: 0.0013468876
test_loss: 0.0017047377
train_loss: 0.0013225875
test_loss: 0.001558967
train_loss: 0.0012322824
test_loss: 0.0016873854
train_loss: 0.0013486771
test_loss: 0.0017625269
train_loss: 0.0013111318
test_loss: 0.0017124013
train_loss: 0.0011892334
test_loss: 0.0016404178
train_loss: 0.0011236602
test_loss: 0.0015101136
train_loss: 0.001059412
test_loss: 0.0015761013
train_loss: 0.001141601
test_loss: 0.0014674008
train_loss: 0.000980921
test_loss: 0.0015619766
train_loss: 0.0010413635
test_loss: 0.0014630359
train_loss: 0.0012025266
test_loss: 0.0014476746
train_loss: 0.0011662556
test_loss: 0.0015085352
train_loss: 0.0010940129
test_loss: 0.0016846104
train_loss: 0.0013209919
test_loss: 0.0017971321
train_loss: 0.0014155803
test_loss: 0.0017683408
train_loss: 0.0013123549
test_loss: 0.0018087503
train_loss: 0.0016075631
test_loss: 0.0016628101
train_loss: 0.0012281198
test_loss: 0.0016846408
train_loss: 0.0011865564
test_loss: 0.0017586496
train_loss: 0.0015661612
test_loss: 0.00158498
train_loss: 0.0013546994
test_loss: 0.0015130207
train_loss: 0.0011551656
test_loss: 0.0017317549
train_loss: 0.0011794802
test_loss: 0.0019613924
train_loss: 0.0013730302
test_loss: 0.0015548884
train_loss: 0.0012440111
test_loss: 0.0017940231
train_loss: 0.001194072
test_loss: 0.0016154656
train_loss: 0.0012098219
test_loss: 0.0015655158
train_loss: 0.0012088391
test_loss: 0.001723698
train_loss: 0.0013625268
test_loss: 0.0016703364
train_loss: 0.0012840799
test_loss: 0.001528948
train_loss: 0.0013446944
test_loss: 0.0016791691
train_loss: 0.0011643735
test_loss: 0.001610974
train_loss: 0.00101163
test_loss: 0.001548495
train_loss: 0.0011633411
test_loss: 0.0016607919
train_loss: 0.0011132814
test_loss: 0.0018358617
train_loss: 0.001512344
test_loss: 0.0016657776
train_loss: 0.0011285581
test_loss: 0.0015318538
train_loss: 0.0011525613
test_loss: 0.0014861339
train_loss: 0.0012293567
test_loss: 0.0017953279
train_loss: 0.0014428012
test_loss: 0.0015932145
train_loss: 0.0013855901
test_loss: 0.001495283
train_loss: 0.0011258773
test_loss: 0.0016122971
train_loss: 0.001410825
test_loss: 0.0018166065
train_loss: 0.0012403473
test_loss: 0.0016070997
train_loss: 0.0012730516
test_loss: 0.0015819164
train_loss: 0.0014228295
test_loss: 0.0017263698
train_loss: 0.00141336
test_loss: 0.0016100267
train_loss: 0.0011045022
test_loss: 0.0018098181
train_loss: 0.0014528412
test_loss: 0.0014912483
train_loss: 0.0012284248
test_loss: 0.0015798935
train_loss: 0.0012836196
test_loss: 0.0017533812
train_loss: 0.0013971089
test_loss: 0.0015149598
train_loss: 0.0013031729
test_loss: 0.0015700283
train_loss: 0.0011319298
test_loss: 0.0018293335
train_loss: 0.0014705404
test_loss: 0.0016276147
train_loss: 0.0012714039
test_loss: 0.0015875881
train_loss: 0.0013654869
test_loss: 0.00165984
train_loss: 0.001382782
test_loss: 0.0016014755
train_loss: 0.0010313606
test_loss: 0.0016895438
train_loss: 0.0012458327
test_loss: 0.0015223178
train_loss: 0.0011879153
test_loss: 0.0014497394
train_loss: 0.0010894054
test_loss: 0.0015989224
train_loss: 0.0012613797
test_loss: 0.0016414891
train_loss: 0.0011279889
test_loss: 0.0015862661
train_loss: 0.0010502137
test_loss: 0.0017121829
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.4/300_300_300_1 --function f1 --psi 0 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 4000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa892074bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8920979d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa89213d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8920b7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa892003f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa892003598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891f9dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891f6d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa892002a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891f28f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891f282f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891eecf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891efb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891ea61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891ea7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891ea7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891f228c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891e5a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891e5aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891df1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891dc48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891d6b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa891dc4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8550a7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8550a7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8550a7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8550a7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa854fff8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa854fffe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa854fbaf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa854fe91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa854fe97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa830036620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa854f85510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa81071eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa8106dff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.06711448
test_loss: 0.056741167
train_loss: 0.032728456
test_loss: 0.031018194
train_loss: 0.03116321
test_loss: 0.028626632
train_loss: 0.017341645
test_loss: 0.015717695
train_loss: 0.013898821
test_loss: 0.0137186255
train_loss: 0.010645634
test_loss: 0.0104582
train_loss: 0.009188957
test_loss: 0.008737582
train_loss: 0.00786911
test_loss: 0.0077452823
train_loss: 0.006118492
test_loss: 0.0060145985
train_loss: 0.0056970026
test_loss: 0.0057030534
train_loss: 0.0043562846
test_loss: 0.0043987595
train_loss: 0.003905026
test_loss: 0.004072622
train_loss: 0.0033378683
test_loss: 0.003325789
train_loss: 0.0029624142
test_loss: 0.0031418742
train_loss: 0.0025061828
test_loss: 0.002830608
train_loss: 0.0023789953
test_loss: 0.0027837497
train_loss: 0.002381856
test_loss: 0.0025029588
train_loss: 0.0022449552
test_loss: 0.0029490627
train_loss: 0.002359456
test_loss: 0.0025538919
train_loss: 0.002217267
test_loss: 0.0023410218
train_loss: 0.0019368696
test_loss: 0.0022772758
train_loss: 0.0019939332
test_loss: 0.0021323487
train_loss: 0.002069268
test_loss: 0.002249831
train_loss: 0.0020325016
test_loss: 0.002097301
train_loss: 0.0017041194
test_loss: 0.0019459026
train_loss: 0.0016500514
test_loss: 0.002053797
train_loss: 0.0017505558
test_loss: 0.0019752027
train_loss: 0.0017892314
test_loss: 0.0023054578
train_loss: 0.0019487358
test_loss: 0.0021102615
train_loss: 0.0017146738
test_loss: 0.0021238355
train_loss: 0.0016279654
test_loss: 0.0020584215
train_loss: 0.0018371379
test_loss: 0.0019337222
train_loss: 0.0017898364
test_loss: 0.0018846962
train_loss: 0.0016663517
test_loss: 0.0020250932
train_loss: 0.0017710661
test_loss: 0.0019942117
train_loss: 0.0015720119
test_loss: 0.0017974594
train_loss: 0.0015249853
test_loss: 0.0018770125
train_loss: 0.0014795176
test_loss: 0.0020214017
train_loss: 0.0018783375
test_loss: 0.0019314019
train_loss: 0.0015260989
test_loss: 0.0019520452
train_loss: 0.0015103678
test_loss: 0.0018339435
train_loss: 0.0017073621
test_loss: 0.0018744252
train_loss: 0.0014159076
test_loss: 0.0016956216
train_loss: 0.0013897698
test_loss: 0.0017631992
train_loss: 0.0012531226
test_loss: 0.0018347198
train_loss: 0.0016115087
test_loss: 0.0019139593
train_loss: 0.0014838718
test_loss: 0.0019244599
train_loss: 0.0013649089
test_loss: 0.0019352123
train_loss: 0.0018468917
test_loss: 0.0018470917
train_loss: 0.0016384093
test_loss: 0.0020414037
train_loss: 0.0018097145
test_loss: 0.0018228214
train_loss: 0.0014424735
test_loss: 0.0018913542
train_loss: 0.0015308177
test_loss: 0.0020035468
train_loss: 0.0013658354
test_loss: 0.0017036526
train_loss: 0.0014603846
test_loss: 0.0016504057
train_loss: 0.001457796
test_loss: 0.00211096
train_loss: 0.0014019413
test_loss: 0.0019344576
train_loss: 0.0014965752
test_loss: 0.0023964755
train_loss: 0.0020799807
test_loss: 0.0019289411
train_loss: 0.0017202497
test_loss: 0.0020937307
train_loss: 0.0016799135
test_loss: 0.002011527
train_loss: 0.0017886402
test_loss: 0.0020363596
train_loss: 0.0017669771
test_loss: 0.0018472403
train_loss: 0.0014845328
test_loss: 0.0018322122
train_loss: 0.0014983509
test_loss: 0.002021466
train_loss: 0.0014890791
test_loss: 0.0018348902
train_loss: 0.0013155371
test_loss: 0.0018281522
train_loss: 0.0016406688
test_loss: 0.0018387665
train_loss: 0.0013118093
test_loss: 0.0018633672
train_loss: 0.0016394994
test_loss: 0.001715776
train_loss: 0.0015647801
test_loss: 0.0017413811
train_loss: 0.0014828362
test_loss: 0.0016766818
train_loss: 0.0014203456
test_loss: 0.0018681757
train_loss: 0.0018528507
test_loss: 0.0019351417
train_loss: 0.0017614773
test_loss: 0.0018000324
train_loss: 0.0014036025
test_loss: 0.0021450263
train_loss: 0.0014436385
test_loss: 0.002411468
train_loss: 0.0018284041
test_loss: 0.0023530815
train_loss: 0.001763647
test_loss: 0.0024624958
train_loss: 0.0019977323
test_loss: 0.0022828416
train_loss: 0.0019420946
test_loss: 0.0019438394
train_loss: 0.0017038224
test_loss: 0.0020069496
train_loss: 0.0014066666
test_loss: 0.00238294
train_loss: 0.0019409383
test_loss: 0.0021762324
train_loss: 0.0020187986
test_loss: 0.0019639498
train_loss: 0.0016228023
test_loss: 0.0020081755
train_loss: 0.0015212131
test_loss: 0.001924086
train_loss: 0.0017292601
test_loss: 0.0017646584
train_loss: 0.0016961448
test_loss: 0.0017326957
train_loss: 0.0015746318
test_loss: 0.0019064432
train_loss: 0.0014790343
test_loss: 0.0019651856
train_loss: 0.0016797658
test_loss: 0.0017853917
train_loss: 0.0017289323
test_loss: 0.0020124787
train_loss: 0.0016750441
test_loss: 0.0017728838
train_loss: 0.0013427979
test_loss: 0.0017529787
train_loss: 0.0014216925
test_loss: 0.0018032042
train_loss: 0.0013758368
test_loss: 0.0016823829
train_loss: 0.0013511373
test_loss: 0.0017338821
train_loss: 0.001439419
test_loss: 0.0019664785
train_loss: 0.0013621297
test_loss: 0.0017688326
train_loss: 0.0014475938
test_loss: 0.0017208614
train_loss: 0.0014058944
test_loss: 0.0019364476
train_loss: 0.0014554925
test_loss: 0.0018498987
train_loss: 0.0016910917
test_loss: 0.0017589973
train_loss: 0.0014775735
test_loss: 0.0017796117
train_loss: 0.0013084572
test_loss: 0.0017766944
train_loss: 0.0013934079
test_loss: 0.001645624
train_loss: 0.0014705907
test_loss: 0.0017661694
train_loss: 0.0012176012
test_loss: 0.0018179885
train_loss: 0.0014435747
test_loss: 0.001796443
train_loss: 0.0013914285
test_loss: 0.0016913431
train_loss: 0.0014988701
test_loss: 0.0017058187
train_loss: 0.001709963
test_loss: 0.0017246796
train_loss: 0.0015214528
test_loss: 0.0016697097
train_loss: 0.0015109698
test_loss: 0.0017640702
train_loss: 0.0014977312
test_loss: 0.001654866
train_loss: 0.0015308291
test_loss: 0.0017495676
train_loss: 0.0013610071
test_loss: 0.0017595997
train_loss: 0.0013818757
test_loss: 0.0018857182
train_loss: 0.0014106354
test_loss: 0.0017547669
train_loss: 0.0013384796
test_loss: 0.0017862867
train_loss: 0.0015036575
test_loss: 0.0017181521
train_loss: 0.0014598453
test_loss: 0.0017323735
train_loss: 0.0013168305
test_loss: 0.0017516083
train_loss: 0.0013261768
test_loss: 0.0019339282
train_loss: 0.0012645596
test_loss: 0.0016765635
train_loss: 0.001440371
test_loss: 0.0019401545
train_loss: 0.0014294388
test_loss: 0.0017681809
train_loss: 0.00148591
test_loss: 0.0019156666
train_loss: 0.0015152836
test_loss: 0.0018169967
train_loss: 0.0013925601
test_loss: 0.0016495532
train_loss: 0.0012495175
test_loss: 0.0016745381
train_loss: 0.0013182812
test_loss: 0.0017395522
train_loss: 0.0012145657
test_loss: 0.0017139311
train_loss: 0.001334838
test_loss: 0.0016193594
train_loss: 0.0012075199
test_loss: 0.0015851294
train_loss: 0.0015197098
test_loss: 0.0016863657
train_loss: 0.0013027498
test_loss: 0.0016412173
train_loss: 0.0014093709
test_loss: 0.0016498946
train_loss: 0.0014328924
test_loss: 0.0018483804
train_loss: 0.0013075683
test_loss: 0.0019575467
train_loss: 0.0016680686
test_loss: 0.0017665434
train_loss: 0.0013771709
test_loss: 0.0018358128
train_loss: 0.0017020439
test_loss: 0.0018373252
train_loss: 0.001531933
test_loss: 0.0018321995
train_loss: 0.001531708
test_loss: 0.0019512532
train_loss: 0.0013599772
test_loss: 0.0018177226
train_loss: 0.0017155139
test_loss: 0.0017591187
train_loss: 0.0015981509
test_loss: 0.0017900682
train_loss: 0.0015798502
test_loss: 0.0017478876
train_loss: 0.0015331382
test_loss: 0.0018334487
train_loss: 0.0013900972
test_loss: 0.0017622873
train_loss: 0.0012085275
test_loss: 0.0015639596
train_loss: 0.0013685317
test_loss: 0.0016214037
train_loss: 0.0016452479
test_loss: 0.0017058719
train_loss: 0.0014696438
test_loss: 0.0017728681
train_loss: 0.0014738333
test_loss: 0.0019093913
train_loss: 0.0015010592
test_loss: 0.0016216255
train_loss: 0.0011981018
test_loss: 0.0017221652
train_loss: 0.0013305222
test_loss: 0.0016965315
train_loss: 0.0013941908
test_loss: 0.0018334715
train_loss: 0.0014256344
test_loss: 0.0018143082
train_loss: 0.0016093364
test_loss: 0.0016866968
train_loss: 0.0015997728
test_loss: 0.0016803863
train_loss: 0.001691162
test_loss: 0.0016820572
train_loss: 0.0014110286
test_loss: 0.0018171936
train_loss: 0.0015206832
test_loss: 0.0018719544
train_loss: 0.0014268409
test_loss: 0.001972015
train_loss: 0.0014882325
test_loss: 0.0017698688
train_loss: 0.0014977911
test_loss: 0.0016899694
train_loss: 0.0013314981
test_loss: 0.001838241
train_loss: 0.0012960867
test_loss: 0.0019551702
train_loss: 0.0015144635
test_loss: 0.0018291645
train_loss: 0.0015851562
test_loss: 0.0018388449
train_loss: 0.0014225666
test_loss: 0.0016395529
train_loss: 0.0014508918
test_loss: 0.0020133725
train_loss: 0.0014858338
test_loss: 0.001807142
train_loss: 0.0016114141
test_loss: 0.0018453783
train_loss: 0.0015241005
test_loss: 0.0020856257
train_loss: 0.0017024808
test_loss: 0.0016664756
train_loss: 0.0012188043
test_loss: 0.0021669115
train_loss: 0.0014684928
test_loss: 0.0017357409
train_loss: 0.0019466763
test_loss: 0.0019793883
train_loss: 0.0015878439
test_loss: 0.0020367603
train_loss: 0.0015465132
test_loss: 0.0018805345
train_loss: 0.0014408063
test_loss: 0.001929166
train_loss: 0.0013070395
test_loss: 0.0019870584
train_loss: 0.0017281494
test_loss: 0.0016719513
train_loss: 0.0014540828
test_loss: 0.001924403
train_loss: 0.0014330758
test_loss: 0.0016808618
train_loss: 0.0013953942
test_loss: 0.0016905658
train_loss: 0.0014662384
test_loss: 0.0017523131
train_loss: 0.0012682161
test_loss: 0.0017103391
train_loss: 0.0010726918
test_loss: 0.0018829285
train_loss: 0.00130677
test_loss: 0.001832075
train_loss: 0.001426773
test_loss: 0.0018375525
train_loss: 0.0012845836
test_loss: 0.0020701622
train_loss: 0.0016928972
test_loss: 0.0017617123
train_loss: 0.0014600703
test_loss: 0.0017797034
train_loss: 0.0015312183
test_loss: 0.0017913904
train_loss: 0.0014940894
test_loss: 0.0018919008
train_loss: 0.0014669257
test_loss: 0.0019790637
train_loss: 0.0016395029
test_loss: 0.0019108003
train_loss: 0.0015287753
test_loss: 0.0017820522
train_loss: 0.0018968631
test_loss: 0.0017314333
train_loss: 0.001476753
test_loss: 0.002076721
train_loss: 0.0015106676
test_loss: 0.0019641456
train_loss: 0.001655428
test_loss: 0.0018649496
train_loss: 0.0016823848
test_loss: 0.0018483559
train_loss: 0.001457632
test_loss: 0.0019191359
train_loss: 0.0017485297
test_loss: 0.0018680329
train_loss: 0.0015175173
test_loss: 0.0018506125
train_loss: 0.0014579293
test_loss: 0.0019471906
train_loss: 0.0012929279
test_loss: 0.0019790614
train_loss: 0.0012917618
test_loss: 0.0017452867
train_loss: 0.0013454837
test_loss: 0.0017319034
train_loss: 0.0012485096
test_loss: 0.0016735976
train_loss: 0.001400059
test_loss: 0.0016862439
train_loss: 0.0012685483
test_loss: 0.0018681997
train_loss: 0.0014580733
test_loss: 0.001797695
train_loss: 0.0015949893
test_loss: 0.0018089052
train_loss: 0.0013935913
test_loss: 0.0018249269
train_loss: 0.0013901792
test_loss: 0.0017225785
train_loss: 0.001303826
test_loss: 0.002011631
train_loss: 0.0014688319
test_loss: 0.0017614868
train_loss: 0.0013175103
test_loss: 0.001673135
train_loss: 0.0013054246
test_loss: 0.0016743363
train_loss: 0.0013012901
test_loss: 0.0016317832
train_loss: 0.0015272081
test_loss: 0.0015832265
train_loss: 0.0012381625
test_loss: 0.0016399812
train_loss: 0.0013352748
test_loss: 0.0016659977
train_loss: 0.0014398585
test_loss: 0.0016980427
train_loss: 0.0012330764
test_loss: 0.0020108076
train_loss: 0.0016583991
test_loss: 0.0017611166
train_loss: 0.0016356905
test_loss: 0.0018901388
train_loss: 0.0013209531
test_loss: 0.0019794165
train_loss: 0.0015202973
test_loss: 0.0019229876
train_loss: 0.0014477719
test_loss: 0.0018911209
train_loss: 0.0015612912
test_loss: 0.0016534025
train_loss: 0.0016123939
test_loss: 0.0017842167
train_loss: 0.0013980148
test_loss: 0.0017362344
train_loss: 0.0014043655
test_loss: 0.0017154728
train_loss: 0.0013353066
test_loss: 0.0018234892
train_loss: 0.0015668138
test_loss: 0.0017311727
train_loss: 0.0014041378
test_loss: 0.0017441033
train_loss: 0.0013935688
test_loss: 0.0016790793
train_loss: 0.0013323335
test_loss: 0.001685066
train_loss: 0.0011810723
test_loss: 0.0016995402
train_loss: 0.0013980424
test_loss: 0.0017487466
train_loss: 0.001375016
test_loss: 0.0018183388
train_loss: 0.0012299925
test_loss: 0.0018296107
train_loss: 0.0012313235
test_loss: 0.0019162013
train_loss: 0.0012183476
test_loss: 0.0018877477
train_loss: 0.0014719167
test_loss: 0.0018019567
train_loss: 0.0015805331
test_loss: 0.001796141
train_loss: 0.001501617
test_loss: 0.0019037209
train_loss: 0.0016866159
test_loss: 0.0017151111
train_loss: 0.0012596884
test_loss: 0.0022881667
train_loss: 0.001562241
test_loss: 0.0018452411
train_loss: 0.0016733867
test_loss: 0.001955794
train_loss: 0.0014353754
test_loss: 0.0019746816
train_loss: 0.0015395045
test_loss: 0.001793142
train_loss: 0.0014883215
test_loss: 0.0017119322
train_loss: 0.0012695657
test_loss: 0.0016547109
train_loss: 0.0013095827
test_loss: 0.0016311845
train_loss: 0.0014000965
test_loss: 0.001773295
train_loss: 0.0013947557
test_loss: 0.0020065662
train_loss: 0.0014402197
test_loss: 0.0018674986
train_loss: 0.0013983693
test_loss: 0.0018686021
train_loss: 0.0016228417
test_loss: 0.0016153618
train_loss: 0.0012765919
test_loss: 0.0017339329
train_loss: 0.0014688219
test_loss: 0.0019580417
train_loss: 0.0013337231
test_loss: 0.001929031
train_loss: 0.0015948346
test_loss: 0.0017285377
train_loss: 0.0012638919
test_loss: 0.0021221014
train_loss: 0.0015997639
test_loss: 0.0016698395
train_loss: 0.0017175148
test_loss: 0.0018803785
train_loss: 0.0014940564
test_loss: 0.0017689023
train_loss: 0.001327095
test_loss: 0.0019349288
train_loss: 0.001531868
test_loss: 0.001687843
train_loss: 0.0016235968
test_loss: 0.0017833048
train_loss: 0.0012446304
test_loss: 0.0019135983
train_loss: 0.0014053769
test_loss: 0.0017907181
train_loss: 0.0015662955
test_loss: 0.0016403197
train_loss: 0.0017031627
test_loss: 0.0017802613
train_loss: 0.0015409909
test_loss: 0.0017900947
train_loss: 0.0013219751
test_loss: 0.0019679524
train_loss: 0.0012920505
test_loss: 0.0020659633
train_loss: 0.001237011
test_loss: 0.0020558243
train_loss: 0.0014863559
test_loss: 0.0018123954
train_loss: 0.0017095497
test_loss: 0.0016943605
train_loss: 0.0017456434
test_loss: 0.0019555013
train_loss: 0.0016048168
test_loss: 0.0018719585
train_loss: 0.001810373
test_loss: 0.0016931902
train_loss: 0.0015477632
test_loss: 0.0018554921
train_loss: 0.0012392142
test_loss: 0.0020712293
train_loss: 0.0015407999
test_loss: 0.0019293122
train_loss: 0.0014705466
test_loss: 0.0019507438
train_loss: 0.0013096498
test_loss: 0.001821785
train_loss: 0.0016057481
test_loss: 0.0017302189
train_loss: 0.0013634266
test_loss: 0.0018796612
train_loss: 0.0014667779
test_loss: 0.0016980334
train_loss: 0.001568985
test_loss: 0.0017961266
train_loss: 0.0014699387
test_loss: 0.0018217403
train_loss: 0.0014480175
test_loss: 0.0017184139
train_loss: 0.0012454916
test_loss: 0.0018908022
train_loss: 0.0015081872
test_loss: 0.0017710635
train_loss: 0.0014857589
test_loss: 0.001804205
train_loss: 0.001346127
test_loss: 0.001753061
train_loss: 0.0016721901
test_loss: 0.0017687817
train_loss: 0.0013931673
test_loss: 0.0018017822
train_loss: 0.0013944198
test_loss: 0.0018340703
train_loss: 0.0016043461
test_loss: 0.0017347418
train_loss: 0.0012617883
test_loss: 0.0019398436
train_loss: 0.0014664407
test_loss: 0.0017762196
train_loss: 0.001469394
test_loss: 0.0016876869
train_loss: 0.0012217542
test_loss: 0.0017684167
train_loss: 0.0014151717
test_loss: 0.0018858017
train_loss: 0.0015845322
test_loss: 0.0016621012
train_loss: 0.0012066844
test_loss: 0.0018037527
train_loss: 0.0012207867
test_loss: 0.0018745855
train_loss: 0.0014647074
test_loss: 0.0016545145
train_loss: 0.0013201754
test_loss: 0.0016936762
train_loss: 0.0012726738
test_loss: 0.0017781155
train_loss: 0.0012812149
test_loss: 0.0016774525
train_loss: 0.0015443193
test_loss: 0.0016524096
train_loss: 0.0014599398
test_loss: 0.001675553
train_loss: 0.0014106117
test_loss: 0.0016631195
train_loss: 0.0015478937
test_loss: 0.0017381483
train_loss: 0.0014106211
test_loss: 0.0019469658
train_loss: 0.0014945783
test_loss: 0.0017542196
train_loss: 0.0015860471
test_loss: 0.0019355639
train_loss: 0.0012429073
test_loss: 0.0018338573
train_loss: 0.0013440866
test_loss: 0.0018371786
train_loss: 0.001366476
test_loss: 0.0016692545
train_loss: 0.001177575
test_loss: 0.0018139016
train_loss: 0.0013348466
test_loss: 0.0019459211
train_loss: 0.0016634278
test_loss: 0.0016850951
train_loss: 0.0014479968
test_loss: 0.0017736572
train_loss: 0.0012950198
test_loss: 0.0018235019
train_loss: 0.0014016833
test_loss: 0.0017028261
train_loss: 0.0014306564
test_loss: 0.0018873145
train_loss: 0.0019696353
test_loss: 0.0017473643
train_loss: 0.0016367817
test_loss: 0.0019094312
train_loss: 0.0013152524
test_loss: 0.002189039
train_loss: 0.001666578
test_loss: 0.0020654285
train_loss: 0.0012828524
test_loss: 0.0018666936
train_loss: 0.0013873072
test_loss: 0.0016934007
train_loss: 0.0011182514
test_loss: 0.0016451398
train_loss: 0.0011436453
test_loss: 0.0017340779
train_loss: 0.0013088251
test_loss: 0.001723533
train_loss: 0.001147334
test_loss: 0.0016095568
train_loss: 0.0011579057
test_loss: 0.0015656517
train_loss: 0.0013392379
test_loss: 0.001602276
train_loss: 0.0015551109
test_loss: 0.001616274
train_loss: 0.0013385802
test_loss: 0.0016723964
train_loss: 0.0014307944
test_loss: 0.0017844356
train_loss: 0.0015372778
test_loss: 0.0018476959
train_loss: 0.0013563468
test_loss: 0.0018765554
train_loss: 0.0012549263
test_loss: 0.0018308957
train_loss: 0.00131539
test_loss: 0.0018636795
train_loss: 0.0013598762
test_loss: 0.0018500717
train_loss: 0.0012684643
test_loss: 0.001756492
train_loss: 0.0013032807
test_loss: 0.0018249641
train_loss: 0.0014523567
test_loss: 0.0019557192
train_loss: 0.0015690716
test_loss: 0.0017805743
train_loss: 0.001436888
test_loss: 0.0018993403
train_loss: 0.0015675232
test_loss: 0.0016890776
train_loss: 0.001190648
test_loss: 0.0017013081
train_loss: 0.0012238262
test_loss: 0.0018285887
train_loss: 0.0012213937
test_loss: 0.0017186897
train_loss: 0.0012808455
test_loss: 0.0017504084
train_loss: 0.0013183083
test_loss: 0.0017319496
train_loss: 0.0012805125
test_loss: 0.0016207529
train_loss: 0.001296563
test_loss: 0.0017372506
train_loss: 0.0013355301
test_loss: 0.0016418849
train_loss: 0.0013027037
test_loss: 0.00160354
train_loss: 0.001326753
test_loss: 0.001590543
train_loss: 0.0012919455
test_loss: 0.001852402
train_loss: 0.0013487259
test_loss: 0.0020199276
train_loss: 0.0015962984
test_loss: 0.0018501322
train_loss: 0.0015143564
test_loss: 0.0016296355
train_loss: 0.001485846
test_loss: 0.0017306971
train_loss: 0.0016018008
test_loss: 0.0020558075
train_loss: 0.0013961513
test_loss: 0.0016885162
train_loss: 0.001244612
test_loss: 0.0017552772
train_loss: 0.0012271816
test_loss: 0.0018337532
train_loss: 0.0012310378
test_loss: 0.0019040937
train_loss: 0.0013845243
test_loss: 0.001881661
train_loss: 0.0015014254
test_loss: 0.0016568154
train_loss: 0.0014954531
test_loss: 0.0016032227
train_loss: 0.0012645003
test_loss: 0.001654582
train_loss: 0.0012315187
test_loss: 0.0017495872
train_loss: 0.0012225456
test_loss: 0.0017848702
train_loss: 0.0013565033
test_loss: 0.0016791901
train_loss: 0.001346454
test_loss: 0.0017894801
train_loss: 0.0015389225
test_loss: 0.0017074783
train_loss: 0.0013425038
test_loss: 0.0016782748
train_loss: 0.0013488203
test_loss: 0.001993499
train_loss: 0.001538491
test_loss: 0.0015970081
train_loss: 0.0011977901
test_loss: 0.0020288955
train_loss: 0.0017028933
test_loss: 0.0016711316
train_loss: 0.0013507407
test_loss: 0.0017792052
train_loss: 0.0013223948
test_loss: 0.0018169349
train_loss: 0.001143183
test_loss: 0.0017126155
train_loss: 0.0012899016
test_loss: 0.0016369274
train_loss: 0.0012168494
test_loss: 0.0017315646
train_loss: 0.0011680315
test_loss: 0.0017663056
train_loss: 0.0010677609
test_loss: 0.0017722059
train_loss: 0.0012126492
test_loss: 0.001890187
train_loss: 0.0014667315
test_loss: 0.0016837189
train_loss: 0.0013977693
test_loss: 0.0017094652
train_loss: 0.0012337894
test_loss: 0.0017818445
train_loss: 0.0012054241
test_loss: 0.0019954217
train_loss: 0.0015959335
test_loss: 0.001734751
train_loss: 0.0012853859
test_loss: 0.00175323
train_loss: 0.001621673
test_loss: 0.0020092535
train_loss: 0.0013472275
test_loss: 0.0018861524
train_loss: 0.0013434357
test_loss: 0.0017829079
train_loss: 0.0014869437
test_loss: 0.0017430924
train_loss: 0.0012644291
test_loss: 0.0018194803
train_loss: 0.0013654856
test_loss: 0.0017721679
train_loss: 0.001547371
test_loss: 0.0017360826
train_loss: 0.0012732612
test_loss: 0.0018532198
train_loss: 0.0012953717
test_loss: 0.0016665601
train_loss: 0.0013151101
test_loss: 0.0017342386
train_loss: 0.0013330168
test_loss: 0.0016741788
train_loss: 0.001356279
test_loss: 0.0016491245
train_loss: 0.001285614
test_loss: 0.0017490169
train_loss: 0.0013074853
test_loss: 0.0016980873
train_loss: 0.0013829642
test_loss: 0.0017924036
train_loss: 0.0011171674
test_loss: 0.0016648632
train_loss: 0.0012631682
test_loss: 0.0017064743
train_loss: 0.0011856443
test_loss: 0.0017903508
train_loss: 0.0013612197
test_loss: 0.0017236803
train_loss: 0.0012750332
test_loss: 0.0019278078
train_loss: 0.0015166834
test_loss: 0.0018361155
train_loss: 0.0014392543
test_loss: 0.0019879264
train_loss: 0.0017962302
test_loss: 0.0017458961
train_loss: 0.0019087488
test_loss: 0.001924886
train_loss: 0.0015041558
test_loss: 0.002114984
train_loss: 0.001621862
test_loss: 0.0019296675
train_loss: 0.0016024068
test_loss: 0.0016395143
train_loss: 0.0012977739
test_loss: 0.0016991377
train_loss: 0.0014141527
test_loss: 0.001716749
train_loss: 0.001297706
test_loss: 0.0016769607
train_loss: 0.0014000756
test_loss: 0.0016113791
train_loss: 0.0012294311
test_loss: 0.0017894269
train_loss: 0.0013761428
test_loss: 0.0015954756
train_loss: 0.0012385184
test_loss: 0.0015993717
train_loss: 0.00121763
test_loss: 0.0016272422
train_loss: 0.0011570062
test_loss: 0.0016060038
train_loss: 0.0012859071
test_loss: 0.0015646812
train_loss: 0.0011833165
test_loss: 0.0016892833
train_loss: 0.0011093954
test_loss: 0.0015931625
train_loss: 0.0012880806
test_loss: 0.0016782773
train_loss: 0.0012932378
test_loss: 0.0016242267
train_loss: 0.0012052087
test_loss: 0.0016841085
train_loss: 0.0016985193
test_loss: 0.0018267827
train_loss: 0.0013806933
test_loss: 0.0017855853
train_loss: 0.0013278322
test_loss: 0.0017032656
train_loss: 0.0013054273
test_loss: 0.0017564654
train_loss: 0.0013526035
test_loss: 0.0017827578
train_loss: 0.0015596829
test_loss: 0.0016220848
train_loss: 0.0012953962
test_loss: 0.0016191459
train_loss: 0.0013228017
test_loss: 0.0015906648
train_loss: 0.0013089125
test_loss: 0.0017187429
train_loss: 0.0013558763
test_loss: 0.0016853323
train_loss: 0.0014331772
test_loss: 0.0017861171
train_loss: 0.0011849827
test_loss: 0.0017700703
train_loss: 0.0011574132
test_loss: 0.0017232143
train_loss: 0.001185224
test_loss: 0.0016253495
train_loss: 0.0011104653
test_loss: 0.0017209491
train_loss: 0.0011627584
test_loss: 0.0017171194
train_loss: 0.0011714317
test_loss: 0.001609913
train_loss: 0.0013236391
test_loss: 0.0017164937
train_loss: 0.001324924
test_loss: 0.0017009598
train_loss: 0.0012217942
test_loss: 0.0016720986
train_loss: 0.0016771883
test_loss: 0.0017347664
train_loss: 0.0012959023
test_loss: 0.001985399
train_loss: 0.001452199
test_loss: 0.001843632
train_loss: 0.0013652114
test_loss: 0.0019174082
train_loss: 0.0015676118
test_loss: 0.0018337879
train_loss: 0.0015136031
test_loss: 0.0018278691
train_loss: 0.0017018795
test_loss: 0.0017899096
train_loss: 0.0012990943
test_loss: 0.001812643
train_loss: 0.0013288198
test_loss: 0.0018988773
train_loss: 0.0013952618
test_loss: 0.0017795059
train_loss: 0.0014304663
test_loss: 0.0016838036
train_loss: 0.0011778102
test_loss: 0.0018613772
train_loss: 0.0015380769
test_loss: 0.0019662597
train_loss: 0.0012680164
test_loss: 0.0019128955
train_loss: 0.0017801791
test_loss: 0.001837512
train_loss: 0.0016763057
test_loss: 0.0018277013
train_loss: 0.001620303
test_loss: 0.0018284105
train_loss: 0.0013571546
test_loss: 0.0021389106
train_loss: 0.0016223062
test_loss: 0.0023232833
train_loss: 0.0016794144
test_loss: 0.0022478849
train_loss: 0.001598223
test_loss: 0.0022910424
train_loss: 0.0016112108
test_loss: 0.00242477
train_loss: 0.0017635043
test_loss: 0.002223039
train_loss: 0.0016798801
test_loss: 0.0021233826
train_loss: 0.001714735
test_loss: 0.0020147178
train_loss: 0.0014585974
test_loss: 0.0020150244
train_loss: 0.0013417468
test_loss: 0.002019867
train_loss: 0.001367501
test_loss: 0.0021889976
train_loss: 0.0015802351
test_loss: 0.0020485667
train_loss: 0.001441214
test_loss: 0.0019584182
train_loss: 0.0012987469
test_loss: 0.001856841
train_loss: 0.0011532125
test_loss: 0.0019629572
train_loss: 0.0014446405
test_loss: 0.001920123
train_loss: 0.0014792061
test_loss: 0.0019077222
train_loss: 0.0014828797
test_loss: 0.0017012813
train_loss: 0.0014265052
test_loss: 0.0016230958
train_loss: 0.0012251448
test_loss: 0.001779344
train_loss: 0.0012945564
test_loss: 0.0019313893
train_loss: 0.0013098182
test_loss: 0.0019284888
train_loss: 0.0014294222
test_loss: 0.0019517384
train_loss: 0.0015029621
test_loss: 0.001723152
train_loss: 0.0013639982
test_loss: 0.0017663033
train_loss: 0.0012858636
test_loss: 0.0016710606
train_loss: 0.0010812399
test_loss: 0.0018083792
train_loss: 0.0012419597
test_loss: 0.0018742422
train_loss: 0.0013433705
test_loss: 0.0017929261
train_loss: 0.0013370109
test_loss: 0.0017222444
train_loss: 0.0012892497
test_loss: 0.0016442225
train_loss: 0.001410228
test_loss: 0.0016901274
train_loss: 0.0013134021
test_loss: 0.0017479699
train_loss: 0.0011843116
test_loss: 0.0017861635
train_loss: 0.0013963378
test_loss: 0.0017046898
train_loss: 0.0012779906
test_loss: 0.0017647643
train_loss: 0.0012726971
test_loss: 0.0017719939
train_loss: 0.0010646557
test_loss: 0.0016173138
train_loss: 0.0010462544
test_loss: 0.0016678636
train_loss: 0.0011668659
test_loss: 0.0016740762
train_loss: 0.0010434132
test_loss: 0.0017487034
train_loss: 0.001279867
test_loss: 0.0017109045
train_loss: 0.0013730873
test_loss: 0.0016868896
train_loss: 0.0012361584
test_loss: 0.0016522454
train_loss: 0.0010513142
test_loss: 0.0015358477
train_loss: 0.001132173
test_loss: 0.0017710063
train_loss: 0.001127753
test_loss: 0.0017070798
train_loss: 0.0012658973
test_loss: 0.0018473299
train_loss: 0.0013070768
test_loss: 0.0018366281
train_loss: 0.0016085678
test_loss: 0.0016072271
train_loss: 0.0012277012
test_loss: 0.0017110885
train_loss: 0.0013515422
test_loss: 0.0016876362
train_loss: 0.0013184557
test_loss: 0.0018731662
train_loss: 0.0014100755
test_loss: 0.0017663406
train_loss: 0.0013929366
test_loss: 0.001795284
train_loss: 0.0014802618
test_loss: 0.0017328715
train_loss: 0.0014403801
test_loss: 0.0018539611
train_loss: 0.0012451041
test_loss: 0.0017285029
train_loss: 0.0013204886
test_loss: 0.0018205026
train_loss: 0.0013628261
test_loss: 0.0017438924
train_loss: 0.0012466368
test_loss: 0.0016232082
train_loss: 0.0012590262
test_loss: 0.001834453
train_loss: 0.0012548356
test_loss: 0.0016573096
train_loss: 0.0012271287
test_loss: 0.0016947321
train_loss: 0.0012103022
test_loss: 0.0016229314
train_loss: 0.0012758774
test_loss: 0.0016773787
train_loss: 0.001655316
test_loss: 0.001752186
train_loss: 0.0012088453
test_loss: 0.001888354
train_loss: 0.0013437769
test_loss: 0.0015970604
train_loss: 0.0013110699
test_loss: 0.0017505421
train_loss: 0.0013981787
test_loss: 0.0018465513
train_loss: 0.0013606749
test_loss: 0.0018051784
train_loss: 0.0012566831
test_loss: 0.001783583
train_loss: 0.0013849952
test_loss: 0.0016235351
train_loss: 0.0011788115
test_loss: 0.0016888112
train_loss: 0.0011446492
test_loss: 0.0017867353
train_loss: 0.0010156799
test_loss: 0.0016977525
train_loss: 0.0012649801
test_loss: 0.0018023931
train_loss: 0.0013869833
test_loss: 0.0018931297
train_loss: 0.0016182498
test_loss: 0.0017336835
train_loss: 0.0015343567
test_loss: 0.0018502019
train_loss: 0.001330288
test_loss: 0.0016177244
train_loss: 0.0010714068
test_loss: 0.0017537465
train_loss: 0.0011068495
test_loss: 0.0017307122
train_loss: 0.0013829549
test_loss: 0.0016560886
train_loss: 0.0013802095
test_loss: 0.0017021166
train_loss: 0.0013080367
test_loss: 0.0016688074
train_loss: 0.0011738562
test_loss: 0.0018184776
train_loss: 0.001381855
test_loss: 0.0020291943
train_loss: 0.001284127
test_loss: 0.001854401
train_loss: 0.0012340513
test_loss: 0.0017684283
train_loss: 0.001227035
test_loss: 0.0016612702
train_loss: 0.0011546535
test_loss: 0.0016646283
train_loss: 0.0012313817
test_loss: 0.001601668
train_loss: 0.0011465664
test_loss: 0.0016139584
train_loss: 0.0012667183
test_loss: 0.0015758318
train_loss: 0.001198532
test_loss: 0.0017578608
train_loss: 0.0011967596
test_loss: 0.001558015
train_loss: 0.0012273027
test_loss: 0.001624147
train_loss: 0.0014096855
test_loss: 0.0015862868
train_loss: 0.0015110779
test_loss: 0.0016491626
train_loss: 0.0013380207
test_loss: 0.0016435268
train_loss: 0.0014416612
test_loss: 0.0015741221
train_loss: 0.0014440658
test_loss: 0.0018747653
train_loss: 0.001251627
test_loss: 0.0016934618
train_loss: 0.0013962825
test_loss: 0.0016407899
train_loss: 0.0014502842
test_loss: 0.0017604529
train_loss: 0.0012922245
test_loss: 0.0020119313
train_loss: 0.0014902605
test_loss: 0.0017432334
train_loss: 0.0012496505
test_loss: 0.0018298728
train_loss: 0.0012783809
test_loss: 0.002000315
train_loss: 0.0013668032
test_loss: 0.0017212179
train_loss: 0.0013501646
test_loss: 0.0016459388
train_loss: 0.001290533
test_loss: 0.0019420799
train_loss: 0.0013858614
test_loss: 0.001706803
train_loss: 0.0013617185
test_loss: 0.0019714723
train_loss: 0.0014359271
test_loss: 0.0017876654
train_loss: 0.0013232174
test_loss: 0.001579711
train_loss: 0.0010405823
test_loss: 0.0018061175
train_loss: 0.0012082428
test_loss: 0.0018870286
train_loss: 0.0015082764
test_loss: 0.001713954
train_loss: 0.0011644376
test_loss: 0.0018474328
train_loss: 0.0014083169
test_loss: 0.0020016872
train_loss: 0.0014285727
test_loss: 0.0018158649
train_loss: 0.0012967944
test_loss: 0.0017815497
train_loss: 0.0014156166
test_loss: 0.0018097204
train_loss: 0.0015411365
test_loss: 0.001800747
train_loss: 0.0014220045
test_loss: 0.0016301537
train_loss: 0.0014104983
test_loss: 0.0016840001
train_loss: 0.0014330841
test_loss: 0.0016545082
train_loss: 0.0013086236
test_loss: 0.0016981006
train_loss: 0.0014181116
test_loss: 0.0016635064
train_loss: 0.0011872854
test_loss: 0.0016781465
train_loss: 0.0011739968
test_loss: 0.0015167557
train_loss: 0.00110838
test_loss: 0.0015876779
train_loss: 0.0010921385
test_loss: 0.0018150797
train_loss: 0.0015609018
test_loss: 0.001769014
train_loss: 0.0013216919
test_loss: 0.0018888867
train_loss: 0.0013302013
test_loss: 0.0017668476
train_loss: 0.0014120125
test_loss: 0.0017341841
train_loss: 0.0013892846
test_loss: 0.0017343721
train_loss: 0.0012262977
test_loss: 0.0020398253
train_loss: 0.0012130577
test_loss: 0.0017729969
train_loss: 0.0014885068
test_loss: 0.0017311176
train_loss: 0.0011676839
test_loss: 0.0017249371
train_loss: 0.0011834458
test_loss: 0.0016392389
train_loss: 0.001457353
test_loss: 0.001659273
train_loss: 0.001409006
test_loss: 0.0017708531
train_loss: 0.0013753164
test_loss: 0.0018233123
train_loss: 0.00165176
test_loss: 0.0017604701
train_loss: 0.0014345411
test_loss: 0.0017486577
train_loss: 0.0013176509
test_loss: 0.0016919639
train_loss: 0.0012502992
test_loss: 0.0017843965
train_loss: 0.0014230197
test_loss: 0.0017127364
train_loss: 0.0011640806
test_loss: 0.0016833463
train_loss: 0.0011498071
test_loss: 0.0017904198
train_loss: 0.0011065273
test_loss: 0.0019029153
train_loss: 0.0013565107
test_loss: 0.0019304876
train_loss: 0.0014566064
test_loss: 0.0018432665
train_loss: 0.0012635891
test_loss: 0.0018579559
train_loss: 0.0013771513
test_loss: 0.0017894468
train_loss: 0.0015234406
test_loss: 0.0018237702
train_loss: 0.0012837137
test_loss: 0.0020963198
train_loss: 0.0015520146
test_loss: 0.001806729
train_loss: 0.0018517671
test_loss: 0.0018770513
train_loss: 0.001564981
test_loss: 0.0019127493
train_loss: 0.0011897688
test_loss: 0.0019314904
train_loss: 0.001283732
test_loss: 0.0017475104
train_loss: 0.0012883081
test_loss: 0.0017550096
train_loss: 0.0013005415
test_loss: 0.0016557089
train_loss: 0.001329848
test_loss: 0.0015661428
train_loss: 0.0015174316
test_loss: 0.0020272871
train_loss: 0.0014333695
test_loss: 0.0017144908
train_loss: 0.0012196812
test_loss: 0.0017779174
train_loss: 0.0013208538
test_loss: 0.0016216597
train_loss: 0.0011941078
test_loss: 0.0017435977
train_loss: 0.0013928649
test_loss: 0.0016470128
train_loss: 0.0012478777
test_loss: 0.0016366694
train_loss: 0.0011431841
test_loss: 0.0018114591
train_loss: 0.0014622337
test_loss: 0.0017659883
train_loss: 0.001309291
test_loss: 0.0016573976
train_loss: 0.0013304213
test_loss: 0.0020119057
train_loss: 0.0012612145
test_loss: 0.0016367597
train_loss: 0.001162739
test_loss: 0.0018589674
train_loss: 0.0013964467
test_loss: 0.0018286182
train_loss: 0.0014720324
test_loss: 0.0017927196
train_loss: 0.001341357
test_loss: 0.0018529664
train_loss: 0.0013417588
test_loss: 0.0017901325
train_loss: 0.0012631204
test_loss: 0.0017421632
train_loss: 0.0010690511
test_loss: 0.0016619484
train_loss: 0.0011370725
test_loss: 0.0016243965
train_loss: 0.0012285938
test_loss: 0.0016380029
train_loss: 0.0013245987
test_loss: 0.0017982819
train_loss: 0.0013205383
test_loss: 0.002206216
train_loss: 0.001329117
test_loss: 0.0021779884
train_loss: 0.0015166232
test_loss: 0.0018827048
train_loss: 0.0020021512
test_loss: 0.0019203248
train_loss: 0.0021049646
test_loss: 0.00201486
train_loss: 0.0022130264
test_loss: 0.0023209038
train_loss: 0.0018202248
test_loss: 0.002129843
train_loss: 0.0019128686
test_loss: 0.0020688404
train_loss: 0.002126514
test_loss: 0.0021301198
train_loss: 0.0019076213
test_loss: 0.002251166
train_loss: 0.0018537107
test_loss: 0.0019595623
train_loss: 0.0021552476
test_loss: 0.0022120504
train_loss: 0.0017778906
test_loss: 0.0020456435
train_loss: 0.00203325
test_loss: 0.0019955423
train_loss: 0.0019292249
test_loss: 0.0022347667
train_loss: 0.0017219894
test_loss: 0.001972627
train_loss: 0.001973969
test_loss: 0.0018931344
train_loss: 0.0018282596
test_loss: 0.0022556111
train_loss: 0.0017647264
test_loss: 0.0019781194
train_loss: 0.0020434805
test_loss: 0.0019322941
train_loss: 0.0019723077
test_loss: 0.0022562905
train_loss: 0.0017700203
test_loss: 0.002003822
train_loss: 0.001730959
test_loss: 0.0019139522
train_loss: 0.0019384418
test_loss: 0.001944782
train_loss: 0.0017877251
test_loss: 0.002134419
train_loss: 0.0016849414
test_loss: 0.0018990644
train_loss: 0.0016840687
test_loss: 0.0019055917
train_loss: 0.0016777024
test_loss: 0.0019519096
train_loss: 0.0014787883
test_loss: 0.0018336468
train_loss: 0.0014049449
test_loss: 0.0016155926
train_loss: 0.0011711752
test_loss: 0.0016763596
train_loss: 0.0012068247
test_loss: 0.0016690869
train_loss: 0.0011148409
test_loss: 0.0016686755
train_loss: 0.0010936874
test_loss: 0.0018259934
train_loss: 0.0011408027
test_loss: 0.0018272435
train_loss: 0.0013467501
test_loss: 0.0016967843
train_loss: 0.0012726283
test_loss: 0.001686892
train_loss: 0.0012565
test_loss: 0.0018110564
train_loss: 0.0015590929
test_loss: 0.0016691923
train_loss: 0.0011910993
test_loss: 0.001906773
train_loss: 0.0013292939
test_loss: 0.0016711676
train_loss: 0.001272836
test_loss: 0.0017548495
train_loss: 0.0013438486
test_loss: 0.0018330852
train_loss: 0.0014168392
test_loss: 0.0017986713
train_loss: 0.001356923
test_loss: 0.0019305912
train_loss: 0.0015889372
test_loss: 0.0016975652
train_loss: 0.001606206
test_loss: 0.0018494977
train_loss: 0.0014069695
test_loss: 0.0019434531
train_loss: 0.001217904
test_loss: 0.0018323654
train_loss: 0.0014831044
test_loss: 0.0016419814
train_loss: 0.0013988533
test_loss: 0.0018265141
train_loss: 0.00165669
test_loss: 0.0016582389
train_loss: 0.0013766033
test_loss: 0.0017427361
train_loss: 0.0016188393
test_loss: 0.0017932816
train_loss: 0.0014601815
test_loss: 0.0020043273
train_loss: 0.0013242456
test_loss: 0.0017888679
train_loss: 0.0014905502
test_loss: 0.0017686642
train_loss: 0.0012421162
test_loss: 0.0017346722
train_loss: 0.0011971511
test_loss: 0.0018078623
train_loss: 0.0013001212
test_loss: 0.001738409
train_loss: 0.0011810798
test_loss: 0.0017381869
train_loss: 0.0012891432
test_loss: 0.0016136388
train_loss: 0.0012064031
test_loss: 0.0017609672
train_loss: 0.0011661324
test_loss: 0.0017712363
train_loss: 0.0012442318
test_loss: 0.0016622168
train_loss: 0.0011201508
test_loss: 0.001851574
train_loss: 0.0014147926
test_loss: 0.0016795718
train_loss: 0.0013354883
test_loss: 0.001804617
train_loss: 0.001208934
test_loss: 0.0018297094
train_loss: 0.0013093193
test_loss: 0.001585808
train_loss: 0.0011968636
test_loss: 0.001585537
train_loss: 0.0013603943
test_loss: 0.0017678166
train_loss: 0.0011288144
test_loss: 0.0018320347
train_loss: 0.0011158478
test_loss: 0.0017471944
train_loss: 0.0012303784
test_loss: 0.0016354314
train_loss: 0.001134451
test_loss: 0.0015765049
train_loss: 0.0012462935
test_loss: 0.0016980242
train_loss: 0.0012704562
test_loss: 0.001647643
train_loss: 0.0010977564
test_loss: 0.0015590843
train_loss: 0.0011295866
test_loss: 0.0016076278
train_loss: 0.0011258663
test_loss: 0.0016306428
train_loss: 0.0011550069
test_loss: 0.0016814107
train_loss: 0.0011804701
test_loss: 0.0019697673
train_loss: 0.0011523407
test_loss: 0.0019778123
train_loss: 0.0014694235
test_loss: 0.0017527165
train_loss: 0.0014463789
test_loss: 0.001716149
train_loss: 0.0012084253
test_loss: 0.0017910383
train_loss: 0.0013833155
test_loss: 0.0016604166
train_loss: 0.0011282094
test_loss: 0.0017936942
train_loss: 0.0011372153
test_loss: 0.00178959
train_loss: 0.0012572362
test_loss: 0.0017124575
train_loss: 0.0012197881
test_loss: 0.0018878849
train_loss: 0.0015477639
test_loss: 0.0018286933
train_loss: 0.001517623
test_loss: 0.001692584
train_loss: 0.0010668946
test_loss: 0.001743166
train_loss: 0.0012007728
test_loss: 0.0016407351
train_loss: 0.0012643023
test_loss: 0.0017231368
train_loss: 0.0014556216
test_loss: 0.0016796569
train_loss: 0.0012522009
test_loss: 0.0017522004
train_loss: 0.0011498856
test_loss: 0.0016843467
train_loss: 0.0011767902
test_loss: 0.0018263183
train_loss: 0.0012744708
test_loss: 0.0017604026
train_loss: 0.001282143
test_loss: 0.0016996713
train_loss: 0.0012622616
test_loss: 0.0016899076
train_loss: 0.001134555
test_loss: 0.0015652914
train_loss: 0.0010800983
test_loss: 0.0017219279
train_loss: 0.0011169289
test_loss: 0.0016355632
train_loss: 0.0013090409
test_loss: 0.0017178212
train_loss: 0.0013690136
test_loss: 0.0016956595
train_loss: 0.0011490309
test_loss: 0.0016213617
train_loss: 0.0011563374
test_loss: 0.0016911827
train_loss: 0.0012148104
test_loss: 0.0015787202
train_loss: 0.001133563
test_loss: 0.0016506109
train_loss: 0.0012551581
test_loss: 0.001739619
train_loss: 0.0013634575
test_loss: 0.0016110656
train_loss: 0.0014139038
test_loss: 0.001767151
train_loss: 0.0011605535
test_loss: 0.0018353399
train_loss: 0.0016172507
test_loss: 0.0016571551
train_loss: 0.0011632281
test_loss: 0.0019146964
train_loss: 0.0011806516
test_loss: 0.0018988458
train_loss: 0.0014877891
test_loss: 0.0017652079
train_loss: 0.0012272947
test_loss: 0.0017283425
train_loss: 0.0011844245
test_loss: 0.0017778458
train_loss: 0.0010241857
test_loss: 0.002029707
train_loss: 0.001365957
test_loss: 0.0018435103
train_loss: 0.0014453126
test_loss: 0.001710487
train_loss: 0.00108943
test_loss: 0.0017256435
train_loss: 0.0012070829
test_loss: 0.0017509464
train_loss: 0.001302009
test_loss: 0.0016703645
train_loss: 0.0013036728
test_loss: 0.0017991206
train_loss: 0.0013447738
test_loss: 0.0017102493
train_loss: 0.0012861246
test_loss: 0.0017064341
train_loss: 0.0013586596
test_loss: 0.0017849894
train_loss: 0.0012077419
test_loss: 0.0018271542
train_loss: 0.0012749373
test_loss: 0.0019819317
train_loss: 0.0016818752
test_loss: 0.0017052252
train_loss: 0.0014145124
test_loss: 0.0018806721
train_loss: 0.0013203719
test_loss: 0.0015990327
train_loss: 0.0013001362
test_loss: 0.0018825913
train_loss: 0.0013344145
test_loss: 0.0018646352
train_loss: 0.0012523289
test_loss: 0.001888757
train_loss: 0.0014805299
test_loss: 0.0017213393
train_loss: 0.0016778902
test_loss: 0.0017431275
train_loss: 0.0011546982
test_loss: 0.0020231449
train_loss: 0.0014243723
test_loss: 0.0020360965
train_loss: 0.0014963674
test_loss: 0.0017268365
train_loss: 0.0012223714
test_loss: 0.0017857157
train_loss: 0.0013212265
test_loss: 0.0018029655
train_loss: 0.0012961373
test_loss: 0.0019797538
train_loss: 0.0014910398
test_loss: 0.0017413319
train_loss: 0.0014954185
test_loss: 0.0015863448
train_loss: 0.001398064
test_loss: 0.0017249214
train_loss: 0.0013301298
test_loss: 0.0017448246
train_loss: 0.0015159474
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.001625166
train_loss: 0.0012610131
test_loss: 0.0015896892
train_loss: 0.0013667664
test_loss: 0.0018401156
train_loss: 0.0012057163
test_loss: 0.001798493
train_loss: 0.0012521205
test_loss: 0.0017573251
train_loss: 0.0013277293
test_loss: 0.0017144798
train_loss: 0.0012284621
test_loss: 0.0018402707
train_loss: 0.0013146959
test_loss: 0.0016018341
train_loss: 0.0010571096
test_loss: 0.0019583227
train_loss: 0.0012765296
test_loss: 0.0018002063
train_loss: 0.0010950193
test_loss: 0.0017623336
train_loss: 0.0011147484
test_loss: 0.0017598018
train_loss: 0.0011575237
test_loss: 0.0016961612
train_loss: 0.0013044538
test_loss: 0.0016816367
train_loss: 0.001224713
test_loss: 0.0016763436
train_loss: 0.0013837137
test_loss: 0.0017346976
train_loss: 0.0013327452
test_loss: 0.0017665733
train_loss: 0.001180112
test_loss: 0.0017355438
train_loss: 0.0014382408
test_loss: 0.0018362675
train_loss: 0.0013450296
test_loss: 0.0020721701
train_loss: 0.0015140529
test_loss: 0.001955782
train_loss: 0.001647255
test_loss: 0.0017380747
train_loss: 0.0015856886
test_loss: 0.0018126765
train_loss: 0.0014899563
test_loss: 0.002021297
train_loss: 0.0013193807
test_loss: 0.001958991
train_loss: 0.0013027976
test_loss: 0.001729984
train_loss: 0.0012656049
test_loss: 0.0016074104
train_loss: 0.0011410739
test_loss: 0.0018596262
train_loss: 0.0013658328
test_loss: 0.001794328
train_loss: 0.0015960673
test_loss: 0.0017671392
train_loss: 0.0017634192
test_loss: 0.0019120054
train_loss: 0.0015923412
test_loss: 0.0017839952
train_loss: 0.0014672363
test_loss: 0.0017739247
train_loss: 0.0016205652
test_loss: 0.0018114131
train_loss: 0.0013552395
test_loss: 0.0017355774
train_loss: 0.0012151235
test_loss: 0.0018491745
train_loss: 0.0012690473
test_loss: 0.0020588276
train_loss: 0.0015708057
test_loss: 0.0018134672
train_loss: 0.0011319645
test_loss: 0.0020299593
train_loss: 0.0014844829
test_loss: 0.0018745187
train_loss: 0.0015281092
test_loss: 0.0017547965
train_loss: 0.0011643659
test_loss: 0.0018537185
train_loss: 0.0012431564
test_loss: 0.0017395517
train_loss: 0.001028178
test_loss: 0.0017296274
train_loss: 0.0011917943
test_loss: 0.001935081
train_loss: 0.0015161225
test_loss: 0.0017919424
train_loss: 0.0011387296
test_loss: 0.001714572
train_loss: 0.0012877292
test_loss: 0.00184425
train_loss: 0.0013954655
test_loss: 0.0017363171
train_loss: 0.0013180989
test_loss: 0.0017980823
train_loss: 0.0012443062
test_loss: 0.0019542256
train_loss: 0.0014901552
test_loss: 0.0017090893
train_loss: 0.0012435863
test_loss: 0.001927689
train_loss: 0.0012641001
test_loss: 0.00188346
train_loss: 0.0014581629
test_loss: 0.0016305121
train_loss: 0.0012915294
test_loss: 0.001672784
train_loss: 0.0010910691
test_loss: 0.0016044824
train_loss: 0.0012545141
test_loss: 0.0016854296
train_loss: 0.0010675214
test_loss: 0.0017434994
train_loss: 0.001256623
test_loss: 0.0018192766
train_loss: 0.0013511146
test_loss: 0.0017225987
train_loss: 0.0015726185
test_loss: 0.0017238205
train_loss: 0.0013434547
test_loss: 0.0018529493
train_loss: 0.0013371046
test_loss: 0.0017445944
train_loss: 0.0015281177
test_loss: 0.0016587104
train_loss: 0.0016446955
test_loss: 0.0016823242
train_loss: 0.0016025507
test_loss: 0.0019897989
train_loss: 0.0012986673
test_loss: 0.0019269906
train_loss: 0.0013509768
test_loss: 0.0020846934
train_loss: 0.0015263679
test_loss: 0.0019514248
train_loss: 0.0014568554
test_loss: 0.0017765986
train_loss: 0.0015000855
test_loss: 0.0016906288
train_loss: 0.0013695537
test_loss: 0.0017250787
train_loss: 0.0014445888
test_loss: 0.0018866202
train_loss: 0.0015119091
test_loss: 0.0019585716
train_loss: 0.0012912845
test_loss: 0.0018595272
train_loss: 0.0012548133
test_loss: 0.0017019943
train_loss: 0.0015594143
test_loss: 0.0019252584
train_loss: 0.001564264
test_loss: 0.0017285451
train_loss: 0.0012381254
test_loss: 0.0017581257
train_loss: 0.0012474435
test_loss: 0.0016783238
train_loss: 0.0011061726
test_loss: 0.0016969187
train_loss: 0.0010357079
test_loss: 0.0015681761
train_loss: 0.0009838211
test_loss: 0.0016068044
train_loss: 0.0012893935
test_loss: 0.0016331772
train_loss: 0.001303982
test_loss: 0.0016239008
train_loss: 0.0011033255
test_loss: 0.001583534
train_loss: 0.0012356641
test_loss: 0.0018175222
train_loss: 0.001380512
test_loss: 0.0017271212
train_loss: 0.0013337863
test_loss: 0.0016867314
train_loss: 0.0012131286
test_loss: 0.0019593376
train_loss: 0.001225394
test_loss: 0.0018737705
train_loss: 0.0016134506
test_loss: 0.001674298
train_loss: 0.0014819293
test_loss: 0.0018182892
train_loss: 0.0012525719
test_loss: 0.0016953261
train_loss: 0.0013710258
test_loss: 0.0015880524
train_loss: 0.0011917339
test_loss: 0.0017481536
train_loss: 0.0016068158
test_loss: 0.0017280467
train_loss: 0.0016032443
test_loss: 0.0018791135
train_loss: 0.0014491822
test_loss: 0.0017912525
train_loss: 0.0018236098
test_loss: 0.0018191235
train_loss: 0.0015317708
test_loss: 0.0018630442
train_loss: 0.0016449904
test_loss: 0.0018961547
train_loss: 0.00182879
test_loss: 0.0020588564
train_loss: 0.0014186315
test_loss: 0.001990157
train_loss: 0.0014456159
test_loss: 0.0018230941
train_loss: 0.0013589757
test_loss: 0.0016311475
train_loss: 0.0013936423
test_loss: 0.0016398731
train_loss: 0.0013395522
test_loss: 0.0018749073
train_loss: 0.0013518198
test_loss: 0.0017207117
train_loss: 0.001259396
test_loss: 0.0018882506
train_loss: 0.0013963645
test_loss: 0.0016744861
train_loss: 0.0013204637
test_loss: 0.0018011702
train_loss: 0.0011613232
test_loss: 0.0018970115
train_loss: 0.001329384
test_loss: 0.00188788
train_loss: 0.0012344837
test_loss: 0.0017741125
train_loss: 0.0012877085
test_loss: 0.0017957079
train_loss: 0.0012169629
test_loss: 0.0019014875
train_loss: 0.0013162335
test_loss: 0.0018497307
train_loss: 0.0015361228
test_loss: 0.0016319583
train_loss: 0.0014130007
test_loss: 0.0018513316
train_loss: 0.0013959935
test_loss: 0.0018002463
train_loss: 0.0013591816
test_loss: 0.00173377
train_loss: 0.0014159172
test_loss: 0.0017968215
train_loss: 0.0015238712
test_loss: 0.0017866625
train_loss: 0.00128861
test_loss: 0.0017641474
train_loss: 0.0012103105
test_loss: 0.0021892104
train_loss: 0.0014827347
test_loss: 0.0017656566
train_loss: 0.0019299551
test_loss: 0.0019338622
train_loss: 0.0015529259
test_loss: 0.0020689927
train_loss: 0.0012302815
test_loss: 0.0020510803
train_loss: 0.001953421
test_loss: 0.0018880024
train_loss: 0.0020704474
test_loss: 0.0020059976
train_loss: 0.002122582
test_loss: 0.0020181937
train_loss: 0.0020844683
test_loss: 0.0024437227
train_loss: 0.002026258
test_loss: 0.0022279553
train_loss: 0.001786556
test_loss: 0.0023041603
train_loss: 0.0018210574
test_loss: 0.0021313247
train_loss: 0.0015297348
test_loss: 0.0023406018
train_loss: 0.0018041912
test_loss: 0.0023573216
train_loss: 0.0016556149
test_loss: 0.0022139659
train_loss: 0.0015626152
test_loss: 0.0019799706
train_loss: 0.0017083208
test_loss: 0.0017457678
train_loss: 0.0018228337
test_loss: 0.0021660104
train_loss: 0.0016523122
test_loss: 0.0019441727
train_loss: 0.0015789744
test_loss: 0.00198543
train_loss: 0.0013514016
test_loss: 0.001895535
train_loss: 0.0013382755
test_loss: 0.0018076405
train_loss: 0.0014182285
test_loss: 0.001687211
train_loss: 0.0013078819
test_loss: 0.0017725696
train_loss: 0.00125845
test_loss: 0.0016118991
train_loss: 0.0011919349
test_loss: 0.0015613514
train_loss: 0.0013526445
test_loss: 0.0016995539
train_loss: 0.001158645
test_loss: 0.0017776932
train_loss: 0.0011311707
test_loss: 0.0016400287
train_loss: 0.0010530483
test_loss: 0.0016700447
train_loss: 0.0010931479
test_loss: 0.0017397198
train_loss: 0.0012520805
test_loss: 0.0016960233
train_loss: 0.0012035294
test_loss: 0.0015774735
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi0.8/300_300_300_1 --function f1 --psi 0 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi0_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 4000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de4219400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de42389d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de41bb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de4246a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de418bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de418b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de4156840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de4106048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de41a02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de41562f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de41a0ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de4079d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de4090950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de405b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de4090e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de40cf840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5de4156158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd07ef6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd07ef9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd0741400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd0728a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd06c30d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd07289d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd06bb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd06bbd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd06bbe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd06bb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd0618950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd0618f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd05cc7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd05fa268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd0555f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd0555730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd059ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd05000d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5dd0536f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
