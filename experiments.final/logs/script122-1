+ RUN=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ LAYERS=300_100_100_100_1
+ case $RUN in
+ PSI='-2 -1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 800 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output120
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output122
+ for fn in f1
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.final/output120/f1_psi0_phi0
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0
+ date
Sun Nov  8 13:46:06 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b122e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b13d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b1227b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b170ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b0f9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b0d59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b0396a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b039d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b0d5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b013d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1af8e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1afbf730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1afbfd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b0392f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1aec3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1aec3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1aefe620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1aefeae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1aec38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04436598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04436268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04413c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04468a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e044728c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04472840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e044bff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04342a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e043157b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04315510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e042e7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e042e4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e0436bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e0436bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e043c08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e041fb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e041fef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 1.9979768
test_loss: 2.0077884
train_loss: 1.9997278
test_loss: 2.0016716
train_loss: 1.9991816
test_loss: 2.0191407
train_loss: 0.38922876
test_loss: 0.38641176
train_loss: 0.37989712
test_loss: 0.38479304
train_loss: 0.38366455
test_loss: 0.38292664
train_loss: 0.3837428
test_loss: 0.380872
train_loss: 0.38521048
test_loss: 0.37858742
train_loss: 0.37760538
test_loss: 0.3761154
train_loss: 0.37242517
test_loss: 0.3734083
train_loss: 0.37131292
test_loss: 0.37057772
train_loss: 0.37158352
test_loss: 0.36752892
train_loss: 0.3629381
test_loss: 0.36423773
train_loss: 0.35437715
test_loss: 0.36069843
train_loss: 0.35828757
test_loss: 0.35697427
train_loss: 0.35007122
test_loss: 0.3529337
train_loss: 0.34369022
test_loss: 0.3487389
train_loss: 0.34249565
test_loss: 0.34428036
train_loss: 0.34080863
test_loss: 0.3394831
train_loss: 0.33409518
test_loss: 0.33444342
train_loss: 0.3263409
test_loss: 0.32903516
train_loss: 0.32187474
test_loss: 0.32336202
train_loss: 0.30718747
test_loss: 0.3173635
train_loss: 0.31490296
test_loss: 0.31110486
train_loss: 0.30404532
test_loss: 0.3046582
train_loss: 0.3035621
test_loss: 0.2981668
train_loss: 0.29531407
test_loss: 0.2917109
train_loss: 0.28227922
test_loss: 0.2854349
train_loss: 0.28115022
test_loss: 0.27912685
train_loss: 0.26904166
test_loss: 0.2727214
train_loss: 0.26890945
test_loss: 0.2661039
train_loss: 0.25565183
test_loss: 0.25922808
train_loss: 0.2540871
test_loss: 0.25208718
train_loss: 0.24411157
test_loss: 0.24464472
train_loss: 0.23604621
test_loss: 0.23686376
train_loss: 0.23003727
test_loss: 0.22877668
train_loss: 0.22188494
test_loss: 0.2203403
train_loss: 0.20615037
test_loss: 0.2115819
train_loss: 0.20372625
test_loss: 0.20245172
train_loss: 0.19295165
test_loss: 0.19294159
train_loss: 0.18424915
test_loss: 0.18307099
train_loss: 0.1698882
test_loss: 0.17287114
train_loss: 0.16284549
test_loss: 0.16231553
train_loss: 0.14915365
test_loss: 0.15164547
train_loss: 0.13659553
test_loss: 0.14121309
train_loss: 0.12845616
test_loss: 0.13128103
train_loss: 0.120695576
test_loss: 0.12192823
train_loss: 0.11338334
test_loss: 0.11321061
train_loss: 0.10540347
test_loss: 0.10522411
train_loss: 0.09683629
test_loss: 0.097996816
train_loss: 0.088374116
test_loss: 0.091564186
train_loss: 0.08450627
test_loss: 0.08608251
train_loss: 0.08412985
test_loss: 0.08159975
train_loss: 0.078665525
test_loss: 0.07815726
train_loss: 0.075947925
test_loss: 0.07559704
train_loss: 0.07375954
test_loss: 0.07379009
train_loss: 0.07346541
test_loss: 0.07255107
train_loss: 0.07115169
test_loss: 0.07168498
train_loss: 0.07179248
test_loss: 0.07109428
train_loss: 0.072028436
test_loss: 0.07069668
train_loss: 0.069473565
test_loss: 0.070375815
train_loss: 0.07008551
test_loss: 0.070136786
train_loss: 0.068046376
test_loss: 0.069946304
train_loss: 0.07000386
test_loss: 0.069782384
train_loss: 0.06768571
test_loss: 0.069634795
train_loss: 0.06953274
test_loss: 0.06950281
train_loss: 0.0699946
test_loss: 0.06937802
train_loss: 0.06838023
test_loss: 0.069250025
train_loss: 0.06768218
test_loss: 0.069127314
train_loss: 0.06945533
test_loss: 0.06902274
train_loss: 0.07232504
test_loss: 0.06891142
train_loss: 0.06902215
test_loss: 0.06878825
train_loss: 0.06754342
test_loss: 0.068663254
train_loss: 0.06909302
test_loss: 0.068546936
train_loss: 0.06887928
test_loss: 0.06839253
train_loss: 0.06943627
test_loss: 0.06826499
train_loss: 0.06763018
test_loss: 0.06810359
train_loss: 0.06632832
test_loss: 0.0679466
train_loss: 0.06726606
test_loss: 0.06776292
train_loss: 0.06656764
test_loss: 0.067549184
train_loss: 0.068852276
test_loss: 0.067322835
train_loss: 0.06825124
test_loss: 0.06707538
train_loss: 0.06664454
test_loss: 0.06680478
train_loss: 0.06552625
test_loss: 0.06653273
train_loss: 0.06587339
test_loss: 0.06622635
train_loss: 0.06640463
test_loss: 0.06591646
train_loss: 0.06597269
test_loss: 0.06555131
train_loss: 0.06567447
test_loss: 0.06525414
train_loss: 0.06486565
test_loss: 0.064905114
train_loss: 0.06482702
test_loss: 0.06455556
train_loss: 0.06527038
test_loss: 0.06424411
train_loss: 0.06389486
test_loss: 0.063876875
train_loss: 0.06454322
test_loss: 0.06351072
train_loss: 0.06331902
test_loss: 0.06312846
train_loss: 0.062444896
test_loss: 0.06273356
train_loss: 0.061312966
test_loss: 0.062267624
train_loss: 0.06211281
test_loss: 0.061816312
train_loss: 0.05962605
test_loss: 0.061316658
train_loss: 0.061433308
test_loss: 0.060674876
train_loss: 0.05963067
test_loss: 0.060031615
train_loss: 0.058408998
test_loss: 0.059331916
train_loss: 0.05838377
test_loss: 0.05861443
train_loss: 0.057346858
test_loss: 0.05791582
train_loss: 0.0570759
test_loss: 0.05708584
train_loss: 0.05670557
test_loss: 0.056358147
train_loss: 0.055448525
test_loss: 0.055662554
train_loss: 0.055421043
test_loss: 0.05484494
train_loss: 0.05361884
test_loss: 0.054040466
train_loss: 0.053135052
test_loss: 0.05302862
train_loss: 0.05258199
test_loss: 0.052132428
train_loss: 0.050805673
test_loss: 0.051024184
train_loss: 0.050777614
test_loss: 0.04964551
train_loss: 0.049151443
test_loss: 0.04792135
train_loss: 0.04597447
test_loss: 0.04539187
train_loss: 0.041651122
test_loss: 0.042245924
train_loss: 0.040918533
test_loss: 0.039379656
train_loss: 0.0377145
test_loss: 0.037551533
train_loss: 0.035373643
test_loss: 0.036387544
train_loss: 0.03570865
test_loss: 0.035556078
train_loss: 0.035763845
test_loss: 0.0351549
train_loss: 0.033421196
test_loss: 0.034653235
train_loss: 0.033715226
test_loss: 0.034625355
train_loss: 0.03420014
test_loss: 0.034340266
train_loss: 0.034193777
test_loss: 0.034322366
train_loss: 0.03362307
test_loss: 0.034146182
train_loss: 0.034439743
test_loss: 0.034163672
train_loss: 0.03390279
test_loss: 0.034150757
train_loss: 0.034698788
test_loss: 0.033956755
train_loss: 0.034842208
test_loss: 0.034100365
train_loss: 0.034345947
test_loss: 0.034061547
train_loss: 0.03382743
test_loss: 0.03392689
train_loss: 0.032622978
test_loss: 0.033975232
train_loss: 0.03462892
test_loss: 0.033831056
train_loss: 0.034095157
test_loss: 0.03378155
train_loss: 0.0344683
test_loss: 0.033801075
train_loss: 0.034354262
test_loss: 0.033953443
train_loss: 0.032922473
test_loss: 0.033895597
train_loss: 0.0324737
test_loss: 0.033632066
train_loss: 0.034364913
test_loss: 0.033586994
train_loss: 0.03336878
test_loss: 0.033587307
train_loss: 0.032911055
test_loss: 0.03346479
train_loss: 0.034232643
test_loss: 0.033461854
train_loss: 0.032819334
test_loss: 0.033423606
train_loss: 0.03233289
test_loss: 0.03315889
train_loss: 0.032172363
test_loss: 0.03320313
train_loss: 0.03180504
test_loss: 0.03303794
train_loss: 0.031827793
test_loss: 0.033048898
train_loss: 0.03163982
test_loss: 0.032762714
train_loss: 0.03222751
test_loss: 0.032445636
train_loss: 0.03245959
test_loss: 0.032551616
train_loss: 0.031543683
test_loss: 0.032154176
train_loss: 0.032448094
test_loss: 0.031917904
train_loss: 0.031198796
test_loss: 0.031644437
train_loss: 0.030547794
test_loss: 0.031456668
train_loss: 0.030444156
test_loss: 0.03122278
train_loss: 0.030002668
test_loss: 0.031083131
train_loss: 0.031149868
test_loss: 0.031051124
train_loss: 0.031144524
test_loss: 0.031226642
train_loss: 0.029536571
test_loss: 0.030888177
train_loss: 0.029889945
test_loss: 0.030840486
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3fdc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3fdcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3ff4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca4027c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3f521e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3f52bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3f62730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3edbc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3e8f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3e8ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3e8f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3e246a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3e247b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3e24bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3db4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3d5bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3d5f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3d5fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3d2dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3d2dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3cf41e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a8387b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a7e26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a803f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a7a3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a7c0ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a7c0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a7c0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a710488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a710730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a710a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc447fcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc447fc6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc447fc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc4470c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc44736950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.00184248399
Iter: 2 loss: 0.00226595439
Iter: 3 loss: 0.00181859126
Iter: 4 loss: 0.00179291691
Iter: 5 loss: 0.00187986623
Iter: 6 loss: 0.00178627728
Iter: 7 loss: 0.00176627259
Iter: 8 loss: 0.00174691214
Iter: 9 loss: 0.00174244936
Iter: 10 loss: 0.0017287056
Iter: 11 loss: 0.00182097242
Iter: 12 loss: 0.00172728603
Iter: 13 loss: 0.00171672564
Iter: 14 loss: 0.00171608524
Iter: 15 loss: 0.00170804828
Iter: 16 loss: 0.0016991524
Iter: 17 loss: 0.00178381498
Iter: 18 loss: 0.00169875531
Iter: 19 loss: 0.00169349252
Iter: 20 loss: 0.0016864707
Iter: 21 loss: 0.00168606662
Iter: 22 loss: 0.0016776647
Iter: 23 loss: 0.00170009548
Iter: 24 loss: 0.00167473755
Iter: 25 loss: 0.0016681815
Iter: 26 loss: 0.00171748258
Iter: 27 loss: 0.00166764518
Iter: 28 loss: 0.00166160928
Iter: 29 loss: 0.00168169918
Iter: 30 loss: 0.00165991299
Iter: 31 loss: 0.00165510317
Iter: 32 loss: 0.00165707443
Iter: 33 loss: 0.00165175018
Iter: 34 loss: 0.00164802303
Iter: 35 loss: 0.00169116352
Iter: 36 loss: 0.00164798531
Iter: 37 loss: 0.00164402323
Iter: 38 loss: 0.00164768845
Iter: 39 loss: 0.00164170773
Iter: 40 loss: 0.00163873634
Iter: 41 loss: 0.00164445047
Iter: 42 loss: 0.00163748604
Iter: 43 loss: 0.00163351675
Iter: 44 loss: 0.00163366145
Iter: 45 loss: 0.00163037598
Iter: 46 loss: 0.00162698538
Iter: 47 loss: 0.0016269628
Iter: 48 loss: 0.00162451132
Iter: 49 loss: 0.00162081642
Iter: 50 loss: 0.0016207411
Iter: 51 loss: 0.00161712605
Iter: 52 loss: 0.00164396677
Iter: 53 loss: 0.00161683222
Iter: 54 loss: 0.00161300006
Iter: 55 loss: 0.00161744864
Iter: 56 loss: 0.00161093846
Iter: 57 loss: 0.00160754332
Iter: 58 loss: 0.00160492701
Iter: 59 loss: 0.00160383619
Iter: 60 loss: 0.00159925176
Iter: 61 loss: 0.00161090284
Iter: 62 loss: 0.00159763626
Iter: 63 loss: 0.00159214903
Iter: 64 loss: 0.00160935789
Iter: 65 loss: 0.00159051979
Iter: 66 loss: 0.00158879044
Iter: 67 loss: 0.00158822746
Iter: 68 loss: 0.00158623583
Iter: 69 loss: 0.00159539143
Iter: 70 loss: 0.00158584351
Iter: 71 loss: 0.00158422138
Iter: 72 loss: 0.00157965557
Iter: 73 loss: 0.0016039951
Iter: 74 loss: 0.00157820201
Iter: 75 loss: 0.00157261849
Iter: 76 loss: 0.00159931439
Iter: 77 loss: 0.00157159695
Iter: 78 loss: 0.00156710902
Iter: 79 loss: 0.00160713075
Iter: 80 loss: 0.00156687445
Iter: 81 loss: 0.00156372343
Iter: 82 loss: 0.00159229932
Iter: 83 loss: 0.00156357489
Iter: 84 loss: 0.00156059535
Iter: 85 loss: 0.00156079303
Iter: 86 loss: 0.00155825843
Iter: 87 loss: 0.00155474013
Iter: 88 loss: 0.00156634394
Iter: 89 loss: 0.00155375572
Iter: 90 loss: 0.00155004929
Iter: 91 loss: 0.00155215175
Iter: 92 loss: 0.00154761318
Iter: 93 loss: 0.00154444878
Iter: 94 loss: 0.00154438755
Iter: 95 loss: 0.00154072628
Iter: 96 loss: 0.00153780589
Iter: 97 loss: 0.00153667061
Iter: 98 loss: 0.001533804
Iter: 99 loss: 0.00153454184
Iter: 100 loss: 0.00153175369
Iter: 101 loss: 0.00152799953
Iter: 102 loss: 0.00154167716
Iter: 103 loss: 0.00152701931
Iter: 104 loss: 0.00152389263
Iter: 105 loss: 0.00152373733
Iter: 106 loss: 0.00152133254
Iter: 107 loss: 0.00151580863
Iter: 108 loss: 0.00152527716
Iter: 109 loss: 0.00151331699
Iter: 110 loss: 0.00150713499
Iter: 111 loss: 0.00152800977
Iter: 112 loss: 0.0015054089
Iter: 113 loss: 0.0015003
Iter: 114 loss: 0.0015257987
Iter: 115 loss: 0.00149941607
Iter: 116 loss: 0.00149483106
Iter: 117 loss: 0.00154654426
Iter: 118 loss: 0.00149473478
Iter: 119 loss: 0.00149169052
Iter: 120 loss: 0.00149408518
Iter: 121 loss: 0.00148982671
Iter: 122 loss: 0.0014859417
Iter: 123 loss: 0.00149707415
Iter: 124 loss: 0.00148469547
Iter: 125 loss: 0.00148013025
Iter: 126 loss: 0.00148310442
Iter: 127 loss: 0.00147722545
Iter: 128 loss: 0.00147302414
Iter: 129 loss: 0.00147300051
Iter: 130 loss: 0.00146879628
Iter: 131 loss: 0.00148674438
Iter: 132 loss: 0.00146789127
Iter: 133 loss: 0.00146394945
Iter: 134 loss: 0.00147829892
Iter: 135 loss: 0.00146298832
Iter: 136 loss: 0.00145851914
Iter: 137 loss: 0.00146154757
Iter: 138 loss: 0.00145566312
Iter: 139 loss: 0.00145033118
Iter: 140 loss: 0.00148410094
Iter: 141 loss: 0.00144969928
Iter: 142 loss: 0.00144471601
Iter: 143 loss: 0.00145707151
Iter: 144 loss: 0.00144295359
Iter: 145 loss: 0.00143766496
Iter: 146 loss: 0.00145382853
Iter: 147 loss: 0.00143600651
Iter: 148 loss: 0.0014321385
Iter: 149 loss: 0.00147940055
Iter: 150 loss: 0.00143208937
Iter: 151 loss: 0.0014281522
Iter: 152 loss: 0.00143687928
Iter: 153 loss: 0.00142662413
Iter: 154 loss: 0.00142323226
Iter: 155 loss: 0.00142985466
Iter: 156 loss: 0.00142181804
Iter: 157 loss: 0.0014176988
Iter: 158 loss: 0.00142726803
Iter: 159 loss: 0.00141614722
Iter: 160 loss: 0.00141252112
Iter: 161 loss: 0.00141684012
Iter: 162 loss: 0.00141060539
Iter: 163 loss: 0.00140833203
Iter: 164 loss: 0.00140787638
Iter: 165 loss: 0.00140543096
Iter: 166 loss: 0.00141106267
Iter: 167 loss: 0.00140448636
Iter: 168 loss: 0.00140098843
Iter: 169 loss: 0.00140594156
Iter: 170 loss: 0.00139926607
Iter: 171 loss: 0.00139503926
Iter: 172 loss: 0.00139695092
Iter: 173 loss: 0.00139212888
Iter: 174 loss: 0.00138680404
Iter: 175 loss: 0.00139625033
Iter: 176 loss: 0.00138441869
Iter: 177 loss: 0.00137893506
Iter: 178 loss: 0.00140959816
Iter: 179 loss: 0.00137815217
Iter: 180 loss: 0.00137319649
Iter: 181 loss: 0.00140608707
Iter: 182 loss: 0.00137268251
Iter: 183 loss: 0.00136831356
Iter: 184 loss: 0.00137267495
Iter: 185 loss: 0.00136581343
Iter: 186 loss: 0.00136184134
Iter: 187 loss: 0.00141377957
Iter: 188 loss: 0.0013618177
Iter: 189 loss: 0.00135803479
Iter: 190 loss: 0.0013700251
Iter: 191 loss: 0.00135695038
Iter: 192 loss: 0.00135401182
Iter: 193 loss: 0.00135617086
Iter: 194 loss: 0.00135217397
Iter: 195 loss: 0.00134872505
Iter: 196 loss: 0.00134603167
Iter: 197 loss: 0.00134495459
Iter: 198 loss: 0.00134048844
Iter: 199 loss: 0.00137119833
Iter: 200 loss: 0.00134004327
Iter: 201 loss: 0.00133635756
Iter: 202 loss: 0.00138234207
Iter: 203 loss: 0.00133632089
Iter: 204 loss: 0.00133367314
Iter: 205 loss: 0.0013350544
Iter: 206 loss: 0.00133191561
Iter: 207 loss: 0.00132863061
Iter: 208 loss: 0.00132396969
Iter: 209 loss: 0.00132380205
Iter: 210 loss: 0.00131842354
Iter: 211 loss: 0.00132937799
Iter: 212 loss: 0.0013162446
Iter: 213 loss: 0.00131166249
Iter: 214 loss: 0.00135689857
Iter: 215 loss: 0.00131150451
Iter: 216 loss: 0.00130730704
Iter: 217 loss: 0.00131771318
Iter: 218 loss: 0.00130582484
Iter: 219 loss: 0.00130222563
Iter: 220 loss: 0.00130623463
Iter: 221 loss: 0.00130026916
Iter: 222 loss: 0.00129661546
Iter: 223 loss: 0.00133558
Iter: 224 loss: 0.00129652047
Iter: 225 loss: 0.00129369041
Iter: 226 loss: 0.00130487373
Iter: 227 loss: 0.0012930443
Iter: 228 loss: 0.00129032065
Iter: 229 loss: 0.00128985371
Iter: 230 loss: 0.00128798746
Iter: 231 loss: 0.00128456717
Iter: 232 loss: 0.00129819382
Iter: 233 loss: 0.00128380628
Iter: 234 loss: 0.00128139893
Iter: 235 loss: 0.00128139742
Iter: 236 loss: 0.00127951009
Iter: 237 loss: 0.00127881882
Iter: 238 loss: 0.00127776712
Iter: 239 loss: 0.00127520203
Iter: 240 loss: 0.0012775861
Iter: 241 loss: 0.00127372425
Iter: 242 loss: 0.00127084192
Iter: 243 loss: 0.00127281388
Iter: 244 loss: 0.00126903341
Iter: 245 loss: 0.00126514479
Iter: 246 loss: 0.00126876903
Iter: 247 loss: 0.0012628939
Iter: 248 loss: 0.00125896255
Iter: 249 loss: 0.00128226588
Iter: 250 loss: 0.00125846348
Iter: 251 loss: 0.00125451828
Iter: 252 loss: 0.00126957777
Iter: 253 loss: 0.00125358324
Iter: 254 loss: 0.00125013571
Iter: 255 loss: 0.00125125796
Iter: 256 loss: 0.0012476783
Iter: 257 loss: 0.00124424882
Iter: 258 loss: 0.00124420505
Iter: 259 loss: 0.00124183123
Iter: 260 loss: 0.00124188268
Iter: 261 loss: 0.00123994274
Iter: 262 loss: 0.0012365235
Iter: 263 loss: 0.00124236895
Iter: 264 loss: 0.00123496959
Iter: 265 loss: 0.00123247027
Iter: 266 loss: 0.00125893066
Iter: 267 loss: 0.00123241195
Iter: 268 loss: 0.00122960401
Iter: 269 loss: 0.0012329712
Iter: 270 loss: 0.0012281189
Iter: 271 loss: 0.001225424
Iter: 272 loss: 0.00122981146
Iter: 273 loss: 0.00122418
Iter: 274 loss: 0.00122133014
Iter: 275 loss: 0.00121880858
Iter: 276 loss: 0.00121806189
Iter: 277 loss: 0.0012131884
Iter: 278 loss: 0.00123698823
Iter: 279 loss: 0.00121232518
Iter: 280 loss: 0.00120800873
Iter: 281 loss: 0.0012136338
Iter: 282 loss: 0.00120579218
Iter: 283 loss: 0.00120151695
Iter: 284 loss: 0.00123645726
Iter: 285 loss: 0.00120124943
Iter: 286 loss: 0.00119736814
Iter: 287 loss: 0.00120211812
Iter: 288 loss: 0.00119534158
Iter: 289 loss: 0.00119272829
Iter: 290 loss: 0.00123378518
Iter: 291 loss: 0.00119272736
Iter: 292 loss: 0.00119021966
Iter: 293 loss: 0.00119101186
Iter: 294 loss: 0.00118842884
Iter: 295 loss: 0.00118548959
Iter: 296 loss: 0.00119408313
Iter: 297 loss: 0.00118457759
Iter: 298 loss: 0.00118189037
Iter: 299 loss: 0.0011874577
Iter: 300 loss: 0.00118079758
Iter: 301 loss: 0.00117817346
Iter: 302 loss: 0.00121840928
Iter: 303 loss: 0.00117817358
Iter: 304 loss: 0.00117646321
Iter: 305 loss: 0.00117484597
Iter: 306 loss: 0.00117445248
Iter: 307 loss: 0.00117117248
Iter: 308 loss: 0.00117071462
Iter: 309 loss: 0.0011683919
Iter: 310 loss: 0.00116440083
Iter: 311 loss: 0.00118133472
Iter: 312 loss: 0.00116353843
Iter: 313 loss: 0.00115973304
Iter: 314 loss: 0.00117424037
Iter: 315 loss: 0.00115880591
Iter: 316 loss: 0.00115581159
Iter: 317 loss: 0.00116440863
Iter: 318 loss: 0.0011548663
Iter: 319 loss: 0.00115124485
Iter: 320 loss: 0.00116020604
Iter: 321 loss: 0.00114995381
Iter: 322 loss: 0.00114629255
Iter: 323 loss: 0.0011570045
Iter: 324 loss: 0.00114514853
Iter: 325 loss: 0.00114147121
Iter: 326 loss: 0.00117962516
Iter: 327 loss: 0.00114136073
Iter: 328 loss: 0.00113925268
Iter: 329 loss: 0.00113991147
Iter: 330 loss: 0.00113774766
Iter: 331 loss: 0.00113481609
Iter: 332 loss: 0.00114251568
Iter: 333 loss: 0.00113382738
Iter: 334 loss: 0.00113219034
Iter: 335 loss: 0.00113208848
Iter: 336 loss: 0.00113069825
Iter: 337 loss: 0.00112854992
Iter: 338 loss: 0.00112851895
Iter: 339 loss: 0.0011256393
Iter: 340 loss: 0.00113165635
Iter: 341 loss: 0.00112450414
Iter: 342 loss: 0.00112171075
Iter: 343 loss: 0.00112119096
Iter: 344 loss: 0.00111931306
Iter: 345 loss: 0.0011151873
Iter: 346 loss: 0.00114075688
Iter: 347 loss: 0.00111470278
Iter: 348 loss: 0.00111129647
Iter: 349 loss: 0.00111335679
Iter: 350 loss: 0.00110910088
Iter: 351 loss: 0.0011053154
Iter: 352 loss: 0.0011359026
Iter: 353 loss: 0.00110507035
Iter: 354 loss: 0.00110221514
Iter: 355 loss: 0.0011047991
Iter: 356 loss: 0.00110055227
Iter: 357 loss: 0.00109776319
Iter: 358 loss: 0.00113742554
Iter: 359 loss: 0.00109775667
Iter: 360 loss: 0.00109548075
Iter: 361 loss: 0.00109341228
Iter: 362 loss: 0.00109284685
Iter: 363 loss: 0.00108946743
Iter: 364 loss: 0.00111195864
Iter: 365 loss: 0.00108910725
Iter: 366 loss: 0.00108679361
Iter: 367 loss: 0.00110396149
Iter: 368 loss: 0.00108660536
Iter: 369 loss: 0.00108406623
Iter: 370 loss: 0.00108330336
Iter: 371 loss: 0.00108178728
Iter: 372 loss: 0.00107933313
Iter: 373 loss: 0.0010832455
Iter: 374 loss: 0.00107819168
Iter: 375 loss: 0.00107526151
Iter: 376 loss: 0.00107700692
Iter: 377 loss: 0.00107336452
Iter: 378 loss: 0.00107007974
Iter: 379 loss: 0.00108048634
Iter: 380 loss: 0.00106912851
Iter: 381 loss: 0.00106519391
Iter: 382 loss: 0.00107279397
Iter: 383 loss: 0.00106354558
Iter: 384 loss: 0.00105987245
Iter: 385 loss: 0.0010717744
Iter: 386 loss: 0.00105883298
Iter: 387 loss: 0.00105488824
Iter: 388 loss: 0.00106588949
Iter: 389 loss: 0.00105361349
Iter: 390 loss: 0.00105068262
Iter: 391 loss: 0.00107540481
Iter: 392 loss: 0.00105051021
Iter: 393 loss: 0.00104750379
Iter: 394 loss: 0.00105213036
Iter: 395 loss: 0.00104608131
Iter: 396 loss: 0.00104360329
Iter: 397 loss: 0.00105228671
Iter: 398 loss: 0.00104295509
Iter: 399 loss: 0.0010408347
Iter: 400 loss: 0.00105814019
Iter: 401 loss: 0.00104069221
Iter: 402 loss: 0.00103862421
Iter: 403 loss: 0.00104248058
Iter: 404 loss: 0.00103774434
Iter: 405 loss: 0.00103594665
Iter: 406 loss: 0.0010337932
Iter: 407 loss: 0.00103357749
Iter: 408 loss: 0.00103020575
Iter: 409 loss: 0.00104072946
Iter: 410 loss: 0.001029226
Iter: 411 loss: 0.00102616637
Iter: 412 loss: 0.00102774694
Iter: 413 loss: 0.0010241433
Iter: 414 loss: 0.00102030963
Iter: 415 loss: 0.00104580552
Iter: 416 loss: 0.00101992139
Iter: 417 loss: 0.00101680937
Iter: 418 loss: 0.00102150301
Iter: 419 loss: 0.0010153231
Iter: 420 loss: 0.00101218233
Iter: 421 loss: 0.00102770189
Iter: 422 loss: 0.00101164461
Iter: 423 loss: 0.00100889755
Iter: 424 loss: 0.00101320772
Iter: 425 loss: 0.00100761664
Iter: 426 loss: 0.0010047087
Iter: 427 loss: 0.00103466539
Iter: 428 loss: 0.00100462441
Iter: 429 loss: 0.00100272521
Iter: 430 loss: 0.00100194558
Iter: 431 loss: 0.00100093975
Iter: 432 loss: 0.000998496311
Iter: 433 loss: 0.00102917221
Iter: 434 loss: 0.000998475
Iter: 435 loss: 0.000996369403
Iter: 436 loss: 0.00100301404
Iter: 437 loss: 0.000995758
Iter: 438 loss: 0.000994086266
Iter: 439 loss: 0.000991004286
Iter: 440 loss: 0.00106339587
Iter: 441 loss: 0.000991001492
Iter: 442 loss: 0.000987347914
Iter: 443 loss: 0.000999579
Iter: 444 loss: 0.000986350118
Iter: 445 loss: 0.000982466387
Iter: 446 loss: 0.000983646489
Iter: 447 loss: 0.000979675213
Iter: 448 loss: 0.00097539695
Iter: 449 loss: 0.00100377854
Iter: 450 loss: 0.000974947121
Iter: 451 loss: 0.000971263682
Iter: 452 loss: 0.000983364414
Iter: 453 loss: 0.000970229856
Iter: 454 loss: 0.000967127969
Iter: 455 loss: 0.000977844116
Iter: 456 loss: 0.000966298452
Iter: 457 loss: 0.000963150174
Iter: 458 loss: 0.0009686423
Iter: 459 loss: 0.000961748185
Iter: 460 loss: 0.000959087862
Iter: 461 loss: 0.000990966568
Iter: 462 loss: 0.000959054509
Iter: 463 loss: 0.000956751406
Iter: 464 loss: 0.000955192139
Iter: 465 loss: 0.00095433963
Iter: 466 loss: 0.000951905036
Iter: 467 loss: 0.000986846164
Iter: 468 loss: 0.000951892871
Iter: 469 loss: 0.000949674693
Iter: 470 loss: 0.000965636573
Iter: 471 loss: 0.000949481619
Iter: 472 loss: 0.000948019442
Iter: 473 loss: 0.000944777858
Iter: 474 loss: 0.000991137815
Iter: 475 loss: 0.000944618
Iter: 476 loss: 0.000941496226
Iter: 477 loss: 0.000952063128
Iter: 478 loss: 0.000940652681
Iter: 479 loss: 0.00093738857
Iter: 480 loss: 0.000941098086
Iter: 481 loss: 0.000935630524
Iter: 482 loss: 0.000932269555
Iter: 483 loss: 0.000940983882
Iter: 484 loss: 0.000931121234
Iter: 485 loss: 0.000927586225
Iter: 486 loss: 0.000942970626
Iter: 487 loss: 0.000926865672
Iter: 488 loss: 0.000923575251
Iter: 489 loss: 0.000934485695
Iter: 490 loss: 0.000922666281
Iter: 491 loss: 0.000919931103
Iter: 492 loss: 0.000926068751
Iter: 493 loss: 0.000918885809
Iter: 494 loss: 0.000916515361
Iter: 495 loss: 0.000922945677
Iter: 496 loss: 0.000915727869
Iter: 497 loss: 0.000912811374
Iter: 498 loss: 0.000919054903
Iter: 499 loss: 0.000911670795
Iter: 500 loss: 0.000909178343
Iter: 501 loss: 0.000905702531
Iter: 502 loss: 0.000905556139
Iter: 503 loss: 0.000902020955
Iter: 504 loss: 0.000915051845
Iter: 505 loss: 0.000901129504
Iter: 506 loss: 0.000899538
Iter: 507 loss: 0.000899239676
Iter: 508 loss: 0.000897327904
Iter: 509 loss: 0.000901444815
Iter: 510 loss: 0.000896586
Iter: 511 loss: 0.000895415491
Iter: 512 loss: 0.000893645221
Iter: 513 loss: 0.000893605757
Iter: 514 loss: 0.000890679657
Iter: 515 loss: 0.0008956003
Iter: 516 loss: 0.000889358
Iter: 517 loss: 0.000886654947
Iter: 518 loss: 0.000885250862
Iter: 519 loss: 0.000884005218
Iter: 520 loss: 0.000879985571
Iter: 521 loss: 0.000905960565
Iter: 522 loss: 0.000879537314
Iter: 523 loss: 0.000875842
Iter: 524 loss: 0.000879255822
Iter: 525 loss: 0.000873704208
Iter: 526 loss: 0.000869798183
Iter: 527 loss: 0.000888639828
Iter: 528 loss: 0.0008690994
Iter: 529 loss: 0.000866440882
Iter: 530 loss: 0.00089071691
Iter: 531 loss: 0.000866320392
Iter: 532 loss: 0.000863734749
Iter: 533 loss: 0.00086404325
Iter: 534 loss: 0.000861741486
Iter: 535 loss: 0.000859081279
Iter: 536 loss: 0.000856435392
Iter: 537 loss: 0.000855887833
Iter: 538 loss: 0.000854983227
Iter: 539 loss: 0.000854229263
Iter: 540 loss: 0.000852676225
Iter: 541 loss: 0.000855275
Iter: 542 loss: 0.000851966906
Iter: 543 loss: 0.000850347
Iter: 544 loss: 0.000850567536
Iter: 545 loss: 0.000849086
Iter: 546 loss: 0.000845893868
Iter: 547 loss: 0.000849045
Iter: 548 loss: 0.000844100898
Iter: 549 loss: 0.00084111304
Iter: 550 loss: 0.000843388785
Iter: 551 loss: 0.000839280663
Iter: 552 loss: 0.00083679962
Iter: 553 loss: 0.000831459649
Iter: 554 loss: 0.000913898286
Iter: 555 loss: 0.000831267564
Iter: 556 loss: 0.000827550539
Iter: 557 loss: 0.000833272
Iter: 558 loss: 0.000825751573
Iter: 559 loss: 0.000823209353
Iter: 560 loss: 0.000824591145
Iter: 561 loss: 0.000821550377
Iter: 562 loss: 0.000819117879
Iter: 563 loss: 0.000828613061
Iter: 564 loss: 0.000818559201
Iter: 565 loss: 0.00081601867
Iter: 566 loss: 0.000825154886
Iter: 567 loss: 0.00081536069
Iter: 568 loss: 0.000812826795
Iter: 569 loss: 0.0008170777
Iter: 570 loss: 0.000811656646
Iter: 571 loss: 0.000809847144
Iter: 572 loss: 0.000832094578
Iter: 573 loss: 0.000809830904
Iter: 574 loss: 0.000808650919
Iter: 575 loss: 0.000810272293
Iter: 576 loss: 0.000808062381
Iter: 577 loss: 0.000806317665
Iter: 578 loss: 0.000802613562
Iter: 579 loss: 0.000864013389
Iter: 580 loss: 0.000802504132
Iter: 581 loss: 0.00079938391
Iter: 582 loss: 0.000826134696
Iter: 583 loss: 0.000799224887
Iter: 584 loss: 0.000796488894
Iter: 585 loss: 0.000801179907
Iter: 586 loss: 0.0007952546
Iter: 587 loss: 0.000793481
Iter: 588 loss: 0.000791480066
Iter: 589 loss: 0.000791198574
Iter: 590 loss: 0.000788321719
Iter: 591 loss: 0.00079207489
Iter: 592 loss: 0.000786869205
Iter: 593 loss: 0.000784511445
Iter: 594 loss: 0.000803059083
Iter: 595 loss: 0.000784294098
Iter: 596 loss: 0.000782766379
Iter: 597 loss: 0.000780675153
Iter: 598 loss: 0.000780579867
Iter: 599 loss: 0.000778391724
Iter: 600 loss: 0.000777862035
Iter: 601 loss: 0.000776472443
Iter: 602 loss: 0.000774664688
Iter: 603 loss: 0.000773811596
Iter: 604 loss: 0.000772922649
Iter: 605 loss: 0.000770661
Iter: 606 loss: 0.00076741213
Iter: 607 loss: 0.000767300371
Iter: 608 loss: 0.000764585449
Iter: 609 loss: 0.000764585566
Iter: 610 loss: 0.000762644573
Iter: 611 loss: 0.000763437536
Iter: 612 loss: 0.000761305448
Iter: 613 loss: 0.000757829635
Iter: 614 loss: 0.000763779623
Iter: 615 loss: 0.00075625
Iter: 616 loss: 0.000753636472
Iter: 617 loss: 0.000771555584
Iter: 618 loss: 0.000753385073
Iter: 619 loss: 0.000750677427
Iter: 620 loss: 0.000750871259
Iter: 621 loss: 0.000748568331
Iter: 622 loss: 0.000745845726
Iter: 623 loss: 0.000749445055
Iter: 624 loss: 0.000744465273
Iter: 625 loss: 0.000741803262
Iter: 626 loss: 0.000763505232
Iter: 627 loss: 0.000741608208
Iter: 628 loss: 0.000739417388
Iter: 629 loss: 0.000764177763
Iter: 630 loss: 0.000739376177
Iter: 631 loss: 0.000738279603
Iter: 632 loss: 0.000737160619
Iter: 633 loss: 0.000736944843
Iter: 634 loss: 0.000734868
Iter: 635 loss: 0.000736180809
Iter: 636 loss: 0.000733530149
Iter: 637 loss: 0.000731048931
Iter: 638 loss: 0.00074165297
Iter: 639 loss: 0.000730523374
Iter: 640 loss: 0.000729031046
Iter: 641 loss: 0.000739557843
Iter: 642 loss: 0.00072889257
Iter: 643 loss: 0.000727373525
Iter: 644 loss: 0.000723695266
Iter: 645 loss: 0.000763852673
Iter: 646 loss: 0.000723295729
Iter: 647 loss: 0.000720486278
Iter: 648 loss: 0.000735214096
Iter: 649 loss: 0.000720033888
Iter: 650 loss: 0.000717662915
Iter: 651 loss: 0.000719762058
Iter: 652 loss: 0.000716275186
Iter: 653 loss: 0.000714465743
Iter: 654 loss: 0.000714239
Iter: 655 loss: 0.000712046633
Iter: 656 loss: 0.000714018242
Iter: 657 loss: 0.000710756285
Iter: 658 loss: 0.000709334388
Iter: 659 loss: 0.000713174697
Iter: 660 loss: 0.00070889818
Iter: 661 loss: 0.000708125066
Iter: 662 loss: 0.000707384432
Iter: 663 loss: 0.000707206549
Iter: 664 loss: 0.000703577069
Iter: 665 loss: 0.0007090125
Iter: 666 loss: 0.000701831596
Iter: 667 loss: 0.000698609394
Iter: 668 loss: 0.000697872136
Iter: 669 loss: 0.000695565715
Iter: 670 loss: 0.000699505559
Iter: 671 loss: 0.000694527407
Iter: 672 loss: 0.000692419941
Iter: 673 loss: 0.000717566407
Iter: 674 loss: 0.00069239171
Iter: 675 loss: 0.000690469285
Iter: 676 loss: 0.000702052319
Iter: 677 loss: 0.00069023564
Iter: 678 loss: 0.000689257286
Iter: 679 loss: 0.000687049527
Iter: 680 loss: 0.000717027055
Iter: 681 loss: 0.000686918967
Iter: 682 loss: 0.000684355269
Iter: 683 loss: 0.000685675244
Iter: 684 loss: 0.000682644197
Iter: 685 loss: 0.000679549237
Iter: 686 loss: 0.000694295391
Iter: 687 loss: 0.000678992597
Iter: 688 loss: 0.000676423428
Iter: 689 loss: 0.00068981247
Iter: 690 loss: 0.000676019583
Iter: 691 loss: 0.000677937525
Iter: 692 loss: 0.00067523832
Iter: 693 loss: 0.000674524
Iter: 694 loss: 0.000672483642
Iter: 695 loss: 0.000681977312
Iter: 696 loss: 0.000671758433
Iter: 697 loss: 0.000668776
Iter: 698 loss: 0.000666798442
Iter: 699 loss: 0.000665665837
Iter: 700 loss: 0.000661449507
Iter: 701 loss: 0.000694010872
Iter: 702 loss: 0.000661133206
Iter: 703 loss: 0.000657894532
Iter: 704 loss: 0.000674052397
Iter: 705 loss: 0.00065732823
Iter: 706 loss: 0.000655512908
Iter: 707 loss: 0.000658098958
Iter: 708 loss: 0.000654616451
Iter: 709 loss: 0.000652730057
Iter: 710 loss: 0.000671460468
Iter: 711 loss: 0.000652663875
Iter: 712 loss: 0.000651285111
Iter: 713 loss: 0.000651950657
Iter: 714 loss: 0.000650351634
Iter: 715 loss: 0.00064869679
Iter: 716 loss: 0.000649208785
Iter: 717 loss: 0.000647514418
Iter: 718 loss: 0.000645646825
Iter: 719 loss: 0.000646781235
Iter: 720 loss: 0.000644433079
Iter: 721 loss: 0.000641677179
Iter: 722 loss: 0.00065248087
Iter: 723 loss: 0.000641034043
Iter: 724 loss: 0.000639457605
Iter: 725 loss: 0.000639312668
Iter: 726 loss: 0.000637438847
Iter: 727 loss: 0.000649441
Iter: 728 loss: 0.000637243502
Iter: 729 loss: 0.0006359939
Iter: 730 loss: 0.000633024727
Iter: 731 loss: 0.000668883557
Iter: 732 loss: 0.000632757205
Iter: 733 loss: 0.000629023765
Iter: 734 loss: 0.000632640207
Iter: 735 loss: 0.000626937
Iter: 736 loss: 0.000622830237
Iter: 737 loss: 0.000685743464
Iter: 738 loss: 0.000622832042
Iter: 739 loss: 0.000621035113
Iter: 740 loss: 0.00062072865
Iter: 741 loss: 0.000619065482
Iter: 742 loss: 0.000629898161
Iter: 743 loss: 0.000618892605
Iter: 744 loss: 0.000617324957
Iter: 745 loss: 0.000615208934
Iter: 746 loss: 0.000615098106
Iter: 747 loss: 0.000612721953
Iter: 748 loss: 0.00062936591
Iter: 749 loss: 0.000612495
Iter: 750 loss: 0.000611037714
Iter: 751 loss: 0.000608808477
Iter: 752 loss: 0.000608769245
Iter: 753 loss: 0.000604954432
Iter: 754 loss: 0.000634463853
Iter: 755 loss: 0.000604674453
Iter: 756 loss: 0.000601910579
Iter: 757 loss: 0.000626907684
Iter: 758 loss: 0.000601787062
Iter: 759 loss: 0.000599702
Iter: 760 loss: 0.000623851607
Iter: 761 loss: 0.000599662133
Iter: 762 loss: 0.00059813878
Iter: 763 loss: 0.000597297563
Iter: 764 loss: 0.000596622471
Iter: 765 loss: 0.000594423385
Iter: 766 loss: 0.000591932796
Iter: 767 loss: 0.00059160043
Iter: 768 loss: 0.000586839858
Iter: 769 loss: 0.000598727085
Iter: 770 loss: 0.000585145201
Iter: 771 loss: 0.00058875361
Iter: 772 loss: 0.000583682733
Iter: 773 loss: 0.000582449604
Iter: 774 loss: 0.000581408618
Iter: 775 loss: 0.000581059838
Iter: 776 loss: 0.000578736479
Iter: 777 loss: 0.000584764581
Iter: 778 loss: 0.000577944855
Iter: 779 loss: 0.000576072372
Iter: 780 loss: 0.000579392421
Iter: 781 loss: 0.000575252576
Iter: 782 loss: 0.000572943711
Iter: 783 loss: 0.000570054865
Iter: 784 loss: 0.000569807598
Iter: 785 loss: 0.000566768227
Iter: 786 loss: 0.000580995576
Iter: 787 loss: 0.000566205
Iter: 788 loss: 0.000563480135
Iter: 789 loss: 0.00058012642
Iter: 790 loss: 0.000563144451
Iter: 791 loss: 0.000561607478
Iter: 792 loss: 0.000561510795
Iter: 793 loss: 0.000559915
Iter: 794 loss: 0.00056077342
Iter: 795 loss: 0.00055884989
Iter: 796 loss: 0.000556456391
Iter: 797 loss: 0.000556727231
Iter: 798 loss: 0.000554620638
Iter: 799 loss: 0.000551312929
Iter: 800 loss: 0.000558902102
Iter: 801 loss: 0.000550066587
Iter: 802 loss: 0.000546640775
Iter: 803 loss: 0.000550661702
Iter: 804 loss: 0.000544814335
Iter: 805 loss: 0.000545905612
Iter: 806 loss: 0.000543230912
Iter: 807 loss: 0.000541653193
Iter: 808 loss: 0.000543612463
Iter: 809 loss: 0.000540836
Iter: 810 loss: 0.000538397231
Iter: 811 loss: 0.000540059118
Iter: 812 loss: 0.000536862179
Iter: 813 loss: 0.000534514606
Iter: 814 loss: 0.00054676435
Iter: 815 loss: 0.000534136139
Iter: 816 loss: 0.000531784084
Iter: 817 loss: 0.000530240242
Iter: 818 loss: 0.00052934594
Iter: 819 loss: 0.000526151736
Iter: 820 loss: 0.000532868435
Iter: 821 loss: 0.000524865231
Iter: 822 loss: 0.000521240407
Iter: 823 loss: 0.000547138043
Iter: 824 loss: 0.000520920148
Iter: 825 loss: 0.000518663437
Iter: 826 loss: 0.000525289041
Iter: 827 loss: 0.000517966517
Iter: 828 loss: 0.000515072781
Iter: 829 loss: 0.000532046659
Iter: 830 loss: 0.000514713931
Iter: 831 loss: 0.000512705883
Iter: 832 loss: 0.000512516883
Iter: 833 loss: 0.000511037884
Iter: 834 loss: 0.000508460565
Iter: 835 loss: 0.000512431259
Iter: 836 loss: 0.000507240649
Iter: 837 loss: 0.000504714088
Iter: 838 loss: 0.000518667803
Iter: 839 loss: 0.000504335098
Iter: 840 loss: 0.000502067734
Iter: 841 loss: 0.000502066687
Iter: 842 loss: 0.000501009286
Iter: 843 loss: 0.000500101829
Iter: 844 loss: 0.000499813585
Iter: 845 loss: 0.000497836096
Iter: 846 loss: 0.000498660957
Iter: 847 loss: 0.000496468041
Iter: 848 loss: 0.000494243111
Iter: 849 loss: 0.000501743169
Iter: 850 loss: 0.000493639556
Iter: 851 loss: 0.000491377
Iter: 852 loss: 0.000491873128
Iter: 853 loss: 0.000489699771
Iter: 854 loss: 0.000487249461
Iter: 855 loss: 0.00048954261
Iter: 856 loss: 0.000485838042
Iter: 857 loss: 0.000483311043
Iter: 858 loss: 0.000516395899
Iter: 859 loss: 0.00048329428
Iter: 860 loss: 0.000481353432
Iter: 861 loss: 0.00048826175
Iter: 862 loss: 0.000480864284
Iter: 863 loss: 0.000478657108
Iter: 864 loss: 0.000486182951
Iter: 865 loss: 0.00047806665
Iter: 866 loss: 0.000476241315
Iter: 867 loss: 0.000474068569
Iter: 868 loss: 0.000473841967
Iter: 869 loss: 0.000471730018
Iter: 870 loss: 0.000471725536
Iter: 871 loss: 0.000470422412
Iter: 872 loss: 0.000470422441
Iter: 873 loss: 0.000469311024
Iter: 874 loss: 0.000467285427
Iter: 875 loss: 0.000515652471
Iter: 876 loss: 0.000467283
Iter: 877 loss: 0.000465425837
Iter: 878 loss: 0.000480426475
Iter: 879 loss: 0.00046529714
Iter: 880 loss: 0.000463824254
Iter: 881 loss: 0.000462971133
Iter: 882 loss: 0.000462344091
Iter: 883 loss: 0.000460398442
Iter: 884 loss: 0.000469690713
Iter: 885 loss: 0.000460046751
Iter: 886 loss: 0.000458218565
Iter: 887 loss: 0.000455732836
Iter: 888 loss: 0.00045561025
Iter: 889 loss: 0.000452654203
Iter: 890 loss: 0.000476027955
Iter: 891 loss: 0.000452448759
Iter: 892 loss: 0.000449657207
Iter: 893 loss: 0.000459425821
Iter: 894 loss: 0.000448928738
Iter: 895 loss: 0.000446601654
Iter: 896 loss: 0.000463133329
Iter: 897 loss: 0.000446393329
Iter: 898 loss: 0.000444075034
Iter: 899 loss: 0.000443161523
Iter: 900 loss: 0.000441914279
Iter: 901 loss: 0.000439372554
Iter: 902 loss: 0.000457640097
Iter: 903 loss: 0.000439146679
Iter: 904 loss: 0.00043775962
Iter: 905 loss: 0.000437646522
Iter: 906 loss: 0.000436328322
Iter: 907 loss: 0.000435777765
Iter: 908 loss: 0.000435088878
Iter: 909 loss: 0.000433568348
Iter: 910 loss: 0.000435731898
Iter: 911 loss: 0.000432817906
Iter: 912 loss: 0.000430883374
Iter: 913 loss: 0.000435711088
Iter: 914 loss: 0.000430201442
Iter: 915 loss: 0.000428645028
Iter: 916 loss: 0.000428269879
Iter: 917 loss: 0.00042727773
Iter: 918 loss: 0.000424735015
Iter: 919 loss: 0.000428590836
Iter: 920 loss: 0.000423526799
Iter: 921 loss: 0.000420780154
Iter: 922 loss: 0.000421072909
Iter: 923 loss: 0.000418658892
Iter: 924 loss: 0.000416142517
Iter: 925 loss: 0.000416136812
Iter: 926 loss: 0.00041390289
Iter: 927 loss: 0.000414886104
Iter: 928 loss: 0.000412378577
Iter: 929 loss: 0.000410067674
Iter: 930 loss: 0.000431001885
Iter: 931 loss: 0.000409957662
Iter: 932 loss: 0.000408076099
Iter: 933 loss: 0.000406823412
Iter: 934 loss: 0.000406111707
Iter: 935 loss: 0.000404697494
Iter: 936 loss: 0.00040461024
Iter: 937 loss: 0.000403044192
Iter: 938 loss: 0.000405122468
Iter: 939 loss: 0.000402248173
Iter: 940 loss: 0.000400751946
Iter: 941 loss: 0.000404030929
Iter: 942 loss: 0.000400171091
Iter: 943 loss: 0.000398900651
Iter: 944 loss: 0.000401520811
Iter: 945 loss: 0.00039839471
Iter: 946 loss: 0.000396776944
Iter: 947 loss: 0.000395784155
Iter: 948 loss: 0.000395124487
Iter: 949 loss: 0.000392856455
Iter: 950 loss: 0.000398845121
Iter: 951 loss: 0.000392090413
Iter: 952 loss: 0.000389537017
Iter: 953 loss: 0.000389331952
Iter: 954 loss: 0.000387425418
Iter: 955 loss: 0.000384215557
Iter: 956 loss: 0.000395268202
Iter: 957 loss: 0.000383362145
Iter: 958 loss: 0.000381651334
Iter: 959 loss: 0.00038157677
Iter: 960 loss: 0.000380123092
Iter: 961 loss: 0.000379712234
Iter: 962 loss: 0.000378829776
Iter: 963 loss: 0.000377058197
Iter: 964 loss: 0.000391969632
Iter: 965 loss: 0.000376955722
Iter: 966 loss: 0.000375643256
Iter: 967 loss: 0.000378544675
Iter: 968 loss: 0.000375147152
Iter: 969 loss: 0.000373586663
Iter: 970 loss: 0.000388203247
Iter: 971 loss: 0.000373520132
Iter: 972 loss: 0.000372621493
Iter: 973 loss: 0.000371804024
Iter: 974 loss: 0.00037158106
Iter: 975 loss: 0.000369967544
Iter: 976 loss: 0.000376775628
Iter: 977 loss: 0.000369626243
Iter: 978 loss: 0.000368292676
Iter: 979 loss: 0.000369434769
Iter: 980 loss: 0.000367502682
Iter: 981 loss: 0.000365841755
Iter: 982 loss: 0.000365679909
Iter: 983 loss: 0.000364463282
Iter: 984 loss: 0.000362330495
Iter: 985 loss: 0.000367852161
Iter: 986 loss: 0.000361598446
Iter: 987 loss: 0.000359373284
Iter: 988 loss: 0.000361122773
Iter: 989 loss: 0.0003580263
Iter: 990 loss: 0.000355401949
Iter: 991 loss: 0.000361553102
Iter: 992 loss: 0.000354425807
Iter: 993 loss: 0.000352911418
Iter: 994 loss: 0.000352709118
Iter: 995 loss: 0.00035149965
Iter: 996 loss: 0.000349631679
Iter: 997 loss: 0.000349605689
Iter: 998 loss: 0.000347619061
Iter: 999 loss: 0.000370939786
Iter: 1000 loss: 0.000347591034
Iter: 1001 loss: 0.000346568821
Iter: 1002 loss: 0.00036225596
Iter: 1003 loss: 0.000346568966
Iter: 1004 loss: 0.000345590233
Iter: 1005 loss: 0.000343431253
Iter: 1006 loss: 0.000374776952
Iter: 1007 loss: 0.000343328167
Iter: 1008 loss: 0.000341893407
Iter: 1009 loss: 0.000363387226
Iter: 1010 loss: 0.000341891195
Iter: 1011 loss: 0.000340491883
Iter: 1012 loss: 0.000340728322
Iter: 1013 loss: 0.000339439604
Iter: 1014 loss: 0.000337758946
Iter: 1015 loss: 0.000340908708
Iter: 1016 loss: 0.000337044214
Iter: 1017 loss: 0.000335299585
Iter: 1018 loss: 0.000334355631
Iter: 1019 loss: 0.000333572389
Iter: 1020 loss: 0.000330828043
Iter: 1021 loss: 0.000339180115
Iter: 1022 loss: 0.000330006849
Iter: 1023 loss: 0.000327370886
Iter: 1024 loss: 0.00033175759
Iter: 1025 loss: 0.000326171052
Iter: 1026 loss: 0.000323817716
Iter: 1027 loss: 0.000333865115
Iter: 1028 loss: 0.000323325949
Iter: 1029 loss: 0.000321698142
Iter: 1030 loss: 0.000321680855
Iter: 1031 loss: 0.000320541265
Iter: 1032 loss: 0.000319022831
Iter: 1033 loss: 0.000318937091
Iter: 1034 loss: 0.000318003353
Iter: 1035 loss: 0.000317734113
Iter: 1036 loss: 0.000316666963
Iter: 1037 loss: 0.000317145343
Iter: 1038 loss: 0.000315944053
Iter: 1039 loss: 0.000314772245
Iter: 1040 loss: 0.000312712451
Iter: 1041 loss: 0.000312712742
Iter: 1042 loss: 0.000311878364
Iter: 1043 loss: 0.000311541837
Iter: 1044 loss: 0.000310422212
Iter: 1045 loss: 0.000309225812
Iter: 1046 loss: 0.000309031748
Iter: 1047 loss: 0.000307463342
Iter: 1048 loss: 0.000309820636
Iter: 1049 loss: 0.000306711241
Iter: 1050 loss: 0.000304736081
Iter: 1051 loss: 0.000307999318
Iter: 1052 loss: 0.000303832348
Iter: 1053 loss: 0.000302007131
Iter: 1054 loss: 0.000302075816
Iter: 1055 loss: 0.000300564396
Iter: 1056 loss: 0.000298480212
Iter: 1057 loss: 0.000306121161
Iter: 1058 loss: 0.000297962368
Iter: 1059 loss: 0.000295785081
Iter: 1060 loss: 0.0003006552
Iter: 1061 loss: 0.000294953788
Iter: 1062 loss: 0.000293099554
Iter: 1063 loss: 0.000313644356
Iter: 1064 loss: 0.000293061894
Iter: 1065 loss: 0.000291394186
Iter: 1066 loss: 0.000296102022
Iter: 1067 loss: 0.000290857628
Iter: 1068 loss: 0.000289537827
Iter: 1069 loss: 0.000289536343
Iter: 1070 loss: 0.000288741896
Iter: 1071 loss: 0.000286754541
Iter: 1072 loss: 0.000305785274
Iter: 1073 loss: 0.000286476832
Iter: 1074 loss: 0.00028440723
Iter: 1075 loss: 0.000297136721
Iter: 1076 loss: 0.000284155831
Iter: 1077 loss: 0.000283235277
Iter: 1078 loss: 0.00028318452
Iter: 1079 loss: 0.00028229662
Iter: 1080 loss: 0.000280281674
Iter: 1081 loss: 0.000307510199
Iter: 1082 loss: 0.000280158652
Iter: 1083 loss: 0.000278391497
Iter: 1084 loss: 0.000280781591
Iter: 1085 loss: 0.000277503597
Iter: 1086 loss: 0.000275564147
Iter: 1087 loss: 0.000291445293
Iter: 1088 loss: 0.000275441
Iter: 1089 loss: 0.000273980666
Iter: 1090 loss: 0.000272817328
Iter: 1091 loss: 0.000272371952
Iter: 1092 loss: 0.000270327087
Iter: 1093 loss: 0.000277010957
Iter: 1094 loss: 0.000269757264
Iter: 1095 loss: 0.000267732656
Iter: 1096 loss: 0.000272792822
Iter: 1097 loss: 0.000267012
Iter: 1098 loss: 0.000265455281
Iter: 1099 loss: 0.000280667969
Iter: 1100 loss: 0.000265401846
Iter: 1101 loss: 0.000264278409
Iter: 1102 loss: 0.000264273782
Iter: 1103 loss: 0.00026341842
Iter: 1104 loss: 0.000263657188
Iter: 1105 loss: 0.000262801681
Iter: 1106 loss: 0.000261921581
Iter: 1107 loss: 0.000260787841
Iter: 1108 loss: 0.000260710076
Iter: 1109 loss: 0.000259501801
Iter: 1110 loss: 0.000277202285
Iter: 1111 loss: 0.000259499822
Iter: 1112 loss: 0.000258210814
Iter: 1113 loss: 0.000260036148
Iter: 1114 loss: 0.000257578155
Iter: 1115 loss: 0.000256604806
Iter: 1116 loss: 0.000255470077
Iter: 1117 loss: 0.000255338411
Iter: 1118 loss: 0.000253461796
Iter: 1119 loss: 0.000262307934
Iter: 1120 loss: 0.000253118167
Iter: 1121 loss: 0.000251590216
Iter: 1122 loss: 0.000251820835
Iter: 1123 loss: 0.000250429643
Iter: 1124 loss: 0.000248719211
Iter: 1125 loss: 0.000264974486
Iter: 1126 loss: 0.000248650846
Iter: 1127 loss: 0.000247248303
Iter: 1128 loss: 0.000247398653
Iter: 1129 loss: 0.000246171199
Iter: 1130 loss: 0.000244696857
Iter: 1131 loss: 0.000250405603
Iter: 1132 loss: 0.000244349474
Iter: 1133 loss: 0.00024366584
Iter: 1134 loss: 0.000243488394
Iter: 1135 loss: 0.000242847091
Iter: 1136 loss: 0.00024362652
Iter: 1137 loss: 0.00024251244
Iter: 1138 loss: 0.000241819886
Iter: 1139 loss: 0.00024044
Iter: 1140 loss: 0.000267186319
Iter: 1141 loss: 0.000240424852
Iter: 1142 loss: 0.000239087938
Iter: 1143 loss: 0.000250007695
Iter: 1144 loss: 0.00023900393
Iter: 1145 loss: 0.000237947053
Iter: 1146 loss: 0.000251007
Iter: 1147 loss: 0.000237934932
Iter: 1148 loss: 0.000237320695
Iter: 1149 loss: 0.000235793355
Iter: 1150 loss: 0.000250470825
Iter: 1151 loss: 0.000235585219
Iter: 1152 loss: 0.000234175648
Iter: 1153 loss: 0.000248102384
Iter: 1154 loss: 0.000234126623
Iter: 1155 loss: 0.000232904596
Iter: 1156 loss: 0.000232609396
Iter: 1157 loss: 0.000231833023
Iter: 1158 loss: 0.00023032623
Iter: 1159 loss: 0.00023637616
Iter: 1160 loss: 0.000229986879
Iter: 1161 loss: 0.000228491699
Iter: 1162 loss: 0.000235517466
Iter: 1163 loss: 0.000228214616
Iter: 1164 loss: 0.000226958029
Iter: 1165 loss: 0.000226079559
Iter: 1166 loss: 0.000225631229
Iter: 1167 loss: 0.000225558368
Iter: 1168 loss: 0.000224883173
Iter: 1169 loss: 0.000224147821
Iter: 1170 loss: 0.000225126889
Iter: 1171 loss: 0.000223777024
Iter: 1172 loss: 0.000222936535
Iter: 1173 loss: 0.000222272647
Iter: 1174 loss: 0.000222013798
Iter: 1175 loss: 0.00022093764
Iter: 1176 loss: 0.000221972674
Iter: 1177 loss: 0.000220323695
Iter: 1178 loss: 0.000219791254
Iter: 1179 loss: 0.000219586436
Iter: 1180 loss: 0.0002190144
Iter: 1181 loss: 0.000217787063
Iter: 1182 loss: 0.000237402855
Iter: 1183 loss: 0.000217746245
Iter: 1184 loss: 0.000216555971
Iter: 1185 loss: 0.000217245892
Iter: 1186 loss: 0.000215781678
Iter: 1187 loss: 0.000214382424
Iter: 1188 loss: 0.000222344606
Iter: 1189 loss: 0.000214185959
Iter: 1190 loss: 0.000212980638
Iter: 1191 loss: 0.000213136867
Iter: 1192 loss: 0.000212059414
Iter: 1193 loss: 0.000210631915
Iter: 1194 loss: 0.000216965476
Iter: 1195 loss: 0.000210348255
Iter: 1196 loss: 0.000209039237
Iter: 1197 loss: 0.000217285255
Iter: 1198 loss: 0.000208888756
Iter: 1199 loss: 0.000207868463
Iter: 1200 loss: 0.000208873433
Iter: 1201 loss: 0.00020729
Iter: 1202 loss: 0.000206818542
Iter: 1203 loss: 0.000206563913
Iter: 1204 loss: 0.000206098222
Iter: 1205 loss: 0.00020503471
Iter: 1206 loss: 0.000219046691
Iter: 1207 loss: 0.000204966185
Iter: 1208 loss: 0.000203783056
Iter: 1209 loss: 0.000208705489
Iter: 1210 loss: 0.000203528791
Iter: 1211 loss: 0.000202765543
Iter: 1212 loss: 0.000209516
Iter: 1213 loss: 0.000202725845
Iter: 1214 loss: 0.000201784147
Iter: 1215 loss: 0.000202281546
Iter: 1216 loss: 0.00020116294
Iter: 1217 loss: 0.000200450741
Iter: 1218 loss: 0.000199427974
Iter: 1219 loss: 0.000199394475
Iter: 1220 loss: 0.00019813882
Iter: 1221 loss: 0.000205395379
Iter: 1222 loss: 0.000197971574
Iter: 1223 loss: 0.000196933615
Iter: 1224 loss: 0.000197191519
Iter: 1225 loss: 0.000196175446
Iter: 1226 loss: 0.000194889668
Iter: 1227 loss: 0.000197983638
Iter: 1228 loss: 0.000194424108
Iter: 1229 loss: 0.000193244196
Iter: 1230 loss: 0.000198959708
Iter: 1231 loss: 0.000193035812
Iter: 1232 loss: 0.000192011896
Iter: 1233 loss: 0.000196140871
Iter: 1234 loss: 0.000191783169
Iter: 1235 loss: 0.000191214873
Iter: 1236 loss: 0.000191167652
Iter: 1237 loss: 0.000190548322
Iter: 1238 loss: 0.000189843093
Iter: 1239 loss: 0.000189753599
Iter: 1240 loss: 0.000189038808
Iter: 1241 loss: 0.000189138082
Iter: 1242 loss: 0.000188498583
Iter: 1243 loss: 0.000187401587
Iter: 1244 loss: 0.000190104969
Iter: 1245 loss: 0.000187011756
Iter: 1246 loss: 0.000186378835
Iter: 1247 loss: 0.000186273945
Iter: 1248 loss: 0.00018589606
Iter: 1249 loss: 0.00018481088
Iter: 1250 loss: 0.000189610117
Iter: 1251 loss: 0.000184403529
Iter: 1252 loss: 0.000183319484
Iter: 1253 loss: 0.000196088193
Iter: 1254 loss: 0.000183304452
Iter: 1255 loss: 0.000182316406
Iter: 1256 loss: 0.000183056109
Iter: 1257 loss: 0.000181708368
Iter: 1258 loss: 0.000180631163
Iter: 1259 loss: 0.000181965821
Iter: 1260 loss: 0.000180067684
Iter: 1261 loss: 0.000178786431
Iter: 1262 loss: 0.000181981333
Iter: 1263 loss: 0.000178335918
Iter: 1264 loss: 0.000177112714
Iter: 1265 loss: 0.000183762255
Iter: 1266 loss: 0.000176932488
Iter: 1267 loss: 0.000175889771
Iter: 1268 loss: 0.000182943128
Iter: 1269 loss: 0.000175784691
Iter: 1270 loss: 0.000175185589
Iter: 1271 loss: 0.000175168621
Iter: 1272 loss: 0.000174769753
Iter: 1273 loss: 0.000173733977
Iter: 1274 loss: 0.000182133634
Iter: 1275 loss: 0.000173543522
Iter: 1276 loss: 0.000172405795
Iter: 1277 loss: 0.000176935326
Iter: 1278 loss: 0.000172147498
Iter: 1279 loss: 0.000171505118
Iter: 1280 loss: 0.000171476931
Iter: 1281 loss: 0.000170803905
Iter: 1282 loss: 0.000170411775
Iter: 1283 loss: 0.00017012535
Iter: 1284 loss: 0.000169422667
Iter: 1285 loss: 0.000168382889
Iter: 1286 loss: 0.000168357539
Iter: 1287 loss: 0.000167180726
Iter: 1288 loss: 0.000174916349
Iter: 1289 loss: 0.000167054211
Iter: 1290 loss: 0.00016589844
Iter: 1291 loss: 0.000169819017
Iter: 1292 loss: 0.000165585021
Iter: 1293 loss: 0.00016460933
Iter: 1294 loss: 0.000163670251
Iter: 1295 loss: 0.000163453573
Iter: 1296 loss: 0.000162100128
Iter: 1297 loss: 0.000170081272
Iter: 1298 loss: 0.000161922406
Iter: 1299 loss: 0.00016108944
Iter: 1300 loss: 0.000161089483
Iter: 1301 loss: 0.00016054188
Iter: 1302 loss: 0.000167040387
Iter: 1303 loss: 0.000160534008
Iter: 1304 loss: 0.000159996489
Iter: 1305 loss: 0.00015996478
Iter: 1306 loss: 0.00015955462
Iter: 1307 loss: 0.000158917072
Iter: 1308 loss: 0.000158737
Iter: 1309 loss: 0.000158349751
Iter: 1310 loss: 0.000157552873
Iter: 1311 loss: 0.000160521944
Iter: 1312 loss: 0.000157355797
Iter: 1313 loss: 0.000156542577
Iter: 1314 loss: 0.000165905702
Iter: 1315 loss: 0.000156527531
Iter: 1316 loss: 0.000156068782
Iter: 1317 loss: 0.000155287285
Iter: 1318 loss: 0.000155284855
Iter: 1319 loss: 0.00015436273
Iter: 1320 loss: 0.000153702538
Iter: 1321 loss: 0.000153380737
Iter: 1322 loss: 0.000152232227
Iter: 1323 loss: 0.000157868053
Iter: 1324 loss: 0.000152031571
Iter: 1325 loss: 0.000151010041
Iter: 1326 loss: 0.000161354576
Iter: 1327 loss: 0.000150978187
Iter: 1328 loss: 0.000150224208
Iter: 1329 loss: 0.000149089276
Iter: 1330 loss: 0.000149065614
Iter: 1331 loss: 0.000147665647
Iter: 1332 loss: 0.000149664978
Iter: 1333 loss: 0.000146976614
Iter: 1334 loss: 0.000145634345
Iter: 1335 loss: 0.00015611235
Iter: 1336 loss: 0.000145539714
Iter: 1337 loss: 0.000146273698
Iter: 1338 loss: 0.000145212558
Iter: 1339 loss: 0.000144943479
Iter: 1340 loss: 0.000144198653
Iter: 1341 loss: 0.000148471561
Iter: 1342 loss: 0.00014398378
Iter: 1343 loss: 0.000143180499
Iter: 1344 loss: 0.000145381026
Iter: 1345 loss: 0.000142919889
Iter: 1346 loss: 0.000142373217
Iter: 1347 loss: 0.000142332894
Iter: 1348 loss: 0.000141899654
Iter: 1349 loss: 0.000142985562
Iter: 1350 loss: 0.000141749028
Iter: 1351 loss: 0.000141297525
Iter: 1352 loss: 0.000140568911
Iter: 1353 loss: 0.000140563119
Iter: 1354 loss: 0.000139752869
Iter: 1355 loss: 0.000140866236
Iter: 1356 loss: 0.000139348733
Iter: 1357 loss: 0.000138458621
Iter: 1358 loss: 0.000140673626
Iter: 1359 loss: 0.000138146512
Iter: 1360 loss: 0.000137265801
Iter: 1361 loss: 0.000142190693
Iter: 1362 loss: 0.000137141295
Iter: 1363 loss: 0.00013627889
Iter: 1364 loss: 0.000138156785
Iter: 1365 loss: 0.000135943425
Iter: 1366 loss: 0.000135201204
Iter: 1367 loss: 0.000135450071
Iter: 1368 loss: 0.000134674716
Iter: 1369 loss: 0.000133871101
Iter: 1370 loss: 0.000140041171
Iter: 1371 loss: 0.00013380812
Iter: 1372 loss: 0.000133120862
Iter: 1373 loss: 0.000141902885
Iter: 1374 loss: 0.000133116
Iter: 1375 loss: 0.000132788758
Iter: 1376 loss: 0.000131861016
Iter: 1377 loss: 0.000136625065
Iter: 1378 loss: 0.000131557754
Iter: 1379 loss: 0.000130552362
Iter: 1380 loss: 0.000131577253
Iter: 1381 loss: 0.000129990614
Iter: 1382 loss: 0.000131094697
Iter: 1383 loss: 0.00012961321
Iter: 1384 loss: 0.000129356427
Iter: 1385 loss: 0.000128770276
Iter: 1386 loss: 0.000136575691
Iter: 1387 loss: 0.0001287341
Iter: 1388 loss: 0.000127934589
Iter: 1389 loss: 0.000132135188
Iter: 1390 loss: 0.000127808875
Iter: 1391 loss: 0.000127290157
Iter: 1392 loss: 0.000126603656
Iter: 1393 loss: 0.000126563202
Iter: 1394 loss: 0.000125732317
Iter: 1395 loss: 0.000125787978
Iter: 1396 loss: 0.000125081919
Iter: 1397 loss: 0.000124218728
Iter: 1398 loss: 0.000127584
Iter: 1399 loss: 0.000124018843
Iter: 1400 loss: 0.000123241305
Iter: 1401 loss: 0.000127263484
Iter: 1402 loss: 0.000123117032
Iter: 1403 loss: 0.000122460813
Iter: 1404 loss: 0.000123309364
Iter: 1405 loss: 0.000122125464
Iter: 1406 loss: 0.00012162132
Iter: 1407 loss: 0.000129414868
Iter: 1408 loss: 0.000121622732
Iter: 1409 loss: 0.000121066034
Iter: 1410 loss: 0.000122818456
Iter: 1411 loss: 0.000120907163
Iter: 1412 loss: 0.000120631797
Iter: 1413 loss: 0.00012004665
Iter: 1414 loss: 0.000129520064
Iter: 1415 loss: 0.000120029152
Iter: 1416 loss: 0.000119481352
Iter: 1417 loss: 0.000125914434
Iter: 1418 loss: 0.000119473494
Iter: 1419 loss: 0.000118890617
Iter: 1420 loss: 0.000121696212
Iter: 1421 loss: 0.000118787924
Iter: 1422 loss: 0.00011845144
Iter: 1423 loss: 0.000117763193
Iter: 1424 loss: 0.000130153538
Iter: 1425 loss: 0.000117750329
Iter: 1426 loss: 0.00011706003
Iter: 1427 loss: 0.000125911203
Iter: 1428 loss: 0.000117054391
Iter: 1429 loss: 0.000116649186
Iter: 1430 loss: 0.000115927905
Iter: 1431 loss: 0.000133841022
Iter: 1432 loss: 0.00011592712
Iter: 1433 loss: 0.000115221883
Iter: 1434 loss: 0.00011749343
Iter: 1435 loss: 0.000115023431
Iter: 1436 loss: 0.000114419643
Iter: 1437 loss: 0.000122414669
Iter: 1438 loss: 0.000114416209
Iter: 1439 loss: 0.000113934213
Iter: 1440 loss: 0.000114216447
Iter: 1441 loss: 0.000113622729
Iter: 1442 loss: 0.000113078204
Iter: 1443 loss: 0.000113784881
Iter: 1444 loss: 0.00011279976
Iter: 1445 loss: 0.000112488517
Iter: 1446 loss: 0.000112452966
Iter: 1447 loss: 0.000112052941
Iter: 1448 loss: 0.000111935238
Iter: 1449 loss: 0.000111692265
Iter: 1450 loss: 0.000111356552
Iter: 1451 loss: 0.000110681438
Iter: 1452 loss: 0.000123400721
Iter: 1453 loss: 0.000110672365
Iter: 1454 loss: 0.000110958608
Iter: 1455 loss: 0.000110442168
Iter: 1456 loss: 0.000110228277
Iter: 1457 loss: 0.000109661378
Iter: 1458 loss: 0.00011391638
Iter: 1459 loss: 0.000109545057
Iter: 1460 loss: 0.000109076405
Iter: 1461 loss: 0.000112456488
Iter: 1462 loss: 0.000109036766
Iter: 1463 loss: 0.000108525688
Iter: 1464 loss: 0.0001102897
Iter: 1465 loss: 0.000108390872
Iter: 1466 loss: 0.000108032968
Iter: 1467 loss: 0.000107414213
Iter: 1468 loss: 0.000107413216
Iter: 1469 loss: 0.000106726991
Iter: 1470 loss: 0.000111247733
Iter: 1471 loss: 0.000106654734
Iter: 1472 loss: 0.000106117164
Iter: 1473 loss: 0.00010772
Iter: 1474 loss: 0.000105951927
Iter: 1475 loss: 0.000105457017
Iter: 1476 loss: 0.000106835374
Iter: 1477 loss: 0.000105298546
Iter: 1478 loss: 0.00010488824
Iter: 1479 loss: 0.000111109555
Iter: 1480 loss: 0.000104888022
Iter: 1481 loss: 0.000104560168
Iter: 1482 loss: 0.000107365478
Iter: 1483 loss: 0.0001045424
Iter: 1484 loss: 0.000104396473
Iter: 1485 loss: 0.000104012819
Iter: 1486 loss: 0.000106787986
Iter: 1487 loss: 0.000103927516
Iter: 1488 loss: 0.000103689599
Iter: 1489 loss: 0.000103673665
Iter: 1490 loss: 0.000103343235
Iter: 1491 loss: 0.000103088882
Iter: 1492 loss: 0.000102983278
Iter: 1493 loss: 0.00010268396
Iter: 1494 loss: 0.000102084894
Iter: 1495 loss: 0.000113719885
Iter: 1496 loss: 0.000102079757
Iter: 1497 loss: 0.000101668942
Iter: 1498 loss: 0.00010160901
Iter: 1499 loss: 0.000101180107
Iter: 1500 loss: 0.000100935409
Iter: 1501 loss: 0.000100749836
Iter: 1502 loss: 0.000100279474
Iter: 1503 loss: 0.000100276593
Iter: 1504 loss: 9.990135e-05
Iter: 1505 loss: 9.93962749e-05
Iter: 1506 loss: 0.000101792262
Iter: 1507 loss: 9.93064677e-05
Iter: 1508 loss: 9.88796673e-05
Iter: 1509 loss: 0.00010130585
Iter: 1510 loss: 9.88201209e-05
Iter: 1511 loss: 9.84475555e-05
Iter: 1512 loss: 9.90489061e-05
Iter: 1513 loss: 9.82756465e-05
Iter: 1514 loss: 9.83564387e-05
Iter: 1515 loss: 9.80750774e-05
Iter: 1516 loss: 9.79404649e-05
Iter: 1517 loss: 9.75910516e-05
Iter: 1518 loss: 0.00010030788
Iter: 1519 loss: 9.75241346e-05
Iter: 1520 loss: 9.71336121e-05
Iter: 1521 loss: 9.90820699e-05
Iter: 1522 loss: 9.70675e-05
Iter: 1523 loss: 9.67224041e-05
Iter: 1524 loss: 0.000101713755
Iter: 1525 loss: 9.67213e-05
Iter: 1526 loss: 9.65620711e-05
Iter: 1527 loss: 9.61733895e-05
Iter: 1528 loss: 0.000100297075
Iter: 1529 loss: 9.61310579e-05
Iter: 1530 loss: 9.57318e-05
Iter: 1531 loss: 9.57558e-05
Iter: 1532 loss: 9.54221e-05
Iter: 1533 loss: 9.50085232e-05
Iter: 1534 loss: 9.95081282e-05
Iter: 1535 loss: 9.49984751e-05
Iter: 1536 loss: 9.45635838e-05
Iter: 1537 loss: 9.54852731e-05
Iter: 1538 loss: 9.4392446e-05
Iter: 1539 loss: 9.39989695e-05
Iter: 1540 loss: 9.39786114e-05
Iter: 1541 loss: 9.36778888e-05
Iter: 1542 loss: 9.32338298e-05
Iter: 1543 loss: 9.78555909e-05
Iter: 1544 loss: 9.32189287e-05
Iter: 1545 loss: 9.2849994e-05
Iter: 1546 loss: 9.29651142e-05
Iter: 1547 loss: 9.25858039e-05
Iter: 1548 loss: 9.22859545e-05
Iter: 1549 loss: 9.22810796e-05
Iter: 1550 loss: 9.19546292e-05
Iter: 1551 loss: 9.2371e-05
Iter: 1552 loss: 9.17875295e-05
Iter: 1553 loss: 9.15622077e-05
Iter: 1554 loss: 9.1284e-05
Iter: 1555 loss: 9.12596661e-05
Iter: 1556 loss: 9.11144161e-05
Iter: 1557 loss: 9.10446761e-05
Iter: 1558 loss: 9.08778238e-05
Iter: 1559 loss: 9.05351699e-05
Iter: 1560 loss: 9.67312662e-05
Iter: 1561 loss: 9.05314373e-05
Iter: 1562 loss: 9.01750172e-05
Iter: 1563 loss: 9.00490486e-05
Iter: 1564 loss: 8.98483559e-05
Iter: 1565 loss: 8.93077813e-05
Iter: 1566 loss: 9.01344465e-05
Iter: 1567 loss: 8.90507654e-05
Iter: 1568 loss: 8.86418e-05
Iter: 1569 loss: 8.86412236e-05
Iter: 1570 loss: 8.82891909e-05
Iter: 1571 loss: 8.98649887e-05
Iter: 1572 loss: 8.82196182e-05
Iter: 1573 loss: 8.79885192e-05
Iter: 1574 loss: 8.81553e-05
Iter: 1575 loss: 8.78461506e-05
Iter: 1576 loss: 8.75283076e-05
Iter: 1577 loss: 8.8588582e-05
Iter: 1578 loss: 8.74418329e-05
Iter: 1579 loss: 8.71794182e-05
Iter: 1580 loss: 8.90349911e-05
Iter: 1581 loss: 8.71544617e-05
Iter: 1582 loss: 8.6896107e-05
Iter: 1583 loss: 8.85725385e-05
Iter: 1584 loss: 8.68686475e-05
Iter: 1585 loss: 8.669417e-05
Iter: 1586 loss: 8.63407622e-05
Iter: 1587 loss: 9.29873131e-05
Iter: 1588 loss: 8.63345922e-05
Iter: 1589 loss: 8.61846493e-05
Iter: 1590 loss: 8.61404551e-05
Iter: 1591 loss: 8.59552383e-05
Iter: 1592 loss: 8.56051411e-05
Iter: 1593 loss: 9.31810646e-05
Iter: 1594 loss: 8.56037368e-05
Iter: 1595 loss: 8.52018275e-05
Iter: 1596 loss: 8.53666279e-05
Iter: 1597 loss: 8.49239295e-05
Iter: 1598 loss: 8.44755341e-05
Iter: 1599 loss: 8.49558055e-05
Iter: 1600 loss: 8.42291556e-05
Iter: 1601 loss: 8.37934058e-05
Iter: 1602 loss: 8.4872765e-05
Iter: 1603 loss: 8.36393e-05
Iter: 1604 loss: 8.34144957e-05
Iter: 1605 loss: 8.33681333e-05
Iter: 1606 loss: 8.31477155e-05
Iter: 1607 loss: 8.28398042e-05
Iter: 1608 loss: 8.28275734e-05
Iter: 1609 loss: 8.2473518e-05
Iter: 1610 loss: 8.37650077e-05
Iter: 1611 loss: 8.23859591e-05
Iter: 1612 loss: 8.20789792e-05
Iter: 1613 loss: 8.33187078e-05
Iter: 1614 loss: 8.20112909e-05
Iter: 1615 loss: 8.17794717e-05
Iter: 1616 loss: 8.35523315e-05
Iter: 1617 loss: 8.1761711e-05
Iter: 1618 loss: 8.15484527e-05
Iter: 1619 loss: 8.13058e-05
Iter: 1620 loss: 8.12739308e-05
Iter: 1621 loss: 8.10600322e-05
Iter: 1622 loss: 8.34772218e-05
Iter: 1623 loss: 8.10564379e-05
Iter: 1624 loss: 8.08085606e-05
Iter: 1625 loss: 8.08870536e-05
Iter: 1626 loss: 8.06342141e-05
Iter: 1627 loss: 8.04226584e-05
Iter: 1628 loss: 8.01802671e-05
Iter: 1629 loss: 8.01529386e-05
Iter: 1630 loss: 7.97744069e-05
Iter: 1631 loss: 7.99814443e-05
Iter: 1632 loss: 7.95261949e-05
Iter: 1633 loss: 7.91070925e-05
Iter: 1634 loss: 7.97309185e-05
Iter: 1635 loss: 7.89053302e-05
Iter: 1636 loss: 7.86384553e-05
Iter: 1637 loss: 7.86256278e-05
Iter: 1638 loss: 7.83321e-05
Iter: 1639 loss: 7.84045e-05
Iter: 1640 loss: 7.81174167e-05
Iter: 1641 loss: 7.78615577e-05
Iter: 1642 loss: 8.00411362e-05
Iter: 1643 loss: 7.78480171e-05
Iter: 1644 loss: 7.76549932e-05
Iter: 1645 loss: 7.8473e-05
Iter: 1646 loss: 7.7611774e-05
Iter: 1647 loss: 7.74709624e-05
Iter: 1648 loss: 7.79339171e-05
Iter: 1649 loss: 7.74308719e-05
Iter: 1650 loss: 7.72480344e-05
Iter: 1651 loss: 7.72738276e-05
Iter: 1652 loss: 7.71071645e-05
Iter: 1653 loss: 7.69356557e-05
Iter: 1654 loss: 7.73426873e-05
Iter: 1655 loss: 7.68733444e-05
Iter: 1656 loss: 7.6643264e-05
Iter: 1657 loss: 7.80047849e-05
Iter: 1658 loss: 7.66129815e-05
Iter: 1659 loss: 7.64959696e-05
Iter: 1660 loss: 7.62146519e-05
Iter: 1661 loss: 7.92610881e-05
Iter: 1662 loss: 7.61847623e-05
Iter: 1663 loss: 7.58072711e-05
Iter: 1664 loss: 7.63426215e-05
Iter: 1665 loss: 7.56213703e-05
Iter: 1666 loss: 7.52501219e-05
Iter: 1667 loss: 7.59079849e-05
Iter: 1668 loss: 7.50871477e-05
Iter: 1669 loss: 7.47266095e-05
Iter: 1670 loss: 7.65248697e-05
Iter: 1671 loss: 7.46642254e-05
Iter: 1672 loss: 7.43857745e-05
Iter: 1673 loss: 7.43850251e-05
Iter: 1674 loss: 7.42188204e-05
Iter: 1675 loss: 7.42155826e-05
Iter: 1676 loss: 7.40840333e-05
Iter: 1677 loss: 7.38827e-05
Iter: 1678 loss: 7.69304897e-05
Iter: 1679 loss: 7.38835515e-05
Iter: 1680 loss: 7.37726805e-05
Iter: 1681 loss: 7.3722018e-05
Iter: 1682 loss: 7.36673828e-05
Iter: 1683 loss: 7.34437199e-05
Iter: 1684 loss: 7.40092219e-05
Iter: 1685 loss: 7.33647612e-05
Iter: 1686 loss: 7.31922191e-05
Iter: 1687 loss: 7.32037e-05
Iter: 1688 loss: 7.30576867e-05
Iter: 1689 loss: 7.28222294e-05
Iter: 1690 loss: 7.59969189e-05
Iter: 1691 loss: 7.28213417e-05
Iter: 1692 loss: 7.27008592e-05
Iter: 1693 loss: 7.23826961e-05
Iter: 1694 loss: 7.47945742e-05
Iter: 1695 loss: 7.23196354e-05
Iter: 1696 loss: 7.19373929e-05
Iter: 1697 loss: 7.30088941e-05
Iter: 1698 loss: 7.18139272e-05
Iter: 1699 loss: 7.14818307e-05
Iter: 1700 loss: 7.19893724e-05
Iter: 1701 loss: 7.13258705e-05
Iter: 1702 loss: 7.09715532e-05
Iter: 1703 loss: 7.1614224e-05
Iter: 1704 loss: 7.08174412e-05
Iter: 1705 loss: 7.06600913e-05
Iter: 1706 loss: 7.06156789e-05
Iter: 1707 loss: 7.04246486e-05
Iter: 1708 loss: 7.01878744e-05
Iter: 1709 loss: 7.01688405e-05
Iter: 1710 loss: 7.0050286e-05
Iter: 1711 loss: 6.99899392e-05
Iter: 1712 loss: 6.98761141e-05
Iter: 1713 loss: 6.97533687e-05
Iter: 1714 loss: 6.97339856e-05
Iter: 1715 loss: 6.94993214e-05
Iter: 1716 loss: 7.03275873e-05
Iter: 1717 loss: 6.94381961e-05
Iter: 1718 loss: 6.92673493e-05
Iter: 1719 loss: 6.92212852e-05
Iter: 1720 loss: 6.91149762e-05
Iter: 1721 loss: 6.89408189e-05
Iter: 1722 loss: 6.89344452e-05
Iter: 1723 loss: 6.88292203e-05
Iter: 1724 loss: 6.8558e-05
Iter: 1725 loss: 7.06944265e-05
Iter: 1726 loss: 6.85066843e-05
Iter: 1727 loss: 6.81526508e-05
Iter: 1728 loss: 6.86465573e-05
Iter: 1729 loss: 6.79786681e-05
Iter: 1730 loss: 6.75907359e-05
Iter: 1731 loss: 6.79439327e-05
Iter: 1732 loss: 6.73656323e-05
Iter: 1733 loss: 6.6956869e-05
Iter: 1734 loss: 6.75742122e-05
Iter: 1735 loss: 6.67612767e-05
Iter: 1736 loss: 6.64104e-05
Iter: 1737 loss: 6.98960721e-05
Iter: 1738 loss: 6.63991086e-05
Iter: 1739 loss: 6.60603e-05
Iter: 1740 loss: 6.82273385e-05
Iter: 1741 loss: 6.60228834e-05
Iter: 1742 loss: 6.58669742e-05
Iter: 1743 loss: 6.67298445e-05
Iter: 1744 loss: 6.58444624e-05
Iter: 1745 loss: 6.56669217e-05
Iter: 1746 loss: 6.60468431e-05
Iter: 1747 loss: 6.55967233e-05
Iter: 1748 loss: 6.54468313e-05
Iter: 1749 loss: 6.56570774e-05
Iter: 1750 loss: 6.5373184e-05
Iter: 1751 loss: 6.52070448e-05
Iter: 1752 loss: 6.52511517e-05
Iter: 1753 loss: 6.5087268e-05
Iter: 1754 loss: 6.49601789e-05
Iter: 1755 loss: 6.49527065e-05
Iter: 1756 loss: 6.48458299e-05
Iter: 1757 loss: 6.45572654e-05
Iter: 1758 loss: 6.63931423e-05
Iter: 1759 loss: 6.44819302e-05
Iter: 1760 loss: 6.41256629e-05
Iter: 1761 loss: 6.47308188e-05
Iter: 1762 loss: 6.39637437e-05
Iter: 1763 loss: 6.36008335e-05
Iter: 1764 loss: 6.46584085e-05
Iter: 1765 loss: 6.34867174e-05
Iter: 1766 loss: 6.31798466e-05
Iter: 1767 loss: 6.37377525e-05
Iter: 1768 loss: 6.30457071e-05
Iter: 1769 loss: 6.27561676e-05
Iter: 1770 loss: 6.36818877e-05
Iter: 1771 loss: 6.26736655e-05
Iter: 1772 loss: 6.25192333e-05
Iter: 1773 loss: 6.24974491e-05
Iter: 1774 loss: 6.23602e-05
Iter: 1775 loss: 6.23338565e-05
Iter: 1776 loss: 6.2242776e-05
Iter: 1777 loss: 6.21110376e-05
Iter: 1778 loss: 6.21048675e-05
Iter: 1779 loss: 6.20440405e-05
Iter: 1780 loss: 6.19023558e-05
Iter: 1781 loss: 6.36935511e-05
Iter: 1782 loss: 6.18917838e-05
Iter: 1783 loss: 6.16810721e-05
Iter: 1784 loss: 6.20313658e-05
Iter: 1785 loss: 6.15846657e-05
Iter: 1786 loss: 6.1455823e-05
Iter: 1787 loss: 6.14560267e-05
Iter: 1788 loss: 6.13285e-05
Iter: 1789 loss: 6.10893694e-05
Iter: 1790 loss: 6.63137762e-05
Iter: 1791 loss: 6.10876887e-05
Iter: 1792 loss: 6.08801565e-05
Iter: 1793 loss: 6.09140079e-05
Iter: 1794 loss: 6.07231923e-05
Iter: 1795 loss: 6.04853558e-05
Iter: 1796 loss: 6.13178854e-05
Iter: 1797 loss: 6.04235538e-05
Iter: 1798 loss: 6.0207e-05
Iter: 1799 loss: 6.05345704e-05
Iter: 1800 loss: 6.01046631e-05
Iter: 1801 loss: 5.98577353e-05
Iter: 1802 loss: 6.02474065e-05
Iter: 1803 loss: 5.97432518e-05
Iter: 1804 loss: 5.95237143e-05
Iter: 1805 loss: 6.27361733e-05
Iter: 1806 loss: 5.95235506e-05
Iter: 1807 loss: 5.93231525e-05
Iter: 1808 loss: 6.03471e-05
Iter: 1809 loss: 5.92902798e-05
Iter: 1810 loss: 5.92182878e-05
Iter: 1811 loss: 5.92080396e-05
Iter: 1812 loss: 5.91512435e-05
Iter: 1813 loss: 5.90110103e-05
Iter: 1814 loss: 6.03535227e-05
Iter: 1815 loss: 5.89921037e-05
Iter: 1816 loss: 5.88311632e-05
Iter: 1817 loss: 5.99428677e-05
Iter: 1818 loss: 5.88155963e-05
Iter: 1819 loss: 5.87281174e-05
Iter: 1820 loss: 5.92265715e-05
Iter: 1821 loss: 5.87159702e-05
Iter: 1822 loss: 5.86000278e-05
Iter: 1823 loss: 5.84644404e-05
Iter: 1824 loss: 5.84491718e-05
Iter: 1825 loss: 5.82972061e-05
Iter: 1826 loss: 5.81489221e-05
Iter: 1827 loss: 5.81180757e-05
Iter: 1828 loss: 5.7864243e-05
Iter: 1829 loss: 5.84568443e-05
Iter: 1830 loss: 5.77710816e-05
Iter: 1831 loss: 5.75074519e-05
Iter: 1832 loss: 5.79954467e-05
Iter: 1833 loss: 5.73934158e-05
Iter: 1834 loss: 5.71244673e-05
Iter: 1835 loss: 5.78695472e-05
Iter: 1836 loss: 5.70376833e-05
Iter: 1837 loss: 5.68179967e-05
Iter: 1838 loss: 5.85882226e-05
Iter: 1839 loss: 5.68023752e-05
Iter: 1840 loss: 5.66428644e-05
Iter: 1841 loss: 5.91124845e-05
Iter: 1842 loss: 5.66435629e-05
Iter: 1843 loss: 5.6571982e-05
Iter: 1844 loss: 5.65718365e-05
Iter: 1845 loss: 5.65091214e-05
Iter: 1846 loss: 5.63650647e-05
Iter: 1847 loss: 5.8263111e-05
Iter: 1848 loss: 5.63554931e-05
Iter: 1849 loss: 5.62265e-05
Iter: 1850 loss: 5.73721554e-05
Iter: 1851 loss: 5.62206806e-05
Iter: 1852 loss: 5.6126144e-05
Iter: 1853 loss: 5.62396781e-05
Iter: 1854 loss: 5.60760091e-05
Iter: 1855 loss: 5.59066393e-05
Iter: 1856 loss: 5.59354266e-05
Iter: 1857 loss: 5.57787353e-05
Iter: 1858 loss: 5.56247833e-05
Iter: 1859 loss: 5.54107755e-05
Iter: 1860 loss: 5.54022336e-05
Iter: 1861 loss: 5.51230296e-05
Iter: 1862 loss: 5.59139626e-05
Iter: 1863 loss: 5.50341501e-05
Iter: 1864 loss: 5.47833188e-05
Iter: 1865 loss: 5.54485414e-05
Iter: 1866 loss: 5.4698281e-05
Iter: 1867 loss: 5.44748764e-05
Iter: 1868 loss: 5.51202866e-05
Iter: 1869 loss: 5.44050781e-05
Iter: 1870 loss: 5.42054477e-05
Iter: 1871 loss: 5.50322802e-05
Iter: 1872 loss: 5.41620284e-05
Iter: 1873 loss: 5.40140099e-05
Iter: 1874 loss: 5.40139e-05
Iter: 1875 loss: 5.39225221e-05
Iter: 1876 loss: 5.50830628e-05
Iter: 1877 loss: 5.39231696e-05
Iter: 1878 loss: 5.3833206e-05
Iter: 1879 loss: 5.36779116e-05
Iter: 1880 loss: 5.3677486e-05
Iter: 1881 loss: 5.35447107e-05
Iter: 1882 loss: 5.41488589e-05
Iter: 1883 loss: 5.35196232e-05
Iter: 1884 loss: 5.3389198e-05
Iter: 1885 loss: 5.37115047e-05
Iter: 1886 loss: 5.33434468e-05
Iter: 1887 loss: 5.31864862e-05
Iter: 1888 loss: 5.36621228e-05
Iter: 1889 loss: 5.31391634e-05
Iter: 1890 loss: 5.30433499e-05
Iter: 1891 loss: 5.28481869e-05
Iter: 1892 loss: 5.64365873e-05
Iter: 1893 loss: 5.28451274e-05
Iter: 1894 loss: 5.26159129e-05
Iter: 1895 loss: 5.31306869e-05
Iter: 1896 loss: 5.25293799e-05
Iter: 1897 loss: 5.22825721e-05
Iter: 1898 loss: 5.27212651e-05
Iter: 1899 loss: 5.21738039e-05
Iter: 1900 loss: 5.19229288e-05
Iter: 1901 loss: 5.2722251e-05
Iter: 1902 loss: 5.18513261e-05
Iter: 1903 loss: 5.1629635e-05
Iter: 1904 loss: 5.23935523e-05
Iter: 1905 loss: 5.15717438e-05
Iter: 1906 loss: 5.14155181e-05
Iter: 1907 loss: 5.14161111e-05
Iter: 1908 loss: 5.13169471e-05
Iter: 1909 loss: 5.1316747e-05
Iter: 1910 loss: 5.12264705e-05
Iter: 1911 loss: 5.11539547e-05
Iter: 1912 loss: 5.11265971e-05
Iter: 1913 loss: 5.10345781e-05
Iter: 1914 loss: 5.11944454e-05
Iter: 1915 loss: 5.09944439e-05
Iter: 1916 loss: 5.08879602e-05
Iter: 1917 loss: 5.13657615e-05
Iter: 1918 loss: 5.08663397e-05
Iter: 1919 loss: 5.07759432e-05
Iter: 1920 loss: 5.1174331e-05
Iter: 1921 loss: 5.07587392e-05
Iter: 1922 loss: 5.069502e-05
Iter: 1923 loss: 5.05440512e-05
Iter: 1924 loss: 5.23728777e-05
Iter: 1925 loss: 5.05322605e-05
Iter: 1926 loss: 5.03490046e-05
Iter: 1927 loss: 5.06484066e-05
Iter: 1928 loss: 5.02650582e-05
Iter: 1929 loss: 5.00478927e-05
Iter: 1930 loss: 5.0615592e-05
Iter: 1931 loss: 4.99746311e-05
Iter: 1932 loss: 4.97859728e-05
Iter: 1933 loss: 5.03305528e-05
Iter: 1934 loss: 4.97283763e-05
Iter: 1935 loss: 4.95625063e-05
Iter: 1936 loss: 5.0212213e-05
Iter: 1937 loss: 4.95234e-05
Iter: 1938 loss: 4.93952e-05
Iter: 1939 loss: 5.06496071e-05
Iter: 1940 loss: 4.93914558e-05
Iter: 1941 loss: 4.93054067e-05
Iter: 1942 loss: 4.93056577e-05
Iter: 1943 loss: 4.92255494e-05
Iter: 1944 loss: 4.92780964e-05
Iter: 1945 loss: 4.91739156e-05
Iter: 1946 loss: 4.911203e-05
Iter: 1947 loss: 4.90404782e-05
Iter: 1948 loss: 4.90323764e-05
Iter: 1949 loss: 4.89009326e-05
Iter: 1950 loss: 4.96494285e-05
Iter: 1951 loss: 4.88820951e-05
Iter: 1952 loss: 4.87882e-05
Iter: 1953 loss: 4.91882456e-05
Iter: 1954 loss: 4.87682482e-05
Iter: 1955 loss: 4.86821446e-05
Iter: 1956 loss: 4.85091114e-05
Iter: 1957 loss: 5.17651642e-05
Iter: 1958 loss: 4.85066921e-05
Iter: 1959 loss: 4.83258846e-05
Iter: 1960 loss: 4.85306955e-05
Iter: 1961 loss: 4.82294417e-05
Iter: 1962 loss: 4.80201961e-05
Iter: 1963 loss: 4.88610422e-05
Iter: 1964 loss: 4.79728405e-05
Iter: 1965 loss: 4.78085603e-05
Iter: 1966 loss: 4.81196803e-05
Iter: 1967 loss: 4.77395406e-05
Iter: 1968 loss: 4.75748893e-05
Iter: 1969 loss: 4.81674506e-05
Iter: 1970 loss: 4.75326597e-05
Iter: 1971 loss: 4.7385729e-05
Iter: 1972 loss: 4.82294818e-05
Iter: 1973 loss: 4.73663495e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.4
+ date
Sun Nov  8 14:46:23 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a76c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a5c5e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a6a8d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a5d3d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a5d3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a51aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a4c9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a453598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a483158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a560510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a48e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a4987b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a498ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623da33620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a40c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623da33950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a41c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d9dcea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d9e1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d9f5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d8e1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d8b06a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d8666a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d885840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d8851e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d972378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d949840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d947950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d947730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d7d4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d7e27b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d779048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d779950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d75f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d6e9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d6c1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.41227785
test_loss: 0.4082657
train_loss: 0.4040471
test_loss: 0.4039109
train_loss: 0.41429153
test_loss: 0.40393004
train_loss: 0.41224855
test_loss: 0.40395027
train_loss: 0.40077212
test_loss: 0.403886
train_loss: 0.40124243
test_loss: 0.40386674
train_loss: 0.40221363
test_loss: 0.40382835
train_loss: 0.40664703
test_loss: 0.40379897
train_loss: 0.40434477
test_loss: 0.403765
train_loss: 0.40362495
test_loss: 0.4036975
train_loss: 0.40908366
test_loss: 0.4036396
train_loss: 0.4039861
test_loss: 0.40363654
train_loss: 0.3977197
test_loss: 0.40357807
train_loss: 0.4009234
test_loss: 0.40353853
train_loss: 0.39998043
test_loss: 0.4034101
train_loss: 0.40735582
test_loss: 0.4033965
train_loss: 0.4040277
test_loss: 0.4033457
train_loss: 0.40260717
test_loss: 0.40327376
train_loss: 0.40210497
test_loss: 0.40315408
train_loss: 0.40239277
test_loss: 0.40312657
train_loss: 0.40801305
test_loss: 0.4030356
train_loss: 0.40468532
test_loss: 0.40294838
train_loss: 0.40918738
test_loss: 0.40284228
train_loss: 0.3971514
test_loss: 0.40270728
train_loss: 0.40230167
test_loss: 0.4026275
train_loss: 0.39949667
test_loss: 0.40249917
train_loss: 0.40636083
test_loss: 0.4024004
train_loss: 0.39958578
test_loss: 0.40230092
train_loss: 0.39530483
test_loss: 0.4021356
train_loss: 0.40488437
test_loss: 0.40200752
train_loss: 0.39976576
test_loss: 0.40187103
train_loss: 0.40373722
test_loss: 0.40172267
train_loss: 0.40870675
test_loss: 0.4015335
train_loss: 0.39141238
test_loss: 0.40140536
train_loss: 0.3961532
test_loss: 0.4012443
train_loss: 0.38994056
test_loss: 0.40102753
train_loss: 0.39950156
test_loss: 0.40081838
train_loss: 0.4006749
test_loss: 0.40059653
train_loss: 0.40483814
test_loss: 0.40043435
train_loss: 0.40315008
test_loss: 0.40015206
train_loss: 0.39484134
test_loss: 0.39992058
train_loss: 0.40096188
test_loss: 0.39965448
train_loss: 0.39975166
test_loss: 0.3993797
train_loss: 0.40038294
test_loss: 0.39911595
train_loss: 0.39360863
test_loss: 0.3988375
train_loss: 0.40356696
test_loss: 0.3985451
train_loss: 0.4008484
test_loss: 0.3981807
train_loss: 0.40121934
test_loss: 0.3978332
train_loss: 0.40472424
test_loss: 0.39750633
train_loss: 0.39354452
test_loss: 0.3971158
train_loss: 0.39142564
test_loss: 0.39674315
train_loss: 0.39736956
test_loss: 0.39631215
train_loss: 0.39958522
test_loss: 0.395867
train_loss: 0.39481175
test_loss: 0.39541733
train_loss: 0.39321345
test_loss: 0.39488113
train_loss: 0.39583284
test_loss: 0.39441663
train_loss: 0.3883125
test_loss: 0.39386478
train_loss: 0.39141172
test_loss: 0.39333338
train_loss: 0.39605972
test_loss: 0.39276654
train_loss: 0.39496577
test_loss: 0.3921382
train_loss: 0.3919925
test_loss: 0.39143878
train_loss: 0.3892085
test_loss: 0.39081708
train_loss: 0.39711806
test_loss: 0.39010608
train_loss: 0.38185486
test_loss: 0.38937074
train_loss: 0.3907802
test_loss: 0.38859162
train_loss: 0.38965416
test_loss: 0.3877621
train_loss: 0.3848688
test_loss: 0.38691851
train_loss: 0.3854093
test_loss: 0.38600096
train_loss: 0.38189098
test_loss: 0.38508224
train_loss: 0.38514575
test_loss: 0.3841057
train_loss: 0.37883878
test_loss: 0.38304648
train_loss: 0.37868792
test_loss: 0.38195413
train_loss: 0.38286263
test_loss: 0.38083294
train_loss: 0.3746828
test_loss: 0.37965018
train_loss: 0.38049233
test_loss: 0.3783341
train_loss: 0.3822375
test_loss: 0.37705812
train_loss: 0.3796229
test_loss: 0.37568444
train_loss: 0.37610438
test_loss: 0.37425762
train_loss: 0.36814526
test_loss: 0.37275556
train_loss: 0.37497655
test_loss: 0.3711827
train_loss: 0.36535916
test_loss: 0.3695413
train_loss: 0.36498773
test_loss: 0.36781797
train_loss: 0.3759227
test_loss: 0.36603132
train_loss: 0.3653143
test_loss: 0.36414838
train_loss: 0.36637586
test_loss: 0.36217672
train_loss: 0.36443648
test_loss: 0.36007634
train_loss: 0.35907173
test_loss: 0.35796964
train_loss: 0.35704294
test_loss: 0.35571888
train_loss: 0.34643638
test_loss: 0.35337022
train_loss: 0.34899065
test_loss: 0.35088632
train_loss: 0.34298104
test_loss: 0.3483262
train_loss: 0.34606734
test_loss: 0.3456597
train_loss: 0.3423369
test_loss: 0.34292325
train_loss: 0.335162
test_loss: 0.3399908
train_loss: 0.3373921
test_loss: 0.3369422
train_loss: 0.32950234
test_loss: 0.33374462
train_loss: 0.323861
test_loss: 0.33045125
train_loss: 0.32479388
test_loss: 0.32700795
train_loss: 0.31898737
test_loss: 0.32346487
train_loss: 0.31761304
test_loss: 0.31972498
train_loss: 0.31231686
test_loss: 0.31581038
train_loss: 0.3128206
test_loss: 0.3117593
train_loss: 0.31004807
test_loss: 0.3075784
train_loss: 0.3021256
test_loss: 0.30314693
train_loss: 0.29777467
test_loss: 0.29857567
train_loss: 0.30042565
test_loss: 0.29381964
train_loss: 0.28879708
test_loss: 0.28884268
train_loss: 0.28360298
test_loss: 0.28368366
train_loss: 0.28352028
test_loss: 0.27830154
train_loss: 0.27289268
test_loss: 0.27271488
train_loss: 0.26548937
test_loss: 0.2668868
train_loss: 0.2575003
test_loss: 0.26078144
train_loss: 0.2608684
test_loss: 0.25439626
train_loss: 0.24742454
test_loss: 0.2477854
train_loss: 0.23842645
test_loss: 0.24080935
train_loss: 0.23152012
test_loss: 0.23358113
train_loss: 0.22275421
test_loss: 0.22598492
train_loss: 0.22011997
test_loss: 0.21809861
train_loss: 0.20813034
test_loss: 0.20988366
train_loss: 0.19854742
test_loss: 0.20136683
train_loss: 0.1952934
test_loss: 0.19255048
train_loss: 0.18142685
test_loss: 0.18350591
train_loss: 0.17004187
test_loss: 0.1743147
train_loss: 0.16582485
test_loss: 0.16508563
train_loss: 0.15721896
test_loss: 0.15598422
train_loss: 0.14675064
test_loss: 0.14718965
train_loss: 0.1375018
test_loss: 0.13885118
train_loss: 0.13038617
test_loss: 0.1311195
train_loss: 0.12272507
test_loss: 0.1240436
train_loss: 0.1166116
test_loss: 0.11768691
train_loss: 0.111613825
test_loss: 0.111995496
train_loss: 0.10641123
test_loss: 0.10694598
train_loss: 0.10558421
test_loss: 0.10250462
train_loss: 0.09676093
test_loss: 0.09863723
train_loss: 0.0953398
test_loss: 0.09531148
train_loss: 0.09271586
test_loss: 0.09245911
train_loss: 0.090756975
test_loss: 0.09003472
train_loss: 0.08664426
test_loss: 0.08798351
train_loss: 0.08563602
test_loss: 0.086255714
train_loss: 0.08401022
test_loss: 0.08479298
train_loss: 0.08435223
test_loss: 0.0835681
train_loss: 0.083201334
test_loss: 0.08252132
train_loss: 0.080240846
test_loss: 0.0816443
train_loss: 0.080833346
test_loss: 0.080873825
train_loss: 0.07907588
test_loss: 0.080210544
train_loss: 0.07967731
test_loss: 0.079610445
train_loss: 0.07999088
test_loss: 0.07907913
train_loss: 0.077032186
test_loss: 0.078588516
train_loss: 0.07688173
test_loss: 0.07812951
train_loss: 0.076900706
test_loss: 0.07769064
train_loss: 0.077897415
test_loss: 0.0772804
train_loss: 0.076946445
test_loss: 0.07686684
train_loss: 0.07715712
test_loss: 0.07646242
train_loss: 0.07641668
test_loss: 0.07606035
train_loss: 0.07604802
test_loss: 0.07566667
train_loss: 0.07397553
test_loss: 0.075278305
train_loss: 0.074443914
test_loss: 0.07487156
train_loss: 0.07549229
test_loss: 0.074471675
train_loss: 0.07555216
test_loss: 0.074054316
train_loss: 0.074010015
test_loss: 0.07365817
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.4/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c6a62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c795d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c795c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c6f8ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c707488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c67e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c637c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c5de7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c5f2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c5b0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c5b0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c580f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c577730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c50dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c50d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c4ef8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c495598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c495d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c4619d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c461488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c461158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c3e3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c3ab950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c3bc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c3bc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c35a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31443fd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3144422950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31443a7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31443d9400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314437a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31443a1158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3144336840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3144352620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f311fe908c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f311fe362f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.00910227746
Iter: 2 loss: 0.0089699775
Iter: 3 loss: 0.0100624356
Iter: 4 loss: 0.00896180794
Iter: 5 loss: 0.0089186
Iter: 6 loss: 0.00891487394
Iter: 7 loss: 0.00887179
Iter: 8 loss: 0.0087988656
Iter: 9 loss: 0.00879876129
Iter: 10 loss: 0.00871505
Iter: 11 loss: 0.0087860534
Iter: 12 loss: 0.00866632629
Iter: 13 loss: 0.00852621906
Iter: 14 loss: 0.00918190554
Iter: 15 loss: 0.0085028708
Iter: 16 loss: 0.00839066692
Iter: 17 loss: 0.00812191702
Iter: 18 loss: 0.0114598107
Iter: 19 loss: 0.00809687
Iter: 20 loss: 0.00785575435
Iter: 21 loss: 0.0111922147
Iter: 22 loss: 0.00785539951
Iter: 23 loss: 0.00763355289
Iter: 24 loss: 0.00780512951
Iter: 25 loss: 0.0075126295
Iter: 26 loss: 0.00735478895
Iter: 27 loss: 0.00899522938
Iter: 28 loss: 0.00734686619
Iter: 29 loss: 0.00720881764
Iter: 30 loss: 0.00754376128
Iter: 31 loss: 0.00716473581
Iter: 32 loss: 0.00707468856
Iter: 33 loss: 0.00705687329
Iter: 34 loss: 0.00700063072
Iter: 35 loss: 0.0069395816
Iter: 36 loss: 0.00697217695
Iter: 37 loss: 0.00690024765
Iter: 38 loss: 0.00686646672
Iter: 39 loss: 0.00695479289
Iter: 40 loss: 0.00685513252
Iter: 41 loss: 0.00681141065
Iter: 42 loss: 0.0073201484
Iter: 43 loss: 0.00681039
Iter: 44 loss: 0.00677411584
Iter: 45 loss: 0.00682338886
Iter: 46 loss: 0.00675644958
Iter: 47 loss: 0.00671755569
Iter: 48 loss: 0.00680492353
Iter: 49 loss: 0.00670283427
Iter: 50 loss: 0.00665169396
Iter: 51 loss: 0.00672010705
Iter: 52 loss: 0.00662585814
Iter: 53 loss: 0.0065676244
Iter: 54 loss: 0.00655284338
Iter: 55 loss: 0.0065169814
Iter: 56 loss: 0.00645280723
Iter: 57 loss: 0.0064056241
Iter: 58 loss: 0.00638434151
Iter: 59 loss: 0.0063073351
Iter: 60 loss: 0.0064717452
Iter: 61 loss: 0.00627671229
Iter: 62 loss: 0.00619100733
Iter: 63 loss: 0.00664061541
Iter: 64 loss: 0.00617869571
Iter: 65 loss: 0.00612725783
Iter: 66 loss: 0.00612865807
Iter: 67 loss: 0.00608772133
Iter: 68 loss: 0.00603241101
Iter: 69 loss: 0.00611212244
Iter: 70 loss: 0.00600218773
Iter: 71 loss: 0.00594219472
Iter: 72 loss: 0.00647506909
Iter: 73 loss: 0.00593881216
Iter: 74 loss: 0.00587498629
Iter: 75 loss: 0.00640340941
Iter: 76 loss: 0.00587123679
Iter: 77 loss: 0.00580976531
Iter: 78 loss: 0.00630124239
Iter: 79 loss: 0.00580613269
Iter: 80 loss: 0.00577145815
Iter: 81 loss: 0.00590776466
Iter: 82 loss: 0.00576359034
Iter: 83 loss: 0.00572892
Iter: 84 loss: 0.00567057263
Iter: 85 loss: 0.00567039941
Iter: 86 loss: 0.00559975393
Iter: 87 loss: 0.00569472555
Iter: 88 loss: 0.00556315109
Iter: 89 loss: 0.00548265455
Iter: 90 loss: 0.00537680183
Iter: 91 loss: 0.00537065137
Iter: 92 loss: 0.00524747651
Iter: 93 loss: 0.00723154657
Iter: 94 loss: 0.00524436636
Iter: 95 loss: 0.0051170243
Iter: 96 loss: 0.00511147315
Iter: 97 loss: 0.00501003303
Iter: 98 loss: 0.00606299145
Iter: 99 loss: 0.00500650797
Iter: 100 loss: 0.00492367521
Iter: 101 loss: 0.00535297114
Iter: 102 loss: 0.00490633771
Iter: 103 loss: 0.00481224433
Iter: 104 loss: 0.00481207483
Iter: 105 loss: 0.00475475565
Iter: 106 loss: 0.00490507856
Iter: 107 loss: 0.00473573804
Iter: 108 loss: 0.00466297707
Iter: 109 loss: 0.00590797747
Iter: 110 loss: 0.00466184039
Iter: 111 loss: 0.0046094358
Iter: 112 loss: 0.0046088947
Iter: 113 loss: 0.0045606941
Iter: 114 loss: 0.00464046188
Iter: 115 loss: 0.00453842245
Iter: 116 loss: 0.00446114689
Iter: 117 loss: 0.00445408234
Iter: 118 loss: 0.00439530611
Iter: 119 loss: 0.00431843894
Iter: 120 loss: 0.0043179607
Iter: 121 loss: 0.00425442867
Iter: 122 loss: 0.00446285727
Iter: 123 loss: 0.00423369789
Iter: 124 loss: 0.00416001771
Iter: 125 loss: 0.00419564452
Iter: 126 loss: 0.00411036331
Iter: 127 loss: 0.00405519269
Iter: 128 loss: 0.00430276245
Iter: 129 loss: 0.00404073391
Iter: 130 loss: 0.00398571556
Iter: 131 loss: 0.00398188969
Iter: 132 loss: 0.0039197728
Iter: 133 loss: 0.00496365409
Iter: 134 loss: 0.00391974533
Iter: 135 loss: 0.00385002326
Iter: 136 loss: 0.00398254814
Iter: 137 loss: 0.00381813059
Iter: 138 loss: 0.00375656
Iter: 139 loss: 0.003933548
Iter: 140 loss: 0.00373656442
Iter: 141 loss: 0.00366662303
Iter: 142 loss: 0.0040651029
Iter: 143 loss: 0.00365389
Iter: 144 loss: 0.0035846394
Iter: 145 loss: 0.00397653878
Iter: 146 loss: 0.0035731406
Iter: 147 loss: 0.00352151
Iter: 148 loss: 0.00352033833
Iter: 149 loss: 0.00347281178
Iter: 150 loss: 0.00368426833
Iter: 151 loss: 0.00346045592
Iter: 152 loss: 0.00339604751
Iter: 153 loss: 0.00396318175
Iter: 154 loss: 0.0033929504
Iter: 155 loss: 0.00334904157
Iter: 156 loss: 0.00355604361
Iter: 157 loss: 0.00334043521
Iter: 158 loss: 0.00331122
Iter: 159 loss: 0.00328800292
Iter: 160 loss: 0.003279394
Iter: 161 loss: 0.00322130602
Iter: 162 loss: 0.00322950026
Iter: 163 loss: 0.00317658531
Iter: 164 loss: 0.00311610545
Iter: 165 loss: 0.00332369911
Iter: 166 loss: 0.00309772743
Iter: 167 loss: 0.00306540821
Iter: 168 loss: 0.00305757439
Iter: 169 loss: 0.00303083053
Iter: 170 loss: 0.00309958775
Iter: 171 loss: 0.00302163488
Iter: 172 loss: 0.00299778115
Iter: 173 loss: 0.00311776879
Iter: 174 loss: 0.0029929257
Iter: 175 loss: 0.00296682375
Iter: 176 loss: 0.00322618149
Iter: 177 loss: 0.00296601141
Iter: 178 loss: 0.00293828291
Iter: 179 loss: 0.00306394976
Iter: 180 loss: 0.00293309754
Iter: 181 loss: 0.00291267
Iter: 182 loss: 0.00303251692
Iter: 183 loss: 0.00290972739
Iter: 184 loss: 0.00289048417
Iter: 185 loss: 0.00287812296
Iter: 186 loss: 0.00287074409
Iter: 187 loss: 0.00283516175
Iter: 188 loss: 0.00317364186
Iter: 189 loss: 0.00283379271
Iter: 190 loss: 0.00281213084
Iter: 191 loss: 0.00285988767
Iter: 192 loss: 0.00280353543
Iter: 193 loss: 0.0027708211
Iter: 194 loss: 0.00281756744
Iter: 195 loss: 0.00275483681
Iter: 196 loss: 0.00272472
Iter: 197 loss: 0.0028554881
Iter: 198 loss: 0.00271847565
Iter: 199 loss: 0.00268609449
Iter: 200 loss: 0.00276954914
Iter: 201 loss: 0.00267467648
Iter: 202 loss: 0.00264488487
Iter: 203 loss: 0.00292709609
Iter: 204 loss: 0.00264369929
Iter: 205 loss: 0.00261637243
Iter: 206 loss: 0.00274165324
Iter: 207 loss: 0.00261088298
Iter: 208 loss: 0.00258882344
Iter: 209 loss: 0.00260861684
Iter: 210 loss: 0.0025760876
Iter: 211 loss: 0.0025485626
Iter: 212 loss: 0.00262700533
Iter: 213 loss: 0.00253978185
Iter: 214 loss: 0.00251658773
Iter: 215 loss: 0.00276407786
Iter: 216 loss: 0.00251597539
Iter: 217 loss: 0.00249811681
Iter: 218 loss: 0.00251414394
Iter: 219 loss: 0.00248750835
Iter: 220 loss: 0.00246924115
Iter: 221 loss: 0.00246910634
Iter: 222 loss: 0.00245418074
Iter: 223 loss: 0.00250079622
Iter: 224 loss: 0.00244999886
Iter: 225 loss: 0.00243874057
Iter: 226 loss: 0.00242069364
Iter: 227 loss: 0.00242053438
Iter: 228 loss: 0.00240113586
Iter: 229 loss: 0.00240471098
Iter: 230 loss: 0.00238650455
Iter: 231 loss: 0.00236928626
Iter: 232 loss: 0.00237110257
Iter: 233 loss: 0.00235544797
Iter: 234 loss: 0.00233259285
Iter: 235 loss: 0.00235402025
Iter: 236 loss: 0.00231947564
Iter: 237 loss: 0.00228881184
Iter: 238 loss: 0.002411291
Iter: 239 loss: 0.0022802914
Iter: 240 loss: 0.00229315553
Iter: 241 loss: 0.00226833369
Iter: 242 loss: 0.00225814898
Iter: 243 loss: 0.00227863318
Iter: 244 loss: 0.00225401437
Iter: 245 loss: 0.00224344013
Iter: 246 loss: 0.00227020588
Iter: 247 loss: 0.00223957957
Iter: 248 loss: 0.00223275973
Iter: 249 loss: 0.00232988
Iter: 250 loss: 0.00223274459
Iter: 251 loss: 0.00222647516
Iter: 252 loss: 0.00222222507
Iter: 253 loss: 0.00221993728
Iter: 254 loss: 0.00220796908
Iter: 255 loss: 0.00226887269
Iter: 256 loss: 0.00220592716
Iter: 257 loss: 0.00219568261
Iter: 258 loss: 0.00233266782
Iter: 259 loss: 0.00219551148
Iter: 260 loss: 0.00218738476
Iter: 261 loss: 0.0021957322
Iter: 262 loss: 0.00218286528
Iter: 263 loss: 0.00217400538
Iter: 264 loss: 0.00217209151
Iter: 265 loss: 0.0021603757
Iter: 266 loss: 0.00216100388
Iter: 267 loss: 0.00215105806
Iter: 268 loss: 0.0021433963
Iter: 269 loss: 0.00215290021
Iter: 270 loss: 0.00213943841
Iter: 271 loss: 0.00213192659
Iter: 272 loss: 0.00216599
Iter: 273 loss: 0.00213006535
Iter: 274 loss: 0.00212404085
Iter: 275 loss: 0.00212402502
Iter: 276 loss: 0.00211878773
Iter: 277 loss: 0.00214575836
Iter: 278 loss: 0.00211794092
Iter: 279 loss: 0.00211348291
Iter: 280 loss: 0.00211564614
Iter: 281 loss: 0.0021104319
Iter: 282 loss: 0.00210483395
Iter: 283 loss: 0.00213414314
Iter: 284 loss: 0.00210394361
Iter: 285 loss: 0.00209575333
Iter: 286 loss: 0.00211541611
Iter: 287 loss: 0.00209272117
Iter: 288 loss: 0.00208686106
Iter: 289 loss: 0.00214614114
Iter: 290 loss: 0.00208666828
Iter: 291 loss: 0.00208151317
Iter: 292 loss: 0.00207552081
Iter: 293 loss: 0.00207482977
Iter: 294 loss: 0.00206704065
Iter: 295 loss: 0.00210854271
Iter: 296 loss: 0.00206582178
Iter: 297 loss: 0.0020620618
Iter: 298 loss: 0.00206110859
Iter: 299 loss: 0.00205828692
Iter: 300 loss: 0.00205800729
Iter: 301 loss: 0.00205599167
Iter: 302 loss: 0.00205192761
Iter: 303 loss: 0.0020517658
Iter: 304 loss: 0.00204671593
Iter: 305 loss: 0.00204604072
Iter: 306 loss: 0.00204248121
Iter: 307 loss: 0.00203519594
Iter: 308 loss: 0.0020628965
Iter: 309 loss: 0.00203346647
Iter: 310 loss: 0.00202810741
Iter: 311 loss: 0.00202790438
Iter: 312 loss: 0.00202475023
Iter: 313 loss: 0.00205540471
Iter: 314 loss: 0.00202463334
Iter: 315 loss: 0.00202112738
Iter: 316 loss: 0.00201972621
Iter: 317 loss: 0.002017861
Iter: 318 loss: 0.00201465283
Iter: 319 loss: 0.00203372817
Iter: 320 loss: 0.00201424072
Iter: 321 loss: 0.00201011868
Iter: 322 loss: 0.0020096451
Iter: 323 loss: 0.00200664881
Iter: 324 loss: 0.00200358499
Iter: 325 loss: 0.00204604911
Iter: 326 loss: 0.00200357824
Iter: 327 loss: 0.00200156262
Iter: 328 loss: 0.00200368138
Iter: 329 loss: 0.00200046622
Iter: 330 loss: 0.00199728087
Iter: 331 loss: 0.00200425694
Iter: 332 loss: 0.00199596328
Iter: 333 loss: 0.00199237932
Iter: 334 loss: 0.00203293655
Iter: 335 loss: 0.00199231436
Iter: 336 loss: 0.00198901934
Iter: 337 loss: 0.00198654342
Iter: 338 loss: 0.0019854675
Iter: 339 loss: 0.0019816
Iter: 340 loss: 0.00198339927
Iter: 341 loss: 0.00197899342
Iter: 342 loss: 0.00197561621
Iter: 343 loss: 0.00198338344
Iter: 344 loss: 0.00197435566
Iter: 345 loss: 0.00197214051
Iter: 346 loss: 0.0019720553
Iter: 347 loss: 0.001969971
Iter: 348 loss: 0.00197434332
Iter: 349 loss: 0.00196914887
Iter: 350 loss: 0.00196738448
Iter: 351 loss: 0.00196843268
Iter: 352 loss: 0.00196624058
Iter: 353 loss: 0.001963038
Iter: 354 loss: 0.00196490251
Iter: 355 loss: 0.00196096743
Iter: 356 loss: 0.00195880793
Iter: 357 loss: 0.00196438958
Iter: 358 loss: 0.00195805985
Iter: 359 loss: 0.00195497833
Iter: 360 loss: 0.00195315434
Iter: 361 loss: 0.00195187202
Iter: 362 loss: 0.00194877444
Iter: 363 loss: 0.00194879621
Iter: 364 loss: 0.00194632111
Iter: 365 loss: 0.00194289919
Iter: 366 loss: 0.00199197698
Iter: 367 loss: 0.00194287859
Iter: 368 loss: 0.00193993642
Iter: 369 loss: 0.00193885271
Iter: 370 loss: 0.00193720986
Iter: 371 loss: 0.00193341984
Iter: 372 loss: 0.00198002439
Iter: 373 loss: 0.00193338876
Iter: 374 loss: 0.00193160982
Iter: 375 loss: 0.00192856253
Iter: 376 loss: 0.00192855834
Iter: 377 loss: 0.00192483782
Iter: 378 loss: 0.00193594606
Iter: 379 loss: 0.00192371756
Iter: 380 loss: 0.00192035758
Iter: 381 loss: 0.00192351534
Iter: 382 loss: 0.00191838341
Iter: 383 loss: 0.00191433681
Iter: 384 loss: 0.00193254778
Iter: 385 loss: 0.00191354309
Iter: 386 loss: 0.00191047927
Iter: 387 loss: 0.00191039476
Iter: 388 loss: 0.00190899277
Iter: 389 loss: 0.00190559379
Iter: 390 loss: 0.00195362256
Iter: 391 loss: 0.00190527958
Iter: 392 loss: 0.00190099317
Iter: 393 loss: 0.00190080609
Iter: 394 loss: 0.0018971283
Iter: 395 loss: 0.00193482009
Iter: 396 loss: 0.00189703784
Iter: 397 loss: 0.0018941795
Iter: 398 loss: 0.00189417938
Iter: 399 loss: 0.00189152989
Iter: 400 loss: 0.001888728
Iter: 401 loss: 0.0018882493
Iter: 402 loss: 0.00188541319
Iter: 403 loss: 0.00192610617
Iter: 404 loss: 0.00188541168
Iter: 405 loss: 0.00188324158
Iter: 406 loss: 0.00188974082
Iter: 407 loss: 0.00188252
Iter: 408 loss: 0.0018794986
Iter: 409 loss: 0.00188298151
Iter: 410 loss: 0.00187787367
Iter: 411 loss: 0.00187502569
Iter: 412 loss: 0.00189249055
Iter: 413 loss: 0.00187470193
Iter: 414 loss: 0.00187159027
Iter: 415 loss: 0.00188192679
Iter: 416 loss: 0.00187062216
Iter: 417 loss: 0.00186826033
Iter: 418 loss: 0.00187026104
Iter: 419 loss: 0.00186687964
Iter: 420 loss: 0.00186423468
Iter: 421 loss: 0.00186348392
Iter: 422 loss: 0.00186182256
Iter: 423 loss: 0.00185922789
Iter: 424 loss: 0.00187785714
Iter: 425 loss: 0.00185901078
Iter: 426 loss: 0.00185694196
Iter: 427 loss: 0.00186835439
Iter: 428 loss: 0.00185664592
Iter: 429 loss: 0.00185526884
Iter: 430 loss: 0.00185773964
Iter: 431 loss: 0.00185463647
Iter: 432 loss: 0.0018520538
Iter: 433 loss: 0.0018560196
Iter: 434 loss: 0.00185083644
Iter: 435 loss: 0.00184826821
Iter: 436 loss: 0.00185616291
Iter: 437 loss: 0.00184754794
Iter: 438 loss: 0.00184522476
Iter: 439 loss: 0.00185026461
Iter: 440 loss: 0.00184429169
Iter: 441 loss: 0.00184180471
Iter: 442 loss: 0.00184281589
Iter: 443 loss: 0.0018400735
Iter: 444 loss: 0.00183658954
Iter: 445 loss: 0.00184360438
Iter: 446 loss: 0.0018351872
Iter: 447 loss: 0.00183263549
Iter: 448 loss: 0.00183518464
Iter: 449 loss: 0.0018310548
Iter: 450 loss: 0.00182586419
Iter: 451 loss: 0.00184711791
Iter: 452 loss: 0.00182469841
Iter: 453 loss: 0.00182328362
Iter: 454 loss: 0.00182125182
Iter: 455 loss: 0.00181856868
Iter: 456 loss: 0.0018420968
Iter: 457 loss: 0.00181842828
Iter: 458 loss: 0.00181673327
Iter: 459 loss: 0.00181514269
Iter: 460 loss: 0.0018147243
Iter: 461 loss: 0.00181220984
Iter: 462 loss: 0.00181218539
Iter: 463 loss: 0.00181048072
Iter: 464 loss: 0.00181714259
Iter: 465 loss: 0.00181007688
Iter: 466 loss: 0.00180822914
Iter: 467 loss: 0.00180562644
Iter: 468 loss: 0.00180551014
Iter: 469 loss: 0.00180092361
Iter: 470 loss: 0.00183093513
Iter: 471 loss: 0.00180042489
Iter: 472 loss: 0.00179985701
Iter: 473 loss: 0.00179893768
Iter: 474 loss: 0.00179783604
Iter: 475 loss: 0.00179578643
Iter: 476 loss: 0.00184578425
Iter: 477 loss: 0.00179578189
Iter: 478 loss: 0.00179272436
Iter: 479 loss: 0.00181362138
Iter: 480 loss: 0.0017923594
Iter: 481 loss: 0.00179017347
Iter: 482 loss: 0.00181682454
Iter: 483 loss: 0.00179015659
Iter: 484 loss: 0.00178833504
Iter: 485 loss: 0.00179153262
Iter: 486 loss: 0.00178744667
Iter: 487 loss: 0.00178500614
Iter: 488 loss: 0.00179341575
Iter: 489 loss: 0.00178434653
Iter: 490 loss: 0.00178195769
Iter: 491 loss: 0.00178798975
Iter: 492 loss: 0.00178112928
Iter: 493 loss: 0.00177929259
Iter: 494 loss: 0.00177969527
Iter: 495 loss: 0.00177792087
Iter: 496 loss: 0.00177604752
Iter: 497 loss: 0.0017765041
Iter: 498 loss: 0.00177468546
Iter: 499 loss: 0.00177191244
Iter: 500 loss: 0.00180415588
Iter: 501 loss: 0.00177186308
Iter: 502 loss: 0.00177054992
Iter: 503 loss: 0.00177208649
Iter: 504 loss: 0.00176982814
Iter: 505 loss: 0.00176777737
Iter: 506 loss: 0.00176929543
Iter: 507 loss: 0.00176651415
Iter: 508 loss: 0.00176399807
Iter: 509 loss: 0.00177956908
Iter: 510 loss: 0.00176366232
Iter: 511 loss: 0.00176177709
Iter: 512 loss: 0.00176581182
Iter: 513 loss: 0.00176099548
Iter: 514 loss: 0.00175859872
Iter: 515 loss: 0.00176584336
Iter: 516 loss: 0.00175795692
Iter: 517 loss: 0.00175642467
Iter: 518 loss: 0.00175618578
Iter: 519 loss: 0.00175451406
Iter: 520 loss: 0.00175476552
Iter: 521 loss: 0.00175321731
Iter: 522 loss: 0.00175060262
Iter: 523 loss: 0.00175370765
Iter: 524 loss: 0.00174925476
Iter: 525 loss: 0.00174729573
Iter: 526 loss: 0.00177107705
Iter: 527 loss: 0.00174726383
Iter: 528 loss: 0.00174553203
Iter: 529 loss: 0.00175644737
Iter: 530 loss: 0.00174530596
Iter: 531 loss: 0.00174408825
Iter: 532 loss: 0.00174463121
Iter: 533 loss: 0.00174326403
Iter: 534 loss: 0.0017414263
Iter: 535 loss: 0.00175292871
Iter: 536 loss: 0.00174121745
Iter: 537 loss: 0.00173944153
Iter: 538 loss: 0.00174972729
Iter: 539 loss: 0.00173918111
Iter: 540 loss: 0.00173792033
Iter: 541 loss: 0.00173566374
Iter: 542 loss: 0.00179173285
Iter: 543 loss: 0.00173566281
Iter: 544 loss: 0.00173326954
Iter: 545 loss: 0.00174547883
Iter: 546 loss: 0.00173290935
Iter: 547 loss: 0.00173167326
Iter: 548 loss: 0.00173685106
Iter: 549 loss: 0.00173135498
Iter: 550 loss: 0.0017295134
Iter: 551 loss: 0.00174371316
Iter: 552 loss: 0.00172938081
Iter: 553 loss: 0.00172771618
Iter: 554 loss: 0.0017281879
Iter: 555 loss: 0.0017265263
Iter: 556 loss: 0.00172448298
Iter: 557 loss: 0.00172659568
Iter: 558 loss: 0.0017233101
Iter: 559 loss: 0.00172020262
Iter: 560 loss: 0.00173230027
Iter: 561 loss: 0.00171946769
Iter: 562 loss: 0.00171885197
Iter: 563 loss: 0.00171798794
Iter: 564 loss: 0.00171660795
Iter: 565 loss: 0.001714963
Iter: 566 loss: 0.00171479071
Iter: 567 loss: 0.00171290233
Iter: 568 loss: 0.00171420653
Iter: 569 loss: 0.00171171897
Iter: 570 loss: 0.00170991966
Iter: 571 loss: 0.00171290059
Iter: 572 loss: 0.00170910778
Iter: 573 loss: 0.00170679542
Iter: 574 loss: 0.00172445015
Iter: 575 loss: 0.00170647388
Iter: 576 loss: 0.00170417887
Iter: 577 loss: 0.00170416583
Iter: 578 loss: 0.00170166837
Iter: 579 loss: 0.00170024112
Iter: 580 loss: 0.00169917161
Iter: 581 loss: 0.00169735483
Iter: 582 loss: 0.0017158296
Iter: 583 loss: 0.00169729011
Iter: 584 loss: 0.00169593422
Iter: 585 loss: 0.0016968057
Iter: 586 loss: 0.00169507216
Iter: 587 loss: 0.00169348391
Iter: 588 loss: 0.00169595261
Iter: 589 loss: 0.00169273291
Iter: 590 loss: 0.00169091
Iter: 591 loss: 0.00169978326
Iter: 592 loss: 0.00169057713
Iter: 593 loss: 0.00168886944
Iter: 594 loss: 0.00168998796
Iter: 595 loss: 0.00168778072
Iter: 596 loss: 0.00168685708
Iter: 597 loss: 0.00168780412
Iter: 598 loss: 0.00168636569
Iter: 599 loss: 0.00168463937
Iter: 600 loss: 0.00168887095
Iter: 601 loss: 0.00168394181
Iter: 602 loss: 0.0016835545
Iter: 603 loss: 0.00168295507
Iter: 604 loss: 0.00168210804
Iter: 605 loss: 0.00167993922
Iter: 606 loss: 0.00169807964
Iter: 607 loss: 0.00167957321
Iter: 608 loss: 0.00167728635
Iter: 609 loss: 0.00169364852
Iter: 610 loss: 0.00167708052
Iter: 611 loss: 0.00167547411
Iter: 612 loss: 0.00168837619
Iter: 613 loss: 0.0016753563
Iter: 614 loss: 0.00167410122
Iter: 615 loss: 0.00167203695
Iter: 616 loss: 0.00167202647
Iter: 617 loss: 0.00166955194
Iter: 618 loss: 0.00168489444
Iter: 619 loss: 0.00166913238
Iter: 620 loss: 0.00166702596
Iter: 621 loss: 0.00169009366
Iter: 622 loss: 0.0016669404
Iter: 623 loss: 0.00166389276
Iter: 624 loss: 0.00167884817
Iter: 625 loss: 0.00166340824
Iter: 626 loss: 0.00166174897
Iter: 627 loss: 0.00168365811
Iter: 628 loss: 0.00166173489
Iter: 629 loss: 0.0016597365
Iter: 630 loss: 0.00166316167
Iter: 631 loss: 0.00165882392
Iter: 632 loss: 0.00165752065
Iter: 633 loss: 0.00165752566
Iter: 634 loss: 0.00165648456
Iter: 635 loss: 0.00165454915
Iter: 636 loss: 0.00165727106
Iter: 637 loss: 0.00165358419
Iter: 638 loss: 0.00165265333
Iter: 639 loss: 0.00165234914
Iter: 640 loss: 0.00165155204
Iter: 641 loss: 0.00165118137
Iter: 642 loss: 0.00165081373
Iter: 643 loss: 0.00164867332
Iter: 644 loss: 0.00165519235
Iter: 645 loss: 0.0016480065
Iter: 646 loss: 0.0016468761
Iter: 647 loss: 0.00164667424
Iter: 648 loss: 0.00164581847
Iter: 649 loss: 0.00164368132
Iter: 650 loss: 0.00166379462
Iter: 651 loss: 0.00164337666
Iter: 652 loss: 0.00164086185
Iter: 653 loss: 0.00168121059
Iter: 654 loss: 0.00164086325
Iter: 655 loss: 0.00163954147
Iter: 656 loss: 0.00164931361
Iter: 657 loss: 0.00163942738
Iter: 658 loss: 0.00163819711
Iter: 659 loss: 0.00163634086
Iter: 660 loss: 0.00163630105
Iter: 661 loss: 0.00163558032
Iter: 662 loss: 0.00163526693
Iter: 663 loss: 0.00163446774
Iter: 664 loss: 0.00163484993
Iter: 665 loss: 0.001633932
Iter: 666 loss: 0.00163238973
Iter: 667 loss: 0.0016310754
Iter: 668 loss: 0.00163063081
Iter: 669 loss: 0.00162905408
Iter: 670 loss: 0.00162853929
Iter: 671 loss: 0.00162661914
Iter: 672 loss: 0.00163171603
Iter: 673 loss: 0.00162598456
Iter: 674 loss: 0.00162362657
Iter: 675 loss: 0.00162867957
Iter: 676 loss: 0.00162270409
Iter: 677 loss: 0.00162129826
Iter: 678 loss: 0.00162190851
Iter: 679 loss: 0.00162032526
Iter: 680 loss: 0.00161929405
Iter: 681 loss: 0.00162032293
Iter: 682 loss: 0.00161872164
Iter: 683 loss: 0.00161696982
Iter: 684 loss: 0.00162541342
Iter: 685 loss: 0.00161663536
Iter: 686 loss: 0.00161558925
Iter: 687 loss: 0.00162482203
Iter: 688 loss: 0.00161554175
Iter: 689 loss: 0.00161484303
Iter: 690 loss: 0.0016137436
Iter: 691 loss: 0.00161373103
Iter: 692 loss: 0.00161231006
Iter: 693 loss: 0.00161400356
Iter: 694 loss: 0.00161156629
Iter: 695 loss: 0.00160981575
Iter: 696 loss: 0.00161559973
Iter: 697 loss: 0.00160932762
Iter: 698 loss: 0.00160696835
Iter: 699 loss: 0.00162388454
Iter: 700 loss: 0.00160676008
Iter: 701 loss: 0.00160561572
Iter: 702 loss: 0.00160434702
Iter: 703 loss: 0.00160416984
Iter: 704 loss: 0.00160321838
Iter: 705 loss: 0.00160360872
Iter: 706 loss: 0.00160256308
Iter: 707 loss: 0.00160146365
Iter: 708 loss: 0.00161302846
Iter: 709 loss: 0.00160143408
Iter: 710 loss: 0.00160020741
Iter: 711 loss: 0.00160223513
Iter: 712 loss: 0.00159964524
Iter: 713 loss: 0.00159857096
Iter: 714 loss: 0.001596
Iter: 715 loss: 0.00163306459
Iter: 716 loss: 0.00159578817
Iter: 717 loss: 0.00159317721
Iter: 718 loss: 0.00159312319
Iter: 719 loss: 0.00159090385
Iter: 720 loss: 0.00161198853
Iter: 721 loss: 0.00159080082
Iter: 722 loss: 0.00158959918
Iter: 723 loss: 0.00159449806
Iter: 724 loss: 0.00158933247
Iter: 725 loss: 0.00158810499
Iter: 726 loss: 0.00158573571
Iter: 727 loss: 0.00163610396
Iter: 728 loss: 0.0015857243
Iter: 729 loss: 0.0015841336
Iter: 730 loss: 0.00160282198
Iter: 731 loss: 0.00158410624
Iter: 732 loss: 0.00158299808
Iter: 733 loss: 0.00158292614
Iter: 734 loss: 0.00158222148
Iter: 735 loss: 0.00158182404
Iter: 736 loss: 0.00158151484
Iter: 737 loss: 0.0015798714
Iter: 738 loss: 0.00158736
Iter: 739 loss: 0.00157955242
Iter: 740 loss: 0.00157800736
Iter: 741 loss: 0.00158576877
Iter: 742 loss: 0.00157772435
Iter: 743 loss: 0.00157673901
Iter: 744 loss: 0.00157874101
Iter: 745 loss: 0.00157634798
Iter: 746 loss: 0.00157515169
Iter: 747 loss: 0.00157515076
Iter: 748 loss: 0.00157440256
Iter: 749 loss: 0.00157311372
Iter: 750 loss: 0.00157311256
Iter: 751 loss: 0.0015713363
Iter: 752 loss: 0.00157324621
Iter: 753 loss: 0.00157034537
Iter: 754 loss: 0.00156957901
Iter: 755 loss: 0.00157813216
Iter: 756 loss: 0.00156956643
Iter: 757 loss: 0.00156898075
Iter: 758 loss: 0.00156764034
Iter: 759 loss: 0.00158514292
Iter: 760 loss: 0.00156754907
Iter: 761 loss: 0.00156616548
Iter: 762 loss: 0.00157433329
Iter: 763 loss: 0.00156599318
Iter: 764 loss: 0.00156489271
Iter: 765 loss: 0.0015688662
Iter: 766 loss: 0.00156462099
Iter: 767 loss: 0.001563614
Iter: 768 loss: 0.0015693868
Iter: 769 loss: 0.00156347768
Iter: 770 loss: 0.00156246964
Iter: 771 loss: 0.00156183087
Iter: 772 loss: 0.00156143634
Iter: 773 loss: 0.00156013772
Iter: 774 loss: 0.00156299188
Iter: 775 loss: 0.00155963458
Iter: 776 loss: 0.00155754155
Iter: 777 loss: 0.00156177743
Iter: 778 loss: 0.0015567115
Iter: 779 loss: 0.00155579019
Iter: 780 loss: 0.00155578693
Iter: 781 loss: 0.00155498739
Iter: 782 loss: 0.0015553924
Iter: 783 loss: 0.00155444932
Iter: 784 loss: 0.0015531627
Iter: 785 loss: 0.0015530308
Iter: 786 loss: 0.00155209145
Iter: 787 loss: 0.0015513564
Iter: 788 loss: 0.00155063812
Iter: 789 loss: 0.00155048026
Iter: 790 loss: 0.00154858339
Iter: 791 loss: 0.00155705749
Iter: 792 loss: 0.00154819374
Iter: 793 loss: 0.00154730841
Iter: 794 loss: 0.00154542713
Iter: 795 loss: 0.0015771637
Iter: 796 loss: 0.00154537195
Iter: 797 loss: 0.00154333399
Iter: 798 loss: 0.00155446026
Iter: 799 loss: 0.00154303294
Iter: 800 loss: 0.00154158473
Iter: 801 loss: 0.00154469709
Iter: 802 loss: 0.00154101709
Iter: 803 loss: 0.00153929158
Iter: 804 loss: 0.00155100413
Iter: 805 loss: 0.00153912324
Iter: 806 loss: 0.00153792812
Iter: 807 loss: 0.00153792766
Iter: 808 loss: 0.00153759145
Iter: 809 loss: 0.00153682637
Iter: 810 loss: 0.00154715183
Iter: 811 loss: 0.00153677701
Iter: 812 loss: 0.00153561006
Iter: 813 loss: 0.00153931289
Iter: 814 loss: 0.00153527339
Iter: 815 loss: 0.00153405976
Iter: 816 loss: 0.00154122501
Iter: 817 loss: 0.00153390644
Iter: 818 loss: 0.00153302494
Iter: 819 loss: 0.00153269817
Iter: 820 loss: 0.0015322098
Iter: 821 loss: 0.00153084425
Iter: 822 loss: 0.0015290645
Iter: 823 loss: 0.00152893935
Iter: 824 loss: 0.00152792991
Iter: 825 loss: 0.00152767589
Iter: 826 loss: 0.00152663875
Iter: 827 loss: 0.00152435875
Iter: 828 loss: 0.00155788031
Iter: 829 loss: 0.00152425747
Iter: 830 loss: 0.00152246
Iter: 831 loss: 0.00152243138
Iter: 832 loss: 0.00152179587
Iter: 833 loss: 0.00152272626
Iter: 834 loss: 0.00152148819
Iter: 835 loss: 0.00152029935
Iter: 836 loss: 0.00151964
Iter: 837 loss: 0.00151911296
Iter: 838 loss: 0.00151805603
Iter: 839 loss: 0.00153211982
Iter: 840 loss: 0.00151804672
Iter: 841 loss: 0.00151711958
Iter: 842 loss: 0.00152384408
Iter: 843 loss: 0.00151703949
Iter: 844 loss: 0.0015165885
Iter: 845 loss: 0.00151659676
Iter: 846 loss: 0.00151622808
Iter: 847 loss: 0.00151545939
Iter: 848 loss: 0.00151923834
Iter: 849 loss: 0.00151532935
Iter: 850 loss: 0.00151466951
Iter: 851 loss: 0.0015132163
Iter: 852 loss: 0.00153436558
Iter: 853 loss: 0.00151314912
Iter: 854 loss: 0.00151312479
Iter: 855 loss: 0.00151241687
Iter: 856 loss: 0.00151160127
Iter: 857 loss: 0.00151313981
Iter: 858 loss: 0.00151125807
Iter: 859 loss: 0.00151073444
Iter: 860 loss: 0.00150972768
Iter: 861 loss: 0.00152986892
Iter: 862 loss: 0.00150972302
Iter: 863 loss: 0.00150830881
Iter: 864 loss: 0.00150981278
Iter: 865 loss: 0.00150754151
Iter: 866 loss: 0.00150571088
Iter: 867 loss: 0.00153041515
Iter: 868 loss: 0.00150569552
Iter: 869 loss: 0.0015048699
Iter: 870 loss: 0.00150276569
Iter: 871 loss: 0.00152151915
Iter: 872 loss: 0.00150243402
Iter: 873 loss: 0.00150018651
Iter: 874 loss: 0.00151664182
Iter: 875 loss: 0.00149999943
Iter: 876 loss: 0.00149872666
Iter: 877 loss: 0.00150747271
Iter: 878 loss: 0.0014985957
Iter: 879 loss: 0.00149687217
Iter: 880 loss: 0.00149886054
Iter: 881 loss: 0.00149595644
Iter: 882 loss: 0.00149513024
Iter: 883 loss: 0.0014978078
Iter: 884 loss: 0.0014948973
Iter: 885 loss: 0.00149427634
Iter: 886 loss: 0.00149857008
Iter: 887 loss: 0.00149422069
Iter: 888 loss: 0.00149362884
Iter: 889 loss: 0.00149333233
Iter: 890 loss: 0.00149305398
Iter: 891 loss: 0.00149150565
Iter: 892 loss: 0.00149403966
Iter: 893 loss: 0.00149078388
Iter: 894 loss: 0.00148831494
Iter: 895 loss: 0.00150582544
Iter: 896 loss: 0.00148809282
Iter: 897 loss: 0.00148735498
Iter: 898 loss: 0.00148587092
Iter: 899 loss: 0.00151421828
Iter: 900 loss: 0.00148585078
Iter: 901 loss: 0.00148476369
Iter: 902 loss: 0.00148447976
Iter: 903 loss: 0.00148380385
Iter: 904 loss: 0.00148309441
Iter: 905 loss: 0.00148251129
Iter: 906 loss: 0.00148230547
Iter: 907 loss: 0.00148131175
Iter: 908 loss: 0.00148114399
Iter: 909 loss: 0.00148044666
Iter: 910 loss: 0.00147898821
Iter: 911 loss: 0.0014815894
Iter: 912 loss: 0.00147832942
Iter: 913 loss: 0.00147706608
Iter: 914 loss: 0.00148308917
Iter: 915 loss: 0.00147683965
Iter: 916 loss: 0.00147627573
Iter: 917 loss: 0.00147675746
Iter: 918 loss: 0.00147594209
Iter: 919 loss: 0.00147492369
Iter: 920 loss: 0.00147428026
Iter: 921 loss: 0.00147387467
Iter: 922 loss: 0.00147293112
Iter: 923 loss: 0.00147285126
Iter: 924 loss: 0.00147226767
Iter: 925 loss: 0.00147116813
Iter: 926 loss: 0.00150257698
Iter: 927 loss: 0.00147117104
Iter: 928 loss: 0.00146987732
Iter: 929 loss: 0.00149076502
Iter: 930 loss: 0.00146987615
Iter: 931 loss: 0.00146875391
Iter: 932 loss: 0.00147499051
Iter: 933 loss: 0.00146859814
Iter: 934 loss: 0.001467644
Iter: 935 loss: 0.00146566913
Iter: 936 loss: 0.00150084333
Iter: 937 loss: 0.00146563444
Iter: 938 loss: 0.00146545493
Iter: 939 loss: 0.0014648576
Iter: 940 loss: 0.00146446866
Iter: 941 loss: 0.00146354758
Iter: 942 loss: 0.00147432
Iter: 943 loss: 0.00146346062
Iter: 944 loss: 0.00146167458
Iter: 945 loss: 0.00146539276
Iter: 946 loss: 0.0014609471
Iter: 947 loss: 0.00146445038
Iter: 948 loss: 0.0014599741
Iter: 949 loss: 0.00145960716
Iter: 950 loss: 0.00145891262
Iter: 951 loss: 0.00147420028
Iter: 952 loss: 0.00145891053
Iter: 953 loss: 0.00145753846
Iter: 954 loss: 0.00145583553
Iter: 955 loss: 0.00145568256
Iter: 956 loss: 0.00145314098
Iter: 957 loss: 0.00145314191
Iter: 958 loss: 0.00145181175
Iter: 959 loss: 0.00145173201
Iter: 960 loss: 0.00145079242
Iter: 961 loss: 0.00145184249
Iter: 962 loss: 0.00145028753
Iter: 963 loss: 0.00144951174
Iter: 964 loss: 0.00145303085
Iter: 965 loss: 0.00144935679
Iter: 966 loss: 0.00144846563
Iter: 967 loss: 0.00144756795
Iter: 968 loss: 0.00144738739
Iter: 969 loss: 0.00144852581
Iter: 970 loss: 0.0014468797
Iter: 971 loss: 0.00144649739
Iter: 972 loss: 0.00144605781
Iter: 973 loss: 0.00144600298
Iter: 974 loss: 0.0014453
Iter: 975 loss: 0.00144483289
Iter: 976 loss: 0.00144457
Iter: 977 loss: 0.00144368771
Iter: 978 loss: 0.0014433933
Iter: 979 loss: 0.0014428841
Iter: 980 loss: 0.00144195801
Iter: 981 loss: 0.00144262216
Iter: 982 loss: 0.00144138688
Iter: 983 loss: 0.00144004705
Iter: 984 loss: 0.00144591299
Iter: 985 loss: 0.00143977627
Iter: 986 loss: 0.00143759279
Iter: 987 loss: 0.00144942827
Iter: 988 loss: 0.00143725786
Iter: 989 loss: 0.0014362596
Iter: 990 loss: 0.00144263776
Iter: 991 loss: 0.00143614784
Iter: 992 loss: 0.00143499125
Iter: 993 loss: 0.00143716624
Iter: 994 loss: 0.00143449451
Iter: 995 loss: 0.00143376773
Iter: 996 loss: 0.00144379353
Iter: 997 loss: 0.00143376482
Iter: 998 loss: 0.00143328134
Iter: 999 loss: 0.00143209589
Iter: 1000 loss: 0.0014446124
Iter: 1001 loss: 0.001431957
Iter: 1002 loss: 0.00143128866
Iter: 1003 loss: 0.00143112883
Iter: 1004 loss: 0.00143052894
Iter: 1005 loss: 0.00143380079
Iter: 1006 loss: 0.00143044477
Iter: 1007 loss: 0.00142997247
Iter: 1008 loss: 0.00143085374
Iter: 1009 loss: 0.00142977014
Iter: 1010 loss: 0.00142905419
Iter: 1011 loss: 0.00142858177
Iter: 1012 loss: 0.00142831262
Iter: 1013 loss: 0.00142705045
Iter: 1014 loss: 0.0014257502
Iter: 1015 loss: 0.00142550783
Iter: 1016 loss: 0.00142367231
Iter: 1017 loss: 0.00142860087
Iter: 1018 loss: 0.0014230432
Iter: 1019 loss: 0.00142159814
Iter: 1020 loss: 0.00142648723
Iter: 1021 loss: 0.0014211979
Iter: 1022 loss: 0.00141979184
Iter: 1023 loss: 0.00142150349
Iter: 1024 loss: 0.00141906156
Iter: 1025 loss: 0.00141720776
Iter: 1026 loss: 0.00144003774
Iter: 1027 loss: 0.00141718669
Iter: 1028 loss: 0.00141651311
Iter: 1029 loss: 0.00141923642
Iter: 1030 loss: 0.00141635805
Iter: 1031 loss: 0.00141542533
Iter: 1032 loss: 0.00141578191
Iter: 1033 loss: 0.00141477468
Iter: 1034 loss: 0.00141406851
Iter: 1035 loss: 0.00141406734
Iter: 1036 loss: 0.00141352788
Iter: 1037 loss: 0.00141331716
Iter: 1038 loss: 0.00141302741
Iter: 1039 loss: 0.00141205802
Iter: 1040 loss: 0.00141450088
Iter: 1041 loss: 0.00141171657
Iter: 1042 loss: 0.00141054241
Iter: 1043 loss: 0.00141672068
Iter: 1044 loss: 0.00141036161
Iter: 1045 loss: 0.00140964682
Iter: 1046 loss: 0.00140882423
Iter: 1047 loss: 0.00140872283
Iter: 1048 loss: 0.00140749221
Iter: 1049 loss: 0.001407417
Iter: 1050 loss: 0.00140647742
Iter: 1051 loss: 0.00140495715
Iter: 1052 loss: 0.00140359625
Iter: 1053 loss: 0.00140319136
Iter: 1054 loss: 0.00140185189
Iter: 1055 loss: 0.00141869346
Iter: 1056 loss: 0.00140184024
Iter: 1057 loss: 0.00140104431
Iter: 1058 loss: 0.00140101905
Iter: 1059 loss: 0.00140045839
Iter: 1060 loss: 0.00140069949
Iter: 1061 loss: 0.00140007073
Iter: 1062 loss: 0.00139917503
Iter: 1063 loss: 0.00140201836
Iter: 1064 loss: 0.00139891938
Iter: 1065 loss: 0.00139835291
Iter: 1066 loss: 0.00140705705
Iter: 1067 loss: 0.00139835419
Iter: 1068 loss: 0.0013978628
Iter: 1069 loss: 0.00139697816
Iter: 1070 loss: 0.00141840114
Iter: 1071 loss: 0.00139697921
Iter: 1072 loss: 0.00139600248
Iter: 1073 loss: 0.001398355
Iter: 1074 loss: 0.00139564776
Iter: 1075 loss: 0.00139477069
Iter: 1076 loss: 0.00140372943
Iter: 1077 loss: 0.00139474473
Iter: 1078 loss: 0.00139422272
Iter: 1079 loss: 0.00139421306
Iter: 1080 loss: 0.00139380246
Iter: 1081 loss: 0.0013929588
Iter: 1082 loss: 0.00139270094
Iter: 1083 loss: 0.00139220478
Iter: 1084 loss: 0.00139110128
Iter: 1085 loss: 0.00138887845
Iter: 1086 loss: 0.0014328768
Iter: 1087 loss: 0.00138885062
Iter: 1088 loss: 0.00138655049
Iter: 1089 loss: 0.00138928043
Iter: 1090 loss: 0.0013853556
Iter: 1091 loss: 0.00138622778
Iter: 1092 loss: 0.00138437026
Iter: 1093 loss: 0.00138374011
Iter: 1094 loss: 0.0013840131
Iter: 1095 loss: 0.00138330879
Iter: 1096 loss: 0.00138254429
Iter: 1097 loss: 0.00138719147
Iter: 1098 loss: 0.00138245069
Iter: 1099 loss: 0.00138190296
Iter: 1100 loss: 0.00138270424
Iter: 1101 loss: 0.00138163916
Iter: 1102 loss: 0.00138072041
Iter: 1103 loss: 0.00138113392
Iter: 1104 loss: 0.00138010085
Iter: 1105 loss: 0.00137939141
Iter: 1106 loss: 0.00137856696
Iter: 1107 loss: 0.001378468
Iter: 1108 loss: 0.00137822831
Iter: 1109 loss: 0.00137771189
Iter: 1110 loss: 0.00137706811
Iter: 1111 loss: 0.00137696404
Iter: 1112 loss: 0.00137651968
Iter: 1113 loss: 0.00137504656
Iter: 1114 loss: 0.00137437903
Iter: 1115 loss: 0.0013736377
Iter: 1116 loss: 0.00137247483
Iter: 1117 loss: 0.00137755659
Iter: 1118 loss: 0.00137223653
Iter: 1119 loss: 0.00137095375
Iter: 1120 loss: 0.00136931846
Iter: 1121 loss: 0.00136919692
Iter: 1122 loss: 0.00136725383
Iter: 1123 loss: 0.00136923348
Iter: 1124 loss: 0.00136615941
Iter: 1125 loss: 0.00136650587
Iter: 1126 loss: 0.00136548595
Iter: 1127 loss: 0.00136472681
Iter: 1128 loss: 0.00136433449
Iter: 1129 loss: 0.00136398058
Iter: 1130 loss: 0.00136368931
Iter: 1131 loss: 0.0013636048
Iter: 1132 loss: 0.00136323506
Iter: 1133 loss: 0.0013630304
Iter: 1134 loss: 0.00136287
Iter: 1135 loss: 0.00136225577
Iter: 1136 loss: 0.00136208604
Iter: 1137 loss: 0.00136170746
Iter: 1138 loss: 0.00136103993
Iter: 1139 loss: 0.00136096217
Iter: 1140 loss: 0.00136049371
Iter: 1141 loss: 0.00136118103
Iter: 1142 loss: 0.00136026286
Iter: 1143 loss: 0.0013593405
Iter: 1144 loss: 0.00135864434
Iter: 1145 loss: 0.00135833584
Iter: 1146 loss: 0.0013574214
Iter: 1147 loss: 0.00135754806
Iter: 1148 loss: 0.00135672605
Iter: 1149 loss: 0.00135525083
Iter: 1150 loss: 0.00135304371
Iter: 1151 loss: 0.00135299424
Iter: 1152 loss: 0.00135184405
Iter: 1153 loss: 0.00135834399
Iter: 1154 loss: 0.00135168387
Iter: 1155 loss: 0.00135083753
Iter: 1156 loss: 0.00136047229
Iter: 1157 loss: 0.00135081843
Iter: 1158 loss: 0.00135028083
Iter: 1159 loss: 0.00135059492
Iter: 1160 loss: 0.00134993054
Iter: 1161 loss: 0.00134898548
Iter: 1162 loss: 0.00134846801
Iter: 1163 loss: 0.00134804938
Iter: 1164 loss: 0.00134737685
Iter: 1165 loss: 0.00134713505
Iter: 1166 loss: 0.00134690921
Iter: 1167 loss: 0.0013462184
Iter: 1168 loss: 0.00134777557
Iter: 1169 loss: 0.00134580513
Iter: 1170 loss: 0.00134455017
Iter: 1171 loss: 0.00135837425
Iter: 1172 loss: 0.00134452339
Iter: 1173 loss: 0.00134332851
Iter: 1174 loss: 0.00134652515
Iter: 1175 loss: 0.00134292385
Iter: 1176 loss: 0.00134145771
Iter: 1177 loss: 0.00134923006
Iter: 1178 loss: 0.00134122884
Iter: 1179 loss: 0.00134080939
Iter: 1180 loss: 0.00134057784
Iter: 1181 loss: 0.00134012697
Iter: 1182 loss: 0.00133925828
Iter: 1183 loss: 0.00135723758
Iter: 1184 loss: 0.00133924885
Iter: 1185 loss: 0.00133847259
Iter: 1186 loss: 0.00133775966
Iter: 1187 loss: 0.00133757154
Iter: 1188 loss: 0.00133681647
Iter: 1189 loss: 0.00133583578
Iter: 1190 loss: 0.00133577047
Iter: 1191 loss: 0.00133468676
Iter: 1192 loss: 0.00133703602
Iter: 1193 loss: 0.00133426499
Iter: 1194 loss: 0.00133285671
Iter: 1195 loss: 0.00133471179
Iter: 1196 loss: 0.0013321389
Iter: 1197 loss: 0.0013317297
Iter: 1198 loss: 0.00133143447
Iter: 1199 loss: 0.00133111654
Iter: 1200 loss: 0.00133103074
Iter: 1201 loss: 0.00133083144
Iter: 1202 loss: 0.00133010885
Iter: 1203 loss: 0.00132901361
Iter: 1204 loss: 0.00132899336
Iter: 1205 loss: 0.00132744119
Iter: 1206 loss: 0.00132743223
Iter: 1207 loss: 0.00132638821
Iter: 1208 loss: 0.00132915471
Iter: 1209 loss: 0.00132603291
Iter: 1210 loss: 0.00132553454
Iter: 1211 loss: 0.00132498634
Iter: 1212 loss: 0.00132490508
Iter: 1213 loss: 0.0013238237
Iter: 1214 loss: 0.00132479006
Iter: 1215 loss: 0.00132319483
Iter: 1216 loss: 0.00132225524
Iter: 1217 loss: 0.00132119772
Iter: 1218 loss: 0.00132105488
Iter: 1219 loss: 0.00132070761
Iter: 1220 loss: 0.00132039608
Iter: 1221 loss: 0.0013197154
Iter: 1222 loss: 0.00132144196
Iter: 1223 loss: 0.0013194778
Iter: 1224 loss: 0.00131865614
Iter: 1225 loss: 0.0013190913
Iter: 1226 loss: 0.00131811
Iter: 1227 loss: 0.00131770724
Iter: 1228 loss: 0.00131747709
Iter: 1229 loss: 0.00131699478
Iter: 1230 loss: 0.0013186977
Iter: 1231 loss: 0.00131687755
Iter: 1232 loss: 0.00131646567
Iter: 1233 loss: 0.00131536869
Iter: 1234 loss: 0.0013233372
Iter: 1235 loss: 0.00131512852
Iter: 1236 loss: 0.00131436344
Iter: 1237 loss: 0.00131906709
Iter: 1238 loss: 0.00131426542
Iter: 1239 loss: 0.00131454389
Iter: 1240 loss: 0.00131381652
Iter: 1241 loss: 0.00131350732
Iter: 1242 loss: 0.00131261814
Iter: 1243 loss: 0.00131660968
Iter: 1244 loss: 0.00131228811
Iter: 1245 loss: 0.00131113583
Iter: 1246 loss: 0.00131748396
Iter: 1247 loss: 0.00131096481
Iter: 1248 loss: 0.00130985654
Iter: 1249 loss: 0.00130784092
Iter: 1250 loss: 0.00135545374
Iter: 1251 loss: 0.00130784046
Iter: 1252 loss: 0.00130598375
Iter: 1253 loss: 0.00132382091
Iter: 1254 loss: 0.00130590354
Iter: 1255 loss: 0.00130406918
Iter: 1256 loss: 0.00132962968
Iter: 1257 loss: 0.0013040629
Iter: 1258 loss: 0.00130354613
Iter: 1259 loss: 0.00130331866
Iter: 1260 loss: 0.00130305486
Iter: 1261 loss: 0.00130238489
Iter: 1262 loss: 0.00130410423
Iter: 1263 loss: 0.00130215194
Iter: 1264 loss: 0.0013011226
Iter: 1265 loss: 0.00131286308
Iter: 1266 loss: 0.0013010985
Iter: 1267 loss: 0.00130027463
Iter: 1268 loss: 0.00130050199
Iter: 1269 loss: 0.00129968137
Iter: 1270 loss: 0.00129863899
Iter: 1271 loss: 0.00129814469
Iter: 1272 loss: 0.001297632
Iter: 1273 loss: 0.0012976689
Iter: 1274 loss: 0.00129707658
Iter: 1275 loss: 0.00129656401
Iter: 1276 loss: 0.00129576435
Iter: 1277 loss: 0.0012957548
Iter: 1278 loss: 0.00129461323
Iter: 1279 loss: 0.00129392534
Iter: 1280 loss: 0.00129345991
Iter: 1281 loss: 0.00129125966
Iter: 1282 loss: 0.00130639644
Iter: 1283 loss: 0.00129104103
Iter: 1284 loss: 0.00129032484
Iter: 1285 loss: 0.00129009667
Iter: 1286 loss: 0.00128967466
Iter: 1287 loss: 0.00128901284
Iter: 1288 loss: 0.00129467109
Iter: 1289 loss: 0.0012889721
Iter: 1290 loss: 0.00128841656
Iter: 1291 loss: 0.00129049248
Iter: 1292 loss: 0.00128827896
Iter: 1293 loss: 0.00128782704
Iter: 1294 loss: 0.00128702051
Iter: 1295 loss: 0.00130714988
Iter: 1296 loss: 0.00128701876
Iter: 1297 loss: 0.00128624984
Iter: 1298 loss: 0.00128623669
Iter: 1299 loss: 0.00128558814
Iter: 1300 loss: 0.00128641655
Iter: 1301 loss: 0.00128525123
Iter: 1302 loss: 0.00128465414
Iter: 1303 loss: 0.00128332712
Iter: 1304 loss: 0.00130268536
Iter: 1305 loss: 0.00128326053
Iter: 1306 loss: 0.00128347403
Iter: 1307 loss: 0.00128264958
Iter: 1308 loss: 0.00128207507
Iter: 1309 loss: 0.00128144864
Iter: 1310 loss: 0.00128134713
Iter: 1311 loss: 0.00128067716
Iter: 1312 loss: 0.00127982406
Iter: 1313 loss: 0.00127976201
Iter: 1314 loss: 0.0012787506
Iter: 1315 loss: 0.00127948204
Iter: 1316 loss: 0.001278123
Iter: 1317 loss: 0.00127737946
Iter: 1318 loss: 0.00127576396
Iter: 1319 loss: 0.00130130327
Iter: 1320 loss: 0.00127570366
Iter: 1321 loss: 0.00127657223
Iter: 1322 loss: 0.00127500261
Iter: 1323 loss: 0.0012743877
Iter: 1324 loss: 0.00127817737
Iter: 1325 loss: 0.00127431564
Iter: 1326 loss: 0.00127388188
Iter: 1327 loss: 0.00127363182
Iter: 1328 loss: 0.00127344707
Iter: 1329 loss: 0.00127278268
Iter: 1330 loss: 0.00127785548
Iter: 1331 loss: 0.00127273565
Iter: 1332 loss: 0.00127210654
Iter: 1333 loss: 0.00127525325
Iter: 1334 loss: 0.00127200503
Iter: 1335 loss: 0.00127150456
Iter: 1336 loss: 0.00126985577
Iter: 1337 loss: 0.00127012271
Iter: 1338 loss: 0.00126821618
Iter: 1339 loss: 0.00127795106
Iter: 1340 loss: 0.00126760523
Iter: 1341 loss: 0.00126689579
Iter: 1342 loss: 0.00127056497
Iter: 1343 loss: 0.0012667838
Iter: 1344 loss: 0.0012660583
Iter: 1345 loss: 0.00126406935
Iter: 1346 loss: 0.00127686188
Iter: 1347 loss: 0.00126356236
Iter: 1348 loss: 0.00126209063
Iter: 1349 loss: 0.00128138834
Iter: 1350 loss: 0.00126207783
Iter: 1351 loss: 0.00126005127
Iter: 1352 loss: 0.00127084227
Iter: 1353 loss: 0.00125973497
Iter: 1354 loss: 0.00125917117
Iter: 1355 loss: 0.00125862
Iter: 1356 loss: 0.00125849666
Iter: 1357 loss: 0.00125732273
Iter: 1358 loss: 0.00126887765
Iter: 1359 loss: 0.00125728082
Iter: 1360 loss: 0.00125666079
Iter: 1361 loss: 0.00125895487
Iter: 1362 loss: 0.00125650968
Iter: 1363 loss: 0.0012558091
Iter: 1364 loss: 0.00125853717
Iter: 1365 loss: 0.00125564681
Iter: 1366 loss: 0.00125467824
Iter: 1367 loss: 0.00125903031
Iter: 1368 loss: 0.00125449104
Iter: 1369 loss: 0.00125352561
Iter: 1370 loss: 0.00125949155
Iter: 1371 loss: 0.00125340722
Iter: 1372 loss: 0.00125286507
Iter: 1373 loss: 0.00125186401
Iter: 1374 loss: 0.00127469748
Iter: 1375 loss: 0.00125186378
Iter: 1376 loss: 0.00125084294
Iter: 1377 loss: 0.00125057681
Iter: 1378 loss: 0.00125005248
Iter: 1379 loss: 0.00124910427
Iter: 1380 loss: 0.00127144577
Iter: 1381 loss: 0.00124909985
Iter: 1382 loss: 0.00124824781
Iter: 1383 loss: 0.00124888332
Iter: 1384 loss: 0.00124772172
Iter: 1385 loss: 0.00124724768
Iter: 1386 loss: 0.00124745851
Iter: 1387 loss: 0.00124692102
Iter: 1388 loss: 0.00124642695
Iter: 1389 loss: 0.00124530436
Iter: 1390 loss: 0.00126045896
Iter: 1391 loss: 0.00124523463
Iter: 1392 loss: 0.00124450331
Iter: 1393 loss: 0.00124441227
Iter: 1394 loss: 0.00124371075
Iter: 1395 loss: 0.00124923
Iter: 1396 loss: 0.00124366302
Iter: 1397 loss: 0.00124297128
Iter: 1398 loss: 0.00124218455
Iter: 1399 loss: 0.00124208024
Iter: 1400 loss: 0.00124164345
Iter: 1401 loss: 0.00124151306
Iter: 1402 loss: 0.00124118547
Iter: 1403 loss: 0.00124053203
Iter: 1404 loss: 0.00125297287
Iter: 1405 loss: 0.00124052586
Iter: 1406 loss: 0.0012398375
Iter: 1407 loss: 0.00123879814
Iter: 1408 loss: 0.00123877986
Iter: 1409 loss: 0.00123806414
Iter: 1410 loss: 0.00124094542
Iter: 1411 loss: 0.00123790745
Iter: 1412 loss: 0.00123748882
Iter: 1413 loss: 0.00123655132
Iter: 1414 loss: 0.00124908634
Iter: 1415 loss: 0.0012364916
Iter: 1416 loss: 0.00123547076
Iter: 1417 loss: 0.00123564946
Iter: 1418 loss: 0.00123469823
Iter: 1419 loss: 0.0012337584
Iter: 1420 loss: 0.00124601054
Iter: 1421 loss: 0.00123375189
Iter: 1422 loss: 0.00123265653
Iter: 1423 loss: 0.00123381079
Iter: 1424 loss: 0.00123205408
Iter: 1425 loss: 0.00123113219
Iter: 1426 loss: 0.0012332527
Iter: 1427 loss: 0.00123078318
Iter: 1428 loss: 0.00122977
Iter: 1429 loss: 0.00122890691
Iter: 1430 loss: 0.00122862007
Iter: 1431 loss: 0.00122719468
Iter: 1432 loss: 0.00122714834
Iter: 1433 loss: 0.00122660713
Iter: 1434 loss: 0.00123004883
Iter: 1435 loss: 0.00122654485
Iter: 1436 loss: 0.00122603821
Iter: 1437 loss: 0.00122823671
Iter: 1438 loss: 0.00122594077
Iter: 1439 loss: 0.00122555252
Iter: 1440 loss: 0.00122733577
Iter: 1441 loss: 0.00122547895
Iter: 1442 loss: 0.00122518104
Iter: 1443 loss: 0.00122809934
Iter: 1444 loss: 0.00122516649
Iter: 1445 loss: 0.00122490921
Iter: 1446 loss: 0.00122403249
Iter: 1447 loss: 0.0012233844
Iter: 1448 loss: 0.00122288545
Iter: 1449 loss: 0.00122056413
Iter: 1450 loss: 0.00123497727
Iter: 1451 loss: 0.00122028147
Iter: 1452 loss: 0.00122255168
Iter: 1453 loss: 0.00121951231
Iter: 1454 loss: 0.00121903606
Iter: 1455 loss: 0.00122058135
Iter: 1456 loss: 0.00121889985
Iter: 1457 loss: 0.00121836516
Iter: 1458 loss: 0.00121699332
Iter: 1459 loss: 0.0012285246
Iter: 1460 loss: 0.00121676433
Iter: 1461 loss: 0.00121503533
Iter: 1462 loss: 0.0012152472
Iter: 1463 loss: 0.00121371134
Iter: 1464 loss: 0.00121286337
Iter: 1465 loss: 0.00122146017
Iter: 1466 loss: 0.00121283531
Iter: 1467 loss: 0.00121205614
Iter: 1468 loss: 0.00122319092
Iter: 1469 loss: 0.00121205486
Iter: 1470 loss: 0.0012114885
Iter: 1471 loss: 0.00121441728
Iter: 1472 loss: 0.00121140108
Iter: 1473 loss: 0.00121108664
Iter: 1474 loss: 0.00121099968
Iter: 1475 loss: 0.00121080654
Iter: 1476 loss: 0.00121022481
Iter: 1477 loss: 0.0012116232
Iter: 1478 loss: 0.00121001177
Iter: 1479 loss: 0.00120901107
Iter: 1480 loss: 0.00120792584
Iter: 1481 loss: 0.00120775984
Iter: 1482 loss: 0.00120693236
Iter: 1483 loss: 0.00120800617
Iter: 1484 loss: 0.00120650523
Iter: 1485 loss: 0.00120553921
Iter: 1486 loss: 0.00120412523
Iter: 1487 loss: 0.0012040846
Iter: 1488 loss: 0.00120449229
Iter: 1489 loss: 0.00120356516
Iter: 1490 loss: 0.0012029534
Iter: 1491 loss: 0.00120121147
Iter: 1492 loss: 0.00120991014
Iter: 1493 loss: 0.0012005975
Iter: 1494 loss: 0.00119915942
Iter: 1495 loss: 0.00120027922
Iter: 1496 loss: 0.00119828386
Iter: 1497 loss: 0.0011970026
Iter: 1498 loss: 0.00119907921
Iter: 1499 loss: 0.00119639956
Iter: 1500 loss: 0.00119577348
Iter: 1501 loss: 0.00119573949
Iter: 1502 loss: 0.0011954254
Iter: 1503 loss: 0.00119615078
Iter: 1504 loss: 0.00119530386
Iter: 1505 loss: 0.00119502726
Iter: 1506 loss: 0.00119450374
Iter: 1507 loss: 0.00120614492
Iter: 1508 loss: 0.00119450164
Iter: 1509 loss: 0.00119361025
Iter: 1510 loss: 0.00120042847
Iter: 1511 loss: 0.00119354529
Iter: 1512 loss: 0.00119307742
Iter: 1513 loss: 0.00119986339
Iter: 1514 loss: 0.00119307905
Iter: 1515 loss: 0.00119269104
Iter: 1516 loss: 0.00119154924
Iter: 1517 loss: 0.00119614683
Iter: 1518 loss: 0.00119107892
Iter: 1519 loss: 0.0011900313
Iter: 1520 loss: 0.00119198207
Iter: 1521 loss: 0.00118959544
Iter: 1522 loss: 0.00118870707
Iter: 1523 loss: 0.00118783687
Iter: 1524 loss: 0.00118764653
Iter: 1525 loss: 0.00118711649
Iter: 1526 loss: 0.00118673174
Iter: 1527 loss: 0.00118628261
Iter: 1528 loss: 0.00118544477
Iter: 1529 loss: 0.00120348041
Iter: 1530 loss: 0.00118544232
Iter: 1531 loss: 0.00118390273
Iter: 1532 loss: 0.00119160244
Iter: 1533 loss: 0.00118364324
Iter: 1534 loss: 0.00118259375
Iter: 1535 loss: 0.00118843047
Iter: 1536 loss: 0.0011824437
Iter: 1537 loss: 0.00118369
Iter: 1538 loss: 0.00118211028
Iter: 1539 loss: 0.00118195871
Iter: 1540 loss: 0.00118164462
Iter: 1541 loss: 0.00118705013
Iter: 1542 loss: 0.00118163391
Iter: 1543 loss: 0.00118051842
Iter: 1544 loss: 0.00118442159
Iter: 1545 loss: 0.00118022494
Iter: 1546 loss: 0.00117887603
Iter: 1547 loss: 0.00117884553
Iter: 1548 loss: 0.00117820839
Iter: 1549 loss: 0.00118106487
Iter: 1550 loss: 0.00117808045
Iter: 1551 loss: 0.00117757765
Iter: 1552 loss: 0.00117636216
Iter: 1553 loss: 0.00118923257
Iter: 1554 loss: 0.00117622875
Iter: 1555 loss: 0.00117461802
Iter: 1556 loss: 0.00118777575
Iter: 1557 loss: 0.00117451977
Iter: 1558 loss: 0.00117379753
Iter: 1559 loss: 0.00117238343
Iter: 1560 loss: 0.00120011438
Iter: 1561 loss: 0.00117237249
Iter: 1562 loss: 0.00116944406
Iter: 1563 loss: 0.00119458896
Iter: 1564 loss: 0.00116928527
Iter: 1565 loss: 0.00116887712
Iter: 1566 loss: 0.00116981182
Iter: 1567 loss: 0.00116871914
Iter: 1568 loss: 0.00116839318
Iter: 1569 loss: 0.0011675139
Iter: 1570 loss: 0.00117306144
Iter: 1571 loss: 0.00116728968
Iter: 1572 loss: 0.00116908061
Iter: 1573 loss: 0.00116689538
Iter: 1574 loss: 0.00116631342
Iter: 1575 loss: 0.00116485101
Iter: 1576 loss: 0.00117882364
Iter: 1577 loss: 0.00116464938
Iter: 1578 loss: 0.0011630638
Iter: 1579 loss: 0.00116306206
Iter: 1580 loss: 0.00116185611
Iter: 1581 loss: 0.00118021213
Iter: 1582 loss: 0.00116185425
Iter: 1583 loss: 0.00116119161
Iter: 1584 loss: 0.00116171292
Iter: 1585 loss: 0.00116079301
Iter: 1586 loss: 0.00116035412
Iter: 1587 loss: 0.0011595285
Iter: 1588 loss: 0.0011777461
Iter: 1589 loss: 0.00115952641
Iter: 1590 loss: 0.00115853362
Iter: 1591 loss: 0.00115850591
Iter: 1592 loss: 0.00115770195
Iter: 1593 loss: 0.00115903048
Iter: 1594 loss: 0.00115733268
Iter: 1595 loss: 0.00115681882
Iter: 1596 loss: 0.00115962268
Iter: 1597 loss: 0.00115674408
Iter: 1598 loss: 0.00115645421
Iter: 1599 loss: 0.00115637551
Iter: 1600 loss: 0.00115610077
Iter: 1601 loss: 0.00115521299
Iter: 1602 loss: 0.00115584326
Iter: 1603 loss: 0.00115444162
Iter: 1604 loss: 0.00115388981
Iter: 1605 loss: 0.00115385419
Iter: 1606 loss: 0.00115348643
Iter: 1607 loss: 0.00115262414
Iter: 1608 loss: 0.00116279896
Iter: 1609 loss: 0.00115255127
Iter: 1610 loss: 0.00115124625
Iter: 1611 loss: 0.00115119631
Iter: 1612 loss: 0.0011501892
Iter: 1613 loss: 0.00114962854
Iter: 1614 loss: 0.00114967488
Iter: 1615 loss: 0.00114919432
Iter: 1616 loss: 0.0011481297
Iter: 1617 loss: 0.00115548377
Iter: 1618 loss: 0.00114802714
Iter: 1619 loss: 0.0011472574
Iter: 1620 loss: 0.00115127326
Iter: 1621 loss: 0.00114713586
Iter: 1622 loss: 0.00114651828
Iter: 1623 loss: 0.00114567368
Iter: 1624 loss: 0.00114562979
Iter: 1625 loss: 0.00114473386
Iter: 1626 loss: 0.00114355958
Iter: 1627 loss: 0.00114348473
Iter: 1628 loss: 0.00114195026
Iter: 1629 loss: 0.00116087333
Iter: 1630 loss: 0.00114193
Iter: 1631 loss: 0.00114178495
Iter: 1632 loss: 0.00114156306
Iter: 1633 loss: 0.00114130857
Iter: 1634 loss: 0.00114094198
Iter: 1635 loss: 0.00114092533
Iter: 1636 loss: 0.00114043523
Iter: 1637 loss: 0.00114038703
Iter: 1638 loss: 0.00113973836
Iter: 1639 loss: 0.00113916281
Iter: 1640 loss: 0.0011389961
Iter: 1641 loss: 0.00113830029
Iter: 1642 loss: 0.0011389103
Iter: 1643 loss: 0.00113789656
Iter: 1644 loss: 0.00113743125
Iter: 1645 loss: 0.00113595475
Iter: 1646 loss: 0.00113778526
Iter: 1647 loss: 0.00113483658
Iter: 1648 loss: 0.001133568
Iter: 1649 loss: 0.00113343261
Iter: 1650 loss: 0.00113228755
Iter: 1651 loss: 0.00114114326
Iter: 1652 loss: 0.00113220967
Iter: 1653 loss: 0.00113139837
Iter: 1654 loss: 0.00113475486
Iter: 1655 loss: 0.00113122072
Iter: 1656 loss: 0.001130388
Iter: 1657 loss: 0.0011314794
Iter: 1658 loss: 0.00112996041
Iter: 1659 loss: 0.00112919637
Iter: 1660 loss: 0.00113317475
Iter: 1661 loss: 0.00112907577
Iter: 1662 loss: 0.00112815667
Iter: 1663 loss: 0.00112764724
Iter: 1664 loss: 0.00112724188
Iter: 1665 loss: 0.0011272952
Iter: 1666 loss: 0.0011266463
Iter: 1667 loss: 0.00112644047
Iter: 1668 loss: 0.00112586399
Iter: 1669 loss: 0.0011289242
Iter: 1670 loss: 0.00112567842
Iter: 1671 loss: 0.00112459273
Iter: 1672 loss: 0.00112457504
Iter: 1673 loss: 0.0011235131
Iter: 1674 loss: 0.00112676248
Iter: 1675 loss: 0.00112319412
Iter: 1676 loss: 0.00112250261
Iter: 1677 loss: 0.00112340949
Iter: 1678 loss: 0.0011221508
Iter: 1679 loss: 0.00112126605
Iter: 1680 loss: 0.00112060236
Iter: 1681 loss: 0.00112030597
Iter: 1682 loss: 0.00111915171
Iter: 1683 loss: 0.00111911411
Iter: 1684 loss: 0.00111817825
Iter: 1685 loss: 0.00111996674
Iter: 1686 loss: 0.0011177907
Iter: 1687 loss: 0.0011171666
Iter: 1688 loss: 0.00111581723
Iter: 1689 loss: 0.00113686174
Iter: 1690 loss: 0.00111576798
Iter: 1691 loss: 0.00111465959
Iter: 1692 loss: 0.00111466064
Iter: 1693 loss: 0.00111379311
Iter: 1694 loss: 0.00112056453
Iter: 1695 loss: 0.00111373398
Iter: 1696 loss: 0.00111358007
Iter: 1697 loss: 0.00111341523
Iter: 1698 loss: 0.00111315702
Iter: 1699 loss: 0.00111282815
Iter: 1700 loss: 0.00111280568
Iter: 1701 loss: 0.00111231534
Iter: 1702 loss: 0.00111330289
Iter: 1703 loss: 0.00111211953
Iter: 1704 loss: 0.00111151475
Iter: 1705 loss: 0.00111371232
Iter: 1706 loss: 0.00111136865
Iter: 1707 loss: 0.00111081812
Iter: 1708 loss: 0.00110933487
Iter: 1709 loss: 0.00111901725
Iter: 1710 loss: 0.00110897399
Iter: 1711 loss: 0.00110669178
Iter: 1712 loss: 0.00111782399
Iter: 1713 loss: 0.00110630365
Iter: 1714 loss: 0.00110516127
Iter: 1715 loss: 0.00110514183
Iter: 1716 loss: 0.00110452203
Iter: 1717 loss: 0.00110295368
Iter: 1718 loss: 0.00111734215
Iter: 1719 loss: 0.0011027141
Iter: 1720 loss: 0.00110108766
Iter: 1721 loss: 0.00111686834
Iter: 1722 loss: 0.00110102748
Iter: 1723 loss: 0.00110017892
Iter: 1724 loss: 0.00110945408
Iter: 1725 loss: 0.00110016088
Iter: 1726 loss: 0.00109940697
Iter: 1727 loss: 0.00109992945
Iter: 1728 loss: 0.00109894481
Iter: 1729 loss: 0.00109837682
Iter: 1730 loss: 0.00110075914
Iter: 1731 loss: 0.00109826028
Iter: 1732 loss: 0.00109757902
Iter: 1733 loss: 0.0010994816
Iter: 1734 loss: 0.00109735806
Iter: 1735 loss: 0.00109676807
Iter: 1736 loss: 0.00109725853
Iter: 1737 loss: 0.00109641277
Iter: 1738 loss: 0.00109571742
Iter: 1739 loss: 0.00110478583
Iter: 1740 loss: 0.00109571207
Iter: 1741 loss: 0.00109534059
Iter: 1742 loss: 0.00109407492
Iter: 1743 loss: 0.00109286862
Iter: 1744 loss: 0.00109231554
Iter: 1745 loss: 0.00109219435
Iter: 1746 loss: 0.00109106652
Iter: 1747 loss: 0.00109057431
Iter: 1748 loss: 0.0010943478
Iter: 1749 loss: 0.0010905324
Iter: 1750 loss: 0.00109004334
Iter: 1751 loss: 0.00109541905
Iter: 1752 loss: 0.00109003321
Iter: 1753 loss: 0.00108967454
Iter: 1754 loss: 0.0010887034
Iter: 1755 loss: 0.00109504093
Iter: 1756 loss: 0.00108846196
Iter: 1757 loss: 0.00108762935
Iter: 1758 loss: 0.00108789327
Iter: 1759 loss: 0.00108703587
Iter: 1760 loss: 0.00108659756
Iter: 1761 loss: 0.00108645088
Iter: 1762 loss: 0.00108619675
Iter: 1763 loss: 0.00108587812
Iter: 1764 loss: 0.00108513818
Iter: 1765 loss: 0.00109479786
Iter: 1766 loss: 0.00108508766
Iter: 1767 loss: 0.00108379545
Iter: 1768 loss: 0.00108800246
Iter: 1769 loss: 0.00108342851
Iter: 1770 loss: 0.00108275318
Iter: 1771 loss: 0.00108396378
Iter: 1772 loss: 0.00108246307
Iter: 1773 loss: 0.00108160544
Iter: 1774 loss: 0.00108182849
Iter: 1775 loss: 0.00108098402
Iter: 1776 loss: 0.00107993116
Iter: 1777 loss: 0.00108671468
Iter: 1778 loss: 0.00107981241
Iter: 1779 loss: 0.00107888726
Iter: 1780 loss: 0.0010779436
Iter: 1781 loss: 0.0010777656
Iter: 1782 loss: 0.00107701763
Iter: 1783 loss: 0.00107552204
Iter: 1784 loss: 0.00110330549
Iter: 1785 loss: 0.00107549969
Iter: 1786 loss: 0.00107377348
Iter: 1787 loss: 0.00109546981
Iter: 1788 loss: 0.00107375858
Iter: 1789 loss: 0.00107293855
Iter: 1790 loss: 0.00107660762
Iter: 1791 loss: 0.00107276929
Iter: 1792 loss: 0.0010721758
Iter: 1793 loss: 0.00107153947
Iter: 1794 loss: 0.00107143587
Iter: 1795 loss: 0.00107067602
Iter: 1796 loss: 0.00107255916
Iter: 1797 loss: 0.00107040606
Iter: 1798 loss: 0.00106952514
Iter: 1799 loss: 0.00107590167
Iter: 1800 loss: 0.00106944877
Iter: 1801 loss: 0.00106880534
Iter: 1802 loss: 0.00106764329
Iter: 1803 loss: 0.00109674409
Iter: 1804 loss: 0.00106764352
Iter: 1805 loss: 0.00106615794
Iter: 1806 loss: 0.00108011707
Iter: 1807 loss: 0.00106609752
Iter: 1808 loss: 0.00106549938
Iter: 1809 loss: 0.00106547505
Iter: 1810 loss: 0.001065013
Iter: 1811 loss: 0.00106433535
Iter: 1812 loss: 0.00106406317
Iter: 1813 loss: 0.00106369937
Iter: 1814 loss: 0.00106312218
Iter: 1815 loss: 0.00106254732
Iter: 1816 loss: 0.00106242718
Iter: 1817 loss: 0.00106128829
Iter: 1818 loss: 0.00106114685
Iter: 1819 loss: 0.00106083788
Iter: 1820 loss: 0.00106059562
Iter: 1821 loss: 0.00106050132
Iter: 1822 loss: 0.00106000935
Iter: 1823 loss: 0.00105915789
Iter: 1824 loss: 0.00105915661
Iter: 1825 loss: 0.00105822249
Iter: 1826 loss: 0.00106610777
Iter: 1827 loss: 0.00105817243
Iter: 1828 loss: 0.00105756545
Iter: 1829 loss: 0.00106033508
Iter: 1830 loss: 0.00105744554
Iter: 1831 loss: 0.00105680968
Iter: 1832 loss: 0.00105784973
Iter: 1833 loss: 0.0010565198
Iter: 1834 loss: 0.00105579384
Iter: 1835 loss: 0.00105914124
Iter: 1836 loss: 0.00105565961
Iter: 1837 loss: 0.00105489651
Iter: 1838 loss: 0.00105399929
Iter: 1839 loss: 0.00105390116
Iter: 1840 loss: 0.00105269917
Iter: 1841 loss: 0.00106674677
Iter: 1842 loss: 0.0010526845
Iter: 1843 loss: 0.00105197402
Iter: 1844 loss: 0.00105278846
Iter: 1845 loss: 0.00105158938
Iter: 1846 loss: 0.00105079927
Iter: 1847 loss: 0.0010489783
Iter: 1848 loss: 0.00107205729
Iter: 1849 loss: 0.00104885618
Iter: 1850 loss: 0.00104920985
Iter: 1851 loss: 0.00104849623
Iter: 1852 loss: 0.00104823802
Iter: 1853 loss: 0.00104829145
Iter: 1854 loss: 0.00104804314
Iter: 1855 loss: 0.00104757422
Iter: 1856 loss: 0.00104609272
Iter: 1857 loss: 0.00104817678
Iter: 1858 loss: 0.00104501646
Iter: 1859 loss: 0.00104459759
Iter: 1860 loss: 0.00104448711
Iter: 1861 loss: 0.0010441083
Iter: 1862 loss: 0.00104338769
Iter: 1863 loss: 0.00105904555
Iter: 1864 loss: 0.00104338769
Iter: 1865 loss: 0.00104256067
Iter: 1866 loss: 0.00104239443
Iter: 1867 loss: 0.00104184612
Iter: 1868 loss: 0.00104159152
Iter: 1869 loss: 0.00104138337
Iter: 1870 loss: 0.00104074436
Iter: 1871 loss: 0.00104213471
Iter: 1872 loss: 0.00104049291
Iter: 1873 loss: 0.00103964983
Iter: 1874 loss: 0.00103841757
Iter: 1875 loss: 0.00103838299
Iter: 1876 loss: 0.00103868172
Iter: 1877 loss: 0.0010374561
Iter: 1878 loss: 0.00103679171
Iter: 1879 loss: 0.00103735144
Iter: 1880 loss: 0.00103640242
Iter: 1881 loss: 0.00103605445
Iter: 1882 loss: 0.00103989639
Iter: 1883 loss: 0.00103604863
Iter: 1884 loss: 0.00103571557
Iter: 1885 loss: 0.00103592279
Iter: 1886 loss: 0.00103550241
Iter: 1887 loss: 0.00103494769
Iter: 1888 loss: 0.00103645702
Iter: 1889 loss: 0.00103477039
Iter: 1890 loss: 0.00103447656
Iter: 1891 loss: 0.0010338818
Iter: 1892 loss: 0.00104449876
Iter: 1893 loss: 0.00103387458
Iter: 1894 loss: 0.00103279576
Iter: 1895 loss: 0.00103382603
Iter: 1896 loss: 0.00103217666
Iter: 1897 loss: 0.00103116268
Iter: 1898 loss: 0.00102910062
Iter: 1899 loss: 0.00106799568
Iter: 1900 loss: 0.00102906721
Iter: 1901 loss: 0.00102765579
Iter: 1902 loss: 0.00102764508
Iter: 1903 loss: 0.00102683797
Iter: 1904 loss: 0.00102565624
Iter: 1905 loss: 0.00102562422
Iter: 1906 loss: 0.00102397148
Iter: 1907 loss: 0.00102385867
Iter: 1908 loss: 0.0010244071
Iter: 1909 loss: 0.0010231575
Iter: 1910 loss: 0.00102274178
Iter: 1911 loss: 0.00102364877
Iter: 1912 loss: 0.00102257775
Iter: 1913 loss: 0.00102182594
Iter: 1914 loss: 0.00102819153
Iter: 1915 loss: 0.0010217838
Iter: 1916 loss: 0.0010207172
Iter: 1917 loss: 0.00102873053
Iter: 1918 loss: 0.00102062942
Iter: 1919 loss: 0.00102008274
Iter: 1920 loss: 0.0010225391
Iter: 1921 loss: 0.00101998029
Iter: 1922 loss: 0.00101958623
Iter: 1923 loss: 0.00101863127
Iter: 1924 loss: 0.00102869328
Iter: 1925 loss: 0.00101851486
Iter: 1926 loss: 0.00101645745
Iter: 1927 loss: 0.00101571577
Iter: 1928 loss: 0.00101456523
Iter: 1929 loss: 0.00101319968
Iter: 1930 loss: 0.00101704802
Iter: 1931 loss: 0.00101275928
Iter: 1932 loss: 0.0010127424
Iter: 1933 loss: 0.00101229281
Iter: 1934 loss: 0.00101197336
Iter: 1935 loss: 0.00101093238
Iter: 1936 loss: 0.0010112602
Iter: 1937 loss: 0.00100992562
Iter: 1938 loss: 0.00100859732
Iter: 1939 loss: 0.00101192133
Iter: 1940 loss: 0.00100812037
Iter: 1941 loss: 0.00100745796
Iter: 1942 loss: 0.0010105978
Iter: 1943 loss: 0.00100733642
Iter: 1944 loss: 0.00100635737
Iter: 1945 loss: 0.0010077348
Iter: 1946 loss: 0.00100587332
Iter: 1947 loss: 0.00100535166
Iter: 1948 loss: 0.00101034075
Iter: 1949 loss: 0.00100533105
Iter: 1950 loss: 0.00100492744
Iter: 1951 loss: 0.00100740662
Iter: 1952 loss: 0.00100487983
Iter: 1953 loss: 0.00100462628
Iter: 1954 loss: 0.00100426539
Iter: 1955 loss: 0.00100425025
Iter: 1956 loss: 0.00100391242
Iter: 1957 loss: 0.00100294244
Iter: 1958 loss: 0.00100761279
Iter: 1959 loss: 0.00100260787
Iter: 1960 loss: 0.00100228423
Iter: 1961 loss: 0.00100174733
Iter: 1962 loss: 0.001001255
Iter: 1963 loss: 0.00100221008
Iter: 1964 loss: 0.00100104965
Iter: 1965 loss: 0.00100043393
Iter: 1966 loss: 0.00100020319
Iter: 1967 loss: 0.000999862328
Iter: 1968 loss: 0.000999185606
Iter: 1969 loss: 0.00100748753
Iter: 1970 loss: 0.000999179
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.8
+ date
Sun Nov  8 15:44:38 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6c1d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6c1d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6c87e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6bab730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6babe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6babb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6ae3c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa59658f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa59659b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6b1e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa59650f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa596510f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5965277b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5705d28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5705d2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5705d20d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa596572620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa596572f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5705a69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5705b0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5965f46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa570487378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5704408c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa570459b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa570455378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa570455b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5703e48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5703857b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5703858c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5703b2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5704df400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa57034e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa57034ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa570364620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa57041cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5702b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 202, in <module>
    grads = tape.gradient(loss, model.trainable_weights)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1266, in _backward_function_wrapper
    processed_args, remapped_captures)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input is not invertible.
	 [[node gradients/MatrixDeterminant_grad/MatrixInverse (defined at biholoNN_train.py:200) ]] [Op:__inference___backward_volume_form_4446_8945]

Function call stack:
__backward_volume_form_4446

+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.8/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732f057488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732f168e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732f168ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732f0b6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732f0b6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732f0b6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732eff98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732efa4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732efa4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732efa4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ef64ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ef1ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ef387b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ef388c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ef38d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ee839d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732eeb92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732eeb9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ee2b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ee2ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ede28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ed8a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732edc0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308bbb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308bac7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308b661e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308b23598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308b452f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308b45378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308aff8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308a96400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308ac11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308ac1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308a7a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308a23d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308a23c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.64056983e-05
Iter: 2 loss: 1.98075613e-05
Iter: 3 loss: 1.45237045e-05
Iter: 4 loss: 1.38381729e-05
Iter: 5 loss: 1.53365108e-05
Iter: 6 loss: 1.35744785e-05
Iter: 7 loss: 1.31374154e-05
Iter: 8 loss: 1.33011108e-05
Iter: 9 loss: 1.28327101e-05
Iter: 10 loss: 1.22324946e-05
Iter: 11 loss: 1.55494126e-05
Iter: 12 loss: 1.21461908e-05
Iter: 13 loss: 1.16421706e-05
Iter: 14 loss: 1.17976324e-05
Iter: 15 loss: 1.12826547e-05
Iter: 16 loss: 1.09972079e-05
Iter: 17 loss: 1.09794237e-05
Iter: 18 loss: 1.07540018e-05
Iter: 19 loss: 1.04723304e-05
Iter: 20 loss: 1.04492492e-05
Iter: 21 loss: 1.00540474e-05
Iter: 22 loss: 1.03075963e-05
Iter: 23 loss: 9.80344612e-06
Iter: 24 loss: 9.2203e-06
Iter: 25 loss: 1.39034973e-05
Iter: 26 loss: 9.18223213e-06
Iter: 27 loss: 8.92807566e-06
Iter: 28 loss: 8.73842691e-06
Iter: 29 loss: 8.65417132e-06
Iter: 30 loss: 8.35203718e-06
Iter: 31 loss: 8.54313475e-06
Iter: 32 loss: 8.15934254e-06
Iter: 33 loss: 7.95247615e-06
Iter: 34 loss: 1.08403601e-05
Iter: 35 loss: 7.95188316e-06
Iter: 36 loss: 7.81998e-06
Iter: 37 loss: 7.74564069e-06
Iter: 38 loss: 7.68848349e-06
Iter: 39 loss: 7.5074513e-06
Iter: 40 loss: 7.50249092e-06
Iter: 41 loss: 7.41675376e-06
Iter: 42 loss: 7.33002798e-06
Iter: 43 loss: 7.31317186e-06
Iter: 44 loss: 7.15385249e-06
Iter: 45 loss: 8.05716809e-06
Iter: 46 loss: 7.1318691e-06
Iter: 47 loss: 6.97896394e-06
Iter: 48 loss: 7.09320102e-06
Iter: 49 loss: 6.88513319e-06
Iter: 50 loss: 6.77041044e-06
Iter: 51 loss: 7.97063e-06
Iter: 52 loss: 6.76731634e-06
Iter: 53 loss: 6.66595042e-06
Iter: 54 loss: 6.7591418e-06
Iter: 55 loss: 6.60733667e-06
Iter: 56 loss: 6.49635331e-06
Iter: 57 loss: 6.49012782e-06
Iter: 58 loss: 6.40557e-06
Iter: 59 loss: 6.30818e-06
Iter: 60 loss: 6.30814475e-06
Iter: 61 loss: 6.23496589e-06
Iter: 62 loss: 6.39466634e-06
Iter: 63 loss: 6.20682249e-06
Iter: 64 loss: 6.15787394e-06
Iter: 65 loss: 6.06764524e-06
Iter: 66 loss: 8.14361738e-06
Iter: 67 loss: 6.06763206e-06
Iter: 68 loss: 5.95045913e-06
Iter: 69 loss: 6.2759691e-06
Iter: 70 loss: 5.91280877e-06
Iter: 71 loss: 5.82476696e-06
Iter: 72 loss: 6.85396571e-06
Iter: 73 loss: 5.82333814e-06
Iter: 74 loss: 5.7637576e-06
Iter: 75 loss: 6.64901972e-06
Iter: 76 loss: 5.76365073e-06
Iter: 77 loss: 5.72183853e-06
Iter: 78 loss: 5.64918446e-06
Iter: 79 loss: 5.64923039e-06
Iter: 80 loss: 5.61475554e-06
Iter: 81 loss: 5.60888384e-06
Iter: 82 loss: 5.57533622e-06
Iter: 83 loss: 5.52101119e-06
Iter: 84 loss: 5.52059601e-06
Iter: 85 loss: 5.48610433e-06
Iter: 86 loss: 5.4842003e-06
Iter: 87 loss: 5.45925286e-06
Iter: 88 loss: 5.49819151e-06
Iter: 89 loss: 5.44769e-06
Iter: 90 loss: 5.42585076e-06
Iter: 91 loss: 5.40924975e-06
Iter: 92 loss: 5.40209749e-06
Iter: 93 loss: 5.36197922e-06
Iter: 94 loss: 5.55097813e-06
Iter: 95 loss: 5.35465824e-06
Iter: 96 loss: 5.30952457e-06
Iter: 97 loss: 5.41495547e-06
Iter: 98 loss: 5.29278714e-06
Iter: 99 loss: 5.25923042e-06
Iter: 100 loss: 5.23567678e-06
Iter: 101 loss: 5.22385517e-06
Iter: 102 loss: 5.16401633e-06
Iter: 103 loss: 5.22005303e-06
Iter: 104 loss: 5.12977113e-06
Iter: 105 loss: 5.08734229e-06
Iter: 106 loss: 5.70339716e-06
Iter: 107 loss: 5.0873341e-06
Iter: 108 loss: 5.04338868e-06
Iter: 109 loss: 5.24457846e-06
Iter: 110 loss: 5.03518413e-06
Iter: 111 loss: 5.01198929e-06
Iter: 112 loss: 5.01886e-06
Iter: 113 loss: 4.9954856e-06
Iter: 114 loss: 4.97785868e-06
Iter: 115 loss: 4.97756673e-06
Iter: 116 loss: 4.96479606e-06
Iter: 117 loss: 4.94533197e-06
Iter: 118 loss: 4.94495362e-06
Iter: 119 loss: 4.92489107e-06
Iter: 120 loss: 5.21171933e-06
Iter: 121 loss: 4.9248024e-06
Iter: 122 loss: 4.90690945e-06
Iter: 123 loss: 4.88580736e-06
Iter: 124 loss: 4.88347177e-06
Iter: 125 loss: 4.8611214e-06
Iter: 126 loss: 4.94917185e-06
Iter: 127 loss: 4.85593046e-06
Iter: 128 loss: 4.83238273e-06
Iter: 129 loss: 4.92112667e-06
Iter: 130 loss: 4.82672567e-06
Iter: 131 loss: 4.79939627e-06
Iter: 132 loss: 4.82956602e-06
Iter: 133 loss: 4.78437596e-06
Iter: 134 loss: 4.75940215e-06
Iter: 135 loss: 4.76056675e-06
Iter: 136 loss: 4.73950286e-06
Iter: 137 loss: 4.7116e-06
Iter: 138 loss: 4.82180667e-06
Iter: 139 loss: 4.70514351e-06
Iter: 140 loss: 4.68840244e-06
Iter: 141 loss: 4.68841608e-06
Iter: 142 loss: 4.6731443e-06
Iter: 143 loss: 4.72709053e-06
Iter: 144 loss: 4.66926303e-06
Iter: 145 loss: 4.6611267e-06
Iter: 146 loss: 4.64723689e-06
Iter: 147 loss: 4.64714867e-06
Iter: 148 loss: 4.62843673e-06
Iter: 149 loss: 4.86064073e-06
Iter: 150 loss: 4.62819571e-06
Iter: 151 loss: 4.61781e-06
Iter: 152 loss: 4.6063451e-06
Iter: 153 loss: 4.60470164e-06
Iter: 154 loss: 4.58567547e-06
Iter: 155 loss: 4.77624235e-06
Iter: 156 loss: 4.58518207e-06
Iter: 157 loss: 4.57256556e-06
Iter: 158 loss: 4.55951204e-06
Iter: 159 loss: 4.55713462e-06
Iter: 160 loss: 4.53881967e-06
Iter: 161 loss: 4.58335353e-06
Iter: 162 loss: 4.53214398e-06
Iter: 163 loss: 4.51119786e-06
Iter: 164 loss: 4.69211727e-06
Iter: 165 loss: 4.51007054e-06
Iter: 166 loss: 4.49571917e-06
Iter: 167 loss: 4.5055076e-06
Iter: 168 loss: 4.48680385e-06
Iter: 169 loss: 4.47295588e-06
Iter: 170 loss: 4.48862602e-06
Iter: 171 loss: 4.46545073e-06
Iter: 172 loss: 4.45046226e-06
Iter: 173 loss: 4.46930335e-06
Iter: 174 loss: 4.44272428e-06
Iter: 175 loss: 4.44280931e-06
Iter: 176 loss: 4.43612316e-06
Iter: 177 loss: 4.43112e-06
Iter: 178 loss: 4.42038572e-06
Iter: 179 loss: 4.59448302e-06
Iter: 180 loss: 4.4200774e-06
Iter: 181 loss: 4.40559916e-06
Iter: 182 loss: 4.42518558e-06
Iter: 183 loss: 4.39843689e-06
Iter: 184 loss: 4.38192365e-06
Iter: 185 loss: 4.59396233e-06
Iter: 186 loss: 4.3818045e-06
Iter: 187 loss: 4.37501512e-06
Iter: 188 loss: 4.36714708e-06
Iter: 189 loss: 4.36616301e-06
Iter: 190 loss: 4.34737e-06
Iter: 191 loss: 4.38502502e-06
Iter: 192 loss: 4.33978494e-06
Iter: 193 loss: 4.32849356e-06
Iter: 194 loss: 4.33842661e-06
Iter: 195 loss: 4.32188881e-06
Iter: 196 loss: 4.31219632e-06
Iter: 197 loss: 4.40084568e-06
Iter: 198 loss: 4.31178296e-06
Iter: 199 loss: 4.30284445e-06
Iter: 200 loss: 4.32575371e-06
Iter: 201 loss: 4.29978491e-06
Iter: 202 loss: 4.29146712e-06
Iter: 203 loss: 4.29065403e-06
Iter: 204 loss: 4.2847023e-06
Iter: 205 loss: 4.27548457e-06
Iter: 206 loss: 4.31270746e-06
Iter: 207 loss: 4.27334271e-06
Iter: 208 loss: 4.26436054e-06
Iter: 209 loss: 4.26264296e-06
Iter: 210 loss: 4.25654616e-06
Iter: 211 loss: 4.24255586e-06
Iter: 212 loss: 4.44717671e-06
Iter: 213 loss: 4.24254722e-06
Iter: 214 loss: 4.2351121e-06
Iter: 215 loss: 4.22463927e-06
Iter: 216 loss: 4.22429912e-06
Iter: 217 loss: 4.21744517e-06
Iter: 218 loss: 4.21735513e-06
Iter: 219 loss: 4.20998913e-06
Iter: 220 loss: 4.20054903e-06
Iter: 221 loss: 4.19993921e-06
Iter: 222 loss: 4.1910971e-06
Iter: 223 loss: 4.31591889e-06
Iter: 224 loss: 4.19102298e-06
Iter: 225 loss: 4.18477111e-06
Iter: 226 loss: 4.20395463e-06
Iter: 227 loss: 4.18285663e-06
Iter: 228 loss: 4.17883e-06
Iter: 229 loss: 4.16761031e-06
Iter: 230 loss: 4.23503434e-06
Iter: 231 loss: 4.16458806e-06
Iter: 232 loss: 4.15458e-06
Iter: 233 loss: 4.15337718e-06
Iter: 234 loss: 4.14715305e-06
Iter: 235 loss: 4.14395163e-06
Iter: 236 loss: 4.14102669e-06
Iter: 237 loss: 4.13052248e-06
Iter: 238 loss: 4.13086582e-06
Iter: 239 loss: 4.12219379e-06
Iter: 240 loss: 4.10830944e-06
Iter: 241 loss: 4.14776696e-06
Iter: 242 loss: 4.1037697e-06
Iter: 243 loss: 4.09702534e-06
Iter: 244 loss: 4.09665699e-06
Iter: 245 loss: 4.09009681e-06
Iter: 246 loss: 4.10056691e-06
Iter: 247 loss: 4.0870159e-06
Iter: 248 loss: 4.08170035e-06
Iter: 249 loss: 4.07524794e-06
Iter: 250 loss: 4.07467451e-06
Iter: 251 loss: 4.07164589e-06
Iter: 252 loss: 4.0697746e-06
Iter: 253 loss: 4.06711524e-06
Iter: 254 loss: 4.06280833e-06
Iter: 255 loss: 4.06276e-06
Iter: 256 loss: 4.05893888e-06
Iter: 257 loss: 4.05887795e-06
Iter: 258 loss: 4.05594392e-06
Iter: 259 loss: 4.04837283e-06
Iter: 260 loss: 4.11291512e-06
Iter: 261 loss: 4.04707635e-06
Iter: 262 loss: 4.0378718e-06
Iter: 263 loss: 4.08533197e-06
Iter: 264 loss: 4.03634749e-06
Iter: 265 loss: 4.02844671e-06
Iter: 266 loss: 4.11836481e-06
Iter: 267 loss: 4.02837713e-06
Iter: 268 loss: 4.02238766e-06
Iter: 269 loss: 4.0162231e-06
Iter: 270 loss: 4.01514944e-06
Iter: 271 loss: 4.00848057e-06
Iter: 272 loss: 4.03810236e-06
Iter: 273 loss: 4.00704812e-06
Iter: 274 loss: 3.99916598e-06
Iter: 275 loss: 4.00438375e-06
Iter: 276 loss: 3.99409782e-06
Iter: 277 loss: 3.99058899e-06
Iter: 278 loss: 3.98903194e-06
Iter: 279 loss: 3.98598149e-06
Iter: 280 loss: 3.98142038e-06
Iter: 281 loss: 3.98137036e-06
Iter: 282 loss: 3.97664098e-06
Iter: 283 loss: 3.99672626e-06
Iter: 284 loss: 3.97567055e-06
Iter: 285 loss: 3.96936093e-06
Iter: 286 loss: 3.98291195e-06
Iter: 287 loss: 3.96700761e-06
Iter: 288 loss: 3.9628726e-06
Iter: 289 loss: 3.97340409e-06
Iter: 290 loss: 3.961517e-06
Iter: 291 loss: 3.95652114e-06
Iter: 292 loss: 3.97323583e-06
Iter: 293 loss: 3.95519783e-06
Iter: 294 loss: 3.95059533e-06
Iter: 295 loss: 3.94143535e-06
Iter: 296 loss: 4.11991141e-06
Iter: 297 loss: 3.94134531e-06
Iter: 298 loss: 3.93606661e-06
Iter: 299 loss: 3.93574373e-06
Iter: 300 loss: 3.93057508e-06
Iter: 301 loss: 3.94053723e-06
Iter: 302 loss: 3.92837592e-06
Iter: 303 loss: 3.92475613e-06
Iter: 304 loss: 3.92068e-06
Iter: 305 loss: 3.92009451e-06
Iter: 306 loss: 3.91314461e-06
Iter: 307 loss: 3.94810831e-06
Iter: 308 loss: 3.91188814e-06
Iter: 309 loss: 3.90708828e-06
Iter: 310 loss: 3.96051519e-06
Iter: 311 loss: 3.90692912e-06
Iter: 312 loss: 3.90218156e-06
Iter: 313 loss: 3.9078227e-06
Iter: 314 loss: 3.89961951e-06
Iter: 315 loss: 3.89515071e-06
Iter: 316 loss: 3.89789056e-06
Iter: 317 loss: 3.89230445e-06
Iter: 318 loss: 3.88936041e-06
Iter: 319 loss: 3.88930312e-06
Iter: 320 loss: 3.88639683e-06
Iter: 321 loss: 3.88027138e-06
Iter: 322 loss: 3.98204793e-06
Iter: 323 loss: 3.88000808e-06
Iter: 324 loss: 3.87600812e-06
Iter: 325 loss: 3.87583304e-06
Iter: 326 loss: 3.87225464e-06
Iter: 327 loss: 3.87093314e-06
Iter: 328 loss: 3.86896045e-06
Iter: 329 loss: 3.86565353e-06
Iter: 330 loss: 3.86408919e-06
Iter: 331 loss: 3.86244665e-06
Iter: 332 loss: 3.85728617e-06
Iter: 333 loss: 3.9176939e-06
Iter: 334 loss: 3.8572507e-06
Iter: 335 loss: 3.85240082e-06
Iter: 336 loss: 3.85186695e-06
Iter: 337 loss: 3.84822806e-06
Iter: 338 loss: 3.84327632e-06
Iter: 339 loss: 3.84015948e-06
Iter: 340 loss: 3.83823772e-06
Iter: 341 loss: 3.83288716e-06
Iter: 342 loss: 3.83287761e-06
Iter: 343 loss: 3.82871576e-06
Iter: 344 loss: 3.85472322e-06
Iter: 345 loss: 3.82819417e-06
Iter: 346 loss: 3.82445842e-06
Iter: 347 loss: 3.82332382e-06
Iter: 348 loss: 3.82106055e-06
Iter: 349 loss: 3.81783957e-06
Iter: 350 loss: 3.83492261e-06
Iter: 351 loss: 3.81720929e-06
Iter: 352 loss: 3.81358768e-06
Iter: 353 loss: 3.82758071e-06
Iter: 354 loss: 3.81269274e-06
Iter: 355 loss: 3.8097e-06
Iter: 356 loss: 3.8064386e-06
Iter: 357 loss: 3.80589063e-06
Iter: 358 loss: 3.80175879e-06
Iter: 359 loss: 3.80177926e-06
Iter: 360 loss: 3.79935341e-06
Iter: 361 loss: 3.79551602e-06
Iter: 362 loss: 3.79551966e-06
Iter: 363 loss: 3.7900154e-06
Iter: 364 loss: 3.78831737e-06
Iter: 365 loss: 3.78498498e-06
Iter: 366 loss: 3.778571e-06
Iter: 367 loss: 3.77833749e-06
Iter: 368 loss: 3.77532842e-06
Iter: 369 loss: 3.7708603e-06
Iter: 370 loss: 3.77076276e-06
Iter: 371 loss: 3.76593766e-06
Iter: 372 loss: 3.78224831e-06
Iter: 373 loss: 3.76470803e-06
Iter: 374 loss: 3.76080061e-06
Iter: 375 loss: 3.80592155e-06
Iter: 376 loss: 3.76066646e-06
Iter: 377 loss: 3.7567786e-06
Iter: 378 loss: 3.76449407e-06
Iter: 379 loss: 3.75520131e-06
Iter: 380 loss: 3.75259629e-06
Iter: 381 loss: 3.75529225e-06
Iter: 382 loss: 3.75111517e-06
Iter: 383 loss: 3.74785827e-06
Iter: 384 loss: 3.7616519e-06
Iter: 385 loss: 3.74723277e-06
Iter: 386 loss: 3.74310662e-06
Iter: 387 loss: 3.74583396e-06
Iter: 388 loss: 3.74047613e-06
Iter: 389 loss: 3.73796502e-06
Iter: 390 loss: 3.75361424e-06
Iter: 391 loss: 3.7376426e-06
Iter: 392 loss: 3.73465946e-06
Iter: 393 loss: 3.73507714e-06
Iter: 394 loss: 3.73238436e-06
Iter: 395 loss: 3.72851605e-06
Iter: 396 loss: 3.72431168e-06
Iter: 397 loss: 3.72369936e-06
Iter: 398 loss: 3.72221712e-06
Iter: 399 loss: 3.72127147e-06
Iter: 400 loss: 3.71895749e-06
Iter: 401 loss: 3.71715146e-06
Iter: 402 loss: 3.71658e-06
Iter: 403 loss: 3.71263945e-06
Iter: 404 loss: 3.71170654e-06
Iter: 405 loss: 3.70932685e-06
Iter: 406 loss: 3.70529119e-06
Iter: 407 loss: 3.7239829e-06
Iter: 408 loss: 3.70454154e-06
Iter: 409 loss: 3.70071484e-06
Iter: 410 loss: 3.7534428e-06
Iter: 411 loss: 3.70075122e-06
Iter: 412 loss: 3.69840359e-06
Iter: 413 loss: 3.69595591e-06
Iter: 414 loss: 3.69553118e-06
Iter: 415 loss: 3.69182453e-06
Iter: 416 loss: 3.70581893e-06
Iter: 417 loss: 3.69093573e-06
Iter: 418 loss: 3.68754195e-06
Iter: 419 loss: 3.71969554e-06
Iter: 420 loss: 3.68739848e-06
Iter: 421 loss: 3.68491965e-06
Iter: 422 loss: 3.68076212e-06
Iter: 423 loss: 3.68074052e-06
Iter: 424 loss: 3.67692292e-06
Iter: 425 loss: 3.67693474e-06
Iter: 426 loss: 3.67420444e-06
Iter: 427 loss: 3.67395091e-06
Iter: 428 loss: 3.67209054e-06
Iter: 429 loss: 3.66930476e-06
Iter: 430 loss: 3.66896347e-06
Iter: 431 loss: 3.66693666e-06
Iter: 432 loss: 3.66505765e-06
Iter: 433 loss: 3.66474023e-06
Iter: 434 loss: 3.66293352e-06
Iter: 435 loss: 3.65974938e-06
Iter: 436 loss: 3.6597653e-06
Iter: 437 loss: 3.65631104e-06
Iter: 438 loss: 3.65622964e-06
Iter: 439 loss: 3.65344022e-06
Iter: 440 loss: 3.65136339e-06
Iter: 441 loss: 3.65089454e-06
Iter: 442 loss: 3.64848802e-06
Iter: 443 loss: 3.6518245e-06
Iter: 444 loss: 3.64730658e-06
Iter: 445 loss: 3.64469406e-06
Iter: 446 loss: 3.63973595e-06
Iter: 447 loss: 3.74784827e-06
Iter: 448 loss: 3.63964682e-06
Iter: 449 loss: 3.63712115e-06
Iter: 450 loss: 3.63632353e-06
Iter: 451 loss: 3.63431582e-06
Iter: 452 loss: 3.63791446e-06
Iter: 453 loss: 3.63345316e-06
Iter: 454 loss: 3.63157e-06
Iter: 455 loss: 3.63113395e-06
Iter: 456 loss: 3.6299225e-06
Iter: 457 loss: 3.62727678e-06
Iter: 458 loss: 3.65380515e-06
Iter: 459 loss: 3.62712353e-06
Iter: 460 loss: 3.62554556e-06
Iter: 461 loss: 3.62228366e-06
Iter: 462 loss: 3.68534347e-06
Iter: 463 loss: 3.62232049e-06
Iter: 464 loss: 3.61875573e-06
Iter: 465 loss: 3.64059588e-06
Iter: 466 loss: 3.6182314e-06
Iter: 467 loss: 3.61530056e-06
Iter: 468 loss: 3.64689822e-06
Iter: 469 loss: 3.61530601e-06
Iter: 470 loss: 3.61330967e-06
Iter: 471 loss: 3.60962235e-06
Iter: 472 loss: 3.68873702e-06
Iter: 473 loss: 3.60968556e-06
Iter: 474 loss: 3.60508238e-06
Iter: 475 loss: 3.61036473e-06
Iter: 476 loss: 3.60257718e-06
Iter: 477 loss: 3.60257422e-06
Iter: 478 loss: 3.60038462e-06
Iter: 479 loss: 3.59868386e-06
Iter: 480 loss: 3.59515298e-06
Iter: 481 loss: 3.65710957e-06
Iter: 482 loss: 3.59509522e-06
Iter: 483 loss: 3.5924686e-06
Iter: 484 loss: 3.61571142e-06
Iter: 485 loss: 3.59243131e-06
Iter: 486 loss: 3.58981697e-06
Iter: 487 loss: 3.59935575e-06
Iter: 488 loss: 3.58923444e-06
Iter: 489 loss: 3.58659508e-06
Iter: 490 loss: 3.58581315e-06
Iter: 491 loss: 3.58424586e-06
Iter: 492 loss: 3.58166471e-06
Iter: 493 loss: 3.61075945e-06
Iter: 494 loss: 3.58162083e-06
Iter: 495 loss: 3.57920726e-06
Iter: 496 loss: 3.57873841e-06
Iter: 497 loss: 3.57713043e-06
Iter: 498 loss: 3.57402951e-06
Iter: 499 loss: 3.57612976e-06
Iter: 500 loss: 3.5720991e-06
Iter: 501 loss: 3.56956753e-06
Iter: 502 loss: 3.59308615e-06
Iter: 503 loss: 3.56945134e-06
Iter: 504 loss: 3.56664123e-06
Iter: 505 loss: 3.57201793e-06
Iter: 506 loss: 3.5655e-06
Iter: 507 loss: 3.56356941e-06
Iter: 508 loss: 3.56233363e-06
Iter: 509 loss: 3.561624e-06
Iter: 510 loss: 3.55889824e-06
Iter: 511 loss: 3.57258841e-06
Iter: 512 loss: 3.55844304e-06
Iter: 513 loss: 3.55606e-06
Iter: 514 loss: 3.58732359e-06
Iter: 515 loss: 3.555994e-06
Iter: 516 loss: 3.55474549e-06
Iter: 517 loss: 3.5519829e-06
Iter: 518 loss: 3.59122168e-06
Iter: 519 loss: 3.55182237e-06
Iter: 520 loss: 3.54840313e-06
Iter: 521 loss: 3.57562226e-06
Iter: 522 loss: 3.54820645e-06
Iter: 523 loss: 3.54444069e-06
Iter: 524 loss: 3.55112707e-06
Iter: 525 loss: 3.5429282e-06
Iter: 526 loss: 3.54096278e-06
Iter: 527 loss: 3.55000179e-06
Iter: 528 loss: 3.54053373e-06
Iter: 529 loss: 3.53854853e-06
Iter: 530 loss: 3.54132544e-06
Iter: 531 loss: 3.53756559e-06
Iter: 532 loss: 3.53485666e-06
Iter: 533 loss: 3.53496898e-06
Iter: 534 loss: 3.53263931e-06
Iter: 535 loss: 3.53008272e-06
Iter: 536 loss: 3.53539758e-06
Iter: 537 loss: 3.52910502e-06
Iter: 538 loss: 3.52740312e-06
Iter: 539 loss: 3.52739e-06
Iter: 540 loss: 3.5260166e-06
Iter: 541 loss: 3.52384814e-06
Iter: 542 loss: 3.52385314e-06
Iter: 543 loss: 3.52100255e-06
Iter: 544 loss: 3.52286884e-06
Iter: 545 loss: 3.51913741e-06
Iter: 546 loss: 3.51738913e-06
Iter: 547 loss: 3.51724884e-06
Iter: 548 loss: 3.51489166e-06
Iter: 549 loss: 3.51063863e-06
Iter: 550 loss: 3.51065682e-06
Iter: 551 loss: 3.50735695e-06
Iter: 552 loss: 3.52376105e-06
Iter: 553 loss: 3.50669234e-06
Iter: 554 loss: 3.50490063e-06
Iter: 555 loss: 3.50480445e-06
Iter: 556 loss: 3.50352116e-06
Iter: 557 loss: 3.50105893e-06
Iter: 558 loss: 3.55360385e-06
Iter: 559 loss: 3.50102937e-06
Iter: 560 loss: 3.49838206e-06
Iter: 561 loss: 3.52452071e-06
Iter: 562 loss: 3.49826769e-06
Iter: 563 loss: 3.49622837e-06
Iter: 564 loss: 3.50270466e-06
Iter: 565 loss: 3.49566653e-06
Iter: 566 loss: 3.49421066e-06
Iter: 567 loss: 3.49293555e-06
Iter: 568 loss: 3.49251286e-06
Iter: 569 loss: 3.48971935e-06
Iter: 570 loss: 3.49512584e-06
Iter: 571 loss: 3.48856656e-06
Iter: 572 loss: 3.48630374e-06
Iter: 573 loss: 3.48630692e-06
Iter: 574 loss: 3.48484673e-06
Iter: 575 loss: 3.48091703e-06
Iter: 576 loss: 3.50162213e-06
Iter: 577 loss: 3.47964396e-06
Iter: 578 loss: 3.47449873e-06
Iter: 579 loss: 3.51183871e-06
Iter: 580 loss: 3.47407399e-06
Iter: 581 loss: 3.4736679e-06
Iter: 582 loss: 3.47255968e-06
Iter: 583 loss: 3.4713687e-06
Iter: 584 loss: 3.46974116e-06
Iter: 585 loss: 3.46970432e-06
Iter: 586 loss: 3.46795582e-06
Iter: 587 loss: 3.46765296e-06
Iter: 588 loss: 3.46638626e-06
Iter: 589 loss: 3.46356023e-06
Iter: 590 loss: 3.50415962e-06
Iter: 591 loss: 3.46355569e-06
Iter: 592 loss: 3.46250658e-06
Iter: 593 loss: 3.46129968e-06
Iter: 594 loss: 3.46108141e-06
Iter: 595 loss: 3.45934814e-06
Iter: 596 loss: 3.47735295e-06
Iter: 597 loss: 3.4593138e-06
Iter: 598 loss: 3.45780563e-06
Iter: 599 loss: 3.45503622e-06
Iter: 600 loss: 3.51232552e-06
Iter: 601 loss: 3.45505305e-06
Iter: 602 loss: 3.45140074e-06
Iter: 603 loss: 3.46363504e-06
Iter: 604 loss: 3.4504219e-06
Iter: 605 loss: 3.44766e-06
Iter: 606 loss: 3.46970069e-06
Iter: 607 loss: 3.44754244e-06
Iter: 608 loss: 3.44449927e-06
Iter: 609 loss: 3.44873115e-06
Iter: 610 loss: 3.44306454e-06
Iter: 611 loss: 3.44044975e-06
Iter: 612 loss: 3.44373802e-06
Iter: 613 loss: 3.43918714e-06
Iter: 614 loss: 3.4371435e-06
Iter: 615 loss: 3.44189516e-06
Iter: 616 loss: 3.43642523e-06
Iter: 617 loss: 3.43448528e-06
Iter: 618 loss: 3.43447391e-06
Iter: 619 loss: 3.43326587e-06
Iter: 620 loss: 3.43086322e-06
Iter: 621 loss: 3.47129708e-06
Iter: 622 loss: 3.43087299e-06
Iter: 623 loss: 3.42892645e-06
Iter: 624 loss: 3.45834246e-06
Iter: 625 loss: 3.42888052e-06
Iter: 626 loss: 3.42699468e-06
Iter: 627 loss: 3.43063448e-06
Iter: 628 loss: 3.42626481e-06
Iter: 629 loss: 3.42487624e-06
Iter: 630 loss: 3.42466365e-06
Iter: 631 loss: 3.42364e-06
Iter: 632 loss: 3.4216032e-06
Iter: 633 loss: 3.43944475e-06
Iter: 634 loss: 3.42147268e-06
Iter: 635 loss: 3.41963391e-06
Iter: 636 loss: 3.41570922e-06
Iter: 637 loss: 3.48160211e-06
Iter: 638 loss: 3.41566215e-06
Iter: 639 loss: 3.41296982e-06
Iter: 640 loss: 3.45201852e-06
Iter: 641 loss: 3.41302348e-06
Iter: 642 loss: 3.41137616e-06
Iter: 643 loss: 3.42143085e-06
Iter: 644 loss: 3.41116606e-06
Iter: 645 loss: 3.40974111e-06
Iter: 646 loss: 3.4153e-06
Iter: 647 loss: 3.40935821e-06
Iter: 648 loss: 3.40820202e-06
Iter: 649 loss: 3.40601605e-06
Iter: 650 loss: 3.45845433e-06
Iter: 651 loss: 3.40602378e-06
Iter: 652 loss: 3.40429665e-06
Iter: 653 loss: 3.40425458e-06
Iter: 654 loss: 3.40259135e-06
Iter: 655 loss: 3.41042551e-06
Iter: 656 loss: 3.40227325e-06
Iter: 657 loss: 3.40109e-06
Iter: 658 loss: 3.39894859e-06
Iter: 659 loss: 3.44305772e-06
Iter: 660 loss: 3.39903568e-06
Iter: 661 loss: 3.39685243e-06
Iter: 662 loss: 3.42031649e-06
Iter: 663 loss: 3.3967392e-06
Iter: 664 loss: 3.39404482e-06
Iter: 665 loss: 3.39444318e-06
Iter: 666 loss: 3.39201779e-06
Iter: 667 loss: 3.38968744e-06
Iter: 668 loss: 3.39370945e-06
Iter: 669 loss: 3.38878249e-06
Iter: 670 loss: 3.38626046e-06
Iter: 671 loss: 3.41300938e-06
Iter: 672 loss: 3.38630912e-06
Iter: 673 loss: 3.38506493e-06
Iter: 674 loss: 3.38321706e-06
Iter: 675 loss: 3.38318796e-06
Iter: 676 loss: 3.38084237e-06
Iter: 677 loss: 3.39167491e-06
Iter: 678 loss: 3.38045743e-06
Iter: 679 loss: 3.37918937e-06
Iter: 680 loss: 3.37920073e-06
Iter: 681 loss: 3.37801748e-06
Iter: 682 loss: 3.37656593e-06
Iter: 683 loss: 3.37638903e-06
Iter: 684 loss: 3.37388519e-06
Iter: 685 loss: 3.3769802e-06
Iter: 686 loss: 3.37262327e-06
Iter: 687 loss: 3.3703343e-06
Iter: 688 loss: 3.38270638e-06
Iter: 689 loss: 3.37009533e-06
Iter: 690 loss: 3.36728021e-06
Iter: 691 loss: 3.37582969e-06
Iter: 692 loss: 3.36645e-06
Iter: 693 loss: 3.36484914e-06
Iter: 694 loss: 3.36296625e-06
Iter: 695 loss: 3.3628462e-06
Iter: 696 loss: 3.36130984e-06
Iter: 697 loss: 3.36118728e-06
Iter: 698 loss: 3.35965478e-06
Iter: 699 loss: 3.35907544e-06
Iter: 700 loss: 3.35806703e-06
Iter: 701 loss: 3.35656409e-06
Iter: 702 loss: 3.36139624e-06
Iter: 703 loss: 3.35614027e-06
Iter: 704 loss: 3.35503637e-06
Iter: 705 loss: 3.36931294e-06
Iter: 706 loss: 3.35498839e-06
Iter: 707 loss: 3.35402865e-06
Iter: 708 loss: 3.35126151e-06
Iter: 709 loss: 3.36218181e-06
Iter: 710 loss: 3.35015784e-06
Iter: 711 loss: 3.34706738e-06
Iter: 712 loss: 3.37877964e-06
Iter: 713 loss: 3.34695e-06
Iter: 714 loss: 3.34497736e-06
Iter: 715 loss: 3.35919412e-06
Iter: 716 loss: 3.34482183e-06
Iter: 717 loss: 3.34307879e-06
Iter: 718 loss: 3.35323557e-06
Iter: 719 loss: 3.34281663e-06
Iter: 720 loss: 3.34108745e-06
Iter: 721 loss: 3.33856929e-06
Iter: 722 loss: 3.33848311e-06
Iter: 723 loss: 3.33624234e-06
Iter: 724 loss: 3.3604972e-06
Iter: 725 loss: 3.33617845e-06
Iter: 726 loss: 3.33483945e-06
Iter: 727 loss: 3.33594539e-06
Iter: 728 loss: 3.33410071e-06
Iter: 729 loss: 3.33251205e-06
Iter: 730 loss: 3.33252751e-06
Iter: 731 loss: 3.33186631e-06
Iter: 732 loss: 3.33029675e-06
Iter: 733 loss: 3.3506376e-06
Iter: 734 loss: 3.33017715e-06
Iter: 735 loss: 3.32827267e-06
Iter: 736 loss: 3.33472963e-06
Iter: 737 loss: 3.32769332e-06
Iter: 738 loss: 3.32555e-06
Iter: 739 loss: 3.34059769e-06
Iter: 740 loss: 3.32531408e-06
Iter: 741 loss: 3.32391528e-06
Iter: 742 loss: 3.32607965e-06
Iter: 743 loss: 3.32327045e-06
Iter: 744 loss: 3.32181753e-06
Iter: 745 loss: 3.32679974e-06
Iter: 746 loss: 3.3213214e-06
Iter: 747 loss: 3.31989554e-06
Iter: 748 loss: 3.31652427e-06
Iter: 749 loss: 3.35829031e-06
Iter: 750 loss: 3.31633396e-06
Iter: 751 loss: 3.31447154e-06
Iter: 752 loss: 3.3145061e-06
Iter: 753 loss: 3.3130907e-06
Iter: 754 loss: 3.31397314e-06
Iter: 755 loss: 3.3122376e-06
Iter: 756 loss: 3.31073079e-06
Iter: 757 loss: 3.31073625e-06
Iter: 758 loss: 3.30967123e-06
Iter: 759 loss: 3.30812713e-06
Iter: 760 loss: 3.30802959e-06
Iter: 761 loss: 3.30614148e-06
Iter: 762 loss: 3.31325782e-06
Iter: 763 loss: 3.30573334e-06
Iter: 764 loss: 3.3035808e-06
Iter: 765 loss: 3.30378862e-06
Iter: 766 loss: 3.30204966e-06
Iter: 767 loss: 3.30140165e-06
Iter: 768 loss: 3.30086664e-06
Iter: 769 loss: 3.29974182e-06
Iter: 770 loss: 3.298519e-06
Iter: 771 loss: 3.29834711e-06
Iter: 772 loss: 3.29681279e-06
Iter: 773 loss: 3.30555872e-06
Iter: 774 loss: 3.29667182e-06
Iter: 775 loss: 3.29510453e-06
Iter: 776 loss: 3.29733643e-06
Iter: 777 loss: 3.29436239e-06
Iter: 778 loss: 3.29316481e-06
Iter: 779 loss: 3.29658769e-06
Iter: 780 loss: 3.29281306e-06
Iter: 781 loss: 3.29168665e-06
Iter: 782 loss: 3.29738668e-06
Iter: 783 loss: 3.29156819e-06
Iter: 784 loss: 3.29064096e-06
Iter: 785 loss: 3.28858823e-06
Iter: 786 loss: 3.3208521e-06
Iter: 787 loss: 3.28857959e-06
Iter: 788 loss: 3.28645228e-06
Iter: 789 loss: 3.29282057e-06
Iter: 790 loss: 3.28584133e-06
Iter: 791 loss: 3.28346414e-06
Iter: 792 loss: 3.29211116e-06
Iter: 793 loss: 3.28287024e-06
Iter: 794 loss: 3.28142141e-06
Iter: 795 loss: 3.29848285e-06
Iter: 796 loss: 3.28143324e-06
Iter: 797 loss: 3.279873e-06
Iter: 798 loss: 3.28149872e-06
Iter: 799 loss: 3.27902785e-06
Iter: 800 loss: 3.27724979e-06
Iter: 801 loss: 3.2790183e-06
Iter: 802 loss: 3.27624775e-06
Iter: 803 loss: 3.27455291e-06
Iter: 804 loss: 3.27949056e-06
Iter: 805 loss: 3.27407042e-06
Iter: 806 loss: 3.27295311e-06
Iter: 807 loss: 3.27297448e-06
Iter: 808 loss: 3.2717935e-06
Iter: 809 loss: 3.26974441e-06
Iter: 810 loss: 3.26967574e-06
Iter: 811 loss: 3.2681653e-06
Iter: 812 loss: 3.27548742e-06
Iter: 813 loss: 3.26784289e-06
Iter: 814 loss: 3.26644067e-06
Iter: 815 loss: 3.27698945e-06
Iter: 816 loss: 3.26630743e-06
Iter: 817 loss: 3.26489771e-06
Iter: 818 loss: 3.26383042e-06
Iter: 819 loss: 3.26345344e-06
Iter: 820 loss: 3.26229701e-06
Iter: 821 loss: 3.26226836e-06
Iter: 822 loss: 3.26125428e-06
Iter: 823 loss: 3.25982728e-06
Iter: 824 loss: 3.25981136e-06
Iter: 825 loss: 3.25833344e-06
Iter: 826 loss: 3.25798464e-06
Iter: 827 loss: 3.25706242e-06
Iter: 828 loss: 3.2547232e-06
Iter: 829 loss: 3.26417012e-06
Iter: 830 loss: 3.2542448e-06
Iter: 831 loss: 3.25226824e-06
Iter: 832 loss: 3.25952897e-06
Iter: 833 loss: 3.25173e-06
Iter: 834 loss: 3.2501423e-06
Iter: 835 loss: 3.27231032e-06
Iter: 836 loss: 3.25012411e-06
Iter: 837 loss: 3.24889561e-06
Iter: 838 loss: 3.24931852e-06
Iter: 839 loss: 3.24805205e-06
Iter: 840 loss: 3.24644816e-06
Iter: 841 loss: 3.24636221e-06
Iter: 842 loss: 3.24517623e-06
Iter: 843 loss: 3.2433727e-06
Iter: 844 loss: 3.25152178e-06
Iter: 845 loss: 3.24306257e-06
Iter: 846 loss: 3.24222287e-06
Iter: 847 loss: 3.24201119e-06
Iter: 848 loss: 3.24138e-06
Iter: 849 loss: 3.2400369e-06
Iter: 850 loss: 3.26538066e-06
Iter: 851 loss: 3.24006305e-06
Iter: 852 loss: 3.23865925e-06
Iter: 853 loss: 3.24721123e-06
Iter: 854 loss: 3.23856125e-06
Iter: 855 loss: 3.23700328e-06
Iter: 856 loss: 3.23936752e-06
Iter: 857 loss: 3.23618292e-06
Iter: 858 loss: 3.23487984e-06
Iter: 859 loss: 3.23441236e-06
Iter: 860 loss: 3.23363884e-06
Iter: 861 loss: 3.23187828e-06
Iter: 862 loss: 3.23197264e-06
Iter: 863 loss: 3.23100949e-06
Iter: 864 loss: 3.22935966e-06
Iter: 865 loss: 3.26540203e-06
Iter: 866 loss: 3.22931e-06
Iter: 867 loss: 3.22701226e-06
Iter: 868 loss: 3.22695109e-06
Iter: 869 loss: 3.22517781e-06
Iter: 870 loss: 3.22392111e-06
Iter: 871 loss: 3.22380174e-06
Iter: 872 loss: 3.22240385e-06
Iter: 873 loss: 3.22604365e-06
Iter: 874 loss: 3.22184951e-06
Iter: 875 loss: 3.22091842e-06
Iter: 876 loss: 3.220998e-06
Iter: 877 loss: 3.22019059e-06
Iter: 878 loss: 3.21902462e-06
Iter: 879 loss: 3.22335222e-06
Iter: 880 loss: 3.21859761e-06
Iter: 881 loss: 3.21767061e-06
Iter: 882 loss: 3.22824326e-06
Iter: 883 loss: 3.21758102e-06
Iter: 884 loss: 3.21634639e-06
Iter: 885 loss: 3.21508014e-06
Iter: 886 loss: 3.21486414e-06
Iter: 887 loss: 3.21316929e-06
Iter: 888 loss: 3.2163457e-06
Iter: 889 loss: 3.21238076e-06
Iter: 890 loss: 3.21114703e-06
Iter: 891 loss: 3.22529036e-06
Iter: 892 loss: 3.21113703e-06
Iter: 893 loss: 3.20962067e-06
Iter: 894 loss: 3.208419e-06
Iter: 895 loss: 3.20795402e-06
Iter: 896 loss: 3.20627714e-06
Iter: 897 loss: 3.21687935e-06
Iter: 898 loss: 3.20608115e-06
Iter: 899 loss: 3.20419304e-06
Iter: 900 loss: 3.21020048e-06
Iter: 901 loss: 3.20366235e-06
Iter: 902 loss: 3.20269282e-06
Iter: 903 loss: 3.20115964e-06
Iter: 904 loss: 3.20118488e-06
Iter: 905 loss: 3.19912397e-06
Iter: 906 loss: 3.20842901e-06
Iter: 907 loss: 3.19867422e-06
Iter: 908 loss: 3.19751507e-06
Iter: 909 loss: 3.20840036e-06
Iter: 910 loss: 3.19739274e-06
Iter: 911 loss: 3.19582637e-06
Iter: 912 loss: 3.19541869e-06
Iter: 913 loss: 3.19438823e-06
Iter: 914 loss: 3.19242895e-06
Iter: 915 loss: 3.19330297e-06
Iter: 916 loss: 3.19118362e-06
Iter: 917 loss: 3.19009223e-06
Iter: 918 loss: 3.18991829e-06
Iter: 919 loss: 3.18867274e-06
Iter: 920 loss: 3.18814341e-06
Iter: 921 loss: 3.18746834e-06
Iter: 922 loss: 3.18557704e-06
Iter: 923 loss: 3.18731668e-06
Iter: 924 loss: 3.18451771e-06
Iter: 925 loss: 3.18316052e-06
Iter: 926 loss: 3.19458377e-06
Iter: 927 loss: 3.18311049e-06
Iter: 928 loss: 3.18181037e-06
Iter: 929 loss: 3.18707248e-06
Iter: 930 loss: 3.18148227e-06
Iter: 931 loss: 3.1806951e-06
Iter: 932 loss: 3.1816985e-06
Iter: 933 loss: 3.18018328e-06
Iter: 934 loss: 3.17949957e-06
Iter: 935 loss: 3.19032142e-06
Iter: 936 loss: 3.17947342e-06
Iter: 937 loss: 3.17896092e-06
Iter: 938 loss: 3.17731519e-06
Iter: 939 loss: 3.18684397e-06
Iter: 940 loss: 3.17694366e-06
Iter: 941 loss: 3.17494664e-06
Iter: 942 loss: 3.18028515e-06
Iter: 943 loss: 3.17429863e-06
Iter: 944 loss: 3.17280319e-06
Iter: 945 loss: 3.19630226e-06
Iter: 946 loss: 3.17280455e-06
Iter: 947 loss: 3.17111744e-06
Iter: 948 loss: 3.17036961e-06
Iter: 949 loss: 3.16946353e-06
Iter: 950 loss: 3.16765909e-06
Iter: 951 loss: 3.17106446e-06
Iter: 952 loss: 3.16683e-06
Iter: 953 loss: 3.16602222e-06
Iter: 954 loss: 3.16591058e-06
Iter: 955 loss: 3.16494948e-06
Iter: 956 loss: 3.1638931e-06
Iter: 957 loss: 3.16377918e-06
Iter: 958 loss: 3.1624586e-06
Iter: 959 loss: 3.16203818e-06
Iter: 960 loss: 3.16127966e-06
Iter: 961 loss: 3.16102251e-06
Iter: 962 loss: 3.16040428e-06
Iter: 963 loss: 3.15980242e-06
Iter: 964 loss: 3.15860598e-06
Iter: 965 loss: 3.18434286e-06
Iter: 966 loss: 3.15862098e-06
Iter: 967 loss: 3.15697457e-06
Iter: 968 loss: 3.16565183e-06
Iter: 969 loss: 3.15679745e-06
Iter: 970 loss: 3.15525904e-06
Iter: 971 loss: 3.16210026e-06
Iter: 972 loss: 3.15497414e-06
Iter: 973 loss: 3.15393936e-06
Iter: 974 loss: 3.15245416e-06
Iter: 975 loss: 3.15242664e-06
Iter: 976 loss: 3.1504685e-06
Iter: 977 loss: 3.15726334e-06
Iter: 978 loss: 3.1499344e-06
Iter: 979 loss: 3.14881026e-06
Iter: 980 loss: 3.16045225e-06
Iter: 981 loss: 3.14878207e-06
Iter: 982 loss: 3.14763656e-06
Iter: 983 loss: 3.1521e-06
Iter: 984 loss: 3.1473935e-06
Iter: 985 loss: 3.14645854e-06
Iter: 986 loss: 3.14491854e-06
Iter: 987 loss: 3.18214461e-06
Iter: 988 loss: 3.14487397e-06
Iter: 989 loss: 3.14442696e-06
Iter: 990 loss: 3.14409294e-06
Iter: 991 loss: 3.14319823e-06
Iter: 992 loss: 3.14195518e-06
Iter: 993 loss: 3.14188787e-06
Iter: 994 loss: 3.14042518e-06
Iter: 995 loss: 3.14030467e-06
Iter: 996 loss: 3.13927239e-06
Iter: 997 loss: 3.13741748e-06
Iter: 998 loss: 3.15037551e-06
Iter: 999 loss: 3.13722126e-06
Iter: 1000 loss: 3.13525152e-06
Iter: 1001 loss: 3.14578847e-06
Iter: 1002 loss: 3.13499777e-06
Iter: 1003 loss: 3.13409191e-06
Iter: 1004 loss: 3.13414921e-06
Iter: 1005 loss: 3.13336113e-06
Iter: 1006 loss: 3.13197233e-06
Iter: 1007 loss: 3.1423142e-06
Iter: 1008 loss: 3.13184319e-06
Iter: 1009 loss: 3.13097394e-06
Iter: 1010 loss: 3.13012083e-06
Iter: 1011 loss: 3.12989846e-06
Iter: 1012 loss: 3.12847123e-06
Iter: 1013 loss: 3.13063083e-06
Iter: 1014 loss: 3.12783482e-06
Iter: 1015 loss: 3.12642578e-06
Iter: 1016 loss: 3.13678402e-06
Iter: 1017 loss: 3.12628117e-06
Iter: 1018 loss: 3.12525663e-06
Iter: 1019 loss: 3.13196506e-06
Iter: 1020 loss: 3.12515158e-06
Iter: 1021 loss: 3.12402199e-06
Iter: 1022 loss: 3.12286829e-06
Iter: 1023 loss: 3.12263501e-06
Iter: 1024 loss: 3.12146722e-06
Iter: 1025 loss: 3.13430837e-06
Iter: 1026 loss: 3.12141924e-06
Iter: 1027 loss: 3.11995564e-06
Iter: 1028 loss: 3.11968142e-06
Iter: 1029 loss: 3.11872054e-06
Iter: 1030 loss: 3.11729491e-06
Iter: 1031 loss: 3.11738677e-06
Iter: 1032 loss: 3.11624535e-06
Iter: 1033 loss: 3.11517761e-06
Iter: 1034 loss: 3.11512485e-06
Iter: 1035 loss: 3.11410372e-06
Iter: 1036 loss: 3.11500548e-06
Iter: 1037 loss: 3.11341273e-06
Iter: 1038 loss: 3.11238273e-06
Iter: 1039 loss: 3.11350232e-06
Iter: 1040 loss: 3.11180111e-06
Iter: 1041 loss: 3.11073018e-06
Iter: 1042 loss: 3.12333532e-06
Iter: 1043 loss: 3.11067583e-06
Iter: 1044 loss: 3.11008716e-06
Iter: 1045 loss: 3.10825681e-06
Iter: 1046 loss: 3.11355689e-06
Iter: 1047 loss: 3.10720702e-06
Iter: 1048 loss: 3.10533414e-06
Iter: 1049 loss: 3.10530481e-06
Iter: 1050 loss: 3.10380779e-06
Iter: 1051 loss: 3.11786152e-06
Iter: 1052 loss: 3.10384212e-06
Iter: 1053 loss: 3.10224414e-06
Iter: 1054 loss: 3.10179803e-06
Iter: 1055 loss: 3.10097948e-06
Iter: 1056 loss: 3.09909933e-06
Iter: 1057 loss: 3.10360701e-06
Iter: 1058 loss: 3.09847519e-06
Iter: 1059 loss: 3.09741677e-06
Iter: 1060 loss: 3.09737743e-06
Iter: 1061 loss: 3.09655638e-06
Iter: 1062 loss: 3.09564666e-06
Iter: 1063 loss: 3.09551388e-06
Iter: 1064 loss: 3.09412e-06
Iter: 1065 loss: 3.09353732e-06
Iter: 1066 loss: 3.09288157e-06
Iter: 1067 loss: 3.09142115e-06
Iter: 1068 loss: 3.11129952e-06
Iter: 1069 loss: 3.09143661e-06
Iter: 1070 loss: 3.0899057e-06
Iter: 1071 loss: 3.09113329e-06
Iter: 1072 loss: 3.08897461e-06
Iter: 1073 loss: 3.08771723e-06
Iter: 1074 loss: 3.08956828e-06
Iter: 1075 loss: 3.08717154e-06
Iter: 1076 loss: 3.08575954e-06
Iter: 1077 loss: 3.09720531e-06
Iter: 1078 loss: 3.08559811e-06
Iter: 1079 loss: 3.08466429e-06
Iter: 1080 loss: 3.08257881e-06
Iter: 1081 loss: 3.10921087e-06
Iter: 1082 loss: 3.08245581e-06
Iter: 1083 loss: 3.08019276e-06
Iter: 1084 loss: 3.09353732e-06
Iter: 1085 loss: 3.07987511e-06
Iter: 1086 loss: 3.07879509e-06
Iter: 1087 loss: 3.07880964e-06
Iter: 1088 loss: 3.07766231e-06
Iter: 1089 loss: 3.07660571e-06
Iter: 1090 loss: 3.07630603e-06
Iter: 1091 loss: 3.07502137e-06
Iter: 1092 loss: 3.08084304e-06
Iter: 1093 loss: 3.07483106e-06
Iter: 1094 loss: 3.073583e-06
Iter: 1095 loss: 3.08463973e-06
Iter: 1096 loss: 3.07353434e-06
Iter: 1097 loss: 3.07263599e-06
Iter: 1098 loss: 3.07043547e-06
Iter: 1099 loss: 3.08586414e-06
Iter: 1100 loss: 3.06986703e-06
Iter: 1101 loss: 3.06752668e-06
Iter: 1102 loss: 3.1011341e-06
Iter: 1103 loss: 3.06754669e-06
Iter: 1104 loss: 3.06646939e-06
Iter: 1105 loss: 3.06646643e-06
Iter: 1106 loss: 3.06569882e-06
Iter: 1107 loss: 3.06447919e-06
Iter: 1108 loss: 3.06452102e-06
Iter: 1109 loss: 3.06274433e-06
Iter: 1110 loss: 3.06803418e-06
Iter: 1111 loss: 3.06221455e-06
Iter: 1112 loss: 3.0607855e-06
Iter: 1113 loss: 3.07856271e-06
Iter: 1114 loss: 3.0607323e-06
Iter: 1115 loss: 3.06002403e-06
Iter: 1116 loss: 3.05854314e-06
Iter: 1117 loss: 3.08326639e-06
Iter: 1118 loss: 3.05849426e-06
Iter: 1119 loss: 3.0567644e-06
Iter: 1120 loss: 3.06350421e-06
Iter: 1121 loss: 3.05634239e-06
Iter: 1122 loss: 3.05518506e-06
Iter: 1123 loss: 3.05511685e-06
Iter: 1124 loss: 3.0538863e-06
Iter: 1125 loss: 3.05323397e-06
Iter: 1126 loss: 3.05276671e-06
Iter: 1127 loss: 3.05102e-06
Iter: 1128 loss: 3.05385379e-06
Iter: 1129 loss: 3.05025651e-06
Iter: 1130 loss: 3.04790274e-06
Iter: 1131 loss: 3.06876154e-06
Iter: 1132 loss: 3.04791297e-06
Iter: 1133 loss: 3.04704508e-06
Iter: 1134 loss: 3.04488458e-06
Iter: 1135 loss: 3.06688298e-06
Iter: 1136 loss: 3.04467949e-06
Iter: 1137 loss: 3.04265814e-06
Iter: 1138 loss: 3.06257562e-06
Iter: 1139 loss: 3.04257219e-06
Iter: 1140 loss: 3.04068544e-06
Iter: 1141 loss: 3.0582446e-06
Iter: 1142 loss: 3.04062178e-06
Iter: 1143 loss: 3.03981824e-06
Iter: 1144 loss: 3.03871138e-06
Iter: 1145 loss: 3.03862635e-06
Iter: 1146 loss: 3.03707702e-06
Iter: 1147 loss: 3.05289723e-06
Iter: 1148 loss: 3.0370752e-06
Iter: 1149 loss: 3.03566594e-06
Iter: 1150 loss: 3.03560341e-06
Iter: 1151 loss: 3.03462184e-06
Iter: 1152 loss: 3.0331687e-06
Iter: 1153 loss: 3.03322349e-06
Iter: 1154 loss: 3.03202887e-06
Iter: 1155 loss: 3.02967419e-06
Iter: 1156 loss: 3.03759498e-06
Iter: 1157 loss: 3.0289948e-06
Iter: 1158 loss: 3.02776152e-06
Iter: 1159 loss: 3.02773651e-06
Iter: 1160 loss: 3.02679473e-06
Iter: 1161 loss: 3.02550552e-06
Iter: 1162 loss: 3.02538092e-06
Iter: 1163 loss: 3.02420403e-06
Iter: 1164 loss: 3.03711658e-06
Iter: 1165 loss: 3.02419426e-06
Iter: 1166 loss: 3.02284707e-06
Iter: 1167 loss: 3.02325452e-06
Iter: 1168 loss: 3.02179797e-06
Iter: 1169 loss: 3.0207234e-06
Iter: 1170 loss: 3.02024182e-06
Iter: 1171 loss: 3.01968794e-06
Iter: 1172 loss: 3.01857722e-06
Iter: 1173 loss: 3.01856653e-06
Iter: 1174 loss: 3.01732393e-06
Iter: 1175 loss: 3.01750288e-06
Iter: 1176 loss: 3.01639807e-06
Iter: 1177 loss: 3.01513819e-06
Iter: 1178 loss: 3.01488353e-06
Iter: 1179 loss: 3.01408068e-06
Iter: 1180 loss: 3.0123017e-06
Iter: 1181 loss: 3.0348865e-06
Iter: 1182 loss: 3.01236764e-06
Iter: 1183 loss: 3.01116734e-06
Iter: 1184 loss: 3.00879083e-06
Iter: 1185 loss: 3.04782043e-06
Iter: 1186 loss: 3.00874126e-06
Iter: 1187 loss: 3.00649572e-06
Iter: 1188 loss: 3.02187163e-06
Iter: 1189 loss: 3.00632655e-06
Iter: 1190 loss: 3.0051242e-06
Iter: 1191 loss: 3.00510692e-06
Iter: 1192 loss: 3.00399734e-06
Iter: 1193 loss: 3.00479905e-06
Iter: 1194 loss: 3.00338797e-06
Iter: 1195 loss: 3.00229021e-06
Iter: 1196 loss: 3.00439297e-06
Iter: 1197 loss: 3.00183228e-06
Iter: 1198 loss: 3.00066858e-06
Iter: 1199 loss: 3.01025875e-06
Iter: 1200 loss: 3.0005433e-06
Iter: 1201 loss: 2.99955082e-06
Iter: 1202 loss: 2.99846693e-06
Iter: 1203 loss: 2.99835574e-06
Iter: 1204 loss: 2.9968096e-06
Iter: 1205 loss: 2.99692647e-06
Iter: 1206 loss: 2.99568569e-06
Iter: 1207 loss: 2.99484259e-06
Iter: 1208 loss: 2.99453313e-06
Iter: 1209 loss: 2.99347562e-06
Iter: 1210 loss: 2.99117164e-06
Iter: 1211 loss: 3.02894387e-06
Iter: 1212 loss: 2.99118051e-06
Iter: 1213 loss: 2.98956297e-06
Iter: 1214 loss: 3.00651027e-06
Iter: 1215 loss: 2.98952205e-06
Iter: 1216 loss: 2.98817758e-06
Iter: 1217 loss: 2.99359317e-06
Iter: 1218 loss: 2.98779196e-06
Iter: 1219 loss: 2.9865746e-06
Iter: 1220 loss: 2.98566283e-06
Iter: 1221 loss: 2.98527902e-06
Iter: 1222 loss: 2.98394116e-06
Iter: 1223 loss: 2.9860139e-06
Iter: 1224 loss: 2.98334e-06
Iter: 1225 loss: 2.98233567e-06
Iter: 1226 loss: 2.98221858e-06
Iter: 1227 loss: 2.98138684e-06
Iter: 1228 loss: 2.98024361e-06
Iter: 1229 loss: 2.98016607e-06
Iter: 1230 loss: 2.97918541e-06
Iter: 1231 loss: 2.97920292e-06
Iter: 1232 loss: 2.97826796e-06
Iter: 1233 loss: 2.97705924e-06
Iter: 1234 loss: 2.97697943e-06
Iter: 1235 loss: 2.97540646e-06
Iter: 1236 loss: 2.97716178e-06
Iter: 1237 loss: 2.97462088e-06
Iter: 1238 loss: 2.97309316e-06
Iter: 1239 loss: 2.98312921e-06
Iter: 1240 loss: 2.9728817e-06
Iter: 1241 loss: 2.97137626e-06
Iter: 1242 loss: 2.97960742e-06
Iter: 1243 loss: 2.97107454e-06
Iter: 1244 loss: 2.96988514e-06
Iter: 1245 loss: 2.96955932e-06
Iter: 1246 loss: 2.9688058e-06
Iter: 1247 loss: 2.96763687e-06
Iter: 1248 loss: 2.97315319e-06
Iter: 1249 loss: 2.96749022e-06
Iter: 1250 loss: 2.96603548e-06
Iter: 1251 loss: 2.96934786e-06
Iter: 1252 loss: 2.96555754e-06
Iter: 1253 loss: 2.9644234e-06
Iter: 1254 loss: 2.96256303e-06
Iter: 1255 loss: 2.96258531e-06
Iter: 1256 loss: 2.96091412e-06
Iter: 1257 loss: 2.97880479e-06
Iter: 1258 loss: 2.96081544e-06
Iter: 1259 loss: 2.95935047e-06
Iter: 1260 loss: 2.97062684e-06
Iter: 1261 loss: 2.95931341e-06
Iter: 1262 loss: 2.95823975e-06
Iter: 1263 loss: 2.95752488e-06
Iter: 1264 loss: 2.95708196e-06
Iter: 1265 loss: 2.95616746e-06
Iter: 1266 loss: 2.95615e-06
Iter: 1267 loss: 2.95538121e-06
Iter: 1268 loss: 2.95354721e-06
Iter: 1269 loss: 2.96657709e-06
Iter: 1270 loss: 2.9531443e-06
Iter: 1271 loss: 2.9511209e-06
Iter: 1272 loss: 2.96546864e-06
Iter: 1273 loss: 2.95100426e-06
Iter: 1274 loss: 2.94999427e-06
Iter: 1275 loss: 2.94997244e-06
Iter: 1276 loss: 2.94929214e-06
Iter: 1277 loss: 2.94825531e-06
Iter: 1278 loss: 2.94815368e-06
Iter: 1279 loss: 2.94655365e-06
Iter: 1280 loss: 2.94695133e-06
Iter: 1281 loss: 2.94542951e-06
Iter: 1282 loss: 2.94415349e-06
Iter: 1283 loss: 2.94410484e-06
Iter: 1284 loss: 2.94315578e-06
Iter: 1285 loss: 2.94240704e-06
Iter: 1286 loss: 2.94213783e-06
Iter: 1287 loss: 2.94066035e-06
Iter: 1288 loss: 2.94043502e-06
Iter: 1289 loss: 2.93931953e-06
Iter: 1290 loss: 2.93835524e-06
Iter: 1291 loss: 2.93832136e-06
Iter: 1292 loss: 2.9372145e-06
Iter: 1293 loss: 2.93764629e-06
Iter: 1294 loss: 2.93642074e-06
Iter: 1295 loss: 2.93556036e-06
Iter: 1296 loss: 2.94076426e-06
Iter: 1297 loss: 2.93546418e-06
Iter: 1298 loss: 2.93446374e-06
Iter: 1299 loss: 2.93460653e-06
Iter: 1300 loss: 2.93377593e-06
Iter: 1301 loss: 2.93277753e-06
Iter: 1302 loss: 2.93176049e-06
Iter: 1303 loss: 2.93154676e-06
Iter: 1304 loss: 2.93008043e-06
Iter: 1305 loss: 2.94664233e-06
Iter: 1306 loss: 2.93000858e-06
Iter: 1307 loss: 2.92843652e-06
Iter: 1308 loss: 2.93154358e-06
Iter: 1309 loss: 2.92773211e-06
Iter: 1310 loss: 2.926668e-06
Iter: 1311 loss: 2.92682398e-06
Iter: 1312 loss: 2.92578557e-06
Iter: 1313 loss: 2.92437062e-06
Iter: 1314 loss: 2.93606695e-06
Iter: 1315 loss: 2.92433242e-06
Iter: 1316 loss: 2.92307323e-06
Iter: 1317 loss: 2.92467212e-06
Iter: 1318 loss: 2.92236905e-06
Iter: 1319 loss: 2.92117966e-06
Iter: 1320 loss: 2.92269897e-06
Iter: 1321 loss: 2.92045752e-06
Iter: 1322 loss: 2.91946299e-06
Iter: 1323 loss: 2.91944571e-06
Iter: 1324 loss: 2.91858578e-06
Iter: 1325 loss: 2.91699098e-06
Iter: 1326 loss: 2.94033566e-06
Iter: 1327 loss: 2.91698302e-06
Iter: 1328 loss: 2.91627521e-06
Iter: 1329 loss: 2.91568199e-06
Iter: 1330 loss: 2.91545734e-06
Iter: 1331 loss: 2.91396441e-06
Iter: 1332 loss: 2.92295226e-06
Iter: 1333 loss: 2.91382116e-06
Iter: 1334 loss: 2.91284618e-06
Iter: 1335 loss: 2.91075094e-06
Iter: 1336 loss: 2.94617303e-06
Iter: 1337 loss: 2.91073343e-06
Iter: 1338 loss: 2.90857088e-06
Iter: 1339 loss: 2.92482036e-06
Iter: 1340 loss: 2.90839398e-06
Iter: 1341 loss: 2.90701746e-06
Iter: 1342 loss: 2.92914137e-06
Iter: 1343 loss: 2.90699404e-06
Iter: 1344 loss: 2.90621233e-06
Iter: 1345 loss: 2.90469575e-06
Iter: 1346 loss: 2.93509538e-06
Iter: 1347 loss: 2.90472121e-06
Iter: 1348 loss: 2.90352455e-06
Iter: 1349 loss: 2.90354865e-06
Iter: 1350 loss: 2.90262187e-06
Iter: 1351 loss: 2.90554203e-06
Iter: 1352 loss: 2.90243906e-06
Iter: 1353 loss: 2.90161211e-06
Iter: 1354 loss: 2.90102798e-06
Iter: 1355 loss: 2.90071398e-06
Iter: 1356 loss: 2.89934792e-06
Iter: 1357 loss: 2.90039088e-06
Iter: 1358 loss: 2.89849686e-06
Iter: 1359 loss: 2.89767831e-06
Iter: 1360 loss: 2.89751233e-06
Iter: 1361 loss: 2.89666787e-06
Iter: 1362 loss: 2.89537707e-06
Iter: 1363 loss: 2.89528271e-06
Iter: 1364 loss: 2.8940924e-06
Iter: 1365 loss: 2.91339848e-06
Iter: 1366 loss: 2.89405489e-06
Iter: 1367 loss: 2.89296395e-06
Iter: 1368 loss: 2.89222908e-06
Iter: 1369 loss: 2.89179525e-06
Iter: 1370 loss: 2.89041554e-06
Iter: 1371 loss: 2.88934871e-06
Iter: 1372 loss: 2.88892033e-06
Iter: 1373 loss: 2.88878891e-06
Iter: 1374 loss: 2.88786669e-06
Iter: 1375 loss: 2.88727711e-06
Iter: 1376 loss: 2.88639785e-06
Iter: 1377 loss: 2.88634283e-06
Iter: 1378 loss: 2.88533329e-06
Iter: 1379 loss: 2.88608021e-06
Iter: 1380 loss: 2.88466072e-06
Iter: 1381 loss: 2.88349588e-06
Iter: 1382 loss: 2.89927198e-06
Iter: 1383 loss: 2.88348247e-06
Iter: 1384 loss: 2.88254796e-06
Iter: 1385 loss: 2.88130877e-06
Iter: 1386 loss: 2.88129422e-06
Iter: 1387 loss: 2.87961984e-06
Iter: 1388 loss: 2.88411456e-06
Iter: 1389 loss: 2.87901821e-06
Iter: 1390 loss: 2.87747912e-06
Iter: 1391 loss: 2.88222282e-06
Iter: 1392 loss: 2.87692956e-06
Iter: 1393 loss: 2.8756815e-06
Iter: 1394 loss: 2.87563535e-06
Iter: 1395 loss: 2.8749073e-06
Iter: 1396 loss: 2.87367766e-06
Iter: 1397 loss: 2.87369085e-06
Iter: 1398 loss: 2.87220382e-06
Iter: 1399 loss: 2.89296804e-06
Iter: 1400 loss: 2.87223793e-06
Iter: 1401 loss: 2.87141052e-06
Iter: 1402 loss: 2.87017542e-06
Iter: 1403 loss: 2.87014609e-06
Iter: 1404 loss: 2.86891645e-06
Iter: 1405 loss: 2.87561852e-06
Iter: 1406 loss: 2.86872228e-06
Iter: 1407 loss: 2.86770364e-06
Iter: 1408 loss: 2.88103911e-06
Iter: 1409 loss: 2.86773275e-06
Iter: 1410 loss: 2.867067e-06
Iter: 1411 loss: 2.8656782e-06
Iter: 1412 loss: 2.88342244e-06
Iter: 1413 loss: 2.86556747e-06
Iter: 1414 loss: 2.86423756e-06
Iter: 1415 loss: 2.8759107e-06
Iter: 1416 loss: 2.86405066e-06
Iter: 1417 loss: 2.86230897e-06
Iter: 1418 loss: 2.86270188e-06
Iter: 1419 loss: 2.86099498e-06
Iter: 1420 loss: 2.8597e-06
Iter: 1421 loss: 2.86385784e-06
Iter: 1422 loss: 2.85928809e-06
Iter: 1423 loss: 2.85783176e-06
Iter: 1424 loss: 2.85925898e-06
Iter: 1425 loss: 2.85702868e-06
Iter: 1426 loss: 2.85608508e-06
Iter: 1427 loss: 2.85603073e-06
Iter: 1428 loss: 2.85526198e-06
Iter: 1429 loss: 2.85477859e-06
Iter: 1430 loss: 2.85440092e-06
Iter: 1431 loss: 2.85353735e-06
Iter: 1432 loss: 2.8621273e-06
Iter: 1433 loss: 2.85353462e-06
Iter: 1434 loss: 2.8525883e-06
Iter: 1435 loss: 2.85133547e-06
Iter: 1436 loss: 2.85134684e-06
Iter: 1437 loss: 2.84991665e-06
Iter: 1438 loss: 2.85515716e-06
Iter: 1439 loss: 2.84950102e-06
Iter: 1440 loss: 2.84874113e-06
Iter: 1441 loss: 2.8486902e-06
Iter: 1442 loss: 2.84801649e-06
Iter: 1443 loss: 2.84597718e-06
Iter: 1444 loss: 2.86402633e-06
Iter: 1445 loss: 2.84578027e-06
Iter: 1446 loss: 2.84381713e-06
Iter: 1447 loss: 2.8576917e-06
Iter: 1448 loss: 2.84368571e-06
Iter: 1449 loss: 2.84202542e-06
Iter: 1450 loss: 2.85793908e-06
Iter: 1451 loss: 2.84205953e-06
Iter: 1452 loss: 2.84109819e-06
Iter: 1453 loss: 2.8403565e-06
Iter: 1454 loss: 2.84002385e-06
Iter: 1455 loss: 2.83869372e-06
Iter: 1456 loss: 2.84292219e-06
Iter: 1457 loss: 2.83831582e-06
Iter: 1458 loss: 2.83746795e-06
Iter: 1459 loss: 2.85155807e-06
Iter: 1460 loss: 2.83746158e-06
Iter: 1461 loss: 2.8366062e-06
Iter: 1462 loss: 2.8368413e-06
Iter: 1463 loss: 2.83601435e-06
Iter: 1464 loss: 2.83487975e-06
Iter: 1465 loss: 2.83658187e-06
Iter: 1466 loss: 2.83421014e-06
Iter: 1467 loss: 2.83304189e-06
Iter: 1468 loss: 2.84466455e-06
Iter: 1469 loss: 2.83294457e-06
Iter: 1470 loss: 2.83225199e-06
Iter: 1471 loss: 2.83048689e-06
Iter: 1472 loss: 2.84916041e-06
Iter: 1473 loss: 2.83037116e-06
Iter: 1474 loss: 2.82905376e-06
Iter: 1475 loss: 2.82892142e-06
Iter: 1476 loss: 2.82761766e-06
Iter: 1477 loss: 2.82816973e-06
Iter: 1478 loss: 2.82655219e-06
Iter: 1479 loss: 2.8256884e-06
Iter: 1480 loss: 2.82557585e-06
Iter: 1481 loss: 2.82491192e-06
Iter: 1482 loss: 2.8237946e-06
Iter: 1483 loss: 2.84102794e-06
Iter: 1484 loss: 2.82380597e-06
Iter: 1485 loss: 2.8228535e-06
Iter: 1486 loss: 2.8231666e-06
Iter: 1487 loss: 2.82217707e-06
Iter: 1488 loss: 2.82133169e-06
Iter: 1489 loss: 2.82055839e-06
Iter: 1490 loss: 2.82032852e-06
Iter: 1491 loss: 2.81892267e-06
Iter: 1492 loss: 2.83517966e-06
Iter: 1493 loss: 2.81889515e-06
Iter: 1494 loss: 2.81758139e-06
Iter: 1495 loss: 2.82018959e-06
Iter: 1496 loss: 2.81706593e-06
Iter: 1497 loss: 2.81571511e-06
Iter: 1498 loss: 2.8192178e-06
Iter: 1499 loss: 2.81528014e-06
Iter: 1500 loss: 2.81407574e-06
Iter: 1501 loss: 2.82122437e-06
Iter: 1502 loss: 2.81392909e-06
Iter: 1503 loss: 2.81282928e-06
Iter: 1504 loss: 2.81079e-06
Iter: 1505 loss: 2.8108102e-06
Iter: 1506 loss: 2.80942368e-06
Iter: 1507 loss: 2.82800556e-06
Iter: 1508 loss: 2.80938775e-06
Iter: 1509 loss: 2.80831705e-06
Iter: 1510 loss: 2.81624193e-06
Iter: 1511 loss: 2.80819427e-06
Iter: 1512 loss: 2.80753102e-06
Iter: 1513 loss: 2.8061645e-06
Iter: 1514 loss: 2.83340432e-06
Iter: 1515 loss: 2.80608879e-06
Iter: 1516 loss: 2.80501445e-06
Iter: 1517 loss: 2.81899975e-06
Iter: 1518 loss: 2.80495078e-06
Iter: 1519 loss: 2.80381914e-06
Iter: 1520 loss: 2.80665631e-06
Iter: 1521 loss: 2.80343602e-06
Iter: 1522 loss: 2.80255472e-06
Iter: 1523 loss: 2.80092536e-06
Iter: 1524 loss: 2.80093082e-06
Iter: 1525 loss: 2.79925371e-06
Iter: 1526 loss: 2.81699477e-06
Iter: 1527 loss: 2.79925575e-06
Iter: 1528 loss: 2.79796132e-06
Iter: 1529 loss: 2.80692439e-06
Iter: 1530 loss: 2.79786332e-06
Iter: 1531 loss: 2.79669234e-06
Iter: 1532 loss: 2.79630467e-06
Iter: 1533 loss: 2.7956653e-06
Iter: 1534 loss: 2.79434744e-06
Iter: 1535 loss: 2.81030589e-06
Iter: 1536 loss: 2.79433516e-06
Iter: 1537 loss: 2.79343953e-06
Iter: 1538 loss: 2.79379469e-06
Iter: 1539 loss: 2.79293977e-06
Iter: 1540 loss: 2.79182814e-06
Iter: 1541 loss: 2.79152823e-06
Iter: 1542 loss: 2.79075948e-06
Iter: 1543 loss: 2.79010965e-06
Iter: 1544 loss: 2.78995822e-06
Iter: 1545 loss: 2.78929087e-06
Iter: 1546 loss: 2.78792413e-06
Iter: 1547 loss: 2.81700068e-06
Iter: 1548 loss: 2.78798143e-06
Iter: 1549 loss: 2.78667108e-06
Iter: 1550 loss: 2.78756806e-06
Iter: 1551 loss: 2.78589096e-06
Iter: 1552 loss: 2.78442303e-06
Iter: 1553 loss: 2.78444736e-06
Iter: 1554 loss: 2.78356447e-06
Iter: 1555 loss: 2.78283892e-06
Iter: 1556 loss: 2.78250809e-06
Iter: 1557 loss: 2.78126367e-06
Iter: 1558 loss: 2.78093921e-06
Iter: 1559 loss: 2.78014e-06
Iter: 1560 loss: 2.77967411e-06
Iter: 1561 loss: 2.77921754e-06
Iter: 1562 loss: 2.77850086e-06
Iter: 1563 loss: 2.77772824e-06
Iter: 1564 loss: 2.77757181e-06
Iter: 1565 loss: 2.77665413e-06
Iter: 1566 loss: 2.78695e-06
Iter: 1567 loss: 2.7765991e-06
Iter: 1568 loss: 2.77590925e-06
Iter: 1569 loss: 2.77612185e-06
Iter: 1570 loss: 2.77537492e-06
Iter: 1571 loss: 2.77428262e-06
Iter: 1572 loss: 2.77405888e-06
Iter: 1573 loss: 2.77331674e-06
Iter: 1574 loss: 2.77246431e-06
Iter: 1575 loss: 2.78780408e-06
Iter: 1576 loss: 2.77243657e-06
Iter: 1577 loss: 2.77126105e-06
Iter: 1578 loss: 2.77040544e-06
Iter: 1579 loss: 2.7700903e-06
Iter: 1580 loss: 2.7686192e-06
Iter: 1581 loss: 2.76900801e-06
Iter: 1582 loss: 2.76755145e-06
Iter: 1583 loss: 2.76607852e-06
Iter: 1584 loss: 2.78375046e-06
Iter: 1585 loss: 2.76601372e-06
Iter: 1586 loss: 2.76444894e-06
Iter: 1587 loss: 2.76689934e-06
Iter: 1588 loss: 2.76380479e-06
Iter: 1589 loss: 2.76273045e-06
Iter: 1590 loss: 2.76193509e-06
Iter: 1591 loss: 2.76157334e-06
Iter: 1592 loss: 2.76052515e-06
Iter: 1593 loss: 2.76054652e-06
Iter: 1594 loss: 2.75931779e-06
Iter: 1595 loss: 2.76074024e-06
Iter: 1596 loss: 2.75877073e-06
Iter: 1597 loss: 2.7578144e-06
Iter: 1598 loss: 2.76143237e-06
Iter: 1599 loss: 2.75763864e-06
Iter: 1600 loss: 2.75668026e-06
Iter: 1601 loss: 2.76012861e-06
Iter: 1602 loss: 2.75645743e-06
Iter: 1603 loss: 2.75553543e-06
Iter: 1604 loss: 2.75444245e-06
Iter: 1605 loss: 2.75430102e-06
Iter: 1606 loss: 2.75299521e-06
Iter: 1607 loss: 2.76474088e-06
Iter: 1608 loss: 2.75286925e-06
Iter: 1609 loss: 2.75167213e-06
Iter: 1610 loss: 2.75807542e-06
Iter: 1611 loss: 2.75148341e-06
Iter: 1612 loss: 2.75043658e-06
Iter: 1613 loss: 2.74894478e-06
Iter: 1614 loss: 2.74896774e-06
Iter: 1615 loss: 2.74740205e-06
Iter: 1616 loss: 2.75378852e-06
Iter: 1617 loss: 2.74706417e-06
Iter: 1618 loss: 2.74615354e-06
Iter: 1619 loss: 2.74611602e-06
Iter: 1620 loss: 2.74555623e-06
Iter: 1621 loss: 2.74401782e-06
Iter: 1622 loss: 2.76106698e-06
Iter: 1623 loss: 2.74393278e-06
Iter: 1624 loss: 2.74223908e-06
Iter: 1625 loss: 2.75006937e-06
Iter: 1626 loss: 2.74190779e-06
Iter: 1627 loss: 2.74106105e-06
Iter: 1628 loss: 2.74096124e-06
Iter: 1629 loss: 2.74020294e-06
Iter: 1630 loss: 2.73849037e-06
Iter: 1631 loss: 2.76831611e-06
Iter: 1632 loss: 2.73845694e-06
Iter: 1633 loss: 2.73743626e-06
Iter: 1634 loss: 2.73725027e-06
Iter: 1635 loss: 2.73649266e-06
Iter: 1636 loss: 2.73549631e-06
Iter: 1637 loss: 2.73539445e-06
Iter: 1638 loss: 2.73407295e-06
Iter: 1639 loss: 2.73719434e-06
Iter: 1640 loss: 2.73359137e-06
Iter: 1641 loss: 2.73238379e-06
Iter: 1642 loss: 2.74795457e-06
Iter: 1643 loss: 2.73237265e-06
Iter: 1644 loss: 2.73127739e-06
Iter: 1645 loss: 2.73127398e-06
Iter: 1646 loss: 2.73052638e-06
Iter: 1647 loss: 2.72946909e-06
Iter: 1648 loss: 2.72898728e-06
Iter: 1649 loss: 2.72846137e-06
Iter: 1650 loss: 2.72714806e-06
Iter: 1651 loss: 2.7446074e-06
Iter: 1652 loss: 2.72714669e-06
Iter: 1653 loss: 2.72588045e-06
Iter: 1654 loss: 2.72780608e-06
Iter: 1655 loss: 2.72526404e-06
Iter: 1656 loss: 2.72421812e-06
Iter: 1657 loss: 2.72235093e-06
Iter: 1658 loss: 2.72238412e-06
Iter: 1659 loss: 2.72121133e-06
Iter: 1660 loss: 2.72107923e-06
Iter: 1661 loss: 2.71972567e-06
Iter: 1662 loss: 2.72124339e-06
Iter: 1663 loss: 2.71901672e-06
Iter: 1664 loss: 2.71810131e-06
Iter: 1665 loss: 2.72211582e-06
Iter: 1666 loss: 2.71794283e-06
Iter: 1667 loss: 2.71687395e-06
Iter: 1668 loss: 2.71783324e-06
Iter: 1669 loss: 2.71621252e-06
Iter: 1670 loss: 2.71512e-06
Iter: 1671 loss: 2.71540489e-06
Iter: 1672 loss: 2.7143326e-06
Iter: 1673 loss: 2.7133633e-06
Iter: 1674 loss: 2.71339763e-06
Iter: 1675 loss: 2.71245699e-06
Iter: 1676 loss: 2.71209e-06
Iter: 1677 loss: 2.71159115e-06
Iter: 1678 loss: 2.71051795e-06
Iter: 1679 loss: 2.71244676e-06
Iter: 1680 loss: 2.70992905e-06
Iter: 1681 loss: 2.70877354e-06
Iter: 1682 loss: 2.71006593e-06
Iter: 1683 loss: 2.70816645e-06
Iter: 1684 loss: 2.70686814e-06
Iter: 1685 loss: 2.72464104e-06
Iter: 1686 loss: 2.70684632e-06
Iter: 1687 loss: 2.70596638e-06
Iter: 1688 loss: 2.7044041e-06
Iter: 1689 loss: 2.70438545e-06
Iter: 1690 loss: 2.70322266e-06
Iter: 1691 loss: 2.7068304e-06
Iter: 1692 loss: 2.70281816e-06
Iter: 1693 loss: 2.70159717e-06
Iter: 1694 loss: 2.72035413e-06
Iter: 1695 loss: 2.70153896e-06
Iter: 1696 loss: 2.70067017e-06
Iter: 1697 loss: 2.70017881e-06
Iter: 1698 loss: 2.6997418e-06
Iter: 1699 loss: 2.6989344e-06
Iter: 1700 loss: 2.71078989e-06
Iter: 1701 loss: 2.69889233e-06
Iter: 1702 loss: 2.69815564e-06
Iter: 1703 loss: 2.69672501e-06
Iter: 1704 loss: 2.72770649e-06
Iter: 1705 loss: 2.69667316e-06
Iter: 1706 loss: 2.6953735e-06
Iter: 1707 loss: 2.70213582e-06
Iter: 1708 loss: 2.69518387e-06
Iter: 1709 loss: 2.69375118e-06
Iter: 1710 loss: 2.70332202e-06
Iter: 1711 loss: 2.69363045e-06
Iter: 1712 loss: 2.6928833e-06
Iter: 1713 loss: 2.6919447e-06
Iter: 1714 loss: 2.69181851e-06
Iter: 1715 loss: 2.69035581e-06
Iter: 1716 loss: 2.69424845e-06
Iter: 1717 loss: 2.68971257e-06
Iter: 1718 loss: 2.68896883e-06
Iter: 1719 loss: 2.68892973e-06
Iter: 1720 loss: 2.68830399e-06
Iter: 1721 loss: 2.68816348e-06
Iter: 1722 loss: 2.68764097e-06
Iter: 1723 loss: 2.68690064e-06
Iter: 1724 loss: 2.68687427e-06
Iter: 1725 loss: 2.68628673e-06
Iter: 1726 loss: 2.68555459e-06
Iter: 1727 loss: 2.6951061e-06
Iter: 1728 loss: 2.68545023e-06
Iter: 1729 loss: 2.68450685e-06
Iter: 1730 loss: 2.68557e-06
Iter: 1731 loss: 2.68401982e-06
Iter: 1732 loss: 2.68318604e-06
Iter: 1733 loss: 2.68359599e-06
Iter: 1734 loss: 2.68260374e-06
Iter: 1735 loss: 2.68119e-06
Iter: 1736 loss: 2.68833173e-06
Iter: 1737 loss: 2.68099438e-06
Iter: 1738 loss: 2.68014242e-06
Iter: 1739 loss: 2.67850055e-06
Iter: 1740 loss: 2.70847613e-06
Iter: 1741 loss: 2.67839823e-06
Iter: 1742 loss: 2.6776479e-06
Iter: 1743 loss: 2.67724363e-06
Iter: 1744 loss: 2.67656355e-06
Iter: 1745 loss: 2.67555015e-06
Iter: 1746 loss: 2.67548489e-06
Iter: 1747 loss: 2.67432915e-06
Iter: 1748 loss: 2.67628934e-06
Iter: 1749 loss: 2.67370888e-06
Iter: 1750 loss: 2.67270434e-06
Iter: 1751 loss: 2.68249141e-06
Iter: 1752 loss: 2.67262567e-06
Iter: 1753 loss: 2.67187988e-06
Iter: 1754 loss: 2.67609175e-06
Iter: 1755 loss: 2.67175665e-06
Iter: 1756 loss: 2.67097289e-06
Iter: 1757 loss: 2.67018049e-06
Iter: 1758 loss: 2.67008386e-06
Iter: 1759 loss: 2.66874258e-06
Iter: 1760 loss: 2.67110045e-06
Iter: 1761 loss: 2.66822644e-06
Iter: 1762 loss: 2.66709867e-06
Iter: 1763 loss: 2.66709958e-06
Iter: 1764 loss: 2.66642951e-06
Iter: 1765 loss: 2.665382e-06
Iter: 1766 loss: 2.66537245e-06
Iter: 1767 loss: 2.66412781e-06
Iter: 1768 loss: 2.67905352e-06
Iter: 1769 loss: 2.66415282e-06
Iter: 1770 loss: 2.66333655e-06
Iter: 1771 loss: 2.66230131e-06
Iter: 1772 loss: 2.66227175e-06
Iter: 1773 loss: 2.66143388e-06
Iter: 1774 loss: 2.6614141e-06
Iter: 1775 loss: 2.66056304e-06
Iter: 1776 loss: 2.66193251e-06
Iter: 1777 loss: 2.66022857e-06
Iter: 1778 loss: 2.65956214e-06
Iter: 1779 loss: 2.65874337e-06
Iter: 1780 loss: 2.65868084e-06
Iter: 1781 loss: 2.65767358e-06
Iter: 1782 loss: 2.66470761e-06
Iter: 1783 loss: 2.6576547e-06
Iter: 1784 loss: 2.65651897e-06
Iter: 1785 loss: 2.66040911e-06
Iter: 1786 loss: 2.65635367e-06
Iter: 1787 loss: 2.65510198e-06
Iter: 1788 loss: 2.65547396e-06
Iter: 1789 loss: 2.65417657e-06
Iter: 1790 loss: 2.65287986e-06
Iter: 1791 loss: 2.65452718e-06
Iter: 1792 loss: 2.65223184e-06
Iter: 1793 loss: 2.65118933e-06
Iter: 1794 loss: 2.6511716e-06
Iter: 1795 loss: 2.65023573e-06
Iter: 1796 loss: 2.64929599e-06
Iter: 1797 loss: 2.64911478e-06
Iter: 1798 loss: 2.64802111e-06
Iter: 1799 loss: 2.65674498e-06
Iter: 1800 loss: 2.64799974e-06
Iter: 1801 loss: 2.64694904e-06
Iter: 1802 loss: 2.64796245e-06
Iter: 1803 loss: 2.64629875e-06
Iter: 1804 loss: 2.64548476e-06
Iter: 1805 loss: 2.64599316e-06
Iter: 1806 loss: 2.64500545e-06
Iter: 1807 loss: 2.64378673e-06
Iter: 1808 loss: 2.65577864e-06
Iter: 1809 loss: 2.64374785e-06
Iter: 1810 loss: 2.64313599e-06
Iter: 1811 loss: 2.64213713e-06
Iter: 1812 loss: 2.64214304e-06
Iter: 1813 loss: 2.64066966e-06
Iter: 1814 loss: 2.6421344e-06
Iter: 1815 loss: 2.63986e-06
Iter: 1816 loss: 2.63858465e-06
Iter: 1817 loss: 2.63852e-06
Iter: 1818 loss: 2.63741595e-06
Iter: 1819 loss: 2.63894322e-06
Iter: 1820 loss: 2.63681159e-06
Iter: 1821 loss: 2.63584025e-06
Iter: 1822 loss: 2.63699258e-06
Iter: 1823 loss: 2.63529137e-06
Iter: 1824 loss: 2.63419133e-06
Iter: 1825 loss: 2.64007167e-06
Iter: 1826 loss: 2.63406946e-06
Iter: 1827 loss: 2.63305219e-06
Iter: 1828 loss: 2.63925676e-06
Iter: 1829 loss: 2.6329883e-06
Iter: 1830 loss: 2.63227389e-06
Iter: 1831 loss: 2.63121979e-06
Iter: 1832 loss: 2.63124866e-06
Iter: 1833 loss: 2.62982462e-06
Iter: 1834 loss: 2.64636969e-06
Iter: 1835 loss: 2.62983463e-06
Iter: 1836 loss: 2.62912044e-06
Iter: 1837 loss: 2.62833464e-06
Iter: 1838 loss: 2.62817139e-06
Iter: 1839 loss: 2.62715866e-06
Iter: 1840 loss: 2.64001164e-06
Iter: 1841 loss: 2.62714684e-06
Iter: 1842 loss: 2.62605636e-06
Iter: 1843 loss: 2.62494837e-06
Iter: 1844 loss: 2.6247576e-06
Iter: 1845 loss: 2.62343838e-06
Iter: 1846 loss: 2.62456865e-06
Iter: 1847 loss: 2.62263552e-06
Iter: 1848 loss: 2.62148205e-06
Iter: 1849 loss: 2.63227594e-06
Iter: 1850 loss: 2.62145863e-06
Iter: 1851 loss: 2.62030426e-06
Iter: 1852 loss: 2.62563526e-06
Iter: 1853 loss: 2.62004869e-06
Iter: 1854 loss: 2.6191417e-06
Iter: 1855 loss: 2.61987816e-06
Iter: 1856 loss: 2.61861578e-06
Iter: 1857 loss: 2.617684e-06
Iter: 1858 loss: 2.61858395e-06
Iter: 1859 loss: 2.6171474e-06
Iter: 1860 loss: 2.61601167e-06
Iter: 1861 loss: 2.63277661e-06
Iter: 1862 loss: 2.61596938e-06
Iter: 1863 loss: 2.61526657e-06
Iter: 1864 loss: 2.6146231e-06
Iter: 1865 loss: 2.61441664e-06
Iter: 1866 loss: 2.61338096e-06
Iter: 1867 loss: 2.6191924e-06
Iter: 1868 loss: 2.6133207e-06
Iter: 1869 loss: 2.61215246e-06
Iter: 1870 loss: 2.61194418e-06
Iter: 1871 loss: 2.61121568e-06
Iter: 1872 loss: 2.61008586e-06
Iter: 1873 loss: 2.61519244e-06
Iter: 1874 loss: 2.60983234e-06
Iter: 1875 loss: 2.60851357e-06
Iter: 1876 loss: 2.61284754e-06
Iter: 1877 loss: 2.60817637e-06
Iter: 1878 loss: 2.60727575e-06
Iter: 1879 loss: 2.60594652e-06
Iter: 1880 loss: 2.60594152e-06
Iter: 1881 loss: 2.60441584e-06
Iter: 1882 loss: 2.6116154e-06
Iter: 1883 loss: 2.60411616e-06
Iter: 1884 loss: 2.60340948e-06
Iter: 1885 loss: 2.60338e-06
Iter: 1886 loss: 2.60249863e-06
Iter: 1887 loss: 2.60143065e-06
Iter: 1888 loss: 2.60139927e-06
Iter: 1889 loss: 2.60021852e-06
Iter: 1890 loss: 2.6053815e-06
Iter: 1891 loss: 2.59996568e-06
Iter: 1892 loss: 2.59900435e-06
Iter: 1893 loss: 2.60618731e-06
Iter: 1894 loss: 2.59887747e-06
Iter: 1895 loss: 2.59790409e-06
Iter: 1896 loss: 2.5988561e-06
Iter: 1897 loss: 2.59729291e-06
Iter: 1898 loss: 2.59616399e-06
Iter: 1899 loss: 2.59586068e-06
Iter: 1900 loss: 2.59516128e-06
Iter: 1901 loss: 2.59377748e-06
Iter: 1902 loss: 2.61441619e-06
Iter: 1903 loss: 2.59376702e-06
Iter: 1904 loss: 2.59301851e-06
Iter: 1905 loss: 2.59214767e-06
Iter: 1906 loss: 2.59210242e-06
Iter: 1907 loss: 2.59091985e-06
Iter: 1908 loss: 2.60842762e-06
Iter: 1909 loss: 2.59094031e-06
Iter: 1910 loss: 2.59011813e-06
Iter: 1911 loss: 2.58970431e-06
Iter: 1912 loss: 2.58931186e-06
Iter: 1913 loss: 2.58847513e-06
Iter: 1914 loss: 2.58777936e-06
Iter: 1915 loss: 2.5875413e-06
Iter: 1916 loss: 2.58609271e-06
Iter: 1917 loss: 2.59195713e-06
Iter: 1918 loss: 2.58579848e-06
Iter: 1919 loss: 2.58473915e-06
Iter: 1920 loss: 2.58466207e-06
Iter: 1921 loss: 2.58397858e-06
Iter: 1922 loss: 2.5827278e-06
Iter: 1923 loss: 2.58271098e-06
Iter: 1924 loss: 2.58115824e-06
Iter: 1925 loss: 2.58409318e-06
Iter: 1926 loss: 2.58053342e-06
Iter: 1927 loss: 2.58036425e-06
Iter: 1928 loss: 2.57985448e-06
Iter: 1929 loss: 2.57936563e-06
Iter: 1930 loss: 2.57828333e-06
Iter: 1931 loss: 2.59965418e-06
Iter: 1932 loss: 2.57834336e-06
Iter: 1933 loss: 2.57732836e-06
Iter: 1934 loss: 2.58526143e-06
Iter: 1935 loss: 2.5773229e-06
Iter: 1936 loss: 2.57656848e-06
Iter: 1937 loss: 2.58048158e-06
Iter: 1938 loss: 2.57645206e-06
Iter: 1939 loss: 2.57584657e-06
Iter: 1940 loss: 2.57547094e-06
Iter: 1941 loss: 2.57529314e-06
Iter: 1942 loss: 2.57455531e-06
Iter: 1943 loss: 2.58592058e-06
Iter: 1944 loss: 2.57456213e-06
Iter: 1945 loss: 2.57403917e-06
Iter: 1946 loss: 2.57296824e-06
Iter: 1947 loss: 2.57296438e-06
Iter: 1948 loss: 2.57188663e-06
Iter: 1949 loss: 2.57323472e-06
Iter: 1950 loss: 2.57132115e-06
Iter: 1951 loss: 2.56991052e-06
Iter: 1952 loss: 2.56968588e-06
Iter: 1953 loss: 2.56866224e-06
Iter: 1954 loss: 2.56906083e-06
Iter: 1955 loss: 2.56803219e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.2
+ date
Sun Nov  8 15:51:09 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfc7d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfcad9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfcadc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfc0fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfc0ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfb522f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfc0fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfa6f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfa6f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfb52510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfabb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfac2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfac2c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfacb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfacbae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfacbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfa41598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfacb158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8fe37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8fd0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8f96840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8ec08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8f3c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8f7a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8f7a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8e5a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8e01950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8df3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8df3048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8df6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8efd268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8daf1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8daf378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8d85400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8d72840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8e8c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 202, in <module>
    grads = tape.gradient(loss, model.trainable_weights)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1266, in _backward_function_wrapper
    processed_args, remapped_captures)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input is not invertible.
	 [[node gradients/MatrixDeterminant_grad/MatrixInverse (defined at biholoNN_train.py:200) ]] [Op:__inference___backward_volume_form_4446_8945]

Function call stack:
__backward_volume_form_4446

+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.2/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f716268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f755d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f7550d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f6c2c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f6c2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f6462f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f6018c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f6012f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f5af268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f572d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f5af2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f539158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f5467b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f5468c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f495b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f4d3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f4c7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f4c7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f4336a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f437f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f437ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c536f8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c536f8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c536d79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c536d78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c53689378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c536adbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c53664950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c5a6048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c5ccea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c563598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c58a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c58a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c539b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c4f1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c4a99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.03043601e-05
Iter: 2 loss: 2.13928288e-05
Iter: 3 loss: 1.76234e-05
Iter: 4 loss: 1.66110494e-05
Iter: 5 loss: 1.98952785e-05
Iter: 6 loss: 1.63260047e-05
Iter: 7 loss: 1.56334208e-05
Iter: 8 loss: 1.71883021e-05
Iter: 9 loss: 1.53712026e-05
Iter: 10 loss: 1.45759068e-05
Iter: 11 loss: 1.67807957e-05
Iter: 12 loss: 1.4320226e-05
Iter: 13 loss: 1.36861881e-05
Iter: 14 loss: 1.51034492e-05
Iter: 15 loss: 1.34441516e-05
Iter: 16 loss: 1.2621188e-05
Iter: 17 loss: 1.51645199e-05
Iter: 18 loss: 1.23790851e-05
Iter: 19 loss: 1.194783e-05
Iter: 20 loss: 1.25094757e-05
Iter: 21 loss: 1.17289765e-05
Iter: 22 loss: 1.1256242e-05
Iter: 23 loss: 1.13565111e-05
Iter: 24 loss: 1.0906233e-05
Iter: 25 loss: 1.03548027e-05
Iter: 26 loss: 1.86317484e-05
Iter: 27 loss: 1.03544726e-05
Iter: 28 loss: 1.00655816e-05
Iter: 29 loss: 9.74680188e-06
Iter: 30 loss: 9.70091241e-06
Iter: 31 loss: 9.34108175e-06
Iter: 32 loss: 1.08189306e-05
Iter: 33 loss: 9.26257053e-06
Iter: 34 loss: 8.96314305e-06
Iter: 35 loss: 1.0014347e-05
Iter: 36 loss: 8.88555587e-06
Iter: 37 loss: 8.62310844e-06
Iter: 38 loss: 8.80394418e-06
Iter: 39 loss: 8.45900922e-06
Iter: 40 loss: 8.26135783e-06
Iter: 41 loss: 8.24843846e-06
Iter: 42 loss: 8.02765771e-06
Iter: 43 loss: 8.03409421e-06
Iter: 44 loss: 7.85297289e-06
Iter: 45 loss: 7.63214666e-06
Iter: 46 loss: 9.42098541e-06
Iter: 47 loss: 7.61778665e-06
Iter: 48 loss: 7.45341276e-06
Iter: 49 loss: 8.00842099e-06
Iter: 50 loss: 7.40920268e-06
Iter: 51 loss: 7.30675038e-06
Iter: 52 loss: 7.79777201e-06
Iter: 53 loss: 7.28873965e-06
Iter: 54 loss: 7.17521198e-06
Iter: 55 loss: 7.19378295e-06
Iter: 56 loss: 7.08961852e-06
Iter: 57 loss: 6.98062786e-06
Iter: 58 loss: 6.94966548e-06
Iter: 59 loss: 6.88325235e-06
Iter: 60 loss: 6.74460534e-06
Iter: 61 loss: 8.24203562e-06
Iter: 62 loss: 6.7412675e-06
Iter: 63 loss: 6.64966456e-06
Iter: 64 loss: 7.08681273e-06
Iter: 65 loss: 6.63303263e-06
Iter: 66 loss: 6.55926442e-06
Iter: 67 loss: 6.53650659e-06
Iter: 68 loss: 6.49291e-06
Iter: 69 loss: 6.39660539e-06
Iter: 70 loss: 6.60120304e-06
Iter: 71 loss: 6.35868309e-06
Iter: 72 loss: 6.24562381e-06
Iter: 73 loss: 6.31917783e-06
Iter: 74 loss: 6.1738665e-06
Iter: 75 loss: 6.04172237e-06
Iter: 76 loss: 6.1550445e-06
Iter: 77 loss: 5.96364498e-06
Iter: 78 loss: 5.87838895e-06
Iter: 79 loss: 5.87365093e-06
Iter: 80 loss: 5.80388041e-06
Iter: 81 loss: 6.4672895e-06
Iter: 82 loss: 5.80116466e-06
Iter: 83 loss: 5.74486694e-06
Iter: 84 loss: 5.68498581e-06
Iter: 85 loss: 5.675171e-06
Iter: 86 loss: 5.64757647e-06
Iter: 87 loss: 5.63767389e-06
Iter: 88 loss: 5.60960234e-06
Iter: 89 loss: 5.54802909e-06
Iter: 90 loss: 6.45463388e-06
Iter: 91 loss: 5.54525877e-06
Iter: 92 loss: 5.50889854e-06
Iter: 93 loss: 5.50454752e-06
Iter: 94 loss: 5.4830316e-06
Iter: 95 loss: 5.43392616e-06
Iter: 96 loss: 6.07193033e-06
Iter: 97 loss: 5.43083479e-06
Iter: 98 loss: 5.38452332e-06
Iter: 99 loss: 5.74633896e-06
Iter: 100 loss: 5.38132599e-06
Iter: 101 loss: 5.34178e-06
Iter: 102 loss: 5.57227395e-06
Iter: 103 loss: 5.33654566e-06
Iter: 104 loss: 5.29839326e-06
Iter: 105 loss: 5.30918624e-06
Iter: 106 loss: 5.27081602e-06
Iter: 107 loss: 5.22167375e-06
Iter: 108 loss: 5.20913909e-06
Iter: 109 loss: 5.17822309e-06
Iter: 110 loss: 5.13113082e-06
Iter: 111 loss: 5.13106352e-06
Iter: 112 loss: 5.10261907e-06
Iter: 113 loss: 5.04431682e-06
Iter: 114 loss: 6.09887229e-06
Iter: 115 loss: 5.04335458e-06
Iter: 116 loss: 5.06397e-06
Iter: 117 loss: 5.01600516e-06
Iter: 118 loss: 4.99762064e-06
Iter: 119 loss: 4.98796635e-06
Iter: 120 loss: 4.9795608e-06
Iter: 121 loss: 4.95567292e-06
Iter: 122 loss: 4.96334542e-06
Iter: 123 loss: 4.93872221e-06
Iter: 124 loss: 4.9133123e-06
Iter: 125 loss: 4.91311675e-06
Iter: 126 loss: 4.90065122e-06
Iter: 127 loss: 4.87634679e-06
Iter: 128 loss: 5.3527292e-06
Iter: 129 loss: 4.87605575e-06
Iter: 130 loss: 4.85213695e-06
Iter: 131 loss: 4.85210285e-06
Iter: 132 loss: 4.836264e-06
Iter: 133 loss: 4.79963455e-06
Iter: 134 loss: 5.26386566e-06
Iter: 135 loss: 4.79722621e-06
Iter: 136 loss: 4.76076275e-06
Iter: 137 loss: 4.99567523e-06
Iter: 138 loss: 4.75675643e-06
Iter: 139 loss: 4.7331514e-06
Iter: 140 loss: 5.04736454e-06
Iter: 141 loss: 4.73309592e-06
Iter: 142 loss: 4.71384556e-06
Iter: 143 loss: 4.73574619e-06
Iter: 144 loss: 4.70338455e-06
Iter: 145 loss: 4.68590133e-06
Iter: 146 loss: 4.65851372e-06
Iter: 147 loss: 4.65817584e-06
Iter: 148 loss: 4.63432161e-06
Iter: 149 loss: 4.9303203e-06
Iter: 150 loss: 4.63401329e-06
Iter: 151 loss: 4.61113314e-06
Iter: 152 loss: 4.59064177e-06
Iter: 153 loss: 4.58483464e-06
Iter: 154 loss: 4.59113107e-06
Iter: 155 loss: 4.57403257e-06
Iter: 156 loss: 4.56242242e-06
Iter: 157 loss: 4.5511506e-06
Iter: 158 loss: 4.5485549e-06
Iter: 159 loss: 4.53276562e-06
Iter: 160 loss: 4.59559169e-06
Iter: 161 loss: 4.52901577e-06
Iter: 162 loss: 4.50839343e-06
Iter: 163 loss: 4.5384345e-06
Iter: 164 loss: 4.49821755e-06
Iter: 165 loss: 4.48187e-06
Iter: 166 loss: 4.50106108e-06
Iter: 167 loss: 4.47326329e-06
Iter: 168 loss: 4.46175727e-06
Iter: 169 loss: 4.56892622e-06
Iter: 170 loss: 4.46129297e-06
Iter: 171 loss: 4.44882699e-06
Iter: 172 loss: 4.4392e-06
Iter: 173 loss: 4.43518911e-06
Iter: 174 loss: 4.41767952e-06
Iter: 175 loss: 4.43229874e-06
Iter: 176 loss: 4.40728036e-06
Iter: 177 loss: 4.39051837e-06
Iter: 178 loss: 4.5599395e-06
Iter: 179 loss: 4.3899754e-06
Iter: 180 loss: 4.37912558e-06
Iter: 181 loss: 4.36842038e-06
Iter: 182 loss: 4.36610708e-06
Iter: 183 loss: 4.34788581e-06
Iter: 184 loss: 4.49545678e-06
Iter: 185 loss: 4.34672711e-06
Iter: 186 loss: 4.3327309e-06
Iter: 187 loss: 4.44548095e-06
Iter: 188 loss: 4.33177502e-06
Iter: 189 loss: 4.31943226e-06
Iter: 190 loss: 4.30987529e-06
Iter: 191 loss: 4.3059058e-06
Iter: 192 loss: 4.28737621e-06
Iter: 193 loss: 4.29194552e-06
Iter: 194 loss: 4.27378745e-06
Iter: 195 loss: 4.27861778e-06
Iter: 196 loss: 4.26689257e-06
Iter: 197 loss: 4.25991129e-06
Iter: 198 loss: 4.24561858e-06
Iter: 199 loss: 4.50120206e-06
Iter: 200 loss: 4.24542577e-06
Iter: 201 loss: 4.23563051e-06
Iter: 202 loss: 4.37932522e-06
Iter: 203 loss: 4.23558504e-06
Iter: 204 loss: 4.22662924e-06
Iter: 205 loss: 4.24192876e-06
Iter: 206 loss: 4.22257563e-06
Iter: 207 loss: 4.21396089e-06
Iter: 208 loss: 4.19953858e-06
Iter: 209 loss: 4.19955722e-06
Iter: 210 loss: 4.19502e-06
Iter: 211 loss: 4.19177377e-06
Iter: 212 loss: 4.18595755e-06
Iter: 213 loss: 4.17423917e-06
Iter: 214 loss: 4.40151098e-06
Iter: 215 loss: 4.17407773e-06
Iter: 216 loss: 4.16128387e-06
Iter: 217 loss: 4.20767719e-06
Iter: 218 loss: 4.15800196e-06
Iter: 219 loss: 4.14690567e-06
Iter: 220 loss: 4.16737112e-06
Iter: 221 loss: 4.14214082e-06
Iter: 222 loss: 4.13315183e-06
Iter: 223 loss: 4.20902825e-06
Iter: 224 loss: 4.13251837e-06
Iter: 225 loss: 4.122272e-06
Iter: 226 loss: 4.14152964e-06
Iter: 227 loss: 4.1178273e-06
Iter: 228 loss: 4.1098e-06
Iter: 229 loss: 4.13673843e-06
Iter: 230 loss: 4.10758821e-06
Iter: 231 loss: 4.10047369e-06
Iter: 232 loss: 4.10315897e-06
Iter: 233 loss: 4.09549057e-06
Iter: 234 loss: 4.08976211e-06
Iter: 235 loss: 4.08930055e-06
Iter: 236 loss: 4.08478536e-06
Iter: 237 loss: 4.07731659e-06
Iter: 238 loss: 4.07731386e-06
Iter: 239 loss: 4.06985419e-06
Iter: 240 loss: 4.13265843e-06
Iter: 241 loss: 4.06943582e-06
Iter: 242 loss: 4.06301342e-06
Iter: 243 loss: 4.08773576e-06
Iter: 244 loss: 4.06142408e-06
Iter: 245 loss: 4.05658693e-06
Iter: 246 loss: 4.04457114e-06
Iter: 247 loss: 4.15372051e-06
Iter: 248 loss: 4.04269031e-06
Iter: 249 loss: 4.03701597e-06
Iter: 250 loss: 4.03557078e-06
Iter: 251 loss: 4.02827209e-06
Iter: 252 loss: 4.02515707e-06
Iter: 253 loss: 4.02121168e-06
Iter: 254 loss: 4.01287571e-06
Iter: 255 loss: 4.02435217e-06
Iter: 256 loss: 4.00879344e-06
Iter: 257 loss: 3.99795636e-06
Iter: 258 loss: 4.00344379e-06
Iter: 259 loss: 3.9907e-06
Iter: 260 loss: 3.98242582e-06
Iter: 261 loss: 4.02435126e-06
Iter: 262 loss: 3.98116117e-06
Iter: 263 loss: 3.97280473e-06
Iter: 264 loss: 4.02979e-06
Iter: 265 loss: 3.97197391e-06
Iter: 266 loss: 3.96376254e-06
Iter: 267 loss: 3.98634256e-06
Iter: 268 loss: 3.9611341e-06
Iter: 269 loss: 3.95592906e-06
Iter: 270 loss: 3.96599444e-06
Iter: 271 loss: 3.95390589e-06
Iter: 272 loss: 3.95007646e-06
Iter: 273 loss: 3.950061e-06
Iter: 274 loss: 3.94700146e-06
Iter: 275 loss: 3.93881965e-06
Iter: 276 loss: 3.98837528e-06
Iter: 277 loss: 3.93649771e-06
Iter: 278 loss: 3.932656e-06
Iter: 279 loss: 3.9314009e-06
Iter: 280 loss: 3.92665879e-06
Iter: 281 loss: 3.92535912e-06
Iter: 282 loss: 3.92252923e-06
Iter: 283 loss: 3.91622052e-06
Iter: 284 loss: 3.91908634e-06
Iter: 285 loss: 3.91193134e-06
Iter: 286 loss: 3.90425066e-06
Iter: 287 loss: 3.92299171e-06
Iter: 288 loss: 3.90150262e-06
Iter: 289 loss: 3.89287061e-06
Iter: 290 loss: 3.95542793e-06
Iter: 291 loss: 3.892108e-06
Iter: 292 loss: 3.88594708e-06
Iter: 293 loss: 3.88843682e-06
Iter: 294 loss: 3.8817675e-06
Iter: 295 loss: 3.87497948e-06
Iter: 296 loss: 3.87394948e-06
Iter: 297 loss: 3.86929241e-06
Iter: 298 loss: 3.86059946e-06
Iter: 299 loss: 3.89533761e-06
Iter: 300 loss: 3.85862586e-06
Iter: 301 loss: 3.85253543e-06
Iter: 302 loss: 3.92711e-06
Iter: 303 loss: 3.85249768e-06
Iter: 304 loss: 3.8471685e-06
Iter: 305 loss: 3.86266311e-06
Iter: 306 loss: 3.84564e-06
Iter: 307 loss: 3.83980205e-06
Iter: 308 loss: 3.84457417e-06
Iter: 309 loss: 3.83638462e-06
Iter: 310 loss: 3.83205042e-06
Iter: 311 loss: 3.83186944e-06
Iter: 312 loss: 3.82931285e-06
Iter: 313 loss: 3.82174312e-06
Iter: 314 loss: 3.84426767e-06
Iter: 315 loss: 3.81785185e-06
Iter: 316 loss: 3.82268945e-06
Iter: 317 loss: 3.81462405e-06
Iter: 318 loss: 3.81195878e-06
Iter: 319 loss: 3.80549409e-06
Iter: 320 loss: 3.87264208e-06
Iter: 321 loss: 3.80473e-06
Iter: 322 loss: 3.79933681e-06
Iter: 323 loss: 3.83245879e-06
Iter: 324 loss: 3.79869243e-06
Iter: 325 loss: 3.79364883e-06
Iter: 326 loss: 3.80596225e-06
Iter: 327 loss: 3.79183848e-06
Iter: 328 loss: 3.78594791e-06
Iter: 329 loss: 3.8244807e-06
Iter: 330 loss: 3.78531786e-06
Iter: 331 loss: 3.78237792e-06
Iter: 332 loss: 3.77988681e-06
Iter: 333 loss: 3.77909146e-06
Iter: 334 loss: 3.77270112e-06
Iter: 335 loss: 3.77501829e-06
Iter: 336 loss: 3.76836351e-06
Iter: 337 loss: 3.76067283e-06
Iter: 338 loss: 3.7846246e-06
Iter: 339 loss: 3.75856553e-06
Iter: 340 loss: 3.75337572e-06
Iter: 341 loss: 3.7840091e-06
Iter: 342 loss: 3.75276204e-06
Iter: 343 loss: 3.74807905e-06
Iter: 344 loss: 3.78939876e-06
Iter: 345 loss: 3.74778097e-06
Iter: 346 loss: 3.7437826e-06
Iter: 347 loss: 3.75104537e-06
Iter: 348 loss: 3.74181218e-06
Iter: 349 loss: 3.73877492e-06
Iter: 350 loss: 3.77079959e-06
Iter: 351 loss: 3.73863554e-06
Iter: 352 loss: 3.73674538e-06
Iter: 353 loss: 3.731672e-06
Iter: 354 loss: 3.76846265e-06
Iter: 355 loss: 3.73064177e-06
Iter: 356 loss: 3.7254224e-06
Iter: 357 loss: 3.72524346e-06
Iter: 358 loss: 3.72096792e-06
Iter: 359 loss: 3.72147861e-06
Iter: 360 loss: 3.71774195e-06
Iter: 361 loss: 3.71360761e-06
Iter: 362 loss: 3.7113648e-06
Iter: 363 loss: 3.70952375e-06
Iter: 364 loss: 3.70351427e-06
Iter: 365 loss: 3.70721477e-06
Iter: 366 loss: 3.69962549e-06
Iter: 367 loss: 3.69552617e-06
Iter: 368 loss: 3.69489953e-06
Iter: 369 loss: 3.69098188e-06
Iter: 370 loss: 3.69307827e-06
Iter: 371 loss: 3.68834253e-06
Iter: 372 loss: 3.68458586e-06
Iter: 373 loss: 3.69393206e-06
Iter: 374 loss: 3.68328051e-06
Iter: 375 loss: 3.67982898e-06
Iter: 376 loss: 3.67888879e-06
Iter: 377 loss: 3.6768156e-06
Iter: 378 loss: 3.67150187e-06
Iter: 379 loss: 3.70989619e-06
Iter: 380 loss: 3.67105145e-06
Iter: 381 loss: 3.66832433e-06
Iter: 382 loss: 3.66824679e-06
Iter: 383 loss: 3.66555582e-06
Iter: 384 loss: 3.66062136e-06
Iter: 385 loss: 3.77662082e-06
Iter: 386 loss: 3.66052404e-06
Iter: 387 loss: 3.65664982e-06
Iter: 388 loss: 3.71453552e-06
Iter: 389 loss: 3.65659616e-06
Iter: 390 loss: 3.6531037e-06
Iter: 391 loss: 3.64784319e-06
Iter: 392 loss: 3.64770403e-06
Iter: 393 loss: 3.64460311e-06
Iter: 394 loss: 3.6440756e-06
Iter: 395 loss: 3.64162679e-06
Iter: 396 loss: 3.63853e-06
Iter: 397 loss: 3.63838649e-06
Iter: 398 loss: 3.6346205e-06
Iter: 399 loss: 3.63814638e-06
Iter: 400 loss: 3.63247796e-06
Iter: 401 loss: 3.627978e-06
Iter: 402 loss: 3.63029721e-06
Iter: 403 loss: 3.62507103e-06
Iter: 404 loss: 3.62096443e-06
Iter: 405 loss: 3.68433189e-06
Iter: 406 loss: 3.62096216e-06
Iter: 407 loss: 3.61698039e-06
Iter: 408 loss: 3.62879905e-06
Iter: 409 loss: 3.61579464e-06
Iter: 410 loss: 3.61275806e-06
Iter: 411 loss: 3.61042248e-06
Iter: 412 loss: 3.6094907e-06
Iter: 413 loss: 3.6041763e-06
Iter: 414 loss: 3.61682442e-06
Iter: 415 loss: 3.6022625e-06
Iter: 416 loss: 3.59808337e-06
Iter: 417 loss: 3.60443687e-06
Iter: 418 loss: 3.59606975e-06
Iter: 419 loss: 3.59196019e-06
Iter: 420 loss: 3.65649453e-06
Iter: 421 loss: 3.5918697e-06
Iter: 422 loss: 3.58905936e-06
Iter: 423 loss: 3.61641855e-06
Iter: 424 loss: 3.58890361e-06
Iter: 425 loss: 3.58699685e-06
Iter: 426 loss: 3.58308557e-06
Iter: 427 loss: 3.64919833e-06
Iter: 428 loss: 3.58295051e-06
Iter: 429 loss: 3.57815179e-06
Iter: 430 loss: 3.61828916e-06
Iter: 431 loss: 3.57788372e-06
Iter: 432 loss: 3.5742e-06
Iter: 433 loss: 3.58244961e-06
Iter: 434 loss: 3.57268618e-06
Iter: 435 loss: 3.56944656e-06
Iter: 436 loss: 3.58497118e-06
Iter: 437 loss: 3.56877831e-06
Iter: 438 loss: 3.5651949e-06
Iter: 439 loss: 3.56381088e-06
Iter: 440 loss: 3.56185069e-06
Iter: 441 loss: 3.55857651e-06
Iter: 442 loss: 3.558873e-06
Iter: 443 loss: 3.55616817e-06
Iter: 444 loss: 3.55205043e-06
Iter: 445 loss: 3.58012835e-06
Iter: 446 loss: 3.55159341e-06
Iter: 447 loss: 3.54781741e-06
Iter: 448 loss: 3.57219233e-06
Iter: 449 loss: 3.5474e-06
Iter: 450 loss: 3.54424878e-06
Iter: 451 loss: 3.54401982e-06
Iter: 452 loss: 3.54166923e-06
Iter: 453 loss: 3.53884434e-06
Iter: 454 loss: 3.53797168e-06
Iter: 455 loss: 3.53625387e-06
Iter: 456 loss: 3.53193309e-06
Iter: 457 loss: 3.5405692e-06
Iter: 458 loss: 3.53003429e-06
Iter: 459 loss: 3.52667348e-06
Iter: 460 loss: 3.5264361e-06
Iter: 461 loss: 3.52350617e-06
Iter: 462 loss: 3.53548103e-06
Iter: 463 loss: 3.52283746e-06
Iter: 464 loss: 3.52112988e-06
Iter: 465 loss: 3.51676135e-06
Iter: 466 loss: 3.55209204e-06
Iter: 467 loss: 3.51602102e-06
Iter: 468 loss: 3.51281528e-06
Iter: 469 loss: 3.51236531e-06
Iter: 470 loss: 3.50985965e-06
Iter: 471 loss: 3.50916025e-06
Iter: 472 loss: 3.5076132e-06
Iter: 473 loss: 3.50388109e-06
Iter: 474 loss: 3.51805693e-06
Iter: 475 loss: 3.50289247e-06
Iter: 476 loss: 3.49970674e-06
Iter: 477 loss: 3.50715982e-06
Iter: 478 loss: 3.49847141e-06
Iter: 479 loss: 3.49595985e-06
Iter: 480 loss: 3.49467837e-06
Iter: 481 loss: 3.49341167e-06
Iter: 482 loss: 3.4899308e-06
Iter: 483 loss: 3.51066819e-06
Iter: 484 loss: 3.48944968e-06
Iter: 485 loss: 3.48613548e-06
Iter: 486 loss: 3.48641242e-06
Iter: 487 loss: 3.483608e-06
Iter: 488 loss: 3.47985701e-06
Iter: 489 loss: 3.479822e-06
Iter: 490 loss: 3.47740911e-06
Iter: 491 loss: 3.47450555e-06
Iter: 492 loss: 3.47424202e-06
Iter: 493 loss: 3.4714667e-06
Iter: 494 loss: 3.47314631e-06
Iter: 495 loss: 3.469788e-06
Iter: 496 loss: 3.46560432e-06
Iter: 497 loss: 3.49332299e-06
Iter: 498 loss: 3.4652162e-06
Iter: 499 loss: 3.460789e-06
Iter: 500 loss: 3.48686353e-06
Iter: 501 loss: 3.46016327e-06
Iter: 502 loss: 3.45859e-06
Iter: 503 loss: 3.45667286e-06
Iter: 504 loss: 3.45643e-06
Iter: 505 loss: 3.4541581e-06
Iter: 506 loss: 3.47896957e-06
Iter: 507 loss: 3.4540742e-06
Iter: 508 loss: 3.45143303e-06
Iter: 509 loss: 3.45003514e-06
Iter: 510 loss: 3.44886894e-06
Iter: 511 loss: 3.4459681e-06
Iter: 512 loss: 3.4673817e-06
Iter: 513 loss: 3.44569457e-06
Iter: 514 loss: 3.44337082e-06
Iter: 515 loss: 3.44767454e-06
Iter: 516 loss: 3.44228692e-06
Iter: 517 loss: 3.43984038e-06
Iter: 518 loss: 3.43684155e-06
Iter: 519 loss: 3.436518e-06
Iter: 520 loss: 3.43273564e-06
Iter: 521 loss: 3.45327021e-06
Iter: 522 loss: 3.4321688e-06
Iter: 523 loss: 3.42844146e-06
Iter: 524 loss: 3.43591455e-06
Iter: 525 loss: 3.42706176e-06
Iter: 526 loss: 3.42360454e-06
Iter: 527 loss: 3.43554484e-06
Iter: 528 loss: 3.42271392e-06
Iter: 529 loss: 3.41807731e-06
Iter: 530 loss: 3.43312081e-06
Iter: 531 loss: 3.41678151e-06
Iter: 532 loss: 3.41398299e-06
Iter: 533 loss: 3.42114618e-06
Iter: 534 loss: 3.41297982e-06
Iter: 535 loss: 3.41053146e-06
Iter: 536 loss: 3.40813153e-06
Iter: 537 loss: 3.40758106e-06
Iter: 538 loss: 3.40291217e-06
Iter: 539 loss: 3.40664519e-06
Iter: 540 loss: 3.40024e-06
Iter: 541 loss: 3.40463771e-06
Iter: 542 loss: 3.39840813e-06
Iter: 543 loss: 3.39742019e-06
Iter: 544 loss: 3.39543158e-06
Iter: 545 loss: 3.43637976e-06
Iter: 546 loss: 3.39536518e-06
Iter: 547 loss: 3.39270559e-06
Iter: 548 loss: 3.39695202e-06
Iter: 549 loss: 3.39142298e-06
Iter: 550 loss: 3.38795326e-06
Iter: 551 loss: 3.40906172e-06
Iter: 552 loss: 3.38750579e-06
Iter: 553 loss: 3.38556083e-06
Iter: 554 loss: 3.38798759e-06
Iter: 555 loss: 3.38456152e-06
Iter: 556 loss: 3.38177983e-06
Iter: 557 loss: 3.38679274e-06
Iter: 558 loss: 3.38054315e-06
Iter: 559 loss: 3.37854317e-06
Iter: 560 loss: 3.3768406e-06
Iter: 561 loss: 3.37631786e-06
Iter: 562 loss: 3.37291885e-06
Iter: 563 loss: 3.38985251e-06
Iter: 564 loss: 3.37244865e-06
Iter: 565 loss: 3.36939343e-06
Iter: 566 loss: 3.37630581e-06
Iter: 567 loss: 3.36833091e-06
Iter: 568 loss: 3.36547487e-06
Iter: 569 loss: 3.3980125e-06
Iter: 570 loss: 3.36546736e-06
Iter: 571 loss: 3.36361018e-06
Iter: 572 loss: 3.36361882e-06
Iter: 573 loss: 3.36215362e-06
Iter: 574 loss: 3.35913523e-06
Iter: 575 loss: 3.35585059e-06
Iter: 576 loss: 3.35544269e-06
Iter: 577 loss: 3.35169921e-06
Iter: 578 loss: 3.37786855e-06
Iter: 579 loss: 3.35135519e-06
Iter: 580 loss: 3.34804304e-06
Iter: 581 loss: 3.35308459e-06
Iter: 582 loss: 3.34653168e-06
Iter: 583 loss: 3.34508377e-06
Iter: 584 loss: 3.34452238e-06
Iter: 585 loss: 3.34280912e-06
Iter: 586 loss: 3.34172546e-06
Iter: 587 loss: 3.34095512e-06
Iter: 588 loss: 3.3391284e-06
Iter: 589 loss: 3.34667857e-06
Iter: 590 loss: 3.3387555e-06
Iter: 591 loss: 3.33640901e-06
Iter: 592 loss: 3.33989919e-06
Iter: 593 loss: 3.33540834e-06
Iter: 594 loss: 3.33362641e-06
Iter: 595 loss: 3.34386391e-06
Iter: 596 loss: 3.33345292e-06
Iter: 597 loss: 3.33174603e-06
Iter: 598 loss: 3.32997274e-06
Iter: 599 loss: 3.32955346e-06
Iter: 600 loss: 3.32641639e-06
Iter: 601 loss: 3.32719173e-06
Iter: 602 loss: 3.32391733e-06
Iter: 603 loss: 3.32094373e-06
Iter: 604 loss: 3.34338893e-06
Iter: 605 loss: 3.32067043e-06
Iter: 606 loss: 3.31787828e-06
Iter: 607 loss: 3.33783464e-06
Iter: 608 loss: 3.31756564e-06
Iter: 609 loss: 3.31624642e-06
Iter: 610 loss: 3.31533693e-06
Iter: 611 loss: 3.31478236e-06
Iter: 612 loss: 3.31217075e-06
Iter: 613 loss: 3.31618503e-06
Iter: 614 loss: 3.31095657e-06
Iter: 615 loss: 3.30830767e-06
Iter: 616 loss: 3.30875309e-06
Iter: 617 loss: 3.30629882e-06
Iter: 618 loss: 3.30311832e-06
Iter: 619 loss: 3.31860429e-06
Iter: 620 loss: 3.30260809e-06
Iter: 621 loss: 3.30070043e-06
Iter: 622 loss: 3.3005872e-06
Iter: 623 loss: 3.29893192e-06
Iter: 624 loss: 3.29823547e-06
Iter: 625 loss: 3.29723935e-06
Iter: 626 loss: 3.29539262e-06
Iter: 627 loss: 3.29635623e-06
Iter: 628 loss: 3.29418572e-06
Iter: 629 loss: 3.29093814e-06
Iter: 630 loss: 3.30849957e-06
Iter: 631 loss: 3.2904436e-06
Iter: 632 loss: 3.28899068e-06
Iter: 633 loss: 3.2907019e-06
Iter: 634 loss: 3.28831334e-06
Iter: 635 loss: 3.28581405e-06
Iter: 636 loss: 3.28627311e-06
Iter: 637 loss: 3.28414853e-06
Iter: 638 loss: 3.28201759e-06
Iter: 639 loss: 3.28690771e-06
Iter: 640 loss: 3.28122132e-06
Iter: 641 loss: 3.27908811e-06
Iter: 642 loss: 3.28547867e-06
Iter: 643 loss: 3.27825364e-06
Iter: 644 loss: 3.27614953e-06
Iter: 645 loss: 3.29813861e-06
Iter: 646 loss: 3.27610087e-06
Iter: 647 loss: 3.27487e-06
Iter: 648 loss: 3.27237331e-06
Iter: 649 loss: 3.31529031e-06
Iter: 650 loss: 3.2723035e-06
Iter: 651 loss: 3.26912186e-06
Iter: 652 loss: 3.27621319e-06
Iter: 653 loss: 3.26797363e-06
Iter: 654 loss: 3.26471513e-06
Iter: 655 loss: 3.29298723e-06
Iter: 656 loss: 3.26456961e-06
Iter: 657 loss: 3.26236045e-06
Iter: 658 loss: 3.26024815e-06
Iter: 659 loss: 3.25980227e-06
Iter: 660 loss: 3.2588955e-06
Iter: 661 loss: 3.25782707e-06
Iter: 662 loss: 3.2565863e-06
Iter: 663 loss: 3.25818337e-06
Iter: 664 loss: 3.25572546e-06
Iter: 665 loss: 3.25469932e-06
Iter: 666 loss: 3.25287783e-06
Iter: 667 loss: 3.25288875e-06
Iter: 668 loss: 3.24943176e-06
Iter: 669 loss: 3.2666162e-06
Iter: 670 loss: 3.24883922e-06
Iter: 671 loss: 3.24738266e-06
Iter: 672 loss: 3.24663165e-06
Iter: 673 loss: 3.24587199e-06
Iter: 674 loss: 3.2434441e-06
Iter: 675 loss: 3.26148779e-06
Iter: 676 loss: 3.24313692e-06
Iter: 677 loss: 3.2410619e-06
Iter: 678 loss: 3.23817449e-06
Iter: 679 loss: 3.23803806e-06
Iter: 680 loss: 3.23471204e-06
Iter: 681 loss: 3.24672487e-06
Iter: 682 loss: 3.23382096e-06
Iter: 683 loss: 3.23144877e-06
Iter: 684 loss: 3.24364373e-06
Iter: 685 loss: 3.2311541e-06
Iter: 686 loss: 3.22859887e-06
Iter: 687 loss: 3.24313601e-06
Iter: 688 loss: 3.22825122e-06
Iter: 689 loss: 3.2264536e-06
Iter: 690 loss: 3.22531673e-06
Iter: 691 loss: 3.22456822e-06
Iter: 692 loss: 3.22214851e-06
Iter: 693 loss: 3.22932374e-06
Iter: 694 loss: 3.22144047e-06
Iter: 695 loss: 3.21934726e-06
Iter: 696 loss: 3.22520918e-06
Iter: 697 loss: 3.21854441e-06
Iter: 698 loss: 3.21603579e-06
Iter: 699 loss: 3.24690018e-06
Iter: 700 loss: 3.21598236e-06
Iter: 701 loss: 3.21486596e-06
Iter: 702 loss: 3.21261109e-06
Iter: 703 loss: 3.2552598e-06
Iter: 704 loss: 3.2126627e-06
Iter: 705 loss: 3.21059588e-06
Iter: 706 loss: 3.24323e-06
Iter: 707 loss: 3.21058769e-06
Iter: 708 loss: 3.20874778e-06
Iter: 709 loss: 3.21064772e-06
Iter: 710 loss: 3.2077005e-06
Iter: 711 loss: 3.20606e-06
Iter: 712 loss: 3.20425693e-06
Iter: 713 loss: 3.20386357e-06
Iter: 714 loss: 3.20164281e-06
Iter: 715 loss: 3.20172785e-06
Iter: 716 loss: 3.20020672e-06
Iter: 717 loss: 3.19816149e-06
Iter: 718 loss: 3.19801848e-06
Iter: 719 loss: 3.19524952e-06
Iter: 720 loss: 3.21025641e-06
Iter: 721 loss: 3.19484e-06
Iter: 722 loss: 3.19283163e-06
Iter: 723 loss: 3.1976615e-06
Iter: 724 loss: 3.19218384e-06
Iter: 725 loss: 3.19006267e-06
Iter: 726 loss: 3.20666095e-06
Iter: 727 loss: 3.18999e-06
Iter: 728 loss: 3.18818684e-06
Iter: 729 loss: 3.1860302e-06
Iter: 730 loss: 3.18586285e-06
Iter: 731 loss: 3.18318803e-06
Iter: 732 loss: 3.2012008e-06
Iter: 733 loss: 3.18299976e-06
Iter: 734 loss: 3.18173124e-06
Iter: 735 loss: 3.18152138e-06
Iter: 736 loss: 3.18076718e-06
Iter: 737 loss: 3.17817376e-06
Iter: 738 loss: 3.18531261e-06
Iter: 739 loss: 3.17672198e-06
Iter: 740 loss: 3.176978e-06
Iter: 741 loss: 3.17547415e-06
Iter: 742 loss: 3.17435024e-06
Iter: 743 loss: 3.17396598e-06
Iter: 744 loss: 3.17329e-06
Iter: 745 loss: 3.17167337e-06
Iter: 746 loss: 3.16959176e-06
Iter: 747 loss: 3.1694367e-06
Iter: 748 loss: 3.16755541e-06
Iter: 749 loss: 3.1675163e-06
Iter: 750 loss: 3.16582054e-06
Iter: 751 loss: 3.16288242e-06
Iter: 752 loss: 3.16280966e-06
Iter: 753 loss: 3.16027445e-06
Iter: 754 loss: 3.17609829e-06
Iter: 755 loss: 3.15998113e-06
Iter: 756 loss: 3.15770421e-06
Iter: 757 loss: 3.16642695e-06
Iter: 758 loss: 3.15710486e-06
Iter: 759 loss: 3.15490252e-06
Iter: 760 loss: 3.16831893e-06
Iter: 761 loss: 3.15454963e-06
Iter: 762 loss: 3.15298439e-06
Iter: 763 loss: 3.15261968e-06
Iter: 764 loss: 3.15163948e-06
Iter: 765 loss: 3.14966769e-06
Iter: 766 loss: 3.16619798e-06
Iter: 767 loss: 3.14955378e-06
Iter: 768 loss: 3.14757381e-06
Iter: 769 loss: 3.15400098e-06
Iter: 770 loss: 3.14701447e-06
Iter: 771 loss: 3.14545241e-06
Iter: 772 loss: 3.14310046e-06
Iter: 773 loss: 3.1430377e-06
Iter: 774 loss: 3.14204362e-06
Iter: 775 loss: 3.14179806e-06
Iter: 776 loss: 3.1405084e-06
Iter: 777 loss: 3.13904866e-06
Iter: 778 loss: 3.13887654e-06
Iter: 779 loss: 3.13709256e-06
Iter: 780 loss: 3.14040426e-06
Iter: 781 loss: 3.13633836e-06
Iter: 782 loss: 3.13459532e-06
Iter: 783 loss: 3.14660792e-06
Iter: 784 loss: 3.13442956e-06
Iter: 785 loss: 3.13267083e-06
Iter: 786 loss: 3.13512373e-06
Iter: 787 loss: 3.13196597e-06
Iter: 788 loss: 3.13052215e-06
Iter: 789 loss: 3.12846e-06
Iter: 790 loss: 3.12848056e-06
Iter: 791 loss: 3.12590828e-06
Iter: 792 loss: 3.15369516e-06
Iter: 793 loss: 3.12598627e-06
Iter: 794 loss: 3.12357156e-06
Iter: 795 loss: 3.13214559e-06
Iter: 796 loss: 3.12292809e-06
Iter: 797 loss: 3.12127304e-06
Iter: 798 loss: 3.12177781e-06
Iter: 799 loss: 3.12018415e-06
Iter: 800 loss: 3.11847816e-06
Iter: 801 loss: 3.11849044e-06
Iter: 802 loss: 3.11707254e-06
Iter: 803 loss: 3.11653093e-06
Iter: 804 loss: 3.11566328e-06
Iter: 805 loss: 3.11448366e-06
Iter: 806 loss: 3.11472104e-06
Iter: 807 loss: 3.11352187e-06
Iter: 808 loss: 3.1116333e-06
Iter: 809 loss: 3.12323755e-06
Iter: 810 loss: 3.11140957e-06
Iter: 811 loss: 3.10933683e-06
Iter: 812 loss: 3.11318126e-06
Iter: 813 loss: 3.10843643e-06
Iter: 814 loss: 3.10675864e-06
Iter: 815 loss: 3.10346377e-06
Iter: 816 loss: 3.17486888e-06
Iter: 817 loss: 3.10345058e-06
Iter: 818 loss: 3.10239557e-06
Iter: 819 loss: 3.10189012e-06
Iter: 820 loss: 3.10038467e-06
Iter: 821 loss: 3.10117048e-06
Iter: 822 loss: 3.09945153e-06
Iter: 823 loss: 3.09775919e-06
Iter: 824 loss: 3.09569714e-06
Iter: 825 loss: 3.09552388e-06
Iter: 826 loss: 3.09260463e-06
Iter: 827 loss: 3.11496842e-06
Iter: 828 loss: 3.0925039e-06
Iter: 829 loss: 3.0909921e-06
Iter: 830 loss: 3.0909514e-06
Iter: 831 loss: 3.08992912e-06
Iter: 832 loss: 3.08722906e-06
Iter: 833 loss: 3.10795713e-06
Iter: 834 loss: 3.08665813e-06
Iter: 835 loss: 3.08608742e-06
Iter: 836 loss: 3.08487915e-06
Iter: 837 loss: 3.08391645e-06
Iter: 838 loss: 3.08321205e-06
Iter: 839 loss: 3.08291828e-06
Iter: 840 loss: 3.08159451e-06
Iter: 841 loss: 3.07945447e-06
Iter: 842 loss: 3.07942855e-06
Iter: 843 loss: 3.07780715e-06
Iter: 844 loss: 3.07753453e-06
Iter: 845 loss: 3.07666141e-06
Iter: 846 loss: 3.07555888e-06
Iter: 847 loss: 3.07552227e-06
Iter: 848 loss: 3.07395158e-06
Iter: 849 loss: 3.07694791e-06
Iter: 850 loss: 3.07334039e-06
Iter: 851 loss: 3.07174264e-06
Iter: 852 loss: 3.07818027e-06
Iter: 853 loss: 3.07136133e-06
Iter: 854 loss: 3.06966263e-06
Iter: 855 loss: 3.07258347e-06
Iter: 856 loss: 3.06885795e-06
Iter: 857 loss: 3.06731636e-06
Iter: 858 loss: 3.06867378e-06
Iter: 859 loss: 3.06631659e-06
Iter: 860 loss: 3.06431821e-06
Iter: 861 loss: 3.07290497e-06
Iter: 862 loss: 3.06392303e-06
Iter: 863 loss: 3.06231368e-06
Iter: 864 loss: 3.0800818e-06
Iter: 865 loss: 3.06231777e-06
Iter: 866 loss: 3.06122297e-06
Iter: 867 loss: 3.05958656e-06
Iter: 868 loss: 3.0595379e-06
Iter: 869 loss: 3.05799767e-06
Iter: 870 loss: 3.05784579e-06
Iter: 871 loss: 3.05683443e-06
Iter: 872 loss: 3.05445747e-06
Iter: 873 loss: 3.0856113e-06
Iter: 874 loss: 3.05432241e-06
Iter: 875 loss: 3.05229105e-06
Iter: 876 loss: 3.07106257e-06
Iter: 877 loss: 3.05221783e-06
Iter: 878 loss: 3.05049502e-06
Iter: 879 loss: 3.06317565e-06
Iter: 880 loss: 3.050332e-06
Iter: 881 loss: 3.04920331e-06
Iter: 882 loss: 3.04676678e-06
Iter: 883 loss: 3.08559333e-06
Iter: 884 loss: 3.04661808e-06
Iter: 885 loss: 3.04427294e-06
Iter: 886 loss: 3.06210109e-06
Iter: 887 loss: 3.0440483e-06
Iter: 888 loss: 3.0424726e-06
Iter: 889 loss: 3.06356537e-06
Iter: 890 loss: 3.04245168e-06
Iter: 891 loss: 3.04153946e-06
Iter: 892 loss: 3.04076275e-06
Iter: 893 loss: 3.04049945e-06
Iter: 894 loss: 3.03850129e-06
Iter: 895 loss: 3.03825209e-06
Iter: 896 loss: 3.03687102e-06
Iter: 897 loss: 3.03480829e-06
Iter: 898 loss: 3.0348117e-06
Iter: 899 loss: 3.03310571e-06
Iter: 900 loss: 3.03626643e-06
Iter: 901 loss: 3.03234401e-06
Iter: 902 loss: 3.03074239e-06
Iter: 903 loss: 3.03603065e-06
Iter: 904 loss: 3.03024126e-06
Iter: 905 loss: 3.02875219e-06
Iter: 906 loss: 3.04142441e-06
Iter: 907 loss: 3.02860872e-06
Iter: 908 loss: 3.02756553e-06
Iter: 909 loss: 3.02525541e-06
Iter: 910 loss: 3.05912772e-06
Iter: 911 loss: 3.02509193e-06
Iter: 912 loss: 3.02394324e-06
Iter: 913 loss: 3.02384251e-06
Iter: 914 loss: 3.0224669e-06
Iter: 915 loss: 3.02187141e-06
Iter: 916 loss: 3.02111243e-06
Iter: 917 loss: 3.01979981e-06
Iter: 918 loss: 3.02225862e-06
Iter: 919 loss: 3.01920591e-06
Iter: 920 loss: 3.01774799e-06
Iter: 921 loss: 3.01898581e-06
Iter: 922 loss: 3.01690443e-06
Iter: 923 loss: 3.0153e-06
Iter: 924 loss: 3.03665911e-06
Iter: 925 loss: 3.01534851e-06
Iter: 926 loss: 3.01390651e-06
Iter: 927 loss: 3.01113323e-06
Iter: 928 loss: 3.06079187e-06
Iter: 929 loss: 3.01101e-06
Iter: 930 loss: 3.00862371e-06
Iter: 931 loss: 3.03006618e-06
Iter: 932 loss: 3.00858119e-06
Iter: 933 loss: 3.00695569e-06
Iter: 934 loss: 3.02273338e-06
Iter: 935 loss: 3.00694023e-06
Iter: 936 loss: 3.00554598e-06
Iter: 937 loss: 3.00751435e-06
Iter: 938 loss: 3.00492138e-06
Iter: 939 loss: 3.00381953e-06
Iter: 940 loss: 3.01223417e-06
Iter: 941 loss: 3.00377815e-06
Iter: 942 loss: 3.00256443e-06
Iter: 943 loss: 3.00208058e-06
Iter: 944 loss: 3.00145211e-06
Iter: 945 loss: 2.99999e-06
Iter: 946 loss: 3.00253146e-06
Iter: 947 loss: 2.99946214e-06
Iter: 948 loss: 2.9982707e-06
Iter: 949 loss: 3.0072847e-06
Iter: 950 loss: 2.99815474e-06
Iter: 951 loss: 2.9967141e-06
Iter: 952 loss: 2.99711769e-06
Iter: 953 loss: 2.99572753e-06
Iter: 954 loss: 2.99448948e-06
Iter: 955 loss: 2.99293333e-06
Iter: 956 loss: 2.99279941e-06
Iter: 957 loss: 2.99046e-06
Iter: 958 loss: 3.00153238e-06
Iter: 959 loss: 2.99005569e-06
Iter: 960 loss: 2.98816713e-06
Iter: 961 loss: 3.01320802e-06
Iter: 962 loss: 2.9880739e-06
Iter: 963 loss: 2.98689156e-06
Iter: 964 loss: 2.98669579e-06
Iter: 965 loss: 2.98573877e-06
Iter: 966 loss: 2.98422174e-06
Iter: 967 loss: 2.98396117e-06
Iter: 968 loss: 2.98277428e-06
Iter: 969 loss: 2.98163241e-06
Iter: 970 loss: 2.98147916e-06
Iter: 971 loss: 2.98004716e-06
Iter: 972 loss: 2.97814472e-06
Iter: 973 loss: 2.97798761e-06
Iter: 974 loss: 2.97666293e-06
Iter: 975 loss: 2.97646238e-06
Iter: 976 loss: 2.97548149e-06
Iter: 977 loss: 2.97454562e-06
Iter: 978 loss: 2.97438328e-06
Iter: 979 loss: 2.97272072e-06
Iter: 980 loss: 2.97229099e-06
Iter: 981 loss: 2.97136489e-06
Iter: 982 loss: 2.97000497e-06
Iter: 983 loss: 2.96990038e-06
Iter: 984 loss: 2.9688008e-06
Iter: 985 loss: 2.9675864e-06
Iter: 986 loss: 2.96746589e-06
Iter: 987 loss: 2.96566259e-06
Iter: 988 loss: 2.96865619e-06
Iter: 989 loss: 2.96497387e-06
Iter: 990 loss: 2.96332428e-06
Iter: 991 loss: 2.96373514e-06
Iter: 992 loss: 2.9621774e-06
Iter: 993 loss: 2.96068492e-06
Iter: 994 loss: 2.96061239e-06
Iter: 995 loss: 2.95939481e-06
Iter: 996 loss: 2.95763311e-06
Iter: 997 loss: 2.95758377e-06
Iter: 998 loss: 2.95566747e-06
Iter: 999 loss: 2.96450571e-06
Iter: 1000 loss: 2.95525842e-06
Iter: 1001 loss: 2.95375708e-06
Iter: 1002 loss: 2.97059114e-06
Iter: 1003 loss: 2.95373661e-06
Iter: 1004 loss: 2.95231371e-06
Iter: 1005 loss: 2.9515104e-06
Iter: 1006 loss: 2.95093196e-06
Iter: 1007 loss: 2.9498151e-06
Iter: 1008 loss: 2.94973415e-06
Iter: 1009 loss: 2.94914753e-06
Iter: 1010 loss: 2.94760503e-06
Iter: 1011 loss: 2.95673044e-06
Iter: 1012 loss: 2.94730626e-06
Iter: 1013 loss: 2.94565393e-06
Iter: 1014 loss: 2.96938492e-06
Iter: 1015 loss: 2.94564961e-06
Iter: 1016 loss: 2.94451888e-06
Iter: 1017 loss: 2.95338577e-06
Iter: 1018 loss: 2.94446863e-06
Iter: 1019 loss: 2.94359461e-06
Iter: 1020 loss: 2.94159963e-06
Iter: 1021 loss: 2.96165331e-06
Iter: 1022 loss: 2.94131678e-06
Iter: 1023 loss: 2.93869471e-06
Iter: 1024 loss: 2.95122663e-06
Iter: 1025 loss: 2.93827429e-06
Iter: 1026 loss: 2.93622134e-06
Iter: 1027 loss: 2.94224697e-06
Iter: 1028 loss: 2.93560606e-06
Iter: 1029 loss: 2.93413586e-06
Iter: 1030 loss: 2.95597511e-06
Iter: 1031 loss: 2.93412359e-06
Iter: 1032 loss: 2.93286803e-06
Iter: 1033 loss: 2.93002086e-06
Iter: 1034 loss: 2.97851716e-06
Iter: 1035 loss: 2.92996106e-06
Iter: 1036 loss: 2.92811774e-06
Iter: 1037 loss: 2.95581322e-06
Iter: 1038 loss: 2.92813411e-06
Iter: 1039 loss: 2.92662025e-06
Iter: 1040 loss: 2.93535163e-06
Iter: 1041 loss: 2.92642972e-06
Iter: 1042 loss: 2.92515529e-06
Iter: 1043 loss: 2.92761388e-06
Iter: 1044 loss: 2.92463847e-06
Iter: 1045 loss: 2.92334153e-06
Iter: 1046 loss: 2.93142239e-06
Iter: 1047 loss: 2.92318964e-06
Iter: 1048 loss: 2.92237678e-06
Iter: 1049 loss: 2.92063373e-06
Iter: 1050 loss: 2.95258906e-06
Iter: 1051 loss: 2.92066443e-06
Iter: 1052 loss: 2.91894594e-06
Iter: 1053 loss: 2.93513267e-06
Iter: 1054 loss: 2.91892093e-06
Iter: 1055 loss: 2.91682863e-06
Iter: 1056 loss: 2.91671563e-06
Iter: 1057 loss: 2.91510059e-06
Iter: 1058 loss: 2.91333436e-06
Iter: 1059 loss: 2.91660945e-06
Iter: 1060 loss: 2.91259312e-06
Iter: 1061 loss: 2.91087918e-06
Iter: 1062 loss: 2.91479319e-06
Iter: 1063 loss: 2.91021252e-06
Iter: 1064 loss: 2.9085063e-06
Iter: 1065 loss: 2.91518108e-06
Iter: 1066 loss: 2.90823846e-06
Iter: 1067 loss: 2.90668254e-06
Iter: 1068 loss: 2.91833067e-06
Iter: 1069 loss: 2.90655044e-06
Iter: 1070 loss: 2.90548905e-06
Iter: 1071 loss: 2.90637536e-06
Iter: 1072 loss: 2.9049022e-06
Iter: 1073 loss: 2.90357752e-06
Iter: 1074 loss: 2.9039752e-06
Iter: 1075 loss: 2.90268099e-06
Iter: 1076 loss: 2.9017383e-06
Iter: 1077 loss: 2.90163098e-06
Iter: 1078 loss: 2.90086746e-06
Iter: 1079 loss: 2.90000753e-06
Iter: 1080 loss: 2.89981381e-06
Iter: 1081 loss: 2.89817399e-06
Iter: 1082 loss: 2.90517141e-06
Iter: 1083 loss: 2.89776813e-06
Iter: 1084 loss: 2.89672334e-06
Iter: 1085 loss: 2.89562286e-06
Iter: 1086 loss: 2.89542572e-06
Iter: 1087 loss: 2.89364107e-06
Iter: 1088 loss: 2.90613571e-06
Iter: 1089 loss: 2.89351556e-06
Iter: 1090 loss: 2.89158561e-06
Iter: 1091 loss: 2.89800278e-06
Iter: 1092 loss: 2.8910174e-06
Iter: 1093 loss: 2.89024729e-06
Iter: 1094 loss: 2.88840693e-06
Iter: 1095 loss: 2.91128799e-06
Iter: 1096 loss: 2.88834235e-06
Iter: 1097 loss: 2.88616138e-06
Iter: 1098 loss: 2.90479375e-06
Iter: 1099 loss: 2.88606543e-06
Iter: 1100 loss: 2.88434853e-06
Iter: 1101 loss: 2.88831438e-06
Iter: 1102 loss: 2.88366118e-06
Iter: 1103 loss: 2.88218303e-06
Iter: 1104 loss: 2.89720356e-06
Iter: 1105 loss: 2.8820923e-06
Iter: 1106 loss: 2.88071988e-06
Iter: 1107 loss: 2.87957e-06
Iter: 1108 loss: 2.87921625e-06
Iter: 1109 loss: 2.8773984e-06
Iter: 1110 loss: 2.88849174e-06
Iter: 1111 loss: 2.87714875e-06
Iter: 1112 loss: 2.87585408e-06
Iter: 1113 loss: 2.89091872e-06
Iter: 1114 loss: 2.87584589e-06
Iter: 1115 loss: 2.8747e-06
Iter: 1116 loss: 2.87363537e-06
Iter: 1117 loss: 2.87334433e-06
Iter: 1118 loss: 2.87205808e-06
Iter: 1119 loss: 2.87204625e-06
Iter: 1120 loss: 2.8712941e-06
Iter: 1121 loss: 2.86918907e-06
Iter: 1122 loss: 2.88151296e-06
Iter: 1123 loss: 2.86863e-06
Iter: 1124 loss: 2.86900604e-06
Iter: 1125 loss: 2.86765589e-06
Iter: 1126 loss: 2.8669765e-06
Iter: 1127 loss: 2.86685918e-06
Iter: 1128 loss: 2.86638442e-06
Iter: 1129 loss: 2.86530576e-06
Iter: 1130 loss: 2.86355203e-06
Iter: 1131 loss: 2.90681646e-06
Iter: 1132 loss: 2.86355248e-06
Iter: 1133 loss: 2.86145814e-06
Iter: 1134 loss: 2.86624572e-06
Iter: 1135 loss: 2.86063869e-06
Iter: 1136 loss: 2.85873853e-06
Iter: 1137 loss: 2.85869919e-06
Iter: 1138 loss: 2.85734086e-06
Iter: 1139 loss: 2.86096383e-06
Iter: 1140 loss: 2.85685019e-06
Iter: 1141 loss: 2.85541205e-06
Iter: 1142 loss: 2.85465603e-06
Iter: 1143 loss: 2.8538866e-06
Iter: 1144 loss: 2.85214037e-06
Iter: 1145 loss: 2.86751e-06
Iter: 1146 loss: 2.85196506e-06
Iter: 1147 loss: 2.85056649e-06
Iter: 1148 loss: 2.86350928e-06
Iter: 1149 loss: 2.85049168e-06
Iter: 1150 loss: 2.84946168e-06
Iter: 1151 loss: 2.85e-06
Iter: 1152 loss: 2.84884686e-06
Iter: 1153 loss: 2.84789394e-06
Iter: 1154 loss: 2.85478154e-06
Iter: 1155 loss: 2.84783209e-06
Iter: 1156 loss: 2.84689827e-06
Iter: 1157 loss: 2.84501493e-06
Iter: 1158 loss: 2.87304965e-06
Iter: 1159 loss: 2.84488851e-06
Iter: 1160 loss: 2.84386624e-06
Iter: 1161 loss: 2.84369366e-06
Iter: 1162 loss: 2.8425934e-06
Iter: 1163 loss: 2.84343878e-06
Iter: 1164 loss: 2.84194175e-06
Iter: 1165 loss: 2.84053976e-06
Iter: 1166 loss: 2.83886834e-06
Iter: 1167 loss: 2.83869986e-06
Iter: 1168 loss: 2.83652798e-06
Iter: 1169 loss: 2.83916916e-06
Iter: 1170 loss: 2.83544546e-06
Iter: 1171 loss: 2.83318468e-06
Iter: 1172 loss: 2.84101861e-06
Iter: 1173 loss: 2.83262443e-06
Iter: 1174 loss: 2.83116196e-06
Iter: 1175 loss: 2.83113923e-06
Iter: 1176 loss: 2.83007375e-06
Iter: 1177 loss: 2.83462077e-06
Iter: 1178 loss: 2.82984547e-06
Iter: 1179 loss: 2.82879864e-06
Iter: 1180 loss: 2.82686278e-06
Iter: 1181 loss: 2.86742443e-06
Iter: 1182 loss: 2.82684186e-06
Iter: 1183 loss: 2.82762721e-06
Iter: 1184 loss: 2.82609449e-06
Iter: 1185 loss: 2.82562951e-06
Iter: 1186 loss: 2.8245156e-06
Iter: 1187 loss: 2.83884219e-06
Iter: 1188 loss: 2.82438168e-06
Iter: 1189 loss: 2.82309929e-06
Iter: 1190 loss: 2.83473219e-06
Iter: 1191 loss: 2.82309247e-06
Iter: 1192 loss: 2.8219913e-06
Iter: 1193 loss: 2.82067163e-06
Iter: 1194 loss: 2.82057499e-06
Iter: 1195 loss: 2.81886582e-06
Iter: 1196 loss: 2.83428881e-06
Iter: 1197 loss: 2.81877828e-06
Iter: 1198 loss: 2.81764915e-06
Iter: 1199 loss: 2.82544852e-06
Iter: 1200 loss: 2.8175125e-06
Iter: 1201 loss: 2.81666416e-06
Iter: 1202 loss: 2.81466714e-06
Iter: 1203 loss: 2.84256748e-06
Iter: 1204 loss: 2.81455755e-06
Iter: 1205 loss: 2.81262e-06
Iter: 1206 loss: 2.82794144e-06
Iter: 1207 loss: 2.81241955e-06
Iter: 1208 loss: 2.81104349e-06
Iter: 1209 loss: 2.81069742e-06
Iter: 1210 loss: 2.80971926e-06
Iter: 1211 loss: 2.80818358e-06
Iter: 1212 loss: 2.8204081e-06
Iter: 1213 loss: 2.80813742e-06
Iter: 1214 loss: 2.80650534e-06
Iter: 1215 loss: 2.81301868e-06
Iter: 1216 loss: 2.80615495e-06
Iter: 1217 loss: 2.80496056e-06
Iter: 1218 loss: 2.80753e-06
Iter: 1219 loss: 2.80448808e-06
Iter: 1220 loss: 2.80352697e-06
Iter: 1221 loss: 2.81346752e-06
Iter: 1222 loss: 2.80353106e-06
Iter: 1223 loss: 2.80246149e-06
Iter: 1224 loss: 2.80123436e-06
Iter: 1225 loss: 2.8011325e-06
Iter: 1226 loss: 2.79988853e-06
Iter: 1227 loss: 2.80690438e-06
Iter: 1228 loss: 2.79975529e-06
Iter: 1229 loss: 2.79844153e-06
Iter: 1230 loss: 2.80047493e-06
Iter: 1231 loss: 2.79772826e-06
Iter: 1232 loss: 2.79677397e-06
Iter: 1233 loss: 2.79851429e-06
Iter: 1234 loss: 2.79628512e-06
Iter: 1235 loss: 2.79500978e-06
Iter: 1236 loss: 2.80150107e-06
Iter: 1237 loss: 2.79466462e-06
Iter: 1238 loss: 2.79360574e-06
Iter: 1239 loss: 2.79297865e-06
Iter: 1240 loss: 2.79253868e-06
Iter: 1241 loss: 2.79098549e-06
Iter: 1242 loss: 2.79329061e-06
Iter: 1243 loss: 2.79026381e-06
Iter: 1244 loss: 2.78869766e-06
Iter: 1245 loss: 2.79510687e-06
Iter: 1246 loss: 2.78823472e-06
Iter: 1247 loss: 2.78681455e-06
Iter: 1248 loss: 2.78618427e-06
Iter: 1249 loss: 2.78544985e-06
Iter: 1250 loss: 2.78420976e-06
Iter: 1251 loss: 2.78405969e-06
Iter: 1252 loss: 2.78284597e-06
Iter: 1253 loss: 2.78240896e-06
Iter: 1254 loss: 2.78172706e-06
Iter: 1255 loss: 2.7804424e-06
Iter: 1256 loss: 2.79493224e-06
Iter: 1257 loss: 2.78039852e-06
Iter: 1258 loss: 2.77913432e-06
Iter: 1259 loss: 2.78099787e-06
Iter: 1260 loss: 2.7785461e-06
Iter: 1261 loss: 2.77758863e-06
Iter: 1262 loss: 2.77671393e-06
Iter: 1263 loss: 2.77640561e-06
Iter: 1264 loss: 2.77529853e-06
Iter: 1265 loss: 2.77533263e-06
Iter: 1266 loss: 2.77436152e-06
Iter: 1267 loss: 2.77356685e-06
Iter: 1268 loss: 2.77336926e-06
Iter: 1269 loss: 2.7724584e-06
Iter: 1270 loss: 2.77246704e-06
Iter: 1271 loss: 2.77186928e-06
Iter: 1272 loss: 2.77039499e-06
Iter: 1273 loss: 2.78687207e-06
Iter: 1274 loss: 2.77019353e-06
Iter: 1275 loss: 2.76860419e-06
Iter: 1276 loss: 2.77767867e-06
Iter: 1277 loss: 2.76836022e-06
Iter: 1278 loss: 2.76696824e-06
Iter: 1279 loss: 2.76741594e-06
Iter: 1280 loss: 2.76595438e-06
Iter: 1281 loss: 2.76425112e-06
Iter: 1282 loss: 2.77485333e-06
Iter: 1283 loss: 2.76406581e-06
Iter: 1284 loss: 2.76258743e-06
Iter: 1285 loss: 2.76231776e-06
Iter: 1286 loss: 2.76124592e-06
Iter: 1287 loss: 2.76032551e-06
Iter: 1288 loss: 2.76013884e-06
Iter: 1289 loss: 2.75904358e-06
Iter: 1290 loss: 2.75780735e-06
Iter: 1291 loss: 2.75767911e-06
Iter: 1292 loss: 2.75712841e-06
Iter: 1293 loss: 2.75685829e-06
Iter: 1294 loss: 2.75627599e-06
Iter: 1295 loss: 2.75543948e-06
Iter: 1296 loss: 2.75532716e-06
Iter: 1297 loss: 2.75425487e-06
Iter: 1298 loss: 2.75364664e-06
Iter: 1299 loss: 2.75313278e-06
Iter: 1300 loss: 2.75167122e-06
Iter: 1301 loss: 2.75901198e-06
Iter: 1302 loss: 2.75147454e-06
Iter: 1303 loss: 2.74998683e-06
Iter: 1304 loss: 2.7637891e-06
Iter: 1305 loss: 2.74999252e-06
Iter: 1306 loss: 2.74906779e-06
Iter: 1307 loss: 2.74834656e-06
Iter: 1308 loss: 2.7480387e-06
Iter: 1309 loss: 2.74670469e-06
Iter: 1310 loss: 2.75846583e-06
Iter: 1311 loss: 2.74669878e-06
Iter: 1312 loss: 2.74549666e-06
Iter: 1313 loss: 2.74464014e-06
Iter: 1314 loss: 2.74427589e-06
Iter: 1315 loss: 2.74282547e-06
Iter: 1316 loss: 2.74532385e-06
Iter: 1317 loss: 2.74210038e-06
Iter: 1318 loss: 2.74073568e-06
Iter: 1319 loss: 2.74727972e-06
Iter: 1320 loss: 2.7405265e-06
Iter: 1321 loss: 2.7390829e-06
Iter: 1322 loss: 2.74081685e-06
Iter: 1323 loss: 2.73833416e-06
Iter: 1324 loss: 2.73715523e-06
Iter: 1325 loss: 2.73717933e-06
Iter: 1326 loss: 2.73626119e-06
Iter: 1327 loss: 2.73642399e-06
Iter: 1328 loss: 2.73557362e-06
Iter: 1329 loss: 2.7343267e-06
Iter: 1330 loss: 2.74271133e-06
Iter: 1331 loss: 2.73414093e-06
Iter: 1332 loss: 2.73315709e-06
Iter: 1333 loss: 2.73195974e-06
Iter: 1334 loss: 2.73179899e-06
Iter: 1335 loss: 2.73008891e-06
Iter: 1336 loss: 2.72979946e-06
Iter: 1337 loss: 2.72871171e-06
Iter: 1338 loss: 2.72737861e-06
Iter: 1339 loss: 2.7273602e-06
Iter: 1340 loss: 2.7258618e-06
Iter: 1341 loss: 2.72691386e-06
Iter: 1342 loss: 2.72507464e-06
Iter: 1343 loss: 2.72419948e-06
Iter: 1344 loss: 2.72714942e-06
Iter: 1345 loss: 2.72401894e-06
Iter: 1346 loss: 2.72288662e-06
Iter: 1347 loss: 2.72275201e-06
Iter: 1348 loss: 2.72198e-06
Iter: 1349 loss: 2.72059765e-06
Iter: 1350 loss: 2.72425632e-06
Iter: 1351 loss: 2.72015041e-06
Iter: 1352 loss: 2.71904014e-06
Iter: 1353 loss: 2.71745694e-06
Iter: 1354 loss: 2.71739646e-06
Iter: 1355 loss: 2.71564613e-06
Iter: 1356 loss: 2.73897899e-06
Iter: 1357 loss: 2.71556e-06
Iter: 1358 loss: 2.71402905e-06
Iter: 1359 loss: 2.72070247e-06
Iter: 1360 loss: 2.71370936e-06
Iter: 1361 loss: 2.71233603e-06
Iter: 1362 loss: 2.72374723e-06
Iter: 1363 loss: 2.7122278e-06
Iter: 1364 loss: 2.71130284e-06
Iter: 1365 loss: 2.7129513e-06
Iter: 1366 loss: 2.71079398e-06
Iter: 1367 loss: 2.70953569e-06
Iter: 1368 loss: 2.70969235e-06
Iter: 1369 loss: 2.70848341e-06
Iter: 1370 loss: 2.70738929e-06
Iter: 1371 loss: 2.70728538e-06
Iter: 1372 loss: 2.70656119e-06
Iter: 1373 loss: 2.7049673e-06
Iter: 1374 loss: 2.71594081e-06
Iter: 1375 loss: 2.70483611e-06
Iter: 1376 loss: 2.70365649e-06
Iter: 1377 loss: 2.71532417e-06
Iter: 1378 loss: 2.70368105e-06
Iter: 1379 loss: 2.70287455e-06
Iter: 1380 loss: 2.70157784e-06
Iter: 1381 loss: 2.73169e-06
Iter: 1382 loss: 2.7015667e-06
Iter: 1383 loss: 2.70063083e-06
Iter: 1384 loss: 2.70057762e-06
Iter: 1385 loss: 2.69979773e-06
Iter: 1386 loss: 2.69869952e-06
Iter: 1387 loss: 2.69863472e-06
Iter: 1388 loss: 2.69703969e-06
Iter: 1389 loss: 2.69823477e-06
Iter: 1390 loss: 2.69604607e-06
Iter: 1391 loss: 2.69424891e-06
Iter: 1392 loss: 2.69777092e-06
Iter: 1393 loss: 2.6934656e-06
Iter: 1394 loss: 2.6923733e-06
Iter: 1395 loss: 2.69230918e-06
Iter: 1396 loss: 2.69123e-06
Iter: 1397 loss: 2.69442353e-06
Iter: 1398 loss: 2.69099291e-06
Iter: 1399 loss: 2.68999e-06
Iter: 1400 loss: 2.69361499e-06
Iter: 1401 loss: 2.68970348e-06
Iter: 1402 loss: 2.68888971e-06
Iter: 1403 loss: 2.69055249e-06
Iter: 1404 loss: 2.68854546e-06
Iter: 1405 loss: 2.68766871e-06
Iter: 1406 loss: 2.68592203e-06
Iter: 1407 loss: 2.72386751e-06
Iter: 1408 loss: 2.68592953e-06
Iter: 1409 loss: 2.68452732e-06
Iter: 1410 loss: 2.7039755e-06
Iter: 1411 loss: 2.6843968e-06
Iter: 1412 loss: 2.68348549e-06
Iter: 1413 loss: 2.69494512e-06
Iter: 1414 loss: 2.68344411e-06
Iter: 1415 loss: 2.68258964e-06
Iter: 1416 loss: 2.68101985e-06
Iter: 1417 loss: 2.71064232e-06
Iter: 1418 loss: 2.68094891e-06
Iter: 1419 loss: 2.67957262e-06
Iter: 1420 loss: 2.69371822e-06
Iter: 1421 loss: 2.67958512e-06
Iter: 1422 loss: 2.67832638e-06
Iter: 1423 loss: 2.68192503e-06
Iter: 1424 loss: 2.67791393e-06
Iter: 1425 loss: 2.67702922e-06
Iter: 1426 loss: 2.67586688e-06
Iter: 1427 loss: 2.67581709e-06
Iter: 1428 loss: 2.67447535e-06
Iter: 1429 loss: 2.68753047e-06
Iter: 1430 loss: 2.6743985e-06
Iter: 1431 loss: 2.67337441e-06
Iter: 1432 loss: 2.67233372e-06
Iter: 1433 loss: 2.6721541e-06
Iter: 1434 loss: 2.67200539e-06
Iter: 1435 loss: 2.67127712e-06
Iter: 1436 loss: 2.67062296e-06
Iter: 1437 loss: 2.66981783e-06
Iter: 1438 loss: 2.66978873e-06
Iter: 1439 loss: 2.66868915e-06
Iter: 1440 loss: 2.6763837e-06
Iter: 1441 loss: 2.66859524e-06
Iter: 1442 loss: 2.66779512e-06
Iter: 1443 loss: 2.66689221e-06
Iter: 1444 loss: 2.66683082e-06
Iter: 1445 loss: 2.66535403e-06
Iter: 1446 loss: 2.6726882e-06
Iter: 1447 loss: 2.66516417e-06
Iter: 1448 loss: 2.66393226e-06
Iter: 1449 loss: 2.67371479e-06
Iter: 1450 loss: 2.66380766e-06
Iter: 1451 loss: 2.66279676e-06
Iter: 1452 loss: 2.6614448e-06
Iter: 1453 loss: 2.66129064e-06
Iter: 1454 loss: 2.6600851e-06
Iter: 1455 loss: 2.67053247e-06
Iter: 1456 loss: 2.66004827e-06
Iter: 1457 loss: 2.65907397e-06
Iter: 1458 loss: 2.66373308e-06
Iter: 1459 loss: 2.65888639e-06
Iter: 1460 loss: 2.65781864e-06
Iter: 1461 loss: 2.65607559e-06
Iter: 1462 loss: 2.65605172e-06
Iter: 1463 loss: 2.6542757e-06
Iter: 1464 loss: 2.65768631e-06
Iter: 1465 loss: 2.65373069e-06
Iter: 1466 loss: 2.65187919e-06
Iter: 1467 loss: 2.66141046e-06
Iter: 1468 loss: 2.65168069e-06
Iter: 1469 loss: 2.65048425e-06
Iter: 1470 loss: 2.66198231e-06
Iter: 1471 loss: 2.65052222e-06
Iter: 1472 loss: 2.64918049e-06
Iter: 1473 loss: 2.65090898e-06
Iter: 1474 loss: 2.64852429e-06
Iter: 1475 loss: 2.64766049e-06
Iter: 1476 loss: 2.65087328e-06
Iter: 1477 loss: 2.64749e-06
Iter: 1478 loss: 2.64655864e-06
Iter: 1479 loss: 2.64698861e-06
Iter: 1480 loss: 2.64593746e-06
Iter: 1481 loss: 2.6449577e-06
Iter: 1482 loss: 2.64595656e-06
Iter: 1483 loss: 2.64438427e-06
Iter: 1484 loss: 2.6433222e-06
Iter: 1485 loss: 2.65566678e-06
Iter: 1486 loss: 2.64339042e-06
Iter: 1487 loss: 2.64223e-06
Iter: 1488 loss: 2.64092523e-06
Iter: 1489 loss: 2.64080677e-06
Iter: 1490 loss: 2.63964466e-06
Iter: 1491 loss: 2.64646e-06
Iter: 1492 loss: 2.63948823e-06
Iter: 1493 loss: 2.63822631e-06
Iter: 1494 loss: 2.64060145e-06
Iter: 1495 loss: 2.63774041e-06
Iter: 1496 loss: 2.63628635e-06
Iter: 1497 loss: 2.6402613e-06
Iter: 1498 loss: 2.63569291e-06
Iter: 1499 loss: 2.63456832e-06
Iter: 1500 loss: 2.63475522e-06
Iter: 1501 loss: 2.63378365e-06
Iter: 1502 loss: 2.63242669e-06
Iter: 1503 loss: 2.63285756e-06
Iter: 1504 loss: 2.63142647e-06
Iter: 1505 loss: 2.63107108e-06
Iter: 1506 loss: 2.63067477e-06
Iter: 1507 loss: 2.62990034e-06
Iter: 1508 loss: 2.63077072e-06
Iter: 1509 loss: 2.62951016e-06
Iter: 1510 loss: 2.62864842e-06
Iter: 1511 loss: 2.62773938e-06
Iter: 1512 loss: 2.62761591e-06
Iter: 1513 loss: 2.62635899e-06
Iter: 1514 loss: 2.64383493e-06
Iter: 1515 loss: 2.62632898e-06
Iter: 1516 loss: 2.62542835e-06
Iter: 1517 loss: 2.6235025e-06
Iter: 1518 loss: 2.65194785e-06
Iter: 1519 loss: 2.62344838e-06
Iter: 1520 loss: 2.62322601e-06
Iter: 1521 loss: 2.62241201e-06
Iter: 1522 loss: 2.62161666e-06
Iter: 1523 loss: 2.62004778e-06
Iter: 1524 loss: 2.64720802e-06
Iter: 1525 loss: 2.62001e-06
Iter: 1526 loss: 2.61852642e-06
Iter: 1527 loss: 2.62445815e-06
Iter: 1528 loss: 2.61826699e-06
Iter: 1529 loss: 2.61697824e-06
Iter: 1530 loss: 2.62887511e-06
Iter: 1531 loss: 2.61691525e-06
Iter: 1532 loss: 2.61593141e-06
Iter: 1533 loss: 2.6170992e-06
Iter: 1534 loss: 2.61543823e-06
Iter: 1535 loss: 2.61453397e-06
Iter: 1536 loss: 2.61432433e-06
Iter: 1537 loss: 2.61374134e-06
Iter: 1538 loss: 2.61246987e-06
Iter: 1539 loss: 2.62067829e-06
Iter: 1540 loss: 2.61225182e-06
Iter: 1541 loss: 2.61148534e-06
Iter: 1542 loss: 2.61148034e-06
Iter: 1543 loss: 2.61064906e-06
Iter: 1544 loss: 2.60891306e-06
Iter: 1545 loss: 2.63289894e-06
Iter: 1546 loss: 2.60882916e-06
Iter: 1547 loss: 2.60751199e-06
Iter: 1548 loss: 2.62363255e-06
Iter: 1549 loss: 2.60752449e-06
Iter: 1550 loss: 2.60652723e-06
Iter: 1551 loss: 2.61004311e-06
Iter: 1552 loss: 2.60623824e-06
Iter: 1553 loss: 2.60538854e-06
Iter: 1554 loss: 2.60460092e-06
Iter: 1555 loss: 2.60425168e-06
Iter: 1556 loss: 2.6032194e-06
Iter: 1557 loss: 2.60316938e-06
Iter: 1558 loss: 2.60248612e-06
Iter: 1559 loss: 2.60101615e-06
Iter: 1560 loss: 2.63265269e-06
Iter: 1561 loss: 2.60105026e-06
Iter: 1562 loss: 2.59969602e-06
Iter: 1563 loss: 2.6016578e-06
Iter: 1564 loss: 2.59910894e-06
Iter: 1565 loss: 2.59777903e-06
Iter: 1566 loss: 2.59774151e-06
Iter: 1567 loss: 2.59696071e-06
Iter: 1568 loss: 2.59735839e-06
Iter: 1569 loss: 2.59632e-06
Iter: 1570 loss: 2.59536341e-06
Iter: 1571 loss: 2.59433455e-06
Iter: 1572 loss: 2.59415219e-06
Iter: 1573 loss: 2.59337799e-06
Iter: 1574 loss: 2.59323951e-06
Iter: 1575 loss: 2.59233184e-06
Iter: 1576 loss: 2.5934969e-06
Iter: 1577 loss: 2.59184617e-06
Iter: 1578 loss: 2.59095827e-06
Iter: 1579 loss: 2.59089757e-06
Iter: 1580 loss: 2.59019134e-06
Iter: 1581 loss: 2.58892942e-06
Iter: 1582 loss: 2.59400895e-06
Iter: 1583 loss: 2.58866658e-06
Iter: 1584 loss: 2.58743376e-06
Iter: 1585 loss: 2.59086164e-06
Iter: 1586 loss: 2.58701675e-06
Iter: 1587 loss: 2.58597629e-06
Iter: 1588 loss: 2.58761838e-06
Iter: 1589 loss: 2.58538876e-06
Iter: 1590 loss: 2.5839031e-06
Iter: 1591 loss: 2.59259787e-06
Iter: 1592 loss: 2.58366049e-06
Iter: 1593 loss: 2.5828474e-06
Iter: 1594 loss: 2.58207365e-06
Iter: 1595 loss: 2.58186537e-06
Iter: 1596 loss: 2.58056548e-06
Iter: 1597 loss: 2.58431646e-06
Iter: 1598 loss: 2.58017917e-06
Iter: 1599 loss: 2.57936335e-06
Iter: 1600 loss: 2.59085914e-06
Iter: 1601 loss: 2.57932425e-06
Iter: 1602 loss: 2.57845636e-06
Iter: 1603 loss: 2.57695115e-06
Iter: 1604 loss: 2.5769059e-06
Iter: 1605 loss: 2.5754066e-06
Iter: 1606 loss: 2.58131e-06
Iter: 1607 loss: 2.57501597e-06
Iter: 1608 loss: 2.57414604e-06
Iter: 1609 loss: 2.57410443e-06
Iter: 1610 loss: 2.57325473e-06
Iter: 1611 loss: 2.57199849e-06
Iter: 1612 loss: 2.57196893e-06
Iter: 1613 loss: 2.57061856e-06
Iter: 1614 loss: 2.571918e-06
Iter: 1615 loss: 2.5698937e-06
Iter: 1616 loss: 2.56823546e-06
Iter: 1617 loss: 2.5854672e-06
Iter: 1618 loss: 2.56823387e-06
Iter: 1619 loss: 2.56713747e-06
Iter: 1620 loss: 2.567102e-06
Iter: 1621 loss: 2.56633825e-06
Iter: 1622 loss: 2.56529029e-06
Iter: 1623 loss: 2.56526482e-06
Iter: 1624 loss: 2.56458588e-06
Iter: 1625 loss: 2.5646641e-06
Iter: 1626 loss: 2.56409839e-06
Iter: 1627 loss: 2.56325757e-06
Iter: 1628 loss: 2.56310045e-06
Iter: 1629 loss: 2.56252497e-06
Iter: 1630 loss: 2.56143494e-06
Iter: 1631 loss: 2.56652766e-06
Iter: 1632 loss: 2.56124804e-06
Iter: 1633 loss: 2.56031626e-06
Iter: 1634 loss: 2.56875705e-06
Iter: 1635 loss: 2.56021167e-06
Iter: 1636 loss: 2.55941723e-06
Iter: 1637 loss: 2.55834857e-06
Iter: 1638 loss: 2.5582342e-06
Iter: 1639 loss: 2.55708846e-06
Iter: 1640 loss: 2.56241401e-06
Iter: 1641 loss: 2.55687223e-06
Iter: 1642 loss: 2.55548548e-06
Iter: 1643 loss: 2.5660654e-06
Iter: 1644 loss: 2.55539589e-06
Iter: 1645 loss: 2.55460054e-06
Iter: 1646 loss: 2.55285499e-06
Iter: 1647 loss: 2.57917804e-06
Iter: 1648 loss: 2.55281111e-06
Iter: 1649 loss: 2.55180566e-06
Iter: 1650 loss: 2.55176928e-06
Iter: 1651 loss: 2.55088617e-06
Iter: 1652 loss: 2.55103146e-06
Iter: 1653 loss: 2.55025e-06
Iter: 1654 loss: 2.54909764e-06
Iter: 1655 loss: 2.55309851e-06
Iter: 1656 loss: 2.5488348e-06
Iter: 1657 loss: 2.54761676e-06
Iter: 1658 loss: 2.55509121e-06
Iter: 1659 loss: 2.54750239e-06
Iter: 1660 loss: 2.54677479e-06
Iter: 1661 loss: 2.54556744e-06
Iter: 1662 loss: 2.54554584e-06
Iter: 1663 loss: 2.54426232e-06
Iter: 1664 loss: 2.5521108e-06
Iter: 1665 loss: 2.54405e-06
Iter: 1666 loss: 2.54283373e-06
Iter: 1667 loss: 2.5478555e-06
Iter: 1668 loss: 2.54254905e-06
Iter: 1669 loss: 2.54121824e-06
Iter: 1670 loss: 2.54602674e-06
Iter: 1671 loss: 2.54081806e-06
Iter: 1672 loss: 2.54004453e-06
Iter: 1673 loss: 2.53869757e-06
Iter: 1674 loss: 2.53866347e-06
Iter: 1675 loss: 2.53830467e-06
Iter: 1676 loss: 2.53779353e-06
Iter: 1677 loss: 2.53705366e-06
Iter: 1678 loss: 2.53642315e-06
Iter: 1679 loss: 2.5362956e-06
Iter: 1680 loss: 2.53518579e-06
Iter: 1681 loss: 2.53446706e-06
Iter: 1682 loss: 2.53409189e-06
Iter: 1683 loss: 2.53296253e-06
Iter: 1684 loss: 2.53299277e-06
Iter: 1685 loss: 2.53197186e-06
Iter: 1686 loss: 2.53267353e-06
Iter: 1687 loss: 2.53133703e-06
Iter: 1688 loss: 2.53060489e-06
Iter: 1689 loss: 2.53966414e-06
Iter: 1690 loss: 2.53060239e-06
Iter: 1691 loss: 2.52986774e-06
Iter: 1692 loss: 2.52885434e-06
Iter: 1693 loss: 2.5287718e-06
Iter: 1694 loss: 2.52762948e-06
Iter: 1695 loss: 2.52933205e-06
Iter: 1696 loss: 2.52717359e-06
Iter: 1697 loss: 2.52600285e-06
Iter: 1698 loss: 2.52809559e-06
Iter: 1699 loss: 2.52529867e-06
Iter: 1700 loss: 2.52427526e-06
Iter: 1701 loss: 2.52428367e-06
Iter: 1702 loss: 2.52337759e-06
Iter: 1703 loss: 2.52246377e-06
Iter: 1704 loss: 2.52227983e-06
Iter: 1705 loss: 2.52113614e-06
Iter: 1706 loss: 2.53067606e-06
Iter: 1707 loss: 2.52104837e-06
Iter: 1708 loss: 2.52004179e-06
Iter: 1709 loss: 2.5262807e-06
Iter: 1710 loss: 2.51999268e-06
Iter: 1711 loss: 2.51913661e-06
Iter: 1712 loss: 2.51794609e-06
Iter: 1713 loss: 2.51791334e-06
Iter: 1714 loss: 2.51650727e-06
Iter: 1715 loss: 2.51549454e-06
Iter: 1716 loss: 2.51503548e-06
Iter: 1717 loss: 2.51533311e-06
Iter: 1718 loss: 2.5143e-06
Iter: 1719 loss: 2.51376423e-06
Iter: 1720 loss: 2.51313713e-06
Iter: 1721 loss: 2.51310894e-06
Iter: 1722 loss: 2.51204574e-06
Iter: 1723 loss: 2.51807342e-06
Iter: 1724 loss: 2.51183701e-06
Iter: 1725 loss: 2.5109066e-06
Iter: 1726 loss: 2.51051915e-06
Iter: 1727 loss: 2.50998346e-06
Iter: 1728 loss: 2.50896846e-06
Iter: 1729 loss: 2.50899575e-06
Iter: 1730 loss: 2.50804487e-06
Iter: 1731 loss: 2.50679841e-06
Iter: 1732 loss: 2.51416122e-06
Iter: 1733 loss: 2.50651897e-06
Iter: 1734 loss: 2.50498624e-06
Iter: 1735 loss: 2.51250094e-06
Iter: 1736 loss: 2.50476319e-06
Iter: 1737 loss: 2.50366156e-06
Iter: 1738 loss: 2.50360085e-06
Iter: 1739 loss: 2.5027407e-06
Iter: 1740 loss: 2.50166613e-06
Iter: 1741 loss: 2.51894289e-06
Iter: 1742 loss: 2.50167113e-06
Iter: 1743 loss: 2.5005902e-06
Iter: 1744 loss: 2.50138442e-06
Iter: 1745 loss: 2.49996538e-06
Iter: 1746 loss: 2.49914842e-06
Iter: 1747 loss: 2.49940672e-06
Iter: 1748 loss: 2.49857135e-06
Iter: 1749 loss: 2.49735581e-06
Iter: 1750 loss: 2.49732739e-06
Iter: 1751 loss: 2.49634786e-06
Iter: 1752 loss: 2.49560867e-06
Iter: 1753 loss: 2.49542336e-06
Iter: 1754 loss: 2.49469417e-06
Iter: 1755 loss: 2.49427308e-06
Iter: 1756 loss: 2.493902e-06
Iter: 1757 loss: 2.49289087e-06
Iter: 1758 loss: 2.50145536e-06
Iter: 1759 loss: 2.49284585e-06
Iter: 1760 loss: 2.49191294e-06
Iter: 1761 loss: 2.49063123e-06
Iter: 1762 loss: 2.49057211e-06
Iter: 1763 loss: 2.4891724e-06
Iter: 1764 loss: 2.4932292e-06
Iter: 1765 loss: 2.48860488e-06
Iter: 1766 loss: 2.48729066e-06
Iter: 1767 loss: 2.4899573e-06
Iter: 1768 loss: 2.48669448e-06
Iter: 1769 loss: 2.48559149e-06
Iter: 1770 loss: 2.48560355e-06
Iter: 1771 loss: 2.48477e-06
Iter: 1772 loss: 2.48356673e-06
Iter: 1773 loss: 2.48356332e-06
Iter: 1774 loss: 2.48305241e-06
Iter: 1775 loss: 2.48276388e-06
Iter: 1776 loss: 2.48218976e-06
Iter: 1777 loss: 2.48167657e-06
Iter: 1778 loss: 2.48153674e-06
Iter: 1779 loss: 2.48064316e-06
Iter: 1780 loss: 2.47968819e-06
Iter: 1781 loss: 2.47958474e-06
Iter: 1782 loss: 2.47824528e-06
Iter: 1783 loss: 2.48730794e-06
Iter: 1784 loss: 2.47818366e-06
Iter: 1785 loss: 2.47698517e-06
Iter: 1786 loss: 2.48552169e-06
Iter: 1787 loss: 2.4769206e-06
Iter: 1788 loss: 2.4759247e-06
Iter: 1789 loss: 2.47828257e-06
Iter: 1790 loss: 2.47556318e-06
Iter: 1791 loss: 2.47477465e-06
Iter: 1792 loss: 2.47740627e-06
Iter: 1793 loss: 2.4745575e-06
Iter: 1794 loss: 2.47354956e-06
Iter: 1795 loss: 2.47332537e-06
Iter: 1796 loss: 2.47269372e-06
Iter: 1797 loss: 2.47141702e-06
Iter: 1798 loss: 2.46939226e-06
Iter: 1799 loss: 2.46927766e-06
Iter: 1800 loss: 2.46776381e-06
Iter: 1801 loss: 2.46774698e-06
Iter: 1802 loss: 2.46679247e-06
Iter: 1803 loss: 2.47390085e-06
Iter: 1804 loss: 2.46670379e-06
Iter: 1805 loss: 2.46568538e-06
Iter: 1806 loss: 2.46638979e-06
Iter: 1807 loss: 2.46505374e-06
Iter: 1808 loss: 2.46448462e-06
Iter: 1809 loss: 2.46449986e-06
Iter: 1810 loss: 2.46391483e-06
Iter: 1811 loss: 2.46252785e-06
Iter: 1812 loss: 2.48157221e-06
Iter: 1813 loss: 2.46240097e-06
Iter: 1814 loss: 2.46106083e-06
Iter: 1815 loss: 2.46166519e-06
Iter: 1816 loss: 2.46012974e-06
Iter: 1817 loss: 2.45828e-06
Iter: 1818 loss: 2.46479522e-06
Iter: 1819 loss: 2.45775482e-06
Iter: 1820 loss: 2.4564938e-06
Iter: 1821 loss: 2.46097443e-06
Iter: 1822 loss: 2.45611477e-06
Iter: 1823 loss: 2.45559659e-06
Iter: 1824 loss: 2.4553874e-06
Iter: 1825 loss: 2.45476349e-06
Iter: 1826 loss: 2.45371166e-06
Iter: 1827 loss: 2.45373053e-06
Iter: 1828 loss: 2.45254637e-06
Iter: 1829 loss: 2.45744468e-06
Iter: 1830 loss: 2.45226875e-06
Iter: 1831 loss: 2.45136289e-06
Iter: 1832 loss: 2.45806e-06
Iter: 1833 loss: 2.45121873e-06
Iter: 1834 loss: 2.45046704e-06
Iter: 1835 loss: 2.44934836e-06
Iter: 1836 loss: 2.44927355e-06
Iter: 1837 loss: 2.44799094e-06
Iter: 1838 loss: 2.45226124e-06
Iter: 1839 loss: 2.44764078e-06
Iter: 1840 loss: 2.44663852e-06
Iter: 1841 loss: 2.44665148e-06
Iter: 1842 loss: 2.44579e-06
Iter: 1843 loss: 2.44581793e-06
Iter: 1844 loss: 2.44520425e-06
Iter: 1845 loss: 2.4439355e-06
Iter: 1846 loss: 2.44895136e-06
Iter: 1847 loss: 2.44370631e-06
Iter: 1848 loss: 2.44267176e-06
Iter: 1849 loss: 2.44210651e-06
Iter: 1850 loss: 2.44166449e-06
Iter: 1851 loss: 2.44056423e-06
Iter: 1852 loss: 2.44052944e-06
Iter: 1853 loss: 2.4396868e-06
Iter: 1854 loss: 2.43816794e-06
Iter: 1855 loss: 2.44945022e-06
Iter: 1856 loss: 2.43799423e-06
Iter: 1857 loss: 2.43692193e-06
Iter: 1858 loss: 2.44239504e-06
Iter: 1859 loss: 2.43678687e-06
Iter: 1860 loss: 2.43638897e-06
Iter: 1861 loss: 2.43636259e-06
Iter: 1862 loss: 2.43583872e-06
Iter: 1863 loss: 2.43452951e-06
Iter: 1864 loss: 2.44304078e-06
Iter: 1865 loss: 2.43424961e-06
Iter: 1866 loss: 2.4328674e-06
Iter: 1867 loss: 2.44230932e-06
Iter: 1868 loss: 2.43277827e-06
Iter: 1869 loss: 2.43167051e-06
Iter: 1870 loss: 2.43872296e-06
Iter: 1871 loss: 2.43157569e-06
Iter: 1872 loss: 2.43042405e-06
Iter: 1873 loss: 2.43042541e-06
Iter: 1874 loss: 2.42955912e-06
Iter: 1875 loss: 2.42843953e-06
Iter: 1876 loss: 2.42946885e-06
Iter: 1877 loss: 2.42775104e-06
Iter: 1878 loss: 2.42686656e-06
Iter: 1879 loss: 2.42678311e-06
Iter: 1880 loss: 2.42613919e-06
Iter: 1881 loss: 2.42580586e-06
Iter: 1882 loss: 2.42547e-06
Iter: 1883 loss: 2.4247e-06
Iter: 1884 loss: 2.43048271e-06
Iter: 1885 loss: 2.42465603e-06
Iter: 1886 loss: 2.42386272e-06
Iter: 1887 loss: 2.42277315e-06
Iter: 1888 loss: 2.42271e-06
Iter: 1889 loss: 2.42163742e-06
Iter: 1890 loss: 2.42359e-06
Iter: 1891 loss: 2.42110241e-06
Iter: 1892 loss: 2.42007e-06
Iter: 1893 loss: 2.42908891e-06
Iter: 1894 loss: 2.41995144e-06
Iter: 1895 loss: 2.41891439e-06
Iter: 1896 loss: 2.42432179e-06
Iter: 1897 loss: 2.41872885e-06
Iter: 1898 loss: 2.41782254e-06
Iter: 1899 loss: 2.4189153e-06
Iter: 1900 loss: 2.41732823e-06
Iter: 1901 loss: 2.41654357e-06
Iter: 1902 loss: 2.41581711e-06
Iter: 1903 loss: 2.41559474e-06
Iter: 1904 loss: 2.41447333e-06
Iter: 1905 loss: 2.43146542e-06
Iter: 1906 loss: 2.41446924e-06
Iter: 1907 loss: 2.41355588e-06
Iter: 1908 loss: 2.41378939e-06
Iter: 1909 loss: 2.41298017e-06
Iter: 1910 loss: 2.41195335e-06
Iter: 1911 loss: 2.41370753e-06
Iter: 1912 loss: 2.41154521e-06
Iter: 1913 loss: 2.4106771e-06
Iter: 1914 loss: 2.41069392e-06
Iter: 1915 loss: 2.41009593e-06
Iter: 1916 loss: 2.40908548e-06
Iter: 1917 loss: 2.40905638e-06
Iter: 1918 loss: 2.408309e-06
Iter: 1919 loss: 2.40828058e-06
Iter: 1920 loss: 2.40767804e-06
Iter: 1921 loss: 2.4061917e-06
Iter: 1922 loss: 2.42387478e-06
Iter: 1923 loss: 2.40604072e-06
Iter: 1924 loss: 2.40463214e-06
Iter: 1925 loss: 2.41176576e-06
Iter: 1926 loss: 2.40433565e-06
Iter: 1927 loss: 2.40314193e-06
Iter: 1928 loss: 2.40981672e-06
Iter: 1929 loss: 2.40291683e-06
Iter: 1930 loss: 2.40183772e-06
Iter: 1931 loss: 2.41546263e-06
Iter: 1932 loss: 2.40186387e-06
Iter: 1933 loss: 2.40133272e-06
Iter: 1934 loss: 2.39988731e-06
Iter: 1935 loss: 2.41476164e-06
Iter: 1936 loss: 2.39976703e-06
Iter: 1937 loss: 2.39832389e-06
Iter: 1938 loss: 2.41277166e-06
Iter: 1939 loss: 2.39832821e-06
Iter: 1940 loss: 2.39734027e-06
Iter: 1941 loss: 2.40080249e-06
Iter: 1942 loss: 2.39703741e-06
Iter: 1943 loss: 2.39593e-06
Iter: 1944 loss: 2.39937776e-06
Iter: 1945 loss: 2.39563178e-06
Iter: 1946 loss: 2.39489145e-06
Iter: 1947 loss: 2.39597057e-06
Iter: 1948 loss: 2.39448332e-06
Iter: 1949 loss: 2.39366773e-06
Iter: 1950 loss: 2.40495865e-06
Iter: 1951 loss: 2.39366409e-06
Iter: 1952 loss: 2.39317637e-06
Iter: 1953 loss: 2.39229166e-06
Iter: 1954 loss: 2.39228166e-06
Iter: 1955 loss: 2.39112569e-06
Iter: 1956 loss: 2.39837186e-06
Iter: 1957 loss: 2.39112774e-06
Iter: 1958 loss: 2.39004112e-06
Iter: 1959 loss: 2.39067094e-06
Iter: 1960 loss: 2.38938924e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.6
+ date
Sun Nov  8 15:57:40 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 1.6 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82dfbc048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82df0e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82df0ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82df5de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82df5df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82df5dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82de2fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82dec79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82ded3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82dec7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82ddb8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82ddad7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82ddada60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82dd907b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82dd90400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8088c19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8088c1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8087f4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8087bb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8088c1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff808887510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e4024950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e4052048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff808855510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff808855ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e405a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d0634598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d0624598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d06241e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d05d4bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d06879d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d05a0620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d05a0a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d058e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d058e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d055d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 202, in <module>
    grads = tape.gradient(loss, model.trainable_weights)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1266, in _backward_function_wrapper
    processed_args, remapped_captures)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input is not invertible.
	 [[node gradients/MatrixDeterminant_grad/MatrixInverse (defined at biholoNN_train.py:200) ]] [Op:__inference___backward_volume_form_4446_8945]

Function call stack:
__backward_volume_form_4446

+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.6/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.6 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.6/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f143e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1533e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1533d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f147c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f147cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f13f22f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f13c6048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f13747b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f138ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1331a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f13008c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f12edf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1304840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1304bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1292b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f124e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f127e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1304b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1201950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f12012f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1204f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbfacbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbf72840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbf87840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbf80620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbf80b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbef1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbf0b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbf0b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbed6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbe947b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbe9a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbe9a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbe509d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbdfa840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbe9a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.28158e-05
Iter: 2 loss: 4.72897555e-05
Iter: 3 loss: 2.83565278e-05
Iter: 4 loss: 2.66955722e-05
Iter: 5 loss: 2.81453285e-05
Iter: 6 loss: 2.57218344e-05
Iter: 7 loss: 2.39228757e-05
Iter: 8 loss: 2.78017942e-05
Iter: 9 loss: 2.32332131e-05
Iter: 10 loss: 2.13844469e-05
Iter: 11 loss: 2.17482047e-05
Iter: 12 loss: 2.0012867e-05
Iter: 13 loss: 1.83110205e-05
Iter: 14 loss: 2.67904361e-05
Iter: 15 loss: 1.80241368e-05
Iter: 16 loss: 1.66652135e-05
Iter: 17 loss: 2.03813561e-05
Iter: 18 loss: 1.62242195e-05
Iter: 19 loss: 1.55788312e-05
Iter: 20 loss: 1.63761069e-05
Iter: 21 loss: 1.52416824e-05
Iter: 22 loss: 1.43867819e-05
Iter: 23 loss: 1.79276885e-05
Iter: 24 loss: 1.42015006e-05
Iter: 25 loss: 1.35409455e-05
Iter: 26 loss: 1.5916512e-05
Iter: 27 loss: 1.33736221e-05
Iter: 28 loss: 1.2821255e-05
Iter: 29 loss: 1.22987649e-05
Iter: 30 loss: 1.21709e-05
Iter: 31 loss: 1.16622432e-05
Iter: 32 loss: 1.16608044e-05
Iter: 33 loss: 1.13310944e-05
Iter: 34 loss: 1.10008959e-05
Iter: 35 loss: 1.09338944e-05
Iter: 36 loss: 1.05545769e-05
Iter: 37 loss: 1.05535373e-05
Iter: 38 loss: 1.02278245e-05
Iter: 39 loss: 1.41049086e-05
Iter: 40 loss: 1.0223559e-05
Iter: 41 loss: 1.00702819e-05
Iter: 42 loss: 1.00836314e-05
Iter: 43 loss: 9.95158916e-06
Iter: 44 loss: 9.72831549e-06
Iter: 45 loss: 1.01671048e-05
Iter: 46 loss: 9.63634375e-06
Iter: 47 loss: 9.40754126e-06
Iter: 48 loss: 9.35929256e-06
Iter: 49 loss: 9.2092414e-06
Iter: 50 loss: 8.97662176e-06
Iter: 51 loss: 1.26480172e-05
Iter: 52 loss: 8.97663e-06
Iter: 53 loss: 8.84997644e-06
Iter: 54 loss: 8.66233313e-06
Iter: 55 loss: 8.65781e-06
Iter: 56 loss: 8.37048174e-06
Iter: 57 loss: 9.83338941e-06
Iter: 58 loss: 8.32358819e-06
Iter: 59 loss: 8.13877796e-06
Iter: 60 loss: 9.80653567e-06
Iter: 61 loss: 8.1301514e-06
Iter: 62 loss: 8.01506485e-06
Iter: 63 loss: 7.9947522e-06
Iter: 64 loss: 7.91647471e-06
Iter: 65 loss: 7.76211527e-06
Iter: 66 loss: 7.83637643e-06
Iter: 67 loss: 7.6581664e-06
Iter: 68 loss: 7.50704839e-06
Iter: 69 loss: 7.50693061e-06
Iter: 70 loss: 7.46155e-06
Iter: 71 loss: 7.46035903e-06
Iter: 72 loss: 7.40662745e-06
Iter: 73 loss: 7.27648239e-06
Iter: 74 loss: 8.65179663e-06
Iter: 75 loss: 7.26192229e-06
Iter: 76 loss: 7.16789418e-06
Iter: 77 loss: 7.1652712e-06
Iter: 78 loss: 7.10489167e-06
Iter: 79 loss: 7.06930177e-06
Iter: 80 loss: 7.0441738e-06
Iter: 81 loss: 6.95108838e-06
Iter: 82 loss: 7.12132305e-06
Iter: 83 loss: 6.91089735e-06
Iter: 84 loss: 6.82676364e-06
Iter: 85 loss: 7.52418873e-06
Iter: 86 loss: 6.82169502e-06
Iter: 87 loss: 6.75838783e-06
Iter: 88 loss: 6.66714914e-06
Iter: 89 loss: 6.66428105e-06
Iter: 90 loss: 6.58393492e-06
Iter: 91 loss: 6.58366025e-06
Iter: 92 loss: 6.51522805e-06
Iter: 93 loss: 6.51564e-06
Iter: 94 loss: 6.46076342e-06
Iter: 95 loss: 6.38276e-06
Iter: 96 loss: 6.82219434e-06
Iter: 97 loss: 6.37182438e-06
Iter: 98 loss: 6.32305273e-06
Iter: 99 loss: 6.25959592e-06
Iter: 100 loss: 6.25543862e-06
Iter: 101 loss: 6.20690753e-06
Iter: 102 loss: 6.19887305e-06
Iter: 103 loss: 6.16389207e-06
Iter: 104 loss: 6.58813633e-06
Iter: 105 loss: 6.16366106e-06
Iter: 106 loss: 6.14069495e-06
Iter: 107 loss: 6.09727658e-06
Iter: 108 loss: 7.03651813e-06
Iter: 109 loss: 6.09696963e-06
Iter: 110 loss: 6.04266734e-06
Iter: 111 loss: 6.47121306e-06
Iter: 112 loss: 6.03887065e-06
Iter: 113 loss: 6.00184194e-06
Iter: 114 loss: 5.98094e-06
Iter: 115 loss: 5.96488e-06
Iter: 116 loss: 5.91479829e-06
Iter: 117 loss: 6.12151598e-06
Iter: 118 loss: 5.90410309e-06
Iter: 119 loss: 5.86606529e-06
Iter: 120 loss: 6.18540435e-06
Iter: 121 loss: 5.86394071e-06
Iter: 122 loss: 5.83786186e-06
Iter: 123 loss: 5.78432537e-06
Iter: 124 loss: 6.72510305e-06
Iter: 125 loss: 5.78322033e-06
Iter: 126 loss: 5.7350162e-06
Iter: 127 loss: 5.73444322e-06
Iter: 128 loss: 5.7010634e-06
Iter: 129 loss: 5.73182251e-06
Iter: 130 loss: 5.681738e-06
Iter: 131 loss: 5.64063612e-06
Iter: 132 loss: 5.69610347e-06
Iter: 133 loss: 5.62023251e-06
Iter: 134 loss: 5.57988687e-06
Iter: 135 loss: 5.59323416e-06
Iter: 136 loss: 5.55127826e-06
Iter: 137 loss: 5.55758243e-06
Iter: 138 loss: 5.53021619e-06
Iter: 139 loss: 5.51423727e-06
Iter: 140 loss: 5.51101402e-06
Iter: 141 loss: 5.50024561e-06
Iter: 142 loss: 5.47787204e-06
Iter: 143 loss: 5.4758857e-06
Iter: 144 loss: 5.45915373e-06
Iter: 145 loss: 5.41863346e-06
Iter: 146 loss: 5.55329825e-06
Iter: 147 loss: 5.40757901e-06
Iter: 148 loss: 5.38243785e-06
Iter: 149 loss: 5.38189624e-06
Iter: 150 loss: 5.36206699e-06
Iter: 151 loss: 5.32820377e-06
Iter: 152 loss: 5.54013423e-06
Iter: 153 loss: 5.32414833e-06
Iter: 154 loss: 5.29795761e-06
Iter: 155 loss: 5.37947562e-06
Iter: 156 loss: 5.29027966e-06
Iter: 157 loss: 5.26447047e-06
Iter: 158 loss: 5.2529731e-06
Iter: 159 loss: 5.23973449e-06
Iter: 160 loss: 5.21428865e-06
Iter: 161 loss: 5.58979264e-06
Iter: 162 loss: 5.21427501e-06
Iter: 163 loss: 5.19049536e-06
Iter: 164 loss: 5.18321622e-06
Iter: 165 loss: 5.16900218e-06
Iter: 166 loss: 5.14589e-06
Iter: 167 loss: 5.24942971e-06
Iter: 168 loss: 5.14132716e-06
Iter: 169 loss: 5.11948292e-06
Iter: 170 loss: 5.16498494e-06
Iter: 171 loss: 5.11077087e-06
Iter: 172 loss: 5.08907397e-06
Iter: 173 loss: 5.08899211e-06
Iter: 174 loss: 5.07753793e-06
Iter: 175 loss: 5.0622557e-06
Iter: 176 loss: 5.06137803e-06
Iter: 177 loss: 5.04394029e-06
Iter: 178 loss: 5.22484061e-06
Iter: 179 loss: 5.04353193e-06
Iter: 180 loss: 5.03071624e-06
Iter: 181 loss: 5.02498551e-06
Iter: 182 loss: 5.0186527e-06
Iter: 183 loss: 4.99859652e-06
Iter: 184 loss: 4.99595262e-06
Iter: 185 loss: 4.98172676e-06
Iter: 186 loss: 4.96350913e-06
Iter: 187 loss: 4.96323173e-06
Iter: 188 loss: 4.95223912e-06
Iter: 189 loss: 4.95440236e-06
Iter: 190 loss: 4.9442242e-06
Iter: 191 loss: 4.92619074e-06
Iter: 192 loss: 4.919807e-06
Iter: 193 loss: 4.90983e-06
Iter: 194 loss: 4.89771173e-06
Iter: 195 loss: 4.89688728e-06
Iter: 196 loss: 4.88538171e-06
Iter: 197 loss: 4.86131921e-06
Iter: 198 loss: 5.27341854e-06
Iter: 199 loss: 4.86081262e-06
Iter: 200 loss: 4.83898657e-06
Iter: 201 loss: 5.03960655e-06
Iter: 202 loss: 4.83797885e-06
Iter: 203 loss: 4.82397581e-06
Iter: 204 loss: 5.02723651e-06
Iter: 205 loss: 4.82383848e-06
Iter: 206 loss: 4.80898689e-06
Iter: 207 loss: 4.80202743e-06
Iter: 208 loss: 4.79446589e-06
Iter: 209 loss: 4.78347647e-06
Iter: 210 loss: 4.84393695e-06
Iter: 211 loss: 4.78176571e-06
Iter: 212 loss: 4.76851937e-06
Iter: 213 loss: 4.76533205e-06
Iter: 214 loss: 4.75698698e-06
Iter: 215 loss: 4.74092e-06
Iter: 216 loss: 4.78767151e-06
Iter: 217 loss: 4.73593354e-06
Iter: 218 loss: 4.72209e-06
Iter: 219 loss: 4.73894943e-06
Iter: 220 loss: 4.71487556e-06
Iter: 221 loss: 4.69996212e-06
Iter: 222 loss: 4.84777593e-06
Iter: 223 loss: 4.69938641e-06
Iter: 224 loss: 4.69035422e-06
Iter: 225 loss: 4.67809241e-06
Iter: 226 loss: 4.67746941e-06
Iter: 227 loss: 4.65611538e-06
Iter: 228 loss: 4.73071259e-06
Iter: 229 loss: 4.65053608e-06
Iter: 230 loss: 4.63811648e-06
Iter: 231 loss: 4.82590531e-06
Iter: 232 loss: 4.6380992e-06
Iter: 233 loss: 4.62922299e-06
Iter: 234 loss: 4.61371837e-06
Iter: 235 loss: 4.61380068e-06
Iter: 236 loss: 4.60119054e-06
Iter: 237 loss: 4.60114461e-06
Iter: 238 loss: 4.59136163e-06
Iter: 239 loss: 4.69015367e-06
Iter: 240 loss: 4.59104103e-06
Iter: 241 loss: 4.58542354e-06
Iter: 242 loss: 4.57474289e-06
Iter: 243 loss: 4.80796643e-06
Iter: 244 loss: 4.57454644e-06
Iter: 245 loss: 4.56602083e-06
Iter: 246 loss: 4.56588623e-06
Iter: 247 loss: 4.55902864e-06
Iter: 248 loss: 4.54636938e-06
Iter: 249 loss: 4.84124803e-06
Iter: 250 loss: 4.54638803e-06
Iter: 251 loss: 4.53414805e-06
Iter: 252 loss: 4.6341429e-06
Iter: 253 loss: 4.53345683e-06
Iter: 254 loss: 4.524818e-06
Iter: 255 loss: 4.53837083e-06
Iter: 256 loss: 4.52087033e-06
Iter: 257 loss: 4.50872449e-06
Iter: 258 loss: 4.53928533e-06
Iter: 259 loss: 4.50443e-06
Iter: 260 loss: 4.4966323e-06
Iter: 261 loss: 4.5062161e-06
Iter: 262 loss: 4.49255094e-06
Iter: 263 loss: 4.4807357e-06
Iter: 264 loss: 4.49732761e-06
Iter: 265 loss: 4.47481125e-06
Iter: 266 loss: 4.46279273e-06
Iter: 267 loss: 4.56991074e-06
Iter: 268 loss: 4.46212107e-06
Iter: 269 loss: 4.45481055e-06
Iter: 270 loss: 4.45151363e-06
Iter: 271 loss: 4.4477e-06
Iter: 272 loss: 4.44452871e-06
Iter: 273 loss: 4.44243278e-06
Iter: 274 loss: 4.43715362e-06
Iter: 275 loss: 4.42322153e-06
Iter: 276 loss: 4.53578696e-06
Iter: 277 loss: 4.42066585e-06
Iter: 278 loss: 4.4090184e-06
Iter: 279 loss: 4.58433533e-06
Iter: 280 loss: 4.4090034e-06
Iter: 281 loss: 4.40127542e-06
Iter: 282 loss: 4.44166e-06
Iter: 283 loss: 4.40010399e-06
Iter: 284 loss: 4.39463474e-06
Iter: 285 loss: 4.3838163e-06
Iter: 286 loss: 4.5955112e-06
Iter: 287 loss: 4.38364259e-06
Iter: 288 loss: 4.37150493e-06
Iter: 289 loss: 4.46646663e-06
Iter: 290 loss: 4.37062818e-06
Iter: 291 loss: 4.36007304e-06
Iter: 292 loss: 4.39172891e-06
Iter: 293 loss: 4.35678066e-06
Iter: 294 loss: 4.34750746e-06
Iter: 295 loss: 4.3778532e-06
Iter: 296 loss: 4.34486401e-06
Iter: 297 loss: 4.33671812e-06
Iter: 298 loss: 4.33761033e-06
Iter: 299 loss: 4.33045e-06
Iter: 300 loss: 4.32194884e-06
Iter: 301 loss: 4.44918442e-06
Iter: 302 loss: 4.32191155e-06
Iter: 303 loss: 4.3163468e-06
Iter: 304 loss: 4.31928947e-06
Iter: 305 loss: 4.31276931e-06
Iter: 306 loss: 4.30479304e-06
Iter: 307 loss: 4.31970693e-06
Iter: 308 loss: 4.30160935e-06
Iter: 309 loss: 4.29289958e-06
Iter: 310 loss: 4.41602788e-06
Iter: 311 loss: 4.29288866e-06
Iter: 312 loss: 4.2895449e-06
Iter: 313 loss: 4.28132216e-06
Iter: 314 loss: 4.37448944e-06
Iter: 315 loss: 4.28042495e-06
Iter: 316 loss: 4.27239229e-06
Iter: 317 loss: 4.27229952e-06
Iter: 318 loss: 4.26663428e-06
Iter: 319 loss: 4.26624638e-06
Iter: 320 loss: 4.2620668e-06
Iter: 321 loss: 4.25416874e-06
Iter: 322 loss: 4.2455622e-06
Iter: 323 loss: 4.2442166e-06
Iter: 324 loss: 4.23898655e-06
Iter: 325 loss: 4.2378324e-06
Iter: 326 loss: 4.23269603e-06
Iter: 327 loss: 4.22840913e-06
Iter: 328 loss: 4.2269603e-06
Iter: 329 loss: 4.21830828e-06
Iter: 330 loss: 4.25473945e-06
Iter: 331 loss: 4.21655568e-06
Iter: 332 loss: 4.20985816e-06
Iter: 333 loss: 4.21339564e-06
Iter: 334 loss: 4.20562947e-06
Iter: 335 loss: 4.19565731e-06
Iter: 336 loss: 4.26326687e-06
Iter: 337 loss: 4.19471326e-06
Iter: 338 loss: 4.18837226e-06
Iter: 339 loss: 4.19633034e-06
Iter: 340 loss: 4.18505851e-06
Iter: 341 loss: 4.18007676e-06
Iter: 342 loss: 4.17985711e-06
Iter: 343 loss: 4.17569618e-06
Iter: 344 loss: 4.17051388e-06
Iter: 345 loss: 4.17013598e-06
Iter: 346 loss: 4.16488911e-06
Iter: 347 loss: 4.17693218e-06
Iter: 348 loss: 4.16288367e-06
Iter: 349 loss: 4.15429349e-06
Iter: 350 loss: 4.16133798e-06
Iter: 351 loss: 4.14921806e-06
Iter: 352 loss: 4.14310034e-06
Iter: 353 loss: 4.16649436e-06
Iter: 354 loss: 4.14161241e-06
Iter: 355 loss: 4.1366493e-06
Iter: 356 loss: 4.13335511e-06
Iter: 357 loss: 4.13143334e-06
Iter: 358 loss: 4.12214195e-06
Iter: 359 loss: 4.19822209e-06
Iter: 360 loss: 4.12164945e-06
Iter: 361 loss: 4.11618203e-06
Iter: 362 loss: 4.11964493e-06
Iter: 363 loss: 4.11262045e-06
Iter: 364 loss: 4.1057674e-06
Iter: 365 loss: 4.1252697e-06
Iter: 366 loss: 4.1035496e-06
Iter: 367 loss: 4.09765471e-06
Iter: 368 loss: 4.12709505e-06
Iter: 369 loss: 4.09668974e-06
Iter: 370 loss: 4.09062113e-06
Iter: 371 loss: 4.10194434e-06
Iter: 372 loss: 4.08793494e-06
Iter: 373 loss: 4.08486358e-06
Iter: 374 loss: 4.08477717e-06
Iter: 375 loss: 4.08151391e-06
Iter: 376 loss: 4.07986863e-06
Iter: 377 loss: 4.07847892e-06
Iter: 378 loss: 4.07370862e-06
Iter: 379 loss: 4.07205744e-06
Iter: 380 loss: 4.06948129e-06
Iter: 381 loss: 4.06569097e-06
Iter: 382 loss: 4.06545041e-06
Iter: 383 loss: 4.06236e-06
Iter: 384 loss: 4.05579067e-06
Iter: 385 loss: 4.15812383e-06
Iter: 386 loss: 4.05557785e-06
Iter: 387 loss: 4.04843058e-06
Iter: 388 loss: 4.07686457e-06
Iter: 389 loss: 4.04691036e-06
Iter: 390 loss: 4.04029743e-06
Iter: 391 loss: 4.07617381e-06
Iter: 392 loss: 4.03951253e-06
Iter: 393 loss: 4.03385047e-06
Iter: 394 loss: 4.05526771e-06
Iter: 395 loss: 4.03254489e-06
Iter: 396 loss: 4.0275072e-06
Iter: 397 loss: 4.02313071e-06
Iter: 398 loss: 4.02169826e-06
Iter: 399 loss: 4.0160312e-06
Iter: 400 loss: 4.10060875e-06
Iter: 401 loss: 4.01602301e-06
Iter: 402 loss: 4.01193165e-06
Iter: 403 loss: 4.0206769e-06
Iter: 404 loss: 4.01046464e-06
Iter: 405 loss: 4.00558292e-06
Iter: 406 loss: 4.01633179e-06
Iter: 407 loss: 4.00367117e-06
Iter: 408 loss: 4.00008867e-06
Iter: 409 loss: 4.00007411e-06
Iter: 410 loss: 3.99784813e-06
Iter: 411 loss: 3.99217424e-06
Iter: 412 loss: 4.04628418e-06
Iter: 413 loss: 3.99152214e-06
Iter: 414 loss: 3.98569227e-06
Iter: 415 loss: 4.04385764e-06
Iter: 416 loss: 3.98552629e-06
Iter: 417 loss: 3.98035172e-06
Iter: 418 loss: 3.99515739e-06
Iter: 419 loss: 3.97888925e-06
Iter: 420 loss: 3.97449821e-06
Iter: 421 loss: 3.96902e-06
Iter: 422 loss: 3.96856e-06
Iter: 423 loss: 3.96392e-06
Iter: 424 loss: 4.01201396e-06
Iter: 425 loss: 3.96383803e-06
Iter: 426 loss: 3.95941242e-06
Iter: 427 loss: 3.96956329e-06
Iter: 428 loss: 3.95761253e-06
Iter: 429 loss: 3.9533e-06
Iter: 430 loss: 3.96536325e-06
Iter: 431 loss: 3.95185907e-06
Iter: 432 loss: 3.94725e-06
Iter: 433 loss: 3.94330664e-06
Iter: 434 loss: 3.94216e-06
Iter: 435 loss: 3.93862592e-06
Iter: 436 loss: 3.93816799e-06
Iter: 437 loss: 3.9355e-06
Iter: 438 loss: 3.93525806e-06
Iter: 439 loss: 3.93338087e-06
Iter: 440 loss: 3.92951824e-06
Iter: 441 loss: 3.96675523e-06
Iter: 442 loss: 3.92930906e-06
Iter: 443 loss: 3.92565e-06
Iter: 444 loss: 3.92542097e-06
Iter: 445 loss: 3.92266247e-06
Iter: 446 loss: 3.91916228e-06
Iter: 447 loss: 3.91936919e-06
Iter: 448 loss: 3.91634603e-06
Iter: 449 loss: 3.91173035e-06
Iter: 450 loss: 3.96632095e-06
Iter: 451 loss: 3.91153844e-06
Iter: 452 loss: 3.90878904e-06
Iter: 453 loss: 3.9069896e-06
Iter: 454 loss: 3.90601144e-06
Iter: 455 loss: 3.90146215e-06
Iter: 456 loss: 3.89958041e-06
Iter: 457 loss: 3.89710931e-06
Iter: 458 loss: 3.89233946e-06
Iter: 459 loss: 3.89227125e-06
Iter: 460 loss: 3.88800527e-06
Iter: 461 loss: 3.88594e-06
Iter: 462 loss: 3.88368335e-06
Iter: 463 loss: 3.87783803e-06
Iter: 464 loss: 3.91129925e-06
Iter: 465 loss: 3.87703949e-06
Iter: 466 loss: 3.87332966e-06
Iter: 467 loss: 3.87851878e-06
Iter: 468 loss: 3.87144382e-06
Iter: 469 loss: 3.86633974e-06
Iter: 470 loss: 3.89359502e-06
Iter: 471 loss: 3.86561942e-06
Iter: 472 loss: 3.86256397e-06
Iter: 473 loss: 3.89179149e-06
Iter: 474 loss: 3.86252714e-06
Iter: 475 loss: 3.85918383e-06
Iter: 476 loss: 3.86013562e-06
Iter: 477 loss: 3.85693875e-06
Iter: 478 loss: 3.85327e-06
Iter: 479 loss: 3.85372641e-06
Iter: 480 loss: 3.85042949e-06
Iter: 481 loss: 3.84753184e-06
Iter: 482 loss: 3.89057277e-06
Iter: 483 loss: 3.84760096e-06
Iter: 484 loss: 3.84479381e-06
Iter: 485 loss: 3.84218492e-06
Iter: 486 loss: 3.84159557e-06
Iter: 487 loss: 3.83732458e-06
Iter: 488 loss: 3.83844645e-06
Iter: 489 loss: 3.83415227e-06
Iter: 490 loss: 3.82870439e-06
Iter: 491 loss: 3.86435204e-06
Iter: 492 loss: 3.8280532e-06
Iter: 493 loss: 3.82463895e-06
Iter: 494 loss: 3.85492785e-06
Iter: 495 loss: 3.82446069e-06
Iter: 496 loss: 3.82174039e-06
Iter: 497 loss: 3.81912287e-06
Iter: 498 loss: 3.81855e-06
Iter: 499 loss: 3.81475274e-06
Iter: 500 loss: 3.84235318e-06
Iter: 501 loss: 3.81441282e-06
Iter: 502 loss: 3.81110476e-06
Iter: 503 loss: 3.81550217e-06
Iter: 504 loss: 3.80947131e-06
Iter: 505 loss: 3.80509118e-06
Iter: 506 loss: 3.83216684e-06
Iter: 507 loss: 3.80448046e-06
Iter: 508 loss: 3.8020562e-06
Iter: 509 loss: 3.84038685e-06
Iter: 510 loss: 3.80207848e-06
Iter: 511 loss: 3.80068718e-06
Iter: 512 loss: 3.79744e-06
Iter: 513 loss: 3.83424958e-06
Iter: 514 loss: 3.79706398e-06
Iter: 515 loss: 3.79269704e-06
Iter: 516 loss: 3.81305426e-06
Iter: 517 loss: 3.79188555e-06
Iter: 518 loss: 3.78796153e-06
Iter: 519 loss: 3.81221321e-06
Iter: 520 loss: 3.78733398e-06
Iter: 521 loss: 3.78473032e-06
Iter: 522 loss: 3.78028335e-06
Iter: 523 loss: 3.78026402e-06
Iter: 524 loss: 3.77628749e-06
Iter: 525 loss: 3.81359769e-06
Iter: 526 loss: 3.77625338e-06
Iter: 527 loss: 3.77292963e-06
Iter: 528 loss: 3.78150912e-06
Iter: 529 loss: 3.7716909e-06
Iter: 530 loss: 3.76829485e-06
Iter: 531 loss: 3.78313507e-06
Iter: 532 loss: 3.76762637e-06
Iter: 533 loss: 3.7652203e-06
Iter: 534 loss: 3.76220055e-06
Iter: 535 loss: 3.76197431e-06
Iter: 536 loss: 3.75626769e-06
Iter: 537 loss: 3.78653544e-06
Iter: 538 loss: 3.75546529e-06
Iter: 539 loss: 3.75251398e-06
Iter: 540 loss: 3.75254035e-06
Iter: 541 loss: 3.75066702e-06
Iter: 542 loss: 3.75875948e-06
Iter: 543 loss: 3.75010541e-06
Iter: 544 loss: 3.74770116e-06
Iter: 545 loss: 3.7438308e-06
Iter: 546 loss: 3.74379943e-06
Iter: 547 loss: 3.74063484e-06
Iter: 548 loss: 3.76116827e-06
Iter: 549 loss: 3.7402715e-06
Iter: 550 loss: 3.7373145e-06
Iter: 551 loss: 3.7525449e-06
Iter: 552 loss: 3.7369407e-06
Iter: 553 loss: 3.73447278e-06
Iter: 554 loss: 3.73362218e-06
Iter: 555 loss: 3.7321463e-06
Iter: 556 loss: 3.72904356e-06
Iter: 557 loss: 3.72761519e-06
Iter: 558 loss: 3.72601244e-06
Iter: 559 loss: 3.72197519e-06
Iter: 560 loss: 3.78247728e-06
Iter: 561 loss: 3.72196564e-06
Iter: 562 loss: 3.71922215e-06
Iter: 563 loss: 3.72376257e-06
Iter: 564 loss: 3.71795318e-06
Iter: 565 loss: 3.71434567e-06
Iter: 566 loss: 3.71304827e-06
Iter: 567 loss: 3.71114061e-06
Iter: 568 loss: 3.70734688e-06
Iter: 569 loss: 3.7389409e-06
Iter: 570 loss: 3.70705584e-06
Iter: 571 loss: 3.70351768e-06
Iter: 572 loss: 3.71243414e-06
Iter: 573 loss: 3.70230919e-06
Iter: 574 loss: 3.69929217e-06
Iter: 575 loss: 3.74247247e-06
Iter: 576 loss: 3.69937356e-06
Iter: 577 loss: 3.6973272e-06
Iter: 578 loss: 3.69929285e-06
Iter: 579 loss: 3.69608074e-06
Iter: 580 loss: 3.69386339e-06
Iter: 581 loss: 3.69047279e-06
Iter: 582 loss: 3.69043732e-06
Iter: 583 loss: 3.688486e-06
Iter: 584 loss: 3.68805036e-06
Iter: 585 loss: 3.6863521e-06
Iter: 586 loss: 3.68413e-06
Iter: 587 loss: 3.68391557e-06
Iter: 588 loss: 3.68049632e-06
Iter: 589 loss: 3.68361657e-06
Iter: 590 loss: 3.67855159e-06
Iter: 591 loss: 3.67550797e-06
Iter: 592 loss: 3.69302779e-06
Iter: 593 loss: 3.67504981e-06
Iter: 594 loss: 3.67188318e-06
Iter: 595 loss: 3.68264023e-06
Iter: 596 loss: 3.67109305e-06
Iter: 597 loss: 3.66811651e-06
Iter: 598 loss: 3.67146413e-06
Iter: 599 loss: 3.66673908e-06
Iter: 600 loss: 3.66330278e-06
Iter: 601 loss: 3.66648737e-06
Iter: 602 loss: 3.66128097e-06
Iter: 603 loss: 3.65852839e-06
Iter: 604 loss: 3.70015755e-06
Iter: 605 loss: 3.65850292e-06
Iter: 606 loss: 3.65612186e-06
Iter: 607 loss: 3.66428435e-06
Iter: 608 loss: 3.65568258e-06
Iter: 609 loss: 3.65278675e-06
Iter: 610 loss: 3.65679443e-06
Iter: 611 loss: 3.65138021e-06
Iter: 612 loss: 3.64880134e-06
Iter: 613 loss: 3.65113124e-06
Iter: 614 loss: 3.64730136e-06
Iter: 615 loss: 3.64475136e-06
Iter: 616 loss: 3.65171286e-06
Iter: 617 loss: 3.64404423e-06
Iter: 618 loss: 3.64080643e-06
Iter: 619 loss: 3.64974244e-06
Iter: 620 loss: 3.63962408e-06
Iter: 621 loss: 3.63685513e-06
Iter: 622 loss: 3.63387358e-06
Iter: 623 loss: 3.6333231e-06
Iter: 624 loss: 3.62979608e-06
Iter: 625 loss: 3.66560698e-06
Iter: 626 loss: 3.62964761e-06
Iter: 627 loss: 3.62708738e-06
Iter: 628 loss: 3.63333061e-06
Iter: 629 loss: 3.62619312e-06
Iter: 630 loss: 3.62237506e-06
Iter: 631 loss: 3.62175842e-06
Iter: 632 loss: 3.61924367e-06
Iter: 633 loss: 3.61570642e-06
Iter: 634 loss: 3.63669983e-06
Iter: 635 loss: 3.61536831e-06
Iter: 636 loss: 3.61239609e-06
Iter: 637 loss: 3.61581328e-06
Iter: 638 loss: 3.61073694e-06
Iter: 639 loss: 3.60821923e-06
Iter: 640 loss: 3.60812101e-06
Iter: 641 loss: 3.60635886e-06
Iter: 642 loss: 3.61297498e-06
Iter: 643 loss: 3.60593231e-06
Iter: 644 loss: 3.60403146e-06
Iter: 645 loss: 3.60123067e-06
Iter: 646 loss: 3.60119975e-06
Iter: 647 loss: 3.59844148e-06
Iter: 648 loss: 3.61860293e-06
Iter: 649 loss: 3.59823048e-06
Iter: 650 loss: 3.59602654e-06
Iter: 651 loss: 3.60651188e-06
Iter: 652 loss: 3.59564365e-06
Iter: 653 loss: 3.59329306e-06
Iter: 654 loss: 3.58951365e-06
Iter: 655 loss: 3.58952343e-06
Iter: 656 loss: 3.58585885e-06
Iter: 657 loss: 3.59606611e-06
Iter: 658 loss: 3.58472334e-06
Iter: 659 loss: 3.58085094e-06
Iter: 660 loss: 3.60548756e-06
Iter: 661 loss: 3.58037346e-06
Iter: 662 loss: 3.57773547e-06
Iter: 663 loss: 3.59730552e-06
Iter: 664 loss: 3.57758699e-06
Iter: 665 loss: 3.57568797e-06
Iter: 666 loss: 3.57206045e-06
Iter: 667 loss: 3.65293818e-06
Iter: 668 loss: 3.57205045e-06
Iter: 669 loss: 3.56902069e-06
Iter: 670 loss: 3.56908504e-06
Iter: 671 loss: 3.56707324e-06
Iter: 672 loss: 3.57702197e-06
Iter: 673 loss: 3.56661781e-06
Iter: 674 loss: 3.56436158e-06
Iter: 675 loss: 3.57202134e-06
Iter: 676 loss: 3.56363171e-06
Iter: 677 loss: 3.5616124e-06
Iter: 678 loss: 3.56582063e-06
Iter: 679 loss: 3.56079022e-06
Iter: 680 loss: 3.55924453e-06
Iter: 681 loss: 3.55772318e-06
Iter: 682 loss: 3.5574908e-06
Iter: 683 loss: 3.55435941e-06
Iter: 684 loss: 3.57465274e-06
Iter: 685 loss: 3.554042e-06
Iter: 686 loss: 3.55149223e-06
Iter: 687 loss: 3.55501516e-06
Iter: 688 loss: 3.55026555e-06
Iter: 689 loss: 3.54793951e-06
Iter: 690 loss: 3.54525946e-06
Iter: 691 loss: 3.54488657e-06
Iter: 692 loss: 3.54187159e-06
Iter: 693 loss: 3.57674139e-06
Iter: 694 loss: 3.54181748e-06
Iter: 695 loss: 3.53905352e-06
Iter: 696 loss: 3.54698136e-06
Iter: 697 loss: 3.53828409e-06
Iter: 698 loss: 3.53557471e-06
Iter: 699 loss: 3.53906603e-06
Iter: 700 loss: 3.53410678e-06
Iter: 701 loss: 3.53119071e-06
Iter: 702 loss: 3.52973984e-06
Iter: 703 loss: 3.52828829e-06
Iter: 704 loss: 3.52562165e-06
Iter: 705 loss: 3.52547568e-06
Iter: 706 loss: 3.52372194e-06
Iter: 707 loss: 3.54026088e-06
Iter: 708 loss: 3.52373399e-06
Iter: 709 loss: 3.52230904e-06
Iter: 710 loss: 3.52152119e-06
Iter: 711 loss: 3.52093957e-06
Iter: 712 loss: 3.51879862e-06
Iter: 713 loss: 3.52047596e-06
Iter: 714 loss: 3.51744643e-06
Iter: 715 loss: 3.51516428e-06
Iter: 716 loss: 3.52515053e-06
Iter: 717 loss: 3.51477092e-06
Iter: 718 loss: 3.51209337e-06
Iter: 719 loss: 3.51781409e-06
Iter: 720 loss: 3.5110595e-06
Iter: 721 loss: 3.50915889e-06
Iter: 722 loss: 3.51019344e-06
Iter: 723 loss: 3.50780942e-06
Iter: 724 loss: 3.50529103e-06
Iter: 725 loss: 3.50640084e-06
Iter: 726 loss: 3.50347864e-06
Iter: 727 loss: 3.50097253e-06
Iter: 728 loss: 3.53512223e-06
Iter: 729 loss: 3.50088499e-06
Iter: 730 loss: 3.49853076e-06
Iter: 731 loss: 3.49856668e-06
Iter: 732 loss: 3.4966838e-06
Iter: 733 loss: 3.4940374e-06
Iter: 734 loss: 3.50311757e-06
Iter: 735 loss: 3.49333754e-06
Iter: 736 loss: 3.4906775e-06
Iter: 737 loss: 3.49064862e-06
Iter: 738 loss: 3.48851586e-06
Iter: 739 loss: 3.48655158e-06
Iter: 740 loss: 3.48609387e-06
Iter: 741 loss: 3.48451772e-06
Iter: 742 loss: 3.48675349e-06
Iter: 743 loss: 3.48378171e-06
Iter: 744 loss: 3.48190338e-06
Iter: 745 loss: 3.4801642e-06
Iter: 746 loss: 3.47976538e-06
Iter: 747 loss: 3.47715718e-06
Iter: 748 loss: 3.49527613e-06
Iter: 749 loss: 3.47691866e-06
Iter: 750 loss: 3.47486207e-06
Iter: 751 loss: 3.48428671e-06
Iter: 752 loss: 3.4745176e-06
Iter: 753 loss: 3.47249488e-06
Iter: 754 loss: 3.47000241e-06
Iter: 755 loss: 3.46966e-06
Iter: 756 loss: 3.46656202e-06
Iter: 757 loss: 3.47194964e-06
Iter: 758 loss: 3.4652337e-06
Iter: 759 loss: 3.46113984e-06
Iter: 760 loss: 3.47338937e-06
Iter: 761 loss: 3.45993453e-06
Iter: 762 loss: 3.45730905e-06
Iter: 763 loss: 3.45733952e-06
Iter: 764 loss: 3.45561716e-06
Iter: 765 loss: 3.45240278e-06
Iter: 766 loss: 3.52028019e-06
Iter: 767 loss: 3.45238391e-06
Iter: 768 loss: 3.44952832e-06
Iter: 769 loss: 3.48754111e-06
Iter: 770 loss: 3.44952173e-06
Iter: 771 loss: 3.44801e-06
Iter: 772 loss: 3.46344905e-06
Iter: 773 loss: 3.44785872e-06
Iter: 774 loss: 3.4461309e-06
Iter: 775 loss: 3.44528507e-06
Iter: 776 loss: 3.44434829e-06
Iter: 777 loss: 3.44186242e-06
Iter: 778 loss: 3.44993032e-06
Iter: 779 loss: 3.44118121e-06
Iter: 780 loss: 3.43912234e-06
Iter: 781 loss: 3.4397865e-06
Iter: 782 loss: 3.43772e-06
Iter: 783 loss: 3.43530837e-06
Iter: 784 loss: 3.45729154e-06
Iter: 785 loss: 3.43519423e-06
Iter: 786 loss: 3.43338024e-06
Iter: 787 loss: 3.43403667e-06
Iter: 788 loss: 3.43210127e-06
Iter: 789 loss: 3.429255e-06
Iter: 790 loss: 3.42669637e-06
Iter: 791 loss: 3.42613816e-06
Iter: 792 loss: 3.42323551e-06
Iter: 793 loss: 3.45602939e-06
Iter: 794 loss: 3.42310977e-06
Iter: 795 loss: 3.42072576e-06
Iter: 796 loss: 3.4274658e-06
Iter: 797 loss: 3.41994087e-06
Iter: 798 loss: 3.4171951e-06
Iter: 799 loss: 3.42426893e-06
Iter: 800 loss: 3.41615441e-06
Iter: 801 loss: 3.41411328e-06
Iter: 802 loss: 3.41468e-06
Iter: 803 loss: 3.41269174e-06
Iter: 804 loss: 3.41030773e-06
Iter: 805 loss: 3.43161878e-06
Iter: 806 loss: 3.41015698e-06
Iter: 807 loss: 3.4079967e-06
Iter: 808 loss: 3.42258159e-06
Iter: 809 loss: 3.40780389e-06
Iter: 810 loss: 3.40591259e-06
Iter: 811 loss: 3.40547967e-06
Iter: 812 loss: 3.40422503e-06
Iter: 813 loss: 3.40235329e-06
Iter: 814 loss: 3.41069904e-06
Iter: 815 loss: 3.40184124e-06
Iter: 816 loss: 3.40030829e-06
Iter: 817 loss: 3.40193969e-06
Iter: 818 loss: 3.39933194e-06
Iter: 819 loss: 3.39670169e-06
Iter: 820 loss: 3.40404154e-06
Iter: 821 loss: 3.3959659e-06
Iter: 822 loss: 3.39387134e-06
Iter: 823 loss: 3.39412736e-06
Iter: 824 loss: 3.39236931e-06
Iter: 825 loss: 3.38909376e-06
Iter: 826 loss: 3.38922564e-06
Iter: 827 loss: 3.38658492e-06
Iter: 828 loss: 3.38382893e-06
Iter: 829 loss: 3.42283715e-06
Iter: 830 loss: 3.38387485e-06
Iter: 831 loss: 3.38126529e-06
Iter: 832 loss: 3.38552e-06
Iter: 833 loss: 3.38019981e-06
Iter: 834 loss: 3.37781512e-06
Iter: 835 loss: 3.38239829e-06
Iter: 836 loss: 3.37670599e-06
Iter: 837 loss: 3.37423216e-06
Iter: 838 loss: 3.37526558e-06
Iter: 839 loss: 3.37247752e-06
Iter: 840 loss: 3.37207894e-06
Iter: 841 loss: 3.37098163e-06
Iter: 842 loss: 3.36999324e-06
Iter: 843 loss: 3.36926405e-06
Iter: 844 loss: 3.36897483e-06
Iter: 845 loss: 3.36720655e-06
Iter: 846 loss: 3.36651829e-06
Iter: 847 loss: 3.36556172e-06
Iter: 848 loss: 3.36297057e-06
Iter: 849 loss: 3.37704796e-06
Iter: 850 loss: 3.36262383e-06
Iter: 851 loss: 3.36059816e-06
Iter: 852 loss: 3.37134406e-06
Iter: 853 loss: 3.36032963e-06
Iter: 854 loss: 3.35830828e-06
Iter: 855 loss: 3.35527147e-06
Iter: 856 loss: 3.35531831e-06
Iter: 857 loss: 3.35239929e-06
Iter: 858 loss: 3.37196775e-06
Iter: 859 loss: 3.35215964e-06
Iter: 860 loss: 3.34969536e-06
Iter: 861 loss: 3.35001846e-06
Iter: 862 loss: 3.34787819e-06
Iter: 863 loss: 3.34604533e-06
Iter: 864 loss: 3.34583456e-06
Iter: 865 loss: 3.34441802e-06
Iter: 866 loss: 3.3417673e-06
Iter: 867 loss: 3.39812163e-06
Iter: 868 loss: 3.3416793e-06
Iter: 869 loss: 3.33847606e-06
Iter: 870 loss: 3.35728737e-06
Iter: 871 loss: 3.33803723e-06
Iter: 872 loss: 3.33719368e-06
Iter: 873 loss: 3.3368708e-06
Iter: 874 loss: 3.33571302e-06
Iter: 875 loss: 3.33457137e-06
Iter: 876 loss: 3.33431285e-06
Iter: 877 loss: 3.33243702e-06
Iter: 878 loss: 3.33684693e-06
Iter: 879 loss: 3.33177923e-06
Iter: 880 loss: 3.32976697e-06
Iter: 881 loss: 3.3315273e-06
Iter: 882 loss: 3.32856644e-06
Iter: 883 loss: 3.32641639e-06
Iter: 884 loss: 3.34772631e-06
Iter: 885 loss: 3.32629497e-06
Iter: 886 loss: 3.32474883e-06
Iter: 887 loss: 3.32758782e-06
Iter: 888 loss: 3.32429386e-06
Iter: 889 loss: 3.32281343e-06
Iter: 890 loss: 3.32015179e-06
Iter: 891 loss: 3.37879396e-06
Iter: 892 loss: 3.32021909e-06
Iter: 893 loss: 3.31715955e-06
Iter: 894 loss: 3.3456115e-06
Iter: 895 loss: 3.31702813e-06
Iter: 896 loss: 3.3148674e-06
Iter: 897 loss: 3.31963815e-06
Iter: 898 loss: 3.31395404e-06
Iter: 899 loss: 3.31101637e-06
Iter: 900 loss: 3.32007721e-06
Iter: 901 loss: 3.31012961e-06
Iter: 902 loss: 3.30778403e-06
Iter: 903 loss: 3.30687567e-06
Iter: 904 loss: 3.3056167e-06
Iter: 905 loss: 3.30360422e-06
Iter: 906 loss: 3.30345961e-06
Iter: 907 loss: 3.30175885e-06
Iter: 908 loss: 3.30975649e-06
Iter: 909 loss: 3.30134435e-06
Iter: 910 loss: 3.29983777e-06
Iter: 911 loss: 3.29794e-06
Iter: 912 loss: 3.29780892e-06
Iter: 913 loss: 3.2956109e-06
Iter: 914 loss: 3.31414844e-06
Iter: 915 loss: 3.29550289e-06
Iter: 916 loss: 3.29418663e-06
Iter: 917 loss: 3.29699014e-06
Iter: 918 loss: 3.29365821e-06
Iter: 919 loss: 3.29180557e-06
Iter: 920 loss: 3.29461227e-06
Iter: 921 loss: 3.29081e-06
Iter: 922 loss: 3.28908322e-06
Iter: 923 loss: 3.29007889e-06
Iter: 924 loss: 3.28789361e-06
Iter: 925 loss: 3.28562169e-06
Iter: 926 loss: 3.2866792e-06
Iter: 927 loss: 3.283968e-06
Iter: 928 loss: 3.28166379e-06
Iter: 929 loss: 3.30087869e-06
Iter: 930 loss: 3.28156898e-06
Iter: 931 loss: 3.27951375e-06
Iter: 932 loss: 3.28486294e-06
Iter: 933 loss: 3.27872704e-06
Iter: 934 loss: 3.27662224e-06
Iter: 935 loss: 3.28223064e-06
Iter: 936 loss: 3.27586963e-06
Iter: 937 loss: 3.27403404e-06
Iter: 938 loss: 3.27192242e-06
Iter: 939 loss: 3.27168254e-06
Iter: 940 loss: 3.27260432e-06
Iter: 941 loss: 3.2703017e-06
Iter: 942 loss: 3.26956615e-06
Iter: 943 loss: 3.2681919e-06
Iter: 944 loss: 3.2681678e-06
Iter: 945 loss: 3.26635904e-06
Iter: 946 loss: 3.26806321e-06
Iter: 947 loss: 3.26528811e-06
Iter: 948 loss: 3.26316604e-06
Iter: 949 loss: 3.27420889e-06
Iter: 950 loss: 3.26288205e-06
Iter: 951 loss: 3.26134295e-06
Iter: 952 loss: 3.26841337e-06
Iter: 953 loss: 3.2609305e-06
Iter: 954 loss: 3.25907195e-06
Iter: 955 loss: 3.25707902e-06
Iter: 956 loss: 3.25676729e-06
Iter: 957 loss: 3.25464953e-06
Iter: 958 loss: 3.2686994e-06
Iter: 959 loss: 3.25440942e-06
Iter: 960 loss: 3.25271071e-06
Iter: 961 loss: 3.25091014e-06
Iter: 962 loss: 3.25048541e-06
Iter: 963 loss: 3.24879488e-06
Iter: 964 loss: 3.248648e-06
Iter: 965 loss: 3.2473522e-06
Iter: 966 loss: 3.24820212e-06
Iter: 967 loss: 3.24652092e-06
Iter: 968 loss: 3.24494545e-06
Iter: 969 loss: 3.24507573e-06
Iter: 970 loss: 3.24352777e-06
Iter: 971 loss: 3.24196571e-06
Iter: 972 loss: 3.25995279e-06
Iter: 973 loss: 3.24183839e-06
Iter: 974 loss: 3.23987069e-06
Iter: 975 loss: 3.24109646e-06
Iter: 976 loss: 3.23842823e-06
Iter: 977 loss: 3.23713311e-06
Iter: 978 loss: 3.24153393e-06
Iter: 979 loss: 3.23686777e-06
Iter: 980 loss: 3.23543554e-06
Iter: 981 loss: 3.23429e-06
Iter: 982 loss: 3.23366544e-06
Iter: 983 loss: 3.23125664e-06
Iter: 984 loss: 3.24931261e-06
Iter: 985 loss: 3.23104678e-06
Iter: 986 loss: 3.22917685e-06
Iter: 987 loss: 3.23598988e-06
Iter: 988 loss: 3.22884034e-06
Iter: 989 loss: 3.22719234e-06
Iter: 990 loss: 3.22394726e-06
Iter: 991 loss: 3.29234899e-06
Iter: 992 loss: 3.22400501e-06
Iter: 993 loss: 3.22134247e-06
Iter: 994 loss: 3.25791e-06
Iter: 995 loss: 3.22140022e-06
Iter: 996 loss: 3.2196167e-06
Iter: 997 loss: 3.22041956e-06
Iter: 998 loss: 3.21842299e-06
Iter: 999 loss: 3.21620655e-06
Iter: 1000 loss: 3.24088842e-06
Iter: 1001 loss: 3.21612674e-06
Iter: 1002 loss: 3.21484458e-06
Iter: 1003 loss: 3.21315724e-06
Iter: 1004 loss: 3.21306061e-06
Iter: 1005 loss: 3.21052426e-06
Iter: 1006 loss: 3.2243006e-06
Iter: 1007 loss: 3.21014841e-06
Iter: 1008 loss: 3.20912432e-06
Iter: 1009 loss: 3.20885283e-06
Iter: 1010 loss: 3.20820163e-06
Iter: 1011 loss: 3.20620256e-06
Iter: 1012 loss: 3.22360825e-06
Iter: 1013 loss: 3.20591653e-06
Iter: 1014 loss: 3.20427944e-06
Iter: 1015 loss: 3.2286589e-06
Iter: 1016 loss: 3.20427353e-06
Iter: 1017 loss: 3.20284175e-06
Iter: 1018 loss: 3.2017424e-06
Iter: 1019 loss: 3.2011892e-06
Iter: 1020 loss: 3.19875744e-06
Iter: 1021 loss: 3.21740299e-06
Iter: 1022 loss: 3.19852052e-06
Iter: 1023 loss: 3.19701326e-06
Iter: 1024 loss: 3.19930518e-06
Iter: 1025 loss: 3.19626088e-06
Iter: 1026 loss: 3.19480137e-06
Iter: 1027 loss: 3.19319042e-06
Iter: 1028 loss: 3.1928796e-06
Iter: 1029 loss: 3.19043261e-06
Iter: 1030 loss: 3.20398067e-06
Iter: 1031 loss: 3.19006813e-06
Iter: 1032 loss: 3.18761954e-06
Iter: 1033 loss: 3.19235392e-06
Iter: 1034 loss: 3.18654111e-06
Iter: 1035 loss: 3.18373259e-06
Iter: 1036 loss: 3.19781066e-06
Iter: 1037 loss: 3.18315961e-06
Iter: 1038 loss: 3.18123e-06
Iter: 1039 loss: 3.18129605e-06
Iter: 1040 loss: 3.17969648e-06
Iter: 1041 loss: 3.17917738e-06
Iter: 1042 loss: 3.1785903e-06
Iter: 1043 loss: 3.17747117e-06
Iter: 1044 loss: 3.17545073e-06
Iter: 1045 loss: 3.17546119e-06
Iter: 1046 loss: 3.17324202e-06
Iter: 1047 loss: 3.17625791e-06
Iter: 1048 loss: 3.17193326e-06
Iter: 1049 loss: 3.17044078e-06
Iter: 1050 loss: 3.17040553e-06
Iter: 1051 loss: 3.16936871e-06
Iter: 1052 loss: 3.16922569e-06
Iter: 1053 loss: 3.16835781e-06
Iter: 1054 loss: 3.16662613e-06
Iter: 1055 loss: 3.17501645e-06
Iter: 1056 loss: 3.16626392e-06
Iter: 1057 loss: 3.16501837e-06
Iter: 1058 loss: 3.16406022e-06
Iter: 1059 loss: 3.16370324e-06
Iter: 1060 loss: 3.1614436e-06
Iter: 1061 loss: 3.16579735e-06
Iter: 1062 loss: 3.16057276e-06
Iter: 1063 loss: 3.15823218e-06
Iter: 1064 loss: 3.16621163e-06
Iter: 1065 loss: 3.15774332e-06
Iter: 1066 loss: 3.15529451e-06
Iter: 1067 loss: 3.16812066e-06
Iter: 1068 loss: 3.15509055e-06
Iter: 1069 loss: 3.15342913e-06
Iter: 1070 loss: 3.15814123e-06
Iter: 1071 loss: 3.15293823e-06
Iter: 1072 loss: 3.15150032e-06
Iter: 1073 loss: 3.15122816e-06
Iter: 1074 loss: 3.15018224e-06
Iter: 1075 loss: 3.14861154e-06
Iter: 1076 loss: 3.14851741e-06
Iter: 1077 loss: 3.14778e-06
Iter: 1078 loss: 3.14568979e-06
Iter: 1079 loss: 3.15808052e-06
Iter: 1080 loss: 3.14515478e-06
Iter: 1081 loss: 3.14289355e-06
Iter: 1082 loss: 3.17254489e-06
Iter: 1083 loss: 3.1428076e-06
Iter: 1084 loss: 3.14163572e-06
Iter: 1085 loss: 3.14863564e-06
Iter: 1086 loss: 3.14142494e-06
Iter: 1087 loss: 3.14025738e-06
Iter: 1088 loss: 3.14013323e-06
Iter: 1089 loss: 3.13930764e-06
Iter: 1090 loss: 3.13687178e-06
Iter: 1091 loss: 3.13761552e-06
Iter: 1092 loss: 3.13523515e-06
Iter: 1093 loss: 3.13332794e-06
Iter: 1094 loss: 3.13608143e-06
Iter: 1095 loss: 3.13254759e-06
Iter: 1096 loss: 3.13007558e-06
Iter: 1097 loss: 3.14137742e-06
Iter: 1098 loss: 3.12978409e-06
Iter: 1099 loss: 3.12831889e-06
Iter: 1100 loss: 3.13838791e-06
Iter: 1101 loss: 3.12806242e-06
Iter: 1102 loss: 3.12656084e-06
Iter: 1103 loss: 3.12648262e-06
Iter: 1104 loss: 3.12547718e-06
Iter: 1105 loss: 3.12332645e-06
Iter: 1106 loss: 3.12838915e-06
Iter: 1107 loss: 3.12263819e-06
Iter: 1108 loss: 3.12164593e-06
Iter: 1109 loss: 3.12155225e-06
Iter: 1110 loss: 3.12040061e-06
Iter: 1111 loss: 3.11840267e-06
Iter: 1112 loss: 3.11840654e-06
Iter: 1113 loss: 3.11708709e-06
Iter: 1114 loss: 3.11974145e-06
Iter: 1115 loss: 3.11656891e-06
Iter: 1116 loss: 3.11450958e-06
Iter: 1117 loss: 3.11877398e-06
Iter: 1118 loss: 3.11364556e-06
Iter: 1119 loss: 3.11188069e-06
Iter: 1120 loss: 3.12664156e-06
Iter: 1121 loss: 3.11177223e-06
Iter: 1122 loss: 3.11072199e-06
Iter: 1123 loss: 3.11391022e-06
Iter: 1124 loss: 3.1103732e-06
Iter: 1125 loss: 3.1092809e-06
Iter: 1126 loss: 3.10692735e-06
Iter: 1127 loss: 3.15695593e-06
Iter: 1128 loss: 3.10693486e-06
Iter: 1129 loss: 3.10528139e-06
Iter: 1130 loss: 3.12200132e-06
Iter: 1131 loss: 3.10529254e-06
Iter: 1132 loss: 3.10365476e-06
Iter: 1133 loss: 3.10409382e-06
Iter: 1134 loss: 3.10240785e-06
Iter: 1135 loss: 3.10011501e-06
Iter: 1136 loss: 3.1150787e-06
Iter: 1137 loss: 3.09989719e-06
Iter: 1138 loss: 3.09811367e-06
Iter: 1139 loss: 3.09878715e-06
Iter: 1140 loss: 3.09682855e-06
Iter: 1141 loss: 3.0952e-06
Iter: 1142 loss: 3.10892528e-06
Iter: 1143 loss: 3.09507e-06
Iter: 1144 loss: 3.09345364e-06
Iter: 1145 loss: 3.10657333e-06
Iter: 1146 loss: 3.09334791e-06
Iter: 1147 loss: 3.09244683e-06
Iter: 1148 loss: 3.09012034e-06
Iter: 1149 loss: 3.11545818e-06
Iter: 1150 loss: 3.08998278e-06
Iter: 1151 loss: 3.08840663e-06
Iter: 1152 loss: 3.08842391e-06
Iter: 1153 loss: 3.08697167e-06
Iter: 1154 loss: 3.08850349e-06
Iter: 1155 loss: 3.08623521e-06
Iter: 1156 loss: 3.08481276e-06
Iter: 1157 loss: 3.09402776e-06
Iter: 1158 loss: 3.08462904e-06
Iter: 1159 loss: 3.08348376e-06
Iter: 1160 loss: 3.08356925e-06
Iter: 1161 loss: 3.08264453e-06
Iter: 1162 loss: 3.08084941e-06
Iter: 1163 loss: 3.08100925e-06
Iter: 1164 loss: 3.07964092e-06
Iter: 1165 loss: 3.0775609e-06
Iter: 1166 loss: 3.08421522e-06
Iter: 1167 loss: 3.07708456e-06
Iter: 1168 loss: 3.07523806e-06
Iter: 1169 loss: 3.09163943e-06
Iter: 1170 loss: 3.07512801e-06
Iter: 1171 loss: 3.07375285e-06
Iter: 1172 loss: 3.0748206e-06
Iter: 1173 loss: 3.07302025e-06
Iter: 1174 loss: 3.07113078e-06
Iter: 1175 loss: 3.0728047e-06
Iter: 1176 loss: 3.0699839e-06
Iter: 1177 loss: 3.06974061e-06
Iter: 1178 loss: 3.06920401e-06
Iter: 1179 loss: 3.0684032e-06
Iter: 1180 loss: 3.06641914e-06
Iter: 1181 loss: 3.08434846e-06
Iter: 1182 loss: 3.06617403e-06
Iter: 1183 loss: 3.06427819e-06
Iter: 1184 loss: 3.07334312e-06
Iter: 1185 loss: 3.06399079e-06
Iter: 1186 loss: 3.06266747e-06
Iter: 1187 loss: 3.07340247e-06
Iter: 1188 loss: 3.06258471e-06
Iter: 1189 loss: 3.06106222e-06
Iter: 1190 loss: 3.05941876e-06
Iter: 1191 loss: 3.05921048e-06
Iter: 1192 loss: 3.0575402e-06
Iter: 1193 loss: 3.05752155e-06
Iter: 1194 loss: 3.05667072e-06
Iter: 1195 loss: 3.05516051e-06
Iter: 1196 loss: 3.05519e-06
Iter: 1197 loss: 3.05294634e-06
Iter: 1198 loss: 3.05706158e-06
Iter: 1199 loss: 3.05198591e-06
Iter: 1200 loss: 3.05033268e-06
Iter: 1201 loss: 3.05874164e-06
Iter: 1202 loss: 3.05004824e-06
Iter: 1203 loss: 3.04803393e-06
Iter: 1204 loss: 3.05294225e-06
Iter: 1205 loss: 3.04726859e-06
Iter: 1206 loss: 3.04600781e-06
Iter: 1207 loss: 3.05004096e-06
Iter: 1208 loss: 3.04559308e-06
Iter: 1209 loss: 3.04397781e-06
Iter: 1210 loss: 3.04554487e-06
Iter: 1211 loss: 3.04310674e-06
Iter: 1212 loss: 3.04165724e-06
Iter: 1213 loss: 3.04161358e-06
Iter: 1214 loss: 3.04098739e-06
Iter: 1215 loss: 3.03910792e-06
Iter: 1216 loss: 3.04913897e-06
Iter: 1217 loss: 3.03864181e-06
Iter: 1218 loss: 3.03673028e-06
Iter: 1219 loss: 3.05798153e-06
Iter: 1220 loss: 3.03663455e-06
Iter: 1221 loss: 3.03544539e-06
Iter: 1222 loss: 3.04833861e-06
Iter: 1223 loss: 3.03538354e-06
Iter: 1224 loss: 3.03437537e-06
Iter: 1225 loss: 3.03339129e-06
Iter: 1226 loss: 3.03312527e-06
Iter: 1227 loss: 3.03146317e-06
Iter: 1228 loss: 3.0445658e-06
Iter: 1229 loss: 3.03136903e-06
Iter: 1230 loss: 3.03012098e-06
Iter: 1231 loss: 3.02823469e-06
Iter: 1232 loss: 3.0281451e-06
Iter: 1233 loss: 3.02651347e-06
Iter: 1234 loss: 3.04849937e-06
Iter: 1235 loss: 3.02646e-06
Iter: 1236 loss: 3.02538342e-06
Iter: 1237 loss: 3.0253741e-06
Iter: 1238 loss: 3.02461785e-06
Iter: 1239 loss: 3.02244143e-06
Iter: 1240 loss: 3.03018487e-06
Iter: 1241 loss: 3.02196622e-06
Iter: 1242 loss: 3.02053127e-06
Iter: 1243 loss: 3.02265971e-06
Iter: 1244 loss: 3.01989621e-06
Iter: 1245 loss: 3.01872387e-06
Iter: 1246 loss: 3.03140177e-06
Iter: 1247 loss: 3.01865521e-06
Iter: 1248 loss: 3.01756245e-06
Iter: 1249 loss: 3.02283297e-06
Iter: 1250 loss: 3.01731461e-06
Iter: 1251 loss: 3.01650698e-06
Iter: 1252 loss: 3.01491582e-06
Iter: 1253 loss: 3.04153536e-06
Iter: 1254 loss: 3.01483351e-06
Iter: 1255 loss: 3.01358432e-06
Iter: 1256 loss: 3.02421859e-06
Iter: 1257 loss: 3.01355e-06
Iter: 1258 loss: 3.01228056e-06
Iter: 1259 loss: 3.01631303e-06
Iter: 1260 loss: 3.01185332e-06
Iter: 1261 loss: 3.01045111e-06
Iter: 1262 loss: 3.01213822e-06
Iter: 1263 loss: 3.00975535e-06
Iter: 1264 loss: 3.00851525e-06
Iter: 1265 loss: 3.01571663e-06
Iter: 1266 loss: 3.00830652e-06
Iter: 1267 loss: 3.00714419e-06
Iter: 1268 loss: 3.00515603e-06
Iter: 1269 loss: 3.00510851e-06
Iter: 1270 loss: 3.00336251e-06
Iter: 1271 loss: 3.01453451e-06
Iter: 1272 loss: 3.00313832e-06
Iter: 1273 loss: 3.00167494e-06
Iter: 1274 loss: 3.00826605e-06
Iter: 1275 loss: 3.00125726e-06
Iter: 1276 loss: 2.99980888e-06
Iter: 1277 loss: 3.00832903e-06
Iter: 1278 loss: 2.99976296e-06
Iter: 1279 loss: 2.99852218e-06
Iter: 1280 loss: 2.99767362e-06
Iter: 1281 loss: 2.99717931e-06
Iter: 1282 loss: 2.99687827e-06
Iter: 1283 loss: 2.99656267e-06
Iter: 1284 loss: 2.99587214e-06
Iter: 1285 loss: 2.99614862e-06
Iter: 1286 loss: 2.99546e-06
Iter: 1287 loss: 2.99464136e-06
Iter: 1288 loss: 2.99300268e-06
Iter: 1289 loss: 3.02199351e-06
Iter: 1290 loss: 2.99289832e-06
Iter: 1291 loss: 2.99167118e-06
Iter: 1292 loss: 3.01057594e-06
Iter: 1293 loss: 2.99164549e-06
Iter: 1294 loss: 2.99033945e-06
Iter: 1295 loss: 2.99417707e-06
Iter: 1296 loss: 2.99007843e-06
Iter: 1297 loss: 2.98877194e-06
Iter: 1298 loss: 2.98929945e-06
Iter: 1299 loss: 2.98790656e-06
Iter: 1300 loss: 2.98652958e-06
Iter: 1301 loss: 2.99479143e-06
Iter: 1302 loss: 2.98632813e-06
Iter: 1303 loss: 2.98515533e-06
Iter: 1304 loss: 2.98436271e-06
Iter: 1305 loss: 2.98390569e-06
Iter: 1306 loss: 2.98251689e-06
Iter: 1307 loss: 2.98334635e-06
Iter: 1308 loss: 2.98164196e-06
Iter: 1309 loss: 2.98023338e-06
Iter: 1310 loss: 3.00299803e-06
Iter: 1311 loss: 2.98024634e-06
Iter: 1312 loss: 2.97927454e-06
Iter: 1313 loss: 2.98272403e-06
Iter: 1314 loss: 2.97893439e-06
Iter: 1315 loss: 2.97792985e-06
Iter: 1316 loss: 2.97823021e-06
Iter: 1317 loss: 2.9771345e-06
Iter: 1318 loss: 2.9764492e-06
Iter: 1319 loss: 2.97640736e-06
Iter: 1320 loss: 2.97573979e-06
Iter: 1321 loss: 2.97426914e-06
Iter: 1322 loss: 2.99663e-06
Iter: 1323 loss: 2.97420956e-06
Iter: 1324 loss: 2.97264933e-06
Iter: 1325 loss: 2.97630027e-06
Iter: 1326 loss: 2.97205634e-06
Iter: 1327 loss: 2.97106499e-06
Iter: 1328 loss: 2.98098394e-06
Iter: 1329 loss: 2.97095039e-06
Iter: 1330 loss: 2.96993949e-06
Iter: 1331 loss: 2.97284328e-06
Iter: 1332 loss: 2.96951589e-06
Iter: 1333 loss: 2.96860617e-06
Iter: 1334 loss: 2.96830603e-06
Iter: 1335 loss: 2.96770122e-06
Iter: 1336 loss: 2.96605867e-06
Iter: 1337 loss: 2.97173483e-06
Iter: 1338 loss: 2.96562234e-06
Iter: 1339 loss: 2.96442158e-06
Iter: 1340 loss: 2.96916232e-06
Iter: 1341 loss: 2.96422331e-06
Iter: 1342 loss: 2.96320923e-06
Iter: 1343 loss: 2.96098233e-06
Iter: 1344 loss: 2.98381792e-06
Iter: 1345 loss: 2.96074495e-06
Iter: 1346 loss: 2.95995096e-06
Iter: 1347 loss: 2.95934251e-06
Iter: 1348 loss: 2.95813834e-06
Iter: 1349 loss: 2.958908e-06
Iter: 1350 loss: 2.95740392e-06
Iter: 1351 loss: 2.95623204e-06
Iter: 1352 loss: 2.96651888e-06
Iter: 1353 loss: 2.95613199e-06
Iter: 1354 loss: 2.95535165e-06
Iter: 1355 loss: 2.96420558e-06
Iter: 1356 loss: 2.95528662e-06
Iter: 1357 loss: 2.95486689e-06
Iter: 1358 loss: 2.95336235e-06
Iter: 1359 loss: 2.96215603e-06
Iter: 1360 loss: 2.95302561e-06
Iter: 1361 loss: 2.95135851e-06
Iter: 1362 loss: 2.96244843e-06
Iter: 1363 loss: 2.95118889e-06
Iter: 1364 loss: 2.95006885e-06
Iter: 1365 loss: 2.96126063e-06
Iter: 1366 loss: 2.94999654e-06
Iter: 1367 loss: 2.94889878e-06
Iter: 1368 loss: 2.95027166e-06
Iter: 1369 loss: 2.9482394e-06
Iter: 1370 loss: 2.94739425e-06
Iter: 1371 loss: 2.94843971e-06
Iter: 1372 loss: 2.946922e-06
Iter: 1373 loss: 2.94552729e-06
Iter: 1374 loss: 2.94611596e-06
Iter: 1375 loss: 2.94460801e-06
Iter: 1376 loss: 2.94300071e-06
Iter: 1377 loss: 2.94756228e-06
Iter: 1378 loss: 2.94241318e-06
Iter: 1379 loss: 2.94078973e-06
Iter: 1380 loss: 2.9396856e-06
Iter: 1381 loss: 2.9390776e-06
Iter: 1382 loss: 2.93814651e-06
Iter: 1383 loss: 2.93789685e-06
Iter: 1384 loss: 2.93683502e-06
Iter: 1385 loss: 2.93644098e-06
Iter: 1386 loss: 2.93584299e-06
Iter: 1387 loss: 2.93509083e-06
Iter: 1388 loss: 2.93500034e-06
Iter: 1389 loss: 2.93433e-06
Iter: 1390 loss: 2.9348671e-06
Iter: 1391 loss: 2.93393032e-06
Iter: 1392 loss: 2.93333028e-06
Iter: 1393 loss: 2.93171979e-06
Iter: 1394 loss: 2.95281279e-06
Iter: 1395 loss: 2.9317257e-06
Iter: 1396 loss: 2.93035328e-06
Iter: 1397 loss: 2.94624351e-06
Iter: 1398 loss: 2.9303269e-06
Iter: 1399 loss: 2.92933123e-06
Iter: 1400 loss: 2.93642142e-06
Iter: 1401 loss: 2.92927962e-06
Iter: 1402 loss: 2.92817026e-06
Iter: 1403 loss: 2.92770846e-06
Iter: 1404 loss: 2.92726122e-06
Iter: 1405 loss: 2.9257053e-06
Iter: 1406 loss: 2.92581558e-06
Iter: 1407 loss: 2.92452523e-06
Iter: 1408 loss: 2.92306368e-06
Iter: 1409 loss: 2.9230473e-06
Iter: 1410 loss: 2.92231903e-06
Iter: 1411 loss: 2.92095046e-06
Iter: 1412 loss: 2.92094455e-06
Iter: 1413 loss: 2.91905485e-06
Iter: 1414 loss: 2.92826326e-06
Iter: 1415 loss: 2.91878337e-06
Iter: 1416 loss: 2.91747665e-06
Iter: 1417 loss: 2.9207722e-06
Iter: 1418 loss: 2.91699371e-06
Iter: 1419 loss: 2.91530978e-06
Iter: 1420 loss: 2.92378536e-06
Iter: 1421 loss: 2.91503738e-06
Iter: 1422 loss: 2.91411652e-06
Iter: 1423 loss: 2.92642653e-06
Iter: 1424 loss: 2.91410697e-06
Iter: 1425 loss: 2.91323477e-06
Iter: 1426 loss: 2.9122707e-06
Iter: 1427 loss: 2.91214883e-06
Iter: 1428 loss: 2.91076367e-06
Iter: 1429 loss: 2.91116157e-06
Iter: 1430 loss: 2.90992898e-06
Iter: 1431 loss: 2.90840671e-06
Iter: 1432 loss: 2.91192805e-06
Iter: 1433 loss: 2.90786511e-06
Iter: 1434 loss: 2.90657817e-06
Iter: 1435 loss: 2.90664866e-06
Iter: 1436 loss: 2.90584853e-06
Iter: 1437 loss: 2.90571279e-06
Iter: 1438 loss: 2.90516687e-06
Iter: 1439 loss: 2.90381536e-06
Iter: 1440 loss: 2.90329922e-06
Iter: 1441 loss: 2.90266257e-06
Iter: 1442 loss: 2.90150729e-06
Iter: 1443 loss: 2.90149228e-06
Iter: 1444 loss: 2.90062371e-06
Iter: 1445 loss: 2.89902982e-06
Iter: 1446 loss: 2.92644495e-06
Iter: 1447 loss: 2.89894933e-06
Iter: 1448 loss: 2.89714944e-06
Iter: 1449 loss: 2.91541051e-06
Iter: 1450 loss: 2.89712852e-06
Iter: 1451 loss: 2.895948e-06
Iter: 1452 loss: 2.89606487e-06
Iter: 1453 loss: 2.89500304e-06
Iter: 1454 loss: 2.89318473e-06
Iter: 1455 loss: 2.91288643e-06
Iter: 1456 loss: 2.89317063e-06
Iter: 1457 loss: 2.89239597e-06
Iter: 1458 loss: 2.90256048e-06
Iter: 1459 loss: 2.89234504e-06
Iter: 1460 loss: 2.89159584e-06
Iter: 1461 loss: 2.88987758e-06
Iter: 1462 loss: 2.91056404e-06
Iter: 1463 loss: 2.88971523e-06
Iter: 1464 loss: 2.88794627e-06
Iter: 1465 loss: 2.89277409e-06
Iter: 1466 loss: 2.88722913e-06
Iter: 1467 loss: 2.88565798e-06
Iter: 1468 loss: 2.89030231e-06
Iter: 1469 loss: 2.88509341e-06
Iter: 1470 loss: 2.88429055e-06
Iter: 1471 loss: 2.88418551e-06
Iter: 1472 loss: 2.88344563e-06
Iter: 1473 loss: 2.88220826e-06
Iter: 1474 loss: 2.88220963e-06
Iter: 1475 loss: 2.88098954e-06
Iter: 1476 loss: 2.8893437e-06
Iter: 1477 loss: 2.88101819e-06
Iter: 1478 loss: 2.88015713e-06
Iter: 1479 loss: 2.88139154e-06
Iter: 1480 loss: 2.87983721e-06
Iter: 1481 loss: 2.87855823e-06
Iter: 1482 loss: 2.87810508e-06
Iter: 1483 loss: 2.8773693e-06
Iter: 1484 loss: 2.87592593e-06
Iter: 1485 loss: 2.88094543e-06
Iter: 1486 loss: 2.87546391e-06
Iter: 1487 loss: 2.87415969e-06
Iter: 1488 loss: 2.87907051e-06
Iter: 1489 loss: 2.87378634e-06
Iter: 1490 loss: 2.87263583e-06
Iter: 1491 loss: 2.88286401e-06
Iter: 1492 loss: 2.87256557e-06
Iter: 1493 loss: 2.87142052e-06
Iter: 1494 loss: 2.87431976e-06
Iter: 1495 loss: 2.87103558e-06
Iter: 1496 loss: 2.86974068e-06
Iter: 1497 loss: 2.87270404e-06
Iter: 1498 loss: 2.86914201e-06
Iter: 1499 loss: 2.8683221e-06
Iter: 1500 loss: 2.86692284e-06
Iter: 1501 loss: 2.89995705e-06
Iter: 1502 loss: 2.86696013e-06
Iter: 1503 loss: 2.86493696e-06
Iter: 1504 loss: 2.87330613e-06
Iter: 1505 loss: 2.86452382e-06
Iter: 1506 loss: 2.86362842e-06
Iter: 1507 loss: 2.86363024e-06
Iter: 1508 loss: 2.86261775e-06
Iter: 1509 loss: 2.86114869e-06
Iter: 1510 loss: 2.8611978e-06
Iter: 1511 loss: 2.85975329e-06
Iter: 1512 loss: 2.86556906e-06
Iter: 1513 loss: 2.85946976e-06
Iter: 1514 loss: 2.8580821e-06
Iter: 1515 loss: 2.86172053e-06
Iter: 1516 loss: 2.85760257e-06
Iter: 1517 loss: 2.85635201e-06
Iter: 1518 loss: 2.86378372e-06
Iter: 1519 loss: 2.85625401e-06
Iter: 1520 loss: 2.85522447e-06
Iter: 1521 loss: 2.85381361e-06
Iter: 1522 loss: 2.85375268e-06
Iter: 1523 loss: 2.85226042e-06
Iter: 1524 loss: 2.86802469e-06
Iter: 1525 loss: 2.8521913e-06
Iter: 1526 loss: 2.85086685e-06
Iter: 1527 loss: 2.8552e-06
Iter: 1528 loss: 2.85059468e-06
Iter: 1529 loss: 2.84926318e-06
Iter: 1530 loss: 2.85962051e-06
Iter: 1531 loss: 2.84913904e-06
Iter: 1532 loss: 2.84820339e-06
Iter: 1533 loss: 2.85178567e-06
Iter: 1534 loss: 2.847936e-06
Iter: 1535 loss: 2.84718544e-06
Iter: 1536 loss: 2.84540261e-06
Iter: 1537 loss: 2.86975455e-06
Iter: 1538 loss: 2.84529824e-06
Iter: 1539 loss: 2.84359658e-06
Iter: 1540 loss: 2.85231499e-06
Iter: 1541 loss: 2.84327916e-06
Iter: 1542 loss: 2.84164071e-06
Iter: 1543 loss: 2.84552198e-06
Iter: 1544 loss: 2.84108296e-06
Iter: 1545 loss: 2.84031762e-06
Iter: 1546 loss: 2.84025373e-06
Iter: 1547 loss: 2.83947838e-06
Iter: 1548 loss: 2.83866962e-06
Iter: 1549 loss: 2.83853274e-06
Iter: 1550 loss: 2.83742656e-06
Iter: 1551 loss: 2.84000021e-06
Iter: 1552 loss: 2.83707914e-06
Iter: 1553 loss: 2.83572581e-06
Iter: 1554 loss: 2.83933514e-06
Iter: 1555 loss: 2.83538611e-06
Iter: 1556 loss: 2.83405e-06
Iter: 1557 loss: 2.83737063e-06
Iter: 1558 loss: 2.83358258e-06
Iter: 1559 loss: 2.83244935e-06
Iter: 1560 loss: 2.83202439e-06
Iter: 1561 loss: 2.83144209e-06
Iter: 1562 loss: 2.82952942e-06
Iter: 1563 loss: 2.83834243e-06
Iter: 1564 loss: 2.8292734e-06
Iter: 1565 loss: 2.82856968e-06
Iter: 1566 loss: 2.82854762e-06
Iter: 1567 loss: 2.82782e-06
Iter: 1568 loss: 2.82748988e-06
Iter: 1569 loss: 2.82714404e-06
Iter: 1570 loss: 2.82612655e-06
Iter: 1571 loss: 2.8308848e-06
Iter: 1572 loss: 2.8259276e-06
Iter: 1573 loss: 2.82522387e-06
Iter: 1574 loss: 2.8242921e-06
Iter: 1575 loss: 2.82425412e-06
Iter: 1576 loss: 2.82279552e-06
Iter: 1577 loss: 2.82623864e-06
Iter: 1578 loss: 2.8222637e-06
Iter: 1579 loss: 2.82086194e-06
Iter: 1580 loss: 2.82478e-06
Iter: 1581 loss: 2.82042856e-06
Iter: 1582 loss: 2.81915754e-06
Iter: 1583 loss: 2.83888335e-06
Iter: 1584 loss: 2.81918346e-06
Iter: 1585 loss: 2.81858593e-06
Iter: 1586 loss: 2.81807388e-06
Iter: 1587 loss: 2.81789971e-06
Iter: 1588 loss: 2.81667872e-06
Iter: 1589 loss: 2.81840357e-06
Iter: 1590 loss: 2.81606776e-06
Iter: 1591 loss: 2.81497523e-06
Iter: 1592 loss: 2.82525934e-06
Iter: 1593 loss: 2.81488292e-06
Iter: 1594 loss: 2.81410826e-06
Iter: 1595 loss: 2.81288021e-06
Iter: 1596 loss: 2.81290409e-06
Iter: 1597 loss: 2.8111117e-06
Iter: 1598 loss: 2.81663733e-06
Iter: 1599 loss: 2.81058101e-06
Iter: 1600 loss: 2.8089712e-06
Iter: 1601 loss: 2.82338624e-06
Iter: 1602 loss: 2.80883978e-06
Iter: 1603 loss: 2.80753466e-06
Iter: 1604 loss: 2.82085239e-06
Iter: 1605 loss: 2.80753216e-06
Iter: 1606 loss: 2.80685936e-06
Iter: 1607 loss: 2.80654285e-06
Iter: 1608 loss: 2.80629979e-06
Iter: 1609 loss: 2.80516952e-06
Iter: 1610 loss: 2.80522499e-06
Iter: 1611 loss: 2.80414633e-06
Iter: 1612 loss: 2.80307313e-06
Iter: 1613 loss: 2.80575841e-06
Iter: 1614 loss: 2.80263066e-06
Iter: 1615 loss: 2.80122367e-06
Iter: 1616 loss: 2.80125369e-06
Iter: 1617 loss: 2.80012205e-06
Iter: 1618 loss: 2.79961796e-06
Iter: 1619 loss: 2.79923279e-06
Iter: 1620 loss: 2.79854839e-06
Iter: 1621 loss: 2.79751362e-06
Iter: 1622 loss: 2.79742858e-06
Iter: 1623 loss: 2.79611413e-06
Iter: 1624 loss: 2.79950564e-06
Iter: 1625 loss: 2.79571441e-06
Iter: 1626 loss: 2.79459482e-06
Iter: 1627 loss: 2.80150834e-06
Iter: 1628 loss: 2.79448113e-06
Iter: 1629 loss: 2.79332198e-06
Iter: 1630 loss: 2.79253982e-06
Iter: 1631 loss: 2.79216306e-06
Iter: 1632 loss: 2.79070423e-06
Iter: 1633 loss: 2.7982212e-06
Iter: 1634 loss: 2.79052961e-06
Iter: 1635 loss: 2.78930793e-06
Iter: 1636 loss: 2.79099186e-06
Iter: 1637 loss: 2.7886349e-06
Iter: 1638 loss: 2.78769971e-06
Iter: 1639 loss: 2.7953788e-06
Iter: 1640 loss: 2.78766902e-06
Iter: 1641 loss: 2.78647826e-06
Iter: 1642 loss: 2.79035248e-06
Iter: 1643 loss: 2.7860774e-06
Iter: 1644 loss: 2.78516313e-06
Iter: 1645 loss: 2.78766447e-06
Iter: 1646 loss: 2.78483026e-06
Iter: 1647 loss: 2.784137e-06
Iter: 1648 loss: 2.78425705e-06
Iter: 1649 loss: 2.78358402e-06
Iter: 1650 loss: 2.78237985e-06
Iter: 1651 loss: 2.78175185e-06
Iter: 1652 loss: 2.78122889e-06
Iter: 1653 loss: 2.77952108e-06
Iter: 1654 loss: 2.7810438e-06
Iter: 1655 loss: 2.77856634e-06
Iter: 1656 loss: 2.77764047e-06
Iter: 1657 loss: 2.77740946e-06
Iter: 1658 loss: 2.77646768e-06
Iter: 1659 loss: 2.77821573e-06
Iter: 1660 loss: 2.77609433e-06
Iter: 1661 loss: 2.77497838e-06
Iter: 1662 loss: 2.77524e-06
Iter: 1663 loss: 2.77415484e-06
Iter: 1664 loss: 2.7730066e-06
Iter: 1665 loss: 2.77880326e-06
Iter: 1666 loss: 2.77276058e-06
Iter: 1667 loss: 2.77143477e-06
Iter: 1668 loss: 2.77113259e-06
Iter: 1669 loss: 2.77032041e-06
Iter: 1670 loss: 2.76896753e-06
Iter: 1671 loss: 2.77664503e-06
Iter: 1672 loss: 2.76889841e-06
Iter: 1673 loss: 2.76766332e-06
Iter: 1674 loss: 2.76801347e-06
Iter: 1675 loss: 2.7667852e-06
Iter: 1676 loss: 2.76622222e-06
Iter: 1677 loss: 2.76585024e-06
Iter: 1678 loss: 2.76517881e-06
Iter: 1679 loss: 2.76409969e-06
Iter: 1680 loss: 2.79305596e-06
Iter: 1681 loss: 2.76408946e-06
Iter: 1682 loss: 2.76252331e-06
Iter: 1683 loss: 2.7676731e-06
Iter: 1684 loss: 2.76207265e-06
Iter: 1685 loss: 2.76090759e-06
Iter: 1686 loss: 2.7630058e-06
Iter: 1687 loss: 2.76040259e-06
Iter: 1688 loss: 2.75932462e-06
Iter: 1689 loss: 2.75896127e-06
Iter: 1690 loss: 2.75826915e-06
Iter: 1691 loss: 2.75696902e-06
Iter: 1692 loss: 2.76500259e-06
Iter: 1693 loss: 2.7567603e-06
Iter: 1694 loss: 2.7556257e-06
Iter: 1695 loss: 2.76744345e-06
Iter: 1696 loss: 2.75564366e-06
Iter: 1697 loss: 2.75461616e-06
Iter: 1698 loss: 2.75589446e-06
Iter: 1699 loss: 2.75410252e-06
Iter: 1700 loss: 2.75311754e-06
Iter: 1701 loss: 2.75244065e-06
Iter: 1702 loss: 2.75213642e-06
Iter: 1703 loss: 2.75034154e-06
Iter: 1704 loss: 2.76232822e-06
Iter: 1705 loss: 2.75015509e-06
Iter: 1706 loss: 2.74917466e-06
Iter: 1707 loss: 2.7483793e-06
Iter: 1708 loss: 2.7480919e-06
Iter: 1709 loss: 2.74583181e-06
Iter: 1710 loss: 2.74952617e-06
Iter: 1711 loss: 2.74479726e-06
Iter: 1712 loss: 2.74468948e-06
Iter: 1713 loss: 2.7441356e-06
Iter: 1714 loss: 2.74355466e-06
Iter: 1715 loss: 2.74290846e-06
Iter: 1716 loss: 2.74266176e-06
Iter: 1717 loss: 2.74169e-06
Iter: 1718 loss: 2.74381023e-06
Iter: 1719 loss: 2.74125819e-06
Iter: 1720 loss: 2.74010699e-06
Iter: 1721 loss: 2.74002741e-06
Iter: 1722 loss: 2.7390231e-06
Iter: 1723 loss: 2.73752039e-06
Iter: 1724 loss: 2.74323929e-06
Iter: 1725 loss: 2.73720161e-06
Iter: 1726 loss: 2.73611886e-06
Iter: 1727 loss: 2.73458318e-06
Iter: 1728 loss: 2.73445175e-06
Iter: 1729 loss: 2.73384171e-06
Iter: 1730 loss: 2.73338833e-06
Iter: 1731 loss: 2.73238e-06
Iter: 1732 loss: 2.73394676e-06
Iter: 1733 loss: 2.73195519e-06
Iter: 1734 loss: 2.73095065e-06
Iter: 1735 loss: 2.73044134e-06
Iter: 1736 loss: 2.72990928e-06
Iter: 1737 loss: 2.72861575e-06
Iter: 1738 loss: 2.7409651e-06
Iter: 1739 loss: 2.72858983e-06
Iter: 1740 loss: 2.72753437e-06
Iter: 1741 loss: 2.72712714e-06
Iter: 1742 loss: 2.72657212e-06
Iter: 1743 loss: 2.72515172e-06
Iter: 1744 loss: 2.73109777e-06
Iter: 1745 loss: 2.72492207e-06
Iter: 1746 loss: 2.72370676e-06
Iter: 1747 loss: 2.72846682e-06
Iter: 1748 loss: 2.72343186e-06
Iter: 1749 loss: 2.72213e-06
Iter: 1750 loss: 2.73215528e-06
Iter: 1751 loss: 2.72199668e-06
Iter: 1752 loss: 2.72132547e-06
Iter: 1753 loss: 2.72093621e-06
Iter: 1754 loss: 2.72070065e-06
Iter: 1755 loss: 2.71929525e-06
Iter: 1756 loss: 2.71911e-06
Iter: 1757 loss: 2.71815748e-06
Iter: 1758 loss: 2.71633098e-06
Iter: 1759 loss: 2.72513648e-06
Iter: 1760 loss: 2.71604767e-06
Iter: 1761 loss: 2.71499812e-06
Iter: 1762 loss: 2.71343197e-06
Iter: 1763 loss: 2.71336148e-06
Iter: 1764 loss: 2.71131694e-06
Iter: 1765 loss: 2.72677721e-06
Iter: 1766 loss: 2.71118734e-06
Iter: 1767 loss: 2.70955866e-06
Iter: 1768 loss: 2.71497402e-06
Iter: 1769 loss: 2.70908731e-06
Iter: 1770 loss: 2.7079443e-06
Iter: 1771 loss: 2.72469697e-06
Iter: 1772 loss: 2.70793271e-06
Iter: 1773 loss: 2.70674263e-06
Iter: 1774 loss: 2.70678379e-06
Iter: 1775 loss: 2.70583951e-06
Iter: 1776 loss: 2.70482656e-06
Iter: 1777 loss: 2.7141291e-06
Iter: 1778 loss: 2.70474175e-06
Iter: 1779 loss: 2.70394435e-06
Iter: 1780 loss: 2.70281089e-06
Iter: 1781 loss: 2.70269857e-06
Iter: 1782 loss: 2.70114128e-06
Iter: 1783 loss: 2.71237354e-06
Iter: 1784 loss: 2.70103055e-06
Iter: 1785 loss: 2.70026294e-06
Iter: 1786 loss: 2.7002261e-06
Iter: 1787 loss: 2.69954398e-06
Iter: 1788 loss: 2.69798511e-06
Iter: 1789 loss: 2.7229562e-06
Iter: 1790 loss: 2.69796374e-06
Iter: 1791 loss: 2.69692282e-06
Iter: 1792 loss: 2.70445321e-06
Iter: 1793 loss: 2.69680231e-06
Iter: 1794 loss: 2.69586599e-06
Iter: 1795 loss: 2.69721272e-06
Iter: 1796 loss: 2.6953378e-06
Iter: 1797 loss: 2.69409657e-06
Iter: 1798 loss: 2.69462771e-06
Iter: 1799 loss: 2.69314069e-06
Iter: 1800 loss: 2.69166026e-06
Iter: 1801 loss: 2.69176826e-06
Iter: 1802 loss: 2.69040106e-06
Iter: 1803 loss: 2.68858389e-06
Iter: 1804 loss: 2.70316423e-06
Iter: 1805 loss: 2.68835515e-06
Iter: 1806 loss: 2.68696499e-06
Iter: 1807 loss: 2.68878784e-06
Iter: 1808 loss: 2.68610665e-06
Iter: 1809 loss: 2.68464646e-06
Iter: 1810 loss: 2.68463555e-06
Iter: 1811 loss: 2.68382473e-06
Iter: 1812 loss: 2.68356189e-06
Iter: 1813 loss: 2.6830437e-06
Iter: 1814 loss: 2.68167719e-06
Iter: 1815 loss: 2.68461508e-06
Iter: 1816 loss: 2.68106487e-06
Iter: 1817 loss: 2.68003305e-06
Iter: 1818 loss: 2.68349618e-06
Iter: 1819 loss: 2.67968858e-06
Iter: 1820 loss: 2.67869473e-06
Iter: 1821 loss: 2.68794611e-06
Iter: 1822 loss: 2.67873747e-06
Iter: 1823 loss: 2.67768633e-06
Iter: 1824 loss: 2.67679843e-06
Iter: 1825 loss: 2.67658447e-06
Iter: 1826 loss: 2.67545238e-06
Iter: 1827 loss: 2.67636233e-06
Iter: 1828 loss: 2.67489804e-06
Iter: 1829 loss: 2.67368546e-06
Iter: 1830 loss: 2.68029726e-06
Iter: 1831 loss: 2.67348514e-06
Iter: 1832 loss: 2.67228688e-06
Iter: 1833 loss: 2.67433893e-06
Iter: 1834 loss: 2.67172186e-06
Iter: 1835 loss: 2.67042606e-06
Iter: 1836 loss: 2.67006e-06
Iter: 1837 loss: 2.66929851e-06
Iter: 1838 loss: 2.66763186e-06
Iter: 1839 loss: 2.67075939e-06
Iter: 1840 loss: 2.66705229e-06
Iter: 1841 loss: 2.66494544e-06
Iter: 1842 loss: 2.67388782e-06
Iter: 1843 loss: 2.66460802e-06
Iter: 1844 loss: 2.66327061e-06
Iter: 1845 loss: 2.66331381e-06
Iter: 1846 loss: 2.66229836e-06
Iter: 1847 loss: 2.66067696e-06
Iter: 1848 loss: 2.66061556e-06
Iter: 1849 loss: 2.65920949e-06
Iter: 1850 loss: 2.67439464e-06
Iter: 1851 loss: 2.65910035e-06
Iter: 1852 loss: 2.65789595e-06
Iter: 1853 loss: 2.65735707e-06
Iter: 1854 loss: 2.65673134e-06
Iter: 1855 loss: 2.65567905e-06
Iter: 1856 loss: 2.65554331e-06
Iter: 1857 loss: 2.65452718e-06
Iter: 1858 loss: 2.65564472e-06
Iter: 1859 loss: 2.65402014e-06
Iter: 1860 loss: 2.6531211e-06
Iter: 1861 loss: 2.65193239e-06
Iter: 1862 loss: 2.65180552e-06
Iter: 1863 loss: 2.6504963e-06
Iter: 1864 loss: 2.66359098e-06
Iter: 1865 loss: 2.65046469e-06
Iter: 1866 loss: 2.64924665e-06
Iter: 1867 loss: 2.64985624e-06
Iter: 1868 loss: 2.64847517e-06
Iter: 1869 loss: 2.64675327e-06
Iter: 1870 loss: 2.64989376e-06
Iter: 1871 loss: 2.64591563e-06
Iter: 1872 loss: 2.64462e-06
Iter: 1873 loss: 2.64272012e-06
Iter: 1874 loss: 2.64268215e-06
Iter: 1875 loss: 2.64027335e-06
Iter: 1876 loss: 2.64029904e-06
Iter: 1877 loss: 2.63913876e-06
Iter: 1878 loss: 2.65497806e-06
Iter: 1879 loss: 2.63920492e-06
Iter: 1880 loss: 2.63825041e-06
Iter: 1881 loss: 2.63660104e-06
Iter: 1882 loss: 2.67782798e-06
Iter: 1883 loss: 2.6365808e-06
Iter: 1884 loss: 2.63509651e-06
Iter: 1885 loss: 2.6548712e-06
Iter: 1886 loss: 2.6350915e-06
Iter: 1887 loss: 2.63394122e-06
Iter: 1888 loss: 2.63337415e-06
Iter: 1889 loss: 2.63286529e-06
Iter: 1890 loss: 2.63209631e-06
Iter: 1891 loss: 2.63190032e-06
Iter: 1892 loss: 2.63110451e-06
Iter: 1893 loss: 2.62984349e-06
Iter: 1894 loss: 2.62983349e-06
Iter: 1895 loss: 2.62845242e-06
Iter: 1896 loss: 2.62997378e-06
Iter: 1897 loss: 2.62766389e-06
Iter: 1898 loss: 2.62646086e-06
Iter: 1899 loss: 2.63696529e-06
Iter: 1900 loss: 2.6264504e-06
Iter: 1901 loss: 2.62525919e-06
Iter: 1902 loss: 2.62507547e-06
Iter: 1903 loss: 2.62425806e-06
Iter: 1904 loss: 2.62255116e-06
Iter: 1905 loss: 2.62805497e-06
Iter: 1906 loss: 2.62198819e-06
Iter: 1907 loss: 2.62086633e-06
Iter: 1908 loss: 2.62103981e-06
Iter: 1909 loss: 2.61990226e-06
Iter: 1910 loss: 2.61826699e-06
Iter: 1911 loss: 2.6328737e-06
Iter: 1912 loss: 2.61826426e-06
Iter: 1913 loss: 2.61687319e-06
Iter: 1914 loss: 2.62594926e-06
Iter: 1915 loss: 2.61676541e-06
Iter: 1916 loss: 2.61575133e-06
Iter: 1917 loss: 2.61512287e-06
Iter: 1918 loss: 2.61472951e-06
Iter: 1919 loss: 2.61350669e-06
Iter: 1920 loss: 2.62258709e-06
Iter: 1921 loss: 2.6134262e-06
Iter: 1922 loss: 2.61216701e-06
Iter: 1923 loss: 2.61151e-06
Iter: 1924 loss: 2.6109019e-06
Iter: 1925 loss: 2.60990601e-06
Iter: 1926 loss: 2.60970046e-06
Iter: 1927 loss: 2.60915135e-06
Iter: 1928 loss: 2.60785782e-06
Iter: 1929 loss: 2.62137e-06
Iter: 1930 loss: 2.6076882e-06
Iter: 1931 loss: 2.60577099e-06
Iter: 1932 loss: 2.60719366e-06
Iter: 1933 loss: 2.60462298e-06
Iter: 1934 loss: 2.60301317e-06
Iter: 1935 loss: 2.60304159e-06
Iter: 1936 loss: 2.60189017e-06
Iter: 1937 loss: 2.60077923e-06
Iter: 1938 loss: 2.60053093e-06
Iter: 1939 loss: 2.59855551e-06
Iter: 1940 loss: 2.60503452e-06
Iter: 1941 loss: 2.59804165e-06
Iter: 1942 loss: 2.59632498e-06
Iter: 1943 loss: 2.59701596e-06
Iter: 1944 loss: 2.59523677e-06
Iter: 1945 loss: 2.594031e-06
Iter: 1946 loss: 2.59395847e-06
Iter: 1947 loss: 2.59280978e-06
Iter: 1948 loss: 2.59382023e-06
Iter: 1949 loss: 2.59230364e-06
Iter: 1950 loss: 2.59099806e-06
Iter: 1951 loss: 2.59079206e-06
Iter: 1952 loss: 2.59001308e-06
Iter: 1953 loss: 2.58870023e-06
Iter: 1954 loss: 2.60509796e-06
Iter: 1955 loss: 2.58864202e-06
Iter: 1956 loss: 2.58755426e-06
Iter: 1957 loss: 2.58887167e-06
Iter: 1958 loss: 2.58699038e-06
Iter: 1959 loss: 2.58526597e-06
Iter: 1960 loss: 2.59089643e-06
Iter: 1961 loss: 2.58471573e-06
Iter: 1962 loss: 2.58391901e-06
Iter: 1963 loss: 2.58294631e-06
Iter: 1964 loss: 2.58279715e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2
+ date
Sun Nov  8 16:04:11 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e536610d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53692a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53622598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e535e0bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e535c4268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e535bebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53471598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53471c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53559840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53559048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53520a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e534081e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53408d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53461730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53445d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53445620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5336e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fe84bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fe84b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e533bdea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e533bd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e533366a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdfb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdfb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdfb0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fe26ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdab8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdc4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdc42f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdc4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e082be7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e082bb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e082bb158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e082bb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e0827e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e081e5158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.6537589
test_loss: 0.6610983
train_loss: 0.56145936
test_loss: 0.55696493
train_loss: 0.39342114
test_loss: 0.38174295
train_loss: 0.20965791
test_loss: 0.214746
train_loss: 0.13717462
test_loss: 0.13476539
train_loss: 0.10763669
test_loss: 0.10500314
train_loss: 0.08613783
test_loss: 0.09595807
train_loss: 0.091599435
test_loss: 0.09288103
train_loss: 0.09224199
test_loss: 0.09155022
train_loss: 0.09556568
test_loss: 0.09060601
train_loss: 0.08582176
test_loss: 0.09018667
train_loss: 0.08899883
test_loss: 0.08954905
train_loss: 0.08178493
test_loss: 0.08932416
train_loss: 0.090852626
test_loss: 0.08875749
train_loss: 0.085421726
test_loss: 0.08855748
train_loss: 0.090917856
test_loss: 0.0878807
train_loss: 0.085175924
test_loss: 0.08734331
train_loss: 0.08163319
test_loss: 0.08699969
train_loss: 0.08525427
test_loss: 0.086642385
train_loss: 0.08127643
test_loss: 0.08598557
train_loss: 0.08328847
test_loss: 0.08536919
train_loss: 0.084068336
test_loss: 0.08466671
train_loss: 0.0792915
test_loss: 0.08371325
train_loss: 0.07872146
test_loss: 0.0830693
train_loss: 0.08066958
test_loss: 0.082016535
train_loss: 0.07425432
test_loss: 0.080990285
train_loss: 0.075029075
test_loss: 0.0796894
train_loss: 0.08584178
test_loss: 0.07851884
train_loss: 0.07207014
test_loss: 0.07690865
train_loss: 0.07442029
test_loss: 0.0752054
train_loss: 0.06958669
test_loss: 0.0732296
train_loss: 0.06330086
test_loss: 0.07097469
train_loss: 0.0686382
test_loss: 0.06932104
train_loss: 0.06563261
test_loss: 0.0668043
train_loss: 0.06291018
test_loss: 0.06458802
train_loss: 0.055906996
test_loss: 0.06270952
train_loss: 0.06205163
test_loss: 0.061015222
train_loss: 0.055826366
test_loss: 0.060106363
train_loss: 0.051825732
test_loss: 0.05911872
train_loss: 0.059004113
test_loss: 0.057741955
train_loss: 0.05199025
test_loss: 0.056835096
train_loss: 0.05561483
test_loss: 0.056723103
train_loss: 0.050882436
test_loss: 0.05601063
train_loss: 0.05492601
test_loss: 0.05499972
train_loss: 0.051588107
test_loss: 0.05522662
train_loss: 0.04819636
test_loss: 0.053918414
train_loss: 0.047663864
test_loss: 0.053734884
train_loss: 0.04767447
test_loss: 0.052731518
train_loss: 0.0460872
test_loss: 0.05123895
train_loss: 0.043525897
test_loss: 0.0511788
train_loss: 0.046866603
test_loss: 0.051062215
train_loss: 0.04591631
test_loss: 0.050613116
train_loss: 0.04762602
test_loss: 0.050537683
train_loss: 0.052772
test_loss: 0.055759743
train_loss: 0.0404984
test_loss: 0.048708502
train_loss: 0.041647665
test_loss: 0.048939813
train_loss: 0.040250745
test_loss: 0.048073027
train_loss: 0.044310372
test_loss: 0.04628696
train_loss: 0.039218374
test_loss: 0.045833677
train_loss: 0.04274359
test_loss: 0.04716038
train_loss: 0.036707833
test_loss: 0.045000035
train_loss: 0.042891476
test_loss: 0.046697114
train_loss: 0.05565113
test_loss: 0.057792604
train_loss: 0.039892104
test_loss: 0.045901038
train_loss: 0.042281974
test_loss: 0.04389049
train_loss: 0.03641046
test_loss: 0.043378286
train_loss: 0.034991562
test_loss: 0.041917566
train_loss: 0.035978254
test_loss: 0.041599207
train_loss: 0.032584526
test_loss: 0.039923828
train_loss: 0.03627246
test_loss: 0.042079277
train_loss: 0.03333759
test_loss: 0.038990684
train_loss: 0.031266686
test_loss: 0.038836278
train_loss: 0.028935041
test_loss: 0.03711207
train_loss: 0.030989008
test_loss: 0.036426052
train_loss: 0.0308114
test_loss: 0.038984258
train_loss: 0.032290358
test_loss: 0.03791695
train_loss: 0.030992849
test_loss: 0.03604621
train_loss: 0.0385143
test_loss: 0.035058163
train_loss: 0.02752202
test_loss: 0.033653438
train_loss: 0.02751996
test_loss: 0.03327996
train_loss: 0.027037635
test_loss: 0.031984884
train_loss: 0.02839782
test_loss: 0.032060787
train_loss: 0.02598431
test_loss: 0.03126104
train_loss: 0.027160045
test_loss: 0.029875845
train_loss: 0.02472438
test_loss: 0.030771933
train_loss: 0.02613867
test_loss: 0.02894462
train_loss: 0.024923056
test_loss: 0.030503733
train_loss: 0.027081478
test_loss: 0.029349418
train_loss: 0.023514427
test_loss: 0.029195115
train_loss: 0.022007208
test_loss: 0.02770924
train_loss: 0.023116004
test_loss: 0.027061371
train_loss: 0.023083355
test_loss: 0.02757444
train_loss: 0.019792182
test_loss: 0.026757434
train_loss: 0.022789497
test_loss: 0.025288733
train_loss: 0.02422043
test_loss: 0.025176218
train_loss: 0.02073208
test_loss: 0.025168678
train_loss: 0.020368088
test_loss: 0.023691721
train_loss: 0.019714031
test_loss: 0.024549752
train_loss: 0.019358277
test_loss: 0.023332125
train_loss: 0.018968333
test_loss: 0.023294652
train_loss: 0.020728536
test_loss: 0.023037706
train_loss: 0.01932754
test_loss: 0.023043137
train_loss: 0.018473797
test_loss: 0.022657068
train_loss: 0.019800493
test_loss: 0.023059452
train_loss: 0.01743267
test_loss: 0.022715803
train_loss: 0.019774463
test_loss: 0.020831885
train_loss: 0.015610364
test_loss: 0.021333927
train_loss: 0.016340265
test_loss: 0.02037909
train_loss: 0.016724037
test_loss: 0.020794073
train_loss: 0.016319763
test_loss: 0.019980023
train_loss: 0.015624826
test_loss: 0.019620137
train_loss: 0.015906518
test_loss: 0.019970862
train_loss: 0.016422369
test_loss: 0.019931575
train_loss: 0.017411474
test_loss: 0.019854093
train_loss: 0.0152088795
test_loss: 0.019101812
train_loss: 0.017279968
test_loss: 0.019705405
train_loss: 0.015828958
test_loss: 0.018733732
train_loss: 0.017493634
test_loss: 0.019885892
train_loss: 0.0154355485
test_loss: 0.019788856
train_loss: 0.013583392
test_loss: 0.017885284
train_loss: 0.015360996
test_loss: 0.018559474
train_loss: 0.013867332
test_loss: 0.018714353
train_loss: 0.013010089
test_loss: 0.01734427
train_loss: 0.014442053
test_loss: 0.018290535
train_loss: 0.013733399
test_loss: 0.017677654
train_loss: 0.013647558
test_loss: 0.017765312
train_loss: 0.013477667
test_loss: 0.017846616
train_loss: 0.013593277
test_loss: 0.018571192
train_loss: 0.013594875
test_loss: 0.017410018
train_loss: 0.013400808
test_loss: 0.017575782
train_loss: 0.013967983
test_loss: 0.01810052
train_loss: 0.013354258
test_loss: 0.016948013
train_loss: 0.01366664
test_loss: 0.017325968
train_loss: 0.013868055
test_loss: 0.017529085
train_loss: 0.014630368
test_loss: 0.01749697
train_loss: 0.012643831
test_loss: 0.016776575
train_loss: 0.012417924
test_loss: 0.016334036
train_loss: 0.013328214
test_loss: 0.017342165
train_loss: 0.012604717
test_loss: 0.016480874
train_loss: 0.013752645
test_loss: 0.016554013
train_loss: 0.013029281
test_loss: 0.016621092
train_loss: 0.014357902
test_loss: 0.01694991
train_loss: 0.012937234
test_loss: 0.016304793
train_loss: 0.012649645
test_loss: 0.016926082
train_loss: 0.014930933
test_loss: 0.018158268
train_loss: 0.012266951
test_loss: 0.016563084
train_loss: 0.011498433
test_loss: 0.015840383
train_loss: 0.013869674
test_loss: 0.016130438
train_loss: 0.014994868
test_loss: 0.016943827
train_loss: 0.013386246
test_loss: 0.016106008
train_loss: 0.011941971
test_loss: 0.015915388
train_loss: 0.01170678
test_loss: 0.016753092
train_loss: 0.014887506
test_loss: 0.015981112
train_loss: 0.011466848
test_loss: 0.015743474
train_loss: 0.013187163
test_loss: 0.01588329
train_loss: 0.010954335
test_loss: 0.015228816
train_loss: 0.012114881
test_loss: 0.015663976
train_loss: 0.01301728
test_loss: 0.016326118
train_loss: 0.011645961
test_loss: 0.01591199
train_loss: 0.011483889
test_loss: 0.016092615
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76671e4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76671e4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76672cee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7667207ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7667222268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7667222d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f766713df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7667108ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7667108b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f766712e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76670e3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7667091400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76670aef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f76670aee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7667001ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f766706c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f766702f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7666fe2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7666fbae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7666fb0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7666f429d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7666effbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7666f20510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7666edc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7666edcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7666e75620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7666e4e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7666e68598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7666e68268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f765f47aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f765f4239d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f765f444378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f765f444048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f765f3f0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f765f3f0bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f765f395510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000340220518
Iter: 2 loss: 0.016489055
Iter: 3 loss: 0.000315732439
Iter: 4 loss: 0.000249811535
Iter: 5 loss: 0.000246085401
Iter: 6 loss: 0.000230278849
Iter: 7 loss: 0.000457730377
Iter: 8 loss: 0.000230234
Iter: 9 loss: 0.00021635789
Iter: 10 loss: 0.000189592669
Iter: 11 loss: 0.000677868084
Iter: 12 loss: 0.000189386672
Iter: 13 loss: 0.00017467729
Iter: 14 loss: 0.000217161316
Iter: 15 loss: 0.00017002871
Iter: 16 loss: 0.000157442642
Iter: 17 loss: 0.000199073635
Iter: 18 loss: 0.000154007081
Iter: 19 loss: 0.000141912795
Iter: 20 loss: 0.000186067395
Iter: 21 loss: 0.000138884032
Iter: 22 loss: 0.000131282053
Iter: 23 loss: 0.000149506188
Iter: 24 loss: 0.000128505606
Iter: 25 loss: 0.000120754848
Iter: 26 loss: 0.000127655294
Iter: 27 loss: 0.000116229443
Iter: 28 loss: 0.000110434543
Iter: 29 loss: 0.000139609736
Iter: 30 loss: 0.000109481516
Iter: 31 loss: 0.000104694904
Iter: 32 loss: 0.00011679616
Iter: 33 loss: 0.00010302577
Iter: 34 loss: 9.85653387e-05
Iter: 35 loss: 0.000111468515
Iter: 36 loss: 9.71747795e-05
Iter: 37 loss: 9.27927758e-05
Iter: 38 loss: 0.000102870727
Iter: 39 loss: 9.11535244e-05
Iter: 40 loss: 8.78823776e-05
Iter: 41 loss: 0.000132635934
Iter: 42 loss: 8.78696592e-05
Iter: 43 loss: 8.62461748e-05
Iter: 44 loss: 0.000111748079
Iter: 45 loss: 8.6247237e-05
Iter: 46 loss: 8.46888652e-05
Iter: 47 loss: 8.52400844e-05
Iter: 48 loss: 8.35980172e-05
Iter: 49 loss: 8.22318543e-05
Iter: 50 loss: 8.0277925e-05
Iter: 51 loss: 8.02123832e-05
Iter: 52 loss: 7.81468334e-05
Iter: 53 loss: 0.000104366241
Iter: 54 loss: 7.81305353e-05
Iter: 55 loss: 7.62899435e-05
Iter: 56 loss: 7.70200568e-05
Iter: 57 loss: 7.50147883e-05
Iter: 58 loss: 7.30141401e-05
Iter: 59 loss: 8.1979204e-05
Iter: 60 loss: 7.26221e-05
Iter: 61 loss: 7.10473905e-05
Iter: 62 loss: 7.41730619e-05
Iter: 63 loss: 7.04015547e-05
Iter: 64 loss: 6.90262823e-05
Iter: 65 loss: 6.84609695e-05
Iter: 66 loss: 6.7735069e-05
Iter: 67 loss: 6.60741352e-05
Iter: 68 loss: 8.24866729e-05
Iter: 69 loss: 6.60183941e-05
Iter: 70 loss: 6.48196292e-05
Iter: 71 loss: 6.65582629e-05
Iter: 72 loss: 6.42375962e-05
Iter: 73 loss: 6.28745693e-05
Iter: 74 loss: 6.88077635e-05
Iter: 75 loss: 6.25967732e-05
Iter: 76 loss: 6.15864774e-05
Iter: 77 loss: 6.44607208e-05
Iter: 78 loss: 6.12674266e-05
Iter: 79 loss: 6.09428571e-05
Iter: 80 loss: 6.06757385e-05
Iter: 81 loss: 6.03690423e-05
Iter: 82 loss: 5.9537153e-05
Iter: 83 loss: 6.49467911e-05
Iter: 84 loss: 5.93267105e-05
Iter: 85 loss: 5.82754619e-05
Iter: 86 loss: 6.28215639e-05
Iter: 87 loss: 5.80607775e-05
Iter: 88 loss: 5.72675708e-05
Iter: 89 loss: 6.41792067e-05
Iter: 90 loss: 5.72255958e-05
Iter: 91 loss: 5.66886774e-05
Iter: 92 loss: 5.71356577e-05
Iter: 93 loss: 5.63701833e-05
Iter: 94 loss: 5.55898077e-05
Iter: 95 loss: 5.6534991e-05
Iter: 96 loss: 5.51805788e-05
Iter: 97 loss: 5.45655e-05
Iter: 98 loss: 5.48562712e-05
Iter: 99 loss: 5.41519694e-05
Iter: 100 loss: 5.32604463e-05
Iter: 101 loss: 5.89777628e-05
Iter: 102 loss: 5.3162039e-05
Iter: 103 loss: 5.26190415e-05
Iter: 104 loss: 5.2577012e-05
Iter: 105 loss: 5.21713882e-05
Iter: 106 loss: 5.13738196e-05
Iter: 107 loss: 5.47826603e-05
Iter: 108 loss: 5.12077168e-05
Iter: 109 loss: 5.06407669e-05
Iter: 110 loss: 5.36323787e-05
Iter: 111 loss: 5.05536227e-05
Iter: 112 loss: 5.014622e-05
Iter: 113 loss: 5.44082322e-05
Iter: 114 loss: 5.01363684e-05
Iter: 115 loss: 4.96934081e-05
Iter: 116 loss: 5.10872924e-05
Iter: 117 loss: 4.95646054e-05
Iter: 118 loss: 4.93451735e-05
Iter: 119 loss: 4.88902951e-05
Iter: 120 loss: 5.67789393e-05
Iter: 121 loss: 4.88810329e-05
Iter: 122 loss: 4.83430558e-05
Iter: 123 loss: 5.22869923e-05
Iter: 124 loss: 4.82982941e-05
Iter: 125 loss: 4.78624279e-05
Iter: 126 loss: 4.96931752e-05
Iter: 127 loss: 4.77696667e-05
Iter: 128 loss: 4.74501794e-05
Iter: 129 loss: 4.80077069e-05
Iter: 130 loss: 4.73107939e-05
Iter: 131 loss: 4.69101797e-05
Iter: 132 loss: 4.72822903e-05
Iter: 133 loss: 4.66795318e-05
Iter: 134 loss: 4.63165561e-05
Iter: 135 loss: 4.66158e-05
Iter: 136 loss: 4.6098754e-05
Iter: 137 loss: 4.56256967e-05
Iter: 138 loss: 4.85155688e-05
Iter: 139 loss: 4.55689878e-05
Iter: 140 loss: 4.52159074e-05
Iter: 141 loss: 4.56678463e-05
Iter: 142 loss: 4.50360458e-05
Iter: 143 loss: 4.45746518e-05
Iter: 144 loss: 4.52427084e-05
Iter: 145 loss: 4.43502613e-05
Iter: 146 loss: 4.3960732e-05
Iter: 147 loss: 4.56956259e-05
Iter: 148 loss: 4.38832831e-05
Iter: 149 loss: 4.36811133e-05
Iter: 150 loss: 4.36373375e-05
Iter: 151 loss: 4.3469041e-05
Iter: 152 loss: 4.32255911e-05
Iter: 153 loss: 4.32177185e-05
Iter: 154 loss: 4.3007989e-05
Iter: 155 loss: 4.2998854e-05
Iter: 156 loss: 4.28372732e-05
Iter: 157 loss: 4.25361177e-05
Iter: 158 loss: 4.46422746e-05
Iter: 159 loss: 4.25090911e-05
Iter: 160 loss: 4.22643097e-05
Iter: 161 loss: 4.33319437e-05
Iter: 162 loss: 4.22153134e-05
Iter: 163 loss: 4.20260403e-05
Iter: 164 loss: 4.19360513e-05
Iter: 165 loss: 4.1843683e-05
Iter: 166 loss: 4.15216055e-05
Iter: 167 loss: 4.29849206e-05
Iter: 168 loss: 4.14596216e-05
Iter: 169 loss: 4.12112786e-05
Iter: 170 loss: 4.1020834e-05
Iter: 171 loss: 4.0941628e-05
Iter: 172 loss: 4.06403306e-05
Iter: 173 loss: 4.3813503e-05
Iter: 174 loss: 4.06333056e-05
Iter: 175 loss: 4.0370589e-05
Iter: 176 loss: 4.0680774e-05
Iter: 177 loss: 4.02322075e-05
Iter: 178 loss: 3.99451528e-05
Iter: 179 loss: 4.05353749e-05
Iter: 180 loss: 3.9829647e-05
Iter: 181 loss: 3.96371652e-05
Iter: 182 loss: 4.24026366e-05
Iter: 183 loss: 3.96364921e-05
Iter: 184 loss: 3.94209565e-05
Iter: 185 loss: 4.0183073e-05
Iter: 186 loss: 3.93654918e-05
Iter: 187 loss: 3.92412621e-05
Iter: 188 loss: 3.90621499e-05
Iter: 189 loss: 3.90567438e-05
Iter: 190 loss: 3.88513581e-05
Iter: 191 loss: 3.95905226e-05
Iter: 192 loss: 3.88000553e-05
Iter: 193 loss: 3.86205174e-05
Iter: 194 loss: 3.98549491e-05
Iter: 195 loss: 3.86034335e-05
Iter: 196 loss: 3.8434715e-05
Iter: 197 loss: 3.85877574e-05
Iter: 198 loss: 3.83362858e-05
Iter: 199 loss: 3.81577556e-05
Iter: 200 loss: 3.84936502e-05
Iter: 201 loss: 3.80816055e-05
Iter: 202 loss: 3.78750956e-05
Iter: 203 loss: 3.84016748e-05
Iter: 204 loss: 3.78037803e-05
Iter: 205 loss: 3.75970267e-05
Iter: 206 loss: 3.77658798e-05
Iter: 207 loss: 3.74734955e-05
Iter: 208 loss: 3.72794493e-05
Iter: 209 loss: 3.77987599e-05
Iter: 210 loss: 3.72146387e-05
Iter: 211 loss: 3.69771078e-05
Iter: 212 loss: 3.79144622e-05
Iter: 213 loss: 3.69224e-05
Iter: 214 loss: 3.67699249e-05
Iter: 215 loss: 3.7435897e-05
Iter: 216 loss: 3.67394714e-05
Iter: 217 loss: 3.66443928e-05
Iter: 218 loss: 3.66437453e-05
Iter: 219 loss: 3.65315027e-05
Iter: 220 loss: 3.63882791e-05
Iter: 221 loss: 3.63777799e-05
Iter: 222 loss: 3.62763676e-05
Iter: 223 loss: 3.62069768e-05
Iter: 224 loss: 3.61694547e-05
Iter: 225 loss: 3.60214435e-05
Iter: 226 loss: 3.7454618e-05
Iter: 227 loss: 3.6014615e-05
Iter: 228 loss: 3.58993566e-05
Iter: 229 loss: 3.62595383e-05
Iter: 230 loss: 3.58653e-05
Iter: 231 loss: 3.57284662e-05
Iter: 232 loss: 3.57702884e-05
Iter: 233 loss: 3.56304226e-05
Iter: 234 loss: 3.54954827e-05
Iter: 235 loss: 3.61477105e-05
Iter: 236 loss: 3.54714139e-05
Iter: 237 loss: 3.53429677e-05
Iter: 238 loss: 3.54379772e-05
Iter: 239 loss: 3.52641073e-05
Iter: 240 loss: 3.50967675e-05
Iter: 241 loss: 3.55747143e-05
Iter: 242 loss: 3.50438495e-05
Iter: 243 loss: 3.49215e-05
Iter: 244 loss: 3.49250222e-05
Iter: 245 loss: 3.4824563e-05
Iter: 246 loss: 3.46505221e-05
Iter: 247 loss: 3.62680476e-05
Iter: 248 loss: 3.46428e-05
Iter: 249 loss: 3.45337867e-05
Iter: 250 loss: 3.49439069e-05
Iter: 251 loss: 3.45072767e-05
Iter: 252 loss: 3.44289547e-05
Iter: 253 loss: 3.44258297e-05
Iter: 254 loss: 3.43784304e-05
Iter: 255 loss: 3.42491403e-05
Iter: 256 loss: 3.50703e-05
Iter: 257 loss: 3.42163548e-05
Iter: 258 loss: 3.40766e-05
Iter: 259 loss: 3.42919302e-05
Iter: 260 loss: 3.40103543e-05
Iter: 261 loss: 3.38947793e-05
Iter: 262 loss: 3.38946702e-05
Iter: 263 loss: 3.38086393e-05
Iter: 264 loss: 3.4078832e-05
Iter: 265 loss: 3.37841411e-05
Iter: 266 loss: 3.36950397e-05
Iter: 267 loss: 3.36409175e-05
Iter: 268 loss: 3.36048324e-05
Iter: 269 loss: 3.34798788e-05
Iter: 270 loss: 3.42709718e-05
Iter: 271 loss: 3.34658143e-05
Iter: 272 loss: 3.33697026e-05
Iter: 273 loss: 3.34483084e-05
Iter: 274 loss: 3.33124335e-05
Iter: 275 loss: 3.3189448e-05
Iter: 276 loss: 3.36926605e-05
Iter: 277 loss: 3.31628253e-05
Iter: 278 loss: 3.30735238e-05
Iter: 279 loss: 3.29854e-05
Iter: 280 loss: 3.29673494e-05
Iter: 281 loss: 3.28390524e-05
Iter: 282 loss: 3.46661909e-05
Iter: 283 loss: 3.28387141e-05
Iter: 284 loss: 3.27657399e-05
Iter: 285 loss: 3.34781798e-05
Iter: 286 loss: 3.27628331e-05
Iter: 287 loss: 3.26763657e-05
Iter: 288 loss: 3.27760572e-05
Iter: 289 loss: 3.26298505e-05
Iter: 290 loss: 3.25575711e-05
Iter: 291 loss: 3.25053261e-05
Iter: 292 loss: 3.24800349e-05
Iter: 293 loss: 3.2383552e-05
Iter: 294 loss: 3.23457862e-05
Iter: 295 loss: 3.22939377e-05
Iter: 296 loss: 3.22059786e-05
Iter: 297 loss: 3.22055821e-05
Iter: 298 loss: 3.21172192e-05
Iter: 299 loss: 3.2259697e-05
Iter: 300 loss: 3.20759864e-05
Iter: 301 loss: 3.19972351e-05
Iter: 302 loss: 3.20802792e-05
Iter: 303 loss: 3.19540268e-05
Iter: 304 loss: 3.18524144e-05
Iter: 305 loss: 3.2165266e-05
Iter: 306 loss: 3.18229286e-05
Iter: 307 loss: 3.17408194e-05
Iter: 308 loss: 3.19765531e-05
Iter: 309 loss: 3.17146114e-05
Iter: 310 loss: 3.16211881e-05
Iter: 311 loss: 3.16840851e-05
Iter: 312 loss: 3.15623838e-05
Iter: 313 loss: 3.1458716e-05
Iter: 314 loss: 3.19320825e-05
Iter: 315 loss: 3.14395402e-05
Iter: 316 loss: 3.13646524e-05
Iter: 317 loss: 3.1470161e-05
Iter: 318 loss: 3.13276541e-05
Iter: 319 loss: 3.12912234e-05
Iter: 320 loss: 3.1274707e-05
Iter: 321 loss: 3.12327247e-05
Iter: 322 loss: 3.11728691e-05
Iter: 323 loss: 3.11711592e-05
Iter: 324 loss: 3.11020558e-05
Iter: 325 loss: 3.10866162e-05
Iter: 326 loss: 3.10418545e-05
Iter: 327 loss: 3.09563766e-05
Iter: 328 loss: 3.11152689e-05
Iter: 329 loss: 3.09201714e-05
Iter: 330 loss: 3.08236558e-05
Iter: 331 loss: 3.11743e-05
Iter: 332 loss: 3.07989467e-05
Iter: 333 loss: 3.07173468e-05
Iter: 334 loss: 3.1861382e-05
Iter: 335 loss: 3.07174378e-05
Iter: 336 loss: 3.06712027e-05
Iter: 337 loss: 3.06079382e-05
Iter: 338 loss: 3.06053116e-05
Iter: 339 loss: 3.05157646e-05
Iter: 340 loss: 3.0846415e-05
Iter: 341 loss: 3.04941914e-05
Iter: 342 loss: 3.04211135e-05
Iter: 343 loss: 3.09543611e-05
Iter: 344 loss: 3.04155365e-05
Iter: 345 loss: 3.03614124e-05
Iter: 346 loss: 3.03178913e-05
Iter: 347 loss: 3.03019024e-05
Iter: 348 loss: 3.02310291e-05
Iter: 349 loss: 3.09823954e-05
Iter: 350 loss: 3.02300014e-05
Iter: 351 loss: 3.01729106e-05
Iter: 352 loss: 3.00949832e-05
Iter: 353 loss: 3.0091247e-05
Iter: 354 loss: 3.01619075e-05
Iter: 355 loss: 3.00527263e-05
Iter: 356 loss: 3.00299725e-05
Iter: 357 loss: 2.99750791e-05
Iter: 358 loss: 3.05740577e-05
Iter: 359 loss: 2.99693129e-05
Iter: 360 loss: 2.99049225e-05
Iter: 361 loss: 2.99624e-05
Iter: 362 loss: 2.98676168e-05
Iter: 363 loss: 2.97965398e-05
Iter: 364 loss: 3.01205982e-05
Iter: 365 loss: 2.97831575e-05
Iter: 366 loss: 2.97186925e-05
Iter: 367 loss: 2.97479946e-05
Iter: 368 loss: 2.96755989e-05
Iter: 369 loss: 2.96387989e-05
Iter: 370 loss: 2.96309445e-05
Iter: 371 loss: 2.95970058e-05
Iter: 372 loss: 2.9508099e-05
Iter: 373 loss: 3.01757682e-05
Iter: 374 loss: 2.9489931e-05
Iter: 375 loss: 2.94214824e-05
Iter: 376 loss: 2.94197453e-05
Iter: 377 loss: 2.93685425e-05
Iter: 378 loss: 2.93845733e-05
Iter: 379 loss: 2.93320863e-05
Iter: 380 loss: 2.92617024e-05
Iter: 381 loss: 2.95154205e-05
Iter: 382 loss: 2.92441164e-05
Iter: 383 loss: 2.91937649e-05
Iter: 384 loss: 2.92114237e-05
Iter: 385 loss: 2.91579308e-05
Iter: 386 loss: 2.90815788e-05
Iter: 387 loss: 2.95522459e-05
Iter: 388 loss: 2.90722928e-05
Iter: 389 loss: 2.90459793e-05
Iter: 390 loss: 2.90363532e-05
Iter: 391 loss: 2.90171211e-05
Iter: 392 loss: 2.89603959e-05
Iter: 393 loss: 2.91641045e-05
Iter: 394 loss: 2.89350628e-05
Iter: 395 loss: 2.88626634e-05
Iter: 396 loss: 2.93283101e-05
Iter: 397 loss: 2.88549963e-05
Iter: 398 loss: 2.88009396e-05
Iter: 399 loss: 2.89015879e-05
Iter: 400 loss: 2.87777566e-05
Iter: 401 loss: 2.87141556e-05
Iter: 402 loss: 2.88399424e-05
Iter: 403 loss: 2.86880295e-05
Iter: 404 loss: 2.86344548e-05
Iter: 405 loss: 2.93477297e-05
Iter: 406 loss: 2.86337545e-05
Iter: 407 loss: 2.85858441e-05
Iter: 408 loss: 2.85392853e-05
Iter: 409 loss: 2.8528757e-05
Iter: 410 loss: 2.84802682e-05
Iter: 411 loss: 2.85779461e-05
Iter: 412 loss: 2.8460523e-05
Iter: 413 loss: 2.83841109e-05
Iter: 414 loss: 2.85127389e-05
Iter: 415 loss: 2.83497629e-05
Iter: 416 loss: 2.82801757e-05
Iter: 417 loss: 2.86042978e-05
Iter: 418 loss: 2.82672809e-05
Iter: 419 loss: 2.82182664e-05
Iter: 420 loss: 2.82561705e-05
Iter: 421 loss: 2.81890734e-05
Iter: 422 loss: 2.81359171e-05
Iter: 423 loss: 2.88074825e-05
Iter: 424 loss: 2.81358462e-05
Iter: 425 loss: 2.80902605e-05
Iter: 426 loss: 2.84676753e-05
Iter: 427 loss: 2.80870845e-05
Iter: 428 loss: 2.8067263e-05
Iter: 429 loss: 2.80218555e-05
Iter: 430 loss: 2.85840433e-05
Iter: 431 loss: 2.80180866e-05
Iter: 432 loss: 2.79567357e-05
Iter: 433 loss: 2.80280656e-05
Iter: 434 loss: 2.79241449e-05
Iter: 435 loss: 2.78605985e-05
Iter: 436 loss: 2.81905923e-05
Iter: 437 loss: 2.78501575e-05
Iter: 438 loss: 2.77914987e-05
Iter: 439 loss: 2.79646447e-05
Iter: 440 loss: 2.77737199e-05
Iter: 441 loss: 2.77269701e-05
Iter: 442 loss: 2.81084449e-05
Iter: 443 loss: 2.77236868e-05
Iter: 444 loss: 2.76759565e-05
Iter: 445 loss: 2.76669016e-05
Iter: 446 loss: 2.76345618e-05
Iter: 447 loss: 2.75884886e-05
Iter: 448 loss: 2.76135943e-05
Iter: 449 loss: 2.75585389e-05
Iter: 450 loss: 2.7503007e-05
Iter: 451 loss: 2.79359429e-05
Iter: 452 loss: 2.74994309e-05
Iter: 453 loss: 2.74518679e-05
Iter: 454 loss: 2.75447455e-05
Iter: 455 loss: 2.74317026e-05
Iter: 456 loss: 2.73794867e-05
Iter: 457 loss: 2.73894857e-05
Iter: 458 loss: 2.73402038e-05
Iter: 459 loss: 2.72928919e-05
Iter: 460 loss: 2.80115601e-05
Iter: 461 loss: 2.72932484e-05
Iter: 462 loss: 2.72417819e-05
Iter: 463 loss: 2.74430658e-05
Iter: 464 loss: 2.72299931e-05
Iter: 465 loss: 2.72038942e-05
Iter: 466 loss: 2.71927056e-05
Iter: 467 loss: 2.71799254e-05
Iter: 468 loss: 2.71448243e-05
Iter: 469 loss: 2.71003591e-05
Iter: 470 loss: 2.70974342e-05
Iter: 471 loss: 2.70407727e-05
Iter: 472 loss: 2.76655373e-05
Iter: 473 loss: 2.70396704e-05
Iter: 474 loss: 2.69989978e-05
Iter: 475 loss: 2.70256824e-05
Iter: 476 loss: 2.69736593e-05
Iter: 477 loss: 2.69181419e-05
Iter: 478 loss: 2.72913021e-05
Iter: 479 loss: 2.6911619e-05
Iter: 480 loss: 2.6871563e-05
Iter: 481 loss: 2.70870041e-05
Iter: 482 loss: 2.68655276e-05
Iter: 483 loss: 2.68317981e-05
Iter: 484 loss: 2.67653e-05
Iter: 485 loss: 2.80204731e-05
Iter: 486 loss: 2.67642918e-05
Iter: 487 loss: 2.6703281e-05
Iter: 488 loss: 2.72147226e-05
Iter: 489 loss: 2.66996831e-05
Iter: 490 loss: 2.66468942e-05
Iter: 491 loss: 2.69237426e-05
Iter: 492 loss: 2.66388415e-05
Iter: 493 loss: 2.65965773e-05
Iter: 494 loss: 2.66401603e-05
Iter: 495 loss: 2.65724739e-05
Iter: 496 loss: 2.65208582e-05
Iter: 497 loss: 2.66292918e-05
Iter: 498 loss: 2.65003928e-05
Iter: 499 loss: 2.64841947e-05
Iter: 500 loss: 2.64685514e-05
Iter: 501 loss: 2.64580631e-05
Iter: 502 loss: 2.6425103e-05
Iter: 503 loss: 2.64674036e-05
Iter: 504 loss: 2.64002701e-05
Iter: 505 loss: 2.633029e-05
Iter: 506 loss: 2.65797407e-05
Iter: 507 loss: 2.63127804e-05
Iter: 508 loss: 2.62694157e-05
Iter: 509 loss: 2.64080463e-05
Iter: 510 loss: 2.62574449e-05
Iter: 511 loss: 2.62086724e-05
Iter: 512 loss: 2.62769227e-05
Iter: 513 loss: 2.61848363e-05
Iter: 514 loss: 2.61418609e-05
Iter: 515 loss: 2.66387e-05
Iter: 516 loss: 2.61410569e-05
Iter: 517 loss: 2.61067216e-05
Iter: 518 loss: 2.61893238e-05
Iter: 519 loss: 2.60942252e-05
Iter: 520 loss: 2.60557536e-05
Iter: 521 loss: 2.60921988e-05
Iter: 522 loss: 2.60337456e-05
Iter: 523 loss: 2.59941371e-05
Iter: 524 loss: 2.59598455e-05
Iter: 525 loss: 2.59486715e-05
Iter: 526 loss: 2.58921282e-05
Iter: 527 loss: 2.65203416e-05
Iter: 528 loss: 2.58909422e-05
Iter: 529 loss: 2.58496402e-05
Iter: 530 loss: 2.60333545e-05
Iter: 531 loss: 2.58412401e-05
Iter: 532 loss: 2.58084419e-05
Iter: 533 loss: 2.58295113e-05
Iter: 534 loss: 2.57875199e-05
Iter: 535 loss: 2.57741794e-05
Iter: 536 loss: 2.57680385e-05
Iter: 537 loss: 2.57464235e-05
Iter: 538 loss: 2.57086394e-05
Iter: 539 loss: 2.5708594e-05
Iter: 540 loss: 2.56782187e-05
Iter: 541 loss: 2.56834e-05
Iter: 542 loss: 2.56564017e-05
Iter: 543 loss: 2.56133771e-05
Iter: 544 loss: 2.57268694e-05
Iter: 545 loss: 2.55991508e-05
Iter: 546 loss: 2.55520663e-05
Iter: 547 loss: 2.56451785e-05
Iter: 548 loss: 2.5532283e-05
Iter: 549 loss: 2.54921852e-05
Iter: 550 loss: 2.57370903e-05
Iter: 551 loss: 2.54876104e-05
Iter: 552 loss: 2.54552288e-05
Iter: 553 loss: 2.56623e-05
Iter: 554 loss: 2.54516817e-05
Iter: 555 loss: 2.54204388e-05
Iter: 556 loss: 2.54260267e-05
Iter: 557 loss: 2.53973158e-05
Iter: 558 loss: 2.53533344e-05
Iter: 559 loss: 2.54158331e-05
Iter: 560 loss: 2.5331512e-05
Iter: 561 loss: 2.52985083e-05
Iter: 562 loss: 2.53909366e-05
Iter: 563 loss: 2.52878599e-05
Iter: 564 loss: 2.52502286e-05
Iter: 565 loss: 2.52968803e-05
Iter: 566 loss: 2.52293594e-05
Iter: 567 loss: 2.51907677e-05
Iter: 568 loss: 2.56399398e-05
Iter: 569 loss: 2.5190071e-05
Iter: 570 loss: 2.51639103e-05
Iter: 571 loss: 2.51332349e-05
Iter: 572 loss: 2.51302408e-05
Iter: 573 loss: 2.5137826e-05
Iter: 574 loss: 2.51095298e-05
Iter: 575 loss: 2.50984158e-05
Iter: 576 loss: 2.50650155e-05
Iter: 577 loss: 2.52023856e-05
Iter: 578 loss: 2.50527792e-05
Iter: 579 loss: 2.50107078e-05
Iter: 580 loss: 2.51681649e-05
Iter: 581 loss: 2.50002231e-05
Iter: 582 loss: 2.49648383e-05
Iter: 583 loss: 2.50138437e-05
Iter: 584 loss: 2.49470595e-05
Iter: 585 loss: 2.48978085e-05
Iter: 586 loss: 2.50878074e-05
Iter: 587 loss: 2.48862107e-05
Iter: 588 loss: 2.48580182e-05
Iter: 589 loss: 2.50396315e-05
Iter: 590 loss: 2.48550859e-05
Iter: 591 loss: 2.48232354e-05
Iter: 592 loss: 2.48714641e-05
Iter: 593 loss: 2.48080396e-05
Iter: 594 loss: 2.47771641e-05
Iter: 595 loss: 2.48383822e-05
Iter: 596 loss: 2.47645876e-05
Iter: 597 loss: 2.47280368e-05
Iter: 598 loss: 2.47114367e-05
Iter: 599 loss: 2.46932832e-05
Iter: 600 loss: 2.46553427e-05
Iter: 601 loss: 2.50426419e-05
Iter: 602 loss: 2.4654044e-05
Iter: 603 loss: 2.46243471e-05
Iter: 604 loss: 2.46753261e-05
Iter: 605 loss: 2.46113414e-05
Iter: 606 loss: 2.45703122e-05
Iter: 607 loss: 2.46738964e-05
Iter: 608 loss: 2.45568317e-05
Iter: 609 loss: 2.45507745e-05
Iter: 610 loss: 2.45413394e-05
Iter: 611 loss: 2.4527686e-05
Iter: 612 loss: 2.44856201e-05
Iter: 613 loss: 2.46073778e-05
Iter: 614 loss: 2.44634102e-05
Iter: 615 loss: 2.44149905e-05
Iter: 616 loss: 2.46350573e-05
Iter: 617 loss: 2.44056973e-05
Iter: 618 loss: 2.436871e-05
Iter: 619 loss: 2.45544725e-05
Iter: 620 loss: 2.43628128e-05
Iter: 621 loss: 2.43304403e-05
Iter: 622 loss: 2.4386014e-05
Iter: 623 loss: 2.43162795e-05
Iter: 624 loss: 2.42784663e-05
Iter: 625 loss: 2.44198e-05
Iter: 626 loss: 2.42697351e-05
Iter: 627 loss: 2.42397109e-05
Iter: 628 loss: 2.45025367e-05
Iter: 629 loss: 2.42385358e-05
Iter: 630 loss: 2.42096885e-05
Iter: 631 loss: 2.41889e-05
Iter: 632 loss: 2.41795533e-05
Iter: 633 loss: 2.41421185e-05
Iter: 634 loss: 2.42913356e-05
Iter: 635 loss: 2.41340949e-05
Iter: 636 loss: 2.40981099e-05
Iter: 637 loss: 2.41012713e-05
Iter: 638 loss: 2.40704667e-05
Iter: 639 loss: 2.40184345e-05
Iter: 640 loss: 2.42142723e-05
Iter: 641 loss: 2.40063819e-05
Iter: 642 loss: 2.39738365e-05
Iter: 643 loss: 2.44224393e-05
Iter: 644 loss: 2.39736382e-05
Iter: 645 loss: 2.39508463e-05
Iter: 646 loss: 2.40066784e-05
Iter: 647 loss: 2.39424407e-05
Iter: 648 loss: 2.39107503e-05
Iter: 649 loss: 2.40843347e-05
Iter: 650 loss: 2.39060646e-05
Iter: 651 loss: 2.38922494e-05
Iter: 652 loss: 2.38563e-05
Iter: 653 loss: 2.41583348e-05
Iter: 654 loss: 2.38499906e-05
Iter: 655 loss: 2.38068969e-05
Iter: 656 loss: 2.39679212e-05
Iter: 657 loss: 2.37964268e-05
Iter: 658 loss: 2.37518288e-05
Iter: 659 loss: 2.3812674e-05
Iter: 660 loss: 2.372987e-05
Iter: 661 loss: 2.36901251e-05
Iter: 662 loss: 2.42528949e-05
Iter: 663 loss: 2.36900778e-05
Iter: 664 loss: 2.36683518e-05
Iter: 665 loss: 2.3685443e-05
Iter: 666 loss: 2.36554151e-05
Iter: 667 loss: 2.36164742e-05
Iter: 668 loss: 2.36996e-05
Iter: 669 loss: 2.36014148e-05
Iter: 670 loss: 2.35696662e-05
Iter: 671 loss: 2.36773576e-05
Iter: 672 loss: 2.35611806e-05
Iter: 673 loss: 2.35358602e-05
Iter: 674 loss: 2.35366479e-05
Iter: 675 loss: 2.35159e-05
Iter: 676 loss: 2.348007e-05
Iter: 677 loss: 2.36184223e-05
Iter: 678 loss: 2.34716026e-05
Iter: 679 loss: 2.34367271e-05
Iter: 680 loss: 2.34853196e-05
Iter: 681 loss: 2.34191575e-05
Iter: 682 loss: 2.33900337e-05
Iter: 683 loss: 2.33901119e-05
Iter: 684 loss: 2.33743558e-05
Iter: 685 loss: 2.35968619e-05
Iter: 686 loss: 2.33740229e-05
Iter: 687 loss: 2.33594183e-05
Iter: 688 loss: 2.33173287e-05
Iter: 689 loss: 2.35514963e-05
Iter: 690 loss: 2.33047213e-05
Iter: 691 loss: 2.32678467e-05
Iter: 692 loss: 2.33536102e-05
Iter: 693 loss: 2.3253926e-05
Iter: 694 loss: 2.32116836e-05
Iter: 695 loss: 2.33354258e-05
Iter: 696 loss: 2.31987506e-05
Iter: 697 loss: 2.31616141e-05
Iter: 698 loss: 2.33553892e-05
Iter: 699 loss: 2.31555132e-05
Iter: 700 loss: 2.31205377e-05
Iter: 701 loss: 2.3221226e-05
Iter: 702 loss: 2.31094946e-05
Iter: 703 loss: 2.30804872e-05
Iter: 704 loss: 2.33425671e-05
Iter: 705 loss: 2.30790865e-05
Iter: 706 loss: 2.30544574e-05
Iter: 707 loss: 2.30627447e-05
Iter: 708 loss: 2.30366841e-05
Iter: 709 loss: 2.30104888e-05
Iter: 710 loss: 2.30384176e-05
Iter: 711 loss: 2.29963334e-05
Iter: 712 loss: 2.29607576e-05
Iter: 713 loss: 2.3033499e-05
Iter: 714 loss: 2.29468496e-05
Iter: 715 loss: 2.29172583e-05
Iter: 716 loss: 2.30717305e-05
Iter: 717 loss: 2.2912287e-05
Iter: 718 loss: 2.28870558e-05
Iter: 719 loss: 2.29836332e-05
Iter: 720 loss: 2.28807548e-05
Iter: 721 loss: 2.28586869e-05
Iter: 722 loss: 2.28588651e-05
Iter: 723 loss: 2.28423341e-05
Iter: 724 loss: 2.28352383e-05
Iter: 725 loss: 2.28265635e-05
Iter: 726 loss: 2.2807033e-05
Iter: 727 loss: 2.27652963e-05
Iter: 728 loss: 2.3391116e-05
Iter: 729 loss: 2.27632445e-05
Iter: 730 loss: 2.27136261e-05
Iter: 731 loss: 2.30262449e-05
Iter: 732 loss: 2.27077689e-05
Iter: 733 loss: 2.26780539e-05
Iter: 734 loss: 2.27225228e-05
Iter: 735 loss: 2.26632801e-05
Iter: 736 loss: 2.26249103e-05
Iter: 737 loss: 2.2896591e-05
Iter: 738 loss: 2.26211014e-05
Iter: 739 loss: 2.25953845e-05
Iter: 740 loss: 2.27296623e-05
Iter: 741 loss: 2.25912609e-05
Iter: 742 loss: 2.25651493e-05
Iter: 743 loss: 2.26257544e-05
Iter: 744 loss: 2.25555086e-05
Iter: 745 loss: 2.25305594e-05
Iter: 746 loss: 2.25304466e-05
Iter: 747 loss: 2.25112781e-05
Iter: 748 loss: 2.24731848e-05
Iter: 749 loss: 2.25072326e-05
Iter: 750 loss: 2.24508985e-05
Iter: 751 loss: 2.24185787e-05
Iter: 752 loss: 2.28161844e-05
Iter: 753 loss: 2.24182149e-05
Iter: 754 loss: 2.23962852e-05
Iter: 755 loss: 2.24218747e-05
Iter: 756 loss: 2.23843726e-05
Iter: 757 loss: 2.23590614e-05
Iter: 758 loss: 2.23587085e-05
Iter: 759 loss: 2.23432908e-05
Iter: 760 loss: 2.23391035e-05
Iter: 761 loss: 2.23294373e-05
Iter: 762 loss: 2.23104726e-05
Iter: 763 loss: 2.22778253e-05
Iter: 764 loss: 2.22778617e-05
Iter: 765 loss: 2.22402032e-05
Iter: 766 loss: 2.24274409e-05
Iter: 767 loss: 2.22341132e-05
Iter: 768 loss: 2.2203365e-05
Iter: 769 loss: 2.22321869e-05
Iter: 770 loss: 2.21860701e-05
Iter: 771 loss: 2.21539885e-05
Iter: 772 loss: 2.24086361e-05
Iter: 773 loss: 2.21515911e-05
Iter: 774 loss: 2.21230966e-05
Iter: 775 loss: 2.22619892e-05
Iter: 776 loss: 2.2118129e-05
Iter: 777 loss: 2.20960756e-05
Iter: 778 loss: 2.22079507e-05
Iter: 779 loss: 2.20925176e-05
Iter: 780 loss: 2.20720121e-05
Iter: 781 loss: 2.20445218e-05
Iter: 782 loss: 2.20426955e-05
Iter: 783 loss: 2.20067777e-05
Iter: 784 loss: 2.2174012e-05
Iter: 785 loss: 2.19999602e-05
Iter: 786 loss: 2.19694793e-05
Iter: 787 loss: 2.20464462e-05
Iter: 788 loss: 2.195864e-05
Iter: 789 loss: 2.19324902e-05
Iter: 790 loss: 2.21852406e-05
Iter: 791 loss: 2.19315243e-05
Iter: 792 loss: 2.19137328e-05
Iter: 793 loss: 2.21954324e-05
Iter: 794 loss: 2.19139656e-05
Iter: 795 loss: 2.19025314e-05
Iter: 796 loss: 2.18799069e-05
Iter: 797 loss: 2.2314236e-05
Iter: 798 loss: 2.18798341e-05
Iter: 799 loss: 2.18516852e-05
Iter: 800 loss: 2.18921e-05
Iter: 801 loss: 2.18379118e-05
Iter: 802 loss: 2.18060104e-05
Iter: 803 loss: 2.18040732e-05
Iter: 804 loss: 2.17797769e-05
Iter: 805 loss: 2.17457819e-05
Iter: 806 loss: 2.19527283e-05
Iter: 807 loss: 2.17416164e-05
Iter: 808 loss: 2.1712538e-05
Iter: 809 loss: 2.17135603e-05
Iter: 810 loss: 2.16892549e-05
Iter: 811 loss: 2.16606131e-05
Iter: 812 loss: 2.16594017e-05
Iter: 813 loss: 2.16402113e-05
Iter: 814 loss: 2.16883109e-05
Iter: 815 loss: 2.16338885e-05
Iter: 816 loss: 2.16116641e-05
Iter: 817 loss: 2.15950367e-05
Iter: 818 loss: 2.15878808e-05
Iter: 819 loss: 2.15545733e-05
Iter: 820 loss: 2.17216839e-05
Iter: 821 loss: 2.15493637e-05
Iter: 822 loss: 2.1526459e-05
Iter: 823 loss: 2.15239907e-05
Iter: 824 loss: 2.15079017e-05
Iter: 825 loss: 2.14800384e-05
Iter: 826 loss: 2.18948735e-05
Iter: 827 loss: 2.14801112e-05
Iter: 828 loss: 2.14631891e-05
Iter: 829 loss: 2.14630199e-05
Iter: 830 loss: 2.14505635e-05
Iter: 831 loss: 2.14204229e-05
Iter: 832 loss: 2.17548968e-05
Iter: 833 loss: 2.1417909e-05
Iter: 834 loss: 2.13883141e-05
Iter: 835 loss: 2.15966029e-05
Iter: 836 loss: 2.13855856e-05
Iter: 837 loss: 2.13648418e-05
Iter: 838 loss: 2.13524436e-05
Iter: 839 loss: 2.13436142e-05
Iter: 840 loss: 2.13098101e-05
Iter: 841 loss: 2.14040629e-05
Iter: 842 loss: 2.12984451e-05
Iter: 843 loss: 2.12667364e-05
Iter: 844 loss: 2.13120984e-05
Iter: 845 loss: 2.12503182e-05
Iter: 846 loss: 2.12270897e-05
Iter: 847 loss: 2.15341133e-05
Iter: 848 loss: 2.12272589e-05
Iter: 849 loss: 2.1202959e-05
Iter: 850 loss: 2.12629329e-05
Iter: 851 loss: 2.11947918e-05
Iter: 852 loss: 2.11732258e-05
Iter: 853 loss: 2.12341802e-05
Iter: 854 loss: 2.11665938e-05
Iter: 855 loss: 2.11458173e-05
Iter: 856 loss: 2.11531296e-05
Iter: 857 loss: 2.11313309e-05
Iter: 858 loss: 2.11060687e-05
Iter: 859 loss: 2.11584629e-05
Iter: 860 loss: 2.10958824e-05
Iter: 861 loss: 2.10689286e-05
Iter: 862 loss: 2.11864099e-05
Iter: 863 loss: 2.1063177e-05
Iter: 864 loss: 2.10502731e-05
Iter: 865 loss: 2.10476082e-05
Iter: 866 loss: 2.1037602e-05
Iter: 867 loss: 2.10222861e-05
Iter: 868 loss: 2.10221133e-05
Iter: 869 loss: 2.10020662e-05
Iter: 870 loss: 2.09965692e-05
Iter: 871 loss: 2.0984724e-05
Iter: 872 loss: 2.09567806e-05
Iter: 873 loss: 2.11086372e-05
Iter: 874 loss: 2.09530735e-05
Iter: 875 loss: 2.09315294e-05
Iter: 876 loss: 2.09210266e-05
Iter: 877 loss: 2.09107675e-05
Iter: 878 loss: 2.08732163e-05
Iter: 879 loss: 2.09440732e-05
Iter: 880 loss: 2.08576e-05
Iter: 881 loss: 2.08230886e-05
Iter: 882 loss: 2.09916943e-05
Iter: 883 loss: 2.08169295e-05
Iter: 884 loss: 2.0796524e-05
Iter: 885 loss: 2.07957346e-05
Iter: 886 loss: 2.07825397e-05
Iter: 887 loss: 2.07794365e-05
Iter: 888 loss: 2.07710018e-05
Iter: 889 loss: 2.07498379e-05
Iter: 890 loss: 2.07571538e-05
Iter: 891 loss: 2.07351241e-05
Iter: 892 loss: 2.07059093e-05
Iter: 893 loss: 2.08113088e-05
Iter: 894 loss: 2.06981258e-05
Iter: 895 loss: 2.06765435e-05
Iter: 896 loss: 2.07356643e-05
Iter: 897 loss: 2.06690929e-05
Iter: 898 loss: 2.06543318e-05
Iter: 899 loss: 2.06528075e-05
Iter: 900 loss: 2.06443037e-05
Iter: 901 loss: 2.06291461e-05
Iter: 902 loss: 2.09727259e-05
Iter: 903 loss: 2.06292316e-05
Iter: 904 loss: 2.06080131e-05
Iter: 905 loss: 2.06029072e-05
Iter: 906 loss: 2.05896795e-05
Iter: 907 loss: 2.05645156e-05
Iter: 908 loss: 2.07834019e-05
Iter: 909 loss: 2.0563104e-05
Iter: 910 loss: 2.0544252e-05
Iter: 911 loss: 2.05265096e-05
Iter: 912 loss: 2.05221513e-05
Iter: 913 loss: 2.04926946e-05
Iter: 914 loss: 2.06783843e-05
Iter: 915 loss: 2.04894095e-05
Iter: 916 loss: 2.04648277e-05
Iter: 917 loss: 2.04579719e-05
Iter: 918 loss: 2.04432708e-05
Iter: 919 loss: 2.04284079e-05
Iter: 920 loss: 2.0423664e-05
Iter: 921 loss: 2.04070311e-05
Iter: 922 loss: 2.0388703e-05
Iter: 923 loss: 2.03860982e-05
Iter: 924 loss: 2.03620693e-05
Iter: 925 loss: 2.04805328e-05
Iter: 926 loss: 2.03580639e-05
Iter: 927 loss: 2.03379495e-05
Iter: 928 loss: 2.03741492e-05
Iter: 929 loss: 2.03295658e-05
Iter: 930 loss: 2.03081217e-05
Iter: 931 loss: 2.03575673e-05
Iter: 932 loss: 2.03000382e-05
Iter: 933 loss: 2.02820665e-05
Iter: 934 loss: 2.0281459e-05
Iter: 935 loss: 2.02702813e-05
Iter: 936 loss: 2.02455922e-05
Iter: 937 loss: 2.0604788e-05
Iter: 938 loss: 2.02444407e-05
Iter: 939 loss: 2.02262036e-05
Iter: 940 loss: 2.03858544e-05
Iter: 941 loss: 2.02255269e-05
Iter: 942 loss: 2.02101055e-05
Iter: 943 loss: 2.01934636e-05
Iter: 944 loss: 2.0190897e-05
Iter: 945 loss: 2.0161222e-05
Iter: 946 loss: 2.03024192e-05
Iter: 947 loss: 2.01555631e-05
Iter: 948 loss: 2.01339772e-05
Iter: 949 loss: 2.01653729e-05
Iter: 950 loss: 2.01235125e-05
Iter: 951 loss: 2.00980867e-05
Iter: 952 loss: 2.01396597e-05
Iter: 953 loss: 2.00867325e-05
Iter: 954 loss: 2.00676e-05
Iter: 955 loss: 2.03149812e-05
Iter: 956 loss: 2.00671057e-05
Iter: 957 loss: 2.00479644e-05
Iter: 958 loss: 2.0084819e-05
Iter: 959 loss: 2.00402e-05
Iter: 960 loss: 2.00227205e-05
Iter: 961 loss: 2.00269551e-05
Iter: 962 loss: 2.00100658e-05
Iter: 963 loss: 1.9986237e-05
Iter: 964 loss: 2.00409886e-05
Iter: 965 loss: 1.99773131e-05
Iter: 966 loss: 1.99558708e-05
Iter: 967 loss: 2.01384864e-05
Iter: 968 loss: 1.99544083e-05
Iter: 969 loss: 1.99439382e-05
Iter: 970 loss: 1.99435672e-05
Iter: 971 loss: 1.99358656e-05
Iter: 972 loss: 1.99153928e-05
Iter: 973 loss: 2.00225204e-05
Iter: 974 loss: 1.99086353e-05
Iter: 975 loss: 1.98910693e-05
Iter: 976 loss: 2.00759023e-05
Iter: 977 loss: 1.98906091e-05
Iter: 978 loss: 1.98727284e-05
Iter: 979 loss: 1.98492125e-05
Iter: 980 loss: 1.984749e-05
Iter: 981 loss: 1.98202706e-05
Iter: 982 loss: 2.01111889e-05
Iter: 983 loss: 1.98196e-05
Iter: 984 loss: 1.98001e-05
Iter: 985 loss: 1.97856571e-05
Iter: 986 loss: 1.97790541e-05
Iter: 987 loss: 1.97476402e-05
Iter: 988 loss: 1.99509359e-05
Iter: 989 loss: 1.97443387e-05
Iter: 990 loss: 1.97261634e-05
Iter: 991 loss: 1.98396028e-05
Iter: 992 loss: 1.97241661e-05
Iter: 993 loss: 1.97067e-05
Iter: 994 loss: 1.97927329e-05
Iter: 995 loss: 1.9703717e-05
Iter: 996 loss: 1.96908186e-05
Iter: 997 loss: 1.96637156e-05
Iter: 998 loss: 2.01362636e-05
Iter: 999 loss: 1.96634392e-05
Iter: 1000 loss: 1.9634921e-05
Iter: 1001 loss: 1.99742972e-05
Iter: 1002 loss: 1.96345591e-05
Iter: 1003 loss: 1.96165111e-05
Iter: 1004 loss: 1.97246118e-05
Iter: 1005 loss: 1.96144665e-05
Iter: 1006 loss: 1.95957145e-05
Iter: 1007 loss: 1.97227128e-05
Iter: 1008 loss: 1.95938865e-05
Iter: 1009 loss: 1.95815919e-05
Iter: 1010 loss: 1.95592784e-05
Iter: 1011 loss: 2.00541836e-05
Iter: 1012 loss: 1.95590328e-05
Iter: 1013 loss: 1.95400498e-05
Iter: 1014 loss: 1.96110777e-05
Iter: 1015 loss: 1.95352222e-05
Iter: 1016 loss: 1.95143712e-05
Iter: 1017 loss: 1.95774483e-05
Iter: 1018 loss: 1.9507901e-05
Iter: 1019 loss: 1.94882123e-05
Iter: 1020 loss: 1.94949534e-05
Iter: 1021 loss: 1.94744571e-05
Iter: 1022 loss: 1.9451858e-05
Iter: 1023 loss: 1.96046385e-05
Iter: 1024 loss: 1.94501627e-05
Iter: 1025 loss: 1.94307358e-05
Iter: 1026 loss: 1.94093845e-05
Iter: 1027 loss: 1.94064414e-05
Iter: 1028 loss: 1.93863689e-05
Iter: 1029 loss: 1.93855049e-05
Iter: 1030 loss: 1.93700253e-05
Iter: 1031 loss: 1.94276854e-05
Iter: 1032 loss: 1.93663363e-05
Iter: 1033 loss: 1.9351035e-05
Iter: 1034 loss: 1.93192463e-05
Iter: 1035 loss: 1.98457601e-05
Iter: 1036 loss: 1.93185333e-05
Iter: 1037 loss: 1.92933385e-05
Iter: 1038 loss: 1.96834808e-05
Iter: 1039 loss: 1.92934021e-05
Iter: 1040 loss: 1.92802272e-05
Iter: 1041 loss: 1.92799489e-05
Iter: 1042 loss: 1.92659027e-05
Iter: 1043 loss: 1.92657317e-05
Iter: 1044 loss: 1.9254272e-05
Iter: 1045 loss: 1.92382395e-05
Iter: 1046 loss: 1.92235384e-05
Iter: 1047 loss: 1.92197622e-05
Iter: 1048 loss: 1.91970666e-05
Iter: 1049 loss: 1.92415137e-05
Iter: 1050 loss: 1.91877334e-05
Iter: 1051 loss: 1.9163379e-05
Iter: 1052 loss: 1.93521155e-05
Iter: 1053 loss: 1.91614417e-05
Iter: 1054 loss: 1.91447e-05
Iter: 1055 loss: 1.913901e-05
Iter: 1056 loss: 1.91295803e-05
Iter: 1057 loss: 1.91020699e-05
Iter: 1058 loss: 1.91825457e-05
Iter: 1059 loss: 1.90933606e-05
Iter: 1060 loss: 1.90712271e-05
Iter: 1061 loss: 1.91574636e-05
Iter: 1062 loss: 1.90656683e-05
Iter: 1063 loss: 1.9044106e-05
Iter: 1064 loss: 1.91206491e-05
Iter: 1065 loss: 1.903886e-05
Iter: 1066 loss: 1.90197534e-05
Iter: 1067 loss: 1.92030311e-05
Iter: 1068 loss: 1.90189457e-05
Iter: 1069 loss: 1.90051705e-05
Iter: 1070 loss: 1.89775237e-05
Iter: 1071 loss: 1.94857457e-05
Iter: 1072 loss: 1.89771381e-05
Iter: 1073 loss: 1.89555685e-05
Iter: 1074 loss: 1.91435502e-05
Iter: 1075 loss: 1.89541061e-05
Iter: 1076 loss: 1.89394232e-05
Iter: 1077 loss: 1.89392831e-05
Iter: 1078 loss: 1.89249022e-05
Iter: 1079 loss: 1.89102357e-05
Iter: 1080 loss: 1.890774e-05
Iter: 1081 loss: 1.88930517e-05
Iter: 1082 loss: 1.89205984e-05
Iter: 1083 loss: 1.88867489e-05
Iter: 1084 loss: 1.88699578e-05
Iter: 1085 loss: 1.88586637e-05
Iter: 1086 loss: 1.88523572e-05
Iter: 1087 loss: 1.88305239e-05
Iter: 1088 loss: 1.91370182e-05
Iter: 1089 loss: 1.88305567e-05
Iter: 1090 loss: 1.88135291e-05
Iter: 1091 loss: 1.879925e-05
Iter: 1092 loss: 1.87942405e-05
Iter: 1093 loss: 1.87725127e-05
Iter: 1094 loss: 1.89019302e-05
Iter: 1095 loss: 1.87698606e-05
Iter: 1096 loss: 1.87481637e-05
Iter: 1097 loss: 1.87612495e-05
Iter: 1098 loss: 1.87342157e-05
Iter: 1099 loss: 1.87112491e-05
Iter: 1100 loss: 1.89638868e-05
Iter: 1101 loss: 1.87111382e-05
Iter: 1102 loss: 1.86932957e-05
Iter: 1103 loss: 1.87656478e-05
Iter: 1104 loss: 1.8689665e-05
Iter: 1105 loss: 1.86761099e-05
Iter: 1106 loss: 1.86689103e-05
Iter: 1107 loss: 1.86627258e-05
Iter: 1108 loss: 1.86448124e-05
Iter: 1109 loss: 1.86985053e-05
Iter: 1110 loss: 1.863949e-05
Iter: 1111 loss: 1.86283432e-05
Iter: 1112 loss: 1.8626e-05
Iter: 1113 loss: 1.86193338e-05
Iter: 1114 loss: 1.86038942e-05
Iter: 1115 loss: 1.88087033e-05
Iter: 1116 loss: 1.86030775e-05
Iter: 1117 loss: 1.85840836e-05
Iter: 1118 loss: 1.86168254e-05
Iter: 1119 loss: 1.85757126e-05
Iter: 1120 loss: 1.85556473e-05
Iter: 1121 loss: 1.85881072e-05
Iter: 1122 loss: 1.8546405e-05
Iter: 1123 loss: 1.85265817e-05
Iter: 1124 loss: 1.8680541e-05
Iter: 1125 loss: 1.8525534e-05
Iter: 1126 loss: 1.85103927e-05
Iter: 1127 loss: 1.85357949e-05
Iter: 1128 loss: 1.8503426e-05
Iter: 1129 loss: 1.84879591e-05
Iter: 1130 loss: 1.84826349e-05
Iter: 1131 loss: 1.8474002e-05
Iter: 1132 loss: 1.84485089e-05
Iter: 1133 loss: 1.85686167e-05
Iter: 1134 loss: 1.84440323e-05
Iter: 1135 loss: 1.84269738e-05
Iter: 1136 loss: 1.85949248e-05
Iter: 1137 loss: 1.84266137e-05
Iter: 1138 loss: 1.84116616e-05
Iter: 1139 loss: 1.84462479e-05
Iter: 1140 loss: 1.84065939e-05
Iter: 1141 loss: 1.83929878e-05
Iter: 1142 loss: 1.84046239e-05
Iter: 1143 loss: 1.83846405e-05
Iter: 1144 loss: 1.83709417e-05
Iter: 1145 loss: 1.83822322e-05
Iter: 1146 loss: 1.8362507e-05
Iter: 1147 loss: 1.83457487e-05
Iter: 1148 loss: 1.83456377e-05
Iter: 1149 loss: 1.83401098e-05
Iter: 1150 loss: 1.83268785e-05
Iter: 1151 loss: 1.84694036e-05
Iter: 1152 loss: 1.83254087e-05
Iter: 1153 loss: 1.83049015e-05
Iter: 1154 loss: 1.83182819e-05
Iter: 1155 loss: 1.82919575e-05
Iter: 1156 loss: 1.82698823e-05
Iter: 1157 loss: 1.84357214e-05
Iter: 1158 loss: 1.82681943e-05
Iter: 1159 loss: 1.82520125e-05
Iter: 1160 loss: 1.82685926e-05
Iter: 1161 loss: 1.8242883e-05
Iter: 1162 loss: 1.82205476e-05
Iter: 1163 loss: 1.83040938e-05
Iter: 1164 loss: 1.82148942e-05
Iter: 1165 loss: 1.81978448e-05
Iter: 1166 loss: 1.8191653e-05
Iter: 1167 loss: 1.81820869e-05
Iter: 1168 loss: 1.81617979e-05
Iter: 1169 loss: 1.83483044e-05
Iter: 1170 loss: 1.81609284e-05
Iter: 1171 loss: 1.81431278e-05
Iter: 1172 loss: 1.81960513e-05
Iter: 1173 loss: 1.81378527e-05
Iter: 1174 loss: 1.81181804e-05
Iter: 1175 loss: 1.82126169e-05
Iter: 1176 loss: 1.81148407e-05
Iter: 1177 loss: 1.81038504e-05
Iter: 1178 loss: 1.81059659e-05
Iter: 1179 loss: 1.80957322e-05
Iter: 1180 loss: 1.80811676e-05
Iter: 1181 loss: 1.81730429e-05
Iter: 1182 loss: 1.8079103e-05
Iter: 1183 loss: 1.80588686e-05
Iter: 1184 loss: 1.80915158e-05
Iter: 1185 loss: 1.80499155e-05
Iter: 1186 loss: 1.80418792e-05
Iter: 1187 loss: 1.80277275e-05
Iter: 1188 loss: 1.83700831e-05
Iter: 1189 loss: 1.80278239e-05
Iter: 1190 loss: 1.80075e-05
Iter: 1191 loss: 1.8079425e-05
Iter: 1192 loss: 1.80021307e-05
Iter: 1193 loss: 1.79849267e-05
Iter: 1194 loss: 1.8047891e-05
Iter: 1195 loss: 1.79802701e-05
Iter: 1196 loss: 1.79645431e-05
Iter: 1197 loss: 1.7995857e-05
Iter: 1198 loss: 1.79576145e-05
Iter: 1199 loss: 1.79376802e-05
Iter: 1200 loss: 1.79928647e-05
Iter: 1201 loss: 1.79311701e-05
Iter: 1202 loss: 1.79141298e-05
Iter: 1203 loss: 1.79222079e-05
Iter: 1204 loss: 1.79029557e-05
Iter: 1205 loss: 1.78816299e-05
Iter: 1206 loss: 1.79667186e-05
Iter: 1207 loss: 1.78770879e-05
Iter: 1208 loss: 1.78629743e-05
Iter: 1209 loss: 1.80803854e-05
Iter: 1210 loss: 1.78626906e-05
Iter: 1211 loss: 1.785108e-05
Iter: 1212 loss: 1.7852115e-05
Iter: 1213 loss: 1.78420396e-05
Iter: 1214 loss: 1.78263399e-05
Iter: 1215 loss: 1.78435876e-05
Iter: 1216 loss: 1.78178107e-05
Iter: 1217 loss: 1.78092287e-05
Iter: 1218 loss: 1.7807768e-05
Iter: 1219 loss: 1.77977417e-05
Iter: 1220 loss: 1.77854199e-05
Iter: 1221 loss: 1.77843431e-05
Iter: 1222 loss: 1.77738875e-05
Iter: 1223 loss: 1.77573802e-05
Iter: 1224 loss: 1.77571419e-05
Iter: 1225 loss: 1.77373895e-05
Iter: 1226 loss: 1.79079543e-05
Iter: 1227 loss: 1.7736169e-05
Iter: 1228 loss: 1.77210022e-05
Iter: 1229 loss: 1.7766064e-05
Iter: 1230 loss: 1.77163492e-05
Iter: 1231 loss: 1.77001129e-05
Iter: 1232 loss: 1.77272559e-05
Iter: 1233 loss: 1.76929116e-05
Iter: 1234 loss: 1.7676537e-05
Iter: 1235 loss: 1.77490383e-05
Iter: 1236 loss: 1.76728263e-05
Iter: 1237 loss: 1.76559806e-05
Iter: 1238 loss: 1.76653848e-05
Iter: 1239 loss: 1.76452413e-05
Iter: 1240 loss: 1.76273097e-05
Iter: 1241 loss: 1.76344292e-05
Iter: 1242 loss: 1.76149551e-05
Iter: 1243 loss: 1.75976202e-05
Iter: 1244 loss: 1.75972727e-05
Iter: 1245 loss: 1.75856949e-05
Iter: 1246 loss: 1.76056365e-05
Iter: 1247 loss: 1.75805271e-05
Iter: 1248 loss: 1.75679706e-05
Iter: 1249 loss: 1.75623227e-05
Iter: 1250 loss: 1.75562982e-05
Iter: 1251 loss: 1.7553386e-05
Iter: 1252 loss: 1.75473106e-05
Iter: 1253 loss: 1.75411587e-05
Iter: 1254 loss: 1.75272871e-05
Iter: 1255 loss: 1.77410839e-05
Iter: 1256 loss: 1.75269888e-05
Iter: 1257 loss: 1.75112364e-05
Iter: 1258 loss: 1.74958077e-05
Iter: 1259 loss: 1.74921206e-05
Iter: 1260 loss: 1.74738852e-05
Iter: 1261 loss: 1.76250869e-05
Iter: 1262 loss: 1.74726538e-05
Iter: 1263 loss: 1.7455659e-05
Iter: 1264 loss: 1.75153855e-05
Iter: 1265 loss: 1.74510533e-05
Iter: 1266 loss: 1.74342022e-05
Iter: 1267 loss: 1.74514826e-05
Iter: 1268 loss: 1.7424929e-05
Iter: 1269 loss: 1.74032702e-05
Iter: 1270 loss: 1.74854977e-05
Iter: 1271 loss: 1.73983808e-05
Iter: 1272 loss: 1.73828212e-05
Iter: 1273 loss: 1.74690431e-05
Iter: 1274 loss: 1.73806147e-05
Iter: 1275 loss: 1.73699736e-05
Iter: 1276 loss: 1.73551452e-05
Iter: 1277 loss: 1.7354283e-05
Iter: 1278 loss: 1.73437566e-05
Iter: 1279 loss: 1.73419e-05
Iter: 1280 loss: 1.7331693e-05
Iter: 1281 loss: 1.73374374e-05
Iter: 1282 loss: 1.73253284e-05
Iter: 1283 loss: 1.73123663e-05
Iter: 1284 loss: 1.73066801e-05
Iter: 1285 loss: 1.72999571e-05
Iter: 1286 loss: 1.7286573e-05
Iter: 1287 loss: 1.72850632e-05
Iter: 1288 loss: 1.72778145e-05
Iter: 1289 loss: 1.72673608e-05
Iter: 1290 loss: 1.72670716e-05
Iter: 1291 loss: 1.7254737e-05
Iter: 1292 loss: 1.72339787e-05
Iter: 1293 loss: 1.72337914e-05
Iter: 1294 loss: 1.72100918e-05
Iter: 1295 loss: 1.74207598e-05
Iter: 1296 loss: 1.72087057e-05
Iter: 1297 loss: 1.71905667e-05
Iter: 1298 loss: 1.7254004e-05
Iter: 1299 loss: 1.71855281e-05
Iter: 1300 loss: 1.71685242e-05
Iter: 1301 loss: 1.72598484e-05
Iter: 1302 loss: 1.71658503e-05
Iter: 1303 loss: 1.71527754e-05
Iter: 1304 loss: 1.71586653e-05
Iter: 1305 loss: 1.71440479e-05
Iter: 1306 loss: 1.71271367e-05
Iter: 1307 loss: 1.72313012e-05
Iter: 1308 loss: 1.71252141e-05
Iter: 1309 loss: 1.71125466e-05
Iter: 1310 loss: 1.71066349e-05
Iter: 1311 loss: 1.71002175e-05
Iter: 1312 loss: 1.7088174e-05
Iter: 1313 loss: 1.70883195e-05
Iter: 1314 loss: 1.70764288e-05
Iter: 1315 loss: 1.70734693e-05
Iter: 1316 loss: 1.70662261e-05
Iter: 1317 loss: 1.70526782e-05
Iter: 1318 loss: 1.71619322e-05
Iter: 1319 loss: 1.70517742e-05
Iter: 1320 loss: 1.70384883e-05
Iter: 1321 loss: 1.71153788e-05
Iter: 1322 loss: 1.70370167e-05
Iter: 1323 loss: 1.70301064e-05
Iter: 1324 loss: 1.7015147e-05
Iter: 1325 loss: 1.72113832e-05
Iter: 1326 loss: 1.70145759e-05
Iter: 1327 loss: 1.69965806e-05
Iter: 1328 loss: 1.7066357e-05
Iter: 1329 loss: 1.69929299e-05
Iter: 1330 loss: 1.69760351e-05
Iter: 1331 loss: 1.69733285e-05
Iter: 1332 loss: 1.69619743e-05
Iter: 1333 loss: 1.69441519e-05
Iter: 1334 loss: 1.71770153e-05
Iter: 1335 loss: 1.69442756e-05
Iter: 1336 loss: 1.69284704e-05
Iter: 1337 loss: 1.69473351e-05
Iter: 1338 loss: 1.6920394e-05
Iter: 1339 loss: 1.69061059e-05
Iter: 1340 loss: 1.69788218e-05
Iter: 1341 loss: 1.69040068e-05
Iter: 1342 loss: 1.68905644e-05
Iter: 1343 loss: 1.68996594e-05
Iter: 1344 loss: 1.68823681e-05
Iter: 1345 loss: 1.68651786e-05
Iter: 1346 loss: 1.69237173e-05
Iter: 1347 loss: 1.68604129e-05
Iter: 1348 loss: 1.68465122e-05
Iter: 1349 loss: 1.68938022e-05
Iter: 1350 loss: 1.6843187e-05
Iter: 1351 loss: 1.68245442e-05
Iter: 1352 loss: 1.68755578e-05
Iter: 1353 loss: 1.68182523e-05
Iter: 1354 loss: 1.68114475e-05
Iter: 1355 loss: 1.68113093e-05
Iter: 1356 loss: 1.68041297e-05
Iter: 1357 loss: 1.67938488e-05
Iter: 1358 loss: 1.67935177e-05
Iter: 1359 loss: 1.67820144e-05
Iter: 1360 loss: 1.67728213e-05
Iter: 1361 loss: 1.67693888e-05
Iter: 1362 loss: 1.67540311e-05
Iter: 1363 loss: 1.6847629e-05
Iter: 1364 loss: 1.67520466e-05
Iter: 1365 loss: 1.67374637e-05
Iter: 1366 loss: 1.67301187e-05
Iter: 1367 loss: 1.67236594e-05
Iter: 1368 loss: 1.67077778e-05
Iter: 1369 loss: 1.69294799e-05
Iter: 1370 loss: 1.67079361e-05
Iter: 1371 loss: 1.66943009e-05
Iter: 1372 loss: 1.67164762e-05
Iter: 1373 loss: 1.66882473e-05
Iter: 1374 loss: 1.66715763e-05
Iter: 1375 loss: 1.66930113e-05
Iter: 1376 loss: 1.66628924e-05
Iter: 1377 loss: 1.66468089e-05
Iter: 1378 loss: 1.67162416e-05
Iter: 1379 loss: 1.66435657e-05
Iter: 1380 loss: 1.66267309e-05
Iter: 1381 loss: 1.66541431e-05
Iter: 1382 loss: 1.66192094e-05
Iter: 1383 loss: 1.66066129e-05
Iter: 1384 loss: 1.67081307e-05
Iter: 1385 loss: 1.66060127e-05
Iter: 1386 loss: 1.65925885e-05
Iter: 1387 loss: 1.66164173e-05
Iter: 1388 loss: 1.65867677e-05
Iter: 1389 loss: 1.65816855e-05
Iter: 1390 loss: 1.65804813e-05
Iter: 1391 loss: 1.65759902e-05
Iter: 1392 loss: 1.65646597e-05
Iter: 1393 loss: 1.66702575e-05
Iter: 1394 loss: 1.65628808e-05
Iter: 1395 loss: 1.65474121e-05
Iter: 1396 loss: 1.65349156e-05
Iter: 1397 loss: 1.65304536e-05
Iter: 1398 loss: 1.6511005e-05
Iter: 1399 loss: 1.66925383e-05
Iter: 1400 loss: 1.65102829e-05
Iter: 1401 loss: 1.64940793e-05
Iter: 1402 loss: 1.65209021e-05
Iter: 1403 loss: 1.64868707e-05
Iter: 1404 loss: 1.64699777e-05
Iter: 1405 loss: 1.65008423e-05
Iter: 1406 loss: 1.64625217e-05
Iter: 1407 loss: 1.64492067e-05
Iter: 1408 loss: 1.6449e-05
Iter: 1409 loss: 1.64395587e-05
Iter: 1410 loss: 1.64274679e-05
Iter: 1411 loss: 1.64265311e-05
Iter: 1412 loss: 1.6409409e-05
Iter: 1413 loss: 1.65323036e-05
Iter: 1414 loss: 1.6407741e-05
Iter: 1415 loss: 1.63940076e-05
Iter: 1416 loss: 1.6411519e-05
Iter: 1417 loss: 1.63871737e-05
Iter: 1418 loss: 1.63723835e-05
Iter: 1419 loss: 1.64763278e-05
Iter: 1420 loss: 1.63710665e-05
Iter: 1421 loss: 1.63602417e-05
Iter: 1422 loss: 1.645671e-05
Iter: 1423 loss: 1.63596524e-05
Iter: 1424 loss: 1.63522891e-05
Iter: 1425 loss: 1.64039739e-05
Iter: 1426 loss: 1.63516925e-05
Iter: 1427 loss: 1.63451641e-05
Iter: 1428 loss: 1.63290078e-05
Iter: 1429 loss: 1.64903086e-05
Iter: 1430 loss: 1.63270161e-05
Iter: 1431 loss: 1.63121786e-05
Iter: 1432 loss: 1.63456989e-05
Iter: 1433 loss: 1.63069526e-05
Iter: 1434 loss: 1.62910474e-05
Iter: 1435 loss: 1.63270306e-05
Iter: 1436 loss: 1.62853812e-05
Iter: 1437 loss: 1.62666474e-05
Iter: 1438 loss: 1.63158657e-05
Iter: 1439 loss: 1.62604956e-05
Iter: 1440 loss: 1.62423257e-05
Iter: 1441 loss: 1.63054101e-05
Iter: 1442 loss: 1.62374763e-05
Iter: 1443 loss: 1.6226335e-05
Iter: 1444 loss: 1.63377554e-05
Iter: 1445 loss: 1.62262622e-05
Iter: 1446 loss: 1.62152246e-05
Iter: 1447 loss: 1.62067881e-05
Iter: 1448 loss: 1.62032338e-05
Iter: 1449 loss: 1.61869757e-05
Iter: 1450 loss: 1.62842553e-05
Iter: 1451 loss: 1.61848657e-05
Iter: 1452 loss: 1.61740863e-05
Iter: 1453 loss: 1.6199363e-05
Iter: 1454 loss: 1.61699572e-05
Iter: 1455 loss: 1.615566e-05
Iter: 1456 loss: 1.61867e-05
Iter: 1457 loss: 1.6149821e-05
Iter: 1458 loss: 1.61421849e-05
Iter: 1459 loss: 1.61412299e-05
Iter: 1460 loss: 1.61356766e-05
Iter: 1461 loss: 1.61475e-05
Iter: 1462 loss: 1.61338176e-05
Iter: 1463 loss: 1.61269181e-05
Iter: 1464 loss: 1.61086591e-05
Iter: 1465 loss: 1.62398865e-05
Iter: 1466 loss: 1.61051e-05
Iter: 1467 loss: 1.60888339e-05
Iter: 1468 loss: 1.61665266e-05
Iter: 1469 loss: 1.6085789e-05
Iter: 1470 loss: 1.60692598e-05
Iter: 1471 loss: 1.6083326e-05
Iter: 1472 loss: 1.60591444e-05
Iter: 1473 loss: 1.60424788e-05
Iter: 1474 loss: 1.61487787e-05
Iter: 1475 loss: 1.6040729e-05
Iter: 1476 loss: 1.60246927e-05
Iter: 1477 loss: 1.60575419e-05
Iter: 1478 loss: 1.60181589e-05
Iter: 1479 loss: 1.6001617e-05
Iter: 1480 loss: 1.60434101e-05
Iter: 1481 loss: 1.59956689e-05
Iter: 1482 loss: 1.5978334e-05
Iter: 1483 loss: 1.61051412e-05
Iter: 1484 loss: 1.59771698e-05
Iter: 1485 loss: 1.59658011e-05
Iter: 1486 loss: 1.59618103e-05
Iter: 1487 loss: 1.59555966e-05
Iter: 1488 loss: 1.59389238e-05
Iter: 1489 loss: 1.60304589e-05
Iter: 1490 loss: 1.59365518e-05
Iter: 1491 loss: 1.59239025e-05
Iter: 1492 loss: 1.60009076e-05
Iter: 1493 loss: 1.59227311e-05
Iter: 1494 loss: 1.59126685e-05
Iter: 1495 loss: 1.60002492e-05
Iter: 1496 loss: 1.59121082e-05
Iter: 1497 loss: 1.59022875e-05
Iter: 1498 loss: 1.59035e-05
Iter: 1499 loss: 1.58948715e-05
Iter: 1500 loss: 1.58817202e-05
Iter: 1501 loss: 1.58925486e-05
Iter: 1502 loss: 1.58734329e-05
Iter: 1503 loss: 1.58605762e-05
Iter: 1504 loss: 1.58510647e-05
Iter: 1505 loss: 1.58469047e-05
Iter: 1506 loss: 1.58297044e-05
Iter: 1507 loss: 1.58562889e-05
Iter: 1508 loss: 1.5822101e-05
Iter: 1509 loss: 1.57994637e-05
Iter: 1510 loss: 1.59014089e-05
Iter: 1511 loss: 1.57953546e-05
Iter: 1512 loss: 1.57833056e-05
Iter: 1513 loss: 1.59008705e-05
Iter: 1514 loss: 1.57826598e-05
Iter: 1515 loss: 1.57732211e-05
Iter: 1516 loss: 1.5775644e-05
Iter: 1517 loss: 1.57663908e-05
Iter: 1518 loss: 1.5752601e-05
Iter: 1519 loss: 1.58304629e-05
Iter: 1520 loss: 1.57505019e-05
Iter: 1521 loss: 1.57383874e-05
Iter: 1522 loss: 1.57463019e-05
Iter: 1523 loss: 1.57305094e-05
Iter: 1524 loss: 1.57172835e-05
Iter: 1525 loss: 1.57597387e-05
Iter: 1526 loss: 1.57130944e-05
Iter: 1527 loss: 1.5701713e-05
Iter: 1528 loss: 1.58132862e-05
Iter: 1529 loss: 1.57015093e-05
Iter: 1530 loss: 1.56913247e-05
Iter: 1531 loss: 1.57468312e-05
Iter: 1532 loss: 1.56895148e-05
Iter: 1533 loss: 1.56795977e-05
Iter: 1534 loss: 1.56805727e-05
Iter: 1535 loss: 1.56718088e-05
Iter: 1536 loss: 1.56612878e-05
Iter: 1537 loss: 1.56968e-05
Iter: 1538 loss: 1.56585775e-05
Iter: 1539 loss: 1.56497936e-05
Iter: 1540 loss: 1.56329115e-05
Iter: 1541 loss: 1.5971289e-05
Iter: 1542 loss: 1.56327696e-05
Iter: 1543 loss: 1.56145397e-05
Iter: 1544 loss: 1.57267277e-05
Iter: 1545 loss: 1.56125243e-05
Iter: 1546 loss: 1.55971375e-05
Iter: 1547 loss: 1.5622496e-05
Iter: 1548 loss: 1.55899797e-05
Iter: 1549 loss: 1.55759099e-05
Iter: 1550 loss: 1.56507485e-05
Iter: 1551 loss: 1.55740054e-05
Iter: 1552 loss: 1.55581238e-05
Iter: 1553 loss: 1.55803518e-05
Iter: 1554 loss: 1.55499984e-05
Iter: 1555 loss: 1.55390881e-05
Iter: 1556 loss: 1.57013073e-05
Iter: 1557 loss: 1.55391117e-05
Iter: 1558 loss: 1.55302259e-05
Iter: 1559 loss: 1.55191483e-05
Iter: 1560 loss: 1.55186244e-05
Iter: 1561 loss: 1.55020134e-05
Iter: 1562 loss: 1.55844937e-05
Iter: 1563 loss: 1.54995378e-05
Iter: 1564 loss: 1.54901772e-05
Iter: 1565 loss: 1.54903082e-05
Iter: 1566 loss: 1.54815898e-05
Iter: 1567 loss: 1.55029775e-05
Iter: 1568 loss: 1.54783265e-05
Iter: 1569 loss: 1.54699974e-05
Iter: 1570 loss: 1.54778645e-05
Iter: 1571 loss: 1.54651752e-05
Iter: 1572 loss: 1.54563531e-05
Iter: 1573 loss: 1.54595255e-05
Iter: 1574 loss: 1.54501686e-05
Iter: 1575 loss: 1.54362078e-05
Iter: 1576 loss: 1.54268255e-05
Iter: 1577 loss: 1.54218578e-05
Iter: 1578 loss: 1.54019399e-05
Iter: 1579 loss: 1.54432291e-05
Iter: 1580 loss: 1.53939673e-05
Iter: 1581 loss: 1.53791916e-05
Iter: 1582 loss: 1.54626905e-05
Iter: 1583 loss: 1.53772417e-05
Iter: 1584 loss: 1.53612636e-05
Iter: 1585 loss: 1.53742767e-05
Iter: 1586 loss: 1.53520396e-05
Iter: 1587 loss: 1.53387118e-05
Iter: 1588 loss: 1.53387336e-05
Iter: 1589 loss: 1.53289438e-05
Iter: 1590 loss: 1.53345645e-05
Iter: 1591 loss: 1.53231904e-05
Iter: 1592 loss: 1.53099209e-05
Iter: 1593 loss: 1.53631e-05
Iter: 1594 loss: 1.53072815e-05
Iter: 1595 loss: 1.52980465e-05
Iter: 1596 loss: 1.53111141e-05
Iter: 1597 loss: 1.52931843e-05
Iter: 1598 loss: 1.52868142e-05
Iter: 1599 loss: 1.5286334e-05
Iter: 1600 loss: 1.52800603e-05
Iter: 1601 loss: 1.52779976e-05
Iter: 1602 loss: 1.52742614e-05
Iter: 1603 loss: 1.52656685e-05
Iter: 1604 loss: 1.52816028e-05
Iter: 1605 loss: 1.52619323e-05
Iter: 1606 loss: 1.5252981e-05
Iter: 1607 loss: 1.52566918e-05
Iter: 1608 loss: 1.52466928e-05
Iter: 1609 loss: 1.52349348e-05
Iter: 1610 loss: 1.52559587e-05
Iter: 1611 loss: 1.52297698e-05
Iter: 1612 loss: 1.52180146e-05
Iter: 1613 loss: 1.52139701e-05
Iter: 1614 loss: 1.52072425e-05
Iter: 1615 loss: 1.51882487e-05
Iter: 1616 loss: 1.52537332e-05
Iter: 1617 loss: 1.5183241e-05
Iter: 1618 loss: 1.51663808e-05
Iter: 1619 loss: 1.52210068e-05
Iter: 1620 loss: 1.51620516e-05
Iter: 1621 loss: 1.51506565e-05
Iter: 1622 loss: 1.53022302e-05
Iter: 1623 loss: 1.51504373e-05
Iter: 1624 loss: 1.51402382e-05
Iter: 1625 loss: 1.5129096e-05
Iter: 1626 loss: 1.51274753e-05
Iter: 1627 loss: 1.51084896e-05
Iter: 1628 loss: 1.52609427e-05
Iter: 1629 loss: 1.51072045e-05
Iter: 1630 loss: 1.50987507e-05
Iter: 1631 loss: 1.51249897e-05
Iter: 1632 loss: 1.50961714e-05
Iter: 1633 loss: 1.50862361e-05
Iter: 1634 loss: 1.51820877e-05
Iter: 1635 loss: 1.50856558e-05
Iter: 1636 loss: 1.50785363e-05
Iter: 1637 loss: 1.50761834e-05
Iter: 1638 loss: 1.50723918e-05
Iter: 1639 loss: 1.50639207e-05
Iter: 1640 loss: 1.51012964e-05
Iter: 1641 loss: 1.50624628e-05
Iter: 1642 loss: 1.50553578e-05
Iter: 1643 loss: 1.50441965e-05
Iter: 1644 loss: 1.50441765e-05
Iter: 1645 loss: 1.50284332e-05
Iter: 1646 loss: 1.51081395e-05
Iter: 1647 loss: 1.50259839e-05
Iter: 1648 loss: 1.50145515e-05
Iter: 1649 loss: 1.50160558e-05
Iter: 1650 loss: 1.50059259e-05
Iter: 1651 loss: 1.49918269e-05
Iter: 1652 loss: 1.5026757e-05
Iter: 1653 loss: 1.49867401e-05
Iter: 1654 loss: 1.49710331e-05
Iter: 1655 loss: 1.50335336e-05
Iter: 1656 loss: 1.49675407e-05
Iter: 1657 loss: 1.49546049e-05
Iter: 1658 loss: 1.50415026e-05
Iter: 1659 loss: 1.49533989e-05
Iter: 1660 loss: 1.49409425e-05
Iter: 1661 loss: 1.49585858e-05
Iter: 1662 loss: 1.49351963e-05
Iter: 1663 loss: 1.49230218e-05
Iter: 1664 loss: 1.49730195e-05
Iter: 1665 loss: 1.49204297e-05
Iter: 1666 loss: 1.49108182e-05
Iter: 1667 loss: 1.49829712e-05
Iter: 1668 loss: 1.49097514e-05
Iter: 1669 loss: 1.4899656e-05
Iter: 1670 loss: 1.49393381e-05
Iter: 1671 loss: 1.48969248e-05
Iter: 1672 loss: 1.48902254e-05
Iter: 1673 loss: 1.48841555e-05
Iter: 1674 loss: 1.48822919e-05
Iter: 1675 loss: 1.48685267e-05
Iter: 1676 loss: 1.49050866e-05
Iter: 1677 loss: 1.48640529e-05
Iter: 1678 loss: 1.48533227e-05
Iter: 1679 loss: 1.48623431e-05
Iter: 1680 loss: 1.48466488e-05
Iter: 1681 loss: 1.48340032e-05
Iter: 1682 loss: 1.48625832e-05
Iter: 1683 loss: 1.48292556e-05
Iter: 1684 loss: 1.48152212e-05
Iter: 1685 loss: 1.48368663e-05
Iter: 1686 loss: 1.48088166e-05
Iter: 1687 loss: 1.47952842e-05
Iter: 1688 loss: 1.4799939e-05
Iter: 1689 loss: 1.47863284e-05
Iter: 1690 loss: 1.47674218e-05
Iter: 1691 loss: 1.4891154e-05
Iter: 1692 loss: 1.47652754e-05
Iter: 1693 loss: 1.47550327e-05
Iter: 1694 loss: 1.4815103e-05
Iter: 1695 loss: 1.47535839e-05
Iter: 1696 loss: 1.47403625e-05
Iter: 1697 loss: 1.47422288e-05
Iter: 1698 loss: 1.47304363e-05
Iter: 1699 loss: 1.47187739e-05
Iter: 1700 loss: 1.48426598e-05
Iter: 1701 loss: 1.4718632e-05
Iter: 1702 loss: 1.47121482e-05
Iter: 1703 loss: 1.47119199e-05
Iter: 1704 loss: 1.47061819e-05
Iter: 1705 loss: 1.47002265e-05
Iter: 1706 loss: 1.46993625e-05
Iter: 1707 loss: 1.46921357e-05
Iter: 1708 loss: 1.47123592e-05
Iter: 1709 loss: 1.46900038e-05
Iter: 1710 loss: 1.46806824e-05
Iter: 1711 loss: 1.46791081e-05
Iter: 1712 loss: 1.46727634e-05
Iter: 1713 loss: 1.46610255e-05
Iter: 1714 loss: 1.46899738e-05
Iter: 1715 loss: 1.46571638e-05
Iter: 1716 loss: 1.464462e-05
Iter: 1717 loss: 1.46527382e-05
Iter: 1718 loss: 1.46369184e-05
Iter: 1719 loss: 1.46199709e-05
Iter: 1720 loss: 1.46786397e-05
Iter: 1721 loss: 1.46157236e-05
Iter: 1722 loss: 1.46025968e-05
Iter: 1723 loss: 1.45968788e-05
Iter: 1724 loss: 1.45903059e-05
Iter: 1725 loss: 1.45746235e-05
Iter: 1726 loss: 1.47674491e-05
Iter: 1727 loss: 1.4574478e-05
Iter: 1728 loss: 1.45636823e-05
Iter: 1729 loss: 1.46017901e-05
Iter: 1730 loss: 1.45607119e-05
Iter: 1731 loss: 1.45472568e-05
Iter: 1732 loss: 1.45630393e-05
Iter: 1733 loss: 1.45395534e-05
Iter: 1734 loss: 1.45297254e-05
Iter: 1735 loss: 1.46517868e-05
Iter: 1736 loss: 1.45299009e-05
Iter: 1737 loss: 1.45211043e-05
Iter: 1738 loss: 1.4585291e-05
Iter: 1739 loss: 1.45206668e-05
Iter: 1740 loss: 1.45148779e-05
Iter: 1741 loss: 1.45065142e-05
Iter: 1742 loss: 1.45062313e-05
Iter: 1743 loss: 1.44964433e-05
Iter: 1744 loss: 1.45493868e-05
Iter: 1745 loss: 1.44950091e-05
Iter: 1746 loss: 1.44858095e-05
Iter: 1747 loss: 1.44955711e-05
Iter: 1748 loss: 1.44807045e-05
Iter: 1749 loss: 1.44691412e-05
Iter: 1750 loss: 1.44610622e-05
Iter: 1751 loss: 1.44571686e-05
Iter: 1752 loss: 1.44431715e-05
Iter: 1753 loss: 1.45902477e-05
Iter: 1754 loss: 1.44430214e-05
Iter: 1755 loss: 1.44344376e-05
Iter: 1756 loss: 1.44534633e-05
Iter: 1757 loss: 1.44312271e-05
Iter: 1758 loss: 1.44215319e-05
Iter: 1759 loss: 1.44098376e-05
Iter: 1760 loss: 1.44084697e-05
Iter: 1761 loss: 1.43953039e-05
Iter: 1762 loss: 1.45552258e-05
Iter: 1763 loss: 1.43950383e-05
Iter: 1764 loss: 1.43830821e-05
Iter: 1765 loss: 1.44040914e-05
Iter: 1766 loss: 1.43775924e-05
Iter: 1767 loss: 1.43628913e-05
Iter: 1768 loss: 1.4440825e-05
Iter: 1769 loss: 1.43607831e-05
Iter: 1770 loss: 1.43536854e-05
Iter: 1771 loss: 1.44590913e-05
Iter: 1772 loss: 1.43537154e-05
Iter: 1773 loss: 1.43457582e-05
Iter: 1774 loss: 1.43396755e-05
Iter: 1775 loss: 1.43372345e-05
Iter: 1776 loss: 1.43278994e-05
Iter: 1777 loss: 1.43492225e-05
Iter: 1778 loss: 1.43245479e-05
Iter: 1779 loss: 1.43154584e-05
Iter: 1780 loss: 1.43405159e-05
Iter: 1781 loss: 1.43127218e-05
Iter: 1782 loss: 1.43024881e-05
Iter: 1783 loss: 1.4314116e-05
Iter: 1784 loss: 1.42966965e-05
Iter: 1785 loss: 1.42869212e-05
Iter: 1786 loss: 1.42876852e-05
Iter: 1787 loss: 1.42790341e-05
Iter: 1788 loss: 1.42648851e-05
Iter: 1789 loss: 1.43300513e-05
Iter: 1790 loss: 1.42618383e-05
Iter: 1791 loss: 1.42514928e-05
Iter: 1792 loss: 1.42894969e-05
Iter: 1793 loss: 1.42489871e-05
Iter: 1794 loss: 1.42375702e-05
Iter: 1795 loss: 1.4228739e-05
Iter: 1796 loss: 1.4225282e-05
Iter: 1797 loss: 1.42104673e-05
Iter: 1798 loss: 1.42977333e-05
Iter: 1799 loss: 1.42082954e-05
Iter: 1800 loss: 1.41946275e-05
Iter: 1801 loss: 1.42802364e-05
Iter: 1802 loss: 1.41931305e-05
Iter: 1803 loss: 1.41810733e-05
Iter: 1804 loss: 1.42284189e-05
Iter: 1805 loss: 1.41786177e-05
Iter: 1806 loss: 1.41727814e-05
Iter: 1807 loss: 1.41722985e-05
Iter: 1808 loss: 1.41674991e-05
Iter: 1809 loss: 1.41561977e-05
Iter: 1810 loss: 1.42875197e-05
Iter: 1811 loss: 1.41552955e-05
Iter: 1812 loss: 1.41450328e-05
Iter: 1813 loss: 1.42233703e-05
Iter: 1814 loss: 1.41444207e-05
Iter: 1815 loss: 1.41360852e-05
Iter: 1816 loss: 1.41594246e-05
Iter: 1817 loss: 1.41337114e-05
Iter: 1818 loss: 1.41244673e-05
Iter: 1819 loss: 1.41211376e-05
Iter: 1820 loss: 1.41160372e-05
Iter: 1821 loss: 1.41038672e-05
Iter: 1822 loss: 1.41400387e-05
Iter: 1823 loss: 1.41000073e-05
Iter: 1824 loss: 1.40889952e-05
Iter: 1825 loss: 1.4111708e-05
Iter: 1826 loss: 1.40844077e-05
Iter: 1827 loss: 1.40727043e-05
Iter: 1828 loss: 1.41208493e-05
Iter: 1829 loss: 1.40700977e-05
Iter: 1830 loss: 1.40579396e-05
Iter: 1831 loss: 1.40505081e-05
Iter: 1832 loss: 1.40456596e-05
Iter: 1833 loss: 1.40310394e-05
Iter: 1834 loss: 1.41228065e-05
Iter: 1835 loss: 1.40296888e-05
Iter: 1836 loss: 1.40191169e-05
Iter: 1837 loss: 1.41291985e-05
Iter: 1838 loss: 1.40186994e-05
Iter: 1839 loss: 1.40102102e-05
Iter: 1840 loss: 1.40448683e-05
Iter: 1841 loss: 1.40082448e-05
Iter: 1842 loss: 1.39986842e-05
Iter: 1843 loss: 1.40536476e-05
Iter: 1844 loss: 1.39974918e-05
Iter: 1845 loss: 1.39918129e-05
Iter: 1846 loss: 1.39839758e-05
Iter: 1847 loss: 1.39838785e-05
Iter: 1848 loss: 1.39748381e-05
Iter: 1849 loss: 1.4011177e-05
Iter: 1850 loss: 1.3972638e-05
Iter: 1851 loss: 1.3961112e-05
Iter: 1852 loss: 1.39701624e-05
Iter: 1853 loss: 1.39539297e-05
Iter: 1854 loss: 1.39424319e-05
Iter: 1855 loss: 1.39900121e-05
Iter: 1856 loss: 1.3939909e-05
Iter: 1857 loss: 1.39300137e-05
Iter: 1858 loss: 1.39233198e-05
Iter: 1859 loss: 1.39194344e-05
Iter: 1860 loss: 1.39066888e-05
Iter: 1861 loss: 1.40149314e-05
Iter: 1862 loss: 1.39059703e-05
Iter: 1863 loss: 1.38948453e-05
Iter: 1864 loss: 1.39006015e-05
Iter: 1865 loss: 1.38875166e-05
Iter: 1866 loss: 1.38754731e-05
Iter: 1867 loss: 1.39403019e-05
Iter: 1868 loss: 1.38733849e-05
Iter: 1869 loss: 1.38634332e-05
Iter: 1870 loss: 1.38525938e-05
Iter: 1871 loss: 1.38508858e-05
Iter: 1872 loss: 1.38390787e-05
Iter: 1873 loss: 1.38383157e-05
Iter: 1874 loss: 1.38330106e-05
Iter: 1875 loss: 1.38331334e-05
Iter: 1876 loss: 1.38283867e-05
Iter: 1877 loss: 1.38355026e-05
Iter: 1878 loss: 1.38258256e-05
Iter: 1879 loss: 1.3820978e-05
Iter: 1880 loss: 1.38100277e-05
Iter: 1881 loss: 1.39870226e-05
Iter: 1882 loss: 1.38098385e-05
Iter: 1883 loss: 1.37992174e-05
Iter: 1884 loss: 1.38813166e-05
Iter: 1885 loss: 1.37985689e-05
Iter: 1886 loss: 1.37879078e-05
Iter: 1887 loss: 1.38163468e-05
Iter: 1888 loss: 1.37846373e-05
Iter: 1889 loss: 1.37755e-05
Iter: 1890 loss: 1.37928528e-05
Iter: 1891 loss: 1.37717234e-05
Iter: 1892 loss: 1.37622337e-05
Iter: 1893 loss: 1.37637235e-05
Iter: 1894 loss: 1.37553961e-05
Iter: 1895 loss: 1.3741691e-05
Iter: 1896 loss: 1.3778119e-05
Iter: 1897 loss: 1.37374382e-05
Iter: 1898 loss: 1.37232755e-05
Iter: 1899 loss: 1.37436036e-05
Iter: 1900 loss: 1.37163124e-05
Iter: 1901 loss: 1.37010538e-05
Iter: 1902 loss: 1.38033911e-05
Iter: 1903 loss: 1.36996223e-05
Iter: 1904 loss: 1.36896506e-05
Iter: 1905 loss: 1.36869403e-05
Iter: 1906 loss: 1.3680793e-05
Iter: 1907 loss: 1.36709423e-05
Iter: 1908 loss: 1.37950547e-05
Iter: 1909 loss: 1.3670955e-05
Iter: 1910 loss: 1.36637964e-05
Iter: 1911 loss: 1.3748213e-05
Iter: 1912 loss: 1.36636972e-05
Iter: 1913 loss: 1.36560593e-05
Iter: 1914 loss: 1.36621238e-05
Iter: 1915 loss: 1.36519029e-05
Iter: 1916 loss: 1.36440749e-05
Iter: 1917 loss: 1.36415183e-05
Iter: 1918 loss: 1.36371509e-05
Iter: 1919 loss: 1.36283761e-05
Iter: 1920 loss: 1.36389835e-05
Iter: 1921 loss: 1.36239523e-05
Iter: 1922 loss: 1.3614268e-05
Iter: 1923 loss: 1.37195739e-05
Iter: 1924 loss: 1.36138806e-05
Iter: 1925 loss: 1.36064091e-05
Iter: 1926 loss: 1.35994096e-05
Iter: 1927 loss: 1.35973141e-05
Iter: 1928 loss: 1.35881692e-05
Iter: 1929 loss: 1.36654799e-05
Iter: 1930 loss: 1.35874616e-05
Iter: 1931 loss: 1.3580373e-05
Iter: 1932 loss: 1.3571509e-05
Iter: 1933 loss: 1.35710397e-05
Iter: 1934 loss: 1.35584396e-05
Iter: 1935 loss: 1.36633025e-05
Iter: 1936 loss: 1.35577939e-05
Iter: 1937 loss: 1.35488253e-05
Iter: 1938 loss: 1.35542359e-05
Iter: 1939 loss: 1.35433138e-05
Iter: 1940 loss: 1.35299915e-05
Iter: 1941 loss: 1.35633054e-05
Iter: 1942 loss: 1.35252212e-05
Iter: 1943 loss: 1.35146593e-05
Iter: 1944 loss: 1.35493983e-05
Iter: 1945 loss: 1.35121454e-05
Iter: 1946 loss: 1.35049359e-05
Iter: 1947 loss: 1.35043047e-05
Iter: 1948 loss: 1.34996235e-05
Iter: 1949 loss: 1.34998254e-05
Iter: 1950 loss: 1.34962029e-05
Iter: 1951 loss: 1.34902148e-05
Iter: 1952 loss: 1.34792263e-05
Iter: 1953 loss: 1.34794691e-05
Iter: 1954 loss: 1.34661932e-05
Iter: 1955 loss: 1.35352384e-05
Iter: 1956 loss: 1.34642878e-05
Iter: 1957 loss: 1.34551556e-05
Iter: 1958 loss: 1.35528771e-05
Iter: 1959 loss: 1.34550155e-05
Iter: 1960 loss: 1.3447403e-05
Iter: 1961 loss: 1.34378342e-05
Iter: 1962 loss: 1.34371376e-05
Iter: 1963 loss: 1.34270394e-05
Iter: 1964 loss: 1.34976835e-05
Iter: 1965 loss: 1.34262027e-05
Iter: 1966 loss: 1.34157081e-05
Iter: 1967 loss: 1.34028251e-05
Iter: 1968 loss: 1.34018937e-05
Iter: 1969 loss: 1.33892154e-05
Iter: 1970 loss: 1.33892263e-05
Iter: 1971 loss: 1.33807853e-05
Iter: 1972 loss: 1.33796821e-05
Iter: 1973 loss: 1.3373442e-05
Iter: 1974 loss: 1.3360519e-05
Iter: 1975 loss: 1.34117381e-05
Iter: 1976 loss: 1.33574813e-05
Iter: 1977 loss: 1.33485537e-05
Iter: 1978 loss: 1.34086322e-05
Iter: 1979 loss: 1.33474696e-05
Iter: 1980 loss: 1.33410922e-05
Iter: 1981 loss: 1.33410485e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2.4
+ date
Sun Nov  8 17:04:04 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 2.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe48f194158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe48f11e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe48f1b06a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe48f1d79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe48f1d76a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe48f1d7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe48efe46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe48f0217b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe48effb1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe48effba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe46db108c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe46db48840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe46db48b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe46daa0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe46daa0400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe46daa02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe48f083598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4482fdbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe44829c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4482a0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4482938c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe46da1a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4481c9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4481e3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4481e3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4481ccae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4481c3598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4481b8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe448169378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe448175bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe44810d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4480a91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4480a9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4480bc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4480d4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe4480d4bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 202, in <module>
    grads = tape.gradient(loss, model.trainable_weights)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1266, in _backward_function_wrapper
    processed_args, remapped_captures)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input is not invertible.
	 [[node gradients/MatrixDeterminant_grad/MatrixInverse (defined at biholoNN_train.py:200) ]] [Op:__inference___backward_volume_form_4446_8945]

Function call stack:
__backward_volume_form_4446

+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2.4/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6c9c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6da7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6da7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6da72f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6cfa268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6c6b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6c2ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6bebae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6beb730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6badae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6beb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6b57400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6b7b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6b7b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6b7bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6ae78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6b06598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6b06d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6a7c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6a74f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c6a12950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84c69ceae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8487cea730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8487d13730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8487d13620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8487d137b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8487ce49d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8487c8cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8487c8c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8487c8c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8487bfce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8487c242f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8487c24bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f846020d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84601bf6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f84601bfb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000140117656
Iter: 2 loss: 0.00147721684
Iter: 3 loss: 0.000125153034
Iter: 4 loss: 0.000118348849
Iter: 5 loss: 0.000111596266
Iter: 6 loss: 0.000110172667
Iter: 7 loss: 0.000104036728
Iter: 8 loss: 0.000120040757
Iter: 9 loss: 0.00010198298
Iter: 10 loss: 9.80350378e-05
Iter: 11 loss: 9.26824578e-05
Iter: 12 loss: 9.24137421e-05
Iter: 13 loss: 8.43307134e-05
Iter: 14 loss: 8.92845565e-05
Iter: 15 loss: 7.91484781e-05
Iter: 16 loss: 7.14134658e-05
Iter: 17 loss: 0.00010987241
Iter: 18 loss: 7.01045647e-05
Iter: 19 loss: 6.50688235e-05
Iter: 20 loss: 8.00028938e-05
Iter: 21 loss: 6.35080069e-05
Iter: 22 loss: 5.93051591e-05
Iter: 23 loss: 6.5702654e-05
Iter: 24 loss: 5.73131583e-05
Iter: 25 loss: 5.37026899e-05
Iter: 26 loss: 7.76757079e-05
Iter: 27 loss: 5.33326202e-05
Iter: 28 loss: 5.1176281e-05
Iter: 29 loss: 5.56138693e-05
Iter: 30 loss: 5.03127885e-05
Iter: 31 loss: 4.78762959e-05
Iter: 32 loss: 5.18418e-05
Iter: 33 loss: 4.67576247e-05
Iter: 34 loss: 4.48914252e-05
Iter: 35 loss: 6.07917464e-05
Iter: 36 loss: 4.47861385e-05
Iter: 37 loss: 4.32818415e-05
Iter: 38 loss: 4.40323856e-05
Iter: 39 loss: 4.2275351e-05
Iter: 40 loss: 4.33540736e-05
Iter: 41 loss: 4.18959462e-05
Iter: 42 loss: 4.13894741e-05
Iter: 43 loss: 4.03719641e-05
Iter: 44 loss: 5.9367514e-05
Iter: 45 loss: 4.03576232e-05
Iter: 46 loss: 3.95547177e-05
Iter: 47 loss: 4.11687433e-05
Iter: 48 loss: 3.92253205e-05
Iter: 49 loss: 3.83765946e-05
Iter: 50 loss: 4.11467081e-05
Iter: 51 loss: 3.81369064e-05
Iter: 52 loss: 3.75311174e-05
Iter: 53 loss: 3.67564498e-05
Iter: 54 loss: 3.67013054e-05
Iter: 55 loss: 3.59092883e-05
Iter: 56 loss: 3.59012047e-05
Iter: 57 loss: 3.54671647e-05
Iter: 58 loss: 3.55507655e-05
Iter: 59 loss: 3.51443232e-05
Iter: 60 loss: 3.4332159e-05
Iter: 61 loss: 3.38447026e-05
Iter: 62 loss: 3.35104e-05
Iter: 63 loss: 3.29315371e-05
Iter: 64 loss: 3.29140639e-05
Iter: 65 loss: 3.24771681e-05
Iter: 66 loss: 3.17168378e-05
Iter: 67 loss: 3.17177582e-05
Iter: 68 loss: 3.10385331e-05
Iter: 69 loss: 3.1033378e-05
Iter: 70 loss: 3.06854708e-05
Iter: 71 loss: 3.03866454e-05
Iter: 72 loss: 3.02918852e-05
Iter: 73 loss: 3.07260743e-05
Iter: 74 loss: 3.00830543e-05
Iter: 75 loss: 2.99105195e-05
Iter: 76 loss: 2.95278987e-05
Iter: 77 loss: 3.51271519e-05
Iter: 78 loss: 2.95102982e-05
Iter: 79 loss: 2.92574241e-05
Iter: 80 loss: 2.94713536e-05
Iter: 81 loss: 2.91074393e-05
Iter: 82 loss: 2.87194016e-05
Iter: 83 loss: 3.2829812e-05
Iter: 84 loss: 2.87106268e-05
Iter: 85 loss: 2.83957e-05
Iter: 86 loss: 2.8272385e-05
Iter: 87 loss: 2.81028661e-05
Iter: 88 loss: 2.77277e-05
Iter: 89 loss: 2.98645937e-05
Iter: 90 loss: 2.767767e-05
Iter: 91 loss: 2.73809364e-05
Iter: 92 loss: 2.77771505e-05
Iter: 93 loss: 2.72321558e-05
Iter: 94 loss: 2.69671546e-05
Iter: 95 loss: 2.69790871e-05
Iter: 96 loss: 2.67573796e-05
Iter: 97 loss: 2.64142818e-05
Iter: 98 loss: 2.84197613e-05
Iter: 99 loss: 2.6369933e-05
Iter: 100 loss: 2.61031255e-05
Iter: 101 loss: 2.65026629e-05
Iter: 102 loss: 2.5974754e-05
Iter: 103 loss: 2.56376807e-05
Iter: 104 loss: 2.62861504e-05
Iter: 105 loss: 2.549493e-05
Iter: 106 loss: 2.53372637e-05
Iter: 107 loss: 2.53120561e-05
Iter: 108 loss: 2.50962476e-05
Iter: 109 loss: 2.57782449e-05
Iter: 110 loss: 2.50350276e-05
Iter: 111 loss: 2.49404493e-05
Iter: 112 loss: 2.47650169e-05
Iter: 113 loss: 2.87802977e-05
Iter: 114 loss: 2.47649932e-05
Iter: 115 loss: 2.45925294e-05
Iter: 116 loss: 2.48510205e-05
Iter: 117 loss: 2.45095325e-05
Iter: 118 loss: 2.43108589e-05
Iter: 119 loss: 2.66186144e-05
Iter: 120 loss: 2.4308978e-05
Iter: 121 loss: 2.41911657e-05
Iter: 122 loss: 2.42128917e-05
Iter: 123 loss: 2.41030903e-05
Iter: 124 loss: 2.39618421e-05
Iter: 125 loss: 2.41582e-05
Iter: 126 loss: 2.3890545e-05
Iter: 127 loss: 2.37062595e-05
Iter: 128 loss: 2.42215574e-05
Iter: 129 loss: 2.36466949e-05
Iter: 130 loss: 2.34549589e-05
Iter: 131 loss: 2.3892253e-05
Iter: 132 loss: 2.33836872e-05
Iter: 133 loss: 2.32654093e-05
Iter: 134 loss: 2.35560983e-05
Iter: 135 loss: 2.32225721e-05
Iter: 136 loss: 2.30549958e-05
Iter: 137 loss: 2.29870493e-05
Iter: 138 loss: 2.28966564e-05
Iter: 139 loss: 2.28077588e-05
Iter: 140 loss: 2.27929013e-05
Iter: 141 loss: 2.27758828e-05
Iter: 142 loss: 2.27531818e-05
Iter: 143 loss: 2.27296841e-05
Iter: 144 loss: 2.26510092e-05
Iter: 145 loss: 2.26785833e-05
Iter: 146 loss: 2.25767326e-05
Iter: 147 loss: 2.24530449e-05
Iter: 148 loss: 2.31806298e-05
Iter: 149 loss: 2.24370051e-05
Iter: 150 loss: 2.23386287e-05
Iter: 151 loss: 2.26494121e-05
Iter: 152 loss: 2.23107163e-05
Iter: 153 loss: 2.21827686e-05
Iter: 154 loss: 2.23951647e-05
Iter: 155 loss: 2.21248683e-05
Iter: 156 loss: 2.20132752e-05
Iter: 157 loss: 2.2186865e-05
Iter: 158 loss: 2.19601316e-05
Iter: 159 loss: 2.18616515e-05
Iter: 160 loss: 2.20792936e-05
Iter: 161 loss: 2.18239e-05
Iter: 162 loss: 2.17089255e-05
Iter: 163 loss: 2.22865219e-05
Iter: 164 loss: 2.16893823e-05
Iter: 165 loss: 2.1604068e-05
Iter: 166 loss: 2.15497348e-05
Iter: 167 loss: 2.15144901e-05
Iter: 168 loss: 2.14108477e-05
Iter: 169 loss: 2.23473889e-05
Iter: 170 loss: 2.14052816e-05
Iter: 171 loss: 2.13278399e-05
Iter: 172 loss: 2.13303538e-05
Iter: 173 loss: 2.12661362e-05
Iter: 174 loss: 2.12922132e-05
Iter: 175 loss: 2.12248942e-05
Iter: 176 loss: 2.11733986e-05
Iter: 177 loss: 2.10658745e-05
Iter: 178 loss: 2.2844597e-05
Iter: 179 loss: 2.10640228e-05
Iter: 180 loss: 2.10110629e-05
Iter: 181 loss: 2.10405324e-05
Iter: 182 loss: 2.09776954e-05
Iter: 183 loss: 2.08935235e-05
Iter: 184 loss: 2.09761783e-05
Iter: 185 loss: 2.08458205e-05
Iter: 186 loss: 2.0770698e-05
Iter: 187 loss: 2.19159592e-05
Iter: 188 loss: 2.07701651e-05
Iter: 189 loss: 2.07232479e-05
Iter: 190 loss: 2.06970981e-05
Iter: 191 loss: 2.06762015e-05
Iter: 192 loss: 2.05932956e-05
Iter: 193 loss: 2.06052355e-05
Iter: 194 loss: 2.05312717e-05
Iter: 195 loss: 2.04501375e-05
Iter: 196 loss: 2.13991843e-05
Iter: 197 loss: 2.04501448e-05
Iter: 198 loss: 2.03935851e-05
Iter: 199 loss: 2.04726475e-05
Iter: 200 loss: 2.03667041e-05
Iter: 201 loss: 2.02998508e-05
Iter: 202 loss: 2.0346677e-05
Iter: 203 loss: 2.02575102e-05
Iter: 204 loss: 2.01742332e-05
Iter: 205 loss: 2.03488908e-05
Iter: 206 loss: 2.01432522e-05
Iter: 207 loss: 2.01038856e-05
Iter: 208 loss: 2.01003731e-05
Iter: 209 loss: 2.00595186e-05
Iter: 210 loss: 2.03998898e-05
Iter: 211 loss: 2.00558679e-05
Iter: 212 loss: 2.00407976e-05
Iter: 213 loss: 1.9992036e-05
Iter: 214 loss: 2.00224422e-05
Iter: 215 loss: 1.99477709e-05
Iter: 216 loss: 1.98740236e-05
Iter: 217 loss: 2.07265693e-05
Iter: 218 loss: 1.98717953e-05
Iter: 219 loss: 1.98324251e-05
Iter: 220 loss: 2.02926476e-05
Iter: 221 loss: 1.98309717e-05
Iter: 222 loss: 1.97943737e-05
Iter: 223 loss: 1.9775749e-05
Iter: 224 loss: 1.97572826e-05
Iter: 225 loss: 1.96983401e-05
Iter: 226 loss: 1.98015241e-05
Iter: 227 loss: 1.96724286e-05
Iter: 228 loss: 1.96202382e-05
Iter: 229 loss: 1.98329981e-05
Iter: 230 loss: 1.96084438e-05
Iter: 231 loss: 1.95677749e-05
Iter: 232 loss: 1.9684343e-05
Iter: 233 loss: 1.95550328e-05
Iter: 234 loss: 1.95004104e-05
Iter: 235 loss: 1.95214598e-05
Iter: 236 loss: 1.94624299e-05
Iter: 237 loss: 1.94125132e-05
Iter: 238 loss: 1.96144974e-05
Iter: 239 loss: 1.94017448e-05
Iter: 240 loss: 1.93487376e-05
Iter: 241 loss: 1.94248878e-05
Iter: 242 loss: 1.9323732e-05
Iter: 243 loss: 1.93774758e-05
Iter: 244 loss: 1.93035357e-05
Iter: 245 loss: 1.92925945e-05
Iter: 246 loss: 1.92570333e-05
Iter: 247 loss: 1.93077649e-05
Iter: 248 loss: 1.92330263e-05
Iter: 249 loss: 1.91883937e-05
Iter: 250 loss: 1.91822764e-05
Iter: 251 loss: 1.91512772e-05
Iter: 252 loss: 1.90982428e-05
Iter: 253 loss: 1.98775015e-05
Iter: 254 loss: 1.9097939e-05
Iter: 255 loss: 1.90564606e-05
Iter: 256 loss: 1.92408861e-05
Iter: 257 loss: 1.90483461e-05
Iter: 258 loss: 1.90155879e-05
Iter: 259 loss: 1.90108403e-05
Iter: 260 loss: 1.89881175e-05
Iter: 261 loss: 1.89434832e-05
Iter: 262 loss: 1.9038991e-05
Iter: 263 loss: 1.89253878e-05
Iter: 264 loss: 1.88767954e-05
Iter: 265 loss: 1.89959283e-05
Iter: 266 loss: 1.8858791e-05
Iter: 267 loss: 1.88165159e-05
Iter: 268 loss: 1.91517538e-05
Iter: 269 loss: 1.88140748e-05
Iter: 270 loss: 1.87874648e-05
Iter: 271 loss: 1.87797104e-05
Iter: 272 loss: 1.87630703e-05
Iter: 273 loss: 1.87120331e-05
Iter: 274 loss: 1.87890837e-05
Iter: 275 loss: 1.86875786e-05
Iter: 276 loss: 1.87542792e-05
Iter: 277 loss: 1.86782363e-05
Iter: 278 loss: 1.86689358e-05
Iter: 279 loss: 1.86423604e-05
Iter: 280 loss: 1.88023514e-05
Iter: 281 loss: 1.86354446e-05
Iter: 282 loss: 1.86033976e-05
Iter: 283 loss: 1.85582758e-05
Iter: 284 loss: 1.85568097e-05
Iter: 285 loss: 1.85143745e-05
Iter: 286 loss: 1.90373794e-05
Iter: 287 loss: 1.85133849e-05
Iter: 288 loss: 1.84781165e-05
Iter: 289 loss: 1.85897e-05
Iter: 290 loss: 1.84681849e-05
Iter: 291 loss: 1.84223227e-05
Iter: 292 loss: 1.85079407e-05
Iter: 293 loss: 1.84025557e-05
Iter: 294 loss: 1.83738157e-05
Iter: 295 loss: 1.84095024e-05
Iter: 296 loss: 1.83587108e-05
Iter: 297 loss: 1.83294578e-05
Iter: 298 loss: 1.84270666e-05
Iter: 299 loss: 1.83211214e-05
Iter: 300 loss: 1.82856456e-05
Iter: 301 loss: 1.83415104e-05
Iter: 302 loss: 1.82685399e-05
Iter: 303 loss: 1.82242875e-05
Iter: 304 loss: 1.83653046e-05
Iter: 305 loss: 1.82118602e-05
Iter: 306 loss: 1.8178398e-05
Iter: 307 loss: 1.82505755e-05
Iter: 308 loss: 1.81653413e-05
Iter: 309 loss: 1.81320502e-05
Iter: 310 loss: 1.82827462e-05
Iter: 311 loss: 1.8126655e-05
Iter: 312 loss: 1.81120686e-05
Iter: 313 loss: 1.81058967e-05
Iter: 314 loss: 1.8097744e-05
Iter: 315 loss: 1.80701572e-05
Iter: 316 loss: 1.81190971e-05
Iter: 317 loss: 1.80522366e-05
Iter: 318 loss: 1.80128263e-05
Iter: 319 loss: 1.80594e-05
Iter: 320 loss: 1.79929502e-05
Iter: 321 loss: 1.79492599e-05
Iter: 322 loss: 1.79775761e-05
Iter: 323 loss: 1.79213421e-05
Iter: 324 loss: 1.79219896e-05
Iter: 325 loss: 1.78986702e-05
Iter: 326 loss: 1.7884202e-05
Iter: 327 loss: 1.78655228e-05
Iter: 328 loss: 1.7864073e-05
Iter: 329 loss: 1.78302889e-05
Iter: 330 loss: 1.78093505e-05
Iter: 331 loss: 1.77959537e-05
Iter: 332 loss: 1.77673901e-05
Iter: 333 loss: 1.81253054e-05
Iter: 334 loss: 1.77667625e-05
Iter: 335 loss: 1.77400107e-05
Iter: 336 loss: 1.77810871e-05
Iter: 337 loss: 1.77280617e-05
Iter: 338 loss: 1.76940321e-05
Iter: 339 loss: 1.77759066e-05
Iter: 340 loss: 1.76821686e-05
Iter: 341 loss: 1.7656881e-05
Iter: 342 loss: 1.77143847e-05
Iter: 343 loss: 1.76476533e-05
Iter: 344 loss: 1.76638569e-05
Iter: 345 loss: 1.76384237e-05
Iter: 346 loss: 1.76315862e-05
Iter: 347 loss: 1.76110971e-05
Iter: 348 loss: 1.77584188e-05
Iter: 349 loss: 1.76071371e-05
Iter: 350 loss: 1.75845926e-05
Iter: 351 loss: 1.7567e-05
Iter: 352 loss: 1.75612149e-05
Iter: 353 loss: 1.75277637e-05
Iter: 354 loss: 1.75762743e-05
Iter: 355 loss: 1.75111709e-05
Iter: 356 loss: 1.74786401e-05
Iter: 357 loss: 1.77867259e-05
Iter: 358 loss: 1.74775341e-05
Iter: 359 loss: 1.74498091e-05
Iter: 360 loss: 1.76315334e-05
Iter: 361 loss: 1.74473171e-05
Iter: 362 loss: 1.74245088e-05
Iter: 363 loss: 1.74420384e-05
Iter: 364 loss: 1.74107845e-05
Iter: 365 loss: 1.73931548e-05
Iter: 366 loss: 1.73838234e-05
Iter: 367 loss: 1.73756234e-05
Iter: 368 loss: 1.73405897e-05
Iter: 369 loss: 1.74107936e-05
Iter: 370 loss: 1.73273074e-05
Iter: 371 loss: 1.72953296e-05
Iter: 372 loss: 1.76202811e-05
Iter: 373 loss: 1.72942273e-05
Iter: 374 loss: 1.72751643e-05
Iter: 375 loss: 1.7356746e-05
Iter: 376 loss: 1.72708496e-05
Iter: 377 loss: 1.72574473e-05
Iter: 378 loss: 1.73306144e-05
Iter: 379 loss: 1.72556738e-05
Iter: 380 loss: 1.72330256e-05
Iter: 381 loss: 1.72394066e-05
Iter: 382 loss: 1.72164328e-05
Iter: 383 loss: 1.72049768e-05
Iter: 384 loss: 1.71889333e-05
Iter: 385 loss: 1.71885422e-05
Iter: 386 loss: 1.71660067e-05
Iter: 387 loss: 1.71562278e-05
Iter: 388 loss: 1.7144419e-05
Iter: 389 loss: 1.71144529e-05
Iter: 390 loss: 1.7387536e-05
Iter: 391 loss: 1.71127922e-05
Iter: 392 loss: 1.70964613e-05
Iter: 393 loss: 1.71548327e-05
Iter: 394 loss: 1.70920503e-05
Iter: 395 loss: 1.70713683e-05
Iter: 396 loss: 1.71593565e-05
Iter: 397 loss: 1.70672465e-05
Iter: 398 loss: 1.70489111e-05
Iter: 399 loss: 1.70352432e-05
Iter: 400 loss: 1.70287803e-05
Iter: 401 loss: 1.69970808e-05
Iter: 402 loss: 1.70131425e-05
Iter: 403 loss: 1.6976e-05
Iter: 404 loss: 1.69508621e-05
Iter: 405 loss: 1.71895772e-05
Iter: 406 loss: 1.69510367e-05
Iter: 407 loss: 1.69285049e-05
Iter: 408 loss: 1.69906161e-05
Iter: 409 loss: 1.69223276e-05
Iter: 410 loss: 1.68985607e-05
Iter: 411 loss: 1.70030544e-05
Iter: 412 loss: 1.68954284e-05
Iter: 413 loss: 1.68924271e-05
Iter: 414 loss: 1.68843653e-05
Iter: 415 loss: 1.68807856e-05
Iter: 416 loss: 1.68689221e-05
Iter: 417 loss: 1.68499719e-05
Iter: 418 loss: 1.68482438e-05
Iter: 419 loss: 1.68202314e-05
Iter: 420 loss: 1.69850427e-05
Iter: 421 loss: 1.68163187e-05
Iter: 422 loss: 1.67938379e-05
Iter: 423 loss: 1.6882308e-05
Iter: 424 loss: 1.67882554e-05
Iter: 425 loss: 1.6769689e-05
Iter: 426 loss: 1.67814596e-05
Iter: 427 loss: 1.67583385e-05
Iter: 428 loss: 1.67337694e-05
Iter: 429 loss: 1.70091298e-05
Iter: 430 loss: 1.67334692e-05
Iter: 431 loss: 1.671672e-05
Iter: 432 loss: 1.67633134e-05
Iter: 433 loss: 1.67114304e-05
Iter: 434 loss: 1.66985574e-05
Iter: 435 loss: 1.66758946e-05
Iter: 436 loss: 1.66755854e-05
Iter: 437 loss: 1.66448808e-05
Iter: 438 loss: 1.67811268e-05
Iter: 439 loss: 1.66388018e-05
Iter: 440 loss: 1.66168738e-05
Iter: 441 loss: 1.67173293e-05
Iter: 442 loss: 1.66122609e-05
Iter: 443 loss: 1.65947786e-05
Iter: 444 loss: 1.67475646e-05
Iter: 445 loss: 1.65930123e-05
Iter: 446 loss: 1.65900128e-05
Iter: 447 loss: 1.65866477e-05
Iter: 448 loss: 1.65806177e-05
Iter: 449 loss: 1.65649035e-05
Iter: 450 loss: 1.66608916e-05
Iter: 451 loss: 1.65607562e-05
Iter: 452 loss: 1.65449892e-05
Iter: 453 loss: 1.65463571e-05
Iter: 454 loss: 1.65333222e-05
Iter: 455 loss: 1.65148867e-05
Iter: 456 loss: 1.65694328e-05
Iter: 457 loss: 1.65086e-05
Iter: 458 loss: 1.64874273e-05
Iter: 459 loss: 1.64921366e-05
Iter: 460 loss: 1.64714129e-05
Iter: 461 loss: 1.64549638e-05
Iter: 462 loss: 1.64539124e-05
Iter: 463 loss: 1.64431895e-05
Iter: 464 loss: 1.64693029e-05
Iter: 465 loss: 1.6439868e-05
Iter: 466 loss: 1.64266694e-05
Iter: 467 loss: 1.64099074e-05
Iter: 468 loss: 1.64074372e-05
Iter: 469 loss: 1.63846416e-05
Iter: 470 loss: 1.64856647e-05
Iter: 471 loss: 1.6379201e-05
Iter: 472 loss: 1.63632358e-05
Iter: 473 loss: 1.63788445e-05
Iter: 474 loss: 1.63534569e-05
Iter: 475 loss: 1.6331076e-05
Iter: 476 loss: 1.64424964e-05
Iter: 477 loss: 1.6326987e-05
Iter: 478 loss: 1.63308323e-05
Iter: 479 loss: 1.63189688e-05
Iter: 480 loss: 1.63128225e-05
Iter: 481 loss: 1.63037967e-05
Iter: 482 loss: 1.63027435e-05
Iter: 483 loss: 1.62932156e-05
Iter: 484 loss: 1.62729357e-05
Iter: 485 loss: 1.65941728e-05
Iter: 486 loss: 1.62730321e-05
Iter: 487 loss: 1.62512224e-05
Iter: 488 loss: 1.63313598e-05
Iter: 489 loss: 1.62470151e-05
Iter: 490 loss: 1.62268425e-05
Iter: 491 loss: 1.63023542e-05
Iter: 492 loss: 1.62216602e-05
Iter: 493 loss: 1.62068281e-05
Iter: 494 loss: 1.62746055e-05
Iter: 495 loss: 1.62038905e-05
Iter: 496 loss: 1.61884636e-05
Iter: 497 loss: 1.62666729e-05
Iter: 498 loss: 1.61866774e-05
Iter: 499 loss: 1.61715943e-05
Iter: 500 loss: 1.61842654e-05
Iter: 501 loss: 1.61635071e-05
Iter: 502 loss: 1.61503831e-05
Iter: 503 loss: 1.61766e-05
Iter: 504 loss: 1.61437129e-05
Iter: 505 loss: 1.6129e-05
Iter: 506 loss: 1.61290282e-05
Iter: 507 loss: 1.61173175e-05
Iter: 508 loss: 1.60922755e-05
Iter: 509 loss: 1.61559983e-05
Iter: 510 loss: 1.60835061e-05
Iter: 511 loss: 1.60951058e-05
Iter: 512 loss: 1.60775053e-05
Iter: 513 loss: 1.6072172e-05
Iter: 514 loss: 1.60768795e-05
Iter: 515 loss: 1.60687359e-05
Iter: 516 loss: 1.60625332e-05
Iter: 517 loss: 1.60459967e-05
Iter: 518 loss: 1.61260832e-05
Iter: 519 loss: 1.60403797e-05
Iter: 520 loss: 1.60184391e-05
Iter: 521 loss: 1.61081261e-05
Iter: 522 loss: 1.60130166e-05
Iter: 523 loss: 1.59958363e-05
Iter: 524 loss: 1.6025e-05
Iter: 525 loss: 1.59877109e-05
Iter: 526 loss: 1.5967431e-05
Iter: 527 loss: 1.60283489e-05
Iter: 528 loss: 1.59611573e-05
Iter: 529 loss: 1.59491428e-05
Iter: 530 loss: 1.59491356e-05
Iter: 531 loss: 1.59371448e-05
Iter: 532 loss: 1.59290503e-05
Iter: 533 loss: 1.59252231e-05
Iter: 534 loss: 1.59075644e-05
Iter: 535 loss: 1.59531301e-05
Iter: 536 loss: 1.59024275e-05
Iter: 537 loss: 1.58849525e-05
Iter: 538 loss: 1.58839066e-05
Iter: 539 loss: 1.58706389e-05
Iter: 540 loss: 1.58495823e-05
Iter: 541 loss: 1.59582269e-05
Iter: 542 loss: 1.58459734e-05
Iter: 543 loss: 1.58351468e-05
Iter: 544 loss: 1.58346011e-05
Iter: 545 loss: 1.58224975e-05
Iter: 546 loss: 1.58904149e-05
Iter: 547 loss: 1.58201801e-05
Iter: 548 loss: 1.58124858e-05
Iter: 549 loss: 1.57972809e-05
Iter: 550 loss: 1.60648106e-05
Iter: 551 loss: 1.5796104e-05
Iter: 552 loss: 1.57836985e-05
Iter: 553 loss: 1.58135899e-05
Iter: 554 loss: 1.57787072e-05
Iter: 555 loss: 1.5764037e-05
Iter: 556 loss: 1.57496725e-05
Iter: 557 loss: 1.57467621e-05
Iter: 558 loss: 1.57252543e-05
Iter: 559 loss: 1.59707633e-05
Iter: 560 loss: 1.57247378e-05
Iter: 561 loss: 1.57123941e-05
Iter: 562 loss: 1.57386421e-05
Iter: 563 loss: 1.57078903e-05
Iter: 564 loss: 1.56884525e-05
Iter: 565 loss: 1.57361865e-05
Iter: 566 loss: 1.56821152e-05
Iter: 567 loss: 1.56629794e-05
Iter: 568 loss: 1.57015329e-05
Iter: 569 loss: 1.56557198e-05
Iter: 570 loss: 1.5642745e-05
Iter: 571 loss: 1.5663727e-05
Iter: 572 loss: 1.56359947e-05
Iter: 573 loss: 1.56202295e-05
Iter: 574 loss: 1.56397255e-05
Iter: 575 loss: 1.56121805e-05
Iter: 576 loss: 1.55936723e-05
Iter: 577 loss: 1.56629067e-05
Iter: 578 loss: 1.55894668e-05
Iter: 579 loss: 1.55964517e-05
Iter: 580 loss: 1.55820053e-05
Iter: 581 loss: 1.55784455e-05
Iter: 582 loss: 1.55674697e-05
Iter: 583 loss: 1.56172e-05
Iter: 584 loss: 1.55630387e-05
Iter: 585 loss: 1.55513899e-05
Iter: 586 loss: 1.55935195e-05
Iter: 587 loss: 1.55482958e-05
Iter: 588 loss: 1.55350208e-05
Iter: 589 loss: 1.55156049e-05
Iter: 590 loss: 1.55145735e-05
Iter: 591 loss: 1.54956488e-05
Iter: 592 loss: 1.56652241e-05
Iter: 593 loss: 1.54958143e-05
Iter: 594 loss: 1.5480502e-05
Iter: 595 loss: 1.5481528e-05
Iter: 596 loss: 1.54690024e-05
Iter: 597 loss: 1.54516529e-05
Iter: 598 loss: 1.56150472e-05
Iter: 599 loss: 1.54508016e-05
Iter: 600 loss: 1.54366753e-05
Iter: 601 loss: 1.55791495e-05
Iter: 602 loss: 1.54367353e-05
Iter: 603 loss: 1.54284862e-05
Iter: 604 loss: 1.54141653e-05
Iter: 605 loss: 1.57328141e-05
Iter: 606 loss: 1.54142599e-05
Iter: 607 loss: 1.53931032e-05
Iter: 608 loss: 1.54445261e-05
Iter: 609 loss: 1.53857654e-05
Iter: 610 loss: 1.53746259e-05
Iter: 611 loss: 1.55282869e-05
Iter: 612 loss: 1.53742549e-05
Iter: 613 loss: 1.53697074e-05
Iter: 614 loss: 1.53694564e-05
Iter: 615 loss: 1.53632736e-05
Iter: 616 loss: 1.53470901e-05
Iter: 617 loss: 1.55112575e-05
Iter: 618 loss: 1.53446472e-05
Iter: 619 loss: 1.53346537e-05
Iter: 620 loss: 1.53514229e-05
Iter: 621 loss: 1.53301135e-05
Iter: 622 loss: 1.53178225e-05
Iter: 623 loss: 1.53378023e-05
Iter: 624 loss: 1.53123692e-05
Iter: 625 loss: 1.52981902e-05
Iter: 626 loss: 1.53115634e-05
Iter: 627 loss: 1.52908633e-05
Iter: 628 loss: 1.52753328e-05
Iter: 629 loss: 1.53048913e-05
Iter: 630 loss: 1.5269432e-05
Iter: 631 loss: 1.52528291e-05
Iter: 632 loss: 1.53235869e-05
Iter: 633 loss: 1.5250348e-05
Iter: 634 loss: 1.52373323e-05
Iter: 635 loss: 1.53480942e-05
Iter: 636 loss: 1.52372095e-05
Iter: 637 loss: 1.52246485e-05
Iter: 638 loss: 1.52316352e-05
Iter: 639 loss: 1.52169359e-05
Iter: 640 loss: 1.52048706e-05
Iter: 641 loss: 1.5237657e-05
Iter: 642 loss: 1.52016237e-05
Iter: 643 loss: 1.51912964e-05
Iter: 644 loss: 1.51789536e-05
Iter: 645 loss: 1.51779568e-05
Iter: 646 loss: 1.51822478e-05
Iter: 647 loss: 1.51704508e-05
Iter: 648 loss: 1.51625936e-05
Iter: 649 loss: 1.51746353e-05
Iter: 650 loss: 1.51592731e-05
Iter: 651 loss: 1.5154802e-05
Iter: 652 loss: 1.51445156e-05
Iter: 653 loss: 1.52894045e-05
Iter: 654 loss: 1.5143326e-05
Iter: 655 loss: 1.51309187e-05
Iter: 656 loss: 1.51601816e-05
Iter: 657 loss: 1.51262866e-05
Iter: 658 loss: 1.51134209e-05
Iter: 659 loss: 1.51915719e-05
Iter: 660 loss: 1.51119457e-05
Iter: 661 loss: 1.51033037e-05
Iter: 662 loss: 1.50969554e-05
Iter: 663 loss: 1.509294e-05
Iter: 664 loss: 1.50765527e-05
Iter: 665 loss: 1.51170925e-05
Iter: 666 loss: 1.50701671e-05
Iter: 667 loss: 1.50556843e-05
Iter: 668 loss: 1.5120665e-05
Iter: 669 loss: 1.50530987e-05
Iter: 670 loss: 1.50416417e-05
Iter: 671 loss: 1.52043103e-05
Iter: 672 loss: 1.50417727e-05
Iter: 673 loss: 1.50336618e-05
Iter: 674 loss: 1.50231226e-05
Iter: 675 loss: 1.50217766e-05
Iter: 676 loss: 1.50053875e-05
Iter: 677 loss: 1.50167389e-05
Iter: 678 loss: 1.49949055e-05
Iter: 679 loss: 1.49816515e-05
Iter: 680 loss: 1.50991045e-05
Iter: 681 loss: 1.49806838e-05
Iter: 682 loss: 1.4979958e-05
Iter: 683 loss: 1.49752213e-05
Iter: 684 loss: 1.49723464e-05
Iter: 685 loss: 1.49638054e-05
Iter: 686 loss: 1.49714469e-05
Iter: 687 loss: 1.49561538e-05
Iter: 688 loss: 1.49383013e-05
Iter: 689 loss: 1.49617244e-05
Iter: 690 loss: 1.49295474e-05
Iter: 691 loss: 1.49165207e-05
Iter: 692 loss: 1.51020249e-05
Iter: 693 loss: 1.49168272e-05
Iter: 694 loss: 1.49069101e-05
Iter: 695 loss: 1.49023672e-05
Iter: 696 loss: 1.48984109e-05
Iter: 697 loss: 1.48813051e-05
Iter: 698 loss: 1.49398311e-05
Iter: 699 loss: 1.48771951e-05
Iter: 700 loss: 1.48661738e-05
Iter: 701 loss: 1.48657909e-05
Iter: 702 loss: 1.48578856e-05
Iter: 703 loss: 1.48428599e-05
Iter: 704 loss: 1.49980297e-05
Iter: 705 loss: 1.48420186e-05
Iter: 706 loss: 1.48313666e-05
Iter: 707 loss: 1.49373482e-05
Iter: 708 loss: 1.4830759e-05
Iter: 709 loss: 1.48241979e-05
Iter: 710 loss: 1.48179606e-05
Iter: 711 loss: 1.48167446e-05
Iter: 712 loss: 1.48037325e-05
Iter: 713 loss: 1.47973442e-05
Iter: 714 loss: 1.47916098e-05
Iter: 715 loss: 1.48123017e-05
Iter: 716 loss: 1.47871033e-05
Iter: 717 loss: 1.47814735e-05
Iter: 718 loss: 1.47701776e-05
Iter: 719 loss: 1.49495372e-05
Iter: 720 loss: 1.47698947e-05
Iter: 721 loss: 1.47620931e-05
Iter: 722 loss: 1.47534283e-05
Iter: 723 loss: 1.47524224e-05
Iter: 724 loss: 1.47402152e-05
Iter: 725 loss: 1.48275585e-05
Iter: 726 loss: 1.47392557e-05
Iter: 727 loss: 1.47269957e-05
Iter: 728 loss: 1.47368928e-05
Iter: 729 loss: 1.47206993e-05
Iter: 730 loss: 1.47096198e-05
Iter: 731 loss: 1.47736691e-05
Iter: 732 loss: 1.47081391e-05
Iter: 733 loss: 1.46975717e-05
Iter: 734 loss: 1.46983875e-05
Iter: 735 loss: 1.46883476e-05
Iter: 736 loss: 1.46734019e-05
Iter: 737 loss: 1.47301744e-05
Iter: 738 loss: 1.46693792e-05
Iter: 739 loss: 1.46595921e-05
Iter: 740 loss: 1.47586979e-05
Iter: 741 loss: 1.46596531e-05
Iter: 742 loss: 1.46491875e-05
Iter: 743 loss: 1.46452621e-05
Iter: 744 loss: 1.4639636e-05
Iter: 745 loss: 1.46260854e-05
Iter: 746 loss: 1.46583961e-05
Iter: 747 loss: 1.46219882e-05
Iter: 748 loss: 1.46145394e-05
Iter: 749 loss: 1.46878283e-05
Iter: 750 loss: 1.46143393e-05
Iter: 751 loss: 1.46040102e-05
Iter: 752 loss: 1.46288303e-05
Iter: 753 loss: 1.4600555e-05
Iter: 754 loss: 1.45946524e-05
Iter: 755 loss: 1.45807544e-05
Iter: 756 loss: 1.47241681e-05
Iter: 757 loss: 1.45791309e-05
Iter: 758 loss: 1.45645627e-05
Iter: 759 loss: 1.46036245e-05
Iter: 760 loss: 1.45604827e-05
Iter: 761 loss: 1.45474123e-05
Iter: 762 loss: 1.4607358e-05
Iter: 763 loss: 1.45455251e-05
Iter: 764 loss: 1.45324793e-05
Iter: 765 loss: 1.45675485e-05
Iter: 766 loss: 1.45283566e-05
Iter: 767 loss: 1.45154572e-05
Iter: 768 loss: 1.45210406e-05
Iter: 769 loss: 1.45066233e-05
Iter: 770 loss: 1.44941005e-05
Iter: 771 loss: 1.45889244e-05
Iter: 772 loss: 1.44930673e-05
Iter: 773 loss: 1.44821488e-05
Iter: 774 loss: 1.44888199e-05
Iter: 775 loss: 1.44756741e-05
Iter: 776 loss: 1.44624482e-05
Iter: 777 loss: 1.46043303e-05
Iter: 778 loss: 1.44619908e-05
Iter: 779 loss: 1.4454039e-05
Iter: 780 loss: 1.44696769e-05
Iter: 781 loss: 1.44505593e-05
Iter: 782 loss: 1.44418727e-05
Iter: 783 loss: 1.44263413e-05
Iter: 784 loss: 1.47637593e-05
Iter: 785 loss: 1.44258956e-05
Iter: 786 loss: 1.44883197e-05
Iter: 787 loss: 1.44219848e-05
Iter: 788 loss: 1.44204914e-05
Iter: 789 loss: 1.44148144e-05
Iter: 790 loss: 1.44274381e-05
Iter: 791 loss: 1.44111091e-05
Iter: 792 loss: 1.44019778e-05
Iter: 793 loss: 1.43958587e-05
Iter: 794 loss: 1.43924763e-05
Iter: 795 loss: 1.43771222e-05
Iter: 796 loss: 1.44895603e-05
Iter: 797 loss: 1.43757361e-05
Iter: 798 loss: 1.43622474e-05
Iter: 799 loss: 1.44234882e-05
Iter: 800 loss: 1.43595125e-05
Iter: 801 loss: 1.43482775e-05
Iter: 802 loss: 1.43946554e-05
Iter: 803 loss: 1.43456236e-05
Iter: 804 loss: 1.43377465e-05
Iter: 805 loss: 1.43367179e-05
Iter: 806 loss: 1.43299758e-05
Iter: 807 loss: 1.43174857e-05
Iter: 808 loss: 1.43489087e-05
Iter: 809 loss: 1.43135958e-05
Iter: 810 loss: 1.43028101e-05
Iter: 811 loss: 1.43632415e-05
Iter: 812 loss: 1.43014713e-05
Iter: 813 loss: 1.42927674e-05
Iter: 814 loss: 1.4368381e-05
Iter: 815 loss: 1.42928402e-05
Iter: 816 loss: 1.42849667e-05
Iter: 817 loss: 1.42840718e-05
Iter: 818 loss: 1.42787794e-05
Iter: 819 loss: 1.42697181e-05
Iter: 820 loss: 1.42945682e-05
Iter: 821 loss: 1.42665986e-05
Iter: 822 loss: 1.42701701e-05
Iter: 823 loss: 1.42636563e-05
Iter: 824 loss: 1.42615154e-05
Iter: 825 loss: 1.42535964e-05
Iter: 826 loss: 1.42524732e-05
Iter: 827 loss: 1.42461813e-05
Iter: 828 loss: 1.42355166e-05
Iter: 829 loss: 1.43002653e-05
Iter: 830 loss: 1.42347026e-05
Iter: 831 loss: 1.42264016e-05
Iter: 832 loss: 1.42208228e-05
Iter: 833 loss: 1.4217132e-05
Iter: 834 loss: 1.42054578e-05
Iter: 835 loss: 1.43340249e-05
Iter: 836 loss: 1.42054023e-05
Iter: 837 loss: 1.41968276e-05
Iter: 838 loss: 1.41971968e-05
Iter: 839 loss: 1.41902856e-05
Iter: 840 loss: 1.41782457e-05
Iter: 841 loss: 1.42576446e-05
Iter: 842 loss: 1.41776991e-05
Iter: 843 loss: 1.41680475e-05
Iter: 844 loss: 1.41858618e-05
Iter: 845 loss: 1.41645323e-05
Iter: 846 loss: 1.4154476e-05
Iter: 847 loss: 1.41692508e-05
Iter: 848 loss: 1.41496203e-05
Iter: 849 loss: 1.41380142e-05
Iter: 850 loss: 1.42252647e-05
Iter: 851 loss: 1.41369355e-05
Iter: 852 loss: 1.41291821e-05
Iter: 853 loss: 1.41374194e-05
Iter: 854 loss: 1.41244618e-05
Iter: 855 loss: 1.4116893e-05
Iter: 856 loss: 1.41580267e-05
Iter: 857 loss: 1.41156952e-05
Iter: 858 loss: 1.41101036e-05
Iter: 859 loss: 1.41100454e-05
Iter: 860 loss: 1.41073288e-05
Iter: 861 loss: 1.41018645e-05
Iter: 862 loss: 1.41213204e-05
Iter: 863 loss: 1.40984193e-05
Iter: 864 loss: 1.40904658e-05
Iter: 865 loss: 1.40957018e-05
Iter: 866 loss: 1.40853635e-05
Iter: 867 loss: 1.40745642e-05
Iter: 868 loss: 1.41150686e-05
Iter: 869 loss: 1.40721422e-05
Iter: 870 loss: 1.40615557e-05
Iter: 871 loss: 1.40741795e-05
Iter: 872 loss: 1.40556667e-05
Iter: 873 loss: 1.40457141e-05
Iter: 874 loss: 1.41101409e-05
Iter: 875 loss: 1.4044449e-05
Iter: 876 loss: 1.40343927e-05
Iter: 877 loss: 1.40373149e-05
Iter: 878 loss: 1.40277516e-05
Iter: 879 loss: 1.40165812e-05
Iter: 880 loss: 1.41852379e-05
Iter: 881 loss: 1.40172024e-05
Iter: 882 loss: 1.40107277e-05
Iter: 883 loss: 1.40149223e-05
Iter: 884 loss: 1.40064694e-05
Iter: 885 loss: 1.39983695e-05
Iter: 886 loss: 1.40462853e-05
Iter: 887 loss: 1.39970298e-05
Iter: 888 loss: 1.39886488e-05
Iter: 889 loss: 1.39865706e-05
Iter: 890 loss: 1.39807307e-05
Iter: 891 loss: 1.39913254e-05
Iter: 892 loss: 1.39788826e-05
Iter: 893 loss: 1.39770191e-05
Iter: 894 loss: 1.39708336e-05
Iter: 895 loss: 1.39719095e-05
Iter: 896 loss: 1.39652266e-05
Iter: 897 loss: 1.39542744e-05
Iter: 898 loss: 1.39710819e-05
Iter: 899 loss: 1.39488839e-05
Iter: 900 loss: 1.39394469e-05
Iter: 901 loss: 1.39927724e-05
Iter: 902 loss: 1.39384811e-05
Iter: 903 loss: 1.39303866e-05
Iter: 904 loss: 1.39378844e-05
Iter: 905 loss: 1.39250624e-05
Iter: 906 loss: 1.39157492e-05
Iter: 907 loss: 1.39282693e-05
Iter: 908 loss: 1.39104959e-05
Iter: 909 loss: 1.38982105e-05
Iter: 910 loss: 1.39204358e-05
Iter: 911 loss: 1.3892547e-05
Iter: 912 loss: 1.38820678e-05
Iter: 913 loss: 1.39548192e-05
Iter: 914 loss: 1.38815758e-05
Iter: 915 loss: 1.38718651e-05
Iter: 916 loss: 1.39205276e-05
Iter: 917 loss: 1.38702835e-05
Iter: 918 loss: 1.38645082e-05
Iter: 919 loss: 1.39104513e-05
Iter: 920 loss: 1.38634578e-05
Iter: 921 loss: 1.38589439e-05
Iter: 922 loss: 1.38683918e-05
Iter: 923 loss: 1.38565683e-05
Iter: 924 loss: 1.38514206e-05
Iter: 925 loss: 1.38600954e-05
Iter: 926 loss: 1.38481946e-05
Iter: 927 loss: 1.38443038e-05
Iter: 928 loss: 1.38436972e-05
Iter: 929 loss: 1.38414016e-05
Iter: 930 loss: 1.38350542e-05
Iter: 931 loss: 1.38599662e-05
Iter: 932 loss: 1.38325395e-05
Iter: 933 loss: 1.38253254e-05
Iter: 934 loss: 1.38280911e-05
Iter: 935 loss: 1.38206451e-05
Iter: 936 loss: 1.38102087e-05
Iter: 937 loss: 1.38143714e-05
Iter: 938 loss: 1.38034811e-05
Iter: 939 loss: 1.37943507e-05
Iter: 940 loss: 1.3860461e-05
Iter: 941 loss: 1.37931111e-05
Iter: 942 loss: 1.37834304e-05
Iter: 943 loss: 1.37903698e-05
Iter: 944 loss: 1.37778716e-05
Iter: 945 loss: 1.37697416e-05
Iter: 946 loss: 1.38747182e-05
Iter: 947 loss: 1.37689349e-05
Iter: 948 loss: 1.37626712e-05
Iter: 949 loss: 1.37614788e-05
Iter: 950 loss: 1.37571342e-05
Iter: 951 loss: 1.374781e-05
Iter: 952 loss: 1.38248961e-05
Iter: 953 loss: 1.37480056e-05
Iter: 954 loss: 1.37397992e-05
Iter: 955 loss: 1.37676116e-05
Iter: 956 loss: 1.37373845e-05
Iter: 957 loss: 1.37305415e-05
Iter: 958 loss: 1.37509487e-05
Iter: 959 loss: 1.37286588e-05
Iter: 960 loss: 1.37267753e-05
Iter: 961 loss: 1.37255629e-05
Iter: 962 loss: 1.37225434e-05
Iter: 963 loss: 1.37160641e-05
Iter: 964 loss: 1.38191463e-05
Iter: 965 loss: 1.37162642e-05
Iter: 966 loss: 1.37104271e-05
Iter: 967 loss: 1.37011311e-05
Iter: 968 loss: 1.37007091e-05
Iter: 969 loss: 1.36896042e-05
Iter: 970 loss: 1.37394563e-05
Iter: 971 loss: 1.36879271e-05
Iter: 972 loss: 1.36796598e-05
Iter: 973 loss: 1.36838107e-05
Iter: 974 loss: 1.36738427e-05
Iter: 975 loss: 1.36613517e-05
Iter: 976 loss: 1.37098114e-05
Iter: 977 loss: 1.36588924e-05
Iter: 978 loss: 1.36511871e-05
Iter: 979 loss: 1.36594517e-05
Iter: 980 loss: 1.36467197e-05
Iter: 981 loss: 1.36362005e-05
Iter: 982 loss: 1.36640192e-05
Iter: 983 loss: 1.36317612e-05
Iter: 984 loss: 1.36221506e-05
Iter: 985 loss: 1.36720864e-05
Iter: 986 loss: 1.36208164e-05
Iter: 987 loss: 1.3611605e-05
Iter: 988 loss: 1.36584795e-05
Iter: 989 loss: 1.36096796e-05
Iter: 990 loss: 1.36013496e-05
Iter: 991 loss: 1.36458557e-05
Iter: 992 loss: 1.36007275e-05
Iter: 993 loss: 1.35953032e-05
Iter: 994 loss: 1.36377294e-05
Iter: 995 loss: 1.35945793e-05
Iter: 996 loss: 1.35887676e-05
Iter: 997 loss: 1.36143963e-05
Iter: 998 loss: 1.3587196e-05
Iter: 999 loss: 1.3584031e-05
Iter: 1000 loss: 1.35789141e-05
Iter: 1001 loss: 1.36820718e-05
Iter: 1002 loss: 1.35785212e-05
Iter: 1003 loss: 1.35709579e-05
Iter: 1004 loss: 1.35713381e-05
Iter: 1005 loss: 1.35653645e-05
Iter: 1006 loss: 1.35565606e-05
Iter: 1007 loss: 1.35875016e-05
Iter: 1008 loss: 1.35533319e-05
Iter: 1009 loss: 1.35461678e-05
Iter: 1010 loss: 1.35733799e-05
Iter: 1011 loss: 1.35437558e-05
Iter: 1012 loss: 1.35362898e-05
Iter: 1013 loss: 1.35351484e-05
Iter: 1014 loss: 1.35296268e-05
Iter: 1015 loss: 1.35167465e-05
Iter: 1016 loss: 1.35567252e-05
Iter: 1017 loss: 1.35130631e-05
Iter: 1018 loss: 1.35047385e-05
Iter: 1019 loss: 1.35468599e-05
Iter: 1020 loss: 1.35029441e-05
Iter: 1021 loss: 1.34939082e-05
Iter: 1022 loss: 1.34979109e-05
Iter: 1023 loss: 1.34880456e-05
Iter: 1024 loss: 1.34795537e-05
Iter: 1025 loss: 1.3556918e-05
Iter: 1026 loss: 1.34790425e-05
Iter: 1027 loss: 1.34705024e-05
Iter: 1028 loss: 1.35076034e-05
Iter: 1029 loss: 1.34689126e-05
Iter: 1030 loss: 1.34642323e-05
Iter: 1031 loss: 1.34633456e-05
Iter: 1032 loss: 1.34583388e-05
Iter: 1033 loss: 1.34509874e-05
Iter: 1034 loss: 1.34510501e-05
Iter: 1035 loss: 1.34464026e-05
Iter: 1036 loss: 1.34438633e-05
Iter: 1037 loss: 1.34423226e-05
Iter: 1038 loss: 1.34349148e-05
Iter: 1039 loss: 1.34471575e-05
Iter: 1040 loss: 1.3430832e-05
Iter: 1041 loss: 1.34206384e-05
Iter: 1042 loss: 1.34322281e-05
Iter: 1043 loss: 1.34154916e-05
Iter: 1044 loss: 1.34067132e-05
Iter: 1045 loss: 1.34260254e-05
Iter: 1046 loss: 1.34038364e-05
Iter: 1047 loss: 1.3394143e-05
Iter: 1048 loss: 1.34076126e-05
Iter: 1049 loss: 1.33905924e-05
Iter: 1050 loss: 1.33786798e-05
Iter: 1051 loss: 1.34297361e-05
Iter: 1052 loss: 1.3376708e-05
Iter: 1053 loss: 1.33661115e-05
Iter: 1054 loss: 1.3397299e-05
Iter: 1055 loss: 1.33631984e-05
Iter: 1056 loss: 1.33544163e-05
Iter: 1057 loss: 1.33500635e-05
Iter: 1058 loss: 1.33470976e-05
Iter: 1059 loss: 1.33372687e-05
Iter: 1060 loss: 1.33374542e-05
Iter: 1061 loss: 1.33338817e-05
Iter: 1062 loss: 1.33338444e-05
Iter: 1063 loss: 1.33297763e-05
Iter: 1064 loss: 1.3345667e-05
Iter: 1065 loss: 1.33292742e-05
Iter: 1066 loss: 1.33250005e-05
Iter: 1067 loss: 1.33143594e-05
Iter: 1068 loss: 1.34161073e-05
Iter: 1069 loss: 1.33132144e-05
Iter: 1070 loss: 1.33041758e-05
Iter: 1071 loss: 1.33253207e-05
Iter: 1072 loss: 1.33003432e-05
Iter: 1073 loss: 1.32921195e-05
Iter: 1074 loss: 1.32991772e-05
Iter: 1075 loss: 1.32871937e-05
Iter: 1076 loss: 1.32777859e-05
Iter: 1077 loss: 1.33469875e-05
Iter: 1078 loss: 1.32773348e-05
Iter: 1079 loss: 1.32688983e-05
Iter: 1080 loss: 1.32985624e-05
Iter: 1081 loss: 1.3267002e-05
Iter: 1082 loss: 1.32606674e-05
Iter: 1083 loss: 1.32518926e-05
Iter: 1084 loss: 1.32515561e-05
Iter: 1085 loss: 1.32402984e-05
Iter: 1086 loss: 1.33284575e-05
Iter: 1087 loss: 1.32395471e-05
Iter: 1088 loss: 1.32316563e-05
Iter: 1089 loss: 1.32415207e-05
Iter: 1090 loss: 1.32284549e-05
Iter: 1091 loss: 1.32176101e-05
Iter: 1092 loss: 1.32480718e-05
Iter: 1093 loss: 1.32149617e-05
Iter: 1094 loss: 1.32049281e-05
Iter: 1095 loss: 1.32416044e-05
Iter: 1096 loss: 1.32021542e-05
Iter: 1097 loss: 1.32074647e-05
Iter: 1098 loss: 1.31986581e-05
Iter: 1099 loss: 1.31966362e-05
Iter: 1100 loss: 1.31934994e-05
Iter: 1101 loss: 1.31932466e-05
Iter: 1102 loss: 1.31892411e-05
Iter: 1103 loss: 1.3189986e-05
Iter: 1104 loss: 1.31854922e-05
Iter: 1105 loss: 1.31807119e-05
Iter: 1106 loss: 1.31757315e-05
Iter: 1107 loss: 1.3174038e-05
Iter: 1108 loss: 1.31674024e-05
Iter: 1109 loss: 1.31961115e-05
Iter: 1110 loss: 1.31652705e-05
Iter: 1111 loss: 1.31576389e-05
Iter: 1112 loss: 1.31531542e-05
Iter: 1113 loss: 1.31502884e-05
Iter: 1114 loss: 1.31379666e-05
Iter: 1115 loss: 1.32288405e-05
Iter: 1116 loss: 1.31362449e-05
Iter: 1117 loss: 1.31303404e-05
Iter: 1118 loss: 1.31723755e-05
Iter: 1119 loss: 1.3129641e-05
Iter: 1120 loss: 1.31225352e-05
Iter: 1121 loss: 1.31177767e-05
Iter: 1122 loss: 1.3115432e-05
Iter: 1123 loss: 1.31064753e-05
Iter: 1124 loss: 1.31592396e-05
Iter: 1125 loss: 1.31056713e-05
Iter: 1126 loss: 1.30990629e-05
Iter: 1127 loss: 1.3098721e-05
Iter: 1128 loss: 1.30936969e-05
Iter: 1129 loss: 1.30838307e-05
Iter: 1130 loss: 1.31447159e-05
Iter: 1131 loss: 1.30821454e-05
Iter: 1132 loss: 1.30856e-05
Iter: 1133 loss: 1.30786184e-05
Iter: 1134 loss: 1.30774342e-05
Iter: 1135 loss: 1.30734561e-05
Iter: 1136 loss: 1.30777244e-05
Iter: 1137 loss: 1.30705366e-05
Iter: 1138 loss: 1.30655089e-05
Iter: 1139 loss: 1.31442885e-05
Iter: 1140 loss: 1.30652024e-05
Iter: 1141 loss: 1.30606431e-05
Iter: 1142 loss: 1.3052123e-05
Iter: 1143 loss: 1.3171586e-05
Iter: 1144 loss: 1.30511198e-05
Iter: 1145 loss: 1.30416865e-05
Iter: 1146 loss: 1.30830249e-05
Iter: 1147 loss: 1.30394401e-05
Iter: 1148 loss: 1.30307008e-05
Iter: 1149 loss: 1.30735116e-05
Iter: 1150 loss: 1.30292719e-05
Iter: 1151 loss: 1.30218614e-05
Iter: 1152 loss: 1.30367098e-05
Iter: 1153 loss: 1.3019152e-05
Iter: 1154 loss: 1.30119533e-05
Iter: 1155 loss: 1.30255285e-05
Iter: 1156 loss: 1.30088956e-05
Iter: 1157 loss: 1.30010367e-05
Iter: 1158 loss: 1.30329081e-05
Iter: 1159 loss: 1.29991258e-05
Iter: 1160 loss: 1.29911577e-05
Iter: 1161 loss: 1.29960363e-05
Iter: 1162 loss: 1.29859418e-05
Iter: 1163 loss: 1.29766613e-05
Iter: 1164 loss: 1.30337385e-05
Iter: 1165 loss: 1.29756354e-05
Iter: 1166 loss: 1.29703021e-05
Iter: 1167 loss: 1.29693653e-05
Iter: 1168 loss: 1.29661976e-05
Iter: 1169 loss: 1.29752916e-05
Iter: 1170 loss: 1.29635755e-05
Iter: 1171 loss: 1.29618275e-05
Iter: 1172 loss: 1.29569171e-05
Iter: 1173 loss: 1.29638502e-05
Iter: 1174 loss: 1.29532509e-05
Iter: 1175 loss: 1.29466853e-05
Iter: 1176 loss: 1.29489799e-05
Iter: 1177 loss: 1.29418349e-05
Iter: 1178 loss: 1.29348755e-05
Iter: 1179 loss: 1.29347673e-05
Iter: 1180 loss: 1.29318923e-05
Iter: 1181 loss: 1.29268265e-05
Iter: 1182 loss: 1.29263171e-05
Iter: 1183 loss: 1.29197197e-05
Iter: 1184 loss: 1.29267773e-05
Iter: 1185 loss: 1.29160726e-05
Iter: 1186 loss: 1.29070795e-05
Iter: 1187 loss: 1.29035307e-05
Iter: 1188 loss: 1.28984711e-05
Iter: 1189 loss: 1.28895408e-05
Iter: 1190 loss: 1.28893134e-05
Iter: 1191 loss: 1.28838119e-05
Iter: 1192 loss: 1.28876673e-05
Iter: 1193 loss: 1.28806287e-05
Iter: 1194 loss: 1.28715601e-05
Iter: 1195 loss: 1.28948577e-05
Iter: 1196 loss: 1.28692627e-05
Iter: 1197 loss: 1.28616039e-05
Iter: 1198 loss: 1.28905904e-05
Iter: 1199 loss: 1.28594984e-05
Iter: 1200 loss: 1.28568718e-05
Iter: 1201 loss: 1.28560314e-05
Iter: 1202 loss: 1.2851815e-05
Iter: 1203 loss: 1.28524316e-05
Iter: 1204 loss: 1.28485317e-05
Iter: 1205 loss: 1.28451829e-05
Iter: 1206 loss: 1.28371721e-05
Iter: 1207 loss: 1.29240252e-05
Iter: 1208 loss: 1.28363881e-05
Iter: 1209 loss: 1.28298125e-05
Iter: 1210 loss: 1.29149303e-05
Iter: 1211 loss: 1.28300035e-05
Iter: 1212 loss: 1.28248275e-05
Iter: 1213 loss: 1.2845424e-05
Iter: 1214 loss: 1.28229731e-05
Iter: 1215 loss: 1.28180982e-05
Iter: 1216 loss: 1.28076135e-05
Iter: 1217 loss: 1.29723339e-05
Iter: 1218 loss: 1.28078245e-05
Iter: 1219 loss: 1.28000365e-05
Iter: 1220 loss: 1.28000011e-05
Iter: 1221 loss: 1.2793661e-05
Iter: 1222 loss: 1.27907133e-05
Iter: 1223 loss: 1.27879957e-05
Iter: 1224 loss: 1.27794101e-05
Iter: 1225 loss: 1.2821547e-05
Iter: 1226 loss: 1.27770145e-05
Iter: 1227 loss: 1.27705589e-05
Iter: 1228 loss: 1.28367847e-05
Iter: 1229 loss: 1.27698113e-05
Iter: 1230 loss: 1.27653429e-05
Iter: 1231 loss: 1.27767262e-05
Iter: 1232 loss: 1.27635567e-05
Iter: 1233 loss: 1.27595704e-05
Iter: 1234 loss: 1.27734647e-05
Iter: 1235 loss: 1.27578896e-05
Iter: 1236 loss: 1.27512803e-05
Iter: 1237 loss: 1.28025722e-05
Iter: 1238 loss: 1.27517196e-05
Iter: 1239 loss: 1.27493367e-05
Iter: 1240 loss: 1.27427884e-05
Iter: 1241 loss: 1.28083102e-05
Iter: 1242 loss: 1.27424573e-05
Iter: 1243 loss: 1.27364237e-05
Iter: 1244 loss: 1.27476578e-05
Iter: 1245 loss: 1.27342555e-05
Iter: 1246 loss: 1.27275434e-05
Iter: 1247 loss: 1.27693729e-05
Iter: 1248 loss: 1.27269886e-05
Iter: 1249 loss: 1.27213489e-05
Iter: 1250 loss: 1.27248913e-05
Iter: 1251 loss: 1.27181393e-05
Iter: 1252 loss: 1.27119365e-05
Iter: 1253 loss: 1.27107814e-05
Iter: 1254 loss: 1.27078019e-05
Iter: 1255 loss: 1.26983596e-05
Iter: 1256 loss: 1.27246431e-05
Iter: 1257 loss: 1.26958103e-05
Iter: 1258 loss: 1.26872865e-05
Iter: 1259 loss: 1.27235644e-05
Iter: 1260 loss: 1.26855575e-05
Iter: 1261 loss: 1.26802379e-05
Iter: 1262 loss: 1.26878249e-05
Iter: 1263 loss: 1.26771756e-05
Iter: 1264 loss: 1.26694622e-05
Iter: 1265 loss: 1.27149578e-05
Iter: 1266 loss: 1.26682871e-05
Iter: 1267 loss: 1.26619834e-05
Iter: 1268 loss: 1.26912673e-05
Iter: 1269 loss: 1.26611922e-05
Iter: 1270 loss: 1.26620871e-05
Iter: 1271 loss: 1.26585e-05
Iter: 1272 loss: 1.26575706e-05
Iter: 1273 loss: 1.26541227e-05
Iter: 1274 loss: 1.26522864e-05
Iter: 1275 loss: 1.26494779e-05
Iter: 1276 loss: 1.26415362e-05
Iter: 1277 loss: 1.26746199e-05
Iter: 1278 loss: 1.263985e-05
Iter: 1279 loss: 1.26334235e-05
Iter: 1280 loss: 1.26773411e-05
Iter: 1281 loss: 1.26331524e-05
Iter: 1282 loss: 1.26279101e-05
Iter: 1283 loss: 1.26423129e-05
Iter: 1284 loss: 1.26267332e-05
Iter: 1285 loss: 1.26220148e-05
Iter: 1286 loss: 1.26179521e-05
Iter: 1287 loss: 1.26162467e-05
Iter: 1288 loss: 1.26082095e-05
Iter: 1289 loss: 1.26213872e-05
Iter: 1290 loss: 1.26046198e-05
Iter: 1291 loss: 1.25986871e-05
Iter: 1292 loss: 1.26260493e-05
Iter: 1293 loss: 1.25970164e-05
Iter: 1294 loss: 1.25895513e-05
Iter: 1295 loss: 1.25908336e-05
Iter: 1296 loss: 1.25842116e-05
Iter: 1297 loss: 1.25762799e-05
Iter: 1298 loss: 1.26499963e-05
Iter: 1299 loss: 1.25763663e-05
Iter: 1300 loss: 1.25704373e-05
Iter: 1301 loss: 1.26057621e-05
Iter: 1302 loss: 1.25699853e-05
Iter: 1303 loss: 1.25696897e-05
Iter: 1304 loss: 1.25684019e-05
Iter: 1305 loss: 1.25662627e-05
Iter: 1306 loss: 1.25604256e-05
Iter: 1307 loss: 1.25674151e-05
Iter: 1308 loss: 1.25550641e-05
Iter: 1309 loss: 1.25479082e-05
Iter: 1310 loss: 1.25732713e-05
Iter: 1311 loss: 1.25463184e-05
Iter: 1312 loss: 1.25400329e-05
Iter: 1313 loss: 1.25705756e-05
Iter: 1314 loss: 1.25390561e-05
Iter: 1315 loss: 1.25334527e-05
Iter: 1316 loss: 1.25531751e-05
Iter: 1317 loss: 1.25322695e-05
Iter: 1318 loss: 1.2526576e-05
Iter: 1319 loss: 1.25273255e-05
Iter: 1320 loss: 1.25229635e-05
Iter: 1321 loss: 1.25170409e-05
Iter: 1322 loss: 1.25312854e-05
Iter: 1323 loss: 1.25144506e-05
Iter: 1324 loss: 1.25084161e-05
Iter: 1325 loss: 1.25058323e-05
Iter: 1326 loss: 1.25019078e-05
Iter: 1327 loss: 1.24930957e-05
Iter: 1328 loss: 1.25678844e-05
Iter: 1329 loss: 1.2492631e-05
Iter: 1330 loss: 1.24873e-05
Iter: 1331 loss: 1.25019369e-05
Iter: 1332 loss: 1.24852195e-05
Iter: 1333 loss: 1.24791222e-05
Iter: 1334 loss: 1.25002225e-05
Iter: 1335 loss: 1.24772714e-05
Iter: 1336 loss: 1.24746775e-05
Iter: 1337 loss: 1.24734133e-05
Iter: 1338 loss: 1.24693652e-05
Iter: 1339 loss: 1.24650369e-05
Iter: 1340 loss: 1.24641601e-05
Iter: 1341 loss: 1.24611979e-05
Iter: 1342 loss: 1.24581165e-05
Iter: 1343 loss: 1.2458122e-05
Iter: 1344 loss: 1.24529688e-05
Iter: 1345 loss: 1.24648332e-05
Iter: 1346 loss: 1.24509215e-05
Iter: 1347 loss: 1.24448206e-05
Iter: 1348 loss: 1.24724847e-05
Iter: 1349 loss: 1.2442908e-05
Iter: 1350 loss: 1.24378148e-05
Iter: 1351 loss: 1.24474209e-05
Iter: 1352 loss: 1.24356429e-05
Iter: 1353 loss: 1.24302833e-05
Iter: 1354 loss: 1.24294666e-05
Iter: 1355 loss: 1.24258149e-05
Iter: 1356 loss: 1.24180206e-05
Iter: 1357 loss: 1.24355493e-05
Iter: 1358 loss: 1.24152266e-05
Iter: 1359 loss: 1.24078961e-05
Iter: 1360 loss: 1.24412036e-05
Iter: 1361 loss: 1.240647e-05
Iter: 1362 loss: 1.24000007e-05
Iter: 1363 loss: 1.24063099e-05
Iter: 1364 loss: 1.23960426e-05
Iter: 1365 loss: 1.23870459e-05
Iter: 1366 loss: 1.24183571e-05
Iter: 1367 loss: 1.23849568e-05
Iter: 1368 loss: 1.23829177e-05
Iter: 1369 loss: 1.23808495e-05
Iter: 1370 loss: 1.23776081e-05
Iter: 1371 loss: 1.23930067e-05
Iter: 1372 loss: 1.23765158e-05
Iter: 1373 loss: 1.23750151e-05
Iter: 1374 loss: 1.23701675e-05
Iter: 1375 loss: 1.23717537e-05
Iter: 1376 loss: 1.23653508e-05
Iter: 1377 loss: 1.23553473e-05
Iter: 1378 loss: 1.24013914e-05
Iter: 1379 loss: 1.23535574e-05
Iter: 1380 loss: 1.23490336e-05
Iter: 1381 loss: 1.2348517e-05
Iter: 1382 loss: 1.23449045e-05
Iter: 1383 loss: 1.23436e-05
Iter: 1384 loss: 1.23408445e-05
Iter: 1385 loss: 1.23360132e-05
Iter: 1386 loss: 1.23422951e-05
Iter: 1387 loss: 1.23339141e-05
Iter: 1388 loss: 1.23265345e-05
Iter: 1389 loss: 1.23372738e-05
Iter: 1390 loss: 1.23230157e-05
Iter: 1391 loss: 1.23172622e-05
Iter: 1392 loss: 1.2330067e-05
Iter: 1393 loss: 1.23151876e-05
Iter: 1394 loss: 1.23070877e-05
Iter: 1395 loss: 1.23248992e-05
Iter: 1396 loss: 1.23038153e-05
Iter: 1397 loss: 1.22979909e-05
Iter: 1398 loss: 1.23287646e-05
Iter: 1399 loss: 1.22970187e-05
Iter: 1400 loss: 1.22925467e-05
Iter: 1401 loss: 1.23415139e-05
Iter: 1402 loss: 1.22918063e-05
Iter: 1403 loss: 1.22875254e-05
Iter: 1404 loss: 1.2321364e-05
Iter: 1405 loss: 1.2286775e-05
Iter: 1406 loss: 1.22840456e-05
Iter: 1407 loss: 1.22767487e-05
Iter: 1408 loss: 1.23510799e-05
Iter: 1409 loss: 1.22757874e-05
Iter: 1410 loss: 1.22692891e-05
Iter: 1411 loss: 1.22735264e-05
Iter: 1412 loss: 1.22652955e-05
Iter: 1413 loss: 1.22589427e-05
Iter: 1414 loss: 1.23276923e-05
Iter: 1415 loss: 1.22592164e-05
Iter: 1416 loss: 1.22535284e-05
Iter: 1417 loss: 1.22602114e-05
Iter: 1418 loss: 1.2250337e-05
Iter: 1419 loss: 1.22435358e-05
Iter: 1420 loss: 1.22522652e-05
Iter: 1421 loss: 1.22390684e-05
Iter: 1422 loss: 1.22335123e-05
Iter: 1423 loss: 1.22519705e-05
Iter: 1424 loss: 1.22317942e-05
Iter: 1425 loss: 1.22254642e-05
Iter: 1426 loss: 1.22270567e-05
Iter: 1427 loss: 1.22210076e-05
Iter: 1428 loss: 1.22121328e-05
Iter: 1429 loss: 1.22551965e-05
Iter: 1430 loss: 1.22110332e-05
Iter: 1431 loss: 1.22051079e-05
Iter: 1432 loss: 1.222234e-05
Iter: 1433 loss: 1.22031597e-05
Iter: 1434 loss: 1.21977537e-05
Iter: 1435 loss: 1.22482234e-05
Iter: 1436 loss: 1.21978555e-05
Iter: 1437 loss: 1.21952116e-05
Iter: 1438 loss: 1.21944477e-05
Iter: 1439 loss: 1.21933299e-05
Iter: 1440 loss: 1.21883058e-05
Iter: 1441 loss: 1.22293295e-05
Iter: 1442 loss: 1.21879766e-05
Iter: 1443 loss: 1.21829944e-05
Iter: 1444 loss: 1.21869743e-05
Iter: 1445 loss: 1.21808389e-05
Iter: 1446 loss: 1.21757548e-05
Iter: 1447 loss: 1.21739404e-05
Iter: 1448 loss: 1.21708035e-05
Iter: 1449 loss: 1.21650146e-05
Iter: 1450 loss: 1.21651465e-05
Iter: 1451 loss: 1.21602397e-05
Iter: 1452 loss: 1.21594067e-05
Iter: 1453 loss: 1.21565135e-05
Iter: 1454 loss: 1.21506619e-05
Iter: 1455 loss: 1.21555859e-05
Iter: 1456 loss: 1.21478115e-05
Iter: 1457 loss: 1.21409576e-05
Iter: 1458 loss: 1.21617941e-05
Iter: 1459 loss: 1.21384483e-05
Iter: 1460 loss: 1.21325256e-05
Iter: 1461 loss: 1.21553421e-05
Iter: 1462 loss: 1.21308512e-05
Iter: 1463 loss: 1.21254316e-05
Iter: 1464 loss: 1.21240782e-05
Iter: 1465 loss: 1.21198163e-05
Iter: 1466 loss: 1.21124449e-05
Iter: 1467 loss: 1.22159618e-05
Iter: 1468 loss: 1.21120811e-05
Iter: 1469 loss: 1.21119883e-05
Iter: 1470 loss: 1.21097273e-05
Iter: 1471 loss: 1.21082885e-05
Iter: 1472 loss: 1.21042494e-05
Iter: 1473 loss: 1.21610046e-05
Iter: 1474 loss: 1.21042576e-05
Iter: 1475 loss: 1.2100978e-05
Iter: 1476 loss: 1.20957302e-05
Iter: 1477 loss: 1.2095511e-05
Iter: 1478 loss: 1.20880377e-05
Iter: 1479 loss: 1.2106444e-05
Iter: 1480 loss: 1.20858667e-05
Iter: 1481 loss: 1.20811619e-05
Iter: 1482 loss: 1.20812847e-05
Iter: 1483 loss: 1.20765708e-05
Iter: 1484 loss: 1.20806017e-05
Iter: 1485 loss: 1.20742488e-05
Iter: 1486 loss: 1.20696568e-05
Iter: 1487 loss: 1.20691311e-05
Iter: 1488 loss: 1.20652476e-05
Iter: 1489 loss: 1.2059214e-05
Iter: 1490 loss: 1.2081694e-05
Iter: 1491 loss: 1.20577952e-05
Iter: 1492 loss: 1.20518989e-05
Iter: 1493 loss: 1.20676987e-05
Iter: 1494 loss: 1.20494515e-05
Iter: 1495 loss: 1.20437699e-05
Iter: 1496 loss: 1.204203e-05
Iter: 1497 loss: 1.20376299e-05
Iter: 1498 loss: 1.2030303e-05
Iter: 1499 loss: 1.21190878e-05
Iter: 1500 loss: 1.20299555e-05
Iter: 1501 loss: 1.20318327e-05
Iter: 1502 loss: 1.20277691e-05
Iter: 1503 loss: 1.2026806e-05
Iter: 1504 loss: 1.20217892e-05
Iter: 1505 loss: 1.20442728e-05
Iter: 1506 loss: 1.20194145e-05
Iter: 1507 loss: 1.20125378e-05
Iter: 1508 loss: 1.2023047e-05
Iter: 1509 loss: 1.20087507e-05
Iter: 1510 loss: 1.20034838e-05
Iter: 1511 loss: 1.20222467e-05
Iter: 1512 loss: 1.20013792e-05
Iter: 1513 loss: 1.1997101e-05
Iter: 1514 loss: 1.19982287e-05
Iter: 1515 loss: 1.19931738e-05
Iter: 1516 loss: 1.19871274e-05
Iter: 1517 loss: 1.20688637e-05
Iter: 1518 loss: 1.19865545e-05
Iter: 1519 loss: 1.19818742e-05
Iter: 1520 loss: 1.19809365e-05
Iter: 1521 loss: 1.1977344e-05
Iter: 1522 loss: 1.19726374e-05
Iter: 1523 loss: 1.19775923e-05
Iter: 1524 loss: 1.1969978e-05
Iter: 1525 loss: 1.1962522e-05
Iter: 1526 loss: 1.19711749e-05
Iter: 1527 loss: 1.19586039e-05
Iter: 1528 loss: 1.19508986e-05
Iter: 1529 loss: 1.19984188e-05
Iter: 1530 loss: 1.19499164e-05
Iter: 1531 loss: 1.1943348e-05
Iter: 1532 loss: 1.19557399e-05
Iter: 1533 loss: 1.19413326e-05
Iter: 1534 loss: 1.19377864e-05
Iter: 1535 loss: 1.19369233e-05
Iter: 1536 loss: 1.19333672e-05
Iter: 1537 loss: 1.19478682e-05
Iter: 1538 loss: 1.19327105e-05
Iter: 1539 loss: 1.19306014e-05
Iter: 1540 loss: 1.19276183e-05
Iter: 1541 loss: 1.19738897e-05
Iter: 1542 loss: 1.19269716e-05
Iter: 1543 loss: 1.19223441e-05
Iter: 1544 loss: 1.19292736e-05
Iter: 1545 loss: 1.19194146e-05
Iter: 1546 loss: 1.19138185e-05
Iter: 1547 loss: 1.19175784e-05
Iter: 1548 loss: 1.19101896e-05
Iter: 1549 loss: 1.19055894e-05
Iter: 1550 loss: 1.19782526e-05
Iter: 1551 loss: 1.19045508e-05
Iter: 1552 loss: 1.19013712e-05
Iter: 1553 loss: 1.1922757e-05
Iter: 1554 loss: 1.19006636e-05
Iter: 1555 loss: 1.18974067e-05
Iter: 1556 loss: 1.18906401e-05
Iter: 1557 loss: 1.19802944e-05
Iter: 1558 loss: 1.18901189e-05
Iter: 1559 loss: 1.18826374e-05
Iter: 1560 loss: 1.1960512e-05
Iter: 1561 loss: 1.18827083e-05
Iter: 1562 loss: 1.18786274e-05
Iter: 1563 loss: 1.18837288e-05
Iter: 1564 loss: 1.18769394e-05
Iter: 1565 loss: 1.18714852e-05
Iter: 1566 loss: 1.18727712e-05
Iter: 1567 loss: 1.18670041e-05
Iter: 1568 loss: 1.18622211e-05
Iter: 1569 loss: 1.18620492e-05
Iter: 1570 loss: 1.18590606e-05
Iter: 1571 loss: 1.18594699e-05
Iter: 1572 loss: 1.18577345e-05
Iter: 1573 loss: 1.18529888e-05
Iter: 1574 loss: 1.1862252e-05
Iter: 1575 loss: 1.18501412e-05
Iter: 1576 loss: 1.18440184e-05
Iter: 1577 loss: 1.1883958e-05
Iter: 1578 loss: 1.18432927e-05
Iter: 1579 loss: 1.18386406e-05
Iter: 1580 loss: 1.18490225e-05
Iter: 1581 loss: 1.18370581e-05
Iter: 1582 loss: 1.18319585e-05
Iter: 1583 loss: 1.18396183e-05
Iter: 1584 loss: 1.18300759e-05
Iter: 1585 loss: 1.18244616e-05
Iter: 1586 loss: 1.18677672e-05
Iter: 1587 loss: 1.18242315e-05
Iter: 1588 loss: 1.18187127e-05
Iter: 1589 loss: 1.18285943e-05
Iter: 1590 loss: 1.18166236e-05
Iter: 1591 loss: 1.18126336e-05
Iter: 1592 loss: 1.18158187e-05
Iter: 1593 loss: 1.18104672e-05
Iter: 1594 loss: 1.18047683e-05
Iter: 1595 loss: 1.18016578e-05
Iter: 1596 loss: 1.17989093e-05
Iter: 1597 loss: 1.17906438e-05
Iter: 1598 loss: 1.18351099e-05
Iter: 1599 loss: 1.1790371e-05
Iter: 1600 loss: 1.178394e-05
Iter: 1601 loss: 1.17966392e-05
Iter: 1602 loss: 1.17807285e-05
Iter: 1603 loss: 1.17796253e-05
Iter: 1604 loss: 1.177781e-05
Iter: 1605 loss: 1.17736217e-05
Iter: 1606 loss: 1.17726813e-05
Iter: 1607 loss: 1.17704585e-05
Iter: 1608 loss: 1.17678355e-05
Iter: 1609 loss: 1.17634827e-05
Iter: 1610 loss: 1.18547841e-05
Iter: 1611 loss: 1.1763681e-05
Iter: 1612 loss: 1.17586478e-05
Iter: 1613 loss: 1.17557593e-05
Iter: 1614 loss: 1.17531863e-05
Iter: 1615 loss: 1.17476611e-05
Iter: 1616 loss: 1.1748074e-05
Iter: 1617 loss: 1.17445261e-05
Iter: 1618 loss: 1.1762213e-05
Iter: 1619 loss: 1.17439031e-05
Iter: 1620 loss: 1.17408235e-05
Iter: 1621 loss: 1.17464651e-05
Iter: 1622 loss: 1.17389809e-05
Iter: 1623 loss: 1.17349591e-05
Iter: 1624 loss: 1.17408263e-05
Iter: 1625 loss: 1.17330837e-05
Iter: 1626 loss: 1.17289028e-05
Iter: 1627 loss: 1.17370901e-05
Iter: 1628 loss: 1.17268428e-05
Iter: 1629 loss: 1.17224536e-05
Iter: 1630 loss: 1.17269938e-05
Iter: 1631 loss: 1.17203817e-05
Iter: 1632 loss: 1.17142754e-05
Iter: 1633 loss: 1.17142545e-05
Iter: 1634 loss: 1.17093095e-05
Iter: 1635 loss: 1.17030668e-05
Iter: 1636 loss: 1.17923519e-05
Iter: 1637 loss: 1.17030313e-05
Iter: 1638 loss: 1.17005511e-05
Iter: 1639 loss: 1.17001337e-05
Iter: 1640 loss: 1.16989177e-05
Iter: 1641 loss: 1.1695387e-05
Iter: 1642 loss: 1.16953925e-05
Iter: 1643 loss: 1.16923038e-05
Iter: 1644 loss: 1.16848732e-05
Iter: 1645 loss: 1.17033414e-05
Iter: 1646 loss: 1.1682986e-05
Iter: 1647 loss: 1.16775736e-05
Iter: 1648 loss: 1.17148247e-05
Iter: 1649 loss: 1.16772753e-05
Iter: 1650 loss: 1.16726824e-05
Iter: 1651 loss: 1.16762412e-05
Iter: 1652 loss: 1.16695055e-05
Iter: 1653 loss: 1.16633746e-05
Iter: 1654 loss: 1.17183163e-05
Iter: 1655 loss: 1.16631272e-05
Iter: 1656 loss: 1.16592601e-05
Iter: 1657 loss: 1.16745414e-05
Iter: 1658 loss: 1.16587316e-05
Iter: 1659 loss: 1.1654929e-05
Iter: 1660 loss: 1.16550427e-05
Iter: 1661 loss: 1.16524598e-05
Iter: 1662 loss: 1.16469892e-05
Iter: 1663 loss: 1.16508909e-05
Iter: 1664 loss: 1.16436677e-05
Iter: 1665 loss: 1.16372157e-05
Iter: 1666 loss: 1.1653844e-05
Iter: 1667 loss: 1.16346364e-05
Iter: 1668 loss: 1.16292449e-05
Iter: 1669 loss: 1.16609699e-05
Iter: 1670 loss: 1.16286301e-05
Iter: 1671 loss: 1.16310339e-05
Iter: 1672 loss: 1.16267738e-05
Iter: 1673 loss: 1.16258207e-05
Iter: 1674 loss: 1.16212977e-05
Iter: 1675 loss: 1.16194587e-05
Iter: 1676 loss: 1.16164192e-05
Iter: 1677 loss: 1.16103347e-05
Iter: 1678 loss: 1.16411484e-05
Iter: 1679 loss: 1.16095189e-05
Iter: 1680 loss: 1.16040737e-05
Iter: 1681 loss: 1.1608161e-05
Iter: 1682 loss: 1.16013543e-05
Iter: 1683 loss: 1.15946223e-05
Iter: 1684 loss: 1.16252413e-05
Iter: 1685 loss: 1.15932671e-05
Iter: 1686 loss: 1.15893727e-05
Iter: 1687 loss: 1.16196952e-05
Iter: 1688 loss: 1.15891798e-05
Iter: 1689 loss: 1.15858811e-05
Iter: 1690 loss: 1.15951316e-05
Iter: 1691 loss: 1.158532e-05
Iter: 1692 loss: 1.15804123e-05
Iter: 1693 loss: 1.15792909e-05
Iter: 1694 loss: 1.15766034e-05
Iter: 1695 loss: 1.1571612e-05
Iter: 1696 loss: 1.1605066e-05
Iter: 1697 loss: 1.15704515e-05
Iter: 1698 loss: 1.15672974e-05
Iter: 1699 loss: 1.15690855e-05
Iter: 1700 loss: 1.15645471e-05
Iter: 1701 loss: 1.15598414e-05
Iter: 1702 loss: 1.15688981e-05
Iter: 1703 loss: 1.15576468e-05
Iter: 1704 loss: 1.15619387e-05
Iter: 1705 loss: 1.15563416e-05
Iter: 1706 loss: 1.15547009e-05
Iter: 1707 loss: 1.15507264e-05
Iter: 1708 loss: 1.15769817e-05
Iter: 1709 loss: 1.15489529e-05
Iter: 1710 loss: 1.15454859e-05
Iter: 1711 loss: 1.15435341e-05
Iter: 1712 loss: 1.15415751e-05
Iter: 1713 loss: 1.15345356e-05
Iter: 1714 loss: 1.15427865e-05
Iter: 1715 loss: 1.15308731e-05
Iter: 1716 loss: 1.15244211e-05
Iter: 1717 loss: 1.15866405e-05
Iter: 1718 loss: 1.15242938e-05
Iter: 1719 loss: 1.1519287e-05
Iter: 1720 loss: 1.15448056e-05
Iter: 1721 loss: 1.15184966e-05
Iter: 1722 loss: 1.1512815e-05
Iter: 1723 loss: 1.15210796e-05
Iter: 1724 loss: 1.15105067e-05
Iter: 1725 loss: 1.15051598e-05
Iter: 1726 loss: 1.15329203e-05
Iter: 1727 loss: 1.15043204e-05
Iter: 1728 loss: 1.15006878e-05
Iter: 1729 loss: 1.15059929e-05
Iter: 1730 loss: 1.14989452e-05
Iter: 1731 loss: 1.1493642e-05
Iter: 1732 loss: 1.14892946e-05
Iter: 1733 loss: 1.14881841e-05
Iter: 1734 loss: 1.14795102e-05
Iter: 1735 loss: 1.15198691e-05
Iter: 1736 loss: 1.1477845e-05
Iter: 1737 loss: 1.1476538e-05
Iter: 1738 loss: 1.1475272e-05
Iter: 1739 loss: 1.14715476e-05
Iter: 1740 loss: 1.14713157e-05
Iter: 1741 loss: 1.14688428e-05
Iter: 1742 loss: 1.14660015e-05
Iter: 1743 loss: 1.14596523e-05
Iter: 1744 loss: 1.15039493e-05
Iter: 1745 loss: 1.14578233e-05
Iter: 1746 loss: 1.14513887e-05
Iter: 1747 loss: 1.15024786e-05
Iter: 1748 loss: 1.14513914e-05
Iter: 1749 loss: 1.14460636e-05
Iter: 1750 loss: 1.14524992e-05
Iter: 1751 loss: 1.14433869e-05
Iter: 1752 loss: 1.1437206e-05
Iter: 1753 loss: 1.14730374e-05
Iter: 1754 loss: 1.14363575e-05
Iter: 1755 loss: 1.14314171e-05
Iter: 1756 loss: 1.14478307e-05
Iter: 1757 loss: 1.14304339e-05
Iter: 1758 loss: 1.14258573e-05
Iter: 1759 loss: 1.14422864e-05
Iter: 1760 loss: 1.14245458e-05
Iter: 1761 loss: 1.14206978e-05
Iter: 1762 loss: 1.14292743e-05
Iter: 1763 loss: 1.14179766e-05
Iter: 1764 loss: 1.14136637e-05
Iter: 1765 loss: 1.14155955e-05
Iter: 1766 loss: 1.14100358e-05
Iter: 1767 loss: 1.14051636e-05
Iter: 1768 loss: 1.1437176e-05
Iter: 1769 loss: 1.14046052e-05
Iter: 1770 loss: 1.14009154e-05
Iter: 1771 loss: 1.14204968e-05
Iter: 1772 loss: 1.14005716e-05
Iter: 1773 loss: 1.13961769e-05
Iter: 1774 loss: 1.14264694e-05
Iter: 1775 loss: 1.13965743e-05
Iter: 1776 loss: 1.13941132e-05
Iter: 1777 loss: 1.1389041e-05
Iter: 1778 loss: 1.14223594e-05
Iter: 1779 loss: 1.13878195e-05
Iter: 1780 loss: 1.13827455e-05
Iter: 1781 loss: 1.1386539e-05
Iter: 1782 loss: 1.13794522e-05
Iter: 1783 loss: 1.13726983e-05
Iter: 1784 loss: 1.13910501e-05
Iter: 1785 loss: 1.13701462e-05
Iter: 1786 loss: 1.13652204e-05
Iter: 1787 loss: 1.14419472e-05
Iter: 1788 loss: 1.13653223e-05
Iter: 1789 loss: 1.13614178e-05
Iter: 1790 loss: 1.13748829e-05
Iter: 1791 loss: 1.13601463e-05
Iter: 1792 loss: 1.13559472e-05
Iter: 1793 loss: 1.13578317e-05
Iter: 1794 loss: 1.13530132e-05
Iter: 1795 loss: 1.13470696e-05
Iter: 1796 loss: 1.13801871e-05
Iter: 1797 loss: 1.13465958e-05
Iter: 1798 loss: 1.1341619e-05
Iter: 1799 loss: 1.13434189e-05
Iter: 1800 loss: 1.1339529e-05
Iter: 1801 loss: 1.13345104e-05
Iter: 1802 loss: 1.13450533e-05
Iter: 1803 loss: 1.1332223e-05
Iter: 1804 loss: 1.13258247e-05
Iter: 1805 loss: 1.13450988e-05
Iter: 1806 loss: 1.13239139e-05
Iter: 1807 loss: 1.13257338e-05
Iter: 1808 loss: 1.13211381e-05
Iter: 1809 loss: 1.1320466e-05
Iter: 1810 loss: 1.13167262e-05
Iter: 1811 loss: 1.13224869e-05
Iter: 1812 loss: 1.13147489e-05
Iter: 1813 loss: 1.13090537e-05
Iter: 1814 loss: 1.13057467e-05
Iter: 1815 loss: 1.13038914e-05
Iter: 1816 loss: 1.12966663e-05
Iter: 1817 loss: 1.13533288e-05
Iter: 1818 loss: 1.12964663e-05
Iter: 1819 loss: 1.12911966e-05
Iter: 1820 loss: 1.12865637e-05
Iter: 1821 loss: 1.12847201e-05
Iter: 1822 loss: 1.12786192e-05
Iter: 1823 loss: 1.12783673e-05
Iter: 1824 loss: 1.12728103e-05
Iter: 1825 loss: 1.13107153e-05
Iter: 1826 loss: 1.12723237e-05
Iter: 1827 loss: 1.12694688e-05
Iter: 1828 loss: 1.12712196e-05
Iter: 1829 loss: 1.12671723e-05
Iter: 1830 loss: 1.12628077e-05
Iter: 1831 loss: 1.1264252e-05
Iter: 1832 loss: 1.12588878e-05
Iter: 1833 loss: 1.12528405e-05
Iter: 1834 loss: 1.12749731e-05
Iter: 1835 loss: 1.12513344e-05
Iter: 1836 loss: 1.12461039e-05
Iter: 1837 loss: 1.12620883e-05
Iter: 1838 loss: 1.12451935e-05
Iter: 1839 loss: 1.12457119e-05
Iter: 1840 loss: 1.12430571e-05
Iter: 1841 loss: 1.12416501e-05
Iter: 1842 loss: 1.1236657e-05
Iter: 1843 loss: 1.12567132e-05
Iter: 1844 loss: 1.12342022e-05
Iter: 1845 loss: 1.12286916e-05
Iter: 1846 loss: 1.12370053e-05
Iter: 1847 loss: 1.12262696e-05
Iter: 1848 loss: 1.12204389e-05
Iter: 1849 loss: 1.12192392e-05
Iter: 1850 loss: 1.12152266e-05
Iter: 1851 loss: 1.12072848e-05
Iter: 1852 loss: 1.12626722e-05
Iter: 1853 loss: 1.12069065e-05
Iter: 1854 loss: 1.12010457e-05
Iter: 1855 loss: 1.12183952e-05
Iter: 1856 loss: 1.11991376e-05
Iter: 1857 loss: 1.11936515e-05
Iter: 1858 loss: 1.12163816e-05
Iter: 1859 loss: 1.11923546e-05
Iter: 1860 loss: 1.11849286e-05
Iter: 1861 loss: 1.12007037e-05
Iter: 1862 loss: 1.11822646e-05
Iter: 1863 loss: 1.11763384e-05
Iter: 1864 loss: 1.11982899e-05
Iter: 1865 loss: 1.1175e-05
Iter: 1866 loss: 1.1170554e-05
Iter: 1867 loss: 1.1186361e-05
Iter: 1868 loss: 1.11701602e-05
Iter: 1869 loss: 1.11659901e-05
Iter: 1870 loss: 1.11629815e-05
Iter: 1871 loss: 1.11613654e-05
Iter: 1872 loss: 1.11611234e-05
Iter: 1873 loss: 1.11592635e-05
Iter: 1874 loss: 1.11563131e-05
Iter: 1875 loss: 1.11577056e-05
Iter: 1876 loss: 1.11540203e-05
Iter: 1877 loss: 1.11521585e-05
Iter: 1878 loss: 1.11470326e-05
Iter: 1879 loss: 1.11929e-05
Iter: 1880 loss: 1.11466e-05
Iter: 1881 loss: 1.11402132e-05
Iter: 1882 loss: 1.11521949e-05
Iter: 1883 loss: 1.11376849e-05
Iter: 1884 loss: 1.11322115e-05
Iter: 1885 loss: 1.11586014e-05
Iter: 1886 loss: 1.11318495e-05
Iter: 1887 loss: 1.11266872e-05
Iter: 1888 loss: 1.11306081e-05
Iter: 1889 loss: 1.11236259e-05
Iter: 1890 loss: 1.11168083e-05
Iter: 1891 loss: 1.11469308e-05
Iter: 1892 loss: 1.11153e-05
Iter: 1893 loss: 1.11118916e-05
Iter: 1894 loss: 1.11114987e-05
Iter: 1895 loss: 1.1108913e-05
Iter: 1896 loss: 1.11070312e-05
Iter: 1897 loss: 1.11057834e-05
Iter: 1898 loss: 1.1101286e-05
Iter: 1899 loss: 1.11126574e-05
Iter: 1900 loss: 1.10994833e-05
Iter: 1901 loss: 1.1094543e-05
Iter: 1902 loss: 1.11108511e-05
Iter: 1903 loss: 1.10929723e-05
Iter: 1904 loss: 1.10900473e-05
Iter: 1905 loss: 1.11124737e-05
Iter: 1906 loss: 1.10898509e-05
Iter: 1907 loss: 1.10866495e-05
Iter: 1908 loss: 1.11141617e-05
Iter: 1909 loss: 1.10863239e-05
Iter: 1910 loss: 1.10845322e-05
Iter: 1911 loss: 1.10795972e-05
Iter: 1912 loss: 1.10971332e-05
Iter: 1913 loss: 1.10773e-05
Iter: 1914 loss: 1.10711753e-05
Iter: 1915 loss: 1.10920901e-05
Iter: 1916 loss: 1.10692636e-05
Iter: 1917 loss: 1.10636847e-05
Iter: 1918 loss: 1.1078e-05
Iter: 1919 loss: 1.10612555e-05
Iter: 1920 loss: 1.10549681e-05
Iter: 1921 loss: 1.10675792e-05
Iter: 1922 loss: 1.10521678e-05
Iter: 1923 loss: 1.10464862e-05
Iter: 1924 loss: 1.10691835e-05
Iter: 1925 loss: 1.10455258e-05
Iter: 1926 loss: 1.10404026e-05
Iter: 1927 loss: 1.10663314e-05
Iter: 1928 loss: 1.10393266e-05
Iter: 1929 loss: 1.10331621e-05
Iter: 1930 loss: 1.10515302e-05
Iter: 1931 loss: 1.10319779e-05
Iter: 1932 loss: 1.10281198e-05
Iter: 1933 loss: 1.10356195e-05
Iter: 1934 loss: 1.10269229e-05
Iter: 1935 loss: 1.10222663e-05
Iter: 1936 loss: 1.10284254e-05
Iter: 1937 loss: 1.10204255e-05
Iter: 1938 loss: 1.10149249e-05
Iter: 1939 loss: 1.10150477e-05
Iter: 1940 loss: 1.10105839e-05
Iter: 1941 loss: 1.10171168e-05
Iter: 1942 loss: 1.10076799e-05
Iter: 1943 loss: 1.1006714e-05
Iter: 1944 loss: 1.10016281e-05
Iter: 1945 loss: 1.10196434e-05
Iter: 1946 loss: 1.0999398e-05
Iter: 1947 loss: 1.09948014e-05
Iter: 1948 loss: 1.10005967e-05
Iter: 1949 loss: 1.0991791e-05
Iter: 1950 loss: 1.09857283e-05
Iter: 1951 loss: 1.09968933e-05
Iter: 1952 loss: 1.0983038e-05
Iter: 1953 loss: 1.09776247e-05
Iter: 1954 loss: 1.09962439e-05
Iter: 1955 loss: 1.09766388e-05
Iter: 1956 loss: 1.09707289e-05
Iter: 1957 loss: 1.09753382e-05
Iter: 1958 loss: 1.09674811e-05
Iter: 1959 loss: 1.09594639e-05
Iter: 1960 loss: 1.09940911e-05
Iter: 1961 loss: 1.09582888e-05
Iter: 1962 loss: 1.09539142e-05
Iter: 1963 loss: 1.09538505e-05
Iter: 1964 loss: 1.09507473e-05
Iter: 1965 loss: 1.09459597e-05
Iter: 1966 loss: 1.09461935e-05
Iter: 1967 loss: 1.09396551e-05
Iter: 1968 loss: 1.09602515e-05
Iter: 1969 loss: 1.09377743e-05
Iter: 1970 loss: 1.09332777e-05
Iter: 1971 loss: 1.09588646e-05
Iter: 1972 loss: 1.09326102e-05
Iter: 1973 loss: 1.09297398e-05
Iter: 1974 loss: 1.09743632e-05
Iter: 1975 loss: 1.09295916e-05
Iter: 1976 loss: 1.09259672e-05
Iter: 1977 loss: 1.09313705e-05
Iter: 1978 loss: 1.09247248e-05
Iter: 1979 loss: 1.09226021e-05
Iter: 1980 loss: 1.09181037e-05
Iter: 1981 loss: 1.09490711e-05
Iter: 1982 loss: 1.09172142e-05
Iter: 1983 loss: 1.09123321e-05
Iter: 1984 loss: 1.09386838e-05
Iter: 1985 loss: 1.09114881e-05
Iter: 1986 loss: 1.09071416e-05
Iter: 1987 loss: 1.0904515e-05
Iter: 1988 loss: 1.09024486e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.4/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2.8
+ date
Sun Nov  8 17:10:39 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 2.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda466de1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda467c1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda467c1d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda467c1488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda4671c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda466a6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda46621b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda467c2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda466ad2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda4656fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda465209d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda46534f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda4651d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda4651dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda46668ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda46668730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda4666c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda4666cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda464b89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda464abf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda1d7a8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda46465378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda1d6b7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda1d6d4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda1d6cd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda1d6e87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda1d6838c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda1d684840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda1d73e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda1d767620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f86cc510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f8765048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f8765f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f8728a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f8686c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9f8722730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 202, in <module>
    grads = tape.gradient(loss, model.trainable_weights)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1266, in _backward_function_wrapper
    processed_args, remapped_captures)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input is not invertible.
	 [[node gradients/MatrixDeterminant_grad/MatrixInverse (defined at biholoNN_train.py:200) ]] [Op:__inference___backward_volume_form_4446_8945]

Function call stack:
__backward_volume_form_4446

+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2.8/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f81a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f90b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f90bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f85f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f7c6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f7bf2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f79fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f7516a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f7351e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f6e3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f6b99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f6d91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f6e1d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f6e1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f6e1158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f683950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f633510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f5f9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f5d79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f5cff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f56e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f523598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f4eb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f510620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f510598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0a0f4fad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09d27d0d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09d27e5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09d27e56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09d2798ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09d27659d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09d2779400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09d2779bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09d2733ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09d26cc6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09ac619510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000114681774
Iter: 2 loss: 0.000276344479
Iter: 3 loss: 0.000104499406
Iter: 4 loss: 9.88615939e-05
Iter: 5 loss: 9.16930658e-05
Iter: 6 loss: 9.11463867e-05
Iter: 7 loss: 8.6631233e-05
Iter: 8 loss: 7.80525588e-05
Iter: 9 loss: 0.000251316174
Iter: 10 loss: 7.8011828e-05
Iter: 11 loss: 7.13899717e-05
Iter: 12 loss: 8.09702397e-05
Iter: 13 loss: 6.81326928e-05
Iter: 14 loss: 6.58663193e-05
Iter: 15 loss: 6.29933493e-05
Iter: 16 loss: 6.27787085e-05
Iter: 17 loss: 5.96660975e-05
Iter: 18 loss: 5.85581511e-05
Iter: 19 loss: 5.6806588e-05
Iter: 20 loss: 5.44669747e-05
Iter: 21 loss: 6.24426757e-05
Iter: 22 loss: 5.38382e-05
Iter: 23 loss: 5.14235726e-05
Iter: 24 loss: 4.94303895e-05
Iter: 25 loss: 4.87329526e-05
Iter: 26 loss: 4.6650719e-05
Iter: 27 loss: 4.66331112e-05
Iter: 28 loss: 4.55519112e-05
Iter: 29 loss: 4.44127072e-05
Iter: 30 loss: 4.42195123e-05
Iter: 31 loss: 4.27517552e-05
Iter: 32 loss: 5.80108426e-05
Iter: 33 loss: 4.27105333e-05
Iter: 34 loss: 4.16605508e-05
Iter: 35 loss: 4.10658358e-05
Iter: 36 loss: 4.06135659e-05
Iter: 37 loss: 3.95265888e-05
Iter: 38 loss: 4.40403601e-05
Iter: 39 loss: 3.92950315e-05
Iter: 40 loss: 3.86131142e-05
Iter: 41 loss: 3.84017112e-05
Iter: 42 loss: 3.79977e-05
Iter: 43 loss: 3.70668495e-05
Iter: 44 loss: 4.69390288e-05
Iter: 45 loss: 3.70440175e-05
Iter: 46 loss: 3.66254571e-05
Iter: 47 loss: 3.65602718e-05
Iter: 48 loss: 3.63170111e-05
Iter: 49 loss: 3.57223398e-05
Iter: 50 loss: 4.18547788e-05
Iter: 51 loss: 3.56534729e-05
Iter: 52 loss: 3.50339142e-05
Iter: 53 loss: 3.90823188e-05
Iter: 54 loss: 3.49686925e-05
Iter: 55 loss: 3.46204142e-05
Iter: 56 loss: 3.48459398e-05
Iter: 57 loss: 3.43999309e-05
Iter: 58 loss: 3.38598147e-05
Iter: 59 loss: 3.50269256e-05
Iter: 60 loss: 3.36487938e-05
Iter: 61 loss: 3.32011841e-05
Iter: 62 loss: 3.33948046e-05
Iter: 63 loss: 3.28944952e-05
Iter: 64 loss: 3.2335578e-05
Iter: 65 loss: 3.73907242e-05
Iter: 66 loss: 3.23107961e-05
Iter: 67 loss: 3.20079089e-05
Iter: 68 loss: 3.17772065e-05
Iter: 69 loss: 3.1678479e-05
Iter: 70 loss: 3.1124986e-05
Iter: 71 loss: 3.55419397e-05
Iter: 72 loss: 3.10879877e-05
Iter: 73 loss: 3.08056551e-05
Iter: 74 loss: 3.32306663e-05
Iter: 75 loss: 3.07888477e-05
Iter: 76 loss: 3.05674403e-05
Iter: 77 loss: 3.0293806e-05
Iter: 78 loss: 3.02699445e-05
Iter: 79 loss: 3.02359513e-05
Iter: 80 loss: 3.01001874e-05
Iter: 81 loss: 2.9948962e-05
Iter: 82 loss: 2.99711701e-05
Iter: 83 loss: 2.9834926e-05
Iter: 84 loss: 2.97040315e-05
Iter: 85 loss: 2.94947185e-05
Iter: 86 loss: 2.9493e-05
Iter: 87 loss: 2.91600336e-05
Iter: 88 loss: 3.00940555e-05
Iter: 89 loss: 2.90532735e-05
Iter: 90 loss: 2.89032723e-05
Iter: 91 loss: 3.03803281e-05
Iter: 92 loss: 2.88979936e-05
Iter: 93 loss: 2.87519924e-05
Iter: 94 loss: 2.84845937e-05
Iter: 95 loss: 3.46143534e-05
Iter: 96 loss: 2.84836733e-05
Iter: 97 loss: 2.82716592e-05
Iter: 98 loss: 3.06399597e-05
Iter: 99 loss: 2.82675755e-05
Iter: 100 loss: 2.80618515e-05
Iter: 101 loss: 2.81940393e-05
Iter: 102 loss: 2.79318447e-05
Iter: 103 loss: 2.77019935e-05
Iter: 104 loss: 2.8510729e-05
Iter: 105 loss: 2.76433602e-05
Iter: 106 loss: 2.74107115e-05
Iter: 107 loss: 2.87503553e-05
Iter: 108 loss: 2.73790811e-05
Iter: 109 loss: 2.72111247e-05
Iter: 110 loss: 2.84369871e-05
Iter: 111 loss: 2.71974259e-05
Iter: 112 loss: 2.71052e-05
Iter: 113 loss: 2.77941745e-05
Iter: 114 loss: 2.70978016e-05
Iter: 115 loss: 2.69852699e-05
Iter: 116 loss: 2.71506469e-05
Iter: 117 loss: 2.69307966e-05
Iter: 118 loss: 2.68438544e-05
Iter: 119 loss: 2.67842679e-05
Iter: 120 loss: 2.67533014e-05
Iter: 121 loss: 2.66557945e-05
Iter: 122 loss: 2.69839766e-05
Iter: 123 loss: 2.66285133e-05
Iter: 124 loss: 2.65092931e-05
Iter: 125 loss: 2.64441405e-05
Iter: 126 loss: 2.63912625e-05
Iter: 127 loss: 2.62513604e-05
Iter: 128 loss: 2.79836713e-05
Iter: 129 loss: 2.62495469e-05
Iter: 130 loss: 2.61440746e-05
Iter: 131 loss: 2.59536646e-05
Iter: 132 loss: 3.05997528e-05
Iter: 133 loss: 2.59542467e-05
Iter: 134 loss: 2.57720549e-05
Iter: 135 loss: 2.84847702e-05
Iter: 136 loss: 2.57715e-05
Iter: 137 loss: 2.56480307e-05
Iter: 138 loss: 2.55651394e-05
Iter: 139 loss: 2.55179093e-05
Iter: 140 loss: 2.53665421e-05
Iter: 141 loss: 2.7680584e-05
Iter: 142 loss: 2.5366362e-05
Iter: 143 loss: 2.52817372e-05
Iter: 144 loss: 2.55141022e-05
Iter: 145 loss: 2.52537084e-05
Iter: 146 loss: 2.51586789e-05
Iter: 147 loss: 2.59919525e-05
Iter: 148 loss: 2.51546535e-05
Iter: 149 loss: 2.50787562e-05
Iter: 150 loss: 2.5483565e-05
Iter: 151 loss: 2.50662652e-05
Iter: 152 loss: 2.50230223e-05
Iter: 153 loss: 2.49798886e-05
Iter: 154 loss: 2.497069e-05
Iter: 155 loss: 2.48951728e-05
Iter: 156 loss: 2.4828696e-05
Iter: 157 loss: 2.48092729e-05
Iter: 158 loss: 2.4670313e-05
Iter: 159 loss: 2.53804e-05
Iter: 160 loss: 2.46477648e-05
Iter: 161 loss: 2.45464253e-05
Iter: 162 loss: 2.48283468e-05
Iter: 163 loss: 2.4513718e-05
Iter: 164 loss: 2.4398505e-05
Iter: 165 loss: 2.45498049e-05
Iter: 166 loss: 2.43398936e-05
Iter: 167 loss: 2.424579e-05
Iter: 168 loss: 2.45597403e-05
Iter: 169 loss: 2.42202404e-05
Iter: 170 loss: 2.40929494e-05
Iter: 171 loss: 2.40422214e-05
Iter: 172 loss: 2.39743858e-05
Iter: 173 loss: 2.38714183e-05
Iter: 174 loss: 2.51928705e-05
Iter: 175 loss: 2.38708599e-05
Iter: 176 loss: 2.37707554e-05
Iter: 177 loss: 2.38306129e-05
Iter: 178 loss: 2.37058139e-05
Iter: 179 loss: 2.36758497e-05
Iter: 180 loss: 2.36472042e-05
Iter: 181 loss: 2.36120977e-05
Iter: 182 loss: 2.37941349e-05
Iter: 183 loss: 2.36068081e-05
Iter: 184 loss: 2.35809021e-05
Iter: 185 loss: 2.35216394e-05
Iter: 186 loss: 2.42726819e-05
Iter: 187 loss: 2.351683e-05
Iter: 188 loss: 2.34401632e-05
Iter: 189 loss: 2.36092837e-05
Iter: 190 loss: 2.34107738e-05
Iter: 191 loss: 2.33450319e-05
Iter: 192 loss: 2.36411761e-05
Iter: 193 loss: 2.33323444e-05
Iter: 194 loss: 2.32690581e-05
Iter: 195 loss: 2.33560258e-05
Iter: 196 loss: 2.32377206e-05
Iter: 197 loss: 2.31723661e-05
Iter: 198 loss: 2.34473282e-05
Iter: 199 loss: 2.31587146e-05
Iter: 200 loss: 2.30929145e-05
Iter: 201 loss: 2.3046694e-05
Iter: 202 loss: 2.30226015e-05
Iter: 203 loss: 2.29460966e-05
Iter: 204 loss: 2.40149166e-05
Iter: 205 loss: 2.29456055e-05
Iter: 206 loss: 2.28970421e-05
Iter: 207 loss: 2.2871749e-05
Iter: 208 loss: 2.28499339e-05
Iter: 209 loss: 2.27770433e-05
Iter: 210 loss: 2.34653489e-05
Iter: 211 loss: 2.27742748e-05
Iter: 212 loss: 2.27426026e-05
Iter: 213 loss: 2.27428609e-05
Iter: 214 loss: 2.27125984e-05
Iter: 215 loss: 2.28143654e-05
Iter: 216 loss: 2.27060555e-05
Iter: 217 loss: 2.26765733e-05
Iter: 218 loss: 2.26326229e-05
Iter: 219 loss: 2.26321972e-05
Iter: 220 loss: 2.25911244e-05
Iter: 221 loss: 2.26992779e-05
Iter: 222 loss: 2.25776857e-05
Iter: 223 loss: 2.25304102e-05
Iter: 224 loss: 2.25076474e-05
Iter: 225 loss: 2.24839496e-05
Iter: 226 loss: 2.24339437e-05
Iter: 227 loss: 2.32208895e-05
Iter: 228 loss: 2.24336e-05
Iter: 229 loss: 2.23978477e-05
Iter: 230 loss: 2.23600946e-05
Iter: 231 loss: 2.23534735e-05
Iter: 232 loss: 2.22805447e-05
Iter: 233 loss: 2.26262073e-05
Iter: 234 loss: 2.22679337e-05
Iter: 235 loss: 2.22182389e-05
Iter: 236 loss: 2.22679337e-05
Iter: 237 loss: 2.21904756e-05
Iter: 238 loss: 2.21205337e-05
Iter: 239 loss: 2.23176321e-05
Iter: 240 loss: 2.20977599e-05
Iter: 241 loss: 2.20430466e-05
Iter: 242 loss: 2.23160278e-05
Iter: 243 loss: 2.20334568e-05
Iter: 244 loss: 2.1982778e-05
Iter: 245 loss: 2.23590614e-05
Iter: 246 loss: 2.19782414e-05
Iter: 247 loss: 2.19404428e-05
Iter: 248 loss: 2.24660507e-05
Iter: 249 loss: 2.19402391e-05
Iter: 250 loss: 2.1920263e-05
Iter: 251 loss: 2.18997411e-05
Iter: 252 loss: 2.18963833e-05
Iter: 253 loss: 2.1867756e-05
Iter: 254 loss: 2.18335881e-05
Iter: 255 loss: 2.18304394e-05
Iter: 256 loss: 2.17652705e-05
Iter: 257 loss: 2.19469293e-05
Iter: 258 loss: 2.1744323e-05
Iter: 259 loss: 2.16986264e-05
Iter: 260 loss: 2.19362046e-05
Iter: 261 loss: 2.16924054e-05
Iter: 262 loss: 2.16418593e-05
Iter: 263 loss: 2.16040389e-05
Iter: 264 loss: 2.15870496e-05
Iter: 265 loss: 2.15216915e-05
Iter: 266 loss: 2.21256705e-05
Iter: 267 loss: 2.15184446e-05
Iter: 268 loss: 2.14649808e-05
Iter: 269 loss: 2.14625e-05
Iter: 270 loss: 2.14213651e-05
Iter: 271 loss: 2.13701387e-05
Iter: 272 loss: 2.18849273e-05
Iter: 273 loss: 2.13683816e-05
Iter: 274 loss: 2.13259955e-05
Iter: 275 loss: 2.13132607e-05
Iter: 276 loss: 2.12879258e-05
Iter: 277 loss: 2.12542891e-05
Iter: 278 loss: 2.12498926e-05
Iter: 279 loss: 2.12287705e-05
Iter: 280 loss: 2.12290633e-05
Iter: 281 loss: 2.12159e-05
Iter: 282 loss: 2.11895585e-05
Iter: 283 loss: 2.17345842e-05
Iter: 284 loss: 2.11891911e-05
Iter: 285 loss: 2.11574188e-05
Iter: 286 loss: 2.11323149e-05
Iter: 287 loss: 2.1121974e-05
Iter: 288 loss: 2.107619e-05
Iter: 289 loss: 2.14114225e-05
Iter: 290 loss: 2.10721046e-05
Iter: 291 loss: 2.10322978e-05
Iter: 292 loss: 2.10473e-05
Iter: 293 loss: 2.10047692e-05
Iter: 294 loss: 2.09592272e-05
Iter: 295 loss: 2.1339376e-05
Iter: 296 loss: 2.09555019e-05
Iter: 297 loss: 2.09264581e-05
Iter: 298 loss: 2.09117643e-05
Iter: 299 loss: 2.08974088e-05
Iter: 300 loss: 2.08472811e-05
Iter: 301 loss: 2.11281658e-05
Iter: 302 loss: 2.08401016e-05
Iter: 303 loss: 2.08102902e-05
Iter: 304 loss: 2.0836962e-05
Iter: 305 loss: 2.07926441e-05
Iter: 306 loss: 2.07490702e-05
Iter: 307 loss: 2.08848469e-05
Iter: 308 loss: 2.07363373e-05
Iter: 309 loss: 2.07025387e-05
Iter: 310 loss: 2.09724258e-05
Iter: 311 loss: 2.07012763e-05
Iter: 312 loss: 2.06773366e-05
Iter: 313 loss: 2.06772602e-05
Iter: 314 loss: 2.06610985e-05
Iter: 315 loss: 2.06344721e-05
Iter: 316 loss: 2.06342229e-05
Iter: 317 loss: 2.06117329e-05
Iter: 318 loss: 2.06504046e-05
Iter: 319 loss: 2.06003424e-05
Iter: 320 loss: 2.0571897e-05
Iter: 321 loss: 2.05520646e-05
Iter: 322 loss: 2.05432389e-05
Iter: 323 loss: 2.04906937e-05
Iter: 324 loss: 2.07794874e-05
Iter: 325 loss: 2.04838198e-05
Iter: 326 loss: 2.04535463e-05
Iter: 327 loss: 2.05787765e-05
Iter: 328 loss: 2.04467797e-05
Iter: 329 loss: 2.04131211e-05
Iter: 330 loss: 2.0396903e-05
Iter: 331 loss: 2.03806158e-05
Iter: 332 loss: 2.03428535e-05
Iter: 333 loss: 2.07137855e-05
Iter: 334 loss: 2.03416093e-05
Iter: 335 loss: 2.03104682e-05
Iter: 336 loss: 2.02906213e-05
Iter: 337 loss: 2.02784249e-05
Iter: 338 loss: 2.02401552e-05
Iter: 339 loss: 2.0595362e-05
Iter: 340 loss: 2.02386182e-05
Iter: 341 loss: 2.02061565e-05
Iter: 342 loss: 2.02650735e-05
Iter: 343 loss: 2.01922103e-05
Iter: 344 loss: 2.01780185e-05
Iter: 345 loss: 2.01706371e-05
Iter: 346 loss: 2.0156207e-05
Iter: 347 loss: 2.01434214e-05
Iter: 348 loss: 2.01394978e-05
Iter: 349 loss: 2.01217572e-05
Iter: 350 loss: 2.01001203e-05
Iter: 351 loss: 2.0097501e-05
Iter: 352 loss: 2.00582017e-05
Iter: 353 loss: 2.00992909e-05
Iter: 354 loss: 2.00368286e-05
Iter: 355 loss: 1.99999413e-05
Iter: 356 loss: 2.03013624e-05
Iter: 357 loss: 1.99978931e-05
Iter: 358 loss: 1.9967707e-05
Iter: 359 loss: 1.99817787e-05
Iter: 360 loss: 1.9947689e-05
Iter: 361 loss: 1.9913683e-05
Iter: 362 loss: 2.01763869e-05
Iter: 363 loss: 1.99116075e-05
Iter: 364 loss: 1.98891703e-05
Iter: 365 loss: 1.98755442e-05
Iter: 366 loss: 1.98653433e-05
Iter: 367 loss: 1.98252601e-05
Iter: 368 loss: 2.00201775e-05
Iter: 369 loss: 1.9818257e-05
Iter: 370 loss: 1.97909831e-05
Iter: 371 loss: 1.98068519e-05
Iter: 372 loss: 1.97738791e-05
Iter: 373 loss: 1.97335958e-05
Iter: 374 loss: 1.9948282e-05
Iter: 375 loss: 1.97271038e-05
Iter: 376 loss: 1.97302761e-05
Iter: 377 loss: 1.97151057e-05
Iter: 378 loss: 1.9706129e-05
Iter: 379 loss: 1.96899982e-05
Iter: 380 loss: 1.96902656e-05
Iter: 381 loss: 1.96701076e-05
Iter: 382 loss: 1.96565707e-05
Iter: 383 loss: 1.96491146e-05
Iter: 384 loss: 1.96239016e-05
Iter: 385 loss: 1.97351856e-05
Iter: 386 loss: 1.96190158e-05
Iter: 387 loss: 1.95939829e-05
Iter: 388 loss: 1.96109759e-05
Iter: 389 loss: 1.95785487e-05
Iter: 390 loss: 1.95509019e-05
Iter: 391 loss: 1.97681111e-05
Iter: 392 loss: 1.9548972e-05
Iter: 393 loss: 1.95308512e-05
Iter: 394 loss: 1.95468747e-05
Iter: 395 loss: 1.95206449e-05
Iter: 396 loss: 1.94929962e-05
Iter: 397 loss: 1.95150187e-05
Iter: 398 loss: 1.94766726e-05
Iter: 399 loss: 1.94498825e-05
Iter: 400 loss: 1.95848716e-05
Iter: 401 loss: 1.94454442e-05
Iter: 402 loss: 1.9416213e-05
Iter: 403 loss: 1.94146596e-05
Iter: 404 loss: 1.93928936e-05
Iter: 405 loss: 1.9366491e-05
Iter: 406 loss: 1.96813016e-05
Iter: 407 loss: 1.93658761e-05
Iter: 408 loss: 1.93493106e-05
Iter: 409 loss: 1.93496599e-05
Iter: 410 loss: 1.93325232e-05
Iter: 411 loss: 1.93246706e-05
Iter: 412 loss: 1.93158485e-05
Iter: 413 loss: 1.93004707e-05
Iter: 414 loss: 1.9318697e-05
Iter: 415 loss: 1.92932494e-05
Iter: 416 loss: 1.9276853e-05
Iter: 417 loss: 1.92568332e-05
Iter: 418 loss: 1.92548068e-05
Iter: 419 loss: 1.92242769e-05
Iter: 420 loss: 1.94529202e-05
Iter: 421 loss: 1.92217885e-05
Iter: 422 loss: 1.92004991e-05
Iter: 423 loss: 1.9234596e-05
Iter: 424 loss: 1.91914e-05
Iter: 425 loss: 1.91628387e-05
Iter: 426 loss: 1.92090702e-05
Iter: 427 loss: 1.9151008e-05
Iter: 428 loss: 1.91252948e-05
Iter: 429 loss: 1.92450825e-05
Iter: 430 loss: 1.91212384e-05
Iter: 431 loss: 1.90963074e-05
Iter: 432 loss: 1.90918618e-05
Iter: 433 loss: 1.90753108e-05
Iter: 434 loss: 1.9050125e-05
Iter: 435 loss: 1.93328488e-05
Iter: 436 loss: 1.90485371e-05
Iter: 437 loss: 1.90285828e-05
Iter: 438 loss: 1.89993189e-05
Iter: 439 loss: 1.89978637e-05
Iter: 440 loss: 1.9012843e-05
Iter: 441 loss: 1.89833609e-05
Iter: 442 loss: 1.89705497e-05
Iter: 443 loss: 1.89949897e-05
Iter: 444 loss: 1.89659459e-05
Iter: 445 loss: 1.89576895e-05
Iter: 446 loss: 1.89398306e-05
Iter: 447 loss: 1.92345979e-05
Iter: 448 loss: 1.89400016e-05
Iter: 449 loss: 1.89143811e-05
Iter: 450 loss: 1.89533075e-05
Iter: 451 loss: 1.89031161e-05
Iter: 452 loss: 1.88802696e-05
Iter: 453 loss: 1.894874e-05
Iter: 454 loss: 1.88750255e-05
Iter: 455 loss: 1.88494014e-05
Iter: 456 loss: 1.89061066e-05
Iter: 457 loss: 1.88397917e-05
Iter: 458 loss: 1.88190515e-05
Iter: 459 loss: 1.89246311e-05
Iter: 460 loss: 1.88161193e-05
Iter: 461 loss: 1.87942187e-05
Iter: 462 loss: 1.8781051e-05
Iter: 463 loss: 1.87727928e-05
Iter: 464 loss: 1.87479382e-05
Iter: 465 loss: 1.90444316e-05
Iter: 466 loss: 1.87477744e-05
Iter: 467 loss: 1.8731047e-05
Iter: 468 loss: 1.87109417e-05
Iter: 469 loss: 1.87080732e-05
Iter: 470 loss: 1.86760553e-05
Iter: 471 loss: 1.88997801e-05
Iter: 472 loss: 1.86728867e-05
Iter: 473 loss: 1.86545149e-05
Iter: 474 loss: 1.88183421e-05
Iter: 475 loss: 1.86538491e-05
Iter: 476 loss: 1.8632727e-05
Iter: 477 loss: 1.87462138e-05
Iter: 478 loss: 1.8629451e-05
Iter: 479 loss: 1.86197794e-05
Iter: 480 loss: 1.86081634e-05
Iter: 481 loss: 1.8606228e-05
Iter: 482 loss: 1.8592e-05
Iter: 483 loss: 1.85838271e-05
Iter: 484 loss: 1.85775862e-05
Iter: 485 loss: 1.85516456e-05
Iter: 486 loss: 1.8656734e-05
Iter: 487 loss: 1.85450135e-05
Iter: 488 loss: 1.85300432e-05
Iter: 489 loss: 1.85905774e-05
Iter: 490 loss: 1.8527393e-05
Iter: 491 loss: 1.85088902e-05
Iter: 492 loss: 1.85181743e-05
Iter: 493 loss: 1.8496552e-05
Iter: 494 loss: 1.84816799e-05
Iter: 495 loss: 1.86466277e-05
Iter: 496 loss: 1.84816599e-05
Iter: 497 loss: 1.84690944e-05
Iter: 498 loss: 1.84533492e-05
Iter: 499 loss: 1.84520395e-05
Iter: 500 loss: 1.84343826e-05
Iter: 501 loss: 1.86489378e-05
Iter: 502 loss: 1.84333094e-05
Iter: 503 loss: 1.84195887e-05
Iter: 504 loss: 1.8410683e-05
Iter: 505 loss: 1.84053224e-05
Iter: 506 loss: 1.83895463e-05
Iter: 507 loss: 1.8634044e-05
Iter: 508 loss: 1.8389379e-05
Iter: 509 loss: 1.83855263e-05
Iter: 510 loss: 1.8383329e-05
Iter: 511 loss: 1.83788234e-05
Iter: 512 loss: 1.83664306e-05
Iter: 513 loss: 1.83964e-05
Iter: 514 loss: 1.83596967e-05
Iter: 515 loss: 1.83394804e-05
Iter: 516 loss: 1.84317141e-05
Iter: 517 loss: 1.8335706e-05
Iter: 518 loss: 1.83227094e-05
Iter: 519 loss: 1.83242373e-05
Iter: 520 loss: 1.83128632e-05
Iter: 521 loss: 1.82898111e-05
Iter: 522 loss: 1.83638949e-05
Iter: 523 loss: 1.82832373e-05
Iter: 524 loss: 1.82683543e-05
Iter: 525 loss: 1.83458651e-05
Iter: 526 loss: 1.82656731e-05
Iter: 527 loss: 1.8250872e-05
Iter: 528 loss: 1.82521562e-05
Iter: 529 loss: 1.82385793e-05
Iter: 530 loss: 1.82220483e-05
Iter: 531 loss: 1.8340088e-05
Iter: 532 loss: 1.82208914e-05
Iter: 533 loss: 1.82045605e-05
Iter: 534 loss: 1.81946743e-05
Iter: 535 loss: 1.81879786e-05
Iter: 536 loss: 1.81719297e-05
Iter: 537 loss: 1.8391258e-05
Iter: 538 loss: 1.81716623e-05
Iter: 539 loss: 1.81586674e-05
Iter: 540 loss: 1.81585365e-05
Iter: 541 loss: 1.81479299e-05
Iter: 542 loss: 1.81580308e-05
Iter: 543 loss: 1.81418709e-05
Iter: 544 loss: 1.81367504e-05
Iter: 545 loss: 1.8127881e-05
Iter: 546 loss: 1.83426855e-05
Iter: 547 loss: 1.81279393e-05
Iter: 548 loss: 1.81192154e-05
Iter: 549 loss: 1.81053074e-05
Iter: 550 loss: 1.81064843e-05
Iter: 551 loss: 1.80872903e-05
Iter: 552 loss: 1.82424847e-05
Iter: 553 loss: 1.80861807e-05
Iter: 554 loss: 1.80736606e-05
Iter: 555 loss: 1.80924544e-05
Iter: 556 loss: 1.80678144e-05
Iter: 557 loss: 1.80502902e-05
Iter: 558 loss: 1.80680381e-05
Iter: 559 loss: 1.80402676e-05
Iter: 560 loss: 1.80243696e-05
Iter: 561 loss: 1.81231262e-05
Iter: 562 loss: 1.8022849e-05
Iter: 563 loss: 1.80043717e-05
Iter: 564 loss: 1.79863418e-05
Iter: 565 loss: 1.79827966e-05
Iter: 566 loss: 1.79657945e-05
Iter: 567 loss: 1.82359254e-05
Iter: 568 loss: 1.79655271e-05
Iter: 569 loss: 1.79505751e-05
Iter: 570 loss: 1.79459712e-05
Iter: 571 loss: 1.79370727e-05
Iter: 572 loss: 1.79204217e-05
Iter: 573 loss: 1.80469742e-05
Iter: 574 loss: 1.79188137e-05
Iter: 575 loss: 1.79126182e-05
Iter: 576 loss: 1.79105318e-05
Iter: 577 loss: 1.79008748e-05
Iter: 578 loss: 1.78905648e-05
Iter: 579 loss: 1.78882874e-05
Iter: 580 loss: 1.78798673e-05
Iter: 581 loss: 1.78811315e-05
Iter: 582 loss: 1.78728078e-05
Iter: 583 loss: 1.78600603e-05
Iter: 584 loss: 1.7855622e-05
Iter: 585 loss: 1.78480168e-05
Iter: 586 loss: 1.78337978e-05
Iter: 587 loss: 1.79855288e-05
Iter: 588 loss: 1.78332193e-05
Iter: 589 loss: 1.78207574e-05
Iter: 590 loss: 1.78257724e-05
Iter: 591 loss: 1.78113987e-05
Iter: 592 loss: 1.77970396e-05
Iter: 593 loss: 1.78938353e-05
Iter: 594 loss: 1.77952061e-05
Iter: 595 loss: 1.77835427e-05
Iter: 596 loss: 1.77812144e-05
Iter: 597 loss: 1.77740949e-05
Iter: 598 loss: 1.77541115e-05
Iter: 599 loss: 1.78093214e-05
Iter: 600 loss: 1.77471884e-05
Iter: 601 loss: 1.77325492e-05
Iter: 602 loss: 1.77810798e-05
Iter: 603 loss: 1.77282291e-05
Iter: 604 loss: 1.77078418e-05
Iter: 605 loss: 1.76960402e-05
Iter: 606 loss: 1.76862086e-05
Iter: 607 loss: 1.7680326e-05
Iter: 608 loss: 1.76752383e-05
Iter: 609 loss: 1.76634458e-05
Iter: 610 loss: 1.77168477e-05
Iter: 611 loss: 1.76611793e-05
Iter: 612 loss: 1.76533958e-05
Iter: 613 loss: 1.76298272e-05
Iter: 614 loss: 1.77398178e-05
Iter: 615 loss: 1.76223366e-05
Iter: 616 loss: 1.76008689e-05
Iter: 617 loss: 1.78670198e-05
Iter: 618 loss: 1.76009416e-05
Iter: 619 loss: 1.75852383e-05
Iter: 620 loss: 1.75846671e-05
Iter: 621 loss: 1.75724599e-05
Iter: 622 loss: 1.75577079e-05
Iter: 623 loss: 1.75579044e-05
Iter: 624 loss: 1.75484038e-05
Iter: 625 loss: 1.7543618e-05
Iter: 626 loss: 1.75394398e-05
Iter: 627 loss: 1.75212099e-05
Iter: 628 loss: 1.75561963e-05
Iter: 629 loss: 1.7513179e-05
Iter: 630 loss: 1.75001369e-05
Iter: 631 loss: 1.75615824e-05
Iter: 632 loss: 1.74980305e-05
Iter: 633 loss: 1.74833076e-05
Iter: 634 loss: 1.7480681e-05
Iter: 635 loss: 1.74718807e-05
Iter: 636 loss: 1.74566867e-05
Iter: 637 loss: 1.76233498e-05
Iter: 638 loss: 1.74562265e-05
Iter: 639 loss: 1.74440502e-05
Iter: 640 loss: 1.7456372e-05
Iter: 641 loss: 1.74380439e-05
Iter: 642 loss: 1.74377928e-05
Iter: 643 loss: 1.74305242e-05
Iter: 644 loss: 1.74277884e-05
Iter: 645 loss: 1.74202833e-05
Iter: 646 loss: 1.74768393e-05
Iter: 647 loss: 1.74190445e-05
Iter: 648 loss: 1.74072775e-05
Iter: 649 loss: 1.73959197e-05
Iter: 650 loss: 1.73937078e-05
Iter: 651 loss: 1.73808912e-05
Iter: 652 loss: 1.75660098e-05
Iter: 653 loss: 1.73811823e-05
Iter: 654 loss: 1.73717181e-05
Iter: 655 loss: 1.73557673e-05
Iter: 656 loss: 1.73553963e-05
Iter: 657 loss: 1.73403641e-05
Iter: 658 loss: 1.73403714e-05
Iter: 659 loss: 1.73291646e-05
Iter: 660 loss: 1.73149383e-05
Iter: 661 loss: 1.73136959e-05
Iter: 662 loss: 1.72975597e-05
Iter: 663 loss: 1.75139448e-05
Iter: 664 loss: 1.72974833e-05
Iter: 665 loss: 1.72869331e-05
Iter: 666 loss: 1.72704895e-05
Iter: 667 loss: 1.72707259e-05
Iter: 668 loss: 1.7250688e-05
Iter: 669 loss: 1.74631423e-05
Iter: 670 loss: 1.72501568e-05
Iter: 671 loss: 1.72383661e-05
Iter: 672 loss: 1.72981636e-05
Iter: 673 loss: 1.72368364e-05
Iter: 674 loss: 1.72320833e-05
Iter: 675 loss: 1.7230861e-05
Iter: 676 loss: 1.7223676e-05
Iter: 677 loss: 1.72145665e-05
Iter: 678 loss: 1.72144391e-05
Iter: 679 loss: 1.7207065e-05
Iter: 680 loss: 1.7196382e-05
Iter: 681 loss: 1.71960746e-05
Iter: 682 loss: 1.7179842e-05
Iter: 683 loss: 1.7280674e-05
Iter: 684 loss: 1.71773827e-05
Iter: 685 loss: 1.71659e-05
Iter: 686 loss: 1.71869688e-05
Iter: 687 loss: 1.71612846e-05
Iter: 688 loss: 1.71478896e-05
Iter: 689 loss: 1.71818319e-05
Iter: 690 loss: 1.71432075e-05
Iter: 691 loss: 1.71319261e-05
Iter: 692 loss: 1.72131331e-05
Iter: 693 loss: 1.7130933e-05
Iter: 694 loss: 1.71201282e-05
Iter: 695 loss: 1.71072061e-05
Iter: 696 loss: 1.71064457e-05
Iter: 697 loss: 1.70902185e-05
Iter: 698 loss: 1.7250808e-05
Iter: 699 loss: 1.70898129e-05
Iter: 700 loss: 1.70768108e-05
Iter: 701 loss: 1.70601506e-05
Iter: 702 loss: 1.70584044e-05
Iter: 703 loss: 1.70445455e-05
Iter: 704 loss: 1.70443782e-05
Iter: 705 loss: 1.70353651e-05
Iter: 706 loss: 1.70961212e-05
Iter: 707 loss: 1.70341373e-05
Iter: 708 loss: 1.70211315e-05
Iter: 709 loss: 1.70260064e-05
Iter: 710 loss: 1.70117382e-05
Iter: 711 loss: 1.70039566e-05
Iter: 712 loss: 1.70059466e-05
Iter: 713 loss: 1.69982432e-05
Iter: 714 loss: 1.6989281e-05
Iter: 715 loss: 1.69788127e-05
Iter: 716 loss: 1.69772e-05
Iter: 717 loss: 1.69621289e-05
Iter: 718 loss: 1.70846561e-05
Iter: 719 loss: 1.69608156e-05
Iter: 720 loss: 1.69476698e-05
Iter: 721 loss: 1.6938804e-05
Iter: 722 loss: 1.69337382e-05
Iter: 723 loss: 1.69149789e-05
Iter: 724 loss: 1.71357788e-05
Iter: 725 loss: 1.69148952e-05
Iter: 726 loss: 1.69049199e-05
Iter: 727 loss: 1.69396662e-05
Iter: 728 loss: 1.69020605e-05
Iter: 729 loss: 1.68911356e-05
Iter: 730 loss: 1.68831084e-05
Iter: 731 loss: 1.68789356e-05
Iter: 732 loss: 1.68659353e-05
Iter: 733 loss: 1.69507621e-05
Iter: 734 loss: 1.68637835e-05
Iter: 735 loss: 1.68494662e-05
Iter: 736 loss: 1.68469887e-05
Iter: 737 loss: 1.68366714e-05
Iter: 738 loss: 1.68208735e-05
Iter: 739 loss: 1.70392686e-05
Iter: 740 loss: 1.6820748e-05
Iter: 741 loss: 1.68173319e-05
Iter: 742 loss: 1.68147562e-05
Iter: 743 loss: 1.68112419e-05
Iter: 744 loss: 1.67979088e-05
Iter: 745 loss: 1.68188799e-05
Iter: 746 loss: 1.67889721e-05
Iter: 747 loss: 1.67764956e-05
Iter: 748 loss: 1.6937669e-05
Iter: 749 loss: 1.67769194e-05
Iter: 750 loss: 1.67659673e-05
Iter: 751 loss: 1.67498492e-05
Iter: 752 loss: 1.67497128e-05
Iter: 753 loss: 1.67387716e-05
Iter: 754 loss: 1.67380713e-05
Iter: 755 loss: 1.67305243e-05
Iter: 756 loss: 1.67205853e-05
Iter: 757 loss: 1.67196285e-05
Iter: 758 loss: 1.6706641e-05
Iter: 759 loss: 1.68588922e-05
Iter: 760 loss: 1.67066592e-05
Iter: 761 loss: 1.66955397e-05
Iter: 762 loss: 1.67143244e-05
Iter: 763 loss: 1.66909103e-05
Iter: 764 loss: 1.66789177e-05
Iter: 765 loss: 1.66956161e-05
Iter: 766 loss: 1.66721911e-05
Iter: 767 loss: 1.66611298e-05
Iter: 768 loss: 1.67020553e-05
Iter: 769 loss: 1.66579248e-05
Iter: 770 loss: 1.66445461e-05
Iter: 771 loss: 1.66657501e-05
Iter: 772 loss: 1.66383616e-05
Iter: 773 loss: 1.6642538e-05
Iter: 774 loss: 1.66337231e-05
Iter: 775 loss: 1.66296632e-05
Iter: 776 loss: 1.66239806e-05
Iter: 777 loss: 1.66236987e-05
Iter: 778 loss: 1.66165937e-05
Iter: 779 loss: 1.65992678e-05
Iter: 780 loss: 1.67780599e-05
Iter: 781 loss: 1.65970359e-05
Iter: 782 loss: 1.65821875e-05
Iter: 783 loss: 1.68100414e-05
Iter: 784 loss: 1.65827332e-05
Iter: 785 loss: 1.65703877e-05
Iter: 786 loss: 1.6563783e-05
Iter: 787 loss: 1.65587444e-05
Iter: 788 loss: 1.65432466e-05
Iter: 789 loss: 1.66878726e-05
Iter: 790 loss: 1.65426882e-05
Iter: 791 loss: 1.65295151e-05
Iter: 792 loss: 1.65376823e-05
Iter: 793 loss: 1.6521848e-05
Iter: 794 loss: 1.65035181e-05
Iter: 795 loss: 1.66157679e-05
Iter: 796 loss: 1.65012898e-05
Iter: 797 loss: 1.64914672e-05
Iter: 798 loss: 1.64825778e-05
Iter: 799 loss: 1.64800913e-05
Iter: 800 loss: 1.64614903e-05
Iter: 801 loss: 1.65156089e-05
Iter: 802 loss: 1.64552603e-05
Iter: 803 loss: 1.64395151e-05
Iter: 804 loss: 1.64999328e-05
Iter: 805 loss: 1.64360208e-05
Iter: 806 loss: 1.642e-05
Iter: 807 loss: 1.64432877e-05
Iter: 808 loss: 1.6412585e-05
Iter: 809 loss: 1.64195862e-05
Iter: 810 loss: 1.64061785e-05
Iter: 811 loss: 1.64025259e-05
Iter: 812 loss: 1.63928071e-05
Iter: 813 loss: 1.64261583e-05
Iter: 814 loss: 1.63887053e-05
Iter: 815 loss: 1.63724617e-05
Iter: 816 loss: 1.6375694e-05
Iter: 817 loss: 1.63618352e-05
Iter: 818 loss: 1.63463028e-05
Iter: 819 loss: 1.64231933e-05
Iter: 820 loss: 1.63436489e-05
Iter: 821 loss: 1.63256445e-05
Iter: 822 loss: 1.635433e-05
Iter: 823 loss: 1.63172463e-05
Iter: 824 loss: 1.63056175e-05
Iter: 825 loss: 1.63742807e-05
Iter: 826 loss: 1.63044551e-05
Iter: 827 loss: 1.62907381e-05
Iter: 828 loss: 1.62906344e-05
Iter: 829 loss: 1.62802771e-05
Iter: 830 loss: 1.6267164e-05
Iter: 831 loss: 1.64654703e-05
Iter: 832 loss: 1.62674e-05
Iter: 833 loss: 1.6258793e-05
Iter: 834 loss: 1.62545e-05
Iter: 835 loss: 1.62511187e-05
Iter: 836 loss: 1.6236625e-05
Iter: 837 loss: 1.62938377e-05
Iter: 838 loss: 1.62336801e-05
Iter: 839 loss: 1.62237593e-05
Iter: 840 loss: 1.62165397e-05
Iter: 841 loss: 1.62123197e-05
Iter: 842 loss: 1.61984826e-05
Iter: 843 loss: 1.61988e-05
Iter: 844 loss: 1.61975222e-05
Iter: 845 loss: 1.61948228e-05
Iter: 846 loss: 1.61924181e-05
Iter: 847 loss: 1.61864082e-05
Iter: 848 loss: 1.61753014e-05
Iter: 849 loss: 1.61745702e-05
Iter: 850 loss: 1.61576318e-05
Iter: 851 loss: 1.6308104e-05
Iter: 852 loss: 1.6156886e-05
Iter: 853 loss: 1.61457465e-05
Iter: 854 loss: 1.61503249e-05
Iter: 855 loss: 1.61383832e-05
Iter: 856 loss: 1.61227072e-05
Iter: 857 loss: 1.62055585e-05
Iter: 858 loss: 1.61199805e-05
Iter: 859 loss: 1.61083972e-05
Iter: 860 loss: 1.61239677e-05
Iter: 861 loss: 1.61034641e-05
Iter: 862 loss: 1.60864874e-05
Iter: 863 loss: 1.61358494e-05
Iter: 864 loss: 1.60814197e-05
Iter: 865 loss: 1.60713225e-05
Iter: 866 loss: 1.61259832e-05
Iter: 867 loss: 1.60694835e-05
Iter: 868 loss: 1.60566033e-05
Iter: 869 loss: 1.60711552e-05
Iter: 870 loss: 1.60497457e-05
Iter: 871 loss: 1.60394156e-05
Iter: 872 loss: 1.60888812e-05
Iter: 873 loss: 1.60375184e-05
Iter: 874 loss: 1.6026348e-05
Iter: 875 loss: 1.60169329e-05
Iter: 876 loss: 1.60133932e-05
Iter: 877 loss: 1.60060736e-05
Iter: 878 loss: 1.60044e-05
Iter: 879 loss: 1.5994734e-05
Iter: 880 loss: 1.60374912e-05
Iter: 881 loss: 1.59932752e-05
Iter: 882 loss: 1.59871033e-05
Iter: 883 loss: 1.59722804e-05
Iter: 884 loss: 1.60755189e-05
Iter: 885 loss: 1.59685987e-05
Iter: 886 loss: 1.59549436e-05
Iter: 887 loss: 1.60074651e-05
Iter: 888 loss: 1.59523734e-05
Iter: 889 loss: 1.59362935e-05
Iter: 890 loss: 1.59733863e-05
Iter: 891 loss: 1.59306219e-05
Iter: 892 loss: 1.59166848e-05
Iter: 893 loss: 1.59085757e-05
Iter: 894 loss: 1.59025712e-05
Iter: 895 loss: 1.58866369e-05
Iter: 896 loss: 1.58864113e-05
Iter: 897 loss: 1.58757557e-05
Iter: 898 loss: 1.58694675e-05
Iter: 899 loss: 1.58642742e-05
Iter: 900 loss: 1.58508228e-05
Iter: 901 loss: 1.59760439e-05
Iter: 902 loss: 1.58500479e-05
Iter: 903 loss: 1.58391085e-05
Iter: 904 loss: 1.58292714e-05
Iter: 905 loss: 1.5826512e-05
Iter: 906 loss: 1.58132025e-05
Iter: 907 loss: 1.58133662e-05
Iter: 908 loss: 1.58069779e-05
Iter: 909 loss: 1.59007213e-05
Iter: 910 loss: 1.58074345e-05
Iter: 911 loss: 1.58016355e-05
Iter: 912 loss: 1.58155744e-05
Iter: 913 loss: 1.58003859e-05
Iter: 914 loss: 1.5793521e-05
Iter: 915 loss: 1.57833147e-05
Iter: 916 loss: 1.5783e-05
Iter: 917 loss: 1.57740942e-05
Iter: 918 loss: 1.57722388e-05
Iter: 919 loss: 1.57666109e-05
Iter: 920 loss: 1.57494087e-05
Iter: 921 loss: 1.58003586e-05
Iter: 922 loss: 1.57450268e-05
Iter: 923 loss: 1.57354607e-05
Iter: 924 loss: 1.57575214e-05
Iter: 925 loss: 1.57327304e-05
Iter: 926 loss: 1.57187205e-05
Iter: 927 loss: 1.57219802e-05
Iter: 928 loss: 1.57096511e-05
Iter: 929 loss: 1.56966271e-05
Iter: 930 loss: 1.57605846e-05
Iter: 931 loss: 1.56941078e-05
Iter: 932 loss: 1.56810929e-05
Iter: 933 loss: 1.57285904e-05
Iter: 934 loss: 1.5677444e-05
Iter: 935 loss: 1.56691858e-05
Iter: 936 loss: 1.5665184e-05
Iter: 937 loss: 1.56614533e-05
Iter: 938 loss: 1.56454698e-05
Iter: 939 loss: 1.57414506e-05
Iter: 940 loss: 1.56438673e-05
Iter: 941 loss: 1.56357892e-05
Iter: 942 loss: 1.57373725e-05
Iter: 943 loss: 1.56359565e-05
Iter: 944 loss: 1.56276292e-05
Iter: 945 loss: 1.56621645e-05
Iter: 946 loss: 1.5625963e-05
Iter: 947 loss: 1.56197275e-05
Iter: 948 loss: 1.56171682e-05
Iter: 949 loss: 1.56138904e-05
Iter: 950 loss: 1.56075203e-05
Iter: 951 loss: 1.5601152e-05
Iter: 952 loss: 1.55995804e-05
Iter: 953 loss: 1.5586782e-05
Iter: 954 loss: 1.56388196e-05
Iter: 955 loss: 1.55842426e-05
Iter: 956 loss: 1.55746256e-05
Iter: 957 loss: 1.55754969e-05
Iter: 958 loss: 1.55674279e-05
Iter: 959 loss: 1.55527341e-05
Iter: 960 loss: 1.56500864e-05
Iter: 961 loss: 1.55516427e-05
Iter: 962 loss: 1.55423058e-05
Iter: 963 loss: 1.55433536e-05
Iter: 964 loss: 1.55351736e-05
Iter: 965 loss: 1.55207035e-05
Iter: 966 loss: 1.55952421e-05
Iter: 967 loss: 1.55180969e-05
Iter: 968 loss: 1.55092257e-05
Iter: 969 loss: 1.5507143e-05
Iter: 970 loss: 1.5500711e-05
Iter: 971 loss: 1.54858099e-05
Iter: 972 loss: 1.56011556e-05
Iter: 973 loss: 1.54848931e-05
Iter: 974 loss: 1.54762638e-05
Iter: 975 loss: 1.55062771e-05
Iter: 976 loss: 1.54737063e-05
Iter: 977 loss: 1.54682475e-05
Iter: 978 loss: 1.54678783e-05
Iter: 979 loss: 1.5463771e-05
Iter: 980 loss: 1.54563368e-05
Iter: 981 loss: 1.56171718e-05
Iter: 982 loss: 1.54562495e-05
Iter: 983 loss: 1.54495465e-05
Iter: 984 loss: 1.54541922e-05
Iter: 985 loss: 1.54451172e-05
Iter: 986 loss: 1.54359932e-05
Iter: 987 loss: 1.54512691e-05
Iter: 988 loss: 1.54313038e-05
Iter: 989 loss: 1.54223635e-05
Iter: 990 loss: 1.54328227e-05
Iter: 991 loss: 1.54171212e-05
Iter: 992 loss: 1.54037152e-05
Iter: 993 loss: 1.54391237e-05
Iter: 994 loss: 1.54001373e-05
Iter: 995 loss: 1.53897745e-05
Iter: 996 loss: 1.54129266e-05
Iter: 997 loss: 1.53855908e-05
Iter: 998 loss: 1.53723104e-05
Iter: 999 loss: 1.54143163e-05
Iter: 1000 loss: 1.53687288e-05
Iter: 1001 loss: 1.53573947e-05
Iter: 1002 loss: 1.53562123e-05
Iter: 1003 loss: 1.5349051e-05
Iter: 1004 loss: 1.53335368e-05
Iter: 1005 loss: 1.54686659e-05
Iter: 1006 loss: 1.53323381e-05
Iter: 1007 loss: 1.5322712e-05
Iter: 1008 loss: 1.53351793e-05
Iter: 1009 loss: 1.53184737e-05
Iter: 1010 loss: 1.53154724e-05
Iter: 1011 loss: 1.53117908e-05
Iter: 1012 loss: 1.53076253e-05
Iter: 1013 loss: 1.5300513e-05
Iter: 1014 loss: 1.53001201e-05
Iter: 1015 loss: 1.52932807e-05
Iter: 1016 loss: 1.52994744e-05
Iter: 1017 loss: 1.52899884e-05
Iter: 1018 loss: 1.52814773e-05
Iter: 1019 loss: 1.5289017e-05
Iter: 1020 loss: 1.52762295e-05
Iter: 1021 loss: 1.5265603e-05
Iter: 1022 loss: 1.52834073e-05
Iter: 1023 loss: 1.52611501e-05
Iter: 1024 loss: 1.52484718e-05
Iter: 1025 loss: 1.52866614e-05
Iter: 1026 loss: 1.524457e-05
Iter: 1027 loss: 1.52351167e-05
Iter: 1028 loss: 1.52414723e-05
Iter: 1029 loss: 1.52299563e-05
Iter: 1030 loss: 1.52165403e-05
Iter: 1031 loss: 1.52955708e-05
Iter: 1032 loss: 1.52147531e-05
Iter: 1033 loss: 1.5206384e-05
Iter: 1034 loss: 1.52123976e-05
Iter: 1035 loss: 1.52012099e-05
Iter: 1036 loss: 1.51882177e-05
Iter: 1037 loss: 1.52220637e-05
Iter: 1038 loss: 1.51840832e-05
Iter: 1039 loss: 1.51753466e-05
Iter: 1040 loss: 1.52176053e-05
Iter: 1041 loss: 1.51742315e-05
Iter: 1042 loss: 1.5168619e-05
Iter: 1043 loss: 1.51686481e-05
Iter: 1044 loss: 1.51612885e-05
Iter: 1045 loss: 1.51561308e-05
Iter: 1046 loss: 1.5153576e-05
Iter: 1047 loss: 1.51480399e-05
Iter: 1048 loss: 1.51448457e-05
Iter: 1049 loss: 1.51433942e-05
Iter: 1050 loss: 1.51338736e-05
Iter: 1051 loss: 1.51329123e-05
Iter: 1052 loss: 1.51252607e-05
Iter: 1053 loss: 1.51144386e-05
Iter: 1054 loss: 1.51939803e-05
Iter: 1055 loss: 1.51130425e-05
Iter: 1056 loss: 1.5103612e-05
Iter: 1057 loss: 1.51119311e-05
Iter: 1058 loss: 1.50982623e-05
Iter: 1059 loss: 1.50889937e-05
Iter: 1060 loss: 1.51295762e-05
Iter: 1061 loss: 1.50871983e-05
Iter: 1062 loss: 1.50769392e-05
Iter: 1063 loss: 1.5076118e-05
Iter: 1064 loss: 1.50675969e-05
Iter: 1065 loss: 1.50562109e-05
Iter: 1066 loss: 1.51291042e-05
Iter: 1067 loss: 1.50546412e-05
Iter: 1068 loss: 1.50426531e-05
Iter: 1069 loss: 1.504998e-05
Iter: 1070 loss: 1.50345149e-05
Iter: 1071 loss: 1.50252763e-05
Iter: 1072 loss: 1.51003896e-05
Iter: 1073 loss: 1.50240849e-05
Iter: 1074 loss: 1.50150154e-05
Iter: 1075 loss: 1.50519245e-05
Iter: 1076 loss: 1.50133601e-05
Iter: 1077 loss: 1.50014439e-05
Iter: 1078 loss: 1.50782671e-05
Iter: 1079 loss: 1.50003243e-05
Iter: 1080 loss: 1.49980333e-05
Iter: 1081 loss: 1.49890111e-05
Iter: 1082 loss: 1.50484684e-05
Iter: 1083 loss: 1.49870575e-05
Iter: 1084 loss: 1.4974672e-05
Iter: 1085 loss: 1.50096903e-05
Iter: 1086 loss: 1.49706775e-05
Iter: 1087 loss: 1.49623329e-05
Iter: 1088 loss: 1.50165824e-05
Iter: 1089 loss: 1.49616299e-05
Iter: 1090 loss: 1.49540992e-05
Iter: 1091 loss: 1.4953157e-05
Iter: 1092 loss: 1.49478319e-05
Iter: 1093 loss: 1.49391417e-05
Iter: 1094 loss: 1.50057513e-05
Iter: 1095 loss: 1.49385633e-05
Iter: 1096 loss: 1.49310799e-05
Iter: 1097 loss: 1.49282932e-05
Iter: 1098 loss: 1.49244343e-05
Iter: 1099 loss: 1.49150301e-05
Iter: 1100 loss: 1.49843554e-05
Iter: 1101 loss: 1.49141379e-05
Iter: 1102 loss: 1.49048174e-05
Iter: 1103 loss: 1.49020552e-05
Iter: 1104 loss: 1.48953386e-05
Iter: 1105 loss: 1.48868303e-05
Iter: 1106 loss: 1.49867137e-05
Iter: 1107 loss: 1.48864747e-05
Iter: 1108 loss: 1.48781728e-05
Iter: 1109 loss: 1.48925228e-05
Iter: 1110 loss: 1.48742301e-05
Iter: 1111 loss: 1.4866695e-05
Iter: 1112 loss: 1.48660474e-05
Iter: 1113 loss: 1.48639501e-05
Iter: 1114 loss: 1.48573299e-05
Iter: 1115 loss: 1.49095449e-05
Iter: 1116 loss: 1.48561539e-05
Iter: 1117 loss: 1.48475419e-05
Iter: 1118 loss: 1.48525805e-05
Iter: 1119 loss: 1.48423087e-05
Iter: 1120 loss: 1.48344861e-05
Iter: 1121 loss: 1.49113675e-05
Iter: 1122 loss: 1.48342042e-05
Iter: 1123 loss: 1.48254621e-05
Iter: 1124 loss: 1.4813364e-05
Iter: 1125 loss: 1.48132576e-05
Iter: 1126 loss: 1.48026375e-05
Iter: 1127 loss: 1.4950464e-05
Iter: 1128 loss: 1.48024556e-05
Iter: 1129 loss: 1.47945384e-05
Iter: 1130 loss: 1.47861483e-05
Iter: 1131 loss: 1.47841656e-05
Iter: 1132 loss: 1.47726605e-05
Iter: 1133 loss: 1.48677391e-05
Iter: 1134 loss: 1.47718529e-05
Iter: 1135 loss: 1.47615083e-05
Iter: 1136 loss: 1.47765459e-05
Iter: 1137 loss: 1.4756728e-05
Iter: 1138 loss: 1.47480641e-05
Iter: 1139 loss: 1.48072168e-05
Iter: 1140 loss: 1.47471528e-05
Iter: 1141 loss: 1.47390037e-05
Iter: 1142 loss: 1.47398951e-05
Iter: 1143 loss: 1.47324736e-05
Iter: 1144 loss: 1.47356386e-05
Iter: 1145 loss: 1.47274222e-05
Iter: 1146 loss: 1.47250921e-05
Iter: 1147 loss: 1.47192841e-05
Iter: 1148 loss: 1.4777258e-05
Iter: 1149 loss: 1.47187739e-05
Iter: 1150 loss: 1.47135033e-05
Iter: 1151 loss: 1.47036199e-05
Iter: 1152 loss: 1.47032551e-05
Iter: 1153 loss: 1.46937837e-05
Iter: 1154 loss: 1.47749797e-05
Iter: 1155 loss: 1.46933271e-05
Iter: 1156 loss: 1.46830234e-05
Iter: 1157 loss: 1.46855209e-05
Iter: 1158 loss: 1.46758339e-05
Iter: 1159 loss: 1.46653074e-05
Iter: 1160 loss: 1.47809014e-05
Iter: 1161 loss: 1.46649945e-05
Iter: 1162 loss: 1.46590155e-05
Iter: 1163 loss: 1.46498933e-05
Iter: 1164 loss: 1.46486655e-05
Iter: 1165 loss: 1.46397215e-05
Iter: 1166 loss: 1.47812189e-05
Iter: 1167 loss: 1.46399434e-05
Iter: 1168 loss: 1.46313614e-05
Iter: 1169 loss: 1.46222037e-05
Iter: 1170 loss: 1.46206758e-05
Iter: 1171 loss: 1.46076427e-05
Iter: 1172 loss: 1.47144565e-05
Iter: 1173 loss: 1.46070233e-05
Iter: 1174 loss: 1.45982021e-05
Iter: 1175 loss: 1.46528973e-05
Iter: 1176 loss: 1.45973008e-05
Iter: 1177 loss: 1.45972926e-05
Iter: 1178 loss: 1.45933072e-05
Iter: 1179 loss: 1.45898166e-05
Iter: 1180 loss: 1.45836184e-05
Iter: 1181 loss: 1.47274832e-05
Iter: 1182 loss: 1.45833064e-05
Iter: 1183 loss: 1.45778195e-05
Iter: 1184 loss: 1.45682789e-05
Iter: 1185 loss: 1.45684189e-05
Iter: 1186 loss: 1.45597587e-05
Iter: 1187 loss: 1.46724651e-05
Iter: 1188 loss: 1.45589702e-05
Iter: 1189 loss: 1.45535942e-05
Iter: 1190 loss: 1.45498398e-05
Iter: 1191 loss: 1.45475196e-05
Iter: 1192 loss: 1.45387367e-05
Iter: 1193 loss: 1.46341554e-05
Iter: 1194 loss: 1.45385566e-05
Iter: 1195 loss: 1.4533076e-05
Iter: 1196 loss: 1.45257745e-05
Iter: 1197 loss: 1.45254071e-05
Iter: 1198 loss: 1.45161785e-05
Iter: 1199 loss: 1.46261882e-05
Iter: 1200 loss: 1.45166196e-05
Iter: 1201 loss: 1.4510546e-05
Iter: 1202 loss: 1.45022377e-05
Iter: 1203 loss: 1.45021331e-05
Iter: 1204 loss: 1.44916494e-05
Iter: 1205 loss: 1.45320373e-05
Iter: 1206 loss: 1.4489312e-05
Iter: 1207 loss: 1.44826354e-05
Iter: 1208 loss: 1.44872465e-05
Iter: 1209 loss: 1.44780279e-05
Iter: 1210 loss: 1.44683681e-05
Iter: 1211 loss: 1.44612732e-05
Iter: 1212 loss: 1.44576479e-05
Iter: 1213 loss: 1.44580663e-05
Iter: 1214 loss: 1.44539608e-05
Iter: 1215 loss: 1.44505048e-05
Iter: 1216 loss: 1.448681e-05
Iter: 1217 loss: 1.44498408e-05
Iter: 1218 loss: 1.44480691e-05
Iter: 1219 loss: 1.44413625e-05
Iter: 1220 loss: 1.44535388e-05
Iter: 1221 loss: 1.44368551e-05
Iter: 1222 loss: 1.44303485e-05
Iter: 1223 loss: 1.45252125e-05
Iter: 1224 loss: 1.44301803e-05
Iter: 1225 loss: 1.44263176e-05
Iter: 1226 loss: 1.4421259e-05
Iter: 1227 loss: 1.44205178e-05
Iter: 1228 loss: 1.44120377e-05
Iter: 1229 loss: 1.44701216e-05
Iter: 1230 loss: 1.44105716e-05
Iter: 1231 loss: 1.44045243e-05
Iter: 1232 loss: 1.44298992e-05
Iter: 1233 loss: 1.44029545e-05
Iter: 1234 loss: 1.43969419e-05
Iter: 1235 loss: 1.43990646e-05
Iter: 1236 loss: 1.43927136e-05
Iter: 1237 loss: 1.43852121e-05
Iter: 1238 loss: 1.44170035e-05
Iter: 1239 loss: 1.4383495e-05
Iter: 1240 loss: 1.43775442e-05
Iter: 1241 loss: 1.43780726e-05
Iter: 1242 loss: 1.43724437e-05
Iter: 1243 loss: 1.4365125e-05
Iter: 1244 loss: 1.44258538e-05
Iter: 1245 loss: 1.43638699e-05
Iter: 1246 loss: 1.43585e-05
Iter: 1247 loss: 1.43673442e-05
Iter: 1248 loss: 1.43561338e-05
Iter: 1249 loss: 1.43535763e-05
Iter: 1250 loss: 1.43525222e-05
Iter: 1251 loss: 1.43488223e-05
Iter: 1252 loss: 1.43426105e-05
Iter: 1253 loss: 1.43425495e-05
Iter: 1254 loss: 1.43390198e-05
Iter: 1255 loss: 1.43367415e-05
Iter: 1256 loss: 1.43352281e-05
Iter: 1257 loss: 1.43265152e-05
Iter: 1258 loss: 1.43244215e-05
Iter: 1259 loss: 1.43197576e-05
Iter: 1260 loss: 1.43095276e-05
Iter: 1261 loss: 1.43674197e-05
Iter: 1262 loss: 1.43081061e-05
Iter: 1263 loss: 1.42996569e-05
Iter: 1264 loss: 1.43259931e-05
Iter: 1265 loss: 1.42975687e-05
Iter: 1266 loss: 1.42905474e-05
Iter: 1267 loss: 1.43235411e-05
Iter: 1268 loss: 1.4289154e-05
Iter: 1269 loss: 1.42826811e-05
Iter: 1270 loss: 1.42949784e-05
Iter: 1271 loss: 1.42798954e-05
Iter: 1272 loss: 1.42720692e-05
Iter: 1273 loss: 1.42764884e-05
Iter: 1274 loss: 1.42669041e-05
Iter: 1275 loss: 1.42590588e-05
Iter: 1276 loss: 1.43001716e-05
Iter: 1277 loss: 1.42573499e-05
Iter: 1278 loss: 1.42496128e-05
Iter: 1279 loss: 1.42479512e-05
Iter: 1280 loss: 1.42434565e-05
Iter: 1281 loss: 1.42366507e-05
Iter: 1282 loss: 1.42365716e-05
Iter: 1283 loss: 1.42302233e-05
Iter: 1284 loss: 1.4293707e-05
Iter: 1285 loss: 1.42301524e-05
Iter: 1286 loss: 1.42285116e-05
Iter: 1287 loss: 1.42233957e-05
Iter: 1288 loss: 1.42477156e-05
Iter: 1289 loss: 1.42215358e-05
Iter: 1290 loss: 1.42138624e-05
Iter: 1291 loss: 1.42399922e-05
Iter: 1292 loss: 1.42119497e-05
Iter: 1293 loss: 1.42057979e-05
Iter: 1294 loss: 1.42057534e-05
Iter: 1295 loss: 1.42017561e-05
Iter: 1296 loss: 1.4191658e-05
Iter: 1297 loss: 1.42434365e-05
Iter: 1298 loss: 1.41892133e-05
Iter: 1299 loss: 1.41819901e-05
Iter: 1300 loss: 1.42057079e-05
Iter: 1301 loss: 1.41799028e-05
Iter: 1302 loss: 1.41729442e-05
Iter: 1303 loss: 1.41927212e-05
Iter: 1304 loss: 1.41699338e-05
Iter: 1305 loss: 1.41634646e-05
Iter: 1306 loss: 1.41932105e-05
Iter: 1307 loss: 1.41618466e-05
Iter: 1308 loss: 1.41576256e-05
Iter: 1309 loss: 1.41651817e-05
Iter: 1310 loss: 1.41554401e-05
Iter: 1311 loss: 1.41494238e-05
Iter: 1312 loss: 1.41531418e-05
Iter: 1313 loss: 1.41449837e-05
Iter: 1314 loss: 1.41385872e-05
Iter: 1315 loss: 1.41773189e-05
Iter: 1316 loss: 1.41378005e-05
Iter: 1317 loss: 1.41387072e-05
Iter: 1318 loss: 1.41349628e-05
Iter: 1319 loss: 1.41336268e-05
Iter: 1320 loss: 1.41282744e-05
Iter: 1321 loss: 1.41350329e-05
Iter: 1322 loss: 1.41242635e-05
Iter: 1323 loss: 1.41153087e-05
Iter: 1324 loss: 1.41305836e-05
Iter: 1325 loss: 1.41114006e-05
Iter: 1326 loss: 1.41033597e-05
Iter: 1327 loss: 1.41306764e-05
Iter: 1328 loss: 1.4100744e-05
Iter: 1329 loss: 1.40933571e-05
Iter: 1330 loss: 1.4133504e-05
Iter: 1331 loss: 1.40916327e-05
Iter: 1332 loss: 1.40870688e-05
Iter: 1333 loss: 1.40866805e-05
Iter: 1334 loss: 1.40821458e-05
Iter: 1335 loss: 1.40736411e-05
Iter: 1336 loss: 1.41112369e-05
Iter: 1337 loss: 1.40713046e-05
Iter: 1338 loss: 1.40640386e-05
Iter: 1339 loss: 1.40986431e-05
Iter: 1340 loss: 1.40619868e-05
Iter: 1341 loss: 1.40556431e-05
Iter: 1342 loss: 1.40613856e-05
Iter: 1343 loss: 1.40520451e-05
Iter: 1344 loss: 1.40434222e-05
Iter: 1345 loss: 1.40645e-05
Iter: 1346 loss: 1.404047e-05
Iter: 1347 loss: 1.40336033e-05
Iter: 1348 loss: 1.40406491e-05
Iter: 1349 loss: 1.40295433e-05
Iter: 1350 loss: 1.40294487e-05
Iter: 1351 loss: 1.40257926e-05
Iter: 1352 loss: 1.40220509e-05
Iter: 1353 loss: 1.40151496e-05
Iter: 1354 loss: 1.40149523e-05
Iter: 1355 loss: 1.40096909e-05
Iter: 1356 loss: 1.40094626e-05
Iter: 1357 loss: 1.40060856e-05
Iter: 1358 loss: 1.40001339e-05
Iter: 1359 loss: 1.4016844e-05
Iter: 1360 loss: 1.39982585e-05
Iter: 1361 loss: 1.39913227e-05
Iter: 1362 loss: 1.39979393e-05
Iter: 1363 loss: 1.3988054e-05
Iter: 1364 loss: 1.39806671e-05
Iter: 1365 loss: 1.39920603e-05
Iter: 1366 loss: 1.39775366e-05
Iter: 1367 loss: 1.3969081e-05
Iter: 1368 loss: 1.40096436e-05
Iter: 1369 loss: 1.39670747e-05
Iter: 1370 loss: 1.39610938e-05
Iter: 1371 loss: 1.3970146e-05
Iter: 1372 loss: 1.39584618e-05
Iter: 1373 loss: 1.3951073e-05
Iter: 1374 loss: 1.39736085e-05
Iter: 1375 loss: 1.39485028e-05
Iter: 1376 loss: 1.39407493e-05
Iter: 1377 loss: 1.39861968e-05
Iter: 1378 loss: 1.39400845e-05
Iter: 1379 loss: 1.39359508e-05
Iter: 1380 loss: 1.39362473e-05
Iter: 1381 loss: 1.39320309e-05
Iter: 1382 loss: 1.39246449e-05
Iter: 1383 loss: 1.39429849e-05
Iter: 1384 loss: 1.3922202e-05
Iter: 1385 loss: 1.39170106e-05
Iter: 1386 loss: 1.39169551e-05
Iter: 1387 loss: 1.3912223e-05
Iter: 1388 loss: 1.39414042e-05
Iter: 1389 loss: 1.39111489e-05
Iter: 1390 loss: 1.39091226e-05
Iter: 1391 loss: 1.39036156e-05
Iter: 1392 loss: 1.39341901e-05
Iter: 1393 loss: 1.39018584e-05
Iter: 1394 loss: 1.38930154e-05
Iter: 1395 loss: 1.39075892e-05
Iter: 1396 loss: 1.38890173e-05
Iter: 1397 loss: 1.38808737e-05
Iter: 1398 loss: 1.39352542e-05
Iter: 1399 loss: 1.38797232e-05
Iter: 1400 loss: 1.38717169e-05
Iter: 1401 loss: 1.38707819e-05
Iter: 1402 loss: 1.38658124e-05
Iter: 1403 loss: 1.38577798e-05
Iter: 1404 loss: 1.39231852e-05
Iter: 1405 loss: 1.38569994e-05
Iter: 1406 loss: 1.38493906e-05
Iter: 1407 loss: 1.38520154e-05
Iter: 1408 loss: 1.38442647e-05
Iter: 1409 loss: 1.38357045e-05
Iter: 1410 loss: 1.38921314e-05
Iter: 1411 loss: 1.38345722e-05
Iter: 1412 loss: 1.38269879e-05
Iter: 1413 loss: 1.38494815e-05
Iter: 1414 loss: 1.38248852e-05
Iter: 1415 loss: 1.38165351e-05
Iter: 1416 loss: 1.38279174e-05
Iter: 1417 loss: 1.38122177e-05
Iter: 1418 loss: 1.38061396e-05
Iter: 1419 loss: 1.38596824e-05
Iter: 1420 loss: 1.38058094e-05
Iter: 1421 loss: 1.38058922e-05
Iter: 1422 loss: 1.38037012e-05
Iter: 1423 loss: 1.38022624e-05
Iter: 1424 loss: 1.37966945e-05
Iter: 1425 loss: 1.38181513e-05
Iter: 1426 loss: 1.37948446e-05
Iter: 1427 loss: 1.37887491e-05
Iter: 1428 loss: 1.3796629e-05
Iter: 1429 loss: 1.37860243e-05
Iter: 1430 loss: 1.37802508e-05
Iter: 1431 loss: 1.37937968e-05
Iter: 1432 loss: 1.37779389e-05
Iter: 1433 loss: 1.37713077e-05
Iter: 1434 loss: 1.37791203e-05
Iter: 1435 loss: 1.3767718e-05
Iter: 1436 loss: 1.3760864e-05
Iter: 1437 loss: 1.37874631e-05
Iter: 1438 loss: 1.37592e-05
Iter: 1439 loss: 1.3752121e-05
Iter: 1440 loss: 1.37739416e-05
Iter: 1441 loss: 1.37493043e-05
Iter: 1442 loss: 1.37432271e-05
Iter: 1443 loss: 1.37652678e-05
Iter: 1444 loss: 1.37421994e-05
Iter: 1445 loss: 1.37362113e-05
Iter: 1446 loss: 1.37472307e-05
Iter: 1447 loss: 1.37331299e-05
Iter: 1448 loss: 1.37265215e-05
Iter: 1449 loss: 1.37524239e-05
Iter: 1450 loss: 1.3724436e-05
Iter: 1451 loss: 1.37193183e-05
Iter: 1452 loss: 1.37330408e-05
Iter: 1453 loss: 1.37174811e-05
Iter: 1454 loss: 1.37155384e-05
Iter: 1455 loss: 1.37142561e-05
Iter: 1456 loss: 1.37113693e-05
Iter: 1457 loss: 1.37054367e-05
Iter: 1458 loss: 1.37056968e-05
Iter: 1459 loss: 1.37019533e-05
Iter: 1460 loss: 1.36999552e-05
Iter: 1461 loss: 1.36979088e-05
Iter: 1462 loss: 1.36916433e-05
Iter: 1463 loss: 1.36989875e-05
Iter: 1464 loss: 1.36883355e-05
Iter: 1465 loss: 1.36805602e-05
Iter: 1466 loss: 1.36987646e-05
Iter: 1467 loss: 1.36776034e-05
Iter: 1468 loss: 1.36702538e-05
Iter: 1469 loss: 1.37045154e-05
Iter: 1470 loss: 1.3669599e-05
Iter: 1471 loss: 1.36624203e-05
Iter: 1472 loss: 1.36723047e-05
Iter: 1473 loss: 1.36589852e-05
Iter: 1474 loss: 1.36534436e-05
Iter: 1475 loss: 1.36765357e-05
Iter: 1476 loss: 1.36526169e-05
Iter: 1477 loss: 1.3646526e-05
Iter: 1478 loss: 1.36507306e-05
Iter: 1479 loss: 1.36431427e-05
Iter: 1480 loss: 1.36359558e-05
Iter: 1481 loss: 1.36810713e-05
Iter: 1482 loss: 1.3634899e-05
Iter: 1483 loss: 1.36294557e-05
Iter: 1484 loss: 1.36395865e-05
Iter: 1485 loss: 1.36268227e-05
Iter: 1486 loss: 1.36245271e-05
Iter: 1487 loss: 1.36236686e-05
Iter: 1488 loss: 1.36197159e-05
Iter: 1489 loss: 1.36226863e-05
Iter: 1490 loss: 1.36173803e-05
Iter: 1491 loss: 1.36150848e-05
Iter: 1492 loss: 1.36101553e-05
Iter: 1493 loss: 1.37088637e-05
Iter: 1494 loss: 1.36103272e-05
Iter: 1495 loss: 1.36042954e-05
Iter: 1496 loss: 1.36132203e-05
Iter: 1497 loss: 1.36010449e-05
Iter: 1498 loss: 1.35945284e-05
Iter: 1499 loss: 1.36131875e-05
Iter: 1500 loss: 1.35927739e-05
Iter: 1501 loss: 1.35867358e-05
Iter: 1502 loss: 1.36079125e-05
Iter: 1503 loss: 1.35855198e-05
Iter: 1504 loss: 1.35793189e-05
Iter: 1505 loss: 1.35849523e-05
Iter: 1506 loss: 1.35749115e-05
Iter: 1507 loss: 1.35691789e-05
Iter: 1508 loss: 1.35935552e-05
Iter: 1509 loss: 1.35674754e-05
Iter: 1510 loss: 1.35599548e-05
Iter: 1511 loss: 1.35665432e-05
Iter: 1512 loss: 1.35556838e-05
Iter: 1513 loss: 1.35493983e-05
Iter: 1514 loss: 1.36003882e-05
Iter: 1515 loss: 1.3548367e-05
Iter: 1516 loss: 1.35426199e-05
Iter: 1517 loss: 1.35422615e-05
Iter: 1518 loss: 1.35380278e-05
Iter: 1519 loss: 1.3533172e-05
Iter: 1520 loss: 1.3533001e-05
Iter: 1521 loss: 1.35284845e-05
Iter: 1522 loss: 1.35562295e-05
Iter: 1523 loss: 1.35281207e-05
Iter: 1524 loss: 1.35262117e-05
Iter: 1525 loss: 1.35205428e-05
Iter: 1526 loss: 1.35471346e-05
Iter: 1527 loss: 1.35185965e-05
Iter: 1528 loss: 1.35111441e-05
Iter: 1529 loss: 1.35591163e-05
Iter: 1530 loss: 1.35107075e-05
Iter: 1531 loss: 1.35042364e-05
Iter: 1532 loss: 1.35073869e-05
Iter: 1533 loss: 1.34995489e-05
Iter: 1534 loss: 1.34919601e-05
Iter: 1535 loss: 1.35141599e-05
Iter: 1536 loss: 1.34902566e-05
Iter: 1537 loss: 1.3481309e-05
Iter: 1538 loss: 1.34909842e-05
Iter: 1539 loss: 1.34775282e-05
Iter: 1540 loss: 1.34700904e-05
Iter: 1541 loss: 1.3509889e-05
Iter: 1542 loss: 1.34685051e-05
Iter: 1543 loss: 1.346113e-05
Iter: 1544 loss: 1.3474566e-05
Iter: 1545 loss: 1.34584416e-05
Iter: 1546 loss: 1.34516131e-05
Iter: 1547 loss: 1.34802121e-05
Iter: 1548 loss: 1.34508136e-05
Iter: 1549 loss: 1.3444429e-05
Iter: 1550 loss: 1.34459642e-05
Iter: 1551 loss: 1.34397906e-05
Iter: 1552 loss: 1.34368302e-05
Iter: 1553 loss: 1.34360744e-05
Iter: 1554 loss: 1.34335896e-05
Iter: 1555 loss: 1.34646443e-05
Iter: 1556 loss: 1.34330439e-05
Iter: 1557 loss: 1.34321072e-05
Iter: 1558 loss: 1.34271613e-05
Iter: 1559 loss: 1.34314614e-05
Iter: 1560 loss: 1.34235734e-05
Iter: 1561 loss: 1.34167922e-05
Iter: 1562 loss: 1.34787833e-05
Iter: 1563 loss: 1.34164493e-05
Iter: 1564 loss: 1.34110178e-05
Iter: 1565 loss: 1.34123757e-05
Iter: 1566 loss: 1.34067e-05
Iter: 1567 loss: 1.33996218e-05
Iter: 1568 loss: 1.34292295e-05
Iter: 1569 loss: 1.33975682e-05
Iter: 1570 loss: 1.33910326e-05
Iter: 1571 loss: 1.34020038e-05
Iter: 1572 loss: 1.3387973e-05
Iter: 1573 loss: 1.33811973e-05
Iter: 1574 loss: 1.33965805e-05
Iter: 1575 loss: 1.33791782e-05
Iter: 1576 loss: 1.33708472e-05
Iter: 1577 loss: 1.3389018e-05
Iter: 1578 loss: 1.33673757e-05
Iter: 1579 loss: 1.33608182e-05
Iter: 1580 loss: 1.33862668e-05
Iter: 1581 loss: 1.33590056e-05
Iter: 1582 loss: 1.33512895e-05
Iter: 1583 loss: 1.33553713e-05
Iter: 1584 loss: 1.33460017e-05
Iter: 1585 loss: 1.33406211e-05
Iter: 1586 loss: 1.33401436e-05
Iter: 1587 loss: 1.33371723e-05
Iter: 1588 loss: 1.33370359e-05
Iter: 1589 loss: 1.33345402e-05
Iter: 1590 loss: 1.33272606e-05
Iter: 1591 loss: 1.33234762e-05
Iter: 1592 loss: 1.33186022e-05
Iter: 1593 loss: 1.33101385e-05
Iter: 1594 loss: 1.33096837e-05
Iter: 1595 loss: 1.33038129e-05
Iter: 1596 loss: 1.33007479e-05
Iter: 1597 loss: 1.32976966e-05
Iter: 1598 loss: 1.32879477e-05
Iter: 1599 loss: 1.33251033e-05
Iter: 1600 loss: 1.32855184e-05
Iter: 1601 loss: 1.32762098e-05
Iter: 1602 loss: 1.32999176e-05
Iter: 1603 loss: 1.32724908e-05
Iter: 1604 loss: 1.32637888e-05
Iter: 1605 loss: 1.32796604e-05
Iter: 1606 loss: 1.32605264e-05
Iter: 1607 loss: 1.32512305e-05
Iter: 1608 loss: 1.32759833e-05
Iter: 1609 loss: 1.32483956e-05
Iter: 1610 loss: 1.32404584e-05
Iter: 1611 loss: 1.32571922e-05
Iter: 1612 loss: 1.32376363e-05
Iter: 1613 loss: 1.32277937e-05
Iter: 1614 loss: 1.32496971e-05
Iter: 1615 loss: 1.32237328e-05
Iter: 1616 loss: 1.3216586e-05
Iter: 1617 loss: 1.32877449e-05
Iter: 1618 loss: 1.3216516e-05
Iter: 1619 loss: 1.32133482e-05
Iter: 1620 loss: 1.32131627e-05
Iter: 1621 loss: 1.32103687e-05
Iter: 1622 loss: 1.32032346e-05
Iter: 1623 loss: 1.32133691e-05
Iter: 1624 loss: 1.3198056e-05
Iter: 1625 loss: 1.31908628e-05
Iter: 1626 loss: 1.32669111e-05
Iter: 1627 loss: 1.31901234e-05
Iter: 1628 loss: 1.3184309e-05
Iter: 1629 loss: 1.31752095e-05
Iter: 1630 loss: 1.31753e-05
Iter: 1631 loss: 1.31646693e-05
Iter: 1632 loss: 1.32988825e-05
Iter: 1633 loss: 1.31646211e-05
Iter: 1634 loss: 1.31573261e-05
Iter: 1635 loss: 1.3183505e-05
Iter: 1636 loss: 1.31547413e-05
Iter: 1637 loss: 1.31493152e-05
Iter: 1638 loss: 1.31460956e-05
Iter: 1639 loss: 1.31434617e-05
Iter: 1640 loss: 1.31336774e-05
Iter: 1641 loss: 1.31750676e-05
Iter: 1642 loss: 1.31317775e-05
Iter: 1643 loss: 1.31242323e-05
Iter: 1644 loss: 1.31436109e-05
Iter: 1645 loss: 1.31211718e-05
Iter: 1646 loss: 1.31128281e-05
Iter: 1647 loss: 1.3156874e-05
Iter: 1648 loss: 1.31110965e-05
Iter: 1649 loss: 1.31051511e-05
Iter: 1650 loss: 1.31465222e-05
Iter: 1651 loss: 1.31050183e-05
Iter: 1652 loss: 1.3103474e-05
Iter: 1653 loss: 1.3102057e-05
Iter: 1654 loss: 1.30994586e-05
Iter: 1655 loss: 1.30928565e-05
Iter: 1656 loss: 1.31128445e-05
Iter: 1657 loss: 1.3089264e-05
Iter: 1658 loss: 1.30831395e-05
Iter: 1659 loss: 1.31157913e-05
Iter: 1660 loss: 1.30815988e-05
Iter: 1661 loss: 1.30748431e-05
Iter: 1662 loss: 1.30664139e-05
Iter: 1663 loss: 1.30652807e-05
Iter: 1664 loss: 1.30560293e-05
Iter: 1665 loss: 1.31913675e-05
Iter: 1666 loss: 1.30557328e-05
Iter: 1667 loss: 1.30491826e-05
Iter: 1668 loss: 1.30669005e-05
Iter: 1669 loss: 1.30465405e-05
Iter: 1670 loss: 1.30399676e-05
Iter: 1671 loss: 1.30531198e-05
Iter: 1672 loss: 1.30370945e-05
Iter: 1673 loss: 1.30300941e-05
Iter: 1674 loss: 1.30584867e-05
Iter: 1675 loss: 1.30286844e-05
Iter: 1676 loss: 1.30235985e-05
Iter: 1677 loss: 1.30295366e-05
Iter: 1678 loss: 1.30204853e-05
Iter: 1679 loss: 1.30133021e-05
Iter: 1680 loss: 1.30366434e-05
Iter: 1681 loss: 1.30114804e-05
Iter: 1682 loss: 1.30057188e-05
Iter: 1683 loss: 1.30276312e-05
Iter: 1684 loss: 1.30042417e-05
Iter: 1685 loss: 1.30047229e-05
Iter: 1686 loss: 1.30017343e-05
Iter: 1687 loss: 1.29996561e-05
Iter: 1688 loss: 1.29944283e-05
Iter: 1689 loss: 1.30335911e-05
Iter: 1690 loss: 1.29930231e-05
Iter: 1691 loss: 1.29892578e-05
Iter: 1692 loss: 1.2996551e-05
Iter: 1693 loss: 1.29873406e-05
Iter: 1694 loss: 1.29813188e-05
Iter: 1695 loss: 1.2975047e-05
Iter: 1696 loss: 1.29744149e-05
Iter: 1697 loss: 1.29674117e-05
Iter: 1698 loss: 1.30712806e-05
Iter: 1699 loss: 1.29671407e-05
Iter: 1700 loss: 1.29626751e-05
Iter: 1701 loss: 1.29636101e-05
Iter: 1702 loss: 1.29585524e-05
Iter: 1703 loss: 1.29508371e-05
Iter: 1704 loss: 1.2972183e-05
Iter: 1705 loss: 1.29485034e-05
Iter: 1706 loss: 1.29408827e-05
Iter: 1707 loss: 1.29689943e-05
Iter: 1708 loss: 1.29394575e-05
Iter: 1709 loss: 1.29334094e-05
Iter: 1710 loss: 1.29410419e-05
Iter: 1711 loss: 1.29305918e-05
Iter: 1712 loss: 1.29225409e-05
Iter: 1713 loss: 1.29447872e-05
Iter: 1714 loss: 1.29196378e-05
Iter: 1715 loss: 1.29128439e-05
Iter: 1716 loss: 1.29333594e-05
Iter: 1717 loss: 1.29108812e-05
Iter: 1718 loss: 1.29092014e-05
Iter: 1719 loss: 1.29077798e-05
Iter: 1720 loss: 1.290412e-05
Iter: 1721 loss: 1.28977681e-05
Iter: 1722 loss: 1.28977599e-05
Iter: 1723 loss: 1.28932261e-05
Iter: 1724 loss: 1.28883048e-05
Iter: 1725 loss: 1.28876127e-05
Iter: 1726 loss: 1.28781967e-05
Iter: 1727 loss: 1.28925785e-05
Iter: 1728 loss: 1.28733673e-05
Iter: 1729 loss: 1.28667434e-05
Iter: 1730 loss: 1.29600594e-05
Iter: 1731 loss: 1.28672655e-05
Iter: 1732 loss: 1.28612719e-05
Iter: 1733 loss: 1.28580978e-05
Iter: 1734 loss: 1.28558195e-05
Iter: 1735 loss: 1.2846649e-05
Iter: 1736 loss: 1.288776e-05
Iter: 1737 loss: 1.28446836e-05
Iter: 1738 loss: 1.28380216e-05
Iter: 1739 loss: 1.28706479e-05
Iter: 1740 loss: 1.28366701e-05
Iter: 1741 loss: 1.28305492e-05
Iter: 1742 loss: 1.28348765e-05
Iter: 1743 loss: 1.28263764e-05
Iter: 1744 loss: 1.28178199e-05
Iter: 1745 loss: 1.28403153e-05
Iter: 1746 loss: 1.28144757e-05
Iter: 1747 loss: 1.28073425e-05
Iter: 1748 loss: 1.28327083e-05
Iter: 1749 loss: 1.28053562e-05
Iter: 1750 loss: 1.28026913e-05
Iter: 1751 loss: 1.28022239e-05
Iter: 1752 loss: 1.27981e-05
Iter: 1753 loss: 1.27954027e-05
Iter: 1754 loss: 1.27931462e-05
Iter: 1755 loss: 1.27896219e-05
Iter: 1756 loss: 1.2782155e-05
Iter: 1757 loss: 1.2891388e-05
Iter: 1758 loss: 1.27814992e-05
Iter: 1759 loss: 1.2770879e-05
Iter: 1760 loss: 1.28025622e-05
Iter: 1761 loss: 1.27675776e-05
Iter: 1762 loss: 1.27599042e-05
Iter: 1763 loss: 1.2840168e-05
Iter: 1764 loss: 1.27597577e-05
Iter: 1765 loss: 1.27540234e-05
Iter: 1766 loss: 1.2750982e-05
Iter: 1767 loss: 1.27480525e-05
Iter: 1768 loss: 1.27396524e-05
Iter: 1769 loss: 1.2801278e-05
Iter: 1770 loss: 1.27384692e-05
Iter: 1771 loss: 1.27326e-05
Iter: 1772 loss: 1.27483754e-05
Iter: 1773 loss: 1.27311832e-05
Iter: 1774 loss: 1.27242556e-05
Iter: 1775 loss: 1.27311596e-05
Iter: 1776 loss: 1.27206677e-05
Iter: 1777 loss: 1.27127123e-05
Iter: 1778 loss: 1.2742471e-05
Iter: 1779 loss: 1.27110625e-05
Iter: 1780 loss: 1.27043295e-05
Iter: 1781 loss: 1.27198582e-05
Iter: 1782 loss: 1.27024705e-05
Iter: 1783 loss: 1.26967543e-05
Iter: 1784 loss: 1.27622206e-05
Iter: 1785 loss: 1.2696979e-05
Iter: 1786 loss: 1.26903597e-05
Iter: 1787 loss: 1.27168114e-05
Iter: 1788 loss: 1.26889281e-05
Iter: 1789 loss: 1.26865261e-05
Iter: 1790 loss: 1.26795367e-05
Iter: 1791 loss: 1.27251733e-05
Iter: 1792 loss: 1.2678458e-05
Iter: 1793 loss: 1.26681971e-05
Iter: 1794 loss: 1.26904142e-05
Iter: 1795 loss: 1.26645082e-05
Iter: 1796 loss: 1.26562427e-05
Iter: 1797 loss: 1.27030798e-05
Iter: 1798 loss: 1.26556961e-05
Iter: 1799 loss: 1.26472532e-05
Iter: 1800 loss: 1.26500981e-05
Iter: 1801 loss: 1.26420182e-05
Iter: 1802 loss: 1.26317373e-05
Iter: 1803 loss: 1.27093626e-05
Iter: 1804 loss: 1.26312243e-05
Iter: 1805 loss: 1.26249251e-05
Iter: 1806 loss: 1.26349642e-05
Iter: 1807 loss: 1.26219211e-05
Iter: 1808 loss: 1.26139512e-05
Iter: 1809 loss: 1.26335453e-05
Iter: 1810 loss: 1.26109944e-05
Iter: 1811 loss: 1.26025097e-05
Iter: 1812 loss: 1.262972e-05
Iter: 1813 loss: 1.26006144e-05
Iter: 1814 loss: 1.25925853e-05
Iter: 1815 loss: 1.26031482e-05
Iter: 1816 loss: 1.25886481e-05
Iter: 1817 loss: 1.25799306e-05
Iter: 1818 loss: 1.26313134e-05
Iter: 1819 loss: 1.25788065e-05
Iter: 1820 loss: 1.25724782e-05
Iter: 1821 loss: 1.2572511e-05
Iter: 1822 loss: 1.25703882e-05
Iter: 1823 loss: 1.256415e-05
Iter: 1824 loss: 1.25904953e-05
Iter: 1825 loss: 1.25613133e-05
Iter: 1826 loss: 1.25523566e-05
Iter: 1827 loss: 1.25742863e-05
Iter: 1828 loss: 1.25493225e-05
Iter: 1829 loss: 1.25416982e-05
Iter: 1830 loss: 1.25682072e-05
Iter: 1831 loss: 1.25393954e-05
Iter: 1832 loss: 1.25313909e-05
Iter: 1833 loss: 1.25412716e-05
Iter: 1834 loss: 1.25270344e-05
Iter: 1835 loss: 1.25191355e-05
Iter: 1836 loss: 1.25565248e-05
Iter: 1837 loss: 1.25176966e-05
Iter: 1838 loss: 1.25105526e-05
Iter: 1839 loss: 1.25142797e-05
Iter: 1840 loss: 1.25062315e-05
Iter: 1841 loss: 1.2495726e-05
Iter: 1842 loss: 1.25420984e-05
Iter: 1843 loss: 1.24942562e-05
Iter: 1844 loss: 1.24867529e-05
Iter: 1845 loss: 1.25311526e-05
Iter: 1846 loss: 1.24854487e-05
Iter: 1847 loss: 1.24802755e-05
Iter: 1848 loss: 1.24882245e-05
Iter: 1849 loss: 1.24774215e-05
Iter: 1850 loss: 1.24699127e-05
Iter: 1851 loss: 1.24849794e-05
Iter: 1852 loss: 1.24672497e-05
Iter: 1853 loss: 1.24679736e-05
Iter: 1854 loss: 1.24640374e-05
Iter: 1855 loss: 1.24619201e-05
Iter: 1856 loss: 1.24569015e-05
Iter: 1857 loss: 1.24790677e-05
Iter: 1858 loss: 1.2454585e-05
Iter: 1859 loss: 1.24478229e-05
Iter: 1860 loss: 1.24578546e-05
Iter: 1861 loss: 1.24452527e-05
Iter: 1862 loss: 1.24383432e-05
Iter: 1863 loss: 1.24554608e-05
Iter: 1864 loss: 1.24359394e-05
Iter: 1865 loss: 1.24291728e-05
Iter: 1866 loss: 1.2442104e-05
Iter: 1867 loss: 1.24258913e-05
Iter: 1868 loss: 1.24186681e-05
Iter: 1869 loss: 1.24432754e-05
Iter: 1870 loss: 1.24167827e-05
Iter: 1871 loss: 1.24105973e-05
Iter: 1872 loss: 1.24257758e-05
Iter: 1873 loss: 1.24072585e-05
Iter: 1874 loss: 1.23990831e-05
Iter: 1875 loss: 1.24183571e-05
Iter: 1876 loss: 1.2395496e-05
Iter: 1877 loss: 1.2388924e-05
Iter: 1878 loss: 1.24223207e-05
Iter: 1879 loss: 1.2387407e-05
Iter: 1880 loss: 1.23815753e-05
Iter: 1881 loss: 1.23872032e-05
Iter: 1882 loss: 1.23785458e-05
Iter: 1883 loss: 1.23711452e-05
Iter: 1884 loss: 1.24198068e-05
Iter: 1885 loss: 1.2370665e-05
Iter: 1886 loss: 1.23716018e-05
Iter: 1887 loss: 1.23681266e-05
Iter: 1888 loss: 1.23666687e-05
Iter: 1889 loss: 1.23622385e-05
Iter: 1890 loss: 1.24141825e-05
Iter: 1891 loss: 1.23622949e-05
Iter: 1892 loss: 1.23583104e-05
Iter: 1893 loss: 1.23557793e-05
Iter: 1894 loss: 1.23544487e-05
Iter: 1895 loss: 1.23477894e-05
Iter: 1896 loss: 1.2360264e-05
Iter: 1897 loss: 1.23452737e-05
Iter: 1898 loss: 1.23383979e-05
Iter: 1899 loss: 1.23568425e-05
Iter: 1900 loss: 1.23358668e-05
Iter: 1901 loss: 1.23290738e-05
Iter: 1902 loss: 1.23471018e-05
Iter: 1903 loss: 1.23264526e-05
Iter: 1904 loss: 1.23199061e-05
Iter: 1905 loss: 1.23363197e-05
Iter: 1906 loss: 1.2317194e-05
Iter: 1907 loss: 1.23096324e-05
Iter: 1908 loss: 1.2324821e-05
Iter: 1909 loss: 1.23064074e-05
Iter: 1910 loss: 1.23004611e-05
Iter: 1911 loss: 1.23411428e-05
Iter: 1912 loss: 1.23000063e-05
Iter: 1913 loss: 1.22946785e-05
Iter: 1914 loss: 1.22964375e-05
Iter: 1915 loss: 1.22908559e-05
Iter: 1916 loss: 1.22835918e-05
Iter: 1917 loss: 1.23130685e-05
Iter: 1918 loss: 1.22817373e-05
Iter: 1919 loss: 1.2284665e-05
Iter: 1920 loss: 1.22796064e-05
Iter: 1921 loss: 1.22782767e-05
Iter: 1922 loss: 1.22733354e-05
Iter: 1923 loss: 1.23379677e-05
Iter: 1924 loss: 1.22734909e-05
Iter: 1925 loss: 1.22691508e-05
Iter: 1926 loss: 1.22682886e-05
Iter: 1927 loss: 1.22649781e-05
Iter: 1928 loss: 1.22593374e-05
Iter: 1929 loss: 1.22838019e-05
Iter: 1930 loss: 1.22581132e-05
Iter: 1931 loss: 1.22537858e-05
Iter: 1932 loss: 1.22600995e-05
Iter: 1933 loss: 1.22510228e-05
Iter: 1934 loss: 1.22452639e-05
Iter: 1935 loss: 1.22565625e-05
Iter: 1936 loss: 1.22429774e-05
Iter: 1937 loss: 1.22374058e-05
Iter: 1938 loss: 1.22604088e-05
Iter: 1939 loss: 1.22359288e-05
Iter: 1940 loss: 1.22304646e-05
Iter: 1941 loss: 1.22419578e-05
Iter: 1942 loss: 1.22280762e-05
Iter: 1943 loss: 1.22231331e-05
Iter: 1944 loss: 1.22488009e-05
Iter: 1945 loss: 1.22219544e-05
Iter: 1946 loss: 1.22172578e-05
Iter: 1947 loss: 1.22135516e-05
Iter: 1948 loss: 1.22122383e-05
Iter: 1949 loss: 1.22045913e-05
Iter: 1950 loss: 1.22501369e-05
Iter: 1951 loss: 1.22031988e-05
Iter: 1952 loss: 1.22020274e-05
Iter: 1953 loss: 1.22008787e-05
Iter: 1954 loss: 1.21974572e-05
Iter: 1955 loss: 1.21961812e-05
Iter: 1956 loss: 1.21940902e-05
Iter: 1957 loss: 1.21910052e-05
Iter: 1958 loss: 1.21847188e-05
Iter: 1959 loss: 1.23097052e-05
Iter: 1960 loss: 1.21845987e-05
Iter: 1961 loss: 1.21780231e-05
Iter: 1962 loss: 1.22252786e-05
Iter: 1963 loss: 1.21776575e-05
Iter: 1964 loss: 1.21717649e-05
Iter: 1965 loss: 1.21761577e-05
Iter: 1966 loss: 1.21696685e-05
Iter: 1967 loss: 1.2162267e-05
Iter: 1968 loss: 1.21804296e-05
Iter: 1969 loss: 1.21601488e-05
Iter: 1970 loss: 1.21534513e-05
Iter: 1971 loss: 1.21745597e-05
Iter: 1972 loss: 1.21511202e-05
Iter: 1973 loss: 1.21449066e-05
Iter: 1974 loss: 1.21601151e-05
Iter: 1975 loss: 1.21425255e-05
Iter: 1976 loss: 1.21364219e-05
Iter: 1977 loss: 1.21582125e-05
Iter: 1978 loss: 1.21350859e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2.8/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi3 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi3
+ date
Sun Nov  8 17:17:05 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi3/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi3/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 3 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi3/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df62e91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df63c2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df64abae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df63d67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df62797b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df62798c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df6263d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df622db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df622dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df61e9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df61e7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df61a8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df61997b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df61999d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df6199d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd2c49510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd2c492f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd2c23e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd2bfc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd2beef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd2bba378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd2b71ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd2bab8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dac243620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dac2236a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dac1c0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dac217598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dac1f3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dac1f3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd2b5b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dac133400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dac18d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dac18da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dac191ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dac26c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dac0d72f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 202, in <module>
    grads = tape.gradient(loss, model.trainable_weights)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1266, in _backward_function_wrapper
    processed_args, remapped_captures)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input is not invertible.
	 [[node gradients/MatrixDeterminant_grad/MatrixInverse (defined at biholoNN_train.py:200) ]] [Op:__inference___backward_volume_form_4446_8945]

Function call stack:
__backward_volume_form_4446

+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi3/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi3/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 3 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi3/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f40924bf268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f409248ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f409248a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f409248a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f40923dd400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f40923dd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4092314f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f40922bfbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f40922bf488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4092278b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f409222d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f40922592f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4092256400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4092256d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f40922562f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4092256598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4092256ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f409216cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f409214e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f40920fe268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4092194840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f40920a9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f40920de598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4092090840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4092090730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4071e75158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4071e408c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4071e6a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4071df2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4071e05b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4071dc5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4071de6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4071de6d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4071d89840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4071d3c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4071cf76a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.2195526e-05
Iter: 2 loss: 4.95058484e-05
Iter: 3 loss: 4.83660406e-05
Iter: 4 loss: 4.72495885e-05
Iter: 5 loss: 4.64894547e-05
Iter: 6 loss: 4.50585139e-05
Iter: 7 loss: 4.13113e-05
Iter: 8 loss: 7.00238597e-05
Iter: 9 loss: 4.05695391e-05
Iter: 10 loss: 3.75242525e-05
Iter: 11 loss: 5.4588636e-05
Iter: 12 loss: 3.70978487e-05
Iter: 13 loss: 3.43902102e-05
Iter: 14 loss: 3.55235024e-05
Iter: 15 loss: 3.25350265e-05
Iter: 16 loss: 3.11188778e-05
Iter: 17 loss: 3.10491741e-05
Iter: 18 loss: 3.01546133e-05
Iter: 19 loss: 2.83495028e-05
Iter: 20 loss: 6.21984655e-05
Iter: 21 loss: 2.83230074e-05
Iter: 22 loss: 2.72303e-05
Iter: 23 loss: 2.71952504e-05
Iter: 24 loss: 2.66113857e-05
Iter: 25 loss: 2.62269805e-05
Iter: 26 loss: 2.60050547e-05
Iter: 27 loss: 2.52062673e-05
Iter: 28 loss: 2.49220757e-05
Iter: 29 loss: 2.44726943e-05
Iter: 30 loss: 2.41127018e-05
Iter: 31 loss: 2.3983237e-05
Iter: 32 loss: 2.37825461e-05
Iter: 33 loss: 2.32857619e-05
Iter: 34 loss: 2.28287936e-05
Iter: 35 loss: 2.27082201e-05
Iter: 36 loss: 2.2111637e-05
Iter: 37 loss: 2.69896918e-05
Iter: 38 loss: 2.2074466e-05
Iter: 39 loss: 2.17339984e-05
Iter: 40 loss: 2.49091e-05
Iter: 41 loss: 2.17195065e-05
Iter: 42 loss: 2.13980202e-05
Iter: 43 loss: 2.25262302e-05
Iter: 44 loss: 2.13148051e-05
Iter: 45 loss: 2.11388688e-05
Iter: 46 loss: 2.08107267e-05
Iter: 47 loss: 2.82876717e-05
Iter: 48 loss: 2.08097845e-05
Iter: 49 loss: 2.03925119e-05
Iter: 50 loss: 2.38678949e-05
Iter: 51 loss: 2.03674699e-05
Iter: 52 loss: 2.0148269e-05
Iter: 53 loss: 2.00340055e-05
Iter: 54 loss: 1.9932666e-05
Iter: 55 loss: 1.94743061e-05
Iter: 56 loss: 2.13340882e-05
Iter: 57 loss: 1.93717187e-05
Iter: 58 loss: 1.91360759e-05
Iter: 59 loss: 1.94143613e-05
Iter: 60 loss: 1.90114188e-05
Iter: 61 loss: 1.87126898e-05
Iter: 62 loss: 2.07961202e-05
Iter: 63 loss: 1.86850357e-05
Iter: 64 loss: 1.85626013e-05
Iter: 65 loss: 1.85607205e-05
Iter: 66 loss: 1.84372257e-05
Iter: 67 loss: 1.82224394e-05
Iter: 68 loss: 1.82219446e-05
Iter: 69 loss: 1.80931911e-05
Iter: 70 loss: 1.96497058e-05
Iter: 71 loss: 1.80923689e-05
Iter: 72 loss: 1.79991184e-05
Iter: 73 loss: 1.78857154e-05
Iter: 74 loss: 1.78753853e-05
Iter: 75 loss: 1.7693852e-05
Iter: 76 loss: 2.00632821e-05
Iter: 77 loss: 1.76927424e-05
Iter: 78 loss: 1.76189242e-05
Iter: 79 loss: 1.75465393e-05
Iter: 80 loss: 1.75310215e-05
Iter: 81 loss: 1.73980698e-05
Iter: 82 loss: 1.76223257e-05
Iter: 83 loss: 1.7338305e-05
Iter: 84 loss: 1.72058881e-05
Iter: 85 loss: 1.72316741e-05
Iter: 86 loss: 1.71066367e-05
Iter: 87 loss: 1.69383748e-05
Iter: 88 loss: 1.91934814e-05
Iter: 89 loss: 1.69374325e-05
Iter: 90 loss: 1.68609586e-05
Iter: 91 loss: 1.68931256e-05
Iter: 92 loss: 1.68084134e-05
Iter: 93 loss: 1.66707378e-05
Iter: 94 loss: 1.68786191e-05
Iter: 95 loss: 1.66048667e-05
Iter: 96 loss: 1.65212568e-05
Iter: 97 loss: 1.75981295e-05
Iter: 98 loss: 1.65203273e-05
Iter: 99 loss: 1.64305675e-05
Iter: 100 loss: 1.65962683e-05
Iter: 101 loss: 1.63922286e-05
Iter: 102 loss: 1.63201548e-05
Iter: 103 loss: 1.63835794e-05
Iter: 104 loss: 1.6277525e-05
Iter: 105 loss: 1.6197122e-05
Iter: 106 loss: 1.61440257e-05
Iter: 107 loss: 1.61140906e-05
Iter: 108 loss: 1.6096099e-05
Iter: 109 loss: 1.60558047e-05
Iter: 110 loss: 1.60199597e-05
Iter: 111 loss: 1.59771243e-05
Iter: 112 loss: 1.59725587e-05
Iter: 113 loss: 1.58923158e-05
Iter: 114 loss: 1.58055045e-05
Iter: 115 loss: 1.57923096e-05
Iter: 116 loss: 1.56895658e-05
Iter: 117 loss: 1.65244819e-05
Iter: 118 loss: 1.56834794e-05
Iter: 119 loss: 1.56032802e-05
Iter: 120 loss: 1.56853457e-05
Iter: 121 loss: 1.55594244e-05
Iter: 122 loss: 1.5474885e-05
Iter: 123 loss: 1.61554308e-05
Iter: 124 loss: 1.54694171e-05
Iter: 125 loss: 1.54074405e-05
Iter: 126 loss: 1.54178051e-05
Iter: 127 loss: 1.53602323e-05
Iter: 128 loss: 1.52931025e-05
Iter: 129 loss: 1.6167116e-05
Iter: 130 loss: 1.52927605e-05
Iter: 131 loss: 1.52636385e-05
Iter: 132 loss: 1.567861e-05
Iter: 133 loss: 1.52639441e-05
Iter: 134 loss: 1.52339817e-05
Iter: 135 loss: 1.51601553e-05
Iter: 136 loss: 1.59437259e-05
Iter: 137 loss: 1.51531422e-05
Iter: 138 loss: 1.5083715e-05
Iter: 139 loss: 1.57990289e-05
Iter: 140 loss: 1.5082087e-05
Iter: 141 loss: 1.50429605e-05
Iter: 142 loss: 1.50835294e-05
Iter: 143 loss: 1.50222186e-05
Iter: 144 loss: 1.49640864e-05
Iter: 145 loss: 1.53781384e-05
Iter: 146 loss: 1.49590142e-05
Iter: 147 loss: 1.49299594e-05
Iter: 148 loss: 1.48905665e-05
Iter: 149 loss: 1.48883355e-05
Iter: 150 loss: 1.48249183e-05
Iter: 151 loss: 1.49775096e-05
Iter: 152 loss: 1.48016516e-05
Iter: 153 loss: 1.47564597e-05
Iter: 154 loss: 1.47949359e-05
Iter: 155 loss: 1.47295395e-05
Iter: 156 loss: 1.46531092e-05
Iter: 157 loss: 1.48421423e-05
Iter: 158 loss: 1.46265411e-05
Iter: 159 loss: 1.45788235e-05
Iter: 160 loss: 1.48308955e-05
Iter: 161 loss: 1.45722279e-05
Iter: 162 loss: 1.45230169e-05
Iter: 163 loss: 1.46203474e-05
Iter: 164 loss: 1.45025197e-05
Iter: 165 loss: 1.44717505e-05
Iter: 166 loss: 1.44715941e-05
Iter: 167 loss: 1.44386104e-05
Iter: 168 loss: 1.44246369e-05
Iter: 169 loss: 1.44076039e-05
Iter: 170 loss: 1.43689804e-05
Iter: 171 loss: 1.44905516e-05
Iter: 172 loss: 1.4358091e-05
Iter: 173 loss: 1.43304842e-05
Iter: 174 loss: 1.43115649e-05
Iter: 175 loss: 1.43015213e-05
Iter: 176 loss: 1.42570098e-05
Iter: 177 loss: 1.49337084e-05
Iter: 178 loss: 1.42566014e-05
Iter: 179 loss: 1.42282624e-05
Iter: 180 loss: 1.42729896e-05
Iter: 181 loss: 1.42154076e-05
Iter: 182 loss: 1.41913279e-05
Iter: 183 loss: 1.41590908e-05
Iter: 184 loss: 1.41574365e-05
Iter: 185 loss: 1.41053115e-05
Iter: 186 loss: 1.42531107e-05
Iter: 187 loss: 1.40887714e-05
Iter: 188 loss: 1.40442344e-05
Iter: 189 loss: 1.42090703e-05
Iter: 190 loss: 1.40326129e-05
Iter: 191 loss: 1.39901813e-05
Iter: 192 loss: 1.41683995e-05
Iter: 193 loss: 1.39812055e-05
Iter: 194 loss: 1.39483254e-05
Iter: 195 loss: 1.39684798e-05
Iter: 196 loss: 1.39282965e-05
Iter: 197 loss: 1.38740561e-05
Iter: 198 loss: 1.40663215e-05
Iter: 199 loss: 1.386037e-05
Iter: 200 loss: 1.38468649e-05
Iter: 201 loss: 1.38389296e-05
Iter: 202 loss: 1.38258547e-05
Iter: 203 loss: 1.37963843e-05
Iter: 204 loss: 1.41639894e-05
Iter: 205 loss: 1.37943362e-05
Iter: 206 loss: 1.37601855e-05
Iter: 207 loss: 1.39214535e-05
Iter: 208 loss: 1.37539282e-05
Iter: 209 loss: 1.37273601e-05
Iter: 210 loss: 1.37896959e-05
Iter: 211 loss: 1.37181451e-05
Iter: 212 loss: 1.36795534e-05
Iter: 213 loss: 1.38010491e-05
Iter: 214 loss: 1.36679901e-05
Iter: 215 loss: 1.36480758e-05
Iter: 216 loss: 1.36765702e-05
Iter: 217 loss: 1.36383251e-05
Iter: 218 loss: 1.36107328e-05
Iter: 219 loss: 1.35638302e-05
Iter: 220 loss: 1.35640066e-05
Iter: 221 loss: 1.35272239e-05
Iter: 222 loss: 1.40987177e-05
Iter: 223 loss: 1.35275186e-05
Iter: 224 loss: 1.34964484e-05
Iter: 225 loss: 1.34996353e-05
Iter: 226 loss: 1.34725524e-05
Iter: 227 loss: 1.34374241e-05
Iter: 228 loss: 1.3632507e-05
Iter: 229 loss: 1.34330512e-05
Iter: 230 loss: 1.33928961e-05
Iter: 231 loss: 1.34241545e-05
Iter: 232 loss: 1.33690137e-05
Iter: 233 loss: 1.33674475e-05
Iter: 234 loss: 1.33542108e-05
Iter: 235 loss: 1.33412022e-05
Iter: 236 loss: 1.33162766e-05
Iter: 237 loss: 1.38351033e-05
Iter: 238 loss: 1.33158192e-05
Iter: 239 loss: 1.32825808e-05
Iter: 240 loss: 1.33720887e-05
Iter: 241 loss: 1.32712066e-05
Iter: 242 loss: 1.32495388e-05
Iter: 243 loss: 1.33704689e-05
Iter: 244 loss: 1.32469258e-05
Iter: 245 loss: 1.32253845e-05
Iter: 246 loss: 1.33303984e-05
Iter: 247 loss: 1.3221339e-05
Iter: 248 loss: 1.32029727e-05
Iter: 249 loss: 1.31990046e-05
Iter: 250 loss: 1.3187333e-05
Iter: 251 loss: 1.31624329e-05
Iter: 252 loss: 1.31512134e-05
Iter: 253 loss: 1.31393772e-05
Iter: 254 loss: 1.31087236e-05
Iter: 255 loss: 1.34211268e-05
Iter: 256 loss: 1.31077413e-05
Iter: 257 loss: 1.30865847e-05
Iter: 258 loss: 1.30823555e-05
Iter: 259 loss: 1.3068443e-05
Iter: 260 loss: 1.30332182e-05
Iter: 261 loss: 1.31490742e-05
Iter: 262 loss: 1.30231856e-05
Iter: 263 loss: 1.30005519e-05
Iter: 264 loss: 1.31326233e-05
Iter: 265 loss: 1.29973705e-05
Iter: 266 loss: 1.29742266e-05
Iter: 267 loss: 1.30307335e-05
Iter: 268 loss: 1.29655318e-05
Iter: 269 loss: 1.29497012e-05
Iter: 270 loss: 1.29491227e-05
Iter: 271 loss: 1.29399e-05
Iter: 272 loss: 1.2915596e-05
Iter: 273 loss: 1.31246798e-05
Iter: 274 loss: 1.29115842e-05
Iter: 275 loss: 1.28866377e-05
Iter: 276 loss: 1.31781298e-05
Iter: 277 loss: 1.28861993e-05
Iter: 278 loss: 1.28732845e-05
Iter: 279 loss: 1.30115704e-05
Iter: 280 loss: 1.287347e-05
Iter: 281 loss: 1.28591755e-05
Iter: 282 loss: 1.28281217e-05
Iter: 283 loss: 1.3309078e-05
Iter: 284 loss: 1.28269949e-05
Iter: 285 loss: 1.27972535e-05
Iter: 286 loss: 1.30306371e-05
Iter: 287 loss: 1.27952853e-05
Iter: 288 loss: 1.27758049e-05
Iter: 289 loss: 1.27627791e-05
Iter: 290 loss: 1.27548401e-05
Iter: 291 loss: 1.27308022e-05
Iter: 292 loss: 1.28927186e-05
Iter: 293 loss: 1.27285839e-05
Iter: 294 loss: 1.27006633e-05
Iter: 295 loss: 1.27113653e-05
Iter: 296 loss: 1.26812583e-05
Iter: 297 loss: 1.26617151e-05
Iter: 298 loss: 1.27886688e-05
Iter: 299 loss: 1.26597843e-05
Iter: 300 loss: 1.26376763e-05
Iter: 301 loss: 1.26582381e-05
Iter: 302 loss: 1.2625107e-05
Iter: 303 loss: 1.2614044e-05
Iter: 304 loss: 1.26105233e-05
Iter: 305 loss: 1.25998531e-05
Iter: 306 loss: 1.25798269e-05
Iter: 307 loss: 1.29666e-05
Iter: 308 loss: 1.25791876e-05
Iter: 309 loss: 1.25573388e-05
Iter: 310 loss: 1.26301129e-05
Iter: 311 loss: 1.25509905e-05
Iter: 312 loss: 1.25363822e-05
Iter: 313 loss: 1.25907609e-05
Iter: 314 loss: 1.25332426e-05
Iter: 315 loss: 1.25139204e-05
Iter: 316 loss: 1.25432816e-05
Iter: 317 loss: 1.25040842e-05
Iter: 318 loss: 1.24852422e-05
Iter: 319 loss: 1.25156112e-05
Iter: 320 loss: 1.24756498e-05
Iter: 321 loss: 1.24601629e-05
Iter: 322 loss: 1.24616763e-05
Iter: 323 loss: 1.24473936e-05
Iter: 324 loss: 1.24213266e-05
Iter: 325 loss: 1.24908111e-05
Iter: 326 loss: 1.24124026e-05
Iter: 327 loss: 1.23945692e-05
Iter: 328 loss: 1.25056522e-05
Iter: 329 loss: 1.23923837e-05
Iter: 330 loss: 1.23754071e-05
Iter: 331 loss: 1.23853961e-05
Iter: 332 loss: 1.23644186e-05
Iter: 333 loss: 1.23446098e-05
Iter: 334 loss: 1.24157268e-05
Iter: 335 loss: 1.23398995e-05
Iter: 336 loss: 1.23175469e-05
Iter: 337 loss: 1.24353628e-05
Iter: 338 loss: 1.23144e-05
Iter: 339 loss: 1.22975443e-05
Iter: 340 loss: 1.25339175e-05
Iter: 341 loss: 1.22977781e-05
Iter: 342 loss: 1.22910051e-05
Iter: 343 loss: 1.22766742e-05
Iter: 344 loss: 1.25139868e-05
Iter: 345 loss: 1.22765741e-05
Iter: 346 loss: 1.22571437e-05
Iter: 347 loss: 1.23327372e-05
Iter: 348 loss: 1.22522724e-05
Iter: 349 loss: 1.22402689e-05
Iter: 350 loss: 1.24290391e-05
Iter: 351 loss: 1.22404354e-05
Iter: 352 loss: 1.22301381e-05
Iter: 353 loss: 1.22139008e-05
Iter: 354 loss: 1.22138936e-05
Iter: 355 loss: 1.2196524e-05
Iter: 356 loss: 1.22933197e-05
Iter: 357 loss: 1.21944367e-05
Iter: 358 loss: 1.21788416e-05
Iter: 359 loss: 1.21490684e-05
Iter: 360 loss: 1.27696821e-05
Iter: 361 loss: 1.21486728e-05
Iter: 362 loss: 1.21350877e-05
Iter: 363 loss: 1.21312387e-05
Iter: 364 loss: 1.21177345e-05
Iter: 365 loss: 1.20974173e-05
Iter: 366 loss: 1.20965342e-05
Iter: 367 loss: 1.20757959e-05
Iter: 368 loss: 1.23178052e-05
Iter: 369 loss: 1.20750974e-05
Iter: 370 loss: 1.20590239e-05
Iter: 371 loss: 1.20852565e-05
Iter: 372 loss: 1.20516452e-05
Iter: 373 loss: 1.20416189e-05
Iter: 374 loss: 1.20404038e-05
Iter: 375 loss: 1.20326513e-05
Iter: 376 loss: 1.20219065e-05
Iter: 377 loss: 1.20214936e-05
Iter: 378 loss: 1.20070245e-05
Iter: 379 loss: 1.20190289e-05
Iter: 380 loss: 1.19978376e-05
Iter: 381 loss: 1.19856686e-05
Iter: 382 loss: 1.19852693e-05
Iter: 383 loss: 1.19760989e-05
Iter: 384 loss: 1.19804099e-05
Iter: 385 loss: 1.19698889e-05
Iter: 386 loss: 1.19588276e-05
Iter: 387 loss: 1.19508386e-05
Iter: 388 loss: 1.19465612e-05
Iter: 389 loss: 1.19296546e-05
Iter: 390 loss: 1.19785254e-05
Iter: 391 loss: 1.19243477e-05
Iter: 392 loss: 1.19050674e-05
Iter: 393 loss: 1.19486085e-05
Iter: 394 loss: 1.1897906e-05
Iter: 395 loss: 1.18827957e-05
Iter: 396 loss: 1.19003907e-05
Iter: 397 loss: 1.18748267e-05
Iter: 398 loss: 1.18545959e-05
Iter: 399 loss: 1.19788874e-05
Iter: 400 loss: 1.18519638e-05
Iter: 401 loss: 1.18405842e-05
Iter: 402 loss: 1.18602675e-05
Iter: 403 loss: 1.18353037e-05
Iter: 404 loss: 1.18217267e-05
Iter: 405 loss: 1.19626293e-05
Iter: 406 loss: 1.18210446e-05
Iter: 407 loss: 1.18085463e-05
Iter: 408 loss: 1.18341159e-05
Iter: 409 loss: 1.18035869e-05
Iter: 410 loss: 1.17966702e-05
Iter: 411 loss: 1.17866839e-05
Iter: 412 loss: 1.17859154e-05
Iter: 413 loss: 1.17744403e-05
Iter: 414 loss: 1.19212127e-05
Iter: 415 loss: 1.17742102e-05
Iter: 416 loss: 1.17655236e-05
Iter: 417 loss: 1.1811293e-05
Iter: 418 loss: 1.17641175e-05
Iter: 419 loss: 1.17570817e-05
Iter: 420 loss: 1.17387599e-05
Iter: 421 loss: 1.19214128e-05
Iter: 422 loss: 1.17368072e-05
Iter: 423 loss: 1.17203499e-05
Iter: 424 loss: 1.19633769e-05
Iter: 425 loss: 1.17202289e-05
Iter: 426 loss: 1.1709003e-05
Iter: 427 loss: 1.17033642e-05
Iter: 428 loss: 1.16980718e-05
Iter: 429 loss: 1.16827068e-05
Iter: 430 loss: 1.1753893e-05
Iter: 431 loss: 1.16804131e-05
Iter: 432 loss: 1.1663029e-05
Iter: 433 loss: 1.16831425e-05
Iter: 434 loss: 1.16538104e-05
Iter: 435 loss: 1.1639695e-05
Iter: 436 loss: 1.17658528e-05
Iter: 437 loss: 1.16382707e-05
Iter: 438 loss: 1.16280544e-05
Iter: 439 loss: 1.16818501e-05
Iter: 440 loss: 1.16263482e-05
Iter: 441 loss: 1.16139572e-05
Iter: 442 loss: 1.16608517e-05
Iter: 443 loss: 1.16110277e-05
Iter: 444 loss: 1.16029478e-05
Iter: 445 loss: 1.16088886e-05
Iter: 446 loss: 1.15986913e-05
Iter: 447 loss: 1.15907296e-05
Iter: 448 loss: 1.15969451e-05
Iter: 449 loss: 1.15858729e-05
Iter: 450 loss: 1.15747662e-05
Iter: 451 loss: 1.16533829e-05
Iter: 452 loss: 1.15732391e-05
Iter: 453 loss: 1.15646835e-05
Iter: 454 loss: 1.15665762e-05
Iter: 455 loss: 1.15584135e-05
Iter: 456 loss: 1.15467174e-05
Iter: 457 loss: 1.15376715e-05
Iter: 458 loss: 1.15339099e-05
Iter: 459 loss: 1.15224657e-05
Iter: 460 loss: 1.16692145e-05
Iter: 461 loss: 1.15222638e-05
Iter: 462 loss: 1.15118655e-05
Iter: 463 loss: 1.15041803e-05
Iter: 464 loss: 1.15010143e-05
Iter: 465 loss: 1.14882987e-05
Iter: 466 loss: 1.15601688e-05
Iter: 467 loss: 1.1486849e-05
Iter: 468 loss: 1.1473513e-05
Iter: 469 loss: 1.14969698e-05
Iter: 470 loss: 1.14679196e-05
Iter: 471 loss: 1.14574559e-05
Iter: 472 loss: 1.15594103e-05
Iter: 473 loss: 1.14569684e-05
Iter: 474 loss: 1.14468749e-05
Iter: 475 loss: 1.15072417e-05
Iter: 476 loss: 1.144622e-05
Iter: 477 loss: 1.1438362e-05
Iter: 478 loss: 1.1440874e-05
Iter: 479 loss: 1.14330269e-05
Iter: 480 loss: 1.14261302e-05
Iter: 481 loss: 1.14275563e-05
Iter: 482 loss: 1.14209697e-05
Iter: 483 loss: 1.14085342e-05
Iter: 484 loss: 1.14674722e-05
Iter: 485 loss: 1.14068253e-05
Iter: 486 loss: 1.13950482e-05
Iter: 487 loss: 1.14222221e-05
Iter: 488 loss: 1.13912974e-05
Iter: 489 loss: 1.13822043e-05
Iter: 490 loss: 1.13739097e-05
Iter: 491 loss: 1.13725791e-05
Iter: 492 loss: 1.13587366e-05
Iter: 493 loss: 1.14383693e-05
Iter: 494 loss: 1.13568076e-05
Iter: 495 loss: 1.13461856e-05
Iter: 496 loss: 1.13557599e-05
Iter: 497 loss: 1.13396518e-05
Iter: 498 loss: 1.13252772e-05
Iter: 499 loss: 1.1366692e-05
Iter: 500 loss: 1.13210272e-05
Iter: 501 loss: 1.13089536e-05
Iter: 502 loss: 1.13283068e-05
Iter: 503 loss: 1.13032966e-05
Iter: 504 loss: 1.12874532e-05
Iter: 505 loss: 1.13570422e-05
Iter: 506 loss: 1.12840271e-05
Iter: 507 loss: 1.12810158e-05
Iter: 508 loss: 1.12779799e-05
Iter: 509 loss: 1.12741418e-05
Iter: 510 loss: 1.12658545e-05
Iter: 511 loss: 1.14142276e-05
Iter: 512 loss: 1.12657553e-05
Iter: 513 loss: 1.12560738e-05
Iter: 514 loss: 1.12886382e-05
Iter: 515 loss: 1.12536272e-05
Iter: 516 loss: 1.12455218e-05
Iter: 517 loss: 1.12686084e-05
Iter: 518 loss: 1.12431117e-05
Iter: 519 loss: 1.12311936e-05
Iter: 520 loss: 1.12426151e-05
Iter: 521 loss: 1.12247808e-05
Iter: 522 loss: 1.12160933e-05
Iter: 523 loss: 1.12559128e-05
Iter: 524 loss: 1.12145462e-05
Iter: 525 loss: 1.12070493e-05
Iter: 526 loss: 1.11958707e-05
Iter: 527 loss: 1.11956033e-05
Iter: 528 loss: 1.11834552e-05
Iter: 529 loss: 1.13338956e-05
Iter: 530 loss: 1.11837735e-05
Iter: 531 loss: 1.11728968e-05
Iter: 532 loss: 1.11677455e-05
Iter: 533 loss: 1.11634208e-05
Iter: 534 loss: 1.11508925e-05
Iter: 535 loss: 1.12486177e-05
Iter: 536 loss: 1.11495829e-05
Iter: 537 loss: 1.11375193e-05
Iter: 538 loss: 1.11407644e-05
Iter: 539 loss: 1.11287636e-05
Iter: 540 loss: 1.11295476e-05
Iter: 541 loss: 1.11239106e-05
Iter: 542 loss: 1.11194986e-05
Iter: 543 loss: 1.11169265e-05
Iter: 544 loss: 1.11145882e-05
Iter: 545 loss: 1.11080371e-05
Iter: 546 loss: 1.11042564e-05
Iter: 547 loss: 1.11011796e-05
Iter: 548 loss: 1.1092071e-05
Iter: 549 loss: 1.11638647e-05
Iter: 550 loss: 1.10911478e-05
Iter: 551 loss: 1.10839101e-05
Iter: 552 loss: 1.1118942e-05
Iter: 553 loss: 1.10825094e-05
Iter: 554 loss: 1.10761139e-05
Iter: 555 loss: 1.10706287e-05
Iter: 556 loss: 1.10686551e-05
Iter: 557 loss: 1.1059843e-05
Iter: 558 loss: 1.10912706e-05
Iter: 559 loss: 1.10574474e-05
Iter: 560 loss: 1.1047603e-05
Iter: 561 loss: 1.10456303e-05
Iter: 562 loss: 1.10392293e-05
Iter: 563 loss: 1.10292049e-05
Iter: 564 loss: 1.11360423e-05
Iter: 565 loss: 1.10288329e-05
Iter: 566 loss: 1.10193268e-05
Iter: 567 loss: 1.1011367e-05
Iter: 568 loss: 1.10084784e-05
Iter: 569 loss: 1.09977518e-05
Iter: 570 loss: 1.10963792e-05
Iter: 571 loss: 1.09971743e-05
Iter: 572 loss: 1.09868597e-05
Iter: 573 loss: 1.10312467e-05
Iter: 574 loss: 1.09848452e-05
Iter: 575 loss: 1.09781249e-05
Iter: 576 loss: 1.09777784e-05
Iter: 577 loss: 1.09743332e-05
Iter: 578 loss: 1.09654775e-05
Iter: 579 loss: 1.10437477e-05
Iter: 580 loss: 1.09639041e-05
Iter: 581 loss: 1.09533721e-05
Iter: 582 loss: 1.10667679e-05
Iter: 583 loss: 1.09529065e-05
Iter: 584 loss: 1.09476441e-05
Iter: 585 loss: 1.09954008e-05
Iter: 586 loss: 1.09471293e-05
Iter: 587 loss: 1.09415723e-05
Iter: 588 loss: 1.09311459e-05
Iter: 589 loss: 1.11634654e-05
Iter: 590 loss: 1.09310704e-05
Iter: 591 loss: 1.09184521e-05
Iter: 592 loss: 1.09727025e-05
Iter: 593 loss: 1.09158327e-05
Iter: 594 loss: 1.0906625e-05
Iter: 595 loss: 1.09097055e-05
Iter: 596 loss: 1.08999957e-05
Iter: 597 loss: 1.08891218e-05
Iter: 598 loss: 1.09817238e-05
Iter: 599 loss: 1.08884806e-05
Iter: 600 loss: 1.0880035e-05
Iter: 601 loss: 1.08719378e-05
Iter: 602 loss: 1.0869986e-05
Iter: 603 loss: 1.08574077e-05
Iter: 604 loss: 1.09999783e-05
Iter: 605 loss: 1.08568029e-05
Iter: 606 loss: 1.08495e-05
Iter: 607 loss: 1.08689128e-05
Iter: 608 loss: 1.08462646e-05
Iter: 609 loss: 1.08404165e-05
Iter: 610 loss: 1.08400809e-05
Iter: 611 loss: 1.08357281e-05
Iter: 612 loss: 1.08311124e-05
Iter: 613 loss: 1.08302629e-05
Iter: 614 loss: 1.08234162e-05
Iter: 615 loss: 1.08313161e-05
Iter: 616 loss: 1.08203658e-05
Iter: 617 loss: 1.08128424e-05
Iter: 618 loss: 1.0885472e-05
Iter: 619 loss: 1.08121876e-05
Iter: 620 loss: 1.08065342e-05
Iter: 621 loss: 1.08166187e-05
Iter: 622 loss: 1.08037193e-05
Iter: 623 loss: 1.07971719e-05
Iter: 624 loss: 1.0786338e-05
Iter: 625 loss: 1.07864907e-05
Iter: 626 loss: 1.07767737e-05
Iter: 627 loss: 1.08927525e-05
Iter: 628 loss: 1.07767355e-05
Iter: 629 loss: 1.07689975e-05
Iter: 630 loss: 1.075816e-05
Iter: 631 loss: 1.0758e-05
Iter: 632 loss: 1.07466094e-05
Iter: 633 loss: 1.08901586e-05
Iter: 634 loss: 1.07467295e-05
Iter: 635 loss: 1.07369488e-05
Iter: 636 loss: 1.07403666e-05
Iter: 637 loss: 1.07306096e-05
Iter: 638 loss: 1.0722255e-05
Iter: 639 loss: 1.07894193e-05
Iter: 640 loss: 1.07220367e-05
Iter: 641 loss: 1.07154501e-05
Iter: 642 loss: 1.07943606e-05
Iter: 643 loss: 1.07158648e-05
Iter: 644 loss: 1.0709311e-05
Iter: 645 loss: 1.07074356e-05
Iter: 646 loss: 1.0703734e-05
Iter: 647 loss: 1.06978914e-05
Iter: 648 loss: 1.07028918e-05
Iter: 649 loss: 1.06941361e-05
Iter: 650 loss: 1.06869556e-05
Iter: 651 loss: 1.07234409e-05
Iter: 652 loss: 1.06854986e-05
Iter: 653 loss: 1.06781499e-05
Iter: 654 loss: 1.07106498e-05
Iter: 655 loss: 1.06765974e-05
Iter: 656 loss: 1.06712014e-05
Iter: 657 loss: 1.066404e-05
Iter: 658 loss: 1.06637754e-05
Iter: 659 loss: 1.06530788e-05
Iter: 660 loss: 1.06968964e-05
Iter: 661 loss: 1.06508905e-05
Iter: 662 loss: 1.06427269e-05
Iter: 663 loss: 1.06448788e-05
Iter: 664 loss: 1.06365633e-05
Iter: 665 loss: 1.06240777e-05
Iter: 666 loss: 1.06703756e-05
Iter: 667 loss: 1.06206262e-05
Iter: 668 loss: 1.06113821e-05
Iter: 669 loss: 1.06425032e-05
Iter: 670 loss: 1.06088737e-05
Iter: 671 loss: 1.05988847e-05
Iter: 672 loss: 1.06113512e-05
Iter: 673 loss: 1.05932322e-05
Iter: 674 loss: 1.05876879e-05
Iter: 675 loss: 1.05874005e-05
Iter: 676 loss: 1.05810086e-05
Iter: 677 loss: 1.05827812e-05
Iter: 678 loss: 1.05762e-05
Iter: 679 loss: 1.05691315e-05
Iter: 680 loss: 1.05747167e-05
Iter: 681 loss: 1.05645895e-05
Iter: 682 loss: 1.05579984e-05
Iter: 683 loss: 1.05841655e-05
Iter: 684 loss: 1.0555972e-05
Iter: 685 loss: 1.05476738e-05
Iter: 686 loss: 1.05610034e-05
Iter: 687 loss: 1.05435211e-05
Iter: 688 loss: 1.05351046e-05
Iter: 689 loss: 1.05724093e-05
Iter: 690 loss: 1.0533704e-05
Iter: 691 loss: 1.05284471e-05
Iter: 692 loss: 1.05216768e-05
Iter: 693 loss: 1.05210438e-05
Iter: 694 loss: 1.05121981e-05
Iter: 695 loss: 1.05904655e-05
Iter: 696 loss: 1.05119116e-05
Iter: 697 loss: 1.05043073e-05
Iter: 698 loss: 1.04971896e-05
Iter: 699 loss: 1.04958581e-05
Iter: 700 loss: 1.04873634e-05
Iter: 701 loss: 1.06076586e-05
Iter: 702 loss: 1.04870214e-05
Iter: 703 loss: 1.04799419e-05
Iter: 704 loss: 1.04756709e-05
Iter: 705 loss: 1.04730898e-05
Iter: 706 loss: 1.04657538e-05
Iter: 707 loss: 1.0465732e-05
Iter: 708 loss: 1.04598494e-05
Iter: 709 loss: 1.0503627e-05
Iter: 710 loss: 1.0459e-05
Iter: 711 loss: 1.04537448e-05
Iter: 712 loss: 1.04467254e-05
Iter: 713 loss: 1.04463033e-05
Iter: 714 loss: 1.04396622e-05
Iter: 715 loss: 1.05160752e-05
Iter: 716 loss: 1.04393139e-05
Iter: 717 loss: 1.04339215e-05
Iter: 718 loss: 1.04448081e-05
Iter: 719 loss: 1.04310948e-05
Iter: 720 loss: 1.04243773e-05
Iter: 721 loss: 1.04302435e-05
Iter: 722 loss: 1.04200253e-05
Iter: 723 loss: 1.04137689e-05
Iter: 724 loss: 1.04261553e-05
Iter: 725 loss: 1.04102292e-05
Iter: 726 loss: 1.04028113e-05
Iter: 727 loss: 1.03955335e-05
Iter: 728 loss: 1.03935663e-05
Iter: 729 loss: 1.0383279e-05
Iter: 730 loss: 1.05134277e-05
Iter: 731 loss: 1.03830243e-05
Iter: 732 loss: 1.03752436e-05
Iter: 733 loss: 1.03742859e-05
Iter: 734 loss: 1.03686434e-05
Iter: 735 loss: 1.03597322e-05
Iter: 736 loss: 1.04081046e-05
Iter: 737 loss: 1.03580751e-05
Iter: 738 loss: 1.03483198e-05
Iter: 739 loss: 1.03630082e-05
Iter: 740 loss: 1.03431594e-05
Iter: 741 loss: 1.03415568e-05
Iter: 742 loss: 1.03382008e-05
Iter: 743 loss: 1.03352259e-05
Iter: 744 loss: 1.03287748e-05
Iter: 745 loss: 1.04086976e-05
Iter: 746 loss: 1.03279217e-05
Iter: 747 loss: 1.03197062e-05
Iter: 748 loss: 1.03552411e-05
Iter: 749 loss: 1.03179818e-05
Iter: 750 loss: 1.03124066e-05
Iter: 751 loss: 1.03637822e-05
Iter: 752 loss: 1.03122784e-05
Iter: 753 loss: 1.03073835e-05
Iter: 754 loss: 1.0300615e-05
Iter: 755 loss: 1.03001985e-05
Iter: 756 loss: 1.02909198e-05
Iter: 757 loss: 1.0338068e-05
Iter: 758 loss: 1.02895519e-05
Iter: 759 loss: 1.02828235e-05
Iter: 760 loss: 1.02759714e-05
Iter: 761 loss: 1.02739414e-05
Iter: 762 loss: 1.02637832e-05
Iter: 763 loss: 1.03696821e-05
Iter: 764 loss: 1.02630675e-05
Iter: 765 loss: 1.02570375e-05
Iter: 766 loss: 1.02570866e-05
Iter: 767 loss: 1.02520971e-05
Iter: 768 loss: 1.02418508e-05
Iter: 769 loss: 1.02788072e-05
Iter: 770 loss: 1.02394097e-05
Iter: 771 loss: 1.02303693e-05
Iter: 772 loss: 1.02505055e-05
Iter: 773 loss: 1.02273298e-05
Iter: 774 loss: 1.02227532e-05
Iter: 775 loss: 1.02218009e-05
Iter: 776 loss: 1.02178929e-05
Iter: 777 loss: 1.02171743e-05
Iter: 778 loss: 1.02144313e-05
Iter: 779 loss: 1.0209742e-05
Iter: 780 loss: 1.02093218e-05
Iter: 781 loss: 1.02056292e-05
Iter: 782 loss: 1.01990554e-05
Iter: 783 loss: 1.02532413e-05
Iter: 784 loss: 1.01988644e-05
Iter: 785 loss: 1.01939595e-05
Iter: 786 loss: 1.02045287e-05
Iter: 787 loss: 1.01917976e-05
Iter: 788 loss: 1.0186689e-05
Iter: 789 loss: 1.01831902e-05
Iter: 790 loss: 1.01813357e-05
Iter: 791 loss: 1.01730338e-05
Iter: 792 loss: 1.02131808e-05
Iter: 793 loss: 1.01714613e-05
Iter: 794 loss: 1.01653386e-05
Iter: 795 loss: 1.01614078e-05
Iter: 796 loss: 1.01583573e-05
Iter: 797 loss: 1.01510195e-05
Iter: 798 loss: 1.02488e-05
Iter: 799 loss: 1.01506621e-05
Iter: 800 loss: 1.01444602e-05
Iter: 801 loss: 1.01344613e-05
Iter: 802 loss: 1.0134183e-05
Iter: 803 loss: 1.01252617e-05
Iter: 804 loss: 1.0125088e-05
Iter: 805 loss: 1.01196019e-05
Iter: 806 loss: 1.01607939e-05
Iter: 807 loss: 1.01196283e-05
Iter: 808 loss: 1.01127198e-05
Iter: 809 loss: 1.01217738e-05
Iter: 810 loss: 1.01099931e-05
Iter: 811 loss: 1.01054702e-05
Iter: 812 loss: 1.01064297e-05
Iter: 813 loss: 1.01020723e-05
Iter: 814 loss: 1.00952584e-05
Iter: 815 loss: 1.01235337e-05
Iter: 816 loss: 1.0094157e-05
Iter: 817 loss: 1.00871603e-05
Iter: 818 loss: 1.01074529e-05
Iter: 819 loss: 1.00851994e-05
Iter: 820 loss: 1.00801717e-05
Iter: 821 loss: 1.00852449e-05
Iter: 822 loss: 1.00776988e-05
Iter: 823 loss: 1.00710949e-05
Iter: 824 loss: 1.00720699e-05
Iter: 825 loss: 1.00657471e-05
Iter: 826 loss: 1.00580164e-05
Iter: 827 loss: 1.00953366e-05
Iter: 828 loss: 1.00562638e-05
Iter: 829 loss: 1.00497218e-05
Iter: 830 loss: 1.00567431e-05
Iter: 831 loss: 1.00458537e-05
Iter: 832 loss: 1.00379511e-05
Iter: 833 loss: 1.00672878e-05
Iter: 834 loss: 1.00360257e-05
Iter: 835 loss: 1.00273483e-05
Iter: 836 loss: 1.00318266e-05
Iter: 837 loss: 1.00218858e-05
Iter: 838 loss: 1.00184552e-05
Iter: 839 loss: 1.00176403e-05
Iter: 840 loss: 1.00134885e-05
Iter: 841 loss: 1.0028205e-05
Iter: 842 loss: 1.00124162e-05
Iter: 843 loss: 1.000886e-05
Iter: 844 loss: 1.00020043e-05
Iter: 845 loss: 1.00022353e-05
Iter: 846 loss: 9.9976e-06
Iter: 847 loss: 1.00649049e-05
Iter: 848 loss: 9.99742497e-06
Iter: 849 loss: 9.99284748e-06
Iter: 850 loss: 9.99387703e-06
Iter: 851 loss: 9.99023e-06
Iter: 852 loss: 9.98387532e-06
Iter: 853 loss: 9.99629083e-06
Iter: 854 loss: 9.9809e-06
Iter: 855 loss: 9.97493771e-06
Iter: 856 loss: 9.9842855e-06
Iter: 857 loss: 9.97221377e-06
Iter: 858 loss: 9.96478775e-06
Iter: 859 loss: 9.97227744e-06
Iter: 860 loss: 9.9606923e-06
Iter: 861 loss: 9.95401115e-06
Iter: 862 loss: 9.9754152e-06
Iter: 863 loss: 9.95220125e-06
Iter: 864 loss: 9.94376569e-06
Iter: 865 loss: 9.94671063e-06
Iter: 866 loss: 9.93776757e-06
Iter: 867 loss: 9.92887453e-06
Iter: 868 loss: 9.97568168e-06
Iter: 869 loss: 9.92752939e-06
Iter: 870 loss: 9.92099467e-06
Iter: 871 loss: 9.96737799e-06
Iter: 872 loss: 9.92009336e-06
Iter: 873 loss: 9.91631623e-06
Iter: 874 loss: 9.91583875e-06
Iter: 875 loss: 9.91297202e-06
Iter: 876 loss: 9.90677381e-06
Iter: 877 loss: 9.99817166e-06
Iter: 878 loss: 9.9071458e-06
Iter: 879 loss: 9.90018361e-06
Iter: 880 loss: 9.93263711e-06
Iter: 881 loss: 9.89904e-06
Iter: 882 loss: 9.89344881e-06
Iter: 883 loss: 9.93743288e-06
Iter: 884 loss: 9.89320779e-06
Iter: 885 loss: 9.88899228e-06
Iter: 886 loss: 9.88441298e-06
Iter: 887 loss: 9.88371539e-06
Iter: 888 loss: 9.87566273e-06
Iter: 889 loss: 9.90073e-06
Iter: 890 loss: 9.8734472e-06
Iter: 891 loss: 9.86724535e-06
Iter: 892 loss: 9.87947351e-06
Iter: 893 loss: 9.8647e-06
Iter: 894 loss: 9.85728548e-06
Iter: 895 loss: 9.86779742e-06
Iter: 896 loss: 9.85347469e-06
Iter: 897 loss: 9.84618782e-06
Iter: 898 loss: 9.85615497e-06
Iter: 899 loss: 9.84225e-06
Iter: 900 loss: 9.83358495e-06
Iter: 901 loss: 9.88828288e-06
Iter: 902 loss: 9.83238e-06
Iter: 903 loss: 9.82711208e-06
Iter: 904 loss: 9.83282553e-06
Iter: 905 loss: 9.82456e-06
Iter: 906 loss: 9.81906578e-06
Iter: 907 loss: 9.81879111e-06
Iter: 908 loss: 9.81474477e-06
Iter: 909 loss: 9.81223457e-06
Iter: 910 loss: 9.81075118e-06
Iter: 911 loss: 9.80564e-06
Iter: 912 loss: 9.81003723e-06
Iter: 913 loss: 9.80280129e-06
Iter: 914 loss: 9.79683136e-06
Iter: 915 loss: 9.8507935e-06
Iter: 916 loss: 9.79611195e-06
Iter: 917 loss: 9.7914035e-06
Iter: 918 loss: 9.79461583e-06
Iter: 919 loss: 9.7886923e-06
Iter: 920 loss: 9.78249591e-06
Iter: 921 loss: 9.78059325e-06
Iter: 922 loss: 9.77721356e-06
Iter: 923 loss: 9.7696684e-06
Iter: 924 loss: 9.82605e-06
Iter: 925 loss: 9.76888623e-06
Iter: 926 loss: 9.76325464e-06
Iter: 927 loss: 9.76174306e-06
Iter: 928 loss: 9.75805051e-06
Iter: 929 loss: 9.7494385e-06
Iter: 930 loss: 9.78903154e-06
Iter: 931 loss: 9.74806881e-06
Iter: 932 loss: 9.74123759e-06
Iter: 933 loss: 9.75203147e-06
Iter: 934 loss: 9.73847818e-06
Iter: 935 loss: 9.73087e-06
Iter: 936 loss: 9.76821684e-06
Iter: 937 loss: 9.72940506e-06
Iter: 938 loss: 9.72627367e-06
Iter: 939 loss: 9.72598264e-06
Iter: 940 loss: 9.72227645e-06
Iter: 941 loss: 9.71503323e-06
Iter: 942 loss: 9.88641477e-06
Iter: 943 loss: 9.71519512e-06
Iter: 944 loss: 9.70939254e-06
Iter: 945 loss: 9.74939394e-06
Iter: 946 loss: 9.70910878e-06
Iter: 947 loss: 9.70482688e-06
Iter: 948 loss: 9.72133785e-06
Iter: 949 loss: 9.70358633e-06
Iter: 950 loss: 9.69807661e-06
Iter: 951 loss: 9.69848497e-06
Iter: 952 loss: 9.69373377e-06
Iter: 953 loss: 9.68813492e-06
Iter: 954 loss: 9.70413e-06
Iter: 955 loss: 9.68616e-06
Iter: 956 loss: 9.680115e-06
Iter: 957 loss: 9.68091354e-06
Iter: 958 loss: 9.67507185e-06
Iter: 959 loss: 9.66817151e-06
Iter: 960 loss: 9.71615736e-06
Iter: 961 loss: 9.66737207e-06
Iter: 962 loss: 9.66167499e-06
Iter: 963 loss: 9.65727941e-06
Iter: 964 loss: 9.65493109e-06
Iter: 965 loss: 9.64795163e-06
Iter: 966 loss: 9.72727e-06
Iter: 967 loss: 9.64791616e-06
Iter: 968 loss: 9.64149e-06
Iter: 969 loss: 9.63501e-06
Iter: 970 loss: 9.63390812e-06
Iter: 971 loss: 9.63320144e-06
Iter: 972 loss: 9.62973354e-06
Iter: 973 loss: 9.6261665e-06
Iter: 974 loss: 9.62293052e-06
Iter: 975 loss: 9.62197191e-06
Iter: 976 loss: 9.61622391e-06
Iter: 977 loss: 9.6243657e-06
Iter: 978 loss: 9.61342448e-06
Iter: 979 loss: 9.60850684e-06
Iter: 980 loss: 9.65465e-06
Iter: 981 loss: 9.60862326e-06
Iter: 982 loss: 9.60396574e-06
Iter: 983 loss: 9.60729722e-06
Iter: 984 loss: 9.60110901e-06
Iter: 985 loss: 9.59524186e-06
Iter: 986 loss: 9.59408499e-06
Iter: 987 loss: 9.58988858e-06
Iter: 988 loss: 9.58380406e-06
Iter: 989 loss: 9.62625109e-06
Iter: 990 loss: 9.58322562e-06
Iter: 991 loss: 9.57721e-06
Iter: 992 loss: 9.57519478e-06
Iter: 993 loss: 9.57162047e-06
Iter: 994 loss: 9.56516305e-06
Iter: 995 loss: 9.60844409e-06
Iter: 996 loss: 9.56398799e-06
Iter: 997 loss: 9.55776068e-06
Iter: 998 loss: 9.56096483e-06
Iter: 999 loss: 9.5533e-06
Iter: 1000 loss: 9.54628649e-06
Iter: 1001 loss: 9.57325483e-06
Iter: 1002 loss: 9.54492862e-06
Iter: 1003 loss: 9.5376563e-06
Iter: 1004 loss: 9.58169221e-06
Iter: 1005 loss: 9.53685867e-06
Iter: 1006 loss: 9.53081417e-06
Iter: 1007 loss: 9.60452235e-06
Iter: 1008 loss: 9.53095e-06
Iter: 1009 loss: 9.52839764e-06
Iter: 1010 loss: 9.52357823e-06
Iter: 1011 loss: 9.60486796e-06
Iter: 1012 loss: 9.52315713e-06
Iter: 1013 loss: 9.51716902e-06
Iter: 1014 loss: 9.55608448e-06
Iter: 1015 loss: 9.51665606e-06
Iter: 1016 loss: 9.51175298e-06
Iter: 1017 loss: 9.54880124e-06
Iter: 1018 loss: 9.51118818e-06
Iter: 1019 loss: 9.50738831e-06
Iter: 1020 loss: 9.49882087e-06
Iter: 1021 loss: 9.62623199e-06
Iter: 1022 loss: 9.49819332e-06
Iter: 1023 loss: 9.49170499e-06
Iter: 1024 loss: 9.49172136e-06
Iter: 1025 loss: 9.48669367e-06
Iter: 1026 loss: 9.48249181e-06
Iter: 1027 loss: 9.4807383e-06
Iter: 1028 loss: 9.47393e-06
Iter: 1029 loss: 9.53321523e-06
Iter: 1030 loss: 9.47312128e-06
Iter: 1031 loss: 9.46774162e-06
Iter: 1032 loss: 9.46622276e-06
Iter: 1033 loss: 9.46284308e-06
Iter: 1034 loss: 9.45475585e-06
Iter: 1035 loss: 9.50059439e-06
Iter: 1036 loss: 9.45382817e-06
Iter: 1037 loss: 9.44782732e-06
Iter: 1038 loss: 9.4706229e-06
Iter: 1039 loss: 9.4462448e-06
Iter: 1040 loss: 9.44069598e-06
Iter: 1041 loss: 9.52515256e-06
Iter: 1042 loss: 9.44071e-06
Iter: 1043 loss: 9.43803661e-06
Iter: 1044 loss: 9.43577288e-06
Iter: 1045 loss: 9.43509258e-06
Iter: 1046 loss: 9.43102896e-06
Iter: 1047 loss: 9.42940187e-06
Iter: 1048 loss: 9.42732277e-06
Iter: 1049 loss: 9.42103725e-06
Iter: 1050 loss: 9.48189336e-06
Iter: 1051 loss: 9.42074803e-06
Iter: 1052 loss: 9.41611415e-06
Iter: 1053 loss: 9.4143e-06
Iter: 1054 loss: 9.41223425e-06
Iter: 1055 loss: 9.40518657e-06
Iter: 1056 loss: 9.41497274e-06
Iter: 1057 loss: 9.40217251e-06
Iter: 1058 loss: 9.39644087e-06
Iter: 1059 loss: 9.42773659e-06
Iter: 1060 loss: 9.39563142e-06
Iter: 1061 loss: 9.3901217e-06
Iter: 1062 loss: 9.39021265e-06
Iter: 1063 loss: 9.38558696e-06
Iter: 1064 loss: 9.37980167e-06
Iter: 1065 loss: 9.42386396e-06
Iter: 1066 loss: 9.37916047e-06
Iter: 1067 loss: 9.37372e-06
Iter: 1068 loss: 9.37087407e-06
Iter: 1069 loss: 9.36857941e-06
Iter: 1070 loss: 9.36323704e-06
Iter: 1071 loss: 9.44494e-06
Iter: 1072 loss: 9.3629551e-06
Iter: 1073 loss: 9.35968274e-06
Iter: 1074 loss: 9.35956359e-06
Iter: 1075 loss: 9.35700155e-06
Iter: 1076 loss: 9.35259777e-06
Iter: 1077 loss: 9.35295066e-06
Iter: 1078 loss: 9.34836316e-06
Iter: 1079 loss: 9.35151184e-06
Iter: 1080 loss: 9.34567652e-06
Iter: 1081 loss: 9.3392573e-06
Iter: 1082 loss: 9.38194717e-06
Iter: 1083 loss: 9.33874435e-06
Iter: 1084 loss: 9.3334238e-06
Iter: 1085 loss: 9.3417e-06
Iter: 1086 loss: 9.33155206e-06
Iter: 1087 loss: 9.32727562e-06
Iter: 1088 loss: 9.32673e-06
Iter: 1089 loss: 9.32334387e-06
Iter: 1090 loss: 9.31707e-06
Iter: 1091 loss: 9.34246236e-06
Iter: 1092 loss: 9.31557315e-06
Iter: 1093 loss: 9.30997703e-06
Iter: 1094 loss: 9.32332932e-06
Iter: 1095 loss: 9.30818533e-06
Iter: 1096 loss: 9.3015251e-06
Iter: 1097 loss: 9.31117575e-06
Iter: 1098 loss: 9.29853741e-06
Iter: 1099 loss: 9.29291946e-06
Iter: 1100 loss: 9.30604619e-06
Iter: 1101 loss: 9.29078305e-06
Iter: 1102 loss: 9.2832579e-06
Iter: 1103 loss: 9.29780072e-06
Iter: 1104 loss: 9.2797718e-06
Iter: 1105 loss: 9.27986e-06
Iter: 1106 loss: 9.27723886e-06
Iter: 1107 loss: 9.27509e-06
Iter: 1108 loss: 9.26973826e-06
Iter: 1109 loss: 9.35880598e-06
Iter: 1110 loss: 9.26953544e-06
Iter: 1111 loss: 9.26378e-06
Iter: 1112 loss: 9.27925066e-06
Iter: 1113 loss: 9.2616292e-06
Iter: 1114 loss: 9.25776294e-06
Iter: 1115 loss: 9.3087574e-06
Iter: 1116 loss: 9.25784479e-06
Iter: 1117 loss: 9.25367749e-06
Iter: 1118 loss: 9.2522605e-06
Iter: 1119 loss: 9.2502105e-06
Iter: 1120 loss: 9.24567848e-06
Iter: 1121 loss: 9.2624241e-06
Iter: 1122 loss: 9.24450705e-06
Iter: 1123 loss: 9.24046253e-06
Iter: 1124 loss: 9.23626158e-06
Iter: 1125 loss: 9.23497919e-06
Iter: 1126 loss: 9.22912568e-06
Iter: 1127 loss: 9.31231625e-06
Iter: 1128 loss: 9.22905838e-06
Iter: 1129 loss: 9.2252094e-06
Iter: 1130 loss: 9.22207801e-06
Iter: 1131 loss: 9.22091567e-06
Iter: 1132 loss: 9.21448e-06
Iter: 1133 loss: 9.25155473e-06
Iter: 1134 loss: 9.21388164e-06
Iter: 1135 loss: 9.20870571e-06
Iter: 1136 loss: 9.20686e-06
Iter: 1137 loss: 9.2035807e-06
Iter: 1138 loss: 9.20052389e-06
Iter: 1139 loss: 9.19953163e-06
Iter: 1140 loss: 9.19611375e-06
Iter: 1141 loss: 9.20776165e-06
Iter: 1142 loss: 9.19473678e-06
Iter: 1143 loss: 9.19203558e-06
Iter: 1144 loss: 9.18515525e-06
Iter: 1145 loss: 9.30128954e-06
Iter: 1146 loss: 9.18535443e-06
Iter: 1147 loss: 9.18220758e-06
Iter: 1148 loss: 9.18149271e-06
Iter: 1149 loss: 9.17820489e-06
Iter: 1150 loss: 9.18146543e-06
Iter: 1151 loss: 9.176817e-06
Iter: 1152 loss: 9.17243415e-06
Iter: 1153 loss: 9.16718636e-06
Iter: 1154 loss: 9.16668068e-06
Iter: 1155 loss: 9.16024237e-06
Iter: 1156 loss: 9.20551884e-06
Iter: 1157 loss: 9.15978e-06
Iter: 1158 loss: 9.15390501e-06
Iter: 1159 loss: 9.15940109e-06
Iter: 1160 loss: 9.15090914e-06
Iter: 1161 loss: 9.14570228e-06
Iter: 1162 loss: 9.18326077e-06
Iter: 1163 loss: 9.14518205e-06
Iter: 1164 loss: 9.14050725e-06
Iter: 1165 loss: 9.13617896e-06
Iter: 1166 loss: 9.13524673e-06
Iter: 1167 loss: 9.12909854e-06
Iter: 1168 loss: 9.19487229e-06
Iter: 1169 loss: 9.12902942e-06
Iter: 1170 loss: 9.12453106e-06
Iter: 1171 loss: 9.13853819e-06
Iter: 1172 loss: 9.12303585e-06
Iter: 1173 loss: 9.1188258e-06
Iter: 1174 loss: 9.17726629e-06
Iter: 1175 loss: 9.11878942e-06
Iter: 1176 loss: 9.1165075e-06
Iter: 1177 loss: 9.11291318e-06
Iter: 1178 loss: 9.19358354e-06
Iter: 1179 loss: 9.11283769e-06
Iter: 1180 loss: 9.10763811e-06
Iter: 1181 loss: 9.12561063e-06
Iter: 1182 loss: 9.10624567e-06
Iter: 1183 loss: 9.10229755e-06
Iter: 1184 loss: 9.14169686e-06
Iter: 1185 loss: 9.1025031e-06
Iter: 1186 loss: 9.09998926e-06
Iter: 1187 loss: 9.09509e-06
Iter: 1188 loss: 9.18603382e-06
Iter: 1189 loss: 9.09495611e-06
Iter: 1190 loss: 9.08849324e-06
Iter: 1191 loss: 9.11411371e-06
Iter: 1192 loss: 9.08673883e-06
Iter: 1193 loss: 9.08195e-06
Iter: 1194 loss: 9.10275776e-06
Iter: 1195 loss: 9.08120819e-06
Iter: 1196 loss: 9.0765252e-06
Iter: 1197 loss: 9.08055517e-06
Iter: 1198 loss: 9.07321919e-06
Iter: 1199 loss: 9.06814057e-06
Iter: 1200 loss: 9.09346545e-06
Iter: 1201 loss: 9.06732748e-06
Iter: 1202 loss: 9.06204332e-06
Iter: 1203 loss: 9.05839079e-06
Iter: 1204 loss: 9.05643719e-06
Iter: 1205 loss: 9.05093111e-06
Iter: 1206 loss: 9.13142685e-06
Iter: 1207 loss: 9.05062552e-06
Iter: 1208 loss: 9.04653e-06
Iter: 1209 loss: 9.09510527e-06
Iter: 1210 loss: 9.04682747e-06
Iter: 1211 loss: 9.04300123e-06
Iter: 1212 loss: 9.03877844e-06
Iter: 1213 loss: 9.03816e-06
Iter: 1214 loss: 9.03446e-06
Iter: 1215 loss: 9.05676279e-06
Iter: 1216 loss: 9.03429827e-06
Iter: 1217 loss: 9.03020555e-06
Iter: 1218 loss: 9.04455e-06
Iter: 1219 loss: 9.02942247e-06
Iter: 1220 loss: 9.02611737e-06
Iter: 1221 loss: 9.02668216e-06
Iter: 1222 loss: 9.02343072e-06
Iter: 1223 loss: 9.01922613e-06
Iter: 1224 loss: 9.01942258e-06
Iter: 1225 loss: 9.01604835e-06
Iter: 1226 loss: 9.01042495e-06
Iter: 1227 loss: 9.0490721e-06
Iter: 1228 loss: 9.00958094e-06
Iter: 1229 loss: 9.00600116e-06
Iter: 1230 loss: 9.00943269e-06
Iter: 1231 loss: 9.00383202e-06
Iter: 1232 loss: 8.99816223e-06
Iter: 1233 loss: 9.00300165e-06
Iter: 1234 loss: 8.99416e-06
Iter: 1235 loss: 8.98875805e-06
Iter: 1236 loss: 9.01898784e-06
Iter: 1237 loss: 8.98814505e-06
Iter: 1238 loss: 8.98307917e-06
Iter: 1239 loss: 8.98752569e-06
Iter: 1240 loss: 8.98005601e-06
Iter: 1241 loss: 8.97898281e-06
Iter: 1242 loss: 8.97729115e-06
Iter: 1243 loss: 8.97536484e-06
Iter: 1244 loss: 8.9722e-06
Iter: 1245 loss: 8.97176051e-06
Iter: 1246 loss: 8.96806523e-06
Iter: 1247 loss: 8.97155132e-06
Iter: 1248 loss: 8.96566598e-06
Iter: 1249 loss: 8.96231904e-06
Iter: 1250 loss: 9.01192743e-06
Iter: 1251 loss: 8.9619507e-06
Iter: 1252 loss: 8.95990524e-06
Iter: 1253 loss: 8.95619451e-06
Iter: 1254 loss: 8.95621633e-06
Iter: 1255 loss: 8.95079e-06
Iter: 1256 loss: 8.96132e-06
Iter: 1257 loss: 8.94865116e-06
Iter: 1258 loss: 8.94425648e-06
Iter: 1259 loss: 8.95629e-06
Iter: 1260 loss: 8.94275399e-06
Iter: 1261 loss: 8.93757078e-06
Iter: 1262 loss: 8.95052835e-06
Iter: 1263 loss: 8.93562719e-06
Iter: 1264 loss: 8.93083597e-06
Iter: 1265 loss: 8.93892229e-06
Iter: 1266 loss: 8.92859134e-06
Iter: 1267 loss: 8.92252e-06
Iter: 1268 loss: 8.94435652e-06
Iter: 1269 loss: 8.92121807e-06
Iter: 1270 loss: 8.91746458e-06
Iter: 1271 loss: 8.92415483e-06
Iter: 1272 loss: 8.9159621e-06
Iter: 1273 loss: 8.91182935e-06
Iter: 1274 loss: 8.95553057e-06
Iter: 1275 loss: 8.91197396e-06
Iter: 1276 loss: 8.90772753e-06
Iter: 1277 loss: 8.92050048e-06
Iter: 1278 loss: 8.90665433e-06
Iter: 1279 loss: 8.90424235e-06
Iter: 1280 loss: 8.90168e-06
Iter: 1281 loss: 8.90151114e-06
Iter: 1282 loss: 8.89697276e-06
Iter: 1283 loss: 8.92921798e-06
Iter: 1284 loss: 8.89653802e-06
Iter: 1285 loss: 8.89241164e-06
Iter: 1286 loss: 8.89997318e-06
Iter: 1287 loss: 8.89077455e-06
Iter: 1288 loss: 8.88731574e-06
Iter: 1289 loss: 8.88388422e-06
Iter: 1290 loss: 8.88331488e-06
Iter: 1291 loss: 8.87732494e-06
Iter: 1292 loss: 8.9142286e-06
Iter: 1293 loss: 8.87697e-06
Iter: 1294 loss: 8.87248461e-06
Iter: 1295 loss: 8.87820624e-06
Iter: 1296 loss: 8.87038823e-06
Iter: 1297 loss: 8.86416183e-06
Iter: 1298 loss: 8.87606438e-06
Iter: 1299 loss: 8.86152156e-06
Iter: 1300 loss: 8.85710415e-06
Iter: 1301 loss: 8.88583236e-06
Iter: 1302 loss: 8.85644658e-06
Iter: 1303 loss: 8.8519173e-06
Iter: 1304 loss: 8.85249119e-06
Iter: 1305 loss: 8.84892506e-06
Iter: 1306 loss: 8.84521432e-06
Iter: 1307 loss: 8.84505607e-06
Iter: 1308 loss: 8.84086e-06
Iter: 1309 loss: 8.84918245e-06
Iter: 1310 loss: 8.83922621e-06
Iter: 1311 loss: 8.8358629e-06
Iter: 1312 loss: 8.83474422e-06
Iter: 1313 loss: 8.83223584e-06
Iter: 1314 loss: 8.82874883e-06
Iter: 1315 loss: 8.8533252e-06
Iter: 1316 loss: 8.82824861e-06
Iter: 1317 loss: 8.82356926e-06
Iter: 1318 loss: 8.82405766e-06
Iter: 1319 loss: 8.82045151e-06
Iter: 1320 loss: 8.81560754e-06
Iter: 1321 loss: 8.82871427e-06
Iter: 1322 loss: 8.8139e-06
Iter: 1323 loss: 8.80990774e-06
Iter: 1324 loss: 8.8083334e-06
Iter: 1325 loss: 8.80562584e-06
Iter: 1326 loss: 8.79914842e-06
Iter: 1327 loss: 8.84339079e-06
Iter: 1328 loss: 8.79893378e-06
Iter: 1329 loss: 8.79372783e-06
Iter: 1330 loss: 8.79752406e-06
Iter: 1331 loss: 8.79014897e-06
Iter: 1332 loss: 8.78394e-06
Iter: 1333 loss: 8.81740834e-06
Iter: 1334 loss: 8.78319952e-06
Iter: 1335 loss: 8.77891853e-06
Iter: 1336 loss: 8.78217543e-06
Iter: 1337 loss: 8.77617458e-06
Iter: 1338 loss: 8.7693461e-06
Iter: 1339 loss: 8.79110303e-06
Iter: 1340 loss: 8.76766262e-06
Iter: 1341 loss: 8.76370905e-06
Iter: 1342 loss: 8.76315426e-06
Iter: 1343 loss: 8.7613189e-06
Iter: 1344 loss: 8.75677e-06
Iter: 1345 loss: 8.84301062e-06
Iter: 1346 loss: 8.75666501e-06
Iter: 1347 loss: 8.75238493e-06
Iter: 1348 loss: 8.77625644e-06
Iter: 1349 loss: 8.75122623e-06
Iter: 1350 loss: 8.74746456e-06
Iter: 1351 loss: 8.77567709e-06
Iter: 1352 loss: 8.74712532e-06
Iter: 1353 loss: 8.7438566e-06
Iter: 1354 loss: 8.73829049e-06
Iter: 1355 loss: 8.87006263e-06
Iter: 1356 loss: 8.73842237e-06
Iter: 1357 loss: 8.7329081e-06
Iter: 1358 loss: 8.77896309e-06
Iter: 1359 loss: 8.73293084e-06
Iter: 1360 loss: 8.72845885e-06
Iter: 1361 loss: 8.72581586e-06
Iter: 1362 loss: 8.72374585e-06
Iter: 1363 loss: 8.71780594e-06
Iter: 1364 loss: 8.78194078e-06
Iter: 1365 loss: 8.71719294e-06
Iter: 1366 loss: 8.71321845e-06
Iter: 1367 loss: 8.71574866e-06
Iter: 1368 loss: 8.71057273e-06
Iter: 1369 loss: 8.70543499e-06
Iter: 1370 loss: 8.72435157e-06
Iter: 1371 loss: 8.70387703e-06
Iter: 1372 loss: 8.69895666e-06
Iter: 1373 loss: 8.71254633e-06
Iter: 1374 loss: 8.69737232e-06
Iter: 1375 loss: 8.69380165e-06
Iter: 1376 loss: 8.69377436e-06
Iter: 1377 loss: 8.69088e-06
Iter: 1378 loss: 8.6895634e-06
Iter: 1379 loss: 8.68838561e-06
Iter: 1380 loss: 8.68518146e-06
Iter: 1381 loss: 8.68631651e-06
Iter: 1382 loss: 8.68324059e-06
Iter: 1383 loss: 8.67806375e-06
Iter: 1384 loss: 8.70448275e-06
Iter: 1385 loss: 8.6773116e-06
Iter: 1386 loss: 8.67312e-06
Iter: 1387 loss: 8.67494418e-06
Iter: 1388 loss: 8.67003837e-06
Iter: 1389 loss: 8.66547089e-06
Iter: 1390 loss: 8.66259506e-06
Iter: 1391 loss: 8.66120172e-06
Iter: 1392 loss: 8.65381e-06
Iter: 1393 loss: 8.70076565e-06
Iter: 1394 loss: 8.65286802e-06
Iter: 1395 loss: 8.64805588e-06
Iter: 1396 loss: 8.65607944e-06
Iter: 1397 loss: 8.64550293e-06
Iter: 1398 loss: 8.64030699e-06
Iter: 1399 loss: 8.67934432e-06
Iter: 1400 loss: 8.63991136e-06
Iter: 1401 loss: 8.6366781e-06
Iter: 1402 loss: 8.63332e-06
Iter: 1403 loss: 8.63230798e-06
Iter: 1404 loss: 8.62503111e-06
Iter: 1405 loss: 8.6609416e-06
Iter: 1406 loss: 8.62406614e-06
Iter: 1407 loss: 8.62193e-06
Iter: 1408 loss: 8.6211312e-06
Iter: 1409 loss: 8.61869921e-06
Iter: 1410 loss: 8.61375702e-06
Iter: 1411 loss: 8.61368699e-06
Iter: 1412 loss: 8.60972614e-06
Iter: 1413 loss: 8.62328125e-06
Iter: 1414 loss: 8.60855107e-06
Iter: 1415 loss: 8.60373802e-06
Iter: 1416 loss: 8.62978413e-06
Iter: 1417 loss: 8.60355249e-06
Iter: 1418 loss: 8.59816828e-06
Iter: 1419 loss: 8.59436568e-06
Iter: 1420 loss: 8.59242755e-06
Iter: 1421 loss: 8.58807471e-06
Iter: 1422 loss: 8.6133723e-06
Iter: 1423 loss: 8.58723888e-06
Iter: 1424 loss: 8.58341082e-06
Iter: 1425 loss: 8.58146723e-06
Iter: 1426 loss: 8.57935447e-06
Iter: 1427 loss: 8.57454143e-06
Iter: 1428 loss: 8.63743389e-06
Iter: 1429 loss: 8.57440682e-06
Iter: 1430 loss: 8.57113355e-06
Iter: 1431 loss: 8.57055329e-06
Iter: 1432 loss: 8.56845054e-06
Iter: 1433 loss: 8.56317274e-06
Iter: 1434 loss: 8.58342719e-06
Iter: 1435 loss: 8.56137649e-06
Iter: 1436 loss: 8.55672442e-06
Iter: 1437 loss: 8.55837061e-06
Iter: 1438 loss: 8.55295912e-06
Iter: 1439 loss: 8.54998689e-06
Iter: 1440 loss: 8.54939572e-06
Iter: 1441 loss: 8.54609152e-06
Iter: 1442 loss: 8.5576421e-06
Iter: 1443 loss: 8.54533846e-06
Iter: 1444 loss: 8.54330392e-06
Iter: 1445 loss: 8.53818528e-06
Iter: 1446 loss: 8.58509611e-06
Iter: 1447 loss: 8.5374113e-06
Iter: 1448 loss: 8.53224628e-06
Iter: 1449 loss: 8.53216261e-06
Iter: 1450 loss: 8.52842732e-06
Iter: 1451 loss: 8.54398058e-06
Iter: 1452 loss: 8.52749e-06
Iter: 1453 loss: 8.52482844e-06
Iter: 1454 loss: 8.52091671e-06
Iter: 1455 loss: 8.52109315e-06
Iter: 1456 loss: 8.51681398e-06
Iter: 1457 loss: 8.53720667e-06
Iter: 1458 loss: 8.51591449e-06
Iter: 1459 loss: 8.51105e-06
Iter: 1460 loss: 8.51714049e-06
Iter: 1461 loss: 8.50847209e-06
Iter: 1462 loss: 8.50470224e-06
Iter: 1463 loss: 8.5325928e-06
Iter: 1464 loss: 8.50450851e-06
Iter: 1465 loss: 8.5006377e-06
Iter: 1466 loss: 8.49652224e-06
Iter: 1467 loss: 8.49546086e-06
Iter: 1468 loss: 8.49051321e-06
Iter: 1469 loss: 8.55514918e-06
Iter: 1470 loss: 8.49040225e-06
Iter: 1471 loss: 8.48676427e-06
Iter: 1472 loss: 8.49115531e-06
Iter: 1473 loss: 8.48528907e-06
Iter: 1474 loss: 8.48122636e-06
Iter: 1475 loss: 8.48107356e-06
Iter: 1476 loss: 8.47930187e-06
Iter: 1477 loss: 8.47617503e-06
Iter: 1478 loss: 8.4760868e-06
Iter: 1479 loss: 8.47186584e-06
Iter: 1480 loss: 8.47611864e-06
Iter: 1481 loss: 8.46952116e-06
Iter: 1482 loss: 8.46523926e-06
Iter: 1483 loss: 8.52297853e-06
Iter: 1484 loss: 8.46517105e-06
Iter: 1485 loss: 8.46321473e-06
Iter: 1486 loss: 8.46066632e-06
Iter: 1487 loss: 8.46023431e-06
Iter: 1488 loss: 8.45579234e-06
Iter: 1489 loss: 8.45692193e-06
Iter: 1490 loss: 8.45236e-06
Iter: 1491 loss: 8.44814258e-06
Iter: 1492 loss: 8.4816e-06
Iter: 1493 loss: 8.44804526e-06
Iter: 1494 loss: 8.44456918e-06
Iter: 1495 loss: 8.45629074e-06
Iter: 1496 loss: 8.44361693e-06
Iter: 1497 loss: 8.44112219e-06
Iter: 1498 loss: 8.44439455e-06
Iter: 1499 loss: 8.43938869e-06
Iter: 1500 loss: 8.43527869e-06
Iter: 1501 loss: 8.44425631e-06
Iter: 1502 loss: 8.43373527e-06
Iter: 1503 loss: 8.43085672e-06
Iter: 1504 loss: 8.43998896e-06
Iter: 1505 loss: 8.4299e-06
Iter: 1506 loss: 8.42710415e-06
Iter: 1507 loss: 8.42724876e-06
Iter: 1508 loss: 8.42487498e-06
Iter: 1509 loss: 8.42202098e-06
Iter: 1510 loss: 8.42166719e-06
Iter: 1511 loss: 8.41940073e-06
Iter: 1512 loss: 8.42544432e-06
Iter: 1513 loss: 8.41848e-06
Iter: 1514 loss: 8.4158728e-06
Iter: 1515 loss: 8.42924e-06
Iter: 1516 loss: 8.41573546e-06
Iter: 1517 loss: 8.41269502e-06
Iter: 1518 loss: 8.41625297e-06
Iter: 1519 loss: 8.41143628e-06
Iter: 1520 loss: 8.40879e-06
Iter: 1521 loss: 8.40653411e-06
Iter: 1522 loss: 8.4058147e-06
Iter: 1523 loss: 8.40232315e-06
Iter: 1524 loss: 8.43857197e-06
Iter: 1525 loss: 8.40198845e-06
Iter: 1526 loss: 8.39975746e-06
Iter: 1527 loss: 8.40307348e-06
Iter: 1528 loss: 8.39858876e-06
Iter: 1529 loss: 8.39485438e-06
Iter: 1530 loss: 8.4001349e-06
Iter: 1531 loss: 8.39328459e-06
Iter: 1532 loss: 8.39033692e-06
Iter: 1533 loss: 8.40439589e-06
Iter: 1534 loss: 8.38977212e-06
Iter: 1535 loss: 8.38694359e-06
Iter: 1536 loss: 8.39209861e-06
Iter: 1537 loss: 8.38523192e-06
Iter: 1538 loss: 8.38481083e-06
Iter: 1539 loss: 8.38411233e-06
Iter: 1540 loss: 8.38283722e-06
Iter: 1541 loss: 8.38113738e-06
Iter: 1542 loss: 8.38116102e-06
Iter: 1543 loss: 8.37919742e-06
Iter: 1544 loss: 8.38102187e-06
Iter: 1545 loss: 8.37775224e-06
Iter: 1546 loss: 8.37604057e-06
Iter: 1547 loss: 8.38577216e-06
Iter: 1548 loss: 8.37606694e-06
Iter: 1549 loss: 8.3734e-06
Iter: 1550 loss: 8.37183325e-06
Iter: 1551 loss: 8.37046719e-06
Iter: 1552 loss: 8.36763775e-06
Iter: 1553 loss: 8.38428059e-06
Iter: 1554 loss: 8.36738e-06
Iter: 1555 loss: 8.36526124e-06
Iter: 1556 loss: 8.36359868e-06
Iter: 1557 loss: 8.36301933e-06
Iter: 1558 loss: 8.35992341e-06
Iter: 1559 loss: 8.38974483e-06
Iter: 1560 loss: 8.35987339e-06
Iter: 1561 loss: 8.35734227e-06
Iter: 1562 loss: 8.36019535e-06
Iter: 1563 loss: 8.35676656e-06
Iter: 1564 loss: 8.35366245e-06
Iter: 1565 loss: 8.36156e-06
Iter: 1566 loss: 8.3525465e-06
Iter: 1567 loss: 8.35019273e-06
Iter: 1568 loss: 8.35163519e-06
Iter: 1569 loss: 8.34859111e-06
Iter: 1570 loss: 8.34578896e-06
Iter: 1571 loss: 8.38171763e-06
Iter: 1572 loss: 8.34565617e-06
Iter: 1573 loss: 8.34332241e-06
Iter: 1574 loss: 8.36758954e-06
Iter: 1575 loss: 8.34334e-06
Iter: 1576 loss: 8.34238e-06
Iter: 1577 loss: 8.33972081e-06
Iter: 1578 loss: 8.36367508e-06
Iter: 1579 loss: 8.33954073e-06
Iter: 1580 loss: 8.33649938e-06
Iter: 1581 loss: 8.35742321e-06
Iter: 1582 loss: 8.33618469e-06
Iter: 1583 loss: 8.3337527e-06
Iter: 1584 loss: 8.35028e-06
Iter: 1585 loss: 8.33355898e-06
Iter: 1586 loss: 8.3317027e-06
Iter: 1587 loss: 8.33000377e-06
Iter: 1588 loss: 8.32963542e-06
Iter: 1589 loss: 8.32728529e-06
Iter: 1590 loss: 8.34320781e-06
Iter: 1591 loss: 8.32728256e-06
Iter: 1592 loss: 8.32514343e-06
Iter: 1593 loss: 8.32293881e-06
Iter: 1594 loss: 8.32244e-06
Iter: 1595 loss: 8.31987927e-06
Iter: 1596 loss: 8.31981e-06
Iter: 1597 loss: 8.31774196e-06
Iter: 1598 loss: 8.31645048e-06
Iter: 1599 loss: 8.31584657e-06
Iter: 1600 loss: 8.31271427e-06
Iter: 1601 loss: 8.33094055e-06
Iter: 1602 loss: 8.31184661e-06
Iter: 1603 loss: 8.30978934e-06
Iter: 1604 loss: 8.31503075e-06
Iter: 1605 loss: 8.3088371e-06
Iter: 1606 loss: 8.30745194e-06
Iter: 1607 loss: 8.3072664e-06
Iter: 1608 loss: 8.30609315e-06
Iter: 1609 loss: 8.30507088e-06
Iter: 1610 loss: 8.30458157e-06
Iter: 1611 loss: 8.30292e-06
Iter: 1612 loss: 8.30100271e-06
Iter: 1613 loss: 8.30085901e-06
Iter: 1614 loss: 8.29849068e-06
Iter: 1615 loss: 8.29847249e-06
Iter: 1616 loss: 8.29689e-06
Iter: 1617 loss: 8.29739383e-06
Iter: 1618 loss: 8.29566e-06
Iter: 1619 loss: 8.29327291e-06
Iter: 1620 loss: 8.28969132e-06
Iter: 1621 loss: 8.29017881e-06
Iter: 1622 loss: 8.28602242e-06
Iter: 1623 loss: 8.3220275e-06
Iter: 1624 loss: 8.28557859e-06
Iter: 1625 loss: 8.28277734e-06
Iter: 1626 loss: 8.28757584e-06
Iter: 1627 loss: 8.28111479e-06
Iter: 1628 loss: 8.27844269e-06
Iter: 1629 loss: 8.3035311e-06
Iter: 1630 loss: 8.27834e-06
Iter: 1631 loss: 8.27606709e-06
Iter: 1632 loss: 8.2742763e-06
Iter: 1633 loss: 8.27340409e-06
Iter: 1634 loss: 8.27089e-06
Iter: 1635 loss: 8.30245699e-06
Iter: 1636 loss: 8.27079e-06
Iter: 1637 loss: 8.26931864e-06
Iter: 1638 loss: 8.26941687e-06
Iter: 1639 loss: 8.2679e-06
Iter: 1640 loss: 8.26441465e-06
Iter: 1641 loss: 8.30835052e-06
Iter: 1642 loss: 8.26434734e-06
Iter: 1643 loss: 8.26175528e-06
Iter: 1644 loss: 8.26915948e-06
Iter: 1645 loss: 8.26037103e-06
Iter: 1646 loss: 8.25768802e-06
Iter: 1647 loss: 8.26425185e-06
Iter: 1648 loss: 8.25654e-06
Iter: 1649 loss: 8.25354e-06
Iter: 1650 loss: 8.28739303e-06
Iter: 1651 loss: 8.25374264e-06
Iter: 1652 loss: 8.25234e-06
Iter: 1653 loss: 8.25063216e-06
Iter: 1654 loss: 8.25041298e-06
Iter: 1655 loss: 8.24789e-06
Iter: 1656 loss: 8.25301322e-06
Iter: 1657 loss: 8.24684685e-06
Iter: 1658 loss: 8.24456856e-06
Iter: 1659 loss: 8.24958715e-06
Iter: 1660 loss: 8.24369454e-06
Iter: 1661 loss: 8.24063136e-06
Iter: 1662 loss: 8.24492145e-06
Iter: 1663 loss: 8.23927621e-06
Iter: 1664 loss: 8.2362094e-06
Iter: 1665 loss: 8.25935786e-06
Iter: 1666 loss: 8.23601749e-06
Iter: 1667 loss: 8.23343726e-06
Iter: 1668 loss: 8.23420851e-06
Iter: 1669 loss: 8.23168284e-06
Iter: 1670 loss: 8.23029495e-06
Iter: 1671 loss: 8.23010669e-06
Iter: 1672 loss: 8.22818765e-06
Iter: 1673 loss: 8.22855236e-06
Iter: 1674 loss: 8.2264869e-06
Iter: 1675 loss: 8.22446e-06
Iter: 1676 loss: 8.22696893e-06
Iter: 1677 loss: 8.22372294e-06
Iter: 1678 loss: 8.22176571e-06
Iter: 1679 loss: 8.22234e-06
Iter: 1680 loss: 8.22056245e-06
Iter: 1681 loss: 8.21738286e-06
Iter: 1682 loss: 8.23999835e-06
Iter: 1683 loss: 8.21737558e-06
Iter: 1684 loss: 8.21526555e-06
Iter: 1685 loss: 8.21557842e-06
Iter: 1686 loss: 8.21352842e-06
Iter: 1687 loss: 8.21132107e-06
Iter: 1688 loss: 8.2108927e-06
Iter: 1689 loss: 8.20865262e-06
Iter: 1690 loss: 8.20588866e-06
Iter: 1691 loss: 8.23578557e-06
Iter: 1692 loss: 8.20558671e-06
Iter: 1693 loss: 8.20368223e-06
Iter: 1694 loss: 8.20206424e-06
Iter: 1695 loss: 8.20135301e-06
Iter: 1696 loss: 8.19867364e-06
Iter: 1697 loss: 8.23809387e-06
Iter: 1698 loss: 8.1985072e-06
Iter: 1699 loss: 8.19693378e-06
Iter: 1700 loss: 8.19696288e-06
Iter: 1701 loss: 8.19544e-06
Iter: 1702 loss: 8.19264096e-06
Iter: 1703 loss: 8.19999877e-06
Iter: 1704 loss: 8.19154229e-06
Iter: 1705 loss: 8.19100933e-06
Iter: 1706 loss: 8.19025263e-06
Iter: 1707 loss: 8.18916578e-06
Iter: 1708 loss: 8.18718127e-06
Iter: 1709 loss: 8.20951573e-06
Iter: 1710 loss: 8.1871467e-06
Iter: 1711 loss: 8.1843009e-06
Iter: 1712 loss: 8.19059369e-06
Iter: 1713 loss: 8.183305e-06
Iter: 1714 loss: 8.18113585e-06
Iter: 1715 loss: 8.19280649e-06
Iter: 1716 loss: 8.18106855e-06
Iter: 1717 loss: 8.17776527e-06
Iter: 1718 loss: 8.17952514e-06
Iter: 1719 loss: 8.17577e-06
Iter: 1720 loss: 8.17376895e-06
Iter: 1721 loss: 8.18060835e-06
Iter: 1722 loss: 8.17317232e-06
Iter: 1723 loss: 8.17073487e-06
Iter: 1724 loss: 8.16744e-06
Iter: 1725 loss: 8.16748434e-06
Iter: 1726 loss: 8.16319334e-06
Iter: 1727 loss: 8.18988e-06
Iter: 1728 loss: 8.16262127e-06
Iter: 1729 loss: 8.15862222e-06
Iter: 1730 loss: 8.16613283e-06
Iter: 1731 loss: 8.15679505e-06
Iter: 1732 loss: 8.1541657e-06
Iter: 1733 loss: 8.17476757e-06
Iter: 1734 loss: 8.15331259e-06
Iter: 1735 loss: 8.15033945e-06
Iter: 1736 loss: 8.15171825e-06
Iter: 1737 loss: 8.14844407e-06
Iter: 1738 loss: 8.14772284e-06
Iter: 1739 loss: 8.14695341e-06
Iter: 1740 loss: 8.1454391e-06
Iter: 1741 loss: 8.14528175e-06
Iter: 1742 loss: 8.1442131e-06
Iter: 1743 loss: 8.14244777e-06
Iter: 1744 loss: 8.13858242e-06
Iter: 1745 loss: 8.19941761e-06
Iter: 1746 loss: 8.13849147e-06
Iter: 1747 loss: 8.13486531e-06
Iter: 1748 loss: 8.18384069e-06
Iter: 1749 loss: 8.13494171e-06
Iter: 1750 loss: 8.13242e-06
Iter: 1751 loss: 8.14235136e-06
Iter: 1752 loss: 8.13117731e-06
Iter: 1753 loss: 8.12818053e-06
Iter: 1754 loss: 8.12745657e-06
Iter: 1755 loss: 8.12550843e-06
Iter: 1756 loss: 8.12196595e-06
Iter: 1757 loss: 8.12630424e-06
Iter: 1758 loss: 8.12026883e-06
Iter: 1759 loss: 8.1152466e-06
Iter: 1760 loss: 8.12195594e-06
Iter: 1761 loss: 8.11270547e-06
Iter: 1762 loss: 8.10836445e-06
Iter: 1763 loss: 8.13222778e-06
Iter: 1764 loss: 8.10784877e-06
Iter: 1765 loss: 8.10354686e-06
Iter: 1766 loss: 8.10716119e-06
Iter: 1767 loss: 8.10068377e-06
Iter: 1768 loss: 8.09677294e-06
Iter: 1769 loss: 8.12960752e-06
Iter: 1770 loss: 8.09654284e-06
Iter: 1771 loss: 8.0928e-06
Iter: 1772 loss: 8.09839912e-06
Iter: 1773 loss: 8.09103403e-06
Iter: 1774 loss: 8.08741606e-06
Iter: 1775 loss: 8.0873524e-06
Iter: 1776 loss: 8.08599725e-06
Iter: 1777 loss: 8.08207733e-06
Iter: 1778 loss: 8.11262908e-06
Iter: 1779 loss: 8.08128152e-06
Iter: 1780 loss: 8.07630749e-06
Iter: 1781 loss: 8.10759138e-06
Iter: 1782 loss: 8.07602373e-06
Iter: 1783 loss: 8.07389551e-06
Iter: 1784 loss: 8.0978989e-06
Iter: 1785 loss: 8.0739137e-06
Iter: 1786 loss: 8.07153083e-06
Iter: 1787 loss: 8.06802927e-06
Iter: 1788 loss: 8.06771914e-06
Iter: 1789 loss: 8.06304161e-06
Iter: 1790 loss: 8.0808768e-06
Iter: 1791 loss: 8.06189928e-06
Iter: 1792 loss: 8.05844866e-06
Iter: 1793 loss: 8.05754553e-06
Iter: 1794 loss: 8.05508898e-06
Iter: 1795 loss: 8.05034324e-06
Iter: 1796 loss: 8.08605728e-06
Iter: 1797 loss: 8.05014406e-06
Iter: 1798 loss: 8.04563479e-06
Iter: 1799 loss: 8.04503725e-06
Iter: 1800 loss: 8.0422069e-06
Iter: 1801 loss: 8.03699e-06
Iter: 1802 loss: 8.08858e-06
Iter: 1803 loss: 8.03689181e-06
Iter: 1804 loss: 8.03311377e-06
Iter: 1805 loss: 8.04501633e-06
Iter: 1806 loss: 8.03211697e-06
Iter: 1807 loss: 8.02924842e-06
Iter: 1808 loss: 8.06852222e-06
Iter: 1809 loss: 8.02917475e-06
Iter: 1810 loss: 8.02696286e-06
Iter: 1811 loss: 8.02527e-06
Iter: 1812 loss: 8.02412615e-06
Iter: 1813 loss: 8.0218e-06
Iter: 1814 loss: 8.02079649e-06
Iter: 1815 loss: 8.01923306e-06
Iter: 1816 loss: 8.01555325e-06
Iter: 1817 loss: 8.03760122e-06
Iter: 1818 loss: 8.01483839e-06
Iter: 1819 loss: 8.01129863e-06
Iter: 1820 loss: 8.03022249e-06
Iter: 1821 loss: 8.01108763e-06
Iter: 1822 loss: 8.00728503e-06
Iter: 1823 loss: 8.00432281e-06
Iter: 1824 loss: 8.00343696e-06
Iter: 1825 loss: 8.00018097e-06
Iter: 1826 loss: 8.02073191e-06
Iter: 1827 loss: 7.99992631e-06
Iter: 1828 loss: 7.99607187e-06
Iter: 1829 loss: 7.99043482e-06
Iter: 1830 loss: 7.99037116e-06
Iter: 1831 loss: 7.9847523e-06
Iter: 1832 loss: 8.03723742e-06
Iter: 1833 loss: 7.98438577e-06
Iter: 1834 loss: 7.97920075e-06
Iter: 1835 loss: 7.99116151e-06
Iter: 1836 loss: 7.97716893e-06
Iter: 1837 loss: 7.97374e-06
Iter: 1838 loss: 7.99503141e-06
Iter: 1839 loss: 7.97319808e-06
Iter: 1840 loss: 7.97006396e-06
Iter: 1841 loss: 8.00755e-06
Iter: 1842 loss: 7.96984e-06
Iter: 1843 loss: 7.96700897e-06
Iter: 1844 loss: 7.96869608e-06
Iter: 1845 loss: 7.9649e-06
Iter: 1846 loss: 7.96260338e-06
Iter: 1847 loss: 7.96001586e-06
Iter: 1848 loss: 7.95958567e-06
Iter: 1849 loss: 7.95437336e-06
Iter: 1850 loss: 7.96183485e-06
Iter: 1851 loss: 7.95218057e-06
Iter: 1852 loss: 7.94802691e-06
Iter: 1853 loss: 7.94786865e-06
Iter: 1854 loss: 7.94507469e-06
Iter: 1855 loss: 7.94462e-06
Iter: 1856 loss: 7.94270727e-06
Iter: 1857 loss: 7.93901381e-06
Iter: 1858 loss: 7.94139669e-06
Iter: 1859 loss: 7.93656e-06
Iter: 1860 loss: 7.93208619e-06
Iter: 1861 loss: 7.94280822e-06
Iter: 1862 loss: 7.93035906e-06
Iter: 1863 loss: 7.92582341e-06
Iter: 1864 loss: 7.94589869e-06
Iter: 1865 loss: 7.92477567e-06
Iter: 1866 loss: 7.92158517e-06
Iter: 1867 loss: 7.92453284e-06
Iter: 1868 loss: 7.91983e-06
Iter: 1869 loss: 7.91496586e-06
Iter: 1870 loss: 7.92763876e-06
Iter: 1871 loss: 7.91298135e-06
Iter: 1872 loss: 7.91084312e-06
Iter: 1873 loss: 7.91039292e-06
Iter: 1874 loss: 7.90761078e-06
Iter: 1875 loss: 7.91092225e-06
Iter: 1876 loss: 7.90639569e-06
Iter: 1877 loss: 7.90329068e-06
Iter: 1878 loss: 7.90402555e-06
Iter: 1879 loss: 7.90123886e-06
Iter: 1880 loss: 7.89823116e-06
Iter: 1881 loss: 7.89833393e-06
Iter: 1882 loss: 7.89560454e-06
Iter: 1883 loss: 7.89171099e-06
Iter: 1884 loss: 7.92829087e-06
Iter: 1885 loss: 7.89152455e-06
Iter: 1886 loss: 7.88896705e-06
Iter: 1887 loss: 7.90513695e-06
Iter: 1888 loss: 7.88870238e-06
Iter: 1889 loss: 7.88639227e-06
Iter: 1890 loss: 7.88097e-06
Iter: 1891 loss: 7.92588799e-06
Iter: 1892 loss: 7.88002581e-06
Iter: 1893 loss: 7.87429235e-06
Iter: 1894 loss: 7.95620326e-06
Iter: 1895 loss: 7.87420322e-06
Iter: 1896 loss: 7.87036697e-06
Iter: 1897 loss: 7.8722569e-06
Iter: 1898 loss: 7.86709461e-06
Iter: 1899 loss: 7.86265e-06
Iter: 1900 loss: 7.8855046e-06
Iter: 1901 loss: 7.86197779e-06
Iter: 1902 loss: 7.85764314e-06
Iter: 1903 loss: 7.86604051e-06
Iter: 1904 loss: 7.85584325e-06
Iter: 1905 loss: 7.85285556e-06
Iter: 1906 loss: 7.89754449e-06
Iter: 1907 loss: 7.85239081e-06
Iter: 1908 loss: 7.85046e-06
Iter: 1909 loss: 7.87962e-06
Iter: 1910 loss: 7.85048724e-06
Iter: 1911 loss: 7.84888744e-06
Iter: 1912 loss: 7.84564872e-06
Iter: 1913 loss: 7.88171565e-06
Iter: 1914 loss: 7.84497843e-06
Iter: 1915 loss: 7.84039457e-06
Iter: 1916 loss: 7.85857083e-06
Iter: 1917 loss: 7.8392768e-06
Iter: 1918 loss: 7.83563701e-06
Iter: 1919 loss: 7.84211807e-06
Iter: 1920 loss: 7.8342855e-06
Iter: 1921 loss: 7.82973075e-06
Iter: 1922 loss: 7.85709562e-06
Iter: 1923 loss: 7.82925417e-06
Iter: 1924 loss: 7.8261246e-06
Iter: 1925 loss: 7.82765e-06
Iter: 1926 loss: 7.82419829e-06
Iter: 1927 loss: 7.8199846e-06
Iter: 1928 loss: 7.81816834e-06
Iter: 1929 loss: 7.81601193e-06
Iter: 1930 loss: 7.81247e-06
Iter: 1931 loss: 7.85622615e-06
Iter: 1932 loss: 7.81241761e-06
Iter: 1933 loss: 7.80877144e-06
Iter: 1934 loss: 7.80502069e-06
Iter: 1935 loss: 7.80408482e-06
Iter: 1936 loss: 7.80017945e-06
Iter: 1937 loss: 7.85147586e-06
Iter: 1938 loss: 7.80018217e-06
Iter: 1939 loss: 7.79699621e-06
Iter: 1940 loss: 7.80407936e-06
Iter: 1941 loss: 7.79521088e-06
Iter: 1942 loss: 7.79381298e-06
Iter: 1943 loss: 7.79320544e-06
Iter: 1944 loss: 7.79148104e-06
Iter: 1945 loss: 7.78858612e-06
Iter: 1946 loss: 7.84705117e-06
Iter: 1947 loss: 7.78858885e-06
Iter: 1948 loss: 7.78449703e-06
Iter: 1949 loss: 7.79149832e-06
Iter: 1950 loss: 7.78213689e-06
Iter: 1951 loss: 7.77952937e-06
Iter: 1952 loss: 7.7921386e-06
Iter: 1953 loss: 7.77903642e-06
Iter: 1954 loss: 7.77594869e-06
Iter: 1955 loss: 7.78706271e-06
Iter: 1956 loss: 7.77505375e-06
Iter: 1957 loss: 7.77145215e-06
Iter: 1958 loss: 7.77302193e-06
Iter: 1959 loss: 7.76896377e-06
Iter: 1960 loss: 7.76524939e-06
Iter: 1961 loss: 7.77049354e-06
Iter: 1962 loss: 7.76324941e-06
Iter: 1963 loss: 7.75931221e-06
Iter: 1964 loss: 7.76796514e-06
Iter: 1965 loss: 7.75715489e-06
Iter: 1966 loss: 7.75334411e-06
Iter: 1967 loss: 7.76180423e-06
Iter: 1968 loss: 7.75181434e-06
Iter: 1969 loss: 7.74694672e-06
Iter: 1970 loss: 7.76578781e-06
Iter: 1971 loss: 7.74605815e-06
Iter: 1972 loss: 7.74294949e-06
Iter: 1973 loss: 7.74567343e-06
Iter: 1974 loss: 7.74063301e-06
Iter: 1975 loss: 7.7375862e-06
Iter: 1976 loss: 7.7373852e-06
Iter: 1977 loss: 7.7343675e-06
Iter: 1978 loss: 7.74512e-06
Iter: 1979 loss: 7.73362808e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi3/300_100_100_100_1
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.final/output120/f1_psi0_phi0
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi0
+ date
Sun Nov  8 17:23:33 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -1 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2303756c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f230376d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2303756ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2303789f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22d726f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22d726f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22d71996a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22d7199bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22d726f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22d717ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b0224268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b01ebc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b02448c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b0244ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b011c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b011cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b00a7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b00db598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b00a7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b01ca9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b01cabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b01cab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22903ed7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2290403f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22904076a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b016f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2290407c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2290324620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2290324268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22903248c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f229037a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b007ed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b007ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f22b009f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f229021ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f229022ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.37270463
test_loss: 0.37314627
train_loss: 0.37946966
test_loss: 0.37300387
train_loss: 0.36978266
test_loss: 0.37289575
train_loss: 0.37715995
test_loss: 0.37268007
train_loss: 0.37503254
test_loss: 0.37245128
train_loss: 0.37484428
test_loss: 0.37220368
train_loss: 0.36921898
test_loss: 0.37188601
train_loss: 0.38095737
test_loss: 0.37160623
train_loss: 0.3743654
test_loss: 0.3712455
train_loss: 0.37057087
test_loss: 0.37085038
train_loss: 0.37037247
test_loss: 0.37042922
train_loss: 0.3728039
test_loss: 0.37002206
train_loss: 0.3713717
test_loss: 0.36953205
train_loss: 0.37146372
test_loss: 0.36899737
train_loss: 0.3750986
test_loss: 0.36844376
train_loss: 0.37095532
test_loss: 0.3679239
train_loss: 0.36510545
test_loss: 0.36729264
train_loss: 0.3713202
test_loss: 0.366657
train_loss: 0.3665481
test_loss: 0.36596966
train_loss: 0.3707743
test_loss: 0.36524415
train_loss: 0.36075416
test_loss: 0.36448127
train_loss: 0.35631096
test_loss: 0.3636435
train_loss: 0.3709425
test_loss: 0.36279547
train_loss: 0.36084622
test_loss: 0.3619324
train_loss: 0.35808384
test_loss: 0.3609453
train_loss: 0.3682801
test_loss: 0.35994798
train_loss: 0.3653556
test_loss: 0.3588503
train_loss: 0.36278266
test_loss: 0.35776067
train_loss: 0.35753503
test_loss: 0.35660186
train_loss: 0.35718933
test_loss: 0.35537702
train_loss: 0.3599599
test_loss: 0.35406935
train_loss: 0.35546535
test_loss: 0.35272294
train_loss: 0.35604775
test_loss: 0.35133085
train_loss: 0.34696284
test_loss: 0.3497957
train_loss: 0.34886673
test_loss: 0.3482796
train_loss: 0.34879857
test_loss: 0.34662497
train_loss: 0.34116235
test_loss: 0.34491518
train_loss: 0.35392004
test_loss: 0.3431374
train_loss: 0.34054905
test_loss: 0.34124094
train_loss: 0.34076422
test_loss: 0.339265
train_loss: 0.339289
test_loss: 0.33719328
train_loss: 0.33222416
test_loss: 0.3350334
train_loss: 0.3289744
test_loss: 0.33276725
train_loss: 0.3269915
test_loss: 0.3303908
train_loss: 0.32873008
test_loss: 0.32788917
train_loss: 0.32400465
test_loss: 0.32528675
train_loss: 0.3279861
test_loss: 0.32256946
train_loss: 0.32910895
test_loss: 0.31969172
train_loss: 0.31512338
test_loss: 0.316723
train_loss: 0.31548962
test_loss: 0.31359488
train_loss: 0.31071475
test_loss: 0.31029585
train_loss: 0.30718592
test_loss: 0.30687276
train_loss: 0.29660416
test_loss: 0.30325824
train_loss: 0.30033916
test_loss: 0.29943767
train_loss: 0.2949737
test_loss: 0.2955373
train_loss: 0.2930785
test_loss: 0.29136544
train_loss: 0.28886437
test_loss: 0.28699535
train_loss: 0.27940226
test_loss: 0.28240258
train_loss: 0.28391457
test_loss: 0.2775863
train_loss: 0.27738613
test_loss: 0.27249253
train_loss: 0.2640203
test_loss: 0.26711273
train_loss: 0.26424772
test_loss: 0.26145384
train_loss: 0.2543975
test_loss: 0.25545046
train_loss: 0.25538108
test_loss: 0.249088
train_loss: 0.2411034
test_loss: 0.2423367
train_loss: 0.2418533
test_loss: 0.23517904
train_loss: 0.2255502
test_loss: 0.22751707
train_loss: 0.21891299
test_loss: 0.21945463
train_loss: 0.20740393
test_loss: 0.2108873
train_loss: 0.20135193
test_loss: 0.2018353
train_loss: 0.19094476
test_loss: 0.19250472
train_loss: 0.18455747
test_loss: 0.18288535
train_loss: 0.17175443
test_loss: 0.17318705
train_loss: 0.16402082
test_loss: 0.1634303
train_loss: 0.15215047
test_loss: 0.15382153
train_loss: 0.14163527
test_loss: 0.14453371
train_loss: 0.1352878
test_loss: 0.135709
train_loss: 0.12638506
test_loss: 0.12736149
train_loss: 0.11931065
test_loss: 0.11946811
train_loss: 0.111595444
test_loss: 0.11213274
train_loss: 0.10652944
test_loss: 0.10541584
train_loss: 0.09954981
test_loss: 0.09933925
train_loss: 0.09273939
test_loss: 0.09392762
train_loss: 0.08854759
test_loss: 0.08915592
train_loss: 0.08506231
test_loss: 0.084986724
train_loss: 0.08172481
test_loss: 0.08135229
train_loss: 0.07594525
test_loss: 0.07816692
train_loss: 0.07638706
test_loss: 0.07539065
train_loss: 0.072340555
test_loss: 0.072949335
train_loss: 0.07136045
test_loss: 0.07080798
train_loss: 0.06822496
test_loss: 0.068889335
train_loss: 0.06599231
test_loss: 0.06719424
train_loss: 0.06452003
test_loss: 0.06566691
train_loss: 0.062935844
test_loss: 0.06426356
train_loss: 0.0630979
test_loss: 0.06299585
train_loss: 0.061470445
test_loss: 0.061829504
train_loss: 0.06161849
test_loss: 0.06076853
train_loss: 0.059973672
test_loss: 0.05977462
train_loss: 0.058476217
test_loss: 0.05883478
train_loss: 0.057387467
test_loss: 0.057957124
train_loss: 0.05725935
test_loss: 0.05712734
train_loss: 0.05568161
test_loss: 0.05631503
train_loss: 0.053916488
test_loss: 0.055525545
train_loss: 0.054408997
test_loss: 0.0547414
train_loss: 0.053514317
test_loss: 0.053976573
train_loss: 0.05235001
test_loss: 0.053204343
train_loss: 0.05255361
test_loss: 0.0524141
train_loss: 0.052157808
test_loss: 0.051621664
train_loss: 0.05136765
test_loss: 0.050831344
train_loss: 0.050552577
test_loss: 0.050020352
train_loss: 0.049384996
test_loss: 0.04919515
train_loss: 0.048355173
test_loss: 0.048385426
train_loss: 0.047328804
test_loss: 0.047549404
train_loss: 0.04520866
test_loss: 0.046706386
train_loss: 0.04631404
test_loss: 0.04586916
train_loss: 0.044616118
test_loss: 0.045028217
train_loss: 0.043283112
test_loss: 0.044174455
train_loss: 0.04303991
test_loss: 0.043315355
train_loss: 0.04207769
test_loss: 0.04246529
train_loss: 0.042532492
test_loss: 0.0415839
train_loss: 0.0394289
test_loss: 0.040721744
train_loss: 0.039729234
test_loss: 0.039823983
train_loss: 0.03838749
test_loss: 0.038953703
train_loss: 0.03742698
test_loss: 0.038056154
train_loss: 0.036420207
test_loss: 0.037150074
train_loss: 0.03557809
test_loss: 0.036256764
train_loss: 0.03531946
test_loss: 0.035284363
train_loss: 0.034267448
test_loss: 0.034305703
train_loss: 0.033805378
test_loss: 0.033327863
train_loss: 0.03202387
test_loss: 0.032286968
train_loss: 0.031868495
test_loss: 0.031250168
train_loss: 0.029460624
test_loss: 0.030134406
train_loss: 0.028789364
test_loss: 0.028855903
train_loss: 0.027332945
test_loss: 0.027637683
train_loss: 0.026430732
test_loss: 0.026324246
train_loss: 0.02510501
test_loss: 0.025098098
train_loss: 0.023547579
test_loss: 0.023780275
train_loss: 0.022279795
test_loss: 0.02253329
train_loss: 0.020682612
test_loss: 0.021510739
train_loss: 0.020313509
test_loss: 0.020595593
train_loss: 0.019109132
test_loss: 0.019652134
train_loss: 0.018655956
test_loss: 0.018988356
train_loss: 0.018814778
test_loss: 0.018481024
train_loss: 0.018005878
test_loss: 0.018238697
train_loss: 0.017630283
test_loss: 0.017707277
train_loss: 0.017239306
test_loss: 0.017508645
train_loss: 0.017220462
test_loss: 0.01735662
train_loss: 0.0168733
test_loss: 0.017008632
train_loss: 0.01681886
test_loss: 0.016888712
train_loss: 0.01672709
test_loss: 0.016874371
train_loss: 0.01648819
test_loss: 0.016677829
train_loss: 0.016515927
test_loss: 0.016700024
train_loss: 0.016503256
test_loss: 0.016485397
train_loss: 0.016157627
test_loss: 0.016499856
train_loss: 0.015991438
test_loss: 0.016603658
train_loss: 0.015859725
test_loss: 0.016333362
train_loss: 0.016393833
test_loss: 0.016396813
train_loss: 0.016383832
test_loss: 0.016241428
train_loss: 0.01596618
test_loss: 0.01628387
train_loss: 0.016067741
test_loss: 0.016280156
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi0/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -1 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi0/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87720c4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f877217b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f877217bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87720fe268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87720fe400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87720fe9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f877208cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8772068048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8772068378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87720109d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8772068b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8771fdd400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8771fdd8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8771fddd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8771fddb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8771fddf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8771ff8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8771f3c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8771ff8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8771ee7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8771ee7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f874d1fda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f874d1c3598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f874d1c32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f874d1c2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f874d1c2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f874d174ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f874d1742f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f874d144ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87281f8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87281f81e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f872814dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f872814d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87281186a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87280ee598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87280f27b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000515093096
Iter: 2 loss: 0.00168546708
Iter: 3 loss: 0.000505597214
Iter: 4 loss: 0.000502445211
Iter: 5 loss: 0.000505384523
Iter: 6 loss: 0.000500621158
Iter: 7 loss: 0.000497545407
Iter: 8 loss: 0.000490935519
Iter: 9 loss: 0.000595501624
Iter: 10 loss: 0.000490704435
Iter: 11 loss: 0.000488611637
Iter: 12 loss: 0.000487718324
Iter: 13 loss: 0.000486710051
Iter: 14 loss: 0.00048388922
Iter: 15 loss: 0.0004990077
Iter: 16 loss: 0.000483006093
Iter: 17 loss: 0.00048182992
Iter: 18 loss: 0.00048180329
Iter: 19 loss: 0.000480926072
Iter: 20 loss: 0.000483088137
Iter: 21 loss: 0.000480608898
Iter: 22 loss: 0.000479912967
Iter: 23 loss: 0.000479348935
Iter: 24 loss: 0.000479142065
Iter: 25 loss: 0.000478496426
Iter: 26 loss: 0.000477405
Iter: 27 loss: 0.000477402762
Iter: 28 loss: 0.000476278918
Iter: 29 loss: 0.000484096236
Iter: 30 loss: 0.000476172543
Iter: 31 loss: 0.000475063251
Iter: 32 loss: 0.000476010464
Iter: 33 loss: 0.000474406435
Iter: 34 loss: 0.000473317
Iter: 35 loss: 0.000472043321
Iter: 36 loss: 0.000471898238
Iter: 37 loss: 0.000470261
Iter: 38 loss: 0.00047289682
Iter: 39 loss: 0.000469504448
Iter: 40 loss: 0.000467821403
Iter: 41 loss: 0.000474292698
Iter: 42 loss: 0.000467421429
Iter: 43 loss: 0.000469352468
Iter: 44 loss: 0.000466860482
Iter: 45 loss: 0.000466618832
Iter: 46 loss: 0.000466160302
Iter: 47 loss: 0.000475932349
Iter: 48 loss: 0.000466159807
Iter: 49 loss: 0.000465450779
Iter: 50 loss: 0.000465055811
Iter: 51 loss: 0.000464744837
Iter: 52 loss: 0.000464284385
Iter: 53 loss: 0.000467769918
Iter: 54 loss: 0.00046424981
Iter: 55 loss: 0.000463675
Iter: 56 loss: 0.000463699922
Iter: 57 loss: 0.000463221717
Iter: 58 loss: 0.000462903612
Iter: 59 loss: 0.000462900905
Iter: 60 loss: 0.000462593627
Iter: 61 loss: 0.000461947784
Iter: 62 loss: 0.000472709769
Iter: 63 loss: 0.000461932621
Iter: 64 loss: 0.000461335032
Iter: 65 loss: 0.000462751748
Iter: 66 loss: 0.000461118703
Iter: 67 loss: 0.000460574229
Iter: 68 loss: 0.000465132529
Iter: 69 loss: 0.00046054268
Iter: 70 loss: 0.000460091309
Iter: 71 loss: 0.000459574483
Iter: 72 loss: 0.000459510367
Iter: 73 loss: 0.000458829454
Iter: 74 loss: 0.000458605064
Iter: 75 loss: 0.000458211172
Iter: 76 loss: 0.000460755953
Iter: 77 loss: 0.000458046852
Iter: 78 loss: 0.000457887276
Iter: 79 loss: 0.000457589282
Iter: 80 loss: 0.000464331068
Iter: 81 loss: 0.000457589864
Iter: 82 loss: 0.000457116519
Iter: 83 loss: 0.000457502232
Iter: 84 loss: 0.000456834183
Iter: 85 loss: 0.000456527254
Iter: 86 loss: 0.000456672278
Iter: 87 loss: 0.000456319249
Iter: 88 loss: 0.00045595548
Iter: 89 loss: 0.000460631418
Iter: 90 loss: 0.000455953239
Iter: 91 loss: 0.000455772737
Iter: 92 loss: 0.000455649279
Iter: 93 loss: 0.000455584785
Iter: 94 loss: 0.000455235946
Iter: 95 loss: 0.000455576228
Iter: 96 loss: 0.000455040514
Iter: 97 loss: 0.000454789522
Iter: 98 loss: 0.000454370573
Iter: 99 loss: 0.00045436903
Iter: 100 loss: 0.000453882152
Iter: 101 loss: 0.000455644535
Iter: 102 loss: 0.000453758897
Iter: 103 loss: 0.000453424436
Iter: 104 loss: 0.000453785
Iter: 105 loss: 0.00045324
Iter: 106 loss: 0.000452952518
Iter: 107 loss: 0.000452597771
Iter: 108 loss: 0.000452567096
Iter: 109 loss: 0.000452225737
Iter: 110 loss: 0.00045611008
Iter: 111 loss: 0.000452218752
Iter: 112 loss: 0.000452014501
Iter: 113 loss: 0.00045155274
Iter: 114 loss: 0.000457626971
Iter: 115 loss: 0.000451523549
Iter: 116 loss: 0.000451124302
Iter: 117 loss: 0.000451117929
Iter: 118 loss: 0.000450910302
Iter: 119 loss: 0.00045053396
Iter: 120 loss: 0.000459562463
Iter: 121 loss: 0.000450534
Iter: 122 loss: 0.000450197607
Iter: 123 loss: 0.000451144238
Iter: 124 loss: 0.000450089894
Iter: 125 loss: 0.000449848
Iter: 126 loss: 0.000449681
Iter: 127 loss: 0.000449592539
Iter: 128 loss: 0.000449065526
Iter: 129 loss: 0.000450902618
Iter: 130 loss: 0.000448928913
Iter: 131 loss: 0.000448641338
Iter: 132 loss: 0.000449775369
Iter: 133 loss: 0.000448575593
Iter: 134 loss: 0.00044834346
Iter: 135 loss: 0.000448163162
Iter: 136 loss: 0.000448090432
Iter: 137 loss: 0.000447803468
Iter: 138 loss: 0.00044857114
Iter: 139 loss: 0.000447708357
Iter: 140 loss: 0.000447469676
Iter: 141 loss: 0.000449171028
Iter: 142 loss: 0.000447447819
Iter: 143 loss: 0.000447240862
Iter: 144 loss: 0.000447630649
Iter: 145 loss: 0.00044715486
Iter: 146 loss: 0.000446961145
Iter: 147 loss: 0.00044687622
Iter: 148 loss: 0.000446778489
Iter: 149 loss: 0.000446538936
Iter: 150 loss: 0.000446328457
Iter: 151 loss: 0.000446266145
Iter: 152 loss: 0.000446054619
Iter: 153 loss: 0.00044599158
Iter: 154 loss: 0.000445823069
Iter: 155 loss: 0.000446482853
Iter: 156 loss: 0.000445782556
Iter: 157 loss: 0.000445599027
Iter: 158 loss: 0.000445318146
Iter: 159 loss: 0.000445313315
Iter: 160 loss: 0.000445019716
Iter: 161 loss: 0.000446592196
Iter: 162 loss: 0.000444974838
Iter: 163 loss: 0.00044467533
Iter: 164 loss: 0.000444894074
Iter: 165 loss: 0.000444489648
Iter: 166 loss: 0.000444229983
Iter: 167 loss: 0.000447826926
Iter: 168 loss: 0.000444228412
Iter: 169 loss: 0.000444048637
Iter: 170 loss: 0.000443555531
Iter: 171 loss: 0.000446589314
Iter: 172 loss: 0.000443424884
Iter: 173 loss: 0.000442908437
Iter: 174 loss: 0.000444786099
Iter: 175 loss: 0.000442779652
Iter: 176 loss: 0.000442451768
Iter: 177 loss: 0.000442386663
Iter: 178 loss: 0.000442148099
Iter: 179 loss: 0.000442052202
Iter: 180 loss: 0.000441925571
Iter: 181 loss: 0.000441620243
Iter: 182 loss: 0.000441090582
Iter: 183 loss: 0.000441091484
Iter: 184 loss: 0.000440674048
Iter: 185 loss: 0.000440673844
Iter: 186 loss: 0.000440296688
Iter: 187 loss: 0.000442144432
Iter: 188 loss: 0.000440230244
Iter: 189 loss: 0.000439910509
Iter: 190 loss: 0.000441858429
Iter: 191 loss: 0.000439872296
Iter: 192 loss: 0.0004396328
Iter: 193 loss: 0.000439322932
Iter: 194 loss: 0.000439300667
Iter: 195 loss: 0.000438939052
Iter: 196 loss: 0.000440189149
Iter: 197 loss: 0.000438842893
Iter: 198 loss: 0.000438634976
Iter: 199 loss: 0.000438624236
Iter: 200 loss: 0.000438440882
Iter: 201 loss: 0.000438066723
Iter: 202 loss: 0.000445671176
Iter: 203 loss: 0.000438062212
Iter: 204 loss: 0.000437803363
Iter: 205 loss: 0.000438249583
Iter: 206 loss: 0.000437688577
Iter: 207 loss: 0.000437396608
Iter: 208 loss: 0.000437029608
Iter: 209 loss: 0.000436999544
Iter: 210 loss: 0.000436588132
Iter: 211 loss: 0.000436294038
Iter: 212 loss: 0.000436150411
Iter: 213 loss: 0.000435702852
Iter: 214 loss: 0.000437165902
Iter: 215 loss: 0.000435574882
Iter: 216 loss: 0.000435303431
Iter: 217 loss: 0.000435472641
Iter: 218 loss: 0.000435129099
Iter: 219 loss: 0.000434725109
Iter: 220 loss: 0.000434721
Iter: 221 loss: 0.000434492162
Iter: 222 loss: 0.000435212743
Iter: 223 loss: 0.000434427377
Iter: 224 loss: 0.000434253248
Iter: 225 loss: 0.000433858921
Iter: 226 loss: 0.000439128547
Iter: 227 loss: 0.000433836249
Iter: 228 loss: 0.000433478737
Iter: 229 loss: 0.000437913754
Iter: 230 loss: 0.000433474925
Iter: 231 loss: 0.000433230685
Iter: 232 loss: 0.000433229085
Iter: 233 loss: 0.000433114066
Iter: 234 loss: 0.000434851332
Iter: 235 loss: 0.00043311427
Iter: 236 loss: 0.000433000911
Iter: 237 loss: 0.000433127279
Iter: 238 loss: 0.000432939152
Iter: 239 loss: 0.000432805158
Iter: 240 loss: 0.00043252614
Iter: 241 loss: 0.000436884526
Iter: 242 loss: 0.000432517583
Iter: 243 loss: 0.000432297675
Iter: 244 loss: 0.000431953871
Iter: 245 loss: 0.000431951135
Iter: 246 loss: 0.000431481807
Iter: 247 loss: 0.000432459754
Iter: 248 loss: 0.000431295717
Iter: 249 loss: 0.000431245368
Iter: 250 loss: 0.00043104752
Iter: 251 loss: 0.000430878135
Iter: 252 loss: 0.000431307446
Iter: 253 loss: 0.000430818152
Iter: 254 loss: 0.000430531189
Iter: 255 loss: 0.000431512279
Iter: 256 loss: 0.000430455082
Iter: 257 loss: 0.000430173124
Iter: 258 loss: 0.000430488784
Iter: 259 loss: 0.000430019572
Iter: 260 loss: 0.000429770618
Iter: 261 loss: 0.000430150365
Iter: 262 loss: 0.000429642067
Iter: 263 loss: 0.000429473293
Iter: 264 loss: 0.000429289852
Iter: 265 loss: 0.000429262116
Iter: 266 loss: 0.000429039821
Iter: 267 loss: 0.000429839
Iter: 268 loss: 0.000428982778
Iter: 269 loss: 0.000428700703
Iter: 270 loss: 0.000428458181
Iter: 271 loss: 0.000428381027
Iter: 272 loss: 0.000427883468
Iter: 273 loss: 0.000429696287
Iter: 274 loss: 0.000427757564
Iter: 275 loss: 0.000427498686
Iter: 276 loss: 0.000427409192
Iter: 277 loss: 0.000427262625
Iter: 278 loss: 0.00042677985
Iter: 279 loss: 0.000426475424
Iter: 280 loss: 0.00042628558
Iter: 281 loss: 0.000425848
Iter: 282 loss: 0.000427503081
Iter: 283 loss: 0.000425733102
Iter: 284 loss: 0.000425400794
Iter: 285 loss: 0.000426424784
Iter: 286 loss: 0.000425312872
Iter: 287 loss: 0.000425008242
Iter: 288 loss: 0.000425004109
Iter: 289 loss: 0.000424718863
Iter: 290 loss: 0.000427004474
Iter: 291 loss: 0.000424699596
Iter: 292 loss: 0.000424572674
Iter: 293 loss: 0.000424239115
Iter: 294 loss: 0.000426495622
Iter: 295 loss: 0.000424163125
Iter: 296 loss: 0.000423801219
Iter: 297 loss: 0.000424277212
Iter: 298 loss: 0.000423617981
Iter: 299 loss: 0.000423348189
Iter: 300 loss: 0.000423694466
Iter: 301 loss: 0.000423215417
Iter: 302 loss: 0.000422875193
Iter: 303 loss: 0.000424117345
Iter: 304 loss: 0.000422790094
Iter: 305 loss: 0.000422429061
Iter: 306 loss: 0.000423771096
Iter: 307 loss: 0.000422339421
Iter: 308 loss: 0.00042200982
Iter: 309 loss: 0.000421692908
Iter: 310 loss: 0.00042161977
Iter: 311 loss: 0.000421023724
Iter: 312 loss: 0.000422782963
Iter: 313 loss: 0.000420819822
Iter: 314 loss: 0.000420585508
Iter: 315 loss: 0.000423916674
Iter: 316 loss: 0.000420584169
Iter: 317 loss: 0.000420459168
Iter: 318 loss: 0.000420345779
Iter: 319 loss: 0.000420310884
Iter: 320 loss: 0.000420040131
Iter: 321 loss: 0.000420649885
Iter: 322 loss: 0.000419939868
Iter: 323 loss: 0.000420190307
Iter: 324 loss: 0.000419833814
Iter: 325 loss: 0.000419744203
Iter: 326 loss: 0.00041981184
Iter: 327 loss: 0.000419689
Iter: 328 loss: 0.000419611577
Iter: 329 loss: 0.000419475778
Iter: 330 loss: 0.000419475749
Iter: 331 loss: 0.000419221848
Iter: 332 loss: 0.000419380929
Iter: 333 loss: 0.000419059652
Iter: 334 loss: 0.00041874568
Iter: 335 loss: 0.000419553049
Iter: 336 loss: 0.00041863849
Iter: 337 loss: 0.000418448588
Iter: 338 loss: 0.000418276846
Iter: 339 loss: 0.000418228505
Iter: 340 loss: 0.000417739677
Iter: 341 loss: 0.000420591561
Iter: 342 loss: 0.000417676143
Iter: 343 loss: 0.000417487696
Iter: 344 loss: 0.000417227915
Iter: 345 loss: 0.000417215459
Iter: 346 loss: 0.00041704037
Iter: 347 loss: 0.000417963834
Iter: 348 loss: 0.000417010975
Iter: 349 loss: 0.000416896422
Iter: 350 loss: 0.000416548806
Iter: 351 loss: 0.000417422969
Iter: 352 loss: 0.000416354189
Iter: 353 loss: 0.000415687333
Iter: 354 loss: 0.000416298455
Iter: 355 loss: 0.00041530421
Iter: 356 loss: 0.000415585382
Iter: 357 loss: 0.00041510578
Iter: 358 loss: 0.000414964365
Iter: 359 loss: 0.000415633869
Iter: 360 loss: 0.000414939539
Iter: 361 loss: 0.000414780923
Iter: 362 loss: 0.000414628303
Iter: 363 loss: 0.00041459463
Iter: 364 loss: 0.000414379407
Iter: 365 loss: 0.000414454727
Iter: 366 loss: 0.00041423121
Iter: 367 loss: 0.000413996604
Iter: 368 loss: 0.000413662
Iter: 369 loss: 0.000413649861
Iter: 370 loss: 0.000413337548
Iter: 371 loss: 0.000414182956
Iter: 372 loss: 0.000413235975
Iter: 373 loss: 0.00041310591
Iter: 374 loss: 0.000413131813
Iter: 375 loss: 0.000413007889
Iter: 376 loss: 0.000412811263
Iter: 377 loss: 0.000412710331
Iter: 378 loss: 0.000412617286
Iter: 379 loss: 0.000412225403
Iter: 380 loss: 0.000412093243
Iter: 381 loss: 0.00041187054
Iter: 382 loss: 0.000411536719
Iter: 383 loss: 0.000411291432
Iter: 384 loss: 0.000411175803
Iter: 385 loss: 0.000411939196
Iter: 386 loss: 0.000411014713
Iter: 387 loss: 0.000410867506
Iter: 388 loss: 0.00041086768
Iter: 389 loss: 0.000410728098
Iter: 390 loss: 0.000411728222
Iter: 391 loss: 0.000410714973
Iter: 392 loss: 0.000410623848
Iter: 393 loss: 0.000411419169
Iter: 394 loss: 0.000410618231
Iter: 395 loss: 0.000410558016
Iter: 396 loss: 0.000410486944
Iter: 397 loss: 0.000410479057
Iter: 398 loss: 0.000410369015
Iter: 399 loss: 0.000410256296
Iter: 400 loss: 0.000410234672
Iter: 401 loss: 0.000410020817
Iter: 402 loss: 0.000409498287
Iter: 403 loss: 0.000414731156
Iter: 404 loss: 0.000409431523
Iter: 405 loss: 0.000408951833
Iter: 406 loss: 0.000408937514
Iter: 407 loss: 0.000408553111
Iter: 408 loss: 0.000409258209
Iter: 409 loss: 0.000408389082
Iter: 410 loss: 0.000408173888
Iter: 411 loss: 0.000409367145
Iter: 412 loss: 0.000408141495
Iter: 413 loss: 0.000407847197
Iter: 414 loss: 0.000407358806
Iter: 415 loss: 0.000407358
Iter: 416 loss: 0.000406953885
Iter: 417 loss: 0.000406995678
Iter: 418 loss: 0.000406641862
Iter: 419 loss: 0.00040628179
Iter: 420 loss: 0.000406574807
Iter: 421 loss: 0.000406048348
Iter: 422 loss: 0.000405817613
Iter: 423 loss: 0.000406580279
Iter: 424 loss: 0.000405753119
Iter: 425 loss: 0.000405695464
Iter: 426 loss: 0.000406028965
Iter: 427 loss: 0.000405688
Iter: 428 loss: 0.000405621686
Iter: 429 loss: 0.00040559523
Iter: 430 loss: 0.000405559316
Iter: 431 loss: 0.000405404106
Iter: 432 loss: 0.000405395229
Iter: 433 loss: 0.000405277417
Iter: 434 loss: 0.000405003
Iter: 435 loss: 0.000406145642
Iter: 436 loss: 0.000404946331
Iter: 437 loss: 0.000404623541
Iter: 438 loss: 0.000404560938
Iter: 439 loss: 0.000404347316
Iter: 440 loss: 0.000404150283
Iter: 441 loss: 0.000404129503
Iter: 442 loss: 0.000403947197
Iter: 443 loss: 0.000403729617
Iter: 444 loss: 0.000403707847
Iter: 445 loss: 0.000403502083
Iter: 446 loss: 0.000404265156
Iter: 447 loss: 0.000403451733
Iter: 448 loss: 0.000403306505
Iter: 449 loss: 0.000403324084
Iter: 450 loss: 0.000403192884
Iter: 451 loss: 0.000402882462
Iter: 452 loss: 0.000403185782
Iter: 453 loss: 0.000402709236
Iter: 454 loss: 0.000402640318
Iter: 455 loss: 0.000402632169
Iter: 456 loss: 0.00040254707
Iter: 457 loss: 0.000402680947
Iter: 458 loss: 0.000402507751
Iter: 459 loss: 0.000402423204
Iter: 460 loss: 0.000402378675
Iter: 461 loss: 0.000402339792
Iter: 462 loss: 0.000402167439
Iter: 463 loss: 0.000402973092
Iter: 464 loss: 0.000402136211
Iter: 465 loss: 0.00040193426
Iter: 466 loss: 0.00040160917
Iter: 467 loss: 0.000401606376
Iter: 468 loss: 0.000401349564
Iter: 469 loss: 0.000401836063
Iter: 470 loss: 0.000401240715
Iter: 471 loss: 0.000401101075
Iter: 472 loss: 0.000400956604
Iter: 473 loss: 0.000400928111
Iter: 474 loss: 0.000400753721
Iter: 475 loss: 0.000401437865
Iter: 476 loss: 0.000400713587
Iter: 477 loss: 0.000400565856
Iter: 478 loss: 0.000400317484
Iter: 479 loss: 0.000400317251
Iter: 480 loss: 0.000399988494
Iter: 481 loss: 0.000399976678
Iter: 482 loss: 0.000399562385
Iter: 483 loss: 0.000399561774
Iter: 484 loss: 0.000399144134
Iter: 485 loss: 0.000401448517
Iter: 486 loss: 0.000399085635
Iter: 487 loss: 0.000399040058
Iter: 488 loss: 0.000398887903
Iter: 489 loss: 0.000398760429
Iter: 490 loss: 0.000398865901
Iter: 491 loss: 0.000398682721
Iter: 492 loss: 0.000398542732
Iter: 493 loss: 0.000398904958
Iter: 494 loss: 0.000398495205
Iter: 495 loss: 0.00039824564
Iter: 496 loss: 0.000398874778
Iter: 497 loss: 0.000398157601
Iter: 498 loss: 0.000398006348
Iter: 499 loss: 0.000399404293
Iter: 500 loss: 0.000397998374
Iter: 501 loss: 0.000397933589
Iter: 502 loss: 0.000397974509
Iter: 503 loss: 0.000397891912
Iter: 504 loss: 0.000397823722
Iter: 505 loss: 0.000397644064
Iter: 506 loss: 0.000398956443
Iter: 507 loss: 0.00039760588
Iter: 508 loss: 0.000397195457
Iter: 509 loss: 0.000402049453
Iter: 510 loss: 0.000397190946
Iter: 511 loss: 0.000396845979
Iter: 512 loss: 0.0003986581
Iter: 513 loss: 0.000396793
Iter: 514 loss: 0.000396674499
Iter: 515 loss: 0.000396565709
Iter: 516 loss: 0.000396538933
Iter: 517 loss: 0.000396406977
Iter: 518 loss: 0.000396202027
Iter: 519 loss: 0.000396199583
Iter: 520 loss: 0.000395988143
Iter: 521 loss: 0.000395753072
Iter: 522 loss: 0.000395720243
Iter: 523 loss: 0.000396385
Iter: 524 loss: 0.000395612733
Iter: 525 loss: 0.000395402167
Iter: 526 loss: 0.000395301729
Iter: 527 loss: 0.000395196723
Iter: 528 loss: 0.000395067764
Iter: 529 loss: 0.000395332812
Iter: 530 loss: 0.000395023089
Iter: 531 loss: 0.000394900679
Iter: 532 loss: 0.00039527737
Iter: 533 loss: 0.000394863717
Iter: 534 loss: 0.000394804403
Iter: 535 loss: 0.000394802832
Iter: 536 loss: 0.000394756789
Iter: 537 loss: 0.000394585775
Iter: 538 loss: 0.000394375093
Iter: 539 loss: 0.000394357805
Iter: 540 loss: 0.000394075731
Iter: 541 loss: 0.000395301206
Iter: 542 loss: 0.0003940186
Iter: 543 loss: 0.000393815222
Iter: 544 loss: 0.000394089875
Iter: 545 loss: 0.000393713708
Iter: 546 loss: 0.000393455324
Iter: 547 loss: 0.000393673603
Iter: 548 loss: 0.000393303257
Iter: 549 loss: 0.000393043156
Iter: 550 loss: 0.000395812618
Iter: 551 loss: 0.00039303751
Iter: 552 loss: 0.000392845192
Iter: 553 loss: 0.000392779068
Iter: 554 loss: 0.00039266987
Iter: 555 loss: 0.000392408139
Iter: 556 loss: 0.000392331451
Iter: 557 loss: 0.000392173795
Iter: 558 loss: 0.000394113071
Iter: 559 loss: 0.000392078247
Iter: 560 loss: 0.0003920421
Iter: 561 loss: 0.000392031856
Iter: 562 loss: 0.000392013579
Iter: 563 loss: 0.000391970389
Iter: 564 loss: 0.000392440124
Iter: 565 loss: 0.000391964713
Iter: 566 loss: 0.000391894777
Iter: 567 loss: 0.000391894602
Iter: 568 loss: 0.000391838141
Iter: 569 loss: 0.000391741021
Iter: 570 loss: 0.000391803624
Iter: 571 loss: 0.000391679554
Iter: 572 loss: 0.000391491281
Iter: 573 loss: 0.000392910035
Iter: 574 loss: 0.000391477603
Iter: 575 loss: 0.000391299778
Iter: 576 loss: 0.000392829592
Iter: 577 loss: 0.000391290348
Iter: 578 loss: 0.000391182373
Iter: 579 loss: 0.000391080452
Iter: 580 loss: 0.000391055859
Iter: 581 loss: 0.000390952278
Iter: 582 loss: 0.000390888599
Iter: 583 loss: 0.000390844943
Iter: 584 loss: 0.000390759029
Iter: 585 loss: 0.000390809466
Iter: 586 loss: 0.00039070216
Iter: 587 loss: 0.000390631671
Iter: 588 loss: 0.000390450354
Iter: 589 loss: 0.00039202944
Iter: 590 loss: 0.000390420289
Iter: 591 loss: 0.000390135567
Iter: 592 loss: 0.000390996633
Iter: 593 loss: 0.000390051166
Iter: 594 loss: 0.00039088988
Iter: 595 loss: 0.000389968976
Iter: 596 loss: 0.000389912
Iter: 597 loss: 0.000389721186
Iter: 598 loss: 0.000389727065
Iter: 599 loss: 0.000389524328
Iter: 600 loss: 0.000389523222
Iter: 601 loss: 0.000389310211
Iter: 602 loss: 0.00038904816
Iter: 603 loss: 0.000389428576
Iter: 604 loss: 0.000388923683
Iter: 605 loss: 0.000388825109
Iter: 606 loss: 0.000389089837
Iter: 607 loss: 0.000388793909
Iter: 608 loss: 0.000388731569
Iter: 609 loss: 0.000388703047
Iter: 610 loss: 0.000388671295
Iter: 611 loss: 0.000388586574
Iter: 612 loss: 0.00038839248
Iter: 613 loss: 0.000391116
Iter: 614 loss: 0.000388383225
Iter: 615 loss: 0.000388655404
Iter: 616 loss: 0.000388233922
Iter: 617 loss: 0.000388402259
Iter: 618 loss: 0.000388111628
Iter: 619 loss: 0.000387969951
Iter: 620 loss: 0.000388468
Iter: 621 loss: 0.000387934851
Iter: 622 loss: 0.000387887587
Iter: 623 loss: 0.000387850043
Iter: 624 loss: 0.000387836
Iter: 625 loss: 0.000387666107
Iter: 626 loss: 0.000387211447
Iter: 627 loss: 0.00039031182
Iter: 628 loss: 0.000387103762
Iter: 629 loss: 0.000387092936
Iter: 630 loss: 0.000386811706
Iter: 631 loss: 0.000386680826
Iter: 632 loss: 0.000386682805
Iter: 633 loss: 0.000386576576
Iter: 634 loss: 0.000386424304
Iter: 635 loss: 0.000386355096
Iter: 636 loss: 0.000386276108
Iter: 637 loss: 0.000386224041
Iter: 638 loss: 0.00038634942
Iter: 639 loss: 0.000386206288
Iter: 640 loss: 0.000386112137
Iter: 641 loss: 0.000386145839
Iter: 642 loss: 0.000386044849
Iter: 643 loss: 0.000385864114
Iter: 644 loss: 0.000387518696
Iter: 645 loss: 0.000385855179
Iter: 646 loss: 0.000385727442
Iter: 647 loss: 0.000386624713
Iter: 648 loss: 0.000385715568
Iter: 649 loss: 0.000385586492
Iter: 650 loss: 0.000385335676
Iter: 651 loss: 0.000390568748
Iter: 652 loss: 0.000385335472
Iter: 653 loss: 0.000385435123
Iter: 654 loss: 0.000385262043
Iter: 655 loss: 0.000385225052
Iter: 656 loss: 0.000385217543
Iter: 657 loss: 0.000385190651
Iter: 658 loss: 0.000385132706
Iter: 659 loss: 0.000385116757
Iter: 660 loss: 0.000385081279
Iter: 661 loss: 0.000385030464
Iter: 662 loss: 0.000384922809
Iter: 663 loss: 0.000387226464
Iter: 664 loss: 0.00038492051
Iter: 665 loss: 0.000384726562
Iter: 666 loss: 0.000385940308
Iter: 667 loss: 0.00038470357
Iter: 668 loss: 0.00038638452
Iter: 669 loss: 0.000384633662
Iter: 670 loss: 0.000384591694
Iter: 671 loss: 0.000384510728
Iter: 672 loss: 0.000386262196
Iter: 673 loss: 0.000384510611
Iter: 674 loss: 0.000384399696
Iter: 675 loss: 0.000384229555
Iter: 676 loss: 0.000384226907
Iter: 677 loss: 0.000384103623
Iter: 678 loss: 0.000383879466
Iter: 679 loss: 0.000389352441
Iter: 680 loss: 0.000383878651
Iter: 681 loss: 0.0003836627
Iter: 682 loss: 0.000385111722
Iter: 683 loss: 0.000383642851
Iter: 684 loss: 0.000383522129
Iter: 685 loss: 0.00038343604
Iter: 686 loss: 0.000383393955
Iter: 687 loss: 0.000383245701
Iter: 688 loss: 0.000383198349
Iter: 689 loss: 0.000383110892
Iter: 690 loss: 0.000389976223
Iter: 691 loss: 0.000382934697
Iter: 692 loss: 0.000382811821
Iter: 693 loss: 0.000383328792
Iter: 694 loss: 0.000382787897
Iter: 695 loss: 0.000382674276
Iter: 696 loss: 0.000383780512
Iter: 697 loss: 0.000382670842
Iter: 698 loss: 0.000382537342
Iter: 699 loss: 0.000382309139
Iter: 700 loss: 0.000382308586
Iter: 701 loss: 0.000381967024
Iter: 702 loss: 0.000382953265
Iter: 703 loss: 0.000381860067
Iter: 704 loss: 0.00038229418
Iter: 705 loss: 0.000381680089
Iter: 706 loss: 0.000381589751
Iter: 707 loss: 0.000381495425
Iter: 708 loss: 0.000381478312
Iter: 709 loss: 0.000381328107
Iter: 710 loss: 0.000382388447
Iter: 711 loss: 0.000381315
Iter: 712 loss: 0.000381213031
Iter: 713 loss: 0.000380976766
Iter: 714 loss: 0.000383840932
Iter: 715 loss: 0.000380956481
Iter: 716 loss: 0.000380991318
Iter: 717 loss: 0.000380832236
Iter: 718 loss: 0.000380763959
Iter: 719 loss: 0.00038056905
Iter: 720 loss: 0.000381437858
Iter: 721 loss: 0.000380496116
Iter: 722 loss: 0.000380312209
Iter: 723 loss: 0.000380777637
Iter: 724 loss: 0.000380247307
Iter: 725 loss: 0.000380153419
Iter: 726 loss: 0.000380973273
Iter: 727 loss: 0.000380150101
Iter: 728 loss: 0.000380075304
Iter: 729 loss: 0.000380076061
Iter: 730 loss: 0.000379985373
Iter: 731 loss: 0.000379716104
Iter: 732 loss: 0.000380594749
Iter: 733 loss: 0.000379589299
Iter: 734 loss: 0.000379394682
Iter: 735 loss: 0.000379901903
Iter: 736 loss: 0.000379329635
Iter: 737 loss: 0.000379191653
Iter: 738 loss: 0.000379517325
Iter: 739 loss: 0.000379141769
Iter: 740 loss: 0.000379024161
Iter: 741 loss: 0.000378974073
Iter: 742 loss: 0.000378913654
Iter: 743 loss: 0.000378768716
Iter: 744 loss: 0.000378873694
Iter: 745 loss: 0.000378681463
Iter: 746 loss: 0.000378520286
Iter: 747 loss: 0.000378911645
Iter: 748 loss: 0.000378463097
Iter: 749 loss: 0.000378225348
Iter: 750 loss: 0.00038011634
Iter: 751 loss: 0.00037821068
Iter: 752 loss: 0.000378113153
Iter: 753 loss: 0.00037804665
Iter: 754 loss: 0.000377869641
Iter: 755 loss: 0.000378031546
Iter: 756 loss: 0.000377767254
Iter: 757 loss: 0.000377591117
Iter: 758 loss: 0.000377585064
Iter: 759 loss: 0.000377447403
Iter: 760 loss: 0.000377298187
Iter: 761 loss: 0.00037726108
Iter: 762 loss: 0.000377123739
Iter: 763 loss: 0.000377363904
Iter: 764 loss: 0.000377061602
Iter: 765 loss: 0.000376995449
Iter: 766 loss: 0.000376949029
Iter: 767 loss: 0.00037692365
Iter: 768 loss: 0.000376796525
Iter: 769 loss: 0.000376927754
Iter: 770 loss: 0.000376724405
Iter: 771 loss: 0.000376721786
Iter: 772 loss: 0.000376660901
Iter: 773 loss: 0.000376634824
Iter: 774 loss: 0.000376559561
Iter: 775 loss: 0.000376806216
Iter: 776 loss: 0.000376523414
Iter: 777 loss: 0.000376386888
Iter: 778 loss: 0.000376347365
Iter: 779 loss: 0.00037626666
Iter: 780 loss: 0.000376185169
Iter: 781 loss: 0.000376083539
Iter: 782 loss: 0.000376074866
Iter: 783 loss: 0.000375950069
Iter: 784 loss: 0.000376504962
Iter: 785 loss: 0.000375923701
Iter: 786 loss: 0.000375799078
Iter: 787 loss: 0.000377713935
Iter: 788 loss: 0.000375797244
Iter: 789 loss: 0.000375743373
Iter: 790 loss: 0.000375643605
Iter: 791 loss: 0.000377894816
Iter: 792 loss: 0.000375642063
Iter: 793 loss: 0.000375538715
Iter: 794 loss: 0.000375824282
Iter: 795 loss: 0.000375503674
Iter: 796 loss: 0.000375422533
Iter: 797 loss: 0.000375423231
Iter: 798 loss: 0.000375326024
Iter: 799 loss: 0.000375135627
Iter: 800 loss: 0.00037882023
Iter: 801 loss: 0.000375135132
Iter: 802 loss: 0.000375102565
Iter: 803 loss: 0.000375076139
Iter: 804 loss: 0.000375066767
Iter: 805 loss: 0.000375024392
Iter: 806 loss: 0.000375223
Iter: 807 loss: 0.000375014788
Iter: 808 loss: 0.000374952739
Iter: 809 loss: 0.000374790223
Iter: 810 loss: 0.00037600589
Iter: 811 loss: 0.000374758034
Iter: 812 loss: 0.000374578143
Iter: 813 loss: 0.000374560768
Iter: 814 loss: 0.00037439
Iter: 815 loss: 0.000374994735
Iter: 816 loss: 0.000374345953
Iter: 817 loss: 0.000374198513
Iter: 818 loss: 0.000374215539
Iter: 819 loss: 0.000374085415
Iter: 820 loss: 0.000373829826
Iter: 821 loss: 0.000374220923
Iter: 822 loss: 0.000373708666
Iter: 823 loss: 0.000379526638
Iter: 824 loss: 0.000373679301
Iter: 825 loss: 0.000373668096
Iter: 826 loss: 0.000373642019
Iter: 827 loss: 0.000374104537
Iter: 828 loss: 0.000373640389
Iter: 829 loss: 0.000373583636
Iter: 830 loss: 0.000373430201
Iter: 831 loss: 0.000374418509
Iter: 832 loss: 0.000373389164
Iter: 833 loss: 0.000384321233
Iter: 834 loss: 0.000373344985
Iter: 835 loss: 0.000373298652
Iter: 836 loss: 0.000373315474
Iter: 837 loss: 0.00037326524
Iter: 838 loss: 0.000373149756
Iter: 839 loss: 0.000373215356
Iter: 840 loss: 0.000373075338
Iter: 841 loss: 0.00037384
Iter: 842 loss: 0.000372953102
Iter: 843 loss: 0.00037291841
Iter: 844 loss: 0.000372833223
Iter: 845 loss: 0.000373551826
Iter: 846 loss: 0.000372818933
Iter: 847 loss: 0.000372670387
Iter: 848 loss: 0.000372488226
Iter: 849 loss: 0.000372469745
Iter: 850 loss: 0.000372279261
Iter: 851 loss: 0.000374554249
Iter: 852 loss: 0.000372275768
Iter: 853 loss: 0.000372139795
Iter: 854 loss: 0.000372102368
Iter: 855 loss: 0.000372021343
Iter: 856 loss: 0.000371793751
Iter: 857 loss: 0.000372376526
Iter: 858 loss: 0.000371709815
Iter: 859 loss: 0.000371596601
Iter: 860 loss: 0.000371276867
Iter: 861 loss: 0.000372944574
Iter: 862 loss: 0.000371177
Iter: 863 loss: 0.000370724476
Iter: 864 loss: 0.000370717782
Iter: 865 loss: 0.000370361144
Iter: 866 loss: 0.000370256108
Iter: 867 loss: 0.000370230875
Iter: 868 loss: 0.00037016545
Iter: 869 loss: 0.00037008847
Iter: 870 loss: 0.000370067195
Iter: 871 loss: 0.00037002092
Iter: 872 loss: 0.000369748916
Iter: 873 loss: 0.000369335292
Iter: 874 loss: 0.000369327929
Iter: 875 loss: 0.000373614
Iter: 876 loss: 0.000369105401
Iter: 877 loss: 0.000369511836
Iter: 878 loss: 0.000369050424
Iter: 879 loss: 0.000368967478
Iter: 880 loss: 0.000368817971
Iter: 881 loss: 0.00037403591
Iter: 882 loss: 0.000368816865
Iter: 883 loss: 0.000368685229
Iter: 884 loss: 0.000368691515
Iter: 885 loss: 0.000368582667
Iter: 886 loss: 0.000368509442
Iter: 887 loss: 0.000368477165
Iter: 888 loss: 0.000368440698
Iter: 889 loss: 0.000368382374
Iter: 890 loss: 0.000368316396
Iter: 891 loss: 0.000368308625
Iter: 892 loss: 0.000368168374
Iter: 893 loss: 0.000367941364
Iter: 894 loss: 0.000367938424
Iter: 895 loss: 0.000367807253
Iter: 896 loss: 0.00036780251
Iter: 897 loss: 0.000368773879
Iter: 898 loss: 0.000367713219
Iter: 899 loss: 0.000367679866
Iter: 900 loss: 0.000367598666
Iter: 901 loss: 0.000368497625
Iter: 902 loss: 0.00036759
Iter: 903 loss: 0.00036724651
Iter: 904 loss: 0.000368036272
Iter: 905 loss: 0.000367119967
Iter: 906 loss: 0.000366931374
Iter: 907 loss: 0.000368679728
Iter: 908 loss: 0.000366922031
Iter: 909 loss: 0.000366836233
Iter: 910 loss: 0.000366742868
Iter: 911 loss: 0.000366727647
Iter: 912 loss: 0.000367072294
Iter: 913 loss: 0.00036663175
Iter: 914 loss: 0.000366570457
Iter: 915 loss: 0.000366834836
Iter: 916 loss: 0.00036655835
Iter: 917 loss: 0.000366528693
Iter: 918 loss: 0.000366542139
Iter: 919 loss: 0.000366508611
Iter: 920 loss: 0.000366474211
Iter: 921 loss: 0.000366401393
Iter: 922 loss: 0.000367478089
Iter: 923 loss: 0.000366398075
Iter: 924 loss: 0.000366472261
Iter: 925 loss: 0.000366243155
Iter: 926 loss: 0.000366014399
Iter: 927 loss: 0.000365921936
Iter: 928 loss: 0.000365798332
Iter: 929 loss: 0.000365709362
Iter: 930 loss: 0.000366293883
Iter: 931 loss: 0.000365700282
Iter: 932 loss: 0.000365646963
Iter: 933 loss: 0.000365633634
Iter: 934 loss: 0.000365577493
Iter: 935 loss: 0.000365494809
Iter: 936 loss: 0.000365492568
Iter: 937 loss: 0.000365427521
Iter: 938 loss: 0.000365244516
Iter: 939 loss: 0.000366145105
Iter: 940 loss: 0.000365177926
Iter: 941 loss: 0.000365069252
Iter: 942 loss: 0.000365145213
Iter: 943 loss: 0.000365002837
Iter: 944 loss: 0.000364946347
Iter: 945 loss: 0.000364956213
Iter: 946 loss: 0.000364902313
Iter: 947 loss: 0.000364879321
Iter: 948 loss: 0.000364872918
Iter: 949 loss: 0.000364857842
Iter: 950 loss: 0.000364770589
Iter: 951 loss: 0.000365482119
Iter: 952 loss: 0.000364764943
Iter: 953 loss: 0.000364663429
Iter: 954 loss: 0.000364975131
Iter: 955 loss: 0.000364634616
Iter: 956 loss: 0.000364573876
Iter: 957 loss: 0.000364401116
Iter: 958 loss: 0.00036524731
Iter: 959 loss: 0.000364343898
Iter: 960 loss: 0.000364080508
Iter: 961 loss: 0.000364508189
Iter: 962 loss: 0.000363956
Iter: 963 loss: 0.000363820407
Iter: 964 loss: 0.000364197884
Iter: 965 loss: 0.000363778032
Iter: 966 loss: 0.000368482346
Iter: 967 loss: 0.000363678206
Iter: 968 loss: 0.000363578059
Iter: 969 loss: 0.000363364874
Iter: 970 loss: 0.000367610948
Iter: 971 loss: 0.000363359344
Iter: 972 loss: 0.000363190979
Iter: 973 loss: 0.000363931
Iter: 974 loss: 0.000363157073
Iter: 975 loss: 0.000363080646
Iter: 976 loss: 0.000362945022
Iter: 977 loss: 0.0003629454
Iter: 978 loss: 0.000362743245
Iter: 979 loss: 0.000364671228
Iter: 980 loss: 0.000362735096
Iter: 981 loss: 0.000362663792
Iter: 982 loss: 0.000362634368
Iter: 983 loss: 0.000362631399
Iter: 984 loss: 0.000362523133
Iter: 985 loss: 0.000362485356
Iter: 986 loss: 0.000362448
Iter: 987 loss: 0.000362297549
Iter: 988 loss: 0.000362665218
Iter: 989 loss: 0.000362244027
Iter: 990 loss: 0.000362149352
Iter: 991 loss: 0.000361997867
Iter: 992 loss: 0.000361996237
Iter: 993 loss: 0.000361880055
Iter: 994 loss: 0.000361950253
Iter: 995 loss: 0.000361803046
Iter: 996 loss: 0.000361691811
Iter: 997 loss: 0.000361552666
Iter: 998 loss: 0.000361541228
Iter: 999 loss: 0.000361383922
Iter: 1000 loss: 0.000362753199
Iter: 1001 loss: 0.000361377199
Iter: 1002 loss: 0.000361162529
Iter: 1003 loss: 0.000361455895
Iter: 1004 loss: 0.000361056183
Iter: 1005 loss: 0.000360680395
Iter: 1006 loss: 0.000363833969
Iter: 1007 loss: 0.000360658916
Iter: 1008 loss: 0.000360528531
Iter: 1009 loss: 0.000360445527
Iter: 1010 loss: 0.00036039477
Iter: 1011 loss: 0.000360190577
Iter: 1012 loss: 0.000360158796
Iter: 1013 loss: 0.000360017642
Iter: 1014 loss: 0.000359880942
Iter: 1015 loss: 0.000359608588
Iter: 1016 loss: 0.000365378917
Iter: 1017 loss: 0.000359605823
Iter: 1018 loss: 0.00035937276
Iter: 1019 loss: 0.000361605082
Iter: 1020 loss: 0.000359363155
Iter: 1021 loss: 0.000359240541
Iter: 1022 loss: 0.000360126316
Iter: 1023 loss: 0.000359232421
Iter: 1024 loss: 0.000359176775
Iter: 1025 loss: 0.000359410449
Iter: 1026 loss: 0.000359165191
Iter: 1027 loss: 0.000359154539
Iter: 1028 loss: 0.000359155
Iter: 1029 loss: 0.000359146186
Iter: 1030 loss: 0.000359001075
Iter: 1031 loss: 0.000359258906
Iter: 1032 loss: 0.000358935795
Iter: 1033 loss: 0.000358776073
Iter: 1034 loss: 0.000358872174
Iter: 1035 loss: 0.000358671794
Iter: 1036 loss: 0.000361693616
Iter: 1037 loss: 0.000358607387
Iter: 1038 loss: 0.000358572725
Iter: 1039 loss: 0.000358571124
Iter: 1040 loss: 0.000358543301
Iter: 1041 loss: 0.000358514371
Iter: 1042 loss: 0.000358494726
Iter: 1043 loss: 0.000358483841
Iter: 1044 loss: 0.000358431105
Iter: 1045 loss: 0.000358427234
Iter: 1046 loss: 0.000358387304
Iter: 1047 loss: 0.00035832866
Iter: 1048 loss: 0.000358199031
Iter: 1049 loss: 0.000360132894
Iter: 1050 loss: 0.000358193181
Iter: 1051 loss: 0.000358129735
Iter: 1052 loss: 0.000358131481
Iter: 1053 loss: 0.000358078076
Iter: 1054 loss: 0.000358011224
Iter: 1055 loss: 0.000358036486
Iter: 1056 loss: 0.00035796623
Iter: 1057 loss: 0.0003584238
Iter: 1058 loss: 0.00035790354
Iter: 1059 loss: 0.00035780418
Iter: 1060 loss: 0.000357748213
Iter: 1061 loss: 0.000357704179
Iter: 1062 loss: 0.000357602257
Iter: 1063 loss: 0.000357770768
Iter: 1064 loss: 0.00035755604
Iter: 1065 loss: 0.000357385172
Iter: 1066 loss: 0.000356973964
Iter: 1067 loss: 0.000361769256
Iter: 1068 loss: 0.000356935459
Iter: 1069 loss: 0.000356656383
Iter: 1070 loss: 0.000359167578
Iter: 1071 loss: 0.000356643199
Iter: 1072 loss: 0.000362465682
Iter: 1073 loss: 0.000356617878
Iter: 1074 loss: 0.000356581
Iter: 1075 loss: 0.000356628443
Iter: 1076 loss: 0.000356561446
Iter: 1077 loss: 0.000356466975
Iter: 1078 loss: 0.000356605684
Iter: 1079 loss: 0.000356421137
Iter: 1080 loss: 0.000356328266
Iter: 1081 loss: 0.000357628
Iter: 1082 loss: 0.000356329
Iter: 1083 loss: 0.000356308796
Iter: 1084 loss: 0.000356260163
Iter: 1085 loss: 0.000356690347
Iter: 1086 loss: 0.00035625149
Iter: 1087 loss: 0.000360833685
Iter: 1088 loss: 0.000356010743
Iter: 1089 loss: 0.00035567052
Iter: 1090 loss: 0.000355526921
Iter: 1091 loss: 0.000355350669
Iter: 1092 loss: 0.000355289172
Iter: 1093 loss: 0.000355173979
Iter: 1094 loss: 0.000357551093
Iter: 1095 loss: 0.000355174096
Iter: 1096 loss: 0.000355041324
Iter: 1097 loss: 0.000355069089
Iter: 1098 loss: 0.000354944437
Iter: 1099 loss: 0.000354903954
Iter: 1100 loss: 0.000354799413
Iter: 1101 loss: 0.000355499855
Iter: 1102 loss: 0.000354773423
Iter: 1103 loss: 0.000354568241
Iter: 1104 loss: 0.000355272874
Iter: 1105 loss: 0.000354512886
Iter: 1106 loss: 0.000354249758
Iter: 1107 loss: 0.000354413438
Iter: 1108 loss: 0.000354083051
Iter: 1109 loss: 0.000353966199
Iter: 1110 loss: 0.000354215212
Iter: 1111 loss: 0.000353916403
Iter: 1112 loss: 0.000353867712
Iter: 1113 loss: 0.000353816606
Iter: 1114 loss: 0.000353808457
Iter: 1115 loss: 0.000360517355
Iter: 1116 loss: 0.000353789888
Iter: 1117 loss: 0.000353765121
Iter: 1118 loss: 0.000353788841
Iter: 1119 loss: 0.00035375313
Iter: 1120 loss: 0.000353664131
Iter: 1121 loss: 0.00035337673
Iter: 1122 loss: 0.000353400799
Iter: 1123 loss: 0.000353082782
Iter: 1124 loss: 0.000353181764
Iter: 1125 loss: 0.000352845644
Iter: 1126 loss: 0.000352877483
Iter: 1127 loss: 0.000352785719
Iter: 1128 loss: 0.000352742587
Iter: 1129 loss: 0.000352695788
Iter: 1130 loss: 0.000352689589
Iter: 1131 loss: 0.000352564413
Iter: 1132 loss: 0.000352905743
Iter: 1133 loss: 0.000352523348
Iter: 1134 loss: 0.000352291856
Iter: 1135 loss: 0.00035309029
Iter: 1136 loss: 0.000352231931
Iter: 1137 loss: 0.000352125731
Iter: 1138 loss: 0.000352190254
Iter: 1139 loss: 0.000352057337
Iter: 1140 loss: 0.000351970899
Iter: 1141 loss: 0.000352359377
Iter: 1142 loss: 0.000351956172
Iter: 1143 loss: 0.000351922499
Iter: 1144 loss: 0.00035181
Iter: 1145 loss: 0.000351814699
Iter: 1146 loss: 0.000351696392
Iter: 1147 loss: 0.000351427938
Iter: 1148 loss: 0.00035517264
Iter: 1149 loss: 0.000351425668
Iter: 1150 loss: 0.000351382681
Iter: 1151 loss: 0.000351528113
Iter: 1152 loss: 0.000351370196
Iter: 1153 loss: 0.000351305789
Iter: 1154 loss: 0.000351096678
Iter: 1155 loss: 0.000351253548
Iter: 1156 loss: 0.000350918621
Iter: 1157 loss: 0.000350834074
Iter: 1158 loss: 0.000351213734
Iter: 1159 loss: 0.000350818678
Iter: 1160 loss: 0.00035077153
Iter: 1161 loss: 0.000350637
Iter: 1162 loss: 0.00035131388
Iter: 1163 loss: 0.000350591668
Iter: 1164 loss: 0.000350419403
Iter: 1165 loss: 0.000350395479
Iter: 1166 loss: 0.000350303773
Iter: 1167 loss: 0.000350220362
Iter: 1168 loss: 0.000350197341
Iter: 1169 loss: 0.000353793963
Iter: 1170 loss: 0.000350148592
Iter: 1171 loss: 0.000350077549
Iter: 1172 loss: 0.000350267394
Iter: 1173 loss: 0.000350055692
Iter: 1174 loss: 0.000350013346
Iter: 1175 loss: 0.000349882175
Iter: 1176 loss: 0.000350076851
Iter: 1177 loss: 0.000349790382
Iter: 1178 loss: 0.000352081523
Iter: 1179 loss: 0.000349710172
Iter: 1180 loss: 0.000349692535
Iter: 1181 loss: 0.000349656504
Iter: 1182 loss: 0.000349622802
Iter: 1183 loss: 0.000349519512
Iter: 1184 loss: 0.000349818351
Iter: 1185 loss: 0.000349466573
Iter: 1186 loss: 0.000349319307
Iter: 1187 loss: 0.000349473412
Iter: 1188 loss: 0.00034923834
Iter: 1189 loss: 0.000349212118
Iter: 1190 loss: 0.000349136186
Iter: 1191 loss: 0.000349509937
Iter: 1192 loss: 0.000349111913
Iter: 1193 loss: 0.000349271664
Iter: 1194 loss: 0.000349036069
Iter: 1195 loss: 0.000349099777
Iter: 1196 loss: 0.000348964852
Iter: 1197 loss: 0.000348828791
Iter: 1198 loss: 0.000348601927
Iter: 1199 loss: 0.000348602276
Iter: 1200 loss: 0.000348376954
Iter: 1201 loss: 0.000348636357
Iter: 1202 loss: 0.000348254864
Iter: 1203 loss: 0.000348622678
Iter: 1204 loss: 0.000348183501
Iter: 1205 loss: 0.000348104397
Iter: 1206 loss: 0.000347869383
Iter: 1207 loss: 0.000348691305
Iter: 1208 loss: 0.000347763649
Iter: 1209 loss: 0.000347292196
Iter: 1210 loss: 0.000348638918
Iter: 1211 loss: 0.000347143563
Iter: 1212 loss: 0.000347512891
Iter: 1213 loss: 0.000347054738
Iter: 1214 loss: 0.00034702511
Iter: 1215 loss: 0.0003469761
Iter: 1216 loss: 0.000346975721
Iter: 1217 loss: 0.000346892251
Iter: 1218 loss: 0.000347328489
Iter: 1219 loss: 0.000346879708
Iter: 1220 loss: 0.000346766552
Iter: 1221 loss: 0.000347802
Iter: 1222 loss: 0.000346760557
Iter: 1223 loss: 0.000346641522
Iter: 1224 loss: 0.000346345361
Iter: 1225 loss: 0.000349341048
Iter: 1226 loss: 0.000346309098
Iter: 1227 loss: 0.0003460554
Iter: 1228 loss: 0.000347413472
Iter: 1229 loss: 0.000346016895
Iter: 1230 loss: 0.000345940643
Iter: 1231 loss: 0.000345865032
Iter: 1232 loss: 0.000345847948
Iter: 1233 loss: 0.000345792796
Iter: 1234 loss: 0.00034572091
Iter: 1235 loss: 0.000345768873
Iter: 1236 loss: 0.00034569361
Iter: 1237 loss: 0.000345678418
Iter: 1238 loss: 0.000345648616
Iter: 1239 loss: 0.000346300949
Iter: 1240 loss: 0.000345647451
Iter: 1241 loss: 0.000345598964
Iter: 1242 loss: 0.00034552053
Iter: 1243 loss: 0.000345520297
Iter: 1244 loss: 0.000345495791
Iter: 1245 loss: 0.000345471432
Iter: 1246 loss: 0.000345468026
Iter: 1247 loss: 0.000345361186
Iter: 1248 loss: 0.000345080916
Iter: 1249 loss: 0.000347131136
Iter: 1250 loss: 0.000345019653
Iter: 1251 loss: 0.00034491322
Iter: 1252 loss: 0.000344894535
Iter: 1253 loss: 0.000344831089
Iter: 1254 loss: 0.000344727188
Iter: 1255 loss: 0.000344725559
Iter: 1256 loss: 0.000344519
Iter: 1257 loss: 0.000344064028
Iter: 1258 loss: 0.000351037364
Iter: 1259 loss: 0.000344043481
Iter: 1260 loss: 0.000343881606
Iter: 1261 loss: 0.000343640713
Iter: 1262 loss: 0.000343634689
Iter: 1263 loss: 0.000343485677
Iter: 1264 loss: 0.000344378117
Iter: 1265 loss: 0.000343471242
Iter: 1266 loss: 0.000343450258
Iter: 1267 loss: 0.000343413965
Iter: 1268 loss: 0.000344338769
Iter: 1269 loss: 0.000343413703
Iter: 1270 loss: 0.000343382766
Iter: 1271 loss: 0.000343370892
Iter: 1272 loss: 0.000343355
Iter: 1273 loss: 0.000343319145
Iter: 1274 loss: 0.000343676482
Iter: 1275 loss: 0.000343317632
Iter: 1276 loss: 0.000343202555
Iter: 1277 loss: 0.000343909138
Iter: 1278 loss: 0.00034318777
Iter: 1279 loss: 0.000343067019
Iter: 1280 loss: 0.000343043706
Iter: 1281 loss: 0.00034292732
Iter: 1282 loss: 0.00034279353
Iter: 1283 loss: 0.000342860294
Iter: 1284 loss: 0.000342660875
Iter: 1285 loss: 0.00034247688
Iter: 1286 loss: 0.000343917171
Iter: 1287 loss: 0.0003424639
Iter: 1288 loss: 0.000342091545
Iter: 1289 loss: 0.000342650746
Iter: 1290 loss: 0.000341912528
Iter: 1291 loss: 0.000343148364
Iter: 1292 loss: 0.000341819657
Iter: 1293 loss: 0.000342514657
Iter: 1294 loss: 0.000341746665
Iter: 1295 loss: 0.000341733
Iter: 1296 loss: 0.000341821171
Iter: 1297 loss: 0.000341732404
Iter: 1298 loss: 0.00034171727
Iter: 1299 loss: 0.000341666804
Iter: 1300 loss: 0.000341725594
Iter: 1301 loss: 0.000341629959
Iter: 1302 loss: 0.000342549465
Iter: 1303 loss: 0.000341558713
Iter: 1304 loss: 0.000341433391
Iter: 1305 loss: 0.000341536681
Iter: 1306 loss: 0.000341356848
Iter: 1307 loss: 0.000341311417
Iter: 1308 loss: 0.000341362727
Iter: 1309 loss: 0.000341287232
Iter: 1310 loss: 0.000341188046
Iter: 1311 loss: 0.00034117265
Iter: 1312 loss: 0.000341106381
Iter: 1313 loss: 0.000340987317
Iter: 1314 loss: 0.000340817816
Iter: 1315 loss: 0.000340812956
Iter: 1316 loss: 0.000340723433
Iter: 1317 loss: 0.000340462895
Iter: 1318 loss: 0.000341589504
Iter: 1319 loss: 0.000340355444
Iter: 1320 loss: 0.000340212893
Iter: 1321 loss: 0.000341213774
Iter: 1322 loss: 0.000340201485
Iter: 1323 loss: 0.000340179424
Iter: 1324 loss: 0.000340180617
Iter: 1325 loss: 0.000340162369
Iter: 1326 loss: 0.000340373314
Iter: 1327 loss: 0.000339995313
Iter: 1328 loss: 0.000339855673
Iter: 1329 loss: 0.00033985154
Iter: 1330 loss: 0.00033972782
Iter: 1331 loss: 0.000339582621
Iter: 1332 loss: 0.000339566235
Iter: 1333 loss: 0.000339479418
Iter: 1334 loss: 0.000339861726
Iter: 1335 loss: 0.000339460094
Iter: 1336 loss: 0.00033942092
Iter: 1337 loss: 0.00033931146
Iter: 1338 loss: 0.000339835649
Iter: 1339 loss: 0.000339272141
Iter: 1340 loss: 0.00033918643
Iter: 1341 loss: 0.000339077262
Iter: 1342 loss: 0.000339070451
Iter: 1343 loss: 0.000338983082
Iter: 1344 loss: 0.000338907645
Iter: 1345 loss: 0.000339770922
Iter: 1346 loss: 0.000338788697
Iter: 1347 loss: 0.00033881588
Iter: 1348 loss: 0.000338678248
Iter: 1349 loss: 0.000338642101
Iter: 1350 loss: 0.000338511018
Iter: 1351 loss: 0.000338207115
Iter: 1352 loss: 0.000345605949
Iter: 1353 loss: 0.000338208163
Iter: 1354 loss: 0.000338870334
Iter: 1355 loss: 0.000338170852
Iter: 1356 loss: 0.000338115031
Iter: 1357 loss: 0.000338484
Iter: 1358 loss: 0.000338110083
Iter: 1359 loss: 0.00033809361
Iter: 1360 loss: 0.000338045211
Iter: 1361 loss: 0.000338185
Iter: 1362 loss: 0.000338019774
Iter: 1363 loss: 0.000338107406
Iter: 1364 loss: 0.000337964331
Iter: 1365 loss: 0.000337942853
Iter: 1366 loss: 0.000337923178
Iter: 1367 loss: 0.00033791663
Iter: 1368 loss: 0.000338041107
Iter: 1369 loss: 0.000337870501
Iter: 1370 loss: 0.000337797159
Iter: 1371 loss: 0.000338142883
Iter: 1372 loss: 0.000337783131
Iter: 1373 loss: 0.0003376198
Iter: 1374 loss: 0.000337341276
Iter: 1375 loss: 0.000337339821
Iter: 1376 loss: 0.000339271879
Iter: 1377 loss: 0.00033726031
Iter: 1378 loss: 0.000337172067
Iter: 1379 loss: 0.000337022444
Iter: 1380 loss: 0.00033702291
Iter: 1381 loss: 0.000336752215
Iter: 1382 loss: 0.000336574565
Iter: 1383 loss: 0.000336470373
Iter: 1384 loss: 0.000336293248
Iter: 1385 loss: 0.000336289173
Iter: 1386 loss: 0.000348381756
Iter: 1387 loss: 0.000336268509
Iter: 1388 loss: 0.000336266705
Iter: 1389 loss: 0.000336256839
Iter: 1390 loss: 0.000336393306
Iter: 1391 loss: 0.000336255471
Iter: 1392 loss: 0.000336236844
Iter: 1393 loss: 0.000336180325
Iter: 1394 loss: 0.000336363941
Iter: 1395 loss: 0.000336153491
Iter: 1396 loss: 0.000336120953
Iter: 1397 loss: 0.000336040539
Iter: 1398 loss: 0.000336831901
Iter: 1399 loss: 0.000336031051
Iter: 1400 loss: 0.00033586449
Iter: 1401 loss: 0.000335863151
Iter: 1402 loss: 0.000335786375
Iter: 1403 loss: 0.000335584802
Iter: 1404 loss: 0.000337015197
Iter: 1405 loss: 0.000335541728
Iter: 1406 loss: 0.000335292483
Iter: 1407 loss: 0.00033596711
Iter: 1408 loss: 0.000335209363
Iter: 1409 loss: 0.000335101213
Iter: 1410 loss: 0.000334819662
Iter: 1411 loss: 0.000337015605
Iter: 1412 loss: 0.000334761717
Iter: 1413 loss: 0.000334595563
Iter: 1414 loss: 0.00033688749
Iter: 1415 loss: 0.000334596261
Iter: 1416 loss: 0.00033449792
Iter: 1417 loss: 0.000334454846
Iter: 1418 loss: 0.000334403245
Iter: 1419 loss: 0.000334516837
Iter: 1420 loss: 0.000334255106
Iter: 1421 loss: 0.000334124896
Iter: 1422 loss: 0.000334106735
Iter: 1423 loss: 0.000334016368
Iter: 1424 loss: 0.00033446186
Iter: 1425 loss: 0.000333993492
Iter: 1426 loss: 0.000333917385
Iter: 1427 loss: 0.000333815959
Iter: 1428 loss: 0.000333809119
Iter: 1429 loss: 0.000333770644
Iter: 1430 loss: 0.00033376916
Iter: 1431 loss: 0.000333743228
Iter: 1432 loss: 0.000333808392
Iter: 1433 loss: 0.000333733857
Iter: 1434 loss: 0.000333698175
Iter: 1435 loss: 0.00033360775
Iter: 1436 loss: 0.000334491197
Iter: 1437 loss: 0.000333596108
Iter: 1438 loss: 0.000333487027
Iter: 1439 loss: 0.000333322794
Iter: 1440 loss: 0.000333319418
Iter: 1441 loss: 0.000333222357
Iter: 1442 loss: 0.000333461969
Iter: 1443 loss: 0.000333186705
Iter: 1444 loss: 0.00033311639
Iter: 1445 loss: 0.000332906027
Iter: 1446 loss: 0.0003336097
Iter: 1447 loss: 0.000332807103
Iter: 1448 loss: 0.000332728901
Iter: 1449 loss: 0.000332659722
Iter: 1450 loss: 0.000332619122
Iter: 1451 loss: 0.000332601339
Iter: 1452 loss: 0.000332567899
Iter: 1453 loss: 0.000332465483
Iter: 1454 loss: 0.000332744385
Iter: 1455 loss: 0.000332411466
Iter: 1456 loss: 0.000332164607
Iter: 1457 loss: 0.000333505392
Iter: 1458 loss: 0.000332128926
Iter: 1459 loss: 0.00033194921
Iter: 1460 loss: 0.000331926567
Iter: 1461 loss: 0.000331907
Iter: 1462 loss: 0.0003319268
Iter: 1463 loss: 0.000331896183
Iter: 1464 loss: 0.000331841118
Iter: 1465 loss: 0.000331753923
Iter: 1466 loss: 0.000331753778
Iter: 1467 loss: 0.000331643387
Iter: 1468 loss: 0.000331364106
Iter: 1469 loss: 0.000333991571
Iter: 1470 loss: 0.000331323827
Iter: 1471 loss: 0.000353178533
Iter: 1472 loss: 0.000331103365
Iter: 1473 loss: 0.000330734038
Iter: 1474 loss: 0.00033228047
Iter: 1475 loss: 0.000330654206
Iter: 1476 loss: 0.000330476265
Iter: 1477 loss: 0.000330983661
Iter: 1478 loss: 0.000330421026
Iter: 1479 loss: 0.000330169685
Iter: 1480 loss: 0.00033044556
Iter: 1481 loss: 0.000330032926
Iter: 1482 loss: 0.00032973598
Iter: 1483 loss: 0.000332777039
Iter: 1484 loss: 0.000329726317
Iter: 1485 loss: 0.00032945245
Iter: 1486 loss: 0.000329292729
Iter: 1487 loss: 0.000329174509
Iter: 1488 loss: 0.000329215662
Iter: 1489 loss: 0.000329143484
Iter: 1490 loss: 0.000329121423
Iter: 1491 loss: 0.000329122093
Iter: 1492 loss: 0.000329106522
Iter: 1493 loss: 0.000329067814
Iter: 1494 loss: 0.000329277187
Iter: 1495 loss: 0.000329052913
Iter: 1496 loss: 0.000328985741
Iter: 1497 loss: 0.000329001487
Iter: 1498 loss: 0.000328936527
Iter: 1499 loss: 0.000328860711
Iter: 1500 loss: 0.00032880751
Iter: 1501 loss: 0.000328779395
Iter: 1502 loss: 0.000328564958
Iter: 1503 loss: 0.000328249123
Iter: 1504 loss: 0.00032824045
Iter: 1505 loss: 0.000327905058
Iter: 1506 loss: 0.000329413451
Iter: 1507 loss: 0.00032783841
Iter: 1508 loss: 0.0003278199
Iter: 1509 loss: 0.000327763555
Iter: 1510 loss: 0.000327673915
Iter: 1511 loss: 0.000327510497
Iter: 1512 loss: 0.000331333926
Iter: 1513 loss: 0.000327509362
Iter: 1514 loss: 0.000327399146
Iter: 1515 loss: 0.000327398593
Iter: 1516 loss: 0.000327281625
Iter: 1517 loss: 0.000328077935
Iter: 1518 loss: 0.000327270565
Iter: 1519 loss: 0.000327249116
Iter: 1520 loss: 0.000327182701
Iter: 1521 loss: 0.000327437039
Iter: 1522 loss: 0.000327156333
Iter: 1523 loss: 0.000327051937
Iter: 1524 loss: 0.000327972171
Iter: 1525 loss: 0.000327046844
Iter: 1526 loss: 0.000327034155
Iter: 1527 loss: 0.000327158428
Iter: 1528 loss: 0.000327033456
Iter: 1529 loss: 0.000326996815
Iter: 1530 loss: 0.000326986483
Iter: 1531 loss: 0.000326906506
Iter: 1532 loss: 0.000326686859
Iter: 1533 loss: 0.000327941088
Iter: 1534 loss: 0.000326621055
Iter: 1535 loss: 0.00032636337
Iter: 1536 loss: 0.000327303482
Iter: 1537 loss: 0.000326299196
Iter: 1538 loss: 0.000326040259
Iter: 1539 loss: 0.000325534784
Iter: 1540 loss: 0.000336444791
Iter: 1541 loss: 0.000325531815
Iter: 1542 loss: 0.000325393747
Iter: 1543 loss: 0.000325382309
Iter: 1544 loss: 0.000325327041
Iter: 1545 loss: 0.000325371511
Iter: 1546 loss: 0.000325293455
Iter: 1547 loss: 0.000325182016
Iter: 1548 loss: 0.000325320812
Iter: 1549 loss: 0.000325123779
Iter: 1550 loss: 0.00032491202
Iter: 1551 loss: 0.00032489904
Iter: 1552 loss: 0.000324850873
Iter: 1553 loss: 0.000324708642
Iter: 1554 loss: 0.000325180357
Iter: 1555 loss: 0.000324642984
Iter: 1556 loss: 0.000326064939
Iter: 1557 loss: 0.000324628607
Iter: 1558 loss: 0.000324611668
Iter: 1559 loss: 0.000324732857
Iter: 1560 loss: 0.000324610184
Iter: 1561 loss: 0.000324518565
Iter: 1562 loss: 0.000324439781
Iter: 1563 loss: 0.000324413093
Iter: 1564 loss: 0.000324134598
Iter: 1565 loss: 0.000324202352
Iter: 1566 loss: 0.000323929
Iter: 1567 loss: 0.000323557295
Iter: 1568 loss: 0.000324206369
Iter: 1569 loss: 0.000323391985
Iter: 1570 loss: 0.000323109591
Iter: 1571 loss: 0.000324547
Iter: 1572 loss: 0.000323064858
Iter: 1573 loss: 0.000323062937
Iter: 1574 loss: 0.000322990818
Iter: 1575 loss: 0.000322929031
Iter: 1576 loss: 0.000322785898
Iter: 1577 loss: 0.00032453693
Iter: 1578 loss: 0.000322773587
Iter: 1579 loss: 0.000322510023
Iter: 1580 loss: 0.00032249681
Iter: 1581 loss: 0.000322316511
Iter: 1582 loss: 0.000322075561
Iter: 1583 loss: 0.000322063337
Iter: 1584 loss: 0.000321954431
Iter: 1585 loss: 0.000322314881
Iter: 1586 loss: 0.00032192457
Iter: 1587 loss: 0.000321917236
Iter: 1588 loss: 0.000321890984
Iter: 1589 loss: 0.000321932195
Iter: 1590 loss: 0.000321874279
Iter: 1591 loss: 0.000321745785
Iter: 1592 loss: 0.000323392334
Iter: 1593 loss: 0.000321746338
Iter: 1594 loss: 0.000321685802
Iter: 1595 loss: 0.000321670843
Iter: 1596 loss: 0.000321633415
Iter: 1597 loss: 0.000321501633
Iter: 1598 loss: 0.000321329862
Iter: 1599 loss: 0.000321317639
Iter: 1600 loss: 0.000321169908
Iter: 1601 loss: 0.000321366475
Iter: 1602 loss: 0.000321092492
Iter: 1603 loss: 0.00032095355
Iter: 1604 loss: 0.000320801162
Iter: 1605 loss: 0.000320777588
Iter: 1606 loss: 0.000320847408
Iter: 1607 loss: 0.000320656836
Iter: 1608 loss: 0.000320501975
Iter: 1609 loss: 0.000320426159
Iter: 1610 loss: 0.000320351362
Iter: 1611 loss: 0.000320183404
Iter: 1612 loss: 0.000320543186
Iter: 1613 loss: 0.000320113846
Iter: 1614 loss: 0.000320202875
Iter: 1615 loss: 0.000320055289
Iter: 1616 loss: 0.000320030784
Iter: 1617 loss: 0.000320038758
Iter: 1618 loss: 0.000320013176
Iter: 1619 loss: 0.000319953018
Iter: 1620 loss: 0.000319848448
Iter: 1621 loss: 0.000319850165
Iter: 1622 loss: 0.000319804763
Iter: 1623 loss: 0.000319688581
Iter: 1624 loss: 0.000320439169
Iter: 1625 loss: 0.000319658982
Iter: 1626 loss: 0.000319335
Iter: 1627 loss: 0.000319554849
Iter: 1628 loss: 0.000319129787
Iter: 1629 loss: 0.000318858074
Iter: 1630 loss: 0.000318826933
Iter: 1631 loss: 0.000318664097
Iter: 1632 loss: 0.000319182815
Iter: 1633 loss: 0.000318618666
Iter: 1634 loss: 0.000318435079
Iter: 1635 loss: 0.000317904574
Iter: 1636 loss: 0.000320286665
Iter: 1637 loss: 0.000317704456
Iter: 1638 loss: 0.0003175608
Iter: 1639 loss: 0.00031747116
Iter: 1640 loss: 0.000317301543
Iter: 1641 loss: 0.000317532162
Iter: 1642 loss: 0.000317217957
Iter: 1643 loss: 0.000324661203
Iter: 1644 loss: 0.00031709121
Iter: 1645 loss: 0.000317066
Iter: 1646 loss: 0.000317051657
Iter: 1647 loss: 0.000317059283
Iter: 1648 loss: 0.000317038
Iter: 1649 loss: 0.00031702657
Iter: 1650 loss: 0.000316989899
Iter: 1651 loss: 0.000316972146
Iter: 1652 loss: 0.000316947582
Iter: 1653 loss: 0.000316850492
Iter: 1654 loss: 0.000316700782
Iter: 1655 loss: 0.000316699268
Iter: 1656 loss: 0.000316587859
Iter: 1657 loss: 0.000316537713
Iter: 1658 loss: 0.000316480815
Iter: 1659 loss: 0.000317855447
Iter: 1660 loss: 0.000316326623
Iter: 1661 loss: 0.00031609452
Iter: 1662 loss: 0.000317309692
Iter: 1663 loss: 0.000316057936
Iter: 1664 loss: 0.000315993791
Iter: 1665 loss: 0.000315780111
Iter: 1666 loss: 0.000315715384
Iter: 1667 loss: 0.000315539539
Iter: 1668 loss: 0.000317235041
Iter: 1669 loss: 0.000315475394
Iter: 1670 loss: 0.000315405719
Iter: 1671 loss: 0.000315255456
Iter: 1672 loss: 0.000317682861
Iter: 1673 loss: 0.000315250159
Iter: 1674 loss: 0.000315124867
Iter: 1675 loss: 0.000315360376
Iter: 1676 loss: 0.000315070676
Iter: 1677 loss: 0.000315053156
Iter: 1678 loss: 0.000314994657
Iter: 1679 loss: 0.000314897508
Iter: 1680 loss: 0.000314889854
Iter: 1681 loss: 0.000314999197
Iter: 1682 loss: 0.000314681616
Iter: 1683 loss: 0.000314793317
Iter: 1684 loss: 0.000314602163
Iter: 1685 loss: 0.000314525911
Iter: 1686 loss: 0.000314564502
Iter: 1687 loss: 0.000314475939
Iter: 1688 loss: 0.000314246921
Iter: 1689 loss: 0.000315968471
Iter: 1690 loss: 0.000314228906
Iter: 1691 loss: 0.000314009754
Iter: 1692 loss: 0.000314663397
Iter: 1693 loss: 0.000313944533
Iter: 1694 loss: 0.00031385661
Iter: 1695 loss: 0.000313591794
Iter: 1696 loss: 0.00031446
Iter: 1697 loss: 0.000313466386
Iter: 1698 loss: 0.000313024793
Iter: 1699 loss: 0.000315067649
Iter: 1700 loss: 0.000312943186
Iter: 1701 loss: 0.000312901655
Iter: 1702 loss: 0.000312782184
Iter: 1703 loss: 0.000315053796
Iter: 1704 loss: 0.000312747201
Iter: 1705 loss: 0.000312734512
Iter: 1706 loss: 0.000312750752
Iter: 1707 loss: 0.000312727934
Iter: 1708 loss: 0.000312712044
Iter: 1709 loss: 0.0003126511
Iter: 1710 loss: 0.0003124082
Iter: 1711 loss: 0.000312805147
Iter: 1712 loss: 0.000312241697
Iter: 1713 loss: 0.000312213699
Iter: 1714 loss: 0.000312133634
Iter: 1715 loss: 0.000312099175
Iter: 1716 loss: 0.000312065327
Iter: 1717 loss: 0.000312057906
Iter: 1718 loss: 0.000311958429
Iter: 1719 loss: 0.000311930256
Iter: 1720 loss: 0.000311780575
Iter: 1721 loss: 0.00031146055
Iter: 1722 loss: 0.000316333433
Iter: 1723 loss: 0.000311447628
Iter: 1724 loss: 0.000311336102
Iter: 1725 loss: 0.000311966636
Iter: 1726 loss: 0.000311322714
Iter: 1727 loss: 0.000311309472
Iter: 1728 loss: 0.000311272219
Iter: 1729 loss: 0.000311561482
Iter: 1730 loss: 0.000311265409
Iter: 1731 loss: 0.000311216514
Iter: 1732 loss: 0.000311052485
Iter: 1733 loss: 0.000311222131
Iter: 1734 loss: 0.000310923788
Iter: 1735 loss: 0.000310843869
Iter: 1736 loss: 0.000310833129
Iter: 1737 loss: 0.000310777337
Iter: 1738 loss: 0.000310607342
Iter: 1739 loss: 0.00031059864
Iter: 1740 loss: 0.000312378135
Iter: 1741 loss: 0.000310537871
Iter: 1742 loss: 0.000310499978
Iter: 1743 loss: 0.000310456817
Iter: 1744 loss: 0.000310451695
Iter: 1745 loss: 0.000310332602
Iter: 1746 loss: 0.000309981493
Iter: 1747 loss: 0.000311243872
Iter: 1748 loss: 0.00030982506
Iter: 1749 loss: 0.000309641066
Iter: 1750 loss: 0.000309637107
Iter: 1751 loss: 0.000309586554
Iter: 1752 loss: 0.000309505034
Iter: 1753 loss: 0.000309504219
Iter: 1754 loss: 0.000309438095
Iter: 1755 loss: 0.000309462
Iter: 1756 loss: 0.000309390656
Iter: 1757 loss: 0.000309340889
Iter: 1758 loss: 0.000309173949
Iter: 1759 loss: 0.00030910474
Iter: 1760 loss: 0.000308978197
Iter: 1761 loss: 0.00031058077
Iter: 1762 loss: 0.000308822608
Iter: 1763 loss: 0.000308702
Iter: 1764 loss: 0.000309567782
Iter: 1765 loss: 0.000308691349
Iter: 1766 loss: 0.000308652467
Iter: 1767 loss: 0.000308523886
Iter: 1768 loss: 0.00030854842
Iter: 1769 loss: 0.00030839612
Iter: 1770 loss: 0.000308238436
Iter: 1771 loss: 0.000308532093
Iter: 1772 loss: 0.000308170507
Iter: 1773 loss: 0.000308057963
Iter: 1774 loss: 0.000307850423
Iter: 1775 loss: 0.000312732067
Iter: 1776 loss: 0.000307850336
Iter: 1777 loss: 0.000307623704
Iter: 1778 loss: 0.000308227725
Iter: 1779 loss: 0.00030754687
Iter: 1780 loss: 0.000307469134
Iter: 1781 loss: 0.000307465409
Iter: 1782 loss: 0.000307375274
Iter: 1783 loss: 0.000308291201
Iter: 1784 loss: 0.000307373411
Iter: 1785 loss: 0.000307345821
Iter: 1786 loss: 0.000307338662
Iter: 1787 loss: 0.000307313458
Iter: 1788 loss: 0.000307436101
Iter: 1789 loss: 0.000307309674
Iter: 1790 loss: 0.000307281385
Iter: 1791 loss: 0.000307201641
Iter: 1792 loss: 0.000307615148
Iter: 1793 loss: 0.000307175796
Iter: 1794 loss: 0.00030713703
Iter: 1795 loss: 0.000307159469
Iter: 1796 loss: 0.000307112408
Iter: 1797 loss: 0.000307104376
Iter: 1798 loss: 0.00030709093
Iter: 1799 loss: 0.000307071634
Iter: 1800 loss: 0.0003070081
Iter: 1801 loss: 0.000307044567
Iter: 1802 loss: 0.000306953327
Iter: 1803 loss: 0.000306869857
Iter: 1804 loss: 0.000306858623
Iter: 1805 loss: 0.000306788424
Iter: 1806 loss: 0.000306794653
Iter: 1807 loss: 0.000306734815
Iter: 1808 loss: 0.000306682021
Iter: 1809 loss: 0.00030665542
Iter: 1810 loss: 0.00030663074
Iter: 1811 loss: 0.000306559581
Iter: 1812 loss: 0.000306556234
Iter: 1813 loss: 0.000306502043
Iter: 1814 loss: 0.000306628412
Iter: 1815 loss: 0.000306399423
Iter: 1816 loss: 0.000306322821
Iter: 1817 loss: 0.000306320435
Iter: 1818 loss: 0.000306283298
Iter: 1819 loss: 0.000306279573
Iter: 1820 loss: 0.000306255708
Iter: 1821 loss: 0.000306168629
Iter: 1822 loss: 0.000305992929
Iter: 1823 loss: 0.000305991358
Iter: 1824 loss: 0.000305844063
Iter: 1825 loss: 0.000307874492
Iter: 1826 loss: 0.000305843772
Iter: 1827 loss: 0.00030583248
Iter: 1828 loss: 0.000305814348
Iter: 1829 loss: 0.00030579156
Iter: 1830 loss: 0.000305720139
Iter: 1831 loss: 0.00030585326
Iter: 1832 loss: 0.000305672875
Iter: 1833 loss: 0.000305576541
Iter: 1834 loss: 0.00030597829
Iter: 1835 loss: 0.00030555611
Iter: 1836 loss: 0.000305486959
Iter: 1837 loss: 0.000306482718
Iter: 1838 loss: 0.00030548661
Iter: 1839 loss: 0.000305440102
Iter: 1840 loss: 0.000305355323
Iter: 1841 loss: 0.000307228911
Iter: 1842 loss: 0.000305354944
Iter: 1843 loss: 0.000305207737
Iter: 1844 loss: 0.000304787885
Iter: 1845 loss: 0.000306826783
Iter: 1846 loss: 0.000304645102
Iter: 1847 loss: 0.000304317538
Iter: 1848 loss: 0.000305065943
Iter: 1849 loss: 0.000304193323
Iter: 1850 loss: 0.000304384739
Iter: 1851 loss: 0.000304142362
Iter: 1852 loss: 0.000304131943
Iter: 1853 loss: 0.000304117828
Iter: 1854 loss: 0.000304106041
Iter: 1855 loss: 0.000304064946
Iter: 1856 loss: 0.000304031244
Iter: 1857 loss: 0.000304011046
Iter: 1858 loss: 0.000303947949
Iter: 1859 loss: 0.000303906
Iter: 1860 loss: 0.000303887588
Iter: 1861 loss: 0.000303856
Iter: 1862 loss: 0.000303834153
Iter: 1863 loss: 0.000303755281
Iter: 1864 loss: 0.000303508743
Iter: 1865 loss: 0.000305884809
Iter: 1866 loss: 0.000303477311
Iter: 1867 loss: 0.000303211447
Iter: 1868 loss: 0.000303855137
Iter: 1869 loss: 0.000303115
Iter: 1870 loss: 0.000303027511
Iter: 1871 loss: 0.000303953915
Iter: 1872 loss: 0.000303026347
Iter: 1873 loss: 0.000302908098
Iter: 1874 loss: 0.000303299777
Iter: 1875 loss: 0.000302876026
Iter: 1876 loss: 0.000302664062
Iter: 1877 loss: 0.000302310655
Iter: 1878 loss: 0.000302309636
Iter: 1879 loss: 0.000302103406
Iter: 1880 loss: 0.000302094093
Iter: 1881 loss: 0.000303009816
Iter: 1882 loss: 0.000302014843
Iter: 1883 loss: 0.000301978376
Iter: 1884 loss: 0.000301948
Iter: 1885 loss: 0.000301937107
Iter: 1886 loss: 0.000301817083
Iter: 1887 loss: 0.000301688822
Iter: 1888 loss: 0.00030166909
Iter: 1889 loss: 0.000301647669
Iter: 1890 loss: 0.00030158204
Iter: 1891 loss: 0.00030164831
Iter: 1892 loss: 0.000301530235
Iter: 1893 loss: 0.000307949609
Iter: 1894 loss: 0.000301512831
Iter: 1895 loss: 0.000301473978
Iter: 1896 loss: 0.000301621447
Iter: 1897 loss: 0.000301465305
Iter: 1898 loss: 0.000301436492
Iter: 1899 loss: 0.00030141254
Iter: 1900 loss: 0.00030136088
Iter: 1901 loss: 0.000301247404
Iter: 1902 loss: 0.00030301488
Iter: 1903 loss: 0.000301245396
Iter: 1904 loss: 0.000301174441
Iter: 1905 loss: 0.000301174528
Iter: 1906 loss: 0.000301128108
Iter: 1907 loss: 0.000300962129
Iter: 1908 loss: 0.000300628424
Iter: 1909 loss: 0.000300625165
Iter: 1910 loss: 0.000300356769
Iter: 1911 loss: 0.000300837215
Iter: 1912 loss: 0.000300239131
Iter: 1913 loss: 0.000300038431
Iter: 1914 loss: 0.000299495034
Iter: 1915 loss: 0.00030302626
Iter: 1916 loss: 0.000299357169
Iter: 1917 loss: 0.000305020629
Iter: 1918 loss: 0.000299343315
Iter: 1919 loss: 0.000299335807
Iter: 1920 loss: 0.000299326814
Iter: 1921 loss: 0.000299325708
Iter: 1922 loss: 0.000299292966
Iter: 1923 loss: 0.000299200939
Iter: 1924 loss: 0.000299716427
Iter: 1925 loss: 0.000299174397
Iter: 1926 loss: 0.000302554283
Iter: 1927 loss: 0.000299139327
Iter: 1928 loss: 0.000299123902
Iter: 1929 loss: 0.000299074978
Iter: 1930 loss: 0.000299056061
Iter: 1931 loss: 0.000299016887
Iter: 1932 loss: 0.000298702362
Iter: 1933 loss: 0.000303115812
Iter: 1934 loss: 0.0002987
Iter: 1935 loss: 0.000298662
Iter: 1936 loss: 0.000298407977
Iter: 1937 loss: 0.000298145576
Iter: 1938 loss: 0.00030010179
Iter: 1939 loss: 0.000298124622
Iter: 1940 loss: 0.000297980965
Iter: 1941 loss: 0.000298657484
Iter: 1942 loss: 0.000297955354
Iter: 1943 loss: 0.000297897088
Iter: 1944 loss: 0.000297755643
Iter: 1945 loss: 0.000299079955
Iter: 1946 loss: 0.00029773498
Iter: 1947 loss: 0.000297683117
Iter: 1948 loss: 0.000297651975
Iter: 1949 loss: 0.000297629245
Iter: 1950 loss: 0.000297570659
Iter: 1951 loss: 0.00029740014
Iter: 1952 loss: 0.00029816461
Iter: 1953 loss: 0.000297335093
Iter: 1954 loss: 0.000297276187
Iter: 1955 loss: 0.000298094761
Iter: 1956 loss: 0.000297276332
Iter: 1957 loss: 0.000297138351
Iter: 1958 loss: 0.000297136692
Iter: 1959 loss: 0.0002971004
Iter: 1960 loss: 0.000297092978
Iter: 1961 loss: 0.000297059974
Iter: 1962 loss: 0.000297016522
Iter: 1963 loss: 0.000297013612
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi0.4
+ date
Sun Nov  8 18:21:49 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -1 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441e4f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441f1d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441d81d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441de49d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441de47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441de4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441c816a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441cf0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441cf0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441d01840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441c328c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441ce6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441ce6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa441c106a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa410efe950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa410f02840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa410f02158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa410ecfc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa410e91950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa410e69f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa410e34d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec695620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec6cc840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec65f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec65f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa410f47378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec6e58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec6f82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec5ebb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec5f1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec593950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec59f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec59fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec576620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec4d69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3ec4b32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.40565598
test_loss: 0.4045492
train_loss: 0.40394166
test_loss: 0.40149972
train_loss: 0.39517406
test_loss: 0.39755356
train_loss: 0.39455432
test_loss: 0.39288634
train_loss: 0.38401815
test_loss: 0.3875267
train_loss: 0.37985298
test_loss: 0.381577
train_loss: 0.3761301
test_loss: 0.37515017
train_loss: 0.37356085
test_loss: 0.36814666
train_loss: 0.35939252
test_loss: 0.360699
train_loss: 0.352919
test_loss: 0.35274625
train_loss: 0.34573337
test_loss: 0.34434488
train_loss: 0.34249318
test_loss: 0.33540604
train_loss: 0.32842034
test_loss: 0.32598302
train_loss: 0.31497273
test_loss: 0.31597275
train_loss: 0.30132994
test_loss: 0.30550906
train_loss: 0.29397097
test_loss: 0.29449823
train_loss: 0.2837493
test_loss: 0.2829797
train_loss: 0.2715929
test_loss: 0.27103385
train_loss: 0.25727332
test_loss: 0.25853363
train_loss: 0.24199197
test_loss: 0.2453881
train_loss: 0.23238868
test_loss: 0.23145613
train_loss: 0.21550743
test_loss: 0.2168208
train_loss: 0.19869603
test_loss: 0.20130254
train_loss: 0.18474072
test_loss: 0.18468864
train_loss: 0.16765413
test_loss: 0.1666462
train_loss: 0.14811365
test_loss: 0.14783995
train_loss: 0.12967415
test_loss: 0.12977588
train_loss: 0.112325944
test_loss: 0.11369164
train_loss: 0.09900617
test_loss: 0.099754386
train_loss: 0.08842984
test_loss: 0.08798114
train_loss: 0.07808818
test_loss: 0.078761734
train_loss: 0.071240485
test_loss: 0.07238682
train_loss: 0.06794643
test_loss: 0.068440914
train_loss: 0.06683601
test_loss: 0.06621399
train_loss: 0.06475988
test_loss: 0.0649139
train_loss: 0.06443333
test_loss: 0.06417064
train_loss: 0.06385224
test_loss: 0.06365855
train_loss: 0.06340399
test_loss: 0.06335616
train_loss: 0.062196314
test_loss: 0.06300882
train_loss: 0.061635323
test_loss: 0.062756635
train_loss: 0.061261795
test_loss: 0.06250695
train_loss: 0.06272128
test_loss: 0.06227946
train_loss: 0.062382914
test_loss: 0.062086236
train_loss: 0.06245359
test_loss: 0.061886854
train_loss: 0.061063133
test_loss: 0.061689653
train_loss: 0.060872726
test_loss: 0.061502278
train_loss: 0.06330612
test_loss: 0.061345804
train_loss: 0.06130358
test_loss: 0.061188392
train_loss: 0.061007626
test_loss: 0.061009552
train_loss: 0.06253231
test_loss: 0.060841117
train_loss: 0.060017128
test_loss: 0.060678028
train_loss: 0.060571946
test_loss: 0.06053648
train_loss: 0.05971859
test_loss: 0.06038268
train_loss: 0.05850177
test_loss: 0.0602221
train_loss: 0.05996546
test_loss: 0.060085196
train_loss: 0.060409848
test_loss: 0.059936047
train_loss: 0.05969233
test_loss: 0.059820145
train_loss: 0.059121776
test_loss: 0.059619848
train_loss: 0.059391256
test_loss: 0.05946788
train_loss: 0.05975885
test_loss: 0.059323628
train_loss: 0.058422014
test_loss: 0.059140828
train_loss: 0.058076166
test_loss: 0.059011582
train_loss: 0.059309766
test_loss: 0.058905978
train_loss: 0.058013275
test_loss: 0.05873743
train_loss: 0.05856541
test_loss: 0.058499053
train_loss: 0.057928868
test_loss: 0.05836137
train_loss: 0.057201385
test_loss: 0.058177978
train_loss: 0.058313653
test_loss: 0.05801383
train_loss: 0.057909943
test_loss: 0.057763137
train_loss: 0.05739156
test_loss: 0.05748981
train_loss: 0.057760216
test_loss: 0.057216976
train_loss: 0.057593565
test_loss: 0.056867894
train_loss: 0.055750895
test_loss: 0.05650694
train_loss: 0.055517826
test_loss: 0.05581994
train_loss: 0.05569364
test_loss: 0.05511143
train_loss: 0.05470706
test_loss: 0.05420434
train_loss: 0.05273118
test_loss: 0.053081766
train_loss: 0.051560406
test_loss: 0.05183598
train_loss: 0.050088458
test_loss: 0.05052728
train_loss: 0.048286878
test_loss: 0.049105603
train_loss: 0.04745782
test_loss: 0.047667127
train_loss: 0.045255814
test_loss: 0.046134997
train_loss: 0.043728616
test_loss: 0.04501002
train_loss: 0.043103904
test_loss: 0.042910006
train_loss: 0.041624926
test_loss: 0.04142769
train_loss: 0.039130595
test_loss: 0.039794095
train_loss: 0.037791055
test_loss: 0.038356848
train_loss: 0.037026923
test_loss: 0.036886245
train_loss: 0.0344273
test_loss: 0.035708327
train_loss: 0.033919472
test_loss: 0.034692496
train_loss: 0.03261311
test_loss: 0.03252293
train_loss: 0.029388415
test_loss: 0.03033503
train_loss: 0.027747506
test_loss: 0.028408607
train_loss: 0.02510947
test_loss: 0.025779296
train_loss: 0.02322945
test_loss: 0.02459931
train_loss: 0.021853477
test_loss: 0.022443935
train_loss: 0.021664336
test_loss: 0.021563688
train_loss: 0.020336587
test_loss: 0.020933526
train_loss: 0.019832801
test_loss: 0.019917572
train_loss: 0.019620823
test_loss: 0.019671401
train_loss: 0.018385263
test_loss: 0.019033693
train_loss: 0.018827287
test_loss: 0.0195834
train_loss: 0.018013421
test_loss: 0.018442249
train_loss: 0.018504415
test_loss: 0.018307999
train_loss: 0.01845751
test_loss: 0.018102685
train_loss: 0.018025026
test_loss: 0.018007908
train_loss: 0.016901022
test_loss: 0.017959153
train_loss: 0.017493483
test_loss: 0.017872795
train_loss: 0.01764906
test_loss: 0.017843043
train_loss: 0.017671905
test_loss: 0.017618591
train_loss: 0.017152451
test_loss: 0.017650565
train_loss: 0.017443871
test_loss: 0.017464422
train_loss: 0.016801655
test_loss: 0.017347747
train_loss: 0.017034825
test_loss: 0.01735508
train_loss: 0.017291911
test_loss: 0.01764841
train_loss: 0.016615106
test_loss: 0.017278666
train_loss: 0.017012
test_loss: 0.017157014
train_loss: 0.016505118
test_loss: 0.017804774
train_loss: 0.016436022
test_loss: 0.017085562
train_loss: 0.017228076
test_loss: 0.016888278
train_loss: 0.017140836
test_loss: 0.016879655
train_loss: 0.016063929
test_loss: 0.01697258
train_loss: 0.017259091
test_loss: 0.01696463
train_loss: 0.016508002
test_loss: 0.016720764
train_loss: 0.016621044
test_loss: 0.017215522
train_loss: 0.01628942
test_loss: 0.016735759
train_loss: 0.016300011
test_loss: 0.01697484
train_loss: 0.01605575
test_loss: 0.01674057
train_loss: 0.016074648
test_loss: 0.016460834
train_loss: 0.016280552
test_loss: 0.016548507
train_loss: 0.015687823
test_loss: 0.016464118
train_loss: 0.015840234
test_loss: 0.016354198
train_loss: 0.01561803
test_loss: 0.017058887
train_loss: 0.016103815
test_loss: 0.016514642
train_loss: 0.015579007
test_loss: 0.016511146
train_loss: 0.015964143
test_loss: 0.016293086
train_loss: 0.0153829595
test_loss: 0.01624914
train_loss: 0.015438644
test_loss: 0.016064921
train_loss: 0.016012855
test_loss: 0.016156916
train_loss: 0.01556605
test_loss: 0.016563497
train_loss: 0.015427014
test_loss: 0.0159392
train_loss: 0.015320222
test_loss: 0.016184475
train_loss: 0.015030393
test_loss: 0.015818467
train_loss: 0.015414182
test_loss: 0.015772114
train_loss: 0.014476782
test_loss: 0.01572763
train_loss: 0.015596923
test_loss: 0.016169744
train_loss: 0.015307919
test_loss: 0.015538555
train_loss: 0.014937011
test_loss: 0.015109979
train_loss: 0.014703168
test_loss: 0.015458591
train_loss: 0.013867842
test_loss: 0.014743022
train_loss: 0.014394375
test_loss: 0.014428084
train_loss: 0.013093289
test_loss: 0.014069783
train_loss: 0.0130539555
test_loss: 0.013688643
train_loss: 0.011929593
test_loss: 0.012923643
train_loss: 0.012289535
test_loss: 0.013375517
train_loss: 0.012041069
test_loss: 0.012005014
train_loss: 0.01123532
test_loss: 0.0112175075
train_loss: 0.0097429305
test_loss: 0.0102030095
train_loss: 0.008711068
test_loss: 0.009411367
train_loss: 0.008343779
test_loss: 0.00852727
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi0.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -1 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi0.4/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a74e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a8839d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a83eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a778ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a792488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a792a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a6ccbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a6ccf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a6811e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a681488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a630e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a5f8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a611598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a5ce620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a5ce488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a55a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a562268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a52f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a4e0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a50bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe31a4afa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30e16fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30e121048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30e139bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30e141488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30e121400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30e118f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30e0d8268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30e069048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30e095ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30e03b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30dfe2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30dfe2488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30dff8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30dfbcc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe30df63730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000108047578
Iter: 2 loss: 0.00189134979
Iter: 3 loss: 8.32708683e-05
Iter: 4 loss: 7.66617e-05
Iter: 5 loss: 0.000104858016
Iter: 6 loss: 7.52038686e-05
Iter: 7 loss: 6.94771516e-05
Iter: 8 loss: 6.29460774e-05
Iter: 9 loss: 6.21361e-05
Iter: 10 loss: 6.35783072e-05
Iter: 11 loss: 6.06889e-05
Iter: 12 loss: 5.98873e-05
Iter: 13 loss: 5.81272761e-05
Iter: 14 loss: 8.38137421e-05
Iter: 15 loss: 5.80479536e-05
Iter: 16 loss: 5.61988563e-05
Iter: 17 loss: 6.50990696e-05
Iter: 18 loss: 5.58683241e-05
Iter: 19 loss: 5.4953689e-05
Iter: 20 loss: 5.56841624e-05
Iter: 21 loss: 5.4400829e-05
Iter: 22 loss: 5.34672799e-05
Iter: 23 loss: 5.62551832e-05
Iter: 24 loss: 5.318482e-05
Iter: 25 loss: 5.26201e-05
Iter: 26 loss: 5.73286634e-05
Iter: 27 loss: 5.25864962e-05
Iter: 28 loss: 5.21514e-05
Iter: 29 loss: 5.11950348e-05
Iter: 30 loss: 6.53634415e-05
Iter: 31 loss: 5.11520193e-05
Iter: 32 loss: 5.04211494e-05
Iter: 33 loss: 5.29558311e-05
Iter: 34 loss: 5.02283729e-05
Iter: 35 loss: 4.94367887e-05
Iter: 36 loss: 5.90445452e-05
Iter: 37 loss: 4.942737e-05
Iter: 38 loss: 4.90660095e-05
Iter: 39 loss: 4.81633178e-05
Iter: 40 loss: 5.67698589e-05
Iter: 41 loss: 4.80381204e-05
Iter: 42 loss: 4.72252577e-05
Iter: 43 loss: 4.99608213e-05
Iter: 44 loss: 4.70045634e-05
Iter: 45 loss: 4.68275211e-05
Iter: 46 loss: 4.67057398e-05
Iter: 47 loss: 4.63520773e-05
Iter: 48 loss: 4.68242943e-05
Iter: 49 loss: 4.61732416e-05
Iter: 50 loss: 4.59431467e-05
Iter: 51 loss: 4.55367117e-05
Iter: 52 loss: 4.55366098e-05
Iter: 53 loss: 4.51778906e-05
Iter: 54 loss: 4.81836614e-05
Iter: 55 loss: 4.51564229e-05
Iter: 56 loss: 4.47893799e-05
Iter: 57 loss: 4.63800316e-05
Iter: 58 loss: 4.47144266e-05
Iter: 59 loss: 4.45598489e-05
Iter: 60 loss: 4.45599799e-05
Iter: 61 loss: 4.43976387e-05
Iter: 62 loss: 4.40673466e-05
Iter: 63 loss: 5.00766546e-05
Iter: 64 loss: 4.40618242e-05
Iter: 65 loss: 4.38138268e-05
Iter: 66 loss: 4.46589293e-05
Iter: 67 loss: 4.374743e-05
Iter: 68 loss: 4.34112517e-05
Iter: 69 loss: 4.5212837e-05
Iter: 70 loss: 4.33598616e-05
Iter: 71 loss: 4.32072e-05
Iter: 72 loss: 4.28956191e-05
Iter: 73 loss: 4.85314886e-05
Iter: 74 loss: 4.28900385e-05
Iter: 75 loss: 4.28581043e-05
Iter: 76 loss: 4.27545274e-05
Iter: 77 loss: 4.25961407e-05
Iter: 78 loss: 4.2358166e-05
Iter: 79 loss: 4.23531637e-05
Iter: 80 loss: 4.23559177e-05
Iter: 81 loss: 4.22564772e-05
Iter: 82 loss: 4.21535442e-05
Iter: 83 loss: 4.19260832e-05
Iter: 84 loss: 4.52675e-05
Iter: 85 loss: 4.19158314e-05
Iter: 86 loss: 4.16463881e-05
Iter: 87 loss: 4.53892862e-05
Iter: 88 loss: 4.16455223e-05
Iter: 89 loss: 4.14121896e-05
Iter: 90 loss: 4.19360513e-05
Iter: 91 loss: 4.13234593e-05
Iter: 92 loss: 4.10921821e-05
Iter: 93 loss: 4.21346704e-05
Iter: 94 loss: 4.10467692e-05
Iter: 95 loss: 4.08338492e-05
Iter: 96 loss: 4.16198527e-05
Iter: 97 loss: 4.07812695e-05
Iter: 98 loss: 4.06335676e-05
Iter: 99 loss: 4.04201419e-05
Iter: 100 loss: 4.04137245e-05
Iter: 101 loss: 4.0174411e-05
Iter: 102 loss: 4.22597659e-05
Iter: 103 loss: 4.01617071e-05
Iter: 104 loss: 4.00450153e-05
Iter: 105 loss: 3.97551848e-05
Iter: 106 loss: 4.25907419e-05
Iter: 107 loss: 3.97172043e-05
Iter: 108 loss: 3.96398646e-05
Iter: 109 loss: 3.95655443e-05
Iter: 110 loss: 3.94023737e-05
Iter: 111 loss: 3.94109775e-05
Iter: 112 loss: 3.92744623e-05
Iter: 113 loss: 3.91329631e-05
Iter: 114 loss: 3.89287525e-05
Iter: 115 loss: 3.89226188e-05
Iter: 116 loss: 3.89503548e-05
Iter: 117 loss: 3.87810142e-05
Iter: 118 loss: 3.8719405e-05
Iter: 119 loss: 3.86979082e-05
Iter: 120 loss: 3.86632637e-05
Iter: 121 loss: 3.85275162e-05
Iter: 122 loss: 3.84449522e-05
Iter: 123 loss: 3.83895567e-05
Iter: 124 loss: 3.82042708e-05
Iter: 125 loss: 3.84784289e-05
Iter: 126 loss: 3.8115184e-05
Iter: 127 loss: 3.78681216e-05
Iter: 128 loss: 3.89245906e-05
Iter: 129 loss: 3.78164041e-05
Iter: 130 loss: 3.76875905e-05
Iter: 131 loss: 3.76855314e-05
Iter: 132 loss: 3.75451273e-05
Iter: 133 loss: 3.75902455e-05
Iter: 134 loss: 3.74452939e-05
Iter: 135 loss: 3.73222e-05
Iter: 136 loss: 3.80224228e-05
Iter: 137 loss: 3.73056391e-05
Iter: 138 loss: 3.71593633e-05
Iter: 139 loss: 3.70701237e-05
Iter: 140 loss: 3.70102935e-05
Iter: 141 loss: 3.68475812e-05
Iter: 142 loss: 3.66794047e-05
Iter: 143 loss: 3.6648431e-05
Iter: 144 loss: 3.67915054e-05
Iter: 145 loss: 3.65686719e-05
Iter: 146 loss: 3.65003543e-05
Iter: 147 loss: 3.63406652e-05
Iter: 148 loss: 3.82677827e-05
Iter: 149 loss: 3.63275976e-05
Iter: 150 loss: 3.62031242e-05
Iter: 151 loss: 3.61836173e-05
Iter: 152 loss: 3.61200291e-05
Iter: 153 loss: 3.60534468e-05
Iter: 154 loss: 3.60414415e-05
Iter: 155 loss: 3.58639882e-05
Iter: 156 loss: 3.66107597e-05
Iter: 157 loss: 3.58265606e-05
Iter: 158 loss: 3.56906967e-05
Iter: 159 loss: 3.56902128e-05
Iter: 160 loss: 3.5612793e-05
Iter: 161 loss: 3.55070515e-05
Iter: 162 loss: 3.55020784e-05
Iter: 163 loss: 3.54819e-05
Iter: 164 loss: 3.54294607e-05
Iter: 165 loss: 3.53695505e-05
Iter: 166 loss: 3.52519746e-05
Iter: 167 loss: 3.76002063e-05
Iter: 168 loss: 3.52511088e-05
Iter: 169 loss: 3.51829e-05
Iter: 170 loss: 3.51695162e-05
Iter: 171 loss: 3.5123725e-05
Iter: 172 loss: 3.49940419e-05
Iter: 173 loss: 3.5654135e-05
Iter: 174 loss: 3.49518632e-05
Iter: 175 loss: 3.4773926e-05
Iter: 176 loss: 3.48936519e-05
Iter: 177 loss: 3.4661869e-05
Iter: 178 loss: 3.4768258e-05
Iter: 179 loss: 3.45893495e-05
Iter: 180 loss: 3.45194e-05
Iter: 181 loss: 3.43725551e-05
Iter: 182 loss: 3.68706824e-05
Iter: 183 loss: 3.43692191e-05
Iter: 184 loss: 3.43148313e-05
Iter: 185 loss: 3.42928797e-05
Iter: 186 loss: 3.42060084e-05
Iter: 187 loss: 3.39762955e-05
Iter: 188 loss: 3.56822711e-05
Iter: 189 loss: 3.39282451e-05
Iter: 190 loss: 3.37637648e-05
Iter: 191 loss: 3.38108366e-05
Iter: 192 loss: 3.36448429e-05
Iter: 193 loss: 3.35006553e-05
Iter: 194 loss: 3.40840634e-05
Iter: 195 loss: 3.34685865e-05
Iter: 196 loss: 3.32956843e-05
Iter: 197 loss: 3.35074728e-05
Iter: 198 loss: 3.3205517e-05
Iter: 199 loss: 3.30457769e-05
Iter: 200 loss: 3.48168251e-05
Iter: 201 loss: 3.30426301e-05
Iter: 202 loss: 3.29588511e-05
Iter: 203 loss: 3.29700779e-05
Iter: 204 loss: 3.28950555e-05
Iter: 205 loss: 3.27722846e-05
Iter: 206 loss: 3.37362e-05
Iter: 207 loss: 3.27636189e-05
Iter: 208 loss: 3.27039597e-05
Iter: 209 loss: 3.25371402e-05
Iter: 210 loss: 3.34254109e-05
Iter: 211 loss: 3.24845241e-05
Iter: 212 loss: 3.230535e-05
Iter: 213 loss: 3.36761696e-05
Iter: 214 loss: 3.22918713e-05
Iter: 215 loss: 3.22281048e-05
Iter: 216 loss: 3.21842672e-05
Iter: 217 loss: 3.21461812e-05
Iter: 218 loss: 3.20883082e-05
Iter: 219 loss: 3.20872132e-05
Iter: 220 loss: 3.19778883e-05
Iter: 221 loss: 3.26304871e-05
Iter: 222 loss: 3.19637402e-05
Iter: 223 loss: 3.19151e-05
Iter: 224 loss: 3.17964259e-05
Iter: 225 loss: 3.30228577e-05
Iter: 226 loss: 3.17819613e-05
Iter: 227 loss: 3.16430742e-05
Iter: 228 loss: 3.24882058e-05
Iter: 229 loss: 3.16260703e-05
Iter: 230 loss: 3.15077232e-05
Iter: 231 loss: 3.21307125e-05
Iter: 232 loss: 3.14892677e-05
Iter: 233 loss: 3.13974269e-05
Iter: 234 loss: 3.17133272e-05
Iter: 235 loss: 3.13731689e-05
Iter: 236 loss: 3.12665397e-05
Iter: 237 loss: 3.12295961e-05
Iter: 238 loss: 3.11690746e-05
Iter: 239 loss: 3.10804135e-05
Iter: 240 loss: 3.10767027e-05
Iter: 241 loss: 3.10172254e-05
Iter: 242 loss: 3.08706731e-05
Iter: 243 loss: 3.23419263e-05
Iter: 244 loss: 3.08525487e-05
Iter: 245 loss: 3.06881921e-05
Iter: 246 loss: 3.06995e-05
Iter: 247 loss: 3.05598878e-05
Iter: 248 loss: 3.0572588e-05
Iter: 249 loss: 3.04810765e-05
Iter: 250 loss: 3.03869583e-05
Iter: 251 loss: 3.02652443e-05
Iter: 252 loss: 3.02571516e-05
Iter: 253 loss: 3.01973596e-05
Iter: 254 loss: 3.01938853e-05
Iter: 255 loss: 3.01148066e-05
Iter: 256 loss: 2.99907169e-05
Iter: 257 loss: 2.99891526e-05
Iter: 258 loss: 2.99105577e-05
Iter: 259 loss: 2.98741797e-05
Iter: 260 loss: 2.98352406e-05
Iter: 261 loss: 2.97458719e-05
Iter: 262 loss: 2.97302322e-05
Iter: 263 loss: 2.96627113e-05
Iter: 264 loss: 2.99156072e-05
Iter: 265 loss: 2.96462676e-05
Iter: 266 loss: 2.95818991e-05
Iter: 267 loss: 2.98045852e-05
Iter: 268 loss: 2.95649843e-05
Iter: 269 loss: 2.95111113e-05
Iter: 270 loss: 2.94716738e-05
Iter: 271 loss: 2.94531274e-05
Iter: 272 loss: 2.93659505e-05
Iter: 273 loss: 2.94435322e-05
Iter: 274 loss: 2.93152789e-05
Iter: 275 loss: 2.91733177e-05
Iter: 276 loss: 3.0080455e-05
Iter: 277 loss: 2.91574725e-05
Iter: 278 loss: 2.90962635e-05
Iter: 279 loss: 2.89775671e-05
Iter: 280 loss: 3.13968558e-05
Iter: 281 loss: 2.89767413e-05
Iter: 282 loss: 2.89840173e-05
Iter: 283 loss: 2.89230593e-05
Iter: 284 loss: 2.8877992e-05
Iter: 285 loss: 2.87498551e-05
Iter: 286 loss: 2.93576504e-05
Iter: 287 loss: 2.8704635e-05
Iter: 288 loss: 2.87631447e-05
Iter: 289 loss: 2.86336235e-05
Iter: 290 loss: 2.86031463e-05
Iter: 291 loss: 2.85238748e-05
Iter: 292 loss: 2.91677898e-05
Iter: 293 loss: 2.8509392e-05
Iter: 294 loss: 2.83372319e-05
Iter: 295 loss: 2.83406862e-05
Iter: 296 loss: 2.82009169e-05
Iter: 297 loss: 2.83961454e-05
Iter: 298 loss: 2.81109515e-05
Iter: 299 loss: 2.80385648e-05
Iter: 300 loss: 2.79974211e-05
Iter: 301 loss: 2.79662781e-05
Iter: 302 loss: 2.78565803e-05
Iter: 303 loss: 2.83252903e-05
Iter: 304 loss: 2.7833883e-05
Iter: 305 loss: 2.77502568e-05
Iter: 306 loss: 2.76649862e-05
Iter: 307 loss: 2.76485571e-05
Iter: 308 loss: 2.75737784e-05
Iter: 309 loss: 2.75569582e-05
Iter: 310 loss: 2.7492657e-05
Iter: 311 loss: 2.73213118e-05
Iter: 312 loss: 2.85120841e-05
Iter: 313 loss: 2.728238e-05
Iter: 314 loss: 2.71373865e-05
Iter: 315 loss: 2.93388966e-05
Iter: 316 loss: 2.71374138e-05
Iter: 317 loss: 2.69831889e-05
Iter: 318 loss: 2.78649732e-05
Iter: 319 loss: 2.69624288e-05
Iter: 320 loss: 2.6902635e-05
Iter: 321 loss: 2.6969703e-05
Iter: 322 loss: 2.68705153e-05
Iter: 323 loss: 2.67616e-05
Iter: 324 loss: 2.73466103e-05
Iter: 325 loss: 2.67453288e-05
Iter: 326 loss: 2.66822153e-05
Iter: 327 loss: 2.65592316e-05
Iter: 328 loss: 2.9092409e-05
Iter: 329 loss: 2.65585495e-05
Iter: 330 loss: 2.63990787e-05
Iter: 331 loss: 2.66244333e-05
Iter: 332 loss: 2.63206766e-05
Iter: 333 loss: 2.63430538e-05
Iter: 334 loss: 2.62672656e-05
Iter: 335 loss: 2.62218291e-05
Iter: 336 loss: 2.61039e-05
Iter: 337 loss: 2.70198525e-05
Iter: 338 loss: 2.60810666e-05
Iter: 339 loss: 2.59756525e-05
Iter: 340 loss: 2.59746193e-05
Iter: 341 loss: 2.59080116e-05
Iter: 342 loss: 2.57430729e-05
Iter: 343 loss: 2.73254809e-05
Iter: 344 loss: 2.57204483e-05
Iter: 345 loss: 2.55916129e-05
Iter: 346 loss: 2.55911691e-05
Iter: 347 loss: 2.54686256e-05
Iter: 348 loss: 2.62516351e-05
Iter: 349 loss: 2.5455005e-05
Iter: 350 loss: 2.53998805e-05
Iter: 351 loss: 2.5263862e-05
Iter: 352 loss: 2.66187562e-05
Iter: 353 loss: 2.5246607e-05
Iter: 354 loss: 2.53182807e-05
Iter: 355 loss: 2.51855272e-05
Iter: 356 loss: 2.51406691e-05
Iter: 357 loss: 2.51036945e-05
Iter: 358 loss: 2.50907797e-05
Iter: 359 loss: 2.49651675e-05
Iter: 360 loss: 2.56615294e-05
Iter: 361 loss: 2.49470504e-05
Iter: 362 loss: 2.49107379e-05
Iter: 363 loss: 2.48800188e-05
Iter: 364 loss: 2.48476827e-05
Iter: 365 loss: 2.47719363e-05
Iter: 366 loss: 2.56917392e-05
Iter: 367 loss: 2.47655371e-05
Iter: 368 loss: 2.4666715e-05
Iter: 369 loss: 2.55713603e-05
Iter: 370 loss: 2.46621057e-05
Iter: 371 loss: 2.46214713e-05
Iter: 372 loss: 2.45315678e-05
Iter: 373 loss: 2.58332384e-05
Iter: 374 loss: 2.45272367e-05
Iter: 375 loss: 2.44348339e-05
Iter: 376 loss: 2.58029686e-05
Iter: 377 loss: 2.4434732e-05
Iter: 378 loss: 2.43794166e-05
Iter: 379 loss: 2.42287424e-05
Iter: 380 loss: 2.51695856e-05
Iter: 381 loss: 2.41893867e-05
Iter: 382 loss: 2.41758135e-05
Iter: 383 loss: 2.41127927e-05
Iter: 384 loss: 2.40259542e-05
Iter: 385 loss: 2.40024037e-05
Iter: 386 loss: 2.39489455e-05
Iter: 387 loss: 2.38786779e-05
Iter: 388 loss: 2.39634901e-05
Iter: 389 loss: 2.38417633e-05
Iter: 390 loss: 2.37744753e-05
Iter: 391 loss: 2.40015233e-05
Iter: 392 loss: 2.37561017e-05
Iter: 393 loss: 2.36855158e-05
Iter: 394 loss: 2.36968262e-05
Iter: 395 loss: 2.36323685e-05
Iter: 396 loss: 2.35615116e-05
Iter: 397 loss: 2.37162476e-05
Iter: 398 loss: 2.35339e-05
Iter: 399 loss: 2.34551e-05
Iter: 400 loss: 2.35602529e-05
Iter: 401 loss: 2.34155705e-05
Iter: 402 loss: 2.3351402e-05
Iter: 403 loss: 2.32888269e-05
Iter: 404 loss: 2.32750426e-05
Iter: 405 loss: 2.31899066e-05
Iter: 406 loss: 2.31828835e-05
Iter: 407 loss: 2.31579725e-05
Iter: 408 loss: 2.30856385e-05
Iter: 409 loss: 2.33769169e-05
Iter: 410 loss: 2.30564619e-05
Iter: 411 loss: 2.29622656e-05
Iter: 412 loss: 2.44100538e-05
Iter: 413 loss: 2.29624202e-05
Iter: 414 loss: 2.29074867e-05
Iter: 415 loss: 2.28317786e-05
Iter: 416 loss: 2.28285498e-05
Iter: 417 loss: 2.2753411e-05
Iter: 418 loss: 2.3183271e-05
Iter: 419 loss: 2.2743232e-05
Iter: 420 loss: 2.26515367e-05
Iter: 421 loss: 2.28292811e-05
Iter: 422 loss: 2.26135326e-05
Iter: 423 loss: 2.25500116e-05
Iter: 424 loss: 2.28231729e-05
Iter: 425 loss: 2.2537075e-05
Iter: 426 loss: 2.24802843e-05
Iter: 427 loss: 2.2507571e-05
Iter: 428 loss: 2.24422038e-05
Iter: 429 loss: 2.23724674e-05
Iter: 430 loss: 2.31030754e-05
Iter: 431 loss: 2.2370572e-05
Iter: 432 loss: 2.23330899e-05
Iter: 433 loss: 2.22431827e-05
Iter: 434 loss: 2.32440434e-05
Iter: 435 loss: 2.22341805e-05
Iter: 436 loss: 2.21388946e-05
Iter: 437 loss: 2.25744407e-05
Iter: 438 loss: 2.21209048e-05
Iter: 439 loss: 2.20574293e-05
Iter: 440 loss: 2.21589507e-05
Iter: 441 loss: 2.20279617e-05
Iter: 442 loss: 2.19245048e-05
Iter: 443 loss: 2.24466912e-05
Iter: 444 loss: 2.19074082e-05
Iter: 445 loss: 2.18591667e-05
Iter: 446 loss: 2.17473353e-05
Iter: 447 loss: 2.3118715e-05
Iter: 448 loss: 2.17388333e-05
Iter: 449 loss: 2.16777917e-05
Iter: 450 loss: 2.1674985e-05
Iter: 451 loss: 2.16065091e-05
Iter: 452 loss: 2.15523469e-05
Iter: 453 loss: 2.15314194e-05
Iter: 454 loss: 2.14678766e-05
Iter: 455 loss: 2.22537092e-05
Iter: 456 loss: 2.14671618e-05
Iter: 457 loss: 2.13931162e-05
Iter: 458 loss: 2.14765314e-05
Iter: 459 loss: 2.13533131e-05
Iter: 460 loss: 2.12906707e-05
Iter: 461 loss: 2.14447664e-05
Iter: 462 loss: 2.12683826e-05
Iter: 463 loss: 2.12392442e-05
Iter: 464 loss: 2.12296509e-05
Iter: 465 loss: 2.12092327e-05
Iter: 466 loss: 2.11805527e-05
Iter: 467 loss: 2.11793376e-05
Iter: 468 loss: 2.11429488e-05
Iter: 469 loss: 2.14615666e-05
Iter: 470 loss: 2.11410552e-05
Iter: 471 loss: 2.11188835e-05
Iter: 472 loss: 2.11067127e-05
Iter: 473 loss: 2.10968428e-05
Iter: 474 loss: 2.10614089e-05
Iter: 475 loss: 2.13445546e-05
Iter: 476 loss: 2.10590733e-05
Iter: 477 loss: 2.1018e-05
Iter: 478 loss: 2.09624213e-05
Iter: 479 loss: 2.09594582e-05
Iter: 480 loss: 2.09080354e-05
Iter: 481 loss: 2.08397578e-05
Iter: 482 loss: 2.08358088e-05
Iter: 483 loss: 2.08049987e-05
Iter: 484 loss: 2.07936555e-05
Iter: 485 loss: 2.07483299e-05
Iter: 486 loss: 2.06726254e-05
Iter: 487 loss: 2.06723962e-05
Iter: 488 loss: 2.06294171e-05
Iter: 489 loss: 2.06258101e-05
Iter: 490 loss: 2.05707402e-05
Iter: 491 loss: 2.04885691e-05
Iter: 492 loss: 2.04867847e-05
Iter: 493 loss: 2.04438802e-05
Iter: 494 loss: 2.10892758e-05
Iter: 495 loss: 2.04439475e-05
Iter: 496 loss: 2.03957643e-05
Iter: 497 loss: 2.0546915e-05
Iter: 498 loss: 2.0381689e-05
Iter: 499 loss: 2.03542259e-05
Iter: 500 loss: 2.04227435e-05
Iter: 501 loss: 2.03445088e-05
Iter: 502 loss: 2.03050258e-05
Iter: 503 loss: 2.02338815e-05
Iter: 504 loss: 2.1981903e-05
Iter: 505 loss: 2.02336014e-05
Iter: 506 loss: 2.01759231e-05
Iter: 507 loss: 2.09388327e-05
Iter: 508 loss: 2.01755975e-05
Iter: 509 loss: 2.01206421e-05
Iter: 510 loss: 2.01371895e-05
Iter: 511 loss: 2.00813483e-05
Iter: 512 loss: 2.00228314e-05
Iter: 513 loss: 1.99274036e-05
Iter: 514 loss: 1.99268543e-05
Iter: 515 loss: 1.98152047e-05
Iter: 516 loss: 2.00637533e-05
Iter: 517 loss: 1.97726604e-05
Iter: 518 loss: 1.97179797e-05
Iter: 519 loss: 1.97054196e-05
Iter: 520 loss: 1.96567617e-05
Iter: 521 loss: 1.95690336e-05
Iter: 522 loss: 2.16776643e-05
Iter: 523 loss: 1.95690482e-05
Iter: 524 loss: 1.95507891e-05
Iter: 525 loss: 1.95185239e-05
Iter: 526 loss: 1.94891145e-05
Iter: 527 loss: 1.94016975e-05
Iter: 528 loss: 1.96910714e-05
Iter: 529 loss: 1.93602755e-05
Iter: 530 loss: 1.93830474e-05
Iter: 531 loss: 1.93314791e-05
Iter: 532 loss: 1.92958796e-05
Iter: 533 loss: 1.92587104e-05
Iter: 534 loss: 1.92523075e-05
Iter: 535 loss: 1.92191419e-05
Iter: 536 loss: 1.96160472e-05
Iter: 537 loss: 1.92187654e-05
Iter: 538 loss: 1.91863328e-05
Iter: 539 loss: 1.9192501e-05
Iter: 540 loss: 1.91621657e-05
Iter: 541 loss: 1.91133513e-05
Iter: 542 loss: 1.93623964e-05
Iter: 543 loss: 1.91052823e-05
Iter: 544 loss: 1.90693063e-05
Iter: 545 loss: 1.90196533e-05
Iter: 546 loss: 1.90173778e-05
Iter: 547 loss: 1.89527127e-05
Iter: 548 loss: 1.9107687e-05
Iter: 549 loss: 1.89294915e-05
Iter: 550 loss: 1.88676222e-05
Iter: 551 loss: 1.91907129e-05
Iter: 552 loss: 1.88580489e-05
Iter: 553 loss: 1.87973164e-05
Iter: 554 loss: 1.88085669e-05
Iter: 555 loss: 1.87521255e-05
Iter: 556 loss: 1.87201331e-05
Iter: 557 loss: 1.87109799e-05
Iter: 558 loss: 1.86750804e-05
Iter: 559 loss: 1.86660509e-05
Iter: 560 loss: 1.86437392e-05
Iter: 561 loss: 1.85966801e-05
Iter: 562 loss: 1.9244646e-05
Iter: 563 loss: 1.85965637e-05
Iter: 564 loss: 1.85749668e-05
Iter: 565 loss: 1.85081517e-05
Iter: 566 loss: 1.86417928e-05
Iter: 567 loss: 1.84660112e-05
Iter: 568 loss: 1.86790603e-05
Iter: 569 loss: 1.84443052e-05
Iter: 570 loss: 1.84302844e-05
Iter: 571 loss: 1.8387138e-05
Iter: 572 loss: 1.85104054e-05
Iter: 573 loss: 1.83643151e-05
Iter: 574 loss: 1.83278098e-05
Iter: 575 loss: 1.83228221e-05
Iter: 576 loss: 1.83005395e-05
Iter: 577 loss: 1.82552758e-05
Iter: 578 loss: 1.90846258e-05
Iter: 579 loss: 1.82546464e-05
Iter: 580 loss: 1.81920987e-05
Iter: 581 loss: 1.88287e-05
Iter: 582 loss: 1.81903561e-05
Iter: 583 loss: 1.81577125e-05
Iter: 584 loss: 1.80796123e-05
Iter: 585 loss: 1.89384536e-05
Iter: 586 loss: 1.80715106e-05
Iter: 587 loss: 1.79913768e-05
Iter: 588 loss: 1.8511686e-05
Iter: 589 loss: 1.79825165e-05
Iter: 590 loss: 1.79205872e-05
Iter: 591 loss: 1.85719473e-05
Iter: 592 loss: 1.79189228e-05
Iter: 593 loss: 1.78767623e-05
Iter: 594 loss: 1.79791059e-05
Iter: 595 loss: 1.7861541e-05
Iter: 596 loss: 1.78259197e-05
Iter: 597 loss: 1.81203868e-05
Iter: 598 loss: 1.78236332e-05
Iter: 599 loss: 1.77896109e-05
Iter: 600 loss: 1.77266338e-05
Iter: 601 loss: 1.91998442e-05
Iter: 602 loss: 1.77267048e-05
Iter: 603 loss: 1.77143047e-05
Iter: 604 loss: 1.76951544e-05
Iter: 605 loss: 1.76762787e-05
Iter: 606 loss: 1.76150697e-05
Iter: 607 loss: 1.76599424e-05
Iter: 608 loss: 1.75627574e-05
Iter: 609 loss: 1.75440528e-05
Iter: 610 loss: 1.752e-05
Iter: 611 loss: 1.74773159e-05
Iter: 612 loss: 1.73834433e-05
Iter: 613 loss: 1.87584592e-05
Iter: 614 loss: 1.73791632e-05
Iter: 615 loss: 1.73508251e-05
Iter: 616 loss: 1.73364988e-05
Iter: 617 loss: 1.72949512e-05
Iter: 618 loss: 1.72168693e-05
Iter: 619 loss: 1.89320599e-05
Iter: 620 loss: 1.72165091e-05
Iter: 621 loss: 1.7159713e-05
Iter: 622 loss: 1.72176915e-05
Iter: 623 loss: 1.71277225e-05
Iter: 624 loss: 1.70920102e-05
Iter: 625 loss: 1.70186358e-05
Iter: 626 loss: 1.83203247e-05
Iter: 627 loss: 1.70172971e-05
Iter: 628 loss: 1.69530704e-05
Iter: 629 loss: 1.70497206e-05
Iter: 630 loss: 1.6922495e-05
Iter: 631 loss: 1.68763145e-05
Iter: 632 loss: 1.68763272e-05
Iter: 633 loss: 1.68311271e-05
Iter: 634 loss: 1.68339229e-05
Iter: 635 loss: 1.6795595e-05
Iter: 636 loss: 1.67467242e-05
Iter: 637 loss: 1.67364342e-05
Iter: 638 loss: 1.67041617e-05
Iter: 639 loss: 1.66732807e-05
Iter: 640 loss: 1.66570499e-05
Iter: 641 loss: 1.66368227e-05
Iter: 642 loss: 1.65761539e-05
Iter: 643 loss: 1.67374728e-05
Iter: 644 loss: 1.65431757e-05
Iter: 645 loss: 1.65503643e-05
Iter: 646 loss: 1.65066831e-05
Iter: 647 loss: 1.64830963e-05
Iter: 648 loss: 1.64250196e-05
Iter: 649 loss: 1.70244366e-05
Iter: 650 loss: 1.64183184e-05
Iter: 651 loss: 1.63917521e-05
Iter: 652 loss: 1.63796758e-05
Iter: 653 loss: 1.63550831e-05
Iter: 654 loss: 1.63301083e-05
Iter: 655 loss: 1.63252225e-05
Iter: 656 loss: 1.62870019e-05
Iter: 657 loss: 1.68539282e-05
Iter: 658 loss: 1.62870438e-05
Iter: 659 loss: 1.6246111e-05
Iter: 660 loss: 1.61504595e-05
Iter: 661 loss: 1.72947348e-05
Iter: 662 loss: 1.61425814e-05
Iter: 663 loss: 1.60715863e-05
Iter: 664 loss: 1.62506258e-05
Iter: 665 loss: 1.60467844e-05
Iter: 666 loss: 1.5976846e-05
Iter: 667 loss: 1.68915394e-05
Iter: 668 loss: 1.59763149e-05
Iter: 669 loss: 1.59340816e-05
Iter: 670 loss: 1.59438096e-05
Iter: 671 loss: 1.59030205e-05
Iter: 672 loss: 1.58683542e-05
Iter: 673 loss: 1.6146605e-05
Iter: 674 loss: 1.58659041e-05
Iter: 675 loss: 1.58422445e-05
Iter: 676 loss: 1.57948489e-05
Iter: 677 loss: 1.67084e-05
Iter: 678 loss: 1.57943905e-05
Iter: 679 loss: 1.57763861e-05
Iter: 680 loss: 1.57698851e-05
Iter: 681 loss: 1.57476952e-05
Iter: 682 loss: 1.5714304e-05
Iter: 683 loss: 1.57137547e-05
Iter: 684 loss: 1.5692196e-05
Iter: 685 loss: 1.56911155e-05
Iter: 686 loss: 1.56730748e-05
Iter: 687 loss: 1.56363949e-05
Iter: 688 loss: 1.63192071e-05
Iter: 689 loss: 1.56359e-05
Iter: 690 loss: 1.56028582e-05
Iter: 691 loss: 1.60186864e-05
Iter: 692 loss: 1.56025017e-05
Iter: 693 loss: 1.55673e-05
Iter: 694 loss: 1.557629e-05
Iter: 695 loss: 1.55416674e-05
Iter: 696 loss: 1.55110483e-05
Iter: 697 loss: 1.54844238e-05
Iter: 698 loss: 1.5476111e-05
Iter: 699 loss: 1.54203444e-05
Iter: 700 loss: 1.60797335e-05
Iter: 701 loss: 1.54195695e-05
Iter: 702 loss: 1.5378535e-05
Iter: 703 loss: 1.57311115e-05
Iter: 704 loss: 1.53761448e-05
Iter: 705 loss: 1.53477558e-05
Iter: 706 loss: 1.56858205e-05
Iter: 707 loss: 1.53474357e-05
Iter: 708 loss: 1.5326641e-05
Iter: 709 loss: 1.52856519e-05
Iter: 710 loss: 1.60728105e-05
Iter: 711 loss: 1.5284988e-05
Iter: 712 loss: 1.5253172e-05
Iter: 713 loss: 1.55894359e-05
Iter: 714 loss: 1.52523635e-05
Iter: 715 loss: 1.52119128e-05
Iter: 716 loss: 1.52018256e-05
Iter: 717 loss: 1.51762979e-05
Iter: 718 loss: 1.51403547e-05
Iter: 719 loss: 1.5401276e-05
Iter: 720 loss: 1.51375061e-05
Iter: 721 loss: 1.5096095e-05
Iter: 722 loss: 1.50799042e-05
Iter: 723 loss: 1.50576707e-05
Iter: 724 loss: 1.50235164e-05
Iter: 725 loss: 1.52659704e-05
Iter: 726 loss: 1.50205797e-05
Iter: 727 loss: 1.49844291e-05
Iter: 728 loss: 1.5061366e-05
Iter: 729 loss: 1.49702882e-05
Iter: 730 loss: 1.49406433e-05
Iter: 731 loss: 1.48732852e-05
Iter: 732 loss: 1.57762806e-05
Iter: 733 loss: 1.48692125e-05
Iter: 734 loss: 1.48077461e-05
Iter: 735 loss: 1.55655762e-05
Iter: 736 loss: 1.48073341e-05
Iter: 737 loss: 1.47588626e-05
Iter: 738 loss: 1.51622198e-05
Iter: 739 loss: 1.47560113e-05
Iter: 740 loss: 1.47328719e-05
Iter: 741 loss: 1.47328938e-05
Iter: 742 loss: 1.47159208e-05
Iter: 743 loss: 1.46761977e-05
Iter: 744 loss: 1.51595505e-05
Iter: 745 loss: 1.46729963e-05
Iter: 746 loss: 1.46358925e-05
Iter: 747 loss: 1.48445433e-05
Iter: 748 loss: 1.46311659e-05
Iter: 749 loss: 1.45864469e-05
Iter: 750 loss: 1.48782074e-05
Iter: 751 loss: 1.45816502e-05
Iter: 752 loss: 1.45608e-05
Iter: 753 loss: 1.45573558e-05
Iter: 754 loss: 1.45426693e-05
Iter: 755 loss: 1.44993792e-05
Iter: 756 loss: 1.45620106e-05
Iter: 757 loss: 1.4478449e-05
Iter: 758 loss: 1.44458636e-05
Iter: 759 loss: 1.45003069e-05
Iter: 760 loss: 1.44311398e-05
Iter: 761 loss: 1.43882116e-05
Iter: 762 loss: 1.46186312e-05
Iter: 763 loss: 1.43818161e-05
Iter: 764 loss: 1.43530469e-05
Iter: 765 loss: 1.42899744e-05
Iter: 766 loss: 1.5223367e-05
Iter: 767 loss: 1.42872614e-05
Iter: 768 loss: 1.4219796e-05
Iter: 769 loss: 1.43976931e-05
Iter: 770 loss: 1.41967721e-05
Iter: 771 loss: 1.41344117e-05
Iter: 772 loss: 1.41345481e-05
Iter: 773 loss: 1.40997281e-05
Iter: 774 loss: 1.44364203e-05
Iter: 775 loss: 1.40985703e-05
Iter: 776 loss: 1.40641787e-05
Iter: 777 loss: 1.3998274e-05
Iter: 778 loss: 1.53703586e-05
Iter: 779 loss: 1.39979011e-05
Iter: 780 loss: 1.39340937e-05
Iter: 781 loss: 1.39469121e-05
Iter: 782 loss: 1.38867199e-05
Iter: 783 loss: 1.38443165e-05
Iter: 784 loss: 1.38302221e-05
Iter: 785 loss: 1.38060914e-05
Iter: 786 loss: 1.37688885e-05
Iter: 787 loss: 1.37683846e-05
Iter: 788 loss: 1.37203697e-05
Iter: 789 loss: 1.4260736e-05
Iter: 790 loss: 1.37194311e-05
Iter: 791 loss: 1.36940926e-05
Iter: 792 loss: 1.36502076e-05
Iter: 793 loss: 1.36501367e-05
Iter: 794 loss: 1.36097769e-05
Iter: 795 loss: 1.36078379e-05
Iter: 796 loss: 1.35795199e-05
Iter: 797 loss: 1.3531866e-05
Iter: 798 loss: 1.35316968e-05
Iter: 799 loss: 1.34880929e-05
Iter: 800 loss: 1.34163656e-05
Iter: 801 loss: 1.34160282e-05
Iter: 802 loss: 1.3372437e-05
Iter: 803 loss: 1.37673851e-05
Iter: 804 loss: 1.33702724e-05
Iter: 805 loss: 1.33592839e-05
Iter: 806 loss: 1.33502799e-05
Iter: 807 loss: 1.33385365e-05
Iter: 808 loss: 1.33204194e-05
Iter: 809 loss: 1.3320293e-05
Iter: 810 loss: 1.32925979e-05
Iter: 811 loss: 1.33027488e-05
Iter: 812 loss: 1.32729592e-05
Iter: 813 loss: 1.32465793e-05
Iter: 814 loss: 1.32464902e-05
Iter: 815 loss: 1.32115802e-05
Iter: 816 loss: 1.32296555e-05
Iter: 817 loss: 1.31884663e-05
Iter: 818 loss: 1.31671941e-05
Iter: 819 loss: 1.33392878e-05
Iter: 820 loss: 1.31657234e-05
Iter: 821 loss: 1.31426823e-05
Iter: 822 loss: 1.31048064e-05
Iter: 823 loss: 1.31045181e-05
Iter: 824 loss: 1.30750032e-05
Iter: 825 loss: 1.3414854e-05
Iter: 826 loss: 1.30745257e-05
Iter: 827 loss: 1.30412209e-05
Iter: 828 loss: 1.30349681e-05
Iter: 829 loss: 1.30124927e-05
Iter: 830 loss: 1.29843029e-05
Iter: 831 loss: 1.29678756e-05
Iter: 832 loss: 1.29560585e-05
Iter: 833 loss: 1.2913606e-05
Iter: 834 loss: 1.32249479e-05
Iter: 835 loss: 1.29099953e-05
Iter: 836 loss: 1.28749543e-05
Iter: 837 loss: 1.29225864e-05
Iter: 838 loss: 1.28571883e-05
Iter: 839 loss: 1.28223583e-05
Iter: 840 loss: 1.28222182e-05
Iter: 841 loss: 1.28039592e-05
Iter: 842 loss: 1.27716903e-05
Iter: 843 loss: 1.27717685e-05
Iter: 844 loss: 1.27368276e-05
Iter: 845 loss: 1.27207959e-05
Iter: 846 loss: 1.27033773e-05
Iter: 847 loss: 1.27056228e-05
Iter: 848 loss: 1.26846971e-05
Iter: 849 loss: 1.26742625e-05
Iter: 850 loss: 1.26448122e-05
Iter: 851 loss: 1.2786455e-05
Iter: 852 loss: 1.26345421e-05
Iter: 853 loss: 1.26157565e-05
Iter: 854 loss: 1.26111509e-05
Iter: 855 loss: 1.25917686e-05
Iter: 856 loss: 1.25455454e-05
Iter: 857 loss: 1.30544477e-05
Iter: 858 loss: 1.25408624e-05
Iter: 859 loss: 1.25065217e-05
Iter: 860 loss: 1.25058259e-05
Iter: 861 loss: 1.24614799e-05
Iter: 862 loss: 1.25750194e-05
Iter: 863 loss: 1.24462531e-05
Iter: 864 loss: 1.24192256e-05
Iter: 865 loss: 1.23940426e-05
Iter: 866 loss: 1.23876225e-05
Iter: 867 loss: 1.23428308e-05
Iter: 868 loss: 1.26563718e-05
Iter: 869 loss: 1.23388272e-05
Iter: 870 loss: 1.23249265e-05
Iter: 871 loss: 1.23223308e-05
Iter: 872 loss: 1.23055925e-05
Iter: 873 loss: 1.22693073e-05
Iter: 874 loss: 1.28150532e-05
Iter: 875 loss: 1.22677629e-05
Iter: 876 loss: 1.22236088e-05
Iter: 877 loss: 1.21922076e-05
Iter: 878 loss: 1.21765643e-05
Iter: 879 loss: 1.2113971e-05
Iter: 880 loss: 1.21063686e-05
Iter: 881 loss: 1.20657223e-05
Iter: 882 loss: 1.22553793e-05
Iter: 883 loss: 1.20578825e-05
Iter: 884 loss: 1.2007049e-05
Iter: 885 loss: 1.23996169e-05
Iter: 886 loss: 1.20031618e-05
Iter: 887 loss: 1.19908991e-05
Iter: 888 loss: 1.2003e-05
Iter: 889 loss: 1.19838915e-05
Iter: 890 loss: 1.19615943e-05
Iter: 891 loss: 1.19388333e-05
Iter: 892 loss: 1.19342358e-05
Iter: 893 loss: 1.19082797e-05
Iter: 894 loss: 1.19587676e-05
Iter: 895 loss: 1.18974167e-05
Iter: 896 loss: 1.18547969e-05
Iter: 897 loss: 1.20119066e-05
Iter: 898 loss: 1.18443668e-05
Iter: 899 loss: 1.18256703e-05
Iter: 900 loss: 1.18073567e-05
Iter: 901 loss: 1.1803404e-05
Iter: 902 loss: 1.17709742e-05
Iter: 903 loss: 1.20744298e-05
Iter: 904 loss: 1.17695254e-05
Iter: 905 loss: 1.17483496e-05
Iter: 906 loss: 1.20231e-05
Iter: 907 loss: 1.17482632e-05
Iter: 908 loss: 1.17370382e-05
Iter: 909 loss: 1.17031259e-05
Iter: 910 loss: 1.17878271e-05
Iter: 911 loss: 1.16842884e-05
Iter: 912 loss: 1.16440551e-05
Iter: 913 loss: 1.16940137e-05
Iter: 914 loss: 1.16232422e-05
Iter: 915 loss: 1.15753919e-05
Iter: 916 loss: 1.18574781e-05
Iter: 917 loss: 1.15692746e-05
Iter: 918 loss: 1.15611329e-05
Iter: 919 loss: 1.15462572e-05
Iter: 920 loss: 1.15331877e-05
Iter: 921 loss: 1.14999639e-05
Iter: 922 loss: 1.17696718e-05
Iter: 923 loss: 1.14938848e-05
Iter: 924 loss: 1.14733339e-05
Iter: 925 loss: 1.1470298e-05
Iter: 926 loss: 1.14558588e-05
Iter: 927 loss: 1.14218228e-05
Iter: 928 loss: 1.18038897e-05
Iter: 929 loss: 1.14185787e-05
Iter: 930 loss: 1.14247214e-05
Iter: 931 loss: 1.14040331e-05
Iter: 932 loss: 1.13945634e-05
Iter: 933 loss: 1.13656406e-05
Iter: 934 loss: 1.14197028e-05
Iter: 935 loss: 1.13469487e-05
Iter: 936 loss: 1.13186707e-05
Iter: 937 loss: 1.13186379e-05
Iter: 938 loss: 1.12912312e-05
Iter: 939 loss: 1.16451292e-05
Iter: 940 loss: 1.12911448e-05
Iter: 941 loss: 1.12724674e-05
Iter: 942 loss: 1.12655371e-05
Iter: 943 loss: 1.12552134e-05
Iter: 944 loss: 1.12345697e-05
Iter: 945 loss: 1.11994195e-05
Iter: 946 loss: 1.11993495e-05
Iter: 947 loss: 1.11764393e-05
Iter: 948 loss: 1.12834286e-05
Iter: 949 loss: 1.11719692e-05
Iter: 950 loss: 1.11573972e-05
Iter: 951 loss: 1.11574955e-05
Iter: 952 loss: 1.11370773e-05
Iter: 953 loss: 1.11226746e-05
Iter: 954 loss: 1.11155987e-05
Iter: 955 loss: 1.10983201e-05
Iter: 956 loss: 1.11700938e-05
Iter: 957 loss: 1.10945321e-05
Iter: 958 loss: 1.10726141e-05
Iter: 959 loss: 1.11069312e-05
Iter: 960 loss: 1.10624096e-05
Iter: 961 loss: 1.10490146e-05
Iter: 962 loss: 1.10665715e-05
Iter: 963 loss: 1.10421888e-05
Iter: 964 loss: 1.10150067e-05
Iter: 965 loss: 1.10126975e-05
Iter: 966 loss: 1.09925677e-05
Iter: 967 loss: 1.09726134e-05
Iter: 968 loss: 1.09462562e-05
Iter: 969 loss: 1.09445955e-05
Iter: 970 loss: 1.0909951e-05
Iter: 971 loss: 1.09395551e-05
Iter: 972 loss: 1.0889572e-05
Iter: 973 loss: 1.08881814e-05
Iter: 974 loss: 1.0872559e-05
Iter: 975 loss: 1.08596387e-05
Iter: 976 loss: 1.09143712e-05
Iter: 977 loss: 1.08568929e-05
Iter: 978 loss: 1.08397271e-05
Iter: 979 loss: 1.08325257e-05
Iter: 980 loss: 1.08236218e-05
Iter: 981 loss: 1.08136101e-05
Iter: 982 loss: 1.08204777e-05
Iter: 983 loss: 1.08071126e-05
Iter: 984 loss: 1.07884825e-05
Iter: 985 loss: 1.08857203e-05
Iter: 986 loss: 1.07854521e-05
Iter: 987 loss: 1.0776299e-05
Iter: 988 loss: 1.0766078e-05
Iter: 989 loss: 1.07646056e-05
Iter: 990 loss: 1.07464639e-05
Iter: 991 loss: 1.08871818e-05
Iter: 992 loss: 1.07451961e-05
Iter: 993 loss: 1.07354936e-05
Iter: 994 loss: 1.07177211e-05
Iter: 995 loss: 1.11365562e-05
Iter: 996 loss: 1.07177948e-05
Iter: 997 loss: 1.07031119e-05
Iter: 998 loss: 1.07020633e-05
Iter: 999 loss: 1.06931875e-05
Iter: 1000 loss: 1.0669386e-05
Iter: 1001 loss: 1.08115964e-05
Iter: 1002 loss: 1.06626776e-05
Iter: 1003 loss: 1.06384405e-05
Iter: 1004 loss: 1.06283642e-05
Iter: 1005 loss: 1.06156594e-05
Iter: 1006 loss: 1.05862564e-05
Iter: 1007 loss: 1.06004409e-05
Iter: 1008 loss: 1.05664858e-05
Iter: 1009 loss: 1.05364197e-05
Iter: 1010 loss: 1.05362305e-05
Iter: 1011 loss: 1.05200234e-05
Iter: 1012 loss: 1.06406824e-05
Iter: 1013 loss: 1.05187137e-05
Iter: 1014 loss: 1.05011695e-05
Iter: 1015 loss: 1.05122217e-05
Iter: 1016 loss: 1.04898181e-05
Iter: 1017 loss: 1.0477037e-05
Iter: 1018 loss: 1.04620531e-05
Iter: 1019 loss: 1.04603714e-05
Iter: 1020 loss: 1.04335986e-05
Iter: 1021 loss: 1.07670603e-05
Iter: 1022 loss: 1.04334358e-05
Iter: 1023 loss: 1.0423817e-05
Iter: 1024 loss: 1.03943794e-05
Iter: 1025 loss: 1.04628107e-05
Iter: 1026 loss: 1.03772136e-05
Iter: 1027 loss: 1.03926086e-05
Iter: 1028 loss: 1.03620478e-05
Iter: 1029 loss: 1.03512521e-05
Iter: 1030 loss: 1.03345501e-05
Iter: 1031 loss: 1.03343918e-05
Iter: 1032 loss: 1.0319829e-05
Iter: 1033 loss: 1.03195289e-05
Iter: 1034 loss: 1.03102402e-05
Iter: 1035 loss: 1.02917011e-05
Iter: 1036 loss: 1.06597872e-05
Iter: 1037 loss: 1.02916074e-05
Iter: 1038 loss: 1.02777321e-05
Iter: 1039 loss: 1.02811118e-05
Iter: 1040 loss: 1.0267735e-05
Iter: 1041 loss: 1.02565809e-05
Iter: 1042 loss: 1.02458034e-05
Iter: 1043 loss: 1.02434924e-05
Iter: 1044 loss: 1.02295908e-05
Iter: 1045 loss: 1.02570684e-05
Iter: 1046 loss: 1.02240174e-05
Iter: 1047 loss: 1.02167905e-05
Iter: 1048 loss: 1.02162467e-05
Iter: 1049 loss: 1.02086078e-05
Iter: 1050 loss: 1.01954884e-05
Iter: 1051 loss: 1.01956175e-05
Iter: 1052 loss: 1.01835649e-05
Iter: 1053 loss: 1.0272317e-05
Iter: 1054 loss: 1.01825381e-05
Iter: 1055 loss: 1.01652549e-05
Iter: 1056 loss: 1.01620844e-05
Iter: 1057 loss: 1.0150201e-05
Iter: 1058 loss: 1.01405349e-05
Iter: 1059 loss: 1.01222922e-05
Iter: 1060 loss: 1.05191848e-05
Iter: 1061 loss: 1.01221649e-05
Iter: 1062 loss: 1.01239721e-05
Iter: 1063 loss: 1.01128007e-05
Iter: 1064 loss: 1.01070837e-05
Iter: 1065 loss: 1.00972575e-05
Iter: 1066 loss: 1.00972738e-05
Iter: 1067 loss: 1.00803391e-05
Iter: 1068 loss: 1.01435662e-05
Iter: 1069 loss: 1.00762554e-05
Iter: 1070 loss: 1.00698553e-05
Iter: 1071 loss: 1.00742145e-05
Iter: 1072 loss: 1.00656971e-05
Iter: 1073 loss: 1.00555053e-05
Iter: 1074 loss: 1.01063079e-05
Iter: 1075 loss: 1.00538928e-05
Iter: 1076 loss: 1.00484685e-05
Iter: 1077 loss: 1.00351481e-05
Iter: 1078 loss: 1.01807882e-05
Iter: 1079 loss: 1.00338566e-05
Iter: 1080 loss: 1.00202496e-05
Iter: 1081 loss: 1.02113918e-05
Iter: 1082 loss: 1.00201369e-05
Iter: 1083 loss: 1.00066372e-05
Iter: 1084 loss: 1.00267389e-05
Iter: 1085 loss: 1.00000561e-05
Iter: 1086 loss: 9.98872565e-06
Iter: 1087 loss: 9.96683411e-06
Iter: 1088 loss: 1.04245701e-05
Iter: 1089 loss: 9.96661129e-06
Iter: 1090 loss: 9.98494397e-06
Iter: 1091 loss: 9.9576373e-06
Iter: 1092 loss: 9.95386e-06
Iter: 1093 loss: 9.94200491e-06
Iter: 1094 loss: 9.9544668e-06
Iter: 1095 loss: 9.93258072e-06
Iter: 1096 loss: 9.92393871e-06
Iter: 1097 loss: 9.9203844e-06
Iter: 1098 loss: 9.91359047e-06
Iter: 1099 loss: 9.89933596e-06
Iter: 1100 loss: 1.01176729e-05
Iter: 1101 loss: 9.89879e-06
Iter: 1102 loss: 9.88459396e-06
Iter: 1103 loss: 9.88446118e-06
Iter: 1104 loss: 9.87784e-06
Iter: 1105 loss: 9.8656983e-06
Iter: 1106 loss: 1.01612659e-05
Iter: 1107 loss: 9.86561281e-06
Iter: 1108 loss: 9.84847429e-06
Iter: 1109 loss: 9.90013268e-06
Iter: 1110 loss: 9.8432356e-06
Iter: 1111 loss: 9.83408e-06
Iter: 1112 loss: 9.82509118e-06
Iter: 1113 loss: 9.82308484e-06
Iter: 1114 loss: 9.80475488e-06
Iter: 1115 loss: 9.85204861e-06
Iter: 1116 loss: 9.79842844e-06
Iter: 1117 loss: 9.79627112e-06
Iter: 1118 loss: 9.79020115e-06
Iter: 1119 loss: 9.78492153e-06
Iter: 1120 loss: 9.76964111e-06
Iter: 1121 loss: 9.83530663e-06
Iter: 1122 loss: 9.76388219e-06
Iter: 1123 loss: 9.75463627e-06
Iter: 1124 loss: 9.75211879e-06
Iter: 1125 loss: 9.74485647e-06
Iter: 1126 loss: 9.72373346e-06
Iter: 1127 loss: 9.81556514e-06
Iter: 1128 loss: 9.71589543e-06
Iter: 1129 loss: 9.71391637e-06
Iter: 1130 loss: 9.70619294e-06
Iter: 1131 loss: 9.69564098e-06
Iter: 1132 loss: 9.67393862e-06
Iter: 1133 loss: 1.00522457e-05
Iter: 1134 loss: 9.67348387e-06
Iter: 1135 loss: 9.65653271e-06
Iter: 1136 loss: 9.65682466e-06
Iter: 1137 loss: 9.64317405e-06
Iter: 1138 loss: 9.64125e-06
Iter: 1139 loss: 9.63164257e-06
Iter: 1140 loss: 9.62531067e-06
Iter: 1141 loss: 9.62622289e-06
Iter: 1142 loss: 9.62063496e-06
Iter: 1143 loss: 9.61037949e-06
Iter: 1144 loss: 9.61819e-06
Iter: 1145 loss: 9.60423677e-06
Iter: 1146 loss: 9.59076e-06
Iter: 1147 loss: 9.71277495e-06
Iter: 1148 loss: 9.59033878e-06
Iter: 1149 loss: 9.58471537e-06
Iter: 1150 loss: 9.5694877e-06
Iter: 1151 loss: 9.64914216e-06
Iter: 1152 loss: 9.56455551e-06
Iter: 1153 loss: 9.55086944e-06
Iter: 1154 loss: 9.54922325e-06
Iter: 1155 loss: 9.5404921e-06
Iter: 1156 loss: 9.52082246e-06
Iter: 1157 loss: 9.77696345e-06
Iter: 1158 loss: 9.5194e-06
Iter: 1159 loss: 9.51853326e-06
Iter: 1160 loss: 9.51019319e-06
Iter: 1161 loss: 9.50446884e-06
Iter: 1162 loss: 9.4897232e-06
Iter: 1163 loss: 9.60352463e-06
Iter: 1164 loss: 9.48677189e-06
Iter: 1165 loss: 9.47808e-06
Iter: 1166 loss: 9.47728e-06
Iter: 1167 loss: 9.46551154e-06
Iter: 1168 loss: 9.4491661e-06
Iter: 1169 loss: 9.44849398e-06
Iter: 1170 loss: 9.43257601e-06
Iter: 1171 loss: 9.49269e-06
Iter: 1172 loss: 9.42877887e-06
Iter: 1173 loss: 9.41809776e-06
Iter: 1174 loss: 9.47687295e-06
Iter: 1175 loss: 9.41655344e-06
Iter: 1176 loss: 9.40853261e-06
Iter: 1177 loss: 9.39882193e-06
Iter: 1178 loss: 9.39783877e-06
Iter: 1179 loss: 9.38917128e-06
Iter: 1180 loss: 9.38505764e-06
Iter: 1181 loss: 9.38067751e-06
Iter: 1182 loss: 9.36590914e-06
Iter: 1183 loss: 9.52644405e-06
Iter: 1184 loss: 9.36548895e-06
Iter: 1185 loss: 9.35848e-06
Iter: 1186 loss: 9.37380173e-06
Iter: 1187 loss: 9.35565367e-06
Iter: 1188 loss: 9.34488799e-06
Iter: 1189 loss: 9.33667252e-06
Iter: 1190 loss: 9.3332219e-06
Iter: 1191 loss: 9.3231065e-06
Iter: 1192 loss: 9.40420614e-06
Iter: 1193 loss: 9.32242074e-06
Iter: 1194 loss: 9.30864735e-06
Iter: 1195 loss: 9.28829377e-06
Iter: 1196 loss: 9.28780628e-06
Iter: 1197 loss: 9.2716291e-06
Iter: 1198 loss: 9.29792805e-06
Iter: 1199 loss: 9.26410303e-06
Iter: 1200 loss: 9.24043161e-06
Iter: 1201 loss: 9.41289727e-06
Iter: 1202 loss: 9.23830066e-06
Iter: 1203 loss: 9.22590152e-06
Iter: 1204 loss: 9.29681846e-06
Iter: 1205 loss: 9.22429354e-06
Iter: 1206 loss: 9.21180435e-06
Iter: 1207 loss: 9.2128048e-06
Iter: 1208 loss: 9.20224738e-06
Iter: 1209 loss: 9.18725254e-06
Iter: 1210 loss: 9.29762064e-06
Iter: 1211 loss: 9.18623664e-06
Iter: 1212 loss: 9.17610851e-06
Iter: 1213 loss: 9.1587217e-06
Iter: 1214 loss: 9.15876535e-06
Iter: 1215 loss: 9.14623342e-06
Iter: 1216 loss: 9.14623e-06
Iter: 1217 loss: 9.13579788e-06
Iter: 1218 loss: 9.14491284e-06
Iter: 1219 loss: 9.12980795e-06
Iter: 1220 loss: 9.11511052e-06
Iter: 1221 loss: 9.14189422e-06
Iter: 1222 loss: 9.10873587e-06
Iter: 1223 loss: 9.09674691e-06
Iter: 1224 loss: 9.09046685e-06
Iter: 1225 loss: 9.08504e-06
Iter: 1226 loss: 9.06338664e-06
Iter: 1227 loss: 9.2819364e-06
Iter: 1228 loss: 9.06266632e-06
Iter: 1229 loss: 9.0527592e-06
Iter: 1230 loss: 9.02857e-06
Iter: 1231 loss: 9.27932e-06
Iter: 1232 loss: 9.02562169e-06
Iter: 1233 loss: 9.0323847e-06
Iter: 1234 loss: 9.01628664e-06
Iter: 1235 loss: 9.00838859e-06
Iter: 1236 loss: 8.99201768e-06
Iter: 1237 loss: 9.29162888e-06
Iter: 1238 loss: 8.99182942e-06
Iter: 1239 loss: 8.97849714e-06
Iter: 1240 loss: 8.97796053e-06
Iter: 1241 loss: 8.97137215e-06
Iter: 1242 loss: 8.97393329e-06
Iter: 1243 loss: 8.96667825e-06
Iter: 1244 loss: 8.95501671e-06
Iter: 1245 loss: 8.95289304e-06
Iter: 1246 loss: 8.94505138e-06
Iter: 1247 loss: 8.93407923e-06
Iter: 1248 loss: 8.93139804e-06
Iter: 1249 loss: 8.92441676e-06
Iter: 1250 loss: 8.91765103e-06
Iter: 1251 loss: 8.91500895e-06
Iter: 1252 loss: 8.90761658e-06
Iter: 1253 loss: 8.90522824e-06
Iter: 1254 loss: 8.90099909e-06
Iter: 1255 loss: 8.8885954e-06
Iter: 1256 loss: 8.87418355e-06
Iter: 1257 loss: 8.8724837e-06
Iter: 1258 loss: 8.85567169e-06
Iter: 1259 loss: 8.96743586e-06
Iter: 1260 loss: 8.85399731e-06
Iter: 1261 loss: 8.83139182e-06
Iter: 1262 loss: 8.87019451e-06
Iter: 1263 loss: 8.8213892e-06
Iter: 1264 loss: 8.80927655e-06
Iter: 1265 loss: 8.79154504e-06
Iter: 1266 loss: 8.79114486e-06
Iter: 1267 loss: 8.79011532e-06
Iter: 1268 loss: 8.78072206e-06
Iter: 1269 loss: 8.77465118e-06
Iter: 1270 loss: 8.76486774e-06
Iter: 1271 loss: 8.76481681e-06
Iter: 1272 loss: 8.75127171e-06
Iter: 1273 loss: 8.88558225e-06
Iter: 1274 loss: 8.75068e-06
Iter: 1275 loss: 8.74299622e-06
Iter: 1276 loss: 8.73925455e-06
Iter: 1277 loss: 8.73530917e-06
Iter: 1278 loss: 8.72018e-06
Iter: 1279 loss: 8.77837647e-06
Iter: 1280 loss: 8.71660905e-06
Iter: 1281 loss: 8.70825534e-06
Iter: 1282 loss: 8.7108865e-06
Iter: 1283 loss: 8.7022263e-06
Iter: 1284 loss: 8.6880691e-06
Iter: 1285 loss: 8.77402545e-06
Iter: 1286 loss: 8.68642201e-06
Iter: 1287 loss: 8.67626932e-06
Iter: 1288 loss: 8.66761911e-06
Iter: 1289 loss: 8.66472874e-06
Iter: 1290 loss: 8.65041e-06
Iter: 1291 loss: 8.6424925e-06
Iter: 1292 loss: 8.63625792e-06
Iter: 1293 loss: 8.6168784e-06
Iter: 1294 loss: 8.78573155e-06
Iter: 1295 loss: 8.61598346e-06
Iter: 1296 loss: 8.59315e-06
Iter: 1297 loss: 8.66083792e-06
Iter: 1298 loss: 8.58644489e-06
Iter: 1299 loss: 8.57685518e-06
Iter: 1300 loss: 8.56027327e-06
Iter: 1301 loss: 8.56013776e-06
Iter: 1302 loss: 8.56481711e-06
Iter: 1303 loss: 8.55297549e-06
Iter: 1304 loss: 8.54816426e-06
Iter: 1305 loss: 8.53642359e-06
Iter: 1306 loss: 8.65362199e-06
Iter: 1307 loss: 8.53500205e-06
Iter: 1308 loss: 8.52343692e-06
Iter: 1309 loss: 8.52320591e-06
Iter: 1310 loss: 8.51775349e-06
Iter: 1311 loss: 8.52088306e-06
Iter: 1312 loss: 8.51458e-06
Iter: 1313 loss: 8.50576544e-06
Iter: 1314 loss: 8.50527795e-06
Iter: 1315 loss: 8.4985877e-06
Iter: 1316 loss: 8.48964191e-06
Iter: 1317 loss: 8.57118084e-06
Iter: 1318 loss: 8.48920718e-06
Iter: 1319 loss: 8.47935826e-06
Iter: 1320 loss: 8.46418e-06
Iter: 1321 loss: 8.46383e-06
Iter: 1322 loss: 8.44959868e-06
Iter: 1323 loss: 8.45322575e-06
Iter: 1324 loss: 8.43909766e-06
Iter: 1325 loss: 8.41908695e-06
Iter: 1326 loss: 8.42086683e-06
Iter: 1327 loss: 8.40367738e-06
Iter: 1328 loss: 8.40016e-06
Iter: 1329 loss: 8.39368e-06
Iter: 1330 loss: 8.38571577e-06
Iter: 1331 loss: 8.36784147e-06
Iter: 1332 loss: 8.62235447e-06
Iter: 1333 loss: 8.36699746e-06
Iter: 1334 loss: 8.35060928e-06
Iter: 1335 loss: 8.3691375e-06
Iter: 1336 loss: 8.34196089e-06
Iter: 1337 loss: 8.33463946e-06
Iter: 1338 loss: 8.3312716e-06
Iter: 1339 loss: 8.32016121e-06
Iter: 1340 loss: 8.29401597e-06
Iter: 1341 loss: 8.59986721e-06
Iter: 1342 loss: 8.29184228e-06
Iter: 1343 loss: 8.28902375e-06
Iter: 1344 loss: 8.28321572e-06
Iter: 1345 loss: 8.27313306e-06
Iter: 1346 loss: 8.24792551e-06
Iter: 1347 loss: 8.48421405e-06
Iter: 1348 loss: 8.24451399e-06
Iter: 1349 loss: 8.24888e-06
Iter: 1350 loss: 8.23790106e-06
Iter: 1351 loss: 8.23065875e-06
Iter: 1352 loss: 8.21803951e-06
Iter: 1353 loss: 8.2180286e-06
Iter: 1354 loss: 8.20619243e-06
Iter: 1355 loss: 8.21740105e-06
Iter: 1356 loss: 8.19935212e-06
Iter: 1357 loss: 8.18505396e-06
Iter: 1358 loss: 8.33505e-06
Iter: 1359 loss: 8.18476292e-06
Iter: 1360 loss: 8.17519867e-06
Iter: 1361 loss: 8.2296865e-06
Iter: 1362 loss: 8.17391629e-06
Iter: 1363 loss: 8.16397642e-06
Iter: 1364 loss: 8.15748535e-06
Iter: 1365 loss: 8.15381736e-06
Iter: 1366 loss: 8.14184659e-06
Iter: 1367 loss: 8.13755742e-06
Iter: 1368 loss: 8.13082534e-06
Iter: 1369 loss: 8.11494647e-06
Iter: 1370 loss: 8.11006066e-06
Iter: 1371 loss: 8.10051642e-06
Iter: 1372 loss: 8.07855395e-06
Iter: 1373 loss: 8.12596954e-06
Iter: 1374 loss: 8.06988646e-06
Iter: 1375 loss: 8.08545155e-06
Iter: 1376 loss: 8.06139542e-06
Iter: 1377 loss: 8.05545369e-06
Iter: 1378 loss: 8.03710282e-06
Iter: 1379 loss: 8.07767537e-06
Iter: 1380 loss: 8.02614704e-06
Iter: 1381 loss: 8.00380622e-06
Iter: 1382 loss: 8.1138387e-06
Iter: 1383 loss: 8.00006728e-06
Iter: 1384 loss: 7.98973451e-06
Iter: 1385 loss: 7.98652582e-06
Iter: 1386 loss: 7.98064775e-06
Iter: 1387 loss: 7.96485256e-06
Iter: 1388 loss: 8.06040589e-06
Iter: 1389 loss: 7.96046e-06
Iter: 1390 loss: 7.95864889e-06
Iter: 1391 loss: 7.95003689e-06
Iter: 1392 loss: 7.9460242e-06
Iter: 1393 loss: 7.93791514e-06
Iter: 1394 loss: 8.08870209e-06
Iter: 1395 loss: 7.93772e-06
Iter: 1396 loss: 7.92555147e-06
Iter: 1397 loss: 7.95915366e-06
Iter: 1398 loss: 7.92151695e-06
Iter: 1399 loss: 7.90977174e-06
Iter: 1400 loss: 8.06925891e-06
Iter: 1401 loss: 7.90971171e-06
Iter: 1402 loss: 7.90569084e-06
Iter: 1403 loss: 7.89236856e-06
Iter: 1404 loss: 7.90506874e-06
Iter: 1405 loss: 7.88172474e-06
Iter: 1406 loss: 7.86108831e-06
Iter: 1407 loss: 7.96753739e-06
Iter: 1408 loss: 7.85768316e-06
Iter: 1409 loss: 7.84058284e-06
Iter: 1410 loss: 7.92918217e-06
Iter: 1411 loss: 7.83783616e-06
Iter: 1412 loss: 7.82381721e-06
Iter: 1413 loss: 7.82363e-06
Iter: 1414 loss: 7.81706876e-06
Iter: 1415 loss: 7.80027949e-06
Iter: 1416 loss: 7.94723201e-06
Iter: 1417 loss: 7.79763195e-06
Iter: 1418 loss: 7.79353559e-06
Iter: 1419 loss: 7.78955382e-06
Iter: 1420 loss: 7.78064441e-06
Iter: 1421 loss: 7.76716115e-06
Iter: 1422 loss: 7.76695106e-06
Iter: 1423 loss: 7.762108e-06
Iter: 1424 loss: 7.76001252e-06
Iter: 1425 loss: 7.75216e-06
Iter: 1426 loss: 7.7316854e-06
Iter: 1427 loss: 7.89355545e-06
Iter: 1428 loss: 7.7278255e-06
Iter: 1429 loss: 7.71361192e-06
Iter: 1430 loss: 7.80084792e-06
Iter: 1431 loss: 7.71183113e-06
Iter: 1432 loss: 7.70636689e-06
Iter: 1433 loss: 7.7014e-06
Iter: 1434 loss: 7.69479902e-06
Iter: 1435 loss: 7.6878232e-06
Iter: 1436 loss: 7.68650898e-06
Iter: 1437 loss: 7.67630081e-06
Iter: 1438 loss: 7.66116409e-06
Iter: 1439 loss: 7.66079211e-06
Iter: 1440 loss: 7.64437937e-06
Iter: 1441 loss: 7.67436359e-06
Iter: 1442 loss: 7.63733624e-06
Iter: 1443 loss: 7.624516e-06
Iter: 1444 loss: 7.80030859e-06
Iter: 1445 loss: 7.62455238e-06
Iter: 1446 loss: 7.6146971e-06
Iter: 1447 loss: 7.73647e-06
Iter: 1448 loss: 7.61462343e-06
Iter: 1449 loss: 7.60945068e-06
Iter: 1450 loss: 7.59878185e-06
Iter: 1451 loss: 7.78534195e-06
Iter: 1452 loss: 7.59846353e-06
Iter: 1453 loss: 7.58852684e-06
Iter: 1454 loss: 7.58826536e-06
Iter: 1455 loss: 7.58403075e-06
Iter: 1456 loss: 7.57470934e-06
Iter: 1457 loss: 7.71554642e-06
Iter: 1458 loss: 7.57434645e-06
Iter: 1459 loss: 7.57323915e-06
Iter: 1460 loss: 7.56867712e-06
Iter: 1461 loss: 7.56635836e-06
Iter: 1462 loss: 7.55818064e-06
Iter: 1463 loss: 7.5555372e-06
Iter: 1464 loss: 7.54879875e-06
Iter: 1465 loss: 7.53213953e-06
Iter: 1466 loss: 7.55197516e-06
Iter: 1467 loss: 7.52339247e-06
Iter: 1468 loss: 7.5423477e-06
Iter: 1469 loss: 7.5189364e-06
Iter: 1470 loss: 7.51387415e-06
Iter: 1471 loss: 7.50249728e-06
Iter: 1472 loss: 7.6645e-06
Iter: 1473 loss: 7.50193976e-06
Iter: 1474 loss: 7.49248557e-06
Iter: 1475 loss: 7.54749908e-06
Iter: 1476 loss: 7.49113178e-06
Iter: 1477 loss: 7.48105867e-06
Iter: 1478 loss: 7.46459773e-06
Iter: 1479 loss: 7.46464275e-06
Iter: 1480 loss: 7.4625591e-06
Iter: 1481 loss: 7.45862462e-06
Iter: 1482 loss: 7.45187253e-06
Iter: 1483 loss: 7.44533645e-06
Iter: 1484 loss: 7.44405588e-06
Iter: 1485 loss: 7.437779e-06
Iter: 1486 loss: 7.49854871e-06
Iter: 1487 loss: 7.43752344e-06
Iter: 1488 loss: 7.43036708e-06
Iter: 1489 loss: 7.42974953e-06
Iter: 1490 loss: 7.42447537e-06
Iter: 1491 loss: 7.41942722e-06
Iter: 1492 loss: 7.41948588e-06
Iter: 1493 loss: 7.4141567e-06
Iter: 1494 loss: 7.40074574e-06
Iter: 1495 loss: 7.52821461e-06
Iter: 1496 loss: 7.3989354e-06
Iter: 1497 loss: 7.38967719e-06
Iter: 1498 loss: 7.39423e-06
Iter: 1499 loss: 7.38341714e-06
Iter: 1500 loss: 7.37625396e-06
Iter: 1501 loss: 7.38691e-06
Iter: 1502 loss: 7.37266146e-06
Iter: 1503 loss: 7.36731636e-06
Iter: 1504 loss: 7.36062339e-06
Iter: 1505 loss: 7.36002858e-06
Iter: 1506 loss: 7.34993591e-06
Iter: 1507 loss: 7.38292556e-06
Iter: 1508 loss: 7.3472238e-06
Iter: 1509 loss: 7.33631259e-06
Iter: 1510 loss: 7.39572033e-06
Iter: 1511 loss: 7.3345559e-06
Iter: 1512 loss: 7.32599437e-06
Iter: 1513 loss: 7.34300284e-06
Iter: 1514 loss: 7.32252283e-06
Iter: 1515 loss: 7.31417913e-06
Iter: 1516 loss: 7.43831743e-06
Iter: 1517 loss: 7.31412365e-06
Iter: 1518 loss: 7.30963438e-06
Iter: 1519 loss: 7.30043121e-06
Iter: 1520 loss: 7.45793386e-06
Iter: 1521 loss: 7.30015e-06
Iter: 1522 loss: 7.28784698e-06
Iter: 1523 loss: 7.41077292e-06
Iter: 1524 loss: 7.28742225e-06
Iter: 1525 loss: 7.28166924e-06
Iter: 1526 loss: 7.28370424e-06
Iter: 1527 loss: 7.27759743e-06
Iter: 1528 loss: 7.26576127e-06
Iter: 1529 loss: 7.28331224e-06
Iter: 1530 loss: 7.26002463e-06
Iter: 1531 loss: 7.25297377e-06
Iter: 1532 loss: 7.23391e-06
Iter: 1533 loss: 7.35986123e-06
Iter: 1534 loss: 7.22928644e-06
Iter: 1535 loss: 7.25656082e-06
Iter: 1536 loss: 7.22372897e-06
Iter: 1537 loss: 7.21835659e-06
Iter: 1538 loss: 7.20618891e-06
Iter: 1539 loss: 7.36084348e-06
Iter: 1540 loss: 7.20536e-06
Iter: 1541 loss: 7.19118907e-06
Iter: 1542 loss: 7.19262971e-06
Iter: 1543 loss: 7.18029241e-06
Iter: 1544 loss: 7.16492559e-06
Iter: 1545 loss: 7.39328061e-06
Iter: 1546 loss: 7.16490104e-06
Iter: 1547 loss: 7.14989392e-06
Iter: 1548 loss: 7.16404884e-06
Iter: 1549 loss: 7.14129419e-06
Iter: 1550 loss: 7.13442432e-06
Iter: 1551 loss: 7.13305599e-06
Iter: 1552 loss: 7.12560177e-06
Iter: 1553 loss: 7.10837048e-06
Iter: 1554 loss: 7.32169383e-06
Iter: 1555 loss: 7.10720951e-06
Iter: 1556 loss: 7.10504264e-06
Iter: 1557 loss: 7.09936194e-06
Iter: 1558 loss: 7.0939368e-06
Iter: 1559 loss: 7.08057178e-06
Iter: 1560 loss: 7.20655362e-06
Iter: 1561 loss: 7.07890194e-06
Iter: 1562 loss: 7.07886329e-06
Iter: 1563 loss: 7.07153413e-06
Iter: 1564 loss: 7.0684705e-06
Iter: 1565 loss: 7.06085166e-06
Iter: 1566 loss: 7.14842918e-06
Iter: 1567 loss: 7.06020865e-06
Iter: 1568 loss: 7.06137371e-06
Iter: 1569 loss: 7.05334787e-06
Iter: 1570 loss: 7.04366266e-06
Iter: 1571 loss: 7.05349476e-06
Iter: 1572 loss: 7.03809337e-06
Iter: 1573 loss: 7.02322268e-06
Iter: 1574 loss: 7.07688241e-06
Iter: 1575 loss: 7.01944373e-06
Iter: 1576 loss: 7.00785768e-06
Iter: 1577 loss: 7.09423512e-06
Iter: 1578 loss: 7.00702e-06
Iter: 1579 loss: 6.99704788e-06
Iter: 1580 loss: 7.01024601e-06
Iter: 1581 loss: 6.99200973e-06
Iter: 1582 loss: 6.98078566e-06
Iter: 1583 loss: 7.05661751e-06
Iter: 1584 loss: 6.97968e-06
Iter: 1585 loss: 6.97389214e-06
Iter: 1586 loss: 7.05037e-06
Iter: 1587 loss: 6.9737971e-06
Iter: 1588 loss: 6.96840198e-06
Iter: 1589 loss: 6.95529616e-06
Iter: 1590 loss: 7.10852146e-06
Iter: 1591 loss: 6.95410063e-06
Iter: 1592 loss: 6.95276094e-06
Iter: 1593 loss: 6.94787195e-06
Iter: 1594 loss: 6.94364917e-06
Iter: 1595 loss: 6.93339189e-06
Iter: 1596 loss: 7.04748345e-06
Iter: 1597 loss: 6.93239554e-06
Iter: 1598 loss: 6.91709465e-06
Iter: 1599 loss: 7.06047194e-06
Iter: 1600 loss: 6.91660534e-06
Iter: 1601 loss: 6.91148944e-06
Iter: 1602 loss: 6.89985382e-06
Iter: 1603 loss: 7.04029617e-06
Iter: 1604 loss: 6.89897479e-06
Iter: 1605 loss: 6.88871933e-06
Iter: 1606 loss: 6.88798718e-06
Iter: 1607 loss: 6.88443743e-06
Iter: 1608 loss: 6.87373813e-06
Iter: 1609 loss: 6.92421872e-06
Iter: 1610 loss: 6.87000238e-06
Iter: 1611 loss: 6.85310897e-06
Iter: 1612 loss: 6.86526528e-06
Iter: 1613 loss: 6.84278348e-06
Iter: 1614 loss: 6.83780036e-06
Iter: 1615 loss: 6.83040889e-06
Iter: 1616 loss: 6.8227223e-06
Iter: 1617 loss: 6.81620349e-06
Iter: 1618 loss: 6.81402616e-06
Iter: 1619 loss: 6.80245557e-06
Iter: 1620 loss: 6.94742221e-06
Iter: 1621 loss: 6.8023719e-06
Iter: 1622 loss: 6.79373943e-06
Iter: 1623 loss: 6.83368489e-06
Iter: 1624 loss: 6.79215646e-06
Iter: 1625 loss: 6.78600463e-06
Iter: 1626 loss: 6.77719481e-06
Iter: 1627 loss: 6.77695061e-06
Iter: 1628 loss: 6.76472609e-06
Iter: 1629 loss: 6.93710263e-06
Iter: 1630 loss: 6.76474156e-06
Iter: 1631 loss: 6.76027457e-06
Iter: 1632 loss: 6.77648677e-06
Iter: 1633 loss: 6.75925321e-06
Iter: 1634 loss: 6.75273e-06
Iter: 1635 loss: 6.74377316e-06
Iter: 1636 loss: 6.74351168e-06
Iter: 1637 loss: 6.7366027e-06
Iter: 1638 loss: 6.74079138e-06
Iter: 1639 loss: 6.73218346e-06
Iter: 1640 loss: 6.71915586e-06
Iter: 1641 loss: 6.79006871e-06
Iter: 1642 loss: 6.71729049e-06
Iter: 1643 loss: 6.71242e-06
Iter: 1644 loss: 6.69837846e-06
Iter: 1645 loss: 6.75480578e-06
Iter: 1646 loss: 6.69258134e-06
Iter: 1647 loss: 6.67747281e-06
Iter: 1648 loss: 6.81249685e-06
Iter: 1649 loss: 6.67661789e-06
Iter: 1650 loss: 6.66419828e-06
Iter: 1651 loss: 6.82384098e-06
Iter: 1652 loss: 6.6641e-06
Iter: 1653 loss: 6.65646667e-06
Iter: 1654 loss: 6.67260974e-06
Iter: 1655 loss: 6.65345897e-06
Iter: 1656 loss: 6.64493518e-06
Iter: 1657 loss: 6.70378768e-06
Iter: 1658 loss: 6.64420804e-06
Iter: 1659 loss: 6.63798846e-06
Iter: 1660 loss: 6.63709852e-06
Iter: 1661 loss: 6.6326611e-06
Iter: 1662 loss: 6.6269813e-06
Iter: 1663 loss: 6.71167163e-06
Iter: 1664 loss: 6.62692673e-06
Iter: 1665 loss: 6.62104139e-06
Iter: 1666 loss: 6.60967225e-06
Iter: 1667 loss: 6.85096666e-06
Iter: 1668 loss: 6.60969863e-06
Iter: 1669 loss: 6.60960177e-06
Iter: 1670 loss: 6.60469595e-06
Iter: 1671 loss: 6.60220576e-06
Iter: 1672 loss: 6.59436955e-06
Iter: 1673 loss: 6.60633759e-06
Iter: 1674 loss: 6.58866793e-06
Iter: 1675 loss: 6.59679336e-06
Iter: 1676 loss: 6.58492e-06
Iter: 1677 loss: 6.5821946e-06
Iter: 1678 loss: 6.57359305e-06
Iter: 1679 loss: 6.58153385e-06
Iter: 1680 loss: 6.56663542e-06
Iter: 1681 loss: 6.5498225e-06
Iter: 1682 loss: 6.57722194e-06
Iter: 1683 loss: 6.54208725e-06
Iter: 1684 loss: 6.53409779e-06
Iter: 1685 loss: 6.53271763e-06
Iter: 1686 loss: 6.52404378e-06
Iter: 1687 loss: 6.53279767e-06
Iter: 1688 loss: 6.51927e-06
Iter: 1689 loss: 6.50952325e-06
Iter: 1690 loss: 6.56259363e-06
Iter: 1691 loss: 6.50801803e-06
Iter: 1692 loss: 6.49922686e-06
Iter: 1693 loss: 6.50415495e-06
Iter: 1694 loss: 6.49342883e-06
Iter: 1695 loss: 6.48353944e-06
Iter: 1696 loss: 6.51133769e-06
Iter: 1697 loss: 6.48047126e-06
Iter: 1698 loss: 6.46834951e-06
Iter: 1699 loss: 6.53120333e-06
Iter: 1700 loss: 6.46645185e-06
Iter: 1701 loss: 6.45965065e-06
Iter: 1702 loss: 6.47153047e-06
Iter: 1703 loss: 6.45664477e-06
Iter: 1704 loss: 6.44412739e-06
Iter: 1705 loss: 6.43110752e-06
Iter: 1706 loss: 6.42865e-06
Iter: 1707 loss: 6.42270606e-06
Iter: 1708 loss: 6.5157551e-06
Iter: 1709 loss: 6.42266059e-06
Iter: 1710 loss: 6.41513316e-06
Iter: 1711 loss: 6.40068538e-06
Iter: 1712 loss: 6.70642794e-06
Iter: 1713 loss: 6.40055896e-06
Iter: 1714 loss: 6.39162909e-06
Iter: 1715 loss: 6.38777146e-06
Iter: 1716 loss: 6.38314668e-06
Iter: 1717 loss: 6.3697089e-06
Iter: 1718 loss: 6.39956852e-06
Iter: 1719 loss: 6.36460663e-06
Iter: 1720 loss: 6.35369815e-06
Iter: 1721 loss: 6.44121201e-06
Iter: 1722 loss: 6.35289507e-06
Iter: 1723 loss: 6.34357e-06
Iter: 1724 loss: 6.44056308e-06
Iter: 1725 loss: 6.343314e-06
Iter: 1726 loss: 6.33675336e-06
Iter: 1727 loss: 6.35668675e-06
Iter: 1728 loss: 6.33489526e-06
Iter: 1729 loss: 6.32888123e-06
Iter: 1730 loss: 6.3292473e-06
Iter: 1731 loss: 6.32414321e-06
Iter: 1732 loss: 6.31709145e-06
Iter: 1733 loss: 6.39563586e-06
Iter: 1734 loss: 6.31682269e-06
Iter: 1735 loss: 6.31061948e-06
Iter: 1736 loss: 6.30653e-06
Iter: 1737 loss: 6.30406339e-06
Iter: 1738 loss: 6.2993181e-06
Iter: 1739 loss: 6.29907845e-06
Iter: 1740 loss: 6.29450096e-06
Iter: 1741 loss: 6.28470389e-06
Iter: 1742 loss: 6.42485338e-06
Iter: 1743 loss: 6.28410316e-06
Iter: 1744 loss: 6.27507416e-06
Iter: 1745 loss: 6.27493e-06
Iter: 1746 loss: 6.27087775e-06
Iter: 1747 loss: 6.25870325e-06
Iter: 1748 loss: 6.29294209e-06
Iter: 1749 loss: 6.25254097e-06
Iter: 1750 loss: 6.23848518e-06
Iter: 1751 loss: 6.27591498e-06
Iter: 1752 loss: 6.23385768e-06
Iter: 1753 loss: 6.21984191e-06
Iter: 1754 loss: 6.25992652e-06
Iter: 1755 loss: 6.21563458e-06
Iter: 1756 loss: 6.205928e-06
Iter: 1757 loss: 6.20588935e-06
Iter: 1758 loss: 6.1966939e-06
Iter: 1759 loss: 6.21484469e-06
Iter: 1760 loss: 6.1928431e-06
Iter: 1761 loss: 6.18317063e-06
Iter: 1762 loss: 6.20969467e-06
Iter: 1763 loss: 6.18003514e-06
Iter: 1764 loss: 6.17200203e-06
Iter: 1765 loss: 6.1994765e-06
Iter: 1766 loss: 6.16999023e-06
Iter: 1767 loss: 6.16175521e-06
Iter: 1768 loss: 6.20800893e-06
Iter: 1769 loss: 6.16045509e-06
Iter: 1770 loss: 6.15523e-06
Iter: 1771 loss: 6.15524687e-06
Iter: 1772 loss: 6.1509254e-06
Iter: 1773 loss: 6.14305827e-06
Iter: 1774 loss: 6.21123127e-06
Iter: 1775 loss: 6.14260534e-06
Iter: 1776 loss: 6.13850534e-06
Iter: 1777 loss: 6.1475962e-06
Iter: 1778 loss: 6.13700286e-06
Iter: 1779 loss: 6.13043358e-06
Iter: 1780 loss: 6.12028771e-06
Iter: 1781 loss: 6.12016447e-06
Iter: 1782 loss: 6.11200858e-06
Iter: 1783 loss: 6.09792369e-06
Iter: 1784 loss: 6.09795552e-06
Iter: 1785 loss: 6.08224127e-06
Iter: 1786 loss: 6.15588579e-06
Iter: 1787 loss: 6.07922038e-06
Iter: 1788 loss: 6.06369304e-06
Iter: 1789 loss: 6.11488213e-06
Iter: 1790 loss: 6.05950117e-06
Iter: 1791 loss: 6.0471184e-06
Iter: 1792 loss: 6.10279039e-06
Iter: 1793 loss: 6.04460456e-06
Iter: 1794 loss: 6.03463e-06
Iter: 1795 loss: 6.03448916e-06
Iter: 1796 loss: 6.02863474e-06
Iter: 1797 loss: 6.03310673e-06
Iter: 1798 loss: 6.02507771e-06
Iter: 1799 loss: 6.0178063e-06
Iter: 1800 loss: 6.03556055e-06
Iter: 1801 loss: 6.01528518e-06
Iter: 1802 loss: 6.00706699e-06
Iter: 1803 loss: 6.05395235e-06
Iter: 1804 loss: 6.00596559e-06
Iter: 1805 loss: 6.00101293e-06
Iter: 1806 loss: 6.00798785e-06
Iter: 1807 loss: 5.99867144e-06
Iter: 1808 loss: 5.99111945e-06
Iter: 1809 loss: 6.00262774e-06
Iter: 1810 loss: 5.98771385e-06
Iter: 1811 loss: 5.98310453e-06
Iter: 1812 loss: 6.03445e-06
Iter: 1813 loss: 5.98302722e-06
Iter: 1814 loss: 5.97794542e-06
Iter: 1815 loss: 5.96707378e-06
Iter: 1816 loss: 6.12979511e-06
Iter: 1817 loss: 5.96661266e-06
Iter: 1818 loss: 5.95780512e-06
Iter: 1819 loss: 5.94658695e-06
Iter: 1820 loss: 5.94573976e-06
Iter: 1821 loss: 5.93162e-06
Iter: 1822 loss: 5.97763801e-06
Iter: 1823 loss: 5.92775086e-06
Iter: 1824 loss: 5.91305297e-06
Iter: 1825 loss: 5.97538519e-06
Iter: 1826 loss: 5.91000617e-06
Iter: 1827 loss: 5.89517e-06
Iter: 1828 loss: 5.924383e-06
Iter: 1829 loss: 5.8891319e-06
Iter: 1830 loss: 5.88679723e-06
Iter: 1831 loss: 5.88350576e-06
Iter: 1832 loss: 5.87735258e-06
Iter: 1833 loss: 5.87127943e-06
Iter: 1834 loss: 5.86998249e-06
Iter: 1835 loss: 5.86429269e-06
Iter: 1836 loss: 5.86431634e-06
Iter: 1837 loss: 5.86014903e-06
Iter: 1838 loss: 5.86110309e-06
Iter: 1839 loss: 5.85711587e-06
Iter: 1840 loss: 5.85109819e-06
Iter: 1841 loss: 5.86239912e-06
Iter: 1842 loss: 5.84864847e-06
Iter: 1843 loss: 5.84221107e-06
Iter: 1844 loss: 5.88329704e-06
Iter: 1845 loss: 5.84151348e-06
Iter: 1846 loss: 5.83667315e-06
Iter: 1847 loss: 5.84750705e-06
Iter: 1848 loss: 5.83490419e-06
Iter: 1849 loss: 5.82877419e-06
Iter: 1850 loss: 5.82466919e-06
Iter: 1851 loss: 5.82235953e-06
Iter: 1852 loss: 5.81654331e-06
Iter: 1853 loss: 5.80932647e-06
Iter: 1854 loss: 5.80861615e-06
Iter: 1855 loss: 5.79851621e-06
Iter: 1856 loss: 5.80442e-06
Iter: 1857 loss: 5.79201242e-06
Iter: 1858 loss: 5.77766559e-06
Iter: 1859 loss: 5.81062432e-06
Iter: 1860 loss: 5.77230094e-06
Iter: 1861 loss: 5.76050343e-06
Iter: 1862 loss: 5.8733408e-06
Iter: 1863 loss: 5.75996091e-06
Iter: 1864 loss: 5.75382273e-06
Iter: 1865 loss: 5.75357126e-06
Iter: 1866 loss: 5.74750175e-06
Iter: 1867 loss: 5.75342938e-06
Iter: 1868 loss: 5.74420574e-06
Iter: 1869 loss: 5.73829902e-06
Iter: 1870 loss: 5.78819254e-06
Iter: 1871 loss: 5.73799525e-06
Iter: 1872 loss: 5.73355555e-06
Iter: 1873 loss: 5.73236821e-06
Iter: 1874 loss: 5.72962199e-06
Iter: 1875 loss: 5.72277577e-06
Iter: 1876 loss: 5.73970874e-06
Iter: 1877 loss: 5.72039426e-06
Iter: 1878 loss: 5.71505097e-06
Iter: 1879 loss: 5.77595256e-06
Iter: 1880 loss: 5.71497367e-06
Iter: 1881 loss: 5.71113378e-06
Iter: 1882 loss: 5.71399687e-06
Iter: 1883 loss: 5.70865e-06
Iter: 1884 loss: 5.70302609e-06
Iter: 1885 loss: 5.69911435e-06
Iter: 1886 loss: 5.69696704e-06
Iter: 1887 loss: 5.69006443e-06
Iter: 1888 loss: 5.68041651e-06
Iter: 1889 loss: 5.67989309e-06
Iter: 1890 loss: 5.66821473e-06
Iter: 1891 loss: 5.70057364e-06
Iter: 1892 loss: 5.66422932e-06
Iter: 1893 loss: 5.6539493e-06
Iter: 1894 loss: 5.68305632e-06
Iter: 1895 loss: 5.65060873e-06
Iter: 1896 loss: 5.64090305e-06
Iter: 1897 loss: 5.6939225e-06
Iter: 1898 loss: 5.63940648e-06
Iter: 1899 loss: 5.63267804e-06
Iter: 1900 loss: 5.63234107e-06
Iter: 1901 loss: 5.62764171e-06
Iter: 1902 loss: 5.63753883e-06
Iter: 1903 loss: 5.62588684e-06
Iter: 1904 loss: 5.62007745e-06
Iter: 1905 loss: 5.63317235e-06
Iter: 1906 loss: 5.61810612e-06
Iter: 1907 loss: 5.61286924e-06
Iter: 1908 loss: 5.6182048e-06
Iter: 1909 loss: 5.60993431e-06
Iter: 1910 loss: 5.6040526e-06
Iter: 1911 loss: 5.62422474e-06
Iter: 1912 loss: 5.60243507e-06
Iter: 1913 loss: 5.59644923e-06
Iter: 1914 loss: 5.63472713e-06
Iter: 1915 loss: 5.5957812e-06
Iter: 1916 loss: 5.59136743e-06
Iter: 1917 loss: 5.59293676e-06
Iter: 1918 loss: 5.58820966e-06
Iter: 1919 loss: 5.58177771e-06
Iter: 1920 loss: 5.57404474e-06
Iter: 1921 loss: 5.57336807e-06
Iter: 1922 loss: 5.5645346e-06
Iter: 1923 loss: 5.56851046e-06
Iter: 1924 loss: 5.55863426e-06
Iter: 1925 loss: 5.5487285e-06
Iter: 1926 loss: 5.5618134e-06
Iter: 1927 loss: 5.54372582e-06
Iter: 1928 loss: 5.53307655e-06
Iter: 1929 loss: 5.56482337e-06
Iter: 1930 loss: 5.53007339e-06
Iter: 1931 loss: 5.52303163e-06
Iter: 1932 loss: 5.52300708e-06
Iter: 1933 loss: 5.51622225e-06
Iter: 1934 loss: 5.5645437e-06
Iter: 1935 loss: 5.51571839e-06
Iter: 1936 loss: 5.51197718e-06
Iter: 1937 loss: 5.53836253e-06
Iter: 1938 loss: 5.51161156e-06
Iter: 1939 loss: 5.5079513e-06
Iter: 1940 loss: 5.50522418e-06
Iter: 1941 loss: 5.50422192e-06
Iter: 1942 loss: 5.49808556e-06
Iter: 1943 loss: 5.50893355e-06
Iter: 1944 loss: 5.49545484e-06
Iter: 1945 loss: 5.49087144e-06
Iter: 1946 loss: 5.53720747e-06
Iter: 1947 loss: 5.49067499e-06
Iter: 1948 loss: 5.48650496e-06
Iter: 1949 loss: 5.49623564e-06
Iter: 1950 loss: 5.48505932e-06
Iter: 1951 loss: 5.48102798e-06
Iter: 1952 loss: 5.4836346e-06
Iter: 1953 loss: 5.47857326e-06
Iter: 1954 loss: 5.4733e-06
Iter: 1955 loss: 5.46848332e-06
Iter: 1956 loss: 5.46728415e-06
Iter: 1957 loss: 5.46003048e-06
Iter: 1958 loss: 5.4603147e-06
Iter: 1959 loss: 5.45441071e-06
Iter: 1960 loss: 5.44481236e-06
Iter: 1961 loss: 5.46159754e-06
Iter: 1962 loss: 5.4406737e-06
Iter: 1963 loss: 5.43157603e-06
Iter: 1964 loss: 5.46370848e-06
Iter: 1965 loss: 5.42929138e-06
Iter: 1966 loss: 5.42409316e-06
Iter: 1967 loss: 5.42395173e-06
Iter: 1968 loss: 5.41932604e-06
Iter: 1969 loss: 5.46403135e-06
Iter: 1970 loss: 5.41912959e-06
Iter: 1971 loss: 5.41684676e-06
Iter: 1972 loss: 5.42905855e-06
Iter: 1973 loss: 5.41646614e-06
Iter: 1974 loss: 5.41371901e-06
Iter: 1975 loss: 5.41068403e-06
Iter: 1976 loss: 5.41034115e-06
Iter: 1977 loss: 5.40548126e-06
Iter: 1978 loss: 5.4212087e-06
Iter: 1979 loss: 5.40407837e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.4/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi0.8
+ date
Sun Nov  8 19:19:48 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -1 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda2f27b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda2f299bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda2f299ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda2f367e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda2f3732f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda2f3737b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda2f3730d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda01ca3048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda01ca32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda01ca3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda01ca1ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda01c5a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda01c5aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda01c06620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda01c93950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda01c6b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda01c6b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc46aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda2f222950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda2f22ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc3f0620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda01bdf598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda01bb8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda01bb8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc38d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc38dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc3ac840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc29d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc29d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc2ac730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc351268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc3141e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc3142f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc30e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc275b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9dc1cb0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.4557442
test_loss: 0.45923102
train_loss: 0.44632053
test_loss: 0.4437042
train_loss: 0.43057245
test_loss: 0.4239297
train_loss: 0.39334846
test_loss: 0.40092602
train_loss: 0.3765123
test_loss: 0.37493512
train_loss: 0.34768057
test_loss: 0.34595796
train_loss: 0.3153663
test_loss: 0.31387198
train_loss: 0.27568537
test_loss: 0.27759987
train_loss: 0.23642895
test_loss: 0.23525457
train_loss: 0.18552645
test_loss: 0.1860788
train_loss: 0.1386924
test_loss: 0.13972917
train_loss: 0.10269061
test_loss: 0.10215109
train_loss: 0.07899491
test_loss: 0.07931851
train_loss: 0.071316004
test_loss: 0.071566984
train_loss: 0.06928871
test_loss: 0.069683544
train_loss: 0.06843823
test_loss: 0.06903906
train_loss: 0.06866035
test_loss: 0.068733476
train_loss: 0.069006294
test_loss: 0.06852606
train_loss: 0.06871893
test_loss: 0.068377905
train_loss: 0.066966
test_loss: 0.0682607
train_loss: 0.06881606
test_loss: 0.068148024
train_loss: 0.06719588
test_loss: 0.068071775
train_loss: 0.06859463
test_loss: 0.068006724
train_loss: 0.067590885
test_loss: 0.067898974
train_loss: 0.06864949
test_loss: 0.06785789
train_loss: 0.06940112
test_loss: 0.06777922
train_loss: 0.06730047
test_loss: 0.06774099
train_loss: 0.06631941
test_loss: 0.06768432
train_loss: 0.06671981
test_loss: 0.06764283
train_loss: 0.066709355
test_loss: 0.06758932
train_loss: 0.066611834
test_loss: 0.06753343
train_loss: 0.066671774
test_loss: 0.067457
train_loss: 0.06637715
test_loss: 0.06737248
train_loss: 0.067388564
test_loss: 0.06724564
train_loss: 0.06662263
test_loss: 0.06719836
train_loss: 0.066864185
test_loss: 0.0670602
train_loss: 0.068307176
test_loss: 0.06691824
train_loss: 0.06703116
test_loss: 0.066770196
train_loss: 0.06591115
test_loss: 0.06661111
train_loss: 0.066591546
test_loss: 0.06645821
train_loss: 0.066404045
test_loss: 0.066233695
train_loss: 0.065135606
test_loss: 0.065941945
train_loss: 0.06538846
test_loss: 0.06565197
train_loss: 0.06570144
test_loss: 0.06532061
train_loss: 0.06395239
test_loss: 0.064887375
train_loss: 0.064540654
test_loss: 0.06445687
train_loss: 0.06431861
test_loss: 0.06378702
train_loss: 0.06264977
test_loss: 0.06313341
train_loss: 0.062368326
test_loss: 0.062305402
train_loss: 0.05972345
test_loss: 0.061472714
train_loss: 0.059768178
test_loss: 0.060474828
train_loss: 0.058994994
test_loss: 0.05932158
train_loss: 0.05709989
test_loss: 0.05805012
train_loss: 0.055723973
test_loss: 0.05646956
train_loss: 0.054882944
test_loss: 0.054385394
train_loss: 0.052798457
test_loss: 0.05219626
train_loss: 0.049219552
test_loss: 0.049362756
train_loss: 0.045011908
test_loss: 0.04650625
train_loss: 0.044090442
test_loss: 0.0436479
train_loss: 0.038208272
test_loss: 0.04010654
train_loss: 0.03528813
test_loss: 0.035885915
train_loss: 0.031274255
test_loss: 0.03185957
train_loss: 0.028774884
test_loss: 0.02877422
train_loss: 0.026355173
test_loss: 0.027061664
train_loss: 0.02471247
test_loss: 0.02398939
train_loss: 0.022668539
test_loss: 0.022569826
train_loss: 0.022989899
test_loss: 0.022424173
train_loss: 0.021081898
test_loss: 0.021505382
train_loss: 0.021252591
test_loss: 0.021031247
train_loss: 0.021205943
test_loss: 0.020706784
train_loss: 0.01972656
test_loss: 0.020273326
train_loss: 0.020247057
test_loss: 0.020299135
train_loss: 0.019301958
test_loss: 0.019990837
train_loss: 0.01923843
test_loss: 0.019799085
train_loss: 0.019512672
test_loss: 0.019532192
train_loss: 0.019476578
test_loss: 0.019481862
train_loss: 0.019003877
test_loss: 0.019306732
train_loss: 0.018869147
test_loss: 0.019388985
train_loss: 0.018783564
test_loss: 0.019115757
train_loss: 0.018355066
test_loss: 0.019033372
train_loss: 0.018796522
test_loss: 0.019235626
train_loss: 0.018951919
test_loss: 0.018785484
train_loss: 0.019190129
test_loss: 0.019584298
train_loss: 0.019169092
test_loss: 0.018800931
train_loss: 0.018813536
test_loss: 0.019102069
train_loss: 0.018488567
test_loss: 0.018518277
train_loss: 0.018333023
test_loss: 0.018779432
train_loss: 0.018273965
test_loss: 0.018399248
train_loss: 0.018090516
test_loss: 0.018377578
train_loss: 0.018556107
test_loss: 0.018519105
train_loss: 0.018423244
test_loss: 0.018441834
train_loss: 0.017799787
test_loss: 0.018078052
train_loss: 0.018723499
test_loss: 0.018144285
train_loss: 0.017449984
test_loss: 0.018901736
train_loss: 0.017701924
test_loss: 0.018003622
train_loss: 0.01757869
test_loss: 0.017999854
train_loss: 0.017431725
test_loss: 0.017869659
train_loss: 0.017380714
test_loss: 0.017835585
train_loss: 0.018183958
test_loss: 0.017796827
train_loss: 0.017296068
test_loss: 0.018087579
train_loss: 0.017513698
test_loss: 0.017728204
train_loss: 0.01736166
test_loss: 0.017527286
train_loss: 0.016747445
test_loss: 0.017674893
train_loss: 0.016883675
test_loss: 0.017605187
train_loss: 0.016986992
test_loss: 0.017491356
train_loss: 0.017391393
test_loss: 0.017521003
train_loss: 0.016385239
test_loss: 0.01736755
train_loss: 0.017407054
test_loss: 0.017596206
train_loss: 0.016718408
test_loss: 0.017373176
train_loss: 0.016925626
test_loss: 0.01698789
train_loss: 0.017383903
test_loss: 0.016860506
train_loss: 0.015427091
test_loss: 0.016913926
train_loss: 0.015502392
test_loss: 0.016927179
train_loss: 0.017030766
test_loss: 0.016683834
train_loss: 0.014713384
test_loss: 0.015847933
train_loss: 0.015019114
test_loss: 0.01564262
train_loss: 0.014623315
test_loss: 0.015494472
train_loss: 0.014942026
test_loss: 0.015005655
train_loss: 0.013465025
test_loss: 0.01455578
train_loss: 0.013086522
test_loss: 0.013854632
train_loss: 0.012975281
test_loss: 0.013588217
train_loss: 0.012524667
test_loss: 0.0131816175
train_loss: 0.011799856
test_loss: 0.012873229
train_loss: 0.011013558
test_loss: 0.011287849
train_loss: 0.010694345
test_loss: 0.0119522
train_loss: 0.010164818
test_loss: 0.009970069
train_loss: 0.009582423
test_loss: 0.010767188
train_loss: 0.0114513645
test_loss: 0.0094810845
train_loss: 0.007970595
test_loss: 0.0087528275
train_loss: 0.008220522
test_loss: 0.008417583
train_loss: 0.0070652296
test_loss: 0.007310567
train_loss: 0.0069081583
test_loss: 0.0067834
train_loss: 0.0064086844
test_loss: 0.0075395405
train_loss: 0.0066479817
test_loss: 0.006723017
train_loss: 0.0060294606
test_loss: 0.006291694
train_loss: 0.005574191
test_loss: 0.0058003296
train_loss: 0.006114714
test_loss: 0.0060795653
train_loss: 0.0062361215
test_loss: 0.0061092223
train_loss: 0.006125693
test_loss: 0.0072451765
train_loss: 0.0053946795
test_loss: 0.0059971428
train_loss: 0.005529823
test_loss: 0.005876746
train_loss: 0.005610911
test_loss: 0.005804496
train_loss: 0.00519224
test_loss: 0.004985507
train_loss: 0.004970672
test_loss: 0.0053274874
train_loss: 0.005456033
test_loss: 0.005317435
train_loss: 0.0052844253
test_loss: 0.0054807644
train_loss: 0.0050399117
test_loss: 0.0053499555
train_loss: 0.0048395437
test_loss: 0.004757224
train_loss: 0.0052593574
test_loss: 0.0052708206
train_loss: 0.0052212044
test_loss: 0.0051157204
train_loss: 0.0050463295
test_loss: 0.004704093
train_loss: 0.0046412144
test_loss: 0.004673437
train_loss: 0.0051025106
test_loss: 0.005251052
train_loss: 0.005006263
test_loss: 0.005058172
train_loss: 0.0052265916
test_loss: 0.0049221586
train_loss: 0.004729877
test_loss: 0.004925913
train_loss: 0.0047502555
test_loss: 0.004873228
train_loss: 0.0047829715
test_loss: 0.004945504
train_loss: 0.005914419
test_loss: 0.005457849
train_loss: 0.0047021536
test_loss: 0.0047479267
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi0.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -1 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi0.8/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdeacc15488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdeaccdaea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdeacd26c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdeacc46488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdeacc78488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdeacbd4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdeacbb8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdeacb610d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdeacb617b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdeacbb8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdeacb61510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde5f660ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde5f654488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde5f654268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde5f5bb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde5f5bb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdeacb1cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde5f5bbae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde5f550950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde5f5af7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde385f4bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde385bd378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde38587598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3858d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3858e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde38545488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde384ebb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde385202f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde38520378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde384e06a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde384857b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde384a2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde384a2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3845d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde384076a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde38407730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.47462337e-05
Iter: 2 loss: 0.000124717975
Iter: 3 loss: 2.71793397e-05
Iter: 4 loss: 2.43771592e-05
Iter: 5 loss: 2.17550387e-05
Iter: 6 loss: 2.10981052e-05
Iter: 7 loss: 1.97803311e-05
Iter: 8 loss: 1.91800427e-05
Iter: 9 loss: 1.80358966e-05
Iter: 10 loss: 2.03797663e-05
Iter: 11 loss: 1.75772038e-05
Iter: 12 loss: 1.65748443e-05
Iter: 13 loss: 1.70695766e-05
Iter: 14 loss: 1.59036717e-05
Iter: 15 loss: 1.51923432e-05
Iter: 16 loss: 1.79528524e-05
Iter: 17 loss: 1.50259439e-05
Iter: 18 loss: 1.43495927e-05
Iter: 19 loss: 1.50675596e-05
Iter: 20 loss: 1.39766435e-05
Iter: 21 loss: 1.34583061e-05
Iter: 22 loss: 1.70427193e-05
Iter: 23 loss: 1.34093934e-05
Iter: 24 loss: 1.29661548e-05
Iter: 25 loss: 1.30858507e-05
Iter: 26 loss: 1.26444666e-05
Iter: 27 loss: 1.21632156e-05
Iter: 28 loss: 1.28454494e-05
Iter: 29 loss: 1.19271917e-05
Iter: 30 loss: 1.15012281e-05
Iter: 31 loss: 1.19970109e-05
Iter: 32 loss: 1.12736143e-05
Iter: 33 loss: 1.09019e-05
Iter: 34 loss: 1.60027048e-05
Iter: 35 loss: 1.0900676e-05
Iter: 36 loss: 1.06470225e-05
Iter: 37 loss: 1.02089907e-05
Iter: 38 loss: 1.02086842e-05
Iter: 39 loss: 9.85889e-06
Iter: 40 loss: 1.09112025e-05
Iter: 41 loss: 9.75347575e-06
Iter: 42 loss: 9.7224538e-06
Iter: 43 loss: 9.6100739e-06
Iter: 44 loss: 9.49466084e-06
Iter: 45 loss: 9.29831458e-06
Iter: 46 loss: 9.29806538e-06
Iter: 47 loss: 9.06884452e-06
Iter: 48 loss: 1.08492404e-05
Iter: 49 loss: 9.05249635e-06
Iter: 50 loss: 8.93691504e-06
Iter: 51 loss: 8.79348e-06
Iter: 52 loss: 8.78126e-06
Iter: 53 loss: 8.64524191e-06
Iter: 54 loss: 8.64261165e-06
Iter: 55 loss: 8.53716665e-06
Iter: 56 loss: 8.5531683e-06
Iter: 57 loss: 8.45739851e-06
Iter: 58 loss: 8.34229468e-06
Iter: 59 loss: 9.08051697e-06
Iter: 60 loss: 8.32971455e-06
Iter: 61 loss: 8.24861854e-06
Iter: 62 loss: 8.36937306e-06
Iter: 63 loss: 8.20972764e-06
Iter: 64 loss: 8.12295275e-06
Iter: 65 loss: 8.8161969e-06
Iter: 66 loss: 8.11702557e-06
Iter: 67 loss: 8.05503259e-06
Iter: 68 loss: 8.03314e-06
Iter: 69 loss: 7.99819281e-06
Iter: 70 loss: 7.92525407e-06
Iter: 71 loss: 8.29578494e-06
Iter: 72 loss: 7.91310049e-06
Iter: 73 loss: 7.8266421e-06
Iter: 74 loss: 8.12684812e-06
Iter: 75 loss: 7.80389291e-06
Iter: 76 loss: 7.74886939e-06
Iter: 77 loss: 7.65143614e-06
Iter: 78 loss: 7.65137429e-06
Iter: 79 loss: 7.63711705e-06
Iter: 80 loss: 7.59961904e-06
Iter: 81 loss: 7.55856581e-06
Iter: 82 loss: 7.5211e-06
Iter: 83 loss: 7.51098787e-06
Iter: 84 loss: 7.43184546e-06
Iter: 85 loss: 7.67650272e-06
Iter: 86 loss: 7.40843461e-06
Iter: 87 loss: 7.3563574e-06
Iter: 88 loss: 7.27881888e-06
Iter: 89 loss: 7.27706447e-06
Iter: 90 loss: 7.20675143e-06
Iter: 91 loss: 7.20663684e-06
Iter: 92 loss: 7.15695387e-06
Iter: 93 loss: 7.29375961e-06
Iter: 94 loss: 7.14068801e-06
Iter: 95 loss: 7.09965252e-06
Iter: 96 loss: 7.45292755e-06
Iter: 97 loss: 7.09732558e-06
Iter: 98 loss: 7.0626611e-06
Iter: 99 loss: 7.06036053e-06
Iter: 100 loss: 7.03408568e-06
Iter: 101 loss: 6.99641259e-06
Iter: 102 loss: 7.17437842e-06
Iter: 103 loss: 6.98942222e-06
Iter: 104 loss: 6.94869823e-06
Iter: 105 loss: 7.15600618e-06
Iter: 106 loss: 6.94206028e-06
Iter: 107 loss: 6.9143357e-06
Iter: 108 loss: 6.93112452e-06
Iter: 109 loss: 6.89641683e-06
Iter: 110 loss: 6.84978204e-06
Iter: 111 loss: 6.97280666e-06
Iter: 112 loss: 6.8340687e-06
Iter: 113 loss: 6.80651237e-06
Iter: 114 loss: 6.87774764e-06
Iter: 115 loss: 6.79707136e-06
Iter: 116 loss: 6.75457113e-06
Iter: 117 loss: 6.8316026e-06
Iter: 118 loss: 6.73616e-06
Iter: 119 loss: 6.71110683e-06
Iter: 120 loss: 6.65893549e-06
Iter: 121 loss: 7.55086921e-06
Iter: 122 loss: 6.65767129e-06
Iter: 123 loss: 6.61677041e-06
Iter: 124 loss: 7.16105205e-06
Iter: 125 loss: 6.61652575e-06
Iter: 126 loss: 6.57991313e-06
Iter: 127 loss: 6.71446742e-06
Iter: 128 loss: 6.57075088e-06
Iter: 129 loss: 6.5403633e-06
Iter: 130 loss: 6.66105325e-06
Iter: 131 loss: 6.53358393e-06
Iter: 132 loss: 6.50449329e-06
Iter: 133 loss: 6.60000751e-06
Iter: 134 loss: 6.49635876e-06
Iter: 135 loss: 6.4698329e-06
Iter: 136 loss: 6.58376e-06
Iter: 137 loss: 6.46420494e-06
Iter: 138 loss: 6.44089232e-06
Iter: 139 loss: 6.43239264e-06
Iter: 140 loss: 6.41927272e-06
Iter: 141 loss: 6.39653308e-06
Iter: 142 loss: 6.67753739e-06
Iter: 143 loss: 6.39625523e-06
Iter: 144 loss: 6.37249332e-06
Iter: 145 loss: 6.3709258e-06
Iter: 146 loss: 6.35302e-06
Iter: 147 loss: 6.32980164e-06
Iter: 148 loss: 6.38969686e-06
Iter: 149 loss: 6.32175943e-06
Iter: 150 loss: 6.29134593e-06
Iter: 151 loss: 6.47021534e-06
Iter: 152 loss: 6.28738e-06
Iter: 153 loss: 6.27191275e-06
Iter: 154 loss: 6.30563454e-06
Iter: 155 loss: 6.26576093e-06
Iter: 156 loss: 6.2435538e-06
Iter: 157 loss: 6.22294965e-06
Iter: 158 loss: 6.21769686e-06
Iter: 159 loss: 6.19476577e-06
Iter: 160 loss: 6.19021e-06
Iter: 161 loss: 6.1752e-06
Iter: 162 loss: 6.14701275e-06
Iter: 163 loss: 6.4291321e-06
Iter: 164 loss: 6.14601049e-06
Iter: 165 loss: 6.12054919e-06
Iter: 166 loss: 6.19041748e-06
Iter: 167 loss: 6.1120918e-06
Iter: 168 loss: 6.0916218e-06
Iter: 169 loss: 6.23073538e-06
Iter: 170 loss: 6.08956771e-06
Iter: 171 loss: 6.07256197e-06
Iter: 172 loss: 6.09289145e-06
Iter: 173 loss: 6.06358662e-06
Iter: 174 loss: 6.04603429e-06
Iter: 175 loss: 6.22178914e-06
Iter: 176 loss: 6.04541583e-06
Iter: 177 loss: 6.03270837e-06
Iter: 178 loss: 6.01541433e-06
Iter: 179 loss: 6.01453303e-06
Iter: 180 loss: 5.99450095e-06
Iter: 181 loss: 6.03063381e-06
Iter: 182 loss: 5.98583028e-06
Iter: 183 loss: 5.97189819e-06
Iter: 184 loss: 5.97020062e-06
Iter: 185 loss: 5.96177279e-06
Iter: 186 loss: 5.95767096e-06
Iter: 187 loss: 5.95362872e-06
Iter: 188 loss: 5.93768527e-06
Iter: 189 loss: 5.97731332e-06
Iter: 190 loss: 5.93200775e-06
Iter: 191 loss: 5.92192282e-06
Iter: 192 loss: 5.92349352e-06
Iter: 193 loss: 5.91432399e-06
Iter: 194 loss: 5.89708588e-06
Iter: 195 loss: 6.00080557e-06
Iter: 196 loss: 5.89485535e-06
Iter: 197 loss: 5.88592411e-06
Iter: 198 loss: 5.8686619e-06
Iter: 199 loss: 6.22710741e-06
Iter: 200 loss: 5.86856322e-06
Iter: 201 loss: 5.85167527e-06
Iter: 202 loss: 6.07186848e-06
Iter: 203 loss: 5.85146563e-06
Iter: 204 loss: 5.83824749e-06
Iter: 205 loss: 5.84790723e-06
Iter: 206 loss: 5.8302021e-06
Iter: 207 loss: 5.81336917e-06
Iter: 208 loss: 5.90681339e-06
Iter: 209 loss: 5.81108179e-06
Iter: 210 loss: 5.79864627e-06
Iter: 211 loss: 5.81411859e-06
Iter: 212 loss: 5.79213656e-06
Iter: 213 loss: 5.78162144e-06
Iter: 214 loss: 5.78154913e-06
Iter: 215 loss: 5.77417268e-06
Iter: 216 loss: 5.76136927e-06
Iter: 217 loss: 6.08009304e-06
Iter: 218 loss: 5.76143475e-06
Iter: 219 loss: 5.74868818e-06
Iter: 220 loss: 5.77543869e-06
Iter: 221 loss: 5.74381647e-06
Iter: 222 loss: 5.73519947e-06
Iter: 223 loss: 5.73418674e-06
Iter: 224 loss: 5.72882e-06
Iter: 225 loss: 5.72163026e-06
Iter: 226 loss: 5.72123e-06
Iter: 227 loss: 5.70758675e-06
Iter: 228 loss: 5.73568e-06
Iter: 229 loss: 5.702e-06
Iter: 230 loss: 5.69351778e-06
Iter: 231 loss: 5.68255427e-06
Iter: 232 loss: 5.68191535e-06
Iter: 233 loss: 5.67583e-06
Iter: 234 loss: 5.67338247e-06
Iter: 235 loss: 5.66657945e-06
Iter: 236 loss: 5.65862365e-06
Iter: 237 loss: 5.65777873e-06
Iter: 238 loss: 5.64672291e-06
Iter: 239 loss: 5.701384e-06
Iter: 240 loss: 5.64483889e-06
Iter: 241 loss: 5.6361323e-06
Iter: 242 loss: 5.64892616e-06
Iter: 243 loss: 5.63190542e-06
Iter: 244 loss: 5.6211361e-06
Iter: 245 loss: 5.65863411e-06
Iter: 246 loss: 5.61834349e-06
Iter: 247 loss: 5.60967419e-06
Iter: 248 loss: 5.61483921e-06
Iter: 249 loss: 5.60421267e-06
Iter: 250 loss: 5.59682394e-06
Iter: 251 loss: 5.59661385e-06
Iter: 252 loss: 5.59103501e-06
Iter: 253 loss: 5.58027386e-06
Iter: 254 loss: 5.79809966e-06
Iter: 255 loss: 5.58015836e-06
Iter: 256 loss: 5.56978921e-06
Iter: 257 loss: 5.58834836e-06
Iter: 258 loss: 5.56548275e-06
Iter: 259 loss: 5.55855422e-06
Iter: 260 loss: 5.55789939e-06
Iter: 261 loss: 5.55316183e-06
Iter: 262 loss: 5.55167026e-06
Iter: 263 loss: 5.54907319e-06
Iter: 264 loss: 5.54040798e-06
Iter: 265 loss: 5.5795158e-06
Iter: 266 loss: 5.53868267e-06
Iter: 267 loss: 5.53452446e-06
Iter: 268 loss: 5.5246278e-06
Iter: 269 loss: 5.6406675e-06
Iter: 270 loss: 5.52385109e-06
Iter: 271 loss: 5.51659559e-06
Iter: 272 loss: 5.51619496e-06
Iter: 273 loss: 5.50983532e-06
Iter: 274 loss: 5.52213714e-06
Iter: 275 loss: 5.50718187e-06
Iter: 276 loss: 5.49992455e-06
Iter: 277 loss: 5.50946515e-06
Iter: 278 loss: 5.49632114e-06
Iter: 279 loss: 5.48935122e-06
Iter: 280 loss: 5.51e-06
Iter: 281 loss: 5.48728622e-06
Iter: 282 loss: 5.47937816e-06
Iter: 283 loss: 5.48962271e-06
Iter: 284 loss: 5.47540776e-06
Iter: 285 loss: 5.46882711e-06
Iter: 286 loss: 5.48218941e-06
Iter: 287 loss: 5.46600722e-06
Iter: 288 loss: 5.4596585e-06
Iter: 289 loss: 5.55905081e-06
Iter: 290 loss: 5.45970761e-06
Iter: 291 loss: 5.45567173e-06
Iter: 292 loss: 5.44671866e-06
Iter: 293 loss: 5.59037244e-06
Iter: 294 loss: 5.44635077e-06
Iter: 295 loss: 5.43778697e-06
Iter: 296 loss: 5.46201818e-06
Iter: 297 loss: 5.43511896e-06
Iter: 298 loss: 5.42997532e-06
Iter: 299 loss: 5.42918679e-06
Iter: 300 loss: 5.42575617e-06
Iter: 301 loss: 5.42518e-06
Iter: 302 loss: 5.42268117e-06
Iter: 303 loss: 5.41586769e-06
Iter: 304 loss: 5.4320135e-06
Iter: 305 loss: 5.41337295e-06
Iter: 306 loss: 5.40907331e-06
Iter: 307 loss: 5.40041492e-06
Iter: 308 loss: 5.57708336e-06
Iter: 309 loss: 5.40042493e-06
Iter: 310 loss: 5.39259099e-06
Iter: 311 loss: 5.51655376e-06
Iter: 312 loss: 5.39254825e-06
Iter: 313 loss: 5.38705399e-06
Iter: 314 loss: 5.39471966e-06
Iter: 315 loss: 5.38447694e-06
Iter: 316 loss: 5.37812775e-06
Iter: 317 loss: 5.42507041e-06
Iter: 318 loss: 5.37754204e-06
Iter: 319 loss: 5.37326559e-06
Iter: 320 loss: 5.3824383e-06
Iter: 321 loss: 5.37155302e-06
Iter: 322 loss: 5.36579819e-06
Iter: 323 loss: 5.36892185e-06
Iter: 324 loss: 5.3620688e-06
Iter: 325 loss: 5.35645358e-06
Iter: 326 loss: 5.35372556e-06
Iter: 327 loss: 5.35109757e-06
Iter: 328 loss: 5.34288029e-06
Iter: 329 loss: 5.36109837e-06
Iter: 330 loss: 5.33961475e-06
Iter: 331 loss: 5.3380636e-06
Iter: 332 loss: 5.33549155e-06
Iter: 333 loss: 5.33246339e-06
Iter: 334 loss: 5.32685272e-06
Iter: 335 loss: 5.45258445e-06
Iter: 336 loss: 5.32674812e-06
Iter: 337 loss: 5.32267859e-06
Iter: 338 loss: 5.3223248e-06
Iter: 339 loss: 5.31979094e-06
Iter: 340 loss: 5.31553587e-06
Iter: 341 loss: 5.31546812e-06
Iter: 342 loss: 5.30997386e-06
Iter: 343 loss: 5.36036487e-06
Iter: 344 loss: 5.30960187e-06
Iter: 345 loss: 5.30669104e-06
Iter: 346 loss: 5.30109719e-06
Iter: 347 loss: 5.42429734e-06
Iter: 348 loss: 5.30103716e-06
Iter: 349 loss: 5.29480894e-06
Iter: 350 loss: 5.3512158e-06
Iter: 351 loss: 5.29453791e-06
Iter: 352 loss: 5.28977853e-06
Iter: 353 loss: 5.28982582e-06
Iter: 354 loss: 5.28595228e-06
Iter: 355 loss: 5.28094097e-06
Iter: 356 loss: 5.28090914e-06
Iter: 357 loss: 5.27748762e-06
Iter: 358 loss: 5.27770317e-06
Iter: 359 loss: 5.27480779e-06
Iter: 360 loss: 5.26903887e-06
Iter: 361 loss: 5.28690634e-06
Iter: 362 loss: 5.26731856e-06
Iter: 363 loss: 5.26315671e-06
Iter: 364 loss: 5.26177473e-06
Iter: 365 loss: 5.25932728e-06
Iter: 366 loss: 5.25632277e-06
Iter: 367 loss: 5.25608e-06
Iter: 368 loss: 5.25310497e-06
Iter: 369 loss: 5.2497362e-06
Iter: 370 loss: 5.24919233e-06
Iter: 371 loss: 5.24638335e-06
Iter: 372 loss: 5.24635198e-06
Iter: 373 loss: 5.24329243e-06
Iter: 374 loss: 5.23734207e-06
Iter: 375 loss: 5.3568865e-06
Iter: 376 loss: 5.23712697e-06
Iter: 377 loss: 5.23351e-06
Iter: 378 loss: 5.23332346e-06
Iter: 379 loss: 5.23061317e-06
Iter: 380 loss: 5.22461823e-06
Iter: 381 loss: 5.3283884e-06
Iter: 382 loss: 5.22450364e-06
Iter: 383 loss: 5.21926722e-06
Iter: 384 loss: 5.26627446e-06
Iter: 385 loss: 5.21919173e-06
Iter: 386 loss: 5.21475249e-06
Iter: 387 loss: 5.21895e-06
Iter: 388 loss: 5.21223046e-06
Iter: 389 loss: 5.2091209e-06
Iter: 390 loss: 5.20914909e-06
Iter: 391 loss: 5.20668709e-06
Iter: 392 loss: 5.20466028e-06
Iter: 393 loss: 5.20411641e-06
Iter: 394 loss: 5.20051799e-06
Iter: 395 loss: 5.23397557e-06
Iter: 396 loss: 5.20043477e-06
Iter: 397 loss: 5.19783725e-06
Iter: 398 loss: 5.19397872e-06
Iter: 399 loss: 5.19392142e-06
Iter: 400 loss: 5.19226069e-06
Iter: 401 loss: 5.19126934e-06
Iter: 402 loss: 5.18940669e-06
Iter: 403 loss: 5.18814977e-06
Iter: 404 loss: 5.18742354e-06
Iter: 405 loss: 5.18318711e-06
Iter: 406 loss: 5.1918405e-06
Iter: 407 loss: 5.18168781e-06
Iter: 408 loss: 5.17908165e-06
Iter: 409 loss: 5.1809111e-06
Iter: 410 loss: 5.17737089e-06
Iter: 411 loss: 5.1734155e-06
Iter: 412 loss: 5.18757861e-06
Iter: 413 loss: 5.17230819e-06
Iter: 414 loss: 5.16944692e-06
Iter: 415 loss: 5.16565115e-06
Iter: 416 loss: 5.16545788e-06
Iter: 417 loss: 5.16083e-06
Iter: 418 loss: 5.19942387e-06
Iter: 419 loss: 5.1606562e-06
Iter: 420 loss: 5.15667398e-06
Iter: 421 loss: 5.16815e-06
Iter: 422 loss: 5.1554639e-06
Iter: 423 loss: 5.15232068e-06
Iter: 424 loss: 5.17757962e-06
Iter: 425 loss: 5.15221382e-06
Iter: 426 loss: 5.14934118e-06
Iter: 427 loss: 5.14722342e-06
Iter: 428 loss: 5.14636395e-06
Iter: 429 loss: 5.14189287e-06
Iter: 430 loss: 5.17248e-06
Iter: 431 loss: 5.14154772e-06
Iter: 432 loss: 5.13836358e-06
Iter: 433 loss: 5.13476516e-06
Iter: 434 loss: 5.13428904e-06
Iter: 435 loss: 5.1311672e-06
Iter: 436 loss: 5.13066925e-06
Iter: 437 loss: 5.12865427e-06
Iter: 438 loss: 5.12574024e-06
Iter: 439 loss: 5.12568e-06
Iter: 440 loss: 5.12096085e-06
Iter: 441 loss: 5.15024158e-06
Iter: 442 loss: 5.12046699e-06
Iter: 443 loss: 5.11824692e-06
Iter: 444 loss: 5.1150837e-06
Iter: 445 loss: 5.11499275e-06
Iter: 446 loss: 5.11030521e-06
Iter: 447 loss: 5.15393276e-06
Iter: 448 loss: 5.11022836e-06
Iter: 449 loss: 5.10810332e-06
Iter: 450 loss: 5.10470727e-06
Iter: 451 loss: 5.1046427e-06
Iter: 452 loss: 5.10109476e-06
Iter: 453 loss: 5.14285421e-06
Iter: 454 loss: 5.10113023e-06
Iter: 455 loss: 5.09811e-06
Iter: 456 loss: 5.12499673e-06
Iter: 457 loss: 5.0980907e-06
Iter: 458 loss: 5.09582742e-06
Iter: 459 loss: 5.09520441e-06
Iter: 460 loss: 5.09393658e-06
Iter: 461 loss: 5.09139409e-06
Iter: 462 loss: 5.10898644e-06
Iter: 463 loss: 5.09106212e-06
Iter: 464 loss: 5.08875655e-06
Iter: 465 loss: 5.08606809e-06
Iter: 466 loss: 5.08582161e-06
Iter: 467 loss: 5.08408039e-06
Iter: 468 loss: 5.08363973e-06
Iter: 469 loss: 5.08185167e-06
Iter: 470 loss: 5.07856112e-06
Iter: 471 loss: 5.07865661e-06
Iter: 472 loss: 5.07659297e-06
Iter: 473 loss: 5.07651112e-06
Iter: 474 loss: 5.07487766e-06
Iter: 475 loss: 5.07155391e-06
Iter: 476 loss: 5.1246252e-06
Iter: 477 loss: 5.07137702e-06
Iter: 478 loss: 5.06868e-06
Iter: 479 loss: 5.06866672e-06
Iter: 480 loss: 5.06647621e-06
Iter: 481 loss: 5.06268816e-06
Iter: 482 loss: 5.06268316e-06
Iter: 483 loss: 5.05884418e-06
Iter: 484 loss: 5.06817059e-06
Iter: 485 loss: 5.05740263e-06
Iter: 486 loss: 5.05427124e-06
Iter: 487 loss: 5.05422759e-06
Iter: 488 loss: 5.05195567e-06
Iter: 489 loss: 5.05491107e-06
Iter: 490 loss: 5.05076605e-06
Iter: 491 loss: 5.047983e-06
Iter: 492 loss: 5.05001935e-06
Iter: 493 loss: 5.04646459e-06
Iter: 494 loss: 5.04362515e-06
Iter: 495 loss: 5.06068864e-06
Iter: 496 loss: 5.04319e-06
Iter: 497 loss: 5.04075297e-06
Iter: 498 loss: 5.03926276e-06
Iter: 499 loss: 5.03825095e-06
Iter: 500 loss: 5.03422643e-06
Iter: 501 loss: 5.0842209e-06
Iter: 502 loss: 5.03404044e-06
Iter: 503 loss: 5.03238243e-06
Iter: 504 loss: 5.03244564e-06
Iter: 505 loss: 5.03109186e-06
Iter: 506 loss: 5.02791363e-06
Iter: 507 loss: 5.03118e-06
Iter: 508 loss: 5.0260237e-06
Iter: 509 loss: 5.02381135e-06
Iter: 510 loss: 5.0287681e-06
Iter: 511 loss: 5.02302919e-06
Iter: 512 loss: 5.01994828e-06
Iter: 513 loss: 5.02651892e-06
Iter: 514 loss: 5.01877275e-06
Iter: 515 loss: 5.01660088e-06
Iter: 516 loss: 5.01439217e-06
Iter: 517 loss: 5.01390468e-06
Iter: 518 loss: 5.01066916e-06
Iter: 519 loss: 5.04179661e-06
Iter: 520 loss: 5.01057912e-06
Iter: 521 loss: 5.00807482e-06
Iter: 522 loss: 5.03220053e-06
Iter: 523 loss: 5.00809711e-06
Iter: 524 loss: 5.00623e-06
Iter: 525 loss: 5.00645183e-06
Iter: 526 loss: 5.00495298e-06
Iter: 527 loss: 5.00253554e-06
Iter: 528 loss: 5.0055387e-06
Iter: 529 loss: 5.00124406e-06
Iter: 530 loss: 4.99834596e-06
Iter: 531 loss: 5.00815349e-06
Iter: 532 loss: 4.99764246e-06
Iter: 533 loss: 4.99562202e-06
Iter: 534 loss: 5.01327304e-06
Iter: 535 loss: 4.99542512e-06
Iter: 536 loss: 4.99340103e-06
Iter: 537 loss: 4.99450744e-06
Iter: 538 loss: 4.99191447e-06
Iter: 539 loss: 4.99028783e-06
Iter: 540 loss: 5.00114675e-06
Iter: 541 loss: 4.99002226e-06
Iter: 542 loss: 4.98797044e-06
Iter: 543 loss: 4.98569079e-06
Iter: 544 loss: 4.98538566e-06
Iter: 545 loss: 4.98368627e-06
Iter: 546 loss: 5.00261285e-06
Iter: 547 loss: 4.98355894e-06
Iter: 548 loss: 4.98161717e-06
Iter: 549 loss: 4.97964174e-06
Iter: 550 loss: 4.97940664e-06
Iter: 551 loss: 4.97709743e-06
Iter: 552 loss: 4.97719384e-06
Iter: 553 loss: 4.97537e-06
Iter: 554 loss: 4.97308429e-06
Iter: 555 loss: 4.97319616e-06
Iter: 556 loss: 4.97108795e-06
Iter: 557 loss: 4.9745604e-06
Iter: 558 loss: 4.97026213e-06
Iter: 559 loss: 4.9680707e-06
Iter: 560 loss: 4.97181463e-06
Iter: 561 loss: 4.96703888e-06
Iter: 562 loss: 4.96504344e-06
Iter: 563 loss: 4.96979328e-06
Iter: 564 loss: 4.96429311e-06
Iter: 565 loss: 4.96206258e-06
Iter: 566 loss: 4.96720304e-06
Iter: 567 loss: 4.96132361e-06
Iter: 568 loss: 4.9598043e-06
Iter: 569 loss: 4.95979839e-06
Iter: 570 loss: 4.95864606e-06
Iter: 571 loss: 4.95728364e-06
Iter: 572 loss: 4.95720587e-06
Iter: 573 loss: 4.95545692e-06
Iter: 574 loss: 4.9735927e-06
Iter: 575 loss: 4.95545373e-06
Iter: 576 loss: 4.95393488e-06
Iter: 577 loss: 4.95176755e-06
Iter: 578 loss: 4.95185486e-06
Iter: 579 loss: 4.95014319e-06
Iter: 580 loss: 4.97250676e-06
Iter: 581 loss: 4.95011682e-06
Iter: 582 loss: 4.94844699e-06
Iter: 583 loss: 4.94726464e-06
Iter: 584 loss: 4.94664891e-06
Iter: 585 loss: 4.94488086e-06
Iter: 586 loss: 4.94531105e-06
Iter: 587 loss: 4.9435921e-06
Iter: 588 loss: 4.94238111e-06
Iter: 589 loss: 4.94196865e-06
Iter: 590 loss: 4.94088181e-06
Iter: 591 loss: 4.94008373e-06
Iter: 592 loss: 4.9395976e-06
Iter: 593 loss: 4.93747e-06
Iter: 594 loss: 4.94202777e-06
Iter: 595 loss: 4.9365658e-06
Iter: 596 loss: 4.93444441e-06
Iter: 597 loss: 4.94084543e-06
Iter: 598 loss: 4.93382731e-06
Iter: 599 loss: 4.93182779e-06
Iter: 600 loss: 4.93977086e-06
Iter: 601 loss: 4.9314458e-06
Iter: 602 loss: 4.92928302e-06
Iter: 603 loss: 4.93880589e-06
Iter: 604 loss: 4.92884374e-06
Iter: 605 loss: 4.92728395e-06
Iter: 606 loss: 4.92770869e-06
Iter: 607 loss: 4.92629079e-06
Iter: 608 loss: 4.92411618e-06
Iter: 609 loss: 4.93562402e-06
Iter: 610 loss: 4.92367963e-06
Iter: 611 loss: 4.92239724e-06
Iter: 612 loss: 4.9203436e-06
Iter: 613 loss: 4.92029312e-06
Iter: 614 loss: 4.9182222e-06
Iter: 615 loss: 4.95320637e-06
Iter: 616 loss: 4.91816536e-06
Iter: 617 loss: 4.91655555e-06
Iter: 618 loss: 4.91434957e-06
Iter: 619 loss: 4.91431729e-06
Iter: 620 loss: 4.91191622e-06
Iter: 621 loss: 4.91293758e-06
Iter: 622 loss: 4.91008313e-06
Iter: 623 loss: 4.90985303e-06
Iter: 624 loss: 4.90871025e-06
Iter: 625 loss: 4.90748744e-06
Iter: 626 loss: 4.90505499e-06
Iter: 627 loss: 4.94994947e-06
Iter: 628 loss: 4.90508864e-06
Iter: 629 loss: 4.90299772e-06
Iter: 630 loss: 4.93197967e-06
Iter: 631 loss: 4.90306775e-06
Iter: 632 loss: 4.90132379e-06
Iter: 633 loss: 4.89927288e-06
Iter: 634 loss: 4.89906688e-06
Iter: 635 loss: 4.89750528e-06
Iter: 636 loss: 4.89745389e-06
Iter: 637 loss: 4.89610647e-06
Iter: 638 loss: 4.89732929e-06
Iter: 639 loss: 4.8953807e-06
Iter: 640 loss: 4.89382728e-06
Iter: 641 loss: 4.89516606e-06
Iter: 642 loss: 4.89293143e-06
Iter: 643 loss: 4.89144486e-06
Iter: 644 loss: 4.9080395e-06
Iter: 645 loss: 4.89134345e-06
Iter: 646 loss: 4.89044487e-06
Iter: 647 loss: 4.88846536e-06
Iter: 648 loss: 4.92020354e-06
Iter: 649 loss: 4.8883212e-06
Iter: 650 loss: 4.88690603e-06
Iter: 651 loss: 4.8868651e-06
Iter: 652 loss: 4.88576461e-06
Iter: 653 loss: 4.88328442e-06
Iter: 654 loss: 4.93135212e-06
Iter: 655 loss: 4.88336445e-06
Iter: 656 loss: 4.88118758e-06
Iter: 657 loss: 4.88756e-06
Iter: 658 loss: 4.88062688e-06
Iter: 659 loss: 4.87919169e-06
Iter: 660 loss: 4.87910074e-06
Iter: 661 loss: 4.87791112e-06
Iter: 662 loss: 4.87573834e-06
Iter: 663 loss: 4.91947685e-06
Iter: 664 loss: 4.87568468e-06
Iter: 665 loss: 4.87367697e-06
Iter: 666 loss: 4.89091963e-06
Iter: 667 loss: 4.87352327e-06
Iter: 668 loss: 4.8713041e-06
Iter: 669 loss: 4.87337365e-06
Iter: 670 loss: 4.87014086e-06
Iter: 671 loss: 4.86909676e-06
Iter: 672 loss: 4.86899762e-06
Iter: 673 loss: 4.86798308e-06
Iter: 674 loss: 4.86622548e-06
Iter: 675 loss: 4.91078208e-06
Iter: 676 loss: 4.86627232e-06
Iter: 677 loss: 4.86434101e-06
Iter: 678 loss: 4.88377827e-06
Iter: 679 loss: 4.86418776e-06
Iter: 680 loss: 4.86282534e-06
Iter: 681 loss: 4.86423778e-06
Iter: 682 loss: 4.86202407e-06
Iter: 683 loss: 4.86007139e-06
Iter: 684 loss: 4.85948567e-06
Iter: 685 loss: 4.85832152e-06
Iter: 686 loss: 4.85686814e-06
Iter: 687 loss: 4.85687497e-06
Iter: 688 loss: 4.85538567e-06
Iter: 689 loss: 4.85267e-06
Iter: 690 loss: 4.91088031e-06
Iter: 691 loss: 4.85265173e-06
Iter: 692 loss: 4.85040255e-06
Iter: 693 loss: 4.8547472e-06
Iter: 694 loss: 4.84933116e-06
Iter: 695 loss: 4.84777365e-06
Iter: 696 loss: 4.84766315e-06
Iter: 697 loss: 4.84662e-06
Iter: 698 loss: 4.84422344e-06
Iter: 699 loss: 4.87234456e-06
Iter: 700 loss: 4.84390966e-06
Iter: 701 loss: 4.84247175e-06
Iter: 702 loss: 4.8425336e-06
Iter: 703 loss: 4.840982e-06
Iter: 704 loss: 4.8409097e-06
Iter: 705 loss: 4.83990971e-06
Iter: 706 loss: 4.83867734e-06
Iter: 707 loss: 4.83867279e-06
Iter: 708 loss: 4.83761733e-06
Iter: 709 loss: 4.83583926e-06
Iter: 710 loss: 4.87858051e-06
Iter: 711 loss: 4.83584972e-06
Iter: 712 loss: 4.83430085e-06
Iter: 713 loss: 4.83421263e-06
Iter: 714 loss: 4.83315489e-06
Iter: 715 loss: 4.83191889e-06
Iter: 716 loss: 4.83167742e-06
Iter: 717 loss: 4.82960968e-06
Iter: 718 loss: 4.83426538e-06
Iter: 719 loss: 4.82866244e-06
Iter: 720 loss: 4.82673113e-06
Iter: 721 loss: 4.82894529e-06
Iter: 722 loss: 4.82572568e-06
Iter: 723 loss: 4.82425139e-06
Iter: 724 loss: 4.82413452e-06
Iter: 725 loss: 4.82299356e-06
Iter: 726 loss: 4.82102132e-06
Iter: 727 loss: 4.82089081e-06
Iter: 728 loss: 4.82013638e-06
Iter: 729 loss: 4.81974439e-06
Iter: 730 loss: 4.81883944e-06
Iter: 731 loss: 4.81633651e-06
Iter: 732 loss: 4.83450185e-06
Iter: 733 loss: 4.81580719e-06
Iter: 734 loss: 4.81308962e-06
Iter: 735 loss: 4.81911684e-06
Iter: 736 loss: 4.81207826e-06
Iter: 737 loss: 4.81100733e-06
Iter: 738 loss: 4.8105926e-06
Iter: 739 loss: 4.80952804e-06
Iter: 740 loss: 4.80980179e-06
Iter: 741 loss: 4.80859399e-06
Iter: 742 loss: 4.8067086e-06
Iter: 743 loss: 4.81103052e-06
Iter: 744 loss: 4.80599738e-06
Iter: 745 loss: 4.80476137e-06
Iter: 746 loss: 4.80703238e-06
Iter: 747 loss: 4.80422523e-06
Iter: 748 loss: 4.80245944e-06
Iter: 749 loss: 4.80420294e-06
Iter: 750 loss: 4.80140034e-06
Iter: 751 loss: 4.7999647e-06
Iter: 752 loss: 4.79936716e-06
Iter: 753 loss: 4.79858409e-06
Iter: 754 loss: 4.79597384e-06
Iter: 755 loss: 4.80434483e-06
Iter: 756 loss: 4.79533719e-06
Iter: 757 loss: 4.79310802e-06
Iter: 758 loss: 4.79501796e-06
Iter: 759 loss: 4.79184291e-06
Iter: 760 loss: 4.79043501e-06
Iter: 761 loss: 4.79029359e-06
Iter: 762 loss: 4.78910215e-06
Iter: 763 loss: 4.78695347e-06
Iter: 764 loss: 4.7868657e-06
Iter: 765 loss: 4.78520542e-06
Iter: 766 loss: 4.80394374e-06
Iter: 767 loss: 4.78513402e-06
Iter: 768 loss: 4.78323545e-06
Iter: 769 loss: 4.7882927e-06
Iter: 770 loss: 4.78282254e-06
Iter: 771 loss: 4.78147194e-06
Iter: 772 loss: 4.78125867e-06
Iter: 773 loss: 4.78038692e-06
Iter: 774 loss: 4.77886897e-06
Iter: 775 loss: 4.77886897e-06
Iter: 776 loss: 4.77815502e-06
Iter: 777 loss: 4.7770327e-06
Iter: 778 loss: 4.77713866e-06
Iter: 779 loss: 4.77498952e-06
Iter: 780 loss: 4.7810322e-06
Iter: 781 loss: 4.77450976e-06
Iter: 782 loss: 4.77309914e-06
Iter: 783 loss: 4.77627418e-06
Iter: 784 loss: 4.77265257e-06
Iter: 785 loss: 4.7707772e-06
Iter: 786 loss: 4.77073627e-06
Iter: 787 loss: 4.76921468e-06
Iter: 788 loss: 4.7674821e-06
Iter: 789 loss: 4.7670178e-06
Iter: 790 loss: 4.765835e-06
Iter: 791 loss: 4.76391551e-06
Iter: 792 loss: 4.79475102e-06
Iter: 793 loss: 4.76387504e-06
Iter: 794 loss: 4.7626072e-06
Iter: 795 loss: 4.76517198e-06
Iter: 796 loss: 4.76203968e-06
Iter: 797 loss: 4.76052082e-06
Iter: 798 loss: 4.77092908e-06
Iter: 799 loss: 4.76044261e-06
Iter: 800 loss: 4.75909565e-06
Iter: 801 loss: 4.75778324e-06
Iter: 802 loss: 4.75767047e-06
Iter: 803 loss: 4.75640354e-06
Iter: 804 loss: 4.75634442e-06
Iter: 805 loss: 4.75543e-06
Iter: 806 loss: 4.75416209e-06
Iter: 807 loss: 4.75413708e-06
Iter: 808 loss: 4.75309753e-06
Iter: 809 loss: 4.75300567e-06
Iter: 810 loss: 4.75228626e-06
Iter: 811 loss: 4.75012166e-06
Iter: 812 loss: 4.76716468e-06
Iter: 813 loss: 4.74970329e-06
Iter: 814 loss: 4.74781245e-06
Iter: 815 loss: 4.75241586e-06
Iter: 816 loss: 4.74710578e-06
Iter: 817 loss: 4.7458866e-06
Iter: 818 loss: 4.74577109e-06
Iter: 819 loss: 4.74506533e-06
Iter: 820 loss: 4.74337685e-06
Iter: 821 loss: 4.75691195e-06
Iter: 822 loss: 4.74301305e-06
Iter: 823 loss: 4.74121225e-06
Iter: 824 loss: 4.74936314e-06
Iter: 825 loss: 4.7407475e-06
Iter: 826 loss: 4.73906766e-06
Iter: 827 loss: 4.74702256e-06
Iter: 828 loss: 4.73880937e-06
Iter: 829 loss: 4.73739101e-06
Iter: 830 loss: 4.74030958e-06
Iter: 831 loss: 4.73681757e-06
Iter: 832 loss: 4.73513046e-06
Iter: 833 loss: 4.73890668e-06
Iter: 834 loss: 4.73445698e-06
Iter: 835 loss: 4.73335786e-06
Iter: 836 loss: 4.73705677e-06
Iter: 837 loss: 4.73296768e-06
Iter: 838 loss: 4.73166483e-06
Iter: 839 loss: 4.74039052e-06
Iter: 840 loss: 4.73162e-06
Iter: 841 loss: 4.73097225e-06
Iter: 842 loss: 4.73282125e-06
Iter: 843 loss: 4.73068303e-06
Iter: 844 loss: 4.72967395e-06
Iter: 845 loss: 4.72971351e-06
Iter: 846 loss: 4.72887632e-06
Iter: 847 loss: 4.72782131e-06
Iter: 848 loss: 4.73537875e-06
Iter: 849 loss: 4.72772535e-06
Iter: 850 loss: 4.72688043e-06
Iter: 851 loss: 4.72538431e-06
Iter: 852 loss: 4.72536931e-06
Iter: 853 loss: 4.72407555e-06
Iter: 854 loss: 4.73422824e-06
Iter: 855 loss: 4.72401553e-06
Iter: 856 loss: 4.7225335e-06
Iter: 857 loss: 4.72634565e-06
Iter: 858 loss: 4.72202464e-06
Iter: 859 loss: 4.72117699e-06
Iter: 860 loss: 4.7191038e-06
Iter: 861 loss: 4.75123261e-06
Iter: 862 loss: 4.71915837e-06
Iter: 863 loss: 4.71695557e-06
Iter: 864 loss: 4.72432384e-06
Iter: 865 loss: 4.71632393e-06
Iter: 866 loss: 4.71522571e-06
Iter: 867 loss: 4.71503245e-06
Iter: 868 loss: 4.71400108e-06
Iter: 869 loss: 4.71283101e-06
Iter: 870 loss: 4.71269e-06
Iter: 871 loss: 4.71131534e-06
Iter: 872 loss: 4.71760541e-06
Iter: 873 loss: 4.71116437e-06
Iter: 874 loss: 4.70952455e-06
Iter: 875 loss: 4.71963e-06
Iter: 876 loss: 4.70934629e-06
Iter: 877 loss: 4.70842588e-06
Iter: 878 loss: 4.70955456e-06
Iter: 879 loss: 4.70797659e-06
Iter: 880 loss: 4.70660962e-06
Iter: 881 loss: 4.70885152e-06
Iter: 882 loss: 4.70604391e-06
Iter: 883 loss: 4.70497343e-06
Iter: 884 loss: 4.70804616e-06
Iter: 885 loss: 4.70474788e-06
Iter: 886 loss: 4.70366831e-06
Iter: 887 loss: 4.70458235e-06
Iter: 888 loss: 4.70299938e-06
Iter: 889 loss: 4.70142641e-06
Iter: 890 loss: 4.7047306e-06
Iter: 891 loss: 4.70086798e-06
Iter: 892 loss: 4.69955e-06
Iter: 893 loss: 4.69909355e-06
Iter: 894 loss: 4.6984369e-06
Iter: 895 loss: 4.6974892e-06
Iter: 896 loss: 4.69740098e-06
Iter: 897 loss: 4.69654424e-06
Iter: 898 loss: 4.69485394e-06
Iter: 899 loss: 4.7198655e-06
Iter: 900 loss: 4.69479437e-06
Iter: 901 loss: 4.69293718e-06
Iter: 902 loss: 4.69523729e-06
Iter: 903 loss: 4.69213273e-06
Iter: 904 loss: 4.69015777e-06
Iter: 905 loss: 4.70169743e-06
Iter: 906 loss: 4.69007682e-06
Iter: 907 loss: 4.68846702e-06
Iter: 908 loss: 4.7040362e-06
Iter: 909 loss: 4.68831786e-06
Iter: 910 loss: 4.68755e-06
Iter: 911 loss: 4.68822e-06
Iter: 912 loss: 4.68700364e-06
Iter: 913 loss: 4.68565213e-06
Iter: 914 loss: 4.69106635e-06
Iter: 915 loss: 4.68529561e-06
Iter: 916 loss: 4.6845289e-06
Iter: 917 loss: 4.68704866e-06
Iter: 918 loss: 4.6843752e-06
Iter: 919 loss: 4.68331973e-06
Iter: 920 loss: 4.68218741e-06
Iter: 921 loss: 4.68194503e-06
Iter: 922 loss: 4.68077815e-06
Iter: 923 loss: 4.68451572e-06
Iter: 924 loss: 4.68062e-06
Iter: 925 loss: 4.67893551e-06
Iter: 926 loss: 4.680453e-06
Iter: 927 loss: 4.67799509e-06
Iter: 928 loss: 4.67677728e-06
Iter: 929 loss: 4.67719292e-06
Iter: 930 loss: 4.67590507e-06
Iter: 931 loss: 4.67435711e-06
Iter: 932 loss: 4.69340057e-06
Iter: 933 loss: 4.67437621e-06
Iter: 934 loss: 4.67345444e-06
Iter: 935 loss: 4.6767409e-06
Iter: 936 loss: 4.67303926e-06
Iter: 937 loss: 4.67213204e-06
Iter: 938 loss: 4.67107475e-06
Iter: 939 loss: 4.67089285e-06
Iter: 940 loss: 4.66951269e-06
Iter: 941 loss: 4.67032169e-06
Iter: 942 loss: 4.66862684e-06
Iter: 943 loss: 4.66723486e-06
Iter: 944 loss: 4.6860996e-06
Iter: 945 loss: 4.66723941e-06
Iter: 946 loss: 4.66605934e-06
Iter: 947 loss: 4.67001246e-06
Iter: 948 loss: 4.66563233e-06
Iter: 949 loss: 4.66442862e-06
Iter: 950 loss: 4.67124482e-06
Iter: 951 loss: 4.66452275e-06
Iter: 952 loss: 4.66346319e-06
Iter: 953 loss: 4.6627847e-06
Iter: 954 loss: 4.66238816e-06
Iter: 955 loss: 4.66155871e-06
Iter: 956 loss: 4.66150914e-06
Iter: 957 loss: 4.66078563e-06
Iter: 958 loss: 4.65931589e-06
Iter: 959 loss: 4.67358541e-06
Iter: 960 loss: 4.65906442e-06
Iter: 961 loss: 4.65808262e-06
Iter: 962 loss: 4.65804169e-06
Iter: 963 loss: 4.65691892e-06
Iter: 964 loss: 4.65580843e-06
Iter: 965 loss: 4.6556188e-06
Iter: 966 loss: 4.65414632e-06
Iter: 967 loss: 4.65629e-06
Iter: 968 loss: 4.65348194e-06
Iter: 969 loss: 4.65192e-06
Iter: 970 loss: 4.6756345e-06
Iter: 971 loss: 4.65184257e-06
Iter: 972 loss: 4.65119911e-06
Iter: 973 loss: 4.64946e-06
Iter: 974 loss: 4.66998154e-06
Iter: 975 loss: 4.64934101e-06
Iter: 976 loss: 4.64774712e-06
Iter: 977 loss: 4.6605237e-06
Iter: 978 loss: 4.64755249e-06
Iter: 979 loss: 4.64620507e-06
Iter: 980 loss: 4.64769755e-06
Iter: 981 loss: 4.64535151e-06
Iter: 982 loss: 4.64445702e-06
Iter: 983 loss: 4.64442928e-06
Iter: 984 loss: 4.64354707e-06
Iter: 985 loss: 4.64464847e-06
Iter: 986 loss: 4.64303503e-06
Iter: 987 loss: 4.6419309e-06
Iter: 988 loss: 4.64597e-06
Iter: 989 loss: 4.64169352e-06
Iter: 990 loss: 4.64095592e-06
Iter: 991 loss: 4.64215191e-06
Iter: 992 loss: 4.64065943e-06
Iter: 993 loss: 4.63934794e-06
Iter: 994 loss: 4.6390287e-06
Iter: 995 loss: 4.63822062e-06
Iter: 996 loss: 4.63700371e-06
Iter: 997 loss: 4.63631659e-06
Iter: 998 loss: 4.63581819e-06
Iter: 999 loss: 4.63467268e-06
Iter: 1000 loss: 4.63441847e-06
Iter: 1001 loss: 4.63375636e-06
Iter: 1002 loss: 4.63181368e-06
Iter: 1003 loss: 4.64952109e-06
Iter: 1004 loss: 4.63158176e-06
Iter: 1005 loss: 4.63028e-06
Iter: 1006 loss: 4.6303503e-06
Iter: 1007 loss: 4.62885964e-06
Iter: 1008 loss: 4.63209562e-06
Iter: 1009 loss: 4.62825938e-06
Iter: 1010 loss: 4.62731532e-06
Iter: 1011 loss: 4.62584239e-06
Iter: 1012 loss: 4.62577918e-06
Iter: 1013 loss: 4.62404114e-06
Iter: 1014 loss: 4.62914886e-06
Iter: 1015 loss: 4.62355183e-06
Iter: 1016 loss: 4.6225573e-06
Iter: 1017 loss: 4.6224759e-06
Iter: 1018 loss: 4.62141179e-06
Iter: 1019 loss: 4.6227442e-06
Iter: 1020 loss: 4.62092339e-06
Iter: 1021 loss: 4.62007483e-06
Iter: 1022 loss: 4.62767912e-06
Iter: 1023 loss: 4.61994205e-06
Iter: 1024 loss: 4.61913487e-06
Iter: 1025 loss: 4.61833679e-06
Iter: 1026 loss: 4.61828404e-06
Iter: 1027 loss: 4.6168434e-06
Iter: 1028 loss: 4.62821026e-06
Iter: 1029 loss: 4.61689069e-06
Iter: 1030 loss: 4.61610125e-06
Iter: 1031 loss: 4.61438276e-06
Iter: 1032 loss: 4.64074037e-06
Iter: 1033 loss: 4.61424315e-06
Iter: 1034 loss: 4.61281525e-06
Iter: 1035 loss: 4.62487151e-06
Iter: 1036 loss: 4.61270474e-06
Iter: 1037 loss: 4.61109357e-06
Iter: 1038 loss: 4.61855143e-06
Iter: 1039 loss: 4.61068521e-06
Iter: 1040 loss: 4.6098171e-06
Iter: 1041 loss: 4.60785031e-06
Iter: 1042 loss: 4.64599e-06
Iter: 1043 loss: 4.60793217e-06
Iter: 1044 loss: 4.60803358e-06
Iter: 1045 loss: 4.60704268e-06
Iter: 1046 loss: 4.60631327e-06
Iter: 1047 loss: 4.60494903e-06
Iter: 1048 loss: 4.62922571e-06
Iter: 1049 loss: 4.60485671e-06
Iter: 1050 loss: 4.60358933e-06
Iter: 1051 loss: 4.62178514e-06
Iter: 1052 loss: 4.6037203e-06
Iter: 1053 loss: 4.6027053e-06
Iter: 1054 loss: 4.60380443e-06
Iter: 1055 loss: 4.60214324e-06
Iter: 1056 loss: 4.60084948e-06
Iter: 1057 loss: 4.60172623e-06
Iter: 1058 loss: 4.59987041e-06
Iter: 1059 loss: 4.59861712e-06
Iter: 1060 loss: 4.60712863e-06
Iter: 1061 loss: 4.59832881e-06
Iter: 1062 loss: 4.59725106e-06
Iter: 1063 loss: 4.59688499e-06
Iter: 1064 loss: 4.59618423e-06
Iter: 1065 loss: 4.59415969e-06
Iter: 1066 loss: 4.60311776e-06
Iter: 1067 loss: 4.5937104e-06
Iter: 1068 loss: 4.59246212e-06
Iter: 1069 loss: 4.59031844e-06
Iter: 1070 loss: 4.590348e-06
Iter: 1071 loss: 4.58842942e-06
Iter: 1072 loss: 4.61416585e-06
Iter: 1073 loss: 4.58842351e-06
Iter: 1074 loss: 4.58690056e-06
Iter: 1075 loss: 4.59600324e-06
Iter: 1076 loss: 4.58672866e-06
Iter: 1077 loss: 4.58564773e-06
Iter: 1078 loss: 4.58409249e-06
Iter: 1079 loss: 4.58397608e-06
Iter: 1080 loss: 4.58289742e-06
Iter: 1081 loss: 4.58288559e-06
Iter: 1082 loss: 4.58156774e-06
Iter: 1083 loss: 4.58092791e-06
Iter: 1084 loss: 4.5804436e-06
Iter: 1085 loss: 4.57912211e-06
Iter: 1086 loss: 4.57933947e-06
Iter: 1087 loss: 4.57819078e-06
Iter: 1088 loss: 4.57704664e-06
Iter: 1089 loss: 4.5769757e-06
Iter: 1090 loss: 4.57620536e-06
Iter: 1091 loss: 4.57869237e-06
Iter: 1092 loss: 4.57600527e-06
Iter: 1093 loss: 4.57516217e-06
Iter: 1094 loss: 4.57503302e-06
Iter: 1095 loss: 4.57442729e-06
Iter: 1096 loss: 4.57312308e-06
Iter: 1097 loss: 4.57869646e-06
Iter: 1098 loss: 4.5728857e-06
Iter: 1099 loss: 4.57187707e-06
Iter: 1100 loss: 4.57343276e-06
Iter: 1101 loss: 4.57157967e-06
Iter: 1102 loss: 4.57025317e-06
Iter: 1103 loss: 4.57106398e-06
Iter: 1104 loss: 4.56941598e-06
Iter: 1105 loss: 4.5682873e-06
Iter: 1106 loss: 4.56781845e-06
Iter: 1107 loss: 4.56717e-06
Iter: 1108 loss: 4.56598127e-06
Iter: 1109 loss: 4.56593762e-06
Iter: 1110 loss: 4.56481757e-06
Iter: 1111 loss: 4.56489806e-06
Iter: 1112 loss: 4.56375892e-06
Iter: 1113 loss: 4.56253883e-06
Iter: 1114 loss: 4.5628276e-06
Iter: 1115 loss: 4.56162934e-06
Iter: 1116 loss: 4.56065209e-06
Iter: 1117 loss: 4.5606389e-06
Iter: 1118 loss: 4.5596189e-06
Iter: 1119 loss: 4.55793815e-06
Iter: 1120 loss: 4.58727845e-06
Iter: 1121 loss: 4.55793452e-06
Iter: 1122 loss: 4.55643203e-06
Iter: 1123 loss: 4.56924317e-06
Iter: 1124 loss: 4.55649388e-06
Iter: 1125 loss: 4.55510326e-06
Iter: 1126 loss: 4.56140788e-06
Iter: 1127 loss: 4.55483041e-06
Iter: 1128 loss: 4.55391091e-06
Iter: 1129 loss: 4.55968075e-06
Iter: 1130 loss: 4.55387089e-06
Iter: 1131 loss: 4.55295594e-06
Iter: 1132 loss: 4.55318059e-06
Iter: 1133 loss: 4.55235568e-06
Iter: 1134 loss: 4.55122881e-06
Iter: 1135 loss: 4.55408917e-06
Iter: 1136 loss: 4.55087502e-06
Iter: 1137 loss: 4.54983729e-06
Iter: 1138 loss: 4.55438203e-06
Iter: 1139 loss: 4.54974e-06
Iter: 1140 loss: 4.5488332e-06
Iter: 1141 loss: 4.54751125e-06
Iter: 1142 loss: 4.54758356e-06
Iter: 1143 loss: 4.54625024e-06
Iter: 1144 loss: 4.55109875e-06
Iter: 1145 loss: 4.54595829e-06
Iter: 1146 loss: 4.54495057e-06
Iter: 1147 loss: 4.54488054e-06
Iter: 1148 loss: 4.54411111e-06
Iter: 1149 loss: 4.54318433e-06
Iter: 1150 loss: 4.54311248e-06
Iter: 1151 loss: 4.54210885e-06
Iter: 1152 loss: 4.54839937e-06
Iter: 1153 loss: 4.54197971e-06
Iter: 1154 loss: 4.54101337e-06
Iter: 1155 loss: 4.5495467e-06
Iter: 1156 loss: 4.54098517e-06
Iter: 1157 loss: 4.54044766e-06
Iter: 1158 loss: 4.53923531e-06
Iter: 1159 loss: 4.55818918e-06
Iter: 1160 loss: 4.5392726e-06
Iter: 1161 loss: 4.53853863e-06
Iter: 1162 loss: 4.53854682e-06
Iter: 1163 loss: 4.5378747e-06
Iter: 1164 loss: 4.53853772e-06
Iter: 1165 loss: 4.53756502e-06
Iter: 1166 loss: 4.53669e-06
Iter: 1167 loss: 4.54104065e-06
Iter: 1168 loss: 4.53660687e-06
Iter: 1169 loss: 4.53590928e-06
Iter: 1170 loss: 4.53698658e-06
Iter: 1171 loss: 4.53557277e-06
Iter: 1172 loss: 4.53487337e-06
Iter: 1173 loss: 4.53530811e-06
Iter: 1174 loss: 4.53431494e-06
Iter: 1175 loss: 4.53348093e-06
Iter: 1176 loss: 4.53846405e-06
Iter: 1177 loss: 4.53322446e-06
Iter: 1178 loss: 4.53251914e-06
Iter: 1179 loss: 4.53156918e-06
Iter: 1180 loss: 4.53154735e-06
Iter: 1181 loss: 4.53068242e-06
Iter: 1182 loss: 4.54387782e-06
Iter: 1183 loss: 4.53058601e-06
Iter: 1184 loss: 4.52961e-06
Iter: 1185 loss: 4.5309389e-06
Iter: 1186 loss: 4.52916356e-06
Iter: 1187 loss: 4.5282527e-06
Iter: 1188 loss: 4.52836366e-06
Iter: 1189 loss: 4.52749737e-06
Iter: 1190 loss: 4.52668246e-06
Iter: 1191 loss: 4.52674658e-06
Iter: 1192 loss: 4.52601307e-06
Iter: 1193 loss: 4.52588938e-06
Iter: 1194 loss: 4.52523818e-06
Iter: 1195 loss: 4.52445283e-06
Iter: 1196 loss: 4.52546647e-06
Iter: 1197 loss: 4.52402355e-06
Iter: 1198 loss: 4.52309268e-06
Iter: 1199 loss: 4.53214943e-06
Iter: 1200 loss: 4.52294353e-06
Iter: 1201 loss: 4.52233871e-06
Iter: 1202 loss: 4.52534e-06
Iter: 1203 loss: 4.52218046e-06
Iter: 1204 loss: 4.52161657e-06
Iter: 1205 loss: 4.52222093e-06
Iter: 1206 loss: 4.52116092e-06
Iter: 1207 loss: 4.52051063e-06
Iter: 1208 loss: 4.52247241e-06
Iter: 1209 loss: 4.52028235e-06
Iter: 1210 loss: 4.51975939e-06
Iter: 1211 loss: 4.52167751e-06
Iter: 1212 loss: 4.51944288e-06
Iter: 1213 loss: 4.51887354e-06
Iter: 1214 loss: 4.51797223e-06
Iter: 1215 loss: 4.5178931e-06
Iter: 1216 loss: 4.51681854e-06
Iter: 1217 loss: 4.51779124e-06
Iter: 1218 loss: 4.51621963e-06
Iter: 1219 loss: 4.51539836e-06
Iter: 1220 loss: 4.51528467e-06
Iter: 1221 loss: 4.51474716e-06
Iter: 1222 loss: 4.51380674e-06
Iter: 1223 loss: 4.51382948e-06
Iter: 1224 loss: 4.51281448e-06
Iter: 1225 loss: 4.51762e-06
Iter: 1226 loss: 4.51261894e-06
Iter: 1227 loss: 4.51171309e-06
Iter: 1228 loss: 4.52365293e-06
Iter: 1229 loss: 4.51171763e-06
Iter: 1230 loss: 4.51126971e-06
Iter: 1231 loss: 4.51029609e-06
Iter: 1232 loss: 4.52803306e-06
Iter: 1233 loss: 4.51031246e-06
Iter: 1234 loss: 4.50949938e-06
Iter: 1235 loss: 4.50952894e-06
Iter: 1236 loss: 4.50880634e-06
Iter: 1237 loss: 4.50954849e-06
Iter: 1238 loss: 4.50841344e-06
Iter: 1239 loss: 4.50756852e-06
Iter: 1240 loss: 4.51341884e-06
Iter: 1241 loss: 4.50743255e-06
Iter: 1242 loss: 4.50688231e-06
Iter: 1243 loss: 4.50772313e-06
Iter: 1244 loss: 4.50666539e-06
Iter: 1245 loss: 4.50595871e-06
Iter: 1246 loss: 4.50625566e-06
Iter: 1247 loss: 4.50541575e-06
Iter: 1248 loss: 4.50462676e-06
Iter: 1249 loss: 4.50924881e-06
Iter: 1250 loss: 4.50439347e-06
Iter: 1251 loss: 4.50379775e-06
Iter: 1252 loss: 4.50255948e-06
Iter: 1253 loss: 4.52760378e-06
Iter: 1254 loss: 4.5026618e-06
Iter: 1255 loss: 4.50176412e-06
Iter: 1256 loss: 4.50176049e-06
Iter: 1257 loss: 4.50076914e-06
Iter: 1258 loss: 4.5009092e-06
Iter: 1259 loss: 4.50017751e-06
Iter: 1260 loss: 4.49916024e-06
Iter: 1261 loss: 4.50010975e-06
Iter: 1262 loss: 4.49861e-06
Iter: 1263 loss: 4.49803201e-06
Iter: 1264 loss: 4.49794879e-06
Iter: 1265 loss: 4.49737354e-06
Iter: 1266 loss: 4.49664185e-06
Iter: 1267 loss: 4.49654863e-06
Iter: 1268 loss: 4.49584e-06
Iter: 1269 loss: 4.49810796e-06
Iter: 1270 loss: 4.49549498e-06
Iter: 1271 loss: 4.49458639e-06
Iter: 1272 loss: 4.50068364e-06
Iter: 1273 loss: 4.49445406e-06
Iter: 1274 loss: 4.49380968e-06
Iter: 1275 loss: 4.49659092e-06
Iter: 1276 loss: 4.49369418e-06
Iter: 1277 loss: 4.4929975e-06
Iter: 1278 loss: 4.49380559e-06
Iter: 1279 loss: 4.49258e-06
Iter: 1280 loss: 4.49181698e-06
Iter: 1281 loss: 4.49360687e-06
Iter: 1282 loss: 4.49150548e-06
Iter: 1283 loss: 4.49093204e-06
Iter: 1284 loss: 4.49341769e-06
Iter: 1285 loss: 4.49068648e-06
Iter: 1286 loss: 4.49002073e-06
Iter: 1287 loss: 4.48892206e-06
Iter: 1288 loss: 4.48883475e-06
Iter: 1289 loss: 4.48771925e-06
Iter: 1290 loss: 4.49073195e-06
Iter: 1291 loss: 4.48737865e-06
Iter: 1292 loss: 4.48648689e-06
Iter: 1293 loss: 4.48647825e-06
Iter: 1294 loss: 4.4858607e-06
Iter: 1295 loss: 4.48491164e-06
Iter: 1296 loss: 4.48490482e-06
Iter: 1297 loss: 4.48392666e-06
Iter: 1298 loss: 4.49267282e-06
Iter: 1299 loss: 4.48393257e-06
Iter: 1300 loss: 4.48290757e-06
Iter: 1301 loss: 4.48933315e-06
Iter: 1302 loss: 4.48289029e-06
Iter: 1303 loss: 4.48245555e-06
Iter: 1304 loss: 4.48153105e-06
Iter: 1305 loss: 4.4814733e-06
Iter: 1306 loss: 4.48075116e-06
Iter: 1307 loss: 4.4912631e-06
Iter: 1308 loss: 4.48070386e-06
Iter: 1309 loss: 4.48001083e-06
Iter: 1310 loss: 4.48106e-06
Iter: 1311 loss: 4.47972343e-06
Iter: 1312 loss: 4.47898674e-06
Iter: 1313 loss: 4.48344326e-06
Iter: 1314 loss: 4.47883804e-06
Iter: 1315 loss: 4.47823732e-06
Iter: 1316 loss: 4.48005e-06
Iter: 1317 loss: 4.47812909e-06
Iter: 1318 loss: 4.47740513e-06
Iter: 1319 loss: 4.47777302e-06
Iter: 1320 loss: 4.47711091e-06
Iter: 1321 loss: 4.47624643e-06
Iter: 1322 loss: 4.48195669e-06
Iter: 1323 loss: 4.47626098e-06
Iter: 1324 loss: 4.47578805e-06
Iter: 1325 loss: 4.47468301e-06
Iter: 1326 loss: 4.49018444e-06
Iter: 1327 loss: 4.47468074e-06
Iter: 1328 loss: 4.47390721e-06
Iter: 1329 loss: 4.47385219e-06
Iter: 1330 loss: 4.47316734e-06
Iter: 1331 loss: 4.47276034e-06
Iter: 1332 loss: 4.47234879e-06
Iter: 1333 loss: 4.47147931e-06
Iter: 1334 loss: 4.47156299e-06
Iter: 1335 loss: 4.47078037e-06
Iter: 1336 loss: 4.46995e-06
Iter: 1337 loss: 4.47000184e-06
Iter: 1338 loss: 4.46918284e-06
Iter: 1339 loss: 4.46941112e-06
Iter: 1340 loss: 4.46858576e-06
Iter: 1341 loss: 4.46793183e-06
Iter: 1342 loss: 4.46869126e-06
Iter: 1343 loss: 4.46768126e-06
Iter: 1344 loss: 4.46679906e-06
Iter: 1345 loss: 4.47227876e-06
Iter: 1346 loss: 4.46667491e-06
Iter: 1347 loss: 4.46594731e-06
Iter: 1348 loss: 4.46846479e-06
Iter: 1349 loss: 4.46580725e-06
Iter: 1350 loss: 4.46509603e-06
Iter: 1351 loss: 4.46621e-06
Iter: 1352 loss: 4.46476e-06
Iter: 1353 loss: 4.46394688e-06
Iter: 1354 loss: 4.46613831e-06
Iter: 1355 loss: 4.46372542e-06
Iter: 1356 loss: 4.4629578e-06
Iter: 1357 loss: 4.46684771e-06
Iter: 1358 loss: 4.46284184e-06
Iter: 1359 loss: 4.46217655e-06
Iter: 1360 loss: 4.46104514e-06
Iter: 1361 loss: 4.46099148e-06
Iter: 1362 loss: 4.4597291e-06
Iter: 1363 loss: 4.46405829e-06
Iter: 1364 loss: 4.45954174e-06
Iter: 1365 loss: 4.45842215e-06
Iter: 1366 loss: 4.46758804e-06
Iter: 1367 loss: 4.45831574e-06
Iter: 1368 loss: 4.45754858e-06
Iter: 1369 loss: 4.45658952e-06
Iter: 1370 loss: 4.4565445e-06
Iter: 1371 loss: 4.45552905e-06
Iter: 1372 loss: 4.46575359e-06
Iter: 1373 loss: 4.45540718e-06
Iter: 1374 loss: 4.45440446e-06
Iter: 1375 loss: 4.45988826e-06
Iter: 1376 loss: 4.45426849e-06
Iter: 1377 loss: 4.45352907e-06
Iter: 1378 loss: 4.45292426e-06
Iter: 1379 loss: 4.45275145e-06
Iter: 1380 loss: 4.45176283e-06
Iter: 1381 loss: 4.46345348e-06
Iter: 1382 loss: 4.45180649e-06
Iter: 1383 loss: 4.45100932e-06
Iter: 1384 loss: 4.45242677e-06
Iter: 1385 loss: 4.45078331e-06
Iter: 1386 loss: 4.44976558e-06
Iter: 1387 loss: 4.45336627e-06
Iter: 1388 loss: 4.44963098e-06
Iter: 1389 loss: 4.44885654e-06
Iter: 1390 loss: 4.45122896e-06
Iter: 1391 loss: 4.4487515e-06
Iter: 1392 loss: 4.44803754e-06
Iter: 1393 loss: 4.44920943e-06
Iter: 1394 loss: 4.44774378e-06
Iter: 1395 loss: 4.4468261e-06
Iter: 1396 loss: 4.44774832e-06
Iter: 1397 loss: 4.44632042e-06
Iter: 1398 loss: 4.44548687e-06
Iter: 1399 loss: 4.44458328e-06
Iter: 1400 loss: 4.44449415e-06
Iter: 1401 loss: 4.44399302e-06
Iter: 1402 loss: 4.44371881e-06
Iter: 1403 loss: 4.44324542e-06
Iter: 1404 loss: 4.44227862e-06
Iter: 1405 loss: 4.46453e-06
Iter: 1406 loss: 4.44212401e-06
Iter: 1407 loss: 4.44125271e-06
Iter: 1408 loss: 4.44602574e-06
Iter: 1409 loss: 4.44109901e-06
Iter: 1410 loss: 4.44034958e-06
Iter: 1411 loss: 4.44030729e-06
Iter: 1412 loss: 4.43992576e-06
Iter: 1413 loss: 4.43892577e-06
Iter: 1414 loss: 4.45895e-06
Iter: 1415 loss: 4.43888e-06
Iter: 1416 loss: 4.43789213e-06
Iter: 1417 loss: 4.44658599e-06
Iter: 1418 loss: 4.43786439e-06
Iter: 1419 loss: 4.43695353e-06
Iter: 1420 loss: 4.4395083e-06
Iter: 1421 loss: 4.43674116e-06
Iter: 1422 loss: 4.43595582e-06
Iter: 1423 loss: 4.44018178e-06
Iter: 1424 loss: 4.43584258e-06
Iter: 1425 loss: 4.43516092e-06
Iter: 1426 loss: 4.43663566e-06
Iter: 1427 loss: 4.43491535e-06
Iter: 1428 loss: 4.43397857e-06
Iter: 1429 loss: 4.43467798e-06
Iter: 1430 loss: 4.43345334e-06
Iter: 1431 loss: 4.43246427e-06
Iter: 1432 loss: 4.43912904e-06
Iter: 1433 loss: 4.43238787e-06
Iter: 1434 loss: 4.4317726e-06
Iter: 1435 loss: 4.43051294e-06
Iter: 1436 loss: 4.4516114e-06
Iter: 1437 loss: 4.43039562e-06
Iter: 1438 loss: 4.42945611e-06
Iter: 1439 loss: 4.42934606e-06
Iter: 1440 loss: 4.42841701e-06
Iter: 1441 loss: 4.42930832e-06
Iter: 1442 loss: 4.42787086e-06
Iter: 1443 loss: 4.42690043e-06
Iter: 1444 loss: 4.42657938e-06
Iter: 1445 loss: 4.42614146e-06
Iter: 1446 loss: 4.42507962e-06
Iter: 1447 loss: 4.44136276e-06
Iter: 1448 loss: 4.42502824e-06
Iter: 1449 loss: 4.42411192e-06
Iter: 1450 loss: 4.42590954e-06
Iter: 1451 loss: 4.42368537e-06
Iter: 1452 loss: 4.42298187e-06
Iter: 1453 loss: 4.42341616e-06
Iter: 1454 loss: 4.42245482e-06
Iter: 1455 loss: 4.4215235e-06
Iter: 1456 loss: 4.42799319e-06
Iter: 1457 loss: 4.42149485e-06
Iter: 1458 loss: 4.4208241e-06
Iter: 1459 loss: 4.42287865e-06
Iter: 1460 loss: 4.42062947e-06
Iter: 1461 loss: 4.41988504e-06
Iter: 1462 loss: 4.42170904e-06
Iter: 1463 loss: 4.4196513e-06
Iter: 1464 loss: 4.41883822e-06
Iter: 1465 loss: 4.42158489e-06
Iter: 1466 loss: 4.41860857e-06
Iter: 1467 loss: 4.41797692e-06
Iter: 1468 loss: 4.4196363e-06
Iter: 1469 loss: 4.4177873e-06
Iter: 1470 loss: 4.41684688e-06
Iter: 1471 loss: 4.41649081e-06
Iter: 1472 loss: 4.41602333e-06
Iter: 1473 loss: 4.41507837e-06
Iter: 1474 loss: 4.41505108e-06
Iter: 1475 loss: 4.4143394e-06
Iter: 1476 loss: 4.41371094e-06
Iter: 1477 loss: 4.41369593e-06
Iter: 1478 loss: 4.41310522e-06
Iter: 1479 loss: 4.41189968e-06
Iter: 1480 loss: 4.43399767e-06
Iter: 1481 loss: 4.4118915e-06
Iter: 1482 loss: 4.41076736e-06
Iter: 1483 loss: 4.41601333e-06
Iter: 1484 loss: 4.41054726e-06
Iter: 1485 loss: 4.40978374e-06
Iter: 1486 loss: 4.40984513e-06
Iter: 1487 loss: 4.40930125e-06
Iter: 1488 loss: 4.40839221e-06
Iter: 1489 loss: 4.40837039e-06
Iter: 1490 loss: 4.40743861e-06
Iter: 1491 loss: 4.41478505e-06
Iter: 1492 loss: 4.40745589e-06
Iter: 1493 loss: 4.40671465e-06
Iter: 1494 loss: 4.40799977e-06
Iter: 1495 loss: 4.40620715e-06
Iter: 1496 loss: 4.40547865e-06
Iter: 1497 loss: 4.40962231e-06
Iter: 1498 loss: 4.40527e-06
Iter: 1499 loss: 4.40464373e-06
Iter: 1500 loss: 4.40764734e-06
Iter: 1501 loss: 4.40446911e-06
Iter: 1502 loss: 4.40362692e-06
Iter: 1503 loss: 4.40379836e-06
Iter: 1504 loss: 4.40321583e-06
Iter: 1505 loss: 4.40231634e-06
Iter: 1506 loss: 4.41109114e-06
Iter: 1507 loss: 4.40216445e-06
Iter: 1508 loss: 4.40173199e-06
Iter: 1509 loss: 4.4004737e-06
Iter: 1510 loss: 4.41381599e-06
Iter: 1511 loss: 4.40037684e-06
Iter: 1512 loss: 4.3996215e-06
Iter: 1513 loss: 4.39952237e-06
Iter: 1514 loss: 4.39867e-06
Iter: 1515 loss: 4.39822406e-06
Iter: 1516 loss: 4.3977966e-06
Iter: 1517 loss: 4.39681298e-06
Iter: 1518 loss: 4.39805672e-06
Iter: 1519 loss: 4.39622727e-06
Iter: 1520 loss: 4.39520591e-06
Iter: 1521 loss: 4.39518226e-06
Iter: 1522 loss: 4.39448559e-06
Iter: 1523 loss: 4.39387532e-06
Iter: 1524 loss: 4.39369796e-06
Iter: 1525 loss: 4.39242831e-06
Iter: 1526 loss: 4.39788118e-06
Iter: 1527 loss: 4.39229962e-06
Iter: 1528 loss: 4.39131964e-06
Iter: 1529 loss: 4.39383e-06
Iter: 1530 loss: 4.39090354e-06
Iter: 1531 loss: 4.38990719e-06
Iter: 1532 loss: 4.39578253e-06
Iter: 1533 loss: 4.38977895e-06
Iter: 1534 loss: 4.38913503e-06
Iter: 1535 loss: 4.39136693e-06
Iter: 1536 loss: 4.38881307e-06
Iter: 1537 loss: 4.3878822e-06
Iter: 1538 loss: 4.38833877e-06
Iter: 1539 loss: 4.38741154e-06
Iter: 1540 loss: 4.38624829e-06
Iter: 1541 loss: 4.39395353e-06
Iter: 1542 loss: 4.3863115e-06
Iter: 1543 loss: 4.38541792e-06
Iter: 1544 loss: 4.38381221e-06
Iter: 1545 loss: 4.40609392e-06
Iter: 1546 loss: 4.38357165e-06
Iter: 1547 loss: 4.38257939e-06
Iter: 1548 loss: 4.38262396e-06
Iter: 1549 loss: 4.38134248e-06
Iter: 1550 loss: 4.38145435e-06
Iter: 1551 loss: 4.38057396e-06
Iter: 1552 loss: 4.37943936e-06
Iter: 1553 loss: 4.37987273e-06
Iter: 1554 loss: 4.37863719e-06
Iter: 1555 loss: 4.37778635e-06
Iter: 1556 loss: 4.37778453e-06
Iter: 1557 loss: 4.37683e-06
Iter: 1558 loss: 4.37680546e-06
Iter: 1559 loss: 4.37620383e-06
Iter: 1560 loss: 4.37520248e-06
Iter: 1561 loss: 4.37719154e-06
Iter: 1562 loss: 4.37467179e-06
Iter: 1563 loss: 4.37354311e-06
Iter: 1564 loss: 4.37853123e-06
Iter: 1565 loss: 4.37321842e-06
Iter: 1566 loss: 4.37220478e-06
Iter: 1567 loss: 4.37764311e-06
Iter: 1568 loss: 4.37207473e-06
Iter: 1569 loss: 4.3713e-06
Iter: 1570 loss: 4.37357721e-06
Iter: 1571 loss: 4.37119206e-06
Iter: 1572 loss: 4.37025392e-06
Iter: 1573 loss: 4.37097469e-06
Iter: 1574 loss: 4.36977916e-06
Iter: 1575 loss: 4.36863229e-06
Iter: 1576 loss: 4.37465269e-06
Iter: 1577 loss: 4.36861092e-06
Iter: 1578 loss: 4.36789469e-06
Iter: 1579 loss: 4.3660084e-06
Iter: 1580 loss: 4.39229916e-06
Iter: 1581 loss: 4.36596383e-06
Iter: 1582 loss: 4.36480559e-06
Iter: 1583 loss: 4.38024881e-06
Iter: 1584 loss: 4.36480695e-06
Iter: 1585 loss: 4.36368418e-06
Iter: 1586 loss: 4.36714481e-06
Iter: 1587 loss: 4.36334e-06
Iter: 1588 loss: 4.36242499e-06
Iter: 1589 loss: 4.36105893e-06
Iter: 1590 loss: 4.36094888e-06
Iter: 1591 loss: 4.35997e-06
Iter: 1592 loss: 4.35995344e-06
Iter: 1593 loss: 4.35874517e-06
Iter: 1594 loss: 4.35961647e-06
Iter: 1595 loss: 4.35800075e-06
Iter: 1596 loss: 4.35686889e-06
Iter: 1597 loss: 4.3574114e-06
Iter: 1598 loss: 4.35620859e-06
Iter: 1599 loss: 4.35459242e-06
Iter: 1600 loss: 4.36635582e-06
Iter: 1601 loss: 4.35449601e-06
Iter: 1602 loss: 4.35351421e-06
Iter: 1603 loss: 4.35765651e-06
Iter: 1604 loss: 4.35330821e-06
Iter: 1605 loss: 4.35227503e-06
Iter: 1606 loss: 4.35420816e-06
Iter: 1607 loss: 4.35178754e-06
Iter: 1608 loss: 4.3507e-06
Iter: 1609 loss: 4.35323091e-06
Iter: 1610 loss: 4.35024731e-06
Iter: 1611 loss: 4.34931e-06
Iter: 1612 loss: 4.35529273e-06
Iter: 1613 loss: 4.34913682e-06
Iter: 1614 loss: 4.34825461e-06
Iter: 1615 loss: 4.34631511e-06
Iter: 1616 loss: 4.38266215e-06
Iter: 1617 loss: 4.34625144e-06
Iter: 1618 loss: 4.34464164e-06
Iter: 1619 loss: 4.35070569e-06
Iter: 1620 loss: 4.34425328e-06
Iter: 1621 loss: 4.34280309e-06
Iter: 1622 loss: 4.36315941e-06
Iter: 1623 loss: 4.34280491e-06
Iter: 1624 loss: 4.34180856e-06
Iter: 1625 loss: 4.3400205e-06
Iter: 1626 loss: 4.37167319e-06
Iter: 1627 loss: 4.33984087e-06
Iter: 1628 loss: 4.33875357e-06
Iter: 1629 loss: 4.33879086e-06
Iter: 1630 loss: 4.33759942e-06
Iter: 1631 loss: 4.33969e-06
Iter: 1632 loss: 4.33711557e-06
Iter: 1633 loss: 4.33603464e-06
Iter: 1634 loss: 4.33544028e-06
Iter: 1635 loss: 4.33484183e-06
Iter: 1636 loss: 4.33312744e-06
Iter: 1637 loss: 4.34734739e-06
Iter: 1638 loss: 4.33307196e-06
Iter: 1639 loss: 4.33183777e-06
Iter: 1640 loss: 4.33579226e-06
Iter: 1641 loss: 4.33161676e-06
Iter: 1642 loss: 4.33026662e-06
Iter: 1643 loss: 4.33391779e-06
Iter: 1644 loss: 4.32995466e-06
Iter: 1645 loss: 4.32861361e-06
Iter: 1646 loss: 4.33137302e-06
Iter: 1647 loss: 4.32821798e-06
Iter: 1648 loss: 4.32703882e-06
Iter: 1649 loss: 4.33214655e-06
Iter: 1650 loss: 4.32675552e-06
Iter: 1651 loss: 4.32561728e-06
Iter: 1652 loss: 4.32373508e-06
Iter: 1653 loss: 4.32374691e-06
Iter: 1654 loss: 4.32205479e-06
Iter: 1655 loss: 4.3242876e-06
Iter: 1656 loss: 4.32120532e-06
Iter: 1657 loss: 4.31935359e-06
Iter: 1658 loss: 4.3192631e-06
Iter: 1659 loss: 4.31798799e-06
Iter: 1660 loss: 4.31556055e-06
Iter: 1661 loss: 4.36396067e-06
Iter: 1662 loss: 4.31554e-06
Iter: 1663 loss: 4.31395483e-06
Iter: 1664 loss: 4.31388798e-06
Iter: 1665 loss: 4.31236367e-06
Iter: 1666 loss: 4.3138989e-06
Iter: 1667 loss: 4.31139824e-06
Iter: 1668 loss: 4.30976161e-06
Iter: 1669 loss: 4.31127182e-06
Iter: 1670 loss: 4.30880391e-06
Iter: 1671 loss: 4.30686214e-06
Iter: 1672 loss: 4.32141496e-06
Iter: 1673 loss: 4.30669252e-06
Iter: 1674 loss: 4.30543832e-06
Iter: 1675 loss: 4.30966429e-06
Iter: 1676 loss: 4.30513046e-06
Iter: 1677 loss: 4.30361752e-06
Iter: 1678 loss: 4.30663704e-06
Iter: 1679 loss: 4.30303726e-06
Iter: 1680 loss: 4.30146974e-06
Iter: 1681 loss: 4.30476121e-06
Iter: 1682 loss: 4.30096134e-06
Iter: 1683 loss: 4.29945385e-06
Iter: 1684 loss: 4.30661476e-06
Iter: 1685 loss: 4.2992724e-06
Iter: 1686 loss: 4.29793135e-06
Iter: 1687 loss: 4.29651891e-06
Iter: 1688 loss: 4.29643387e-06
Iter: 1689 loss: 4.29456441e-06
Iter: 1690 loss: 4.2958518e-06
Iter: 1691 loss: 4.29342435e-06
Iter: 1692 loss: 4.29127385e-06
Iter: 1693 loss: 4.32377101e-06
Iter: 1694 loss: 4.29127385e-06
Iter: 1695 loss: 4.28999874e-06
Iter: 1696 loss: 4.2877432e-06
Iter: 1697 loss: 4.28770136e-06
Iter: 1698 loss: 4.28622207e-06
Iter: 1699 loss: 4.28607291e-06
Iter: 1700 loss: 4.28450903e-06
Iter: 1701 loss: 4.2845686e-06
Iter: 1702 loss: 4.28320345e-06
Iter: 1703 loss: 4.28158137e-06
Iter: 1704 loss: 4.28748353e-06
Iter: 1705 loss: 4.28106387e-06
Iter: 1706 loss: 4.2792517e-06
Iter: 1707 loss: 4.28652402e-06
Iter: 1708 loss: 4.27896475e-06
Iter: 1709 loss: 4.27744544e-06
Iter: 1710 loss: 4.2828724e-06
Iter: 1711 loss: 4.27692066e-06
Iter: 1712 loss: 4.27528266e-06
Iter: 1713 loss: 4.27762052e-06
Iter: 1714 loss: 4.27449686e-06
Iter: 1715 loss: 4.27261239e-06
Iter: 1716 loss: 4.27822488e-06
Iter: 1717 loss: 4.27206851e-06
Iter: 1718 loss: 4.27038412e-06
Iter: 1719 loss: 4.27621126e-06
Iter: 1720 loss: 4.26996576e-06
Iter: 1721 loss: 4.26837823e-06
Iter: 1722 loss: 4.26664565e-06
Iter: 1723 loss: 4.26649649e-06
Iter: 1724 loss: 4.26433508e-06
Iter: 1725 loss: 4.27167288e-06
Iter: 1726 loss: 4.26385168e-06
Iter: 1727 loss: 4.26139559e-06
Iter: 1728 loss: 4.27798614e-06
Iter: 1729 loss: 4.26108727e-06
Iter: 1730 loss: 4.25972485e-06
Iter: 1731 loss: 4.25716871e-06
Iter: 1732 loss: 4.25719645e-06
Iter: 1733 loss: 4.2558554e-06
Iter: 1734 loss: 4.25554708e-06
Iter: 1735 loss: 4.25398775e-06
Iter: 1736 loss: 4.25248345e-06
Iter: 1737 loss: 4.25217786e-06
Iter: 1738 loss: 4.25015196e-06
Iter: 1739 loss: 4.26296765e-06
Iter: 1740 loss: 4.24982545e-06
Iter: 1741 loss: 4.24769723e-06
Iter: 1742 loss: 4.2511524e-06
Iter: 1743 loss: 4.24673135e-06
Iter: 1744 loss: 4.24479231e-06
Iter: 1745 loss: 4.25927965e-06
Iter: 1746 loss: 4.24469454e-06
Iter: 1747 loss: 4.24299287e-06
Iter: 1748 loss: 4.24461041e-06
Iter: 1749 loss: 4.24190148e-06
Iter: 1750 loss: 4.2400161e-06
Iter: 1751 loss: 4.24666177e-06
Iter: 1752 loss: 4.23964184e-06
Iter: 1753 loss: 4.23788197e-06
Iter: 1754 loss: 4.24158679e-06
Iter: 1755 loss: 4.23730489e-06
Iter: 1756 loss: 4.23546498e-06
Iter: 1757 loss: 4.23368147e-06
Iter: 1758 loss: 4.23329129e-06
Iter: 1759 loss: 4.23080382e-06
Iter: 1760 loss: 4.2390775e-06
Iter: 1761 loss: 4.2300826e-06
Iter: 1762 loss: 4.22716266e-06
Iter: 1763 loss: 4.24881046e-06
Iter: 1764 loss: 4.22700259e-06
Iter: 1765 loss: 4.22518497e-06
Iter: 1766 loss: 4.22257153e-06
Iter: 1767 loss: 4.22250378e-06
Iter: 1768 loss: 4.22092353e-06
Iter: 1769 loss: 4.2205e-06
Iter: 1770 loss: 4.21888126e-06
Iter: 1771 loss: 4.21649611e-06
Iter: 1772 loss: 4.21632558e-06
Iter: 1773 loss: 4.21441791e-06
Iter: 1774 loss: 4.23969686e-06
Iter: 1775 loss: 4.2142824e-06
Iter: 1776 loss: 4.21246477e-06
Iter: 1777 loss: 4.21355207e-06
Iter: 1778 loss: 4.21110281e-06
Iter: 1779 loss: 4.20920514e-06
Iter: 1780 loss: 4.22841276e-06
Iter: 1781 loss: 4.20911647e-06
Iter: 1782 loss: 4.20747847e-06
Iter: 1783 loss: 4.2062693e-06
Iter: 1784 loss: 4.20575816e-06
Iter: 1785 loss: 4.20326387e-06
Iter: 1786 loss: 4.21540744e-06
Iter: 1787 loss: 4.20273591e-06
Iter: 1788 loss: 4.20059405e-06
Iter: 1789 loss: 4.20610149e-06
Iter: 1790 loss: 4.19972639e-06
Iter: 1791 loss: 4.19743628e-06
Iter: 1792 loss: 4.19481557e-06
Iter: 1793 loss: 4.19443586e-06
Iter: 1794 loss: 4.19136222e-06
Iter: 1795 loss: 4.20581182e-06
Iter: 1796 loss: 4.19069056e-06
Iter: 1797 loss: 4.18763966e-06
Iter: 1798 loss: 4.21767027e-06
Iter: 1799 loss: 4.18752415e-06
Iter: 1800 loss: 4.18598665e-06
Iter: 1801 loss: 4.18288573e-06
Iter: 1802 loss: 4.24126665e-06
Iter: 1803 loss: 4.18274522e-06
Iter: 1804 loss: 4.18149421e-06
Iter: 1805 loss: 4.18088848e-06
Iter: 1806 loss: 4.17918181e-06
Iter: 1807 loss: 4.17661522e-06
Iter: 1808 loss: 4.17660203e-06
Iter: 1809 loss: 4.17428055e-06
Iter: 1810 loss: 4.20263314e-06
Iter: 1811 loss: 4.17422507e-06
Iter: 1812 loss: 4.17193223e-06
Iter: 1813 loss: 4.17295769e-06
Iter: 1814 loss: 4.17034971e-06
Iter: 1815 loss: 4.16858347e-06
Iter: 1816 loss: 4.19332628e-06
Iter: 1817 loss: 4.16845387e-06
Iter: 1818 loss: 4.16688363e-06
Iter: 1819 loss: 4.16630428e-06
Iter: 1820 loss: 4.16549392e-06
Iter: 1821 loss: 4.16310968e-06
Iter: 1822 loss: 4.17165529e-06
Iter: 1823 loss: 4.16268449e-06
Iter: 1824 loss: 4.16045441e-06
Iter: 1825 loss: 4.16468765e-06
Iter: 1826 loss: 4.1596586e-06
Iter: 1827 loss: 4.15724026e-06
Iter: 1828 loss: 4.15545856e-06
Iter: 1829 loss: 4.15458317e-06
Iter: 1830 loss: 4.15145496e-06
Iter: 1831 loss: 4.15771046e-06
Iter: 1832 loss: 4.15020622e-06
Iter: 1833 loss: 4.1471435e-06
Iter: 1834 loss: 4.14715805e-06
Iter: 1835 loss: 4.14508031e-06
Iter: 1836 loss: 4.14082388e-06
Iter: 1837 loss: 4.21791674e-06
Iter: 1838 loss: 4.14072929e-06
Iter: 1839 loss: 4.13896487e-06
Iter: 1840 loss: 4.13850694e-06
Iter: 1841 loss: 4.13633552e-06
Iter: 1842 loss: 4.13474436e-06
Iter: 1843 loss: 4.13403268e-06
Iter: 1844 loss: 4.13179714e-06
Iter: 1845 loss: 4.15119712e-06
Iter: 1846 loss: 4.13166072e-06
Iter: 1847 loss: 4.12921872e-06
Iter: 1848 loss: 4.13034832e-06
Iter: 1849 loss: 4.12759573e-06
Iter: 1850 loss: 4.12543886e-06
Iter: 1851 loss: 4.14106626e-06
Iter: 1852 loss: 4.12530153e-06
Iter: 1853 loss: 4.12303234e-06
Iter: 1854 loss: 4.12592726e-06
Iter: 1855 loss: 4.12193822e-06
Iter: 1856 loss: 4.11964265e-06
Iter: 1857 loss: 4.127382e-06
Iter: 1858 loss: 4.11901556e-06
Iter: 1859 loss: 4.1170465e-06
Iter: 1860 loss: 4.11810834e-06
Iter: 1861 loss: 4.1157482e-06
Iter: 1862 loss: 4.11297697e-06
Iter: 1863 loss: 4.1137223e-06
Iter: 1864 loss: 4.11088376e-06
Iter: 1865 loss: 4.10783832e-06
Iter: 1866 loss: 4.11197561e-06
Iter: 1867 loss: 4.10616076e-06
Iter: 1868 loss: 4.1034632e-06
Iter: 1869 loss: 4.1033104e-06
Iter: 1870 loss: 4.10150187e-06
Iter: 1871 loss: 4.09800668e-06
Iter: 1872 loss: 4.17555657e-06
Iter: 1873 loss: 4.09800214e-06
Iter: 1874 loss: 4.09642416e-06
Iter: 1875 loss: 4.0961404e-06
Iter: 1876 loss: 4.09424092e-06
Iter: 1877 loss: 4.09275117e-06
Iter: 1878 loss: 4.09213953e-06
Iter: 1879 loss: 4.09019867e-06
Iter: 1880 loss: 4.10808616e-06
Iter: 1881 loss: 4.09000586e-06
Iter: 1882 loss: 4.08799133e-06
Iter: 1883 loss: 4.0889322e-06
Iter: 1884 loss: 4.086628e-06
Iter: 1885 loss: 4.08497726e-06
Iter: 1886 loss: 4.09541235e-06
Iter: 1887 loss: 4.08474307e-06
Iter: 1888 loss: 4.08311735e-06
Iter: 1889 loss: 4.08665164e-06
Iter: 1890 loss: 4.08238975e-06
Iter: 1891 loss: 4.08062533e-06
Iter: 1892 loss: 4.08392134e-06
Iter: 1893 loss: 4.07984317e-06
Iter: 1894 loss: 4.07802509e-06
Iter: 1895 loss: 4.08028882e-06
Iter: 1896 loss: 4.07704101e-06
Iter: 1897 loss: 4.07494463e-06
Iter: 1898 loss: 4.07617972e-06
Iter: 1899 loss: 4.07361858e-06
Iter: 1900 loss: 4.07121024e-06
Iter: 1901 loss: 4.07115112e-06
Iter: 1902 loss: 4.06924119e-06
Iter: 1903 loss: 4.06768959e-06
Iter: 1904 loss: 4.06725485e-06
Iter: 1905 loss: 4.06559911e-06
Iter: 1906 loss: 4.06316e-06
Iter: 1907 loss: 4.06305708e-06
Iter: 1908 loss: 4.06079471e-06
Iter: 1909 loss: 4.07518519e-06
Iter: 1910 loss: 4.06035133e-06
Iter: 1911 loss: 4.0577097e-06
Iter: 1912 loss: 4.06793652e-06
Iter: 1913 loss: 4.05707397e-06
Iter: 1914 loss: 4.05540686e-06
Iter: 1915 loss: 4.05443961e-06
Iter: 1916 loss: 4.05377523e-06
Iter: 1917 loss: 4.05146193e-06
Iter: 1918 loss: 4.05149422e-06
Iter: 1919 loss: 4.05025867e-06
Iter: 1920 loss: 4.04963248e-06
Iter: 1921 loss: 4.04892035e-06
Iter: 1922 loss: 4.04705952e-06
Iter: 1923 loss: 4.06262916e-06
Iter: 1924 loss: 4.04682e-06
Iter: 1925 loss: 4.04536513e-06
Iter: 1926 loss: 4.04699495e-06
Iter: 1927 loss: 4.04457614e-06
Iter: 1928 loss: 4.0429295e-06
Iter: 1929 loss: 4.04433604e-06
Iter: 1930 loss: 4.04184311e-06
Iter: 1931 loss: 4.03975309e-06
Iter: 1932 loss: 4.04233197e-06
Iter: 1933 loss: 4.03869899e-06
Iter: 1934 loss: 4.03640342e-06
Iter: 1935 loss: 4.03637114e-06
Iter: 1936 loss: 4.03461945e-06
Iter: 1937 loss: 4.03235845e-06
Iter: 1938 loss: 4.05223318e-06
Iter: 1939 loss: 4.03214744e-06
Iter: 1940 loss: 4.02965952e-06
Iter: 1941 loss: 4.03811555e-06
Iter: 1942 loss: 4.02898331e-06
Iter: 1943 loss: 4.02735168e-06
Iter: 1944 loss: 4.02614705e-06
Iter: 1945 loss: 4.0256823e-06
Iter: 1946 loss: 4.02392379e-06
Iter: 1947 loss: 4.02398609e-06
Iter: 1948 loss: 4.02277919e-06
Iter: 1949 loss: 4.02036721e-06
Iter: 1950 loss: 4.0595578e-06
Iter: 1951 loss: 4.0203563e-06
Iter: 1952 loss: 4.01942089e-06
Iter: 1953 loss: 4.01893976e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi0.8/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.2 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi1.2
+ date
Sun Nov  8 20:17:58 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.2/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.2/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -1 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3e6d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3cec730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3d546a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3dcdc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3db11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3db1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3c306a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3c30ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3b761e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3b76ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3b76620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3af7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3af7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3b236a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3b59950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3b237b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3ac5510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3ba7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcad2dac730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcad2dc9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcad2d23d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf3a7b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcad2cae840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcad2c8b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcad2c8b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcad2cc58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcad2ce4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcad2cd88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcad2cd8048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaac356488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaac3bb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaac3fc048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaac3fc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaac3f3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaac335a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaac3030d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 1.9996483
test_loss: 1.9964339
train_loss: 1.995882
test_loss: 1.9970106
train_loss: 1.9944133
test_loss: 1.9964759
train_loss: 1.9952252
test_loss: 1.9961687
train_loss: 1.9959402
test_loss: 1.9968288
train_loss: 1.9954488
test_loss: 1.9965693
train_loss: 1.999551
test_loss: 1.9958217
train_loss: 1.9920026
test_loss: 1.9972048
train_loss: 1.995575
test_loss: 1.9962366
train_loss: 1.9895403
test_loss: 1.9967451
train_loss: 1.993063
test_loss: 1.9969525
train_loss: 1.9896435
test_loss: 1.9977627
train_loss: 1.9888914
test_loss: 1.9962809
train_loss: 1.995319
test_loss: 1.9971045
train_loss: 1.9995006
test_loss: 1.9959366
train_loss: 1.99981
test_loss: 1.9955702
train_loss: 1.995904
test_loss: 1.9962006
train_loss: 1.9984677
test_loss: 1.996105
train_loss: 2.002016
test_loss: 1.9997852
train_loss: 1.9982198
test_loss: 1.9967413
train_loss: 1.9969939
test_loss: 1.997501
train_loss: 1.9991966
test_loss: 1.997452
train_loss: 1.9964652
test_loss: 1.9974511
train_loss: 1.9966488
test_loss: 1.9956465
train_loss: 1.996223
test_loss: 1.9968168
train_loss: 1.990435
test_loss: 1.9971567
train_loss: 1.9993446
test_loss: 1.9958494
train_loss: 1.9955902
test_loss: 1.9962419
train_loss: 1.9935572
test_loss: 1.9971832
train_loss: 1.9979684
test_loss: 1.9965383
train_loss: 1.9983644
test_loss: 1.9961661
train_loss: 0.4980098
test_loss: 0.49385
train_loss: 0.49835503
test_loss: 0.49614665
train_loss: 0.5043824
test_loss: 0.494754
train_loss: 0.48678395
test_loss: 0.49328208
train_loss: 0.49316344
test_loss: 0.49178874
train_loss: 0.49331734
test_loss: 0.49023917
train_loss: 0.49605063
test_loss: 0.48857138
train_loss: 0.485424
test_loss: 0.48678204
train_loss: 0.4956994
test_loss: 0.48497254
train_loss: 0.48015326
test_loss: 0.48307762
train_loss: 0.48003656
test_loss: 0.48106974
train_loss: 0.48482352
test_loss: 0.47898757
train_loss: 0.48423705
test_loss: 0.47671992
train_loss: 0.47502512
test_loss: 0.47444165
train_loss: 0.46874768
test_loss: 0.472039
train_loss: 0.47199258
test_loss: 0.4694921
train_loss: 0.46569812
test_loss: 0.4668323
train_loss: 0.46277806
test_loss: 0.46406454
train_loss: 0.4461049
test_loss: 0.46118647
train_loss: 0.456784
test_loss: 0.45803598
train_loss: 0.45442337
test_loss: 0.45495197
train_loss: 0.44804052
test_loss: 0.45162222
train_loss: 0.45622647
test_loss: 0.44812655
train_loss: 0.44028065
test_loss: 0.44454995
train_loss: 0.4473689
test_loss: 0.44074568
train_loss: 0.4404006
test_loss: 0.4367568
train_loss: 0.4260411
test_loss: 0.43262205
train_loss: 0.42930657
test_loss: 0.4283082
train_loss: 0.42177683
test_loss: 0.42380783
train_loss: 0.4120943
test_loss: 0.41908115
train_loss: 0.40954608
test_loss: 0.41417482
train_loss: 0.4044368
test_loss: 0.4090666
train_loss: 0.41090295
test_loss: 0.40372187
train_loss: 0.39651254
test_loss: 0.3981234
train_loss: 0.3902942
test_loss: 0.3923103
train_loss: 0.3802039
test_loss: 0.38625196
train_loss: 0.3846575
test_loss: 0.37993124
train_loss: 0.36867094
test_loss: 0.3733842
train_loss: 0.3646084
test_loss: 0.3664796
train_loss: 0.3526226
test_loss: 0.35934636
train_loss: 0.34929067
test_loss: 0.35187984
train_loss: 0.34159517
test_loss: 0.3440529
train_loss: 0.33471888
test_loss: 0.33594683
train_loss: 0.32957003
test_loss: 0.32752317
train_loss: 0.32090774
test_loss: 0.31873447
train_loss: 0.30604544
test_loss: 0.30963227
train_loss: 0.2994732
test_loss: 0.3001684
train_loss: 0.28980267
test_loss: 0.2902447
train_loss: 0.27643567
test_loss: 0.27999556
train_loss: 0.27087244
test_loss: 0.26941192
train_loss: 0.2599401
test_loss: 0.2584715
train_loss: 0.24163684
test_loss: 0.24721986
train_loss: 0.22823757
test_loss: 0.23566946
train_loss: 0.2279217
test_loss: 0.2239869
train_loss: 0.21254224
test_loss: 0.2121722
train_loss: 0.19841126
test_loss: 0.20037268
train_loss: 0.18628998
test_loss: 0.1887689
train_loss: 0.17749313
test_loss: 0.17756072
train_loss: 0.16763197
test_loss: 0.16693653
train_loss: 0.1526066
test_loss: 0.15694784
train_loss: 0.14669275
test_loss: 0.14767297
train_loss: 0.13557109
test_loss: 0.13905343
train_loss: 0.12989512
test_loss: 0.13103703
train_loss: 0.12434956
test_loss: 0.12345548
train_loss: 0.11642212
test_loss: 0.11626185
train_loss: 0.11139919
test_loss: 0.10935375
train_loss: 0.10119006
test_loss: 0.102815375
train_loss: 0.09834736
test_loss: 0.09670609
train_loss: 0.09156489
test_loss: 0.09113553
train_loss: 0.08560639
test_loss: 0.086266786
train_loss: 0.08167416
test_loss: 0.082186535
train_loss: 0.07957184
test_loss: 0.07900756
train_loss: 0.0757533
test_loss: 0.076639384
train_loss: 0.07477024
test_loss: 0.074960016
train_loss: 0.07331178
test_loss: 0.07379104
train_loss: 0.07444152
test_loss: 0.0729902
train_loss: 0.0726802
test_loss: 0.072414756
train_loss: 0.072816394
test_loss: 0.071998425
train_loss: 0.071447514
test_loss: 0.071655974
train_loss: 0.071521446
test_loss: 0.07136977
train_loss: 0.070944734
test_loss: 0.071132004
train_loss: 0.06984752
test_loss: 0.07090517
train_loss: 0.07046655
test_loss: 0.07070148
train_loss: 0.07210716
test_loss: 0.070499994
train_loss: 0.06933585
test_loss: 0.070294246
train_loss: 0.06910522
test_loss: 0.07011755
train_loss: 0.070284426
test_loss: 0.069933385
train_loss: 0.07234308
test_loss: 0.06977603
train_loss: 0.07083249
test_loss: 0.06959426
train_loss: 0.06904961
test_loss: 0.069408484
train_loss: 0.06975184
test_loss: 0.0692378
train_loss: 0.06900199
test_loss: 0.06907633
train_loss: 0.06930819
test_loss: 0.068913266
train_loss: 0.06783174
test_loss: 0.06874792
train_loss: 0.06944495
test_loss: 0.06857278
train_loss: 0.067095295
test_loss: 0.068395175
train_loss: 0.06832368
test_loss: 0.06821983
train_loss: 0.068511695
test_loss: 0.06804559
train_loss: 0.067336455
test_loss: 0.067860074
train_loss: 0.06607013
test_loss: 0.067686
train_loss: 0.06724896
test_loss: 0.06747658
train_loss: 0.06732088
test_loss: 0.067276046
train_loss: 0.06543092
test_loss: 0.0670483
train_loss: 0.06680305
test_loss: 0.066822655
train_loss: 0.0659215
test_loss: 0.06655345
train_loss: 0.06646177
test_loss: 0.066283405
train_loss: 0.06589696
test_loss: 0.06598292
train_loss: 0.06395882
test_loss: 0.06563466
train_loss: 0.06412692
test_loss: 0.065263495
train_loss: 0.06485325
test_loss: 0.06481793
train_loss: 0.06502744
test_loss: 0.0643432
train_loss: 0.06516955
test_loss: 0.06375946
train_loss: 0.06407879
test_loss: 0.063089095
train_loss: 0.06251717
test_loss: 0.062335525
train_loss: 0.059811726
test_loss: 0.061486218
train_loss: 0.060365632
test_loss: 0.0604781
train_loss: 0.057680413
test_loss: 0.059323784
train_loss: 0.058932424
test_loss: 0.057995696
train_loss: 0.055488825
test_loss: 0.056492075
train_loss: 0.054103546
test_loss: 0.054784443
train_loss: 0.053576052
test_loss: 0.052941725
train_loss: 0.049452856
test_loss: 0.051021688
train_loss: 0.046938058
test_loss: 0.04898487
train_loss: 0.046467327
test_loss: 0.046848394
train_loss: 0.043095566
test_loss: 0.044542786
train_loss: 0.041003585
test_loss: 0.041992098
train_loss: 0.037384767
test_loss: 0.03894495
train_loss: 0.03507518
test_loss: 0.03566537
train_loss: 0.03127675
test_loss: 0.032226205
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi1.2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-1_phi1.2/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -1 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-1_phi1.2/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f6f1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f6b18c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f6b1ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f6650d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f665ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f5e62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f5a7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f556598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f6656a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f5060d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f51cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f4e68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f4e69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f49fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f49fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f42d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f3fb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f4232f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f3e17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f42d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f383378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f339d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf6f36b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf5abeb620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf5abeb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf5ab99400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf5ab566a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf5ab56d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf5ab14048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf5ab3ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf3265c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf3261b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf3261b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf326377b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf3265c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf32596268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.00176236336
Iter: 2 loss: 0.0017650451
Iter: 3 loss: 0.00172309577
Iter: 4 loss: 0.00170260307
Iter: 5 loss: 0.00178958173
Iter: 6 loss: 0.0016982524
Iter: 7 loss: 0.00168490945
Iter: 8 loss: 0.0017518797
Iter: 9 loss: 0.00168268289
Iter: 10 loss: 0.00167061586
Iter: 11 loss: 0.00168098812
Iter: 12 loss: 0.00166351849
Iter: 13 loss: 0.00164765795
Iter: 14 loss: 0.00165497605
Iter: 15 loss: 0.00163690932
Iter: 16 loss: 0.0016135131
Iter: 17 loss: 0.00166318333
Iter: 18 loss: 0.00160423666
Iter: 19 loss: 0.00157731678
Iter: 20 loss: 0.00154173223
Iter: 21 loss: 0.0015395626
Iter: 22 loss: 0.00148815732
Iter: 23 loss: 0.00176267966
Iter: 24 loss: 0.00147989742
Iter: 25 loss: 0.00143714773
Iter: 26 loss: 0.00183259684
Iter: 27 loss: 0.00143529684
Iter: 28 loss: 0.00140373106
Iter: 29 loss: 0.00147546083
Iter: 30 loss: 0.00139179034
Iter: 31 loss: 0.00136172457
Iter: 32 loss: 0.00138436095
Iter: 33 loss: 0.00134330685
Iter: 34 loss: 0.00131295715
Iter: 35 loss: 0.00136879296
Iter: 36 loss: 0.00130005321
Iter: 37 loss: 0.00127052073
Iter: 38 loss: 0.00138107711
Iter: 39 loss: 0.00126343709
Iter: 40 loss: 0.00124514848
Iter: 41 loss: 0.00124412298
Iter: 42 loss: 0.00122893462
Iter: 43 loss: 0.00122598768
Iter: 44 loss: 0.0012158968
Iter: 45 loss: 0.00119341724
Iter: 46 loss: 0.00132360891
Iter: 47 loss: 0.0011904533
Iter: 48 loss: 0.00117596658
Iter: 49 loss: 0.00117796846
Iter: 50 loss: 0.00116495718
Iter: 51 loss: 0.00114472234
Iter: 52 loss: 0.00123156386
Iter: 53 loss: 0.00114065141
Iter: 54 loss: 0.00112468621
Iter: 55 loss: 0.00113620306
Iter: 56 loss: 0.0011148965
Iter: 57 loss: 0.00109617668
Iter: 58 loss: 0.00114787882
Iter: 59 loss: 0.00109016837
Iter: 60 loss: 0.00107464148
Iter: 61 loss: 0.00119978911
Iter: 62 loss: 0.0010736488
Iter: 63 loss: 0.00106105779
Iter: 64 loss: 0.00109752303
Iter: 65 loss: 0.00105716463
Iter: 66 loss: 0.00104640052
Iter: 67 loss: 0.00106401928
Iter: 68 loss: 0.00104150805
Iter: 69 loss: 0.00102988386
Iter: 70 loss: 0.00105660153
Iter: 71 loss: 0.00102559221
Iter: 72 loss: 0.00101497571
Iter: 73 loss: 0.00101673743
Iter: 74 loss: 0.00100701326
Iter: 75 loss: 0.000997364288
Iter: 76 loss: 0.000997345662
Iter: 77 loss: 0.000988366082
Iter: 78 loss: 0.00104161655
Iter: 79 loss: 0.000987224863
Iter: 80 loss: 0.000982162775
Iter: 81 loss: 0.000990457367
Iter: 82 loss: 0.000979849719
Iter: 83 loss: 0.000973289774
Iter: 84 loss: 0.000981555553
Iter: 85 loss: 0.00096991
Iter: 86 loss: 0.000963562
Iter: 87 loss: 0.00097923493
Iter: 88 loss: 0.000961321755
Iter: 89 loss: 0.000955454248
Iter: 90 loss: 0.000967269705
Iter: 91 loss: 0.000953074254
Iter: 92 loss: 0.000946422515
Iter: 93 loss: 0.000957553624
Iter: 94 loss: 0.000943421619
Iter: 95 loss: 0.000936956145
Iter: 96 loss: 0.000944808708
Iter: 97 loss: 0.0009335879
Iter: 98 loss: 0.000926621142
Iter: 99 loss: 0.00101066416
Iter: 100 loss: 0.000926539709
Iter: 101 loss: 0.000922062609
Iter: 102 loss: 0.000917652
Iter: 103 loss: 0.000916717341
Iter: 104 loss: 0.000910905888
Iter: 105 loss: 0.000934691052
Iter: 106 loss: 0.000909640454
Iter: 107 loss: 0.000904089771
Iter: 108 loss: 0.00092581223
Iter: 109 loss: 0.000902823289
Iter: 110 loss: 0.000898763188
Iter: 111 loss: 0.000933978474
Iter: 112 loss: 0.000898551545
Iter: 113 loss: 0.000894418685
Iter: 114 loss: 0.000910034927
Iter: 115 loss: 0.000893433462
Iter: 116 loss: 0.000890960451
Iter: 117 loss: 0.000892270298
Iter: 118 loss: 0.000889327493
Iter: 119 loss: 0.000885571586
Iter: 120 loss: 0.000899748295
Iter: 121 loss: 0.0008846774
Iter: 122 loss: 0.000882160966
Iter: 123 loss: 0.000881504675
Iter: 124 loss: 0.000879936852
Iter: 125 loss: 0.000876303704
Iter: 126 loss: 0.00089781225
Iter: 127 loss: 0.000875845551
Iter: 128 loss: 0.000872456294
Iter: 129 loss: 0.000876104692
Iter: 130 loss: 0.000870605814
Iter: 131 loss: 0.000867286057
Iter: 132 loss: 0.000882603053
Iter: 133 loss: 0.00086666469
Iter: 134 loss: 0.00086335279
Iter: 135 loss: 0.000876165577
Iter: 136 loss: 0.000862586428
Iter: 137 loss: 0.000859702122
Iter: 138 loss: 0.000861241424
Iter: 139 loss: 0.000857807
Iter: 140 loss: 0.000854475
Iter: 141 loss: 0.000856900297
Iter: 142 loss: 0.000852434139
Iter: 143 loss: 0.000849151518
Iter: 144 loss: 0.0008760934
Iter: 145 loss: 0.000848948664
Iter: 146 loss: 0.000846296374
Iter: 147 loss: 0.000877081242
Iter: 148 loss: 0.000846256
Iter: 149 loss: 0.000844105904
Iter: 150 loss: 0.000844681752
Iter: 151 loss: 0.000842550304
Iter: 152 loss: 0.000840078108
Iter: 153 loss: 0.000847119722
Iter: 154 loss: 0.000839303
Iter: 155 loss: 0.000836422376
Iter: 156 loss: 0.000844901195
Iter: 157 loss: 0.000835540646
Iter: 158 loss: 0.000833421422
Iter: 159 loss: 0.000831480371
Iter: 160 loss: 0.000830963661
Iter: 161 loss: 0.000827770797
Iter: 162 loss: 0.00087164744
Iter: 163 loss: 0.000827759621
Iter: 164 loss: 0.000825300696
Iter: 165 loss: 0.000830349803
Iter: 166 loss: 0.000824305811
Iter: 167 loss: 0.000821989262
Iter: 168 loss: 0.000830422156
Iter: 169 loss: 0.000821412366
Iter: 170 loss: 0.00081875769
Iter: 171 loss: 0.000826655189
Iter: 172 loss: 0.000817951863
Iter: 173 loss: 0.000815494452
Iter: 174 loss: 0.000817522756
Iter: 175 loss: 0.000814030878
Iter: 176 loss: 0.000811274396
Iter: 177 loss: 0.000815025822
Iter: 178 loss: 0.000809898542
Iter: 179 loss: 0.000807061908
Iter: 180 loss: 0.000818522531
Iter: 181 loss: 0.000806431577
Iter: 182 loss: 0.000804718584
Iter: 183 loss: 0.000804552808
Iter: 184 loss: 0.000803093717
Iter: 185 loss: 0.000800745678
Iter: 186 loss: 0.000800728099
Iter: 187 loss: 0.000798247871
Iter: 188 loss: 0.000811113277
Iter: 189 loss: 0.000797861314
Iter: 190 loss: 0.000795240747
Iter: 191 loss: 0.000803348899
Iter: 192 loss: 0.000794475258
Iter: 193 loss: 0.000792500796
Iter: 194 loss: 0.000791068247
Iter: 195 loss: 0.000790398044
Iter: 196 loss: 0.00078759226
Iter: 197 loss: 0.0008196329
Iter: 198 loss: 0.000787546276
Iter: 199 loss: 0.000785439915
Iter: 200 loss: 0.000786262
Iter: 201 loss: 0.00078398909
Iter: 202 loss: 0.000781736395
Iter: 203 loss: 0.000797419227
Iter: 204 loss: 0.000781527953
Iter: 205 loss: 0.000779386261
Iter: 206 loss: 0.000779292546
Iter: 207 loss: 0.000777643814
Iter: 208 loss: 0.000774865621
Iter: 209 loss: 0.000790830294
Iter: 210 loss: 0.000774492568
Iter: 211 loss: 0.000772301457
Iter: 212 loss: 0.000771195279
Iter: 213 loss: 0.000770170649
Iter: 214 loss: 0.000766455312
Iter: 215 loss: 0.000772629282
Iter: 216 loss: 0.000764763216
Iter: 217 loss: 0.000764107
Iter: 218 loss: 0.000762795098
Iter: 219 loss: 0.000761356
Iter: 220 loss: 0.000760086696
Iter: 221 loss: 0.000759712362
Iter: 222 loss: 0.000757508678
Iter: 223 loss: 0.000762201555
Iter: 224 loss: 0.000756633584
Iter: 225 loss: 0.000754199631
Iter: 226 loss: 0.000772554078
Iter: 227 loss: 0.000754015811
Iter: 228 loss: 0.000752108113
Iter: 229 loss: 0.000749721541
Iter: 230 loss: 0.000749527826
Iter: 231 loss: 0.000746536069
Iter: 232 loss: 0.000758723065
Iter: 233 loss: 0.000745883095
Iter: 234 loss: 0.000743139419
Iter: 235 loss: 0.000762522046
Iter: 236 loss: 0.000742894947
Iter: 237 loss: 0.000740837946
Iter: 238 loss: 0.000743446639
Iter: 239 loss: 0.000739776588
Iter: 240 loss: 0.000737558
Iter: 241 loss: 0.000745026453
Iter: 242 loss: 0.000736958929
Iter: 243 loss: 0.000734635047
Iter: 244 loss: 0.000741614203
Iter: 245 loss: 0.000733936788
Iter: 246 loss: 0.000731999287
Iter: 247 loss: 0.000730757485
Iter: 248 loss: 0.000730000786
Iter: 249 loss: 0.000727774808
Iter: 250 loss: 0.000752417
Iter: 251 loss: 0.000727729872
Iter: 252 loss: 0.000725985796
Iter: 253 loss: 0.000743505545
Iter: 254 loss: 0.000725929393
Iter: 255 loss: 0.000724343234
Iter: 256 loss: 0.000723249454
Iter: 257 loss: 0.000722674129
Iter: 258 loss: 0.000720939483
Iter: 259 loss: 0.000725951511
Iter: 260 loss: 0.000720401062
Iter: 261 loss: 0.000718241732
Iter: 262 loss: 0.000726655126
Iter: 263 loss: 0.000717737945
Iter: 264 loss: 0.000716086244
Iter: 265 loss: 0.000715458067
Iter: 266 loss: 0.000714560156
Iter: 267 loss: 0.000712102104
Iter: 268 loss: 0.000718103605
Iter: 269 loss: 0.000711226661
Iter: 270 loss: 0.000708730076
Iter: 271 loss: 0.000716688344
Iter: 272 loss: 0.000708013715
Iter: 273 loss: 0.000705823943
Iter: 274 loss: 0.000717177929
Iter: 275 loss: 0.000705476268
Iter: 276 loss: 0.000703473808
Iter: 277 loss: 0.000711663859
Iter: 278 loss: 0.000703038764
Iter: 279 loss: 0.000701381941
Iter: 280 loss: 0.000701394747
Iter: 281 loss: 0.000700056669
Iter: 282 loss: 0.000697916781
Iter: 283 loss: 0.000707646541
Iter: 284 loss: 0.000697507756
Iter: 285 loss: 0.000695384
Iter: 286 loss: 0.000706320512
Iter: 287 loss: 0.00069504173
Iter: 288 loss: 0.000693488168
Iter: 289 loss: 0.000711890869
Iter: 290 loss: 0.000693465059
Iter: 291 loss: 0.000692417962
Iter: 292 loss: 0.00069067406
Iter: 293 loss: 0.00069066952
Iter: 294 loss: 0.000688977714
Iter: 295 loss: 0.000700731878
Iter: 296 loss: 0.000688816188
Iter: 297 loss: 0.00068712316
Iter: 298 loss: 0.000691548397
Iter: 299 loss: 0.000686556567
Iter: 300 loss: 0.000685183099
Iter: 301 loss: 0.000684114289
Iter: 302 loss: 0.000683681224
Iter: 303 loss: 0.000681476609
Iter: 304 loss: 0.000687885564
Iter: 305 loss: 0.000680791
Iter: 306 loss: 0.000678876415
Iter: 307 loss: 0.000691977
Iter: 308 loss: 0.000678691955
Iter: 309 loss: 0.000676994794
Iter: 310 loss: 0.000681531848
Iter: 311 loss: 0.000676430238
Iter: 312 loss: 0.000674645882
Iter: 313 loss: 0.000681710662
Iter: 314 loss: 0.000674237614
Iter: 315 loss: 0.000672831608
Iter: 316 loss: 0.000672934577
Iter: 317 loss: 0.00067173678
Iter: 318 loss: 0.000669925648
Iter: 319 loss: 0.000688890228
Iter: 320 loss: 0.000669876928
Iter: 321 loss: 0.00066870416
Iter: 322 loss: 0.000682849437
Iter: 323 loss: 0.0006686899
Iter: 324 loss: 0.000667613116
Iter: 325 loss: 0.00066692906
Iter: 326 loss: 0.000666508684
Iter: 327 loss: 0.000665026426
Iter: 328 loss: 0.000668265682
Iter: 329 loss: 0.000664445106
Iter: 330 loss: 0.000662990496
Iter: 331 loss: 0.000669908419
Iter: 332 loss: 0.00066272862
Iter: 333 loss: 0.000661222148
Iter: 334 loss: 0.000661275815
Iter: 335 loss: 0.00066003151
Iter: 336 loss: 0.000658271601
Iter: 337 loss: 0.000665865256
Iter: 338 loss: 0.000657911762
Iter: 339 loss: 0.000656465185
Iter: 340 loss: 0.000657292316
Iter: 341 loss: 0.000655524
Iter: 342 loss: 0.000653639901
Iter: 343 loss: 0.000655734388
Iter: 344 loss: 0.000652620802
Iter: 345 loss: 0.00065069733
Iter: 346 loss: 0.000671737478
Iter: 347 loss: 0.000650655478
Iter: 348 loss: 0.000649024849
Iter: 349 loss: 0.000651754497
Iter: 350 loss: 0.000648286135
Iter: 351 loss: 0.000646658416
Iter: 352 loss: 0.0006499429
Iter: 353 loss: 0.000646000379
Iter: 354 loss: 0.000644632149
Iter: 355 loss: 0.000658508274
Iter: 356 loss: 0.000644588959
Iter: 357 loss: 0.000643424
Iter: 358 loss: 0.000651628245
Iter: 359 loss: 0.000643317238
Iter: 360 loss: 0.00064261863
Iter: 361 loss: 0.000641827413
Iter: 362 loss: 0.00064172328
Iter: 363 loss: 0.000640496262
Iter: 364 loss: 0.000645814638
Iter: 365 loss: 0.000640246086
Iter: 366 loss: 0.000639175763
Iter: 367 loss: 0.00064181
Iter: 368 loss: 0.000638797414
Iter: 369 loss: 0.000637584308
Iter: 370 loss: 0.000638713362
Iter: 371 loss: 0.000636884128
Iter: 372 loss: 0.000635373755
Iter: 373 loss: 0.000636777258
Iter: 374 loss: 0.000634505239
Iter: 375 loss: 0.000632868963
Iter: 376 loss: 0.000634062104
Iter: 377 loss: 0.000631858245
Iter: 378 loss: 0.000630038558
Iter: 379 loss: 0.000640757382
Iter: 380 loss: 0.000629803515
Iter: 381 loss: 0.000628181617
Iter: 382 loss: 0.000637055724
Iter: 383 loss: 0.000627943082
Iter: 384 loss: 0.0006265681
Iter: 385 loss: 0.000630029
Iter: 386 loss: 0.000626088469
Iter: 387 loss: 0.000624717795
Iter: 388 loss: 0.000627349189
Iter: 389 loss: 0.00062413956
Iter: 390 loss: 0.000623044849
Iter: 391 loss: 0.000623042812
Iter: 392 loss: 0.000622074585
Iter: 393 loss: 0.000622433843
Iter: 394 loss: 0.000621392333
Iter: 395 loss: 0.000620378531
Iter: 396 loss: 0.00061967934
Iter: 397 loss: 0.000619311933
Iter: 398 loss: 0.000617969665
Iter: 399 loss: 0.000637371
Iter: 400 loss: 0.000617967104
Iter: 401 loss: 0.000616864068
Iter: 402 loss: 0.000617753132
Iter: 403 loss: 0.000616202829
Iter: 404 loss: 0.00061478233
Iter: 405 loss: 0.00061666267
Iter: 406 loss: 0.000614063581
Iter: 407 loss: 0.000612271309
Iter: 408 loss: 0.000614839722
Iter: 409 loss: 0.000611394877
Iter: 410 loss: 0.000609676179
Iter: 411 loss: 0.000620978884
Iter: 412 loss: 0.00060949527
Iter: 413 loss: 0.000608103641
Iter: 414 loss: 0.00061013177
Iter: 415 loss: 0.000607427
Iter: 416 loss: 0.000605935813
Iter: 417 loss: 0.000607693684
Iter: 418 loss: 0.000605145935
Iter: 419 loss: 0.000603643362
Iter: 420 loss: 0.00061902852
Iter: 421 loss: 0.000603601628
Iter: 422 loss: 0.000602373388
Iter: 423 loss: 0.000603589637
Iter: 424 loss: 0.000601682405
Iter: 425 loss: 0.000600509928
Iter: 426 loss: 0.000619248836
Iter: 427 loss: 0.000600509578
Iter: 428 loss: 0.000599474472
Iter: 429 loss: 0.000598662882
Iter: 430 loss: 0.00059834175
Iter: 431 loss: 0.000596997852
Iter: 432 loss: 0.000601416279
Iter: 433 loss: 0.000596622296
Iter: 434 loss: 0.0005953699
Iter: 435 loss: 0.00061064586
Iter: 436 loss: 0.000595356
Iter: 437 loss: 0.000594339508
Iter: 438 loss: 0.000594037701
Iter: 439 loss: 0.000593428849
Iter: 440 loss: 0.000591859687
Iter: 441 loss: 0.000595298596
Iter: 442 loss: 0.000591254
Iter: 443 loss: 0.000589856703
Iter: 444 loss: 0.000594751153
Iter: 445 loss: 0.000589489
Iter: 446 loss: 0.000587987597
Iter: 447 loss: 0.000591589487
Iter: 448 loss: 0.000587448478
Iter: 449 loss: 0.000586028094
Iter: 450 loss: 0.000587111106
Iter: 451 loss: 0.000585166737
Iter: 452 loss: 0.000583404209
Iter: 453 loss: 0.000590459676
Iter: 454 loss: 0.000583001354
Iter: 455 loss: 0.000581276
Iter: 456 loss: 0.000589614
Iter: 457 loss: 0.000580971537
Iter: 458 loss: 0.000579599757
Iter: 459 loss: 0.000586304697
Iter: 460 loss: 0.000579361
Iter: 461 loss: 0.000577996485
Iter: 462 loss: 0.000581624
Iter: 463 loss: 0.000577542465
Iter: 464 loss: 0.00057627
Iter: 465 loss: 0.000576557301
Iter: 466 loss: 0.000575324229
Iter: 467 loss: 0.000573665
Iter: 468 loss: 0.000574941107
Iter: 469 loss: 0.000572658
Iter: 470 loss: 0.000571441546
Iter: 471 loss: 0.000571355
Iter: 472 loss: 0.000570239383
Iter: 473 loss: 0.000571684
Iter: 474 loss: 0.000569669821
Iter: 475 loss: 0.000568612246
Iter: 476 loss: 0.00056788337
Iter: 477 loss: 0.000567495823
Iter: 478 loss: 0.000565616239
Iter: 479 loss: 0.000573517405
Iter: 480 loss: 0.00056521839
Iter: 481 loss: 0.000563526875
Iter: 482 loss: 0.000564673857
Iter: 483 loss: 0.000562462839
Iter: 484 loss: 0.000560646527
Iter: 485 loss: 0.000563764537
Iter: 486 loss: 0.000559840351
Iter: 487 loss: 0.000558004947
Iter: 488 loss: 0.000572322868
Iter: 489 loss: 0.000557875843
Iter: 490 loss: 0.000556519488
Iter: 491 loss: 0.000565865543
Iter: 492 loss: 0.000556389627
Iter: 493 loss: 0.000555199105
Iter: 494 loss: 0.000557311461
Iter: 495 loss: 0.000554680359
Iter: 496 loss: 0.000553043908
Iter: 497 loss: 0.000554026163
Iter: 498 loss: 0.000551985519
Iter: 499 loss: 0.000550494
Iter: 500 loss: 0.000551626668
Iter: 501 loss: 0.000549583754
Iter: 502 loss: 0.000547733915
Iter: 503 loss: 0.000556633342
Iter: 504 loss: 0.000547404401
Iter: 505 loss: 0.000545883842
Iter: 506 loss: 0.000552339
Iter: 507 loss: 0.000545569288
Iter: 508 loss: 0.000544138369
Iter: 509 loss: 0.000550410303
Iter: 510 loss: 0.000543845934
Iter: 511 loss: 0.000542406924
Iter: 512 loss: 0.000543094764
Iter: 513 loss: 0.000541443122
Iter: 514 loss: 0.000539718545
Iter: 515 loss: 0.000541657559
Iter: 516 loss: 0.000538787514
Iter: 517 loss: 0.000537042564
Iter: 518 loss: 0.000544420851
Iter: 519 loss: 0.000536681036
Iter: 520 loss: 0.000535050349
Iter: 521 loss: 0.000537707645
Iter: 522 loss: 0.000534299819
Iter: 523 loss: 0.000532975653
Iter: 524 loss: 0.000548204
Iter: 525 loss: 0.000532955746
Iter: 526 loss: 0.000531822676
Iter: 527 loss: 0.000537039596
Iter: 528 loss: 0.000531609519
Iter: 529 loss: 0.000530530524
Iter: 530 loss: 0.000531988451
Iter: 531 loss: 0.000529985235
Iter: 532 loss: 0.000528810313
Iter: 533 loss: 0.000530830119
Iter: 534 loss: 0.000528286386
Iter: 535 loss: 0.000526965363
Iter: 536 loss: 0.00052816188
Iter: 537 loss: 0.000526198826
Iter: 538 loss: 0.000524754345
Iter: 539 loss: 0.000527670549
Iter: 540 loss: 0.000524166797
Iter: 541 loss: 0.000522716378
Iter: 542 loss: 0.000534360763
Iter: 543 loss: 0.000522619928
Iter: 544 loss: 0.000521323294
Iter: 545 loss: 0.000526962162
Iter: 546 loss: 0.000521059439
Iter: 547 loss: 0.000520110421
Iter: 548 loss: 0.000522596296
Iter: 549 loss: 0.000519789639
Iter: 550 loss: 0.000518822228
Iter: 551 loss: 0.000518864254
Iter: 552 loss: 0.000518060406
Iter: 553 loss: 0.000516711269
Iter: 554 loss: 0.000520759961
Iter: 555 loss: 0.000516303582
Iter: 556 loss: 0.000515048858
Iter: 557 loss: 0.000517194625
Iter: 558 loss: 0.00051448989
Iter: 559 loss: 0.00051372766
Iter: 560 loss: 0.000513604085
Iter: 561 loss: 0.000512893428
Iter: 562 loss: 0.000513071544
Iter: 563 loss: 0.000512374449
Iter: 564 loss: 0.000511492544
Iter: 565 loss: 0.00051294273
Iter: 566 loss: 0.000511089107
Iter: 567 loss: 0.000510046142
Iter: 568 loss: 0.000511695864
Iter: 569 loss: 0.000509558362
Iter: 570 loss: 0.000508535421
Iter: 571 loss: 0.000509619131
Iter: 572 loss: 0.00050797133
Iter: 573 loss: 0.000506815
Iter: 574 loss: 0.000509533507
Iter: 575 loss: 0.000506390177
Iter: 576 loss: 0.000505259261
Iter: 577 loss: 0.000515986525
Iter: 578 loss: 0.000505215663
Iter: 579 loss: 0.000504266645
Iter: 580 loss: 0.000505071948
Iter: 581 loss: 0.000503704534
Iter: 582 loss: 0.000502498
Iter: 583 loss: 0.000503389514
Iter: 584 loss: 0.000501760107
Iter: 585 loss: 0.000500329072
Iter: 586 loss: 0.000504318392
Iter: 587 loss: 0.000499871618
Iter: 588 loss: 0.000498656
Iter: 589 loss: 0.000502468785
Iter: 590 loss: 0.000498304435
Iter: 591 loss: 0.000497402041
Iter: 592 loss: 0.000505782082
Iter: 593 loss: 0.000497364497
Iter: 594 loss: 0.000496546214
Iter: 595 loss: 0.000501719886
Iter: 596 loss: 0.000496454129
Iter: 597 loss: 0.000495805813
Iter: 598 loss: 0.00049564417
Iter: 599 loss: 0.000495235319
Iter: 600 loss: 0.000494414358
Iter: 601 loss: 0.000496862049
Iter: 602 loss: 0.000494164589
Iter: 603 loss: 0.00049331889
Iter: 604 loss: 0.00049397297
Iter: 605 loss: 0.000492805033
Iter: 606 loss: 0.000491761661
Iter: 607 loss: 0.000493116386
Iter: 608 loss: 0.000491230341
Iter: 609 loss: 0.000490100414
Iter: 610 loss: 0.000496038352
Iter: 611 loss: 0.0004899255
Iter: 612 loss: 0.000489009137
Iter: 613 loss: 0.000495003536
Iter: 614 loss: 0.000488910591
Iter: 615 loss: 0.000488090707
Iter: 616 loss: 0.000488791033
Iter: 617 loss: 0.000487604906
Iter: 618 loss: 0.000486678677
Iter: 619 loss: 0.000486297911
Iter: 620 loss: 0.00048580984
Iter: 621 loss: 0.000484548334
Iter: 622 loss: 0.000490374165
Iter: 623 loss: 0.000484311546
Iter: 624 loss: 0.000483139185
Iter: 625 loss: 0.000486047647
Iter: 626 loss: 0.000482727395
Iter: 627 loss: 0.00048194913
Iter: 628 loss: 0.000481903728
Iter: 629 loss: 0.000481267576
Iter: 630 loss: 0.000481064839
Iter: 631 loss: 0.000480693532
Iter: 632 loss: 0.000479824317
Iter: 633 loss: 0.000481028925
Iter: 634 loss: 0.000479393406
Iter: 635 loss: 0.000478531467
Iter: 636 loss: 0.000482234
Iter: 637 loss: 0.000478354574
Iter: 638 loss: 0.000477597525
Iter: 639 loss: 0.000477938738
Iter: 640 loss: 0.000477082736
Iter: 641 loss: 0.000476121728
Iter: 642 loss: 0.00047847192
Iter: 643 loss: 0.000475778943
Iter: 644 loss: 0.000474761735
Iter: 645 loss: 0.000480029092
Iter: 646 loss: 0.000474599365
Iter: 647 loss: 0.000473703025
Iter: 648 loss: 0.000478020054
Iter: 649 loss: 0.000473545137
Iter: 650 loss: 0.00047275424
Iter: 651 loss: 0.000471959531
Iter: 652 loss: 0.000471798237
Iter: 653 loss: 0.000470562954
Iter: 654 loss: 0.00047388277
Iter: 655 loss: 0.000470153522
Iter: 656 loss: 0.000468924496
Iter: 657 loss: 0.000474640139
Iter: 658 loss: 0.000468695449
Iter: 659 loss: 0.00046769125
Iter: 660 loss: 0.000473045686
Iter: 661 loss: 0.000467536767
Iter: 662 loss: 0.000466683705
Iter: 663 loss: 0.000476695976
Iter: 664 loss: 0.000466671743
Iter: 665 loss: 0.000466138561
Iter: 666 loss: 0.000465473568
Iter: 667 loss: 0.000465418154
Iter: 668 loss: 0.000464543409
Iter: 669 loss: 0.000466369209
Iter: 670 loss: 0.000464193348
Iter: 671 loss: 0.000463164499
Iter: 672 loss: 0.000466878555
Iter: 673 loss: 0.000462903
Iter: 674 loss: 0.000461981224
Iter: 675 loss: 0.00046203853
Iter: 676 loss: 0.000461258343
Iter: 677 loss: 0.000459954841
Iter: 678 loss: 0.000465345162
Iter: 679 loss: 0.000459671428
Iter: 680 loss: 0.000458622148
Iter: 681 loss: 0.000468286249
Iter: 682 loss: 0.000458578405
Iter: 683 loss: 0.000457677088
Iter: 684 loss: 0.000457986869
Iter: 685 loss: 0.000457040122
Iter: 686 loss: 0.00045602044
Iter: 687 loss: 0.000455805217
Iter: 688 loss: 0.000455135887
Iter: 689 loss: 0.000453709072
Iter: 690 loss: 0.000462950527
Iter: 691 loss: 0.000453553861
Iter: 692 loss: 0.000452363776
Iter: 693 loss: 0.000455464964
Iter: 694 loss: 0.000451958593
Iter: 695 loss: 0.000451137486
Iter: 696 loss: 0.000451128057
Iter: 697 loss: 0.000450322463
Iter: 698 loss: 0.000449975953
Iter: 699 loss: 0.000449559826
Iter: 700 loss: 0.000448569772
Iter: 701 loss: 0.00044925508
Iter: 702 loss: 0.000447950268
Iter: 703 loss: 0.000446837279
Iter: 704 loss: 0.000452213746
Iter: 705 loss: 0.000446639489
Iter: 706 loss: 0.000445663143
Iter: 707 loss: 0.000447583356
Iter: 708 loss: 0.000445259764
Iter: 709 loss: 0.000444310019
Iter: 710 loss: 0.000447439699
Iter: 711 loss: 0.000444046455
Iter: 712 loss: 0.000443087367
Iter: 713 loss: 0.000445152575
Iter: 714 loss: 0.000442714925
Iter: 715 loss: 0.00044178078
Iter: 716 loss: 0.000447533152
Iter: 717 loss: 0.000441669603
Iter: 718 loss: 0.000440808828
Iter: 719 loss: 0.000441560638
Iter: 720 loss: 0.000440302712
Iter: 721 loss: 0.000439244643
Iter: 722 loss: 0.000440941891
Iter: 723 loss: 0.000438751798
Iter: 724 loss: 0.000437674171
Iter: 725 loss: 0.000438284245
Iter: 726 loss: 0.000436969975
Iter: 727 loss: 0.000435721478
Iter: 728 loss: 0.000442277757
Iter: 729 loss: 0.000435526
Iter: 730 loss: 0.000434891495
Iter: 731 loss: 0.000434823771
Iter: 732 loss: 0.000434324087
Iter: 733 loss: 0.000433602574
Iter: 734 loss: 0.000433580717
Iter: 735 loss: 0.00043262777
Iter: 736 loss: 0.000433564215
Iter: 737 loss: 0.000432088273
Iter: 738 loss: 0.000431012711
Iter: 739 loss: 0.000435883529
Iter: 740 loss: 0.000430805
Iter: 741 loss: 0.000429838168
Iter: 742 loss: 0.000432373112
Iter: 743 loss: 0.000429509557
Iter: 744 loss: 0.000428458443
Iter: 745 loss: 0.000431106135
Iter: 746 loss: 0.000428088882
Iter: 747 loss: 0.000427070423
Iter: 748 loss: 0.000429830106
Iter: 749 loss: 0.00042673212
Iter: 750 loss: 0.000425662642
Iter: 751 loss: 0.000431738503
Iter: 752 loss: 0.000425514765
Iter: 753 loss: 0.000424643164
Iter: 754 loss: 0.000425007136
Iter: 755 loss: 0.000424042315
Iter: 756 loss: 0.000422950368
Iter: 757 loss: 0.000423901947
Iter: 758 loss: 0.000422305602
Iter: 759 loss: 0.000420945842
Iter: 760 loss: 0.000422640645
Iter: 761 loss: 0.000420238532
Iter: 762 loss: 0.000419632619
Iter: 763 loss: 0.000419454649
Iter: 764 loss: 0.000418672716
Iter: 765 loss: 0.000418967422
Iter: 766 loss: 0.000418126408
Iter: 767 loss: 0.000417348143
Iter: 768 loss: 0.000417663425
Iter: 769 loss: 0.000416810188
Iter: 770 loss: 0.000415837509
Iter: 771 loss: 0.000418470358
Iter: 772 loss: 0.000415514514
Iter: 773 loss: 0.000414510665
Iter: 774 loss: 0.00041710076
Iter: 775 loss: 0.000414163922
Iter: 776 loss: 0.000413112168
Iter: 777 loss: 0.000416734314
Iter: 778 loss: 0.000412832742
Iter: 779 loss: 0.000411842193
Iter: 780 loss: 0.000413913367
Iter: 781 loss: 0.000411446148
Iter: 782 loss: 0.000410467503
Iter: 783 loss: 0.000416369236
Iter: 784 loss: 0.000410346489
Iter: 785 loss: 0.000409447239
Iter: 786 loss: 0.000410885026
Iter: 787 loss: 0.000409030647
Iter: 788 loss: 0.000408097
Iter: 789 loss: 0.000408495718
Iter: 790 loss: 0.000407455489
Iter: 791 loss: 0.000406271021
Iter: 792 loss: 0.000408440537
Iter: 793 loss: 0.000405762403
Iter: 794 loss: 0.000404663297
Iter: 795 loss: 0.000411926652
Iter: 796 loss: 0.000404546765
Iter: 797 loss: 0.00040369411
Iter: 798 loss: 0.000415256072
Iter: 799 loss: 0.000403689512
Iter: 800 loss: 0.000403014419
Iter: 801 loss: 0.000402722973
Iter: 802 loss: 0.000402375648
Iter: 803 loss: 0.000401392172
Iter: 804 loss: 0.000401502824
Iter: 805 loss: 0.000400637859
Iter: 806 loss: 0.00039952193
Iter: 807 loss: 0.000402245874
Iter: 808 loss: 0.000399121083
Iter: 809 loss: 0.000398054544
Iter: 810 loss: 0.000405467581
Iter: 811 loss: 0.0003979537
Iter: 812 loss: 0.000396940479
Iter: 813 loss: 0.000398166594
Iter: 814 loss: 0.000396407268
Iter: 815 loss: 0.000395304
Iter: 816 loss: 0.000398549862
Iter: 817 loss: 0.000394963135
Iter: 818 loss: 0.000393855327
Iter: 819 loss: 0.000400358083
Iter: 820 loss: 0.000393709633
Iter: 821 loss: 0.000392823713
Iter: 822 loss: 0.000393687631
Iter: 823 loss: 0.000392319023
Iter: 824 loss: 0.000391367241
Iter: 825 loss: 0.000391860667
Iter: 826 loss: 0.000390733068
Iter: 827 loss: 0.000389581284
Iter: 828 loss: 0.000393487891
Iter: 829 loss: 0.00038926964
Iter: 830 loss: 0.000388380024
Iter: 831 loss: 0.000399838202
Iter: 832 loss: 0.000388372457
Iter: 833 loss: 0.000387510925
Iter: 834 loss: 0.000389838242
Iter: 835 loss: 0.000387226144
Iter: 836 loss: 0.0003864881
Iter: 837 loss: 0.000386486528
Iter: 838 loss: 0.000385896128
Iter: 839 loss: 0.000384913961
Iter: 840 loss: 0.000384987477
Iter: 841 loss: 0.000384148443
Iter: 842 loss: 0.000382901286
Iter: 843 loss: 0.000388501794
Iter: 844 loss: 0.000382657367
Iter: 845 loss: 0.000381594233
Iter: 846 loss: 0.000389709632
Iter: 847 loss: 0.00038151379
Iter: 848 loss: 0.000380638288
Iter: 849 loss: 0.000380654645
Iter: 850 loss: 0.00037994163
Iter: 851 loss: 0.000378914207
Iter: 852 loss: 0.000388122979
Iter: 853 loss: 0.000378862722
Iter: 854 loss: 0.000378027733
Iter: 855 loss: 0.000379370351
Iter: 856 loss: 0.000377640972
Iter: 857 loss: 0.00037671937
Iter: 858 loss: 0.000377577089
Iter: 859 loss: 0.000376187381
Iter: 860 loss: 0.000375220901
Iter: 861 loss: 0.000377447403
Iter: 862 loss: 0.000374859606
Iter: 863 loss: 0.000373783434
Iter: 864 loss: 0.000377793564
Iter: 865 loss: 0.00037352246
Iter: 866 loss: 0.00037268555
Iter: 867 loss: 0.000380549
Iter: 868 loss: 0.000372652023
Iter: 869 loss: 0.000371775182
Iter: 870 loss: 0.000373172574
Iter: 871 loss: 0.000371366128
Iter: 872 loss: 0.00037062238
Iter: 873 loss: 0.000370038615
Iter: 874 loss: 0.000369807589
Iter: 875 loss: 0.000368697627
Iter: 876 loss: 0.000371449481
Iter: 877 loss: 0.000368305045
Iter: 878 loss: 0.000367239583
Iter: 879 loss: 0.000374211406
Iter: 880 loss: 0.000367126078
Iter: 881 loss: 0.000366082
Iter: 882 loss: 0.000369161135
Iter: 883 loss: 0.00036575878
Iter: 884 loss: 0.000364884792
Iter: 885 loss: 0.000366470049
Iter: 886 loss: 0.000364503649
Iter: 887 loss: 0.000363514351
Iter: 888 loss: 0.000369167305
Iter: 889 loss: 0.000363379775
Iter: 890 loss: 0.000362554507
Iter: 891 loss: 0.000362955238
Iter: 892 loss: 0.000361999351
Iter: 893 loss: 0.000360950187
Iter: 894 loss: 0.00036248361
Iter: 895 loss: 0.000360440143
Iter: 896 loss: 0.000359417696
Iter: 897 loss: 0.000364231208
Iter: 898 loss: 0.000359230267
Iter: 899 loss: 0.000358277379
Iter: 900 loss: 0.000362945168
Iter: 901 loss: 0.000358109915
Iter: 902 loss: 0.000357296085
Iter: 903 loss: 0.000366258202
Iter: 904 loss: 0.000357278594
Iter: 905 loss: 0.000356650038
Iter: 906 loss: 0.000355563185
Iter: 907 loss: 0.00035556333
Iter: 908 loss: 0.000354263466
Iter: 909 loss: 0.000356870471
Iter: 910 loss: 0.000353730953
Iter: 911 loss: 0.000352530857
Iter: 912 loss: 0.000357425131
Iter: 913 loss: 0.000352265197
Iter: 914 loss: 0.000351229333
Iter: 915 loss: 0.00036004337
Iter: 916 loss: 0.00035116967
Iter: 917 loss: 0.000350333081
Iter: 918 loss: 0.000350890652
Iter: 919 loss: 0.000349806272
Iter: 920 loss: 0.000348847767
Iter: 921 loss: 0.00035463876
Iter: 922 loss: 0.000348729925
Iter: 923 loss: 0.000347833266
Iter: 924 loss: 0.000348910777
Iter: 925 loss: 0.00034735896
Iter: 926 loss: 0.000346392393
Iter: 927 loss: 0.000349233887
Iter: 928 loss: 0.000346095
Iter: 929 loss: 0.000345144537
Iter: 930 loss: 0.000346492627
Iter: 931 loss: 0.000344677828
Iter: 932 loss: 0.000343600928
Iter: 933 loss: 0.000347664
Iter: 934 loss: 0.000343338819
Iter: 935 loss: 0.000342487532
Iter: 936 loss: 0.000352605246
Iter: 937 loss: 0.000342476502
Iter: 938 loss: 0.000341658044
Iter: 939 loss: 0.000342077226
Iter: 940 loss: 0.000341113278
Iter: 941 loss: 0.00034037052
Iter: 942 loss: 0.000340240047
Iter: 943 loss: 0.000339732447
Iter: 944 loss: 0.000338619226
Iter: 945 loss: 0.000341467501
Iter: 946 loss: 0.000338236336
Iter: 947 loss: 0.000336962345
Iter: 948 loss: 0.000341414183
Iter: 949 loss: 0.000336624
Iter: 950 loss: 0.000335640856
Iter: 951 loss: 0.000338481
Iter: 952 loss: 0.000335334567
Iter: 953 loss: 0.000334375305
Iter: 954 loss: 0.000341712759
Iter: 955 loss: 0.00033430144
Iter: 956 loss: 0.000333522737
Iter: 957 loss: 0.000333702745
Iter: 958 loss: 0.000332948664
Iter: 959 loss: 0.000332065334
Iter: 960 loss: 0.000338011421
Iter: 961 loss: 0.000331976917
Iter: 962 loss: 0.000331145944
Iter: 963 loss: 0.000330803974
Iter: 964 loss: 0.000330366311
Iter: 965 loss: 0.000329115457
Iter: 966 loss: 0.000331625663
Iter: 967 loss: 0.000328602298
Iter: 968 loss: 0.000327499
Iter: 969 loss: 0.000340609316
Iter: 970 loss: 0.000327483984
Iter: 971 loss: 0.000326572364
Iter: 972 loss: 0.000332208496
Iter: 973 loss: 0.000326464738
Iter: 974 loss: 0.000325782836
Iter: 975 loss: 0.000325004105
Iter: 976 loss: 0.00032490649
Iter: 977 loss: 0.000323765067
Iter: 978 loss: 0.000326241221
Iter: 979 loss: 0.000323323591
Iter: 980 loss: 0.00032206747
Iter: 981 loss: 0.000324116263
Iter: 982 loss: 0.000321489439
Iter: 983 loss: 0.000320327701
Iter: 984 loss: 0.000323476037
Iter: 985 loss: 0.000319944258
Iter: 986 loss: 0.00031897903
Iter: 987 loss: 0.000328207912
Iter: 988 loss: 0.000318941195
Iter: 989 loss: 0.000318057428
Iter: 990 loss: 0.000320227526
Iter: 991 loss: 0.000317742641
Iter: 992 loss: 0.000316885853
Iter: 993 loss: 0.00031756406
Iter: 994 loss: 0.000316368736
Iter: 995 loss: 0.000315367186
Iter: 996 loss: 0.000322079897
Iter: 997 loss: 0.000315265497
Iter: 998 loss: 0.000314481353
Iter: 999 loss: 0.000313832454
Iter: 1000 loss: 0.000313606375
Iter: 1001 loss: 0.000312308781
Iter: 1002 loss: 0.000317164377
Iter: 1003 loss: 0.000311991345
Iter: 1004 loss: 0.000311306678
Iter: 1005 loss: 0.000311261334
Iter: 1006 loss: 0.000310666335
Iter: 1007 loss: 0.000310815172
Iter: 1008 loss: 0.000310233445
Iter: 1009 loss: 0.000309476891
Iter: 1010 loss: 0.000308921386
Iter: 1011 loss: 0.000308665738
Iter: 1012 loss: 0.000307547394
Iter: 1013 loss: 0.000312436197
Iter: 1014 loss: 0.000307321083
Iter: 1015 loss: 0.000306351285
Iter: 1016 loss: 0.000307444367
Iter: 1017 loss: 0.000305827183
Iter: 1018 loss: 0.000304753135
Iter: 1019 loss: 0.0003077375
Iter: 1020 loss: 0.000304405636
Iter: 1021 loss: 0.00030360595
Iter: 1022 loss: 0.000303605193
Iter: 1023 loss: 0.000302953762
Iter: 1024 loss: 0.00030295609
Iter: 1025 loss: 0.00030243292
Iter: 1026 loss: 0.000301547232
Iter: 1027 loss: 0.000303936307
Iter: 1028 loss: 0.000301254971
Iter: 1029 loss: 0.000300322543
Iter: 1030 loss: 0.000303531182
Iter: 1031 loss: 0.000300072745
Iter: 1032 loss: 0.000299263978
Iter: 1033 loss: 0.00029921814
Iter: 1034 loss: 0.000298601546
Iter: 1035 loss: 0.000297558901
Iter: 1036 loss: 0.000306131056
Iter: 1037 loss: 0.000297494349
Iter: 1038 loss: 0.000296716637
Iter: 1039 loss: 0.00030420674
Iter: 1040 loss: 0.000296686892
Iter: 1041 loss: 0.000296089333
Iter: 1042 loss: 0.000296081533
Iter: 1043 loss: 0.000295608537
Iter: 1044 loss: 0.000294899452
Iter: 1045 loss: 0.000294942409
Iter: 1046 loss: 0.0002943445
Iter: 1047 loss: 0.000293428602
Iter: 1048 loss: 0.000296545186
Iter: 1049 loss: 0.000293181452
Iter: 1050 loss: 0.000292233191
Iter: 1051 loss: 0.000292738201
Iter: 1052 loss: 0.00029160647
Iter: 1053 loss: 0.000290454162
Iter: 1054 loss: 0.000296104641
Iter: 1055 loss: 0.000290255033
Iter: 1056 loss: 0.000289318647
Iter: 1057 loss: 0.000300021784
Iter: 1058 loss: 0.000289302669
Iter: 1059 loss: 0.000288663723
Iter: 1060 loss: 0.0002889756
Iter: 1061 loss: 0.000288235082
Iter: 1062 loss: 0.000287366565
Iter: 1063 loss: 0.000289524556
Iter: 1064 loss: 0.000287060597
Iter: 1065 loss: 0.000286186405
Iter: 1066 loss: 0.000287885661
Iter: 1067 loss: 0.000285822782
Iter: 1068 loss: 0.000285010057
Iter: 1069 loss: 0.000288665498
Iter: 1070 loss: 0.000284852402
Iter: 1071 loss: 0.000284096663
Iter: 1072 loss: 0.000287855975
Iter: 1073 loss: 0.000283967936
Iter: 1074 loss: 0.000283222587
Iter: 1075 loss: 0.000286413851
Iter: 1076 loss: 0.000283068395
Iter: 1077 loss: 0.000282455178
Iter: 1078 loss: 0.000281715969
Iter: 1079 loss: 0.000281644054
Iter: 1080 loss: 0.000280712382
Iter: 1081 loss: 0.000282977911
Iter: 1082 loss: 0.000280375069
Iter: 1083 loss: 0.000279392116
Iter: 1084 loss: 0.000283309433
Iter: 1085 loss: 0.000279170665
Iter: 1086 loss: 0.000278279535
Iter: 1087 loss: 0.000280675973
Iter: 1088 loss: 0.000277983636
Iter: 1089 loss: 0.000277071493
Iter: 1090 loss: 0.000279615168
Iter: 1091 loss: 0.000276779669
Iter: 1092 loss: 0.000275891914
Iter: 1093 loss: 0.000279237633
Iter: 1094 loss: 0.000275678292
Iter: 1095 loss: 0.000274817459
Iter: 1096 loss: 0.00028056
Iter: 1097 loss: 0.000274728547
Iter: 1098 loss: 0.000274122052
Iter: 1099 loss: 0.000273465121
Iter: 1100 loss: 0.000273362239
Iter: 1101 loss: 0.000272443984
Iter: 1102 loss: 0.000279119471
Iter: 1103 loss: 0.000272366218
Iter: 1104 loss: 0.000271594
Iter: 1105 loss: 0.000275948463
Iter: 1106 loss: 0.000271485362
Iter: 1107 loss: 0.000270866585
Iter: 1108 loss: 0.000274898077
Iter: 1109 loss: 0.000270801072
Iter: 1110 loss: 0.000270225952
Iter: 1111 loss: 0.000269707118
Iter: 1112 loss: 0.000269563199
Iter: 1113 loss: 0.000268728647
Iter: 1114 loss: 0.000268687465
Iter: 1115 loss: 0.000268046744
Iter: 1116 loss: 0.00026688
Iter: 1117 loss: 0.000272831094
Iter: 1118 loss: 0.000266689662
Iter: 1119 loss: 0.000265714945
Iter: 1120 loss: 0.000269201846
Iter: 1121 loss: 0.000265465613
Iter: 1122 loss: 0.000264614559
Iter: 1123 loss: 0.000265965558
Iter: 1124 loss: 0.000264219358
Iter: 1125 loss: 0.000263282272
Iter: 1126 loss: 0.000266820833
Iter: 1127 loss: 0.000263057242
Iter: 1128 loss: 0.000262216083
Iter: 1129 loss: 0.000268219621
Iter: 1130 loss: 0.000262141344
Iter: 1131 loss: 0.0002613299
Iter: 1132 loss: 0.000261298788
Iter: 1133 loss: 0.000260671688
Iter: 1134 loss: 0.000259763357
Iter: 1135 loss: 0.00026173552
Iter: 1136 loss: 0.00025940995
Iter: 1137 loss: 0.00025851067
Iter: 1138 loss: 0.000264973496
Iter: 1139 loss: 0.000258433603
Iter: 1140 loss: 0.000257759588
Iter: 1141 loss: 0.000261964218
Iter: 1142 loss: 0.000257681211
Iter: 1143 loss: 0.000256994244
Iter: 1144 loss: 0.000257698703
Iter: 1145 loss: 0.000256610016
Iter: 1146 loss: 0.000255994208
Iter: 1147 loss: 0.000255867257
Iter: 1148 loss: 0.000255461404
Iter: 1149 loss: 0.000254518382
Iter: 1150 loss: 0.00025736721
Iter: 1151 loss: 0.000254235259
Iter: 1152 loss: 0.000253293314
Iter: 1153 loss: 0.000255007064
Iter: 1154 loss: 0.000252885482
Iter: 1155 loss: 0.000251964433
Iter: 1156 loss: 0.000253161532
Iter: 1157 loss: 0.000251493417
Iter: 1158 loss: 0.000250507554
Iter: 1159 loss: 0.000255309336
Iter: 1160 loss: 0.000250335608
Iter: 1161 loss: 0.000249576871
Iter: 1162 loss: 0.000255949883
Iter: 1163 loss: 0.000249533099
Iter: 1164 loss: 0.000248862052
Iter: 1165 loss: 0.000250378507
Iter: 1166 loss: 0.000248609256
Iter: 1167 loss: 0.00024791804
Iter: 1168 loss: 0.000248355325
Iter: 1169 loss: 0.000247476622
Iter: 1170 loss: 0.0002465992
Iter: 1171 loss: 0.000249205565
Iter: 1172 loss: 0.000246333337
Iter: 1173 loss: 0.000245517585
Iter: 1174 loss: 0.000250062614
Iter: 1175 loss: 0.000245402
Iter: 1176 loss: 0.000244710507
Iter: 1177 loss: 0.000251076184
Iter: 1178 loss: 0.000244679453
Iter: 1179 loss: 0.000244227296
Iter: 1180 loss: 0.000243355142
Iter: 1181 loss: 0.000261508743
Iter: 1182 loss: 0.000243349306
Iter: 1183 loss: 0.000242363836
Iter: 1184 loss: 0.000248740136
Iter: 1185 loss: 0.000242257636
Iter: 1186 loss: 0.000241461617
Iter: 1187 loss: 0.000242078124
Iter: 1188 loss: 0.000240978756
Iter: 1189 loss: 0.000240108842
Iter: 1190 loss: 0.000242334921
Iter: 1191 loss: 0.000239809742
Iter: 1192 loss: 0.000239021363
Iter: 1193 loss: 0.000242340058
Iter: 1194 loss: 0.000238854875
Iter: 1195 loss: 0.000238038425
Iter: 1196 loss: 0.000240043257
Iter: 1197 loss: 0.000237747896
Iter: 1198 loss: 0.000236990832
Iter: 1199 loss: 0.000241008762
Iter: 1200 loss: 0.000236873515
Iter: 1201 loss: 0.000236197651
Iter: 1202 loss: 0.000239521862
Iter: 1203 loss: 0.000236082284
Iter: 1204 loss: 0.000235494386
Iter: 1205 loss: 0.000235121741
Iter: 1206 loss: 0.000234890598
Iter: 1207 loss: 0.000234113802
Iter: 1208 loss: 0.000238713226
Iter: 1209 loss: 0.00023401517
Iter: 1210 loss: 0.000233440689
Iter: 1211 loss: 0.00024161319
Iter: 1212 loss: 0.000233439088
Iter: 1213 loss: 0.000232950842
Iter: 1214 loss: 0.000232417427
Iter: 1215 loss: 0.000232336868
Iter: 1216 loss: 0.000231577025
Iter: 1217 loss: 0.000233754021
Iter: 1218 loss: 0.000231338432
Iter: 1219 loss: 0.000230535836
Iter: 1220 loss: 0.000231047161
Iter: 1221 loss: 0.0002300255
Iter: 1222 loss: 0.000229092941
Iter: 1223 loss: 0.000230506586
Iter: 1224 loss: 0.000228648772
Iter: 1225 loss: 0.000227613637
Iter: 1226 loss: 0.000234732172
Iter: 1227 loss: 0.000227514611
Iter: 1228 loss: 0.000226692224
Iter: 1229 loss: 0.000227431476
Iter: 1230 loss: 0.000226214062
Iter: 1231 loss: 0.000225254567
Iter: 1232 loss: 0.000227901342
Iter: 1233 loss: 0.000224942924
Iter: 1234 loss: 0.000224234478
Iter: 1235 loss: 0.000235377694
Iter: 1236 loss: 0.000224233663
Iter: 1237 loss: 0.000223639683
Iter: 1238 loss: 0.000223160925
Iter: 1239 loss: 0.000222983508
Iter: 1240 loss: 0.000222067931
Iter: 1241 loss: 0.000224235293
Iter: 1242 loss: 0.000221734081
Iter: 1243 loss: 0.000221044902
Iter: 1244 loss: 0.00022104349
Iter: 1245 loss: 0.00022045002
Iter: 1246 loss: 0.00022155061
Iter: 1247 loss: 0.000220196293
Iter: 1248 loss: 0.000219635229
Iter: 1249 loss: 0.000219562731
Iter: 1250 loss: 0.000219164431
Iter: 1251 loss: 0.000218404282
Iter: 1252 loss: 0.000219987924
Iter: 1253 loss: 0.000218101355
Iter: 1254 loss: 0.000217288107
Iter: 1255 loss: 0.000218396715
Iter: 1256 loss: 0.000216881992
Iter: 1257 loss: 0.000216115048
Iter: 1258 loss: 0.00022222074
Iter: 1259 loss: 0.000216062414
Iter: 1260 loss: 0.000215425709
Iter: 1261 loss: 0.000215146662
Iter: 1262 loss: 0.000214821441
Iter: 1263 loss: 0.000213939202
Iter: 1264 loss: 0.000217554858
Iter: 1265 loss: 0.000213746171
Iter: 1266 loss: 0.000213056323
Iter: 1267 loss: 0.000218806817
Iter: 1268 loss: 0.000213016057
Iter: 1269 loss: 0.000212370142
Iter: 1270 loss: 0.000213884705
Iter: 1271 loss: 0.000212132916
Iter: 1272 loss: 0.000211491977
Iter: 1273 loss: 0.000212076775
Iter: 1274 loss: 0.000211119419
Iter: 1275 loss: 0.000210278798
Iter: 1276 loss: 0.00021251764
Iter: 1277 loss: 0.000209998499
Iter: 1278 loss: 0.000209395308
Iter: 1279 loss: 0.000209393329
Iter: 1280 loss: 0.000208921745
Iter: 1281 loss: 0.000208526093
Iter: 1282 loss: 0.000208393205
Iter: 1283 loss: 0.000207811827
Iter: 1284 loss: 0.000208301775
Iter: 1285 loss: 0.000207467616
Iter: 1286 loss: 0.000206757148
Iter: 1287 loss: 0.000210218888
Iter: 1288 loss: 0.000206633515
Iter: 1289 loss: 0.000206048644
Iter: 1290 loss: 0.000207152072
Iter: 1291 loss: 0.000205801189
Iter: 1292 loss: 0.000205123797
Iter: 1293 loss: 0.000205968652
Iter: 1294 loss: 0.000204772557
Iter: 1295 loss: 0.000203967094
Iter: 1296 loss: 0.000204756216
Iter: 1297 loss: 0.000203509873
Iter: 1298 loss: 0.000202638417
Iter: 1299 loss: 0.000211006685
Iter: 1300 loss: 0.000202605588
Iter: 1301 loss: 0.000201911273
Iter: 1302 loss: 0.000203513031
Iter: 1303 loss: 0.000201653
Iter: 1304 loss: 0.00020096646
Iter: 1305 loss: 0.000205038
Iter: 1306 loss: 0.000200878174
Iter: 1307 loss: 0.000200301409
Iter: 1308 loss: 0.000200278344
Iter: 1309 loss: 0.000199832401
Iter: 1310 loss: 0.000199243
Iter: 1311 loss: 0.00020559167
Iter: 1312 loss: 0.000199229486
Iter: 1313 loss: 0.000198624228
Iter: 1314 loss: 0.000199818664
Iter: 1315 loss: 0.000198373804
Iter: 1316 loss: 0.000197891233
Iter: 1317 loss: 0.00019731278
Iter: 1318 loss: 0.000197254238
Iter: 1319 loss: 0.00019639259
Iter: 1320 loss: 0.000201762974
Iter: 1321 loss: 0.00019629224
Iter: 1322 loss: 0.000195631
Iter: 1323 loss: 0.000196360881
Iter: 1324 loss: 0.000195270361
Iter: 1325 loss: 0.000194478183
Iter: 1326 loss: 0.000196603884
Iter: 1327 loss: 0.000194216642
Iter: 1328 loss: 0.000193365908
Iter: 1329 loss: 0.000195794564
Iter: 1330 loss: 0.00019309725
Iter: 1331 loss: 0.000192345586
Iter: 1332 loss: 0.000194493157
Iter: 1333 loss: 0.000192109146
Iter: 1334 loss: 0.000191341082
Iter: 1335 loss: 0.000193046653
Iter: 1336 loss: 0.000191046493
Iter: 1337 loss: 0.000190447376
Iter: 1338 loss: 0.000198238646
Iter: 1339 loss: 0.000190443359
Iter: 1340 loss: 0.00018992486
Iter: 1341 loss: 0.000189724393
Iter: 1342 loss: 0.000189442668
Iter: 1343 loss: 0.000188838792
Iter: 1344 loss: 0.000192142295
Iter: 1345 loss: 0.000188751379
Iter: 1346 loss: 0.000188233375
Iter: 1347 loss: 0.000192432679
Iter: 1348 loss: 0.000188200051
Iter: 1349 loss: 0.000187788071
Iter: 1350 loss: 0.000187780039
Iter: 1351 loss: 0.00018745492
Iter: 1352 loss: 0.000186920312
Iter: 1353 loss: 0.000186831094
Iter: 1354 loss: 0.000186463891
Iter: 1355 loss: 0.000185698591
Iter: 1356 loss: 0.000186762627
Iter: 1357 loss: 0.000185319121
Iter: 1358 loss: 0.000184593067
Iter: 1359 loss: 0.0001901918
Iter: 1360 loss: 0.000184539284
Iter: 1361 loss: 0.000183850381
Iter: 1362 loss: 0.000184831748
Iter: 1363 loss: 0.000183511031
Iter: 1364 loss: 0.000182767544
Iter: 1365 loss: 0.000183444339
Iter: 1366 loss: 0.000182335018
Iter: 1367 loss: 0.000181502546
Iter: 1368 loss: 0.000185949219
Iter: 1369 loss: 0.000181375828
Iter: 1370 loss: 0.000180692601
Iter: 1371 loss: 0.000184007775
Iter: 1372 loss: 0.000180572024
Iter: 1373 loss: 0.000179954746
Iter: 1374 loss: 0.000182825126
Iter: 1375 loss: 0.000179839801
Iter: 1376 loss: 0.000179323339
Iter: 1377 loss: 0.000179911105
Iter: 1378 loss: 0.000179046212
Iter: 1379 loss: 0.000178511924
Iter: 1380 loss: 0.000182303193
Iter: 1381 loss: 0.000178464135
Iter: 1382 loss: 0.000178011935
Iter: 1383 loss: 0.000180287723
Iter: 1384 loss: 0.000177937472
Iter: 1385 loss: 0.000177582173
Iter: 1386 loss: 0.000176943169
Iter: 1387 loss: 0.000192583626
Iter: 1388 loss: 0.000176943286
Iter: 1389 loss: 0.000176274159
Iter: 1390 loss: 0.000179084425
Iter: 1391 loss: 0.000176132191
Iter: 1392 loss: 0.000175494642
Iter: 1393 loss: 0.000177648821
Iter: 1394 loss: 0.000175322697
Iter: 1395 loss: 0.000174705521
Iter: 1396 loss: 0.000175782159
Iter: 1397 loss: 0.0001744318
Iter: 1398 loss: 0.000173795488
Iter: 1399 loss: 0.000175565423
Iter: 1400 loss: 0.000173589302
Iter: 1401 loss: 0.000172940432
Iter: 1402 loss: 0.000176497531
Iter: 1403 loss: 0.000172845655
Iter: 1404 loss: 0.000172342494
Iter: 1405 loss: 0.000172385407
Iter: 1406 loss: 0.00017195307
Iter: 1407 loss: 0.000171411433
Iter: 1408 loss: 0.000176941452
Iter: 1409 loss: 0.000171395252
Iter: 1410 loss: 0.000170876621
Iter: 1411 loss: 0.000172072614
Iter: 1412 loss: 0.000170683634
Iter: 1413 loss: 0.000170237734
Iter: 1414 loss: 0.000171200707
Iter: 1415 loss: 0.000170064202
Iter: 1416 loss: 0.000169640538
Iter: 1417 loss: 0.000174704066
Iter: 1418 loss: 0.000169634965
Iter: 1419 loss: 0.000169308565
Iter: 1420 loss: 0.000168975617
Iter: 1421 loss: 0.0001689114
Iter: 1422 loss: 0.000168409722
Iter: 1423 loss: 0.000169100676
Iter: 1424 loss: 0.000168160099
Iter: 1425 loss: 0.000167580059
Iter: 1426 loss: 0.000169178136
Iter: 1427 loss: 0.000167392238
Iter: 1428 loss: 0.000166844751
Iter: 1429 loss: 0.00016756219
Iter: 1430 loss: 0.000166566417
Iter: 1431 loss: 0.00016599847
Iter: 1432 loss: 0.000169342791
Iter: 1433 loss: 0.00016592574
Iter: 1434 loss: 0.000165397025
Iter: 1435 loss: 0.000166588754
Iter: 1436 loss: 0.00016519778
Iter: 1437 loss: 0.000164679921
Iter: 1438 loss: 0.000165608042
Iter: 1439 loss: 0.000164454657
Iter: 1440 loss: 0.000163969758
Iter: 1441 loss: 0.000166222075
Iter: 1442 loss: 0.00016387846
Iter: 1443 loss: 0.000163373945
Iter: 1444 loss: 0.000165178295
Iter: 1445 loss: 0.000163244898
Iter: 1446 loss: 0.000162809185
Iter: 1447 loss: 0.000165053934
Iter: 1448 loss: 0.000162738652
Iter: 1449 loss: 0.00016235892
Iter: 1450 loss: 0.000163019009
Iter: 1451 loss: 0.000162190641
Iter: 1452 loss: 0.000161774893
Iter: 1453 loss: 0.000165136327
Iter: 1454 loss: 0.000161747477
Iter: 1455 loss: 0.000161479868
Iter: 1456 loss: 0.000160972093
Iter: 1457 loss: 0.000171993044
Iter: 1458 loss: 0.000160970172
Iter: 1459 loss: 0.000160394658
Iter: 1460 loss: 0.000162091354
Iter: 1461 loss: 0.000160217838
Iter: 1462 loss: 0.000159652554
Iter: 1463 loss: 0.000162147626
Iter: 1464 loss: 0.000159538977
Iter: 1465 loss: 0.000159026706
Iter: 1466 loss: 0.000160015203
Iter: 1467 loss: 0.00015881224
Iter: 1468 loss: 0.000158274255
Iter: 1469 loss: 0.00015992875
Iter: 1470 loss: 0.000158116076
Iter: 1471 loss: 0.000157644594
Iter: 1472 loss: 0.000158919487
Iter: 1473 loss: 0.000157488801
Iter: 1474 loss: 0.000156986265
Iter: 1475 loss: 0.000159028335
Iter: 1476 loss: 0.000156874696
Iter: 1477 loss: 0.000156432652
Iter: 1478 loss: 0.000157056973
Iter: 1479 loss: 0.000156214577
Iter: 1480 loss: 0.000155798218
Iter: 1481 loss: 0.000160199
Iter: 1482 loss: 0.000155788075
Iter: 1483 loss: 0.000155422487
Iter: 1484 loss: 0.000155700589
Iter: 1485 loss: 0.000155199945
Iter: 1486 loss: 0.000154864916
Iter: 1487 loss: 0.000158928946
Iter: 1488 loss: 0.000154860638
Iter: 1489 loss: 0.000154552137
Iter: 1490 loss: 0.000154063688
Iter: 1491 loss: 0.000154058682
Iter: 1492 loss: 0.000153562287
Iter: 1493 loss: 0.00015521099
Iter: 1494 loss: 0.000153425121
Iter: 1495 loss: 0.000152918277
Iter: 1496 loss: 0.000153764879
Iter: 1497 loss: 0.000152688444
Iter: 1498 loss: 0.000152169232
Iter: 1499 loss: 0.000153315545
Iter: 1500 loss: 0.000151970395
Iter: 1501 loss: 0.00015148356
Iter: 1502 loss: 0.000153390109
Iter: 1503 loss: 0.000151371962
Iter: 1504 loss: 0.000150862485
Iter: 1505 loss: 0.000152254506
Iter: 1506 loss: 0.000150695603
Iter: 1507 loss: 0.000150213484
Iter: 1508 loss: 0.000151369532
Iter: 1509 loss: 0.000150038482
Iter: 1510 loss: 0.000149503758
Iter: 1511 loss: 0.000150433028
Iter: 1512 loss: 0.000149267056
Iter: 1513 loss: 0.000148759485
Iter: 1514 loss: 0.000153206
Iter: 1515 loss: 0.000148732361
Iter: 1516 loss: 0.000148305262
Iter: 1517 loss: 0.000149579952
Iter: 1518 loss: 0.000148174367
Iter: 1519 loss: 0.000147798593
Iter: 1520 loss: 0.00014984372
Iter: 1521 loss: 0.000147742612
Iter: 1522 loss: 0.000147397805
Iter: 1523 loss: 0.000148208492
Iter: 1524 loss: 0.000147272105
Iter: 1525 loss: 0.000146921826
Iter: 1526 loss: 0.000146937644
Iter: 1527 loss: 0.00014664662
Iter: 1528 loss: 0.000146193328
Iter: 1529 loss: 0.000146278049
Iter: 1530 loss: 0.000145854807
Iter: 1531 loss: 0.000145266415
Iter: 1532 loss: 0.000146150298
Iter: 1533 loss: 0.000144985592
Iter: 1534 loss: 0.000144369682
Iter: 1535 loss: 0.000149319807
Iter: 1536 loss: 0.000144329359
Iter: 1537 loss: 0.000143842917
Iter: 1538 loss: 0.000145042548
Iter: 1539 loss: 0.000143669458
Iter: 1540 loss: 0.000143191923
Iter: 1541 loss: 0.000143585436
Iter: 1542 loss: 0.000142906443
Iter: 1543 loss: 0.000142412915
Iter: 1544 loss: 0.000146656035
Iter: 1545 loss: 0.00014238627
Iter: 1546 loss: 0.000141992787
Iter: 1547 loss: 0.000142796
Iter: 1548 loss: 0.000141834054
Iter: 1549 loss: 0.000141468627
Iter: 1550 loss: 0.000144180827
Iter: 1551 loss: 0.000141439188
Iter: 1552 loss: 0.000141123252
Iter: 1553 loss: 0.000142004166
Iter: 1554 loss: 0.000141021286
Iter: 1555 loss: 0.000140710705
Iter: 1556 loss: 0.000142032673
Iter: 1557 loss: 0.000140645905
Iter: 1558 loss: 0.000140343167
Iter: 1559 loss: 0.000140567034
Iter: 1560 loss: 0.000140157281
Iter: 1561 loss: 0.00013984929
Iter: 1562 loss: 0.000139810727
Iter: 1563 loss: 0.00013959143
Iter: 1564 loss: 0.000139150332
Iter: 1565 loss: 0.000140536111
Iter: 1566 loss: 0.000139022042
Iter: 1567 loss: 0.000138594653
Iter: 1568 loss: 0.000139686104
Iter: 1569 loss: 0.000138446645
Iter: 1570 loss: 0.000138009957
Iter: 1571 loss: 0.000138382573
Iter: 1572 loss: 0.000137751573
Iter: 1573 loss: 0.000137285097
Iter: 1574 loss: 0.000139808617
Iter: 1575 loss: 0.000137214811
Iter: 1576 loss: 0.000136815652
Iter: 1577 loss: 0.000138832576
Iter: 1578 loss: 0.000136748815
Iter: 1579 loss: 0.000136381452
Iter: 1580 loss: 0.000136445597
Iter: 1581 loss: 0.000136104878
Iter: 1582 loss: 0.000135704511
Iter: 1583 loss: 0.00013940761
Iter: 1584 loss: 0.000135687267
Iter: 1585 loss: 0.000135320079
Iter: 1586 loss: 0.000136583491
Iter: 1587 loss: 0.000135223032
Iter: 1588 loss: 0.000134922229
Iter: 1589 loss: 0.000136743809
Iter: 1590 loss: 0.000134885224
Iter: 1591 loss: 0.000134598929
Iter: 1592 loss: 0.00013466258
Iter: 1593 loss: 0.000134387927
Iter: 1594 loss: 0.000133993075
Iter: 1595 loss: 0.000134593909
Iter: 1596 loss: 0.000133805312
Iter: 1597 loss: 0.000133456691
Iter: 1598 loss: 0.000133902882
Iter: 1599 loss: 0.000133277281
Iter: 1600 loss: 0.000132872228
Iter: 1601 loss: 0.000133383044
Iter: 1602 loss: 0.000132663787
Iter: 1603 loss: 0.000132184301
Iter: 1604 loss: 0.000133576861
Iter: 1605 loss: 0.00013203593
Iter: 1606 loss: 0.000131540291
Iter: 1607 loss: 0.000133216206
Iter: 1608 loss: 0.000131406588
Iter: 1609 loss: 0.000130926754
Iter: 1610 loss: 0.000132277215
Iter: 1611 loss: 0.00013077457
Iter: 1612 loss: 0.000130342218
Iter: 1613 loss: 0.000131278008
Iter: 1614 loss: 0.00013017407
Iter: 1615 loss: 0.000129766544
Iter: 1616 loss: 0.000132369882
Iter: 1617 loss: 0.000129721753
Iter: 1618 loss: 0.000129358406
Iter: 1619 loss: 0.000130497516
Iter: 1620 loss: 0.000129252352
Iter: 1621 loss: 0.000128950313
Iter: 1622 loss: 0.000131962588
Iter: 1623 loss: 0.000128940766
Iter: 1624 loss: 0.000128711923
Iter: 1625 loss: 0.000128849802
Iter: 1626 loss: 0.000128564818
Iter: 1627 loss: 0.000128239612
Iter: 1628 loss: 0.000128904794
Iter: 1629 loss: 0.000128108746
Iter: 1630 loss: 0.000127825013
Iter: 1631 loss: 0.000127590698
Iter: 1632 loss: 0.0001275088
Iter: 1633 loss: 0.000127096151
Iter: 1634 loss: 0.000130075816
Iter: 1635 loss: 0.000127060339
Iter: 1636 loss: 0.000126696876
Iter: 1637 loss: 0.000126785555
Iter: 1638 loss: 0.000126431521
Iter: 1639 loss: 0.000125984545
Iter: 1640 loss: 0.000127159845
Iter: 1641 loss: 0.000125832827
Iter: 1642 loss: 0.000125377803
Iter: 1643 loss: 0.000126701212
Iter: 1644 loss: 0.000125237479
Iter: 1645 loss: 0.000124805723
Iter: 1646 loss: 0.000126443862
Iter: 1647 loss: 0.000124702681
Iter: 1648 loss: 0.000124274273
Iter: 1649 loss: 0.000125700841
Iter: 1650 loss: 0.000124157727
Iter: 1651 loss: 0.000123784979
Iter: 1652 loss: 0.000125258914
Iter: 1653 loss: 0.000123699836
Iter: 1654 loss: 0.000123397302
Iter: 1655 loss: 0.00012614041
Iter: 1656 loss: 0.000123383768
Iter: 1657 loss: 0.000123113336
Iter: 1658 loss: 0.000123450678
Iter: 1659 loss: 0.00012297272
Iter: 1660 loss: 0.000122678772
Iter: 1661 loss: 0.000123999
Iter: 1662 loss: 0.000122621583
Iter: 1663 loss: 0.000122383819
Iter: 1664 loss: 0.000122199592
Iter: 1665 loss: 0.000122124213
Iter: 1666 loss: 0.000121741192
Iter: 1667 loss: 0.00012333937
Iter: 1668 loss: 0.000121659075
Iter: 1669 loss: 0.000121338904
Iter: 1670 loss: 0.000121449979
Iter: 1671 loss: 0.000121113524
Iter: 1672 loss: 0.000120730387
Iter: 1673 loss: 0.000122079742
Iter: 1674 loss: 0.000120631645
Iter: 1675 loss: 0.000120249424
Iter: 1676 loss: 0.000122111174
Iter: 1677 loss: 0.000120183613
Iter: 1678 loss: 0.000119871947
Iter: 1679 loss: 0.000119798722
Iter: 1680 loss: 0.00011959867
Iter: 1681 loss: 0.000119170843
Iter: 1682 loss: 0.000121203921
Iter: 1683 loss: 0.000119094228
Iter: 1684 loss: 0.000118720898
Iter: 1685 loss: 0.000120748649
Iter: 1686 loss: 0.000118665201
Iter: 1687 loss: 0.000118356664
Iter: 1688 loss: 0.000119846438
Iter: 1689 loss: 0.000118301803
Iter: 1690 loss: 0.000118054508
Iter: 1691 loss: 0.000120166405
Iter: 1692 loss: 0.000118040472
Iter: 1693 loss: 0.000117836047
Iter: 1694 loss: 0.000117774296
Iter: 1695 loss: 0.000117652955
Iter: 1696 loss: 0.000117318959
Iter: 1697 loss: 0.000118281234
Iter: 1698 loss: 0.00011721487
Iter: 1699 loss: 0.000116940551
Iter: 1700 loss: 0.000117092037
Iter: 1701 loss: 0.000116760726
Iter: 1702 loss: 0.0001164157
Iter: 1703 loss: 0.000116942923
Iter: 1704 loss: 0.000116252028
Iter: 1705 loss: 0.000115910705
Iter: 1706 loss: 0.000117788
Iter: 1707 loss: 0.000115861323
Iter: 1708 loss: 0.000115556926
Iter: 1709 loss: 0.000115832008
Iter: 1710 loss: 0.000115378876
Iter: 1711 loss: 0.00011502875
Iter: 1712 loss: 0.000115636991
Iter: 1713 loss: 0.000114873787
Iter: 1714 loss: 0.00011453218
Iter: 1715 loss: 0.000116789401
Iter: 1716 loss: 0.000114496943
Iter: 1717 loss: 0.000114175891
Iter: 1718 loss: 0.000114453054
Iter: 1719 loss: 0.000113986287
Iter: 1720 loss: 0.000113650647
Iter: 1721 loss: 0.000114686562
Iter: 1722 loss: 0.000113552313
Iter: 1723 loss: 0.000113233917
Iter: 1724 loss: 0.000117411764
Iter: 1725 loss: 0.000113231341
Iter: 1726 loss: 0.000113003145
Iter: 1727 loss: 0.000113475449
Iter: 1728 loss: 0.000112911323
Iter: 1729 loss: 0.000112666035
Iter: 1730 loss: 0.000113036862
Iter: 1731 loss: 0.00011255018
Iter: 1732 loss: 0.000112299189
Iter: 1733 loss: 0.000112698253
Iter: 1734 loss: 0.000112182352
Iter: 1735 loss: 0.000111880727
Iter: 1736 loss: 0.000112168811
Iter: 1737 loss: 0.000111708563
Iter: 1738 loss: 0.000111395537
Iter: 1739 loss: 0.000111663183
Iter: 1740 loss: 0.000111210422
Iter: 1741 loss: 0.000110886467
Iter: 1742 loss: 0.000113769391
Iter: 1743 loss: 0.000110871093
Iter: 1744 loss: 0.00011061901
Iter: 1745 loss: 0.000110885405
Iter: 1746 loss: 0.000110480643
Iter: 1747 loss: 0.0001101944
Iter: 1748 loss: 0.000110823858
Iter: 1749 loss: 0.000110084889
Iter: 1750 loss: 0.000109759087
Iter: 1751 loss: 0.00011027608
Iter: 1752 loss: 0.000109607106
Iter: 1753 loss: 0.000109266271
Iter: 1754 loss: 0.000110756308
Iter: 1755 loss: 0.00010919811
Iter: 1756 loss: 0.000108918553
Iter: 1757 loss: 0.000111227972
Iter: 1758 loss: 0.000108902219
Iter: 1759 loss: 0.000108695269
Iter: 1760 loss: 0.000110244371
Iter: 1761 loss: 0.000108679247
Iter: 1762 loss: 0.000108497516
Iter: 1763 loss: 0.000108388645
Iter: 1764 loss: 0.000108314176
Iter: 1765 loss: 0.000108066895
Iter: 1766 loss: 0.000109718472
Iter: 1767 loss: 0.000108041815
Iter: 1768 loss: 0.000107851294
Iter: 1769 loss: 0.000107698834
Iter: 1770 loss: 0.000107641179
Iter: 1771 loss: 0.000107356973
Iter: 1772 loss: 0.000108765955
Iter: 1773 loss: 0.000107308508
Iter: 1774 loss: 0.000107045053
Iter: 1775 loss: 0.000107312328
Iter: 1776 loss: 0.000106897045
Iter: 1777 loss: 0.000106581436
Iter: 1778 loss: 0.000106912601
Iter: 1779 loss: 0.000106407
Iter: 1780 loss: 0.000106081861
Iter: 1781 loss: 0.000108803404
Iter: 1782 loss: 0.000106062835
Iter: 1783 loss: 0.000105778134
Iter: 1784 loss: 0.000105882114
Iter: 1785 loss: 0.000105578532
Iter: 1786 loss: 0.000105257655
Iter: 1787 loss: 0.000105777093
Iter: 1788 loss: 0.000105109953
Iter: 1789 loss: 0.000104808867
Iter: 1790 loss: 0.000107487234
Iter: 1791 loss: 0.000104793631
Iter: 1792 loss: 0.000104559724
Iter: 1793 loss: 0.000106805404
Iter: 1794 loss: 0.000104550621
Iter: 1795 loss: 0.00010436005
Iter: 1796 loss: 0.000104696643
Iter: 1797 loss: 0.000104276645
Iter: 1798 loss: 0.000104094666
Iter: 1799 loss: 0.000104369203
Iter: 1800 loss: 0.000104007711
Iter: 1801 loss: 0.000103788348
Iter: 1802 loss: 0.000104145816
Iter: 1803 loss: 0.000103687336
Iter: 1804 loss: 0.000103457
Iter: 1805 loss: 0.000103764847
Iter: 1806 loss: 0.000103340673
Iter: 1807 loss: 0.000103083192
Iter: 1808 loss: 0.000103578284
Iter: 1809 loss: 0.000102974969
Iter: 1810 loss: 0.000102700629
Iter: 1811 loss: 0.00010341945
Iter: 1812 loss: 0.000102606951
Iter: 1813 loss: 0.000102312755
Iter: 1814 loss: 0.000103092039
Iter: 1815 loss: 0.000102214028
Iter: 1816 loss: 0.000101939993
Iter: 1817 loss: 0.000102349906
Iter: 1818 loss: 0.000101809303
Iter: 1819 loss: 0.000101505699
Iter: 1820 loss: 0.000102817772
Iter: 1821 loss: 0.000101443031
Iter: 1822 loss: 0.000101150035
Iter: 1823 loss: 0.00010193068
Iter: 1824 loss: 0.000101051352
Iter: 1825 loss: 0.000100775738
Iter: 1826 loss: 0.000101071033
Iter: 1827 loss: 0.000100624078
Iter: 1828 loss: 0.000100448102
Iter: 1829 loss: 0.000100413214
Iter: 1830 loss: 0.000100286125
Iter: 1831 loss: 0.000100054123
Iter: 1832 loss: 0.000105539475
Iter: 1833 loss: 0.000100053956
Iter: 1834 loss: 9.97909083e-05
Iter: 1835 loss: 0.000102163802
Iter: 1836 loss: 9.97785683e-05
Iter: 1837 loss: 9.95923183e-05
Iter: 1838 loss: 9.93897847e-05
Iter: 1839 loss: 9.93586436e-05
Iter: 1840 loss: 9.90628323e-05
Iter: 1841 loss: 0.000100973528
Iter: 1842 loss: 9.90307963e-05
Iter: 1843 loss: 9.87753883e-05
Iter: 1844 loss: 9.89547771e-05
Iter: 1845 loss: 9.86164086e-05
Iter: 1846 loss: 9.8333061e-05
Iter: 1847 loss: 9.91862325e-05
Iter: 1848 loss: 9.82476486e-05
Iter: 1849 loss: 9.79711403e-05
Iter: 1850 loss: 9.89567197e-05
Iter: 1851 loss: 9.79001707e-05
Iter: 1852 loss: 9.76551528e-05
Iter: 1853 loss: 9.83967038e-05
Iter: 1854 loss: 9.75815565e-05
Iter: 1855 loss: 9.73155329e-05
Iter: 1856 loss: 9.76371084e-05
Iter: 1857 loss: 9.71754e-05
Iter: 1858 loss: 9.68812165e-05
Iter: 1859 loss: 9.75356961e-05
Iter: 1860 loss: 9.67691303e-05
Iter: 1861 loss: 9.65106155e-05
Iter: 1862 loss: 9.97898751e-05
Iter: 1863 loss: 9.65086801e-05
Iter: 1864 loss: 9.62922713e-05
Iter: 1865 loss: 9.73116476e-05
Iter: 1866 loss: 9.62528648e-05
Iter: 1867 loss: 9.60901962e-05
Iter: 1868 loss: 9.60807811e-05
Iter: 1869 loss: 9.59570461e-05
Iter: 1870 loss: 9.5772135e-05
Iter: 1871 loss: 9.66556472e-05
Iter: 1872 loss: 9.57388111e-05
Iter: 1873 loss: 9.55407595e-05
Iter: 1874 loss: 9.56438e-05
Iter: 1875 loss: 9.54096904e-05
Iter: 1876 loss: 9.51948e-05
Iter: 1877 loss: 9.51610418e-05
Iter: 1878 loss: 9.50121394e-05
Iter: 1879 loss: 9.47336957e-05
Iter: 1880 loss: 9.7013246e-05
Iter: 1881 loss: 9.47164299e-05
Iter: 1882 loss: 9.44632e-05
Iter: 1883 loss: 9.48301167e-05
Iter: 1884 loss: 9.43400082e-05
Iter: 1885 loss: 9.40754762e-05
Iter: 1886 loss: 9.41345352e-05
Iter: 1887 loss: 9.3879833e-05
Iter: 1888 loss: 9.36131255e-05
Iter: 1889 loss: 9.65321378e-05
Iter: 1890 loss: 9.36075e-05
Iter: 1891 loss: 9.33666161e-05
Iter: 1892 loss: 9.34883647e-05
Iter: 1893 loss: 9.32060066e-05
Iter: 1894 loss: 9.2938375e-05
Iter: 1895 loss: 9.39926831e-05
Iter: 1896 loss: 9.28777517e-05
Iter: 1897 loss: 9.26767898e-05
Iter: 1898 loss: 9.26768553e-05
Iter: 1899 loss: 9.25056374e-05
Iter: 1900 loss: 9.24089254e-05
Iter: 1901 loss: 9.23344414e-05
Iter: 1902 loss: 9.21295068e-05
Iter: 1903 loss: 9.27404908e-05
Iter: 1904 loss: 9.20666134e-05
Iter: 1905 loss: 9.18612932e-05
Iter: 1906 loss: 9.2679511e-05
Iter: 1907 loss: 9.18145233e-05
Iter: 1908 loss: 9.1638969e-05
Iter: 1909 loss: 9.166362e-05
Iter: 1910 loss: 9.15059645e-05
Iter: 1911 loss: 9.1261827e-05
Iter: 1912 loss: 9.17906582e-05
Iter: 1913 loss: 9.11665047e-05
Iter: 1914 loss: 9.09106893e-05
Iter: 1915 loss: 9.13703e-05
Iter: 1916 loss: 9.07995272e-05
Iter: 1917 loss: 9.05671768e-05
Iter: 1918 loss: 9.2238246e-05
Iter: 1919 loss: 9.05472116e-05
Iter: 1920 loss: 9.03359614e-05
Iter: 1921 loss: 9.05668348e-05
Iter: 1922 loss: 9.02203246e-05
Iter: 1923 loss: 8.99788283e-05
Iter: 1924 loss: 9.0213558e-05
Iter: 1925 loss: 8.9841531e-05
Iter: 1926 loss: 8.96168494e-05
Iter: 1927 loss: 9.10275121e-05
Iter: 1928 loss: 8.95902558e-05
Iter: 1929 loss: 8.93777324e-05
Iter: 1930 loss: 9.04705739e-05
Iter: 1931 loss: 8.93433607e-05
Iter: 1932 loss: 8.92038079e-05
Iter: 1933 loss: 9.12306568e-05
Iter: 1934 loss: 8.92038079e-05
Iter: 1935 loss: 8.90960218e-05
Iter: 1936 loss: 8.88446812e-05
Iter: 1937 loss: 9.18745936e-05
Iter: 1938 loss: 8.88239156e-05
Iter: 1939 loss: 8.86450143e-05
Iter: 1940 loss: 8.86443668e-05
Iter: 1941 loss: 8.84871915e-05
Iter: 1942 loss: 8.85116897e-05
Iter: 1943 loss: 8.83683751e-05
Iter: 1944 loss: 8.81924279e-05
Iter: 1945 loss: 8.83750181e-05
Iter: 1946 loss: 8.80945081e-05
Iter: 1947 loss: 8.78674618e-05
Iter: 1948 loss: 8.86106573e-05
Iter: 1949 loss: 8.78043211e-05
Iter: 1950 loss: 8.75961268e-05
Iter: 1951 loss: 8.79939762e-05
Iter: 1952 loss: 8.75079277e-05
Iter: 1953 loss: 8.72772507e-05
Iter: 1954 loss: 8.8155175e-05
Iter: 1955 loss: 8.72222881e-05
Iter: 1956 loss: 8.70065e-05
Iter: 1957 loss: 8.75289479e-05
Iter: 1958 loss: 8.69289142e-05
Iter: 1959 loss: 8.67058698e-05
Iter: 1960 loss: 8.69507639e-05
Iter: 1961 loss: 8.65844486e-05
Iter: 1962 loss: 8.633158e-05
Iter: 1963 loss: 8.72862438e-05
Iter: 1964 loss: 8.62700399e-05
Iter: 1965 loss: 8.61140288e-05
Iter: 1966 loss: 8.61116278e-05
Iter: 1967 loss: 8.59660213e-05
Iter: 1968 loss: 8.59814681e-05
Iter: 1969 loss: 8.58542044e-05
Iter: 1970 loss: 8.56894039e-05
Iter: 1971 loss: 8.58983694e-05
Iter: 1972 loss: 8.56040861e-05
Iter: 1973 loss: 8.54034879e-05
Iter: 1974 loss: 8.56850311e-05
Iter: 1975 loss: 8.53047241e-05
Iter: 1976 loss: 8.50939396e-05
Iter: 1977 loss: 8.68746e-05
Iter: 1978 loss: 8.50821089e-05
Iter: 1979 loss: 8.49353091e-05
Iter: 1980 loss: 8.47075e-05
Iter: 1981 loss: 8.47041811e-05
Iter: 1982 loss: 8.44570313e-05
Iter: 1983 loss: 8.53352249e-05
Iter: 1984 loss: 8.43932648e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ exit 1
