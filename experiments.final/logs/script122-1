+ RUN=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ LAYERS=300_100_100_100_1
+ case $RUN in
+ PSI='-2 -1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 800 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output120
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output122
+ for fn in f1
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.final/output120/f1_psi0_phi0
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0
+ date
Sun Nov  8 13:46:06 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b122e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b13d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b1227b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b170ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b0f9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b0d59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b0396a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b039d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b0d5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b013d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1af8e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1afbf730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1afbfd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1b0392f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1aec3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1aec3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1aefe620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1aefeae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e1aec38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04436598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04436268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04413c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04468a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e044728c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04472840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e044bff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04342a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e043157b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e04315510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e042e7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e042e4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e0436bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e0436bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e043c08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e041fb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e041fef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 1.9979768
test_loss: 2.0077884
train_loss: 1.9997278
test_loss: 2.0016716
train_loss: 1.9991816
test_loss: 2.0191407
train_loss: 0.38922876
test_loss: 0.38641176
train_loss: 0.37989712
test_loss: 0.38479304
train_loss: 0.38366455
test_loss: 0.38292664
train_loss: 0.3837428
test_loss: 0.380872
train_loss: 0.38521048
test_loss: 0.37858742
train_loss: 0.37760538
test_loss: 0.3761154
train_loss: 0.37242517
test_loss: 0.3734083
train_loss: 0.37131292
test_loss: 0.37057772
train_loss: 0.37158352
test_loss: 0.36752892
train_loss: 0.3629381
test_loss: 0.36423773
train_loss: 0.35437715
test_loss: 0.36069843
train_loss: 0.35828757
test_loss: 0.35697427
train_loss: 0.35007122
test_loss: 0.3529337
train_loss: 0.34369022
test_loss: 0.3487389
train_loss: 0.34249565
test_loss: 0.34428036
train_loss: 0.34080863
test_loss: 0.3394831
train_loss: 0.33409518
test_loss: 0.33444342
train_loss: 0.3263409
test_loss: 0.32903516
train_loss: 0.32187474
test_loss: 0.32336202
train_loss: 0.30718747
test_loss: 0.3173635
train_loss: 0.31490296
test_loss: 0.31110486
train_loss: 0.30404532
test_loss: 0.3046582
train_loss: 0.3035621
test_loss: 0.2981668
train_loss: 0.29531407
test_loss: 0.2917109
train_loss: 0.28227922
test_loss: 0.2854349
train_loss: 0.28115022
test_loss: 0.27912685
train_loss: 0.26904166
test_loss: 0.2727214
train_loss: 0.26890945
test_loss: 0.2661039
train_loss: 0.25565183
test_loss: 0.25922808
train_loss: 0.2540871
test_loss: 0.25208718
train_loss: 0.24411157
test_loss: 0.24464472
train_loss: 0.23604621
test_loss: 0.23686376
train_loss: 0.23003727
test_loss: 0.22877668
train_loss: 0.22188494
test_loss: 0.2203403
train_loss: 0.20615037
test_loss: 0.2115819
train_loss: 0.20372625
test_loss: 0.20245172
train_loss: 0.19295165
test_loss: 0.19294159
train_loss: 0.18424915
test_loss: 0.18307099
train_loss: 0.1698882
test_loss: 0.17287114
train_loss: 0.16284549
test_loss: 0.16231553
train_loss: 0.14915365
test_loss: 0.15164547
train_loss: 0.13659553
test_loss: 0.14121309
train_loss: 0.12845616
test_loss: 0.13128103
train_loss: 0.120695576
test_loss: 0.12192823
train_loss: 0.11338334
test_loss: 0.11321061
train_loss: 0.10540347
test_loss: 0.10522411
train_loss: 0.09683629
test_loss: 0.097996816
train_loss: 0.088374116
test_loss: 0.091564186
train_loss: 0.08450627
test_loss: 0.08608251
train_loss: 0.08412985
test_loss: 0.08159975
train_loss: 0.078665525
test_loss: 0.07815726
train_loss: 0.075947925
test_loss: 0.07559704
train_loss: 0.07375954
test_loss: 0.07379009
train_loss: 0.07346541
test_loss: 0.07255107
train_loss: 0.07115169
test_loss: 0.07168498
train_loss: 0.07179248
test_loss: 0.07109428
train_loss: 0.072028436
test_loss: 0.07069668
train_loss: 0.069473565
test_loss: 0.070375815
train_loss: 0.07008551
test_loss: 0.070136786
train_loss: 0.068046376
test_loss: 0.069946304
train_loss: 0.07000386
test_loss: 0.069782384
train_loss: 0.06768571
test_loss: 0.069634795
train_loss: 0.06953274
test_loss: 0.06950281
train_loss: 0.0699946
test_loss: 0.06937802
train_loss: 0.06838023
test_loss: 0.069250025
train_loss: 0.06768218
test_loss: 0.069127314
train_loss: 0.06945533
test_loss: 0.06902274
train_loss: 0.07232504
test_loss: 0.06891142
train_loss: 0.06902215
test_loss: 0.06878825
train_loss: 0.06754342
test_loss: 0.068663254
train_loss: 0.06909302
test_loss: 0.068546936
train_loss: 0.06887928
test_loss: 0.06839253
train_loss: 0.06943627
test_loss: 0.06826499
train_loss: 0.06763018
test_loss: 0.06810359
train_loss: 0.06632832
test_loss: 0.0679466
train_loss: 0.06726606
test_loss: 0.06776292
train_loss: 0.06656764
test_loss: 0.067549184
train_loss: 0.068852276
test_loss: 0.067322835
train_loss: 0.06825124
test_loss: 0.06707538
train_loss: 0.06664454
test_loss: 0.06680478
train_loss: 0.06552625
test_loss: 0.06653273
train_loss: 0.06587339
test_loss: 0.06622635
train_loss: 0.06640463
test_loss: 0.06591646
train_loss: 0.06597269
test_loss: 0.06555131
train_loss: 0.06567447
test_loss: 0.06525414
train_loss: 0.06486565
test_loss: 0.064905114
train_loss: 0.06482702
test_loss: 0.06455556
train_loss: 0.06527038
test_loss: 0.06424411
train_loss: 0.06389486
test_loss: 0.063876875
train_loss: 0.06454322
test_loss: 0.06351072
train_loss: 0.06331902
test_loss: 0.06312846
train_loss: 0.062444896
test_loss: 0.06273356
train_loss: 0.061312966
test_loss: 0.062267624
train_loss: 0.06211281
test_loss: 0.061816312
train_loss: 0.05962605
test_loss: 0.061316658
train_loss: 0.061433308
test_loss: 0.060674876
train_loss: 0.05963067
test_loss: 0.060031615
train_loss: 0.058408998
test_loss: 0.059331916
train_loss: 0.05838377
test_loss: 0.05861443
train_loss: 0.057346858
test_loss: 0.05791582
train_loss: 0.0570759
test_loss: 0.05708584
train_loss: 0.05670557
test_loss: 0.056358147
train_loss: 0.055448525
test_loss: 0.055662554
train_loss: 0.055421043
test_loss: 0.05484494
train_loss: 0.05361884
test_loss: 0.054040466
train_loss: 0.053135052
test_loss: 0.05302862
train_loss: 0.05258199
test_loss: 0.052132428
train_loss: 0.050805673
test_loss: 0.051024184
train_loss: 0.050777614
test_loss: 0.04964551
train_loss: 0.049151443
test_loss: 0.04792135
train_loss: 0.04597447
test_loss: 0.04539187
train_loss: 0.041651122
test_loss: 0.042245924
train_loss: 0.040918533
test_loss: 0.039379656
train_loss: 0.0377145
test_loss: 0.037551533
train_loss: 0.035373643
test_loss: 0.036387544
train_loss: 0.03570865
test_loss: 0.035556078
train_loss: 0.035763845
test_loss: 0.0351549
train_loss: 0.033421196
test_loss: 0.034653235
train_loss: 0.033715226
test_loss: 0.034625355
train_loss: 0.03420014
test_loss: 0.034340266
train_loss: 0.034193777
test_loss: 0.034322366
train_loss: 0.03362307
test_loss: 0.034146182
train_loss: 0.034439743
test_loss: 0.034163672
train_loss: 0.03390279
test_loss: 0.034150757
train_loss: 0.034698788
test_loss: 0.033956755
train_loss: 0.034842208
test_loss: 0.034100365
train_loss: 0.034345947
test_loss: 0.034061547
train_loss: 0.03382743
test_loss: 0.03392689
train_loss: 0.032622978
test_loss: 0.033975232
train_loss: 0.03462892
test_loss: 0.033831056
train_loss: 0.034095157
test_loss: 0.03378155
train_loss: 0.0344683
test_loss: 0.033801075
train_loss: 0.034354262
test_loss: 0.033953443
train_loss: 0.032922473
test_loss: 0.033895597
train_loss: 0.0324737
test_loss: 0.033632066
train_loss: 0.034364913
test_loss: 0.033586994
train_loss: 0.03336878
test_loss: 0.033587307
train_loss: 0.032911055
test_loss: 0.03346479
train_loss: 0.034232643
test_loss: 0.033461854
train_loss: 0.032819334
test_loss: 0.033423606
train_loss: 0.03233289
test_loss: 0.03315889
train_loss: 0.032172363
test_loss: 0.03320313
train_loss: 0.03180504
test_loss: 0.03303794
train_loss: 0.031827793
test_loss: 0.033048898
train_loss: 0.03163982
test_loss: 0.032762714
train_loss: 0.03222751
test_loss: 0.032445636
train_loss: 0.03245959
test_loss: 0.032551616
train_loss: 0.031543683
test_loss: 0.032154176
train_loss: 0.032448094
test_loss: 0.031917904
train_loss: 0.031198796
test_loss: 0.031644437
train_loss: 0.030547794
test_loss: 0.031456668
train_loss: 0.030444156
test_loss: 0.03122278
train_loss: 0.030002668
test_loss: 0.031083131
train_loss: 0.031149868
test_loss: 0.031051124
train_loss: 0.031144524
test_loss: 0.031226642
train_loss: 0.029536571
test_loss: 0.030888177
train_loss: 0.029889945
test_loss: 0.030840486
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3fdc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3fdcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3ff4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca4027c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3f521e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3f52bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3f62730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3edbc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3e8f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3e8ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3e8f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3e246a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3e247b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3e24bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3db4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3d5bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3d5f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3d5fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3d2dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3d2dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdca3cf41e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a8387b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a7e26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a803f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a7a3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a7c0ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a7c0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a7c0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a710488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a710730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc6a710a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc447fcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc447fc6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc447fc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc4470c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdc44736950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.00184248399
Iter: 2 loss: 0.00226595439
Iter: 3 loss: 0.00181859126
Iter: 4 loss: 0.00179291691
Iter: 5 loss: 0.00187986623
Iter: 6 loss: 0.00178627728
Iter: 7 loss: 0.00176627259
Iter: 8 loss: 0.00174691214
Iter: 9 loss: 0.00174244936
Iter: 10 loss: 0.0017287056
Iter: 11 loss: 0.00182097242
Iter: 12 loss: 0.00172728603
Iter: 13 loss: 0.00171672564
Iter: 14 loss: 0.00171608524
Iter: 15 loss: 0.00170804828
Iter: 16 loss: 0.0016991524
Iter: 17 loss: 0.00178381498
Iter: 18 loss: 0.00169875531
Iter: 19 loss: 0.00169349252
Iter: 20 loss: 0.0016864707
Iter: 21 loss: 0.00168606662
Iter: 22 loss: 0.0016776647
Iter: 23 loss: 0.00170009548
Iter: 24 loss: 0.00167473755
Iter: 25 loss: 0.0016681815
Iter: 26 loss: 0.00171748258
Iter: 27 loss: 0.00166764518
Iter: 28 loss: 0.00166160928
Iter: 29 loss: 0.00168169918
Iter: 30 loss: 0.00165991299
Iter: 31 loss: 0.00165510317
Iter: 32 loss: 0.00165707443
Iter: 33 loss: 0.00165175018
Iter: 34 loss: 0.00164802303
Iter: 35 loss: 0.00169116352
Iter: 36 loss: 0.00164798531
Iter: 37 loss: 0.00164402323
Iter: 38 loss: 0.00164768845
Iter: 39 loss: 0.00164170773
Iter: 40 loss: 0.00163873634
Iter: 41 loss: 0.00164445047
Iter: 42 loss: 0.00163748604
Iter: 43 loss: 0.00163351675
Iter: 44 loss: 0.00163366145
Iter: 45 loss: 0.00163037598
Iter: 46 loss: 0.00162698538
Iter: 47 loss: 0.0016269628
Iter: 48 loss: 0.00162451132
Iter: 49 loss: 0.00162081642
Iter: 50 loss: 0.0016207411
Iter: 51 loss: 0.00161712605
Iter: 52 loss: 0.00164396677
Iter: 53 loss: 0.00161683222
Iter: 54 loss: 0.00161300006
Iter: 55 loss: 0.00161744864
Iter: 56 loss: 0.00161093846
Iter: 57 loss: 0.00160754332
Iter: 58 loss: 0.00160492701
Iter: 59 loss: 0.00160383619
Iter: 60 loss: 0.00159925176
Iter: 61 loss: 0.00161090284
Iter: 62 loss: 0.00159763626
Iter: 63 loss: 0.00159214903
Iter: 64 loss: 0.00160935789
Iter: 65 loss: 0.00159051979
Iter: 66 loss: 0.00158879044
Iter: 67 loss: 0.00158822746
Iter: 68 loss: 0.00158623583
Iter: 69 loss: 0.00159539143
Iter: 70 loss: 0.00158584351
Iter: 71 loss: 0.00158422138
Iter: 72 loss: 0.00157965557
Iter: 73 loss: 0.0016039951
Iter: 74 loss: 0.00157820201
Iter: 75 loss: 0.00157261849
Iter: 76 loss: 0.00159931439
Iter: 77 loss: 0.00157159695
Iter: 78 loss: 0.00156710902
Iter: 79 loss: 0.00160713075
Iter: 80 loss: 0.00156687445
Iter: 81 loss: 0.00156372343
Iter: 82 loss: 0.00159229932
Iter: 83 loss: 0.00156357489
Iter: 84 loss: 0.00156059535
Iter: 85 loss: 0.00156079303
Iter: 86 loss: 0.00155825843
Iter: 87 loss: 0.00155474013
Iter: 88 loss: 0.00156634394
Iter: 89 loss: 0.00155375572
Iter: 90 loss: 0.00155004929
Iter: 91 loss: 0.00155215175
Iter: 92 loss: 0.00154761318
Iter: 93 loss: 0.00154444878
Iter: 94 loss: 0.00154438755
Iter: 95 loss: 0.00154072628
Iter: 96 loss: 0.00153780589
Iter: 97 loss: 0.00153667061
Iter: 98 loss: 0.001533804
Iter: 99 loss: 0.00153454184
Iter: 100 loss: 0.00153175369
Iter: 101 loss: 0.00152799953
Iter: 102 loss: 0.00154167716
Iter: 103 loss: 0.00152701931
Iter: 104 loss: 0.00152389263
Iter: 105 loss: 0.00152373733
Iter: 106 loss: 0.00152133254
Iter: 107 loss: 0.00151580863
Iter: 108 loss: 0.00152527716
Iter: 109 loss: 0.00151331699
Iter: 110 loss: 0.00150713499
Iter: 111 loss: 0.00152800977
Iter: 112 loss: 0.0015054089
Iter: 113 loss: 0.0015003
Iter: 114 loss: 0.0015257987
Iter: 115 loss: 0.00149941607
Iter: 116 loss: 0.00149483106
Iter: 117 loss: 0.00154654426
Iter: 118 loss: 0.00149473478
Iter: 119 loss: 0.00149169052
Iter: 120 loss: 0.00149408518
Iter: 121 loss: 0.00148982671
Iter: 122 loss: 0.0014859417
Iter: 123 loss: 0.00149707415
Iter: 124 loss: 0.00148469547
Iter: 125 loss: 0.00148013025
Iter: 126 loss: 0.00148310442
Iter: 127 loss: 0.00147722545
Iter: 128 loss: 0.00147302414
Iter: 129 loss: 0.00147300051
Iter: 130 loss: 0.00146879628
Iter: 131 loss: 0.00148674438
Iter: 132 loss: 0.00146789127
Iter: 133 loss: 0.00146394945
Iter: 134 loss: 0.00147829892
Iter: 135 loss: 0.00146298832
Iter: 136 loss: 0.00145851914
Iter: 137 loss: 0.00146154757
Iter: 138 loss: 0.00145566312
Iter: 139 loss: 0.00145033118
Iter: 140 loss: 0.00148410094
Iter: 141 loss: 0.00144969928
Iter: 142 loss: 0.00144471601
Iter: 143 loss: 0.00145707151
Iter: 144 loss: 0.00144295359
Iter: 145 loss: 0.00143766496
Iter: 146 loss: 0.00145382853
Iter: 147 loss: 0.00143600651
Iter: 148 loss: 0.0014321385
Iter: 149 loss: 0.00147940055
Iter: 150 loss: 0.00143208937
Iter: 151 loss: 0.0014281522
Iter: 152 loss: 0.00143687928
Iter: 153 loss: 0.00142662413
Iter: 154 loss: 0.00142323226
Iter: 155 loss: 0.00142985466
Iter: 156 loss: 0.00142181804
Iter: 157 loss: 0.0014176988
Iter: 158 loss: 0.00142726803
Iter: 159 loss: 0.00141614722
Iter: 160 loss: 0.00141252112
Iter: 161 loss: 0.00141684012
Iter: 162 loss: 0.00141060539
Iter: 163 loss: 0.00140833203
Iter: 164 loss: 0.00140787638
Iter: 165 loss: 0.00140543096
Iter: 166 loss: 0.00141106267
Iter: 167 loss: 0.00140448636
Iter: 168 loss: 0.00140098843
Iter: 169 loss: 0.00140594156
Iter: 170 loss: 0.00139926607
Iter: 171 loss: 0.00139503926
Iter: 172 loss: 0.00139695092
Iter: 173 loss: 0.00139212888
Iter: 174 loss: 0.00138680404
Iter: 175 loss: 0.00139625033
Iter: 176 loss: 0.00138441869
Iter: 177 loss: 0.00137893506
Iter: 178 loss: 0.00140959816
Iter: 179 loss: 0.00137815217
Iter: 180 loss: 0.00137319649
Iter: 181 loss: 0.00140608707
Iter: 182 loss: 0.00137268251
Iter: 183 loss: 0.00136831356
Iter: 184 loss: 0.00137267495
Iter: 185 loss: 0.00136581343
Iter: 186 loss: 0.00136184134
Iter: 187 loss: 0.00141377957
Iter: 188 loss: 0.0013618177
Iter: 189 loss: 0.00135803479
Iter: 190 loss: 0.0013700251
Iter: 191 loss: 0.00135695038
Iter: 192 loss: 0.00135401182
Iter: 193 loss: 0.00135617086
Iter: 194 loss: 0.00135217397
Iter: 195 loss: 0.00134872505
Iter: 196 loss: 0.00134603167
Iter: 197 loss: 0.00134495459
Iter: 198 loss: 0.00134048844
Iter: 199 loss: 0.00137119833
Iter: 200 loss: 0.00134004327
Iter: 201 loss: 0.00133635756
Iter: 202 loss: 0.00138234207
Iter: 203 loss: 0.00133632089
Iter: 204 loss: 0.00133367314
Iter: 205 loss: 0.0013350544
Iter: 206 loss: 0.00133191561
Iter: 207 loss: 0.00132863061
Iter: 208 loss: 0.00132396969
Iter: 209 loss: 0.00132380205
Iter: 210 loss: 0.00131842354
Iter: 211 loss: 0.00132937799
Iter: 212 loss: 0.0013162446
Iter: 213 loss: 0.00131166249
Iter: 214 loss: 0.00135689857
Iter: 215 loss: 0.00131150451
Iter: 216 loss: 0.00130730704
Iter: 217 loss: 0.00131771318
Iter: 218 loss: 0.00130582484
Iter: 219 loss: 0.00130222563
Iter: 220 loss: 0.00130623463
Iter: 221 loss: 0.00130026916
Iter: 222 loss: 0.00129661546
Iter: 223 loss: 0.00133558
Iter: 224 loss: 0.00129652047
Iter: 225 loss: 0.00129369041
Iter: 226 loss: 0.00130487373
Iter: 227 loss: 0.0012930443
Iter: 228 loss: 0.00129032065
Iter: 229 loss: 0.00128985371
Iter: 230 loss: 0.00128798746
Iter: 231 loss: 0.00128456717
Iter: 232 loss: 0.00129819382
Iter: 233 loss: 0.00128380628
Iter: 234 loss: 0.00128139893
Iter: 235 loss: 0.00128139742
Iter: 236 loss: 0.00127951009
Iter: 237 loss: 0.00127881882
Iter: 238 loss: 0.00127776712
Iter: 239 loss: 0.00127520203
Iter: 240 loss: 0.0012775861
Iter: 241 loss: 0.00127372425
Iter: 242 loss: 0.00127084192
Iter: 243 loss: 0.00127281388
Iter: 244 loss: 0.00126903341
Iter: 245 loss: 0.00126514479
Iter: 246 loss: 0.00126876903
Iter: 247 loss: 0.0012628939
Iter: 248 loss: 0.00125896255
Iter: 249 loss: 0.00128226588
Iter: 250 loss: 0.00125846348
Iter: 251 loss: 0.00125451828
Iter: 252 loss: 0.00126957777
Iter: 253 loss: 0.00125358324
Iter: 254 loss: 0.00125013571
Iter: 255 loss: 0.00125125796
Iter: 256 loss: 0.0012476783
Iter: 257 loss: 0.00124424882
Iter: 258 loss: 0.00124420505
Iter: 259 loss: 0.00124183123
Iter: 260 loss: 0.00124188268
Iter: 261 loss: 0.00123994274
Iter: 262 loss: 0.0012365235
Iter: 263 loss: 0.00124236895
Iter: 264 loss: 0.00123496959
Iter: 265 loss: 0.00123247027
Iter: 266 loss: 0.00125893066
Iter: 267 loss: 0.00123241195
Iter: 268 loss: 0.00122960401
Iter: 269 loss: 0.0012329712
Iter: 270 loss: 0.0012281189
Iter: 271 loss: 0.001225424
Iter: 272 loss: 0.00122981146
Iter: 273 loss: 0.00122418
Iter: 274 loss: 0.00122133014
Iter: 275 loss: 0.00121880858
Iter: 276 loss: 0.00121806189
Iter: 277 loss: 0.0012131884
Iter: 278 loss: 0.00123698823
Iter: 279 loss: 0.00121232518
Iter: 280 loss: 0.00120800873
Iter: 281 loss: 0.0012136338
Iter: 282 loss: 0.00120579218
Iter: 283 loss: 0.00120151695
Iter: 284 loss: 0.00123645726
Iter: 285 loss: 0.00120124943
Iter: 286 loss: 0.00119736814
Iter: 287 loss: 0.00120211812
Iter: 288 loss: 0.00119534158
Iter: 289 loss: 0.00119272829
Iter: 290 loss: 0.00123378518
Iter: 291 loss: 0.00119272736
Iter: 292 loss: 0.00119021966
Iter: 293 loss: 0.00119101186
Iter: 294 loss: 0.00118842884
Iter: 295 loss: 0.00118548959
Iter: 296 loss: 0.00119408313
Iter: 297 loss: 0.00118457759
Iter: 298 loss: 0.00118189037
Iter: 299 loss: 0.0011874577
Iter: 300 loss: 0.00118079758
Iter: 301 loss: 0.00117817346
Iter: 302 loss: 0.00121840928
Iter: 303 loss: 0.00117817358
Iter: 304 loss: 0.00117646321
Iter: 305 loss: 0.00117484597
Iter: 306 loss: 0.00117445248
Iter: 307 loss: 0.00117117248
Iter: 308 loss: 0.00117071462
Iter: 309 loss: 0.0011683919
Iter: 310 loss: 0.00116440083
Iter: 311 loss: 0.00118133472
Iter: 312 loss: 0.00116353843
Iter: 313 loss: 0.00115973304
Iter: 314 loss: 0.00117424037
Iter: 315 loss: 0.00115880591
Iter: 316 loss: 0.00115581159
Iter: 317 loss: 0.00116440863
Iter: 318 loss: 0.0011548663
Iter: 319 loss: 0.00115124485
Iter: 320 loss: 0.00116020604
Iter: 321 loss: 0.00114995381
Iter: 322 loss: 0.00114629255
Iter: 323 loss: 0.0011570045
Iter: 324 loss: 0.00114514853
Iter: 325 loss: 0.00114147121
Iter: 326 loss: 0.00117962516
Iter: 327 loss: 0.00114136073
Iter: 328 loss: 0.00113925268
Iter: 329 loss: 0.00113991147
Iter: 330 loss: 0.00113774766
Iter: 331 loss: 0.00113481609
Iter: 332 loss: 0.00114251568
Iter: 333 loss: 0.00113382738
Iter: 334 loss: 0.00113219034
Iter: 335 loss: 0.00113208848
Iter: 336 loss: 0.00113069825
Iter: 337 loss: 0.00112854992
Iter: 338 loss: 0.00112851895
Iter: 339 loss: 0.0011256393
Iter: 340 loss: 0.00113165635
Iter: 341 loss: 0.00112450414
Iter: 342 loss: 0.00112171075
Iter: 343 loss: 0.00112119096
Iter: 344 loss: 0.00111931306
Iter: 345 loss: 0.0011151873
Iter: 346 loss: 0.00114075688
Iter: 347 loss: 0.00111470278
Iter: 348 loss: 0.00111129647
Iter: 349 loss: 0.00111335679
Iter: 350 loss: 0.00110910088
Iter: 351 loss: 0.0011053154
Iter: 352 loss: 0.0011359026
Iter: 353 loss: 0.00110507035
Iter: 354 loss: 0.00110221514
Iter: 355 loss: 0.0011047991
Iter: 356 loss: 0.00110055227
Iter: 357 loss: 0.00109776319
Iter: 358 loss: 0.00113742554
Iter: 359 loss: 0.00109775667
Iter: 360 loss: 0.00109548075
Iter: 361 loss: 0.00109341228
Iter: 362 loss: 0.00109284685
Iter: 363 loss: 0.00108946743
Iter: 364 loss: 0.00111195864
Iter: 365 loss: 0.00108910725
Iter: 366 loss: 0.00108679361
Iter: 367 loss: 0.00110396149
Iter: 368 loss: 0.00108660536
Iter: 369 loss: 0.00108406623
Iter: 370 loss: 0.00108330336
Iter: 371 loss: 0.00108178728
Iter: 372 loss: 0.00107933313
Iter: 373 loss: 0.0010832455
Iter: 374 loss: 0.00107819168
Iter: 375 loss: 0.00107526151
Iter: 376 loss: 0.00107700692
Iter: 377 loss: 0.00107336452
Iter: 378 loss: 0.00107007974
Iter: 379 loss: 0.00108048634
Iter: 380 loss: 0.00106912851
Iter: 381 loss: 0.00106519391
Iter: 382 loss: 0.00107279397
Iter: 383 loss: 0.00106354558
Iter: 384 loss: 0.00105987245
Iter: 385 loss: 0.0010717744
Iter: 386 loss: 0.00105883298
Iter: 387 loss: 0.00105488824
Iter: 388 loss: 0.00106588949
Iter: 389 loss: 0.00105361349
Iter: 390 loss: 0.00105068262
Iter: 391 loss: 0.00107540481
Iter: 392 loss: 0.00105051021
Iter: 393 loss: 0.00104750379
Iter: 394 loss: 0.00105213036
Iter: 395 loss: 0.00104608131
Iter: 396 loss: 0.00104360329
Iter: 397 loss: 0.00105228671
Iter: 398 loss: 0.00104295509
Iter: 399 loss: 0.0010408347
Iter: 400 loss: 0.00105814019
Iter: 401 loss: 0.00104069221
Iter: 402 loss: 0.00103862421
Iter: 403 loss: 0.00104248058
Iter: 404 loss: 0.00103774434
Iter: 405 loss: 0.00103594665
Iter: 406 loss: 0.0010337932
Iter: 407 loss: 0.00103357749
Iter: 408 loss: 0.00103020575
Iter: 409 loss: 0.00104072946
Iter: 410 loss: 0.001029226
Iter: 411 loss: 0.00102616637
Iter: 412 loss: 0.00102774694
Iter: 413 loss: 0.0010241433
Iter: 414 loss: 0.00102030963
Iter: 415 loss: 0.00104580552
Iter: 416 loss: 0.00101992139
Iter: 417 loss: 0.00101680937
Iter: 418 loss: 0.00102150301
Iter: 419 loss: 0.0010153231
Iter: 420 loss: 0.00101218233
Iter: 421 loss: 0.00102770189
Iter: 422 loss: 0.00101164461
Iter: 423 loss: 0.00100889755
Iter: 424 loss: 0.00101320772
Iter: 425 loss: 0.00100761664
Iter: 426 loss: 0.0010047087
Iter: 427 loss: 0.00103466539
Iter: 428 loss: 0.00100462441
Iter: 429 loss: 0.00100272521
Iter: 430 loss: 0.00100194558
Iter: 431 loss: 0.00100093975
Iter: 432 loss: 0.000998496311
Iter: 433 loss: 0.00102917221
Iter: 434 loss: 0.000998475
Iter: 435 loss: 0.000996369403
Iter: 436 loss: 0.00100301404
Iter: 437 loss: 0.000995758
Iter: 438 loss: 0.000994086266
Iter: 439 loss: 0.000991004286
Iter: 440 loss: 0.00106339587
Iter: 441 loss: 0.000991001492
Iter: 442 loss: 0.000987347914
Iter: 443 loss: 0.000999579
Iter: 444 loss: 0.000986350118
Iter: 445 loss: 0.000982466387
Iter: 446 loss: 0.000983646489
Iter: 447 loss: 0.000979675213
Iter: 448 loss: 0.00097539695
Iter: 449 loss: 0.00100377854
Iter: 450 loss: 0.000974947121
Iter: 451 loss: 0.000971263682
Iter: 452 loss: 0.000983364414
Iter: 453 loss: 0.000970229856
Iter: 454 loss: 0.000967127969
Iter: 455 loss: 0.000977844116
Iter: 456 loss: 0.000966298452
Iter: 457 loss: 0.000963150174
Iter: 458 loss: 0.0009686423
Iter: 459 loss: 0.000961748185
Iter: 460 loss: 0.000959087862
Iter: 461 loss: 0.000990966568
Iter: 462 loss: 0.000959054509
Iter: 463 loss: 0.000956751406
Iter: 464 loss: 0.000955192139
Iter: 465 loss: 0.00095433963
Iter: 466 loss: 0.000951905036
Iter: 467 loss: 0.000986846164
Iter: 468 loss: 0.000951892871
Iter: 469 loss: 0.000949674693
Iter: 470 loss: 0.000965636573
Iter: 471 loss: 0.000949481619
Iter: 472 loss: 0.000948019442
Iter: 473 loss: 0.000944777858
Iter: 474 loss: 0.000991137815
Iter: 475 loss: 0.000944618
Iter: 476 loss: 0.000941496226
Iter: 477 loss: 0.000952063128
Iter: 478 loss: 0.000940652681
Iter: 479 loss: 0.00093738857
Iter: 480 loss: 0.000941098086
Iter: 481 loss: 0.000935630524
Iter: 482 loss: 0.000932269555
Iter: 483 loss: 0.000940983882
Iter: 484 loss: 0.000931121234
Iter: 485 loss: 0.000927586225
Iter: 486 loss: 0.000942970626
Iter: 487 loss: 0.000926865672
Iter: 488 loss: 0.000923575251
Iter: 489 loss: 0.000934485695
Iter: 490 loss: 0.000922666281
Iter: 491 loss: 0.000919931103
Iter: 492 loss: 0.000926068751
Iter: 493 loss: 0.000918885809
Iter: 494 loss: 0.000916515361
Iter: 495 loss: 0.000922945677
Iter: 496 loss: 0.000915727869
Iter: 497 loss: 0.000912811374
Iter: 498 loss: 0.000919054903
Iter: 499 loss: 0.000911670795
Iter: 500 loss: 0.000909178343
Iter: 501 loss: 0.000905702531
Iter: 502 loss: 0.000905556139
Iter: 503 loss: 0.000902020955
Iter: 504 loss: 0.000915051845
Iter: 505 loss: 0.000901129504
Iter: 506 loss: 0.000899538
Iter: 507 loss: 0.000899239676
Iter: 508 loss: 0.000897327904
Iter: 509 loss: 0.000901444815
Iter: 510 loss: 0.000896586
Iter: 511 loss: 0.000895415491
Iter: 512 loss: 0.000893645221
Iter: 513 loss: 0.000893605757
Iter: 514 loss: 0.000890679657
Iter: 515 loss: 0.0008956003
Iter: 516 loss: 0.000889358
Iter: 517 loss: 0.000886654947
Iter: 518 loss: 0.000885250862
Iter: 519 loss: 0.000884005218
Iter: 520 loss: 0.000879985571
Iter: 521 loss: 0.000905960565
Iter: 522 loss: 0.000879537314
Iter: 523 loss: 0.000875842
Iter: 524 loss: 0.000879255822
Iter: 525 loss: 0.000873704208
Iter: 526 loss: 0.000869798183
Iter: 527 loss: 0.000888639828
Iter: 528 loss: 0.0008690994
Iter: 529 loss: 0.000866440882
Iter: 530 loss: 0.00089071691
Iter: 531 loss: 0.000866320392
Iter: 532 loss: 0.000863734749
Iter: 533 loss: 0.00086404325
Iter: 534 loss: 0.000861741486
Iter: 535 loss: 0.000859081279
Iter: 536 loss: 0.000856435392
Iter: 537 loss: 0.000855887833
Iter: 538 loss: 0.000854983227
Iter: 539 loss: 0.000854229263
Iter: 540 loss: 0.000852676225
Iter: 541 loss: 0.000855275
Iter: 542 loss: 0.000851966906
Iter: 543 loss: 0.000850347
Iter: 544 loss: 0.000850567536
Iter: 545 loss: 0.000849086
Iter: 546 loss: 0.000845893868
Iter: 547 loss: 0.000849045
Iter: 548 loss: 0.000844100898
Iter: 549 loss: 0.00084111304
Iter: 550 loss: 0.000843388785
Iter: 551 loss: 0.000839280663
Iter: 552 loss: 0.00083679962
Iter: 553 loss: 0.000831459649
Iter: 554 loss: 0.000913898286
Iter: 555 loss: 0.000831267564
Iter: 556 loss: 0.000827550539
Iter: 557 loss: 0.000833272
Iter: 558 loss: 0.000825751573
Iter: 559 loss: 0.000823209353
Iter: 560 loss: 0.000824591145
Iter: 561 loss: 0.000821550377
Iter: 562 loss: 0.000819117879
Iter: 563 loss: 0.000828613061
Iter: 564 loss: 0.000818559201
Iter: 565 loss: 0.00081601867
Iter: 566 loss: 0.000825154886
Iter: 567 loss: 0.00081536069
Iter: 568 loss: 0.000812826795
Iter: 569 loss: 0.0008170777
Iter: 570 loss: 0.000811656646
Iter: 571 loss: 0.000809847144
Iter: 572 loss: 0.000832094578
Iter: 573 loss: 0.000809830904
Iter: 574 loss: 0.000808650919
Iter: 575 loss: 0.000810272293
Iter: 576 loss: 0.000808062381
Iter: 577 loss: 0.000806317665
Iter: 578 loss: 0.000802613562
Iter: 579 loss: 0.000864013389
Iter: 580 loss: 0.000802504132
Iter: 581 loss: 0.00079938391
Iter: 582 loss: 0.000826134696
Iter: 583 loss: 0.000799224887
Iter: 584 loss: 0.000796488894
Iter: 585 loss: 0.000801179907
Iter: 586 loss: 0.0007952546
Iter: 587 loss: 0.000793481
Iter: 588 loss: 0.000791480066
Iter: 589 loss: 0.000791198574
Iter: 590 loss: 0.000788321719
Iter: 591 loss: 0.00079207489
Iter: 592 loss: 0.000786869205
Iter: 593 loss: 0.000784511445
Iter: 594 loss: 0.000803059083
Iter: 595 loss: 0.000784294098
Iter: 596 loss: 0.000782766379
Iter: 597 loss: 0.000780675153
Iter: 598 loss: 0.000780579867
Iter: 599 loss: 0.000778391724
Iter: 600 loss: 0.000777862035
Iter: 601 loss: 0.000776472443
Iter: 602 loss: 0.000774664688
Iter: 603 loss: 0.000773811596
Iter: 604 loss: 0.000772922649
Iter: 605 loss: 0.000770661
Iter: 606 loss: 0.00076741213
Iter: 607 loss: 0.000767300371
Iter: 608 loss: 0.000764585449
Iter: 609 loss: 0.000764585566
Iter: 610 loss: 0.000762644573
Iter: 611 loss: 0.000763437536
Iter: 612 loss: 0.000761305448
Iter: 613 loss: 0.000757829635
Iter: 614 loss: 0.000763779623
Iter: 615 loss: 0.00075625
Iter: 616 loss: 0.000753636472
Iter: 617 loss: 0.000771555584
Iter: 618 loss: 0.000753385073
Iter: 619 loss: 0.000750677427
Iter: 620 loss: 0.000750871259
Iter: 621 loss: 0.000748568331
Iter: 622 loss: 0.000745845726
Iter: 623 loss: 0.000749445055
Iter: 624 loss: 0.000744465273
Iter: 625 loss: 0.000741803262
Iter: 626 loss: 0.000763505232
Iter: 627 loss: 0.000741608208
Iter: 628 loss: 0.000739417388
Iter: 629 loss: 0.000764177763
Iter: 630 loss: 0.000739376177
Iter: 631 loss: 0.000738279603
Iter: 632 loss: 0.000737160619
Iter: 633 loss: 0.000736944843
Iter: 634 loss: 0.000734868
Iter: 635 loss: 0.000736180809
Iter: 636 loss: 0.000733530149
Iter: 637 loss: 0.000731048931
Iter: 638 loss: 0.00074165297
Iter: 639 loss: 0.000730523374
Iter: 640 loss: 0.000729031046
Iter: 641 loss: 0.000739557843
Iter: 642 loss: 0.00072889257
Iter: 643 loss: 0.000727373525
Iter: 644 loss: 0.000723695266
Iter: 645 loss: 0.000763852673
Iter: 646 loss: 0.000723295729
Iter: 647 loss: 0.000720486278
Iter: 648 loss: 0.000735214096
Iter: 649 loss: 0.000720033888
Iter: 650 loss: 0.000717662915
Iter: 651 loss: 0.000719762058
Iter: 652 loss: 0.000716275186
Iter: 653 loss: 0.000714465743
Iter: 654 loss: 0.000714239
Iter: 655 loss: 0.000712046633
Iter: 656 loss: 0.000714018242
Iter: 657 loss: 0.000710756285
Iter: 658 loss: 0.000709334388
Iter: 659 loss: 0.000713174697
Iter: 660 loss: 0.00070889818
Iter: 661 loss: 0.000708125066
Iter: 662 loss: 0.000707384432
Iter: 663 loss: 0.000707206549
Iter: 664 loss: 0.000703577069
Iter: 665 loss: 0.0007090125
Iter: 666 loss: 0.000701831596
Iter: 667 loss: 0.000698609394
Iter: 668 loss: 0.000697872136
Iter: 669 loss: 0.000695565715
Iter: 670 loss: 0.000699505559
Iter: 671 loss: 0.000694527407
Iter: 672 loss: 0.000692419941
Iter: 673 loss: 0.000717566407
Iter: 674 loss: 0.00069239171
Iter: 675 loss: 0.000690469285
Iter: 676 loss: 0.000702052319
Iter: 677 loss: 0.00069023564
Iter: 678 loss: 0.000689257286
Iter: 679 loss: 0.000687049527
Iter: 680 loss: 0.000717027055
Iter: 681 loss: 0.000686918967
Iter: 682 loss: 0.000684355269
Iter: 683 loss: 0.000685675244
Iter: 684 loss: 0.000682644197
Iter: 685 loss: 0.000679549237
Iter: 686 loss: 0.000694295391
Iter: 687 loss: 0.000678992597
Iter: 688 loss: 0.000676423428
Iter: 689 loss: 0.00068981247
Iter: 690 loss: 0.000676019583
Iter: 691 loss: 0.000677937525
Iter: 692 loss: 0.00067523832
Iter: 693 loss: 0.000674524
Iter: 694 loss: 0.000672483642
Iter: 695 loss: 0.000681977312
Iter: 696 loss: 0.000671758433
Iter: 697 loss: 0.000668776
Iter: 698 loss: 0.000666798442
Iter: 699 loss: 0.000665665837
Iter: 700 loss: 0.000661449507
Iter: 701 loss: 0.000694010872
Iter: 702 loss: 0.000661133206
Iter: 703 loss: 0.000657894532
Iter: 704 loss: 0.000674052397
Iter: 705 loss: 0.00065732823
Iter: 706 loss: 0.000655512908
Iter: 707 loss: 0.000658098958
Iter: 708 loss: 0.000654616451
Iter: 709 loss: 0.000652730057
Iter: 710 loss: 0.000671460468
Iter: 711 loss: 0.000652663875
Iter: 712 loss: 0.000651285111
Iter: 713 loss: 0.000651950657
Iter: 714 loss: 0.000650351634
Iter: 715 loss: 0.00064869679
Iter: 716 loss: 0.000649208785
Iter: 717 loss: 0.000647514418
Iter: 718 loss: 0.000645646825
Iter: 719 loss: 0.000646781235
Iter: 720 loss: 0.000644433079
Iter: 721 loss: 0.000641677179
Iter: 722 loss: 0.00065248087
Iter: 723 loss: 0.000641034043
Iter: 724 loss: 0.000639457605
Iter: 725 loss: 0.000639312668
Iter: 726 loss: 0.000637438847
Iter: 727 loss: 0.000649441
Iter: 728 loss: 0.000637243502
Iter: 729 loss: 0.0006359939
Iter: 730 loss: 0.000633024727
Iter: 731 loss: 0.000668883557
Iter: 732 loss: 0.000632757205
Iter: 733 loss: 0.000629023765
Iter: 734 loss: 0.000632640207
Iter: 735 loss: 0.000626937
Iter: 736 loss: 0.000622830237
Iter: 737 loss: 0.000685743464
Iter: 738 loss: 0.000622832042
Iter: 739 loss: 0.000621035113
Iter: 740 loss: 0.00062072865
Iter: 741 loss: 0.000619065482
Iter: 742 loss: 0.000629898161
Iter: 743 loss: 0.000618892605
Iter: 744 loss: 0.000617324957
Iter: 745 loss: 0.000615208934
Iter: 746 loss: 0.000615098106
Iter: 747 loss: 0.000612721953
Iter: 748 loss: 0.00062936591
Iter: 749 loss: 0.000612495
Iter: 750 loss: 0.000611037714
Iter: 751 loss: 0.000608808477
Iter: 752 loss: 0.000608769245
Iter: 753 loss: 0.000604954432
Iter: 754 loss: 0.000634463853
Iter: 755 loss: 0.000604674453
Iter: 756 loss: 0.000601910579
Iter: 757 loss: 0.000626907684
Iter: 758 loss: 0.000601787062
Iter: 759 loss: 0.000599702
Iter: 760 loss: 0.000623851607
Iter: 761 loss: 0.000599662133
Iter: 762 loss: 0.00059813878
Iter: 763 loss: 0.000597297563
Iter: 764 loss: 0.000596622471
Iter: 765 loss: 0.000594423385
Iter: 766 loss: 0.000591932796
Iter: 767 loss: 0.00059160043
Iter: 768 loss: 0.000586839858
Iter: 769 loss: 0.000598727085
Iter: 770 loss: 0.000585145201
Iter: 771 loss: 0.00058875361
Iter: 772 loss: 0.000583682733
Iter: 773 loss: 0.000582449604
Iter: 774 loss: 0.000581408618
Iter: 775 loss: 0.000581059838
Iter: 776 loss: 0.000578736479
Iter: 777 loss: 0.000584764581
Iter: 778 loss: 0.000577944855
Iter: 779 loss: 0.000576072372
Iter: 780 loss: 0.000579392421
Iter: 781 loss: 0.000575252576
Iter: 782 loss: 0.000572943711
Iter: 783 loss: 0.000570054865
Iter: 784 loss: 0.000569807598
Iter: 785 loss: 0.000566768227
Iter: 786 loss: 0.000580995576
Iter: 787 loss: 0.000566205
Iter: 788 loss: 0.000563480135
Iter: 789 loss: 0.00058012642
Iter: 790 loss: 0.000563144451
Iter: 791 loss: 0.000561607478
Iter: 792 loss: 0.000561510795
Iter: 793 loss: 0.000559915
Iter: 794 loss: 0.00056077342
Iter: 795 loss: 0.00055884989
Iter: 796 loss: 0.000556456391
Iter: 797 loss: 0.000556727231
Iter: 798 loss: 0.000554620638
Iter: 799 loss: 0.000551312929
Iter: 800 loss: 0.000558902102
Iter: 801 loss: 0.000550066587
Iter: 802 loss: 0.000546640775
Iter: 803 loss: 0.000550661702
Iter: 804 loss: 0.000544814335
Iter: 805 loss: 0.000545905612
Iter: 806 loss: 0.000543230912
Iter: 807 loss: 0.000541653193
Iter: 808 loss: 0.000543612463
Iter: 809 loss: 0.000540836
Iter: 810 loss: 0.000538397231
Iter: 811 loss: 0.000540059118
Iter: 812 loss: 0.000536862179
Iter: 813 loss: 0.000534514606
Iter: 814 loss: 0.00054676435
Iter: 815 loss: 0.000534136139
Iter: 816 loss: 0.000531784084
Iter: 817 loss: 0.000530240242
Iter: 818 loss: 0.00052934594
Iter: 819 loss: 0.000526151736
Iter: 820 loss: 0.000532868435
Iter: 821 loss: 0.000524865231
Iter: 822 loss: 0.000521240407
Iter: 823 loss: 0.000547138043
Iter: 824 loss: 0.000520920148
Iter: 825 loss: 0.000518663437
Iter: 826 loss: 0.000525289041
Iter: 827 loss: 0.000517966517
Iter: 828 loss: 0.000515072781
Iter: 829 loss: 0.000532046659
Iter: 830 loss: 0.000514713931
Iter: 831 loss: 0.000512705883
Iter: 832 loss: 0.000512516883
Iter: 833 loss: 0.000511037884
Iter: 834 loss: 0.000508460565
Iter: 835 loss: 0.000512431259
Iter: 836 loss: 0.000507240649
Iter: 837 loss: 0.000504714088
Iter: 838 loss: 0.000518667803
Iter: 839 loss: 0.000504335098
Iter: 840 loss: 0.000502067734
Iter: 841 loss: 0.000502066687
Iter: 842 loss: 0.000501009286
Iter: 843 loss: 0.000500101829
Iter: 844 loss: 0.000499813585
Iter: 845 loss: 0.000497836096
Iter: 846 loss: 0.000498660957
Iter: 847 loss: 0.000496468041
Iter: 848 loss: 0.000494243111
Iter: 849 loss: 0.000501743169
Iter: 850 loss: 0.000493639556
Iter: 851 loss: 0.000491377
Iter: 852 loss: 0.000491873128
Iter: 853 loss: 0.000489699771
Iter: 854 loss: 0.000487249461
Iter: 855 loss: 0.00048954261
Iter: 856 loss: 0.000485838042
Iter: 857 loss: 0.000483311043
Iter: 858 loss: 0.000516395899
Iter: 859 loss: 0.00048329428
Iter: 860 loss: 0.000481353432
Iter: 861 loss: 0.00048826175
Iter: 862 loss: 0.000480864284
Iter: 863 loss: 0.000478657108
Iter: 864 loss: 0.000486182951
Iter: 865 loss: 0.00047806665
Iter: 866 loss: 0.000476241315
Iter: 867 loss: 0.000474068569
Iter: 868 loss: 0.000473841967
Iter: 869 loss: 0.000471730018
Iter: 870 loss: 0.000471725536
Iter: 871 loss: 0.000470422412
Iter: 872 loss: 0.000470422441
Iter: 873 loss: 0.000469311024
Iter: 874 loss: 0.000467285427
Iter: 875 loss: 0.000515652471
Iter: 876 loss: 0.000467283
Iter: 877 loss: 0.000465425837
Iter: 878 loss: 0.000480426475
Iter: 879 loss: 0.00046529714
Iter: 880 loss: 0.000463824254
Iter: 881 loss: 0.000462971133
Iter: 882 loss: 0.000462344091
Iter: 883 loss: 0.000460398442
Iter: 884 loss: 0.000469690713
Iter: 885 loss: 0.000460046751
Iter: 886 loss: 0.000458218565
Iter: 887 loss: 0.000455732836
Iter: 888 loss: 0.00045561025
Iter: 889 loss: 0.000452654203
Iter: 890 loss: 0.000476027955
Iter: 891 loss: 0.000452448759
Iter: 892 loss: 0.000449657207
Iter: 893 loss: 0.000459425821
Iter: 894 loss: 0.000448928738
Iter: 895 loss: 0.000446601654
Iter: 896 loss: 0.000463133329
Iter: 897 loss: 0.000446393329
Iter: 898 loss: 0.000444075034
Iter: 899 loss: 0.000443161523
Iter: 900 loss: 0.000441914279
Iter: 901 loss: 0.000439372554
Iter: 902 loss: 0.000457640097
Iter: 903 loss: 0.000439146679
Iter: 904 loss: 0.00043775962
Iter: 905 loss: 0.000437646522
Iter: 906 loss: 0.000436328322
Iter: 907 loss: 0.000435777765
Iter: 908 loss: 0.000435088878
Iter: 909 loss: 0.000433568348
Iter: 910 loss: 0.000435731898
Iter: 911 loss: 0.000432817906
Iter: 912 loss: 0.000430883374
Iter: 913 loss: 0.000435711088
Iter: 914 loss: 0.000430201442
Iter: 915 loss: 0.000428645028
Iter: 916 loss: 0.000428269879
Iter: 917 loss: 0.00042727773
Iter: 918 loss: 0.000424735015
Iter: 919 loss: 0.000428590836
Iter: 920 loss: 0.000423526799
Iter: 921 loss: 0.000420780154
Iter: 922 loss: 0.000421072909
Iter: 923 loss: 0.000418658892
Iter: 924 loss: 0.000416142517
Iter: 925 loss: 0.000416136812
Iter: 926 loss: 0.00041390289
Iter: 927 loss: 0.000414886104
Iter: 928 loss: 0.000412378577
Iter: 929 loss: 0.000410067674
Iter: 930 loss: 0.000431001885
Iter: 931 loss: 0.000409957662
Iter: 932 loss: 0.000408076099
Iter: 933 loss: 0.000406823412
Iter: 934 loss: 0.000406111707
Iter: 935 loss: 0.000404697494
Iter: 936 loss: 0.00040461024
Iter: 937 loss: 0.000403044192
Iter: 938 loss: 0.000405122468
Iter: 939 loss: 0.000402248173
Iter: 940 loss: 0.000400751946
Iter: 941 loss: 0.000404030929
Iter: 942 loss: 0.000400171091
Iter: 943 loss: 0.000398900651
Iter: 944 loss: 0.000401520811
Iter: 945 loss: 0.00039839471
Iter: 946 loss: 0.000396776944
Iter: 947 loss: 0.000395784155
Iter: 948 loss: 0.000395124487
Iter: 949 loss: 0.000392856455
Iter: 950 loss: 0.000398845121
Iter: 951 loss: 0.000392090413
Iter: 952 loss: 0.000389537017
Iter: 953 loss: 0.000389331952
Iter: 954 loss: 0.000387425418
Iter: 955 loss: 0.000384215557
Iter: 956 loss: 0.000395268202
Iter: 957 loss: 0.000383362145
Iter: 958 loss: 0.000381651334
Iter: 959 loss: 0.00038157677
Iter: 960 loss: 0.000380123092
Iter: 961 loss: 0.000379712234
Iter: 962 loss: 0.000378829776
Iter: 963 loss: 0.000377058197
Iter: 964 loss: 0.000391969632
Iter: 965 loss: 0.000376955722
Iter: 966 loss: 0.000375643256
Iter: 967 loss: 0.000378544675
Iter: 968 loss: 0.000375147152
Iter: 969 loss: 0.000373586663
Iter: 970 loss: 0.000388203247
Iter: 971 loss: 0.000373520132
Iter: 972 loss: 0.000372621493
Iter: 973 loss: 0.000371804024
Iter: 974 loss: 0.00037158106
Iter: 975 loss: 0.000369967544
Iter: 976 loss: 0.000376775628
Iter: 977 loss: 0.000369626243
Iter: 978 loss: 0.000368292676
Iter: 979 loss: 0.000369434769
Iter: 980 loss: 0.000367502682
Iter: 981 loss: 0.000365841755
Iter: 982 loss: 0.000365679909
Iter: 983 loss: 0.000364463282
Iter: 984 loss: 0.000362330495
Iter: 985 loss: 0.000367852161
Iter: 986 loss: 0.000361598446
Iter: 987 loss: 0.000359373284
Iter: 988 loss: 0.000361122773
Iter: 989 loss: 0.0003580263
Iter: 990 loss: 0.000355401949
Iter: 991 loss: 0.000361553102
Iter: 992 loss: 0.000354425807
Iter: 993 loss: 0.000352911418
Iter: 994 loss: 0.000352709118
Iter: 995 loss: 0.00035149965
Iter: 996 loss: 0.000349631679
Iter: 997 loss: 0.000349605689
Iter: 998 loss: 0.000347619061
Iter: 999 loss: 0.000370939786
Iter: 1000 loss: 0.000347591034
Iter: 1001 loss: 0.000346568821
Iter: 1002 loss: 0.00036225596
Iter: 1003 loss: 0.000346568966
Iter: 1004 loss: 0.000345590233
Iter: 1005 loss: 0.000343431253
Iter: 1006 loss: 0.000374776952
Iter: 1007 loss: 0.000343328167
Iter: 1008 loss: 0.000341893407
Iter: 1009 loss: 0.000363387226
Iter: 1010 loss: 0.000341891195
Iter: 1011 loss: 0.000340491883
Iter: 1012 loss: 0.000340728322
Iter: 1013 loss: 0.000339439604
Iter: 1014 loss: 0.000337758946
Iter: 1015 loss: 0.000340908708
Iter: 1016 loss: 0.000337044214
Iter: 1017 loss: 0.000335299585
Iter: 1018 loss: 0.000334355631
Iter: 1019 loss: 0.000333572389
Iter: 1020 loss: 0.000330828043
Iter: 1021 loss: 0.000339180115
Iter: 1022 loss: 0.000330006849
Iter: 1023 loss: 0.000327370886
Iter: 1024 loss: 0.00033175759
Iter: 1025 loss: 0.000326171052
Iter: 1026 loss: 0.000323817716
Iter: 1027 loss: 0.000333865115
Iter: 1028 loss: 0.000323325949
Iter: 1029 loss: 0.000321698142
Iter: 1030 loss: 0.000321680855
Iter: 1031 loss: 0.000320541265
Iter: 1032 loss: 0.000319022831
Iter: 1033 loss: 0.000318937091
Iter: 1034 loss: 0.000318003353
Iter: 1035 loss: 0.000317734113
Iter: 1036 loss: 0.000316666963
Iter: 1037 loss: 0.000317145343
Iter: 1038 loss: 0.000315944053
Iter: 1039 loss: 0.000314772245
Iter: 1040 loss: 0.000312712451
Iter: 1041 loss: 0.000312712742
Iter: 1042 loss: 0.000311878364
Iter: 1043 loss: 0.000311541837
Iter: 1044 loss: 0.000310422212
Iter: 1045 loss: 0.000309225812
Iter: 1046 loss: 0.000309031748
Iter: 1047 loss: 0.000307463342
Iter: 1048 loss: 0.000309820636
Iter: 1049 loss: 0.000306711241
Iter: 1050 loss: 0.000304736081
Iter: 1051 loss: 0.000307999318
Iter: 1052 loss: 0.000303832348
Iter: 1053 loss: 0.000302007131
Iter: 1054 loss: 0.000302075816
Iter: 1055 loss: 0.000300564396
Iter: 1056 loss: 0.000298480212
Iter: 1057 loss: 0.000306121161
Iter: 1058 loss: 0.000297962368
Iter: 1059 loss: 0.000295785081
Iter: 1060 loss: 0.0003006552
Iter: 1061 loss: 0.000294953788
Iter: 1062 loss: 0.000293099554
Iter: 1063 loss: 0.000313644356
Iter: 1064 loss: 0.000293061894
Iter: 1065 loss: 0.000291394186
Iter: 1066 loss: 0.000296102022
Iter: 1067 loss: 0.000290857628
Iter: 1068 loss: 0.000289537827
Iter: 1069 loss: 0.000289536343
Iter: 1070 loss: 0.000288741896
Iter: 1071 loss: 0.000286754541
Iter: 1072 loss: 0.000305785274
Iter: 1073 loss: 0.000286476832
Iter: 1074 loss: 0.00028440723
Iter: 1075 loss: 0.000297136721
Iter: 1076 loss: 0.000284155831
Iter: 1077 loss: 0.000283235277
Iter: 1078 loss: 0.00028318452
Iter: 1079 loss: 0.00028229662
Iter: 1080 loss: 0.000280281674
Iter: 1081 loss: 0.000307510199
Iter: 1082 loss: 0.000280158652
Iter: 1083 loss: 0.000278391497
Iter: 1084 loss: 0.000280781591
Iter: 1085 loss: 0.000277503597
Iter: 1086 loss: 0.000275564147
Iter: 1087 loss: 0.000291445293
Iter: 1088 loss: 0.000275441
Iter: 1089 loss: 0.000273980666
Iter: 1090 loss: 0.000272817328
Iter: 1091 loss: 0.000272371952
Iter: 1092 loss: 0.000270327087
Iter: 1093 loss: 0.000277010957
Iter: 1094 loss: 0.000269757264
Iter: 1095 loss: 0.000267732656
Iter: 1096 loss: 0.000272792822
Iter: 1097 loss: 0.000267012
Iter: 1098 loss: 0.000265455281
Iter: 1099 loss: 0.000280667969
Iter: 1100 loss: 0.000265401846
Iter: 1101 loss: 0.000264278409
Iter: 1102 loss: 0.000264273782
Iter: 1103 loss: 0.00026341842
Iter: 1104 loss: 0.000263657188
Iter: 1105 loss: 0.000262801681
Iter: 1106 loss: 0.000261921581
Iter: 1107 loss: 0.000260787841
Iter: 1108 loss: 0.000260710076
Iter: 1109 loss: 0.000259501801
Iter: 1110 loss: 0.000277202285
Iter: 1111 loss: 0.000259499822
Iter: 1112 loss: 0.000258210814
Iter: 1113 loss: 0.000260036148
Iter: 1114 loss: 0.000257578155
Iter: 1115 loss: 0.000256604806
Iter: 1116 loss: 0.000255470077
Iter: 1117 loss: 0.000255338411
Iter: 1118 loss: 0.000253461796
Iter: 1119 loss: 0.000262307934
Iter: 1120 loss: 0.000253118167
Iter: 1121 loss: 0.000251590216
Iter: 1122 loss: 0.000251820835
Iter: 1123 loss: 0.000250429643
Iter: 1124 loss: 0.000248719211
Iter: 1125 loss: 0.000264974486
Iter: 1126 loss: 0.000248650846
Iter: 1127 loss: 0.000247248303
Iter: 1128 loss: 0.000247398653
Iter: 1129 loss: 0.000246171199
Iter: 1130 loss: 0.000244696857
Iter: 1131 loss: 0.000250405603
Iter: 1132 loss: 0.000244349474
Iter: 1133 loss: 0.00024366584
Iter: 1134 loss: 0.000243488394
Iter: 1135 loss: 0.000242847091
Iter: 1136 loss: 0.00024362652
Iter: 1137 loss: 0.00024251244
Iter: 1138 loss: 0.000241819886
Iter: 1139 loss: 0.00024044
Iter: 1140 loss: 0.000267186319
Iter: 1141 loss: 0.000240424852
Iter: 1142 loss: 0.000239087938
Iter: 1143 loss: 0.000250007695
Iter: 1144 loss: 0.00023900393
Iter: 1145 loss: 0.000237947053
Iter: 1146 loss: 0.000251007
Iter: 1147 loss: 0.000237934932
Iter: 1148 loss: 0.000237320695
Iter: 1149 loss: 0.000235793355
Iter: 1150 loss: 0.000250470825
Iter: 1151 loss: 0.000235585219
Iter: 1152 loss: 0.000234175648
Iter: 1153 loss: 0.000248102384
Iter: 1154 loss: 0.000234126623
Iter: 1155 loss: 0.000232904596
Iter: 1156 loss: 0.000232609396
Iter: 1157 loss: 0.000231833023
Iter: 1158 loss: 0.00023032623
Iter: 1159 loss: 0.00023637616
Iter: 1160 loss: 0.000229986879
Iter: 1161 loss: 0.000228491699
Iter: 1162 loss: 0.000235517466
Iter: 1163 loss: 0.000228214616
Iter: 1164 loss: 0.000226958029
Iter: 1165 loss: 0.000226079559
Iter: 1166 loss: 0.000225631229
Iter: 1167 loss: 0.000225558368
Iter: 1168 loss: 0.000224883173
Iter: 1169 loss: 0.000224147821
Iter: 1170 loss: 0.000225126889
Iter: 1171 loss: 0.000223777024
Iter: 1172 loss: 0.000222936535
Iter: 1173 loss: 0.000222272647
Iter: 1174 loss: 0.000222013798
Iter: 1175 loss: 0.00022093764
Iter: 1176 loss: 0.000221972674
Iter: 1177 loss: 0.000220323695
Iter: 1178 loss: 0.000219791254
Iter: 1179 loss: 0.000219586436
Iter: 1180 loss: 0.0002190144
Iter: 1181 loss: 0.000217787063
Iter: 1182 loss: 0.000237402855
Iter: 1183 loss: 0.000217746245
Iter: 1184 loss: 0.000216555971
Iter: 1185 loss: 0.000217245892
Iter: 1186 loss: 0.000215781678
Iter: 1187 loss: 0.000214382424
Iter: 1188 loss: 0.000222344606
Iter: 1189 loss: 0.000214185959
Iter: 1190 loss: 0.000212980638
Iter: 1191 loss: 0.000213136867
Iter: 1192 loss: 0.000212059414
Iter: 1193 loss: 0.000210631915
Iter: 1194 loss: 0.000216965476
Iter: 1195 loss: 0.000210348255
Iter: 1196 loss: 0.000209039237
Iter: 1197 loss: 0.000217285255
Iter: 1198 loss: 0.000208888756
Iter: 1199 loss: 0.000207868463
Iter: 1200 loss: 0.000208873433
Iter: 1201 loss: 0.00020729
Iter: 1202 loss: 0.000206818542
Iter: 1203 loss: 0.000206563913
Iter: 1204 loss: 0.000206098222
Iter: 1205 loss: 0.00020503471
Iter: 1206 loss: 0.000219046691
Iter: 1207 loss: 0.000204966185
Iter: 1208 loss: 0.000203783056
Iter: 1209 loss: 0.000208705489
Iter: 1210 loss: 0.000203528791
Iter: 1211 loss: 0.000202765543
Iter: 1212 loss: 0.000209516
Iter: 1213 loss: 0.000202725845
Iter: 1214 loss: 0.000201784147
Iter: 1215 loss: 0.000202281546
Iter: 1216 loss: 0.00020116294
Iter: 1217 loss: 0.000200450741
Iter: 1218 loss: 0.000199427974
Iter: 1219 loss: 0.000199394475
Iter: 1220 loss: 0.00019813882
Iter: 1221 loss: 0.000205395379
Iter: 1222 loss: 0.000197971574
Iter: 1223 loss: 0.000196933615
Iter: 1224 loss: 0.000197191519
Iter: 1225 loss: 0.000196175446
Iter: 1226 loss: 0.000194889668
Iter: 1227 loss: 0.000197983638
Iter: 1228 loss: 0.000194424108
Iter: 1229 loss: 0.000193244196
Iter: 1230 loss: 0.000198959708
Iter: 1231 loss: 0.000193035812
Iter: 1232 loss: 0.000192011896
Iter: 1233 loss: 0.000196140871
Iter: 1234 loss: 0.000191783169
Iter: 1235 loss: 0.000191214873
Iter: 1236 loss: 0.000191167652
Iter: 1237 loss: 0.000190548322
Iter: 1238 loss: 0.000189843093
Iter: 1239 loss: 0.000189753599
Iter: 1240 loss: 0.000189038808
Iter: 1241 loss: 0.000189138082
Iter: 1242 loss: 0.000188498583
Iter: 1243 loss: 0.000187401587
Iter: 1244 loss: 0.000190104969
Iter: 1245 loss: 0.000187011756
Iter: 1246 loss: 0.000186378835
Iter: 1247 loss: 0.000186273945
Iter: 1248 loss: 0.00018589606
Iter: 1249 loss: 0.00018481088
Iter: 1250 loss: 0.000189610117
Iter: 1251 loss: 0.000184403529
Iter: 1252 loss: 0.000183319484
Iter: 1253 loss: 0.000196088193
Iter: 1254 loss: 0.000183304452
Iter: 1255 loss: 0.000182316406
Iter: 1256 loss: 0.000183056109
Iter: 1257 loss: 0.000181708368
Iter: 1258 loss: 0.000180631163
Iter: 1259 loss: 0.000181965821
Iter: 1260 loss: 0.000180067684
Iter: 1261 loss: 0.000178786431
Iter: 1262 loss: 0.000181981333
Iter: 1263 loss: 0.000178335918
Iter: 1264 loss: 0.000177112714
Iter: 1265 loss: 0.000183762255
Iter: 1266 loss: 0.000176932488
Iter: 1267 loss: 0.000175889771
Iter: 1268 loss: 0.000182943128
Iter: 1269 loss: 0.000175784691
Iter: 1270 loss: 0.000175185589
Iter: 1271 loss: 0.000175168621
Iter: 1272 loss: 0.000174769753
Iter: 1273 loss: 0.000173733977
Iter: 1274 loss: 0.000182133634
Iter: 1275 loss: 0.000173543522
Iter: 1276 loss: 0.000172405795
Iter: 1277 loss: 0.000176935326
Iter: 1278 loss: 0.000172147498
Iter: 1279 loss: 0.000171505118
Iter: 1280 loss: 0.000171476931
Iter: 1281 loss: 0.000170803905
Iter: 1282 loss: 0.000170411775
Iter: 1283 loss: 0.00017012535
Iter: 1284 loss: 0.000169422667
Iter: 1285 loss: 0.000168382889
Iter: 1286 loss: 0.000168357539
Iter: 1287 loss: 0.000167180726
Iter: 1288 loss: 0.000174916349
Iter: 1289 loss: 0.000167054211
Iter: 1290 loss: 0.00016589844
Iter: 1291 loss: 0.000169819017
Iter: 1292 loss: 0.000165585021
Iter: 1293 loss: 0.00016460933
Iter: 1294 loss: 0.000163670251
Iter: 1295 loss: 0.000163453573
Iter: 1296 loss: 0.000162100128
Iter: 1297 loss: 0.000170081272
Iter: 1298 loss: 0.000161922406
Iter: 1299 loss: 0.00016108944
Iter: 1300 loss: 0.000161089483
Iter: 1301 loss: 0.00016054188
Iter: 1302 loss: 0.000167040387
Iter: 1303 loss: 0.000160534008
Iter: 1304 loss: 0.000159996489
Iter: 1305 loss: 0.00015996478
Iter: 1306 loss: 0.00015955462
Iter: 1307 loss: 0.000158917072
Iter: 1308 loss: 0.000158737
Iter: 1309 loss: 0.000158349751
Iter: 1310 loss: 0.000157552873
Iter: 1311 loss: 0.000160521944
Iter: 1312 loss: 0.000157355797
Iter: 1313 loss: 0.000156542577
Iter: 1314 loss: 0.000165905702
Iter: 1315 loss: 0.000156527531
Iter: 1316 loss: 0.000156068782
Iter: 1317 loss: 0.000155287285
Iter: 1318 loss: 0.000155284855
Iter: 1319 loss: 0.00015436273
Iter: 1320 loss: 0.000153702538
Iter: 1321 loss: 0.000153380737
Iter: 1322 loss: 0.000152232227
Iter: 1323 loss: 0.000157868053
Iter: 1324 loss: 0.000152031571
Iter: 1325 loss: 0.000151010041
Iter: 1326 loss: 0.000161354576
Iter: 1327 loss: 0.000150978187
Iter: 1328 loss: 0.000150224208
Iter: 1329 loss: 0.000149089276
Iter: 1330 loss: 0.000149065614
Iter: 1331 loss: 0.000147665647
Iter: 1332 loss: 0.000149664978
Iter: 1333 loss: 0.000146976614
Iter: 1334 loss: 0.000145634345
Iter: 1335 loss: 0.00015611235
Iter: 1336 loss: 0.000145539714
Iter: 1337 loss: 0.000146273698
Iter: 1338 loss: 0.000145212558
Iter: 1339 loss: 0.000144943479
Iter: 1340 loss: 0.000144198653
Iter: 1341 loss: 0.000148471561
Iter: 1342 loss: 0.00014398378
Iter: 1343 loss: 0.000143180499
Iter: 1344 loss: 0.000145381026
Iter: 1345 loss: 0.000142919889
Iter: 1346 loss: 0.000142373217
Iter: 1347 loss: 0.000142332894
Iter: 1348 loss: 0.000141899654
Iter: 1349 loss: 0.000142985562
Iter: 1350 loss: 0.000141749028
Iter: 1351 loss: 0.000141297525
Iter: 1352 loss: 0.000140568911
Iter: 1353 loss: 0.000140563119
Iter: 1354 loss: 0.000139752869
Iter: 1355 loss: 0.000140866236
Iter: 1356 loss: 0.000139348733
Iter: 1357 loss: 0.000138458621
Iter: 1358 loss: 0.000140673626
Iter: 1359 loss: 0.000138146512
Iter: 1360 loss: 0.000137265801
Iter: 1361 loss: 0.000142190693
Iter: 1362 loss: 0.000137141295
Iter: 1363 loss: 0.00013627889
Iter: 1364 loss: 0.000138156785
Iter: 1365 loss: 0.000135943425
Iter: 1366 loss: 0.000135201204
Iter: 1367 loss: 0.000135450071
Iter: 1368 loss: 0.000134674716
Iter: 1369 loss: 0.000133871101
Iter: 1370 loss: 0.000140041171
Iter: 1371 loss: 0.00013380812
Iter: 1372 loss: 0.000133120862
Iter: 1373 loss: 0.000141902885
Iter: 1374 loss: 0.000133116
Iter: 1375 loss: 0.000132788758
Iter: 1376 loss: 0.000131861016
Iter: 1377 loss: 0.000136625065
Iter: 1378 loss: 0.000131557754
Iter: 1379 loss: 0.000130552362
Iter: 1380 loss: 0.000131577253
Iter: 1381 loss: 0.000129990614
Iter: 1382 loss: 0.000131094697
Iter: 1383 loss: 0.00012961321
Iter: 1384 loss: 0.000129356427
Iter: 1385 loss: 0.000128770276
Iter: 1386 loss: 0.000136575691
Iter: 1387 loss: 0.0001287341
Iter: 1388 loss: 0.000127934589
Iter: 1389 loss: 0.000132135188
Iter: 1390 loss: 0.000127808875
Iter: 1391 loss: 0.000127290157
Iter: 1392 loss: 0.000126603656
Iter: 1393 loss: 0.000126563202
Iter: 1394 loss: 0.000125732317
Iter: 1395 loss: 0.000125787978
Iter: 1396 loss: 0.000125081919
Iter: 1397 loss: 0.000124218728
Iter: 1398 loss: 0.000127584
Iter: 1399 loss: 0.000124018843
Iter: 1400 loss: 0.000123241305
Iter: 1401 loss: 0.000127263484
Iter: 1402 loss: 0.000123117032
Iter: 1403 loss: 0.000122460813
Iter: 1404 loss: 0.000123309364
Iter: 1405 loss: 0.000122125464
Iter: 1406 loss: 0.00012162132
Iter: 1407 loss: 0.000129414868
Iter: 1408 loss: 0.000121622732
Iter: 1409 loss: 0.000121066034
Iter: 1410 loss: 0.000122818456
Iter: 1411 loss: 0.000120907163
Iter: 1412 loss: 0.000120631797
Iter: 1413 loss: 0.00012004665
Iter: 1414 loss: 0.000129520064
Iter: 1415 loss: 0.000120029152
Iter: 1416 loss: 0.000119481352
Iter: 1417 loss: 0.000125914434
Iter: 1418 loss: 0.000119473494
Iter: 1419 loss: 0.000118890617
Iter: 1420 loss: 0.000121696212
Iter: 1421 loss: 0.000118787924
Iter: 1422 loss: 0.00011845144
Iter: 1423 loss: 0.000117763193
Iter: 1424 loss: 0.000130153538
Iter: 1425 loss: 0.000117750329
Iter: 1426 loss: 0.00011706003
Iter: 1427 loss: 0.000125911203
Iter: 1428 loss: 0.000117054391
Iter: 1429 loss: 0.000116649186
Iter: 1430 loss: 0.000115927905
Iter: 1431 loss: 0.000133841022
Iter: 1432 loss: 0.00011592712
Iter: 1433 loss: 0.000115221883
Iter: 1434 loss: 0.00011749343
Iter: 1435 loss: 0.000115023431
Iter: 1436 loss: 0.000114419643
Iter: 1437 loss: 0.000122414669
Iter: 1438 loss: 0.000114416209
Iter: 1439 loss: 0.000113934213
Iter: 1440 loss: 0.000114216447
Iter: 1441 loss: 0.000113622729
Iter: 1442 loss: 0.000113078204
Iter: 1443 loss: 0.000113784881
Iter: 1444 loss: 0.00011279976
Iter: 1445 loss: 0.000112488517
Iter: 1446 loss: 0.000112452966
Iter: 1447 loss: 0.000112052941
Iter: 1448 loss: 0.000111935238
Iter: 1449 loss: 0.000111692265
Iter: 1450 loss: 0.000111356552
Iter: 1451 loss: 0.000110681438
Iter: 1452 loss: 0.000123400721
Iter: 1453 loss: 0.000110672365
Iter: 1454 loss: 0.000110958608
Iter: 1455 loss: 0.000110442168
Iter: 1456 loss: 0.000110228277
Iter: 1457 loss: 0.000109661378
Iter: 1458 loss: 0.00011391638
Iter: 1459 loss: 0.000109545057
Iter: 1460 loss: 0.000109076405
Iter: 1461 loss: 0.000112456488
Iter: 1462 loss: 0.000109036766
Iter: 1463 loss: 0.000108525688
Iter: 1464 loss: 0.0001102897
Iter: 1465 loss: 0.000108390872
Iter: 1466 loss: 0.000108032968
Iter: 1467 loss: 0.000107414213
Iter: 1468 loss: 0.000107413216
Iter: 1469 loss: 0.000106726991
Iter: 1470 loss: 0.000111247733
Iter: 1471 loss: 0.000106654734
Iter: 1472 loss: 0.000106117164
Iter: 1473 loss: 0.00010772
Iter: 1474 loss: 0.000105951927
Iter: 1475 loss: 0.000105457017
Iter: 1476 loss: 0.000106835374
Iter: 1477 loss: 0.000105298546
Iter: 1478 loss: 0.00010488824
Iter: 1479 loss: 0.000111109555
Iter: 1480 loss: 0.000104888022
Iter: 1481 loss: 0.000104560168
Iter: 1482 loss: 0.000107365478
Iter: 1483 loss: 0.0001045424
Iter: 1484 loss: 0.000104396473
Iter: 1485 loss: 0.000104012819
Iter: 1486 loss: 0.000106787986
Iter: 1487 loss: 0.000103927516
Iter: 1488 loss: 0.000103689599
Iter: 1489 loss: 0.000103673665
Iter: 1490 loss: 0.000103343235
Iter: 1491 loss: 0.000103088882
Iter: 1492 loss: 0.000102983278
Iter: 1493 loss: 0.00010268396
Iter: 1494 loss: 0.000102084894
Iter: 1495 loss: 0.000113719885
Iter: 1496 loss: 0.000102079757
Iter: 1497 loss: 0.000101668942
Iter: 1498 loss: 0.00010160901
Iter: 1499 loss: 0.000101180107
Iter: 1500 loss: 0.000100935409
Iter: 1501 loss: 0.000100749836
Iter: 1502 loss: 0.000100279474
Iter: 1503 loss: 0.000100276593
Iter: 1504 loss: 9.990135e-05
Iter: 1505 loss: 9.93962749e-05
Iter: 1506 loss: 0.000101792262
Iter: 1507 loss: 9.93064677e-05
Iter: 1508 loss: 9.88796673e-05
Iter: 1509 loss: 0.00010130585
Iter: 1510 loss: 9.88201209e-05
Iter: 1511 loss: 9.84475555e-05
Iter: 1512 loss: 9.90489061e-05
Iter: 1513 loss: 9.82756465e-05
Iter: 1514 loss: 9.83564387e-05
Iter: 1515 loss: 9.80750774e-05
Iter: 1516 loss: 9.79404649e-05
Iter: 1517 loss: 9.75910516e-05
Iter: 1518 loss: 0.00010030788
Iter: 1519 loss: 9.75241346e-05
Iter: 1520 loss: 9.71336121e-05
Iter: 1521 loss: 9.90820699e-05
Iter: 1522 loss: 9.70675e-05
Iter: 1523 loss: 9.67224041e-05
Iter: 1524 loss: 0.000101713755
Iter: 1525 loss: 9.67213e-05
Iter: 1526 loss: 9.65620711e-05
Iter: 1527 loss: 9.61733895e-05
Iter: 1528 loss: 0.000100297075
Iter: 1529 loss: 9.61310579e-05
Iter: 1530 loss: 9.57318e-05
Iter: 1531 loss: 9.57558e-05
Iter: 1532 loss: 9.54221e-05
Iter: 1533 loss: 9.50085232e-05
Iter: 1534 loss: 9.95081282e-05
Iter: 1535 loss: 9.49984751e-05
Iter: 1536 loss: 9.45635838e-05
Iter: 1537 loss: 9.54852731e-05
Iter: 1538 loss: 9.4392446e-05
Iter: 1539 loss: 9.39989695e-05
Iter: 1540 loss: 9.39786114e-05
Iter: 1541 loss: 9.36778888e-05
Iter: 1542 loss: 9.32338298e-05
Iter: 1543 loss: 9.78555909e-05
Iter: 1544 loss: 9.32189287e-05
Iter: 1545 loss: 9.2849994e-05
Iter: 1546 loss: 9.29651142e-05
Iter: 1547 loss: 9.25858039e-05
Iter: 1548 loss: 9.22859545e-05
Iter: 1549 loss: 9.22810796e-05
Iter: 1550 loss: 9.19546292e-05
Iter: 1551 loss: 9.2371e-05
Iter: 1552 loss: 9.17875295e-05
Iter: 1553 loss: 9.15622077e-05
Iter: 1554 loss: 9.1284e-05
Iter: 1555 loss: 9.12596661e-05
Iter: 1556 loss: 9.11144161e-05
Iter: 1557 loss: 9.10446761e-05
Iter: 1558 loss: 9.08778238e-05
Iter: 1559 loss: 9.05351699e-05
Iter: 1560 loss: 9.67312662e-05
Iter: 1561 loss: 9.05314373e-05
Iter: 1562 loss: 9.01750172e-05
Iter: 1563 loss: 9.00490486e-05
Iter: 1564 loss: 8.98483559e-05
Iter: 1565 loss: 8.93077813e-05
Iter: 1566 loss: 9.01344465e-05
Iter: 1567 loss: 8.90507654e-05
Iter: 1568 loss: 8.86418e-05
Iter: 1569 loss: 8.86412236e-05
Iter: 1570 loss: 8.82891909e-05
Iter: 1571 loss: 8.98649887e-05
Iter: 1572 loss: 8.82196182e-05
Iter: 1573 loss: 8.79885192e-05
Iter: 1574 loss: 8.81553e-05
Iter: 1575 loss: 8.78461506e-05
Iter: 1576 loss: 8.75283076e-05
Iter: 1577 loss: 8.8588582e-05
Iter: 1578 loss: 8.74418329e-05
Iter: 1579 loss: 8.71794182e-05
Iter: 1580 loss: 8.90349911e-05
Iter: 1581 loss: 8.71544617e-05
Iter: 1582 loss: 8.6896107e-05
Iter: 1583 loss: 8.85725385e-05
Iter: 1584 loss: 8.68686475e-05
Iter: 1585 loss: 8.669417e-05
Iter: 1586 loss: 8.63407622e-05
Iter: 1587 loss: 9.29873131e-05
Iter: 1588 loss: 8.63345922e-05
Iter: 1589 loss: 8.61846493e-05
Iter: 1590 loss: 8.61404551e-05
Iter: 1591 loss: 8.59552383e-05
Iter: 1592 loss: 8.56051411e-05
Iter: 1593 loss: 9.31810646e-05
Iter: 1594 loss: 8.56037368e-05
Iter: 1595 loss: 8.52018275e-05
Iter: 1596 loss: 8.53666279e-05
Iter: 1597 loss: 8.49239295e-05
Iter: 1598 loss: 8.44755341e-05
Iter: 1599 loss: 8.49558055e-05
Iter: 1600 loss: 8.42291556e-05
Iter: 1601 loss: 8.37934058e-05
Iter: 1602 loss: 8.4872765e-05
Iter: 1603 loss: 8.36393e-05
Iter: 1604 loss: 8.34144957e-05
Iter: 1605 loss: 8.33681333e-05
Iter: 1606 loss: 8.31477155e-05
Iter: 1607 loss: 8.28398042e-05
Iter: 1608 loss: 8.28275734e-05
Iter: 1609 loss: 8.2473518e-05
Iter: 1610 loss: 8.37650077e-05
Iter: 1611 loss: 8.23859591e-05
Iter: 1612 loss: 8.20789792e-05
Iter: 1613 loss: 8.33187078e-05
Iter: 1614 loss: 8.20112909e-05
Iter: 1615 loss: 8.17794717e-05
Iter: 1616 loss: 8.35523315e-05
Iter: 1617 loss: 8.1761711e-05
Iter: 1618 loss: 8.15484527e-05
Iter: 1619 loss: 8.13058e-05
Iter: 1620 loss: 8.12739308e-05
Iter: 1621 loss: 8.10600322e-05
Iter: 1622 loss: 8.34772218e-05
Iter: 1623 loss: 8.10564379e-05
Iter: 1624 loss: 8.08085606e-05
Iter: 1625 loss: 8.08870536e-05
Iter: 1626 loss: 8.06342141e-05
Iter: 1627 loss: 8.04226584e-05
Iter: 1628 loss: 8.01802671e-05
Iter: 1629 loss: 8.01529386e-05
Iter: 1630 loss: 7.97744069e-05
Iter: 1631 loss: 7.99814443e-05
Iter: 1632 loss: 7.95261949e-05
Iter: 1633 loss: 7.91070925e-05
Iter: 1634 loss: 7.97309185e-05
Iter: 1635 loss: 7.89053302e-05
Iter: 1636 loss: 7.86384553e-05
Iter: 1637 loss: 7.86256278e-05
Iter: 1638 loss: 7.83321e-05
Iter: 1639 loss: 7.84045e-05
Iter: 1640 loss: 7.81174167e-05
Iter: 1641 loss: 7.78615577e-05
Iter: 1642 loss: 8.00411362e-05
Iter: 1643 loss: 7.78480171e-05
Iter: 1644 loss: 7.76549932e-05
Iter: 1645 loss: 7.8473e-05
Iter: 1646 loss: 7.7611774e-05
Iter: 1647 loss: 7.74709624e-05
Iter: 1648 loss: 7.79339171e-05
Iter: 1649 loss: 7.74308719e-05
Iter: 1650 loss: 7.72480344e-05
Iter: 1651 loss: 7.72738276e-05
Iter: 1652 loss: 7.71071645e-05
Iter: 1653 loss: 7.69356557e-05
Iter: 1654 loss: 7.73426873e-05
Iter: 1655 loss: 7.68733444e-05
Iter: 1656 loss: 7.6643264e-05
Iter: 1657 loss: 7.80047849e-05
Iter: 1658 loss: 7.66129815e-05
Iter: 1659 loss: 7.64959696e-05
Iter: 1660 loss: 7.62146519e-05
Iter: 1661 loss: 7.92610881e-05
Iter: 1662 loss: 7.61847623e-05
Iter: 1663 loss: 7.58072711e-05
Iter: 1664 loss: 7.63426215e-05
Iter: 1665 loss: 7.56213703e-05
Iter: 1666 loss: 7.52501219e-05
Iter: 1667 loss: 7.59079849e-05
Iter: 1668 loss: 7.50871477e-05
Iter: 1669 loss: 7.47266095e-05
Iter: 1670 loss: 7.65248697e-05
Iter: 1671 loss: 7.46642254e-05
Iter: 1672 loss: 7.43857745e-05
Iter: 1673 loss: 7.43850251e-05
Iter: 1674 loss: 7.42188204e-05
Iter: 1675 loss: 7.42155826e-05
Iter: 1676 loss: 7.40840333e-05
Iter: 1677 loss: 7.38827e-05
Iter: 1678 loss: 7.69304897e-05
Iter: 1679 loss: 7.38835515e-05
Iter: 1680 loss: 7.37726805e-05
Iter: 1681 loss: 7.3722018e-05
Iter: 1682 loss: 7.36673828e-05
Iter: 1683 loss: 7.34437199e-05
Iter: 1684 loss: 7.40092219e-05
Iter: 1685 loss: 7.33647612e-05
Iter: 1686 loss: 7.31922191e-05
Iter: 1687 loss: 7.32037e-05
Iter: 1688 loss: 7.30576867e-05
Iter: 1689 loss: 7.28222294e-05
Iter: 1690 loss: 7.59969189e-05
Iter: 1691 loss: 7.28213417e-05
Iter: 1692 loss: 7.27008592e-05
Iter: 1693 loss: 7.23826961e-05
Iter: 1694 loss: 7.47945742e-05
Iter: 1695 loss: 7.23196354e-05
Iter: 1696 loss: 7.19373929e-05
Iter: 1697 loss: 7.30088941e-05
Iter: 1698 loss: 7.18139272e-05
Iter: 1699 loss: 7.14818307e-05
Iter: 1700 loss: 7.19893724e-05
Iter: 1701 loss: 7.13258705e-05
Iter: 1702 loss: 7.09715532e-05
Iter: 1703 loss: 7.1614224e-05
Iter: 1704 loss: 7.08174412e-05
Iter: 1705 loss: 7.06600913e-05
Iter: 1706 loss: 7.06156789e-05
Iter: 1707 loss: 7.04246486e-05
Iter: 1708 loss: 7.01878744e-05
Iter: 1709 loss: 7.01688405e-05
Iter: 1710 loss: 7.0050286e-05
Iter: 1711 loss: 6.99899392e-05
Iter: 1712 loss: 6.98761141e-05
Iter: 1713 loss: 6.97533687e-05
Iter: 1714 loss: 6.97339856e-05
Iter: 1715 loss: 6.94993214e-05
Iter: 1716 loss: 7.03275873e-05
Iter: 1717 loss: 6.94381961e-05
Iter: 1718 loss: 6.92673493e-05
Iter: 1719 loss: 6.92212852e-05
Iter: 1720 loss: 6.91149762e-05
Iter: 1721 loss: 6.89408189e-05
Iter: 1722 loss: 6.89344452e-05
Iter: 1723 loss: 6.88292203e-05
Iter: 1724 loss: 6.8558e-05
Iter: 1725 loss: 7.06944265e-05
Iter: 1726 loss: 6.85066843e-05
Iter: 1727 loss: 6.81526508e-05
Iter: 1728 loss: 6.86465573e-05
Iter: 1729 loss: 6.79786681e-05
Iter: 1730 loss: 6.75907359e-05
Iter: 1731 loss: 6.79439327e-05
Iter: 1732 loss: 6.73656323e-05
Iter: 1733 loss: 6.6956869e-05
Iter: 1734 loss: 6.75742122e-05
Iter: 1735 loss: 6.67612767e-05
Iter: 1736 loss: 6.64104e-05
Iter: 1737 loss: 6.98960721e-05
Iter: 1738 loss: 6.63991086e-05
Iter: 1739 loss: 6.60603e-05
Iter: 1740 loss: 6.82273385e-05
Iter: 1741 loss: 6.60228834e-05
Iter: 1742 loss: 6.58669742e-05
Iter: 1743 loss: 6.67298445e-05
Iter: 1744 loss: 6.58444624e-05
Iter: 1745 loss: 6.56669217e-05
Iter: 1746 loss: 6.60468431e-05
Iter: 1747 loss: 6.55967233e-05
Iter: 1748 loss: 6.54468313e-05
Iter: 1749 loss: 6.56570774e-05
Iter: 1750 loss: 6.5373184e-05
Iter: 1751 loss: 6.52070448e-05
Iter: 1752 loss: 6.52511517e-05
Iter: 1753 loss: 6.5087268e-05
Iter: 1754 loss: 6.49601789e-05
Iter: 1755 loss: 6.49527065e-05
Iter: 1756 loss: 6.48458299e-05
Iter: 1757 loss: 6.45572654e-05
Iter: 1758 loss: 6.63931423e-05
Iter: 1759 loss: 6.44819302e-05
Iter: 1760 loss: 6.41256629e-05
Iter: 1761 loss: 6.47308188e-05
Iter: 1762 loss: 6.39637437e-05
Iter: 1763 loss: 6.36008335e-05
Iter: 1764 loss: 6.46584085e-05
Iter: 1765 loss: 6.34867174e-05
Iter: 1766 loss: 6.31798466e-05
Iter: 1767 loss: 6.37377525e-05
Iter: 1768 loss: 6.30457071e-05
Iter: 1769 loss: 6.27561676e-05
Iter: 1770 loss: 6.36818877e-05
Iter: 1771 loss: 6.26736655e-05
Iter: 1772 loss: 6.25192333e-05
Iter: 1773 loss: 6.24974491e-05
Iter: 1774 loss: 6.23602e-05
Iter: 1775 loss: 6.23338565e-05
Iter: 1776 loss: 6.2242776e-05
Iter: 1777 loss: 6.21110376e-05
Iter: 1778 loss: 6.21048675e-05
Iter: 1779 loss: 6.20440405e-05
Iter: 1780 loss: 6.19023558e-05
Iter: 1781 loss: 6.36935511e-05
Iter: 1782 loss: 6.18917838e-05
Iter: 1783 loss: 6.16810721e-05
Iter: 1784 loss: 6.20313658e-05
Iter: 1785 loss: 6.15846657e-05
Iter: 1786 loss: 6.1455823e-05
Iter: 1787 loss: 6.14560267e-05
Iter: 1788 loss: 6.13285e-05
Iter: 1789 loss: 6.10893694e-05
Iter: 1790 loss: 6.63137762e-05
Iter: 1791 loss: 6.10876887e-05
Iter: 1792 loss: 6.08801565e-05
Iter: 1793 loss: 6.09140079e-05
Iter: 1794 loss: 6.07231923e-05
Iter: 1795 loss: 6.04853558e-05
Iter: 1796 loss: 6.13178854e-05
Iter: 1797 loss: 6.04235538e-05
Iter: 1798 loss: 6.0207e-05
Iter: 1799 loss: 6.05345704e-05
Iter: 1800 loss: 6.01046631e-05
Iter: 1801 loss: 5.98577353e-05
Iter: 1802 loss: 6.02474065e-05
Iter: 1803 loss: 5.97432518e-05
Iter: 1804 loss: 5.95237143e-05
Iter: 1805 loss: 6.27361733e-05
Iter: 1806 loss: 5.95235506e-05
Iter: 1807 loss: 5.93231525e-05
Iter: 1808 loss: 6.03471e-05
Iter: 1809 loss: 5.92902798e-05
Iter: 1810 loss: 5.92182878e-05
Iter: 1811 loss: 5.92080396e-05
Iter: 1812 loss: 5.91512435e-05
Iter: 1813 loss: 5.90110103e-05
Iter: 1814 loss: 6.03535227e-05
Iter: 1815 loss: 5.89921037e-05
Iter: 1816 loss: 5.88311632e-05
Iter: 1817 loss: 5.99428677e-05
Iter: 1818 loss: 5.88155963e-05
Iter: 1819 loss: 5.87281174e-05
Iter: 1820 loss: 5.92265715e-05
Iter: 1821 loss: 5.87159702e-05
Iter: 1822 loss: 5.86000278e-05
Iter: 1823 loss: 5.84644404e-05
Iter: 1824 loss: 5.84491718e-05
Iter: 1825 loss: 5.82972061e-05
Iter: 1826 loss: 5.81489221e-05
Iter: 1827 loss: 5.81180757e-05
Iter: 1828 loss: 5.7864243e-05
Iter: 1829 loss: 5.84568443e-05
Iter: 1830 loss: 5.77710816e-05
Iter: 1831 loss: 5.75074519e-05
Iter: 1832 loss: 5.79954467e-05
Iter: 1833 loss: 5.73934158e-05
Iter: 1834 loss: 5.71244673e-05
Iter: 1835 loss: 5.78695472e-05
Iter: 1836 loss: 5.70376833e-05
Iter: 1837 loss: 5.68179967e-05
Iter: 1838 loss: 5.85882226e-05
Iter: 1839 loss: 5.68023752e-05
Iter: 1840 loss: 5.66428644e-05
Iter: 1841 loss: 5.91124845e-05
Iter: 1842 loss: 5.66435629e-05
Iter: 1843 loss: 5.6571982e-05
Iter: 1844 loss: 5.65718365e-05
Iter: 1845 loss: 5.65091214e-05
Iter: 1846 loss: 5.63650647e-05
Iter: 1847 loss: 5.8263111e-05
Iter: 1848 loss: 5.63554931e-05
Iter: 1849 loss: 5.62265e-05
Iter: 1850 loss: 5.73721554e-05
Iter: 1851 loss: 5.62206806e-05
Iter: 1852 loss: 5.6126144e-05
Iter: 1853 loss: 5.62396781e-05
Iter: 1854 loss: 5.60760091e-05
Iter: 1855 loss: 5.59066393e-05
Iter: 1856 loss: 5.59354266e-05
Iter: 1857 loss: 5.57787353e-05
Iter: 1858 loss: 5.56247833e-05
Iter: 1859 loss: 5.54107755e-05
Iter: 1860 loss: 5.54022336e-05
Iter: 1861 loss: 5.51230296e-05
Iter: 1862 loss: 5.59139626e-05
Iter: 1863 loss: 5.50341501e-05
Iter: 1864 loss: 5.47833188e-05
Iter: 1865 loss: 5.54485414e-05
Iter: 1866 loss: 5.4698281e-05
Iter: 1867 loss: 5.44748764e-05
Iter: 1868 loss: 5.51202866e-05
Iter: 1869 loss: 5.44050781e-05
Iter: 1870 loss: 5.42054477e-05
Iter: 1871 loss: 5.50322802e-05
Iter: 1872 loss: 5.41620284e-05
Iter: 1873 loss: 5.40140099e-05
Iter: 1874 loss: 5.40139e-05
Iter: 1875 loss: 5.39225221e-05
Iter: 1876 loss: 5.50830628e-05
Iter: 1877 loss: 5.39231696e-05
Iter: 1878 loss: 5.3833206e-05
Iter: 1879 loss: 5.36779116e-05
Iter: 1880 loss: 5.3677486e-05
Iter: 1881 loss: 5.35447107e-05
Iter: 1882 loss: 5.41488589e-05
Iter: 1883 loss: 5.35196232e-05
Iter: 1884 loss: 5.3389198e-05
Iter: 1885 loss: 5.37115047e-05
Iter: 1886 loss: 5.33434468e-05
Iter: 1887 loss: 5.31864862e-05
Iter: 1888 loss: 5.36621228e-05
Iter: 1889 loss: 5.31391634e-05
Iter: 1890 loss: 5.30433499e-05
Iter: 1891 loss: 5.28481869e-05
Iter: 1892 loss: 5.64365873e-05
Iter: 1893 loss: 5.28451274e-05
Iter: 1894 loss: 5.26159129e-05
Iter: 1895 loss: 5.31306869e-05
Iter: 1896 loss: 5.25293799e-05
Iter: 1897 loss: 5.22825721e-05
Iter: 1898 loss: 5.27212651e-05
Iter: 1899 loss: 5.21738039e-05
Iter: 1900 loss: 5.19229288e-05
Iter: 1901 loss: 5.2722251e-05
Iter: 1902 loss: 5.18513261e-05
Iter: 1903 loss: 5.1629635e-05
Iter: 1904 loss: 5.23935523e-05
Iter: 1905 loss: 5.15717438e-05
Iter: 1906 loss: 5.14155181e-05
Iter: 1907 loss: 5.14161111e-05
Iter: 1908 loss: 5.13169471e-05
Iter: 1909 loss: 5.1316747e-05
Iter: 1910 loss: 5.12264705e-05
Iter: 1911 loss: 5.11539547e-05
Iter: 1912 loss: 5.11265971e-05
Iter: 1913 loss: 5.10345781e-05
Iter: 1914 loss: 5.11944454e-05
Iter: 1915 loss: 5.09944439e-05
Iter: 1916 loss: 5.08879602e-05
Iter: 1917 loss: 5.13657615e-05
Iter: 1918 loss: 5.08663397e-05
Iter: 1919 loss: 5.07759432e-05
Iter: 1920 loss: 5.1174331e-05
Iter: 1921 loss: 5.07587392e-05
Iter: 1922 loss: 5.069502e-05
Iter: 1923 loss: 5.05440512e-05
Iter: 1924 loss: 5.23728777e-05
Iter: 1925 loss: 5.05322605e-05
Iter: 1926 loss: 5.03490046e-05
Iter: 1927 loss: 5.06484066e-05
Iter: 1928 loss: 5.02650582e-05
Iter: 1929 loss: 5.00478927e-05
Iter: 1930 loss: 5.0615592e-05
Iter: 1931 loss: 4.99746311e-05
Iter: 1932 loss: 4.97859728e-05
Iter: 1933 loss: 5.03305528e-05
Iter: 1934 loss: 4.97283763e-05
Iter: 1935 loss: 4.95625063e-05
Iter: 1936 loss: 5.0212213e-05
Iter: 1937 loss: 4.95234e-05
Iter: 1938 loss: 4.93952e-05
Iter: 1939 loss: 5.06496071e-05
Iter: 1940 loss: 4.93914558e-05
Iter: 1941 loss: 4.93054067e-05
Iter: 1942 loss: 4.93056577e-05
Iter: 1943 loss: 4.92255494e-05
Iter: 1944 loss: 4.92780964e-05
Iter: 1945 loss: 4.91739156e-05
Iter: 1946 loss: 4.911203e-05
Iter: 1947 loss: 4.90404782e-05
Iter: 1948 loss: 4.90323764e-05
Iter: 1949 loss: 4.89009326e-05
Iter: 1950 loss: 4.96494285e-05
Iter: 1951 loss: 4.88820951e-05
Iter: 1952 loss: 4.87882e-05
Iter: 1953 loss: 4.91882456e-05
Iter: 1954 loss: 4.87682482e-05
Iter: 1955 loss: 4.86821446e-05
Iter: 1956 loss: 4.85091114e-05
Iter: 1957 loss: 5.17651642e-05
Iter: 1958 loss: 4.85066921e-05
Iter: 1959 loss: 4.83258846e-05
Iter: 1960 loss: 4.85306955e-05
Iter: 1961 loss: 4.82294417e-05
Iter: 1962 loss: 4.80201961e-05
Iter: 1963 loss: 4.88610422e-05
Iter: 1964 loss: 4.79728405e-05
Iter: 1965 loss: 4.78085603e-05
Iter: 1966 loss: 4.81196803e-05
Iter: 1967 loss: 4.77395406e-05
Iter: 1968 loss: 4.75748893e-05
Iter: 1969 loss: 4.81674506e-05
Iter: 1970 loss: 4.75326597e-05
Iter: 1971 loss: 4.7385729e-05
Iter: 1972 loss: 4.82294818e-05
Iter: 1973 loss: 4.73663495e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.4
+ date
Sun Nov  8 14:46:23 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a76c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a5c5e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a6a8d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a5d3d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a5d3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a51aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a4c9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a453598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a483158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a560510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a48e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a4987b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a498ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623da33620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a40c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623da33950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f624a41c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d9dcea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d9e1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d9f5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d8e1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d8b06a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d8666a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d885840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d8851e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d972378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d949840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d947950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d947730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d7d4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d7e27b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d779048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d779950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d75f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d6e9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f623d6c1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.41227785
test_loss: 0.4082657
train_loss: 0.4040471
test_loss: 0.4039109
train_loss: 0.41429153
test_loss: 0.40393004
train_loss: 0.41224855
test_loss: 0.40395027
train_loss: 0.40077212
test_loss: 0.403886
train_loss: 0.40124243
test_loss: 0.40386674
train_loss: 0.40221363
test_loss: 0.40382835
train_loss: 0.40664703
test_loss: 0.40379897
train_loss: 0.40434477
test_loss: 0.403765
train_loss: 0.40362495
test_loss: 0.4036975
train_loss: 0.40908366
test_loss: 0.4036396
train_loss: 0.4039861
test_loss: 0.40363654
train_loss: 0.3977197
test_loss: 0.40357807
train_loss: 0.4009234
test_loss: 0.40353853
train_loss: 0.39998043
test_loss: 0.4034101
train_loss: 0.40735582
test_loss: 0.4033965
train_loss: 0.4040277
test_loss: 0.4033457
train_loss: 0.40260717
test_loss: 0.40327376
train_loss: 0.40210497
test_loss: 0.40315408
train_loss: 0.40239277
test_loss: 0.40312657
train_loss: 0.40801305
test_loss: 0.4030356
train_loss: 0.40468532
test_loss: 0.40294838
train_loss: 0.40918738
test_loss: 0.40284228
train_loss: 0.3971514
test_loss: 0.40270728
train_loss: 0.40230167
test_loss: 0.4026275
train_loss: 0.39949667
test_loss: 0.40249917
train_loss: 0.40636083
test_loss: 0.4024004
train_loss: 0.39958578
test_loss: 0.40230092
train_loss: 0.39530483
test_loss: 0.4021356
train_loss: 0.40488437
test_loss: 0.40200752
train_loss: 0.39976576
test_loss: 0.40187103
train_loss: 0.40373722
test_loss: 0.40172267
train_loss: 0.40870675
test_loss: 0.4015335
train_loss: 0.39141238
test_loss: 0.40140536
train_loss: 0.3961532
test_loss: 0.4012443
train_loss: 0.38994056
test_loss: 0.40102753
train_loss: 0.39950156
test_loss: 0.40081838
train_loss: 0.4006749
test_loss: 0.40059653
train_loss: 0.40483814
test_loss: 0.40043435
train_loss: 0.40315008
test_loss: 0.40015206
train_loss: 0.39484134
test_loss: 0.39992058
train_loss: 0.40096188
test_loss: 0.39965448
train_loss: 0.39975166
test_loss: 0.3993797
train_loss: 0.40038294
test_loss: 0.39911595
train_loss: 0.39360863
test_loss: 0.3988375
train_loss: 0.40356696
test_loss: 0.3985451
train_loss: 0.4008484
test_loss: 0.3981807
train_loss: 0.40121934
test_loss: 0.3978332
train_loss: 0.40472424
test_loss: 0.39750633
train_loss: 0.39354452
test_loss: 0.3971158
train_loss: 0.39142564
test_loss: 0.39674315
train_loss: 0.39736956
test_loss: 0.39631215
train_loss: 0.39958522
test_loss: 0.395867
train_loss: 0.39481175
test_loss: 0.39541733
train_loss: 0.39321345
test_loss: 0.39488113
train_loss: 0.39583284
test_loss: 0.39441663
train_loss: 0.3883125
test_loss: 0.39386478
train_loss: 0.39141172
test_loss: 0.39333338
train_loss: 0.39605972
test_loss: 0.39276654
train_loss: 0.39496577
test_loss: 0.3921382
train_loss: 0.3919925
test_loss: 0.39143878
train_loss: 0.3892085
test_loss: 0.39081708
train_loss: 0.39711806
test_loss: 0.39010608
train_loss: 0.38185486
test_loss: 0.38937074
train_loss: 0.3907802
test_loss: 0.38859162
train_loss: 0.38965416
test_loss: 0.3877621
train_loss: 0.3848688
test_loss: 0.38691851
train_loss: 0.3854093
test_loss: 0.38600096
train_loss: 0.38189098
test_loss: 0.38508224
train_loss: 0.38514575
test_loss: 0.3841057
train_loss: 0.37883878
test_loss: 0.38304648
train_loss: 0.37868792
test_loss: 0.38195413
train_loss: 0.38286263
test_loss: 0.38083294
train_loss: 0.3746828
test_loss: 0.37965018
train_loss: 0.38049233
test_loss: 0.3783341
train_loss: 0.3822375
test_loss: 0.37705812
train_loss: 0.3796229
test_loss: 0.37568444
train_loss: 0.37610438
test_loss: 0.37425762
train_loss: 0.36814526
test_loss: 0.37275556
train_loss: 0.37497655
test_loss: 0.3711827
train_loss: 0.36535916
test_loss: 0.3695413
train_loss: 0.36498773
test_loss: 0.36781797
train_loss: 0.3759227
test_loss: 0.36603132
train_loss: 0.3653143
test_loss: 0.36414838
train_loss: 0.36637586
test_loss: 0.36217672
train_loss: 0.36443648
test_loss: 0.36007634
train_loss: 0.35907173
test_loss: 0.35796964
train_loss: 0.35704294
test_loss: 0.35571888
train_loss: 0.34643638
test_loss: 0.35337022
train_loss: 0.34899065
test_loss: 0.35088632
train_loss: 0.34298104
test_loss: 0.3483262
train_loss: 0.34606734
test_loss: 0.3456597
train_loss: 0.3423369
test_loss: 0.34292325
train_loss: 0.335162
test_loss: 0.3399908
train_loss: 0.3373921
test_loss: 0.3369422
train_loss: 0.32950234
test_loss: 0.33374462
train_loss: 0.323861
test_loss: 0.33045125
train_loss: 0.32479388
test_loss: 0.32700795
train_loss: 0.31898737
test_loss: 0.32346487
train_loss: 0.31761304
test_loss: 0.31972498
train_loss: 0.31231686
test_loss: 0.31581038
train_loss: 0.3128206
test_loss: 0.3117593
train_loss: 0.31004807
test_loss: 0.3075784
train_loss: 0.3021256
test_loss: 0.30314693
train_loss: 0.29777467
test_loss: 0.29857567
train_loss: 0.30042565
test_loss: 0.29381964
train_loss: 0.28879708
test_loss: 0.28884268
train_loss: 0.28360298
test_loss: 0.28368366
train_loss: 0.28352028
test_loss: 0.27830154
train_loss: 0.27289268
test_loss: 0.27271488
train_loss: 0.26548937
test_loss: 0.2668868
train_loss: 0.2575003
test_loss: 0.26078144
train_loss: 0.2608684
test_loss: 0.25439626
train_loss: 0.24742454
test_loss: 0.2477854
train_loss: 0.23842645
test_loss: 0.24080935
train_loss: 0.23152012
test_loss: 0.23358113
train_loss: 0.22275421
test_loss: 0.22598492
train_loss: 0.22011997
test_loss: 0.21809861
train_loss: 0.20813034
test_loss: 0.20988366
train_loss: 0.19854742
test_loss: 0.20136683
train_loss: 0.1952934
test_loss: 0.19255048
train_loss: 0.18142685
test_loss: 0.18350591
train_loss: 0.17004187
test_loss: 0.1743147
train_loss: 0.16582485
test_loss: 0.16508563
train_loss: 0.15721896
test_loss: 0.15598422
train_loss: 0.14675064
test_loss: 0.14718965
train_loss: 0.1375018
test_loss: 0.13885118
train_loss: 0.13038617
test_loss: 0.1311195
train_loss: 0.12272507
test_loss: 0.1240436
train_loss: 0.1166116
test_loss: 0.11768691
train_loss: 0.111613825
test_loss: 0.111995496
train_loss: 0.10641123
test_loss: 0.10694598
train_loss: 0.10558421
test_loss: 0.10250462
train_loss: 0.09676093
test_loss: 0.09863723
train_loss: 0.0953398
test_loss: 0.09531148
train_loss: 0.09271586
test_loss: 0.09245911
train_loss: 0.090756975
test_loss: 0.09003472
train_loss: 0.08664426
test_loss: 0.08798351
train_loss: 0.08563602
test_loss: 0.086255714
train_loss: 0.08401022
test_loss: 0.08479298
train_loss: 0.08435223
test_loss: 0.0835681
train_loss: 0.083201334
test_loss: 0.08252132
train_loss: 0.080240846
test_loss: 0.0816443
train_loss: 0.080833346
test_loss: 0.080873825
train_loss: 0.07907588
test_loss: 0.080210544
train_loss: 0.07967731
test_loss: 0.079610445
train_loss: 0.07999088
test_loss: 0.07907913
train_loss: 0.077032186
test_loss: 0.078588516
train_loss: 0.07688173
test_loss: 0.07812951
train_loss: 0.076900706
test_loss: 0.07769064
train_loss: 0.077897415
test_loss: 0.0772804
train_loss: 0.076946445
test_loss: 0.07686684
train_loss: 0.07715712
test_loss: 0.07646242
train_loss: 0.07641668
test_loss: 0.07606035
train_loss: 0.07604802
test_loss: 0.07566667
train_loss: 0.07397553
test_loss: 0.075278305
train_loss: 0.074443914
test_loss: 0.07487156
train_loss: 0.07549229
test_loss: 0.074471675
train_loss: 0.07555216
test_loss: 0.074054316
train_loss: 0.074010015
test_loss: 0.07365817
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.4/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c6a62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c795d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c795c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c6f8ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c707488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c67e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c637c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c5de7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c5f2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c5b0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c5b0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c580f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c577730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c50dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c50d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c4ef8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c495598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c495d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c4619d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c461488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c461158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c3e3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c3ab950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c3bc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c3bc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f315c35a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31443fd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3144422950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31443a7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31443d9400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f314437a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31443a1158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3144336840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3144352620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f311fe908c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f311fe362f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.00910227746
Iter: 2 loss: 0.0089699775
Iter: 3 loss: 0.0100624356
Iter: 4 loss: 0.00896180794
Iter: 5 loss: 0.0089186
Iter: 6 loss: 0.00891487394
Iter: 7 loss: 0.00887179
Iter: 8 loss: 0.0087988656
Iter: 9 loss: 0.00879876129
Iter: 10 loss: 0.00871505
Iter: 11 loss: 0.0087860534
Iter: 12 loss: 0.00866632629
Iter: 13 loss: 0.00852621906
Iter: 14 loss: 0.00918190554
Iter: 15 loss: 0.0085028708
Iter: 16 loss: 0.00839066692
Iter: 17 loss: 0.00812191702
Iter: 18 loss: 0.0114598107
Iter: 19 loss: 0.00809687
Iter: 20 loss: 0.00785575435
Iter: 21 loss: 0.0111922147
Iter: 22 loss: 0.00785539951
Iter: 23 loss: 0.00763355289
Iter: 24 loss: 0.00780512951
Iter: 25 loss: 0.0075126295
Iter: 26 loss: 0.00735478895
Iter: 27 loss: 0.00899522938
Iter: 28 loss: 0.00734686619
Iter: 29 loss: 0.00720881764
Iter: 30 loss: 0.00754376128
Iter: 31 loss: 0.00716473581
Iter: 32 loss: 0.00707468856
Iter: 33 loss: 0.00705687329
Iter: 34 loss: 0.00700063072
Iter: 35 loss: 0.0069395816
Iter: 36 loss: 0.00697217695
Iter: 37 loss: 0.00690024765
Iter: 38 loss: 0.00686646672
Iter: 39 loss: 0.00695479289
Iter: 40 loss: 0.00685513252
Iter: 41 loss: 0.00681141065
Iter: 42 loss: 0.0073201484
Iter: 43 loss: 0.00681039
Iter: 44 loss: 0.00677411584
Iter: 45 loss: 0.00682338886
Iter: 46 loss: 0.00675644958
Iter: 47 loss: 0.00671755569
Iter: 48 loss: 0.00680492353
Iter: 49 loss: 0.00670283427
Iter: 50 loss: 0.00665169396
Iter: 51 loss: 0.00672010705
Iter: 52 loss: 0.00662585814
Iter: 53 loss: 0.0065676244
Iter: 54 loss: 0.00655284338
Iter: 55 loss: 0.0065169814
Iter: 56 loss: 0.00645280723
Iter: 57 loss: 0.0064056241
Iter: 58 loss: 0.00638434151
Iter: 59 loss: 0.0063073351
Iter: 60 loss: 0.0064717452
Iter: 61 loss: 0.00627671229
Iter: 62 loss: 0.00619100733
Iter: 63 loss: 0.00664061541
Iter: 64 loss: 0.00617869571
Iter: 65 loss: 0.00612725783
Iter: 66 loss: 0.00612865807
Iter: 67 loss: 0.00608772133
Iter: 68 loss: 0.00603241101
Iter: 69 loss: 0.00611212244
Iter: 70 loss: 0.00600218773
Iter: 71 loss: 0.00594219472
Iter: 72 loss: 0.00647506909
Iter: 73 loss: 0.00593881216
Iter: 74 loss: 0.00587498629
Iter: 75 loss: 0.00640340941
Iter: 76 loss: 0.00587123679
Iter: 77 loss: 0.00580976531
Iter: 78 loss: 0.00630124239
Iter: 79 loss: 0.00580613269
Iter: 80 loss: 0.00577145815
Iter: 81 loss: 0.00590776466
Iter: 82 loss: 0.00576359034
Iter: 83 loss: 0.00572892
Iter: 84 loss: 0.00567057263
Iter: 85 loss: 0.00567039941
Iter: 86 loss: 0.00559975393
Iter: 87 loss: 0.00569472555
Iter: 88 loss: 0.00556315109
Iter: 89 loss: 0.00548265455
Iter: 90 loss: 0.00537680183
Iter: 91 loss: 0.00537065137
Iter: 92 loss: 0.00524747651
Iter: 93 loss: 0.00723154657
Iter: 94 loss: 0.00524436636
Iter: 95 loss: 0.0051170243
Iter: 96 loss: 0.00511147315
Iter: 97 loss: 0.00501003303
Iter: 98 loss: 0.00606299145
Iter: 99 loss: 0.00500650797
Iter: 100 loss: 0.00492367521
Iter: 101 loss: 0.00535297114
Iter: 102 loss: 0.00490633771
Iter: 103 loss: 0.00481224433
Iter: 104 loss: 0.00481207483
Iter: 105 loss: 0.00475475565
Iter: 106 loss: 0.00490507856
Iter: 107 loss: 0.00473573804
Iter: 108 loss: 0.00466297707
Iter: 109 loss: 0.00590797747
Iter: 110 loss: 0.00466184039
Iter: 111 loss: 0.0046094358
Iter: 112 loss: 0.0046088947
Iter: 113 loss: 0.0045606941
Iter: 114 loss: 0.00464046188
Iter: 115 loss: 0.00453842245
Iter: 116 loss: 0.00446114689
Iter: 117 loss: 0.00445408234
Iter: 118 loss: 0.00439530611
Iter: 119 loss: 0.00431843894
Iter: 120 loss: 0.0043179607
Iter: 121 loss: 0.00425442867
Iter: 122 loss: 0.00446285727
Iter: 123 loss: 0.00423369789
Iter: 124 loss: 0.00416001771
Iter: 125 loss: 0.00419564452
Iter: 126 loss: 0.00411036331
Iter: 127 loss: 0.00405519269
Iter: 128 loss: 0.00430276245
Iter: 129 loss: 0.00404073391
Iter: 130 loss: 0.00398571556
Iter: 131 loss: 0.00398188969
Iter: 132 loss: 0.0039197728
Iter: 133 loss: 0.00496365409
Iter: 134 loss: 0.00391974533
Iter: 135 loss: 0.00385002326
Iter: 136 loss: 0.00398254814
Iter: 137 loss: 0.00381813059
Iter: 138 loss: 0.00375656
Iter: 139 loss: 0.003933548
Iter: 140 loss: 0.00373656442
Iter: 141 loss: 0.00366662303
Iter: 142 loss: 0.0040651029
Iter: 143 loss: 0.00365389
Iter: 144 loss: 0.0035846394
Iter: 145 loss: 0.00397653878
Iter: 146 loss: 0.0035731406
Iter: 147 loss: 0.00352151
Iter: 148 loss: 0.00352033833
Iter: 149 loss: 0.00347281178
Iter: 150 loss: 0.00368426833
Iter: 151 loss: 0.00346045592
Iter: 152 loss: 0.00339604751
Iter: 153 loss: 0.00396318175
Iter: 154 loss: 0.0033929504
Iter: 155 loss: 0.00334904157
Iter: 156 loss: 0.00355604361
Iter: 157 loss: 0.00334043521
Iter: 158 loss: 0.00331122
Iter: 159 loss: 0.00328800292
Iter: 160 loss: 0.003279394
Iter: 161 loss: 0.00322130602
Iter: 162 loss: 0.00322950026
Iter: 163 loss: 0.00317658531
Iter: 164 loss: 0.00311610545
Iter: 165 loss: 0.00332369911
Iter: 166 loss: 0.00309772743
Iter: 167 loss: 0.00306540821
Iter: 168 loss: 0.00305757439
Iter: 169 loss: 0.00303083053
Iter: 170 loss: 0.00309958775
Iter: 171 loss: 0.00302163488
Iter: 172 loss: 0.00299778115
Iter: 173 loss: 0.00311776879
Iter: 174 loss: 0.0029929257
Iter: 175 loss: 0.00296682375
Iter: 176 loss: 0.00322618149
Iter: 177 loss: 0.00296601141
Iter: 178 loss: 0.00293828291
Iter: 179 loss: 0.00306394976
Iter: 180 loss: 0.00293309754
Iter: 181 loss: 0.00291267
Iter: 182 loss: 0.00303251692
Iter: 183 loss: 0.00290972739
Iter: 184 loss: 0.00289048417
Iter: 185 loss: 0.00287812296
Iter: 186 loss: 0.00287074409
Iter: 187 loss: 0.00283516175
Iter: 188 loss: 0.00317364186
Iter: 189 loss: 0.00283379271
Iter: 190 loss: 0.00281213084
Iter: 191 loss: 0.00285988767
Iter: 192 loss: 0.00280353543
Iter: 193 loss: 0.0027708211
Iter: 194 loss: 0.00281756744
Iter: 195 loss: 0.00275483681
Iter: 196 loss: 0.00272472
Iter: 197 loss: 0.0028554881
Iter: 198 loss: 0.00271847565
Iter: 199 loss: 0.00268609449
Iter: 200 loss: 0.00276954914
Iter: 201 loss: 0.00267467648
Iter: 202 loss: 0.00264488487
Iter: 203 loss: 0.00292709609
Iter: 204 loss: 0.00264369929
Iter: 205 loss: 0.00261637243
Iter: 206 loss: 0.00274165324
Iter: 207 loss: 0.00261088298
Iter: 208 loss: 0.00258882344
Iter: 209 loss: 0.00260861684
Iter: 210 loss: 0.0025760876
Iter: 211 loss: 0.0025485626
Iter: 212 loss: 0.00262700533
Iter: 213 loss: 0.00253978185
Iter: 214 loss: 0.00251658773
Iter: 215 loss: 0.00276407786
Iter: 216 loss: 0.00251597539
Iter: 217 loss: 0.00249811681
Iter: 218 loss: 0.00251414394
Iter: 219 loss: 0.00248750835
Iter: 220 loss: 0.00246924115
Iter: 221 loss: 0.00246910634
Iter: 222 loss: 0.00245418074
Iter: 223 loss: 0.00250079622
Iter: 224 loss: 0.00244999886
Iter: 225 loss: 0.00243874057
Iter: 226 loss: 0.00242069364
Iter: 227 loss: 0.00242053438
Iter: 228 loss: 0.00240113586
Iter: 229 loss: 0.00240471098
Iter: 230 loss: 0.00238650455
Iter: 231 loss: 0.00236928626
Iter: 232 loss: 0.00237110257
Iter: 233 loss: 0.00235544797
Iter: 234 loss: 0.00233259285
Iter: 235 loss: 0.00235402025
Iter: 236 loss: 0.00231947564
Iter: 237 loss: 0.00228881184
Iter: 238 loss: 0.002411291
Iter: 239 loss: 0.0022802914
Iter: 240 loss: 0.00229315553
Iter: 241 loss: 0.00226833369
Iter: 242 loss: 0.00225814898
Iter: 243 loss: 0.00227863318
Iter: 244 loss: 0.00225401437
Iter: 245 loss: 0.00224344013
Iter: 246 loss: 0.00227020588
Iter: 247 loss: 0.00223957957
Iter: 248 loss: 0.00223275973
Iter: 249 loss: 0.00232988
Iter: 250 loss: 0.00223274459
Iter: 251 loss: 0.00222647516
Iter: 252 loss: 0.00222222507
Iter: 253 loss: 0.00221993728
Iter: 254 loss: 0.00220796908
Iter: 255 loss: 0.00226887269
Iter: 256 loss: 0.00220592716
Iter: 257 loss: 0.00219568261
Iter: 258 loss: 0.00233266782
Iter: 259 loss: 0.00219551148
Iter: 260 loss: 0.00218738476
Iter: 261 loss: 0.0021957322
Iter: 262 loss: 0.00218286528
Iter: 263 loss: 0.00217400538
Iter: 264 loss: 0.00217209151
Iter: 265 loss: 0.0021603757
Iter: 266 loss: 0.00216100388
Iter: 267 loss: 0.00215105806
Iter: 268 loss: 0.0021433963
Iter: 269 loss: 0.00215290021
Iter: 270 loss: 0.00213943841
Iter: 271 loss: 0.00213192659
Iter: 272 loss: 0.00216599
Iter: 273 loss: 0.00213006535
Iter: 274 loss: 0.00212404085
Iter: 275 loss: 0.00212402502
Iter: 276 loss: 0.00211878773
Iter: 277 loss: 0.00214575836
Iter: 278 loss: 0.00211794092
Iter: 279 loss: 0.00211348291
Iter: 280 loss: 0.00211564614
Iter: 281 loss: 0.0021104319
Iter: 282 loss: 0.00210483395
Iter: 283 loss: 0.00213414314
Iter: 284 loss: 0.00210394361
Iter: 285 loss: 0.00209575333
Iter: 286 loss: 0.00211541611
Iter: 287 loss: 0.00209272117
Iter: 288 loss: 0.00208686106
Iter: 289 loss: 0.00214614114
Iter: 290 loss: 0.00208666828
Iter: 291 loss: 0.00208151317
Iter: 292 loss: 0.00207552081
Iter: 293 loss: 0.00207482977
Iter: 294 loss: 0.00206704065
Iter: 295 loss: 0.00210854271
Iter: 296 loss: 0.00206582178
Iter: 297 loss: 0.0020620618
Iter: 298 loss: 0.00206110859
Iter: 299 loss: 0.00205828692
Iter: 300 loss: 0.00205800729
Iter: 301 loss: 0.00205599167
Iter: 302 loss: 0.00205192761
Iter: 303 loss: 0.0020517658
Iter: 304 loss: 0.00204671593
Iter: 305 loss: 0.00204604072
Iter: 306 loss: 0.00204248121
Iter: 307 loss: 0.00203519594
Iter: 308 loss: 0.0020628965
Iter: 309 loss: 0.00203346647
Iter: 310 loss: 0.00202810741
Iter: 311 loss: 0.00202790438
Iter: 312 loss: 0.00202475023
Iter: 313 loss: 0.00205540471
Iter: 314 loss: 0.00202463334
Iter: 315 loss: 0.00202112738
Iter: 316 loss: 0.00201972621
Iter: 317 loss: 0.002017861
Iter: 318 loss: 0.00201465283
Iter: 319 loss: 0.00203372817
Iter: 320 loss: 0.00201424072
Iter: 321 loss: 0.00201011868
Iter: 322 loss: 0.0020096451
Iter: 323 loss: 0.00200664881
Iter: 324 loss: 0.00200358499
Iter: 325 loss: 0.00204604911
Iter: 326 loss: 0.00200357824
Iter: 327 loss: 0.00200156262
Iter: 328 loss: 0.00200368138
Iter: 329 loss: 0.00200046622
Iter: 330 loss: 0.00199728087
Iter: 331 loss: 0.00200425694
Iter: 332 loss: 0.00199596328
Iter: 333 loss: 0.00199237932
Iter: 334 loss: 0.00203293655
Iter: 335 loss: 0.00199231436
Iter: 336 loss: 0.00198901934
Iter: 337 loss: 0.00198654342
Iter: 338 loss: 0.0019854675
Iter: 339 loss: 0.0019816
Iter: 340 loss: 0.00198339927
Iter: 341 loss: 0.00197899342
Iter: 342 loss: 0.00197561621
Iter: 343 loss: 0.00198338344
Iter: 344 loss: 0.00197435566
Iter: 345 loss: 0.00197214051
Iter: 346 loss: 0.0019720553
Iter: 347 loss: 0.001969971
Iter: 348 loss: 0.00197434332
Iter: 349 loss: 0.00196914887
Iter: 350 loss: 0.00196738448
Iter: 351 loss: 0.00196843268
Iter: 352 loss: 0.00196624058
Iter: 353 loss: 0.001963038
Iter: 354 loss: 0.00196490251
Iter: 355 loss: 0.00196096743
Iter: 356 loss: 0.00195880793
Iter: 357 loss: 0.00196438958
Iter: 358 loss: 0.00195805985
Iter: 359 loss: 0.00195497833
Iter: 360 loss: 0.00195315434
Iter: 361 loss: 0.00195187202
Iter: 362 loss: 0.00194877444
Iter: 363 loss: 0.00194879621
Iter: 364 loss: 0.00194632111
Iter: 365 loss: 0.00194289919
Iter: 366 loss: 0.00199197698
Iter: 367 loss: 0.00194287859
Iter: 368 loss: 0.00193993642
Iter: 369 loss: 0.00193885271
Iter: 370 loss: 0.00193720986
Iter: 371 loss: 0.00193341984
Iter: 372 loss: 0.00198002439
Iter: 373 loss: 0.00193338876
Iter: 374 loss: 0.00193160982
Iter: 375 loss: 0.00192856253
Iter: 376 loss: 0.00192855834
Iter: 377 loss: 0.00192483782
Iter: 378 loss: 0.00193594606
Iter: 379 loss: 0.00192371756
Iter: 380 loss: 0.00192035758
Iter: 381 loss: 0.00192351534
Iter: 382 loss: 0.00191838341
Iter: 383 loss: 0.00191433681
Iter: 384 loss: 0.00193254778
Iter: 385 loss: 0.00191354309
Iter: 386 loss: 0.00191047927
Iter: 387 loss: 0.00191039476
Iter: 388 loss: 0.00190899277
Iter: 389 loss: 0.00190559379
Iter: 390 loss: 0.00195362256
Iter: 391 loss: 0.00190527958
Iter: 392 loss: 0.00190099317
Iter: 393 loss: 0.00190080609
Iter: 394 loss: 0.0018971283
Iter: 395 loss: 0.00193482009
Iter: 396 loss: 0.00189703784
Iter: 397 loss: 0.0018941795
Iter: 398 loss: 0.00189417938
Iter: 399 loss: 0.00189152989
Iter: 400 loss: 0.001888728
Iter: 401 loss: 0.0018882493
Iter: 402 loss: 0.00188541319
Iter: 403 loss: 0.00192610617
Iter: 404 loss: 0.00188541168
Iter: 405 loss: 0.00188324158
Iter: 406 loss: 0.00188974082
Iter: 407 loss: 0.00188252
Iter: 408 loss: 0.0018794986
Iter: 409 loss: 0.00188298151
Iter: 410 loss: 0.00187787367
Iter: 411 loss: 0.00187502569
Iter: 412 loss: 0.00189249055
Iter: 413 loss: 0.00187470193
Iter: 414 loss: 0.00187159027
Iter: 415 loss: 0.00188192679
Iter: 416 loss: 0.00187062216
Iter: 417 loss: 0.00186826033
Iter: 418 loss: 0.00187026104
Iter: 419 loss: 0.00186687964
Iter: 420 loss: 0.00186423468
Iter: 421 loss: 0.00186348392
Iter: 422 loss: 0.00186182256
Iter: 423 loss: 0.00185922789
Iter: 424 loss: 0.00187785714
Iter: 425 loss: 0.00185901078
Iter: 426 loss: 0.00185694196
Iter: 427 loss: 0.00186835439
Iter: 428 loss: 0.00185664592
Iter: 429 loss: 0.00185526884
Iter: 430 loss: 0.00185773964
Iter: 431 loss: 0.00185463647
Iter: 432 loss: 0.0018520538
Iter: 433 loss: 0.0018560196
Iter: 434 loss: 0.00185083644
Iter: 435 loss: 0.00184826821
Iter: 436 loss: 0.00185616291
Iter: 437 loss: 0.00184754794
Iter: 438 loss: 0.00184522476
Iter: 439 loss: 0.00185026461
Iter: 440 loss: 0.00184429169
Iter: 441 loss: 0.00184180471
Iter: 442 loss: 0.00184281589
Iter: 443 loss: 0.0018400735
Iter: 444 loss: 0.00183658954
Iter: 445 loss: 0.00184360438
Iter: 446 loss: 0.0018351872
Iter: 447 loss: 0.00183263549
Iter: 448 loss: 0.00183518464
Iter: 449 loss: 0.0018310548
Iter: 450 loss: 0.00182586419
Iter: 451 loss: 0.00184711791
Iter: 452 loss: 0.00182469841
Iter: 453 loss: 0.00182328362
Iter: 454 loss: 0.00182125182
Iter: 455 loss: 0.00181856868
Iter: 456 loss: 0.0018420968
Iter: 457 loss: 0.00181842828
Iter: 458 loss: 0.00181673327
Iter: 459 loss: 0.00181514269
Iter: 460 loss: 0.0018147243
Iter: 461 loss: 0.00181220984
Iter: 462 loss: 0.00181218539
Iter: 463 loss: 0.00181048072
Iter: 464 loss: 0.00181714259
Iter: 465 loss: 0.00181007688
Iter: 466 loss: 0.00180822914
Iter: 467 loss: 0.00180562644
Iter: 468 loss: 0.00180551014
Iter: 469 loss: 0.00180092361
Iter: 470 loss: 0.00183093513
Iter: 471 loss: 0.00180042489
Iter: 472 loss: 0.00179985701
Iter: 473 loss: 0.00179893768
Iter: 474 loss: 0.00179783604
Iter: 475 loss: 0.00179578643
Iter: 476 loss: 0.00184578425
Iter: 477 loss: 0.00179578189
Iter: 478 loss: 0.00179272436
Iter: 479 loss: 0.00181362138
Iter: 480 loss: 0.0017923594
Iter: 481 loss: 0.00179017347
Iter: 482 loss: 0.00181682454
Iter: 483 loss: 0.00179015659
Iter: 484 loss: 0.00178833504
Iter: 485 loss: 0.00179153262
Iter: 486 loss: 0.00178744667
Iter: 487 loss: 0.00178500614
Iter: 488 loss: 0.00179341575
Iter: 489 loss: 0.00178434653
Iter: 490 loss: 0.00178195769
Iter: 491 loss: 0.00178798975
Iter: 492 loss: 0.00178112928
Iter: 493 loss: 0.00177929259
Iter: 494 loss: 0.00177969527
Iter: 495 loss: 0.00177792087
Iter: 496 loss: 0.00177604752
Iter: 497 loss: 0.0017765041
Iter: 498 loss: 0.00177468546
Iter: 499 loss: 0.00177191244
Iter: 500 loss: 0.00180415588
Iter: 501 loss: 0.00177186308
Iter: 502 loss: 0.00177054992
Iter: 503 loss: 0.00177208649
Iter: 504 loss: 0.00176982814
Iter: 505 loss: 0.00176777737
Iter: 506 loss: 0.00176929543
Iter: 507 loss: 0.00176651415
Iter: 508 loss: 0.00176399807
Iter: 509 loss: 0.00177956908
Iter: 510 loss: 0.00176366232
Iter: 511 loss: 0.00176177709
Iter: 512 loss: 0.00176581182
Iter: 513 loss: 0.00176099548
Iter: 514 loss: 0.00175859872
Iter: 515 loss: 0.00176584336
Iter: 516 loss: 0.00175795692
Iter: 517 loss: 0.00175642467
Iter: 518 loss: 0.00175618578
Iter: 519 loss: 0.00175451406
Iter: 520 loss: 0.00175476552
Iter: 521 loss: 0.00175321731
Iter: 522 loss: 0.00175060262
Iter: 523 loss: 0.00175370765
Iter: 524 loss: 0.00174925476
Iter: 525 loss: 0.00174729573
Iter: 526 loss: 0.00177107705
Iter: 527 loss: 0.00174726383
Iter: 528 loss: 0.00174553203
Iter: 529 loss: 0.00175644737
Iter: 530 loss: 0.00174530596
Iter: 531 loss: 0.00174408825
Iter: 532 loss: 0.00174463121
Iter: 533 loss: 0.00174326403
Iter: 534 loss: 0.0017414263
Iter: 535 loss: 0.00175292871
Iter: 536 loss: 0.00174121745
Iter: 537 loss: 0.00173944153
Iter: 538 loss: 0.00174972729
Iter: 539 loss: 0.00173918111
Iter: 540 loss: 0.00173792033
Iter: 541 loss: 0.00173566374
Iter: 542 loss: 0.00179173285
Iter: 543 loss: 0.00173566281
Iter: 544 loss: 0.00173326954
Iter: 545 loss: 0.00174547883
Iter: 546 loss: 0.00173290935
Iter: 547 loss: 0.00173167326
Iter: 548 loss: 0.00173685106
Iter: 549 loss: 0.00173135498
Iter: 550 loss: 0.0017295134
Iter: 551 loss: 0.00174371316
Iter: 552 loss: 0.00172938081
Iter: 553 loss: 0.00172771618
Iter: 554 loss: 0.0017281879
Iter: 555 loss: 0.0017265263
Iter: 556 loss: 0.00172448298
Iter: 557 loss: 0.00172659568
Iter: 558 loss: 0.0017233101
Iter: 559 loss: 0.00172020262
Iter: 560 loss: 0.00173230027
Iter: 561 loss: 0.00171946769
Iter: 562 loss: 0.00171885197
Iter: 563 loss: 0.00171798794
Iter: 564 loss: 0.00171660795
Iter: 565 loss: 0.001714963
Iter: 566 loss: 0.00171479071
Iter: 567 loss: 0.00171290233
Iter: 568 loss: 0.00171420653
Iter: 569 loss: 0.00171171897
Iter: 570 loss: 0.00170991966
Iter: 571 loss: 0.00171290059
Iter: 572 loss: 0.00170910778
Iter: 573 loss: 0.00170679542
Iter: 574 loss: 0.00172445015
Iter: 575 loss: 0.00170647388
Iter: 576 loss: 0.00170417887
Iter: 577 loss: 0.00170416583
Iter: 578 loss: 0.00170166837
Iter: 579 loss: 0.00170024112
Iter: 580 loss: 0.00169917161
Iter: 581 loss: 0.00169735483
Iter: 582 loss: 0.0017158296
Iter: 583 loss: 0.00169729011
Iter: 584 loss: 0.00169593422
Iter: 585 loss: 0.0016968057
Iter: 586 loss: 0.00169507216
Iter: 587 loss: 0.00169348391
Iter: 588 loss: 0.00169595261
Iter: 589 loss: 0.00169273291
Iter: 590 loss: 0.00169091
Iter: 591 loss: 0.00169978326
Iter: 592 loss: 0.00169057713
Iter: 593 loss: 0.00168886944
Iter: 594 loss: 0.00168998796
Iter: 595 loss: 0.00168778072
Iter: 596 loss: 0.00168685708
Iter: 597 loss: 0.00168780412
Iter: 598 loss: 0.00168636569
Iter: 599 loss: 0.00168463937
Iter: 600 loss: 0.00168887095
Iter: 601 loss: 0.00168394181
Iter: 602 loss: 0.0016835545
Iter: 603 loss: 0.00168295507
Iter: 604 loss: 0.00168210804
Iter: 605 loss: 0.00167993922
Iter: 606 loss: 0.00169807964
Iter: 607 loss: 0.00167957321
Iter: 608 loss: 0.00167728635
Iter: 609 loss: 0.00169364852
Iter: 610 loss: 0.00167708052
Iter: 611 loss: 0.00167547411
Iter: 612 loss: 0.00168837619
Iter: 613 loss: 0.0016753563
Iter: 614 loss: 0.00167410122
Iter: 615 loss: 0.00167203695
Iter: 616 loss: 0.00167202647
Iter: 617 loss: 0.00166955194
Iter: 618 loss: 0.00168489444
Iter: 619 loss: 0.00166913238
Iter: 620 loss: 0.00166702596
Iter: 621 loss: 0.00169009366
Iter: 622 loss: 0.0016669404
Iter: 623 loss: 0.00166389276
Iter: 624 loss: 0.00167884817
Iter: 625 loss: 0.00166340824
Iter: 626 loss: 0.00166174897
Iter: 627 loss: 0.00168365811
Iter: 628 loss: 0.00166173489
Iter: 629 loss: 0.0016597365
Iter: 630 loss: 0.00166316167
Iter: 631 loss: 0.00165882392
Iter: 632 loss: 0.00165752065
Iter: 633 loss: 0.00165752566
Iter: 634 loss: 0.00165648456
Iter: 635 loss: 0.00165454915
Iter: 636 loss: 0.00165727106
Iter: 637 loss: 0.00165358419
Iter: 638 loss: 0.00165265333
Iter: 639 loss: 0.00165234914
Iter: 640 loss: 0.00165155204
Iter: 641 loss: 0.00165118137
Iter: 642 loss: 0.00165081373
Iter: 643 loss: 0.00164867332
Iter: 644 loss: 0.00165519235
Iter: 645 loss: 0.0016480065
Iter: 646 loss: 0.0016468761
Iter: 647 loss: 0.00164667424
Iter: 648 loss: 0.00164581847
Iter: 649 loss: 0.00164368132
Iter: 650 loss: 0.00166379462
Iter: 651 loss: 0.00164337666
Iter: 652 loss: 0.00164086185
Iter: 653 loss: 0.00168121059
Iter: 654 loss: 0.00164086325
Iter: 655 loss: 0.00163954147
Iter: 656 loss: 0.00164931361
Iter: 657 loss: 0.00163942738
Iter: 658 loss: 0.00163819711
Iter: 659 loss: 0.00163634086
Iter: 660 loss: 0.00163630105
Iter: 661 loss: 0.00163558032
Iter: 662 loss: 0.00163526693
Iter: 663 loss: 0.00163446774
Iter: 664 loss: 0.00163484993
Iter: 665 loss: 0.001633932
Iter: 666 loss: 0.00163238973
Iter: 667 loss: 0.0016310754
Iter: 668 loss: 0.00163063081
Iter: 669 loss: 0.00162905408
Iter: 670 loss: 0.00162853929
Iter: 671 loss: 0.00162661914
Iter: 672 loss: 0.00163171603
Iter: 673 loss: 0.00162598456
Iter: 674 loss: 0.00162362657
Iter: 675 loss: 0.00162867957
Iter: 676 loss: 0.00162270409
Iter: 677 loss: 0.00162129826
Iter: 678 loss: 0.00162190851
Iter: 679 loss: 0.00162032526
Iter: 680 loss: 0.00161929405
Iter: 681 loss: 0.00162032293
Iter: 682 loss: 0.00161872164
Iter: 683 loss: 0.00161696982
Iter: 684 loss: 0.00162541342
Iter: 685 loss: 0.00161663536
Iter: 686 loss: 0.00161558925
Iter: 687 loss: 0.00162482203
Iter: 688 loss: 0.00161554175
Iter: 689 loss: 0.00161484303
Iter: 690 loss: 0.0016137436
Iter: 691 loss: 0.00161373103
Iter: 692 loss: 0.00161231006
Iter: 693 loss: 0.00161400356
Iter: 694 loss: 0.00161156629
Iter: 695 loss: 0.00160981575
Iter: 696 loss: 0.00161559973
Iter: 697 loss: 0.00160932762
Iter: 698 loss: 0.00160696835
Iter: 699 loss: 0.00162388454
Iter: 700 loss: 0.00160676008
Iter: 701 loss: 0.00160561572
Iter: 702 loss: 0.00160434702
Iter: 703 loss: 0.00160416984
Iter: 704 loss: 0.00160321838
Iter: 705 loss: 0.00160360872
Iter: 706 loss: 0.00160256308
Iter: 707 loss: 0.00160146365
Iter: 708 loss: 0.00161302846
Iter: 709 loss: 0.00160143408
Iter: 710 loss: 0.00160020741
Iter: 711 loss: 0.00160223513
Iter: 712 loss: 0.00159964524
Iter: 713 loss: 0.00159857096
Iter: 714 loss: 0.001596
Iter: 715 loss: 0.00163306459
Iter: 716 loss: 0.00159578817
Iter: 717 loss: 0.00159317721
Iter: 718 loss: 0.00159312319
Iter: 719 loss: 0.00159090385
Iter: 720 loss: 0.00161198853
Iter: 721 loss: 0.00159080082
Iter: 722 loss: 0.00158959918
Iter: 723 loss: 0.00159449806
Iter: 724 loss: 0.00158933247
Iter: 725 loss: 0.00158810499
Iter: 726 loss: 0.00158573571
Iter: 727 loss: 0.00163610396
Iter: 728 loss: 0.0015857243
Iter: 729 loss: 0.0015841336
Iter: 730 loss: 0.00160282198
Iter: 731 loss: 0.00158410624
Iter: 732 loss: 0.00158299808
Iter: 733 loss: 0.00158292614
Iter: 734 loss: 0.00158222148
Iter: 735 loss: 0.00158182404
Iter: 736 loss: 0.00158151484
Iter: 737 loss: 0.0015798714
Iter: 738 loss: 0.00158736
Iter: 739 loss: 0.00157955242
Iter: 740 loss: 0.00157800736
Iter: 741 loss: 0.00158576877
Iter: 742 loss: 0.00157772435
Iter: 743 loss: 0.00157673901
Iter: 744 loss: 0.00157874101
Iter: 745 loss: 0.00157634798
Iter: 746 loss: 0.00157515169
Iter: 747 loss: 0.00157515076
Iter: 748 loss: 0.00157440256
Iter: 749 loss: 0.00157311372
Iter: 750 loss: 0.00157311256
Iter: 751 loss: 0.0015713363
Iter: 752 loss: 0.00157324621
Iter: 753 loss: 0.00157034537
Iter: 754 loss: 0.00156957901
Iter: 755 loss: 0.00157813216
Iter: 756 loss: 0.00156956643
Iter: 757 loss: 0.00156898075
Iter: 758 loss: 0.00156764034
Iter: 759 loss: 0.00158514292
Iter: 760 loss: 0.00156754907
Iter: 761 loss: 0.00156616548
Iter: 762 loss: 0.00157433329
Iter: 763 loss: 0.00156599318
Iter: 764 loss: 0.00156489271
Iter: 765 loss: 0.0015688662
Iter: 766 loss: 0.00156462099
Iter: 767 loss: 0.001563614
Iter: 768 loss: 0.0015693868
Iter: 769 loss: 0.00156347768
Iter: 770 loss: 0.00156246964
Iter: 771 loss: 0.00156183087
Iter: 772 loss: 0.00156143634
Iter: 773 loss: 0.00156013772
Iter: 774 loss: 0.00156299188
Iter: 775 loss: 0.00155963458
Iter: 776 loss: 0.00155754155
Iter: 777 loss: 0.00156177743
Iter: 778 loss: 0.0015567115
Iter: 779 loss: 0.00155579019
Iter: 780 loss: 0.00155578693
Iter: 781 loss: 0.00155498739
Iter: 782 loss: 0.0015553924
Iter: 783 loss: 0.00155444932
Iter: 784 loss: 0.0015531627
Iter: 785 loss: 0.0015530308
Iter: 786 loss: 0.00155209145
Iter: 787 loss: 0.0015513564
Iter: 788 loss: 0.00155063812
Iter: 789 loss: 0.00155048026
Iter: 790 loss: 0.00154858339
Iter: 791 loss: 0.00155705749
Iter: 792 loss: 0.00154819374
Iter: 793 loss: 0.00154730841
Iter: 794 loss: 0.00154542713
Iter: 795 loss: 0.0015771637
Iter: 796 loss: 0.00154537195
Iter: 797 loss: 0.00154333399
Iter: 798 loss: 0.00155446026
Iter: 799 loss: 0.00154303294
Iter: 800 loss: 0.00154158473
Iter: 801 loss: 0.00154469709
Iter: 802 loss: 0.00154101709
Iter: 803 loss: 0.00153929158
Iter: 804 loss: 0.00155100413
Iter: 805 loss: 0.00153912324
Iter: 806 loss: 0.00153792812
Iter: 807 loss: 0.00153792766
Iter: 808 loss: 0.00153759145
Iter: 809 loss: 0.00153682637
Iter: 810 loss: 0.00154715183
Iter: 811 loss: 0.00153677701
Iter: 812 loss: 0.00153561006
Iter: 813 loss: 0.00153931289
Iter: 814 loss: 0.00153527339
Iter: 815 loss: 0.00153405976
Iter: 816 loss: 0.00154122501
Iter: 817 loss: 0.00153390644
Iter: 818 loss: 0.00153302494
Iter: 819 loss: 0.00153269817
Iter: 820 loss: 0.0015322098
Iter: 821 loss: 0.00153084425
Iter: 822 loss: 0.0015290645
Iter: 823 loss: 0.00152893935
Iter: 824 loss: 0.00152792991
Iter: 825 loss: 0.00152767589
Iter: 826 loss: 0.00152663875
Iter: 827 loss: 0.00152435875
Iter: 828 loss: 0.00155788031
Iter: 829 loss: 0.00152425747
Iter: 830 loss: 0.00152246
Iter: 831 loss: 0.00152243138
Iter: 832 loss: 0.00152179587
Iter: 833 loss: 0.00152272626
Iter: 834 loss: 0.00152148819
Iter: 835 loss: 0.00152029935
Iter: 836 loss: 0.00151964
Iter: 837 loss: 0.00151911296
Iter: 838 loss: 0.00151805603
Iter: 839 loss: 0.00153211982
Iter: 840 loss: 0.00151804672
Iter: 841 loss: 0.00151711958
Iter: 842 loss: 0.00152384408
Iter: 843 loss: 0.00151703949
Iter: 844 loss: 0.0015165885
Iter: 845 loss: 0.00151659676
Iter: 846 loss: 0.00151622808
Iter: 847 loss: 0.00151545939
Iter: 848 loss: 0.00151923834
Iter: 849 loss: 0.00151532935
Iter: 850 loss: 0.00151466951
Iter: 851 loss: 0.0015132163
Iter: 852 loss: 0.00153436558
Iter: 853 loss: 0.00151314912
Iter: 854 loss: 0.00151312479
Iter: 855 loss: 0.00151241687
Iter: 856 loss: 0.00151160127
Iter: 857 loss: 0.00151313981
Iter: 858 loss: 0.00151125807
Iter: 859 loss: 0.00151073444
Iter: 860 loss: 0.00150972768
Iter: 861 loss: 0.00152986892
Iter: 862 loss: 0.00150972302
Iter: 863 loss: 0.00150830881
Iter: 864 loss: 0.00150981278
Iter: 865 loss: 0.00150754151
Iter: 866 loss: 0.00150571088
Iter: 867 loss: 0.00153041515
Iter: 868 loss: 0.00150569552
Iter: 869 loss: 0.0015048699
Iter: 870 loss: 0.00150276569
Iter: 871 loss: 0.00152151915
Iter: 872 loss: 0.00150243402
Iter: 873 loss: 0.00150018651
Iter: 874 loss: 0.00151664182
Iter: 875 loss: 0.00149999943
Iter: 876 loss: 0.00149872666
Iter: 877 loss: 0.00150747271
Iter: 878 loss: 0.0014985957
Iter: 879 loss: 0.00149687217
Iter: 880 loss: 0.00149886054
Iter: 881 loss: 0.00149595644
Iter: 882 loss: 0.00149513024
Iter: 883 loss: 0.0014978078
Iter: 884 loss: 0.0014948973
Iter: 885 loss: 0.00149427634
Iter: 886 loss: 0.00149857008
Iter: 887 loss: 0.00149422069
Iter: 888 loss: 0.00149362884
Iter: 889 loss: 0.00149333233
Iter: 890 loss: 0.00149305398
Iter: 891 loss: 0.00149150565
Iter: 892 loss: 0.00149403966
Iter: 893 loss: 0.00149078388
Iter: 894 loss: 0.00148831494
Iter: 895 loss: 0.00150582544
Iter: 896 loss: 0.00148809282
Iter: 897 loss: 0.00148735498
Iter: 898 loss: 0.00148587092
Iter: 899 loss: 0.00151421828
Iter: 900 loss: 0.00148585078
Iter: 901 loss: 0.00148476369
Iter: 902 loss: 0.00148447976
Iter: 903 loss: 0.00148380385
Iter: 904 loss: 0.00148309441
Iter: 905 loss: 0.00148251129
Iter: 906 loss: 0.00148230547
Iter: 907 loss: 0.00148131175
Iter: 908 loss: 0.00148114399
Iter: 909 loss: 0.00148044666
Iter: 910 loss: 0.00147898821
Iter: 911 loss: 0.0014815894
Iter: 912 loss: 0.00147832942
Iter: 913 loss: 0.00147706608
Iter: 914 loss: 0.00148308917
Iter: 915 loss: 0.00147683965
Iter: 916 loss: 0.00147627573
Iter: 917 loss: 0.00147675746
Iter: 918 loss: 0.00147594209
Iter: 919 loss: 0.00147492369
Iter: 920 loss: 0.00147428026
Iter: 921 loss: 0.00147387467
Iter: 922 loss: 0.00147293112
Iter: 923 loss: 0.00147285126
Iter: 924 loss: 0.00147226767
Iter: 925 loss: 0.00147116813
Iter: 926 loss: 0.00150257698
Iter: 927 loss: 0.00147117104
Iter: 928 loss: 0.00146987732
Iter: 929 loss: 0.00149076502
Iter: 930 loss: 0.00146987615
Iter: 931 loss: 0.00146875391
Iter: 932 loss: 0.00147499051
Iter: 933 loss: 0.00146859814
Iter: 934 loss: 0.001467644
Iter: 935 loss: 0.00146566913
Iter: 936 loss: 0.00150084333
Iter: 937 loss: 0.00146563444
Iter: 938 loss: 0.00146545493
Iter: 939 loss: 0.0014648576
Iter: 940 loss: 0.00146446866
Iter: 941 loss: 0.00146354758
Iter: 942 loss: 0.00147432
Iter: 943 loss: 0.00146346062
Iter: 944 loss: 0.00146167458
Iter: 945 loss: 0.00146539276
Iter: 946 loss: 0.0014609471
Iter: 947 loss: 0.00146445038
Iter: 948 loss: 0.0014599741
Iter: 949 loss: 0.00145960716
Iter: 950 loss: 0.00145891262
Iter: 951 loss: 0.00147420028
Iter: 952 loss: 0.00145891053
Iter: 953 loss: 0.00145753846
Iter: 954 loss: 0.00145583553
Iter: 955 loss: 0.00145568256
Iter: 956 loss: 0.00145314098
Iter: 957 loss: 0.00145314191
Iter: 958 loss: 0.00145181175
Iter: 959 loss: 0.00145173201
Iter: 960 loss: 0.00145079242
Iter: 961 loss: 0.00145184249
Iter: 962 loss: 0.00145028753
Iter: 963 loss: 0.00144951174
Iter: 964 loss: 0.00145303085
Iter: 965 loss: 0.00144935679
Iter: 966 loss: 0.00144846563
Iter: 967 loss: 0.00144756795
Iter: 968 loss: 0.00144738739
Iter: 969 loss: 0.00144852581
Iter: 970 loss: 0.0014468797
Iter: 971 loss: 0.00144649739
Iter: 972 loss: 0.00144605781
Iter: 973 loss: 0.00144600298
Iter: 974 loss: 0.0014453
Iter: 975 loss: 0.00144483289
Iter: 976 loss: 0.00144457
Iter: 977 loss: 0.00144368771
Iter: 978 loss: 0.0014433933
Iter: 979 loss: 0.0014428841
Iter: 980 loss: 0.00144195801
Iter: 981 loss: 0.00144262216
Iter: 982 loss: 0.00144138688
Iter: 983 loss: 0.00144004705
Iter: 984 loss: 0.00144591299
Iter: 985 loss: 0.00143977627
Iter: 986 loss: 0.00143759279
Iter: 987 loss: 0.00144942827
Iter: 988 loss: 0.00143725786
Iter: 989 loss: 0.0014362596
Iter: 990 loss: 0.00144263776
Iter: 991 loss: 0.00143614784
Iter: 992 loss: 0.00143499125
Iter: 993 loss: 0.00143716624
Iter: 994 loss: 0.00143449451
Iter: 995 loss: 0.00143376773
Iter: 996 loss: 0.00144379353
Iter: 997 loss: 0.00143376482
Iter: 998 loss: 0.00143328134
Iter: 999 loss: 0.00143209589
Iter: 1000 loss: 0.0014446124
Iter: 1001 loss: 0.001431957
Iter: 1002 loss: 0.00143128866
Iter: 1003 loss: 0.00143112883
Iter: 1004 loss: 0.00143052894
Iter: 1005 loss: 0.00143380079
Iter: 1006 loss: 0.00143044477
Iter: 1007 loss: 0.00142997247
Iter: 1008 loss: 0.00143085374
Iter: 1009 loss: 0.00142977014
Iter: 1010 loss: 0.00142905419
Iter: 1011 loss: 0.00142858177
Iter: 1012 loss: 0.00142831262
Iter: 1013 loss: 0.00142705045
Iter: 1014 loss: 0.0014257502
Iter: 1015 loss: 0.00142550783
Iter: 1016 loss: 0.00142367231
Iter: 1017 loss: 0.00142860087
Iter: 1018 loss: 0.0014230432
Iter: 1019 loss: 0.00142159814
Iter: 1020 loss: 0.00142648723
Iter: 1021 loss: 0.0014211979
Iter: 1022 loss: 0.00141979184
Iter: 1023 loss: 0.00142150349
Iter: 1024 loss: 0.00141906156
Iter: 1025 loss: 0.00141720776
Iter: 1026 loss: 0.00144003774
Iter: 1027 loss: 0.00141718669
Iter: 1028 loss: 0.00141651311
Iter: 1029 loss: 0.00141923642
Iter: 1030 loss: 0.00141635805
Iter: 1031 loss: 0.00141542533
Iter: 1032 loss: 0.00141578191
Iter: 1033 loss: 0.00141477468
Iter: 1034 loss: 0.00141406851
Iter: 1035 loss: 0.00141406734
Iter: 1036 loss: 0.00141352788
Iter: 1037 loss: 0.00141331716
Iter: 1038 loss: 0.00141302741
Iter: 1039 loss: 0.00141205802
Iter: 1040 loss: 0.00141450088
Iter: 1041 loss: 0.00141171657
Iter: 1042 loss: 0.00141054241
Iter: 1043 loss: 0.00141672068
Iter: 1044 loss: 0.00141036161
Iter: 1045 loss: 0.00140964682
Iter: 1046 loss: 0.00140882423
Iter: 1047 loss: 0.00140872283
Iter: 1048 loss: 0.00140749221
Iter: 1049 loss: 0.001407417
Iter: 1050 loss: 0.00140647742
Iter: 1051 loss: 0.00140495715
Iter: 1052 loss: 0.00140359625
Iter: 1053 loss: 0.00140319136
Iter: 1054 loss: 0.00140185189
Iter: 1055 loss: 0.00141869346
Iter: 1056 loss: 0.00140184024
Iter: 1057 loss: 0.00140104431
Iter: 1058 loss: 0.00140101905
Iter: 1059 loss: 0.00140045839
Iter: 1060 loss: 0.00140069949
Iter: 1061 loss: 0.00140007073
Iter: 1062 loss: 0.00139917503
Iter: 1063 loss: 0.00140201836
Iter: 1064 loss: 0.00139891938
Iter: 1065 loss: 0.00139835291
Iter: 1066 loss: 0.00140705705
Iter: 1067 loss: 0.00139835419
Iter: 1068 loss: 0.0013978628
Iter: 1069 loss: 0.00139697816
Iter: 1070 loss: 0.00141840114
Iter: 1071 loss: 0.00139697921
Iter: 1072 loss: 0.00139600248
Iter: 1073 loss: 0.001398355
Iter: 1074 loss: 0.00139564776
Iter: 1075 loss: 0.00139477069
Iter: 1076 loss: 0.00140372943
Iter: 1077 loss: 0.00139474473
Iter: 1078 loss: 0.00139422272
Iter: 1079 loss: 0.00139421306
Iter: 1080 loss: 0.00139380246
Iter: 1081 loss: 0.0013929588
Iter: 1082 loss: 0.00139270094
Iter: 1083 loss: 0.00139220478
Iter: 1084 loss: 0.00139110128
Iter: 1085 loss: 0.00138887845
Iter: 1086 loss: 0.0014328768
Iter: 1087 loss: 0.00138885062
Iter: 1088 loss: 0.00138655049
Iter: 1089 loss: 0.00138928043
Iter: 1090 loss: 0.0013853556
Iter: 1091 loss: 0.00138622778
Iter: 1092 loss: 0.00138437026
Iter: 1093 loss: 0.00138374011
Iter: 1094 loss: 0.0013840131
Iter: 1095 loss: 0.00138330879
Iter: 1096 loss: 0.00138254429
Iter: 1097 loss: 0.00138719147
Iter: 1098 loss: 0.00138245069
Iter: 1099 loss: 0.00138190296
Iter: 1100 loss: 0.00138270424
Iter: 1101 loss: 0.00138163916
Iter: 1102 loss: 0.00138072041
Iter: 1103 loss: 0.00138113392
Iter: 1104 loss: 0.00138010085
Iter: 1105 loss: 0.00137939141
Iter: 1106 loss: 0.00137856696
Iter: 1107 loss: 0.001378468
Iter: 1108 loss: 0.00137822831
Iter: 1109 loss: 0.00137771189
Iter: 1110 loss: 0.00137706811
Iter: 1111 loss: 0.00137696404
Iter: 1112 loss: 0.00137651968
Iter: 1113 loss: 0.00137504656
Iter: 1114 loss: 0.00137437903
Iter: 1115 loss: 0.0013736377
Iter: 1116 loss: 0.00137247483
Iter: 1117 loss: 0.00137755659
Iter: 1118 loss: 0.00137223653
Iter: 1119 loss: 0.00137095375
Iter: 1120 loss: 0.00136931846
Iter: 1121 loss: 0.00136919692
Iter: 1122 loss: 0.00136725383
Iter: 1123 loss: 0.00136923348
Iter: 1124 loss: 0.00136615941
Iter: 1125 loss: 0.00136650587
Iter: 1126 loss: 0.00136548595
Iter: 1127 loss: 0.00136472681
Iter: 1128 loss: 0.00136433449
Iter: 1129 loss: 0.00136398058
Iter: 1130 loss: 0.00136368931
Iter: 1131 loss: 0.0013636048
Iter: 1132 loss: 0.00136323506
Iter: 1133 loss: 0.0013630304
Iter: 1134 loss: 0.00136287
Iter: 1135 loss: 0.00136225577
Iter: 1136 loss: 0.00136208604
Iter: 1137 loss: 0.00136170746
Iter: 1138 loss: 0.00136103993
Iter: 1139 loss: 0.00136096217
Iter: 1140 loss: 0.00136049371
Iter: 1141 loss: 0.00136118103
Iter: 1142 loss: 0.00136026286
Iter: 1143 loss: 0.0013593405
Iter: 1144 loss: 0.00135864434
Iter: 1145 loss: 0.00135833584
Iter: 1146 loss: 0.0013574214
Iter: 1147 loss: 0.00135754806
Iter: 1148 loss: 0.00135672605
Iter: 1149 loss: 0.00135525083
Iter: 1150 loss: 0.00135304371
Iter: 1151 loss: 0.00135299424
Iter: 1152 loss: 0.00135184405
Iter: 1153 loss: 0.00135834399
Iter: 1154 loss: 0.00135168387
Iter: 1155 loss: 0.00135083753
Iter: 1156 loss: 0.00136047229
Iter: 1157 loss: 0.00135081843
Iter: 1158 loss: 0.00135028083
Iter: 1159 loss: 0.00135059492
Iter: 1160 loss: 0.00134993054
Iter: 1161 loss: 0.00134898548
Iter: 1162 loss: 0.00134846801
Iter: 1163 loss: 0.00134804938
Iter: 1164 loss: 0.00134737685
Iter: 1165 loss: 0.00134713505
Iter: 1166 loss: 0.00134690921
Iter: 1167 loss: 0.0013462184
Iter: 1168 loss: 0.00134777557
Iter: 1169 loss: 0.00134580513
Iter: 1170 loss: 0.00134455017
Iter: 1171 loss: 0.00135837425
Iter: 1172 loss: 0.00134452339
Iter: 1173 loss: 0.00134332851
Iter: 1174 loss: 0.00134652515
Iter: 1175 loss: 0.00134292385
Iter: 1176 loss: 0.00134145771
Iter: 1177 loss: 0.00134923006
Iter: 1178 loss: 0.00134122884
Iter: 1179 loss: 0.00134080939
Iter: 1180 loss: 0.00134057784
Iter: 1181 loss: 0.00134012697
Iter: 1182 loss: 0.00133925828
Iter: 1183 loss: 0.00135723758
Iter: 1184 loss: 0.00133924885
Iter: 1185 loss: 0.00133847259
Iter: 1186 loss: 0.00133775966
Iter: 1187 loss: 0.00133757154
Iter: 1188 loss: 0.00133681647
Iter: 1189 loss: 0.00133583578
Iter: 1190 loss: 0.00133577047
Iter: 1191 loss: 0.00133468676
Iter: 1192 loss: 0.00133703602
Iter: 1193 loss: 0.00133426499
Iter: 1194 loss: 0.00133285671
Iter: 1195 loss: 0.00133471179
Iter: 1196 loss: 0.0013321389
Iter: 1197 loss: 0.0013317297
Iter: 1198 loss: 0.00133143447
Iter: 1199 loss: 0.00133111654
Iter: 1200 loss: 0.00133103074
Iter: 1201 loss: 0.00133083144
Iter: 1202 loss: 0.00133010885
Iter: 1203 loss: 0.00132901361
Iter: 1204 loss: 0.00132899336
Iter: 1205 loss: 0.00132744119
Iter: 1206 loss: 0.00132743223
Iter: 1207 loss: 0.00132638821
Iter: 1208 loss: 0.00132915471
Iter: 1209 loss: 0.00132603291
Iter: 1210 loss: 0.00132553454
Iter: 1211 loss: 0.00132498634
Iter: 1212 loss: 0.00132490508
Iter: 1213 loss: 0.0013238237
Iter: 1214 loss: 0.00132479006
Iter: 1215 loss: 0.00132319483
Iter: 1216 loss: 0.00132225524
Iter: 1217 loss: 0.00132119772
Iter: 1218 loss: 0.00132105488
Iter: 1219 loss: 0.00132070761
Iter: 1220 loss: 0.00132039608
Iter: 1221 loss: 0.0013197154
Iter: 1222 loss: 0.00132144196
Iter: 1223 loss: 0.0013194778
Iter: 1224 loss: 0.00131865614
Iter: 1225 loss: 0.0013190913
Iter: 1226 loss: 0.00131811
Iter: 1227 loss: 0.00131770724
Iter: 1228 loss: 0.00131747709
Iter: 1229 loss: 0.00131699478
Iter: 1230 loss: 0.0013186977
Iter: 1231 loss: 0.00131687755
Iter: 1232 loss: 0.00131646567
Iter: 1233 loss: 0.00131536869
Iter: 1234 loss: 0.0013233372
Iter: 1235 loss: 0.00131512852
Iter: 1236 loss: 0.00131436344
Iter: 1237 loss: 0.00131906709
Iter: 1238 loss: 0.00131426542
Iter: 1239 loss: 0.00131454389
Iter: 1240 loss: 0.00131381652
Iter: 1241 loss: 0.00131350732
Iter: 1242 loss: 0.00131261814
Iter: 1243 loss: 0.00131660968
Iter: 1244 loss: 0.00131228811
Iter: 1245 loss: 0.00131113583
Iter: 1246 loss: 0.00131748396
Iter: 1247 loss: 0.00131096481
Iter: 1248 loss: 0.00130985654
Iter: 1249 loss: 0.00130784092
Iter: 1250 loss: 0.00135545374
Iter: 1251 loss: 0.00130784046
Iter: 1252 loss: 0.00130598375
Iter: 1253 loss: 0.00132382091
Iter: 1254 loss: 0.00130590354
Iter: 1255 loss: 0.00130406918
Iter: 1256 loss: 0.00132962968
Iter: 1257 loss: 0.0013040629
Iter: 1258 loss: 0.00130354613
Iter: 1259 loss: 0.00130331866
Iter: 1260 loss: 0.00130305486
Iter: 1261 loss: 0.00130238489
Iter: 1262 loss: 0.00130410423
Iter: 1263 loss: 0.00130215194
Iter: 1264 loss: 0.0013011226
Iter: 1265 loss: 0.00131286308
Iter: 1266 loss: 0.0013010985
Iter: 1267 loss: 0.00130027463
Iter: 1268 loss: 0.00130050199
Iter: 1269 loss: 0.00129968137
Iter: 1270 loss: 0.00129863899
Iter: 1271 loss: 0.00129814469
Iter: 1272 loss: 0.001297632
Iter: 1273 loss: 0.0012976689
Iter: 1274 loss: 0.00129707658
Iter: 1275 loss: 0.00129656401
Iter: 1276 loss: 0.00129576435
Iter: 1277 loss: 0.0012957548
Iter: 1278 loss: 0.00129461323
Iter: 1279 loss: 0.00129392534
Iter: 1280 loss: 0.00129345991
Iter: 1281 loss: 0.00129125966
Iter: 1282 loss: 0.00130639644
Iter: 1283 loss: 0.00129104103
Iter: 1284 loss: 0.00129032484
Iter: 1285 loss: 0.00129009667
Iter: 1286 loss: 0.00128967466
Iter: 1287 loss: 0.00128901284
Iter: 1288 loss: 0.00129467109
Iter: 1289 loss: 0.0012889721
Iter: 1290 loss: 0.00128841656
Iter: 1291 loss: 0.00129049248
Iter: 1292 loss: 0.00128827896
Iter: 1293 loss: 0.00128782704
Iter: 1294 loss: 0.00128702051
Iter: 1295 loss: 0.00130714988
Iter: 1296 loss: 0.00128701876
Iter: 1297 loss: 0.00128624984
Iter: 1298 loss: 0.00128623669
Iter: 1299 loss: 0.00128558814
Iter: 1300 loss: 0.00128641655
Iter: 1301 loss: 0.00128525123
Iter: 1302 loss: 0.00128465414
Iter: 1303 loss: 0.00128332712
Iter: 1304 loss: 0.00130268536
Iter: 1305 loss: 0.00128326053
Iter: 1306 loss: 0.00128347403
Iter: 1307 loss: 0.00128264958
Iter: 1308 loss: 0.00128207507
Iter: 1309 loss: 0.00128144864
Iter: 1310 loss: 0.00128134713
Iter: 1311 loss: 0.00128067716
Iter: 1312 loss: 0.00127982406
Iter: 1313 loss: 0.00127976201
Iter: 1314 loss: 0.0012787506
Iter: 1315 loss: 0.00127948204
Iter: 1316 loss: 0.001278123
Iter: 1317 loss: 0.00127737946
Iter: 1318 loss: 0.00127576396
Iter: 1319 loss: 0.00130130327
Iter: 1320 loss: 0.00127570366
Iter: 1321 loss: 0.00127657223
Iter: 1322 loss: 0.00127500261
Iter: 1323 loss: 0.0012743877
Iter: 1324 loss: 0.00127817737
Iter: 1325 loss: 0.00127431564
Iter: 1326 loss: 0.00127388188
Iter: 1327 loss: 0.00127363182
Iter: 1328 loss: 0.00127344707
Iter: 1329 loss: 0.00127278268
Iter: 1330 loss: 0.00127785548
Iter: 1331 loss: 0.00127273565
Iter: 1332 loss: 0.00127210654
Iter: 1333 loss: 0.00127525325
Iter: 1334 loss: 0.00127200503
Iter: 1335 loss: 0.00127150456
Iter: 1336 loss: 0.00126985577
Iter: 1337 loss: 0.00127012271
Iter: 1338 loss: 0.00126821618
Iter: 1339 loss: 0.00127795106
Iter: 1340 loss: 0.00126760523
Iter: 1341 loss: 0.00126689579
Iter: 1342 loss: 0.00127056497
Iter: 1343 loss: 0.0012667838
Iter: 1344 loss: 0.0012660583
Iter: 1345 loss: 0.00126406935
Iter: 1346 loss: 0.00127686188
Iter: 1347 loss: 0.00126356236
Iter: 1348 loss: 0.00126209063
Iter: 1349 loss: 0.00128138834
Iter: 1350 loss: 0.00126207783
Iter: 1351 loss: 0.00126005127
Iter: 1352 loss: 0.00127084227
Iter: 1353 loss: 0.00125973497
Iter: 1354 loss: 0.00125917117
Iter: 1355 loss: 0.00125862
Iter: 1356 loss: 0.00125849666
Iter: 1357 loss: 0.00125732273
Iter: 1358 loss: 0.00126887765
Iter: 1359 loss: 0.00125728082
Iter: 1360 loss: 0.00125666079
Iter: 1361 loss: 0.00125895487
Iter: 1362 loss: 0.00125650968
Iter: 1363 loss: 0.0012558091
Iter: 1364 loss: 0.00125853717
Iter: 1365 loss: 0.00125564681
Iter: 1366 loss: 0.00125467824
Iter: 1367 loss: 0.00125903031
Iter: 1368 loss: 0.00125449104
Iter: 1369 loss: 0.00125352561
Iter: 1370 loss: 0.00125949155
Iter: 1371 loss: 0.00125340722
Iter: 1372 loss: 0.00125286507
Iter: 1373 loss: 0.00125186401
Iter: 1374 loss: 0.00127469748
Iter: 1375 loss: 0.00125186378
Iter: 1376 loss: 0.00125084294
Iter: 1377 loss: 0.00125057681
Iter: 1378 loss: 0.00125005248
Iter: 1379 loss: 0.00124910427
Iter: 1380 loss: 0.00127144577
Iter: 1381 loss: 0.00124909985
Iter: 1382 loss: 0.00124824781
Iter: 1383 loss: 0.00124888332
Iter: 1384 loss: 0.00124772172
Iter: 1385 loss: 0.00124724768
Iter: 1386 loss: 0.00124745851
Iter: 1387 loss: 0.00124692102
Iter: 1388 loss: 0.00124642695
Iter: 1389 loss: 0.00124530436
Iter: 1390 loss: 0.00126045896
Iter: 1391 loss: 0.00124523463
Iter: 1392 loss: 0.00124450331
Iter: 1393 loss: 0.00124441227
Iter: 1394 loss: 0.00124371075
Iter: 1395 loss: 0.00124923
Iter: 1396 loss: 0.00124366302
Iter: 1397 loss: 0.00124297128
Iter: 1398 loss: 0.00124218455
Iter: 1399 loss: 0.00124208024
Iter: 1400 loss: 0.00124164345
Iter: 1401 loss: 0.00124151306
Iter: 1402 loss: 0.00124118547
Iter: 1403 loss: 0.00124053203
Iter: 1404 loss: 0.00125297287
Iter: 1405 loss: 0.00124052586
Iter: 1406 loss: 0.0012398375
Iter: 1407 loss: 0.00123879814
Iter: 1408 loss: 0.00123877986
Iter: 1409 loss: 0.00123806414
Iter: 1410 loss: 0.00124094542
Iter: 1411 loss: 0.00123790745
Iter: 1412 loss: 0.00123748882
Iter: 1413 loss: 0.00123655132
Iter: 1414 loss: 0.00124908634
Iter: 1415 loss: 0.0012364916
Iter: 1416 loss: 0.00123547076
Iter: 1417 loss: 0.00123564946
Iter: 1418 loss: 0.00123469823
Iter: 1419 loss: 0.0012337584
Iter: 1420 loss: 0.00124601054
Iter: 1421 loss: 0.00123375189
Iter: 1422 loss: 0.00123265653
Iter: 1423 loss: 0.00123381079
Iter: 1424 loss: 0.00123205408
Iter: 1425 loss: 0.00123113219
Iter: 1426 loss: 0.0012332527
Iter: 1427 loss: 0.00123078318
Iter: 1428 loss: 0.00122977
Iter: 1429 loss: 0.00122890691
Iter: 1430 loss: 0.00122862007
Iter: 1431 loss: 0.00122719468
Iter: 1432 loss: 0.00122714834
Iter: 1433 loss: 0.00122660713
Iter: 1434 loss: 0.00123004883
Iter: 1435 loss: 0.00122654485
Iter: 1436 loss: 0.00122603821
Iter: 1437 loss: 0.00122823671
Iter: 1438 loss: 0.00122594077
Iter: 1439 loss: 0.00122555252
Iter: 1440 loss: 0.00122733577
Iter: 1441 loss: 0.00122547895
Iter: 1442 loss: 0.00122518104
Iter: 1443 loss: 0.00122809934
Iter: 1444 loss: 0.00122516649
Iter: 1445 loss: 0.00122490921
Iter: 1446 loss: 0.00122403249
Iter: 1447 loss: 0.0012233844
Iter: 1448 loss: 0.00122288545
Iter: 1449 loss: 0.00122056413
Iter: 1450 loss: 0.00123497727
Iter: 1451 loss: 0.00122028147
Iter: 1452 loss: 0.00122255168
Iter: 1453 loss: 0.00121951231
Iter: 1454 loss: 0.00121903606
Iter: 1455 loss: 0.00122058135
Iter: 1456 loss: 0.00121889985
Iter: 1457 loss: 0.00121836516
Iter: 1458 loss: 0.00121699332
Iter: 1459 loss: 0.0012285246
Iter: 1460 loss: 0.00121676433
Iter: 1461 loss: 0.00121503533
Iter: 1462 loss: 0.0012152472
Iter: 1463 loss: 0.00121371134
Iter: 1464 loss: 0.00121286337
Iter: 1465 loss: 0.00122146017
Iter: 1466 loss: 0.00121283531
Iter: 1467 loss: 0.00121205614
Iter: 1468 loss: 0.00122319092
Iter: 1469 loss: 0.00121205486
Iter: 1470 loss: 0.0012114885
Iter: 1471 loss: 0.00121441728
Iter: 1472 loss: 0.00121140108
Iter: 1473 loss: 0.00121108664
Iter: 1474 loss: 0.00121099968
Iter: 1475 loss: 0.00121080654
Iter: 1476 loss: 0.00121022481
Iter: 1477 loss: 0.0012116232
Iter: 1478 loss: 0.00121001177
Iter: 1479 loss: 0.00120901107
Iter: 1480 loss: 0.00120792584
Iter: 1481 loss: 0.00120775984
Iter: 1482 loss: 0.00120693236
Iter: 1483 loss: 0.00120800617
Iter: 1484 loss: 0.00120650523
Iter: 1485 loss: 0.00120553921
Iter: 1486 loss: 0.00120412523
Iter: 1487 loss: 0.0012040846
Iter: 1488 loss: 0.00120449229
Iter: 1489 loss: 0.00120356516
Iter: 1490 loss: 0.0012029534
Iter: 1491 loss: 0.00120121147
Iter: 1492 loss: 0.00120991014
Iter: 1493 loss: 0.0012005975
Iter: 1494 loss: 0.00119915942
Iter: 1495 loss: 0.00120027922
Iter: 1496 loss: 0.00119828386
Iter: 1497 loss: 0.0011970026
Iter: 1498 loss: 0.00119907921
Iter: 1499 loss: 0.00119639956
Iter: 1500 loss: 0.00119577348
Iter: 1501 loss: 0.00119573949
Iter: 1502 loss: 0.0011954254
Iter: 1503 loss: 0.00119615078
Iter: 1504 loss: 0.00119530386
Iter: 1505 loss: 0.00119502726
Iter: 1506 loss: 0.00119450374
Iter: 1507 loss: 0.00120614492
Iter: 1508 loss: 0.00119450164
Iter: 1509 loss: 0.00119361025
Iter: 1510 loss: 0.00120042847
Iter: 1511 loss: 0.00119354529
Iter: 1512 loss: 0.00119307742
Iter: 1513 loss: 0.00119986339
Iter: 1514 loss: 0.00119307905
Iter: 1515 loss: 0.00119269104
Iter: 1516 loss: 0.00119154924
Iter: 1517 loss: 0.00119614683
Iter: 1518 loss: 0.00119107892
Iter: 1519 loss: 0.0011900313
Iter: 1520 loss: 0.00119198207
Iter: 1521 loss: 0.00118959544
Iter: 1522 loss: 0.00118870707
Iter: 1523 loss: 0.00118783687
Iter: 1524 loss: 0.00118764653
Iter: 1525 loss: 0.00118711649
Iter: 1526 loss: 0.00118673174
Iter: 1527 loss: 0.00118628261
Iter: 1528 loss: 0.00118544477
Iter: 1529 loss: 0.00120348041
Iter: 1530 loss: 0.00118544232
Iter: 1531 loss: 0.00118390273
Iter: 1532 loss: 0.00119160244
Iter: 1533 loss: 0.00118364324
Iter: 1534 loss: 0.00118259375
Iter: 1535 loss: 0.00118843047
Iter: 1536 loss: 0.0011824437
Iter: 1537 loss: 0.00118369
Iter: 1538 loss: 0.00118211028
Iter: 1539 loss: 0.00118195871
Iter: 1540 loss: 0.00118164462
Iter: 1541 loss: 0.00118705013
Iter: 1542 loss: 0.00118163391
Iter: 1543 loss: 0.00118051842
Iter: 1544 loss: 0.00118442159
Iter: 1545 loss: 0.00118022494
Iter: 1546 loss: 0.00117887603
Iter: 1547 loss: 0.00117884553
Iter: 1548 loss: 0.00117820839
Iter: 1549 loss: 0.00118106487
Iter: 1550 loss: 0.00117808045
Iter: 1551 loss: 0.00117757765
Iter: 1552 loss: 0.00117636216
Iter: 1553 loss: 0.00118923257
Iter: 1554 loss: 0.00117622875
Iter: 1555 loss: 0.00117461802
Iter: 1556 loss: 0.00118777575
Iter: 1557 loss: 0.00117451977
Iter: 1558 loss: 0.00117379753
Iter: 1559 loss: 0.00117238343
Iter: 1560 loss: 0.00120011438
Iter: 1561 loss: 0.00117237249
Iter: 1562 loss: 0.00116944406
Iter: 1563 loss: 0.00119458896
Iter: 1564 loss: 0.00116928527
Iter: 1565 loss: 0.00116887712
Iter: 1566 loss: 0.00116981182
Iter: 1567 loss: 0.00116871914
Iter: 1568 loss: 0.00116839318
Iter: 1569 loss: 0.0011675139
Iter: 1570 loss: 0.00117306144
Iter: 1571 loss: 0.00116728968
Iter: 1572 loss: 0.00116908061
Iter: 1573 loss: 0.00116689538
Iter: 1574 loss: 0.00116631342
Iter: 1575 loss: 0.00116485101
Iter: 1576 loss: 0.00117882364
Iter: 1577 loss: 0.00116464938
Iter: 1578 loss: 0.0011630638
Iter: 1579 loss: 0.00116306206
Iter: 1580 loss: 0.00116185611
Iter: 1581 loss: 0.00118021213
Iter: 1582 loss: 0.00116185425
Iter: 1583 loss: 0.00116119161
Iter: 1584 loss: 0.00116171292
Iter: 1585 loss: 0.00116079301
Iter: 1586 loss: 0.00116035412
Iter: 1587 loss: 0.0011595285
Iter: 1588 loss: 0.0011777461
Iter: 1589 loss: 0.00115952641
Iter: 1590 loss: 0.00115853362
Iter: 1591 loss: 0.00115850591
Iter: 1592 loss: 0.00115770195
Iter: 1593 loss: 0.00115903048
Iter: 1594 loss: 0.00115733268
Iter: 1595 loss: 0.00115681882
Iter: 1596 loss: 0.00115962268
Iter: 1597 loss: 0.00115674408
Iter: 1598 loss: 0.00115645421
Iter: 1599 loss: 0.00115637551
Iter: 1600 loss: 0.00115610077
Iter: 1601 loss: 0.00115521299
Iter: 1602 loss: 0.00115584326
Iter: 1603 loss: 0.00115444162
Iter: 1604 loss: 0.00115388981
Iter: 1605 loss: 0.00115385419
Iter: 1606 loss: 0.00115348643
Iter: 1607 loss: 0.00115262414
Iter: 1608 loss: 0.00116279896
Iter: 1609 loss: 0.00115255127
Iter: 1610 loss: 0.00115124625
Iter: 1611 loss: 0.00115119631
Iter: 1612 loss: 0.0011501892
Iter: 1613 loss: 0.00114962854
Iter: 1614 loss: 0.00114967488
Iter: 1615 loss: 0.00114919432
Iter: 1616 loss: 0.0011481297
Iter: 1617 loss: 0.00115548377
Iter: 1618 loss: 0.00114802714
Iter: 1619 loss: 0.0011472574
Iter: 1620 loss: 0.00115127326
Iter: 1621 loss: 0.00114713586
Iter: 1622 loss: 0.00114651828
Iter: 1623 loss: 0.00114567368
Iter: 1624 loss: 0.00114562979
Iter: 1625 loss: 0.00114473386
Iter: 1626 loss: 0.00114355958
Iter: 1627 loss: 0.00114348473
Iter: 1628 loss: 0.00114195026
Iter: 1629 loss: 0.00116087333
Iter: 1630 loss: 0.00114193
Iter: 1631 loss: 0.00114178495
Iter: 1632 loss: 0.00114156306
Iter: 1633 loss: 0.00114130857
Iter: 1634 loss: 0.00114094198
Iter: 1635 loss: 0.00114092533
Iter: 1636 loss: 0.00114043523
Iter: 1637 loss: 0.00114038703
Iter: 1638 loss: 0.00113973836
Iter: 1639 loss: 0.00113916281
Iter: 1640 loss: 0.0011389961
Iter: 1641 loss: 0.00113830029
Iter: 1642 loss: 0.0011389103
Iter: 1643 loss: 0.00113789656
Iter: 1644 loss: 0.00113743125
Iter: 1645 loss: 0.00113595475
Iter: 1646 loss: 0.00113778526
Iter: 1647 loss: 0.00113483658
Iter: 1648 loss: 0.001133568
Iter: 1649 loss: 0.00113343261
Iter: 1650 loss: 0.00113228755
Iter: 1651 loss: 0.00114114326
Iter: 1652 loss: 0.00113220967
Iter: 1653 loss: 0.00113139837
Iter: 1654 loss: 0.00113475486
Iter: 1655 loss: 0.00113122072
Iter: 1656 loss: 0.001130388
Iter: 1657 loss: 0.0011314794
Iter: 1658 loss: 0.00112996041
Iter: 1659 loss: 0.00112919637
Iter: 1660 loss: 0.00113317475
Iter: 1661 loss: 0.00112907577
Iter: 1662 loss: 0.00112815667
Iter: 1663 loss: 0.00112764724
Iter: 1664 loss: 0.00112724188
Iter: 1665 loss: 0.0011272952
Iter: 1666 loss: 0.0011266463
Iter: 1667 loss: 0.00112644047
Iter: 1668 loss: 0.00112586399
Iter: 1669 loss: 0.0011289242
Iter: 1670 loss: 0.00112567842
Iter: 1671 loss: 0.00112459273
Iter: 1672 loss: 0.00112457504
Iter: 1673 loss: 0.0011235131
Iter: 1674 loss: 0.00112676248
Iter: 1675 loss: 0.00112319412
Iter: 1676 loss: 0.00112250261
Iter: 1677 loss: 0.00112340949
Iter: 1678 loss: 0.0011221508
Iter: 1679 loss: 0.00112126605
Iter: 1680 loss: 0.00112060236
Iter: 1681 loss: 0.00112030597
Iter: 1682 loss: 0.00111915171
Iter: 1683 loss: 0.00111911411
Iter: 1684 loss: 0.00111817825
Iter: 1685 loss: 0.00111996674
Iter: 1686 loss: 0.0011177907
Iter: 1687 loss: 0.0011171666
Iter: 1688 loss: 0.00111581723
Iter: 1689 loss: 0.00113686174
Iter: 1690 loss: 0.00111576798
Iter: 1691 loss: 0.00111465959
Iter: 1692 loss: 0.00111466064
Iter: 1693 loss: 0.00111379311
Iter: 1694 loss: 0.00112056453
Iter: 1695 loss: 0.00111373398
Iter: 1696 loss: 0.00111358007
Iter: 1697 loss: 0.00111341523
Iter: 1698 loss: 0.00111315702
Iter: 1699 loss: 0.00111282815
Iter: 1700 loss: 0.00111280568
Iter: 1701 loss: 0.00111231534
Iter: 1702 loss: 0.00111330289
Iter: 1703 loss: 0.00111211953
Iter: 1704 loss: 0.00111151475
Iter: 1705 loss: 0.00111371232
Iter: 1706 loss: 0.00111136865
Iter: 1707 loss: 0.00111081812
Iter: 1708 loss: 0.00110933487
Iter: 1709 loss: 0.00111901725
Iter: 1710 loss: 0.00110897399
Iter: 1711 loss: 0.00110669178
Iter: 1712 loss: 0.00111782399
Iter: 1713 loss: 0.00110630365
Iter: 1714 loss: 0.00110516127
Iter: 1715 loss: 0.00110514183
Iter: 1716 loss: 0.00110452203
Iter: 1717 loss: 0.00110295368
Iter: 1718 loss: 0.00111734215
Iter: 1719 loss: 0.0011027141
Iter: 1720 loss: 0.00110108766
Iter: 1721 loss: 0.00111686834
Iter: 1722 loss: 0.00110102748
Iter: 1723 loss: 0.00110017892
Iter: 1724 loss: 0.00110945408
Iter: 1725 loss: 0.00110016088
Iter: 1726 loss: 0.00109940697
Iter: 1727 loss: 0.00109992945
Iter: 1728 loss: 0.00109894481
Iter: 1729 loss: 0.00109837682
Iter: 1730 loss: 0.00110075914
Iter: 1731 loss: 0.00109826028
Iter: 1732 loss: 0.00109757902
Iter: 1733 loss: 0.0010994816
Iter: 1734 loss: 0.00109735806
Iter: 1735 loss: 0.00109676807
Iter: 1736 loss: 0.00109725853
Iter: 1737 loss: 0.00109641277
Iter: 1738 loss: 0.00109571742
Iter: 1739 loss: 0.00110478583
Iter: 1740 loss: 0.00109571207
Iter: 1741 loss: 0.00109534059
Iter: 1742 loss: 0.00109407492
Iter: 1743 loss: 0.00109286862
Iter: 1744 loss: 0.00109231554
Iter: 1745 loss: 0.00109219435
Iter: 1746 loss: 0.00109106652
Iter: 1747 loss: 0.00109057431
Iter: 1748 loss: 0.0010943478
Iter: 1749 loss: 0.0010905324
Iter: 1750 loss: 0.00109004334
Iter: 1751 loss: 0.00109541905
Iter: 1752 loss: 0.00109003321
Iter: 1753 loss: 0.00108967454
Iter: 1754 loss: 0.0010887034
Iter: 1755 loss: 0.00109504093
Iter: 1756 loss: 0.00108846196
Iter: 1757 loss: 0.00108762935
Iter: 1758 loss: 0.00108789327
Iter: 1759 loss: 0.00108703587
Iter: 1760 loss: 0.00108659756
Iter: 1761 loss: 0.00108645088
Iter: 1762 loss: 0.00108619675
Iter: 1763 loss: 0.00108587812
Iter: 1764 loss: 0.00108513818
Iter: 1765 loss: 0.00109479786
Iter: 1766 loss: 0.00108508766
Iter: 1767 loss: 0.00108379545
Iter: 1768 loss: 0.00108800246
Iter: 1769 loss: 0.00108342851
Iter: 1770 loss: 0.00108275318
Iter: 1771 loss: 0.00108396378
Iter: 1772 loss: 0.00108246307
Iter: 1773 loss: 0.00108160544
Iter: 1774 loss: 0.00108182849
Iter: 1775 loss: 0.00108098402
Iter: 1776 loss: 0.00107993116
Iter: 1777 loss: 0.00108671468
Iter: 1778 loss: 0.00107981241
Iter: 1779 loss: 0.00107888726
Iter: 1780 loss: 0.0010779436
Iter: 1781 loss: 0.0010777656
Iter: 1782 loss: 0.00107701763
Iter: 1783 loss: 0.00107552204
Iter: 1784 loss: 0.00110330549
Iter: 1785 loss: 0.00107549969
Iter: 1786 loss: 0.00107377348
Iter: 1787 loss: 0.00109546981
Iter: 1788 loss: 0.00107375858
Iter: 1789 loss: 0.00107293855
Iter: 1790 loss: 0.00107660762
Iter: 1791 loss: 0.00107276929
Iter: 1792 loss: 0.0010721758
Iter: 1793 loss: 0.00107153947
Iter: 1794 loss: 0.00107143587
Iter: 1795 loss: 0.00107067602
Iter: 1796 loss: 0.00107255916
Iter: 1797 loss: 0.00107040606
Iter: 1798 loss: 0.00106952514
Iter: 1799 loss: 0.00107590167
Iter: 1800 loss: 0.00106944877
Iter: 1801 loss: 0.00106880534
Iter: 1802 loss: 0.00106764329
Iter: 1803 loss: 0.00109674409
Iter: 1804 loss: 0.00106764352
Iter: 1805 loss: 0.00106615794
Iter: 1806 loss: 0.00108011707
Iter: 1807 loss: 0.00106609752
Iter: 1808 loss: 0.00106549938
Iter: 1809 loss: 0.00106547505
Iter: 1810 loss: 0.001065013
Iter: 1811 loss: 0.00106433535
Iter: 1812 loss: 0.00106406317
Iter: 1813 loss: 0.00106369937
Iter: 1814 loss: 0.00106312218
Iter: 1815 loss: 0.00106254732
Iter: 1816 loss: 0.00106242718
Iter: 1817 loss: 0.00106128829
Iter: 1818 loss: 0.00106114685
Iter: 1819 loss: 0.00106083788
Iter: 1820 loss: 0.00106059562
Iter: 1821 loss: 0.00106050132
Iter: 1822 loss: 0.00106000935
Iter: 1823 loss: 0.00105915789
Iter: 1824 loss: 0.00105915661
Iter: 1825 loss: 0.00105822249
Iter: 1826 loss: 0.00106610777
Iter: 1827 loss: 0.00105817243
Iter: 1828 loss: 0.00105756545
Iter: 1829 loss: 0.00106033508
Iter: 1830 loss: 0.00105744554
Iter: 1831 loss: 0.00105680968
Iter: 1832 loss: 0.00105784973
Iter: 1833 loss: 0.0010565198
Iter: 1834 loss: 0.00105579384
Iter: 1835 loss: 0.00105914124
Iter: 1836 loss: 0.00105565961
Iter: 1837 loss: 0.00105489651
Iter: 1838 loss: 0.00105399929
Iter: 1839 loss: 0.00105390116
Iter: 1840 loss: 0.00105269917
Iter: 1841 loss: 0.00106674677
Iter: 1842 loss: 0.0010526845
Iter: 1843 loss: 0.00105197402
Iter: 1844 loss: 0.00105278846
Iter: 1845 loss: 0.00105158938
Iter: 1846 loss: 0.00105079927
Iter: 1847 loss: 0.0010489783
Iter: 1848 loss: 0.00107205729
Iter: 1849 loss: 0.00104885618
Iter: 1850 loss: 0.00104920985
Iter: 1851 loss: 0.00104849623
Iter: 1852 loss: 0.00104823802
Iter: 1853 loss: 0.00104829145
Iter: 1854 loss: 0.00104804314
Iter: 1855 loss: 0.00104757422
Iter: 1856 loss: 0.00104609272
Iter: 1857 loss: 0.00104817678
Iter: 1858 loss: 0.00104501646
Iter: 1859 loss: 0.00104459759
Iter: 1860 loss: 0.00104448711
Iter: 1861 loss: 0.0010441083
Iter: 1862 loss: 0.00104338769
Iter: 1863 loss: 0.00105904555
Iter: 1864 loss: 0.00104338769
Iter: 1865 loss: 0.00104256067
Iter: 1866 loss: 0.00104239443
Iter: 1867 loss: 0.00104184612
Iter: 1868 loss: 0.00104159152
Iter: 1869 loss: 0.00104138337
Iter: 1870 loss: 0.00104074436
Iter: 1871 loss: 0.00104213471
Iter: 1872 loss: 0.00104049291
Iter: 1873 loss: 0.00103964983
Iter: 1874 loss: 0.00103841757
Iter: 1875 loss: 0.00103838299
Iter: 1876 loss: 0.00103868172
Iter: 1877 loss: 0.0010374561
Iter: 1878 loss: 0.00103679171
Iter: 1879 loss: 0.00103735144
Iter: 1880 loss: 0.00103640242
Iter: 1881 loss: 0.00103605445
Iter: 1882 loss: 0.00103989639
Iter: 1883 loss: 0.00103604863
Iter: 1884 loss: 0.00103571557
Iter: 1885 loss: 0.00103592279
Iter: 1886 loss: 0.00103550241
Iter: 1887 loss: 0.00103494769
Iter: 1888 loss: 0.00103645702
Iter: 1889 loss: 0.00103477039
Iter: 1890 loss: 0.00103447656
Iter: 1891 loss: 0.0010338818
Iter: 1892 loss: 0.00104449876
Iter: 1893 loss: 0.00103387458
Iter: 1894 loss: 0.00103279576
Iter: 1895 loss: 0.00103382603
Iter: 1896 loss: 0.00103217666
Iter: 1897 loss: 0.00103116268
Iter: 1898 loss: 0.00102910062
Iter: 1899 loss: 0.00106799568
Iter: 1900 loss: 0.00102906721
Iter: 1901 loss: 0.00102765579
Iter: 1902 loss: 0.00102764508
Iter: 1903 loss: 0.00102683797
Iter: 1904 loss: 0.00102565624
Iter: 1905 loss: 0.00102562422
Iter: 1906 loss: 0.00102397148
Iter: 1907 loss: 0.00102385867
Iter: 1908 loss: 0.0010244071
Iter: 1909 loss: 0.0010231575
Iter: 1910 loss: 0.00102274178
Iter: 1911 loss: 0.00102364877
Iter: 1912 loss: 0.00102257775
Iter: 1913 loss: 0.00102182594
Iter: 1914 loss: 0.00102819153
Iter: 1915 loss: 0.0010217838
Iter: 1916 loss: 0.0010207172
Iter: 1917 loss: 0.00102873053
Iter: 1918 loss: 0.00102062942
Iter: 1919 loss: 0.00102008274
Iter: 1920 loss: 0.0010225391
Iter: 1921 loss: 0.00101998029
Iter: 1922 loss: 0.00101958623
Iter: 1923 loss: 0.00101863127
Iter: 1924 loss: 0.00102869328
Iter: 1925 loss: 0.00101851486
Iter: 1926 loss: 0.00101645745
Iter: 1927 loss: 0.00101571577
Iter: 1928 loss: 0.00101456523
Iter: 1929 loss: 0.00101319968
Iter: 1930 loss: 0.00101704802
Iter: 1931 loss: 0.00101275928
Iter: 1932 loss: 0.0010127424
Iter: 1933 loss: 0.00101229281
Iter: 1934 loss: 0.00101197336
Iter: 1935 loss: 0.00101093238
Iter: 1936 loss: 0.0010112602
Iter: 1937 loss: 0.00100992562
Iter: 1938 loss: 0.00100859732
Iter: 1939 loss: 0.00101192133
Iter: 1940 loss: 0.00100812037
Iter: 1941 loss: 0.00100745796
Iter: 1942 loss: 0.0010105978
Iter: 1943 loss: 0.00100733642
Iter: 1944 loss: 0.00100635737
Iter: 1945 loss: 0.0010077348
Iter: 1946 loss: 0.00100587332
Iter: 1947 loss: 0.00100535166
Iter: 1948 loss: 0.00101034075
Iter: 1949 loss: 0.00100533105
Iter: 1950 loss: 0.00100492744
Iter: 1951 loss: 0.00100740662
Iter: 1952 loss: 0.00100487983
Iter: 1953 loss: 0.00100462628
Iter: 1954 loss: 0.00100426539
Iter: 1955 loss: 0.00100425025
Iter: 1956 loss: 0.00100391242
Iter: 1957 loss: 0.00100294244
Iter: 1958 loss: 0.00100761279
Iter: 1959 loss: 0.00100260787
Iter: 1960 loss: 0.00100228423
Iter: 1961 loss: 0.00100174733
Iter: 1962 loss: 0.001001255
Iter: 1963 loss: 0.00100221008
Iter: 1964 loss: 0.00100104965
Iter: 1965 loss: 0.00100043393
Iter: 1966 loss: 0.00100020319
Iter: 1967 loss: 0.000999862328
Iter: 1968 loss: 0.000999185606
Iter: 1969 loss: 0.00100748753
Iter: 1970 loss: 0.000999179
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.4/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.8
+ date
Sun Nov  8 15:44:38 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6c1d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6c1d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6c87e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6bab730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6babe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6babb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6ae3c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa59658f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa59659b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5c6b1e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa59650f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa596510f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5965277b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5705d28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5705d2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5705d20d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa596572620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa596572f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5705a69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5705b0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5965f46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa570487378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5704408c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa570459b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa570455378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa570455b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5703e48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5703857b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5703858c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5703b2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5704df400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa57034e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa57034ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa570364620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa57041cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa5702b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 202, in <module>
    grads = tape.gradient(loss, model.trainable_weights)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1266, in _backward_function_wrapper
    processed_args, remapped_captures)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input is not invertible.
	 [[node gradients/MatrixDeterminant_grad/MatrixInverse (defined at biholoNN_train.py:200) ]] [Op:__inference___backward_volume_form_4446_8945]

Function call stack:
__backward_volume_form_4446

+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi0.8/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732f057488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732f168e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732f168ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732f0b6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732f0b6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732f0b6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732eff98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732efa4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732efa4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732efa4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ef64ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ef1ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ef387b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ef388c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ef38d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ee839d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732eeb92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732eeb9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ee2b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ee2ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ede28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732ed8a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f732edc0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308bbb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308bac7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308b661e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308b23598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308b452f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308b45378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308aff8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308a96400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308ac11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308ac1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308a7a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308a23d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7308a23c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.64056983e-05
Iter: 2 loss: 1.98075613e-05
Iter: 3 loss: 1.45237045e-05
Iter: 4 loss: 1.38381729e-05
Iter: 5 loss: 1.53365108e-05
Iter: 6 loss: 1.35744785e-05
Iter: 7 loss: 1.31374154e-05
Iter: 8 loss: 1.33011108e-05
Iter: 9 loss: 1.28327101e-05
Iter: 10 loss: 1.22324946e-05
Iter: 11 loss: 1.55494126e-05
Iter: 12 loss: 1.21461908e-05
Iter: 13 loss: 1.16421706e-05
Iter: 14 loss: 1.17976324e-05
Iter: 15 loss: 1.12826547e-05
Iter: 16 loss: 1.09972079e-05
Iter: 17 loss: 1.09794237e-05
Iter: 18 loss: 1.07540018e-05
Iter: 19 loss: 1.04723304e-05
Iter: 20 loss: 1.04492492e-05
Iter: 21 loss: 1.00540474e-05
Iter: 22 loss: 1.03075963e-05
Iter: 23 loss: 9.80344612e-06
Iter: 24 loss: 9.2203e-06
Iter: 25 loss: 1.39034973e-05
Iter: 26 loss: 9.18223213e-06
Iter: 27 loss: 8.92807566e-06
Iter: 28 loss: 8.73842691e-06
Iter: 29 loss: 8.65417132e-06
Iter: 30 loss: 8.35203718e-06
Iter: 31 loss: 8.54313475e-06
Iter: 32 loss: 8.15934254e-06
Iter: 33 loss: 7.95247615e-06
Iter: 34 loss: 1.08403601e-05
Iter: 35 loss: 7.95188316e-06
Iter: 36 loss: 7.81998e-06
Iter: 37 loss: 7.74564069e-06
Iter: 38 loss: 7.68848349e-06
Iter: 39 loss: 7.5074513e-06
Iter: 40 loss: 7.50249092e-06
Iter: 41 loss: 7.41675376e-06
Iter: 42 loss: 7.33002798e-06
Iter: 43 loss: 7.31317186e-06
Iter: 44 loss: 7.15385249e-06
Iter: 45 loss: 8.05716809e-06
Iter: 46 loss: 7.1318691e-06
Iter: 47 loss: 6.97896394e-06
Iter: 48 loss: 7.09320102e-06
Iter: 49 loss: 6.88513319e-06
Iter: 50 loss: 6.77041044e-06
Iter: 51 loss: 7.97063e-06
Iter: 52 loss: 6.76731634e-06
Iter: 53 loss: 6.66595042e-06
Iter: 54 loss: 6.7591418e-06
Iter: 55 loss: 6.60733667e-06
Iter: 56 loss: 6.49635331e-06
Iter: 57 loss: 6.49012782e-06
Iter: 58 loss: 6.40557e-06
Iter: 59 loss: 6.30818e-06
Iter: 60 loss: 6.30814475e-06
Iter: 61 loss: 6.23496589e-06
Iter: 62 loss: 6.39466634e-06
Iter: 63 loss: 6.20682249e-06
Iter: 64 loss: 6.15787394e-06
Iter: 65 loss: 6.06764524e-06
Iter: 66 loss: 8.14361738e-06
Iter: 67 loss: 6.06763206e-06
Iter: 68 loss: 5.95045913e-06
Iter: 69 loss: 6.2759691e-06
Iter: 70 loss: 5.91280877e-06
Iter: 71 loss: 5.82476696e-06
Iter: 72 loss: 6.85396571e-06
Iter: 73 loss: 5.82333814e-06
Iter: 74 loss: 5.7637576e-06
Iter: 75 loss: 6.64901972e-06
Iter: 76 loss: 5.76365073e-06
Iter: 77 loss: 5.72183853e-06
Iter: 78 loss: 5.64918446e-06
Iter: 79 loss: 5.64923039e-06
Iter: 80 loss: 5.61475554e-06
Iter: 81 loss: 5.60888384e-06
Iter: 82 loss: 5.57533622e-06
Iter: 83 loss: 5.52101119e-06
Iter: 84 loss: 5.52059601e-06
Iter: 85 loss: 5.48610433e-06
Iter: 86 loss: 5.4842003e-06
Iter: 87 loss: 5.45925286e-06
Iter: 88 loss: 5.49819151e-06
Iter: 89 loss: 5.44769e-06
Iter: 90 loss: 5.42585076e-06
Iter: 91 loss: 5.40924975e-06
Iter: 92 loss: 5.40209749e-06
Iter: 93 loss: 5.36197922e-06
Iter: 94 loss: 5.55097813e-06
Iter: 95 loss: 5.35465824e-06
Iter: 96 loss: 5.30952457e-06
Iter: 97 loss: 5.41495547e-06
Iter: 98 loss: 5.29278714e-06
Iter: 99 loss: 5.25923042e-06
Iter: 100 loss: 5.23567678e-06
Iter: 101 loss: 5.22385517e-06
Iter: 102 loss: 5.16401633e-06
Iter: 103 loss: 5.22005303e-06
Iter: 104 loss: 5.12977113e-06
Iter: 105 loss: 5.08734229e-06
Iter: 106 loss: 5.70339716e-06
Iter: 107 loss: 5.0873341e-06
Iter: 108 loss: 5.04338868e-06
Iter: 109 loss: 5.24457846e-06
Iter: 110 loss: 5.03518413e-06
Iter: 111 loss: 5.01198929e-06
Iter: 112 loss: 5.01886e-06
Iter: 113 loss: 4.9954856e-06
Iter: 114 loss: 4.97785868e-06
Iter: 115 loss: 4.97756673e-06
Iter: 116 loss: 4.96479606e-06
Iter: 117 loss: 4.94533197e-06
Iter: 118 loss: 4.94495362e-06
Iter: 119 loss: 4.92489107e-06
Iter: 120 loss: 5.21171933e-06
Iter: 121 loss: 4.9248024e-06
Iter: 122 loss: 4.90690945e-06
Iter: 123 loss: 4.88580736e-06
Iter: 124 loss: 4.88347177e-06
Iter: 125 loss: 4.8611214e-06
Iter: 126 loss: 4.94917185e-06
Iter: 127 loss: 4.85593046e-06
Iter: 128 loss: 4.83238273e-06
Iter: 129 loss: 4.92112667e-06
Iter: 130 loss: 4.82672567e-06
Iter: 131 loss: 4.79939627e-06
Iter: 132 loss: 4.82956602e-06
Iter: 133 loss: 4.78437596e-06
Iter: 134 loss: 4.75940215e-06
Iter: 135 loss: 4.76056675e-06
Iter: 136 loss: 4.73950286e-06
Iter: 137 loss: 4.7116e-06
Iter: 138 loss: 4.82180667e-06
Iter: 139 loss: 4.70514351e-06
Iter: 140 loss: 4.68840244e-06
Iter: 141 loss: 4.68841608e-06
Iter: 142 loss: 4.6731443e-06
Iter: 143 loss: 4.72709053e-06
Iter: 144 loss: 4.66926303e-06
Iter: 145 loss: 4.6611267e-06
Iter: 146 loss: 4.64723689e-06
Iter: 147 loss: 4.64714867e-06
Iter: 148 loss: 4.62843673e-06
Iter: 149 loss: 4.86064073e-06
Iter: 150 loss: 4.62819571e-06
Iter: 151 loss: 4.61781e-06
Iter: 152 loss: 4.6063451e-06
Iter: 153 loss: 4.60470164e-06
Iter: 154 loss: 4.58567547e-06
Iter: 155 loss: 4.77624235e-06
Iter: 156 loss: 4.58518207e-06
Iter: 157 loss: 4.57256556e-06
Iter: 158 loss: 4.55951204e-06
Iter: 159 loss: 4.55713462e-06
Iter: 160 loss: 4.53881967e-06
Iter: 161 loss: 4.58335353e-06
Iter: 162 loss: 4.53214398e-06
Iter: 163 loss: 4.51119786e-06
Iter: 164 loss: 4.69211727e-06
Iter: 165 loss: 4.51007054e-06
Iter: 166 loss: 4.49571917e-06
Iter: 167 loss: 4.5055076e-06
Iter: 168 loss: 4.48680385e-06
Iter: 169 loss: 4.47295588e-06
Iter: 170 loss: 4.48862602e-06
Iter: 171 loss: 4.46545073e-06
Iter: 172 loss: 4.45046226e-06
Iter: 173 loss: 4.46930335e-06
Iter: 174 loss: 4.44272428e-06
Iter: 175 loss: 4.44280931e-06
Iter: 176 loss: 4.43612316e-06
Iter: 177 loss: 4.43112e-06
Iter: 178 loss: 4.42038572e-06
Iter: 179 loss: 4.59448302e-06
Iter: 180 loss: 4.4200774e-06
Iter: 181 loss: 4.40559916e-06
Iter: 182 loss: 4.42518558e-06
Iter: 183 loss: 4.39843689e-06
Iter: 184 loss: 4.38192365e-06
Iter: 185 loss: 4.59396233e-06
Iter: 186 loss: 4.3818045e-06
Iter: 187 loss: 4.37501512e-06
Iter: 188 loss: 4.36714708e-06
Iter: 189 loss: 4.36616301e-06
Iter: 190 loss: 4.34737e-06
Iter: 191 loss: 4.38502502e-06
Iter: 192 loss: 4.33978494e-06
Iter: 193 loss: 4.32849356e-06
Iter: 194 loss: 4.33842661e-06
Iter: 195 loss: 4.32188881e-06
Iter: 196 loss: 4.31219632e-06
Iter: 197 loss: 4.40084568e-06
Iter: 198 loss: 4.31178296e-06
Iter: 199 loss: 4.30284445e-06
Iter: 200 loss: 4.32575371e-06
Iter: 201 loss: 4.29978491e-06
Iter: 202 loss: 4.29146712e-06
Iter: 203 loss: 4.29065403e-06
Iter: 204 loss: 4.2847023e-06
Iter: 205 loss: 4.27548457e-06
Iter: 206 loss: 4.31270746e-06
Iter: 207 loss: 4.27334271e-06
Iter: 208 loss: 4.26436054e-06
Iter: 209 loss: 4.26264296e-06
Iter: 210 loss: 4.25654616e-06
Iter: 211 loss: 4.24255586e-06
Iter: 212 loss: 4.44717671e-06
Iter: 213 loss: 4.24254722e-06
Iter: 214 loss: 4.2351121e-06
Iter: 215 loss: 4.22463927e-06
Iter: 216 loss: 4.22429912e-06
Iter: 217 loss: 4.21744517e-06
Iter: 218 loss: 4.21735513e-06
Iter: 219 loss: 4.20998913e-06
Iter: 220 loss: 4.20054903e-06
Iter: 221 loss: 4.19993921e-06
Iter: 222 loss: 4.1910971e-06
Iter: 223 loss: 4.31591889e-06
Iter: 224 loss: 4.19102298e-06
Iter: 225 loss: 4.18477111e-06
Iter: 226 loss: 4.20395463e-06
Iter: 227 loss: 4.18285663e-06
Iter: 228 loss: 4.17883e-06
Iter: 229 loss: 4.16761031e-06
Iter: 230 loss: 4.23503434e-06
Iter: 231 loss: 4.16458806e-06
Iter: 232 loss: 4.15458e-06
Iter: 233 loss: 4.15337718e-06
Iter: 234 loss: 4.14715305e-06
Iter: 235 loss: 4.14395163e-06
Iter: 236 loss: 4.14102669e-06
Iter: 237 loss: 4.13052248e-06
Iter: 238 loss: 4.13086582e-06
Iter: 239 loss: 4.12219379e-06
Iter: 240 loss: 4.10830944e-06
Iter: 241 loss: 4.14776696e-06
Iter: 242 loss: 4.1037697e-06
Iter: 243 loss: 4.09702534e-06
Iter: 244 loss: 4.09665699e-06
Iter: 245 loss: 4.09009681e-06
Iter: 246 loss: 4.10056691e-06
Iter: 247 loss: 4.0870159e-06
Iter: 248 loss: 4.08170035e-06
Iter: 249 loss: 4.07524794e-06
Iter: 250 loss: 4.07467451e-06
Iter: 251 loss: 4.07164589e-06
Iter: 252 loss: 4.0697746e-06
Iter: 253 loss: 4.06711524e-06
Iter: 254 loss: 4.06280833e-06
Iter: 255 loss: 4.06276e-06
Iter: 256 loss: 4.05893888e-06
Iter: 257 loss: 4.05887795e-06
Iter: 258 loss: 4.05594392e-06
Iter: 259 loss: 4.04837283e-06
Iter: 260 loss: 4.11291512e-06
Iter: 261 loss: 4.04707635e-06
Iter: 262 loss: 4.0378718e-06
Iter: 263 loss: 4.08533197e-06
Iter: 264 loss: 4.03634749e-06
Iter: 265 loss: 4.02844671e-06
Iter: 266 loss: 4.11836481e-06
Iter: 267 loss: 4.02837713e-06
Iter: 268 loss: 4.02238766e-06
Iter: 269 loss: 4.0162231e-06
Iter: 270 loss: 4.01514944e-06
Iter: 271 loss: 4.00848057e-06
Iter: 272 loss: 4.03810236e-06
Iter: 273 loss: 4.00704812e-06
Iter: 274 loss: 3.99916598e-06
Iter: 275 loss: 4.00438375e-06
Iter: 276 loss: 3.99409782e-06
Iter: 277 loss: 3.99058899e-06
Iter: 278 loss: 3.98903194e-06
Iter: 279 loss: 3.98598149e-06
Iter: 280 loss: 3.98142038e-06
Iter: 281 loss: 3.98137036e-06
Iter: 282 loss: 3.97664098e-06
Iter: 283 loss: 3.99672626e-06
Iter: 284 loss: 3.97567055e-06
Iter: 285 loss: 3.96936093e-06
Iter: 286 loss: 3.98291195e-06
Iter: 287 loss: 3.96700761e-06
Iter: 288 loss: 3.9628726e-06
Iter: 289 loss: 3.97340409e-06
Iter: 290 loss: 3.961517e-06
Iter: 291 loss: 3.95652114e-06
Iter: 292 loss: 3.97323583e-06
Iter: 293 loss: 3.95519783e-06
Iter: 294 loss: 3.95059533e-06
Iter: 295 loss: 3.94143535e-06
Iter: 296 loss: 4.11991141e-06
Iter: 297 loss: 3.94134531e-06
Iter: 298 loss: 3.93606661e-06
Iter: 299 loss: 3.93574373e-06
Iter: 300 loss: 3.93057508e-06
Iter: 301 loss: 3.94053723e-06
Iter: 302 loss: 3.92837592e-06
Iter: 303 loss: 3.92475613e-06
Iter: 304 loss: 3.92068e-06
Iter: 305 loss: 3.92009451e-06
Iter: 306 loss: 3.91314461e-06
Iter: 307 loss: 3.94810831e-06
Iter: 308 loss: 3.91188814e-06
Iter: 309 loss: 3.90708828e-06
Iter: 310 loss: 3.96051519e-06
Iter: 311 loss: 3.90692912e-06
Iter: 312 loss: 3.90218156e-06
Iter: 313 loss: 3.9078227e-06
Iter: 314 loss: 3.89961951e-06
Iter: 315 loss: 3.89515071e-06
Iter: 316 loss: 3.89789056e-06
Iter: 317 loss: 3.89230445e-06
Iter: 318 loss: 3.88936041e-06
Iter: 319 loss: 3.88930312e-06
Iter: 320 loss: 3.88639683e-06
Iter: 321 loss: 3.88027138e-06
Iter: 322 loss: 3.98204793e-06
Iter: 323 loss: 3.88000808e-06
Iter: 324 loss: 3.87600812e-06
Iter: 325 loss: 3.87583304e-06
Iter: 326 loss: 3.87225464e-06
Iter: 327 loss: 3.87093314e-06
Iter: 328 loss: 3.86896045e-06
Iter: 329 loss: 3.86565353e-06
Iter: 330 loss: 3.86408919e-06
Iter: 331 loss: 3.86244665e-06
Iter: 332 loss: 3.85728617e-06
Iter: 333 loss: 3.9176939e-06
Iter: 334 loss: 3.8572507e-06
Iter: 335 loss: 3.85240082e-06
Iter: 336 loss: 3.85186695e-06
Iter: 337 loss: 3.84822806e-06
Iter: 338 loss: 3.84327632e-06
Iter: 339 loss: 3.84015948e-06
Iter: 340 loss: 3.83823772e-06
Iter: 341 loss: 3.83288716e-06
Iter: 342 loss: 3.83287761e-06
Iter: 343 loss: 3.82871576e-06
Iter: 344 loss: 3.85472322e-06
Iter: 345 loss: 3.82819417e-06
Iter: 346 loss: 3.82445842e-06
Iter: 347 loss: 3.82332382e-06
Iter: 348 loss: 3.82106055e-06
Iter: 349 loss: 3.81783957e-06
Iter: 350 loss: 3.83492261e-06
Iter: 351 loss: 3.81720929e-06
Iter: 352 loss: 3.81358768e-06
Iter: 353 loss: 3.82758071e-06
Iter: 354 loss: 3.81269274e-06
Iter: 355 loss: 3.8097e-06
Iter: 356 loss: 3.8064386e-06
Iter: 357 loss: 3.80589063e-06
Iter: 358 loss: 3.80175879e-06
Iter: 359 loss: 3.80177926e-06
Iter: 360 loss: 3.79935341e-06
Iter: 361 loss: 3.79551602e-06
Iter: 362 loss: 3.79551966e-06
Iter: 363 loss: 3.7900154e-06
Iter: 364 loss: 3.78831737e-06
Iter: 365 loss: 3.78498498e-06
Iter: 366 loss: 3.778571e-06
Iter: 367 loss: 3.77833749e-06
Iter: 368 loss: 3.77532842e-06
Iter: 369 loss: 3.7708603e-06
Iter: 370 loss: 3.77076276e-06
Iter: 371 loss: 3.76593766e-06
Iter: 372 loss: 3.78224831e-06
Iter: 373 loss: 3.76470803e-06
Iter: 374 loss: 3.76080061e-06
Iter: 375 loss: 3.80592155e-06
Iter: 376 loss: 3.76066646e-06
Iter: 377 loss: 3.7567786e-06
Iter: 378 loss: 3.76449407e-06
Iter: 379 loss: 3.75520131e-06
Iter: 380 loss: 3.75259629e-06
Iter: 381 loss: 3.75529225e-06
Iter: 382 loss: 3.75111517e-06
Iter: 383 loss: 3.74785827e-06
Iter: 384 loss: 3.7616519e-06
Iter: 385 loss: 3.74723277e-06
Iter: 386 loss: 3.74310662e-06
Iter: 387 loss: 3.74583396e-06
Iter: 388 loss: 3.74047613e-06
Iter: 389 loss: 3.73796502e-06
Iter: 390 loss: 3.75361424e-06
Iter: 391 loss: 3.7376426e-06
Iter: 392 loss: 3.73465946e-06
Iter: 393 loss: 3.73507714e-06
Iter: 394 loss: 3.73238436e-06
Iter: 395 loss: 3.72851605e-06
Iter: 396 loss: 3.72431168e-06
Iter: 397 loss: 3.72369936e-06
Iter: 398 loss: 3.72221712e-06
Iter: 399 loss: 3.72127147e-06
Iter: 400 loss: 3.71895749e-06
Iter: 401 loss: 3.71715146e-06
Iter: 402 loss: 3.71658e-06
Iter: 403 loss: 3.71263945e-06
Iter: 404 loss: 3.71170654e-06
Iter: 405 loss: 3.70932685e-06
Iter: 406 loss: 3.70529119e-06
Iter: 407 loss: 3.7239829e-06
Iter: 408 loss: 3.70454154e-06
Iter: 409 loss: 3.70071484e-06
Iter: 410 loss: 3.7534428e-06
Iter: 411 loss: 3.70075122e-06
Iter: 412 loss: 3.69840359e-06
Iter: 413 loss: 3.69595591e-06
Iter: 414 loss: 3.69553118e-06
Iter: 415 loss: 3.69182453e-06
Iter: 416 loss: 3.70581893e-06
Iter: 417 loss: 3.69093573e-06
Iter: 418 loss: 3.68754195e-06
Iter: 419 loss: 3.71969554e-06
Iter: 420 loss: 3.68739848e-06
Iter: 421 loss: 3.68491965e-06
Iter: 422 loss: 3.68076212e-06
Iter: 423 loss: 3.68074052e-06
Iter: 424 loss: 3.67692292e-06
Iter: 425 loss: 3.67693474e-06
Iter: 426 loss: 3.67420444e-06
Iter: 427 loss: 3.67395091e-06
Iter: 428 loss: 3.67209054e-06
Iter: 429 loss: 3.66930476e-06
Iter: 430 loss: 3.66896347e-06
Iter: 431 loss: 3.66693666e-06
Iter: 432 loss: 3.66505765e-06
Iter: 433 loss: 3.66474023e-06
Iter: 434 loss: 3.66293352e-06
Iter: 435 loss: 3.65974938e-06
Iter: 436 loss: 3.6597653e-06
Iter: 437 loss: 3.65631104e-06
Iter: 438 loss: 3.65622964e-06
Iter: 439 loss: 3.65344022e-06
Iter: 440 loss: 3.65136339e-06
Iter: 441 loss: 3.65089454e-06
Iter: 442 loss: 3.64848802e-06
Iter: 443 loss: 3.6518245e-06
Iter: 444 loss: 3.64730658e-06
Iter: 445 loss: 3.64469406e-06
Iter: 446 loss: 3.63973595e-06
Iter: 447 loss: 3.74784827e-06
Iter: 448 loss: 3.63964682e-06
Iter: 449 loss: 3.63712115e-06
Iter: 450 loss: 3.63632353e-06
Iter: 451 loss: 3.63431582e-06
Iter: 452 loss: 3.63791446e-06
Iter: 453 loss: 3.63345316e-06
Iter: 454 loss: 3.63157e-06
Iter: 455 loss: 3.63113395e-06
Iter: 456 loss: 3.6299225e-06
Iter: 457 loss: 3.62727678e-06
Iter: 458 loss: 3.65380515e-06
Iter: 459 loss: 3.62712353e-06
Iter: 460 loss: 3.62554556e-06
Iter: 461 loss: 3.62228366e-06
Iter: 462 loss: 3.68534347e-06
Iter: 463 loss: 3.62232049e-06
Iter: 464 loss: 3.61875573e-06
Iter: 465 loss: 3.64059588e-06
Iter: 466 loss: 3.6182314e-06
Iter: 467 loss: 3.61530056e-06
Iter: 468 loss: 3.64689822e-06
Iter: 469 loss: 3.61530601e-06
Iter: 470 loss: 3.61330967e-06
Iter: 471 loss: 3.60962235e-06
Iter: 472 loss: 3.68873702e-06
Iter: 473 loss: 3.60968556e-06
Iter: 474 loss: 3.60508238e-06
Iter: 475 loss: 3.61036473e-06
Iter: 476 loss: 3.60257718e-06
Iter: 477 loss: 3.60257422e-06
Iter: 478 loss: 3.60038462e-06
Iter: 479 loss: 3.59868386e-06
Iter: 480 loss: 3.59515298e-06
Iter: 481 loss: 3.65710957e-06
Iter: 482 loss: 3.59509522e-06
Iter: 483 loss: 3.5924686e-06
Iter: 484 loss: 3.61571142e-06
Iter: 485 loss: 3.59243131e-06
Iter: 486 loss: 3.58981697e-06
Iter: 487 loss: 3.59935575e-06
Iter: 488 loss: 3.58923444e-06
Iter: 489 loss: 3.58659508e-06
Iter: 490 loss: 3.58581315e-06
Iter: 491 loss: 3.58424586e-06
Iter: 492 loss: 3.58166471e-06
Iter: 493 loss: 3.61075945e-06
Iter: 494 loss: 3.58162083e-06
Iter: 495 loss: 3.57920726e-06
Iter: 496 loss: 3.57873841e-06
Iter: 497 loss: 3.57713043e-06
Iter: 498 loss: 3.57402951e-06
Iter: 499 loss: 3.57612976e-06
Iter: 500 loss: 3.5720991e-06
Iter: 501 loss: 3.56956753e-06
Iter: 502 loss: 3.59308615e-06
Iter: 503 loss: 3.56945134e-06
Iter: 504 loss: 3.56664123e-06
Iter: 505 loss: 3.57201793e-06
Iter: 506 loss: 3.5655e-06
Iter: 507 loss: 3.56356941e-06
Iter: 508 loss: 3.56233363e-06
Iter: 509 loss: 3.561624e-06
Iter: 510 loss: 3.55889824e-06
Iter: 511 loss: 3.57258841e-06
Iter: 512 loss: 3.55844304e-06
Iter: 513 loss: 3.55606e-06
Iter: 514 loss: 3.58732359e-06
Iter: 515 loss: 3.555994e-06
Iter: 516 loss: 3.55474549e-06
Iter: 517 loss: 3.5519829e-06
Iter: 518 loss: 3.59122168e-06
Iter: 519 loss: 3.55182237e-06
Iter: 520 loss: 3.54840313e-06
Iter: 521 loss: 3.57562226e-06
Iter: 522 loss: 3.54820645e-06
Iter: 523 loss: 3.54444069e-06
Iter: 524 loss: 3.55112707e-06
Iter: 525 loss: 3.5429282e-06
Iter: 526 loss: 3.54096278e-06
Iter: 527 loss: 3.55000179e-06
Iter: 528 loss: 3.54053373e-06
Iter: 529 loss: 3.53854853e-06
Iter: 530 loss: 3.54132544e-06
Iter: 531 loss: 3.53756559e-06
Iter: 532 loss: 3.53485666e-06
Iter: 533 loss: 3.53496898e-06
Iter: 534 loss: 3.53263931e-06
Iter: 535 loss: 3.53008272e-06
Iter: 536 loss: 3.53539758e-06
Iter: 537 loss: 3.52910502e-06
Iter: 538 loss: 3.52740312e-06
Iter: 539 loss: 3.52739e-06
Iter: 540 loss: 3.5260166e-06
Iter: 541 loss: 3.52384814e-06
Iter: 542 loss: 3.52385314e-06
Iter: 543 loss: 3.52100255e-06
Iter: 544 loss: 3.52286884e-06
Iter: 545 loss: 3.51913741e-06
Iter: 546 loss: 3.51738913e-06
Iter: 547 loss: 3.51724884e-06
Iter: 548 loss: 3.51489166e-06
Iter: 549 loss: 3.51063863e-06
Iter: 550 loss: 3.51065682e-06
Iter: 551 loss: 3.50735695e-06
Iter: 552 loss: 3.52376105e-06
Iter: 553 loss: 3.50669234e-06
Iter: 554 loss: 3.50490063e-06
Iter: 555 loss: 3.50480445e-06
Iter: 556 loss: 3.50352116e-06
Iter: 557 loss: 3.50105893e-06
Iter: 558 loss: 3.55360385e-06
Iter: 559 loss: 3.50102937e-06
Iter: 560 loss: 3.49838206e-06
Iter: 561 loss: 3.52452071e-06
Iter: 562 loss: 3.49826769e-06
Iter: 563 loss: 3.49622837e-06
Iter: 564 loss: 3.50270466e-06
Iter: 565 loss: 3.49566653e-06
Iter: 566 loss: 3.49421066e-06
Iter: 567 loss: 3.49293555e-06
Iter: 568 loss: 3.49251286e-06
Iter: 569 loss: 3.48971935e-06
Iter: 570 loss: 3.49512584e-06
Iter: 571 loss: 3.48856656e-06
Iter: 572 loss: 3.48630374e-06
Iter: 573 loss: 3.48630692e-06
Iter: 574 loss: 3.48484673e-06
Iter: 575 loss: 3.48091703e-06
Iter: 576 loss: 3.50162213e-06
Iter: 577 loss: 3.47964396e-06
Iter: 578 loss: 3.47449873e-06
Iter: 579 loss: 3.51183871e-06
Iter: 580 loss: 3.47407399e-06
Iter: 581 loss: 3.4736679e-06
Iter: 582 loss: 3.47255968e-06
Iter: 583 loss: 3.4713687e-06
Iter: 584 loss: 3.46974116e-06
Iter: 585 loss: 3.46970432e-06
Iter: 586 loss: 3.46795582e-06
Iter: 587 loss: 3.46765296e-06
Iter: 588 loss: 3.46638626e-06
Iter: 589 loss: 3.46356023e-06
Iter: 590 loss: 3.50415962e-06
Iter: 591 loss: 3.46355569e-06
Iter: 592 loss: 3.46250658e-06
Iter: 593 loss: 3.46129968e-06
Iter: 594 loss: 3.46108141e-06
Iter: 595 loss: 3.45934814e-06
Iter: 596 loss: 3.47735295e-06
Iter: 597 loss: 3.4593138e-06
Iter: 598 loss: 3.45780563e-06
Iter: 599 loss: 3.45503622e-06
Iter: 600 loss: 3.51232552e-06
Iter: 601 loss: 3.45505305e-06
Iter: 602 loss: 3.45140074e-06
Iter: 603 loss: 3.46363504e-06
Iter: 604 loss: 3.4504219e-06
Iter: 605 loss: 3.44766e-06
Iter: 606 loss: 3.46970069e-06
Iter: 607 loss: 3.44754244e-06
Iter: 608 loss: 3.44449927e-06
Iter: 609 loss: 3.44873115e-06
Iter: 610 loss: 3.44306454e-06
Iter: 611 loss: 3.44044975e-06
Iter: 612 loss: 3.44373802e-06
Iter: 613 loss: 3.43918714e-06
Iter: 614 loss: 3.4371435e-06
Iter: 615 loss: 3.44189516e-06
Iter: 616 loss: 3.43642523e-06
Iter: 617 loss: 3.43448528e-06
Iter: 618 loss: 3.43447391e-06
Iter: 619 loss: 3.43326587e-06
Iter: 620 loss: 3.43086322e-06
Iter: 621 loss: 3.47129708e-06
Iter: 622 loss: 3.43087299e-06
Iter: 623 loss: 3.42892645e-06
Iter: 624 loss: 3.45834246e-06
Iter: 625 loss: 3.42888052e-06
Iter: 626 loss: 3.42699468e-06
Iter: 627 loss: 3.43063448e-06
Iter: 628 loss: 3.42626481e-06
Iter: 629 loss: 3.42487624e-06
Iter: 630 loss: 3.42466365e-06
Iter: 631 loss: 3.42364e-06
Iter: 632 loss: 3.4216032e-06
Iter: 633 loss: 3.43944475e-06
Iter: 634 loss: 3.42147268e-06
Iter: 635 loss: 3.41963391e-06
Iter: 636 loss: 3.41570922e-06
Iter: 637 loss: 3.48160211e-06
Iter: 638 loss: 3.41566215e-06
Iter: 639 loss: 3.41296982e-06
Iter: 640 loss: 3.45201852e-06
Iter: 641 loss: 3.41302348e-06
Iter: 642 loss: 3.41137616e-06
Iter: 643 loss: 3.42143085e-06
Iter: 644 loss: 3.41116606e-06
Iter: 645 loss: 3.40974111e-06
Iter: 646 loss: 3.4153e-06
Iter: 647 loss: 3.40935821e-06
Iter: 648 loss: 3.40820202e-06
Iter: 649 loss: 3.40601605e-06
Iter: 650 loss: 3.45845433e-06
Iter: 651 loss: 3.40602378e-06
Iter: 652 loss: 3.40429665e-06
Iter: 653 loss: 3.40425458e-06
Iter: 654 loss: 3.40259135e-06
Iter: 655 loss: 3.41042551e-06
Iter: 656 loss: 3.40227325e-06
Iter: 657 loss: 3.40109e-06
Iter: 658 loss: 3.39894859e-06
Iter: 659 loss: 3.44305772e-06
Iter: 660 loss: 3.39903568e-06
Iter: 661 loss: 3.39685243e-06
Iter: 662 loss: 3.42031649e-06
Iter: 663 loss: 3.3967392e-06
Iter: 664 loss: 3.39404482e-06
Iter: 665 loss: 3.39444318e-06
Iter: 666 loss: 3.39201779e-06
Iter: 667 loss: 3.38968744e-06
Iter: 668 loss: 3.39370945e-06
Iter: 669 loss: 3.38878249e-06
Iter: 670 loss: 3.38626046e-06
Iter: 671 loss: 3.41300938e-06
Iter: 672 loss: 3.38630912e-06
Iter: 673 loss: 3.38506493e-06
Iter: 674 loss: 3.38321706e-06
Iter: 675 loss: 3.38318796e-06
Iter: 676 loss: 3.38084237e-06
Iter: 677 loss: 3.39167491e-06
Iter: 678 loss: 3.38045743e-06
Iter: 679 loss: 3.37918937e-06
Iter: 680 loss: 3.37920073e-06
Iter: 681 loss: 3.37801748e-06
Iter: 682 loss: 3.37656593e-06
Iter: 683 loss: 3.37638903e-06
Iter: 684 loss: 3.37388519e-06
Iter: 685 loss: 3.3769802e-06
Iter: 686 loss: 3.37262327e-06
Iter: 687 loss: 3.3703343e-06
Iter: 688 loss: 3.38270638e-06
Iter: 689 loss: 3.37009533e-06
Iter: 690 loss: 3.36728021e-06
Iter: 691 loss: 3.37582969e-06
Iter: 692 loss: 3.36645e-06
Iter: 693 loss: 3.36484914e-06
Iter: 694 loss: 3.36296625e-06
Iter: 695 loss: 3.3628462e-06
Iter: 696 loss: 3.36130984e-06
Iter: 697 loss: 3.36118728e-06
Iter: 698 loss: 3.35965478e-06
Iter: 699 loss: 3.35907544e-06
Iter: 700 loss: 3.35806703e-06
Iter: 701 loss: 3.35656409e-06
Iter: 702 loss: 3.36139624e-06
Iter: 703 loss: 3.35614027e-06
Iter: 704 loss: 3.35503637e-06
Iter: 705 loss: 3.36931294e-06
Iter: 706 loss: 3.35498839e-06
Iter: 707 loss: 3.35402865e-06
Iter: 708 loss: 3.35126151e-06
Iter: 709 loss: 3.36218181e-06
Iter: 710 loss: 3.35015784e-06
Iter: 711 loss: 3.34706738e-06
Iter: 712 loss: 3.37877964e-06
Iter: 713 loss: 3.34695e-06
Iter: 714 loss: 3.34497736e-06
Iter: 715 loss: 3.35919412e-06
Iter: 716 loss: 3.34482183e-06
Iter: 717 loss: 3.34307879e-06
Iter: 718 loss: 3.35323557e-06
Iter: 719 loss: 3.34281663e-06
Iter: 720 loss: 3.34108745e-06
Iter: 721 loss: 3.33856929e-06
Iter: 722 loss: 3.33848311e-06
Iter: 723 loss: 3.33624234e-06
Iter: 724 loss: 3.3604972e-06
Iter: 725 loss: 3.33617845e-06
Iter: 726 loss: 3.33483945e-06
Iter: 727 loss: 3.33594539e-06
Iter: 728 loss: 3.33410071e-06
Iter: 729 loss: 3.33251205e-06
Iter: 730 loss: 3.33252751e-06
Iter: 731 loss: 3.33186631e-06
Iter: 732 loss: 3.33029675e-06
Iter: 733 loss: 3.3506376e-06
Iter: 734 loss: 3.33017715e-06
Iter: 735 loss: 3.32827267e-06
Iter: 736 loss: 3.33472963e-06
Iter: 737 loss: 3.32769332e-06
Iter: 738 loss: 3.32555e-06
Iter: 739 loss: 3.34059769e-06
Iter: 740 loss: 3.32531408e-06
Iter: 741 loss: 3.32391528e-06
Iter: 742 loss: 3.32607965e-06
Iter: 743 loss: 3.32327045e-06
Iter: 744 loss: 3.32181753e-06
Iter: 745 loss: 3.32679974e-06
Iter: 746 loss: 3.3213214e-06
Iter: 747 loss: 3.31989554e-06
Iter: 748 loss: 3.31652427e-06
Iter: 749 loss: 3.35829031e-06
Iter: 750 loss: 3.31633396e-06
Iter: 751 loss: 3.31447154e-06
Iter: 752 loss: 3.3145061e-06
Iter: 753 loss: 3.3130907e-06
Iter: 754 loss: 3.31397314e-06
Iter: 755 loss: 3.3122376e-06
Iter: 756 loss: 3.31073079e-06
Iter: 757 loss: 3.31073625e-06
Iter: 758 loss: 3.30967123e-06
Iter: 759 loss: 3.30812713e-06
Iter: 760 loss: 3.30802959e-06
Iter: 761 loss: 3.30614148e-06
Iter: 762 loss: 3.31325782e-06
Iter: 763 loss: 3.30573334e-06
Iter: 764 loss: 3.3035808e-06
Iter: 765 loss: 3.30378862e-06
Iter: 766 loss: 3.30204966e-06
Iter: 767 loss: 3.30140165e-06
Iter: 768 loss: 3.30086664e-06
Iter: 769 loss: 3.29974182e-06
Iter: 770 loss: 3.298519e-06
Iter: 771 loss: 3.29834711e-06
Iter: 772 loss: 3.29681279e-06
Iter: 773 loss: 3.30555872e-06
Iter: 774 loss: 3.29667182e-06
Iter: 775 loss: 3.29510453e-06
Iter: 776 loss: 3.29733643e-06
Iter: 777 loss: 3.29436239e-06
Iter: 778 loss: 3.29316481e-06
Iter: 779 loss: 3.29658769e-06
Iter: 780 loss: 3.29281306e-06
Iter: 781 loss: 3.29168665e-06
Iter: 782 loss: 3.29738668e-06
Iter: 783 loss: 3.29156819e-06
Iter: 784 loss: 3.29064096e-06
Iter: 785 loss: 3.28858823e-06
Iter: 786 loss: 3.3208521e-06
Iter: 787 loss: 3.28857959e-06
Iter: 788 loss: 3.28645228e-06
Iter: 789 loss: 3.29282057e-06
Iter: 790 loss: 3.28584133e-06
Iter: 791 loss: 3.28346414e-06
Iter: 792 loss: 3.29211116e-06
Iter: 793 loss: 3.28287024e-06
Iter: 794 loss: 3.28142141e-06
Iter: 795 loss: 3.29848285e-06
Iter: 796 loss: 3.28143324e-06
Iter: 797 loss: 3.279873e-06
Iter: 798 loss: 3.28149872e-06
Iter: 799 loss: 3.27902785e-06
Iter: 800 loss: 3.27724979e-06
Iter: 801 loss: 3.2790183e-06
Iter: 802 loss: 3.27624775e-06
Iter: 803 loss: 3.27455291e-06
Iter: 804 loss: 3.27949056e-06
Iter: 805 loss: 3.27407042e-06
Iter: 806 loss: 3.27295311e-06
Iter: 807 loss: 3.27297448e-06
Iter: 808 loss: 3.2717935e-06
Iter: 809 loss: 3.26974441e-06
Iter: 810 loss: 3.26967574e-06
Iter: 811 loss: 3.2681653e-06
Iter: 812 loss: 3.27548742e-06
Iter: 813 loss: 3.26784289e-06
Iter: 814 loss: 3.26644067e-06
Iter: 815 loss: 3.27698945e-06
Iter: 816 loss: 3.26630743e-06
Iter: 817 loss: 3.26489771e-06
Iter: 818 loss: 3.26383042e-06
Iter: 819 loss: 3.26345344e-06
Iter: 820 loss: 3.26229701e-06
Iter: 821 loss: 3.26226836e-06
Iter: 822 loss: 3.26125428e-06
Iter: 823 loss: 3.25982728e-06
Iter: 824 loss: 3.25981136e-06
Iter: 825 loss: 3.25833344e-06
Iter: 826 loss: 3.25798464e-06
Iter: 827 loss: 3.25706242e-06
Iter: 828 loss: 3.2547232e-06
Iter: 829 loss: 3.26417012e-06
Iter: 830 loss: 3.2542448e-06
Iter: 831 loss: 3.25226824e-06
Iter: 832 loss: 3.25952897e-06
Iter: 833 loss: 3.25173e-06
Iter: 834 loss: 3.2501423e-06
Iter: 835 loss: 3.27231032e-06
Iter: 836 loss: 3.25012411e-06
Iter: 837 loss: 3.24889561e-06
Iter: 838 loss: 3.24931852e-06
Iter: 839 loss: 3.24805205e-06
Iter: 840 loss: 3.24644816e-06
Iter: 841 loss: 3.24636221e-06
Iter: 842 loss: 3.24517623e-06
Iter: 843 loss: 3.2433727e-06
Iter: 844 loss: 3.25152178e-06
Iter: 845 loss: 3.24306257e-06
Iter: 846 loss: 3.24222287e-06
Iter: 847 loss: 3.24201119e-06
Iter: 848 loss: 3.24138e-06
Iter: 849 loss: 3.2400369e-06
Iter: 850 loss: 3.26538066e-06
Iter: 851 loss: 3.24006305e-06
Iter: 852 loss: 3.23865925e-06
Iter: 853 loss: 3.24721123e-06
Iter: 854 loss: 3.23856125e-06
Iter: 855 loss: 3.23700328e-06
Iter: 856 loss: 3.23936752e-06
Iter: 857 loss: 3.23618292e-06
Iter: 858 loss: 3.23487984e-06
Iter: 859 loss: 3.23441236e-06
Iter: 860 loss: 3.23363884e-06
Iter: 861 loss: 3.23187828e-06
Iter: 862 loss: 3.23197264e-06
Iter: 863 loss: 3.23100949e-06
Iter: 864 loss: 3.22935966e-06
Iter: 865 loss: 3.26540203e-06
Iter: 866 loss: 3.22931e-06
Iter: 867 loss: 3.22701226e-06
Iter: 868 loss: 3.22695109e-06
Iter: 869 loss: 3.22517781e-06
Iter: 870 loss: 3.22392111e-06
Iter: 871 loss: 3.22380174e-06
Iter: 872 loss: 3.22240385e-06
Iter: 873 loss: 3.22604365e-06
Iter: 874 loss: 3.22184951e-06
Iter: 875 loss: 3.22091842e-06
Iter: 876 loss: 3.220998e-06
Iter: 877 loss: 3.22019059e-06
Iter: 878 loss: 3.21902462e-06
Iter: 879 loss: 3.22335222e-06
Iter: 880 loss: 3.21859761e-06
Iter: 881 loss: 3.21767061e-06
Iter: 882 loss: 3.22824326e-06
Iter: 883 loss: 3.21758102e-06
Iter: 884 loss: 3.21634639e-06
Iter: 885 loss: 3.21508014e-06
Iter: 886 loss: 3.21486414e-06
Iter: 887 loss: 3.21316929e-06
Iter: 888 loss: 3.2163457e-06
Iter: 889 loss: 3.21238076e-06
Iter: 890 loss: 3.21114703e-06
Iter: 891 loss: 3.22529036e-06
Iter: 892 loss: 3.21113703e-06
Iter: 893 loss: 3.20962067e-06
Iter: 894 loss: 3.208419e-06
Iter: 895 loss: 3.20795402e-06
Iter: 896 loss: 3.20627714e-06
Iter: 897 loss: 3.21687935e-06
Iter: 898 loss: 3.20608115e-06
Iter: 899 loss: 3.20419304e-06
Iter: 900 loss: 3.21020048e-06
Iter: 901 loss: 3.20366235e-06
Iter: 902 loss: 3.20269282e-06
Iter: 903 loss: 3.20115964e-06
Iter: 904 loss: 3.20118488e-06
Iter: 905 loss: 3.19912397e-06
Iter: 906 loss: 3.20842901e-06
Iter: 907 loss: 3.19867422e-06
Iter: 908 loss: 3.19751507e-06
Iter: 909 loss: 3.20840036e-06
Iter: 910 loss: 3.19739274e-06
Iter: 911 loss: 3.19582637e-06
Iter: 912 loss: 3.19541869e-06
Iter: 913 loss: 3.19438823e-06
Iter: 914 loss: 3.19242895e-06
Iter: 915 loss: 3.19330297e-06
Iter: 916 loss: 3.19118362e-06
Iter: 917 loss: 3.19009223e-06
Iter: 918 loss: 3.18991829e-06
Iter: 919 loss: 3.18867274e-06
Iter: 920 loss: 3.18814341e-06
Iter: 921 loss: 3.18746834e-06
Iter: 922 loss: 3.18557704e-06
Iter: 923 loss: 3.18731668e-06
Iter: 924 loss: 3.18451771e-06
Iter: 925 loss: 3.18316052e-06
Iter: 926 loss: 3.19458377e-06
Iter: 927 loss: 3.18311049e-06
Iter: 928 loss: 3.18181037e-06
Iter: 929 loss: 3.18707248e-06
Iter: 930 loss: 3.18148227e-06
Iter: 931 loss: 3.1806951e-06
Iter: 932 loss: 3.1816985e-06
Iter: 933 loss: 3.18018328e-06
Iter: 934 loss: 3.17949957e-06
Iter: 935 loss: 3.19032142e-06
Iter: 936 loss: 3.17947342e-06
Iter: 937 loss: 3.17896092e-06
Iter: 938 loss: 3.17731519e-06
Iter: 939 loss: 3.18684397e-06
Iter: 940 loss: 3.17694366e-06
Iter: 941 loss: 3.17494664e-06
Iter: 942 loss: 3.18028515e-06
Iter: 943 loss: 3.17429863e-06
Iter: 944 loss: 3.17280319e-06
Iter: 945 loss: 3.19630226e-06
Iter: 946 loss: 3.17280455e-06
Iter: 947 loss: 3.17111744e-06
Iter: 948 loss: 3.17036961e-06
Iter: 949 loss: 3.16946353e-06
Iter: 950 loss: 3.16765909e-06
Iter: 951 loss: 3.17106446e-06
Iter: 952 loss: 3.16683e-06
Iter: 953 loss: 3.16602222e-06
Iter: 954 loss: 3.16591058e-06
Iter: 955 loss: 3.16494948e-06
Iter: 956 loss: 3.1638931e-06
Iter: 957 loss: 3.16377918e-06
Iter: 958 loss: 3.1624586e-06
Iter: 959 loss: 3.16203818e-06
Iter: 960 loss: 3.16127966e-06
Iter: 961 loss: 3.16102251e-06
Iter: 962 loss: 3.16040428e-06
Iter: 963 loss: 3.15980242e-06
Iter: 964 loss: 3.15860598e-06
Iter: 965 loss: 3.18434286e-06
Iter: 966 loss: 3.15862098e-06
Iter: 967 loss: 3.15697457e-06
Iter: 968 loss: 3.16565183e-06
Iter: 969 loss: 3.15679745e-06
Iter: 970 loss: 3.15525904e-06
Iter: 971 loss: 3.16210026e-06
Iter: 972 loss: 3.15497414e-06
Iter: 973 loss: 3.15393936e-06
Iter: 974 loss: 3.15245416e-06
Iter: 975 loss: 3.15242664e-06
Iter: 976 loss: 3.1504685e-06
Iter: 977 loss: 3.15726334e-06
Iter: 978 loss: 3.1499344e-06
Iter: 979 loss: 3.14881026e-06
Iter: 980 loss: 3.16045225e-06
Iter: 981 loss: 3.14878207e-06
Iter: 982 loss: 3.14763656e-06
Iter: 983 loss: 3.1521e-06
Iter: 984 loss: 3.1473935e-06
Iter: 985 loss: 3.14645854e-06
Iter: 986 loss: 3.14491854e-06
Iter: 987 loss: 3.18214461e-06
Iter: 988 loss: 3.14487397e-06
Iter: 989 loss: 3.14442696e-06
Iter: 990 loss: 3.14409294e-06
Iter: 991 loss: 3.14319823e-06
Iter: 992 loss: 3.14195518e-06
Iter: 993 loss: 3.14188787e-06
Iter: 994 loss: 3.14042518e-06
Iter: 995 loss: 3.14030467e-06
Iter: 996 loss: 3.13927239e-06
Iter: 997 loss: 3.13741748e-06
Iter: 998 loss: 3.15037551e-06
Iter: 999 loss: 3.13722126e-06
Iter: 1000 loss: 3.13525152e-06
Iter: 1001 loss: 3.14578847e-06
Iter: 1002 loss: 3.13499777e-06
Iter: 1003 loss: 3.13409191e-06
Iter: 1004 loss: 3.13414921e-06
Iter: 1005 loss: 3.13336113e-06
Iter: 1006 loss: 3.13197233e-06
Iter: 1007 loss: 3.1423142e-06
Iter: 1008 loss: 3.13184319e-06
Iter: 1009 loss: 3.13097394e-06
Iter: 1010 loss: 3.13012083e-06
Iter: 1011 loss: 3.12989846e-06
Iter: 1012 loss: 3.12847123e-06
Iter: 1013 loss: 3.13063083e-06
Iter: 1014 loss: 3.12783482e-06
Iter: 1015 loss: 3.12642578e-06
Iter: 1016 loss: 3.13678402e-06
Iter: 1017 loss: 3.12628117e-06
Iter: 1018 loss: 3.12525663e-06
Iter: 1019 loss: 3.13196506e-06
Iter: 1020 loss: 3.12515158e-06
Iter: 1021 loss: 3.12402199e-06
Iter: 1022 loss: 3.12286829e-06
Iter: 1023 loss: 3.12263501e-06
Iter: 1024 loss: 3.12146722e-06
Iter: 1025 loss: 3.13430837e-06
Iter: 1026 loss: 3.12141924e-06
Iter: 1027 loss: 3.11995564e-06
Iter: 1028 loss: 3.11968142e-06
Iter: 1029 loss: 3.11872054e-06
Iter: 1030 loss: 3.11729491e-06
Iter: 1031 loss: 3.11738677e-06
Iter: 1032 loss: 3.11624535e-06
Iter: 1033 loss: 3.11517761e-06
Iter: 1034 loss: 3.11512485e-06
Iter: 1035 loss: 3.11410372e-06
Iter: 1036 loss: 3.11500548e-06
Iter: 1037 loss: 3.11341273e-06
Iter: 1038 loss: 3.11238273e-06
Iter: 1039 loss: 3.11350232e-06
Iter: 1040 loss: 3.11180111e-06
Iter: 1041 loss: 3.11073018e-06
Iter: 1042 loss: 3.12333532e-06
Iter: 1043 loss: 3.11067583e-06
Iter: 1044 loss: 3.11008716e-06
Iter: 1045 loss: 3.10825681e-06
Iter: 1046 loss: 3.11355689e-06
Iter: 1047 loss: 3.10720702e-06
Iter: 1048 loss: 3.10533414e-06
Iter: 1049 loss: 3.10530481e-06
Iter: 1050 loss: 3.10380779e-06
Iter: 1051 loss: 3.11786152e-06
Iter: 1052 loss: 3.10384212e-06
Iter: 1053 loss: 3.10224414e-06
Iter: 1054 loss: 3.10179803e-06
Iter: 1055 loss: 3.10097948e-06
Iter: 1056 loss: 3.09909933e-06
Iter: 1057 loss: 3.10360701e-06
Iter: 1058 loss: 3.09847519e-06
Iter: 1059 loss: 3.09741677e-06
Iter: 1060 loss: 3.09737743e-06
Iter: 1061 loss: 3.09655638e-06
Iter: 1062 loss: 3.09564666e-06
Iter: 1063 loss: 3.09551388e-06
Iter: 1064 loss: 3.09412e-06
Iter: 1065 loss: 3.09353732e-06
Iter: 1066 loss: 3.09288157e-06
Iter: 1067 loss: 3.09142115e-06
Iter: 1068 loss: 3.11129952e-06
Iter: 1069 loss: 3.09143661e-06
Iter: 1070 loss: 3.0899057e-06
Iter: 1071 loss: 3.09113329e-06
Iter: 1072 loss: 3.08897461e-06
Iter: 1073 loss: 3.08771723e-06
Iter: 1074 loss: 3.08956828e-06
Iter: 1075 loss: 3.08717154e-06
Iter: 1076 loss: 3.08575954e-06
Iter: 1077 loss: 3.09720531e-06
Iter: 1078 loss: 3.08559811e-06
Iter: 1079 loss: 3.08466429e-06
Iter: 1080 loss: 3.08257881e-06
Iter: 1081 loss: 3.10921087e-06
Iter: 1082 loss: 3.08245581e-06
Iter: 1083 loss: 3.08019276e-06
Iter: 1084 loss: 3.09353732e-06
Iter: 1085 loss: 3.07987511e-06
Iter: 1086 loss: 3.07879509e-06
Iter: 1087 loss: 3.07880964e-06
Iter: 1088 loss: 3.07766231e-06
Iter: 1089 loss: 3.07660571e-06
Iter: 1090 loss: 3.07630603e-06
Iter: 1091 loss: 3.07502137e-06
Iter: 1092 loss: 3.08084304e-06
Iter: 1093 loss: 3.07483106e-06
Iter: 1094 loss: 3.073583e-06
Iter: 1095 loss: 3.08463973e-06
Iter: 1096 loss: 3.07353434e-06
Iter: 1097 loss: 3.07263599e-06
Iter: 1098 loss: 3.07043547e-06
Iter: 1099 loss: 3.08586414e-06
Iter: 1100 loss: 3.06986703e-06
Iter: 1101 loss: 3.06752668e-06
Iter: 1102 loss: 3.1011341e-06
Iter: 1103 loss: 3.06754669e-06
Iter: 1104 loss: 3.06646939e-06
Iter: 1105 loss: 3.06646643e-06
Iter: 1106 loss: 3.06569882e-06
Iter: 1107 loss: 3.06447919e-06
Iter: 1108 loss: 3.06452102e-06
Iter: 1109 loss: 3.06274433e-06
Iter: 1110 loss: 3.06803418e-06
Iter: 1111 loss: 3.06221455e-06
Iter: 1112 loss: 3.0607855e-06
Iter: 1113 loss: 3.07856271e-06
Iter: 1114 loss: 3.0607323e-06
Iter: 1115 loss: 3.06002403e-06
Iter: 1116 loss: 3.05854314e-06
Iter: 1117 loss: 3.08326639e-06
Iter: 1118 loss: 3.05849426e-06
Iter: 1119 loss: 3.0567644e-06
Iter: 1120 loss: 3.06350421e-06
Iter: 1121 loss: 3.05634239e-06
Iter: 1122 loss: 3.05518506e-06
Iter: 1123 loss: 3.05511685e-06
Iter: 1124 loss: 3.0538863e-06
Iter: 1125 loss: 3.05323397e-06
Iter: 1126 loss: 3.05276671e-06
Iter: 1127 loss: 3.05102e-06
Iter: 1128 loss: 3.05385379e-06
Iter: 1129 loss: 3.05025651e-06
Iter: 1130 loss: 3.04790274e-06
Iter: 1131 loss: 3.06876154e-06
Iter: 1132 loss: 3.04791297e-06
Iter: 1133 loss: 3.04704508e-06
Iter: 1134 loss: 3.04488458e-06
Iter: 1135 loss: 3.06688298e-06
Iter: 1136 loss: 3.04467949e-06
Iter: 1137 loss: 3.04265814e-06
Iter: 1138 loss: 3.06257562e-06
Iter: 1139 loss: 3.04257219e-06
Iter: 1140 loss: 3.04068544e-06
Iter: 1141 loss: 3.0582446e-06
Iter: 1142 loss: 3.04062178e-06
Iter: 1143 loss: 3.03981824e-06
Iter: 1144 loss: 3.03871138e-06
Iter: 1145 loss: 3.03862635e-06
Iter: 1146 loss: 3.03707702e-06
Iter: 1147 loss: 3.05289723e-06
Iter: 1148 loss: 3.0370752e-06
Iter: 1149 loss: 3.03566594e-06
Iter: 1150 loss: 3.03560341e-06
Iter: 1151 loss: 3.03462184e-06
Iter: 1152 loss: 3.0331687e-06
Iter: 1153 loss: 3.03322349e-06
Iter: 1154 loss: 3.03202887e-06
Iter: 1155 loss: 3.02967419e-06
Iter: 1156 loss: 3.03759498e-06
Iter: 1157 loss: 3.0289948e-06
Iter: 1158 loss: 3.02776152e-06
Iter: 1159 loss: 3.02773651e-06
Iter: 1160 loss: 3.02679473e-06
Iter: 1161 loss: 3.02550552e-06
Iter: 1162 loss: 3.02538092e-06
Iter: 1163 loss: 3.02420403e-06
Iter: 1164 loss: 3.03711658e-06
Iter: 1165 loss: 3.02419426e-06
Iter: 1166 loss: 3.02284707e-06
Iter: 1167 loss: 3.02325452e-06
Iter: 1168 loss: 3.02179797e-06
Iter: 1169 loss: 3.0207234e-06
Iter: 1170 loss: 3.02024182e-06
Iter: 1171 loss: 3.01968794e-06
Iter: 1172 loss: 3.01857722e-06
Iter: 1173 loss: 3.01856653e-06
Iter: 1174 loss: 3.01732393e-06
Iter: 1175 loss: 3.01750288e-06
Iter: 1176 loss: 3.01639807e-06
Iter: 1177 loss: 3.01513819e-06
Iter: 1178 loss: 3.01488353e-06
Iter: 1179 loss: 3.01408068e-06
Iter: 1180 loss: 3.0123017e-06
Iter: 1181 loss: 3.0348865e-06
Iter: 1182 loss: 3.01236764e-06
Iter: 1183 loss: 3.01116734e-06
Iter: 1184 loss: 3.00879083e-06
Iter: 1185 loss: 3.04782043e-06
Iter: 1186 loss: 3.00874126e-06
Iter: 1187 loss: 3.00649572e-06
Iter: 1188 loss: 3.02187163e-06
Iter: 1189 loss: 3.00632655e-06
Iter: 1190 loss: 3.0051242e-06
Iter: 1191 loss: 3.00510692e-06
Iter: 1192 loss: 3.00399734e-06
Iter: 1193 loss: 3.00479905e-06
Iter: 1194 loss: 3.00338797e-06
Iter: 1195 loss: 3.00229021e-06
Iter: 1196 loss: 3.00439297e-06
Iter: 1197 loss: 3.00183228e-06
Iter: 1198 loss: 3.00066858e-06
Iter: 1199 loss: 3.01025875e-06
Iter: 1200 loss: 3.0005433e-06
Iter: 1201 loss: 2.99955082e-06
Iter: 1202 loss: 2.99846693e-06
Iter: 1203 loss: 2.99835574e-06
Iter: 1204 loss: 2.9968096e-06
Iter: 1205 loss: 2.99692647e-06
Iter: 1206 loss: 2.99568569e-06
Iter: 1207 loss: 2.99484259e-06
Iter: 1208 loss: 2.99453313e-06
Iter: 1209 loss: 2.99347562e-06
Iter: 1210 loss: 2.99117164e-06
Iter: 1211 loss: 3.02894387e-06
Iter: 1212 loss: 2.99118051e-06
Iter: 1213 loss: 2.98956297e-06
Iter: 1214 loss: 3.00651027e-06
Iter: 1215 loss: 2.98952205e-06
Iter: 1216 loss: 2.98817758e-06
Iter: 1217 loss: 2.99359317e-06
Iter: 1218 loss: 2.98779196e-06
Iter: 1219 loss: 2.9865746e-06
Iter: 1220 loss: 2.98566283e-06
Iter: 1221 loss: 2.98527902e-06
Iter: 1222 loss: 2.98394116e-06
Iter: 1223 loss: 2.9860139e-06
Iter: 1224 loss: 2.98334e-06
Iter: 1225 loss: 2.98233567e-06
Iter: 1226 loss: 2.98221858e-06
Iter: 1227 loss: 2.98138684e-06
Iter: 1228 loss: 2.98024361e-06
Iter: 1229 loss: 2.98016607e-06
Iter: 1230 loss: 2.97918541e-06
Iter: 1231 loss: 2.97920292e-06
Iter: 1232 loss: 2.97826796e-06
Iter: 1233 loss: 2.97705924e-06
Iter: 1234 loss: 2.97697943e-06
Iter: 1235 loss: 2.97540646e-06
Iter: 1236 loss: 2.97716178e-06
Iter: 1237 loss: 2.97462088e-06
Iter: 1238 loss: 2.97309316e-06
Iter: 1239 loss: 2.98312921e-06
Iter: 1240 loss: 2.9728817e-06
Iter: 1241 loss: 2.97137626e-06
Iter: 1242 loss: 2.97960742e-06
Iter: 1243 loss: 2.97107454e-06
Iter: 1244 loss: 2.96988514e-06
Iter: 1245 loss: 2.96955932e-06
Iter: 1246 loss: 2.9688058e-06
Iter: 1247 loss: 2.96763687e-06
Iter: 1248 loss: 2.97315319e-06
Iter: 1249 loss: 2.96749022e-06
Iter: 1250 loss: 2.96603548e-06
Iter: 1251 loss: 2.96934786e-06
Iter: 1252 loss: 2.96555754e-06
Iter: 1253 loss: 2.9644234e-06
Iter: 1254 loss: 2.96256303e-06
Iter: 1255 loss: 2.96258531e-06
Iter: 1256 loss: 2.96091412e-06
Iter: 1257 loss: 2.97880479e-06
Iter: 1258 loss: 2.96081544e-06
Iter: 1259 loss: 2.95935047e-06
Iter: 1260 loss: 2.97062684e-06
Iter: 1261 loss: 2.95931341e-06
Iter: 1262 loss: 2.95823975e-06
Iter: 1263 loss: 2.95752488e-06
Iter: 1264 loss: 2.95708196e-06
Iter: 1265 loss: 2.95616746e-06
Iter: 1266 loss: 2.95615e-06
Iter: 1267 loss: 2.95538121e-06
Iter: 1268 loss: 2.95354721e-06
Iter: 1269 loss: 2.96657709e-06
Iter: 1270 loss: 2.9531443e-06
Iter: 1271 loss: 2.9511209e-06
Iter: 1272 loss: 2.96546864e-06
Iter: 1273 loss: 2.95100426e-06
Iter: 1274 loss: 2.94999427e-06
Iter: 1275 loss: 2.94997244e-06
Iter: 1276 loss: 2.94929214e-06
Iter: 1277 loss: 2.94825531e-06
Iter: 1278 loss: 2.94815368e-06
Iter: 1279 loss: 2.94655365e-06
Iter: 1280 loss: 2.94695133e-06
Iter: 1281 loss: 2.94542951e-06
Iter: 1282 loss: 2.94415349e-06
Iter: 1283 loss: 2.94410484e-06
Iter: 1284 loss: 2.94315578e-06
Iter: 1285 loss: 2.94240704e-06
Iter: 1286 loss: 2.94213783e-06
Iter: 1287 loss: 2.94066035e-06
Iter: 1288 loss: 2.94043502e-06
Iter: 1289 loss: 2.93931953e-06
Iter: 1290 loss: 2.93835524e-06
Iter: 1291 loss: 2.93832136e-06
Iter: 1292 loss: 2.9372145e-06
Iter: 1293 loss: 2.93764629e-06
Iter: 1294 loss: 2.93642074e-06
Iter: 1295 loss: 2.93556036e-06
Iter: 1296 loss: 2.94076426e-06
Iter: 1297 loss: 2.93546418e-06
Iter: 1298 loss: 2.93446374e-06
Iter: 1299 loss: 2.93460653e-06
Iter: 1300 loss: 2.93377593e-06
Iter: 1301 loss: 2.93277753e-06
Iter: 1302 loss: 2.93176049e-06
Iter: 1303 loss: 2.93154676e-06
Iter: 1304 loss: 2.93008043e-06
Iter: 1305 loss: 2.94664233e-06
Iter: 1306 loss: 2.93000858e-06
Iter: 1307 loss: 2.92843652e-06
Iter: 1308 loss: 2.93154358e-06
Iter: 1309 loss: 2.92773211e-06
Iter: 1310 loss: 2.926668e-06
Iter: 1311 loss: 2.92682398e-06
Iter: 1312 loss: 2.92578557e-06
Iter: 1313 loss: 2.92437062e-06
Iter: 1314 loss: 2.93606695e-06
Iter: 1315 loss: 2.92433242e-06
Iter: 1316 loss: 2.92307323e-06
Iter: 1317 loss: 2.92467212e-06
Iter: 1318 loss: 2.92236905e-06
Iter: 1319 loss: 2.92117966e-06
Iter: 1320 loss: 2.92269897e-06
Iter: 1321 loss: 2.92045752e-06
Iter: 1322 loss: 2.91946299e-06
Iter: 1323 loss: 2.91944571e-06
Iter: 1324 loss: 2.91858578e-06
Iter: 1325 loss: 2.91699098e-06
Iter: 1326 loss: 2.94033566e-06
Iter: 1327 loss: 2.91698302e-06
Iter: 1328 loss: 2.91627521e-06
Iter: 1329 loss: 2.91568199e-06
Iter: 1330 loss: 2.91545734e-06
Iter: 1331 loss: 2.91396441e-06
Iter: 1332 loss: 2.92295226e-06
Iter: 1333 loss: 2.91382116e-06
Iter: 1334 loss: 2.91284618e-06
Iter: 1335 loss: 2.91075094e-06
Iter: 1336 loss: 2.94617303e-06
Iter: 1337 loss: 2.91073343e-06
Iter: 1338 loss: 2.90857088e-06
Iter: 1339 loss: 2.92482036e-06
Iter: 1340 loss: 2.90839398e-06
Iter: 1341 loss: 2.90701746e-06
Iter: 1342 loss: 2.92914137e-06
Iter: 1343 loss: 2.90699404e-06
Iter: 1344 loss: 2.90621233e-06
Iter: 1345 loss: 2.90469575e-06
Iter: 1346 loss: 2.93509538e-06
Iter: 1347 loss: 2.90472121e-06
Iter: 1348 loss: 2.90352455e-06
Iter: 1349 loss: 2.90354865e-06
Iter: 1350 loss: 2.90262187e-06
Iter: 1351 loss: 2.90554203e-06
Iter: 1352 loss: 2.90243906e-06
Iter: 1353 loss: 2.90161211e-06
Iter: 1354 loss: 2.90102798e-06
Iter: 1355 loss: 2.90071398e-06
Iter: 1356 loss: 2.89934792e-06
Iter: 1357 loss: 2.90039088e-06
Iter: 1358 loss: 2.89849686e-06
Iter: 1359 loss: 2.89767831e-06
Iter: 1360 loss: 2.89751233e-06
Iter: 1361 loss: 2.89666787e-06
Iter: 1362 loss: 2.89537707e-06
Iter: 1363 loss: 2.89528271e-06
Iter: 1364 loss: 2.8940924e-06
Iter: 1365 loss: 2.91339848e-06
Iter: 1366 loss: 2.89405489e-06
Iter: 1367 loss: 2.89296395e-06
Iter: 1368 loss: 2.89222908e-06
Iter: 1369 loss: 2.89179525e-06
Iter: 1370 loss: 2.89041554e-06
Iter: 1371 loss: 2.88934871e-06
Iter: 1372 loss: 2.88892033e-06
Iter: 1373 loss: 2.88878891e-06
Iter: 1374 loss: 2.88786669e-06
Iter: 1375 loss: 2.88727711e-06
Iter: 1376 loss: 2.88639785e-06
Iter: 1377 loss: 2.88634283e-06
Iter: 1378 loss: 2.88533329e-06
Iter: 1379 loss: 2.88608021e-06
Iter: 1380 loss: 2.88466072e-06
Iter: 1381 loss: 2.88349588e-06
Iter: 1382 loss: 2.89927198e-06
Iter: 1383 loss: 2.88348247e-06
Iter: 1384 loss: 2.88254796e-06
Iter: 1385 loss: 2.88130877e-06
Iter: 1386 loss: 2.88129422e-06
Iter: 1387 loss: 2.87961984e-06
Iter: 1388 loss: 2.88411456e-06
Iter: 1389 loss: 2.87901821e-06
Iter: 1390 loss: 2.87747912e-06
Iter: 1391 loss: 2.88222282e-06
Iter: 1392 loss: 2.87692956e-06
Iter: 1393 loss: 2.8756815e-06
Iter: 1394 loss: 2.87563535e-06
Iter: 1395 loss: 2.8749073e-06
Iter: 1396 loss: 2.87367766e-06
Iter: 1397 loss: 2.87369085e-06
Iter: 1398 loss: 2.87220382e-06
Iter: 1399 loss: 2.89296804e-06
Iter: 1400 loss: 2.87223793e-06
Iter: 1401 loss: 2.87141052e-06
Iter: 1402 loss: 2.87017542e-06
Iter: 1403 loss: 2.87014609e-06
Iter: 1404 loss: 2.86891645e-06
Iter: 1405 loss: 2.87561852e-06
Iter: 1406 loss: 2.86872228e-06
Iter: 1407 loss: 2.86770364e-06
Iter: 1408 loss: 2.88103911e-06
Iter: 1409 loss: 2.86773275e-06
Iter: 1410 loss: 2.867067e-06
Iter: 1411 loss: 2.8656782e-06
Iter: 1412 loss: 2.88342244e-06
Iter: 1413 loss: 2.86556747e-06
Iter: 1414 loss: 2.86423756e-06
Iter: 1415 loss: 2.8759107e-06
Iter: 1416 loss: 2.86405066e-06
Iter: 1417 loss: 2.86230897e-06
Iter: 1418 loss: 2.86270188e-06
Iter: 1419 loss: 2.86099498e-06
Iter: 1420 loss: 2.8597e-06
Iter: 1421 loss: 2.86385784e-06
Iter: 1422 loss: 2.85928809e-06
Iter: 1423 loss: 2.85783176e-06
Iter: 1424 loss: 2.85925898e-06
Iter: 1425 loss: 2.85702868e-06
Iter: 1426 loss: 2.85608508e-06
Iter: 1427 loss: 2.85603073e-06
Iter: 1428 loss: 2.85526198e-06
Iter: 1429 loss: 2.85477859e-06
Iter: 1430 loss: 2.85440092e-06
Iter: 1431 loss: 2.85353735e-06
Iter: 1432 loss: 2.8621273e-06
Iter: 1433 loss: 2.85353462e-06
Iter: 1434 loss: 2.8525883e-06
Iter: 1435 loss: 2.85133547e-06
Iter: 1436 loss: 2.85134684e-06
Iter: 1437 loss: 2.84991665e-06
Iter: 1438 loss: 2.85515716e-06
Iter: 1439 loss: 2.84950102e-06
Iter: 1440 loss: 2.84874113e-06
Iter: 1441 loss: 2.8486902e-06
Iter: 1442 loss: 2.84801649e-06
Iter: 1443 loss: 2.84597718e-06
Iter: 1444 loss: 2.86402633e-06
Iter: 1445 loss: 2.84578027e-06
Iter: 1446 loss: 2.84381713e-06
Iter: 1447 loss: 2.8576917e-06
Iter: 1448 loss: 2.84368571e-06
Iter: 1449 loss: 2.84202542e-06
Iter: 1450 loss: 2.85793908e-06
Iter: 1451 loss: 2.84205953e-06
Iter: 1452 loss: 2.84109819e-06
Iter: 1453 loss: 2.8403565e-06
Iter: 1454 loss: 2.84002385e-06
Iter: 1455 loss: 2.83869372e-06
Iter: 1456 loss: 2.84292219e-06
Iter: 1457 loss: 2.83831582e-06
Iter: 1458 loss: 2.83746795e-06
Iter: 1459 loss: 2.85155807e-06
Iter: 1460 loss: 2.83746158e-06
Iter: 1461 loss: 2.8366062e-06
Iter: 1462 loss: 2.8368413e-06
Iter: 1463 loss: 2.83601435e-06
Iter: 1464 loss: 2.83487975e-06
Iter: 1465 loss: 2.83658187e-06
Iter: 1466 loss: 2.83421014e-06
Iter: 1467 loss: 2.83304189e-06
Iter: 1468 loss: 2.84466455e-06
Iter: 1469 loss: 2.83294457e-06
Iter: 1470 loss: 2.83225199e-06
Iter: 1471 loss: 2.83048689e-06
Iter: 1472 loss: 2.84916041e-06
Iter: 1473 loss: 2.83037116e-06
Iter: 1474 loss: 2.82905376e-06
Iter: 1475 loss: 2.82892142e-06
Iter: 1476 loss: 2.82761766e-06
Iter: 1477 loss: 2.82816973e-06
Iter: 1478 loss: 2.82655219e-06
Iter: 1479 loss: 2.8256884e-06
Iter: 1480 loss: 2.82557585e-06
Iter: 1481 loss: 2.82491192e-06
Iter: 1482 loss: 2.8237946e-06
Iter: 1483 loss: 2.84102794e-06
Iter: 1484 loss: 2.82380597e-06
Iter: 1485 loss: 2.8228535e-06
Iter: 1486 loss: 2.8231666e-06
Iter: 1487 loss: 2.82217707e-06
Iter: 1488 loss: 2.82133169e-06
Iter: 1489 loss: 2.82055839e-06
Iter: 1490 loss: 2.82032852e-06
Iter: 1491 loss: 2.81892267e-06
Iter: 1492 loss: 2.83517966e-06
Iter: 1493 loss: 2.81889515e-06
Iter: 1494 loss: 2.81758139e-06
Iter: 1495 loss: 2.82018959e-06
Iter: 1496 loss: 2.81706593e-06
Iter: 1497 loss: 2.81571511e-06
Iter: 1498 loss: 2.8192178e-06
Iter: 1499 loss: 2.81528014e-06
Iter: 1500 loss: 2.81407574e-06
Iter: 1501 loss: 2.82122437e-06
Iter: 1502 loss: 2.81392909e-06
Iter: 1503 loss: 2.81282928e-06
Iter: 1504 loss: 2.81079e-06
Iter: 1505 loss: 2.8108102e-06
Iter: 1506 loss: 2.80942368e-06
Iter: 1507 loss: 2.82800556e-06
Iter: 1508 loss: 2.80938775e-06
Iter: 1509 loss: 2.80831705e-06
Iter: 1510 loss: 2.81624193e-06
Iter: 1511 loss: 2.80819427e-06
Iter: 1512 loss: 2.80753102e-06
Iter: 1513 loss: 2.8061645e-06
Iter: 1514 loss: 2.83340432e-06
Iter: 1515 loss: 2.80608879e-06
Iter: 1516 loss: 2.80501445e-06
Iter: 1517 loss: 2.81899975e-06
Iter: 1518 loss: 2.80495078e-06
Iter: 1519 loss: 2.80381914e-06
Iter: 1520 loss: 2.80665631e-06
Iter: 1521 loss: 2.80343602e-06
Iter: 1522 loss: 2.80255472e-06
Iter: 1523 loss: 2.80092536e-06
Iter: 1524 loss: 2.80093082e-06
Iter: 1525 loss: 2.79925371e-06
Iter: 1526 loss: 2.81699477e-06
Iter: 1527 loss: 2.79925575e-06
Iter: 1528 loss: 2.79796132e-06
Iter: 1529 loss: 2.80692439e-06
Iter: 1530 loss: 2.79786332e-06
Iter: 1531 loss: 2.79669234e-06
Iter: 1532 loss: 2.79630467e-06
Iter: 1533 loss: 2.7956653e-06
Iter: 1534 loss: 2.79434744e-06
Iter: 1535 loss: 2.81030589e-06
Iter: 1536 loss: 2.79433516e-06
Iter: 1537 loss: 2.79343953e-06
Iter: 1538 loss: 2.79379469e-06
Iter: 1539 loss: 2.79293977e-06
Iter: 1540 loss: 2.79182814e-06
Iter: 1541 loss: 2.79152823e-06
Iter: 1542 loss: 2.79075948e-06
Iter: 1543 loss: 2.79010965e-06
Iter: 1544 loss: 2.78995822e-06
Iter: 1545 loss: 2.78929087e-06
Iter: 1546 loss: 2.78792413e-06
Iter: 1547 loss: 2.81700068e-06
Iter: 1548 loss: 2.78798143e-06
Iter: 1549 loss: 2.78667108e-06
Iter: 1550 loss: 2.78756806e-06
Iter: 1551 loss: 2.78589096e-06
Iter: 1552 loss: 2.78442303e-06
Iter: 1553 loss: 2.78444736e-06
Iter: 1554 loss: 2.78356447e-06
Iter: 1555 loss: 2.78283892e-06
Iter: 1556 loss: 2.78250809e-06
Iter: 1557 loss: 2.78126367e-06
Iter: 1558 loss: 2.78093921e-06
Iter: 1559 loss: 2.78014e-06
Iter: 1560 loss: 2.77967411e-06
Iter: 1561 loss: 2.77921754e-06
Iter: 1562 loss: 2.77850086e-06
Iter: 1563 loss: 2.77772824e-06
Iter: 1564 loss: 2.77757181e-06
Iter: 1565 loss: 2.77665413e-06
Iter: 1566 loss: 2.78695e-06
Iter: 1567 loss: 2.7765991e-06
Iter: 1568 loss: 2.77590925e-06
Iter: 1569 loss: 2.77612185e-06
Iter: 1570 loss: 2.77537492e-06
Iter: 1571 loss: 2.77428262e-06
Iter: 1572 loss: 2.77405888e-06
Iter: 1573 loss: 2.77331674e-06
Iter: 1574 loss: 2.77246431e-06
Iter: 1575 loss: 2.78780408e-06
Iter: 1576 loss: 2.77243657e-06
Iter: 1577 loss: 2.77126105e-06
Iter: 1578 loss: 2.77040544e-06
Iter: 1579 loss: 2.7700903e-06
Iter: 1580 loss: 2.7686192e-06
Iter: 1581 loss: 2.76900801e-06
Iter: 1582 loss: 2.76755145e-06
Iter: 1583 loss: 2.76607852e-06
Iter: 1584 loss: 2.78375046e-06
Iter: 1585 loss: 2.76601372e-06
Iter: 1586 loss: 2.76444894e-06
Iter: 1587 loss: 2.76689934e-06
Iter: 1588 loss: 2.76380479e-06
Iter: 1589 loss: 2.76273045e-06
Iter: 1590 loss: 2.76193509e-06
Iter: 1591 loss: 2.76157334e-06
Iter: 1592 loss: 2.76052515e-06
Iter: 1593 loss: 2.76054652e-06
Iter: 1594 loss: 2.75931779e-06
Iter: 1595 loss: 2.76074024e-06
Iter: 1596 loss: 2.75877073e-06
Iter: 1597 loss: 2.7578144e-06
Iter: 1598 loss: 2.76143237e-06
Iter: 1599 loss: 2.75763864e-06
Iter: 1600 loss: 2.75668026e-06
Iter: 1601 loss: 2.76012861e-06
Iter: 1602 loss: 2.75645743e-06
Iter: 1603 loss: 2.75553543e-06
Iter: 1604 loss: 2.75444245e-06
Iter: 1605 loss: 2.75430102e-06
Iter: 1606 loss: 2.75299521e-06
Iter: 1607 loss: 2.76474088e-06
Iter: 1608 loss: 2.75286925e-06
Iter: 1609 loss: 2.75167213e-06
Iter: 1610 loss: 2.75807542e-06
Iter: 1611 loss: 2.75148341e-06
Iter: 1612 loss: 2.75043658e-06
Iter: 1613 loss: 2.74894478e-06
Iter: 1614 loss: 2.74896774e-06
Iter: 1615 loss: 2.74740205e-06
Iter: 1616 loss: 2.75378852e-06
Iter: 1617 loss: 2.74706417e-06
Iter: 1618 loss: 2.74615354e-06
Iter: 1619 loss: 2.74611602e-06
Iter: 1620 loss: 2.74555623e-06
Iter: 1621 loss: 2.74401782e-06
Iter: 1622 loss: 2.76106698e-06
Iter: 1623 loss: 2.74393278e-06
Iter: 1624 loss: 2.74223908e-06
Iter: 1625 loss: 2.75006937e-06
Iter: 1626 loss: 2.74190779e-06
Iter: 1627 loss: 2.74106105e-06
Iter: 1628 loss: 2.74096124e-06
Iter: 1629 loss: 2.74020294e-06
Iter: 1630 loss: 2.73849037e-06
Iter: 1631 loss: 2.76831611e-06
Iter: 1632 loss: 2.73845694e-06
Iter: 1633 loss: 2.73743626e-06
Iter: 1634 loss: 2.73725027e-06
Iter: 1635 loss: 2.73649266e-06
Iter: 1636 loss: 2.73549631e-06
Iter: 1637 loss: 2.73539445e-06
Iter: 1638 loss: 2.73407295e-06
Iter: 1639 loss: 2.73719434e-06
Iter: 1640 loss: 2.73359137e-06
Iter: 1641 loss: 2.73238379e-06
Iter: 1642 loss: 2.74795457e-06
Iter: 1643 loss: 2.73237265e-06
Iter: 1644 loss: 2.73127739e-06
Iter: 1645 loss: 2.73127398e-06
Iter: 1646 loss: 2.73052638e-06
Iter: 1647 loss: 2.72946909e-06
Iter: 1648 loss: 2.72898728e-06
Iter: 1649 loss: 2.72846137e-06
Iter: 1650 loss: 2.72714806e-06
Iter: 1651 loss: 2.7446074e-06
Iter: 1652 loss: 2.72714669e-06
Iter: 1653 loss: 2.72588045e-06
Iter: 1654 loss: 2.72780608e-06
Iter: 1655 loss: 2.72526404e-06
Iter: 1656 loss: 2.72421812e-06
Iter: 1657 loss: 2.72235093e-06
Iter: 1658 loss: 2.72238412e-06
Iter: 1659 loss: 2.72121133e-06
Iter: 1660 loss: 2.72107923e-06
Iter: 1661 loss: 2.71972567e-06
Iter: 1662 loss: 2.72124339e-06
Iter: 1663 loss: 2.71901672e-06
Iter: 1664 loss: 2.71810131e-06
Iter: 1665 loss: 2.72211582e-06
Iter: 1666 loss: 2.71794283e-06
Iter: 1667 loss: 2.71687395e-06
Iter: 1668 loss: 2.71783324e-06
Iter: 1669 loss: 2.71621252e-06
Iter: 1670 loss: 2.71512e-06
Iter: 1671 loss: 2.71540489e-06
Iter: 1672 loss: 2.7143326e-06
Iter: 1673 loss: 2.7133633e-06
Iter: 1674 loss: 2.71339763e-06
Iter: 1675 loss: 2.71245699e-06
Iter: 1676 loss: 2.71209e-06
Iter: 1677 loss: 2.71159115e-06
Iter: 1678 loss: 2.71051795e-06
Iter: 1679 loss: 2.71244676e-06
Iter: 1680 loss: 2.70992905e-06
Iter: 1681 loss: 2.70877354e-06
Iter: 1682 loss: 2.71006593e-06
Iter: 1683 loss: 2.70816645e-06
Iter: 1684 loss: 2.70686814e-06
Iter: 1685 loss: 2.72464104e-06
Iter: 1686 loss: 2.70684632e-06
Iter: 1687 loss: 2.70596638e-06
Iter: 1688 loss: 2.7044041e-06
Iter: 1689 loss: 2.70438545e-06
Iter: 1690 loss: 2.70322266e-06
Iter: 1691 loss: 2.7068304e-06
Iter: 1692 loss: 2.70281816e-06
Iter: 1693 loss: 2.70159717e-06
Iter: 1694 loss: 2.72035413e-06
Iter: 1695 loss: 2.70153896e-06
Iter: 1696 loss: 2.70067017e-06
Iter: 1697 loss: 2.70017881e-06
Iter: 1698 loss: 2.6997418e-06
Iter: 1699 loss: 2.6989344e-06
Iter: 1700 loss: 2.71078989e-06
Iter: 1701 loss: 2.69889233e-06
Iter: 1702 loss: 2.69815564e-06
Iter: 1703 loss: 2.69672501e-06
Iter: 1704 loss: 2.72770649e-06
Iter: 1705 loss: 2.69667316e-06
Iter: 1706 loss: 2.6953735e-06
Iter: 1707 loss: 2.70213582e-06
Iter: 1708 loss: 2.69518387e-06
Iter: 1709 loss: 2.69375118e-06
Iter: 1710 loss: 2.70332202e-06
Iter: 1711 loss: 2.69363045e-06
Iter: 1712 loss: 2.6928833e-06
Iter: 1713 loss: 2.6919447e-06
Iter: 1714 loss: 2.69181851e-06
Iter: 1715 loss: 2.69035581e-06
Iter: 1716 loss: 2.69424845e-06
Iter: 1717 loss: 2.68971257e-06
Iter: 1718 loss: 2.68896883e-06
Iter: 1719 loss: 2.68892973e-06
Iter: 1720 loss: 2.68830399e-06
Iter: 1721 loss: 2.68816348e-06
Iter: 1722 loss: 2.68764097e-06
Iter: 1723 loss: 2.68690064e-06
Iter: 1724 loss: 2.68687427e-06
Iter: 1725 loss: 2.68628673e-06
Iter: 1726 loss: 2.68555459e-06
Iter: 1727 loss: 2.6951061e-06
Iter: 1728 loss: 2.68545023e-06
Iter: 1729 loss: 2.68450685e-06
Iter: 1730 loss: 2.68557e-06
Iter: 1731 loss: 2.68401982e-06
Iter: 1732 loss: 2.68318604e-06
Iter: 1733 loss: 2.68359599e-06
Iter: 1734 loss: 2.68260374e-06
Iter: 1735 loss: 2.68119e-06
Iter: 1736 loss: 2.68833173e-06
Iter: 1737 loss: 2.68099438e-06
Iter: 1738 loss: 2.68014242e-06
Iter: 1739 loss: 2.67850055e-06
Iter: 1740 loss: 2.70847613e-06
Iter: 1741 loss: 2.67839823e-06
Iter: 1742 loss: 2.6776479e-06
Iter: 1743 loss: 2.67724363e-06
Iter: 1744 loss: 2.67656355e-06
Iter: 1745 loss: 2.67555015e-06
Iter: 1746 loss: 2.67548489e-06
Iter: 1747 loss: 2.67432915e-06
Iter: 1748 loss: 2.67628934e-06
Iter: 1749 loss: 2.67370888e-06
Iter: 1750 loss: 2.67270434e-06
Iter: 1751 loss: 2.68249141e-06
Iter: 1752 loss: 2.67262567e-06
Iter: 1753 loss: 2.67187988e-06
Iter: 1754 loss: 2.67609175e-06
Iter: 1755 loss: 2.67175665e-06
Iter: 1756 loss: 2.67097289e-06
Iter: 1757 loss: 2.67018049e-06
Iter: 1758 loss: 2.67008386e-06
Iter: 1759 loss: 2.66874258e-06
Iter: 1760 loss: 2.67110045e-06
Iter: 1761 loss: 2.66822644e-06
Iter: 1762 loss: 2.66709867e-06
Iter: 1763 loss: 2.66709958e-06
Iter: 1764 loss: 2.66642951e-06
Iter: 1765 loss: 2.665382e-06
Iter: 1766 loss: 2.66537245e-06
Iter: 1767 loss: 2.66412781e-06
Iter: 1768 loss: 2.67905352e-06
Iter: 1769 loss: 2.66415282e-06
Iter: 1770 loss: 2.66333655e-06
Iter: 1771 loss: 2.66230131e-06
Iter: 1772 loss: 2.66227175e-06
Iter: 1773 loss: 2.66143388e-06
Iter: 1774 loss: 2.6614141e-06
Iter: 1775 loss: 2.66056304e-06
Iter: 1776 loss: 2.66193251e-06
Iter: 1777 loss: 2.66022857e-06
Iter: 1778 loss: 2.65956214e-06
Iter: 1779 loss: 2.65874337e-06
Iter: 1780 loss: 2.65868084e-06
Iter: 1781 loss: 2.65767358e-06
Iter: 1782 loss: 2.66470761e-06
Iter: 1783 loss: 2.6576547e-06
Iter: 1784 loss: 2.65651897e-06
Iter: 1785 loss: 2.66040911e-06
Iter: 1786 loss: 2.65635367e-06
Iter: 1787 loss: 2.65510198e-06
Iter: 1788 loss: 2.65547396e-06
Iter: 1789 loss: 2.65417657e-06
Iter: 1790 loss: 2.65287986e-06
Iter: 1791 loss: 2.65452718e-06
Iter: 1792 loss: 2.65223184e-06
Iter: 1793 loss: 2.65118933e-06
Iter: 1794 loss: 2.6511716e-06
Iter: 1795 loss: 2.65023573e-06
Iter: 1796 loss: 2.64929599e-06
Iter: 1797 loss: 2.64911478e-06
Iter: 1798 loss: 2.64802111e-06
Iter: 1799 loss: 2.65674498e-06
Iter: 1800 loss: 2.64799974e-06
Iter: 1801 loss: 2.64694904e-06
Iter: 1802 loss: 2.64796245e-06
Iter: 1803 loss: 2.64629875e-06
Iter: 1804 loss: 2.64548476e-06
Iter: 1805 loss: 2.64599316e-06
Iter: 1806 loss: 2.64500545e-06
Iter: 1807 loss: 2.64378673e-06
Iter: 1808 loss: 2.65577864e-06
Iter: 1809 loss: 2.64374785e-06
Iter: 1810 loss: 2.64313599e-06
Iter: 1811 loss: 2.64213713e-06
Iter: 1812 loss: 2.64214304e-06
Iter: 1813 loss: 2.64066966e-06
Iter: 1814 loss: 2.6421344e-06
Iter: 1815 loss: 2.63986e-06
Iter: 1816 loss: 2.63858465e-06
Iter: 1817 loss: 2.63852e-06
Iter: 1818 loss: 2.63741595e-06
Iter: 1819 loss: 2.63894322e-06
Iter: 1820 loss: 2.63681159e-06
Iter: 1821 loss: 2.63584025e-06
Iter: 1822 loss: 2.63699258e-06
Iter: 1823 loss: 2.63529137e-06
Iter: 1824 loss: 2.63419133e-06
Iter: 1825 loss: 2.64007167e-06
Iter: 1826 loss: 2.63406946e-06
Iter: 1827 loss: 2.63305219e-06
Iter: 1828 loss: 2.63925676e-06
Iter: 1829 loss: 2.6329883e-06
Iter: 1830 loss: 2.63227389e-06
Iter: 1831 loss: 2.63121979e-06
Iter: 1832 loss: 2.63124866e-06
Iter: 1833 loss: 2.62982462e-06
Iter: 1834 loss: 2.64636969e-06
Iter: 1835 loss: 2.62983463e-06
Iter: 1836 loss: 2.62912044e-06
Iter: 1837 loss: 2.62833464e-06
Iter: 1838 loss: 2.62817139e-06
Iter: 1839 loss: 2.62715866e-06
Iter: 1840 loss: 2.64001164e-06
Iter: 1841 loss: 2.62714684e-06
Iter: 1842 loss: 2.62605636e-06
Iter: 1843 loss: 2.62494837e-06
Iter: 1844 loss: 2.6247576e-06
Iter: 1845 loss: 2.62343838e-06
Iter: 1846 loss: 2.62456865e-06
Iter: 1847 loss: 2.62263552e-06
Iter: 1848 loss: 2.62148205e-06
Iter: 1849 loss: 2.63227594e-06
Iter: 1850 loss: 2.62145863e-06
Iter: 1851 loss: 2.62030426e-06
Iter: 1852 loss: 2.62563526e-06
Iter: 1853 loss: 2.62004869e-06
Iter: 1854 loss: 2.6191417e-06
Iter: 1855 loss: 2.61987816e-06
Iter: 1856 loss: 2.61861578e-06
Iter: 1857 loss: 2.617684e-06
Iter: 1858 loss: 2.61858395e-06
Iter: 1859 loss: 2.6171474e-06
Iter: 1860 loss: 2.61601167e-06
Iter: 1861 loss: 2.63277661e-06
Iter: 1862 loss: 2.61596938e-06
Iter: 1863 loss: 2.61526657e-06
Iter: 1864 loss: 2.6146231e-06
Iter: 1865 loss: 2.61441664e-06
Iter: 1866 loss: 2.61338096e-06
Iter: 1867 loss: 2.6191924e-06
Iter: 1868 loss: 2.6133207e-06
Iter: 1869 loss: 2.61215246e-06
Iter: 1870 loss: 2.61194418e-06
Iter: 1871 loss: 2.61121568e-06
Iter: 1872 loss: 2.61008586e-06
Iter: 1873 loss: 2.61519244e-06
Iter: 1874 loss: 2.60983234e-06
Iter: 1875 loss: 2.60851357e-06
Iter: 1876 loss: 2.61284754e-06
Iter: 1877 loss: 2.60817637e-06
Iter: 1878 loss: 2.60727575e-06
Iter: 1879 loss: 2.60594652e-06
Iter: 1880 loss: 2.60594152e-06
Iter: 1881 loss: 2.60441584e-06
Iter: 1882 loss: 2.6116154e-06
Iter: 1883 loss: 2.60411616e-06
Iter: 1884 loss: 2.60340948e-06
Iter: 1885 loss: 2.60338e-06
Iter: 1886 loss: 2.60249863e-06
Iter: 1887 loss: 2.60143065e-06
Iter: 1888 loss: 2.60139927e-06
Iter: 1889 loss: 2.60021852e-06
Iter: 1890 loss: 2.6053815e-06
Iter: 1891 loss: 2.59996568e-06
Iter: 1892 loss: 2.59900435e-06
Iter: 1893 loss: 2.60618731e-06
Iter: 1894 loss: 2.59887747e-06
Iter: 1895 loss: 2.59790409e-06
Iter: 1896 loss: 2.5988561e-06
Iter: 1897 loss: 2.59729291e-06
Iter: 1898 loss: 2.59616399e-06
Iter: 1899 loss: 2.59586068e-06
Iter: 1900 loss: 2.59516128e-06
Iter: 1901 loss: 2.59377748e-06
Iter: 1902 loss: 2.61441619e-06
Iter: 1903 loss: 2.59376702e-06
Iter: 1904 loss: 2.59301851e-06
Iter: 1905 loss: 2.59214767e-06
Iter: 1906 loss: 2.59210242e-06
Iter: 1907 loss: 2.59091985e-06
Iter: 1908 loss: 2.60842762e-06
Iter: 1909 loss: 2.59094031e-06
Iter: 1910 loss: 2.59011813e-06
Iter: 1911 loss: 2.58970431e-06
Iter: 1912 loss: 2.58931186e-06
Iter: 1913 loss: 2.58847513e-06
Iter: 1914 loss: 2.58777936e-06
Iter: 1915 loss: 2.5875413e-06
Iter: 1916 loss: 2.58609271e-06
Iter: 1917 loss: 2.59195713e-06
Iter: 1918 loss: 2.58579848e-06
Iter: 1919 loss: 2.58473915e-06
Iter: 1920 loss: 2.58466207e-06
Iter: 1921 loss: 2.58397858e-06
Iter: 1922 loss: 2.5827278e-06
Iter: 1923 loss: 2.58271098e-06
Iter: 1924 loss: 2.58115824e-06
Iter: 1925 loss: 2.58409318e-06
Iter: 1926 loss: 2.58053342e-06
Iter: 1927 loss: 2.58036425e-06
Iter: 1928 loss: 2.57985448e-06
Iter: 1929 loss: 2.57936563e-06
Iter: 1930 loss: 2.57828333e-06
Iter: 1931 loss: 2.59965418e-06
Iter: 1932 loss: 2.57834336e-06
Iter: 1933 loss: 2.57732836e-06
Iter: 1934 loss: 2.58526143e-06
Iter: 1935 loss: 2.5773229e-06
Iter: 1936 loss: 2.57656848e-06
Iter: 1937 loss: 2.58048158e-06
Iter: 1938 loss: 2.57645206e-06
Iter: 1939 loss: 2.57584657e-06
Iter: 1940 loss: 2.57547094e-06
Iter: 1941 loss: 2.57529314e-06
Iter: 1942 loss: 2.57455531e-06
Iter: 1943 loss: 2.58592058e-06
Iter: 1944 loss: 2.57456213e-06
Iter: 1945 loss: 2.57403917e-06
Iter: 1946 loss: 2.57296824e-06
Iter: 1947 loss: 2.57296438e-06
Iter: 1948 loss: 2.57188663e-06
Iter: 1949 loss: 2.57323472e-06
Iter: 1950 loss: 2.57132115e-06
Iter: 1951 loss: 2.56991052e-06
Iter: 1952 loss: 2.56968588e-06
Iter: 1953 loss: 2.56866224e-06
Iter: 1954 loss: 2.56906083e-06
Iter: 1955 loss: 2.56803219e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi0.8/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.2
+ date
Sun Nov  8 15:51:09 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfc7d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfcad9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfcadc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfc0fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfc0ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfb522f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfc0fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfa6f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfa6f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfb52510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfabb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfac2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfac2c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfacb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfacbae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfacbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfa41598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fdfacb158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8fe37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8fd0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8f96840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8ec08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8f3c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8f7a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8f7a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8e5a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8e01950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8df3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8df3048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8df6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8efd268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8daf1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8daf378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8d85400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8d72840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6fd8e8c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 202, in <module>
    grads = tape.gradient(loss, model.trainable_weights)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1266, in _backward_function_wrapper
    processed_args, remapped_captures)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input is not invertible.
	 [[node gradients/MatrixDeterminant_grad/MatrixInverse (defined at biholoNN_train.py:200) ]] [Op:__inference___backward_volume_form_4446_8945]

Function call stack:
__backward_volume_form_4446

+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.2/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f716268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f755d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f7550d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f6c2c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f6c2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f6462f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f6018c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f6012f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f5af268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f572d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f5af2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f539158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f5467b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f5468c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f495b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f4d3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f4c7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f4c7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f4336a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f437f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c8f437ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c536f8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c536f8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c536d79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c536d78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c53689378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c536adbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c53664950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c5a6048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c5ccea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c563598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c58a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c58a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c539b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c4f1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4c2c4a99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.03043601e-05
Iter: 2 loss: 2.13928288e-05
Iter: 3 loss: 1.76234e-05
Iter: 4 loss: 1.66110494e-05
Iter: 5 loss: 1.98952785e-05
Iter: 6 loss: 1.63260047e-05
Iter: 7 loss: 1.56334208e-05
Iter: 8 loss: 1.71883021e-05
Iter: 9 loss: 1.53712026e-05
Iter: 10 loss: 1.45759068e-05
Iter: 11 loss: 1.67807957e-05
Iter: 12 loss: 1.4320226e-05
Iter: 13 loss: 1.36861881e-05
Iter: 14 loss: 1.51034492e-05
Iter: 15 loss: 1.34441516e-05
Iter: 16 loss: 1.2621188e-05
Iter: 17 loss: 1.51645199e-05
Iter: 18 loss: 1.23790851e-05
Iter: 19 loss: 1.194783e-05
Iter: 20 loss: 1.25094757e-05
Iter: 21 loss: 1.17289765e-05
Iter: 22 loss: 1.1256242e-05
Iter: 23 loss: 1.13565111e-05
Iter: 24 loss: 1.0906233e-05
Iter: 25 loss: 1.03548027e-05
Iter: 26 loss: 1.86317484e-05
Iter: 27 loss: 1.03544726e-05
Iter: 28 loss: 1.00655816e-05
Iter: 29 loss: 9.74680188e-06
Iter: 30 loss: 9.70091241e-06
Iter: 31 loss: 9.34108175e-06
Iter: 32 loss: 1.08189306e-05
Iter: 33 loss: 9.26257053e-06
Iter: 34 loss: 8.96314305e-06
Iter: 35 loss: 1.0014347e-05
Iter: 36 loss: 8.88555587e-06
Iter: 37 loss: 8.62310844e-06
Iter: 38 loss: 8.80394418e-06
Iter: 39 loss: 8.45900922e-06
Iter: 40 loss: 8.26135783e-06
Iter: 41 loss: 8.24843846e-06
Iter: 42 loss: 8.02765771e-06
Iter: 43 loss: 8.03409421e-06
Iter: 44 loss: 7.85297289e-06
Iter: 45 loss: 7.63214666e-06
Iter: 46 loss: 9.42098541e-06
Iter: 47 loss: 7.61778665e-06
Iter: 48 loss: 7.45341276e-06
Iter: 49 loss: 8.00842099e-06
Iter: 50 loss: 7.40920268e-06
Iter: 51 loss: 7.30675038e-06
Iter: 52 loss: 7.79777201e-06
Iter: 53 loss: 7.28873965e-06
Iter: 54 loss: 7.17521198e-06
Iter: 55 loss: 7.19378295e-06
Iter: 56 loss: 7.08961852e-06
Iter: 57 loss: 6.98062786e-06
Iter: 58 loss: 6.94966548e-06
Iter: 59 loss: 6.88325235e-06
Iter: 60 loss: 6.74460534e-06
Iter: 61 loss: 8.24203562e-06
Iter: 62 loss: 6.7412675e-06
Iter: 63 loss: 6.64966456e-06
Iter: 64 loss: 7.08681273e-06
Iter: 65 loss: 6.63303263e-06
Iter: 66 loss: 6.55926442e-06
Iter: 67 loss: 6.53650659e-06
Iter: 68 loss: 6.49291e-06
Iter: 69 loss: 6.39660539e-06
Iter: 70 loss: 6.60120304e-06
Iter: 71 loss: 6.35868309e-06
Iter: 72 loss: 6.24562381e-06
Iter: 73 loss: 6.31917783e-06
Iter: 74 loss: 6.1738665e-06
Iter: 75 loss: 6.04172237e-06
Iter: 76 loss: 6.1550445e-06
Iter: 77 loss: 5.96364498e-06
Iter: 78 loss: 5.87838895e-06
Iter: 79 loss: 5.87365093e-06
Iter: 80 loss: 5.80388041e-06
Iter: 81 loss: 6.4672895e-06
Iter: 82 loss: 5.80116466e-06
Iter: 83 loss: 5.74486694e-06
Iter: 84 loss: 5.68498581e-06
Iter: 85 loss: 5.675171e-06
Iter: 86 loss: 5.64757647e-06
Iter: 87 loss: 5.63767389e-06
Iter: 88 loss: 5.60960234e-06
Iter: 89 loss: 5.54802909e-06
Iter: 90 loss: 6.45463388e-06
Iter: 91 loss: 5.54525877e-06
Iter: 92 loss: 5.50889854e-06
Iter: 93 loss: 5.50454752e-06
Iter: 94 loss: 5.4830316e-06
Iter: 95 loss: 5.43392616e-06
Iter: 96 loss: 6.07193033e-06
Iter: 97 loss: 5.43083479e-06
Iter: 98 loss: 5.38452332e-06
Iter: 99 loss: 5.74633896e-06
Iter: 100 loss: 5.38132599e-06
Iter: 101 loss: 5.34178e-06
Iter: 102 loss: 5.57227395e-06
Iter: 103 loss: 5.33654566e-06
Iter: 104 loss: 5.29839326e-06
Iter: 105 loss: 5.30918624e-06
Iter: 106 loss: 5.27081602e-06
Iter: 107 loss: 5.22167375e-06
Iter: 108 loss: 5.20913909e-06
Iter: 109 loss: 5.17822309e-06
Iter: 110 loss: 5.13113082e-06
Iter: 111 loss: 5.13106352e-06
Iter: 112 loss: 5.10261907e-06
Iter: 113 loss: 5.04431682e-06
Iter: 114 loss: 6.09887229e-06
Iter: 115 loss: 5.04335458e-06
Iter: 116 loss: 5.06397e-06
Iter: 117 loss: 5.01600516e-06
Iter: 118 loss: 4.99762064e-06
Iter: 119 loss: 4.98796635e-06
Iter: 120 loss: 4.9795608e-06
Iter: 121 loss: 4.95567292e-06
Iter: 122 loss: 4.96334542e-06
Iter: 123 loss: 4.93872221e-06
Iter: 124 loss: 4.9133123e-06
Iter: 125 loss: 4.91311675e-06
Iter: 126 loss: 4.90065122e-06
Iter: 127 loss: 4.87634679e-06
Iter: 128 loss: 5.3527292e-06
Iter: 129 loss: 4.87605575e-06
Iter: 130 loss: 4.85213695e-06
Iter: 131 loss: 4.85210285e-06
Iter: 132 loss: 4.836264e-06
Iter: 133 loss: 4.79963455e-06
Iter: 134 loss: 5.26386566e-06
Iter: 135 loss: 4.79722621e-06
Iter: 136 loss: 4.76076275e-06
Iter: 137 loss: 4.99567523e-06
Iter: 138 loss: 4.75675643e-06
Iter: 139 loss: 4.7331514e-06
Iter: 140 loss: 5.04736454e-06
Iter: 141 loss: 4.73309592e-06
Iter: 142 loss: 4.71384556e-06
Iter: 143 loss: 4.73574619e-06
Iter: 144 loss: 4.70338455e-06
Iter: 145 loss: 4.68590133e-06
Iter: 146 loss: 4.65851372e-06
Iter: 147 loss: 4.65817584e-06
Iter: 148 loss: 4.63432161e-06
Iter: 149 loss: 4.9303203e-06
Iter: 150 loss: 4.63401329e-06
Iter: 151 loss: 4.61113314e-06
Iter: 152 loss: 4.59064177e-06
Iter: 153 loss: 4.58483464e-06
Iter: 154 loss: 4.59113107e-06
Iter: 155 loss: 4.57403257e-06
Iter: 156 loss: 4.56242242e-06
Iter: 157 loss: 4.5511506e-06
Iter: 158 loss: 4.5485549e-06
Iter: 159 loss: 4.53276562e-06
Iter: 160 loss: 4.59559169e-06
Iter: 161 loss: 4.52901577e-06
Iter: 162 loss: 4.50839343e-06
Iter: 163 loss: 4.5384345e-06
Iter: 164 loss: 4.49821755e-06
Iter: 165 loss: 4.48187e-06
Iter: 166 loss: 4.50106108e-06
Iter: 167 loss: 4.47326329e-06
Iter: 168 loss: 4.46175727e-06
Iter: 169 loss: 4.56892622e-06
Iter: 170 loss: 4.46129297e-06
Iter: 171 loss: 4.44882699e-06
Iter: 172 loss: 4.4392e-06
Iter: 173 loss: 4.43518911e-06
Iter: 174 loss: 4.41767952e-06
Iter: 175 loss: 4.43229874e-06
Iter: 176 loss: 4.40728036e-06
Iter: 177 loss: 4.39051837e-06
Iter: 178 loss: 4.5599395e-06
Iter: 179 loss: 4.3899754e-06
Iter: 180 loss: 4.37912558e-06
Iter: 181 loss: 4.36842038e-06
Iter: 182 loss: 4.36610708e-06
Iter: 183 loss: 4.34788581e-06
Iter: 184 loss: 4.49545678e-06
Iter: 185 loss: 4.34672711e-06
Iter: 186 loss: 4.3327309e-06
Iter: 187 loss: 4.44548095e-06
Iter: 188 loss: 4.33177502e-06
Iter: 189 loss: 4.31943226e-06
Iter: 190 loss: 4.30987529e-06
Iter: 191 loss: 4.3059058e-06
Iter: 192 loss: 4.28737621e-06
Iter: 193 loss: 4.29194552e-06
Iter: 194 loss: 4.27378745e-06
Iter: 195 loss: 4.27861778e-06
Iter: 196 loss: 4.26689257e-06
Iter: 197 loss: 4.25991129e-06
Iter: 198 loss: 4.24561858e-06
Iter: 199 loss: 4.50120206e-06
Iter: 200 loss: 4.24542577e-06
Iter: 201 loss: 4.23563051e-06
Iter: 202 loss: 4.37932522e-06
Iter: 203 loss: 4.23558504e-06
Iter: 204 loss: 4.22662924e-06
Iter: 205 loss: 4.24192876e-06
Iter: 206 loss: 4.22257563e-06
Iter: 207 loss: 4.21396089e-06
Iter: 208 loss: 4.19953858e-06
Iter: 209 loss: 4.19955722e-06
Iter: 210 loss: 4.19502e-06
Iter: 211 loss: 4.19177377e-06
Iter: 212 loss: 4.18595755e-06
Iter: 213 loss: 4.17423917e-06
Iter: 214 loss: 4.40151098e-06
Iter: 215 loss: 4.17407773e-06
Iter: 216 loss: 4.16128387e-06
Iter: 217 loss: 4.20767719e-06
Iter: 218 loss: 4.15800196e-06
Iter: 219 loss: 4.14690567e-06
Iter: 220 loss: 4.16737112e-06
Iter: 221 loss: 4.14214082e-06
Iter: 222 loss: 4.13315183e-06
Iter: 223 loss: 4.20902825e-06
Iter: 224 loss: 4.13251837e-06
Iter: 225 loss: 4.122272e-06
Iter: 226 loss: 4.14152964e-06
Iter: 227 loss: 4.1178273e-06
Iter: 228 loss: 4.1098e-06
Iter: 229 loss: 4.13673843e-06
Iter: 230 loss: 4.10758821e-06
Iter: 231 loss: 4.10047369e-06
Iter: 232 loss: 4.10315897e-06
Iter: 233 loss: 4.09549057e-06
Iter: 234 loss: 4.08976211e-06
Iter: 235 loss: 4.08930055e-06
Iter: 236 loss: 4.08478536e-06
Iter: 237 loss: 4.07731659e-06
Iter: 238 loss: 4.07731386e-06
Iter: 239 loss: 4.06985419e-06
Iter: 240 loss: 4.13265843e-06
Iter: 241 loss: 4.06943582e-06
Iter: 242 loss: 4.06301342e-06
Iter: 243 loss: 4.08773576e-06
Iter: 244 loss: 4.06142408e-06
Iter: 245 loss: 4.05658693e-06
Iter: 246 loss: 4.04457114e-06
Iter: 247 loss: 4.15372051e-06
Iter: 248 loss: 4.04269031e-06
Iter: 249 loss: 4.03701597e-06
Iter: 250 loss: 4.03557078e-06
Iter: 251 loss: 4.02827209e-06
Iter: 252 loss: 4.02515707e-06
Iter: 253 loss: 4.02121168e-06
Iter: 254 loss: 4.01287571e-06
Iter: 255 loss: 4.02435217e-06
Iter: 256 loss: 4.00879344e-06
Iter: 257 loss: 3.99795636e-06
Iter: 258 loss: 4.00344379e-06
Iter: 259 loss: 3.9907e-06
Iter: 260 loss: 3.98242582e-06
Iter: 261 loss: 4.02435126e-06
Iter: 262 loss: 3.98116117e-06
Iter: 263 loss: 3.97280473e-06
Iter: 264 loss: 4.02979e-06
Iter: 265 loss: 3.97197391e-06
Iter: 266 loss: 3.96376254e-06
Iter: 267 loss: 3.98634256e-06
Iter: 268 loss: 3.9611341e-06
Iter: 269 loss: 3.95592906e-06
Iter: 270 loss: 3.96599444e-06
Iter: 271 loss: 3.95390589e-06
Iter: 272 loss: 3.95007646e-06
Iter: 273 loss: 3.950061e-06
Iter: 274 loss: 3.94700146e-06
Iter: 275 loss: 3.93881965e-06
Iter: 276 loss: 3.98837528e-06
Iter: 277 loss: 3.93649771e-06
Iter: 278 loss: 3.932656e-06
Iter: 279 loss: 3.9314009e-06
Iter: 280 loss: 3.92665879e-06
Iter: 281 loss: 3.92535912e-06
Iter: 282 loss: 3.92252923e-06
Iter: 283 loss: 3.91622052e-06
Iter: 284 loss: 3.91908634e-06
Iter: 285 loss: 3.91193134e-06
Iter: 286 loss: 3.90425066e-06
Iter: 287 loss: 3.92299171e-06
Iter: 288 loss: 3.90150262e-06
Iter: 289 loss: 3.89287061e-06
Iter: 290 loss: 3.95542793e-06
Iter: 291 loss: 3.892108e-06
Iter: 292 loss: 3.88594708e-06
Iter: 293 loss: 3.88843682e-06
Iter: 294 loss: 3.8817675e-06
Iter: 295 loss: 3.87497948e-06
Iter: 296 loss: 3.87394948e-06
Iter: 297 loss: 3.86929241e-06
Iter: 298 loss: 3.86059946e-06
Iter: 299 loss: 3.89533761e-06
Iter: 300 loss: 3.85862586e-06
Iter: 301 loss: 3.85253543e-06
Iter: 302 loss: 3.92711e-06
Iter: 303 loss: 3.85249768e-06
Iter: 304 loss: 3.8471685e-06
Iter: 305 loss: 3.86266311e-06
Iter: 306 loss: 3.84564e-06
Iter: 307 loss: 3.83980205e-06
Iter: 308 loss: 3.84457417e-06
Iter: 309 loss: 3.83638462e-06
Iter: 310 loss: 3.83205042e-06
Iter: 311 loss: 3.83186944e-06
Iter: 312 loss: 3.82931285e-06
Iter: 313 loss: 3.82174312e-06
Iter: 314 loss: 3.84426767e-06
Iter: 315 loss: 3.81785185e-06
Iter: 316 loss: 3.82268945e-06
Iter: 317 loss: 3.81462405e-06
Iter: 318 loss: 3.81195878e-06
Iter: 319 loss: 3.80549409e-06
Iter: 320 loss: 3.87264208e-06
Iter: 321 loss: 3.80473e-06
Iter: 322 loss: 3.79933681e-06
Iter: 323 loss: 3.83245879e-06
Iter: 324 loss: 3.79869243e-06
Iter: 325 loss: 3.79364883e-06
Iter: 326 loss: 3.80596225e-06
Iter: 327 loss: 3.79183848e-06
Iter: 328 loss: 3.78594791e-06
Iter: 329 loss: 3.8244807e-06
Iter: 330 loss: 3.78531786e-06
Iter: 331 loss: 3.78237792e-06
Iter: 332 loss: 3.77988681e-06
Iter: 333 loss: 3.77909146e-06
Iter: 334 loss: 3.77270112e-06
Iter: 335 loss: 3.77501829e-06
Iter: 336 loss: 3.76836351e-06
Iter: 337 loss: 3.76067283e-06
Iter: 338 loss: 3.7846246e-06
Iter: 339 loss: 3.75856553e-06
Iter: 340 loss: 3.75337572e-06
Iter: 341 loss: 3.7840091e-06
Iter: 342 loss: 3.75276204e-06
Iter: 343 loss: 3.74807905e-06
Iter: 344 loss: 3.78939876e-06
Iter: 345 loss: 3.74778097e-06
Iter: 346 loss: 3.7437826e-06
Iter: 347 loss: 3.75104537e-06
Iter: 348 loss: 3.74181218e-06
Iter: 349 loss: 3.73877492e-06
Iter: 350 loss: 3.77079959e-06
Iter: 351 loss: 3.73863554e-06
Iter: 352 loss: 3.73674538e-06
Iter: 353 loss: 3.731672e-06
Iter: 354 loss: 3.76846265e-06
Iter: 355 loss: 3.73064177e-06
Iter: 356 loss: 3.7254224e-06
Iter: 357 loss: 3.72524346e-06
Iter: 358 loss: 3.72096792e-06
Iter: 359 loss: 3.72147861e-06
Iter: 360 loss: 3.71774195e-06
Iter: 361 loss: 3.71360761e-06
Iter: 362 loss: 3.7113648e-06
Iter: 363 loss: 3.70952375e-06
Iter: 364 loss: 3.70351427e-06
Iter: 365 loss: 3.70721477e-06
Iter: 366 loss: 3.69962549e-06
Iter: 367 loss: 3.69552617e-06
Iter: 368 loss: 3.69489953e-06
Iter: 369 loss: 3.69098188e-06
Iter: 370 loss: 3.69307827e-06
Iter: 371 loss: 3.68834253e-06
Iter: 372 loss: 3.68458586e-06
Iter: 373 loss: 3.69393206e-06
Iter: 374 loss: 3.68328051e-06
Iter: 375 loss: 3.67982898e-06
Iter: 376 loss: 3.67888879e-06
Iter: 377 loss: 3.6768156e-06
Iter: 378 loss: 3.67150187e-06
Iter: 379 loss: 3.70989619e-06
Iter: 380 loss: 3.67105145e-06
Iter: 381 loss: 3.66832433e-06
Iter: 382 loss: 3.66824679e-06
Iter: 383 loss: 3.66555582e-06
Iter: 384 loss: 3.66062136e-06
Iter: 385 loss: 3.77662082e-06
Iter: 386 loss: 3.66052404e-06
Iter: 387 loss: 3.65664982e-06
Iter: 388 loss: 3.71453552e-06
Iter: 389 loss: 3.65659616e-06
Iter: 390 loss: 3.6531037e-06
Iter: 391 loss: 3.64784319e-06
Iter: 392 loss: 3.64770403e-06
Iter: 393 loss: 3.64460311e-06
Iter: 394 loss: 3.6440756e-06
Iter: 395 loss: 3.64162679e-06
Iter: 396 loss: 3.63853e-06
Iter: 397 loss: 3.63838649e-06
Iter: 398 loss: 3.6346205e-06
Iter: 399 loss: 3.63814638e-06
Iter: 400 loss: 3.63247796e-06
Iter: 401 loss: 3.627978e-06
Iter: 402 loss: 3.63029721e-06
Iter: 403 loss: 3.62507103e-06
Iter: 404 loss: 3.62096443e-06
Iter: 405 loss: 3.68433189e-06
Iter: 406 loss: 3.62096216e-06
Iter: 407 loss: 3.61698039e-06
Iter: 408 loss: 3.62879905e-06
Iter: 409 loss: 3.61579464e-06
Iter: 410 loss: 3.61275806e-06
Iter: 411 loss: 3.61042248e-06
Iter: 412 loss: 3.6094907e-06
Iter: 413 loss: 3.6041763e-06
Iter: 414 loss: 3.61682442e-06
Iter: 415 loss: 3.6022625e-06
Iter: 416 loss: 3.59808337e-06
Iter: 417 loss: 3.60443687e-06
Iter: 418 loss: 3.59606975e-06
Iter: 419 loss: 3.59196019e-06
Iter: 420 loss: 3.65649453e-06
Iter: 421 loss: 3.5918697e-06
Iter: 422 loss: 3.58905936e-06
Iter: 423 loss: 3.61641855e-06
Iter: 424 loss: 3.58890361e-06
Iter: 425 loss: 3.58699685e-06
Iter: 426 loss: 3.58308557e-06
Iter: 427 loss: 3.64919833e-06
Iter: 428 loss: 3.58295051e-06
Iter: 429 loss: 3.57815179e-06
Iter: 430 loss: 3.61828916e-06
Iter: 431 loss: 3.57788372e-06
Iter: 432 loss: 3.5742e-06
Iter: 433 loss: 3.58244961e-06
Iter: 434 loss: 3.57268618e-06
Iter: 435 loss: 3.56944656e-06
Iter: 436 loss: 3.58497118e-06
Iter: 437 loss: 3.56877831e-06
Iter: 438 loss: 3.5651949e-06
Iter: 439 loss: 3.56381088e-06
Iter: 440 loss: 3.56185069e-06
Iter: 441 loss: 3.55857651e-06
Iter: 442 loss: 3.558873e-06
Iter: 443 loss: 3.55616817e-06
Iter: 444 loss: 3.55205043e-06
Iter: 445 loss: 3.58012835e-06
Iter: 446 loss: 3.55159341e-06
Iter: 447 loss: 3.54781741e-06
Iter: 448 loss: 3.57219233e-06
Iter: 449 loss: 3.5474e-06
Iter: 450 loss: 3.54424878e-06
Iter: 451 loss: 3.54401982e-06
Iter: 452 loss: 3.54166923e-06
Iter: 453 loss: 3.53884434e-06
Iter: 454 loss: 3.53797168e-06
Iter: 455 loss: 3.53625387e-06
Iter: 456 loss: 3.53193309e-06
Iter: 457 loss: 3.5405692e-06
Iter: 458 loss: 3.53003429e-06
Iter: 459 loss: 3.52667348e-06
Iter: 460 loss: 3.5264361e-06
Iter: 461 loss: 3.52350617e-06
Iter: 462 loss: 3.53548103e-06
Iter: 463 loss: 3.52283746e-06
Iter: 464 loss: 3.52112988e-06
Iter: 465 loss: 3.51676135e-06
Iter: 466 loss: 3.55209204e-06
Iter: 467 loss: 3.51602102e-06
Iter: 468 loss: 3.51281528e-06
Iter: 469 loss: 3.51236531e-06
Iter: 470 loss: 3.50985965e-06
Iter: 471 loss: 3.50916025e-06
Iter: 472 loss: 3.5076132e-06
Iter: 473 loss: 3.50388109e-06
Iter: 474 loss: 3.51805693e-06
Iter: 475 loss: 3.50289247e-06
Iter: 476 loss: 3.49970674e-06
Iter: 477 loss: 3.50715982e-06
Iter: 478 loss: 3.49847141e-06
Iter: 479 loss: 3.49595985e-06
Iter: 480 loss: 3.49467837e-06
Iter: 481 loss: 3.49341167e-06
Iter: 482 loss: 3.4899308e-06
Iter: 483 loss: 3.51066819e-06
Iter: 484 loss: 3.48944968e-06
Iter: 485 loss: 3.48613548e-06
Iter: 486 loss: 3.48641242e-06
Iter: 487 loss: 3.483608e-06
Iter: 488 loss: 3.47985701e-06
Iter: 489 loss: 3.479822e-06
Iter: 490 loss: 3.47740911e-06
Iter: 491 loss: 3.47450555e-06
Iter: 492 loss: 3.47424202e-06
Iter: 493 loss: 3.4714667e-06
Iter: 494 loss: 3.47314631e-06
Iter: 495 loss: 3.469788e-06
Iter: 496 loss: 3.46560432e-06
Iter: 497 loss: 3.49332299e-06
Iter: 498 loss: 3.4652162e-06
Iter: 499 loss: 3.460789e-06
Iter: 500 loss: 3.48686353e-06
Iter: 501 loss: 3.46016327e-06
Iter: 502 loss: 3.45859e-06
Iter: 503 loss: 3.45667286e-06
Iter: 504 loss: 3.45643e-06
Iter: 505 loss: 3.4541581e-06
Iter: 506 loss: 3.47896957e-06
Iter: 507 loss: 3.4540742e-06
Iter: 508 loss: 3.45143303e-06
Iter: 509 loss: 3.45003514e-06
Iter: 510 loss: 3.44886894e-06
Iter: 511 loss: 3.4459681e-06
Iter: 512 loss: 3.4673817e-06
Iter: 513 loss: 3.44569457e-06
Iter: 514 loss: 3.44337082e-06
Iter: 515 loss: 3.44767454e-06
Iter: 516 loss: 3.44228692e-06
Iter: 517 loss: 3.43984038e-06
Iter: 518 loss: 3.43684155e-06
Iter: 519 loss: 3.436518e-06
Iter: 520 loss: 3.43273564e-06
Iter: 521 loss: 3.45327021e-06
Iter: 522 loss: 3.4321688e-06
Iter: 523 loss: 3.42844146e-06
Iter: 524 loss: 3.43591455e-06
Iter: 525 loss: 3.42706176e-06
Iter: 526 loss: 3.42360454e-06
Iter: 527 loss: 3.43554484e-06
Iter: 528 loss: 3.42271392e-06
Iter: 529 loss: 3.41807731e-06
Iter: 530 loss: 3.43312081e-06
Iter: 531 loss: 3.41678151e-06
Iter: 532 loss: 3.41398299e-06
Iter: 533 loss: 3.42114618e-06
Iter: 534 loss: 3.41297982e-06
Iter: 535 loss: 3.41053146e-06
Iter: 536 loss: 3.40813153e-06
Iter: 537 loss: 3.40758106e-06
Iter: 538 loss: 3.40291217e-06
Iter: 539 loss: 3.40664519e-06
Iter: 540 loss: 3.40024e-06
Iter: 541 loss: 3.40463771e-06
Iter: 542 loss: 3.39840813e-06
Iter: 543 loss: 3.39742019e-06
Iter: 544 loss: 3.39543158e-06
Iter: 545 loss: 3.43637976e-06
Iter: 546 loss: 3.39536518e-06
Iter: 547 loss: 3.39270559e-06
Iter: 548 loss: 3.39695202e-06
Iter: 549 loss: 3.39142298e-06
Iter: 550 loss: 3.38795326e-06
Iter: 551 loss: 3.40906172e-06
Iter: 552 loss: 3.38750579e-06
Iter: 553 loss: 3.38556083e-06
Iter: 554 loss: 3.38798759e-06
Iter: 555 loss: 3.38456152e-06
Iter: 556 loss: 3.38177983e-06
Iter: 557 loss: 3.38679274e-06
Iter: 558 loss: 3.38054315e-06
Iter: 559 loss: 3.37854317e-06
Iter: 560 loss: 3.3768406e-06
Iter: 561 loss: 3.37631786e-06
Iter: 562 loss: 3.37291885e-06
Iter: 563 loss: 3.38985251e-06
Iter: 564 loss: 3.37244865e-06
Iter: 565 loss: 3.36939343e-06
Iter: 566 loss: 3.37630581e-06
Iter: 567 loss: 3.36833091e-06
Iter: 568 loss: 3.36547487e-06
Iter: 569 loss: 3.3980125e-06
Iter: 570 loss: 3.36546736e-06
Iter: 571 loss: 3.36361018e-06
Iter: 572 loss: 3.36361882e-06
Iter: 573 loss: 3.36215362e-06
Iter: 574 loss: 3.35913523e-06
Iter: 575 loss: 3.35585059e-06
Iter: 576 loss: 3.35544269e-06
Iter: 577 loss: 3.35169921e-06
Iter: 578 loss: 3.37786855e-06
Iter: 579 loss: 3.35135519e-06
Iter: 580 loss: 3.34804304e-06
Iter: 581 loss: 3.35308459e-06
Iter: 582 loss: 3.34653168e-06
Iter: 583 loss: 3.34508377e-06
Iter: 584 loss: 3.34452238e-06
Iter: 585 loss: 3.34280912e-06
Iter: 586 loss: 3.34172546e-06
Iter: 587 loss: 3.34095512e-06
Iter: 588 loss: 3.3391284e-06
Iter: 589 loss: 3.34667857e-06
Iter: 590 loss: 3.3387555e-06
Iter: 591 loss: 3.33640901e-06
Iter: 592 loss: 3.33989919e-06
Iter: 593 loss: 3.33540834e-06
Iter: 594 loss: 3.33362641e-06
Iter: 595 loss: 3.34386391e-06
Iter: 596 loss: 3.33345292e-06
Iter: 597 loss: 3.33174603e-06
Iter: 598 loss: 3.32997274e-06
Iter: 599 loss: 3.32955346e-06
Iter: 600 loss: 3.32641639e-06
Iter: 601 loss: 3.32719173e-06
Iter: 602 loss: 3.32391733e-06
Iter: 603 loss: 3.32094373e-06
Iter: 604 loss: 3.34338893e-06
Iter: 605 loss: 3.32067043e-06
Iter: 606 loss: 3.31787828e-06
Iter: 607 loss: 3.33783464e-06
Iter: 608 loss: 3.31756564e-06
Iter: 609 loss: 3.31624642e-06
Iter: 610 loss: 3.31533693e-06
Iter: 611 loss: 3.31478236e-06
Iter: 612 loss: 3.31217075e-06
Iter: 613 loss: 3.31618503e-06
Iter: 614 loss: 3.31095657e-06
Iter: 615 loss: 3.30830767e-06
Iter: 616 loss: 3.30875309e-06
Iter: 617 loss: 3.30629882e-06
Iter: 618 loss: 3.30311832e-06
Iter: 619 loss: 3.31860429e-06
Iter: 620 loss: 3.30260809e-06
Iter: 621 loss: 3.30070043e-06
Iter: 622 loss: 3.3005872e-06
Iter: 623 loss: 3.29893192e-06
Iter: 624 loss: 3.29823547e-06
Iter: 625 loss: 3.29723935e-06
Iter: 626 loss: 3.29539262e-06
Iter: 627 loss: 3.29635623e-06
Iter: 628 loss: 3.29418572e-06
Iter: 629 loss: 3.29093814e-06
Iter: 630 loss: 3.30849957e-06
Iter: 631 loss: 3.2904436e-06
Iter: 632 loss: 3.28899068e-06
Iter: 633 loss: 3.2907019e-06
Iter: 634 loss: 3.28831334e-06
Iter: 635 loss: 3.28581405e-06
Iter: 636 loss: 3.28627311e-06
Iter: 637 loss: 3.28414853e-06
Iter: 638 loss: 3.28201759e-06
Iter: 639 loss: 3.28690771e-06
Iter: 640 loss: 3.28122132e-06
Iter: 641 loss: 3.27908811e-06
Iter: 642 loss: 3.28547867e-06
Iter: 643 loss: 3.27825364e-06
Iter: 644 loss: 3.27614953e-06
Iter: 645 loss: 3.29813861e-06
Iter: 646 loss: 3.27610087e-06
Iter: 647 loss: 3.27487e-06
Iter: 648 loss: 3.27237331e-06
Iter: 649 loss: 3.31529031e-06
Iter: 650 loss: 3.2723035e-06
Iter: 651 loss: 3.26912186e-06
Iter: 652 loss: 3.27621319e-06
Iter: 653 loss: 3.26797363e-06
Iter: 654 loss: 3.26471513e-06
Iter: 655 loss: 3.29298723e-06
Iter: 656 loss: 3.26456961e-06
Iter: 657 loss: 3.26236045e-06
Iter: 658 loss: 3.26024815e-06
Iter: 659 loss: 3.25980227e-06
Iter: 660 loss: 3.2588955e-06
Iter: 661 loss: 3.25782707e-06
Iter: 662 loss: 3.2565863e-06
Iter: 663 loss: 3.25818337e-06
Iter: 664 loss: 3.25572546e-06
Iter: 665 loss: 3.25469932e-06
Iter: 666 loss: 3.25287783e-06
Iter: 667 loss: 3.25288875e-06
Iter: 668 loss: 3.24943176e-06
Iter: 669 loss: 3.2666162e-06
Iter: 670 loss: 3.24883922e-06
Iter: 671 loss: 3.24738266e-06
Iter: 672 loss: 3.24663165e-06
Iter: 673 loss: 3.24587199e-06
Iter: 674 loss: 3.2434441e-06
Iter: 675 loss: 3.26148779e-06
Iter: 676 loss: 3.24313692e-06
Iter: 677 loss: 3.2410619e-06
Iter: 678 loss: 3.23817449e-06
Iter: 679 loss: 3.23803806e-06
Iter: 680 loss: 3.23471204e-06
Iter: 681 loss: 3.24672487e-06
Iter: 682 loss: 3.23382096e-06
Iter: 683 loss: 3.23144877e-06
Iter: 684 loss: 3.24364373e-06
Iter: 685 loss: 3.2311541e-06
Iter: 686 loss: 3.22859887e-06
Iter: 687 loss: 3.24313601e-06
Iter: 688 loss: 3.22825122e-06
Iter: 689 loss: 3.2264536e-06
Iter: 690 loss: 3.22531673e-06
Iter: 691 loss: 3.22456822e-06
Iter: 692 loss: 3.22214851e-06
Iter: 693 loss: 3.22932374e-06
Iter: 694 loss: 3.22144047e-06
Iter: 695 loss: 3.21934726e-06
Iter: 696 loss: 3.22520918e-06
Iter: 697 loss: 3.21854441e-06
Iter: 698 loss: 3.21603579e-06
Iter: 699 loss: 3.24690018e-06
Iter: 700 loss: 3.21598236e-06
Iter: 701 loss: 3.21486596e-06
Iter: 702 loss: 3.21261109e-06
Iter: 703 loss: 3.2552598e-06
Iter: 704 loss: 3.2126627e-06
Iter: 705 loss: 3.21059588e-06
Iter: 706 loss: 3.24323e-06
Iter: 707 loss: 3.21058769e-06
Iter: 708 loss: 3.20874778e-06
Iter: 709 loss: 3.21064772e-06
Iter: 710 loss: 3.2077005e-06
Iter: 711 loss: 3.20606e-06
Iter: 712 loss: 3.20425693e-06
Iter: 713 loss: 3.20386357e-06
Iter: 714 loss: 3.20164281e-06
Iter: 715 loss: 3.20172785e-06
Iter: 716 loss: 3.20020672e-06
Iter: 717 loss: 3.19816149e-06
Iter: 718 loss: 3.19801848e-06
Iter: 719 loss: 3.19524952e-06
Iter: 720 loss: 3.21025641e-06
Iter: 721 loss: 3.19484e-06
Iter: 722 loss: 3.19283163e-06
Iter: 723 loss: 3.1976615e-06
Iter: 724 loss: 3.19218384e-06
Iter: 725 loss: 3.19006267e-06
Iter: 726 loss: 3.20666095e-06
Iter: 727 loss: 3.18999e-06
Iter: 728 loss: 3.18818684e-06
Iter: 729 loss: 3.1860302e-06
Iter: 730 loss: 3.18586285e-06
Iter: 731 loss: 3.18318803e-06
Iter: 732 loss: 3.2012008e-06
Iter: 733 loss: 3.18299976e-06
Iter: 734 loss: 3.18173124e-06
Iter: 735 loss: 3.18152138e-06
Iter: 736 loss: 3.18076718e-06
Iter: 737 loss: 3.17817376e-06
Iter: 738 loss: 3.18531261e-06
Iter: 739 loss: 3.17672198e-06
Iter: 740 loss: 3.176978e-06
Iter: 741 loss: 3.17547415e-06
Iter: 742 loss: 3.17435024e-06
Iter: 743 loss: 3.17396598e-06
Iter: 744 loss: 3.17329e-06
Iter: 745 loss: 3.17167337e-06
Iter: 746 loss: 3.16959176e-06
Iter: 747 loss: 3.1694367e-06
Iter: 748 loss: 3.16755541e-06
Iter: 749 loss: 3.1675163e-06
Iter: 750 loss: 3.16582054e-06
Iter: 751 loss: 3.16288242e-06
Iter: 752 loss: 3.16280966e-06
Iter: 753 loss: 3.16027445e-06
Iter: 754 loss: 3.17609829e-06
Iter: 755 loss: 3.15998113e-06
Iter: 756 loss: 3.15770421e-06
Iter: 757 loss: 3.16642695e-06
Iter: 758 loss: 3.15710486e-06
Iter: 759 loss: 3.15490252e-06
Iter: 760 loss: 3.16831893e-06
Iter: 761 loss: 3.15454963e-06
Iter: 762 loss: 3.15298439e-06
Iter: 763 loss: 3.15261968e-06
Iter: 764 loss: 3.15163948e-06
Iter: 765 loss: 3.14966769e-06
Iter: 766 loss: 3.16619798e-06
Iter: 767 loss: 3.14955378e-06
Iter: 768 loss: 3.14757381e-06
Iter: 769 loss: 3.15400098e-06
Iter: 770 loss: 3.14701447e-06
Iter: 771 loss: 3.14545241e-06
Iter: 772 loss: 3.14310046e-06
Iter: 773 loss: 3.1430377e-06
Iter: 774 loss: 3.14204362e-06
Iter: 775 loss: 3.14179806e-06
Iter: 776 loss: 3.1405084e-06
Iter: 777 loss: 3.13904866e-06
Iter: 778 loss: 3.13887654e-06
Iter: 779 loss: 3.13709256e-06
Iter: 780 loss: 3.14040426e-06
Iter: 781 loss: 3.13633836e-06
Iter: 782 loss: 3.13459532e-06
Iter: 783 loss: 3.14660792e-06
Iter: 784 loss: 3.13442956e-06
Iter: 785 loss: 3.13267083e-06
Iter: 786 loss: 3.13512373e-06
Iter: 787 loss: 3.13196597e-06
Iter: 788 loss: 3.13052215e-06
Iter: 789 loss: 3.12846e-06
Iter: 790 loss: 3.12848056e-06
Iter: 791 loss: 3.12590828e-06
Iter: 792 loss: 3.15369516e-06
Iter: 793 loss: 3.12598627e-06
Iter: 794 loss: 3.12357156e-06
Iter: 795 loss: 3.13214559e-06
Iter: 796 loss: 3.12292809e-06
Iter: 797 loss: 3.12127304e-06
Iter: 798 loss: 3.12177781e-06
Iter: 799 loss: 3.12018415e-06
Iter: 800 loss: 3.11847816e-06
Iter: 801 loss: 3.11849044e-06
Iter: 802 loss: 3.11707254e-06
Iter: 803 loss: 3.11653093e-06
Iter: 804 loss: 3.11566328e-06
Iter: 805 loss: 3.11448366e-06
Iter: 806 loss: 3.11472104e-06
Iter: 807 loss: 3.11352187e-06
Iter: 808 loss: 3.1116333e-06
Iter: 809 loss: 3.12323755e-06
Iter: 810 loss: 3.11140957e-06
Iter: 811 loss: 3.10933683e-06
Iter: 812 loss: 3.11318126e-06
Iter: 813 loss: 3.10843643e-06
Iter: 814 loss: 3.10675864e-06
Iter: 815 loss: 3.10346377e-06
Iter: 816 loss: 3.17486888e-06
Iter: 817 loss: 3.10345058e-06
Iter: 818 loss: 3.10239557e-06
Iter: 819 loss: 3.10189012e-06
Iter: 820 loss: 3.10038467e-06
Iter: 821 loss: 3.10117048e-06
Iter: 822 loss: 3.09945153e-06
Iter: 823 loss: 3.09775919e-06
Iter: 824 loss: 3.09569714e-06
Iter: 825 loss: 3.09552388e-06
Iter: 826 loss: 3.09260463e-06
Iter: 827 loss: 3.11496842e-06
Iter: 828 loss: 3.0925039e-06
Iter: 829 loss: 3.0909921e-06
Iter: 830 loss: 3.0909514e-06
Iter: 831 loss: 3.08992912e-06
Iter: 832 loss: 3.08722906e-06
Iter: 833 loss: 3.10795713e-06
Iter: 834 loss: 3.08665813e-06
Iter: 835 loss: 3.08608742e-06
Iter: 836 loss: 3.08487915e-06
Iter: 837 loss: 3.08391645e-06
Iter: 838 loss: 3.08321205e-06
Iter: 839 loss: 3.08291828e-06
Iter: 840 loss: 3.08159451e-06
Iter: 841 loss: 3.07945447e-06
Iter: 842 loss: 3.07942855e-06
Iter: 843 loss: 3.07780715e-06
Iter: 844 loss: 3.07753453e-06
Iter: 845 loss: 3.07666141e-06
Iter: 846 loss: 3.07555888e-06
Iter: 847 loss: 3.07552227e-06
Iter: 848 loss: 3.07395158e-06
Iter: 849 loss: 3.07694791e-06
Iter: 850 loss: 3.07334039e-06
Iter: 851 loss: 3.07174264e-06
Iter: 852 loss: 3.07818027e-06
Iter: 853 loss: 3.07136133e-06
Iter: 854 loss: 3.06966263e-06
Iter: 855 loss: 3.07258347e-06
Iter: 856 loss: 3.06885795e-06
Iter: 857 loss: 3.06731636e-06
Iter: 858 loss: 3.06867378e-06
Iter: 859 loss: 3.06631659e-06
Iter: 860 loss: 3.06431821e-06
Iter: 861 loss: 3.07290497e-06
Iter: 862 loss: 3.06392303e-06
Iter: 863 loss: 3.06231368e-06
Iter: 864 loss: 3.0800818e-06
Iter: 865 loss: 3.06231777e-06
Iter: 866 loss: 3.06122297e-06
Iter: 867 loss: 3.05958656e-06
Iter: 868 loss: 3.0595379e-06
Iter: 869 loss: 3.05799767e-06
Iter: 870 loss: 3.05784579e-06
Iter: 871 loss: 3.05683443e-06
Iter: 872 loss: 3.05445747e-06
Iter: 873 loss: 3.0856113e-06
Iter: 874 loss: 3.05432241e-06
Iter: 875 loss: 3.05229105e-06
Iter: 876 loss: 3.07106257e-06
Iter: 877 loss: 3.05221783e-06
Iter: 878 loss: 3.05049502e-06
Iter: 879 loss: 3.06317565e-06
Iter: 880 loss: 3.050332e-06
Iter: 881 loss: 3.04920331e-06
Iter: 882 loss: 3.04676678e-06
Iter: 883 loss: 3.08559333e-06
Iter: 884 loss: 3.04661808e-06
Iter: 885 loss: 3.04427294e-06
Iter: 886 loss: 3.06210109e-06
Iter: 887 loss: 3.0440483e-06
Iter: 888 loss: 3.0424726e-06
Iter: 889 loss: 3.06356537e-06
Iter: 890 loss: 3.04245168e-06
Iter: 891 loss: 3.04153946e-06
Iter: 892 loss: 3.04076275e-06
Iter: 893 loss: 3.04049945e-06
Iter: 894 loss: 3.03850129e-06
Iter: 895 loss: 3.03825209e-06
Iter: 896 loss: 3.03687102e-06
Iter: 897 loss: 3.03480829e-06
Iter: 898 loss: 3.0348117e-06
Iter: 899 loss: 3.03310571e-06
Iter: 900 loss: 3.03626643e-06
Iter: 901 loss: 3.03234401e-06
Iter: 902 loss: 3.03074239e-06
Iter: 903 loss: 3.03603065e-06
Iter: 904 loss: 3.03024126e-06
Iter: 905 loss: 3.02875219e-06
Iter: 906 loss: 3.04142441e-06
Iter: 907 loss: 3.02860872e-06
Iter: 908 loss: 3.02756553e-06
Iter: 909 loss: 3.02525541e-06
Iter: 910 loss: 3.05912772e-06
Iter: 911 loss: 3.02509193e-06
Iter: 912 loss: 3.02394324e-06
Iter: 913 loss: 3.02384251e-06
Iter: 914 loss: 3.0224669e-06
Iter: 915 loss: 3.02187141e-06
Iter: 916 loss: 3.02111243e-06
Iter: 917 loss: 3.01979981e-06
Iter: 918 loss: 3.02225862e-06
Iter: 919 loss: 3.01920591e-06
Iter: 920 loss: 3.01774799e-06
Iter: 921 loss: 3.01898581e-06
Iter: 922 loss: 3.01690443e-06
Iter: 923 loss: 3.0153e-06
Iter: 924 loss: 3.03665911e-06
Iter: 925 loss: 3.01534851e-06
Iter: 926 loss: 3.01390651e-06
Iter: 927 loss: 3.01113323e-06
Iter: 928 loss: 3.06079187e-06
Iter: 929 loss: 3.01101e-06
Iter: 930 loss: 3.00862371e-06
Iter: 931 loss: 3.03006618e-06
Iter: 932 loss: 3.00858119e-06
Iter: 933 loss: 3.00695569e-06
Iter: 934 loss: 3.02273338e-06
Iter: 935 loss: 3.00694023e-06
Iter: 936 loss: 3.00554598e-06
Iter: 937 loss: 3.00751435e-06
Iter: 938 loss: 3.00492138e-06
Iter: 939 loss: 3.00381953e-06
Iter: 940 loss: 3.01223417e-06
Iter: 941 loss: 3.00377815e-06
Iter: 942 loss: 3.00256443e-06
Iter: 943 loss: 3.00208058e-06
Iter: 944 loss: 3.00145211e-06
Iter: 945 loss: 2.99999e-06
Iter: 946 loss: 3.00253146e-06
Iter: 947 loss: 2.99946214e-06
Iter: 948 loss: 2.9982707e-06
Iter: 949 loss: 3.0072847e-06
Iter: 950 loss: 2.99815474e-06
Iter: 951 loss: 2.9967141e-06
Iter: 952 loss: 2.99711769e-06
Iter: 953 loss: 2.99572753e-06
Iter: 954 loss: 2.99448948e-06
Iter: 955 loss: 2.99293333e-06
Iter: 956 loss: 2.99279941e-06
Iter: 957 loss: 2.99046e-06
Iter: 958 loss: 3.00153238e-06
Iter: 959 loss: 2.99005569e-06
Iter: 960 loss: 2.98816713e-06
Iter: 961 loss: 3.01320802e-06
Iter: 962 loss: 2.9880739e-06
Iter: 963 loss: 2.98689156e-06
Iter: 964 loss: 2.98669579e-06
Iter: 965 loss: 2.98573877e-06
Iter: 966 loss: 2.98422174e-06
Iter: 967 loss: 2.98396117e-06
Iter: 968 loss: 2.98277428e-06
Iter: 969 loss: 2.98163241e-06
Iter: 970 loss: 2.98147916e-06
Iter: 971 loss: 2.98004716e-06
Iter: 972 loss: 2.97814472e-06
Iter: 973 loss: 2.97798761e-06
Iter: 974 loss: 2.97666293e-06
Iter: 975 loss: 2.97646238e-06
Iter: 976 loss: 2.97548149e-06
Iter: 977 loss: 2.97454562e-06
Iter: 978 loss: 2.97438328e-06
Iter: 979 loss: 2.97272072e-06
Iter: 980 loss: 2.97229099e-06
Iter: 981 loss: 2.97136489e-06
Iter: 982 loss: 2.97000497e-06
Iter: 983 loss: 2.96990038e-06
Iter: 984 loss: 2.9688008e-06
Iter: 985 loss: 2.9675864e-06
Iter: 986 loss: 2.96746589e-06
Iter: 987 loss: 2.96566259e-06
Iter: 988 loss: 2.96865619e-06
Iter: 989 loss: 2.96497387e-06
Iter: 990 loss: 2.96332428e-06
Iter: 991 loss: 2.96373514e-06
Iter: 992 loss: 2.9621774e-06
Iter: 993 loss: 2.96068492e-06
Iter: 994 loss: 2.96061239e-06
Iter: 995 loss: 2.95939481e-06
Iter: 996 loss: 2.95763311e-06
Iter: 997 loss: 2.95758377e-06
Iter: 998 loss: 2.95566747e-06
Iter: 999 loss: 2.96450571e-06
Iter: 1000 loss: 2.95525842e-06
Iter: 1001 loss: 2.95375708e-06
Iter: 1002 loss: 2.97059114e-06
Iter: 1003 loss: 2.95373661e-06
Iter: 1004 loss: 2.95231371e-06
Iter: 1005 loss: 2.9515104e-06
Iter: 1006 loss: 2.95093196e-06
Iter: 1007 loss: 2.9498151e-06
Iter: 1008 loss: 2.94973415e-06
Iter: 1009 loss: 2.94914753e-06
Iter: 1010 loss: 2.94760503e-06
Iter: 1011 loss: 2.95673044e-06
Iter: 1012 loss: 2.94730626e-06
Iter: 1013 loss: 2.94565393e-06
Iter: 1014 loss: 2.96938492e-06
Iter: 1015 loss: 2.94564961e-06
Iter: 1016 loss: 2.94451888e-06
Iter: 1017 loss: 2.95338577e-06
Iter: 1018 loss: 2.94446863e-06
Iter: 1019 loss: 2.94359461e-06
Iter: 1020 loss: 2.94159963e-06
Iter: 1021 loss: 2.96165331e-06
Iter: 1022 loss: 2.94131678e-06
Iter: 1023 loss: 2.93869471e-06
Iter: 1024 loss: 2.95122663e-06
Iter: 1025 loss: 2.93827429e-06
Iter: 1026 loss: 2.93622134e-06
Iter: 1027 loss: 2.94224697e-06
Iter: 1028 loss: 2.93560606e-06
Iter: 1029 loss: 2.93413586e-06
Iter: 1030 loss: 2.95597511e-06
Iter: 1031 loss: 2.93412359e-06
Iter: 1032 loss: 2.93286803e-06
Iter: 1033 loss: 2.93002086e-06
Iter: 1034 loss: 2.97851716e-06
Iter: 1035 loss: 2.92996106e-06
Iter: 1036 loss: 2.92811774e-06
Iter: 1037 loss: 2.95581322e-06
Iter: 1038 loss: 2.92813411e-06
Iter: 1039 loss: 2.92662025e-06
Iter: 1040 loss: 2.93535163e-06
Iter: 1041 loss: 2.92642972e-06
Iter: 1042 loss: 2.92515529e-06
Iter: 1043 loss: 2.92761388e-06
Iter: 1044 loss: 2.92463847e-06
Iter: 1045 loss: 2.92334153e-06
Iter: 1046 loss: 2.93142239e-06
Iter: 1047 loss: 2.92318964e-06
Iter: 1048 loss: 2.92237678e-06
Iter: 1049 loss: 2.92063373e-06
Iter: 1050 loss: 2.95258906e-06
Iter: 1051 loss: 2.92066443e-06
Iter: 1052 loss: 2.91894594e-06
Iter: 1053 loss: 2.93513267e-06
Iter: 1054 loss: 2.91892093e-06
Iter: 1055 loss: 2.91682863e-06
Iter: 1056 loss: 2.91671563e-06
Iter: 1057 loss: 2.91510059e-06
Iter: 1058 loss: 2.91333436e-06
Iter: 1059 loss: 2.91660945e-06
Iter: 1060 loss: 2.91259312e-06
Iter: 1061 loss: 2.91087918e-06
Iter: 1062 loss: 2.91479319e-06
Iter: 1063 loss: 2.91021252e-06
Iter: 1064 loss: 2.9085063e-06
Iter: 1065 loss: 2.91518108e-06
Iter: 1066 loss: 2.90823846e-06
Iter: 1067 loss: 2.90668254e-06
Iter: 1068 loss: 2.91833067e-06
Iter: 1069 loss: 2.90655044e-06
Iter: 1070 loss: 2.90548905e-06
Iter: 1071 loss: 2.90637536e-06
Iter: 1072 loss: 2.9049022e-06
Iter: 1073 loss: 2.90357752e-06
Iter: 1074 loss: 2.9039752e-06
Iter: 1075 loss: 2.90268099e-06
Iter: 1076 loss: 2.9017383e-06
Iter: 1077 loss: 2.90163098e-06
Iter: 1078 loss: 2.90086746e-06
Iter: 1079 loss: 2.90000753e-06
Iter: 1080 loss: 2.89981381e-06
Iter: 1081 loss: 2.89817399e-06
Iter: 1082 loss: 2.90517141e-06
Iter: 1083 loss: 2.89776813e-06
Iter: 1084 loss: 2.89672334e-06
Iter: 1085 loss: 2.89562286e-06
Iter: 1086 loss: 2.89542572e-06
Iter: 1087 loss: 2.89364107e-06
Iter: 1088 loss: 2.90613571e-06
Iter: 1089 loss: 2.89351556e-06
Iter: 1090 loss: 2.89158561e-06
Iter: 1091 loss: 2.89800278e-06
Iter: 1092 loss: 2.8910174e-06
Iter: 1093 loss: 2.89024729e-06
Iter: 1094 loss: 2.88840693e-06
Iter: 1095 loss: 2.91128799e-06
Iter: 1096 loss: 2.88834235e-06
Iter: 1097 loss: 2.88616138e-06
Iter: 1098 loss: 2.90479375e-06
Iter: 1099 loss: 2.88606543e-06
Iter: 1100 loss: 2.88434853e-06
Iter: 1101 loss: 2.88831438e-06
Iter: 1102 loss: 2.88366118e-06
Iter: 1103 loss: 2.88218303e-06
Iter: 1104 loss: 2.89720356e-06
Iter: 1105 loss: 2.8820923e-06
Iter: 1106 loss: 2.88071988e-06
Iter: 1107 loss: 2.87957e-06
Iter: 1108 loss: 2.87921625e-06
Iter: 1109 loss: 2.8773984e-06
Iter: 1110 loss: 2.88849174e-06
Iter: 1111 loss: 2.87714875e-06
Iter: 1112 loss: 2.87585408e-06
Iter: 1113 loss: 2.89091872e-06
Iter: 1114 loss: 2.87584589e-06
Iter: 1115 loss: 2.8747e-06
Iter: 1116 loss: 2.87363537e-06
Iter: 1117 loss: 2.87334433e-06
Iter: 1118 loss: 2.87205808e-06
Iter: 1119 loss: 2.87204625e-06
Iter: 1120 loss: 2.8712941e-06
Iter: 1121 loss: 2.86918907e-06
Iter: 1122 loss: 2.88151296e-06
Iter: 1123 loss: 2.86863e-06
Iter: 1124 loss: 2.86900604e-06
Iter: 1125 loss: 2.86765589e-06
Iter: 1126 loss: 2.8669765e-06
Iter: 1127 loss: 2.86685918e-06
Iter: 1128 loss: 2.86638442e-06
Iter: 1129 loss: 2.86530576e-06
Iter: 1130 loss: 2.86355203e-06
Iter: 1131 loss: 2.90681646e-06
Iter: 1132 loss: 2.86355248e-06
Iter: 1133 loss: 2.86145814e-06
Iter: 1134 loss: 2.86624572e-06
Iter: 1135 loss: 2.86063869e-06
Iter: 1136 loss: 2.85873853e-06
Iter: 1137 loss: 2.85869919e-06
Iter: 1138 loss: 2.85734086e-06
Iter: 1139 loss: 2.86096383e-06
Iter: 1140 loss: 2.85685019e-06
Iter: 1141 loss: 2.85541205e-06
Iter: 1142 loss: 2.85465603e-06
Iter: 1143 loss: 2.8538866e-06
Iter: 1144 loss: 2.85214037e-06
Iter: 1145 loss: 2.86751e-06
Iter: 1146 loss: 2.85196506e-06
Iter: 1147 loss: 2.85056649e-06
Iter: 1148 loss: 2.86350928e-06
Iter: 1149 loss: 2.85049168e-06
Iter: 1150 loss: 2.84946168e-06
Iter: 1151 loss: 2.85e-06
Iter: 1152 loss: 2.84884686e-06
Iter: 1153 loss: 2.84789394e-06
Iter: 1154 loss: 2.85478154e-06
Iter: 1155 loss: 2.84783209e-06
Iter: 1156 loss: 2.84689827e-06
Iter: 1157 loss: 2.84501493e-06
Iter: 1158 loss: 2.87304965e-06
Iter: 1159 loss: 2.84488851e-06
Iter: 1160 loss: 2.84386624e-06
Iter: 1161 loss: 2.84369366e-06
Iter: 1162 loss: 2.8425934e-06
Iter: 1163 loss: 2.84343878e-06
Iter: 1164 loss: 2.84194175e-06
Iter: 1165 loss: 2.84053976e-06
Iter: 1166 loss: 2.83886834e-06
Iter: 1167 loss: 2.83869986e-06
Iter: 1168 loss: 2.83652798e-06
Iter: 1169 loss: 2.83916916e-06
Iter: 1170 loss: 2.83544546e-06
Iter: 1171 loss: 2.83318468e-06
Iter: 1172 loss: 2.84101861e-06
Iter: 1173 loss: 2.83262443e-06
Iter: 1174 loss: 2.83116196e-06
Iter: 1175 loss: 2.83113923e-06
Iter: 1176 loss: 2.83007375e-06
Iter: 1177 loss: 2.83462077e-06
Iter: 1178 loss: 2.82984547e-06
Iter: 1179 loss: 2.82879864e-06
Iter: 1180 loss: 2.82686278e-06
Iter: 1181 loss: 2.86742443e-06
Iter: 1182 loss: 2.82684186e-06
Iter: 1183 loss: 2.82762721e-06
Iter: 1184 loss: 2.82609449e-06
Iter: 1185 loss: 2.82562951e-06
Iter: 1186 loss: 2.8245156e-06
Iter: 1187 loss: 2.83884219e-06
Iter: 1188 loss: 2.82438168e-06
Iter: 1189 loss: 2.82309929e-06
Iter: 1190 loss: 2.83473219e-06
Iter: 1191 loss: 2.82309247e-06
Iter: 1192 loss: 2.8219913e-06
Iter: 1193 loss: 2.82067163e-06
Iter: 1194 loss: 2.82057499e-06
Iter: 1195 loss: 2.81886582e-06
Iter: 1196 loss: 2.83428881e-06
Iter: 1197 loss: 2.81877828e-06
Iter: 1198 loss: 2.81764915e-06
Iter: 1199 loss: 2.82544852e-06
Iter: 1200 loss: 2.8175125e-06
Iter: 1201 loss: 2.81666416e-06
Iter: 1202 loss: 2.81466714e-06
Iter: 1203 loss: 2.84256748e-06
Iter: 1204 loss: 2.81455755e-06
Iter: 1205 loss: 2.81262e-06
Iter: 1206 loss: 2.82794144e-06
Iter: 1207 loss: 2.81241955e-06
Iter: 1208 loss: 2.81104349e-06
Iter: 1209 loss: 2.81069742e-06
Iter: 1210 loss: 2.80971926e-06
Iter: 1211 loss: 2.80818358e-06
Iter: 1212 loss: 2.8204081e-06
Iter: 1213 loss: 2.80813742e-06
Iter: 1214 loss: 2.80650534e-06
Iter: 1215 loss: 2.81301868e-06
Iter: 1216 loss: 2.80615495e-06
Iter: 1217 loss: 2.80496056e-06
Iter: 1218 loss: 2.80753e-06
Iter: 1219 loss: 2.80448808e-06
Iter: 1220 loss: 2.80352697e-06
Iter: 1221 loss: 2.81346752e-06
Iter: 1222 loss: 2.80353106e-06
Iter: 1223 loss: 2.80246149e-06
Iter: 1224 loss: 2.80123436e-06
Iter: 1225 loss: 2.8011325e-06
Iter: 1226 loss: 2.79988853e-06
Iter: 1227 loss: 2.80690438e-06
Iter: 1228 loss: 2.79975529e-06
Iter: 1229 loss: 2.79844153e-06
Iter: 1230 loss: 2.80047493e-06
Iter: 1231 loss: 2.79772826e-06
Iter: 1232 loss: 2.79677397e-06
Iter: 1233 loss: 2.79851429e-06
Iter: 1234 loss: 2.79628512e-06
Iter: 1235 loss: 2.79500978e-06
Iter: 1236 loss: 2.80150107e-06
Iter: 1237 loss: 2.79466462e-06
Iter: 1238 loss: 2.79360574e-06
Iter: 1239 loss: 2.79297865e-06
Iter: 1240 loss: 2.79253868e-06
Iter: 1241 loss: 2.79098549e-06
Iter: 1242 loss: 2.79329061e-06
Iter: 1243 loss: 2.79026381e-06
Iter: 1244 loss: 2.78869766e-06
Iter: 1245 loss: 2.79510687e-06
Iter: 1246 loss: 2.78823472e-06
Iter: 1247 loss: 2.78681455e-06
Iter: 1248 loss: 2.78618427e-06
Iter: 1249 loss: 2.78544985e-06
Iter: 1250 loss: 2.78420976e-06
Iter: 1251 loss: 2.78405969e-06
Iter: 1252 loss: 2.78284597e-06
Iter: 1253 loss: 2.78240896e-06
Iter: 1254 loss: 2.78172706e-06
Iter: 1255 loss: 2.7804424e-06
Iter: 1256 loss: 2.79493224e-06
Iter: 1257 loss: 2.78039852e-06
Iter: 1258 loss: 2.77913432e-06
Iter: 1259 loss: 2.78099787e-06
Iter: 1260 loss: 2.7785461e-06
Iter: 1261 loss: 2.77758863e-06
Iter: 1262 loss: 2.77671393e-06
Iter: 1263 loss: 2.77640561e-06
Iter: 1264 loss: 2.77529853e-06
Iter: 1265 loss: 2.77533263e-06
Iter: 1266 loss: 2.77436152e-06
Iter: 1267 loss: 2.77356685e-06
Iter: 1268 loss: 2.77336926e-06
Iter: 1269 loss: 2.7724584e-06
Iter: 1270 loss: 2.77246704e-06
Iter: 1271 loss: 2.77186928e-06
Iter: 1272 loss: 2.77039499e-06
Iter: 1273 loss: 2.78687207e-06
Iter: 1274 loss: 2.77019353e-06
Iter: 1275 loss: 2.76860419e-06
Iter: 1276 loss: 2.77767867e-06
Iter: 1277 loss: 2.76836022e-06
Iter: 1278 loss: 2.76696824e-06
Iter: 1279 loss: 2.76741594e-06
Iter: 1280 loss: 2.76595438e-06
Iter: 1281 loss: 2.76425112e-06
Iter: 1282 loss: 2.77485333e-06
Iter: 1283 loss: 2.76406581e-06
Iter: 1284 loss: 2.76258743e-06
Iter: 1285 loss: 2.76231776e-06
Iter: 1286 loss: 2.76124592e-06
Iter: 1287 loss: 2.76032551e-06
Iter: 1288 loss: 2.76013884e-06
Iter: 1289 loss: 2.75904358e-06
Iter: 1290 loss: 2.75780735e-06
Iter: 1291 loss: 2.75767911e-06
Iter: 1292 loss: 2.75712841e-06
Iter: 1293 loss: 2.75685829e-06
Iter: 1294 loss: 2.75627599e-06
Iter: 1295 loss: 2.75543948e-06
Iter: 1296 loss: 2.75532716e-06
Iter: 1297 loss: 2.75425487e-06
Iter: 1298 loss: 2.75364664e-06
Iter: 1299 loss: 2.75313278e-06
Iter: 1300 loss: 2.75167122e-06
Iter: 1301 loss: 2.75901198e-06
Iter: 1302 loss: 2.75147454e-06
Iter: 1303 loss: 2.74998683e-06
Iter: 1304 loss: 2.7637891e-06
Iter: 1305 loss: 2.74999252e-06
Iter: 1306 loss: 2.74906779e-06
Iter: 1307 loss: 2.74834656e-06
Iter: 1308 loss: 2.7480387e-06
Iter: 1309 loss: 2.74670469e-06
Iter: 1310 loss: 2.75846583e-06
Iter: 1311 loss: 2.74669878e-06
Iter: 1312 loss: 2.74549666e-06
Iter: 1313 loss: 2.74464014e-06
Iter: 1314 loss: 2.74427589e-06
Iter: 1315 loss: 2.74282547e-06
Iter: 1316 loss: 2.74532385e-06
Iter: 1317 loss: 2.74210038e-06
Iter: 1318 loss: 2.74073568e-06
Iter: 1319 loss: 2.74727972e-06
Iter: 1320 loss: 2.7405265e-06
Iter: 1321 loss: 2.7390829e-06
Iter: 1322 loss: 2.74081685e-06
Iter: 1323 loss: 2.73833416e-06
Iter: 1324 loss: 2.73715523e-06
Iter: 1325 loss: 2.73717933e-06
Iter: 1326 loss: 2.73626119e-06
Iter: 1327 loss: 2.73642399e-06
Iter: 1328 loss: 2.73557362e-06
Iter: 1329 loss: 2.7343267e-06
Iter: 1330 loss: 2.74271133e-06
Iter: 1331 loss: 2.73414093e-06
Iter: 1332 loss: 2.73315709e-06
Iter: 1333 loss: 2.73195974e-06
Iter: 1334 loss: 2.73179899e-06
Iter: 1335 loss: 2.73008891e-06
Iter: 1336 loss: 2.72979946e-06
Iter: 1337 loss: 2.72871171e-06
Iter: 1338 loss: 2.72737861e-06
Iter: 1339 loss: 2.7273602e-06
Iter: 1340 loss: 2.7258618e-06
Iter: 1341 loss: 2.72691386e-06
Iter: 1342 loss: 2.72507464e-06
Iter: 1343 loss: 2.72419948e-06
Iter: 1344 loss: 2.72714942e-06
Iter: 1345 loss: 2.72401894e-06
Iter: 1346 loss: 2.72288662e-06
Iter: 1347 loss: 2.72275201e-06
Iter: 1348 loss: 2.72198e-06
Iter: 1349 loss: 2.72059765e-06
Iter: 1350 loss: 2.72425632e-06
Iter: 1351 loss: 2.72015041e-06
Iter: 1352 loss: 2.71904014e-06
Iter: 1353 loss: 2.71745694e-06
Iter: 1354 loss: 2.71739646e-06
Iter: 1355 loss: 2.71564613e-06
Iter: 1356 loss: 2.73897899e-06
Iter: 1357 loss: 2.71556e-06
Iter: 1358 loss: 2.71402905e-06
Iter: 1359 loss: 2.72070247e-06
Iter: 1360 loss: 2.71370936e-06
Iter: 1361 loss: 2.71233603e-06
Iter: 1362 loss: 2.72374723e-06
Iter: 1363 loss: 2.7122278e-06
Iter: 1364 loss: 2.71130284e-06
Iter: 1365 loss: 2.7129513e-06
Iter: 1366 loss: 2.71079398e-06
Iter: 1367 loss: 2.70953569e-06
Iter: 1368 loss: 2.70969235e-06
Iter: 1369 loss: 2.70848341e-06
Iter: 1370 loss: 2.70738929e-06
Iter: 1371 loss: 2.70728538e-06
Iter: 1372 loss: 2.70656119e-06
Iter: 1373 loss: 2.7049673e-06
Iter: 1374 loss: 2.71594081e-06
Iter: 1375 loss: 2.70483611e-06
Iter: 1376 loss: 2.70365649e-06
Iter: 1377 loss: 2.71532417e-06
Iter: 1378 loss: 2.70368105e-06
Iter: 1379 loss: 2.70287455e-06
Iter: 1380 loss: 2.70157784e-06
Iter: 1381 loss: 2.73169e-06
Iter: 1382 loss: 2.7015667e-06
Iter: 1383 loss: 2.70063083e-06
Iter: 1384 loss: 2.70057762e-06
Iter: 1385 loss: 2.69979773e-06
Iter: 1386 loss: 2.69869952e-06
Iter: 1387 loss: 2.69863472e-06
Iter: 1388 loss: 2.69703969e-06
Iter: 1389 loss: 2.69823477e-06
Iter: 1390 loss: 2.69604607e-06
Iter: 1391 loss: 2.69424891e-06
Iter: 1392 loss: 2.69777092e-06
Iter: 1393 loss: 2.6934656e-06
Iter: 1394 loss: 2.6923733e-06
Iter: 1395 loss: 2.69230918e-06
Iter: 1396 loss: 2.69123e-06
Iter: 1397 loss: 2.69442353e-06
Iter: 1398 loss: 2.69099291e-06
Iter: 1399 loss: 2.68999e-06
Iter: 1400 loss: 2.69361499e-06
Iter: 1401 loss: 2.68970348e-06
Iter: 1402 loss: 2.68888971e-06
Iter: 1403 loss: 2.69055249e-06
Iter: 1404 loss: 2.68854546e-06
Iter: 1405 loss: 2.68766871e-06
Iter: 1406 loss: 2.68592203e-06
Iter: 1407 loss: 2.72386751e-06
Iter: 1408 loss: 2.68592953e-06
Iter: 1409 loss: 2.68452732e-06
Iter: 1410 loss: 2.7039755e-06
Iter: 1411 loss: 2.6843968e-06
Iter: 1412 loss: 2.68348549e-06
Iter: 1413 loss: 2.69494512e-06
Iter: 1414 loss: 2.68344411e-06
Iter: 1415 loss: 2.68258964e-06
Iter: 1416 loss: 2.68101985e-06
Iter: 1417 loss: 2.71064232e-06
Iter: 1418 loss: 2.68094891e-06
Iter: 1419 loss: 2.67957262e-06
Iter: 1420 loss: 2.69371822e-06
Iter: 1421 loss: 2.67958512e-06
Iter: 1422 loss: 2.67832638e-06
Iter: 1423 loss: 2.68192503e-06
Iter: 1424 loss: 2.67791393e-06
Iter: 1425 loss: 2.67702922e-06
Iter: 1426 loss: 2.67586688e-06
Iter: 1427 loss: 2.67581709e-06
Iter: 1428 loss: 2.67447535e-06
Iter: 1429 loss: 2.68753047e-06
Iter: 1430 loss: 2.6743985e-06
Iter: 1431 loss: 2.67337441e-06
Iter: 1432 loss: 2.67233372e-06
Iter: 1433 loss: 2.6721541e-06
Iter: 1434 loss: 2.67200539e-06
Iter: 1435 loss: 2.67127712e-06
Iter: 1436 loss: 2.67062296e-06
Iter: 1437 loss: 2.66981783e-06
Iter: 1438 loss: 2.66978873e-06
Iter: 1439 loss: 2.66868915e-06
Iter: 1440 loss: 2.6763837e-06
Iter: 1441 loss: 2.66859524e-06
Iter: 1442 loss: 2.66779512e-06
Iter: 1443 loss: 2.66689221e-06
Iter: 1444 loss: 2.66683082e-06
Iter: 1445 loss: 2.66535403e-06
Iter: 1446 loss: 2.6726882e-06
Iter: 1447 loss: 2.66516417e-06
Iter: 1448 loss: 2.66393226e-06
Iter: 1449 loss: 2.67371479e-06
Iter: 1450 loss: 2.66380766e-06
Iter: 1451 loss: 2.66279676e-06
Iter: 1452 loss: 2.6614448e-06
Iter: 1453 loss: 2.66129064e-06
Iter: 1454 loss: 2.6600851e-06
Iter: 1455 loss: 2.67053247e-06
Iter: 1456 loss: 2.66004827e-06
Iter: 1457 loss: 2.65907397e-06
Iter: 1458 loss: 2.66373308e-06
Iter: 1459 loss: 2.65888639e-06
Iter: 1460 loss: 2.65781864e-06
Iter: 1461 loss: 2.65607559e-06
Iter: 1462 loss: 2.65605172e-06
Iter: 1463 loss: 2.6542757e-06
Iter: 1464 loss: 2.65768631e-06
Iter: 1465 loss: 2.65373069e-06
Iter: 1466 loss: 2.65187919e-06
Iter: 1467 loss: 2.66141046e-06
Iter: 1468 loss: 2.65168069e-06
Iter: 1469 loss: 2.65048425e-06
Iter: 1470 loss: 2.66198231e-06
Iter: 1471 loss: 2.65052222e-06
Iter: 1472 loss: 2.64918049e-06
Iter: 1473 loss: 2.65090898e-06
Iter: 1474 loss: 2.64852429e-06
Iter: 1475 loss: 2.64766049e-06
Iter: 1476 loss: 2.65087328e-06
Iter: 1477 loss: 2.64749e-06
Iter: 1478 loss: 2.64655864e-06
Iter: 1479 loss: 2.64698861e-06
Iter: 1480 loss: 2.64593746e-06
Iter: 1481 loss: 2.6449577e-06
Iter: 1482 loss: 2.64595656e-06
Iter: 1483 loss: 2.64438427e-06
Iter: 1484 loss: 2.6433222e-06
Iter: 1485 loss: 2.65566678e-06
Iter: 1486 loss: 2.64339042e-06
Iter: 1487 loss: 2.64223e-06
Iter: 1488 loss: 2.64092523e-06
Iter: 1489 loss: 2.64080677e-06
Iter: 1490 loss: 2.63964466e-06
Iter: 1491 loss: 2.64646e-06
Iter: 1492 loss: 2.63948823e-06
Iter: 1493 loss: 2.63822631e-06
Iter: 1494 loss: 2.64060145e-06
Iter: 1495 loss: 2.63774041e-06
Iter: 1496 loss: 2.63628635e-06
Iter: 1497 loss: 2.6402613e-06
Iter: 1498 loss: 2.63569291e-06
Iter: 1499 loss: 2.63456832e-06
Iter: 1500 loss: 2.63475522e-06
Iter: 1501 loss: 2.63378365e-06
Iter: 1502 loss: 2.63242669e-06
Iter: 1503 loss: 2.63285756e-06
Iter: 1504 loss: 2.63142647e-06
Iter: 1505 loss: 2.63107108e-06
Iter: 1506 loss: 2.63067477e-06
Iter: 1507 loss: 2.62990034e-06
Iter: 1508 loss: 2.63077072e-06
Iter: 1509 loss: 2.62951016e-06
Iter: 1510 loss: 2.62864842e-06
Iter: 1511 loss: 2.62773938e-06
Iter: 1512 loss: 2.62761591e-06
Iter: 1513 loss: 2.62635899e-06
Iter: 1514 loss: 2.64383493e-06
Iter: 1515 loss: 2.62632898e-06
Iter: 1516 loss: 2.62542835e-06
Iter: 1517 loss: 2.6235025e-06
Iter: 1518 loss: 2.65194785e-06
Iter: 1519 loss: 2.62344838e-06
Iter: 1520 loss: 2.62322601e-06
Iter: 1521 loss: 2.62241201e-06
Iter: 1522 loss: 2.62161666e-06
Iter: 1523 loss: 2.62004778e-06
Iter: 1524 loss: 2.64720802e-06
Iter: 1525 loss: 2.62001e-06
Iter: 1526 loss: 2.61852642e-06
Iter: 1527 loss: 2.62445815e-06
Iter: 1528 loss: 2.61826699e-06
Iter: 1529 loss: 2.61697824e-06
Iter: 1530 loss: 2.62887511e-06
Iter: 1531 loss: 2.61691525e-06
Iter: 1532 loss: 2.61593141e-06
Iter: 1533 loss: 2.6170992e-06
Iter: 1534 loss: 2.61543823e-06
Iter: 1535 loss: 2.61453397e-06
Iter: 1536 loss: 2.61432433e-06
Iter: 1537 loss: 2.61374134e-06
Iter: 1538 loss: 2.61246987e-06
Iter: 1539 loss: 2.62067829e-06
Iter: 1540 loss: 2.61225182e-06
Iter: 1541 loss: 2.61148534e-06
Iter: 1542 loss: 2.61148034e-06
Iter: 1543 loss: 2.61064906e-06
Iter: 1544 loss: 2.60891306e-06
Iter: 1545 loss: 2.63289894e-06
Iter: 1546 loss: 2.60882916e-06
Iter: 1547 loss: 2.60751199e-06
Iter: 1548 loss: 2.62363255e-06
Iter: 1549 loss: 2.60752449e-06
Iter: 1550 loss: 2.60652723e-06
Iter: 1551 loss: 2.61004311e-06
Iter: 1552 loss: 2.60623824e-06
Iter: 1553 loss: 2.60538854e-06
Iter: 1554 loss: 2.60460092e-06
Iter: 1555 loss: 2.60425168e-06
Iter: 1556 loss: 2.6032194e-06
Iter: 1557 loss: 2.60316938e-06
Iter: 1558 loss: 2.60248612e-06
Iter: 1559 loss: 2.60101615e-06
Iter: 1560 loss: 2.63265269e-06
Iter: 1561 loss: 2.60105026e-06
Iter: 1562 loss: 2.59969602e-06
Iter: 1563 loss: 2.6016578e-06
Iter: 1564 loss: 2.59910894e-06
Iter: 1565 loss: 2.59777903e-06
Iter: 1566 loss: 2.59774151e-06
Iter: 1567 loss: 2.59696071e-06
Iter: 1568 loss: 2.59735839e-06
Iter: 1569 loss: 2.59632e-06
Iter: 1570 loss: 2.59536341e-06
Iter: 1571 loss: 2.59433455e-06
Iter: 1572 loss: 2.59415219e-06
Iter: 1573 loss: 2.59337799e-06
Iter: 1574 loss: 2.59323951e-06
Iter: 1575 loss: 2.59233184e-06
Iter: 1576 loss: 2.5934969e-06
Iter: 1577 loss: 2.59184617e-06
Iter: 1578 loss: 2.59095827e-06
Iter: 1579 loss: 2.59089757e-06
Iter: 1580 loss: 2.59019134e-06
Iter: 1581 loss: 2.58892942e-06
Iter: 1582 loss: 2.59400895e-06
Iter: 1583 loss: 2.58866658e-06
Iter: 1584 loss: 2.58743376e-06
Iter: 1585 loss: 2.59086164e-06
Iter: 1586 loss: 2.58701675e-06
Iter: 1587 loss: 2.58597629e-06
Iter: 1588 loss: 2.58761838e-06
Iter: 1589 loss: 2.58538876e-06
Iter: 1590 loss: 2.5839031e-06
Iter: 1591 loss: 2.59259787e-06
Iter: 1592 loss: 2.58366049e-06
Iter: 1593 loss: 2.5828474e-06
Iter: 1594 loss: 2.58207365e-06
Iter: 1595 loss: 2.58186537e-06
Iter: 1596 loss: 2.58056548e-06
Iter: 1597 loss: 2.58431646e-06
Iter: 1598 loss: 2.58017917e-06
Iter: 1599 loss: 2.57936335e-06
Iter: 1600 loss: 2.59085914e-06
Iter: 1601 loss: 2.57932425e-06
Iter: 1602 loss: 2.57845636e-06
Iter: 1603 loss: 2.57695115e-06
Iter: 1604 loss: 2.5769059e-06
Iter: 1605 loss: 2.5754066e-06
Iter: 1606 loss: 2.58131e-06
Iter: 1607 loss: 2.57501597e-06
Iter: 1608 loss: 2.57414604e-06
Iter: 1609 loss: 2.57410443e-06
Iter: 1610 loss: 2.57325473e-06
Iter: 1611 loss: 2.57199849e-06
Iter: 1612 loss: 2.57196893e-06
Iter: 1613 loss: 2.57061856e-06
Iter: 1614 loss: 2.571918e-06
Iter: 1615 loss: 2.5698937e-06
Iter: 1616 loss: 2.56823546e-06
Iter: 1617 loss: 2.5854672e-06
Iter: 1618 loss: 2.56823387e-06
Iter: 1619 loss: 2.56713747e-06
Iter: 1620 loss: 2.567102e-06
Iter: 1621 loss: 2.56633825e-06
Iter: 1622 loss: 2.56529029e-06
Iter: 1623 loss: 2.56526482e-06
Iter: 1624 loss: 2.56458588e-06
Iter: 1625 loss: 2.5646641e-06
Iter: 1626 loss: 2.56409839e-06
Iter: 1627 loss: 2.56325757e-06
Iter: 1628 loss: 2.56310045e-06
Iter: 1629 loss: 2.56252497e-06
Iter: 1630 loss: 2.56143494e-06
Iter: 1631 loss: 2.56652766e-06
Iter: 1632 loss: 2.56124804e-06
Iter: 1633 loss: 2.56031626e-06
Iter: 1634 loss: 2.56875705e-06
Iter: 1635 loss: 2.56021167e-06
Iter: 1636 loss: 2.55941723e-06
Iter: 1637 loss: 2.55834857e-06
Iter: 1638 loss: 2.5582342e-06
Iter: 1639 loss: 2.55708846e-06
Iter: 1640 loss: 2.56241401e-06
Iter: 1641 loss: 2.55687223e-06
Iter: 1642 loss: 2.55548548e-06
Iter: 1643 loss: 2.5660654e-06
Iter: 1644 loss: 2.55539589e-06
Iter: 1645 loss: 2.55460054e-06
Iter: 1646 loss: 2.55285499e-06
Iter: 1647 loss: 2.57917804e-06
Iter: 1648 loss: 2.55281111e-06
Iter: 1649 loss: 2.55180566e-06
Iter: 1650 loss: 2.55176928e-06
Iter: 1651 loss: 2.55088617e-06
Iter: 1652 loss: 2.55103146e-06
Iter: 1653 loss: 2.55025e-06
Iter: 1654 loss: 2.54909764e-06
Iter: 1655 loss: 2.55309851e-06
Iter: 1656 loss: 2.5488348e-06
Iter: 1657 loss: 2.54761676e-06
Iter: 1658 loss: 2.55509121e-06
Iter: 1659 loss: 2.54750239e-06
Iter: 1660 loss: 2.54677479e-06
Iter: 1661 loss: 2.54556744e-06
Iter: 1662 loss: 2.54554584e-06
Iter: 1663 loss: 2.54426232e-06
Iter: 1664 loss: 2.5521108e-06
Iter: 1665 loss: 2.54405e-06
Iter: 1666 loss: 2.54283373e-06
Iter: 1667 loss: 2.5478555e-06
Iter: 1668 loss: 2.54254905e-06
Iter: 1669 loss: 2.54121824e-06
Iter: 1670 loss: 2.54602674e-06
Iter: 1671 loss: 2.54081806e-06
Iter: 1672 loss: 2.54004453e-06
Iter: 1673 loss: 2.53869757e-06
Iter: 1674 loss: 2.53866347e-06
Iter: 1675 loss: 2.53830467e-06
Iter: 1676 loss: 2.53779353e-06
Iter: 1677 loss: 2.53705366e-06
Iter: 1678 loss: 2.53642315e-06
Iter: 1679 loss: 2.5362956e-06
Iter: 1680 loss: 2.53518579e-06
Iter: 1681 loss: 2.53446706e-06
Iter: 1682 loss: 2.53409189e-06
Iter: 1683 loss: 2.53296253e-06
Iter: 1684 loss: 2.53299277e-06
Iter: 1685 loss: 2.53197186e-06
Iter: 1686 loss: 2.53267353e-06
Iter: 1687 loss: 2.53133703e-06
Iter: 1688 loss: 2.53060489e-06
Iter: 1689 loss: 2.53966414e-06
Iter: 1690 loss: 2.53060239e-06
Iter: 1691 loss: 2.52986774e-06
Iter: 1692 loss: 2.52885434e-06
Iter: 1693 loss: 2.5287718e-06
Iter: 1694 loss: 2.52762948e-06
Iter: 1695 loss: 2.52933205e-06
Iter: 1696 loss: 2.52717359e-06
Iter: 1697 loss: 2.52600285e-06
Iter: 1698 loss: 2.52809559e-06
Iter: 1699 loss: 2.52529867e-06
Iter: 1700 loss: 2.52427526e-06
Iter: 1701 loss: 2.52428367e-06
Iter: 1702 loss: 2.52337759e-06
Iter: 1703 loss: 2.52246377e-06
Iter: 1704 loss: 2.52227983e-06
Iter: 1705 loss: 2.52113614e-06
Iter: 1706 loss: 2.53067606e-06
Iter: 1707 loss: 2.52104837e-06
Iter: 1708 loss: 2.52004179e-06
Iter: 1709 loss: 2.5262807e-06
Iter: 1710 loss: 2.51999268e-06
Iter: 1711 loss: 2.51913661e-06
Iter: 1712 loss: 2.51794609e-06
Iter: 1713 loss: 2.51791334e-06
Iter: 1714 loss: 2.51650727e-06
Iter: 1715 loss: 2.51549454e-06
Iter: 1716 loss: 2.51503548e-06
Iter: 1717 loss: 2.51533311e-06
Iter: 1718 loss: 2.5143e-06
Iter: 1719 loss: 2.51376423e-06
Iter: 1720 loss: 2.51313713e-06
Iter: 1721 loss: 2.51310894e-06
Iter: 1722 loss: 2.51204574e-06
Iter: 1723 loss: 2.51807342e-06
Iter: 1724 loss: 2.51183701e-06
Iter: 1725 loss: 2.5109066e-06
Iter: 1726 loss: 2.51051915e-06
Iter: 1727 loss: 2.50998346e-06
Iter: 1728 loss: 2.50896846e-06
Iter: 1729 loss: 2.50899575e-06
Iter: 1730 loss: 2.50804487e-06
Iter: 1731 loss: 2.50679841e-06
Iter: 1732 loss: 2.51416122e-06
Iter: 1733 loss: 2.50651897e-06
Iter: 1734 loss: 2.50498624e-06
Iter: 1735 loss: 2.51250094e-06
Iter: 1736 loss: 2.50476319e-06
Iter: 1737 loss: 2.50366156e-06
Iter: 1738 loss: 2.50360085e-06
Iter: 1739 loss: 2.5027407e-06
Iter: 1740 loss: 2.50166613e-06
Iter: 1741 loss: 2.51894289e-06
Iter: 1742 loss: 2.50167113e-06
Iter: 1743 loss: 2.5005902e-06
Iter: 1744 loss: 2.50138442e-06
Iter: 1745 loss: 2.49996538e-06
Iter: 1746 loss: 2.49914842e-06
Iter: 1747 loss: 2.49940672e-06
Iter: 1748 loss: 2.49857135e-06
Iter: 1749 loss: 2.49735581e-06
Iter: 1750 loss: 2.49732739e-06
Iter: 1751 loss: 2.49634786e-06
Iter: 1752 loss: 2.49560867e-06
Iter: 1753 loss: 2.49542336e-06
Iter: 1754 loss: 2.49469417e-06
Iter: 1755 loss: 2.49427308e-06
Iter: 1756 loss: 2.493902e-06
Iter: 1757 loss: 2.49289087e-06
Iter: 1758 loss: 2.50145536e-06
Iter: 1759 loss: 2.49284585e-06
Iter: 1760 loss: 2.49191294e-06
Iter: 1761 loss: 2.49063123e-06
Iter: 1762 loss: 2.49057211e-06
Iter: 1763 loss: 2.4891724e-06
Iter: 1764 loss: 2.4932292e-06
Iter: 1765 loss: 2.48860488e-06
Iter: 1766 loss: 2.48729066e-06
Iter: 1767 loss: 2.4899573e-06
Iter: 1768 loss: 2.48669448e-06
Iter: 1769 loss: 2.48559149e-06
Iter: 1770 loss: 2.48560355e-06
Iter: 1771 loss: 2.48477e-06
Iter: 1772 loss: 2.48356673e-06
Iter: 1773 loss: 2.48356332e-06
Iter: 1774 loss: 2.48305241e-06
Iter: 1775 loss: 2.48276388e-06
Iter: 1776 loss: 2.48218976e-06
Iter: 1777 loss: 2.48167657e-06
Iter: 1778 loss: 2.48153674e-06
Iter: 1779 loss: 2.48064316e-06
Iter: 1780 loss: 2.47968819e-06
Iter: 1781 loss: 2.47958474e-06
Iter: 1782 loss: 2.47824528e-06
Iter: 1783 loss: 2.48730794e-06
Iter: 1784 loss: 2.47818366e-06
Iter: 1785 loss: 2.47698517e-06
Iter: 1786 loss: 2.48552169e-06
Iter: 1787 loss: 2.4769206e-06
Iter: 1788 loss: 2.4759247e-06
Iter: 1789 loss: 2.47828257e-06
Iter: 1790 loss: 2.47556318e-06
Iter: 1791 loss: 2.47477465e-06
Iter: 1792 loss: 2.47740627e-06
Iter: 1793 loss: 2.4745575e-06
Iter: 1794 loss: 2.47354956e-06
Iter: 1795 loss: 2.47332537e-06
Iter: 1796 loss: 2.47269372e-06
Iter: 1797 loss: 2.47141702e-06
Iter: 1798 loss: 2.46939226e-06
Iter: 1799 loss: 2.46927766e-06
Iter: 1800 loss: 2.46776381e-06
Iter: 1801 loss: 2.46774698e-06
Iter: 1802 loss: 2.46679247e-06
Iter: 1803 loss: 2.47390085e-06
Iter: 1804 loss: 2.46670379e-06
Iter: 1805 loss: 2.46568538e-06
Iter: 1806 loss: 2.46638979e-06
Iter: 1807 loss: 2.46505374e-06
Iter: 1808 loss: 2.46448462e-06
Iter: 1809 loss: 2.46449986e-06
Iter: 1810 loss: 2.46391483e-06
Iter: 1811 loss: 2.46252785e-06
Iter: 1812 loss: 2.48157221e-06
Iter: 1813 loss: 2.46240097e-06
Iter: 1814 loss: 2.46106083e-06
Iter: 1815 loss: 2.46166519e-06
Iter: 1816 loss: 2.46012974e-06
Iter: 1817 loss: 2.45828e-06
Iter: 1818 loss: 2.46479522e-06
Iter: 1819 loss: 2.45775482e-06
Iter: 1820 loss: 2.4564938e-06
Iter: 1821 loss: 2.46097443e-06
Iter: 1822 loss: 2.45611477e-06
Iter: 1823 loss: 2.45559659e-06
Iter: 1824 loss: 2.4553874e-06
Iter: 1825 loss: 2.45476349e-06
Iter: 1826 loss: 2.45371166e-06
Iter: 1827 loss: 2.45373053e-06
Iter: 1828 loss: 2.45254637e-06
Iter: 1829 loss: 2.45744468e-06
Iter: 1830 loss: 2.45226875e-06
Iter: 1831 loss: 2.45136289e-06
Iter: 1832 loss: 2.45806e-06
Iter: 1833 loss: 2.45121873e-06
Iter: 1834 loss: 2.45046704e-06
Iter: 1835 loss: 2.44934836e-06
Iter: 1836 loss: 2.44927355e-06
Iter: 1837 loss: 2.44799094e-06
Iter: 1838 loss: 2.45226124e-06
Iter: 1839 loss: 2.44764078e-06
Iter: 1840 loss: 2.44663852e-06
Iter: 1841 loss: 2.44665148e-06
Iter: 1842 loss: 2.44579e-06
Iter: 1843 loss: 2.44581793e-06
Iter: 1844 loss: 2.44520425e-06
Iter: 1845 loss: 2.4439355e-06
Iter: 1846 loss: 2.44895136e-06
Iter: 1847 loss: 2.44370631e-06
Iter: 1848 loss: 2.44267176e-06
Iter: 1849 loss: 2.44210651e-06
Iter: 1850 loss: 2.44166449e-06
Iter: 1851 loss: 2.44056423e-06
Iter: 1852 loss: 2.44052944e-06
Iter: 1853 loss: 2.4396868e-06
Iter: 1854 loss: 2.43816794e-06
Iter: 1855 loss: 2.44945022e-06
Iter: 1856 loss: 2.43799423e-06
Iter: 1857 loss: 2.43692193e-06
Iter: 1858 loss: 2.44239504e-06
Iter: 1859 loss: 2.43678687e-06
Iter: 1860 loss: 2.43638897e-06
Iter: 1861 loss: 2.43636259e-06
Iter: 1862 loss: 2.43583872e-06
Iter: 1863 loss: 2.43452951e-06
Iter: 1864 loss: 2.44304078e-06
Iter: 1865 loss: 2.43424961e-06
Iter: 1866 loss: 2.4328674e-06
Iter: 1867 loss: 2.44230932e-06
Iter: 1868 loss: 2.43277827e-06
Iter: 1869 loss: 2.43167051e-06
Iter: 1870 loss: 2.43872296e-06
Iter: 1871 loss: 2.43157569e-06
Iter: 1872 loss: 2.43042405e-06
Iter: 1873 loss: 2.43042541e-06
Iter: 1874 loss: 2.42955912e-06
Iter: 1875 loss: 2.42843953e-06
Iter: 1876 loss: 2.42946885e-06
Iter: 1877 loss: 2.42775104e-06
Iter: 1878 loss: 2.42686656e-06
Iter: 1879 loss: 2.42678311e-06
Iter: 1880 loss: 2.42613919e-06
Iter: 1881 loss: 2.42580586e-06
Iter: 1882 loss: 2.42547e-06
Iter: 1883 loss: 2.4247e-06
Iter: 1884 loss: 2.43048271e-06
Iter: 1885 loss: 2.42465603e-06
Iter: 1886 loss: 2.42386272e-06
Iter: 1887 loss: 2.42277315e-06
Iter: 1888 loss: 2.42271e-06
Iter: 1889 loss: 2.42163742e-06
Iter: 1890 loss: 2.42359e-06
Iter: 1891 loss: 2.42110241e-06
Iter: 1892 loss: 2.42007e-06
Iter: 1893 loss: 2.42908891e-06
Iter: 1894 loss: 2.41995144e-06
Iter: 1895 loss: 2.41891439e-06
Iter: 1896 loss: 2.42432179e-06
Iter: 1897 loss: 2.41872885e-06
Iter: 1898 loss: 2.41782254e-06
Iter: 1899 loss: 2.4189153e-06
Iter: 1900 loss: 2.41732823e-06
Iter: 1901 loss: 2.41654357e-06
Iter: 1902 loss: 2.41581711e-06
Iter: 1903 loss: 2.41559474e-06
Iter: 1904 loss: 2.41447333e-06
Iter: 1905 loss: 2.43146542e-06
Iter: 1906 loss: 2.41446924e-06
Iter: 1907 loss: 2.41355588e-06
Iter: 1908 loss: 2.41378939e-06
Iter: 1909 loss: 2.41298017e-06
Iter: 1910 loss: 2.41195335e-06
Iter: 1911 loss: 2.41370753e-06
Iter: 1912 loss: 2.41154521e-06
Iter: 1913 loss: 2.4106771e-06
Iter: 1914 loss: 2.41069392e-06
Iter: 1915 loss: 2.41009593e-06
Iter: 1916 loss: 2.40908548e-06
Iter: 1917 loss: 2.40905638e-06
Iter: 1918 loss: 2.408309e-06
Iter: 1919 loss: 2.40828058e-06
Iter: 1920 loss: 2.40767804e-06
Iter: 1921 loss: 2.4061917e-06
Iter: 1922 loss: 2.42387478e-06
Iter: 1923 loss: 2.40604072e-06
Iter: 1924 loss: 2.40463214e-06
Iter: 1925 loss: 2.41176576e-06
Iter: 1926 loss: 2.40433565e-06
Iter: 1927 loss: 2.40314193e-06
Iter: 1928 loss: 2.40981672e-06
Iter: 1929 loss: 2.40291683e-06
Iter: 1930 loss: 2.40183772e-06
Iter: 1931 loss: 2.41546263e-06
Iter: 1932 loss: 2.40186387e-06
Iter: 1933 loss: 2.40133272e-06
Iter: 1934 loss: 2.39988731e-06
Iter: 1935 loss: 2.41476164e-06
Iter: 1936 loss: 2.39976703e-06
Iter: 1937 loss: 2.39832389e-06
Iter: 1938 loss: 2.41277166e-06
Iter: 1939 loss: 2.39832821e-06
Iter: 1940 loss: 2.39734027e-06
Iter: 1941 loss: 2.40080249e-06
Iter: 1942 loss: 2.39703741e-06
Iter: 1943 loss: 2.39593e-06
Iter: 1944 loss: 2.39937776e-06
Iter: 1945 loss: 2.39563178e-06
Iter: 1946 loss: 2.39489145e-06
Iter: 1947 loss: 2.39597057e-06
Iter: 1948 loss: 2.39448332e-06
Iter: 1949 loss: 2.39366773e-06
Iter: 1950 loss: 2.40495865e-06
Iter: 1951 loss: 2.39366409e-06
Iter: 1952 loss: 2.39317637e-06
Iter: 1953 loss: 2.39229166e-06
Iter: 1954 loss: 2.39228166e-06
Iter: 1955 loss: 2.39112569e-06
Iter: 1956 loss: 2.39837186e-06
Iter: 1957 loss: 2.39112774e-06
Iter: 1958 loss: 2.39004112e-06
Iter: 1959 loss: 2.39067094e-06
Iter: 1960 loss: 2.38938924e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.2/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.6
+ date
Sun Nov  8 15:57:40 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 1.6 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82dfbc048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82df0e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82df0ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82df5de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82df5df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82df5dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82de2fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82dec79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82ded3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82dec7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82ddb8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82ddad7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82ddada60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82dd907b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff82dd90400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8088c19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8088c1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8087f4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8087bb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8088c1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff808887510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e4024950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e4052048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff808855510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff808855ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e405a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d0634598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d0624598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d06241e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d05d4bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d06879d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d05a0620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d05a0a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d058e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d058e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7d055d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 202, in <module>
    grads = tape.gradient(loss, model.trainable_weights)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1266, in _backward_function_wrapper
    processed_args, remapped_captures)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input is not invertible.
	 [[node gradients/MatrixDeterminant_grad/MatrixInverse (defined at biholoNN_train.py:200) ]] [Op:__inference___backward_volume_form_4446_8945]

Function call stack:
__backward_volume_form_4446

+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.6/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/300_100_100_100_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.6 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi1.6/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f143e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1533e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1533d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f147c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f147cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f13f22f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f13c6048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f13747b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f138ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1331a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f13008c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f12edf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1304840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1304bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1292b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f124e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f127e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1304b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1201950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f12012f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9f1204f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbfacbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbf72840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbf87840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbf80620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbf80b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbef1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbf0b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbf0b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbed6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbe947b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbe9a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbe9a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbe509d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbdfa840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9dbe9a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.28158e-05
Iter: 2 loss: 4.72897555e-05
Iter: 3 loss: 2.83565278e-05
Iter: 4 loss: 2.66955722e-05
Iter: 5 loss: 2.81453285e-05
Iter: 6 loss: 2.57218344e-05
Iter: 7 loss: 2.39228757e-05
Iter: 8 loss: 2.78017942e-05
Iter: 9 loss: 2.32332131e-05
Iter: 10 loss: 2.13844469e-05
Iter: 11 loss: 2.17482047e-05
Iter: 12 loss: 2.0012867e-05
Iter: 13 loss: 1.83110205e-05
Iter: 14 loss: 2.67904361e-05
Iter: 15 loss: 1.80241368e-05
Iter: 16 loss: 1.66652135e-05
Iter: 17 loss: 2.03813561e-05
Iter: 18 loss: 1.62242195e-05
Iter: 19 loss: 1.55788312e-05
Iter: 20 loss: 1.63761069e-05
Iter: 21 loss: 1.52416824e-05
Iter: 22 loss: 1.43867819e-05
Iter: 23 loss: 1.79276885e-05
Iter: 24 loss: 1.42015006e-05
Iter: 25 loss: 1.35409455e-05
Iter: 26 loss: 1.5916512e-05
Iter: 27 loss: 1.33736221e-05
Iter: 28 loss: 1.2821255e-05
Iter: 29 loss: 1.22987649e-05
Iter: 30 loss: 1.21709e-05
Iter: 31 loss: 1.16622432e-05
Iter: 32 loss: 1.16608044e-05
Iter: 33 loss: 1.13310944e-05
Iter: 34 loss: 1.10008959e-05
Iter: 35 loss: 1.09338944e-05
Iter: 36 loss: 1.05545769e-05
Iter: 37 loss: 1.05535373e-05
Iter: 38 loss: 1.02278245e-05
Iter: 39 loss: 1.41049086e-05
Iter: 40 loss: 1.0223559e-05
Iter: 41 loss: 1.00702819e-05
Iter: 42 loss: 1.00836314e-05
Iter: 43 loss: 9.95158916e-06
Iter: 44 loss: 9.72831549e-06
Iter: 45 loss: 1.01671048e-05
Iter: 46 loss: 9.63634375e-06
Iter: 47 loss: 9.40754126e-06
Iter: 48 loss: 9.35929256e-06
Iter: 49 loss: 9.2092414e-06
Iter: 50 loss: 8.97662176e-06
Iter: 51 loss: 1.26480172e-05
Iter: 52 loss: 8.97663e-06
Iter: 53 loss: 8.84997644e-06
Iter: 54 loss: 8.66233313e-06
Iter: 55 loss: 8.65781e-06
Iter: 56 loss: 8.37048174e-06
Iter: 57 loss: 9.83338941e-06
Iter: 58 loss: 8.32358819e-06
Iter: 59 loss: 8.13877796e-06
Iter: 60 loss: 9.80653567e-06
Iter: 61 loss: 8.1301514e-06
Iter: 62 loss: 8.01506485e-06
Iter: 63 loss: 7.9947522e-06
Iter: 64 loss: 7.91647471e-06
Iter: 65 loss: 7.76211527e-06
Iter: 66 loss: 7.83637643e-06
Iter: 67 loss: 7.6581664e-06
Iter: 68 loss: 7.50704839e-06
Iter: 69 loss: 7.50693061e-06
Iter: 70 loss: 7.46155e-06
Iter: 71 loss: 7.46035903e-06
Iter: 72 loss: 7.40662745e-06
Iter: 73 loss: 7.27648239e-06
Iter: 74 loss: 8.65179663e-06
Iter: 75 loss: 7.26192229e-06
Iter: 76 loss: 7.16789418e-06
Iter: 77 loss: 7.1652712e-06
Iter: 78 loss: 7.10489167e-06
Iter: 79 loss: 7.06930177e-06
Iter: 80 loss: 7.0441738e-06
Iter: 81 loss: 6.95108838e-06
Iter: 82 loss: 7.12132305e-06
Iter: 83 loss: 6.91089735e-06
Iter: 84 loss: 6.82676364e-06
Iter: 85 loss: 7.52418873e-06
Iter: 86 loss: 6.82169502e-06
Iter: 87 loss: 6.75838783e-06
Iter: 88 loss: 6.66714914e-06
Iter: 89 loss: 6.66428105e-06
Iter: 90 loss: 6.58393492e-06
Iter: 91 loss: 6.58366025e-06
Iter: 92 loss: 6.51522805e-06
Iter: 93 loss: 6.51564e-06
Iter: 94 loss: 6.46076342e-06
Iter: 95 loss: 6.38276e-06
Iter: 96 loss: 6.82219434e-06
Iter: 97 loss: 6.37182438e-06
Iter: 98 loss: 6.32305273e-06
Iter: 99 loss: 6.25959592e-06
Iter: 100 loss: 6.25543862e-06
Iter: 101 loss: 6.20690753e-06
Iter: 102 loss: 6.19887305e-06
Iter: 103 loss: 6.16389207e-06
Iter: 104 loss: 6.58813633e-06
Iter: 105 loss: 6.16366106e-06
Iter: 106 loss: 6.14069495e-06
Iter: 107 loss: 6.09727658e-06
Iter: 108 loss: 7.03651813e-06
Iter: 109 loss: 6.09696963e-06
Iter: 110 loss: 6.04266734e-06
Iter: 111 loss: 6.47121306e-06
Iter: 112 loss: 6.03887065e-06
Iter: 113 loss: 6.00184194e-06
Iter: 114 loss: 5.98094e-06
Iter: 115 loss: 5.96488e-06
Iter: 116 loss: 5.91479829e-06
Iter: 117 loss: 6.12151598e-06
Iter: 118 loss: 5.90410309e-06
Iter: 119 loss: 5.86606529e-06
Iter: 120 loss: 6.18540435e-06
Iter: 121 loss: 5.86394071e-06
Iter: 122 loss: 5.83786186e-06
Iter: 123 loss: 5.78432537e-06
Iter: 124 loss: 6.72510305e-06
Iter: 125 loss: 5.78322033e-06
Iter: 126 loss: 5.7350162e-06
Iter: 127 loss: 5.73444322e-06
Iter: 128 loss: 5.7010634e-06
Iter: 129 loss: 5.73182251e-06
Iter: 130 loss: 5.681738e-06
Iter: 131 loss: 5.64063612e-06
Iter: 132 loss: 5.69610347e-06
Iter: 133 loss: 5.62023251e-06
Iter: 134 loss: 5.57988687e-06
Iter: 135 loss: 5.59323416e-06
Iter: 136 loss: 5.55127826e-06
Iter: 137 loss: 5.55758243e-06
Iter: 138 loss: 5.53021619e-06
Iter: 139 loss: 5.51423727e-06
Iter: 140 loss: 5.51101402e-06
Iter: 141 loss: 5.50024561e-06
Iter: 142 loss: 5.47787204e-06
Iter: 143 loss: 5.4758857e-06
Iter: 144 loss: 5.45915373e-06
Iter: 145 loss: 5.41863346e-06
Iter: 146 loss: 5.55329825e-06
Iter: 147 loss: 5.40757901e-06
Iter: 148 loss: 5.38243785e-06
Iter: 149 loss: 5.38189624e-06
Iter: 150 loss: 5.36206699e-06
Iter: 151 loss: 5.32820377e-06
Iter: 152 loss: 5.54013423e-06
Iter: 153 loss: 5.32414833e-06
Iter: 154 loss: 5.29795761e-06
Iter: 155 loss: 5.37947562e-06
Iter: 156 loss: 5.29027966e-06
Iter: 157 loss: 5.26447047e-06
Iter: 158 loss: 5.2529731e-06
Iter: 159 loss: 5.23973449e-06
Iter: 160 loss: 5.21428865e-06
Iter: 161 loss: 5.58979264e-06
Iter: 162 loss: 5.21427501e-06
Iter: 163 loss: 5.19049536e-06
Iter: 164 loss: 5.18321622e-06
Iter: 165 loss: 5.16900218e-06
Iter: 166 loss: 5.14589e-06
Iter: 167 loss: 5.24942971e-06
Iter: 168 loss: 5.14132716e-06
Iter: 169 loss: 5.11948292e-06
Iter: 170 loss: 5.16498494e-06
Iter: 171 loss: 5.11077087e-06
Iter: 172 loss: 5.08907397e-06
Iter: 173 loss: 5.08899211e-06
Iter: 174 loss: 5.07753793e-06
Iter: 175 loss: 5.0622557e-06
Iter: 176 loss: 5.06137803e-06
Iter: 177 loss: 5.04394029e-06
Iter: 178 loss: 5.22484061e-06
Iter: 179 loss: 5.04353193e-06
Iter: 180 loss: 5.03071624e-06
Iter: 181 loss: 5.02498551e-06
Iter: 182 loss: 5.0186527e-06
Iter: 183 loss: 4.99859652e-06
Iter: 184 loss: 4.99595262e-06
Iter: 185 loss: 4.98172676e-06
Iter: 186 loss: 4.96350913e-06
Iter: 187 loss: 4.96323173e-06
Iter: 188 loss: 4.95223912e-06
Iter: 189 loss: 4.95440236e-06
Iter: 190 loss: 4.9442242e-06
Iter: 191 loss: 4.92619074e-06
Iter: 192 loss: 4.919807e-06
Iter: 193 loss: 4.90983e-06
Iter: 194 loss: 4.89771173e-06
Iter: 195 loss: 4.89688728e-06
Iter: 196 loss: 4.88538171e-06
Iter: 197 loss: 4.86131921e-06
Iter: 198 loss: 5.27341854e-06
Iter: 199 loss: 4.86081262e-06
Iter: 200 loss: 4.83898657e-06
Iter: 201 loss: 5.03960655e-06
Iter: 202 loss: 4.83797885e-06
Iter: 203 loss: 4.82397581e-06
Iter: 204 loss: 5.02723651e-06
Iter: 205 loss: 4.82383848e-06
Iter: 206 loss: 4.80898689e-06
Iter: 207 loss: 4.80202743e-06
Iter: 208 loss: 4.79446589e-06
Iter: 209 loss: 4.78347647e-06
Iter: 210 loss: 4.84393695e-06
Iter: 211 loss: 4.78176571e-06
Iter: 212 loss: 4.76851937e-06
Iter: 213 loss: 4.76533205e-06
Iter: 214 loss: 4.75698698e-06
Iter: 215 loss: 4.74092e-06
Iter: 216 loss: 4.78767151e-06
Iter: 217 loss: 4.73593354e-06
Iter: 218 loss: 4.72209e-06
Iter: 219 loss: 4.73894943e-06
Iter: 220 loss: 4.71487556e-06
Iter: 221 loss: 4.69996212e-06
Iter: 222 loss: 4.84777593e-06
Iter: 223 loss: 4.69938641e-06
Iter: 224 loss: 4.69035422e-06
Iter: 225 loss: 4.67809241e-06
Iter: 226 loss: 4.67746941e-06
Iter: 227 loss: 4.65611538e-06
Iter: 228 loss: 4.73071259e-06
Iter: 229 loss: 4.65053608e-06
Iter: 230 loss: 4.63811648e-06
Iter: 231 loss: 4.82590531e-06
Iter: 232 loss: 4.6380992e-06
Iter: 233 loss: 4.62922299e-06
Iter: 234 loss: 4.61371837e-06
Iter: 235 loss: 4.61380068e-06
Iter: 236 loss: 4.60119054e-06
Iter: 237 loss: 4.60114461e-06
Iter: 238 loss: 4.59136163e-06
Iter: 239 loss: 4.69015367e-06
Iter: 240 loss: 4.59104103e-06
Iter: 241 loss: 4.58542354e-06
Iter: 242 loss: 4.57474289e-06
Iter: 243 loss: 4.80796643e-06
Iter: 244 loss: 4.57454644e-06
Iter: 245 loss: 4.56602083e-06
Iter: 246 loss: 4.56588623e-06
Iter: 247 loss: 4.55902864e-06
Iter: 248 loss: 4.54636938e-06
Iter: 249 loss: 4.84124803e-06
Iter: 250 loss: 4.54638803e-06
Iter: 251 loss: 4.53414805e-06
Iter: 252 loss: 4.6341429e-06
Iter: 253 loss: 4.53345683e-06
Iter: 254 loss: 4.524818e-06
Iter: 255 loss: 4.53837083e-06
Iter: 256 loss: 4.52087033e-06
Iter: 257 loss: 4.50872449e-06
Iter: 258 loss: 4.53928533e-06
Iter: 259 loss: 4.50443e-06
Iter: 260 loss: 4.4966323e-06
Iter: 261 loss: 4.5062161e-06
Iter: 262 loss: 4.49255094e-06
Iter: 263 loss: 4.4807357e-06
Iter: 264 loss: 4.49732761e-06
Iter: 265 loss: 4.47481125e-06
Iter: 266 loss: 4.46279273e-06
Iter: 267 loss: 4.56991074e-06
Iter: 268 loss: 4.46212107e-06
Iter: 269 loss: 4.45481055e-06
Iter: 270 loss: 4.45151363e-06
Iter: 271 loss: 4.4477e-06
Iter: 272 loss: 4.44452871e-06
Iter: 273 loss: 4.44243278e-06
Iter: 274 loss: 4.43715362e-06
Iter: 275 loss: 4.42322153e-06
Iter: 276 loss: 4.53578696e-06
Iter: 277 loss: 4.42066585e-06
Iter: 278 loss: 4.4090184e-06
Iter: 279 loss: 4.58433533e-06
Iter: 280 loss: 4.4090034e-06
Iter: 281 loss: 4.40127542e-06
Iter: 282 loss: 4.44166e-06
Iter: 283 loss: 4.40010399e-06
Iter: 284 loss: 4.39463474e-06
Iter: 285 loss: 4.3838163e-06
Iter: 286 loss: 4.5955112e-06
Iter: 287 loss: 4.38364259e-06
Iter: 288 loss: 4.37150493e-06
Iter: 289 loss: 4.46646663e-06
Iter: 290 loss: 4.37062818e-06
Iter: 291 loss: 4.36007304e-06
Iter: 292 loss: 4.39172891e-06
Iter: 293 loss: 4.35678066e-06
Iter: 294 loss: 4.34750746e-06
Iter: 295 loss: 4.3778532e-06
Iter: 296 loss: 4.34486401e-06
Iter: 297 loss: 4.33671812e-06
Iter: 298 loss: 4.33761033e-06
Iter: 299 loss: 4.33045e-06
Iter: 300 loss: 4.32194884e-06
Iter: 301 loss: 4.44918442e-06
Iter: 302 loss: 4.32191155e-06
Iter: 303 loss: 4.3163468e-06
Iter: 304 loss: 4.31928947e-06
Iter: 305 loss: 4.31276931e-06
Iter: 306 loss: 4.30479304e-06
Iter: 307 loss: 4.31970693e-06
Iter: 308 loss: 4.30160935e-06
Iter: 309 loss: 4.29289958e-06
Iter: 310 loss: 4.41602788e-06
Iter: 311 loss: 4.29288866e-06
Iter: 312 loss: 4.2895449e-06
Iter: 313 loss: 4.28132216e-06
Iter: 314 loss: 4.37448944e-06
Iter: 315 loss: 4.28042495e-06
Iter: 316 loss: 4.27239229e-06
Iter: 317 loss: 4.27229952e-06
Iter: 318 loss: 4.26663428e-06
Iter: 319 loss: 4.26624638e-06
Iter: 320 loss: 4.2620668e-06
Iter: 321 loss: 4.25416874e-06
Iter: 322 loss: 4.2455622e-06
Iter: 323 loss: 4.2442166e-06
Iter: 324 loss: 4.23898655e-06
Iter: 325 loss: 4.2378324e-06
Iter: 326 loss: 4.23269603e-06
Iter: 327 loss: 4.22840913e-06
Iter: 328 loss: 4.2269603e-06
Iter: 329 loss: 4.21830828e-06
Iter: 330 loss: 4.25473945e-06
Iter: 331 loss: 4.21655568e-06
Iter: 332 loss: 4.20985816e-06
Iter: 333 loss: 4.21339564e-06
Iter: 334 loss: 4.20562947e-06
Iter: 335 loss: 4.19565731e-06
Iter: 336 loss: 4.26326687e-06
Iter: 337 loss: 4.19471326e-06
Iter: 338 loss: 4.18837226e-06
Iter: 339 loss: 4.19633034e-06
Iter: 340 loss: 4.18505851e-06
Iter: 341 loss: 4.18007676e-06
Iter: 342 loss: 4.17985711e-06
Iter: 343 loss: 4.17569618e-06
Iter: 344 loss: 4.17051388e-06
Iter: 345 loss: 4.17013598e-06
Iter: 346 loss: 4.16488911e-06
Iter: 347 loss: 4.17693218e-06
Iter: 348 loss: 4.16288367e-06
Iter: 349 loss: 4.15429349e-06
Iter: 350 loss: 4.16133798e-06
Iter: 351 loss: 4.14921806e-06
Iter: 352 loss: 4.14310034e-06
Iter: 353 loss: 4.16649436e-06
Iter: 354 loss: 4.14161241e-06
Iter: 355 loss: 4.1366493e-06
Iter: 356 loss: 4.13335511e-06
Iter: 357 loss: 4.13143334e-06
Iter: 358 loss: 4.12214195e-06
Iter: 359 loss: 4.19822209e-06
Iter: 360 loss: 4.12164945e-06
Iter: 361 loss: 4.11618203e-06
Iter: 362 loss: 4.11964493e-06
Iter: 363 loss: 4.11262045e-06
Iter: 364 loss: 4.1057674e-06
Iter: 365 loss: 4.1252697e-06
Iter: 366 loss: 4.1035496e-06
Iter: 367 loss: 4.09765471e-06
Iter: 368 loss: 4.12709505e-06
Iter: 369 loss: 4.09668974e-06
Iter: 370 loss: 4.09062113e-06
Iter: 371 loss: 4.10194434e-06
Iter: 372 loss: 4.08793494e-06
Iter: 373 loss: 4.08486358e-06
Iter: 374 loss: 4.08477717e-06
Iter: 375 loss: 4.08151391e-06
Iter: 376 loss: 4.07986863e-06
Iter: 377 loss: 4.07847892e-06
Iter: 378 loss: 4.07370862e-06
Iter: 379 loss: 4.07205744e-06
Iter: 380 loss: 4.06948129e-06
Iter: 381 loss: 4.06569097e-06
Iter: 382 loss: 4.06545041e-06
Iter: 383 loss: 4.06236e-06
Iter: 384 loss: 4.05579067e-06
Iter: 385 loss: 4.15812383e-06
Iter: 386 loss: 4.05557785e-06
Iter: 387 loss: 4.04843058e-06
Iter: 388 loss: 4.07686457e-06
Iter: 389 loss: 4.04691036e-06
Iter: 390 loss: 4.04029743e-06
Iter: 391 loss: 4.07617381e-06
Iter: 392 loss: 4.03951253e-06
Iter: 393 loss: 4.03385047e-06
Iter: 394 loss: 4.05526771e-06
Iter: 395 loss: 4.03254489e-06
Iter: 396 loss: 4.0275072e-06
Iter: 397 loss: 4.02313071e-06
Iter: 398 loss: 4.02169826e-06
Iter: 399 loss: 4.0160312e-06
Iter: 400 loss: 4.10060875e-06
Iter: 401 loss: 4.01602301e-06
Iter: 402 loss: 4.01193165e-06
Iter: 403 loss: 4.0206769e-06
Iter: 404 loss: 4.01046464e-06
Iter: 405 loss: 4.00558292e-06
Iter: 406 loss: 4.01633179e-06
Iter: 407 loss: 4.00367117e-06
Iter: 408 loss: 4.00008867e-06
Iter: 409 loss: 4.00007411e-06
Iter: 410 loss: 3.99784813e-06
Iter: 411 loss: 3.99217424e-06
Iter: 412 loss: 4.04628418e-06
Iter: 413 loss: 3.99152214e-06
Iter: 414 loss: 3.98569227e-06
Iter: 415 loss: 4.04385764e-06
Iter: 416 loss: 3.98552629e-06
Iter: 417 loss: 3.98035172e-06
Iter: 418 loss: 3.99515739e-06
Iter: 419 loss: 3.97888925e-06
Iter: 420 loss: 3.97449821e-06
Iter: 421 loss: 3.96902e-06
Iter: 422 loss: 3.96856e-06
Iter: 423 loss: 3.96392e-06
Iter: 424 loss: 4.01201396e-06
Iter: 425 loss: 3.96383803e-06
Iter: 426 loss: 3.95941242e-06
Iter: 427 loss: 3.96956329e-06
Iter: 428 loss: 3.95761253e-06
Iter: 429 loss: 3.9533e-06
Iter: 430 loss: 3.96536325e-06
Iter: 431 loss: 3.95185907e-06
Iter: 432 loss: 3.94725e-06
Iter: 433 loss: 3.94330664e-06
Iter: 434 loss: 3.94216e-06
Iter: 435 loss: 3.93862592e-06
Iter: 436 loss: 3.93816799e-06
Iter: 437 loss: 3.9355e-06
Iter: 438 loss: 3.93525806e-06
Iter: 439 loss: 3.93338087e-06
Iter: 440 loss: 3.92951824e-06
Iter: 441 loss: 3.96675523e-06
Iter: 442 loss: 3.92930906e-06
Iter: 443 loss: 3.92565e-06
Iter: 444 loss: 3.92542097e-06
Iter: 445 loss: 3.92266247e-06
Iter: 446 loss: 3.91916228e-06
Iter: 447 loss: 3.91936919e-06
Iter: 448 loss: 3.91634603e-06
Iter: 449 loss: 3.91173035e-06
Iter: 450 loss: 3.96632095e-06
Iter: 451 loss: 3.91153844e-06
Iter: 452 loss: 3.90878904e-06
Iter: 453 loss: 3.9069896e-06
Iter: 454 loss: 3.90601144e-06
Iter: 455 loss: 3.90146215e-06
Iter: 456 loss: 3.89958041e-06
Iter: 457 loss: 3.89710931e-06
Iter: 458 loss: 3.89233946e-06
Iter: 459 loss: 3.89227125e-06
Iter: 460 loss: 3.88800527e-06
Iter: 461 loss: 3.88594e-06
Iter: 462 loss: 3.88368335e-06
Iter: 463 loss: 3.87783803e-06
Iter: 464 loss: 3.91129925e-06
Iter: 465 loss: 3.87703949e-06
Iter: 466 loss: 3.87332966e-06
Iter: 467 loss: 3.87851878e-06
Iter: 468 loss: 3.87144382e-06
Iter: 469 loss: 3.86633974e-06
Iter: 470 loss: 3.89359502e-06
Iter: 471 loss: 3.86561942e-06
Iter: 472 loss: 3.86256397e-06
Iter: 473 loss: 3.89179149e-06
Iter: 474 loss: 3.86252714e-06
Iter: 475 loss: 3.85918383e-06
Iter: 476 loss: 3.86013562e-06
Iter: 477 loss: 3.85693875e-06
Iter: 478 loss: 3.85327e-06
Iter: 479 loss: 3.85372641e-06
Iter: 480 loss: 3.85042949e-06
Iter: 481 loss: 3.84753184e-06
Iter: 482 loss: 3.89057277e-06
Iter: 483 loss: 3.84760096e-06
Iter: 484 loss: 3.84479381e-06
Iter: 485 loss: 3.84218492e-06
Iter: 486 loss: 3.84159557e-06
Iter: 487 loss: 3.83732458e-06
Iter: 488 loss: 3.83844645e-06
Iter: 489 loss: 3.83415227e-06
Iter: 490 loss: 3.82870439e-06
Iter: 491 loss: 3.86435204e-06
Iter: 492 loss: 3.8280532e-06
Iter: 493 loss: 3.82463895e-06
Iter: 494 loss: 3.85492785e-06
Iter: 495 loss: 3.82446069e-06
Iter: 496 loss: 3.82174039e-06
Iter: 497 loss: 3.81912287e-06
Iter: 498 loss: 3.81855e-06
Iter: 499 loss: 3.81475274e-06
Iter: 500 loss: 3.84235318e-06
Iter: 501 loss: 3.81441282e-06
Iter: 502 loss: 3.81110476e-06
Iter: 503 loss: 3.81550217e-06
Iter: 504 loss: 3.80947131e-06
Iter: 505 loss: 3.80509118e-06
Iter: 506 loss: 3.83216684e-06
Iter: 507 loss: 3.80448046e-06
Iter: 508 loss: 3.8020562e-06
Iter: 509 loss: 3.84038685e-06
Iter: 510 loss: 3.80207848e-06
Iter: 511 loss: 3.80068718e-06
Iter: 512 loss: 3.79744e-06
Iter: 513 loss: 3.83424958e-06
Iter: 514 loss: 3.79706398e-06
Iter: 515 loss: 3.79269704e-06
Iter: 516 loss: 3.81305426e-06
Iter: 517 loss: 3.79188555e-06
Iter: 518 loss: 3.78796153e-06
Iter: 519 loss: 3.81221321e-06
Iter: 520 loss: 3.78733398e-06
Iter: 521 loss: 3.78473032e-06
Iter: 522 loss: 3.78028335e-06
Iter: 523 loss: 3.78026402e-06
Iter: 524 loss: 3.77628749e-06
Iter: 525 loss: 3.81359769e-06
Iter: 526 loss: 3.77625338e-06
Iter: 527 loss: 3.77292963e-06
Iter: 528 loss: 3.78150912e-06
Iter: 529 loss: 3.7716909e-06
Iter: 530 loss: 3.76829485e-06
Iter: 531 loss: 3.78313507e-06
Iter: 532 loss: 3.76762637e-06
Iter: 533 loss: 3.7652203e-06
Iter: 534 loss: 3.76220055e-06
Iter: 535 loss: 3.76197431e-06
Iter: 536 loss: 3.75626769e-06
Iter: 537 loss: 3.78653544e-06
Iter: 538 loss: 3.75546529e-06
Iter: 539 loss: 3.75251398e-06
Iter: 540 loss: 3.75254035e-06
Iter: 541 loss: 3.75066702e-06
Iter: 542 loss: 3.75875948e-06
Iter: 543 loss: 3.75010541e-06
Iter: 544 loss: 3.74770116e-06
Iter: 545 loss: 3.7438308e-06
Iter: 546 loss: 3.74379943e-06
Iter: 547 loss: 3.74063484e-06
Iter: 548 loss: 3.76116827e-06
Iter: 549 loss: 3.7402715e-06
Iter: 550 loss: 3.7373145e-06
Iter: 551 loss: 3.7525449e-06
Iter: 552 loss: 3.7369407e-06
Iter: 553 loss: 3.73447278e-06
Iter: 554 loss: 3.73362218e-06
Iter: 555 loss: 3.7321463e-06
Iter: 556 loss: 3.72904356e-06
Iter: 557 loss: 3.72761519e-06
Iter: 558 loss: 3.72601244e-06
Iter: 559 loss: 3.72197519e-06
Iter: 560 loss: 3.78247728e-06
Iter: 561 loss: 3.72196564e-06
Iter: 562 loss: 3.71922215e-06
Iter: 563 loss: 3.72376257e-06
Iter: 564 loss: 3.71795318e-06
Iter: 565 loss: 3.71434567e-06
Iter: 566 loss: 3.71304827e-06
Iter: 567 loss: 3.71114061e-06
Iter: 568 loss: 3.70734688e-06
Iter: 569 loss: 3.7389409e-06
Iter: 570 loss: 3.70705584e-06
Iter: 571 loss: 3.70351768e-06
Iter: 572 loss: 3.71243414e-06
Iter: 573 loss: 3.70230919e-06
Iter: 574 loss: 3.69929217e-06
Iter: 575 loss: 3.74247247e-06
Iter: 576 loss: 3.69937356e-06
Iter: 577 loss: 3.6973272e-06
Iter: 578 loss: 3.69929285e-06
Iter: 579 loss: 3.69608074e-06
Iter: 580 loss: 3.69386339e-06
Iter: 581 loss: 3.69047279e-06
Iter: 582 loss: 3.69043732e-06
Iter: 583 loss: 3.688486e-06
Iter: 584 loss: 3.68805036e-06
Iter: 585 loss: 3.6863521e-06
Iter: 586 loss: 3.68413e-06
Iter: 587 loss: 3.68391557e-06
Iter: 588 loss: 3.68049632e-06
Iter: 589 loss: 3.68361657e-06
Iter: 590 loss: 3.67855159e-06
Iter: 591 loss: 3.67550797e-06
Iter: 592 loss: 3.69302779e-06
Iter: 593 loss: 3.67504981e-06
Iter: 594 loss: 3.67188318e-06
Iter: 595 loss: 3.68264023e-06
Iter: 596 loss: 3.67109305e-06
Iter: 597 loss: 3.66811651e-06
Iter: 598 loss: 3.67146413e-06
Iter: 599 loss: 3.66673908e-06
Iter: 600 loss: 3.66330278e-06
Iter: 601 loss: 3.66648737e-06
Iter: 602 loss: 3.66128097e-06
Iter: 603 loss: 3.65852839e-06
Iter: 604 loss: 3.70015755e-06
Iter: 605 loss: 3.65850292e-06
Iter: 606 loss: 3.65612186e-06
Iter: 607 loss: 3.66428435e-06
Iter: 608 loss: 3.65568258e-06
Iter: 609 loss: 3.65278675e-06
Iter: 610 loss: 3.65679443e-06
Iter: 611 loss: 3.65138021e-06
Iter: 612 loss: 3.64880134e-06
Iter: 613 loss: 3.65113124e-06
Iter: 614 loss: 3.64730136e-06
Iter: 615 loss: 3.64475136e-06
Iter: 616 loss: 3.65171286e-06
Iter: 617 loss: 3.64404423e-06
Iter: 618 loss: 3.64080643e-06
Iter: 619 loss: 3.64974244e-06
Iter: 620 loss: 3.63962408e-06
Iter: 621 loss: 3.63685513e-06
Iter: 622 loss: 3.63387358e-06
Iter: 623 loss: 3.6333231e-06
Iter: 624 loss: 3.62979608e-06
Iter: 625 loss: 3.66560698e-06
Iter: 626 loss: 3.62964761e-06
Iter: 627 loss: 3.62708738e-06
Iter: 628 loss: 3.63333061e-06
Iter: 629 loss: 3.62619312e-06
Iter: 630 loss: 3.62237506e-06
Iter: 631 loss: 3.62175842e-06
Iter: 632 loss: 3.61924367e-06
Iter: 633 loss: 3.61570642e-06
Iter: 634 loss: 3.63669983e-06
Iter: 635 loss: 3.61536831e-06
Iter: 636 loss: 3.61239609e-06
Iter: 637 loss: 3.61581328e-06
Iter: 638 loss: 3.61073694e-06
Iter: 639 loss: 3.60821923e-06
Iter: 640 loss: 3.60812101e-06
Iter: 641 loss: 3.60635886e-06
Iter: 642 loss: 3.61297498e-06
Iter: 643 loss: 3.60593231e-06
Iter: 644 loss: 3.60403146e-06
Iter: 645 loss: 3.60123067e-06
Iter: 646 loss: 3.60119975e-06
Iter: 647 loss: 3.59844148e-06
Iter: 648 loss: 3.61860293e-06
Iter: 649 loss: 3.59823048e-06
Iter: 650 loss: 3.59602654e-06
Iter: 651 loss: 3.60651188e-06
Iter: 652 loss: 3.59564365e-06
Iter: 653 loss: 3.59329306e-06
Iter: 654 loss: 3.58951365e-06
Iter: 655 loss: 3.58952343e-06
Iter: 656 loss: 3.58585885e-06
Iter: 657 loss: 3.59606611e-06
Iter: 658 loss: 3.58472334e-06
Iter: 659 loss: 3.58085094e-06
Iter: 660 loss: 3.60548756e-06
Iter: 661 loss: 3.58037346e-06
Iter: 662 loss: 3.57773547e-06
Iter: 663 loss: 3.59730552e-06
Iter: 664 loss: 3.57758699e-06
Iter: 665 loss: 3.57568797e-06
Iter: 666 loss: 3.57206045e-06
Iter: 667 loss: 3.65293818e-06
Iter: 668 loss: 3.57205045e-06
Iter: 669 loss: 3.56902069e-06
Iter: 670 loss: 3.56908504e-06
Iter: 671 loss: 3.56707324e-06
Iter: 672 loss: 3.57702197e-06
Iter: 673 loss: 3.56661781e-06
Iter: 674 loss: 3.56436158e-06
Iter: 675 loss: 3.57202134e-06
Iter: 676 loss: 3.56363171e-06
Iter: 677 loss: 3.5616124e-06
Iter: 678 loss: 3.56582063e-06
Iter: 679 loss: 3.56079022e-06
Iter: 680 loss: 3.55924453e-06
Iter: 681 loss: 3.55772318e-06
Iter: 682 loss: 3.5574908e-06
Iter: 683 loss: 3.55435941e-06
Iter: 684 loss: 3.57465274e-06
Iter: 685 loss: 3.554042e-06
Iter: 686 loss: 3.55149223e-06
Iter: 687 loss: 3.55501516e-06
Iter: 688 loss: 3.55026555e-06
Iter: 689 loss: 3.54793951e-06
Iter: 690 loss: 3.54525946e-06
Iter: 691 loss: 3.54488657e-06
Iter: 692 loss: 3.54187159e-06
Iter: 693 loss: 3.57674139e-06
Iter: 694 loss: 3.54181748e-06
Iter: 695 loss: 3.53905352e-06
Iter: 696 loss: 3.54698136e-06
Iter: 697 loss: 3.53828409e-06
Iter: 698 loss: 3.53557471e-06
Iter: 699 loss: 3.53906603e-06
Iter: 700 loss: 3.53410678e-06
Iter: 701 loss: 3.53119071e-06
Iter: 702 loss: 3.52973984e-06
Iter: 703 loss: 3.52828829e-06
Iter: 704 loss: 3.52562165e-06
Iter: 705 loss: 3.52547568e-06
Iter: 706 loss: 3.52372194e-06
Iter: 707 loss: 3.54026088e-06
Iter: 708 loss: 3.52373399e-06
Iter: 709 loss: 3.52230904e-06
Iter: 710 loss: 3.52152119e-06
Iter: 711 loss: 3.52093957e-06
Iter: 712 loss: 3.51879862e-06
Iter: 713 loss: 3.52047596e-06
Iter: 714 loss: 3.51744643e-06
Iter: 715 loss: 3.51516428e-06
Iter: 716 loss: 3.52515053e-06
Iter: 717 loss: 3.51477092e-06
Iter: 718 loss: 3.51209337e-06
Iter: 719 loss: 3.51781409e-06
Iter: 720 loss: 3.5110595e-06
Iter: 721 loss: 3.50915889e-06
Iter: 722 loss: 3.51019344e-06
Iter: 723 loss: 3.50780942e-06
Iter: 724 loss: 3.50529103e-06
Iter: 725 loss: 3.50640084e-06
Iter: 726 loss: 3.50347864e-06
Iter: 727 loss: 3.50097253e-06
Iter: 728 loss: 3.53512223e-06
Iter: 729 loss: 3.50088499e-06
Iter: 730 loss: 3.49853076e-06
Iter: 731 loss: 3.49856668e-06
Iter: 732 loss: 3.4966838e-06
Iter: 733 loss: 3.4940374e-06
Iter: 734 loss: 3.50311757e-06
Iter: 735 loss: 3.49333754e-06
Iter: 736 loss: 3.4906775e-06
Iter: 737 loss: 3.49064862e-06
Iter: 738 loss: 3.48851586e-06
Iter: 739 loss: 3.48655158e-06
Iter: 740 loss: 3.48609387e-06
Iter: 741 loss: 3.48451772e-06
Iter: 742 loss: 3.48675349e-06
Iter: 743 loss: 3.48378171e-06
Iter: 744 loss: 3.48190338e-06
Iter: 745 loss: 3.4801642e-06
Iter: 746 loss: 3.47976538e-06
Iter: 747 loss: 3.47715718e-06
Iter: 748 loss: 3.49527613e-06
Iter: 749 loss: 3.47691866e-06
Iter: 750 loss: 3.47486207e-06
Iter: 751 loss: 3.48428671e-06
Iter: 752 loss: 3.4745176e-06
Iter: 753 loss: 3.47249488e-06
Iter: 754 loss: 3.47000241e-06
Iter: 755 loss: 3.46966e-06
Iter: 756 loss: 3.46656202e-06
Iter: 757 loss: 3.47194964e-06
Iter: 758 loss: 3.4652337e-06
Iter: 759 loss: 3.46113984e-06
Iter: 760 loss: 3.47338937e-06
Iter: 761 loss: 3.45993453e-06
Iter: 762 loss: 3.45730905e-06
Iter: 763 loss: 3.45733952e-06
Iter: 764 loss: 3.45561716e-06
Iter: 765 loss: 3.45240278e-06
Iter: 766 loss: 3.52028019e-06
Iter: 767 loss: 3.45238391e-06
Iter: 768 loss: 3.44952832e-06
Iter: 769 loss: 3.48754111e-06
Iter: 770 loss: 3.44952173e-06
Iter: 771 loss: 3.44801e-06
Iter: 772 loss: 3.46344905e-06
Iter: 773 loss: 3.44785872e-06
Iter: 774 loss: 3.4461309e-06
Iter: 775 loss: 3.44528507e-06
Iter: 776 loss: 3.44434829e-06
Iter: 777 loss: 3.44186242e-06
Iter: 778 loss: 3.44993032e-06
Iter: 779 loss: 3.44118121e-06
Iter: 780 loss: 3.43912234e-06
Iter: 781 loss: 3.4397865e-06
Iter: 782 loss: 3.43772e-06
Iter: 783 loss: 3.43530837e-06
Iter: 784 loss: 3.45729154e-06
Iter: 785 loss: 3.43519423e-06
Iter: 786 loss: 3.43338024e-06
Iter: 787 loss: 3.43403667e-06
Iter: 788 loss: 3.43210127e-06
Iter: 789 loss: 3.429255e-06
Iter: 790 loss: 3.42669637e-06
Iter: 791 loss: 3.42613816e-06
Iter: 792 loss: 3.42323551e-06
Iter: 793 loss: 3.45602939e-06
Iter: 794 loss: 3.42310977e-06
Iter: 795 loss: 3.42072576e-06
Iter: 796 loss: 3.4274658e-06
Iter: 797 loss: 3.41994087e-06
Iter: 798 loss: 3.4171951e-06
Iter: 799 loss: 3.42426893e-06
Iter: 800 loss: 3.41615441e-06
Iter: 801 loss: 3.41411328e-06
Iter: 802 loss: 3.41468e-06
Iter: 803 loss: 3.41269174e-06
Iter: 804 loss: 3.41030773e-06
Iter: 805 loss: 3.43161878e-06
Iter: 806 loss: 3.41015698e-06
Iter: 807 loss: 3.4079967e-06
Iter: 808 loss: 3.42258159e-06
Iter: 809 loss: 3.40780389e-06
Iter: 810 loss: 3.40591259e-06
Iter: 811 loss: 3.40547967e-06
Iter: 812 loss: 3.40422503e-06
Iter: 813 loss: 3.40235329e-06
Iter: 814 loss: 3.41069904e-06
Iter: 815 loss: 3.40184124e-06
Iter: 816 loss: 3.40030829e-06
Iter: 817 loss: 3.40193969e-06
Iter: 818 loss: 3.39933194e-06
Iter: 819 loss: 3.39670169e-06
Iter: 820 loss: 3.40404154e-06
Iter: 821 loss: 3.3959659e-06
Iter: 822 loss: 3.39387134e-06
Iter: 823 loss: 3.39412736e-06
Iter: 824 loss: 3.39236931e-06
Iter: 825 loss: 3.38909376e-06
Iter: 826 loss: 3.38922564e-06
Iter: 827 loss: 3.38658492e-06
Iter: 828 loss: 3.38382893e-06
Iter: 829 loss: 3.42283715e-06
Iter: 830 loss: 3.38387485e-06
Iter: 831 loss: 3.38126529e-06
Iter: 832 loss: 3.38552e-06
Iter: 833 loss: 3.38019981e-06
Iter: 834 loss: 3.37781512e-06
Iter: 835 loss: 3.38239829e-06
Iter: 836 loss: 3.37670599e-06
Iter: 837 loss: 3.37423216e-06
Iter: 838 loss: 3.37526558e-06
Iter: 839 loss: 3.37247752e-06
Iter: 840 loss: 3.37207894e-06
Iter: 841 loss: 3.37098163e-06
Iter: 842 loss: 3.36999324e-06
Iter: 843 loss: 3.36926405e-06
Iter: 844 loss: 3.36897483e-06
Iter: 845 loss: 3.36720655e-06
Iter: 846 loss: 3.36651829e-06
Iter: 847 loss: 3.36556172e-06
Iter: 848 loss: 3.36297057e-06
Iter: 849 loss: 3.37704796e-06
Iter: 850 loss: 3.36262383e-06
Iter: 851 loss: 3.36059816e-06
Iter: 852 loss: 3.37134406e-06
Iter: 853 loss: 3.36032963e-06
Iter: 854 loss: 3.35830828e-06
Iter: 855 loss: 3.35527147e-06
Iter: 856 loss: 3.35531831e-06
Iter: 857 loss: 3.35239929e-06
Iter: 858 loss: 3.37196775e-06
Iter: 859 loss: 3.35215964e-06
Iter: 860 loss: 3.34969536e-06
Iter: 861 loss: 3.35001846e-06
Iter: 862 loss: 3.34787819e-06
Iter: 863 loss: 3.34604533e-06
Iter: 864 loss: 3.34583456e-06
Iter: 865 loss: 3.34441802e-06
Iter: 866 loss: 3.3417673e-06
Iter: 867 loss: 3.39812163e-06
Iter: 868 loss: 3.3416793e-06
Iter: 869 loss: 3.33847606e-06
Iter: 870 loss: 3.35728737e-06
Iter: 871 loss: 3.33803723e-06
Iter: 872 loss: 3.33719368e-06
Iter: 873 loss: 3.3368708e-06
Iter: 874 loss: 3.33571302e-06
Iter: 875 loss: 3.33457137e-06
Iter: 876 loss: 3.33431285e-06
Iter: 877 loss: 3.33243702e-06
Iter: 878 loss: 3.33684693e-06
Iter: 879 loss: 3.33177923e-06
Iter: 880 loss: 3.32976697e-06
Iter: 881 loss: 3.3315273e-06
Iter: 882 loss: 3.32856644e-06
Iter: 883 loss: 3.32641639e-06
Iter: 884 loss: 3.34772631e-06
Iter: 885 loss: 3.32629497e-06
Iter: 886 loss: 3.32474883e-06
Iter: 887 loss: 3.32758782e-06
Iter: 888 loss: 3.32429386e-06
Iter: 889 loss: 3.32281343e-06
Iter: 890 loss: 3.32015179e-06
Iter: 891 loss: 3.37879396e-06
Iter: 892 loss: 3.32021909e-06
Iter: 893 loss: 3.31715955e-06
Iter: 894 loss: 3.3456115e-06
Iter: 895 loss: 3.31702813e-06
Iter: 896 loss: 3.3148674e-06
Iter: 897 loss: 3.31963815e-06
Iter: 898 loss: 3.31395404e-06
Iter: 899 loss: 3.31101637e-06
Iter: 900 loss: 3.32007721e-06
Iter: 901 loss: 3.31012961e-06
Iter: 902 loss: 3.30778403e-06
Iter: 903 loss: 3.30687567e-06
Iter: 904 loss: 3.3056167e-06
Iter: 905 loss: 3.30360422e-06
Iter: 906 loss: 3.30345961e-06
Iter: 907 loss: 3.30175885e-06
Iter: 908 loss: 3.30975649e-06
Iter: 909 loss: 3.30134435e-06
Iter: 910 loss: 3.29983777e-06
Iter: 911 loss: 3.29794e-06
Iter: 912 loss: 3.29780892e-06
Iter: 913 loss: 3.2956109e-06
Iter: 914 loss: 3.31414844e-06
Iter: 915 loss: 3.29550289e-06
Iter: 916 loss: 3.29418663e-06
Iter: 917 loss: 3.29699014e-06
Iter: 918 loss: 3.29365821e-06
Iter: 919 loss: 3.29180557e-06
Iter: 920 loss: 3.29461227e-06
Iter: 921 loss: 3.29081e-06
Iter: 922 loss: 3.28908322e-06
Iter: 923 loss: 3.29007889e-06
Iter: 924 loss: 3.28789361e-06
Iter: 925 loss: 3.28562169e-06
Iter: 926 loss: 3.2866792e-06
Iter: 927 loss: 3.283968e-06
Iter: 928 loss: 3.28166379e-06
Iter: 929 loss: 3.30087869e-06
Iter: 930 loss: 3.28156898e-06
Iter: 931 loss: 3.27951375e-06
Iter: 932 loss: 3.28486294e-06
Iter: 933 loss: 3.27872704e-06
Iter: 934 loss: 3.27662224e-06
Iter: 935 loss: 3.28223064e-06
Iter: 936 loss: 3.27586963e-06
Iter: 937 loss: 3.27403404e-06
Iter: 938 loss: 3.27192242e-06
Iter: 939 loss: 3.27168254e-06
Iter: 940 loss: 3.27260432e-06
Iter: 941 loss: 3.2703017e-06
Iter: 942 loss: 3.26956615e-06
Iter: 943 loss: 3.2681919e-06
Iter: 944 loss: 3.2681678e-06
Iter: 945 loss: 3.26635904e-06
Iter: 946 loss: 3.26806321e-06
Iter: 947 loss: 3.26528811e-06
Iter: 948 loss: 3.26316604e-06
Iter: 949 loss: 3.27420889e-06
Iter: 950 loss: 3.26288205e-06
Iter: 951 loss: 3.26134295e-06
Iter: 952 loss: 3.26841337e-06
Iter: 953 loss: 3.2609305e-06
Iter: 954 loss: 3.25907195e-06
Iter: 955 loss: 3.25707902e-06
Iter: 956 loss: 3.25676729e-06
Iter: 957 loss: 3.25464953e-06
Iter: 958 loss: 3.2686994e-06
Iter: 959 loss: 3.25440942e-06
Iter: 960 loss: 3.25271071e-06
Iter: 961 loss: 3.25091014e-06
Iter: 962 loss: 3.25048541e-06
Iter: 963 loss: 3.24879488e-06
Iter: 964 loss: 3.248648e-06
Iter: 965 loss: 3.2473522e-06
Iter: 966 loss: 3.24820212e-06
Iter: 967 loss: 3.24652092e-06
Iter: 968 loss: 3.24494545e-06
Iter: 969 loss: 3.24507573e-06
Iter: 970 loss: 3.24352777e-06
Iter: 971 loss: 3.24196571e-06
Iter: 972 loss: 3.25995279e-06
Iter: 973 loss: 3.24183839e-06
Iter: 974 loss: 3.23987069e-06
Iter: 975 loss: 3.24109646e-06
Iter: 976 loss: 3.23842823e-06
Iter: 977 loss: 3.23713311e-06
Iter: 978 loss: 3.24153393e-06
Iter: 979 loss: 3.23686777e-06
Iter: 980 loss: 3.23543554e-06
Iter: 981 loss: 3.23429e-06
Iter: 982 loss: 3.23366544e-06
Iter: 983 loss: 3.23125664e-06
Iter: 984 loss: 3.24931261e-06
Iter: 985 loss: 3.23104678e-06
Iter: 986 loss: 3.22917685e-06
Iter: 987 loss: 3.23598988e-06
Iter: 988 loss: 3.22884034e-06
Iter: 989 loss: 3.22719234e-06
Iter: 990 loss: 3.22394726e-06
Iter: 991 loss: 3.29234899e-06
Iter: 992 loss: 3.22400501e-06
Iter: 993 loss: 3.22134247e-06
Iter: 994 loss: 3.25791e-06
Iter: 995 loss: 3.22140022e-06
Iter: 996 loss: 3.2196167e-06
Iter: 997 loss: 3.22041956e-06
Iter: 998 loss: 3.21842299e-06
Iter: 999 loss: 3.21620655e-06
Iter: 1000 loss: 3.24088842e-06
Iter: 1001 loss: 3.21612674e-06
Iter: 1002 loss: 3.21484458e-06
Iter: 1003 loss: 3.21315724e-06
Iter: 1004 loss: 3.21306061e-06
Iter: 1005 loss: 3.21052426e-06
Iter: 1006 loss: 3.2243006e-06
Iter: 1007 loss: 3.21014841e-06
Iter: 1008 loss: 3.20912432e-06
Iter: 1009 loss: 3.20885283e-06
Iter: 1010 loss: 3.20820163e-06
Iter: 1011 loss: 3.20620256e-06
Iter: 1012 loss: 3.22360825e-06
Iter: 1013 loss: 3.20591653e-06
Iter: 1014 loss: 3.20427944e-06
Iter: 1015 loss: 3.2286589e-06
Iter: 1016 loss: 3.20427353e-06
Iter: 1017 loss: 3.20284175e-06
Iter: 1018 loss: 3.2017424e-06
Iter: 1019 loss: 3.2011892e-06
Iter: 1020 loss: 3.19875744e-06
Iter: 1021 loss: 3.21740299e-06
Iter: 1022 loss: 3.19852052e-06
Iter: 1023 loss: 3.19701326e-06
Iter: 1024 loss: 3.19930518e-06
Iter: 1025 loss: 3.19626088e-06
Iter: 1026 loss: 3.19480137e-06
Iter: 1027 loss: 3.19319042e-06
Iter: 1028 loss: 3.1928796e-06
Iter: 1029 loss: 3.19043261e-06
Iter: 1030 loss: 3.20398067e-06
Iter: 1031 loss: 3.19006813e-06
Iter: 1032 loss: 3.18761954e-06
Iter: 1033 loss: 3.19235392e-06
Iter: 1034 loss: 3.18654111e-06
Iter: 1035 loss: 3.18373259e-06
Iter: 1036 loss: 3.19781066e-06
Iter: 1037 loss: 3.18315961e-06
Iter: 1038 loss: 3.18123e-06
Iter: 1039 loss: 3.18129605e-06
Iter: 1040 loss: 3.17969648e-06
Iter: 1041 loss: 3.17917738e-06
Iter: 1042 loss: 3.1785903e-06
Iter: 1043 loss: 3.17747117e-06
Iter: 1044 loss: 3.17545073e-06
Iter: 1045 loss: 3.17546119e-06
Iter: 1046 loss: 3.17324202e-06
Iter: 1047 loss: 3.17625791e-06
Iter: 1048 loss: 3.17193326e-06
Iter: 1049 loss: 3.17044078e-06
Iter: 1050 loss: 3.17040553e-06
Iter: 1051 loss: 3.16936871e-06
Iter: 1052 loss: 3.16922569e-06
Iter: 1053 loss: 3.16835781e-06
Iter: 1054 loss: 3.16662613e-06
Iter: 1055 loss: 3.17501645e-06
Iter: 1056 loss: 3.16626392e-06
Iter: 1057 loss: 3.16501837e-06
Iter: 1058 loss: 3.16406022e-06
Iter: 1059 loss: 3.16370324e-06
Iter: 1060 loss: 3.1614436e-06
Iter: 1061 loss: 3.16579735e-06
Iter: 1062 loss: 3.16057276e-06
Iter: 1063 loss: 3.15823218e-06
Iter: 1064 loss: 3.16621163e-06
Iter: 1065 loss: 3.15774332e-06
Iter: 1066 loss: 3.15529451e-06
Iter: 1067 loss: 3.16812066e-06
Iter: 1068 loss: 3.15509055e-06
Iter: 1069 loss: 3.15342913e-06
Iter: 1070 loss: 3.15814123e-06
Iter: 1071 loss: 3.15293823e-06
Iter: 1072 loss: 3.15150032e-06
Iter: 1073 loss: 3.15122816e-06
Iter: 1074 loss: 3.15018224e-06
Iter: 1075 loss: 3.14861154e-06
Iter: 1076 loss: 3.14851741e-06
Iter: 1077 loss: 3.14778e-06
Iter: 1078 loss: 3.14568979e-06
Iter: 1079 loss: 3.15808052e-06
Iter: 1080 loss: 3.14515478e-06
Iter: 1081 loss: 3.14289355e-06
Iter: 1082 loss: 3.17254489e-06
Iter: 1083 loss: 3.1428076e-06
Iter: 1084 loss: 3.14163572e-06
Iter: 1085 loss: 3.14863564e-06
Iter: 1086 loss: 3.14142494e-06
Iter: 1087 loss: 3.14025738e-06
Iter: 1088 loss: 3.14013323e-06
Iter: 1089 loss: 3.13930764e-06
Iter: 1090 loss: 3.13687178e-06
Iter: 1091 loss: 3.13761552e-06
Iter: 1092 loss: 3.13523515e-06
Iter: 1093 loss: 3.13332794e-06
Iter: 1094 loss: 3.13608143e-06
Iter: 1095 loss: 3.13254759e-06
Iter: 1096 loss: 3.13007558e-06
Iter: 1097 loss: 3.14137742e-06
Iter: 1098 loss: 3.12978409e-06
Iter: 1099 loss: 3.12831889e-06
Iter: 1100 loss: 3.13838791e-06
Iter: 1101 loss: 3.12806242e-06
Iter: 1102 loss: 3.12656084e-06
Iter: 1103 loss: 3.12648262e-06
Iter: 1104 loss: 3.12547718e-06
Iter: 1105 loss: 3.12332645e-06
Iter: 1106 loss: 3.12838915e-06
Iter: 1107 loss: 3.12263819e-06
Iter: 1108 loss: 3.12164593e-06
Iter: 1109 loss: 3.12155225e-06
Iter: 1110 loss: 3.12040061e-06
Iter: 1111 loss: 3.11840267e-06
Iter: 1112 loss: 3.11840654e-06
Iter: 1113 loss: 3.11708709e-06
Iter: 1114 loss: 3.11974145e-06
Iter: 1115 loss: 3.11656891e-06
Iter: 1116 loss: 3.11450958e-06
Iter: 1117 loss: 3.11877398e-06
Iter: 1118 loss: 3.11364556e-06
Iter: 1119 loss: 3.11188069e-06
Iter: 1120 loss: 3.12664156e-06
Iter: 1121 loss: 3.11177223e-06
Iter: 1122 loss: 3.11072199e-06
Iter: 1123 loss: 3.11391022e-06
Iter: 1124 loss: 3.1103732e-06
Iter: 1125 loss: 3.1092809e-06
Iter: 1126 loss: 3.10692735e-06
Iter: 1127 loss: 3.15695593e-06
Iter: 1128 loss: 3.10693486e-06
Iter: 1129 loss: 3.10528139e-06
Iter: 1130 loss: 3.12200132e-06
Iter: 1131 loss: 3.10529254e-06
Iter: 1132 loss: 3.10365476e-06
Iter: 1133 loss: 3.10409382e-06
Iter: 1134 loss: 3.10240785e-06
Iter: 1135 loss: 3.10011501e-06
Iter: 1136 loss: 3.1150787e-06
Iter: 1137 loss: 3.09989719e-06
Iter: 1138 loss: 3.09811367e-06
Iter: 1139 loss: 3.09878715e-06
Iter: 1140 loss: 3.09682855e-06
Iter: 1141 loss: 3.0952e-06
Iter: 1142 loss: 3.10892528e-06
Iter: 1143 loss: 3.09507e-06
Iter: 1144 loss: 3.09345364e-06
Iter: 1145 loss: 3.10657333e-06
Iter: 1146 loss: 3.09334791e-06
Iter: 1147 loss: 3.09244683e-06
Iter: 1148 loss: 3.09012034e-06
Iter: 1149 loss: 3.11545818e-06
Iter: 1150 loss: 3.08998278e-06
Iter: 1151 loss: 3.08840663e-06
Iter: 1152 loss: 3.08842391e-06
Iter: 1153 loss: 3.08697167e-06
Iter: 1154 loss: 3.08850349e-06
Iter: 1155 loss: 3.08623521e-06
Iter: 1156 loss: 3.08481276e-06
Iter: 1157 loss: 3.09402776e-06
Iter: 1158 loss: 3.08462904e-06
Iter: 1159 loss: 3.08348376e-06
Iter: 1160 loss: 3.08356925e-06
Iter: 1161 loss: 3.08264453e-06
Iter: 1162 loss: 3.08084941e-06
Iter: 1163 loss: 3.08100925e-06
Iter: 1164 loss: 3.07964092e-06
Iter: 1165 loss: 3.0775609e-06
Iter: 1166 loss: 3.08421522e-06
Iter: 1167 loss: 3.07708456e-06
Iter: 1168 loss: 3.07523806e-06
Iter: 1169 loss: 3.09163943e-06
Iter: 1170 loss: 3.07512801e-06
Iter: 1171 loss: 3.07375285e-06
Iter: 1172 loss: 3.0748206e-06
Iter: 1173 loss: 3.07302025e-06
Iter: 1174 loss: 3.07113078e-06
Iter: 1175 loss: 3.0728047e-06
Iter: 1176 loss: 3.0699839e-06
Iter: 1177 loss: 3.06974061e-06
Iter: 1178 loss: 3.06920401e-06
Iter: 1179 loss: 3.0684032e-06
Iter: 1180 loss: 3.06641914e-06
Iter: 1181 loss: 3.08434846e-06
Iter: 1182 loss: 3.06617403e-06
Iter: 1183 loss: 3.06427819e-06
Iter: 1184 loss: 3.07334312e-06
Iter: 1185 loss: 3.06399079e-06
Iter: 1186 loss: 3.06266747e-06
Iter: 1187 loss: 3.07340247e-06
Iter: 1188 loss: 3.06258471e-06
Iter: 1189 loss: 3.06106222e-06
Iter: 1190 loss: 3.05941876e-06
Iter: 1191 loss: 3.05921048e-06
Iter: 1192 loss: 3.0575402e-06
Iter: 1193 loss: 3.05752155e-06
Iter: 1194 loss: 3.05667072e-06
Iter: 1195 loss: 3.05516051e-06
Iter: 1196 loss: 3.05519e-06
Iter: 1197 loss: 3.05294634e-06
Iter: 1198 loss: 3.05706158e-06
Iter: 1199 loss: 3.05198591e-06
Iter: 1200 loss: 3.05033268e-06
Iter: 1201 loss: 3.05874164e-06
Iter: 1202 loss: 3.05004824e-06
Iter: 1203 loss: 3.04803393e-06
Iter: 1204 loss: 3.05294225e-06
Iter: 1205 loss: 3.04726859e-06
Iter: 1206 loss: 3.04600781e-06
Iter: 1207 loss: 3.05004096e-06
Iter: 1208 loss: 3.04559308e-06
Iter: 1209 loss: 3.04397781e-06
Iter: 1210 loss: 3.04554487e-06
Iter: 1211 loss: 3.04310674e-06
Iter: 1212 loss: 3.04165724e-06
Iter: 1213 loss: 3.04161358e-06
Iter: 1214 loss: 3.04098739e-06
Iter: 1215 loss: 3.03910792e-06
Iter: 1216 loss: 3.04913897e-06
Iter: 1217 loss: 3.03864181e-06
Iter: 1218 loss: 3.03673028e-06
Iter: 1219 loss: 3.05798153e-06
Iter: 1220 loss: 3.03663455e-06
Iter: 1221 loss: 3.03544539e-06
Iter: 1222 loss: 3.04833861e-06
Iter: 1223 loss: 3.03538354e-06
Iter: 1224 loss: 3.03437537e-06
Iter: 1225 loss: 3.03339129e-06
Iter: 1226 loss: 3.03312527e-06
Iter: 1227 loss: 3.03146317e-06
Iter: 1228 loss: 3.0445658e-06
Iter: 1229 loss: 3.03136903e-06
Iter: 1230 loss: 3.03012098e-06
Iter: 1231 loss: 3.02823469e-06
Iter: 1232 loss: 3.0281451e-06
Iter: 1233 loss: 3.02651347e-06
Iter: 1234 loss: 3.04849937e-06
Iter: 1235 loss: 3.02646e-06
Iter: 1236 loss: 3.02538342e-06
Iter: 1237 loss: 3.0253741e-06
Iter: 1238 loss: 3.02461785e-06
Iter: 1239 loss: 3.02244143e-06
Iter: 1240 loss: 3.03018487e-06
Iter: 1241 loss: 3.02196622e-06
Iter: 1242 loss: 3.02053127e-06
Iter: 1243 loss: 3.02265971e-06
Iter: 1244 loss: 3.01989621e-06
Iter: 1245 loss: 3.01872387e-06
Iter: 1246 loss: 3.03140177e-06
Iter: 1247 loss: 3.01865521e-06
Iter: 1248 loss: 3.01756245e-06
Iter: 1249 loss: 3.02283297e-06
Iter: 1250 loss: 3.01731461e-06
Iter: 1251 loss: 3.01650698e-06
Iter: 1252 loss: 3.01491582e-06
Iter: 1253 loss: 3.04153536e-06
Iter: 1254 loss: 3.01483351e-06
Iter: 1255 loss: 3.01358432e-06
Iter: 1256 loss: 3.02421859e-06
Iter: 1257 loss: 3.01355e-06
Iter: 1258 loss: 3.01228056e-06
Iter: 1259 loss: 3.01631303e-06
Iter: 1260 loss: 3.01185332e-06
Iter: 1261 loss: 3.01045111e-06
Iter: 1262 loss: 3.01213822e-06
Iter: 1263 loss: 3.00975535e-06
Iter: 1264 loss: 3.00851525e-06
Iter: 1265 loss: 3.01571663e-06
Iter: 1266 loss: 3.00830652e-06
Iter: 1267 loss: 3.00714419e-06
Iter: 1268 loss: 3.00515603e-06
Iter: 1269 loss: 3.00510851e-06
Iter: 1270 loss: 3.00336251e-06
Iter: 1271 loss: 3.01453451e-06
Iter: 1272 loss: 3.00313832e-06
Iter: 1273 loss: 3.00167494e-06
Iter: 1274 loss: 3.00826605e-06
Iter: 1275 loss: 3.00125726e-06
Iter: 1276 loss: 2.99980888e-06
Iter: 1277 loss: 3.00832903e-06
Iter: 1278 loss: 2.99976296e-06
Iter: 1279 loss: 2.99852218e-06
Iter: 1280 loss: 2.99767362e-06
Iter: 1281 loss: 2.99717931e-06
Iter: 1282 loss: 2.99687827e-06
Iter: 1283 loss: 2.99656267e-06
Iter: 1284 loss: 2.99587214e-06
Iter: 1285 loss: 2.99614862e-06
Iter: 1286 loss: 2.99546e-06
Iter: 1287 loss: 2.99464136e-06
Iter: 1288 loss: 2.99300268e-06
Iter: 1289 loss: 3.02199351e-06
Iter: 1290 loss: 2.99289832e-06
Iter: 1291 loss: 2.99167118e-06
Iter: 1292 loss: 3.01057594e-06
Iter: 1293 loss: 2.99164549e-06
Iter: 1294 loss: 2.99033945e-06
Iter: 1295 loss: 2.99417707e-06
Iter: 1296 loss: 2.99007843e-06
Iter: 1297 loss: 2.98877194e-06
Iter: 1298 loss: 2.98929945e-06
Iter: 1299 loss: 2.98790656e-06
Iter: 1300 loss: 2.98652958e-06
Iter: 1301 loss: 2.99479143e-06
Iter: 1302 loss: 2.98632813e-06
Iter: 1303 loss: 2.98515533e-06
Iter: 1304 loss: 2.98436271e-06
Iter: 1305 loss: 2.98390569e-06
Iter: 1306 loss: 2.98251689e-06
Iter: 1307 loss: 2.98334635e-06
Iter: 1308 loss: 2.98164196e-06
Iter: 1309 loss: 2.98023338e-06
Iter: 1310 loss: 3.00299803e-06
Iter: 1311 loss: 2.98024634e-06
Iter: 1312 loss: 2.97927454e-06
Iter: 1313 loss: 2.98272403e-06
Iter: 1314 loss: 2.97893439e-06
Iter: 1315 loss: 2.97792985e-06
Iter: 1316 loss: 2.97823021e-06
Iter: 1317 loss: 2.9771345e-06
Iter: 1318 loss: 2.9764492e-06
Iter: 1319 loss: 2.97640736e-06
Iter: 1320 loss: 2.97573979e-06
Iter: 1321 loss: 2.97426914e-06
Iter: 1322 loss: 2.99663e-06
Iter: 1323 loss: 2.97420956e-06
Iter: 1324 loss: 2.97264933e-06
Iter: 1325 loss: 2.97630027e-06
Iter: 1326 loss: 2.97205634e-06
Iter: 1327 loss: 2.97106499e-06
Iter: 1328 loss: 2.98098394e-06
Iter: 1329 loss: 2.97095039e-06
Iter: 1330 loss: 2.96993949e-06
Iter: 1331 loss: 2.97284328e-06
Iter: 1332 loss: 2.96951589e-06
Iter: 1333 loss: 2.96860617e-06
Iter: 1334 loss: 2.96830603e-06
Iter: 1335 loss: 2.96770122e-06
Iter: 1336 loss: 2.96605867e-06
Iter: 1337 loss: 2.97173483e-06
Iter: 1338 loss: 2.96562234e-06
Iter: 1339 loss: 2.96442158e-06
Iter: 1340 loss: 2.96916232e-06
Iter: 1341 loss: 2.96422331e-06
Iter: 1342 loss: 2.96320923e-06
Iter: 1343 loss: 2.96098233e-06
Iter: 1344 loss: 2.98381792e-06
Iter: 1345 loss: 2.96074495e-06
Iter: 1346 loss: 2.95995096e-06
Iter: 1347 loss: 2.95934251e-06
Iter: 1348 loss: 2.95813834e-06
Iter: 1349 loss: 2.958908e-06
Iter: 1350 loss: 2.95740392e-06
Iter: 1351 loss: 2.95623204e-06
Iter: 1352 loss: 2.96651888e-06
Iter: 1353 loss: 2.95613199e-06
Iter: 1354 loss: 2.95535165e-06
Iter: 1355 loss: 2.96420558e-06
Iter: 1356 loss: 2.95528662e-06
Iter: 1357 loss: 2.95486689e-06
Iter: 1358 loss: 2.95336235e-06
Iter: 1359 loss: 2.96215603e-06
Iter: 1360 loss: 2.95302561e-06
Iter: 1361 loss: 2.95135851e-06
Iter: 1362 loss: 2.96244843e-06
Iter: 1363 loss: 2.95118889e-06
Iter: 1364 loss: 2.95006885e-06
Iter: 1365 loss: 2.96126063e-06
Iter: 1366 loss: 2.94999654e-06
Iter: 1367 loss: 2.94889878e-06
Iter: 1368 loss: 2.95027166e-06
Iter: 1369 loss: 2.9482394e-06
Iter: 1370 loss: 2.94739425e-06
Iter: 1371 loss: 2.94843971e-06
Iter: 1372 loss: 2.946922e-06
Iter: 1373 loss: 2.94552729e-06
Iter: 1374 loss: 2.94611596e-06
Iter: 1375 loss: 2.94460801e-06
Iter: 1376 loss: 2.94300071e-06
Iter: 1377 loss: 2.94756228e-06
Iter: 1378 loss: 2.94241318e-06
Iter: 1379 loss: 2.94078973e-06
Iter: 1380 loss: 2.9396856e-06
Iter: 1381 loss: 2.9390776e-06
Iter: 1382 loss: 2.93814651e-06
Iter: 1383 loss: 2.93789685e-06
Iter: 1384 loss: 2.93683502e-06
Iter: 1385 loss: 2.93644098e-06
Iter: 1386 loss: 2.93584299e-06
Iter: 1387 loss: 2.93509083e-06
Iter: 1388 loss: 2.93500034e-06
Iter: 1389 loss: 2.93433e-06
Iter: 1390 loss: 2.9348671e-06
Iter: 1391 loss: 2.93393032e-06
Iter: 1392 loss: 2.93333028e-06
Iter: 1393 loss: 2.93171979e-06
Iter: 1394 loss: 2.95281279e-06
Iter: 1395 loss: 2.9317257e-06
Iter: 1396 loss: 2.93035328e-06
Iter: 1397 loss: 2.94624351e-06
Iter: 1398 loss: 2.9303269e-06
Iter: 1399 loss: 2.92933123e-06
Iter: 1400 loss: 2.93642142e-06
Iter: 1401 loss: 2.92927962e-06
Iter: 1402 loss: 2.92817026e-06
Iter: 1403 loss: 2.92770846e-06
Iter: 1404 loss: 2.92726122e-06
Iter: 1405 loss: 2.9257053e-06
Iter: 1406 loss: 2.92581558e-06
Iter: 1407 loss: 2.92452523e-06
Iter: 1408 loss: 2.92306368e-06
Iter: 1409 loss: 2.9230473e-06
Iter: 1410 loss: 2.92231903e-06
Iter: 1411 loss: 2.92095046e-06
Iter: 1412 loss: 2.92094455e-06
Iter: 1413 loss: 2.91905485e-06
Iter: 1414 loss: 2.92826326e-06
Iter: 1415 loss: 2.91878337e-06
Iter: 1416 loss: 2.91747665e-06
Iter: 1417 loss: 2.9207722e-06
Iter: 1418 loss: 2.91699371e-06
Iter: 1419 loss: 2.91530978e-06
Iter: 1420 loss: 2.92378536e-06
Iter: 1421 loss: 2.91503738e-06
Iter: 1422 loss: 2.91411652e-06
Iter: 1423 loss: 2.92642653e-06
Iter: 1424 loss: 2.91410697e-06
Iter: 1425 loss: 2.91323477e-06
Iter: 1426 loss: 2.9122707e-06
Iter: 1427 loss: 2.91214883e-06
Iter: 1428 loss: 2.91076367e-06
Iter: 1429 loss: 2.91116157e-06
Iter: 1430 loss: 2.90992898e-06
Iter: 1431 loss: 2.90840671e-06
Iter: 1432 loss: 2.91192805e-06
Iter: 1433 loss: 2.90786511e-06
Iter: 1434 loss: 2.90657817e-06
Iter: 1435 loss: 2.90664866e-06
Iter: 1436 loss: 2.90584853e-06
Iter: 1437 loss: 2.90571279e-06
Iter: 1438 loss: 2.90516687e-06
Iter: 1439 loss: 2.90381536e-06
Iter: 1440 loss: 2.90329922e-06
Iter: 1441 loss: 2.90266257e-06
Iter: 1442 loss: 2.90150729e-06
Iter: 1443 loss: 2.90149228e-06
Iter: 1444 loss: 2.90062371e-06
Iter: 1445 loss: 2.89902982e-06
Iter: 1446 loss: 2.92644495e-06
Iter: 1447 loss: 2.89894933e-06
Iter: 1448 loss: 2.89714944e-06
Iter: 1449 loss: 2.91541051e-06
Iter: 1450 loss: 2.89712852e-06
Iter: 1451 loss: 2.895948e-06
Iter: 1452 loss: 2.89606487e-06
Iter: 1453 loss: 2.89500304e-06
Iter: 1454 loss: 2.89318473e-06
Iter: 1455 loss: 2.91288643e-06
Iter: 1456 loss: 2.89317063e-06
Iter: 1457 loss: 2.89239597e-06
Iter: 1458 loss: 2.90256048e-06
Iter: 1459 loss: 2.89234504e-06
Iter: 1460 loss: 2.89159584e-06
Iter: 1461 loss: 2.88987758e-06
Iter: 1462 loss: 2.91056404e-06
Iter: 1463 loss: 2.88971523e-06
Iter: 1464 loss: 2.88794627e-06
Iter: 1465 loss: 2.89277409e-06
Iter: 1466 loss: 2.88722913e-06
Iter: 1467 loss: 2.88565798e-06
Iter: 1468 loss: 2.89030231e-06
Iter: 1469 loss: 2.88509341e-06
Iter: 1470 loss: 2.88429055e-06
Iter: 1471 loss: 2.88418551e-06
Iter: 1472 loss: 2.88344563e-06
Iter: 1473 loss: 2.88220826e-06
Iter: 1474 loss: 2.88220963e-06
Iter: 1475 loss: 2.88098954e-06
Iter: 1476 loss: 2.8893437e-06
Iter: 1477 loss: 2.88101819e-06
Iter: 1478 loss: 2.88015713e-06
Iter: 1479 loss: 2.88139154e-06
Iter: 1480 loss: 2.87983721e-06
Iter: 1481 loss: 2.87855823e-06
Iter: 1482 loss: 2.87810508e-06
Iter: 1483 loss: 2.8773693e-06
Iter: 1484 loss: 2.87592593e-06
Iter: 1485 loss: 2.88094543e-06
Iter: 1486 loss: 2.87546391e-06
Iter: 1487 loss: 2.87415969e-06
Iter: 1488 loss: 2.87907051e-06
Iter: 1489 loss: 2.87378634e-06
Iter: 1490 loss: 2.87263583e-06
Iter: 1491 loss: 2.88286401e-06
Iter: 1492 loss: 2.87256557e-06
Iter: 1493 loss: 2.87142052e-06
Iter: 1494 loss: 2.87431976e-06
Iter: 1495 loss: 2.87103558e-06
Iter: 1496 loss: 2.86974068e-06
Iter: 1497 loss: 2.87270404e-06
Iter: 1498 loss: 2.86914201e-06
Iter: 1499 loss: 2.8683221e-06
Iter: 1500 loss: 2.86692284e-06
Iter: 1501 loss: 2.89995705e-06
Iter: 1502 loss: 2.86696013e-06
Iter: 1503 loss: 2.86493696e-06
Iter: 1504 loss: 2.87330613e-06
Iter: 1505 loss: 2.86452382e-06
Iter: 1506 loss: 2.86362842e-06
Iter: 1507 loss: 2.86363024e-06
Iter: 1508 loss: 2.86261775e-06
Iter: 1509 loss: 2.86114869e-06
Iter: 1510 loss: 2.8611978e-06
Iter: 1511 loss: 2.85975329e-06
Iter: 1512 loss: 2.86556906e-06
Iter: 1513 loss: 2.85946976e-06
Iter: 1514 loss: 2.8580821e-06
Iter: 1515 loss: 2.86172053e-06
Iter: 1516 loss: 2.85760257e-06
Iter: 1517 loss: 2.85635201e-06
Iter: 1518 loss: 2.86378372e-06
Iter: 1519 loss: 2.85625401e-06
Iter: 1520 loss: 2.85522447e-06
Iter: 1521 loss: 2.85381361e-06
Iter: 1522 loss: 2.85375268e-06
Iter: 1523 loss: 2.85226042e-06
Iter: 1524 loss: 2.86802469e-06
Iter: 1525 loss: 2.8521913e-06
Iter: 1526 loss: 2.85086685e-06
Iter: 1527 loss: 2.8552e-06
Iter: 1528 loss: 2.85059468e-06
Iter: 1529 loss: 2.84926318e-06
Iter: 1530 loss: 2.85962051e-06
Iter: 1531 loss: 2.84913904e-06
Iter: 1532 loss: 2.84820339e-06
Iter: 1533 loss: 2.85178567e-06
Iter: 1534 loss: 2.847936e-06
Iter: 1535 loss: 2.84718544e-06
Iter: 1536 loss: 2.84540261e-06
Iter: 1537 loss: 2.86975455e-06
Iter: 1538 loss: 2.84529824e-06
Iter: 1539 loss: 2.84359658e-06
Iter: 1540 loss: 2.85231499e-06
Iter: 1541 loss: 2.84327916e-06
Iter: 1542 loss: 2.84164071e-06
Iter: 1543 loss: 2.84552198e-06
Iter: 1544 loss: 2.84108296e-06
Iter: 1545 loss: 2.84031762e-06
Iter: 1546 loss: 2.84025373e-06
Iter: 1547 loss: 2.83947838e-06
Iter: 1548 loss: 2.83866962e-06
Iter: 1549 loss: 2.83853274e-06
Iter: 1550 loss: 2.83742656e-06
Iter: 1551 loss: 2.84000021e-06
Iter: 1552 loss: 2.83707914e-06
Iter: 1553 loss: 2.83572581e-06
Iter: 1554 loss: 2.83933514e-06
Iter: 1555 loss: 2.83538611e-06
Iter: 1556 loss: 2.83405e-06
Iter: 1557 loss: 2.83737063e-06
Iter: 1558 loss: 2.83358258e-06
Iter: 1559 loss: 2.83244935e-06
Iter: 1560 loss: 2.83202439e-06
Iter: 1561 loss: 2.83144209e-06
Iter: 1562 loss: 2.82952942e-06
Iter: 1563 loss: 2.83834243e-06
Iter: 1564 loss: 2.8292734e-06
Iter: 1565 loss: 2.82856968e-06
Iter: 1566 loss: 2.82854762e-06
Iter: 1567 loss: 2.82782e-06
Iter: 1568 loss: 2.82748988e-06
Iter: 1569 loss: 2.82714404e-06
Iter: 1570 loss: 2.82612655e-06
Iter: 1571 loss: 2.8308848e-06
Iter: 1572 loss: 2.8259276e-06
Iter: 1573 loss: 2.82522387e-06
Iter: 1574 loss: 2.8242921e-06
Iter: 1575 loss: 2.82425412e-06
Iter: 1576 loss: 2.82279552e-06
Iter: 1577 loss: 2.82623864e-06
Iter: 1578 loss: 2.8222637e-06
Iter: 1579 loss: 2.82086194e-06
Iter: 1580 loss: 2.82478e-06
Iter: 1581 loss: 2.82042856e-06
Iter: 1582 loss: 2.81915754e-06
Iter: 1583 loss: 2.83888335e-06
Iter: 1584 loss: 2.81918346e-06
Iter: 1585 loss: 2.81858593e-06
Iter: 1586 loss: 2.81807388e-06
Iter: 1587 loss: 2.81789971e-06
Iter: 1588 loss: 2.81667872e-06
Iter: 1589 loss: 2.81840357e-06
Iter: 1590 loss: 2.81606776e-06
Iter: 1591 loss: 2.81497523e-06
Iter: 1592 loss: 2.82525934e-06
Iter: 1593 loss: 2.81488292e-06
Iter: 1594 loss: 2.81410826e-06
Iter: 1595 loss: 2.81288021e-06
Iter: 1596 loss: 2.81290409e-06
Iter: 1597 loss: 2.8111117e-06
Iter: 1598 loss: 2.81663733e-06
Iter: 1599 loss: 2.81058101e-06
Iter: 1600 loss: 2.8089712e-06
Iter: 1601 loss: 2.82338624e-06
Iter: 1602 loss: 2.80883978e-06
Iter: 1603 loss: 2.80753466e-06
Iter: 1604 loss: 2.82085239e-06
Iter: 1605 loss: 2.80753216e-06
Iter: 1606 loss: 2.80685936e-06
Iter: 1607 loss: 2.80654285e-06
Iter: 1608 loss: 2.80629979e-06
Iter: 1609 loss: 2.80516952e-06
Iter: 1610 loss: 2.80522499e-06
Iter: 1611 loss: 2.80414633e-06
Iter: 1612 loss: 2.80307313e-06
Iter: 1613 loss: 2.80575841e-06
Iter: 1614 loss: 2.80263066e-06
Iter: 1615 loss: 2.80122367e-06
Iter: 1616 loss: 2.80125369e-06
Iter: 1617 loss: 2.80012205e-06
Iter: 1618 loss: 2.79961796e-06
Iter: 1619 loss: 2.79923279e-06
Iter: 1620 loss: 2.79854839e-06
Iter: 1621 loss: 2.79751362e-06
Iter: 1622 loss: 2.79742858e-06
Iter: 1623 loss: 2.79611413e-06
Iter: 1624 loss: 2.79950564e-06
Iter: 1625 loss: 2.79571441e-06
Iter: 1626 loss: 2.79459482e-06
Iter: 1627 loss: 2.80150834e-06
Iter: 1628 loss: 2.79448113e-06
Iter: 1629 loss: 2.79332198e-06
Iter: 1630 loss: 2.79253982e-06
Iter: 1631 loss: 2.79216306e-06
Iter: 1632 loss: 2.79070423e-06
Iter: 1633 loss: 2.7982212e-06
Iter: 1634 loss: 2.79052961e-06
Iter: 1635 loss: 2.78930793e-06
Iter: 1636 loss: 2.79099186e-06
Iter: 1637 loss: 2.7886349e-06
Iter: 1638 loss: 2.78769971e-06
Iter: 1639 loss: 2.7953788e-06
Iter: 1640 loss: 2.78766902e-06
Iter: 1641 loss: 2.78647826e-06
Iter: 1642 loss: 2.79035248e-06
Iter: 1643 loss: 2.7860774e-06
Iter: 1644 loss: 2.78516313e-06
Iter: 1645 loss: 2.78766447e-06
Iter: 1646 loss: 2.78483026e-06
Iter: 1647 loss: 2.784137e-06
Iter: 1648 loss: 2.78425705e-06
Iter: 1649 loss: 2.78358402e-06
Iter: 1650 loss: 2.78237985e-06
Iter: 1651 loss: 2.78175185e-06
Iter: 1652 loss: 2.78122889e-06
Iter: 1653 loss: 2.77952108e-06
Iter: 1654 loss: 2.7810438e-06
Iter: 1655 loss: 2.77856634e-06
Iter: 1656 loss: 2.77764047e-06
Iter: 1657 loss: 2.77740946e-06
Iter: 1658 loss: 2.77646768e-06
Iter: 1659 loss: 2.77821573e-06
Iter: 1660 loss: 2.77609433e-06
Iter: 1661 loss: 2.77497838e-06
Iter: 1662 loss: 2.77524e-06
Iter: 1663 loss: 2.77415484e-06
Iter: 1664 loss: 2.7730066e-06
Iter: 1665 loss: 2.77880326e-06
Iter: 1666 loss: 2.77276058e-06
Iter: 1667 loss: 2.77143477e-06
Iter: 1668 loss: 2.77113259e-06
Iter: 1669 loss: 2.77032041e-06
Iter: 1670 loss: 2.76896753e-06
Iter: 1671 loss: 2.77664503e-06
Iter: 1672 loss: 2.76889841e-06
Iter: 1673 loss: 2.76766332e-06
Iter: 1674 loss: 2.76801347e-06
Iter: 1675 loss: 2.7667852e-06
Iter: 1676 loss: 2.76622222e-06
Iter: 1677 loss: 2.76585024e-06
Iter: 1678 loss: 2.76517881e-06
Iter: 1679 loss: 2.76409969e-06
Iter: 1680 loss: 2.79305596e-06
Iter: 1681 loss: 2.76408946e-06
Iter: 1682 loss: 2.76252331e-06
Iter: 1683 loss: 2.7676731e-06
Iter: 1684 loss: 2.76207265e-06
Iter: 1685 loss: 2.76090759e-06
Iter: 1686 loss: 2.7630058e-06
Iter: 1687 loss: 2.76040259e-06
Iter: 1688 loss: 2.75932462e-06
Iter: 1689 loss: 2.75896127e-06
Iter: 1690 loss: 2.75826915e-06
Iter: 1691 loss: 2.75696902e-06
Iter: 1692 loss: 2.76500259e-06
Iter: 1693 loss: 2.7567603e-06
Iter: 1694 loss: 2.7556257e-06
Iter: 1695 loss: 2.76744345e-06
Iter: 1696 loss: 2.75564366e-06
Iter: 1697 loss: 2.75461616e-06
Iter: 1698 loss: 2.75589446e-06
Iter: 1699 loss: 2.75410252e-06
Iter: 1700 loss: 2.75311754e-06
Iter: 1701 loss: 2.75244065e-06
Iter: 1702 loss: 2.75213642e-06
Iter: 1703 loss: 2.75034154e-06
Iter: 1704 loss: 2.76232822e-06
Iter: 1705 loss: 2.75015509e-06
Iter: 1706 loss: 2.74917466e-06
Iter: 1707 loss: 2.7483793e-06
Iter: 1708 loss: 2.7480919e-06
Iter: 1709 loss: 2.74583181e-06
Iter: 1710 loss: 2.74952617e-06
Iter: 1711 loss: 2.74479726e-06
Iter: 1712 loss: 2.74468948e-06
Iter: 1713 loss: 2.7441356e-06
Iter: 1714 loss: 2.74355466e-06
Iter: 1715 loss: 2.74290846e-06
Iter: 1716 loss: 2.74266176e-06
Iter: 1717 loss: 2.74169e-06
Iter: 1718 loss: 2.74381023e-06
Iter: 1719 loss: 2.74125819e-06
Iter: 1720 loss: 2.74010699e-06
Iter: 1721 loss: 2.74002741e-06
Iter: 1722 loss: 2.7390231e-06
Iter: 1723 loss: 2.73752039e-06
Iter: 1724 loss: 2.74323929e-06
Iter: 1725 loss: 2.73720161e-06
Iter: 1726 loss: 2.73611886e-06
Iter: 1727 loss: 2.73458318e-06
Iter: 1728 loss: 2.73445175e-06
Iter: 1729 loss: 2.73384171e-06
Iter: 1730 loss: 2.73338833e-06
Iter: 1731 loss: 2.73238e-06
Iter: 1732 loss: 2.73394676e-06
Iter: 1733 loss: 2.73195519e-06
Iter: 1734 loss: 2.73095065e-06
Iter: 1735 loss: 2.73044134e-06
Iter: 1736 loss: 2.72990928e-06
Iter: 1737 loss: 2.72861575e-06
Iter: 1738 loss: 2.7409651e-06
Iter: 1739 loss: 2.72858983e-06
Iter: 1740 loss: 2.72753437e-06
Iter: 1741 loss: 2.72712714e-06
Iter: 1742 loss: 2.72657212e-06
Iter: 1743 loss: 2.72515172e-06
Iter: 1744 loss: 2.73109777e-06
Iter: 1745 loss: 2.72492207e-06
Iter: 1746 loss: 2.72370676e-06
Iter: 1747 loss: 2.72846682e-06
Iter: 1748 loss: 2.72343186e-06
Iter: 1749 loss: 2.72213e-06
Iter: 1750 loss: 2.73215528e-06
Iter: 1751 loss: 2.72199668e-06
Iter: 1752 loss: 2.72132547e-06
Iter: 1753 loss: 2.72093621e-06
Iter: 1754 loss: 2.72070065e-06
Iter: 1755 loss: 2.71929525e-06
Iter: 1756 loss: 2.71911e-06
Iter: 1757 loss: 2.71815748e-06
Iter: 1758 loss: 2.71633098e-06
Iter: 1759 loss: 2.72513648e-06
Iter: 1760 loss: 2.71604767e-06
Iter: 1761 loss: 2.71499812e-06
Iter: 1762 loss: 2.71343197e-06
Iter: 1763 loss: 2.71336148e-06
Iter: 1764 loss: 2.71131694e-06
Iter: 1765 loss: 2.72677721e-06
Iter: 1766 loss: 2.71118734e-06
Iter: 1767 loss: 2.70955866e-06
Iter: 1768 loss: 2.71497402e-06
Iter: 1769 loss: 2.70908731e-06
Iter: 1770 loss: 2.7079443e-06
Iter: 1771 loss: 2.72469697e-06
Iter: 1772 loss: 2.70793271e-06
Iter: 1773 loss: 2.70674263e-06
Iter: 1774 loss: 2.70678379e-06
Iter: 1775 loss: 2.70583951e-06
Iter: 1776 loss: 2.70482656e-06
Iter: 1777 loss: 2.7141291e-06
Iter: 1778 loss: 2.70474175e-06
Iter: 1779 loss: 2.70394435e-06
Iter: 1780 loss: 2.70281089e-06
Iter: 1781 loss: 2.70269857e-06
Iter: 1782 loss: 2.70114128e-06
Iter: 1783 loss: 2.71237354e-06
Iter: 1784 loss: 2.70103055e-06
Iter: 1785 loss: 2.70026294e-06
Iter: 1786 loss: 2.7002261e-06
Iter: 1787 loss: 2.69954398e-06
Iter: 1788 loss: 2.69798511e-06
Iter: 1789 loss: 2.7229562e-06
Iter: 1790 loss: 2.69796374e-06
Iter: 1791 loss: 2.69692282e-06
Iter: 1792 loss: 2.70445321e-06
Iter: 1793 loss: 2.69680231e-06
Iter: 1794 loss: 2.69586599e-06
Iter: 1795 loss: 2.69721272e-06
Iter: 1796 loss: 2.6953378e-06
Iter: 1797 loss: 2.69409657e-06
Iter: 1798 loss: 2.69462771e-06
Iter: 1799 loss: 2.69314069e-06
Iter: 1800 loss: 2.69166026e-06
Iter: 1801 loss: 2.69176826e-06
Iter: 1802 loss: 2.69040106e-06
Iter: 1803 loss: 2.68858389e-06
Iter: 1804 loss: 2.70316423e-06
Iter: 1805 loss: 2.68835515e-06
Iter: 1806 loss: 2.68696499e-06
Iter: 1807 loss: 2.68878784e-06
Iter: 1808 loss: 2.68610665e-06
Iter: 1809 loss: 2.68464646e-06
Iter: 1810 loss: 2.68463555e-06
Iter: 1811 loss: 2.68382473e-06
Iter: 1812 loss: 2.68356189e-06
Iter: 1813 loss: 2.6830437e-06
Iter: 1814 loss: 2.68167719e-06
Iter: 1815 loss: 2.68461508e-06
Iter: 1816 loss: 2.68106487e-06
Iter: 1817 loss: 2.68003305e-06
Iter: 1818 loss: 2.68349618e-06
Iter: 1819 loss: 2.67968858e-06
Iter: 1820 loss: 2.67869473e-06
Iter: 1821 loss: 2.68794611e-06
Iter: 1822 loss: 2.67873747e-06
Iter: 1823 loss: 2.67768633e-06
Iter: 1824 loss: 2.67679843e-06
Iter: 1825 loss: 2.67658447e-06
Iter: 1826 loss: 2.67545238e-06
Iter: 1827 loss: 2.67636233e-06
Iter: 1828 loss: 2.67489804e-06
Iter: 1829 loss: 2.67368546e-06
Iter: 1830 loss: 2.68029726e-06
Iter: 1831 loss: 2.67348514e-06
Iter: 1832 loss: 2.67228688e-06
Iter: 1833 loss: 2.67433893e-06
Iter: 1834 loss: 2.67172186e-06
Iter: 1835 loss: 2.67042606e-06
Iter: 1836 loss: 2.67006e-06
Iter: 1837 loss: 2.66929851e-06
Iter: 1838 loss: 2.66763186e-06
Iter: 1839 loss: 2.67075939e-06
Iter: 1840 loss: 2.66705229e-06
Iter: 1841 loss: 2.66494544e-06
Iter: 1842 loss: 2.67388782e-06
Iter: 1843 loss: 2.66460802e-06
Iter: 1844 loss: 2.66327061e-06
Iter: 1845 loss: 2.66331381e-06
Iter: 1846 loss: 2.66229836e-06
Iter: 1847 loss: 2.66067696e-06
Iter: 1848 loss: 2.66061556e-06
Iter: 1849 loss: 2.65920949e-06
Iter: 1850 loss: 2.67439464e-06
Iter: 1851 loss: 2.65910035e-06
Iter: 1852 loss: 2.65789595e-06
Iter: 1853 loss: 2.65735707e-06
Iter: 1854 loss: 2.65673134e-06
Iter: 1855 loss: 2.65567905e-06
Iter: 1856 loss: 2.65554331e-06
Iter: 1857 loss: 2.65452718e-06
Iter: 1858 loss: 2.65564472e-06
Iter: 1859 loss: 2.65402014e-06
Iter: 1860 loss: 2.6531211e-06
Iter: 1861 loss: 2.65193239e-06
Iter: 1862 loss: 2.65180552e-06
Iter: 1863 loss: 2.6504963e-06
Iter: 1864 loss: 2.66359098e-06
Iter: 1865 loss: 2.65046469e-06
Iter: 1866 loss: 2.64924665e-06
Iter: 1867 loss: 2.64985624e-06
Iter: 1868 loss: 2.64847517e-06
Iter: 1869 loss: 2.64675327e-06
Iter: 1870 loss: 2.64989376e-06
Iter: 1871 loss: 2.64591563e-06
Iter: 1872 loss: 2.64462e-06
Iter: 1873 loss: 2.64272012e-06
Iter: 1874 loss: 2.64268215e-06
Iter: 1875 loss: 2.64027335e-06
Iter: 1876 loss: 2.64029904e-06
Iter: 1877 loss: 2.63913876e-06
Iter: 1878 loss: 2.65497806e-06
Iter: 1879 loss: 2.63920492e-06
Iter: 1880 loss: 2.63825041e-06
Iter: 1881 loss: 2.63660104e-06
Iter: 1882 loss: 2.67782798e-06
Iter: 1883 loss: 2.6365808e-06
Iter: 1884 loss: 2.63509651e-06
Iter: 1885 loss: 2.6548712e-06
Iter: 1886 loss: 2.6350915e-06
Iter: 1887 loss: 2.63394122e-06
Iter: 1888 loss: 2.63337415e-06
Iter: 1889 loss: 2.63286529e-06
Iter: 1890 loss: 2.63209631e-06
Iter: 1891 loss: 2.63190032e-06
Iter: 1892 loss: 2.63110451e-06
Iter: 1893 loss: 2.62984349e-06
Iter: 1894 loss: 2.62983349e-06
Iter: 1895 loss: 2.62845242e-06
Iter: 1896 loss: 2.62997378e-06
Iter: 1897 loss: 2.62766389e-06
Iter: 1898 loss: 2.62646086e-06
Iter: 1899 loss: 2.63696529e-06
Iter: 1900 loss: 2.6264504e-06
Iter: 1901 loss: 2.62525919e-06
Iter: 1902 loss: 2.62507547e-06
Iter: 1903 loss: 2.62425806e-06
Iter: 1904 loss: 2.62255116e-06
Iter: 1905 loss: 2.62805497e-06
Iter: 1906 loss: 2.62198819e-06
Iter: 1907 loss: 2.62086633e-06
Iter: 1908 loss: 2.62103981e-06
Iter: 1909 loss: 2.61990226e-06
Iter: 1910 loss: 2.61826699e-06
Iter: 1911 loss: 2.6328737e-06
Iter: 1912 loss: 2.61826426e-06
Iter: 1913 loss: 2.61687319e-06
Iter: 1914 loss: 2.62594926e-06
Iter: 1915 loss: 2.61676541e-06
Iter: 1916 loss: 2.61575133e-06
Iter: 1917 loss: 2.61512287e-06
Iter: 1918 loss: 2.61472951e-06
Iter: 1919 loss: 2.61350669e-06
Iter: 1920 loss: 2.62258709e-06
Iter: 1921 loss: 2.6134262e-06
Iter: 1922 loss: 2.61216701e-06
Iter: 1923 loss: 2.61151e-06
Iter: 1924 loss: 2.6109019e-06
Iter: 1925 loss: 2.60990601e-06
Iter: 1926 loss: 2.60970046e-06
Iter: 1927 loss: 2.60915135e-06
Iter: 1928 loss: 2.60785782e-06
Iter: 1929 loss: 2.62137e-06
Iter: 1930 loss: 2.6076882e-06
Iter: 1931 loss: 2.60577099e-06
Iter: 1932 loss: 2.60719366e-06
Iter: 1933 loss: 2.60462298e-06
Iter: 1934 loss: 2.60301317e-06
Iter: 1935 loss: 2.60304159e-06
Iter: 1936 loss: 2.60189017e-06
Iter: 1937 loss: 2.60077923e-06
Iter: 1938 loss: 2.60053093e-06
Iter: 1939 loss: 2.59855551e-06
Iter: 1940 loss: 2.60503452e-06
Iter: 1941 loss: 2.59804165e-06
Iter: 1942 loss: 2.59632498e-06
Iter: 1943 loss: 2.59701596e-06
Iter: 1944 loss: 2.59523677e-06
Iter: 1945 loss: 2.594031e-06
Iter: 1946 loss: 2.59395847e-06
Iter: 1947 loss: 2.59280978e-06
Iter: 1948 loss: 2.59382023e-06
Iter: 1949 loss: 2.59230364e-06
Iter: 1950 loss: 2.59099806e-06
Iter: 1951 loss: 2.59079206e-06
Iter: 1952 loss: 2.59001308e-06
Iter: 1953 loss: 2.58870023e-06
Iter: 1954 loss: 2.60509796e-06
Iter: 1955 loss: 2.58864202e-06
Iter: 1956 loss: 2.58755426e-06
Iter: 1957 loss: 2.58887167e-06
Iter: 1958 loss: 2.58699038e-06
Iter: 1959 loss: 2.58526597e-06
Iter: 1960 loss: 2.59089643e-06
Iter: 1961 loss: 2.58471573e-06
Iter: 1962 loss: 2.58391901e-06
Iter: 1963 loss: 2.58294631e-06
Iter: 1964 loss: 2.58279715e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi1.6/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi-2_phi2
+ date
Sun Nov  8 16:04:11 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi -2 --phi 2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi-2_phi2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e536610d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53692a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53622598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e535e0bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e535c4268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e535bebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53471598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53471c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53559840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53559048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53520a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e534081e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53408d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53461730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53445d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e53445620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e5336e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fe84bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fe84b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e533bdea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e533bd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e533366a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdfb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdfb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdfb0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fe26ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdab8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdc4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdc42f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e2fdc4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e082be7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e082bb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e082bb158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e082bb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e0827e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2e081e5158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
