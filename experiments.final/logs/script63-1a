+ RUN=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='-2 -1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0
+ date
Wed Oct 21 09:18:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4
+ date
Wed Oct 21 09:18:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8
+ date
Wed Oct 21 09:18:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2
+ date
Wed Oct 21 09:18:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1 --function f1 --psi -2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9b22b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9b726a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9ab7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9ac47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9b22ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9af9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e99b5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e99c2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e99727b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a44d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a19510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a00d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a0c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a0cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e98d77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e98ed6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e98ed840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9844a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9844c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e985a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c835a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c835dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c82e6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c82ae620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c82ae378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c8309840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e99417b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e991d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e991d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50804496a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5080449488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f508039b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f508039b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5080399488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50803990d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50803ffea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.021841295
test_loss: 0.021190982
train_loss: 0.010608679
test_loss: 0.010815483
train_loss: 0.0082497625
test_loss: 0.008591315
train_loss: 0.00708322
test_loss: 0.0075814873
train_loss: 0.0068265256
test_loss: 0.0071001314
train_loss: 0.006518971
test_loss: 0.0070450567
train_loss: 0.0061302795
test_loss: 0.006783094
train_loss: 0.0061742733
test_loss: 0.006875619
train_loss: 0.0062974775
test_loss: 0.006556472
train_loss: 0.0060953405
test_loss: 0.0063496255
train_loss: 0.0056433785
test_loss: 0.006631164
train_loss: 0.005513102
test_loss: 0.00639936
train_loss: 0.00548035
test_loss: 0.0063825627
train_loss: 0.0059448555
test_loss: 0.006330065
train_loss: 0.005315227
test_loss: 0.006339741
train_loss: 0.0055789137
test_loss: 0.006004008
train_loss: 0.0050865356
test_loss: 0.0058268653
train_loss: 0.0051076366
test_loss: 0.005969283
train_loss: 0.004924215
test_loss: 0.0058081066
train_loss: 0.0050095497
test_loss: 0.005802244
train_loss: 0.005174317
test_loss: 0.0060232976
train_loss: 0.0051678903
test_loss: 0.0057226224
train_loss: 0.0050662807
test_loss: 0.005852796
train_loss: 0.0047696074
test_loss: 0.005673038
train_loss: 0.004694115
test_loss: 0.0056269285
train_loss: 0.0045562396
test_loss: 0.005580263
train_loss: 0.0050458717
test_loss: 0.0056806253
train_loss: 0.0049402784
test_loss: 0.005546032
train_loss: 0.004821866
test_loss: 0.005376556
train_loss: 0.0049817916
test_loss: 0.0055849296
train_loss: 0.0046052565
test_loss: 0.0054069404
train_loss: 0.0043715825
test_loss: 0.005639102
train_loss: 0.004626459
test_loss: 0.005438026
train_loss: 0.004508894
test_loss: 0.005966049
train_loss: 0.00471092
test_loss: 0.0054718344
train_loss: 0.0044156965
test_loss: 0.005196775
train_loss: 0.0044202507
test_loss: 0.005259384
train_loss: 0.0043004313
test_loss: 0.0052551245
train_loss: 0.004591899
test_loss: 0.0051469738
train_loss: 0.0042014853
test_loss: 0.0050134053
train_loss: 0.0045157587
test_loss: 0.005051113
train_loss: 0.004162776
test_loss: 0.0051346477
train_loss: 0.004140989
test_loss: 0.005033076
train_loss: 0.0042116703
test_loss: 0.004955166
train_loss: 0.0042584306
test_loss: 0.0051701204
train_loss: 0.004121415
test_loss: 0.0049946597
train_loss: 0.0039855214
test_loss: 0.0050211763
train_loss: 0.0042400146
test_loss: 0.004937819
train_loss: 0.0041921153
test_loss: 0.0052647037
train_loss: 0.00414148
test_loss: 0.005065789
train_loss: 0.004051873
test_loss: 0.0049560163
train_loss: 0.004102352
test_loss: 0.004780862
train_loss: 0.004172706
test_loss: 0.0048953523
train_loss: 0.004236169
test_loss: 0.0047613024
train_loss: 0.004184692
test_loss: 0.004940241
train_loss: 0.0042022434
test_loss: 0.004836364
train_loss: 0.0044966806
test_loss: 0.005120908
train_loss: 0.0041001155
test_loss: 0.0047676796
train_loss: 0.0038242566
test_loss: 0.004846425
train_loss: 0.0038725014
test_loss: 0.004655126
train_loss: 0.003856185
test_loss: 0.004693798
train_loss: 0.003966935
test_loss: 0.0047858646
train_loss: 0.00372295
test_loss: 0.0046691033
train_loss: 0.0036993688
test_loss: 0.004639943
train_loss: 0.003870769
test_loss: 0.004800081
train_loss: 0.0037584687
test_loss: 0.004569838
train_loss: 0.0040200073
test_loss: 0.004919059
train_loss: 0.003925394
test_loss: 0.0046241865
train_loss: 0.0041175904
test_loss: 0.004848432
train_loss: 0.004006299
test_loss: 0.004644277
train_loss: 0.0037973644
test_loss: 0.0046702204
train_loss: 0.004097294
test_loss: 0.004639361
train_loss: 0.0036880947
test_loss: 0.0044482863
train_loss: 0.0037412606
test_loss: 0.0046261065
train_loss: 0.003558064
test_loss: 0.0045383824
train_loss: 0.003692758
test_loss: 0.004538348
train_loss: 0.0038698863
test_loss: 0.0046091992
train_loss: 0.0036645648
test_loss: 0.004601266
train_loss: 0.0036385392
test_loss: 0.0044518462
train_loss: 0.0038880892
test_loss: 0.004554447
train_loss: 0.0037284382
test_loss: 0.004600406
train_loss: 0.004082782
test_loss: 0.004589605
train_loss: 0.0038754195
test_loss: 0.0048241587
train_loss: 0.003733444
test_loss: 0.0046430263
train_loss: 0.0036973548
test_loss: 0.0045421873
train_loss: 0.003638552
test_loss: 0.0045489115
train_loss: 0.0036792196
test_loss: 0.0043367227
train_loss: 0.004068888
test_loss: 0.0047253836
train_loss: 0.0036568886
test_loss: 0.0043918155
train_loss: 0.003390548
test_loss: 0.00424622
train_loss: 0.0035902546
test_loss: 0.004474839
train_loss: 0.0034680502
test_loss: 0.004420526
train_loss: 0.0034753156
test_loss: 0.004374778
train_loss: 0.0035549896
test_loss: 0.004380691
train_loss: 0.0034945216
test_loss: 0.0043683974
train_loss: 0.0038243001
test_loss: 0.004662072
train_loss: 0.004022667
test_loss: 0.004794557
train_loss: 0.0034577705
test_loss: 0.004392472
train_loss: 0.0036199007
test_loss: 0.0045067198
train_loss: 0.00353831
test_loss: 0.0044276766
train_loss: 0.0036012155
test_loss: 0.0044241757
train_loss: 0.0035990742
test_loss: 0.004489387
train_loss: 0.0037386124
test_loss: 0.0043465146
train_loss: 0.003406996
test_loss: 0.0044392766
train_loss: 0.0034681724
test_loss: 0.0045839013
train_loss: 0.0033967523
test_loss: 0.0042565363
train_loss: 0.0034690737
test_loss: 0.004341821
train_loss: 0.0035353894
test_loss: 0.0043229796
train_loss: 0.003724771
test_loss: 0.0041764616
train_loss: 0.0035361974
test_loss: 0.0044078594
train_loss: 0.0037153205
test_loss: 0.004267634
train_loss: 0.0033385325
test_loss: 0.004349864
train_loss: 0.0033595082
test_loss: 0.0043363185
train_loss: 0.0033958983
test_loss: 0.0041910657
train_loss: 0.003577192
test_loss: 0.0042507634
train_loss: 0.0035773632
test_loss: 0.0042026043
train_loss: 0.0034938883
test_loss: 0.0043790475
train_loss: 0.003397062
test_loss: 0.004433453
train_loss: 0.003276146
test_loss: 0.0041495953
train_loss: 0.0035223844
test_loss: 0.0043665865
train_loss: 0.003427176
test_loss: 0.004234013
train_loss: 0.0032945615
test_loss: 0.004025006
train_loss: 0.0034588529
test_loss: 0.0042100404
train_loss: 0.0035324898
test_loss: 0.0043724286
train_loss: 0.0036485717
test_loss: 0.0043292004
train_loss: 0.0033324508
test_loss: 0.004356608
train_loss: 0.0031989715
test_loss: 0.0041698646
train_loss: 0.0034474775
test_loss: 0.0041663074
train_loss: 0.0033746161
test_loss: 0.0041520563
train_loss: 0.003161972
test_loss: 0.004123709
train_loss: 0.0035073082
test_loss: 0.004151492
train_loss: 0.0033225352
test_loss: 0.004208934
train_loss: 0.0033651793
test_loss: 0.004177568
train_loss: 0.003442727
test_loss: 0.004336745
train_loss: 0.0035048046
test_loss: 0.0042063035
train_loss: 0.0033639758
test_loss: 0.004158043
train_loss: 0.0035304795
test_loss: 0.0042504296
train_loss: 0.0031742651
test_loss: 0.004219253
train_loss: 0.0033586337
test_loss: 0.00409006
train_loss: 0.0032927855
test_loss: 0.004120284
train_loss: 0.0032985099
test_loss: 0.0041039283
train_loss: 0.0033064133
test_loss: 0.004110253
train_loss: 0.0033553708
test_loss: 0.004088762
train_loss: 0.0033088347
test_loss: 0.004149748
train_loss: 0.0032853917
test_loss: 0.004151167
train_loss: 0.0034417538
test_loss: 0.004026686
train_loss: 0.0034844421
test_loss: 0.0040770923
train_loss: 0.0033039316
test_loss: 0.004080111
train_loss: 0.0033956615
test_loss: 0.004034297
train_loss: 0.003151541
test_loss: 0.003997235
train_loss: 0.0031756815
test_loss: 0.00402418
train_loss: 0.0033338487
test_loss: 0.0043195137
train_loss: 0.0034177809
test_loss: 0.004218853
train_loss: 0.0033868751
test_loss: 0.004200719
train_loss: 0.0031526329
test_loss: 0.003979364
train_loss: 0.0031833244
test_loss: 0.0040464946
train_loss: 0.0032902663
test_loss: 0.0040161796
train_loss: 0.0033820316
test_loss: 0.0041696746
train_loss: 0.0032661783
test_loss: 0.0039988738
train_loss: 0.0031531448
test_loss: 0.003859729
train_loss: 0.0035377615
test_loss: 0.0043278807
train_loss: 0.0034150805
test_loss: 0.0041430765
train_loss: 0.0031819183
test_loss: 0.00401621
train_loss: 0.0035598695
test_loss: 0.0042127264
train_loss: 0.0034008445
test_loss: 0.004156681
train_loss: 0.003470745
test_loss: 0.004208918
train_loss: 0.003145379
test_loss: 0.004034552
train_loss: 0.0031843118
test_loss: 0.0042366995
train_loss: 0.003354589
test_loss: 0.004049832
train_loss: 0.0033137514
test_loss: 0.004058835
train_loss: 0.00313726
test_loss: 0.0040056384
train_loss: 0.0034294957
test_loss: 0.0039841365
train_loss: 0.0033327998
test_loss: 0.0041349693
train_loss: 0.0031591468
test_loss: 0.00395399
train_loss: 0.0032596975
test_loss: 0.0041411766
train_loss: 0.0031511695
test_loss: 0.004096768
train_loss: 0.003407426
test_loss: 0.004408958
train_loss: 0.003154217
test_loss: 0.004029545
train_loss: 0.003078462
test_loss: 0.0039146133
train_loss: 0.0030417177
test_loss: 0.003919839
train_loss: 0.003363831
test_loss: 0.003975986
train_loss: 0.0030494335
test_loss: 0.003932963
train_loss: 0.0029614186
test_loss: 0.003951232
train_loss: 0.0029175165
test_loss: 0.0038829348
train_loss: 0.0031781567
test_loss: 0.0042306352
train_loss: 0.0030915693
test_loss: 0.003855718
train_loss: 0.00335219
test_loss: 0.0040464727
train_loss: 0.0030960785
test_loss: 0.0038979773
train_loss: 0.0032562981
test_loss: 0.0038694707
train_loss: 0.0031729683
test_loss: 0.0039271857
train_loss: 0.002934087
test_loss: 0.0039678146
train_loss: 0.0030972427
test_loss: 0.003834162
train_loss: 0.0032029748
test_loss: 0.0042075827
train_loss: 0.0031119022
test_loss: 0.003964558
train_loss: 0.0029382012
test_loss: 0.0039826967
train_loss: 0.0030713405
test_loss: 0.0039547244
train_loss: 0.0029903757
test_loss: 0.0038941575
train_loss: 0.0030570296
test_loss: 0.0039137914
train_loss: 0.0032718522
test_loss: 0.0039771823
train_loss: 0.003146044
test_loss: 0.0038048541
train_loss: 0.0030899292
test_loss: 0.0038960555
train_loss: 0.0034838666
test_loss: 0.0044543142
train_loss: 0.0035003624
test_loss: 0.00391882
train_loss: 0.0033080685
test_loss: 0.0040218285
train_loss: 0.0031085974
test_loss: 0.0040022815
train_loss: 0.003146377
test_loss: 0.0039450275
train_loss: 0.0030544037
test_loss: 0.0038104581
train_loss: 0.0028704728
test_loss: 0.0038368104
train_loss: 0.0033312533
test_loss: 0.00401676
train_loss: 0.003214291
test_loss: 0.003959109
train_loss: 0.0032419872
test_loss: 0.003985854
train_loss: 0.003291354
test_loss: 0.0037861145
train_loss: 0.0029321664
test_loss: 0.0038425939
train_loss: 0.002964269
test_loss: 0.0038829183
train_loss: 0.0029569168
test_loss: 0.0039116303
train_loss: 0.003175784
test_loss: 0.0036848253
train_loss: 0.003012405
test_loss: 0.0037501024
train_loss: 0.0030583232
test_loss: 0.0038621225
train_loss: 0.0031565384
test_loss: 0.0039946395
train_loss: 0.0031691547
test_loss: 0.004046898
train_loss: 0.0032384119
test_loss: 0.0038915833
train_loss: 0.0029655942
test_loss: 0.0036773316
train_loss: 0.0030389237
test_loss: 0.003960643
train_loss: 0.0029732012
test_loss: 0.0038708092
train_loss: 0.003029606
test_loss: 0.0037517492
train_loss: 0.0031063221
test_loss: 0.003871775
train_loss: 0.0033233548
test_loss: 0.004067644
train_loss: 0.0031052907
test_loss: 0.004012438
train_loss: 0.0028731704
test_loss: 0.0037699686
train_loss: 0.0032047632
test_loss: 0.0040116245
train_loss: 0.0028656465
test_loss: 0.0039103664
train_loss: 0.0030436446
test_loss: 0.0037083037
train_loss: 0.0031686444
test_loss: 0.0038148144
train_loss: 0.0033478043
test_loss: 0.0039903163
train_loss: 0.0029617837
test_loss: 0.003818497
train_loss: 0.003127126
test_loss: 0.003902102
train_loss: 0.00326161
test_loss: 0.003868237
train_loss: 0.0030529033
test_loss: 0.0038171066
train_loss: 0.0028423702
test_loss: 0.003807607
train_loss: 0.0032052146
test_loss: 0.0038790165
train_loss: 0.003102321
test_loss: 0.0038269043
train_loss: 0.003397848
test_loss: 0.003890391
train_loss: 0.0029168134
test_loss: 0.003867908
train_loss: 0.0027219574
test_loss: 0.0036292213
train_loss: 0.003290067
test_loss: 0.0038310539
train_loss: 0.0029884102
test_loss: 0.0037565012
train_loss: 0.0029240295
test_loss: 0.0037978822
train_loss: 0.0030036552
test_loss: 0.0037614403
train_loss: 0.0029924854
test_loss: 0.0036819964
train_loss: 0.0030523571
test_loss: 0.0037935511
train_loss: 0.0029963742
test_loss: 0.003992253
train_loss: 0.0030861704
test_loss: 0.00376423
train_loss: 0.0028806813
test_loss: 0.0036919995
train_loss: 0.003037244
test_loss: 0.0038240496
train_loss: 0.0029208022
test_loss: 0.003747258
train_loss: 0.0028616218
test_loss: 0.0039069518
train_loss: 0.002960842
test_loss: 0.0037635947
train_loss: 0.0030126213
test_loss: 0.0038140758
train_loss: 0.0028117998
test_loss: 0.003714278
train_loss: 0.0028595333
test_loss: 0.0037033367
train_loss: 0.002803964
test_loss: 0.0039047468
train_loss: 0.0028563635
test_loss: 0.003801638
train_loss: 0.0032267876
test_loss: 0.0037097272
train_loss: 0.0031301412
test_loss: 0.003797508
train_loss: 0.0029942042
test_loss: 0.0036605538
train_loss: 0.0034471997
test_loss: 0.0038871232
train_loss: 0.0030007514
test_loss: 0.0038887362
train_loss: 0.0028834974
test_loss: 0.0036277068
train_loss: 0.002962173
test_loss: 0.003691451
train_loss: 0.002987802
test_loss: 0.0037808141
train_loss: 0.0031115145
test_loss: 0.0037630922
train_loss: 0.0027827364
test_loss: 0.0037555827
train_loss: 0.0029499228
test_loss: 0.0037936715
train_loss: 0.0029349022
test_loss: 0.0037182476
train_loss: 0.003287441
test_loss: 0.0037883804
train_loss: 0.003119721
test_loss: 0.0037924806
train_loss: 0.0029170474
test_loss: 0.003772275
train_loss: 0.002745871
test_loss: 0.0038009868
train_loss: 0.0029141828
test_loss: 0.0039026823
train_loss: 0.0031784046
test_loss: 0.0037163768
train_loss: 0.0028864045
test_loss: 0.0036838902
train_loss: 0.0028225058
test_loss: 0.0036645923
train_loss: 0.0029255908
test_loss: 0.0035713208
train_loss: 0.0029321932
test_loss: 0.003592519
train_loss: 0.003040615
test_loss: 0.0038118034
train_loss: 0.0031078288
test_loss: 0.00378615
train_loss: 0.0028037312
test_loss: 0.0036009513
train_loss: 0.0028741616
test_loss: 0.003734072
train_loss: 0.002691111
test_loss: 0.00354364
train_loss: 0.0029639248
test_loss: 0.00374795
train_loss: 0.0030055786
test_loss: 0.003801165
train_loss: 0.0031194272
test_loss: 0.003803547
train_loss: 0.0031606061
test_loss: 0.003915665
train_loss: 0.0029516444
test_loss: 0.0036123034
train_loss: 0.0030157373
test_loss: 0.0036762692
train_loss: 0.0027217246
test_loss: 0.0036942854
train_loss: 0.0029880437
test_loss: 0.0036955394
train_loss: 0.002965066
test_loss: 0.0036608633
train_loss: 0.003007018
test_loss: 0.0036643627
train_loss: 0.0029387425
test_loss: 0.0037095197
train_loss: 0.0029163244
test_loss: 0.003753064
train_loss: 0.0029524472
test_loss: 0.0037465799
train_loss: 0.0028498522
test_loss: 0.0038284783
train_loss: 0.0028253146
test_loss: 0.0037675072
train_loss: 0.0029026205
test_loss: 0.003642134
train_loss: 0.0028066335
test_loss: 0.0037291776
train_loss: 0.0026640643
test_loss: 0.0037435764
train_loss: 0.0030139324
test_loss: 0.0036504057
train_loss: 0.0027695762
test_loss: 0.003584185
train_loss: 0.0028310819
test_loss: 0.0036965448
train_loss: 0.0030135247
test_loss: 0.003697132
train_loss: 0.0027707643
test_loss: 0.0036001734
train_loss: 0.002774871
test_loss: 0.0036425157
train_loss: 0.002881335
test_loss: 0.0037480548
train_loss: 0.0028582627
test_loss: 0.0037377705
train_loss: 0.0028382163
test_loss: 0.0036181994
train_loss: 0.00271264
test_loss: 0.0035210066
train_loss: 0.0027191637
test_loss: 0.0035751327
train_loss: 0.0028662905
test_loss: 0.003880308
train_loss: 0.0028012583
test_loss: 0.0037539373
train_loss: 0.0028950514
test_loss: 0.0036111097
train_loss: 0.0028856522
test_loss: 0.0035697399
train_loss: 0.0028543584
test_loss: 0.0036028358
train_loss: 0.0029244558
test_loss: 0.0036291913
train_loss: 0.0030154367
test_loss: 0.0036841321
train_loss: 0.0027377056
test_loss: 0.0036359068
train_loss: 0.0029205116
test_loss: 0.0036465314
train_loss: 0.003078429
test_loss: 0.003734277
train_loss: 0.0032282257
test_loss: 0.0037917313
train_loss: 0.0029114562
test_loss: 0.0037341996
train_loss: 0.0028034337
test_loss: 0.0040023457
train_loss: 0.003085792
test_loss: 0.0036210837
train_loss: 0.0028262194
test_loss: 0.003643603
train_loss: 0.0027047691
test_loss: 0.0035887896
train_loss: 0.0028485355
test_loss: 0.003565661
train_loss: 0.0029159319
test_loss: 0.0036095148
train_loss: 0.002874794
test_loss: 0.0036862462
train_loss: 0.002982467
test_loss: 0.0035485718
train_loss: 0.0029358163
test_loss: 0.003647636
train_loss: 0.0028367913
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0035291896
train_loss: 0.0028066682
test_loss: 0.0035367357
train_loss: 0.0029031252
test_loss: 0.0035374996
train_loss: 0.0028760089
test_loss: 0.0036449549
train_loss: 0.0029838847
test_loss: 0.0036058207
train_loss: 0.0029187964
test_loss: 0.0037986175
train_loss: 0.0028903268
test_loss: 0.0037031362
train_loss: 0.002752013
test_loss: 0.003571209
train_loss: 0.003081764
test_loss: 0.003681048
train_loss: 0.0029990412
test_loss: 0.0037395768
train_loss: 0.00299531
test_loss: 0.0037525147
train_loss: 0.0028696833
test_loss: 0.0037340724
train_loss: 0.0028437593
test_loss: 0.0036058698
train_loss: 0.0028861226
test_loss: 0.0036706203
train_loss: 0.0026039255
test_loss: 0.0036999793
train_loss: 0.0028622397
test_loss: 0.0035458535
train_loss: 0.003085244
test_loss: 0.0036206387
train_loss: 0.0028323743
test_loss: 0.003486491
train_loss: 0.0026026191
test_loss: 0.0035378668
train_loss: 0.0027583083
test_loss: 0.0037109533
train_loss: 0.0028571498
test_loss: 0.0035815265
train_loss: 0.003019749
test_loss: 0.0036981814
train_loss: 0.002657531
test_loss: 0.003467456
train_loss: 0.002788214
test_loss: 0.0036381492
train_loss: 0.0027566939
test_loss: 0.0035154393
train_loss: 0.0027273665
test_loss: 0.0036199696
train_loss: 0.0027977042
test_loss: 0.0036227063
train_loss: 0.002885798
test_loss: 0.003599205
train_loss: 0.0029284698
test_loss: 0.003856682
train_loss: 0.002910701
test_loss: 0.0036426187
train_loss: 0.002676687
test_loss: 0.003681007
train_loss: 0.0029046247
test_loss: 0.0035938805
train_loss: 0.002885936
test_loss: 0.0036207472
train_loss: 0.002709941
test_loss: 0.0035106086
train_loss: 0.0031035105
test_loss: 0.003605149
train_loss: 0.0028213423
test_loss: 0.0036343895
train_loss: 0.0028842299
test_loss: 0.00358041
train_loss: 0.0027680171
test_loss: 0.0036884292
train_loss: 0.0027636783
test_loss: 0.0036197784
train_loss: 0.0032647036
test_loss: 0.0037888617
train_loss: 0.0028447271
test_loss: 0.0036200534
train_loss: 0.0025678966
test_loss: 0.0035349347
train_loss: 0.0027102737
test_loss: 0.003700044
train_loss: 0.0025451037
test_loss: 0.003565952
train_loss: 0.0025979725
test_loss: 0.0035203903
train_loss: 0.0027935964
test_loss: 0.0035999184
train_loss: 0.0027170917
test_loss: 0.0036065595
train_loss: 0.0026319618
test_loss: 0.0035291351
train_loss: 0.002830022
test_loss: 0.0036171635
train_loss: 0.002752849
test_loss: 0.003581333
train_loss: 0.002824122
test_loss: 0.0035926965
train_loss: 0.0028149828
test_loss: 0.0035012153
train_loss: 0.0028305957
test_loss: 0.0036234923
train_loss: 0.0026705984
test_loss: 0.0035380616
train_loss: 0.002645508
test_loss: 0.0035274222
train_loss: 0.0029350359
test_loss: 0.0037134807
train_loss: 0.0029833647
test_loss: 0.0034768076
train_loss: 0.0026921427
test_loss: 0.0034785944
train_loss: 0.002711893
test_loss: 0.003469985
train_loss: 0.0027580962
test_loss: 0.0035596061
train_loss: 0.0026624938
test_loss: 0.003534568
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accb5c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accc11598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accc11400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accc11620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accaff488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accaffc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accafa950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accaa2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accaa21e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca66c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca66b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca668c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca21598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca21ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acc98e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acc9c26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90cd58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90cd5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90cd0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90cd02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90c549d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90c76ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90c768c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90bf8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90bf8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c4b5ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c4679d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c40f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c40f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c42f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c3dd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c39c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c39c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c3c08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c371bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c371ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.39151507e-05
Iter: 2 loss: 1.15776511e-05
Iter: 3 loss: 1.15670409e-05
Iter: 4 loss: 1.04372666e-05
Iter: 5 loss: 1.41253367e-05
Iter: 6 loss: 1.01219703e-05
Iter: 7 loss: 9.39565416e-06
Iter: 8 loss: 1.12101934e-05
Iter: 9 loss: 9.14048e-06
Iter: 10 loss: 8.44455735e-06
Iter: 11 loss: 9.89043474e-06
Iter: 12 loss: 8.16658394e-06
Iter: 13 loss: 7.55381961e-06
Iter: 14 loss: 9.64080573e-06
Iter: 15 loss: 7.39142251e-06
Iter: 16 loss: 6.92532558e-06
Iter: 17 loss: 8.34802813e-06
Iter: 18 loss: 6.78676406e-06
Iter: 19 loss: 6.31352577e-06
Iter: 20 loss: 9.29022826e-06
Iter: 21 loss: 6.25934445e-06
Iter: 22 loss: 5.98558381e-06
Iter: 23 loss: 5.57204203e-06
Iter: 24 loss: 5.56421719e-06
Iter: 25 loss: 5.33348339e-06
Iter: 26 loss: 5.33270304e-06
Iter: 27 loss: 5.1222687e-06
Iter: 28 loss: 6.33172976e-06
Iter: 29 loss: 5.09403071e-06
Iter: 30 loss: 4.97948031e-06
Iter: 31 loss: 4.84331485e-06
Iter: 32 loss: 4.82912219e-06
Iter: 33 loss: 4.62396156e-06
Iter: 34 loss: 4.52918448e-06
Iter: 35 loss: 4.42759847e-06
Iter: 36 loss: 4.42623423e-06
Iter: 37 loss: 4.30261252e-06
Iter: 38 loss: 4.18212858e-06
Iter: 39 loss: 4.1516796e-06
Iter: 40 loss: 4.07590414e-06
Iter: 41 loss: 3.9467086e-06
Iter: 42 loss: 4.76706737e-06
Iter: 43 loss: 3.93212213e-06
Iter: 44 loss: 3.82164899e-06
Iter: 45 loss: 3.97085114e-06
Iter: 46 loss: 3.76627167e-06
Iter: 47 loss: 3.67665234e-06
Iter: 48 loss: 4.15410659e-06
Iter: 49 loss: 3.66291306e-06
Iter: 50 loss: 3.59429032e-06
Iter: 51 loss: 3.77607694e-06
Iter: 52 loss: 3.5712028e-06
Iter: 53 loss: 3.48747039e-06
Iter: 54 loss: 3.69876057e-06
Iter: 55 loss: 3.45841931e-06
Iter: 56 loss: 3.3917654e-06
Iter: 57 loss: 3.29580257e-06
Iter: 58 loss: 3.29278555e-06
Iter: 59 loss: 3.19292985e-06
Iter: 60 loss: 4.2427132e-06
Iter: 61 loss: 3.19041465e-06
Iter: 62 loss: 3.08820745e-06
Iter: 63 loss: 3.54603139e-06
Iter: 64 loss: 3.06819038e-06
Iter: 65 loss: 3.01279169e-06
Iter: 66 loss: 2.93867561e-06
Iter: 67 loss: 2.93452786e-06
Iter: 68 loss: 2.85861915e-06
Iter: 69 loss: 3.22380174e-06
Iter: 70 loss: 2.84511975e-06
Iter: 71 loss: 2.79763253e-06
Iter: 72 loss: 3.27414e-06
Iter: 73 loss: 2.79613027e-06
Iter: 74 loss: 2.74551439e-06
Iter: 75 loss: 2.96842609e-06
Iter: 76 loss: 2.73538035e-06
Iter: 77 loss: 2.70731721e-06
Iter: 78 loss: 2.70018836e-06
Iter: 79 loss: 2.68258759e-06
Iter: 80 loss: 2.6271282e-06
Iter: 81 loss: 2.75146158e-06
Iter: 82 loss: 2.60604202e-06
Iter: 83 loss: 2.5683064e-06
Iter: 84 loss: 2.68267718e-06
Iter: 85 loss: 2.55703299e-06
Iter: 86 loss: 2.52139e-06
Iter: 87 loss: 2.72045236e-06
Iter: 88 loss: 2.51628262e-06
Iter: 89 loss: 2.48626748e-06
Iter: 90 loss: 2.57236434e-06
Iter: 91 loss: 2.47684488e-06
Iter: 92 loss: 2.44950661e-06
Iter: 93 loss: 2.4459714e-06
Iter: 94 loss: 2.42653232e-06
Iter: 95 loss: 2.39642986e-06
Iter: 96 loss: 2.49138498e-06
Iter: 97 loss: 2.38762027e-06
Iter: 98 loss: 2.36663755e-06
Iter: 99 loss: 2.36616825e-06
Iter: 100 loss: 2.3512107e-06
Iter: 101 loss: 2.31255399e-06
Iter: 102 loss: 2.62732783e-06
Iter: 103 loss: 2.30561091e-06
Iter: 104 loss: 2.26571319e-06
Iter: 105 loss: 2.30969977e-06
Iter: 106 loss: 2.2439383e-06
Iter: 107 loss: 2.2311649e-06
Iter: 108 loss: 2.22154904e-06
Iter: 109 loss: 2.2003353e-06
Iter: 110 loss: 2.2527372e-06
Iter: 111 loss: 2.19285175e-06
Iter: 112 loss: 2.1781575e-06
Iter: 113 loss: 2.18298942e-06
Iter: 114 loss: 2.1678145e-06
Iter: 115 loss: 2.14918555e-06
Iter: 116 loss: 2.29640659e-06
Iter: 117 loss: 2.14792794e-06
Iter: 118 loss: 2.13522753e-06
Iter: 119 loss: 2.11624274e-06
Iter: 120 loss: 2.11585984e-06
Iter: 121 loss: 2.09899304e-06
Iter: 122 loss: 2.09835298e-06
Iter: 123 loss: 2.08592564e-06
Iter: 124 loss: 2.07464382e-06
Iter: 125 loss: 2.07152766e-06
Iter: 126 loss: 2.0455102e-06
Iter: 127 loss: 2.07075664e-06
Iter: 128 loss: 2.03064405e-06
Iter: 129 loss: 2.01388616e-06
Iter: 130 loss: 2.10423877e-06
Iter: 131 loss: 2.0113614e-06
Iter: 132 loss: 1.99534156e-06
Iter: 133 loss: 2.13406292e-06
Iter: 134 loss: 1.99448959e-06
Iter: 135 loss: 1.98368139e-06
Iter: 136 loss: 1.96676274e-06
Iter: 137 loss: 1.96655037e-06
Iter: 138 loss: 1.94907284e-06
Iter: 139 loss: 1.95629627e-06
Iter: 140 loss: 1.9369586e-06
Iter: 141 loss: 1.92714469e-06
Iter: 142 loss: 1.92554126e-06
Iter: 143 loss: 1.91264758e-06
Iter: 144 loss: 1.92515586e-06
Iter: 145 loss: 1.90524941e-06
Iter: 146 loss: 1.89456887e-06
Iter: 147 loss: 1.89647051e-06
Iter: 148 loss: 1.88653132e-06
Iter: 149 loss: 1.87275782e-06
Iter: 150 loss: 2.01811827e-06
Iter: 151 loss: 1.87241085e-06
Iter: 152 loss: 1.86464922e-06
Iter: 153 loss: 1.85863212e-06
Iter: 154 loss: 1.8561592e-06
Iter: 155 loss: 1.84946714e-06
Iter: 156 loss: 1.84897169e-06
Iter: 157 loss: 1.84358601e-06
Iter: 158 loss: 1.83424686e-06
Iter: 159 loss: 1.83422765e-06
Iter: 160 loss: 1.82319661e-06
Iter: 161 loss: 1.89276557e-06
Iter: 162 loss: 1.82190831e-06
Iter: 163 loss: 1.8143835e-06
Iter: 164 loss: 1.81736527e-06
Iter: 165 loss: 1.80920051e-06
Iter: 166 loss: 1.79732672e-06
Iter: 167 loss: 1.88569391e-06
Iter: 168 loss: 1.79635379e-06
Iter: 169 loss: 1.78729317e-06
Iter: 170 loss: 1.7776041e-06
Iter: 171 loss: 1.77603988e-06
Iter: 172 loss: 1.76464539e-06
Iter: 173 loss: 1.76504511e-06
Iter: 174 loss: 1.75570301e-06
Iter: 175 loss: 1.74737079e-06
Iter: 176 loss: 1.74681236e-06
Iter: 177 loss: 1.73861645e-06
Iter: 178 loss: 1.78259131e-06
Iter: 179 loss: 1.73739159e-06
Iter: 180 loss: 1.73338151e-06
Iter: 181 loss: 1.72410103e-06
Iter: 182 loss: 1.83919894e-06
Iter: 183 loss: 1.72339696e-06
Iter: 184 loss: 1.71291958e-06
Iter: 185 loss: 1.71291651e-06
Iter: 186 loss: 1.70709097e-06
Iter: 187 loss: 1.70094756e-06
Iter: 188 loss: 1.69993575e-06
Iter: 189 loss: 1.6925411e-06
Iter: 190 loss: 1.69252326e-06
Iter: 191 loss: 1.6867225e-06
Iter: 192 loss: 1.68159465e-06
Iter: 193 loss: 1.68001429e-06
Iter: 194 loss: 1.67304597e-06
Iter: 195 loss: 1.70410374e-06
Iter: 196 loss: 1.67159635e-06
Iter: 197 loss: 1.6653853e-06
Iter: 198 loss: 1.69371117e-06
Iter: 199 loss: 1.66421046e-06
Iter: 200 loss: 1.65988683e-06
Iter: 201 loss: 1.7063486e-06
Iter: 202 loss: 1.65976076e-06
Iter: 203 loss: 1.65635038e-06
Iter: 204 loss: 1.64852634e-06
Iter: 205 loss: 1.75052935e-06
Iter: 206 loss: 1.64804192e-06
Iter: 207 loss: 1.640037e-06
Iter: 208 loss: 1.64015216e-06
Iter: 209 loss: 1.6335689e-06
Iter: 210 loss: 1.62289336e-06
Iter: 211 loss: 1.72598232e-06
Iter: 212 loss: 1.62250399e-06
Iter: 213 loss: 1.61919615e-06
Iter: 214 loss: 1.61838727e-06
Iter: 215 loss: 1.61431899e-06
Iter: 216 loss: 1.60702166e-06
Iter: 217 loss: 1.78155517e-06
Iter: 218 loss: 1.60703667e-06
Iter: 219 loss: 1.60276136e-06
Iter: 220 loss: 1.63276059e-06
Iter: 221 loss: 1.60237244e-06
Iter: 222 loss: 1.5970428e-06
Iter: 223 loss: 1.59932699e-06
Iter: 224 loss: 1.59338879e-06
Iter: 225 loss: 1.58822661e-06
Iter: 226 loss: 1.60133698e-06
Iter: 227 loss: 1.58642331e-06
Iter: 228 loss: 1.57982254e-06
Iter: 229 loss: 1.61124376e-06
Iter: 230 loss: 1.57868465e-06
Iter: 231 loss: 1.57440388e-06
Iter: 232 loss: 1.56988904e-06
Iter: 233 loss: 1.56911574e-06
Iter: 234 loss: 1.56238525e-06
Iter: 235 loss: 1.6111477e-06
Iter: 236 loss: 1.56183876e-06
Iter: 237 loss: 1.5563744e-06
Iter: 238 loss: 1.59132196e-06
Iter: 239 loss: 1.55574378e-06
Iter: 240 loss: 1.55143618e-06
Iter: 241 loss: 1.56441945e-06
Iter: 242 loss: 1.55012992e-06
Iter: 243 loss: 1.54732675e-06
Iter: 244 loss: 1.5420643e-06
Iter: 245 loss: 1.66355812e-06
Iter: 246 loss: 1.54207191e-06
Iter: 247 loss: 1.53615963e-06
Iter: 248 loss: 1.55617226e-06
Iter: 249 loss: 1.53454243e-06
Iter: 250 loss: 1.52799703e-06
Iter: 251 loss: 1.53664416e-06
Iter: 252 loss: 1.52470693e-06
Iter: 253 loss: 1.52372593e-06
Iter: 254 loss: 1.52169503e-06
Iter: 255 loss: 1.51848383e-06
Iter: 256 loss: 1.5138246e-06
Iter: 257 loss: 1.51367e-06
Iter: 258 loss: 1.50962421e-06
Iter: 259 loss: 1.50897495e-06
Iter: 260 loss: 1.50623612e-06
Iter: 261 loss: 1.50288065e-06
Iter: 262 loss: 1.50283381e-06
Iter: 263 loss: 1.49973e-06
Iter: 264 loss: 1.50632468e-06
Iter: 265 loss: 1.49844664e-06
Iter: 266 loss: 1.49524635e-06
Iter: 267 loss: 1.4911127e-06
Iter: 268 loss: 1.49082973e-06
Iter: 269 loss: 1.48912613e-06
Iter: 270 loss: 1.48801098e-06
Iter: 271 loss: 1.48570803e-06
Iter: 272 loss: 1.48104345e-06
Iter: 273 loss: 1.56970623e-06
Iter: 274 loss: 1.48102185e-06
Iter: 275 loss: 1.47626065e-06
Iter: 276 loss: 1.51571612e-06
Iter: 277 loss: 1.47592255e-06
Iter: 278 loss: 1.47267167e-06
Iter: 279 loss: 1.47634091e-06
Iter: 280 loss: 1.47090486e-06
Iter: 281 loss: 1.46696311e-06
Iter: 282 loss: 1.50871369e-06
Iter: 283 loss: 1.4668401e-06
Iter: 284 loss: 1.46507716e-06
Iter: 285 loss: 1.46107914e-06
Iter: 286 loss: 1.51487313e-06
Iter: 287 loss: 1.46079401e-06
Iter: 288 loss: 1.45668491e-06
Iter: 289 loss: 1.49061498e-06
Iter: 290 loss: 1.45638592e-06
Iter: 291 loss: 1.45337208e-06
Iter: 292 loss: 1.50114533e-06
Iter: 293 loss: 1.45335559e-06
Iter: 294 loss: 1.45151648e-06
Iter: 295 loss: 1.44632838e-06
Iter: 296 loss: 1.47381286e-06
Iter: 297 loss: 1.4446988e-06
Iter: 298 loss: 1.43863883e-06
Iter: 299 loss: 1.470445e-06
Iter: 300 loss: 1.43770126e-06
Iter: 301 loss: 1.43466423e-06
Iter: 302 loss: 1.471828e-06
Iter: 303 loss: 1.43463603e-06
Iter: 304 loss: 1.43154216e-06
Iter: 305 loss: 1.44033459e-06
Iter: 306 loss: 1.43052512e-06
Iter: 307 loss: 1.42835393e-06
Iter: 308 loss: 1.42765862e-06
Iter: 309 loss: 1.42637634e-06
Iter: 310 loss: 1.4239082e-06
Iter: 311 loss: 1.42390741e-06
Iter: 312 loss: 1.42158217e-06
Iter: 313 loss: 1.41880923e-06
Iter: 314 loss: 1.41855469e-06
Iter: 315 loss: 1.41505893e-06
Iter: 316 loss: 1.42293879e-06
Iter: 317 loss: 1.41378302e-06
Iter: 318 loss: 1.40956047e-06
Iter: 319 loss: 1.42864508e-06
Iter: 320 loss: 1.40875238e-06
Iter: 321 loss: 1.40599e-06
Iter: 322 loss: 1.43946113e-06
Iter: 323 loss: 1.40597797e-06
Iter: 324 loss: 1.40416455e-06
Iter: 325 loss: 1.40049656e-06
Iter: 326 loss: 1.46260027e-06
Iter: 327 loss: 1.40038901e-06
Iter: 328 loss: 1.39685312e-06
Iter: 329 loss: 1.40448026e-06
Iter: 330 loss: 1.39552617e-06
Iter: 331 loss: 1.39173733e-06
Iter: 332 loss: 1.40567977e-06
Iter: 333 loss: 1.39080657e-06
Iter: 334 loss: 1.38959217e-06
Iter: 335 loss: 1.38906273e-06
Iter: 336 loss: 1.3871869e-06
Iter: 337 loss: 1.3827422e-06
Iter: 338 loss: 1.43449211e-06
Iter: 339 loss: 1.38237863e-06
Iter: 340 loss: 1.37976804e-06
Iter: 341 loss: 1.40370685e-06
Iter: 342 loss: 1.37968709e-06
Iter: 343 loss: 1.37772849e-06
Iter: 344 loss: 1.39317262e-06
Iter: 345 loss: 1.37758377e-06
Iter: 346 loss: 1.37576558e-06
Iter: 347 loss: 1.3731526e-06
Iter: 348 loss: 1.37308041e-06
Iter: 349 loss: 1.37076086e-06
Iter: 350 loss: 1.38861992e-06
Iter: 351 loss: 1.37058862e-06
Iter: 352 loss: 1.36804988e-06
Iter: 353 loss: 1.37861639e-06
Iter: 354 loss: 1.36749486e-06
Iter: 355 loss: 1.36550398e-06
Iter: 356 loss: 1.36376184e-06
Iter: 357 loss: 1.36325616e-06
Iter: 358 loss: 1.35947323e-06
Iter: 359 loss: 1.37232303e-06
Iter: 360 loss: 1.35846494e-06
Iter: 361 loss: 1.35628829e-06
Iter: 362 loss: 1.38587234e-06
Iter: 363 loss: 1.35627886e-06
Iter: 364 loss: 1.35436426e-06
Iter: 365 loss: 1.35503319e-06
Iter: 366 loss: 1.35301912e-06
Iter: 367 loss: 1.35088726e-06
Iter: 368 loss: 1.34922971e-06
Iter: 369 loss: 1.34856964e-06
Iter: 370 loss: 1.34579159e-06
Iter: 371 loss: 1.3475526e-06
Iter: 372 loss: 1.34404763e-06
Iter: 373 loss: 1.34522247e-06
Iter: 374 loss: 1.34265599e-06
Iter: 375 loss: 1.34160155e-06
Iter: 376 loss: 1.33885487e-06
Iter: 377 loss: 1.35668643e-06
Iter: 378 loss: 1.33816332e-06
Iter: 379 loss: 1.33464619e-06
Iter: 380 loss: 1.35067478e-06
Iter: 381 loss: 1.33398703e-06
Iter: 382 loss: 1.33176218e-06
Iter: 383 loss: 1.33173921e-06
Iter: 384 loss: 1.33055073e-06
Iter: 385 loss: 1.32791115e-06
Iter: 386 loss: 1.36498215e-06
Iter: 387 loss: 1.32781099e-06
Iter: 388 loss: 1.32630612e-06
Iter: 389 loss: 1.3262046e-06
Iter: 390 loss: 1.32468745e-06
Iter: 391 loss: 1.32728837e-06
Iter: 392 loss: 1.32393848e-06
Iter: 393 loss: 1.32258208e-06
Iter: 394 loss: 1.32180912e-06
Iter: 395 loss: 1.32124796e-06
Iter: 396 loss: 1.31858303e-06
Iter: 397 loss: 1.3243872e-06
Iter: 398 loss: 1.31749698e-06
Iter: 399 loss: 1.31561171e-06
Iter: 400 loss: 1.31562558e-06
Iter: 401 loss: 1.31404386e-06
Iter: 402 loss: 1.31278489e-06
Iter: 403 loss: 1.31232912e-06
Iter: 404 loss: 1.3100248e-06
Iter: 405 loss: 1.30956755e-06
Iter: 406 loss: 1.30807757e-06
Iter: 407 loss: 1.3074042e-06
Iter: 408 loss: 1.30695594e-06
Iter: 409 loss: 1.30571948e-06
Iter: 410 loss: 1.30421938e-06
Iter: 411 loss: 1.30404874e-06
Iter: 412 loss: 1.30224316e-06
Iter: 413 loss: 1.30073045e-06
Iter: 414 loss: 1.30022227e-06
Iter: 415 loss: 1.29838531e-06
Iter: 416 loss: 1.29826219e-06
Iter: 417 loss: 1.29667433e-06
Iter: 418 loss: 1.29395482e-06
Iter: 419 loss: 1.36273627e-06
Iter: 420 loss: 1.29395949e-06
Iter: 421 loss: 1.29087653e-06
Iter: 422 loss: 1.30093713e-06
Iter: 423 loss: 1.29001387e-06
Iter: 424 loss: 1.28838167e-06
Iter: 425 loss: 1.28816316e-06
Iter: 426 loss: 1.28709644e-06
Iter: 427 loss: 1.28504962e-06
Iter: 428 loss: 1.32731782e-06
Iter: 429 loss: 1.28504814e-06
Iter: 430 loss: 1.28325769e-06
Iter: 431 loss: 1.29735804e-06
Iter: 432 loss: 1.2831033e-06
Iter: 433 loss: 1.28153692e-06
Iter: 434 loss: 1.28759257e-06
Iter: 435 loss: 1.28109446e-06
Iter: 436 loss: 1.27935778e-06
Iter: 437 loss: 1.2834455e-06
Iter: 438 loss: 1.27869077e-06
Iter: 439 loss: 1.2770488e-06
Iter: 440 loss: 1.27544399e-06
Iter: 441 loss: 1.27509327e-06
Iter: 442 loss: 1.27260751e-06
Iter: 443 loss: 1.2760861e-06
Iter: 444 loss: 1.27137218e-06
Iter: 445 loss: 1.27041778e-06
Iter: 446 loss: 1.2699129e-06
Iter: 447 loss: 1.26851376e-06
Iter: 448 loss: 1.26676366e-06
Iter: 449 loss: 1.26655891e-06
Iter: 450 loss: 1.26525049e-06
Iter: 451 loss: 1.26520263e-06
Iter: 452 loss: 1.26414102e-06
Iter: 453 loss: 1.26241332e-06
Iter: 454 loss: 1.2886519e-06
Iter: 455 loss: 1.26241162e-06
Iter: 456 loss: 1.26099894e-06
Iter: 457 loss: 1.25988845e-06
Iter: 458 loss: 1.25943825e-06
Iter: 459 loss: 1.25739393e-06
Iter: 460 loss: 1.25765177e-06
Iter: 461 loss: 1.2558678e-06
Iter: 462 loss: 1.25538111e-06
Iter: 463 loss: 1.25454153e-06
Iter: 464 loss: 1.25362976e-06
Iter: 465 loss: 1.25137331e-06
Iter: 466 loss: 1.2692542e-06
Iter: 467 loss: 1.25096221e-06
Iter: 468 loss: 1.24930045e-06
Iter: 469 loss: 1.24930466e-06
Iter: 470 loss: 1.24799965e-06
Iter: 471 loss: 1.25280155e-06
Iter: 472 loss: 1.24772396e-06
Iter: 473 loss: 1.24640223e-06
Iter: 474 loss: 1.24946757e-06
Iter: 475 loss: 1.24588121e-06
Iter: 476 loss: 1.24458802e-06
Iter: 477 loss: 1.24347378e-06
Iter: 478 loss: 1.24313988e-06
Iter: 479 loss: 1.24090559e-06
Iter: 480 loss: 1.24412702e-06
Iter: 481 loss: 1.23975269e-06
Iter: 482 loss: 1.23912264e-06
Iter: 483 loss: 1.23838095e-06
Iter: 484 loss: 1.23774362e-06
Iter: 485 loss: 1.23569066e-06
Iter: 486 loss: 1.23947257e-06
Iter: 487 loss: 1.23435962e-06
Iter: 488 loss: 1.23196139e-06
Iter: 489 loss: 1.2557839e-06
Iter: 490 loss: 1.23192785e-06
Iter: 491 loss: 1.23041218e-06
Iter: 492 loss: 1.24241e-06
Iter: 493 loss: 1.23028019e-06
Iter: 494 loss: 1.22890174e-06
Iter: 495 loss: 1.23555287e-06
Iter: 496 loss: 1.2286e-06
Iter: 497 loss: 1.22751385e-06
Iter: 498 loss: 1.22585243e-06
Iter: 499 loss: 1.2258547e-06
Iter: 500 loss: 1.22408937e-06
Iter: 501 loss: 1.22404413e-06
Iter: 502 loss: 1.22323468e-06
Iter: 503 loss: 1.2211093e-06
Iter: 504 loss: 1.23953191e-06
Iter: 505 loss: 1.22078018e-06
Iter: 506 loss: 1.21866196e-06
Iter: 507 loss: 1.2411391e-06
Iter: 508 loss: 1.21860796e-06
Iter: 509 loss: 1.21726566e-06
Iter: 510 loss: 1.2252533e-06
Iter: 511 loss: 1.21712662e-06
Iter: 512 loss: 1.21598373e-06
Iter: 513 loss: 1.22117399e-06
Iter: 514 loss: 1.21575772e-06
Iter: 515 loss: 1.21451285e-06
Iter: 516 loss: 1.21368339e-06
Iter: 517 loss: 1.21320886e-06
Iter: 518 loss: 1.21167454e-06
Iter: 519 loss: 1.21827748e-06
Iter: 520 loss: 1.21136293e-06
Iter: 521 loss: 1.20933862e-06
Iter: 522 loss: 1.21554581e-06
Iter: 523 loss: 1.20872994e-06
Iter: 524 loss: 1.20761774e-06
Iter: 525 loss: 1.20562515e-06
Iter: 526 loss: 1.205643e-06
Iter: 527 loss: 1.20319623e-06
Iter: 528 loss: 1.21050482e-06
Iter: 529 loss: 1.20242544e-06
Iter: 530 loss: 1.20082746e-06
Iter: 531 loss: 1.21503217e-06
Iter: 532 loss: 1.2007705e-06
Iter: 533 loss: 1.19933884e-06
Iter: 534 loss: 1.21167875e-06
Iter: 535 loss: 1.19924061e-06
Iter: 536 loss: 1.19853053e-06
Iter: 537 loss: 1.19740457e-06
Iter: 538 loss: 1.19739354e-06
Iter: 539 loss: 1.19608717e-06
Iter: 540 loss: 1.19608603e-06
Iter: 541 loss: 1.19522326e-06
Iter: 542 loss: 1.19392121e-06
Iter: 543 loss: 1.19388415e-06
Iter: 544 loss: 1.19227809e-06
Iter: 545 loss: 1.19073457e-06
Iter: 546 loss: 1.19042761e-06
Iter: 547 loss: 1.18858952e-06
Iter: 548 loss: 1.18854587e-06
Iter: 549 loss: 1.18729577e-06
Iter: 550 loss: 1.19057643e-06
Iter: 551 loss: 1.18689911e-06
Iter: 552 loss: 1.18606249e-06
Iter: 553 loss: 1.18604703e-06
Iter: 554 loss: 1.18527191e-06
Iter: 555 loss: 1.18365915e-06
Iter: 556 loss: 1.21286439e-06
Iter: 557 loss: 1.1836446e-06
Iter: 558 loss: 1.18290916e-06
Iter: 559 loss: 1.18275864e-06
Iter: 560 loss: 1.18197761e-06
Iter: 561 loss: 1.17991897e-06
Iter: 562 loss: 1.19774677e-06
Iter: 563 loss: 1.17958734e-06
Iter: 564 loss: 1.17748891e-06
Iter: 565 loss: 1.18438948e-06
Iter: 566 loss: 1.17692139e-06
Iter: 567 loss: 1.17507716e-06
Iter: 568 loss: 1.17924037e-06
Iter: 569 loss: 1.17435377e-06
Iter: 570 loss: 1.17379182e-06
Iter: 571 loss: 1.1734478e-06
Iter: 572 loss: 1.17276625e-06
Iter: 573 loss: 1.17147704e-06
Iter: 574 loss: 1.19760909e-06
Iter: 575 loss: 1.17148147e-06
Iter: 576 loss: 1.17052673e-06
Iter: 577 loss: 1.17052014e-06
Iter: 578 loss: 1.16949218e-06
Iter: 579 loss: 1.16858109e-06
Iter: 580 loss: 1.16826811e-06
Iter: 581 loss: 1.16690694e-06
Iter: 582 loss: 1.16869603e-06
Iter: 583 loss: 1.16620913e-06
Iter: 584 loss: 1.16477713e-06
Iter: 585 loss: 1.17562604e-06
Iter: 586 loss: 1.16467345e-06
Iter: 587 loss: 1.16337446e-06
Iter: 588 loss: 1.17079924e-06
Iter: 589 loss: 1.16319609e-06
Iter: 590 loss: 1.16201625e-06
Iter: 591 loss: 1.16219576e-06
Iter: 592 loss: 1.16113642e-06
Iter: 593 loss: 1.16041952e-06
Iter: 594 loss: 1.16042e-06
Iter: 595 loss: 1.15973012e-06
Iter: 596 loss: 1.15823354e-06
Iter: 597 loss: 1.17854677e-06
Iter: 598 loss: 1.15815237e-06
Iter: 599 loss: 1.15688442e-06
Iter: 600 loss: 1.16047113e-06
Iter: 601 loss: 1.1564714e-06
Iter: 602 loss: 1.15503099e-06
Iter: 603 loss: 1.15449916e-06
Iter: 604 loss: 1.1536772e-06
Iter: 605 loss: 1.15279101e-06
Iter: 606 loss: 1.1524603e-06
Iter: 607 loss: 1.15133196e-06
Iter: 608 loss: 1.14977047e-06
Iter: 609 loss: 1.1497084e-06
Iter: 610 loss: 1.14875206e-06
Iter: 611 loss: 1.14874081e-06
Iter: 612 loss: 1.14781665e-06
Iter: 613 loss: 1.14748605e-06
Iter: 614 loss: 1.14693751e-06
Iter: 615 loss: 1.14594445e-06
Iter: 616 loss: 1.14577256e-06
Iter: 617 loss: 1.14503177e-06
Iter: 618 loss: 1.14367015e-06
Iter: 619 loss: 1.15102e-06
Iter: 620 loss: 1.1434704e-06
Iter: 621 loss: 1.14244881e-06
Iter: 622 loss: 1.15455953e-06
Iter: 623 loss: 1.14246086e-06
Iter: 624 loss: 1.14155546e-06
Iter: 625 loss: 1.14274121e-06
Iter: 626 loss: 1.14105626e-06
Iter: 627 loss: 1.14009651e-06
Iter: 628 loss: 1.1402318e-06
Iter: 629 loss: 1.13940007e-06
Iter: 630 loss: 1.13782983e-06
Iter: 631 loss: 1.14753163e-06
Iter: 632 loss: 1.13766282e-06
Iter: 633 loss: 1.13668261e-06
Iter: 634 loss: 1.13549288e-06
Iter: 635 loss: 1.13540204e-06
Iter: 636 loss: 1.13414058e-06
Iter: 637 loss: 1.13810415e-06
Iter: 638 loss: 1.13377666e-06
Iter: 639 loss: 1.13280839e-06
Iter: 640 loss: 1.14442923e-06
Iter: 641 loss: 1.13278e-06
Iter: 642 loss: 1.13178044e-06
Iter: 643 loss: 1.13300973e-06
Iter: 644 loss: 1.13128942e-06
Iter: 645 loss: 1.13031479e-06
Iter: 646 loss: 1.13020678e-06
Iter: 647 loss: 1.12951955e-06
Iter: 648 loss: 1.12779e-06
Iter: 649 loss: 1.13902945e-06
Iter: 650 loss: 1.12758471e-06
Iter: 651 loss: 1.12668181e-06
Iter: 652 loss: 1.12482473e-06
Iter: 653 loss: 1.15309467e-06
Iter: 654 loss: 1.12474049e-06
Iter: 655 loss: 1.12286637e-06
Iter: 656 loss: 1.13142028e-06
Iter: 657 loss: 1.12251792e-06
Iter: 658 loss: 1.12177372e-06
Iter: 659 loss: 1.12162e-06
Iter: 660 loss: 1.12086855e-06
Iter: 661 loss: 1.12235273e-06
Iter: 662 loss: 1.12051907e-06
Iter: 663 loss: 1.11984707e-06
Iter: 664 loss: 1.11931058e-06
Iter: 665 loss: 1.11908389e-06
Iter: 666 loss: 1.11767645e-06
Iter: 667 loss: 1.12542853e-06
Iter: 668 loss: 1.11747499e-06
Iter: 669 loss: 1.11642339e-06
Iter: 670 loss: 1.11817349e-06
Iter: 671 loss: 1.1159608e-06
Iter: 672 loss: 1.11490215e-06
Iter: 673 loss: 1.1140479e-06
Iter: 674 loss: 1.1137322e-06
Iter: 675 loss: 1.11229929e-06
Iter: 676 loss: 1.11720919e-06
Iter: 677 loss: 1.11192639e-06
Iter: 678 loss: 1.11083955e-06
Iter: 679 loss: 1.11825443e-06
Iter: 680 loss: 1.11070631e-06
Iter: 681 loss: 1.10978419e-06
Iter: 682 loss: 1.11952011e-06
Iter: 683 loss: 1.10976475e-06
Iter: 684 loss: 1.10917085e-06
Iter: 685 loss: 1.10802262e-06
Iter: 686 loss: 1.13290423e-06
Iter: 687 loss: 1.10801739e-06
Iter: 688 loss: 1.1066154e-06
Iter: 689 loss: 1.12401312e-06
Iter: 690 loss: 1.10664632e-06
Iter: 691 loss: 1.10574115e-06
Iter: 692 loss: 1.10424276e-06
Iter: 693 loss: 1.13842202e-06
Iter: 694 loss: 1.10424116e-06
Iter: 695 loss: 1.10262317e-06
Iter: 696 loss: 1.1074701e-06
Iter: 697 loss: 1.10214933e-06
Iter: 698 loss: 1.10085728e-06
Iter: 699 loss: 1.10936639e-06
Iter: 700 loss: 1.10067845e-06
Iter: 701 loss: 1.09947439e-06
Iter: 702 loss: 1.11454267e-06
Iter: 703 loss: 1.09945768e-06
Iter: 704 loss: 1.09897519e-06
Iter: 705 loss: 1.0981604e-06
Iter: 706 loss: 1.09813527e-06
Iter: 707 loss: 1.09709117e-06
Iter: 708 loss: 1.1052407e-06
Iter: 709 loss: 1.09701807e-06
Iter: 710 loss: 1.09612233e-06
Iter: 711 loss: 1.0986962e-06
Iter: 712 loss: 1.0958538e-06
Iter: 713 loss: 1.09482562e-06
Iter: 714 loss: 1.09406642e-06
Iter: 715 loss: 1.0937672e-06
Iter: 716 loss: 1.09262328e-06
Iter: 717 loss: 1.09594919e-06
Iter: 718 loss: 1.09227938e-06
Iter: 719 loss: 1.09110056e-06
Iter: 720 loss: 1.09075825e-06
Iter: 721 loss: 1.09009898e-06
Iter: 722 loss: 1.0905037e-06
Iter: 723 loss: 1.08950758e-06
Iter: 724 loss: 1.08902066e-06
Iter: 725 loss: 1.08784354e-06
Iter: 726 loss: 1.09848293e-06
Iter: 727 loss: 1.08762481e-06
Iter: 728 loss: 1.08627466e-06
Iter: 729 loss: 1.0899023e-06
Iter: 730 loss: 1.08580412e-06
Iter: 731 loss: 1.08450877e-06
Iter: 732 loss: 1.09008386e-06
Iter: 733 loss: 1.08424638e-06
Iter: 734 loss: 1.08286213e-06
Iter: 735 loss: 1.09291943e-06
Iter: 736 loss: 1.08273764e-06
Iter: 737 loss: 1.08203767e-06
Iter: 738 loss: 1.0817472e-06
Iter: 739 loss: 1.08133474e-06
Iter: 740 loss: 1.08028837e-06
Iter: 741 loss: 1.09615439e-06
Iter: 742 loss: 1.08028564e-06
Iter: 743 loss: 1.07984863e-06
Iter: 744 loss: 1.07903975e-06
Iter: 745 loss: 1.07903702e-06
Iter: 746 loss: 1.07822939e-06
Iter: 747 loss: 1.09124608e-06
Iter: 748 loss: 1.07823462e-06
Iter: 749 loss: 1.0777004e-06
Iter: 750 loss: 1.07694416e-06
Iter: 751 loss: 1.07693177e-06
Iter: 752 loss: 1.07574579e-06
Iter: 753 loss: 1.07767107e-06
Iter: 754 loss: 1.07516348e-06
Iter: 755 loss: 1.07396136e-06
Iter: 756 loss: 1.07781932e-06
Iter: 757 loss: 1.07360029e-06
Iter: 758 loss: 1.07275207e-06
Iter: 759 loss: 1.07274082e-06
Iter: 760 loss: 1.07212361e-06
Iter: 761 loss: 1.07112783e-06
Iter: 762 loss: 1.07113146e-06
Iter: 763 loss: 1.07000255e-06
Iter: 764 loss: 1.0709764e-06
Iter: 765 loss: 1.0693542e-06
Iter: 766 loss: 1.06851144e-06
Iter: 767 loss: 1.07919402e-06
Iter: 768 loss: 1.0684937e-06
Iter: 769 loss: 1.06744551e-06
Iter: 770 loss: 1.06781192e-06
Iter: 771 loss: 1.06674543e-06
Iter: 772 loss: 1.06588732e-06
Iter: 773 loss: 1.07143387e-06
Iter: 774 loss: 1.06576795e-06
Iter: 775 loss: 1.06478728e-06
Iter: 776 loss: 1.06622497e-06
Iter: 777 loss: 1.06432367e-06
Iter: 778 loss: 1.06334267e-06
Iter: 779 loss: 1.06220068e-06
Iter: 780 loss: 1.06207176e-06
Iter: 781 loss: 1.06176708e-06
Iter: 782 loss: 1.06134826e-06
Iter: 783 loss: 1.06091466e-06
Iter: 784 loss: 1.0598709e-06
Iter: 785 loss: 1.07474671e-06
Iter: 786 loss: 1.05979893e-06
Iter: 787 loss: 1.05886193e-06
Iter: 788 loss: 1.06568268e-06
Iter: 789 loss: 1.05875711e-06
Iter: 790 loss: 1.05781055e-06
Iter: 791 loss: 1.0580751e-06
Iter: 792 loss: 1.05713389e-06
Iter: 793 loss: 1.05608217e-06
Iter: 794 loss: 1.0574613e-06
Iter: 795 loss: 1.05551339e-06
Iter: 796 loss: 1.05457684e-06
Iter: 797 loss: 1.05454365e-06
Iter: 798 loss: 1.05382105e-06
Iter: 799 loss: 1.05288325e-06
Iter: 800 loss: 1.05279025e-06
Iter: 801 loss: 1.05176446e-06
Iter: 802 loss: 1.05190816e-06
Iter: 803 loss: 1.05093102e-06
Iter: 804 loss: 1.04984247e-06
Iter: 805 loss: 1.05478989e-06
Iter: 806 loss: 1.04967012e-06
Iter: 807 loss: 1.04883759e-06
Iter: 808 loss: 1.04879791e-06
Iter: 809 loss: 1.04827427e-06
Iter: 810 loss: 1.04782418e-06
Iter: 811 loss: 1.04764672e-06
Iter: 812 loss: 1.04657056e-06
Iter: 813 loss: 1.05184131e-06
Iter: 814 loss: 1.04639059e-06
Iter: 815 loss: 1.04577691e-06
Iter: 816 loss: 1.04461583e-06
Iter: 817 loss: 1.06638765e-06
Iter: 818 loss: 1.04459605e-06
Iter: 819 loss: 1.04370258e-06
Iter: 820 loss: 1.04367859e-06
Iter: 821 loss: 1.04276103e-06
Iter: 822 loss: 1.04278388e-06
Iter: 823 loss: 1.04206435e-06
Iter: 824 loss: 1.04124069e-06
Iter: 825 loss: 1.04164735e-06
Iter: 826 loss: 1.0406593e-06
Iter: 827 loss: 1.03984814e-06
Iter: 828 loss: 1.04564e-06
Iter: 829 loss: 1.03978834e-06
Iter: 830 loss: 1.03893194e-06
Iter: 831 loss: 1.03855677e-06
Iter: 832 loss: 1.03811146e-06
Iter: 833 loss: 1.03711727e-06
Iter: 834 loss: 1.04109597e-06
Iter: 835 loss: 1.0369024e-06
Iter: 836 loss: 1.03573188e-06
Iter: 837 loss: 1.04645505e-06
Iter: 838 loss: 1.03570687e-06
Iter: 839 loss: 1.03500395e-06
Iter: 840 loss: 1.03392517e-06
Iter: 841 loss: 1.03390596e-06
Iter: 842 loss: 1.03274101e-06
Iter: 843 loss: 1.03583659e-06
Iter: 844 loss: 1.03241416e-06
Iter: 845 loss: 1.0322118e-06
Iter: 846 loss: 1.03189404e-06
Iter: 847 loss: 1.03150546e-06
Iter: 848 loss: 1.0305946e-06
Iter: 849 loss: 1.04199762e-06
Iter: 850 loss: 1.03053446e-06
Iter: 851 loss: 1.02999195e-06
Iter: 852 loss: 1.02999013e-06
Iter: 853 loss: 1.02939293e-06
Iter: 854 loss: 1.02829824e-06
Iter: 855 loss: 1.05267145e-06
Iter: 856 loss: 1.02828938e-06
Iter: 857 loss: 1.02757099e-06
Iter: 858 loss: 1.02756508e-06
Iter: 859 loss: 1.02683271e-06
Iter: 860 loss: 1.02734384e-06
Iter: 861 loss: 1.02638808e-06
Iter: 862 loss: 1.02555896e-06
Iter: 863 loss: 1.02444233e-06
Iter: 864 loss: 1.02438082e-06
Iter: 865 loss: 1.02331228e-06
Iter: 866 loss: 1.0261856e-06
Iter: 867 loss: 1.02291381e-06
Iter: 868 loss: 1.02172169e-06
Iter: 869 loss: 1.0289026e-06
Iter: 870 loss: 1.02157901e-06
Iter: 871 loss: 1.02059494e-06
Iter: 872 loss: 1.02810895e-06
Iter: 873 loss: 1.02050035e-06
Iter: 874 loss: 1.01989713e-06
Iter: 875 loss: 1.02617366e-06
Iter: 876 loss: 1.01990361e-06
Iter: 877 loss: 1.01941828e-06
Iter: 878 loss: 1.01833189e-06
Iter: 879 loss: 1.03465709e-06
Iter: 880 loss: 1.01827447e-06
Iter: 881 loss: 1.01718456e-06
Iter: 882 loss: 1.02118679e-06
Iter: 883 loss: 1.01688897e-06
Iter: 884 loss: 1.01642377e-06
Iter: 885 loss: 1.01634919e-06
Iter: 886 loss: 1.01577575e-06
Iter: 887 loss: 1.01490298e-06
Iter: 888 loss: 1.01489104e-06
Iter: 889 loss: 1.01399246e-06
Iter: 890 loss: 1.01361627e-06
Iter: 891 loss: 1.01313879e-06
Iter: 892 loss: 1.01263663e-06
Iter: 893 loss: 1.01252203e-06
Iter: 894 loss: 1.01181013e-06
Iter: 895 loss: 1.0106055e-06
Iter: 896 loss: 1.01062358e-06
Iter: 897 loss: 1.00966656e-06
Iter: 898 loss: 1.01757644e-06
Iter: 899 loss: 1.00957391e-06
Iter: 900 loss: 1.00877151e-06
Iter: 901 loss: 1.0124013e-06
Iter: 902 loss: 1.00860473e-06
Iter: 903 loss: 1.00794864e-06
Iter: 904 loss: 1.00723628e-06
Iter: 905 loss: 1.00712759e-06
Iter: 906 loss: 1.00624322e-06
Iter: 907 loss: 1.00941554e-06
Iter: 908 loss: 1.00599095e-06
Iter: 909 loss: 1.00498335e-06
Iter: 910 loss: 1.01389583e-06
Iter: 911 loss: 1.00489706e-06
Iter: 912 loss: 1.00442639e-06
Iter: 913 loss: 1.00381442e-06
Iter: 914 loss: 1.00378179e-06
Iter: 915 loss: 1.00290799e-06
Iter: 916 loss: 1.00637783e-06
Iter: 917 loss: 1.00270472e-06
Iter: 918 loss: 1.00195734e-06
Iter: 919 loss: 1.01043156e-06
Iter: 920 loss: 1.00192824e-06
Iter: 921 loss: 1.00133775e-06
Iter: 922 loss: 1.00180546e-06
Iter: 923 loss: 1.00092302e-06
Iter: 924 loss: 1.00027148e-06
Iter: 925 loss: 9.9966428e-07
Iter: 926 loss: 9.99490453e-07
Iter: 927 loss: 9.98693395e-07
Iter: 928 loss: 1.00529428e-06
Iter: 929 loss: 9.98638257e-07
Iter: 930 loss: 9.97749794e-07
Iter: 931 loss: 1.00176703e-06
Iter: 932 loss: 9.97672601e-07
Iter: 933 loss: 9.97110874e-07
Iter: 934 loss: 9.96228e-07
Iter: 935 loss: 9.96235144e-07
Iter: 936 loss: 9.95524715e-07
Iter: 937 loss: 9.9547583e-07
Iter: 938 loss: 9.94872835e-07
Iter: 939 loss: 9.93561571e-07
Iter: 940 loss: 1.01409e-06
Iter: 941 loss: 9.93511321e-07
Iter: 942 loss: 9.92299e-07
Iter: 943 loss: 1.00003274e-06
Iter: 944 loss: 9.92211199e-07
Iter: 945 loss: 9.91516231e-07
Iter: 946 loss: 1.00202419e-06
Iter: 947 loss: 9.91522e-07
Iter: 948 loss: 9.90819444e-07
Iter: 949 loss: 9.90375838e-07
Iter: 950 loss: 9.90099579e-07
Iter: 951 loss: 9.89381761e-07
Iter: 952 loss: 9.89261707e-07
Iter: 953 loss: 9.88760803e-07
Iter: 954 loss: 9.87824933e-07
Iter: 955 loss: 9.9496333e-07
Iter: 956 loss: 9.8774467e-07
Iter: 957 loss: 9.87049589e-07
Iter: 958 loss: 9.94633524e-07
Iter: 959 loss: 9.8704e-07
Iter: 960 loss: 9.86408168e-07
Iter: 961 loss: 9.86565851e-07
Iter: 962 loss: 9.85959e-07
Iter: 963 loss: 9.85132829e-07
Iter: 964 loss: 9.84683311e-07
Iter: 965 loss: 9.84349299e-07
Iter: 966 loss: 9.8327132e-07
Iter: 967 loss: 9.86018222e-07
Iter: 968 loss: 9.82901e-07
Iter: 969 loss: 9.81983248e-07
Iter: 970 loss: 9.81970516e-07
Iter: 971 loss: 9.81537141e-07
Iter: 972 loss: 9.80559776e-07
Iter: 973 loss: 9.95754e-07
Iter: 974 loss: 9.8054079e-07
Iter: 975 loss: 9.79785341e-07
Iter: 976 loss: 9.79765559e-07
Iter: 977 loss: 9.79187917e-07
Iter: 978 loss: 9.78933713e-07
Iter: 979 loss: 9.78638809e-07
Iter: 980 loss: 9.77905e-07
Iter: 981 loss: 9.76992624e-07
Iter: 982 loss: 9.76911906e-07
Iter: 983 loss: 9.76279e-07
Iter: 984 loss: 9.7607824e-07
Iter: 985 loss: 9.75345074e-07
Iter: 986 loss: 9.75294483e-07
Iter: 987 loss: 9.74751742e-07
Iter: 988 loss: 9.73936e-07
Iter: 989 loss: 9.73260512e-07
Iter: 990 loss: 9.73026886e-07
Iter: 991 loss: 9.72933208e-07
Iter: 992 loss: 9.72562361e-07
Iter: 993 loss: 9.72159569e-07
Iter: 994 loss: 9.71617e-07
Iter: 995 loss: 9.71564759e-07
Iter: 996 loss: 9.70936185e-07
Iter: 997 loss: 9.70203132e-07
Iter: 998 loss: 9.70144129e-07
Iter: 999 loss: 9.69324788e-07
Iter: 1000 loss: 9.69311486e-07
Iter: 1001 loss: 9.68533641e-07
Iter: 1002 loss: 9.69266694e-07
Iter: 1003 loss: 9.6805627e-07
Iter: 1004 loss: 9.67306391e-07
Iter: 1005 loss: 9.69231451e-07
Iter: 1006 loss: 9.67063443e-07
Iter: 1007 loss: 9.6632084e-07
Iter: 1008 loss: 9.67715e-07
Iter: 1009 loss: 9.65999106e-07
Iter: 1010 loss: 9.65200343e-07
Iter: 1011 loss: 9.70511564e-07
Iter: 1012 loss: 9.65123377e-07
Iter: 1013 loss: 9.64608375e-07
Iter: 1014 loss: 9.64453079e-07
Iter: 1015 loss: 9.64076207e-07
Iter: 1016 loss: 9.63287903e-07
Iter: 1017 loss: 9.63106572e-07
Iter: 1018 loss: 9.62644435e-07
Iter: 1019 loss: 9.61998353e-07
Iter: 1020 loss: 9.61972091e-07
Iter: 1021 loss: 9.6128e-07
Iter: 1022 loss: 9.61423552e-07
Iter: 1023 loss: 9.60726879e-07
Iter: 1024 loss: 9.59934368e-07
Iter: 1025 loss: 9.59906288e-07
Iter: 1026 loss: 9.5931739e-07
Iter: 1027 loss: 9.58789542e-07
Iter: 1028 loss: 9.58724513e-07
Iter: 1029 loss: 9.58140276e-07
Iter: 1030 loss: 9.57115617e-07
Iter: 1031 loss: 9.57117663e-07
Iter: 1032 loss: 9.56335271e-07
Iter: 1033 loss: 9.59456202e-07
Iter: 1034 loss: 9.56157692e-07
Iter: 1035 loss: 9.5560722e-07
Iter: 1036 loss: 9.55615178e-07
Iter: 1037 loss: 9.55129735e-07
Iter: 1038 loss: 9.54388497e-07
Iter: 1039 loss: 9.54387133e-07
Iter: 1040 loss: 9.53460926e-07
Iter: 1041 loss: 9.59764066e-07
Iter: 1042 loss: 9.53381971e-07
Iter: 1043 loss: 9.5272361e-07
Iter: 1044 loss: 9.54894631e-07
Iter: 1045 loss: 9.52543e-07
Iter: 1046 loss: 9.51810478e-07
Iter: 1047 loss: 9.52600033e-07
Iter: 1048 loss: 9.5147675e-07
Iter: 1049 loss: 9.50587605e-07
Iter: 1050 loss: 9.49843468e-07
Iter: 1051 loss: 9.49539412e-07
Iter: 1052 loss: 9.48510149e-07
Iter: 1053 loss: 9.58549e-07
Iter: 1054 loss: 9.48486843e-07
Iter: 1055 loss: 9.47596845e-07
Iter: 1056 loss: 9.57301836e-07
Iter: 1057 loss: 9.4759821e-07
Iter: 1058 loss: 9.47162221e-07
Iter: 1059 loss: 9.46227374e-07
Iter: 1060 loss: 9.61590331e-07
Iter: 1061 loss: 9.4621e-07
Iter: 1062 loss: 9.45534964e-07
Iter: 1063 loss: 9.4553e-07
Iter: 1064 loss: 9.44826752e-07
Iter: 1065 loss: 9.47128456e-07
Iter: 1066 loss: 9.44656335e-07
Iter: 1067 loss: 9.44113651e-07
Iter: 1068 loss: 9.43085411e-07
Iter: 1069 loss: 9.63727189e-07
Iter: 1070 loss: 9.43084444e-07
Iter: 1071 loss: 9.4260372e-07
Iter: 1072 loss: 9.4246974e-07
Iter: 1073 loss: 9.41873395e-07
Iter: 1074 loss: 9.41151143e-07
Iter: 1075 loss: 9.41073438e-07
Iter: 1076 loss: 9.40349423e-07
Iter: 1077 loss: 9.45626823e-07
Iter: 1078 loss: 9.40283144e-07
Iter: 1079 loss: 9.39653319e-07
Iter: 1080 loss: 9.41789665e-07
Iter: 1081 loss: 9.39488416e-07
Iter: 1082 loss: 9.38891617e-07
Iter: 1083 loss: 9.39613642e-07
Iter: 1084 loss: 9.38592336e-07
Iter: 1085 loss: 9.37847858e-07
Iter: 1086 loss: 9.38320795e-07
Iter: 1087 loss: 9.37332175e-07
Iter: 1088 loss: 9.36617482e-07
Iter: 1089 loss: 9.39559925e-07
Iter: 1090 loss: 9.36447236e-07
Iter: 1091 loss: 9.35791149e-07
Iter: 1092 loss: 9.4444863e-07
Iter: 1093 loss: 9.35782282e-07
Iter: 1094 loss: 9.35315029e-07
Iter: 1095 loss: 9.34219202e-07
Iter: 1096 loss: 9.45129273e-07
Iter: 1097 loss: 9.3405896e-07
Iter: 1098 loss: 9.33259e-07
Iter: 1099 loss: 9.4569e-07
Iter: 1100 loss: 9.33260537e-07
Iter: 1101 loss: 9.32475302e-07
Iter: 1102 loss: 9.37864968e-07
Iter: 1103 loss: 9.32447165e-07
Iter: 1104 loss: 9.3197e-07
Iter: 1105 loss: 9.30968781e-07
Iter: 1106 loss: 9.49628941e-07
Iter: 1107 loss: 9.30975261e-07
Iter: 1108 loss: 9.30518127e-07
Iter: 1109 loss: 9.30433316e-07
Iter: 1110 loss: 9.29911835e-07
Iter: 1111 loss: 9.29319754e-07
Iter: 1112 loss: 9.29244379e-07
Iter: 1113 loss: 9.2839332e-07
Iter: 1114 loss: 9.29436965e-07
Iter: 1115 loss: 9.27959491e-07
Iter: 1116 loss: 9.27091378e-07
Iter: 1117 loss: 9.38153846e-07
Iter: 1118 loss: 9.27083533e-07
Iter: 1119 loss: 9.26594055e-07
Iter: 1120 loss: 9.26777489e-07
Iter: 1121 loss: 9.26232133e-07
Iter: 1122 loss: 9.25569964e-07
Iter: 1123 loss: 9.26881512e-07
Iter: 1124 loss: 9.25280233e-07
Iter: 1125 loss: 9.24639608e-07
Iter: 1126 loss: 9.26854511e-07
Iter: 1127 loss: 9.24463791e-07
Iter: 1128 loss: 9.23945549e-07
Iter: 1129 loss: 9.31251748e-07
Iter: 1130 loss: 9.23951802e-07
Iter: 1131 loss: 9.23614948e-07
Iter: 1132 loss: 9.22829031e-07
Iter: 1133 loss: 9.32174714e-07
Iter: 1134 loss: 9.22767185e-07
Iter: 1135 loss: 9.21986441e-07
Iter: 1136 loss: 9.27547376e-07
Iter: 1137 loss: 9.21915102e-07
Iter: 1138 loss: 9.21094511e-07
Iter: 1139 loss: 9.27300277e-07
Iter: 1140 loss: 9.21024366e-07
Iter: 1141 loss: 9.20607533e-07
Iter: 1142 loss: 9.19734532e-07
Iter: 1143 loss: 9.35846515e-07
Iter: 1144 loss: 9.19721231e-07
Iter: 1145 loss: 9.19177467e-07
Iter: 1146 loss: 9.19105503e-07
Iter: 1147 loss: 9.1863393e-07
Iter: 1148 loss: 9.18312708e-07
Iter: 1149 loss: 9.18168e-07
Iter: 1150 loss: 9.17576e-07
Iter: 1151 loss: 9.17991315e-07
Iter: 1152 loss: 9.17228e-07
Iter: 1153 loss: 9.16519753e-07
Iter: 1154 loss: 9.23923892e-07
Iter: 1155 loss: 9.16506906e-07
Iter: 1156 loss: 9.16073759e-07
Iter: 1157 loss: 9.15667499e-07
Iter: 1158 loss: 9.15537385e-07
Iter: 1159 loss: 9.14697182e-07
Iter: 1160 loss: 9.16408851e-07
Iter: 1161 loss: 9.14329348e-07
Iter: 1162 loss: 9.13570261e-07
Iter: 1163 loss: 9.21093545e-07
Iter: 1164 loss: 9.13527799e-07
Iter: 1165 loss: 9.12986593e-07
Iter: 1166 loss: 9.16130659e-07
Iter: 1167 loss: 9.12904056e-07
Iter: 1168 loss: 9.12506096e-07
Iter: 1169 loss: 9.11631673e-07
Iter: 1170 loss: 9.25030463e-07
Iter: 1171 loss: 9.11559255e-07
Iter: 1172 loss: 9.11385655e-07
Iter: 1173 loss: 9.11177665e-07
Iter: 1174 loss: 9.1077095e-07
Iter: 1175 loss: 9.10464507e-07
Iter: 1176 loss: 9.10348035e-07
Iter: 1177 loss: 9.09827861e-07
Iter: 1178 loss: 9.09901928e-07
Iter: 1179 loss: 9.09447408e-07
Iter: 1180 loss: 9.08756533e-07
Iter: 1181 loss: 9.16517308e-07
Iter: 1182 loss: 9.08732773e-07
Iter: 1183 loss: 9.08217373e-07
Iter: 1184 loss: 9.07495235e-07
Iter: 1185 loss: 9.07453455e-07
Iter: 1186 loss: 9.0669937e-07
Iter: 1187 loss: 9.12774567e-07
Iter: 1188 loss: 9.06632351e-07
Iter: 1189 loss: 9.05867296e-07
Iter: 1190 loss: 9.07947651e-07
Iter: 1191 loss: 9.05573927e-07
Iter: 1192 loss: 9.05003276e-07
Iter: 1193 loss: 9.05370143e-07
Iter: 1194 loss: 9.04625267e-07
Iter: 1195 loss: 9.03958778e-07
Iter: 1196 loss: 9.0768674e-07
Iter: 1197 loss: 9.038489e-07
Iter: 1198 loss: 9.03333614e-07
Iter: 1199 loss: 9.05144134e-07
Iter: 1200 loss: 9.03206171e-07
Iter: 1201 loss: 9.0259357e-07
Iter: 1202 loss: 9.053133e-07
Iter: 1203 loss: 9.02460386e-07
Iter: 1204 loss: 9.02041847e-07
Iter: 1205 loss: 9.01320618e-07
Iter: 1206 loss: 9.01309761e-07
Iter: 1207 loss: 9.00718589e-07
Iter: 1208 loss: 9.00715861e-07
Iter: 1209 loss: 9.00126906e-07
Iter: 1210 loss: 8.99467182e-07
Iter: 1211 loss: 8.99389306e-07
Iter: 1212 loss: 8.98832127e-07
Iter: 1213 loss: 9.00294083e-07
Iter: 1214 loss: 8.98620101e-07
Iter: 1215 loss: 8.97813663e-07
Iter: 1216 loss: 9.01456e-07
Iter: 1217 loss: 8.97657401e-07
Iter: 1218 loss: 8.97182531e-07
Iter: 1219 loss: 8.97043151e-07
Iter: 1220 loss: 8.96753136e-07
Iter: 1221 loss: 8.96224378e-07
Iter: 1222 loss: 8.99999463e-07
Iter: 1223 loss: 8.96197548e-07
Iter: 1224 loss: 8.95637868e-07
Iter: 1225 loss: 8.96293159e-07
Iter: 1226 loss: 8.95326e-07
Iter: 1227 loss: 8.94771915e-07
Iter: 1228 loss: 8.95171183e-07
Iter: 1229 loss: 8.94441257e-07
Iter: 1230 loss: 8.93719857e-07
Iter: 1231 loss: 8.9537e-07
Iter: 1232 loss: 8.93420633e-07
Iter: 1233 loss: 8.92680589e-07
Iter: 1234 loss: 8.96801453e-07
Iter: 1235 loss: 8.92576736e-07
Iter: 1236 loss: 8.91844422e-07
Iter: 1237 loss: 8.95803566e-07
Iter: 1238 loss: 8.91706236e-07
Iter: 1239 loss: 8.91236709e-07
Iter: 1240 loss: 8.90902697e-07
Iter: 1241 loss: 8.90723868e-07
Iter: 1242 loss: 8.90335457e-07
Iter: 1243 loss: 8.90274464e-07
Iter: 1244 loss: 8.89948922e-07
Iter: 1245 loss: 8.8916704e-07
Iter: 1246 loss: 9.0083438e-07
Iter: 1247 loss: 8.89152034e-07
Iter: 1248 loss: 8.88537e-07
Iter: 1249 loss: 8.9355126e-07
Iter: 1250 loss: 8.88490149e-07
Iter: 1251 loss: 8.87802e-07
Iter: 1252 loss: 8.90037427e-07
Iter: 1253 loss: 8.87569797e-07
Iter: 1254 loss: 8.87081e-07
Iter: 1255 loss: 8.86115458e-07
Iter: 1256 loss: 9.09860432e-07
Iter: 1257 loss: 8.86131602e-07
Iter: 1258 loss: 8.85522923e-07
Iter: 1259 loss: 8.85470513e-07
Iter: 1260 loss: 8.8488008e-07
Iter: 1261 loss: 8.84621e-07
Iter: 1262 loss: 8.84323754e-07
Iter: 1263 loss: 8.83585699e-07
Iter: 1264 loss: 8.85219e-07
Iter: 1265 loss: 8.83282894e-07
Iter: 1266 loss: 8.82789e-07
Iter: 1267 loss: 8.87085605e-07
Iter: 1268 loss: 8.82740437e-07
Iter: 1269 loss: 8.82259428e-07
Iter: 1270 loss: 8.8303e-07
Iter: 1271 loss: 8.82022675e-07
Iter: 1272 loss: 8.8127382e-07
Iter: 1273 loss: 8.83438702e-07
Iter: 1274 loss: 8.81086237e-07
Iter: 1275 loss: 8.80585958e-07
Iter: 1276 loss: 8.80813559e-07
Iter: 1277 loss: 8.8026718e-07
Iter: 1278 loss: 8.79678112e-07
Iter: 1279 loss: 8.87514091e-07
Iter: 1280 loss: 8.79655374e-07
Iter: 1281 loss: 8.79224899e-07
Iter: 1282 loss: 8.78410049e-07
Iter: 1283 loss: 8.95328e-07
Iter: 1284 loss: 8.78403739e-07
Iter: 1285 loss: 8.77835305e-07
Iter: 1286 loss: 8.77852813e-07
Iter: 1287 loss: 8.77294724e-07
Iter: 1288 loss: 8.77699108e-07
Iter: 1289 loss: 8.76952186e-07
Iter: 1290 loss: 8.76478e-07
Iter: 1291 loss: 8.76365675e-07
Iter: 1292 loss: 8.76099193e-07
Iter: 1293 loss: 8.75499325e-07
Iter: 1294 loss: 8.80942252e-07
Iter: 1295 loss: 8.75431965e-07
Iter: 1296 loss: 8.7477963e-07
Iter: 1297 loss: 8.74924069e-07
Iter: 1298 loss: 8.74326304e-07
Iter: 1299 loss: 8.73651743e-07
Iter: 1300 loss: 8.73483941e-07
Iter: 1301 loss: 8.7309536e-07
Iter: 1302 loss: 8.72236853e-07
Iter: 1303 loss: 8.80950324e-07
Iter: 1304 loss: 8.7223043e-07
Iter: 1305 loss: 8.71666714e-07
Iter: 1306 loss: 8.77476737e-07
Iter: 1307 loss: 8.71657221e-07
Iter: 1308 loss: 8.71216457e-07
Iter: 1309 loss: 8.71401085e-07
Iter: 1310 loss: 8.70936674e-07
Iter: 1311 loss: 8.70440033e-07
Iter: 1312 loss: 8.7111107e-07
Iter: 1313 loss: 8.70179178e-07
Iter: 1314 loss: 8.69610801e-07
Iter: 1315 loss: 8.76360502e-07
Iter: 1316 loss: 8.69602673e-07
Iter: 1317 loss: 8.69325277e-07
Iter: 1318 loss: 8.68734105e-07
Iter: 1319 loss: 8.80451e-07
Iter: 1320 loss: 8.68748543e-07
Iter: 1321 loss: 8.68173288e-07
Iter: 1322 loss: 8.73946078e-07
Iter: 1323 loss: 8.68172322e-07
Iter: 1324 loss: 8.67583253e-07
Iter: 1325 loss: 8.67991218e-07
Iter: 1326 loss: 8.67194785e-07
Iter: 1327 loss: 8.66613277e-07
Iter: 1328 loss: 8.65758352e-07
Iter: 1329 loss: 8.65742891e-07
Iter: 1330 loss: 8.65263587e-07
Iter: 1331 loss: 8.6513154e-07
Iter: 1332 loss: 8.64667584e-07
Iter: 1333 loss: 8.64961294e-07
Iter: 1334 loss: 8.64356821e-07
Iter: 1335 loss: 8.63942773e-07
Iter: 1336 loss: 8.63925379e-07
Iter: 1337 loss: 8.63575e-07
Iter: 1338 loss: 8.62926584e-07
Iter: 1339 loss: 8.64298272e-07
Iter: 1340 loss: 8.62704269e-07
Iter: 1341 loss: 8.62183924e-07
Iter: 1342 loss: 8.62179434e-07
Iter: 1343 loss: 8.6175703e-07
Iter: 1344 loss: 8.62711261e-07
Iter: 1345 loss: 8.61582e-07
Iter: 1346 loss: 8.61098329e-07
Iter: 1347 loss: 8.60942066e-07
Iter: 1348 loss: 8.60671548e-07
Iter: 1349 loss: 8.60208331e-07
Iter: 1350 loss: 8.60202931e-07
Iter: 1351 loss: 8.59809461e-07
Iter: 1352 loss: 8.5947454e-07
Iter: 1353 loss: 8.59342379e-07
Iter: 1354 loss: 8.58846306e-07
Iter: 1355 loss: 8.58806288e-07
Iter: 1356 loss: 8.58469775e-07
Iter: 1357 loss: 8.58080625e-07
Iter: 1358 loss: 8.58009e-07
Iter: 1359 loss: 8.57700172e-07
Iter: 1360 loss: 8.57006512e-07
Iter: 1361 loss: 8.69067151e-07
Iter: 1362 loss: 8.56999634e-07
Iter: 1363 loss: 8.5624265e-07
Iter: 1364 loss: 8.58034184e-07
Iter: 1365 loss: 8.55983387e-07
Iter: 1366 loss: 8.55186045e-07
Iter: 1367 loss: 8.64149115e-07
Iter: 1368 loss: 8.5514597e-07
Iter: 1369 loss: 8.54708446e-07
Iter: 1370 loss: 8.54099881e-07
Iter: 1371 loss: 8.54065775e-07
Iter: 1372 loss: 8.53262236e-07
Iter: 1373 loss: 8.55711505e-07
Iter: 1374 loss: 8.53005417e-07
Iter: 1375 loss: 8.52430503e-07
Iter: 1376 loss: 8.58726821e-07
Iter: 1377 loss: 8.52435107e-07
Iter: 1378 loss: 8.51949835e-07
Iter: 1379 loss: 8.54658481e-07
Iter: 1380 loss: 8.51883499e-07
Iter: 1381 loss: 8.51474624e-07
Iter: 1382 loss: 8.51327172e-07
Iter: 1383 loss: 8.51059326e-07
Iter: 1384 loss: 8.50534434e-07
Iter: 1385 loss: 8.54138534e-07
Iter: 1386 loss: 8.50464914e-07
Iter: 1387 loss: 8.49924504e-07
Iter: 1388 loss: 8.51635605e-07
Iter: 1389 loss: 8.49728679e-07
Iter: 1390 loss: 8.4936562e-07
Iter: 1391 loss: 8.48699528e-07
Iter: 1392 loss: 8.65722711e-07
Iter: 1393 loss: 8.48695549e-07
Iter: 1394 loss: 8.47936462e-07
Iter: 1395 loss: 8.54191455e-07
Iter: 1396 loss: 8.47864385e-07
Iter: 1397 loss: 8.47235469e-07
Iter: 1398 loss: 8.53011784e-07
Iter: 1399 loss: 8.47231547e-07
Iter: 1400 loss: 8.46876503e-07
Iter: 1401 loss: 8.46138505e-07
Iter: 1402 loss: 8.58499391e-07
Iter: 1403 loss: 8.46136345e-07
Iter: 1404 loss: 8.45556883e-07
Iter: 1405 loss: 8.54753239e-07
Iter: 1406 loss: 8.45543582e-07
Iter: 1407 loss: 8.44961676e-07
Iter: 1408 loss: 8.45554382e-07
Iter: 1409 loss: 8.44645342e-07
Iter: 1410 loss: 8.44047577e-07
Iter: 1411 loss: 8.4381378e-07
Iter: 1412 loss: 8.43491421e-07
Iter: 1413 loss: 8.4257988e-07
Iter: 1414 loss: 8.45158297e-07
Iter: 1415 loss: 8.4230561e-07
Iter: 1416 loss: 8.41607061e-07
Iter: 1417 loss: 8.47360411e-07
Iter: 1418 loss: 8.41557608e-07
Iter: 1419 loss: 8.4101805e-07
Iter: 1420 loss: 8.44628062e-07
Iter: 1421 loss: 8.40978373e-07
Iter: 1422 loss: 8.40451094e-07
Iter: 1423 loss: 8.41507926e-07
Iter: 1424 loss: 8.40218831e-07
Iter: 1425 loss: 8.39677796e-07
Iter: 1426 loss: 8.39827749e-07
Iter: 1427 loss: 8.39329743e-07
Iter: 1428 loss: 8.38963558e-07
Iter: 1429 loss: 8.38940366e-07
Iter: 1430 loss: 8.38595497e-07
Iter: 1431 loss: 8.37870118e-07
Iter: 1432 loss: 8.49858e-07
Iter: 1433 loss: 8.37841469e-07
Iter: 1434 loss: 8.37115124e-07
Iter: 1435 loss: 8.38341805e-07
Iter: 1436 loss: 8.36792481e-07
Iter: 1437 loss: 8.36326933e-07
Iter: 1438 loss: 8.36301126e-07
Iter: 1439 loss: 8.35830804e-07
Iter: 1440 loss: 8.35377193e-07
Iter: 1441 loss: 8.3531063e-07
Iter: 1442 loss: 8.34759248e-07
Iter: 1443 loss: 8.37099606e-07
Iter: 1444 loss: 8.34701837e-07
Iter: 1445 loss: 8.3406826e-07
Iter: 1446 loss: 8.35692731e-07
Iter: 1447 loss: 8.33847253e-07
Iter: 1448 loss: 8.334e-07
Iter: 1449 loss: 8.333908e-07
Iter: 1450 loss: 8.33012848e-07
Iter: 1451 loss: 8.32453338e-07
Iter: 1452 loss: 8.33647846e-07
Iter: 1453 loss: 8.3221147e-07
Iter: 1454 loss: 8.31520936e-07
Iter: 1455 loss: 8.33690535e-07
Iter: 1456 loss: 8.3132835e-07
Iter: 1457 loss: 8.30584838e-07
Iter: 1458 loss: 8.32059754e-07
Iter: 1459 loss: 8.30260774e-07
Iter: 1460 loss: 8.29841213e-07
Iter: 1461 loss: 8.29817338e-07
Iter: 1462 loss: 8.29436317e-07
Iter: 1463 loss: 8.29650901e-07
Iter: 1464 loss: 8.29196665e-07
Iter: 1465 loss: 8.28857935e-07
Iter: 1466 loss: 8.2857639e-07
Iter: 1467 loss: 8.28493626e-07
Iter: 1468 loss: 8.28011878e-07
Iter: 1469 loss: 8.28027908e-07
Iter: 1470 loss: 8.2773829e-07
Iter: 1471 loss: 8.27323788e-07
Iter: 1472 loss: 8.27299573e-07
Iter: 1473 loss: 8.26760925e-07
Iter: 1474 loss: 8.276009e-07
Iter: 1475 loss: 8.26542873e-07
Iter: 1476 loss: 8.2596739e-07
Iter: 1477 loss: 8.33727427e-07
Iter: 1478 loss: 8.25964e-07
Iter: 1479 loss: 8.25560903e-07
Iter: 1480 loss: 8.2505295e-07
Iter: 1481 loss: 8.25001e-07
Iter: 1482 loss: 8.24518338e-07
Iter: 1483 loss: 8.24523e-07
Iter: 1484 loss: 8.24167046e-07
Iter: 1485 loss: 8.23714e-07
Iter: 1486 loss: 8.23663186e-07
Iter: 1487 loss: 8.23144262e-07
Iter: 1488 loss: 8.25143502e-07
Iter: 1489 loss: 8.23061e-07
Iter: 1490 loss: 8.2260533e-07
Iter: 1491 loss: 8.23974517e-07
Iter: 1492 loss: 8.22473567e-07
Iter: 1493 loss: 8.21972833e-07
Iter: 1494 loss: 8.21611138e-07
Iter: 1495 loss: 8.21479659e-07
Iter: 1496 loss: 8.21098922e-07
Iter: 1497 loss: 8.21003312e-07
Iter: 1498 loss: 8.20585285e-07
Iter: 1499 loss: 8.20625189e-07
Iter: 1500 loss: 8.20263722e-07
Iter: 1501 loss: 8.19815796e-07
Iter: 1502 loss: 8.19696766e-07
Iter: 1503 loss: 8.1944853e-07
Iter: 1504 loss: 8.18780052e-07
Iter: 1505 loss: 8.27045255e-07
Iter: 1506 loss: 8.18786418e-07
Iter: 1507 loss: 8.18479123e-07
Iter: 1508 loss: 8.18031367e-07
Iter: 1509 loss: 8.18018293e-07
Iter: 1510 loss: 8.17549505e-07
Iter: 1511 loss: 8.22550817e-07
Iter: 1512 loss: 8.17532964e-07
Iter: 1513 loss: 8.17116302e-07
Iter: 1514 loss: 8.17781824e-07
Iter: 1515 loss: 8.16902684e-07
Iter: 1516 loss: 8.16600505e-07
Iter: 1517 loss: 8.17475325e-07
Iter: 1518 loss: 8.1649091e-07
Iter: 1519 loss: 8.16001375e-07
Iter: 1520 loss: 8.16546333e-07
Iter: 1521 loss: 8.15734893e-07
Iter: 1522 loss: 8.15293504e-07
Iter: 1523 loss: 8.15409578e-07
Iter: 1524 loss: 8.14975067e-07
Iter: 1525 loss: 8.14446707e-07
Iter: 1526 loss: 8.16043269e-07
Iter: 1527 loss: 8.14276518e-07
Iter: 1528 loss: 8.13649137e-07
Iter: 1529 loss: 8.14796635e-07
Iter: 1530 loss: 8.13354177e-07
Iter: 1531 loss: 8.12905341e-07
Iter: 1532 loss: 8.15919918e-07
Iter: 1533 loss: 8.1287078e-07
Iter: 1534 loss: 8.12392557e-07
Iter: 1535 loss: 8.15182602e-07
Iter: 1536 loss: 8.12348731e-07
Iter: 1537 loss: 8.12027565e-07
Iter: 1538 loss: 8.11537234e-07
Iter: 1539 loss: 8.11546272e-07
Iter: 1540 loss: 8.11194468e-07
Iter: 1541 loss: 8.11138136e-07
Iter: 1542 loss: 8.10791903e-07
Iter: 1543 loss: 8.10122344e-07
Iter: 1544 loss: 8.23152789e-07
Iter: 1545 loss: 8.10125641e-07
Iter: 1546 loss: 8.09563062e-07
Iter: 1547 loss: 8.13820691e-07
Iter: 1548 loss: 8.09506389e-07
Iter: 1549 loss: 8.08925677e-07
Iter: 1550 loss: 8.11208e-07
Iter: 1551 loss: 8.08770437e-07
Iter: 1552 loss: 8.08332459e-07
Iter: 1553 loss: 8.0825231e-07
Iter: 1554 loss: 8.07953029e-07
Iter: 1555 loss: 8.07434844e-07
Iter: 1556 loss: 8.07427909e-07
Iter: 1557 loss: 8.07114134e-07
Iter: 1558 loss: 8.0665427e-07
Iter: 1559 loss: 8.06651144e-07
Iter: 1560 loss: 8.06058779e-07
Iter: 1561 loss: 8.07400568e-07
Iter: 1562 loss: 8.05824129e-07
Iter: 1563 loss: 8.05158038e-07
Iter: 1564 loss: 8.08034429e-07
Iter: 1565 loss: 8.05010927e-07
Iter: 1566 loss: 8.0444272e-07
Iter: 1567 loss: 8.04887577e-07
Iter: 1568 loss: 8.0409518e-07
Iter: 1569 loss: 8.03754801e-07
Iter: 1570 loss: 8.03671696e-07
Iter: 1571 loss: 8.03376395e-07
Iter: 1572 loss: 8.02793238e-07
Iter: 1573 loss: 8.16756312e-07
Iter: 1574 loss: 8.02785678e-07
Iter: 1575 loss: 8.02372142e-07
Iter: 1576 loss: 8.08660729e-07
Iter: 1577 loss: 8.02381521e-07
Iter: 1578 loss: 8.01973556e-07
Iter: 1579 loss: 8.01928252e-07
Iter: 1580 loss: 8.01611918e-07
Iter: 1581 loss: 8.01262729e-07
Iter: 1582 loss: 8.01011652e-07
Iter: 1583 loss: 8.00863688e-07
Iter: 1584 loss: 8.00352893e-07
Iter: 1585 loss: 8.05562877e-07
Iter: 1586 loss: 8.00317821e-07
Iter: 1587 loss: 7.99724205e-07
Iter: 1588 loss: 8.00240514e-07
Iter: 1589 loss: 7.99373879e-07
Iter: 1590 loss: 7.98935332e-07
Iter: 1591 loss: 7.99367058e-07
Iter: 1592 loss: 7.98661517e-07
Iter: 1593 loss: 7.98050905e-07
Iter: 1594 loss: 8.03749117e-07
Iter: 1595 loss: 7.98038e-07
Iter: 1596 loss: 7.97644759e-07
Iter: 1597 loss: 7.97187852e-07
Iter: 1598 loss: 7.97138455e-07
Iter: 1599 loss: 7.96570816e-07
Iter: 1600 loss: 7.9960364e-07
Iter: 1601 loss: 7.96507038e-07
Iter: 1602 loss: 7.95979304e-07
Iter: 1603 loss: 7.97609118e-07
Iter: 1604 loss: 7.95837479e-07
Iter: 1605 loss: 7.95502217e-07
Iter: 1606 loss: 8.00494036e-07
Iter: 1607 loss: 7.95490791e-07
Iter: 1608 loss: 7.95138931e-07
Iter: 1609 loss: 7.94664629e-07
Iter: 1610 loss: 7.94649111e-07
Iter: 1611 loss: 7.942715e-07
Iter: 1612 loss: 7.98230872e-07
Iter: 1613 loss: 7.94263e-07
Iter: 1614 loss: 7.93847903e-07
Iter: 1615 loss: 7.93673905e-07
Iter: 1616 loss: 7.93458e-07
Iter: 1617 loss: 7.92934657e-07
Iter: 1618 loss: 7.92747869e-07
Iter: 1619 loss: 7.92466949e-07
Iter: 1620 loss: 7.91943876e-07
Iter: 1621 loss: 7.96556435e-07
Iter: 1622 loss: 7.91900675e-07
Iter: 1623 loss: 7.91380103e-07
Iter: 1624 loss: 7.94299524e-07
Iter: 1625 loss: 7.91309049e-07
Iter: 1626 loss: 7.91020057e-07
Iter: 1627 loss: 7.90723561e-07
Iter: 1628 loss: 7.90674108e-07
Iter: 1629 loss: 7.90236072e-07
Iter: 1630 loss: 7.9671662e-07
Iter: 1631 loss: 7.90240961e-07
Iter: 1632 loss: 7.89881256e-07
Iter: 1633 loss: 7.89227101e-07
Iter: 1634 loss: 7.89220394e-07
Iter: 1635 loss: 7.88630132e-07
Iter: 1636 loss: 7.91029947e-07
Iter: 1637 loss: 7.88515877e-07
Iter: 1638 loss: 7.87953923e-07
Iter: 1639 loss: 7.91200705e-07
Iter: 1640 loss: 7.87836484e-07
Iter: 1641 loss: 7.87487124e-07
Iter: 1642 loss: 7.92109176e-07
Iter: 1643 loss: 7.87498834e-07
Iter: 1644 loss: 7.87171473e-07
Iter: 1645 loss: 7.87068814e-07
Iter: 1646 loss: 7.86932901e-07
Iter: 1647 loss: 7.86596274e-07
Iter: 1648 loss: 7.87670274e-07
Iter: 1649 loss: 7.86499186e-07
Iter: 1650 loss: 7.86074111e-07
Iter: 1651 loss: 7.87024476e-07
Iter: 1652 loss: 7.85914438e-07
Iter: 1653 loss: 7.85611064e-07
Iter: 1654 loss: 7.85075486e-07
Iter: 1655 loss: 7.85089128e-07
Iter: 1656 loss: 7.84429517e-07
Iter: 1657 loss: 7.85979807e-07
Iter: 1658 loss: 7.84195663e-07
Iter: 1659 loss: 7.83616258e-07
Iter: 1660 loss: 7.83605174e-07
Iter: 1661 loss: 7.83067492e-07
Iter: 1662 loss: 7.82813174e-07
Iter: 1663 loss: 7.825588e-07
Iter: 1664 loss: 7.82162488e-07
Iter: 1665 loss: 7.85436782e-07
Iter: 1666 loss: 7.82128552e-07
Iter: 1667 loss: 7.81618496e-07
Iter: 1668 loss: 7.8196382e-07
Iter: 1669 loss: 7.81325298e-07
Iter: 1670 loss: 7.8092512e-07
Iter: 1671 loss: 7.80942457e-07
Iter: 1672 loss: 7.80613846e-07
Iter: 1673 loss: 7.80086395e-07
Iter: 1674 loss: 7.82443919e-07
Iter: 1675 loss: 7.80010282e-07
Iter: 1676 loss: 7.79479194e-07
Iter: 1677 loss: 7.82938514e-07
Iter: 1678 loss: 7.79412915e-07
Iter: 1679 loss: 7.78929575e-07
Iter: 1680 loss: 7.7953581e-07
Iter: 1681 loss: 7.78697654e-07
Iter: 1682 loss: 7.78189928e-07
Iter: 1683 loss: 7.77961532e-07
Iter: 1684 loss: 7.77701871e-07
Iter: 1685 loss: 7.77260652e-07
Iter: 1686 loss: 7.77230525e-07
Iter: 1687 loss: 7.76961826e-07
Iter: 1688 loss: 7.76330921e-07
Iter: 1689 loss: 7.85008297e-07
Iter: 1690 loss: 7.76302954e-07
Iter: 1691 loss: 7.75555691e-07
Iter: 1692 loss: 7.77033108e-07
Iter: 1693 loss: 7.752584e-07
Iter: 1694 loss: 7.74836963e-07
Iter: 1695 loss: 7.74811269e-07
Iter: 1696 loss: 7.74360956e-07
Iter: 1697 loss: 7.74927685e-07
Iter: 1698 loss: 7.74131422e-07
Iter: 1699 loss: 7.73734143e-07
Iter: 1700 loss: 7.74196e-07
Iter: 1701 loss: 7.73517058e-07
Iter: 1702 loss: 7.72818e-07
Iter: 1703 loss: 7.74187129e-07
Iter: 1704 loss: 7.72532871e-07
Iter: 1705 loss: 7.72051067e-07
Iter: 1706 loss: 7.71700115e-07
Iter: 1707 loss: 7.71522366e-07
Iter: 1708 loss: 7.70925112e-07
Iter: 1709 loss: 7.74414104e-07
Iter: 1710 loss: 7.70830184e-07
Iter: 1711 loss: 7.70531472e-07
Iter: 1712 loss: 7.70500264e-07
Iter: 1713 loss: 7.70275392e-07
Iter: 1714 loss: 7.69896474e-07
Iter: 1715 loss: 7.77750188e-07
Iter: 1716 loss: 7.69875385e-07
Iter: 1717 loss: 7.69528924e-07
Iter: 1718 loss: 7.73769727e-07
Iter: 1719 loss: 7.69524036e-07
Iter: 1720 loss: 7.6916217e-07
Iter: 1721 loss: 7.68985387e-07
Iter: 1722 loss: 7.68786208e-07
Iter: 1723 loss: 7.68253358e-07
Iter: 1724 loss: 7.69467874e-07
Iter: 1725 loss: 7.68077712e-07
Iter: 1726 loss: 7.67497966e-07
Iter: 1727 loss: 7.67763481e-07
Iter: 1728 loss: 7.67145e-07
Iter: 1729 loss: 7.66445226e-07
Iter: 1730 loss: 7.66889286e-07
Iter: 1731 loss: 7.6600503e-07
Iter: 1732 loss: 7.65561254e-07
Iter: 1733 loss: 7.65496623e-07
Iter: 1734 loss: 7.65096843e-07
Iter: 1735 loss: 7.65587174e-07
Iter: 1736 loss: 7.64893286e-07
Iter: 1737 loss: 7.64600088e-07
Iter: 1738 loss: 7.64965534e-07
Iter: 1739 loss: 7.64405797e-07
Iter: 1740 loss: 7.63894491e-07
Iter: 1741 loss: 7.64382321e-07
Iter: 1742 loss: 7.63568892e-07
Iter: 1743 loss: 7.63063554e-07
Iter: 1744 loss: 7.62624836e-07
Iter: 1745 loss: 7.62490117e-07
Iter: 1746 loss: 7.62388936e-07
Iter: 1747 loss: 7.62137915e-07
Iter: 1748 loss: 7.61796059e-07
Iter: 1749 loss: 7.61074602e-07
Iter: 1750 loss: 7.72692033e-07
Iter: 1751 loss: 7.61053229e-07
Iter: 1752 loss: 7.60495084e-07
Iter: 1753 loss: 7.65568529e-07
Iter: 1754 loss: 7.60467742e-07
Iter: 1755 loss: 7.59966269e-07
Iter: 1756 loss: 7.63062303e-07
Iter: 1757 loss: 7.59894533e-07
Iter: 1758 loss: 7.59629302e-07
Iter: 1759 loss: 7.58931719e-07
Iter: 1760 loss: 7.67404515e-07
Iter: 1761 loss: 7.5891711e-07
Iter: 1762 loss: 7.58225383e-07
Iter: 1763 loss: 7.61487968e-07
Iter: 1764 loss: 7.58111e-07
Iter: 1765 loss: 7.57477437e-07
Iter: 1766 loss: 7.59577688e-07
Iter: 1767 loss: 7.57293662e-07
Iter: 1768 loss: 7.56776217e-07
Iter: 1769 loss: 7.65112304e-07
Iter: 1770 loss: 7.56774057e-07
Iter: 1771 loss: 7.56379393e-07
Iter: 1772 loss: 7.56265763e-07
Iter: 1773 loss: 7.56026566e-07
Iter: 1774 loss: 7.55472627e-07
Iter: 1775 loss: 7.57674627e-07
Iter: 1776 loss: 7.55352971e-07
Iter: 1777 loss: 7.54992755e-07
Iter: 1778 loss: 7.57321459e-07
Iter: 1779 loss: 7.54939606e-07
Iter: 1780 loss: 7.54595931e-07
Iter: 1781 loss: 7.54863606e-07
Iter: 1782 loss: 7.54373048e-07
Iter: 1783 loss: 7.54013513e-07
Iter: 1784 loss: 7.55676922e-07
Iter: 1785 loss: 7.53965082e-07
Iter: 1786 loss: 7.53487768e-07
Iter: 1787 loss: 7.53806603e-07
Iter: 1788 loss: 7.53208155e-07
Iter: 1789 loss: 7.52813492e-07
Iter: 1790 loss: 7.5230696e-07
Iter: 1791 loss: 7.52284336e-07
Iter: 1792 loss: 7.51967718e-07
Iter: 1793 loss: 7.51879611e-07
Iter: 1794 loss: 7.51560492e-07
Iter: 1795 loss: 7.50904519e-07
Iter: 1796 loss: 7.61976708e-07
Iter: 1797 loss: 7.50903268e-07
Iter: 1798 loss: 7.50351887e-07
Iter: 1799 loss: 7.51945095e-07
Iter: 1800 loss: 7.50144522e-07
Iter: 1801 loss: 7.49613719e-07
Iter: 1802 loss: 7.49981155e-07
Iter: 1803 loss: 7.49268906e-07
Iter: 1804 loss: 7.49125e-07
Iter: 1805 loss: 7.4894848e-07
Iter: 1806 loss: 7.48695641e-07
Iter: 1807 loss: 7.48501407e-07
Iter: 1808 loss: 7.48413527e-07
Iter: 1809 loss: 7.47987826e-07
Iter: 1810 loss: 7.4885628e-07
Iter: 1811 loss: 7.47856518e-07
Iter: 1812 loss: 7.47416095e-07
Iter: 1813 loss: 7.49516175e-07
Iter: 1814 loss: 7.47308206e-07
Iter: 1815 loss: 7.4696294e-07
Iter: 1816 loss: 7.46729597e-07
Iter: 1817 loss: 7.46605394e-07
Iter: 1818 loss: 7.46235e-07
Iter: 1819 loss: 7.4620641e-07
Iter: 1820 loss: 7.46018429e-07
Iter: 1821 loss: 7.45590512e-07
Iter: 1822 loss: 7.51563107e-07
Iter: 1823 loss: 7.45540262e-07
Iter: 1824 loss: 7.45107741e-07
Iter: 1825 loss: 7.49677895e-07
Iter: 1826 loss: 7.45114733e-07
Iter: 1827 loss: 7.44760882e-07
Iter: 1828 loss: 7.45502859e-07
Iter: 1829 loss: 7.44586373e-07
Iter: 1830 loss: 7.44177896e-07
Iter: 1831 loss: 7.44492695e-07
Iter: 1832 loss: 7.43917951e-07
Iter: 1833 loss: 7.43441831e-07
Iter: 1834 loss: 7.43054784e-07
Iter: 1835 loss: 7.42927568e-07
Iter: 1836 loss: 7.42213615e-07
Iter: 1837 loss: 7.4561666e-07
Iter: 1838 loss: 7.42083e-07
Iter: 1839 loss: 7.41717713e-07
Iter: 1840 loss: 7.41699694e-07
Iter: 1841 loss: 7.41350107e-07
Iter: 1842 loss: 7.41760061e-07
Iter: 1843 loss: 7.41186909e-07
Iter: 1844 loss: 7.40954931e-07
Iter: 1845 loss: 7.41416557e-07
Iter: 1846 loss: 7.40862902e-07
Iter: 1847 loss: 7.40547137e-07
Iter: 1848 loss: 7.41349197e-07
Iter: 1849 loss: 7.40441e-07
Iter: 1850 loss: 7.40132634e-07
Iter: 1851 loss: 7.40077212e-07
Iter: 1852 loss: 7.39854556e-07
Iter: 1853 loss: 7.39496727e-07
Iter: 1854 loss: 7.39482516e-07
Iter: 1855 loss: 7.39258439e-07
Iter: 1856 loss: 7.38710696e-07
Iter: 1857 loss: 7.46128194e-07
Iter: 1858 loss: 7.3867875e-07
Iter: 1859 loss: 7.38297842e-07
Iter: 1860 loss: 7.38285735e-07
Iter: 1861 loss: 7.37910341e-07
Iter: 1862 loss: 7.38411416e-07
Iter: 1863 loss: 7.37730716e-07
Iter: 1864 loss: 7.37461278e-07
Iter: 1865 loss: 7.38580411e-07
Iter: 1866 loss: 7.37379e-07
Iter: 1867 loss: 7.37102312e-07
Iter: 1868 loss: 7.36973391e-07
Iter: 1869 loss: 7.36831e-07
Iter: 1870 loss: 7.36413199e-07
Iter: 1871 loss: 7.36725042e-07
Iter: 1872 loss: 7.3613171e-07
Iter: 1873 loss: 7.35678e-07
Iter: 1874 loss: 7.37622713e-07
Iter: 1875 loss: 7.35566516e-07
Iter: 1876 loss: 7.35010417e-07
Iter: 1877 loss: 7.39995e-07
Iter: 1878 loss: 7.35002e-07
Iter: 1879 loss: 7.34714604e-07
Iter: 1880 loss: 7.34188575e-07
Iter: 1881 loss: 7.44220699e-07
Iter: 1882 loss: 7.34183743e-07
Iter: 1883 loss: 7.33895263e-07
Iter: 1884 loss: 7.33829893e-07
Iter: 1885 loss: 7.33605e-07
Iter: 1886 loss: 7.33581601e-07
Iter: 1887 loss: 7.33391e-07
Iter: 1888 loss: 7.33145839e-07
Iter: 1889 loss: 7.33139359e-07
Iter: 1890 loss: 7.32952515e-07
Iter: 1891 loss: 7.32533067e-07
Iter: 1892 loss: 7.37424557e-07
Iter: 1893 loss: 7.32476963e-07
Iter: 1894 loss: 7.32010392e-07
Iter: 1895 loss: 7.34031971e-07
Iter: 1896 loss: 7.31929504e-07
Iter: 1897 loss: 7.31449632e-07
Iter: 1898 loss: 7.35960953e-07
Iter: 1899 loss: 7.31401656e-07
Iter: 1900 loss: 7.3113614e-07
Iter: 1901 loss: 7.30784109e-07
Iter: 1902 loss: 7.30753754e-07
Iter: 1903 loss: 7.3021215e-07
Iter: 1904 loss: 7.32923695e-07
Iter: 1905 loss: 7.30134047e-07
Iter: 1906 loss: 7.29724718e-07
Iter: 1907 loss: 7.30380521e-07
Iter: 1908 loss: 7.29537874e-07
Iter: 1909 loss: 7.29118085e-07
Iter: 1910 loss: 7.29428052e-07
Iter: 1911 loss: 7.28870873e-07
Iter: 1912 loss: 7.28622695e-07
Iter: 1913 loss: 7.28589725e-07
Iter: 1914 loss: 7.28338762e-07
Iter: 1915 loss: 7.28303576e-07
Iter: 1916 loss: 7.28104624e-07
Iter: 1917 loss: 7.27796078e-07
Iter: 1918 loss: 7.27175461e-07
Iter: 1919 loss: 7.40926112e-07
Iter: 1920 loss: 7.27174324e-07
Iter: 1921 loss: 7.26833264e-07
Iter: 1922 loss: 7.26784151e-07
Iter: 1923 loss: 7.26398525e-07
Iter: 1924 loss: 7.27555346e-07
Iter: 1925 loss: 7.26239819e-07
Iter: 1926 loss: 7.2596174e-07
Iter: 1927 loss: 7.25774953e-07
Iter: 1928 loss: 7.25642622e-07
Iter: 1929 loss: 7.25385917e-07
Iter: 1930 loss: 7.25321172e-07
Iter: 1931 loss: 7.25169571e-07
Iter: 1932 loss: 7.24824645e-07
Iter: 1933 loss: 7.32046715e-07
Iter: 1934 loss: 7.24803385e-07
Iter: 1935 loss: 7.24586926e-07
Iter: 1936 loss: 7.2457965e-07
Iter: 1937 loss: 7.2433e-07
Iter: 1938 loss: 7.23896505e-07
Iter: 1939 loss: 7.23891958e-07
Iter: 1940 loss: 7.23429594e-07
Iter: 1941 loss: 7.25010182e-07
Iter: 1942 loss: 7.23302492e-07
Iter: 1943 loss: 7.22830805e-07
Iter: 1944 loss: 7.23430048e-07
Iter: 1945 loss: 7.22570121e-07
Iter: 1946 loss: 7.2210355e-07
Iter: 1947 loss: 7.232548e-07
Iter: 1948 loss: 7.21936658e-07
Iter: 1949 loss: 7.21740491e-07
Iter: 1950 loss: 7.21705248e-07
Iter: 1951 loss: 7.21506353e-07
Iter: 1952 loss: 7.21093102e-07
Iter: 1953 loss: 7.29219096e-07
Iter: 1954 loss: 7.21107824e-07
Iter: 1955 loss: 7.207301e-07
Iter: 1956 loss: 7.21479864e-07
Iter: 1957 loss: 7.20606067e-07
Iter: 1958 loss: 7.20260459e-07
Iter: 1959 loss: 7.24562653e-07
Iter: 1960 loss: 7.20255912e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6
+ date
Wed Oct 21 10:54:04 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1 --function f1 --psi -2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4a2d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4ab8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4ab8b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc497e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc497e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4a360d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc49e7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48fb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48fb2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc49468c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc494f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48de730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48dee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4868510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4858840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48ab6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48ab510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc15f5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc15a8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc15c2f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc47ff840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc47fff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc47b7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14d0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14d0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14d02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc1497950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14ac598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14ac488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14486a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc1488f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc13a01e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc1398620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc13989d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc1517268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc13cf8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.024691213
test_loss: 0.0237323
train_loss: 0.010842392
test_loss: 0.011419016
train_loss: 0.007116771
test_loss: 0.007922669
train_loss: 0.0064131664
test_loss: 0.006636905
train_loss: 0.005281979
test_loss: 0.006147934
train_loss: 0.0047164983
test_loss: 0.0057614245
train_loss: 0.004674913
test_loss: 0.005322989
train_loss: 0.0045824
test_loss: 0.005414203
train_loss: 0.0041051935
test_loss: 0.0052017383
train_loss: 0.004320536
test_loss: 0.005181114
train_loss: 0.004142129
test_loss: 0.0050392495
train_loss: 0.004244495
test_loss: 0.0049497406
train_loss: 0.0042857896
test_loss: 0.0050960793
train_loss: 0.0042080595
test_loss: 0.0051698475
train_loss: 0.0043510217
test_loss: 0.0050986395
train_loss: 0.004027813
test_loss: 0.0049993377
train_loss: 0.0038732968
test_loss: 0.004940592
train_loss: 0.0040004146
test_loss: 0.0049524987
train_loss: 0.0039799362
test_loss: 0.005197263
train_loss: 0.004079961
test_loss: 0.0051902323
train_loss: 0.0038258857
test_loss: 0.0048476877
train_loss: 0.0037381086
test_loss: 0.0051992675
train_loss: 0.0039823186
test_loss: 0.0051554856
train_loss: 0.0038241805
test_loss: 0.004694985
train_loss: 0.004007274
test_loss: 0.004820642
train_loss: 0.0039051543
test_loss: 0.0050420915
train_loss: 0.00395396
test_loss: 0.004856654
train_loss: 0.0035287442
test_loss: 0.004927245
train_loss: 0.00350927
test_loss: 0.0046581747
train_loss: 0.00418158
test_loss: 0.005022465
train_loss: 0.0038871085
test_loss: 0.004700234
train_loss: 0.0038000366
test_loss: 0.004685028
train_loss: 0.003844186
test_loss: 0.0046972265
train_loss: 0.0038138689
test_loss: 0.004952
train_loss: 0.0035941121
test_loss: 0.0049776454
train_loss: 0.003615374
test_loss: 0.004712863
train_loss: 0.0039812727
test_loss: 0.0047391746
train_loss: 0.0037626356
test_loss: 0.0047305925
train_loss: 0.003762607
test_loss: 0.004675068
train_loss: 0.0037230016
test_loss: 0.004727916
train_loss: 0.003724188
test_loss: 0.0046702963
train_loss: 0.0039138356
test_loss: 0.0050081788
train_loss: 0.0036303664
test_loss: 0.004895965
train_loss: 0.003679594
test_loss: 0.0047841244
train_loss: 0.0034801103
test_loss: 0.004692602
train_loss: 0.0034944648
test_loss: 0.004569122
train_loss: 0.0039518345
test_loss: 0.004640182
train_loss: 0.003487298
test_loss: 0.004734398
train_loss: 0.004106274
test_loss: 0.005033981
train_loss: 0.003713897
test_loss: 0.004684965
train_loss: 0.0037537552
test_loss: 0.004660607
train_loss: 0.0034680252
test_loss: 0.004712301
train_loss: 0.0035195854
test_loss: 0.0046598194
train_loss: 0.00362779
test_loss: 0.0048129107
train_loss: 0.003804081
test_loss: 0.004764039
train_loss: 0.0036958647
test_loss: 0.0045626042
train_loss: 0.003851566
test_loss: 0.0048199533
train_loss: 0.0036141388
test_loss: 0.0045647663
train_loss: 0.0033203545
test_loss: 0.0046149786
train_loss: 0.0035452119
test_loss: 0.0047885654
train_loss: 0.0036022621
test_loss: 0.0045928126
train_loss: 0.0037102306
test_loss: 0.0045405896
train_loss: 0.0037369826
test_loss: 0.004641317
train_loss: 0.0034051647
test_loss: 0.004583089
train_loss: 0.0035641866
test_loss: 0.0045976713
train_loss: 0.0034342837
test_loss: 0.0046813004
train_loss: 0.0035445197
test_loss: 0.004801152
train_loss: 0.0035137886
test_loss: 0.0047444855
train_loss: 0.003620115
test_loss: 0.0046300823
train_loss: 0.0036195056
test_loss: 0.0047449097
train_loss: 0.0037288738
test_loss: 0.004566075
train_loss: 0.0035948516
test_loss: 0.004673381
train_loss: 0.0037011078
test_loss: 0.0047536506
train_loss: 0.003648867
test_loss: 0.0045618773
train_loss: 0.0034338206
test_loss: 0.004595958
train_loss: 0.0033270912
test_loss: 0.0046496405
train_loss: 0.0036668035
test_loss: 0.0044393134
train_loss: 0.003586249
test_loss: 0.004757923
train_loss: 0.0035798892
test_loss: 0.0044870046
train_loss: 0.0037309397
test_loss: 0.004747658
train_loss: 0.003694287
test_loss: 0.004675271
train_loss: 0.00357808
test_loss: 0.004722557
train_loss: 0.004040352
test_loss: 0.0046410384
train_loss: 0.0035997597
test_loss: 0.0045483923
train_loss: 0.003485582
test_loss: 0.004665477
train_loss: 0.003309299
test_loss: 0.0045024767
train_loss: 0.0036919415
test_loss: 0.0046965927
train_loss: 0.0034668879
test_loss: 0.004543792
train_loss: 0.003545037
test_loss: 0.004533045
train_loss: 0.0037685272
test_loss: 0.004732998
train_loss: 0.0037402278
test_loss: 0.004585319
train_loss: 0.0041473396
test_loss: 0.004862735
train_loss: 0.0037072808
test_loss: 0.004615098
train_loss: 0.0036835764
test_loss: 0.0046203835
train_loss: 0.0033572377
test_loss: 0.004481441
train_loss: 0.0035815062
test_loss: 0.004567552
train_loss: 0.0037262528
test_loss: 0.004621657
train_loss: 0.003776738
test_loss: 0.004882351
train_loss: 0.0035449623
test_loss: 0.004597182
train_loss: 0.0034089475
test_loss: 0.004543895
train_loss: 0.0032098396
test_loss: 0.0042779325
train_loss: 0.0033442576
test_loss: 0.0045264564
train_loss: 0.003332823
test_loss: 0.0046022004
train_loss: 0.0030668227
test_loss: 0.0043513263
train_loss: 0.0038594585
test_loss: 0.0044974703
train_loss: 0.0044911033
test_loss: 0.005063654
train_loss: 0.0034529855
test_loss: 0.0047582258
train_loss: 0.0034862724
test_loss: 0.004475605
train_loss: 0.003520229
test_loss: 0.0045257397
train_loss: 0.0034551537
test_loss: 0.0044991365
train_loss: 0.0034477445
test_loss: 0.0044286423
train_loss: 0.0037994375
test_loss: 0.004737329
train_loss: 0.0036487337
test_loss: 0.0046095243
train_loss: 0.003500712
test_loss: 0.004721702
train_loss: 0.0033969027
test_loss: 0.004482158
train_loss: 0.003571251
test_loss: 0.0045729843
train_loss: 0.003546694
test_loss: 0.0044388846
train_loss: 0.0032294015
test_loss: 0.0045479117
train_loss: 0.003600503
test_loss: 0.0046392516
train_loss: 0.0037136697
test_loss: 0.0046531297
train_loss: 0.0031953603
test_loss: 0.004419859
train_loss: 0.0035659834
test_loss: 0.004576433
train_loss: 0.0034946767
test_loss: 0.0045767007
train_loss: 0.0034131955
test_loss: 0.0044750194
train_loss: 0.0034713657
test_loss: 0.004516781
train_loss: 0.003486358
test_loss: 0.0044710836
train_loss: 0.003476811
test_loss: 0.0045201695
train_loss: 0.0033427551
test_loss: 0.004725129
train_loss: 0.0034218347
test_loss: 0.0045927796
train_loss: 0.0037656561
test_loss: 0.0046469034
train_loss: 0.0034300836
test_loss: 0.0047430396
train_loss: 0.00374057
test_loss: 0.004963045
train_loss: 0.0035538687
test_loss: 0.004936328
train_loss: 0.0033708422
test_loss: 0.0044534416
train_loss: 0.0033620833
test_loss: 0.004463273
train_loss: 0.0031597614
test_loss: 0.0044799014
train_loss: 0.0034665735
test_loss: 0.004650817
train_loss: 0.0033823787
test_loss: 0.004686303
train_loss: 0.0030904058
test_loss: 0.0044926507
train_loss: 0.003257215
test_loss: 0.004431937
train_loss: 0.003704973
test_loss: 0.004363583
train_loss: 0.0033866125
test_loss: 0.004856785
train_loss: 0.0037239569
test_loss: 0.0047267633
train_loss: 0.0035857032
test_loss: 0.004709822
train_loss: 0.0034022275
test_loss: 0.0045679267
train_loss: 0.0036118417
test_loss: 0.004500926
train_loss: 0.0034407575
test_loss: 0.004474888
train_loss: 0.0033970731
test_loss: 0.0045028604
train_loss: 0.0035017228
test_loss: 0.0044638696
train_loss: 0.0036691641
test_loss: 0.004441474
train_loss: 0.0037063167
test_loss: 0.0047729043
train_loss: 0.00347918
test_loss: 0.0043853167
train_loss: 0.0037614824
test_loss: 0.0045680236
train_loss: 0.0031794915
test_loss: 0.0043530636
train_loss: 0.0035371587
test_loss: 0.0043390486
train_loss: 0.003328125
test_loss: 0.004464878
train_loss: 0.0036568306
test_loss: 0.004563007
train_loss: 0.0035900294
test_loss: 0.0044642
train_loss: 0.0032377755
test_loss: 0.0043934872
train_loss: 0.0034197825
test_loss: 0.004508509
train_loss: 0.003368649
test_loss: 0.0045834044
train_loss: 0.003434019
test_loss: 0.004531462
train_loss: 0.0033439202
test_loss: 0.0043636747
train_loss: 0.003439004
test_loss: 0.004546827
train_loss: 0.0035141464
test_loss: 0.004440849
train_loss: 0.0035718041
test_loss: 0.0046576257
train_loss: 0.0033899602
test_loss: 0.0044634305
train_loss: 0.0033793459
test_loss: 0.0045467657
train_loss: 0.0033908514
test_loss: 0.0044389884
train_loss: 0.0031990306
test_loss: 0.0043796278
train_loss: 0.0037099319