+ RUN=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='-2 -1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0
+ date
Wed Oct 21 09:18:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4
+ date
Wed Oct 21 09:18:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8
+ date
Wed Oct 21 09:18:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2
+ date
Wed Oct 21 09:18:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1 --function f1 --psi -2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9b22b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9b726a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9ab7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9ac47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9b22ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9af9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e99b5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e99c2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e99727b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a44d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a19510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a00d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a0c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a0cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e98d77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e98ed6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e98ed840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9844a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9844c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e985a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c835a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c835dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c82e6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c82ae620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c82ae378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c8309840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e99417b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e991d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e991d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50804496a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5080449488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f508039b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f508039b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5080399488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50803990d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50803ffea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.021841295
test_loss: 0.021190982
train_loss: 0.010608679
test_loss: 0.010815483
train_loss: 0.0082497625
test_loss: 0.008591315
train_loss: 0.00708322
test_loss: 0.0075814873
train_loss: 0.0068265256
test_loss: 0.0071001314
train_loss: 0.006518971
test_loss: 0.0070450567
train_loss: 0.0061302795
test_loss: 0.006783094
train_loss: 0.0061742733
test_loss: 0.006875619
train_loss: 0.0062974775
test_loss: 0.006556472
train_loss: 0.0060953405
test_loss: 0.0063496255
train_loss: 0.0056433785
test_loss: 0.006631164
train_loss: 0.005513102
test_loss: 0.00639936
train_loss: 0.00548035
test_loss: 0.0063825627
train_loss: 0.0059448555
test_loss: 0.006330065
train_loss: 0.005315227
test_loss: 0.006339741
train_loss: 0.0055789137
test_loss: 0.006004008
train_loss: 0.0050865356
test_loss: 0.0058268653
train_loss: 0.0051076366
test_loss: 0.005969283
train_loss: 0.004924215
test_loss: 0.0058081066
train_loss: 0.0050095497
test_loss: 0.005802244
train_loss: 0.005174317
test_loss: 0.0060232976
train_loss: 0.0051678903
test_loss: 0.0057226224
train_loss: 0.0050662807
test_loss: 0.005852796
train_loss: 0.0047696074
test_loss: 0.005673038
train_loss: 0.004694115
test_loss: 0.0056269285
train_loss: 0.0045562396
test_loss: 0.005580263
train_loss: 0.0050458717
test_loss: 0.0056806253
train_loss: 0.0049402784
test_loss: 0.005546032
train_loss: 0.004821866
test_loss: 0.005376556
train_loss: 0.0049817916
test_loss: 0.0055849296
train_loss: 0.0046052565
test_loss: 0.0054069404
train_loss: 0.0043715825
test_loss: 0.005639102
train_loss: 0.004626459
test_loss: 0.005438026
train_loss: 0.004508894
test_loss: 0.005966049
train_loss: 0.00471092
test_loss: 0.0054718344
train_loss: 0.0044156965
test_loss: 0.005196775
train_loss: 0.0044202507
test_loss: 0.005259384
train_loss: 0.0043004313
test_loss: 0.0052551245
train_loss: 0.004591899
test_loss: 0.0051469738
train_loss: 0.0042014853
test_loss: 0.0050134053
train_loss: 0.0045157587
test_loss: 0.005051113
train_loss: 0.004162776
test_loss: 0.0051346477
train_loss: 0.004140989
test_loss: 0.005033076
train_loss: 0.0042116703
test_loss: 0.004955166
train_loss: 0.0042584306
test_loss: 0.0051701204
train_loss: 0.004121415
test_loss: 0.0049946597
train_loss: 0.0039855214
test_loss: 0.0050211763
train_loss: 0.0042400146
test_loss: 0.004937819
train_loss: 0.0041921153
test_loss: 0.0052647037
train_loss: 0.00414148
test_loss: 0.005065789
train_loss: 0.004051873
test_loss: 0.0049560163
train_loss: 0.004102352
test_loss: 0.004780862
train_loss: 0.004172706
test_loss: 0.0048953523
train_loss: 0.004236169
test_loss: 0.0047613024
train_loss: 0.004184692
test_loss: 0.004940241
train_loss: 0.0042022434
test_loss: 0.004836364
train_loss: 0.0044966806
test_loss: 0.005120908
train_loss: 0.0041001155
test_loss: 0.0047676796
train_loss: 0.0038242566
test_loss: 0.004846425
train_loss: 0.0038725014
test_loss: 0.004655126
train_loss: 0.003856185
test_loss: 0.004693798
train_loss: 0.003966935
test_loss: 0.0047858646
train_loss: 0.00372295
test_loss: 0.0046691033
train_loss: 0.0036993688
test_loss: 0.004639943
train_loss: 0.003870769
test_loss: 0.004800081
train_loss: 0.0037584687
test_loss: 0.004569838
train_loss: 0.0040200073
test_loss: 0.004919059
train_loss: 0.003925394
test_loss: 0.0046241865
train_loss: 0.0041175904
test_loss: 0.004848432
train_loss: 0.004006299
test_loss: 0.004644277
train_loss: 0.0037973644
test_loss: 0.0046702204
train_loss: 0.004097294
test_loss: 0.004639361
train_loss: 0.0036880947
test_loss: 0.0044482863
train_loss: 0.0037412606
test_loss: 0.0046261065
train_loss: 0.003558064
test_loss: 0.0045383824
train_loss: 0.003692758
test_loss: 0.004538348
train_loss: 0.0038698863
test_loss: 0.0046091992
train_loss: 0.0036645648
test_loss: 0.004601266
train_loss: 0.0036385392
test_loss: 0.0044518462
train_loss: 0.0038880892
test_loss: 0.004554447
train_loss: 0.0037284382
test_loss: 0.004600406
train_loss: 0.004082782
test_loss: 0.004589605
train_loss: 0.0038754195
test_loss: 0.0048241587
train_loss: 0.003733444
test_loss: 0.0046430263
train_loss: 0.0036973548
test_loss: 0.0045421873
train_loss: 0.003638552
test_loss: 0.0045489115
train_loss: 0.0036792196
test_loss: 0.0043367227
train_loss: 0.004068888
test_loss: 0.0047253836
train_loss: 0.0036568886
test_loss: 0.0043918155
train_loss: 0.003390548
test_loss: 0.00424622
train_loss: 0.0035902546
test_loss: 0.004474839
train_loss: 0.0034680502
test_loss: 0.004420526
train_loss: 0.0034753156
test_loss: 0.004374778
train_loss: 0.0035549896
test_loss: 0.004380691
train_loss: 0.0034945216
test_loss: 0.0043683974
train_loss: 0.0038243001
test_loss: 0.004662072
train_loss: 0.004022667
test_loss: 0.004794557
train_loss: 0.0034577705
test_loss: 0.004392472
train_loss: 0.0036199007
test_loss: 0.0045067198
train_loss: 0.00353831
test_loss: 0.0044276766
train_loss: 0.0036012155
test_loss: 0.0044241757
train_loss: 0.0035990742
test_loss: 0.004489387
train_loss: 0.0037386124
test_loss: 0.0043465146
train_loss: 0.003406996
test_loss: 0.0044392766
train_loss: 0.0034681724
test_loss: 0.0045839013
train_loss: 0.0033967523
test_loss: 0.0042565363
train_loss: 0.0034690737
test_loss: 0.004341821
train_loss: 0.0035353894
test_loss: 0.0043229796
train_loss: 0.003724771
test_loss: 0.0041764616
train_loss: 0.0035361974
test_loss: 0.0044078594
train_loss: 0.0037153205
test_loss: 0.004267634
train_loss: 0.0033385325
test_loss: 0.004349864
train_loss: 0.0033595082
test_loss: 0.0043363185
train_loss: 0.0033958983
test_loss: 0.0041910657
train_loss: 0.003577192
test_loss: 0.0042507634
train_loss: 0.0035773632
test_loss: 0.0042026043
train_loss: 0.0034938883
test_loss: 0.0043790475
train_loss: 0.003397062
test_loss: 0.004433453
train_loss: 0.003276146
test_loss: 0.0041495953
train_loss: 0.0035223844
test_loss: 0.0043665865
train_loss: 0.003427176
test_loss: 0.004234013
train_loss: 0.0032945615
test_loss: 0.004025006
train_loss: 0.0034588529
test_loss: 0.0042100404
train_loss: 0.0035324898
test_loss: 0.0043724286
train_loss: 0.0036485717
test_loss: 0.0043292004
train_loss: 0.0033324508
test_loss: 0.004356608
train_loss: 0.0031989715
test_loss: 0.0041698646
train_loss: 0.0034474775
test_loss: 0.0041663074
train_loss: 0.0033746161
test_loss: 0.0041520563
train_loss: 0.003161972
test_loss: 0.004123709
train_loss: 0.0035073082
test_loss: 0.004151492
train_loss: 0.0033225352
test_loss: 0.004208934
train_loss: 0.0033651793
test_loss: 0.004177568
train_loss: 0.003442727
test_loss: 0.004336745
train_loss: 0.0035048046
test_loss: 0.0042063035
train_loss: 0.0033639758
test_loss: 0.004158043
train_loss: 0.0035304795
test_loss: 0.0042504296
train_loss: 0.0031742651
test_loss: 0.004219253
train_loss: 0.0033586337
test_loss: 0.00409006
train_loss: 0.0032927855
test_loss: 0.004120284
train_loss: 0.0032985099
test_loss: 0.0041039283
train_loss: 0.0033064133
test_loss: 0.004110253
train_loss: 0.0033553708
test_loss: 0.004088762
train_loss: 0.0033088347
test_loss: 0.004149748
train_loss: 0.0032853917
test_loss: 0.004151167
train_loss: 0.0034417538
test_loss: 0.004026686
train_loss: 0.0034844421
test_loss: 0.0040770923
train_loss: 0.0033039316
test_loss: 0.004080111
train_loss: 0.0033956615
test_loss: 0.004034297
train_loss: 0.003151541
test_loss: 0.003997235
train_loss: 0.0031756815
test_loss: 0.00402418
train_loss: 0.0033338487
test_loss: 0.0043195137
train_loss: 0.0034177809
test_loss: 0.004218853
train_loss: 0.0033868751
test_loss: 0.004200719
train_loss: 0.0031526329
test_loss: 0.003979364
train_loss: 0.0031833244
test_loss: 0.0040464946
train_loss: 0.0032902663
test_loss: 0.0040161796
train_loss: 0.0033820316
test_loss: 0.0041696746
train_loss: 0.0032661783
test_loss: 0.0039988738
train_loss: 0.0031531448
test_loss: 0.003859729
train_loss: 0.0035377615
test_loss: 0.0043278807
train_loss: 0.0034150805
test_loss: 0.0041430765
train_loss: 0.0031819183
test_loss: 0.00401621
train_loss: 0.0035598695
test_loss: 0.0042127264
train_loss: 0.0034008445
test_loss: 0.004156681
train_loss: 0.003470745
test_loss: 0.004208918
train_loss: 0.003145379
test_loss: 0.004034552
train_loss: 0.0031843118
test_loss: 0.0042366995
train_loss: 0.003354589
test_loss: 0.004049832
train_loss: 0.0033137514
test_loss: 0.004058835
train_loss: 0.00313726
test_loss: 0.0040056384
train_loss: 0.0034294957
test_loss: 0.0039841365
train_loss: 0.0033327998
test_loss: 0.0041349693
train_loss: 0.0031591468
test_loss: 0.00395399
train_loss: 0.0032596975
test_loss: 0.0041411766
train_loss: 0.0031511695
test_loss: 0.004096768
train_loss: 0.003407426
test_loss: 0.004408958
train_loss: 0.003154217
test_loss: 0.004029545
train_loss: 0.003078462
test_loss: 0.0039146133
train_loss: 0.0030417177
test_loss: 0.003919839
train_loss: 0.003363831
test_loss: 0.003975986
train_loss: 0.0030494335
test_loss: 0.003932963
train_loss: 0.0029614186
test_loss: 0.003951232
train_loss: 0.0029175165
test_loss: 0.0038829348
train_loss: 0.0031781567
test_loss: 0.0042306352
train_loss: 0.0030915693
test_loss: 0.003855718
train_loss: 0.00335219
test_loss: 0.0040464727
train_loss: 0.0030960785
test_loss: 0.0038979773
train_loss: 0.0032562981
test_loss: 0.0038694707
train_loss: 0.0031729683
test_loss: 0.0039271857
train_loss: 0.002934087
test_loss: 0.0039678146
train_loss: 0.0030972427
test_loss: 0.003834162
train_loss: 0.0032029748
test_loss: 0.0042075827
train_loss: 0.0031119022
test_loss: 0.003964558
train_loss: 0.0029382012
test_loss: 0.0039826967
train_loss: 0.0030713405
test_loss: 0.0039547244
train_loss: 0.0029903757
test_loss: 0.0038941575
train_loss: 0.0030570296
test_loss: 0.0039137914
train_loss: 0.0032718522
test_loss: 0.0039771823
train_loss: 0.003146044
test_loss: 0.0038048541
train_loss: 0.0030899292
test_loss: 0.0038960555
train_loss: 0.0034838666
test_loss: 0.0044543142
train_loss: 0.0035003624
test_loss: 0.00391882
train_loss: 0.0033080685
test_loss: 0.0040218285
train_loss: 0.0031085974
test_loss: 0.0040022815
train_loss: 0.003146377
test_loss: 0.0039450275
train_loss: 0.0030544037
test_loss: 0.0038104581
train_loss: 0.0028704728
test_loss: 0.0038368104
train_loss: 0.0033312533
test_loss: 0.00401676
train_loss: 0.003214291
test_loss: 0.003959109
train_loss: 0.0032419872
test_loss: 0.003985854
train_loss: 0.003291354
test_loss: 0.0037861145
train_loss: 0.0029321664
test_loss: 0.0038425939
train_loss: 0.002964269
test_loss: 0.0038829183
train_loss: 0.0029569168
test_loss: 0.0039116303
train_loss: 0.003175784
test_loss: 0.0036848253
train_loss: 0.003012405
test_loss: 0.0037501024
train_loss: 0.0030583232
test_loss: 0.0038621225
train_loss: 0.0031565384
test_loss: 0.0039946395
train_loss: 0.0031691547
test_loss: 0.004046898
train_loss: 0.0032384119
test_loss: 0.0038915833
train_loss: 0.0029655942
test_loss: 0.0036773316
train_loss: 0.0030389237
test_loss: 0.003960643
train_loss: 0.0029732012
test_loss: 0.0038708092
train_loss: 0.003029606
test_loss: 0.0037517492
train_loss: 0.0031063221
test_loss: 0.003871775
train_loss: 0.0033233548
test_loss: 0.004067644
train_loss: 0.0031052907
test_loss: 0.004012438
train_loss: 0.0028731704
test_loss: 0.0037699686
train_loss: 0.0032047632
test_loss: 0.0040116245
train_loss: 0.0028656465
test_loss: 0.0039103664
train_loss: 0.0030436446
test_loss: 0.0037083037
train_loss: 0.0031686444
test_loss: 0.0038148144
train_loss: 0.0033478043
test_loss: 0.0039903163
train_loss: 0.0029617837
test_loss: 0.003818497
train_loss: 0.003127126
test_loss: 0.003902102
train_loss: 0.00326161
test_loss: 0.003868237
train_loss: 0.0030529033
test_loss: 0.0038171066
train_loss: 0.0028423702
test_loss: 0.003807607
train_loss: 0.0032052146
test_loss: 0.0038790165
train_loss: 0.003102321
test_loss: 0.0038269043
train_loss: 0.003397848
test_loss: 0.003890391
train_loss: 0.0029168134
test_loss: 0.003867908
train_loss: 0.0027219574
test_loss: 0.0036292213
train_loss: 0.003290067
test_loss: 0.0038310539
train_loss: 0.0029884102
test_loss: 0.0037565012
train_loss: 0.0029240295
test_loss: 0.0037978822
train_loss: 0.0030036552
test_loss: 0.0037614403
train_loss: 0.0029924854
test_loss: 0.0036819964
train_loss: 0.0030523571
test_loss: 0.0037935511
train_loss: 0.0029963742
test_loss: 0.003992253
train_loss: 0.0030861704
test_loss: 0.00376423
train_loss: 0.0028806813
test_loss: 0.0036919995
train_loss: 0.003037244
test_loss: 0.0038240496
train_loss: 0.0029208022
test_loss: 0.003747258
train_loss: 0.0028616218
test_loss: 0.0039069518
train_loss: 0.002960842
test_loss: 0.0037635947
train_loss: 0.0030126213
test_loss: 0.0038140758
train_loss: 0.0028117998
test_loss: 0.003714278
train_loss: 0.0028595333
test_loss: 0.0037033367
train_loss: 0.002803964
test_loss: 0.0039047468
train_loss: 0.0028563635
test_loss: 0.003801638
train_loss: 0.0032267876
test_loss: 0.0037097272
train_loss: 0.0031301412
test_loss: 0.003797508
train_loss: 0.0029942042
test_loss: 0.0036605538
train_loss: 0.0034471997
test_loss: 0.0038871232
train_loss: 0.0030007514
test_loss: 0.0038887362
train_loss: 0.0028834974
test_loss: 0.0036277068
train_loss: 0.002962173
test_loss: 0.003691451
train_loss: 0.002987802
test_loss: 0.0037808141
train_loss: 0.0031115145
test_loss: 0.0037630922
train_loss: 0.0027827364
test_loss: 0.0037555827
train_loss: 0.0029499228
test_loss: 0.0037936715
train_loss: 0.0029349022
test_loss: 0.0037182476
train_loss: 0.003287441
test_loss: 0.0037883804
train_loss: 0.003119721
test_loss: 0.0037924806
train_loss: 0.0029170474
test_loss: 0.003772275
train_loss: 0.002745871
test_loss: 0.0038009868
train_loss: 0.0029141828
test_loss: 0.0039026823
train_loss: 0.0031784046
test_loss: 0.0037163768
train_loss: 0.0028864045
test_loss: 0.0036838902
train_loss: 0.0028225058
test_loss: 0.0036645923
train_loss: 0.0029255908
test_loss: 0.0035713208
train_loss: 0.0029321932
test_loss: 0.003592519
train_loss: 0.003040615
test_loss: 0.0038118034
train_loss: 0.0031078288
test_loss: 0.00378615
train_loss: 0.0028037312
test_loss: 0.0036009513
train_loss: 0.0028741616
test_loss: 0.003734072
train_loss: 0.002691111
test_loss: 0.00354364
train_loss: 0.0029639248
test_loss: 0.00374795
train_loss: 0.0030055786
test_loss: 0.003801165
train_loss: 0.0031194272
test_loss: 0.003803547
train_loss: 0.0031606061
test_loss: 0.003915665
train_loss: 0.0029516444
test_loss: 0.0036123034
train_loss: 0.0030157373
test_loss: 0.0036762692
train_loss: 0.0027217246
test_loss: 0.0036942854
train_loss: 0.0029880437
test_loss: 0.0036955394
train_loss: 0.002965066
test_loss: 0.0036608633
train_loss: 0.003007018
test_loss: 0.0036643627
train_loss: 0.0029387425
test_loss: 0.0037095197
train_loss: 0.0029163244
test_loss: 0.003753064
train_loss: 0.0029524472
test_loss: 0.0037465799
train_loss: 0.0028498522
test_loss: 0.0038284783
train_loss: 0.0028253146
test_loss: 0.0037675072
train_loss: 0.0029026205
test_loss: 0.003642134
train_loss: 0.0028066335
test_loss: 0.0037291776
train_loss: 0.0026640643
test_loss: 0.0037435764
train_loss: 0.0030139324
test_loss: 0.0036504057
train_loss: 0.0027695762
test_loss: 0.003584185
train_loss: 0.0028310819
test_loss: 0.0036965448
train_loss: 0.0030135247
test_loss: 0.003697132
train_loss: 0.0027707643
test_loss: 0.0036001734
train_loss: 0.002774871
test_loss: 0.0036425157
train_loss: 0.002881335
test_loss: 0.0037480548
train_loss: 0.0028582627
test_loss: 0.0037377705
train_loss: 0.0028382163
test_loss: 0.0036181994
train_loss: 0.00271264
test_loss: 0.0035210066
train_loss: 0.0027191637
test_loss: 0.0035751327
train_loss: 0.0028662905
test_loss: 0.003880308
train_loss: 0.0028012583
test_loss: 0.0037539373
train_loss: 0.0028950514
test_loss: 0.0036111097
train_loss: 0.0028856522
test_loss: 0.0035697399
train_loss: 0.0028543584
test_loss: 0.0036028358
train_loss: 0.0029244558
test_loss: 0.0036291913
train_loss: 0.0030154367
test_loss: 0.0036841321
train_loss: 0.0027377056
test_loss: 0.0036359068
train_loss: 0.0029205116
test_loss: 0.0036465314
train_loss: 0.003078429
test_loss: 0.003734277
train_loss: 0.0032282257
test_loss: 0.0037917313
train_loss: 0.0029114562
test_loss: 0.0037341996
train_loss: 0.0028034337
test_loss: 0.0040023457
train_loss: 0.003085792
test_loss: 0.0036210837
train_loss: 0.0028262194
test_loss: 0.003643603
train_loss: 0.0027047691
test_loss: 0.0035887896
train_loss: 0.0028485355
test_loss: 0.003565661
train_loss: 0.0029159319
test_loss: 0.0036095148
train_loss: 0.002874794
test_loss: 0.0036862462
train_loss: 0.002982467
test_loss: 0.0035485718
train_loss: 0.0029358163
test_loss: 0.003647636
train_loss: 0.0028367913
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0035291896
train_loss: 0.0028066682
test_loss: 0.0035367357
train_loss: 0.0029031252
test_loss: 0.0035374996
train_loss: 0.0028760089
test_loss: 0.0036449549
train_loss: 0.0029838847
test_loss: 0.0036058207
train_loss: 0.0029187964
test_loss: 0.0037986175
train_loss: 0.0028903268
test_loss: 0.0037031362
train_loss: 0.002752013
test_loss: 0.003571209
train_loss: 0.003081764
test_loss: 0.003681048
train_loss: 0.0029990412
test_loss: 0.0037395768
train_loss: 0.00299531
test_loss: 0.0037525147
train_loss: 0.0028696833
test_loss: 0.0037340724
train_loss: 0.0028437593
test_loss: 0.0036058698
train_loss: 0.0028861226
test_loss: 0.0036706203
train_loss: 0.0026039255
test_loss: 0.0036999793
train_loss: 0.0028622397
test_loss: 0.0035458535
train_loss: 0.003085244
test_loss: 0.0036206387
train_loss: 0.0028323743
test_loss: 0.003486491
train_loss: 0.0026026191
test_loss: 0.0035378668
train_loss: 0.0027583083
test_loss: 0.0037109533
train_loss: 0.0028571498
test_loss: 0.0035815265
train_loss: 0.003019749
test_loss: 0.0036981814
train_loss: 0.002657531
test_loss: 0.003467456
train_loss: 0.002788214
test_loss: 0.0036381492
train_loss: 0.0027566939
test_loss: 0.0035154393
train_loss: 0.0027273665
test_loss: 0.0036199696
train_loss: 0.0027977042
test_loss: 0.0036227063
train_loss: 0.002885798
test_loss: 0.003599205
train_loss: 0.0029284698
test_loss: 0.003856682
train_loss: 0.002910701
test_loss: 0.0036426187
train_loss: 0.002676687
test_loss: 0.003681007
train_loss: 0.0029046247
test_loss: 0.0035938805
train_loss: 0.002885936
test_loss: 0.0036207472
train_loss: 0.002709941
test_loss: 0.0035106086
train_loss: 0.0031035105
test_loss: 0.003605149
train_loss: 0.0028213423
test_loss: 0.0036343895
train_loss: 0.0028842299
test_loss: 0.00358041
train_loss: 0.0027680171
test_loss: 0.0036884292
train_loss: 0.0027636783
test_loss: 0.0036197784
train_loss: 0.0032647036
test_loss: 0.0037888617
train_loss: 0.0028447271
test_loss: 0.0036200534
train_loss: 0.0025678966
test_loss: 0.0035349347
train_loss: 0.0027102737
test_loss: 0.003700044
train_loss: 0.0025451037
test_loss: 0.003565952
train_loss: 0.0025979725
test_loss: 0.0035203903
train_loss: 0.0027935964
test_loss: 0.0035999184
train_loss: 0.0027170917
test_loss: 0.0036065595
train_loss: 0.0026319618
test_loss: 0.0035291351
train_loss: 0.002830022
test_loss: 0.0036171635
train_loss: 0.002752849
test_loss: 0.003581333
train_loss: 0.002824122
test_loss: 0.0035926965
train_loss: 0.0028149828
test_loss: 0.0035012153
train_loss: 0.0028305957
test_loss: 0.0036234923
train_loss: 0.0026705984
test_loss: 0.0035380616
train_loss: 0.002645508
test_loss: 0.0035274222
train_loss: 0.0029350359
test_loss: 0.0037134807
train_loss: 0.0029833647
test_loss: 0.0034768076
train_loss: 0.0026921427
test_loss: 0.0034785944
train_loss: 0.002711893
test_loss: 0.003469985
train_loss: 0.0027580962
test_loss: 0.0035596061
train_loss: 0.0026624938
test_loss: 0.003534568
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accb5c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accc11598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accc11400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accc11620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accaff488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accaffc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accafa950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accaa2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accaa21e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca66c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca66b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca668c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca21598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca21ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acc98e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acc9c26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90cd58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90cd5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90cd0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90cd02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90c549d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90c76ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90c768c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90bf8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90bf8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c4b5ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c4679d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c40f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c40f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c42f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c3dd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c39c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c39c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c3c08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c371bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c371ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.39151507e-05
Iter: 2 loss: 1.15776511e-05
Iter: 3 loss: 1.15670409e-05
Iter: 4 loss: 1.04372666e-05
Iter: 5 loss: 1.41253367e-05
Iter: 6 loss: 1.01219703e-05
Iter: 7 loss: 9.39565416e-06
Iter: 8 loss: 1.12101934e-05
Iter: 9 loss: 9.14048e-06
Iter: 10 loss: 8.44455735e-06
Iter: 11 loss: 9.89043474e-06
Iter: 12 loss: 8.16658394e-06
Iter: 13 loss: 7.55381961e-06
Iter: 14 loss: 9.64080573e-06
Iter: 15 loss: 7.39142251e-06
Iter: 16 loss: 6.92532558e-06
Iter: 17 loss: 8.34802813e-06
Iter: 18 loss: 6.78676406e-06
Iter: 19 loss: 6.31352577e-06
Iter: 20 loss: 9.29022826e-06
Iter: 21 loss: 6.25934445e-06
Iter: 22 loss: 5.98558381e-06
Iter: 23 loss: 5.57204203e-06
Iter: 24 loss: 5.56421719e-06
Iter: 25 loss: 5.33348339e-06
Iter: 26 loss: 5.33270304e-06
Iter: 27 loss: 5.1222687e-06
Iter: 28 loss: 6.33172976e-06
Iter: 29 loss: 5.09403071e-06
Iter: 30 loss: 4.97948031e-06
Iter: 31 loss: 4.84331485e-06
Iter: 32 loss: 4.82912219e-06
Iter: 33 loss: 4.62396156e-06
Iter: 34 loss: 4.52918448e-06
Iter: 35 loss: 4.42759847e-06
Iter: 36 loss: 4.42623423e-06
Iter: 37 loss: 4.30261252e-06
Iter: 38 loss: 4.18212858e-06
Iter: 39 loss: 4.1516796e-06
Iter: 40 loss: 4.07590414e-06
Iter: 41 loss: 3.9467086e-06
Iter: 42 loss: 4.76706737e-06
Iter: 43 loss: 3.93212213e-06
Iter: 44 loss: 3.82164899e-06
Iter: 45 loss: 3.97085114e-06
Iter: 46 loss: 3.76627167e-06
Iter: 47 loss: 3.67665234e-06
Iter: 48 loss: 4.15410659e-06
Iter: 49 loss: 3.66291306e-06
Iter: 50 loss: 3.59429032e-06
Iter: 51 loss: 3.77607694e-06
Iter: 52 loss: 3.5712028e-06
Iter: 53 loss: 3.48747039e-06
Iter: 54 loss: 3.69876057e-06
Iter: 55 loss: 3.45841931e-06
Iter: 56 loss: 3.3917654e-06
Iter: 57 loss: 3.29580257e-06
Iter: 58 loss: 3.29278555e-06
Iter: 59 loss: 3.19292985e-06
Iter: 60 loss: 4.2427132e-06
Iter: 61 loss: 3.19041465e-06
Iter: 62 loss: 3.08820745e-06
Iter: 63 loss: 3.54603139e-06
Iter: 64 loss: 3.06819038e-06
Iter: 65 loss: 3.01279169e-06
Iter: 66 loss: 2.93867561e-06
Iter: 67 loss: 2.93452786e-06
Iter: 68 loss: 2.85861915e-06
Iter: 69 loss: 3.22380174e-06
Iter: 70 loss: 2.84511975e-06
Iter: 71 loss: 2.79763253e-06
Iter: 72 loss: 3.27414e-06
Iter: 73 loss: 2.79613027e-06
Iter: 74 loss: 2.74551439e-06
Iter: 75 loss: 2.96842609e-06
Iter: 76 loss: 2.73538035e-06
Iter: 77 loss: 2.70731721e-06
Iter: 78 loss: 2.70018836e-06
Iter: 79 loss: 2.68258759e-06
Iter: 80 loss: 2.6271282e-06
Iter: 81 loss: 2.75146158e-06
Iter: 82 loss: 2.60604202e-06
Iter: 83 loss: 2.5683064e-06
Iter: 84 loss: 2.68267718e-06
Iter: 85 loss: 2.55703299e-06
Iter: 86 loss: 2.52139e-06
Iter: 87 loss: 2.72045236e-06
Iter: 88 loss: 2.51628262e-06
Iter: 89 loss: 2.48626748e-06
Iter: 90 loss: 2.57236434e-06
Iter: 91 loss: 2.47684488e-06
Iter: 92 loss: 2.44950661e-06
Iter: 93 loss: 2.4459714e-06
Iter: 94 loss: 2.42653232e-06
Iter: 95 loss: 2.39642986e-06
Iter: 96 loss: 2.49138498e-06
Iter: 97 loss: 2.38762027e-06
Iter: 98 loss: 2.36663755e-06
Iter: 99 loss: 2.36616825e-06
Iter: 100 loss: 2.3512107e-06
Iter: 101 loss: 2.31255399e-06
Iter: 102 loss: 2.62732783e-06
Iter: 103 loss: 2.30561091e-06
Iter: 104 loss: 2.26571319e-06
Iter: 105 loss: 2.30969977e-06
Iter: 106 loss: 2.2439383e-06
Iter: 107 loss: 2.2311649e-06
Iter: 108 loss: 2.22154904e-06
Iter: 109 loss: 2.2003353e-06
Iter: 110 loss: 2.2527372e-06
Iter: 111 loss: 2.19285175e-06
Iter: 112 loss: 2.1781575e-06
Iter: 113 loss: 2.18298942e-06
Iter: 114 loss: 2.1678145e-06
Iter: 115 loss: 2.14918555e-06
Iter: 116 loss: 2.29640659e-06
Iter: 117 loss: 2.14792794e-06
Iter: 118 loss: 2.13522753e-06
Iter: 119 loss: 2.11624274e-06
Iter: 120 loss: 2.11585984e-06
Iter: 121 loss: 2.09899304e-06
Iter: 122 loss: 2.09835298e-06
Iter: 123 loss: 2.08592564e-06
Iter: 124 loss: 2.07464382e-06
Iter: 125 loss: 2.07152766e-06
Iter: 126 loss: 2.0455102e-06
Iter: 127 loss: 2.07075664e-06
Iter: 128 loss: 2.03064405e-06
Iter: 129 loss: 2.01388616e-06
Iter: 130 loss: 2.10423877e-06
Iter: 131 loss: 2.0113614e-06
Iter: 132 loss: 1.99534156e-06
Iter: 133 loss: 2.13406292e-06
Iter: 134 loss: 1.99448959e-06
Iter: 135 loss: 1.98368139e-06
Iter: 136 loss: 1.96676274e-06
Iter: 137 loss: 1.96655037e-06
Iter: 138 loss: 1.94907284e-06
Iter: 139 loss: 1.95629627e-06
Iter: 140 loss: 1.9369586e-06
Iter: 141 loss: 1.92714469e-06
Iter: 142 loss: 1.92554126e-06
Iter: 143 loss: 1.91264758e-06
Iter: 144 loss: 1.92515586e-06
Iter: 145 loss: 1.90524941e-06
Iter: 146 loss: 1.89456887e-06
Iter: 147 loss: 1.89647051e-06
Iter: 148 loss: 1.88653132e-06
Iter: 149 loss: 1.87275782e-06
Iter: 150 loss: 2.01811827e-06
Iter: 151 loss: 1.87241085e-06
Iter: 152 loss: 1.86464922e-06
Iter: 153 loss: 1.85863212e-06
Iter: 154 loss: 1.8561592e-06
Iter: 155 loss: 1.84946714e-06
Iter: 156 loss: 1.84897169e-06
Iter: 157 loss: 1.84358601e-06
Iter: 158 loss: 1.83424686e-06
Iter: 159 loss: 1.83422765e-06
Iter: 160 loss: 1.82319661e-06
Iter: 161 loss: 1.89276557e-06
Iter: 162 loss: 1.82190831e-06
Iter: 163 loss: 1.8143835e-06
Iter: 164 loss: 1.81736527e-06
Iter: 165 loss: 1.80920051e-06
Iter: 166 loss: 1.79732672e-06
Iter: 167 loss: 1.88569391e-06
Iter: 168 loss: 1.79635379e-06
Iter: 169 loss: 1.78729317e-06
Iter: 170 loss: 1.7776041e-06
Iter: 171 loss: 1.77603988e-06
Iter: 172 loss: 1.76464539e-06
Iter: 173 loss: 1.76504511e-06
Iter: 174 loss: 1.75570301e-06
Iter: 175 loss: 1.74737079e-06
Iter: 176 loss: 1.74681236e-06
Iter: 177 loss: 1.73861645e-06
Iter: 178 loss: 1.78259131e-06
Iter: 179 loss: 1.73739159e-06
Iter: 180 loss: 1.73338151e-06
Iter: 181 loss: 1.72410103e-06
Iter: 182 loss: 1.83919894e-06
Iter: 183 loss: 1.72339696e-06
Iter: 184 loss: 1.71291958e-06
Iter: 185 loss: 1.71291651e-06
Iter: 186 loss: 1.70709097e-06
Iter: 187 loss: 1.70094756e-06
Iter: 188 loss: 1.69993575e-06
Iter: 189 loss: 1.6925411e-06
Iter: 190 loss: 1.69252326e-06
Iter: 191 loss: 1.6867225e-06
Iter: 192 loss: 1.68159465e-06
Iter: 193 loss: 1.68001429e-06
Iter: 194 loss: 1.67304597e-06
Iter: 195 loss: 1.70410374e-06
Iter: 196 loss: 1.67159635e-06
Iter: 197 loss: 1.6653853e-06
Iter: 198 loss: 1.69371117e-06
Iter: 199 loss: 1.66421046e-06
Iter: 200 loss: 1.65988683e-06
Iter: 201 loss: 1.7063486e-06
Iter: 202 loss: 1.65976076e-06
Iter: 203 loss: 1.65635038e-06
Iter: 204 loss: 1.64852634e-06
Iter: 205 loss: 1.75052935e-06
Iter: 206 loss: 1.64804192e-06
Iter: 207 loss: 1.640037e-06
Iter: 208 loss: 1.64015216e-06
Iter: 209 loss: 1.6335689e-06
Iter: 210 loss: 1.62289336e-06
Iter: 211 loss: 1.72598232e-06
Iter: 212 loss: 1.62250399e-06
Iter: 213 loss: 1.61919615e-06
Iter: 214 loss: 1.61838727e-06
Iter: 215 loss: 1.61431899e-06
Iter: 216 loss: 1.60702166e-06
Iter: 217 loss: 1.78155517e-06
Iter: 218 loss: 1.60703667e-06
Iter: 219 loss: 1.60276136e-06
Iter: 220 loss: 1.63276059e-06
Iter: 221 loss: 1.60237244e-06
Iter: 222 loss: 1.5970428e-06
Iter: 223 loss: 1.59932699e-06
Iter: 224 loss: 1.59338879e-06
Iter: 225 loss: 1.58822661e-06
Iter: 226 loss: 1.60133698e-06
Iter: 227 loss: 1.58642331e-06
Iter: 228 loss: 1.57982254e-06
Iter: 229 loss: 1.61124376e-06
Iter: 230 loss: 1.57868465e-06
Iter: 231 loss: 1.57440388e-06
Iter: 232 loss: 1.56988904e-06
Iter: 233 loss: 1.56911574e-06
Iter: 234 loss: 1.56238525e-06
Iter: 235 loss: 1.6111477e-06
Iter: 236 loss: 1.56183876e-06
Iter: 237 loss: 1.5563744e-06
Iter: 238 loss: 1.59132196e-06
Iter: 239 loss: 1.55574378e-06
Iter: 240 loss: 1.55143618e-06
Iter: 241 loss: 1.56441945e-06
Iter: 242 loss: 1.55012992e-06
Iter: 243 loss: 1.54732675e-06
Iter: 244 loss: 1.5420643e-06
Iter: 245 loss: 1.66355812e-06
Iter: 246 loss: 1.54207191e-06
Iter: 247 loss: 1.53615963e-06
Iter: 248 loss: 1.55617226e-06
Iter: 249 loss: 1.53454243e-06
Iter: 250 loss: 1.52799703e-06
Iter: 251 loss: 1.53664416e-06
Iter: 252 loss: 1.52470693e-06
Iter: 253 loss: 1.52372593e-06
Iter: 254 loss: 1.52169503e-06
Iter: 255 loss: 1.51848383e-06
Iter: 256 loss: 1.5138246e-06
Iter: 257 loss: 1.51367e-06
Iter: 258 loss: 1.50962421e-06
Iter: 259 loss: 1.50897495e-06
Iter: 260 loss: 1.50623612e-06
Iter: 261 loss: 1.50288065e-06
Iter: 262 loss: 1.50283381e-06
Iter: 263 loss: 1.49973e-06
Iter: 264 loss: 1.50632468e-06
Iter: 265 loss: 1.49844664e-06
Iter: 266 loss: 1.49524635e-06
Iter: 267 loss: 1.4911127e-06
Iter: 268 loss: 1.49082973e-06
Iter: 269 loss: 1.48912613e-06
Iter: 270 loss: 1.48801098e-06
Iter: 271 loss: 1.48570803e-06
Iter: 272 loss: 1.48104345e-06
Iter: 273 loss: 1.56970623e-06
Iter: 274 loss: 1.48102185e-06
Iter: 275 loss: 1.47626065e-06
Iter: 276 loss: 1.51571612e-06
Iter: 277 loss: 1.47592255e-06
Iter: 278 loss: 1.47267167e-06
Iter: 279 loss: 1.47634091e-06
Iter: 280 loss: 1.47090486e-06
Iter: 281 loss: 1.46696311e-06
Iter: 282 loss: 1.50871369e-06
Iter: 283 loss: 1.4668401e-06
Iter: 284 loss: 1.46507716e-06
Iter: 285 loss: 1.46107914e-06
Iter: 286 loss: 1.51487313e-06
Iter: 287 loss: 1.46079401e-06
Iter: 288 loss: 1.45668491e-06
Iter: 289 loss: 1.49061498e-06
Iter: 290 loss: 1.45638592e-06
Iter: 291 loss: 1.45337208e-06
Iter: 292 loss: 1.50114533e-06
Iter: 293 loss: 1.45335559e-06
Iter: 294 loss: 1.45151648e-06
Iter: 295 loss: 1.44632838e-06
Iter: 296 loss: 1.47381286e-06
Iter: 297 loss: 1.4446988e-06
Iter: 298 loss: 1.43863883e-06
Iter: 299 loss: 1.470445e-06
Iter: 300 loss: 1.43770126e-06
Iter: 301 loss: 1.43466423e-06
Iter: 302 loss: 1.471828e-06
Iter: 303 loss: 1.43463603e-06
Iter: 304 loss: 1.43154216e-06
Iter: 305 loss: 1.44033459e-06
Iter: 306 loss: 1.43052512e-06
Iter: 307 loss: 1.42835393e-06
Iter: 308 loss: 1.42765862e-06
Iter: 309 loss: 1.42637634e-06
Iter: 310 loss: 1.4239082e-06
Iter: 311 loss: 1.42390741e-06
Iter: 312 loss: 1.42158217e-06
Iter: 313 loss: 1.41880923e-06
Iter: 314 loss: 1.41855469e-06
Iter: 315 loss: 1.41505893e-06
Iter: 316 loss: 1.42293879e-06
Iter: 317 loss: 1.41378302e-06
Iter: 318 loss: 1.40956047e-06
Iter: 319 loss: 1.42864508e-06
Iter: 320 loss: 1.40875238e-06
Iter: 321 loss: 1.40599e-06
Iter: 322 loss: 1.43946113e-06
Iter: 323 loss: 1.40597797e-06
Iter: 324 loss: 1.40416455e-06
Iter: 325 loss: 1.40049656e-06
Iter: 326 loss: 1.46260027e-06
Iter: 327 loss: 1.40038901e-06
Iter: 328 loss: 1.39685312e-06
Iter: 329 loss: 1.40448026e-06
Iter: 330 loss: 1.39552617e-06
Iter: 331 loss: 1.39173733e-06
Iter: 332 loss: 1.40567977e-06
Iter: 333 loss: 1.39080657e-06
Iter: 334 loss: 1.38959217e-06
Iter: 335 loss: 1.38906273e-06
Iter: 336 loss: 1.3871869e-06
Iter: 337 loss: 1.3827422e-06
Iter: 338 loss: 1.43449211e-06
Iter: 339 loss: 1.38237863e-06
Iter: 340 loss: 1.37976804e-06
Iter: 341 loss: 1.40370685e-06
Iter: 342 loss: 1.37968709e-06
Iter: 343 loss: 1.37772849e-06
Iter: 344 loss: 1.39317262e-06
Iter: 345 loss: 1.37758377e-06
Iter: 346 loss: 1.37576558e-06
Iter: 347 loss: 1.3731526e-06
Iter: 348 loss: 1.37308041e-06
Iter: 349 loss: 1.37076086e-06
Iter: 350 loss: 1.38861992e-06
Iter: 351 loss: 1.37058862e-06
Iter: 352 loss: 1.36804988e-06
Iter: 353 loss: 1.37861639e-06
Iter: 354 loss: 1.36749486e-06
Iter: 355 loss: 1.36550398e-06
Iter: 356 loss: 1.36376184e-06
Iter: 357 loss: 1.36325616e-06
Iter: 358 loss: 1.35947323e-06
Iter: 359 loss: 1.37232303e-06
Iter: 360 loss: 1.35846494e-06
Iter: 361 loss: 1.35628829e-06
Iter: 362 loss: 1.38587234e-06
Iter: 363 loss: 1.35627886e-06
Iter: 364 loss: 1.35436426e-06
Iter: 365 loss: 1.35503319e-06
Iter: 366 loss: 1.35301912e-06
Iter: 367 loss: 1.35088726e-06
Iter: 368 loss: 1.34922971e-06
Iter: 369 loss: 1.34856964e-06
Iter: 370 loss: 1.34579159e-06
Iter: 371 loss: 1.3475526e-06
Iter: 372 loss: 1.34404763e-06
Iter: 373 loss: 1.34522247e-06
Iter: 374 loss: 1.34265599e-06
Iter: 375 loss: 1.34160155e-06
Iter: 376 loss: 1.33885487e-06
Iter: 377 loss: 1.35668643e-06
Iter: 378 loss: 1.33816332e-06
Iter: 379 loss: 1.33464619e-06
Iter: 380 loss: 1.35067478e-06
Iter: 381 loss: 1.33398703e-06
Iter: 382 loss: 1.33176218e-06
Iter: 383 loss: 1.33173921e-06
Iter: 384 loss: 1.33055073e-06
Iter: 385 loss: 1.32791115e-06
Iter: 386 loss: 1.36498215e-06
Iter: 387 loss: 1.32781099e-06
Iter: 388 loss: 1.32630612e-06
Iter: 389 loss: 1.3262046e-06
Iter: 390 loss: 1.32468745e-06
Iter: 391 loss: 1.32728837e-06
Iter: 392 loss: 1.32393848e-06
Iter: 393 loss: 1.32258208e-06
Iter: 394 loss: 1.32180912e-06
Iter: 395 loss: 1.32124796e-06
Iter: 396 loss: 1.31858303e-06
Iter: 397 loss: 1.3243872e-06
Iter: 398 loss: 1.31749698e-06
Iter: 399 loss: 1.31561171e-06
Iter: 400 loss: 1.31562558e-06
Iter: 401 loss: 1.31404386e-06
Iter: 402 loss: 1.31278489e-06
Iter: 403 loss: 1.31232912e-06
Iter: 404 loss: 1.3100248e-06
Iter: 405 loss: 1.30956755e-06
Iter: 406 loss: 1.30807757e-06
Iter: 407 loss: 1.3074042e-06
Iter: 408 loss: 1.30695594e-06
Iter: 409 loss: 1.30571948e-06
Iter: 410 loss: 1.30421938e-06
Iter: 411 loss: 1.30404874e-06
Iter: 412 loss: 1.30224316e-06
Iter: 413 loss: 1.30073045e-06
Iter: 414 loss: 1.30022227e-06
Iter: 415 loss: 1.29838531e-06
Iter: 416 loss: 1.29826219e-06
Iter: 417 loss: 1.29667433e-06
Iter: 418 loss: 1.29395482e-06
Iter: 419 loss: 1.36273627e-06
Iter: 420 loss: 1.29395949e-06
Iter: 421 loss: 1.29087653e-06
Iter: 422 loss: 1.30093713e-06
Iter: 423 loss: 1.29001387e-06
Iter: 424 loss: 1.28838167e-06
Iter: 425 loss: 1.28816316e-06
Iter: 426 loss: 1.28709644e-06
Iter: 427 loss: 1.28504962e-06
Iter: 428 loss: 1.32731782e-06
Iter: 429 loss: 1.28504814e-06
Iter: 430 loss: 1.28325769e-06
Iter: 431 loss: 1.29735804e-06
Iter: 432 loss: 1.2831033e-06
Iter: 433 loss: 1.28153692e-06
Iter: 434 loss: 1.28759257e-06
Iter: 435 loss: 1.28109446e-06
Iter: 436 loss: 1.27935778e-06
Iter: 437 loss: 1.2834455e-06
Iter: 438 loss: 1.27869077e-06
Iter: 439 loss: 1.2770488e-06
Iter: 440 loss: 1.27544399e-06
Iter: 441 loss: 1.27509327e-06
Iter: 442 loss: 1.27260751e-06
Iter: 443 loss: 1.2760861e-06
Iter: 444 loss: 1.27137218e-06
Iter: 445 loss: 1.27041778e-06
Iter: 446 loss: 1.2699129e-06
Iter: 447 loss: 1.26851376e-06
Iter: 448 loss: 1.26676366e-06
Iter: 449 loss: 1.26655891e-06
Iter: 450 loss: 1.26525049e-06
Iter: 451 loss: 1.26520263e-06
Iter: 452 loss: 1.26414102e-06
Iter: 453 loss: 1.26241332e-06
Iter: 454 loss: 1.2886519e-06
Iter: 455 loss: 1.26241162e-06
Iter: 456 loss: 1.26099894e-06
Iter: 457 loss: 1.25988845e-06
Iter: 458 loss: 1.25943825e-06
Iter: 459 loss: 1.25739393e-06
Iter: 460 loss: 1.25765177e-06
Iter: 461 loss: 1.2558678e-06
Iter: 462 loss: 1.25538111e-06
Iter: 463 loss: 1.25454153e-06
Iter: 464 loss: 1.25362976e-06
Iter: 465 loss: 1.25137331e-06
Iter: 466 loss: 1.2692542e-06
Iter: 467 loss: 1.25096221e-06
Iter: 468 loss: 1.24930045e-06
Iter: 469 loss: 1.24930466e-06
Iter: 470 loss: 1.24799965e-06
Iter: 471 loss: 1.25280155e-06
Iter: 472 loss: 1.24772396e-06
Iter: 473 loss: 1.24640223e-06
Iter: 474 loss: 1.24946757e-06
Iter: 475 loss: 1.24588121e-06
Iter: 476 loss: 1.24458802e-06
Iter: 477 loss: 1.24347378e-06
Iter: 478 loss: 1.24313988e-06
Iter: 479 loss: 1.24090559e-06
Iter: 480 loss: 1.24412702e-06
Iter: 481 loss: 1.23975269e-06
Iter: 482 loss: 1.23912264e-06
Iter: 483 loss: 1.23838095e-06
Iter: 484 loss: 1.23774362e-06
Iter: 485 loss: 1.23569066e-06
Iter: 486 loss: 1.23947257e-06
Iter: 487 loss: 1.23435962e-06
Iter: 488 loss: 1.23196139e-06
Iter: 489 loss: 1.2557839e-06
Iter: 490 loss: 1.23192785e-06
Iter: 491 loss: 1.23041218e-06
Iter: 492 loss: 1.24241e-06
Iter: 493 loss: 1.23028019e-06
Iter: 494 loss: 1.22890174e-06
Iter: 495 loss: 1.23555287e-06
Iter: 496 loss: 1.2286e-06
Iter: 497 loss: 1.22751385e-06
Iter: 498 loss: 1.22585243e-06
Iter: 499 loss: 1.2258547e-06
Iter: 500 loss: 1.22408937e-06
Iter: 501 loss: 1.22404413e-06
Iter: 502 loss: 1.22323468e-06
Iter: 503 loss: 1.2211093e-06
Iter: 504 loss: 1.23953191e-06
Iter: 505 loss: 1.22078018e-06
Iter: 506 loss: 1.21866196e-06
Iter: 507 loss: 1.2411391e-06
Iter: 508 loss: 1.21860796e-06
Iter: 509 loss: 1.21726566e-06
Iter: 510 loss: 1.2252533e-06
Iter: 511 loss: 1.21712662e-06
Iter: 512 loss: 1.21598373e-06
Iter: 513 loss: 1.22117399e-06
Iter: 514 loss: 1.21575772e-06
Iter: 515 loss: 1.21451285e-06
Iter: 516 loss: 1.21368339e-06
Iter: 517 loss: 1.21320886e-06
Iter: 518 loss: 1.21167454e-06
Iter: 519 loss: 1.21827748e-06
Iter: 520 loss: 1.21136293e-06
Iter: 521 loss: 1.20933862e-06
Iter: 522 loss: 1.21554581e-06
Iter: 523 loss: 1.20872994e-06
Iter: 524 loss: 1.20761774e-06
Iter: 525 loss: 1.20562515e-06
Iter: 526 loss: 1.205643e-06
Iter: 527 loss: 1.20319623e-06
Iter: 528 loss: 1.21050482e-06
Iter: 529 loss: 1.20242544e-06
Iter: 530 loss: 1.20082746e-06
Iter: 531 loss: 1.21503217e-06
Iter: 532 loss: 1.2007705e-06
Iter: 533 loss: 1.19933884e-06
Iter: 534 loss: 1.21167875e-06
Iter: 535 loss: 1.19924061e-06
Iter: 536 loss: 1.19853053e-06
Iter: 537 loss: 1.19740457e-06
Iter: 538 loss: 1.19739354e-06
Iter: 539 loss: 1.19608717e-06
Iter: 540 loss: 1.19608603e-06
Iter: 541 loss: 1.19522326e-06
Iter: 542 loss: 1.19392121e-06
Iter: 543 loss: 1.19388415e-06
Iter: 544 loss: 1.19227809e-06
Iter: 545 loss: 1.19073457e-06
Iter: 546 loss: 1.19042761e-06
Iter: 547 loss: 1.18858952e-06
Iter: 548 loss: 1.18854587e-06
Iter: 549 loss: 1.18729577e-06
Iter: 550 loss: 1.19057643e-06
Iter: 551 loss: 1.18689911e-06
Iter: 552 loss: 1.18606249e-06
Iter: 553 loss: 1.18604703e-06
Iter: 554 loss: 1.18527191e-06
Iter: 555 loss: 1.18365915e-06
Iter: 556 loss: 1.21286439e-06
Iter: 557 loss: 1.1836446e-06
Iter: 558 loss: 1.18290916e-06
Iter: 559 loss: 1.18275864e-06
Iter: 560 loss: 1.18197761e-06
Iter: 561 loss: 1.17991897e-06
Iter: 562 loss: 1.19774677e-06
Iter: 563 loss: 1.17958734e-06
Iter: 564 loss: 1.17748891e-06
Iter: 565 loss: 1.18438948e-06
Iter: 566 loss: 1.17692139e-06
Iter: 567 loss: 1.17507716e-06
Iter: 568 loss: 1.17924037e-06
Iter: 569 loss: 1.17435377e-06
Iter: 570 loss: 1.17379182e-06
Iter: 571 loss: 1.1734478e-06
Iter: 572 loss: 1.17276625e-06
Iter: 573 loss: 1.17147704e-06
Iter: 574 loss: 1.19760909e-06
Iter: 575 loss: 1.17148147e-06
Iter: 576 loss: 1.17052673e-06
Iter: 577 loss: 1.17052014e-06
Iter: 578 loss: 1.16949218e-06
Iter: 579 loss: 1.16858109e-06
Iter: 580 loss: 1.16826811e-06
Iter: 581 loss: 1.16690694e-06
Iter: 582 loss: 1.16869603e-06
Iter: 583 loss: 1.16620913e-06
Iter: 584 loss: 1.16477713e-06
Iter: 585 loss: 1.17562604e-06
Iter: 586 loss: 1.16467345e-06
Iter: 587 loss: 1.16337446e-06
Iter: 588 loss: 1.17079924e-06
Iter: 589 loss: 1.16319609e-06
Iter: 590 loss: 1.16201625e-06
Iter: 591 loss: 1.16219576e-06
Iter: 592 loss: 1.16113642e-06
Iter: 593 loss: 1.16041952e-06
Iter: 594 loss: 1.16042e-06
Iter: 595 loss: 1.15973012e-06
Iter: 596 loss: 1.15823354e-06
Iter: 597 loss: 1.17854677e-06
Iter: 598 loss: 1.15815237e-06
Iter: 599 loss: 1.15688442e-06
Iter: 600 loss: 1.16047113e-06
Iter: 601 loss: 1.1564714e-06
Iter: 602 loss: 1.15503099e-06
Iter: 603 loss: 1.15449916e-06
Iter: 604 loss: 1.1536772e-06
Iter: 605 loss: 1.15279101e-06
Iter: 606 loss: 1.1524603e-06
Iter: 607 loss: 1.15133196e-06
Iter: 608 loss: 1.14977047e-06
Iter: 609 loss: 1.1497084e-06
Iter: 610 loss: 1.14875206e-06
Iter: 611 loss: 1.14874081e-06
Iter: 612 loss: 1.14781665e-06
Iter: 613 loss: 1.14748605e-06
Iter: 614 loss: 1.14693751e-06
Iter: 615 loss: 1.14594445e-06
Iter: 616 loss: 1.14577256e-06
Iter: 617 loss: 1.14503177e-06
Iter: 618 loss: 1.14367015e-06
Iter: 619 loss: 1.15102e-06
Iter: 620 loss: 1.1434704e-06
Iter: 621 loss: 1.14244881e-06
Iter: 622 loss: 1.15455953e-06
Iter: 623 loss: 1.14246086e-06
Iter: 624 loss: 1.14155546e-06
Iter: 625 loss: 1.14274121e-06
Iter: 626 loss: 1.14105626e-06
Iter: 627 loss: 1.14009651e-06
Iter: 628 loss: 1.1402318e-06
Iter: 629 loss: 1.13940007e-06
Iter: 630 loss: 1.13782983e-06
Iter: 631 loss: 1.14753163e-06
Iter: 632 loss: 1.13766282e-06
Iter: 633 loss: 1.13668261e-06
Iter: 634 loss: 1.13549288e-06
Iter: 635 loss: 1.13540204e-06
Iter: 636 loss: 1.13414058e-06
Iter: 637 loss: 1.13810415e-06
Iter: 638 loss: 1.13377666e-06
Iter: 639 loss: 1.13280839e-06
Iter: 640 loss: 1.14442923e-06
Iter: 641 loss: 1.13278e-06
Iter: 642 loss: 1.13178044e-06
Iter: 643 loss: 1.13300973e-06
Iter: 644 loss: 1.13128942e-06
Iter: 645 loss: 1.13031479e-06
Iter: 646 loss: 1.13020678e-06
Iter: 647 loss: 1.12951955e-06
Iter: 648 loss: 1.12779e-06
Iter: 649 loss: 1.13902945e-06
Iter: 650 loss: 1.12758471e-06
Iter: 651 loss: 1.12668181e-06
Iter: 652 loss: 1.12482473e-06
Iter: 653 loss: 1.15309467e-06
Iter: 654 loss: 1.12474049e-06
Iter: 655 loss: 1.12286637e-06
Iter: 656 loss: 1.13142028e-06
Iter: 657 loss: 1.12251792e-06
Iter: 658 loss: 1.12177372e-06
Iter: 659 loss: 1.12162e-06
Iter: 660 loss: 1.12086855e-06
Iter: 661 loss: 1.12235273e-06
Iter: 662 loss: 1.12051907e-06
Iter: 663 loss: 1.11984707e-06
Iter: 664 loss: 1.11931058e-06
Iter: 665 loss: 1.11908389e-06
Iter: 666 loss: 1.11767645e-06
Iter: 667 loss: 1.12542853e-06
Iter: 668 loss: 1.11747499e-06
Iter: 669 loss: 1.11642339e-06
Iter: 670 loss: 1.11817349e-06
Iter: 671 loss: 1.1159608e-06
Iter: 672 loss: 1.11490215e-06
Iter: 673 loss: 1.1140479e-06
Iter: 674 loss: 1.1137322e-06
Iter: 675 loss: 1.11229929e-06
Iter: 676 loss: 1.11720919e-06
Iter: 677 loss: 1.11192639e-06
Iter: 678 loss: 1.11083955e-06
Iter: 679 loss: 1.11825443e-06
Iter: 680 loss: 1.11070631e-06
Iter: 681 loss: 1.10978419e-06
Iter: 682 loss: 1.11952011e-06
Iter: 683 loss: 1.10976475e-06
Iter: 684 loss: 1.10917085e-06
Iter: 685 loss: 1.10802262e-06
Iter: 686 loss: 1.13290423e-06
Iter: 687 loss: 1.10801739e-06
Iter: 688 loss: 1.1066154e-06
Iter: 689 loss: 1.12401312e-06
Iter: 690 loss: 1.10664632e-06
Iter: 691 loss: 1.10574115e-06
Iter: 692 loss: 1.10424276e-06
Iter: 693 loss: 1.13842202e-06
Iter: 694 loss: 1.10424116e-06
Iter: 695 loss: 1.10262317e-06
Iter: 696 loss: 1.1074701e-06
Iter: 697 loss: 1.10214933e-06
Iter: 698 loss: 1.10085728e-06
Iter: 699 loss: 1.10936639e-06
Iter: 700 loss: 1.10067845e-06
Iter: 701 loss: 1.09947439e-06
Iter: 702 loss: 1.11454267e-06
Iter: 703 loss: 1.09945768e-06
Iter: 704 loss: 1.09897519e-06
Iter: 705 loss: 1.0981604e-06
Iter: 706 loss: 1.09813527e-06
Iter: 707 loss: 1.09709117e-06
Iter: 708 loss: 1.1052407e-06
Iter: 709 loss: 1.09701807e-06
Iter: 710 loss: 1.09612233e-06
Iter: 711 loss: 1.0986962e-06
Iter: 712 loss: 1.0958538e-06
Iter: 713 loss: 1.09482562e-06
Iter: 714 loss: 1.09406642e-06
Iter: 715 loss: 1.0937672e-06
Iter: 716 loss: 1.09262328e-06
Iter: 717 loss: 1.09594919e-06
Iter: 718 loss: 1.09227938e-06
Iter: 719 loss: 1.09110056e-06
Iter: 720 loss: 1.09075825e-06
Iter: 721 loss: 1.09009898e-06
Iter: 722 loss: 1.0905037e-06
Iter: 723 loss: 1.08950758e-06
Iter: 724 loss: 1.08902066e-06
Iter: 725 loss: 1.08784354e-06
Iter: 726 loss: 1.09848293e-06
Iter: 727 loss: 1.08762481e-06
Iter: 728 loss: 1.08627466e-06
Iter: 729 loss: 1.0899023e-06
Iter: 730 loss: 1.08580412e-06
Iter: 731 loss: 1.08450877e-06
Iter: 732 loss: 1.09008386e-06
Iter: 733 loss: 1.08424638e-06
Iter: 734 loss: 1.08286213e-06
Iter: 735 loss: 1.09291943e-06
Iter: 736 loss: 1.08273764e-06
Iter: 737 loss: 1.08203767e-06
Iter: 738 loss: 1.0817472e-06
Iter: 739 loss: 1.08133474e-06
Iter: 740 loss: 1.08028837e-06
Iter: 741 loss: 1.09615439e-06
Iter: 742 loss: 1.08028564e-06
Iter: 743 loss: 1.07984863e-06
Iter: 744 loss: 1.07903975e-06
Iter: 745 loss: 1.07903702e-06
Iter: 746 loss: 1.07822939e-06
Iter: 747 loss: 1.09124608e-06
Iter: 748 loss: 1.07823462e-06
Iter: 749 loss: 1.0777004e-06
Iter: 750 loss: 1.07694416e-06
Iter: 751 loss: 1.07693177e-06
Iter: 752 loss: 1.07574579e-06
Iter: 753 loss: 1.07767107e-06
Iter: 754 loss: 1.07516348e-06
Iter: 755 loss: 1.07396136e-06
Iter: 756 loss: 1.07781932e-06
Iter: 757 loss: 1.07360029e-06
Iter: 758 loss: 1.07275207e-06
Iter: 759 loss: 1.07274082e-06
Iter: 760 loss: 1.07212361e-06
Iter: 761 loss: 1.07112783e-06
Iter: 762 loss: 1.07113146e-06
Iter: 763 loss: 1.07000255e-06
Iter: 764 loss: 1.0709764e-06
Iter: 765 loss: 1.0693542e-06
Iter: 766 loss: 1.06851144e-06
Iter: 767 loss: 1.07919402e-06
Iter: 768 loss: 1.0684937e-06
Iter: 769 loss: 1.06744551e-06
Iter: 770 loss: 1.06781192e-06
Iter: 771 loss: 1.06674543e-06
Iter: 772 loss: 1.06588732e-06
Iter: 773 loss: 1.07143387e-06
Iter: 774 loss: 1.06576795e-06
Iter: 775 loss: 1.06478728e-06
Iter: 776 loss: 1.06622497e-06
Iter: 777 loss: 1.06432367e-06
Iter: 778 loss: 1.06334267e-06
Iter: 779 loss: 1.06220068e-06
Iter: 780 loss: 1.06207176e-06
Iter: 781 loss: 1.06176708e-06
Iter: 782 loss: 1.06134826e-06
Iter: 783 loss: 1.06091466e-06
Iter: 784 loss: 1.0598709e-06
Iter: 785 loss: 1.07474671e-06
Iter: 786 loss: 1.05979893e-06
Iter: 787 loss: 1.05886193e-06
Iter: 788 loss: 1.06568268e-06
Iter: 789 loss: 1.05875711e-06
Iter: 790 loss: 1.05781055e-06
Iter: 791 loss: 1.0580751e-06
Iter: 792 loss: 1.05713389e-06
Iter: 793 loss: 1.05608217e-06
Iter: 794 loss: 1.0574613e-06
Iter: 795 loss: 1.05551339e-06
Iter: 796 loss: 1.05457684e-06
Iter: 797 loss: 1.05454365e-06
Iter: 798 loss: 1.05382105e-06
Iter: 799 loss: 1.05288325e-06
Iter: 800 loss: 1.05279025e-06
Iter: 801 loss: 1.05176446e-06
Iter: 802 loss: 1.05190816e-06
Iter: 803 loss: 1.05093102e-06
Iter: 804 loss: 1.04984247e-06
Iter: 805 loss: 1.05478989e-06
Iter: 806 loss: 1.04967012e-06
Iter: 807 loss: 1.04883759e-06
Iter: 808 loss: 1.04879791e-06
Iter: 809 loss: 1.04827427e-06
Iter: 810 loss: 1.04782418e-06
Iter: 811 loss: 1.04764672e-06
Iter: 812 loss: 1.04657056e-06
Iter: 813 loss: 1.05184131e-06
Iter: 814 loss: 1.04639059e-06
Iter: 815 loss: 1.04577691e-06
Iter: 816 loss: 1.04461583e-06
Iter: 817 loss: 1.06638765e-06
Iter: 818 loss: 1.04459605e-06
Iter: 819 loss: 1.04370258e-06
Iter: 820 loss: 1.04367859e-06
Iter: 821 loss: 1.04276103e-06
Iter: 822 loss: 1.04278388e-06
Iter: 823 loss: 1.04206435e-06
Iter: 824 loss: 1.04124069e-06
Iter: 825 loss: 1.04164735e-06
Iter: 826 loss: 1.0406593e-06
Iter: 827 loss: 1.03984814e-06
Iter: 828 loss: 1.04564e-06
Iter: 829 loss: 1.03978834e-06
Iter: 830 loss: 1.03893194e-06
Iter: 831 loss: 1.03855677e-06
Iter: 832 loss: 1.03811146e-06
Iter: 833 loss: 1.03711727e-06
Iter: 834 loss: 1.04109597e-06
Iter: 835 loss: 1.0369024e-06
Iter: 836 loss: 1.03573188e-06
Iter: 837 loss: 1.04645505e-06
Iter: 838 loss: 1.03570687e-06
Iter: 839 loss: 1.03500395e-06
Iter: 840 loss: 1.03392517e-06
Iter: 841 loss: 1.03390596e-06
Iter: 842 loss: 1.03274101e-06
Iter: 843 loss: 1.03583659e-06
Iter: 844 loss: 1.03241416e-06
Iter: 845 loss: 1.0322118e-06
Iter: 846 loss: 1.03189404e-06
Iter: 847 loss: 1.03150546e-06
Iter: 848 loss: 1.0305946e-06
Iter: 849 loss: 1.04199762e-06
Iter: 850 loss: 1.03053446e-06
Iter: 851 loss: 1.02999195e-06
Iter: 852 loss: 1.02999013e-06
Iter: 853 loss: 1.02939293e-06
Iter: 854 loss: 1.02829824e-06
Iter: 855 loss: 1.05267145e-06
Iter: 856 loss: 1.02828938e-06
Iter: 857 loss: 1.02757099e-06
Iter: 858 loss: 1.02756508e-06
Iter: 859 loss: 1.02683271e-06
Iter: 860 loss: 1.02734384e-06
Iter: 861 loss: 1.02638808e-06
Iter: 862 loss: 1.02555896e-06
Iter: 863 loss: 1.02444233e-06
Iter: 864 loss: 1.02438082e-06
Iter: 865 loss: 1.02331228e-06
Iter: 866 loss: 1.0261856e-06
Iter: 867 loss: 1.02291381e-06
Iter: 868 loss: 1.02172169e-06
Iter: 869 loss: 1.0289026e-06
Iter: 870 loss: 1.02157901e-06
Iter: 871 loss: 1.02059494e-06
Iter: 872 loss: 1.02810895e-06
Iter: 873 loss: 1.02050035e-06
Iter: 874 loss: 1.01989713e-06
Iter: 875 loss: 1.02617366e-06
Iter: 876 loss: 1.01990361e-06
Iter: 877 loss: 1.01941828e-06
Iter: 878 loss: 1.01833189e-06
Iter: 879 loss: 1.03465709e-06
Iter: 880 loss: 1.01827447e-06
Iter: 881 loss: 1.01718456e-06
Iter: 882 loss: 1.02118679e-06
Iter: 883 loss: 1.01688897e-06
Iter: 884 loss: 1.01642377e-06
Iter: 885 loss: 1.01634919e-06
Iter: 886 loss: 1.01577575e-06
Iter: 887 loss: 1.01490298e-06
Iter: 888 loss: 1.01489104e-06
Iter: 889 loss: 1.01399246e-06
Iter: 890 loss: 1.01361627e-06
Iter: 891 loss: 1.01313879e-06
Iter: 892 loss: 1.01263663e-06
Iter: 893 loss: 1.01252203e-06
Iter: 894 loss: 1.01181013e-06
Iter: 895 loss: 1.0106055e-06
Iter: 896 loss: 1.01062358e-06
Iter: 897 loss: 1.00966656e-06
Iter: 898 loss: 1.01757644e-06
Iter: 899 loss: 1.00957391e-06
Iter: 900 loss: 1.00877151e-06
Iter: 901 loss: 1.0124013e-06
Iter: 902 loss: 1.00860473e-06
Iter: 903 loss: 1.00794864e-06
Iter: 904 loss: 1.00723628e-06
Iter: 905 loss: 1.00712759e-06
Iter: 906 loss: 1.00624322e-06
Iter: 907 loss: 1.00941554e-06
Iter: 908 loss: 1.00599095e-06
Iter: 909 loss: 1.00498335e-06
Iter: 910 loss: 1.01389583e-06
Iter: 911 loss: 1.00489706e-06
Iter: 912 loss: 1.00442639e-06
Iter: 913 loss: 1.00381442e-06
Iter: 914 loss: 1.00378179e-06
Iter: 915 loss: 1.00290799e-06
Iter: 916 loss: 1.00637783e-06
Iter: 917 loss: 1.00270472e-06
Iter: 918 loss: 1.00195734e-06
Iter: 919 loss: 1.01043156e-06
Iter: 920 loss: 1.00192824e-06
Iter: 921 loss: 1.00133775e-06
Iter: 922 loss: 1.00180546e-06
Iter: 923 loss: 1.00092302e-06
Iter: 924 loss: 1.00027148e-06
Iter: 925 loss: 9.9966428e-07
Iter: 926 loss: 9.99490453e-07
Iter: 927 loss: 9.98693395e-07
Iter: 928 loss: 1.00529428e-06
Iter: 929 loss: 9.98638257e-07
Iter: 930 loss: 9.97749794e-07
Iter: 931 loss: 1.00176703e-06
Iter: 932 loss: 9.97672601e-07
Iter: 933 loss: 9.97110874e-07
Iter: 934 loss: 9.96228e-07
Iter: 935 loss: 9.96235144e-07
Iter: 936 loss: 9.95524715e-07
Iter: 937 loss: 9.9547583e-07
Iter: 938 loss: 9.94872835e-07
Iter: 939 loss: 9.93561571e-07
Iter: 940 loss: 1.01409e-06
Iter: 941 loss: 9.93511321e-07
Iter: 942 loss: 9.92299e-07
Iter: 943 loss: 1.00003274e-06
Iter: 944 loss: 9.92211199e-07
Iter: 945 loss: 9.91516231e-07
Iter: 946 loss: 1.00202419e-06
Iter: 947 loss: 9.91522e-07
Iter: 948 loss: 9.90819444e-07
Iter: 949 loss: 9.90375838e-07
Iter: 950 loss: 9.90099579e-07
Iter: 951 loss: 9.89381761e-07
Iter: 952 loss: 9.89261707e-07
Iter: 953 loss: 9.88760803e-07
Iter: 954 loss: 9.87824933e-07
Iter: 955 loss: 9.9496333e-07
Iter: 956 loss: 9.8774467e-07
Iter: 957 loss: 9.87049589e-07
Iter: 958 loss: 9.94633524e-07
Iter: 959 loss: 9.8704e-07
Iter: 960 loss: 9.86408168e-07
Iter: 961 loss: 9.86565851e-07
Iter: 962 loss: 9.85959e-07
Iter: 963 loss: 9.85132829e-07
Iter: 964 loss: 9.84683311e-07
Iter: 965 loss: 9.84349299e-07
Iter: 966 loss: 9.8327132e-07
Iter: 967 loss: 9.86018222e-07
Iter: 968 loss: 9.82901e-07
Iter: 969 loss: 9.81983248e-07
Iter: 970 loss: 9.81970516e-07
Iter: 971 loss: 9.81537141e-07
Iter: 972 loss: 9.80559776e-07
Iter: 973 loss: 9.95754e-07
Iter: 974 loss: 9.8054079e-07
Iter: 975 loss: 9.79785341e-07
Iter: 976 loss: 9.79765559e-07
Iter: 977 loss: 9.79187917e-07
Iter: 978 loss: 9.78933713e-07
Iter: 979 loss: 9.78638809e-07
Iter: 980 loss: 9.77905e-07
Iter: 981 loss: 9.76992624e-07
Iter: 982 loss: 9.76911906e-07
Iter: 983 loss: 9.76279e-07
Iter: 984 loss: 9.7607824e-07
Iter: 985 loss: 9.75345074e-07
Iter: 986 loss: 9.75294483e-07
Iter: 987 loss: 9.74751742e-07
Iter: 988 loss: 9.73936e-07
Iter: 989 loss: 9.73260512e-07
Iter: 990 loss: 9.73026886e-07
Iter: 991 loss: 9.72933208e-07
Iter: 992 loss: 9.72562361e-07
Iter: 993 loss: 9.72159569e-07
Iter: 994 loss: 9.71617e-07
Iter: 995 loss: 9.71564759e-07
Iter: 996 loss: 9.70936185e-07
Iter: 997 loss: 9.70203132e-07
Iter: 998 loss: 9.70144129e-07
Iter: 999 loss: 9.69324788e-07
Iter: 1000 loss: 9.69311486e-07
Iter: 1001 loss: 9.68533641e-07
Iter: 1002 loss: 9.69266694e-07
Iter: 1003 loss: 9.6805627e-07
Iter: 1004 loss: 9.67306391e-07
Iter: 1005 loss: 9.69231451e-07
Iter: 1006 loss: 9.67063443e-07
Iter: 1007 loss: 9.6632084e-07
Iter: 1008 loss: 9.67715e-07
Iter: 1009 loss: 9.65999106e-07
Iter: 1010 loss: 9.65200343e-07
Iter: 1011 loss: 9.70511564e-07
Iter: 1012 loss: 9.65123377e-07
Iter: 1013 loss: 9.64608375e-07
Iter: 1014 loss: 9.64453079e-07
Iter: 1015 loss: 9.64076207e-07
Iter: 1016 loss: 9.63287903e-07
Iter: 1017 loss: 9.63106572e-07
Iter: 1018 loss: 9.62644435e-07
Iter: 1019 loss: 9.61998353e-07
Iter: 1020 loss: 9.61972091e-07
Iter: 1021 loss: 9.6128e-07
Iter: 1022 loss: 9.61423552e-07
Iter: 1023 loss: 9.60726879e-07
Iter: 1024 loss: 9.59934368e-07
Iter: 1025 loss: 9.59906288e-07
Iter: 1026 loss: 9.5931739e-07
Iter: 1027 loss: 9.58789542e-07
Iter: 1028 loss: 9.58724513e-07
Iter: 1029 loss: 9.58140276e-07
Iter: 1030 loss: 9.57115617e-07
Iter: 1031 loss: 9.57117663e-07
Iter: 1032 loss: 9.56335271e-07
Iter: 1033 loss: 9.59456202e-07
Iter: 1034 loss: 9.56157692e-07
Iter: 1035 loss: 9.5560722e-07
Iter: 1036 loss: 9.55615178e-07
Iter: 1037 loss: 9.55129735e-07
Iter: 1038 loss: 9.54388497e-07
Iter: 1039 loss: 9.54387133e-07
Iter: 1040 loss: 9.53460926e-07
Iter: 1041 loss: 9.59764066e-07
Iter: 1042 loss: 9.53381971e-07
Iter: 1043 loss: 9.5272361e-07
Iter: 1044 loss: 9.54894631e-07
Iter: 1045 loss: 9.52543e-07
Iter: 1046 loss: 9.51810478e-07
Iter: 1047 loss: 9.52600033e-07
Iter: 1048 loss: 9.5147675e-07
Iter: 1049 loss: 9.50587605e-07
Iter: 1050 loss: 9.49843468e-07
Iter: 1051 loss: 9.49539412e-07
Iter: 1052 loss: 9.48510149e-07
Iter: 1053 loss: 9.58549e-07
Iter: 1054 loss: 9.48486843e-07
Iter: 1055 loss: 9.47596845e-07
Iter: 1056 loss: 9.57301836e-07
Iter: 1057 loss: 9.4759821e-07
Iter: 1058 loss: 9.47162221e-07
Iter: 1059 loss: 9.46227374e-07
Iter: 1060 loss: 9.61590331e-07
Iter: 1061 loss: 9.4621e-07
Iter: 1062 loss: 9.45534964e-07
Iter: 1063 loss: 9.4553e-07
Iter: 1064 loss: 9.44826752e-07
Iter: 1065 loss: 9.47128456e-07
Iter: 1066 loss: 9.44656335e-07
Iter: 1067 loss: 9.44113651e-07
Iter: 1068 loss: 9.43085411e-07
Iter: 1069 loss: 9.63727189e-07
Iter: 1070 loss: 9.43084444e-07
Iter: 1071 loss: 9.4260372e-07
Iter: 1072 loss: 9.4246974e-07
Iter: 1073 loss: 9.41873395e-07
Iter: 1074 loss: 9.41151143e-07
Iter: 1075 loss: 9.41073438e-07
Iter: 1076 loss: 9.40349423e-07
Iter: 1077 loss: 9.45626823e-07
Iter: 1078 loss: 9.40283144e-07
Iter: 1079 loss: 9.39653319e-07
Iter: 1080 loss: 9.41789665e-07
Iter: 1081 loss: 9.39488416e-07
Iter: 1082 loss: 9.38891617e-07
Iter: 1083 loss: 9.39613642e-07
Iter: 1084 loss: 9.38592336e-07
Iter: 1085 loss: 9.37847858e-07
Iter: 1086 loss: 9.38320795e-07
Iter: 1087 loss: 9.37332175e-07
Iter: 1088 loss: 9.36617482e-07
Iter: 1089 loss: 9.39559925e-07
Iter: 1090 loss: 9.36447236e-07
Iter: 1091 loss: 9.35791149e-07
Iter: 1092 loss: 9.4444863e-07
Iter: 1093 loss: 9.35782282e-07
Iter: 1094 loss: 9.35315029e-07
Iter: 1095 loss: 9.34219202e-07
Iter: 1096 loss: 9.45129273e-07
Iter: 1097 loss: 9.3405896e-07
Iter: 1098 loss: 9.33259e-07
Iter: 1099 loss: 9.4569e-07
Iter: 1100 loss: 9.33260537e-07
Iter: 1101 loss: 9.32475302e-07
Iter: 1102 loss: 9.37864968e-07
Iter: 1103 loss: 9.32447165e-07
Iter: 1104 loss: 9.3197e-07
Iter: 1105 loss: 9.30968781e-07
Iter: 1106 loss: 9.49628941e-07
Iter: 1107 loss: 9.30975261e-07
Iter: 1108 loss: 9.30518127e-07
Iter: 1109 loss: 9.30433316e-07
Iter: 1110 loss: 9.29911835e-07
Iter: 1111 loss: 9.29319754e-07
Iter: 1112 loss: 9.29244379e-07
Iter: 1113 loss: 9.2839332e-07
Iter: 1114 loss: 9.29436965e-07
Iter: 1115 loss: 9.27959491e-07
Iter: 1116 loss: 9.27091378e-07
Iter: 1117 loss: 9.38153846e-07
Iter: 1118 loss: 9.27083533e-07
Iter: 1119 loss: 9.26594055e-07
Iter: 1120 loss: 9.26777489e-07
Iter: 1121 loss: 9.26232133e-07
Iter: 1122 loss: 9.25569964e-07
Iter: 1123 loss: 9.26881512e-07
Iter: 1124 loss: 9.25280233e-07
Iter: 1125 loss: 9.24639608e-07
Iter: 1126 loss: 9.26854511e-07
Iter: 1127 loss: 9.24463791e-07
Iter: 1128 loss: 9.23945549e-07
Iter: 1129 loss: 9.31251748e-07
Iter: 1130 loss: 9.23951802e-07
Iter: 1131 loss: 9.23614948e-07
Iter: 1132 loss: 9.22829031e-07
Iter: 1133 loss: 9.32174714e-07
Iter: 1134 loss: 9.22767185e-07
Iter: 1135 loss: 9.21986441e-07
Iter: 1136 loss: 9.27547376e-07
Iter: 1137 loss: 9.21915102e-07
Iter: 1138 loss: 9.21094511e-07
Iter: 1139 loss: 9.27300277e-07
Iter: 1140 loss: 9.21024366e-07
Iter: 1141 loss: 9.20607533e-07
Iter: 1142 loss: 9.19734532e-07
Iter: 1143 loss: 9.35846515e-07
Iter: 1144 loss: 9.19721231e-07
Iter: 1145 loss: 9.19177467e-07
Iter: 1146 loss: 9.19105503e-07
Iter: 1147 loss: 9.1863393e-07
Iter: 1148 loss: 9.18312708e-07
Iter: 1149 loss: 9.18168e-07
Iter: 1150 loss: 9.17576e-07
Iter: 1151 loss: 9.17991315e-07
Iter: 1152 loss: 9.17228e-07
Iter: 1153 loss: 9.16519753e-07
Iter: 1154 loss: 9.23923892e-07
Iter: 1155 loss: 9.16506906e-07
Iter: 1156 loss: 9.16073759e-07
Iter: 1157 loss: 9.15667499e-07
Iter: 1158 loss: 9.15537385e-07
Iter: 1159 loss: 9.14697182e-07
Iter: 1160 loss: 9.16408851e-07
Iter: 1161 loss: 9.14329348e-07
Iter: 1162 loss: 9.13570261e-07
Iter: 1163 loss: 9.21093545e-07
Iter: 1164 loss: 9.13527799e-07
Iter: 1165 loss: 9.12986593e-07
Iter: 1166 loss: 9.16130659e-07
Iter: 1167 loss: 9.12904056e-07
Iter: 1168 loss: 9.12506096e-07
Iter: 1169 loss: 9.11631673e-07
Iter: 1170 loss: 9.25030463e-07
Iter: 1171 loss: 9.11559255e-07
Iter: 1172 loss: 9.11385655e-07
Iter: 1173 loss: 9.11177665e-07
Iter: 1174 loss: 9.1077095e-07
Iter: 1175 loss: 9.10464507e-07
Iter: 1176 loss: 9.10348035e-07
Iter: 1177 loss: 9.09827861e-07
Iter: 1178 loss: 9.09901928e-07
Iter: 1179 loss: 9.09447408e-07
Iter: 1180 loss: 9.08756533e-07
Iter: 1181 loss: 9.16517308e-07
Iter: 1182 loss: 9.08732773e-07
Iter: 1183 loss: 9.08217373e-07
Iter: 1184 loss: 9.07495235e-07
Iter: 1185 loss: 9.07453455e-07
Iter: 1186 loss: 9.0669937e-07
Iter: 1187 loss: 9.12774567e-07
Iter: 1188 loss: 9.06632351e-07
Iter: 1189 loss: 9.05867296e-07
Iter: 1190 loss: 9.07947651e-07
Iter: 1191 loss: 9.05573927e-07
Iter: 1192 loss: 9.05003276e-07
Iter: 1193 loss: 9.05370143e-07
Iter: 1194 loss: 9.04625267e-07
Iter: 1195 loss: 9.03958778e-07
Iter: 1196 loss: 9.0768674e-07
Iter: 1197 loss: 9.038489e-07
Iter: 1198 loss: 9.03333614e-07
Iter: 1199 loss: 9.05144134e-07
Iter: 1200 loss: 9.03206171e-07
Iter: 1201 loss: 9.0259357e-07
Iter: 1202 loss: 9.053133e-07
Iter: 1203 loss: 9.02460386e-07
Iter: 1204 loss: 9.02041847e-07
Iter: 1205 loss: 9.01320618e-07
Iter: 1206 loss: 9.01309761e-07
Iter: 1207 loss: 9.00718589e-07
Iter: 1208 loss: 9.00715861e-07
Iter: 1209 loss: 9.00126906e-07
Iter: 1210 loss: 8.99467182e-07
Iter: 1211 loss: 8.99389306e-07
Iter: 1212 loss: 8.98832127e-07
Iter: 1213 loss: 9.00294083e-07
Iter: 1214 loss: 8.98620101e-07
Iter: 1215 loss: 8.97813663e-07
Iter: 1216 loss: 9.01456e-07
Iter: 1217 loss: 8.97657401e-07
Iter: 1218 loss: 8.97182531e-07
Iter: 1219 loss: 8.97043151e-07
Iter: 1220 loss: 8.96753136e-07
Iter: 1221 loss: 8.96224378e-07
Iter: 1222 loss: 8.99999463e-07
Iter: 1223 loss: 8.96197548e-07
Iter: 1224 loss: 8.95637868e-07
Iter: 1225 loss: 8.96293159e-07
Iter: 1226 loss: 8.95326e-07
Iter: 1227 loss: 8.94771915e-07
Iter: 1228 loss: 8.95171183e-07
Iter: 1229 loss: 8.94441257e-07
Iter: 1230 loss: 8.93719857e-07
Iter: 1231 loss: 8.9537e-07
Iter: 1232 loss: 8.93420633e-07
Iter: 1233 loss: 8.92680589e-07
Iter: 1234 loss: 8.96801453e-07
Iter: 1235 loss: 8.92576736e-07
Iter: 1236 loss: 8.91844422e-07
Iter: 1237 loss: 8.95803566e-07
Iter: 1238 loss: 8.91706236e-07
Iter: 1239 loss: 8.91236709e-07
Iter: 1240 loss: 8.90902697e-07
Iter: 1241 loss: 8.90723868e-07
Iter: 1242 loss: 8.90335457e-07
Iter: 1243 loss: 8.90274464e-07
Iter: 1244 loss: 8.89948922e-07
Iter: 1245 loss: 8.8916704e-07
Iter: 1246 loss: 9.0083438e-07
Iter: 1247 loss: 8.89152034e-07
Iter: 1248 loss: 8.88537e-07
Iter: 1249 loss: 8.9355126e-07
Iter: 1250 loss: 8.88490149e-07
Iter: 1251 loss: 8.87802e-07
Iter: 1252 loss: 8.90037427e-07
Iter: 1253 loss: 8.87569797e-07
Iter: 1254 loss: 8.87081e-07
Iter: 1255 loss: 8.86115458e-07
Iter: 1256 loss: 9.09860432e-07
Iter: 1257 loss: 8.86131602e-07
Iter: 1258 loss: 8.85522923e-07
Iter: 1259 loss: 8.85470513e-07
Iter: 1260 loss: 8.8488008e-07
Iter: 1261 loss: 8.84621e-07
Iter: 1262 loss: 8.84323754e-07
Iter: 1263 loss: 8.83585699e-07
Iter: 1264 loss: 8.85219e-07
Iter: 1265 loss: 8.83282894e-07
Iter: 1266 loss: 8.82789e-07
Iter: 1267 loss: 8.87085605e-07
Iter: 1268 loss: 8.82740437e-07
Iter: 1269 loss: 8.82259428e-07
Iter: 1270 loss: 8.8303e-07
Iter: 1271 loss: 8.82022675e-07
Iter: 1272 loss: 8.8127382e-07
Iter: 1273 loss: 8.83438702e-07
Iter: 1274 loss: 8.81086237e-07
Iter: 1275 loss: 8.80585958e-07
Iter: 1276 loss: 8.80813559e-07
Iter: 1277 loss: 8.8026718e-07
Iter: 1278 loss: 8.79678112e-07
Iter: 1279 loss: 8.87514091e-07
Iter: 1280 loss: 8.79655374e-07
Iter: 1281 loss: 8.79224899e-07
Iter: 1282 loss: 8.78410049e-07
Iter: 1283 loss: 8.95328e-07
Iter: 1284 loss: 8.78403739e-07
Iter: 1285 loss: 8.77835305e-07
Iter: 1286 loss: 8.77852813e-07
Iter: 1287 loss: 8.77294724e-07
Iter: 1288 loss: 8.77699108e-07
Iter: 1289 loss: 8.76952186e-07
Iter: 1290 loss: 8.76478e-07
Iter: 1291 loss: 8.76365675e-07
Iter: 1292 loss: 8.76099193e-07
Iter: 1293 loss: 8.75499325e-07
Iter: 1294 loss: 8.80942252e-07
Iter: 1295 loss: 8.75431965e-07
Iter: 1296 loss: 8.7477963e-07
Iter: 1297 loss: 8.74924069e-07
Iter: 1298 loss: 8.74326304e-07
Iter: 1299 loss: 8.73651743e-07
Iter: 1300 loss: 8.73483941e-07
Iter: 1301 loss: 8.7309536e-07
Iter: 1302 loss: 8.72236853e-07
Iter: 1303 loss: 8.80950324e-07
Iter: 1304 loss: 8.7223043e-07
Iter: 1305 loss: 8.71666714e-07
Iter: 1306 loss: 8.77476737e-07
Iter: 1307 loss: 8.71657221e-07
Iter: 1308 loss: 8.71216457e-07
Iter: 1309 loss: 8.71401085e-07
Iter: 1310 loss: 8.70936674e-07
Iter: 1311 loss: 8.70440033e-07
Iter: 1312 loss: 8.7111107e-07
Iter: 1313 loss: 8.70179178e-07
Iter: 1314 loss: 8.69610801e-07
Iter: 1315 loss: 8.76360502e-07
Iter: 1316 loss: 8.69602673e-07
Iter: 1317 loss: 8.69325277e-07
Iter: 1318 loss: 8.68734105e-07
Iter: 1319 loss: 8.80451e-07
Iter: 1320 loss: 8.68748543e-07
Iter: 1321 loss: 8.68173288e-07
Iter: 1322 loss: 8.73946078e-07
Iter: 1323 loss: 8.68172322e-07
Iter: 1324 loss: 8.67583253e-07
Iter: 1325 loss: 8.67991218e-07
Iter: 1326 loss: 8.67194785e-07
Iter: 1327 loss: 8.66613277e-07
Iter: 1328 loss: 8.65758352e-07
Iter: 1329 loss: 8.65742891e-07
Iter: 1330 loss: 8.65263587e-07
Iter: 1331 loss: 8.6513154e-07
Iter: 1332 loss: 8.64667584e-07
Iter: 1333 loss: 8.64961294e-07
Iter: 1334 loss: 8.64356821e-07
Iter: 1335 loss: 8.63942773e-07
Iter: 1336 loss: 8.63925379e-07
Iter: 1337 loss: 8.63575e-07
Iter: 1338 loss: 8.62926584e-07
Iter: 1339 loss: 8.64298272e-07
Iter: 1340 loss: 8.62704269e-07
Iter: 1341 loss: 8.62183924e-07
Iter: 1342 loss: 8.62179434e-07
Iter: 1343 loss: 8.6175703e-07
Iter: 1344 loss: 8.62711261e-07
Iter: 1345 loss: 8.61582e-07
Iter: 1346 loss: 8.61098329e-07
Iter: 1347 loss: 8.60942066e-07
Iter: 1348 loss: 8.60671548e-07
Iter: 1349 loss: 8.60208331e-07
Iter: 1350 loss: 8.60202931e-07
Iter: 1351 loss: 8.59809461e-07
Iter: 1352 loss: 8.5947454e-07
Iter: 1353 loss: 8.59342379e-07
Iter: 1354 loss: 8.58846306e-07
Iter: 1355 loss: 8.58806288e-07
Iter: 1356 loss: 8.58469775e-07
Iter: 1357 loss: 8.58080625e-07
Iter: 1358 loss: 8.58009e-07
Iter: 1359 loss: 8.57700172e-07
Iter: 1360 loss: 8.57006512e-07
Iter: 1361 loss: 8.69067151e-07
Iter: 1362 loss: 8.56999634e-07
Iter: 1363 loss: 8.5624265e-07
Iter: 1364 loss: 8.58034184e-07
Iter: 1365 loss: 8.55983387e-07
Iter: 1366 loss: 8.55186045e-07
Iter: 1367 loss: 8.64149115e-07
Iter: 1368 loss: 8.5514597e-07
Iter: 1369 loss: 8.54708446e-07
Iter: 1370 loss: 8.54099881e-07
Iter: 1371 loss: 8.54065775e-07
Iter: 1372 loss: 8.53262236e-07
Iter: 1373 loss: 8.55711505e-07
Iter: 1374 loss: 8.53005417e-07
Iter: 1375 loss: 8.52430503e-07
Iter: 1376 loss: 8.58726821e-07
Iter: 1377 loss: 8.52435107e-07
Iter: 1378 loss: 8.51949835e-07
Iter: 1379 loss: 8.54658481e-07
Iter: 1380 loss: 8.51883499e-07
Iter: 1381 loss: 8.51474624e-07
Iter: 1382 loss: 8.51327172e-07
Iter: 1383 loss: 8.51059326e-07
Iter: 1384 loss: 8.50534434e-07
Iter: 1385 loss: 8.54138534e-07
Iter: 1386 loss: 8.50464914e-07
Iter: 1387 loss: 8.49924504e-07
Iter: 1388 loss: 8.51635605e-07
Iter: 1389 loss: 8.49728679e-07
Iter: 1390 loss: 8.4936562e-07
Iter: 1391 loss: 8.48699528e-07
Iter: 1392 loss: 8.65722711e-07
Iter: 1393 loss: 8.48695549e-07
Iter: 1394 loss: 8.47936462e-07
Iter: 1395 loss: 8.54191455e-07
Iter: 1396 loss: 8.47864385e-07
Iter: 1397 loss: 8.47235469e-07
Iter: 1398 loss: 8.53011784e-07
Iter: 1399 loss: 8.47231547e-07
Iter: 1400 loss: 8.46876503e-07
Iter: 1401 loss: 8.46138505e-07
Iter: 1402 loss: 8.58499391e-07
Iter: 1403 loss: 8.46136345e-07
Iter: 1404 loss: 8.45556883e-07
Iter: 1405 loss: 8.54753239e-07
Iter: 1406 loss: 8.45543582e-07
Iter: 1407 loss: 8.44961676e-07
Iter: 1408 loss: 8.45554382e-07
Iter: 1409 loss: 8.44645342e-07
Iter: 1410 loss: 8.44047577e-07
Iter: 1411 loss: 8.4381378e-07
Iter: 1412 loss: 8.43491421e-07
Iter: 1413 loss: 8.4257988e-07
Iter: 1414 loss: 8.45158297e-07
Iter: 1415 loss: 8.4230561e-07
Iter: 1416 loss: 8.41607061e-07
Iter: 1417 loss: 8.47360411e-07
Iter: 1418 loss: 8.41557608e-07
Iter: 1419 loss: 8.4101805e-07
Iter: 1420 loss: 8.44628062e-07
Iter: 1421 loss: 8.40978373e-07
Iter: 1422 loss: 8.40451094e-07
Iter: 1423 loss: 8.41507926e-07
Iter: 1424 loss: 8.40218831e-07
Iter: 1425 loss: 8.39677796e-07
Iter: 1426 loss: 8.39827749e-07
Iter: 1427 loss: 8.39329743e-07
Iter: 1428 loss: 8.38963558e-07
Iter: 1429 loss: 8.38940366e-07
Iter: 1430 loss: 8.38595497e-07
Iter: 1431 loss: 8.37870118e-07
Iter: 1432 loss: 8.49858e-07
Iter: 1433 loss: 8.37841469e-07
Iter: 1434 loss: 8.37115124e-07
Iter: 1435 loss: 8.38341805e-07
Iter: 1436 loss: 8.36792481e-07
Iter: 1437 loss: 8.36326933e-07
Iter: 1438 loss: 8.36301126e-07
Iter: 1439 loss: 8.35830804e-07
Iter: 1440 loss: 8.35377193e-07
Iter: 1441 loss: 8.3531063e-07
Iter: 1442 loss: 8.34759248e-07
Iter: 1443 loss: 8.37099606e-07
Iter: 1444 loss: 8.34701837e-07
Iter: 1445 loss: 8.3406826e-07
Iter: 1446 loss: 8.35692731e-07
Iter: 1447 loss: 8.33847253e-07
Iter: 1448 loss: 8.334e-07
Iter: 1449 loss: 8.333908e-07
Iter: 1450 loss: 8.33012848e-07
Iter: 1451 loss: 8.32453338e-07
Iter: 1452 loss: 8.33647846e-07
Iter: 1453 loss: 8.3221147e-07
Iter: 1454 loss: 8.31520936e-07
Iter: 1455 loss: 8.33690535e-07
Iter: 1456 loss: 8.3132835e-07
Iter: 1457 loss: 8.30584838e-07
Iter: 1458 loss: 8.32059754e-07
Iter: 1459 loss: 8.30260774e-07
Iter: 1460 loss: 8.29841213e-07
Iter: 1461 loss: 8.29817338e-07
Iter: 1462 loss: 8.29436317e-07
Iter: 1463 loss: 8.29650901e-07
Iter: 1464 loss: 8.29196665e-07
Iter: 1465 loss: 8.28857935e-07
Iter: 1466 loss: 8.2857639e-07
Iter: 1467 loss: 8.28493626e-07
Iter: 1468 loss: 8.28011878e-07
Iter: 1469 loss: 8.28027908e-07
Iter: 1470 loss: 8.2773829e-07
Iter: 1471 loss: 8.27323788e-07
Iter: 1472 loss: 8.27299573e-07
Iter: 1473 loss: 8.26760925e-07
Iter: 1474 loss: 8.276009e-07
Iter: 1475 loss: 8.26542873e-07
Iter: 1476 loss: 8.2596739e-07
Iter: 1477 loss: 8.33727427e-07
Iter: 1478 loss: 8.25964e-07
Iter: 1479 loss: 8.25560903e-07
Iter: 1480 loss: 8.2505295e-07
Iter: 1481 loss: 8.25001e-07
Iter: 1482 loss: 8.24518338e-07
Iter: 1483 loss: 8.24523e-07
Iter: 1484 loss: 8.24167046e-07
Iter: 1485 loss: 8.23714e-07
Iter: 1486 loss: 8.23663186e-07
Iter: 1487 loss: 8.23144262e-07
Iter: 1488 loss: 8.25143502e-07
Iter: 1489 loss: 8.23061e-07
Iter: 1490 loss: 8.2260533e-07
Iter: 1491 loss: 8.23974517e-07
Iter: 1492 loss: 8.22473567e-07
Iter: 1493 loss: 8.21972833e-07
Iter: 1494 loss: 8.21611138e-07
Iter: 1495 loss: 8.21479659e-07
Iter: 1496 loss: 8.21098922e-07
Iter: 1497 loss: 8.21003312e-07
Iter: 1498 loss: 8.20585285e-07
Iter: 1499 loss: 8.20625189e-07
Iter: 1500 loss: 8.20263722e-07
Iter: 1501 loss: 8.19815796e-07
Iter: 1502 loss: 8.19696766e-07
Iter: 1503 loss: 8.1944853e-07
Iter: 1504 loss: 8.18780052e-07
Iter: 1505 loss: 8.27045255e-07
Iter: 1506 loss: 8.18786418e-07
Iter: 1507 loss: 8.18479123e-07
Iter: 1508 loss: 8.18031367e-07
Iter: 1509 loss: 8.18018293e-07
Iter: 1510 loss: 8.17549505e-07
Iter: 1511 loss: 8.22550817e-07
Iter: 1512 loss: 8.17532964e-07
Iter: 1513 loss: 8.17116302e-07
Iter: 1514 loss: 8.17781824e-07
Iter: 1515 loss: 8.16902684e-07
Iter: 1516 loss: 8.16600505e-07
Iter: 1517 loss: 8.17475325e-07
Iter: 1518 loss: 8.1649091e-07
Iter: 1519 loss: 8.16001375e-07
Iter: 1520 loss: 8.16546333e-07
Iter: 1521 loss: 8.15734893e-07
Iter: 1522 loss: 8.15293504e-07
Iter: 1523 loss: 8.15409578e-07
Iter: 1524 loss: 8.14975067e-07
Iter: 1525 loss: 8.14446707e-07
Iter: 1526 loss: 8.16043269e-07
Iter: 1527 loss: 8.14276518e-07
Iter: 1528 loss: 8.13649137e-07
Iter: 1529 loss: 8.14796635e-07
Iter: 1530 loss: 8.13354177e-07
Iter: 1531 loss: 8.12905341e-07
Iter: 1532 loss: 8.15919918e-07
Iter: 1533 loss: 8.1287078e-07
Iter: 1534 loss: 8.12392557e-07
Iter: 1535 loss: 8.15182602e-07
Iter: 1536 loss: 8.12348731e-07
Iter: 1537 loss: 8.12027565e-07
Iter: 1538 loss: 8.11537234e-07
Iter: 1539 loss: 8.11546272e-07
Iter: 1540 loss: 8.11194468e-07
Iter: 1541 loss: 8.11138136e-07
Iter: 1542 loss: 8.10791903e-07
Iter: 1543 loss: 8.10122344e-07
Iter: 1544 loss: 8.23152789e-07
Iter: 1545 loss: 8.10125641e-07
Iter: 1546 loss: 8.09563062e-07
Iter: 1547 loss: 8.13820691e-07
Iter: 1548 loss: 8.09506389e-07
Iter: 1549 loss: 8.08925677e-07
Iter: 1550 loss: 8.11208e-07
Iter: 1551 loss: 8.08770437e-07
Iter: 1552 loss: 8.08332459e-07
Iter: 1553 loss: 8.0825231e-07
Iter: 1554 loss: 8.07953029e-07
Iter: 1555 loss: 8.07434844e-07
Iter: 1556 loss: 8.07427909e-07
Iter: 1557 loss: 8.07114134e-07
Iter: 1558 loss: 8.0665427e-07
Iter: 1559 loss: 8.06651144e-07
Iter: 1560 loss: 8.06058779e-07
Iter: 1561 loss: 8.07400568e-07
Iter: 1562 loss: 8.05824129e-07
Iter: 1563 loss: 8.05158038e-07
Iter: 1564 loss: 8.08034429e-07
Iter: 1565 loss: 8.05010927e-07
Iter: 1566 loss: 8.0444272e-07
Iter: 1567 loss: 8.04887577e-07
Iter: 1568 loss: 8.0409518e-07
Iter: 1569 loss: 8.03754801e-07
Iter: 1570 loss: 8.03671696e-07
Iter: 1571 loss: 8.03376395e-07
Iter: 1572 loss: 8.02793238e-07
Iter: 1573 loss: 8.16756312e-07
Iter: 1574 loss: 8.02785678e-07
Iter: 1575 loss: 8.02372142e-07
Iter: 1576 loss: 8.08660729e-07
Iter: 1577 loss: 8.02381521e-07
Iter: 1578 loss: 8.01973556e-07
Iter: 1579 loss: 8.01928252e-07
Iter: 1580 loss: 8.01611918e-07
Iter: 1581 loss: 8.01262729e-07
Iter: 1582 loss: 8.01011652e-07
Iter: 1583 loss: 8.00863688e-07
Iter: 1584 loss: 8.00352893e-07
Iter: 1585 loss: 8.05562877e-07
Iter: 1586 loss: 8.00317821e-07
Iter: 1587 loss: 7.99724205e-07
Iter: 1588 loss: 8.00240514e-07
Iter: 1589 loss: 7.99373879e-07
Iter: 1590 loss: 7.98935332e-07
Iter: 1591 loss: 7.99367058e-07
Iter: 1592 loss: 7.98661517e-07
Iter: 1593 loss: 7.98050905e-07
Iter: 1594 loss: 8.03749117e-07
Iter: 1595 loss: 7.98038e-07
Iter: 1596 loss: 7.97644759e-07
Iter: 1597 loss: 7.97187852e-07
Iter: 1598 loss: 7.97138455e-07
Iter: 1599 loss: 7.96570816e-07
Iter: 1600 loss: 7.9960364e-07
Iter: 1601 loss: 7.96507038e-07
Iter: 1602 loss: 7.95979304e-07
Iter: 1603 loss: 7.97609118e-07
Iter: 1604 loss: 7.95837479e-07
Iter: 1605 loss: 7.95502217e-07
Iter: 1606 loss: 8.00494036e-07
Iter: 1607 loss: 7.95490791e-07
Iter: 1608 loss: 7.95138931e-07
Iter: 1609 loss: 7.94664629e-07
Iter: 1610 loss: 7.94649111e-07
Iter: 1611 loss: 7.942715e-07
Iter: 1612 loss: 7.98230872e-07
Iter: 1613 loss: 7.94263e-07
Iter: 1614 loss: 7.93847903e-07
Iter: 1615 loss: 7.93673905e-07
Iter: 1616 loss: 7.93458e-07
Iter: 1617 loss: 7.92934657e-07
Iter: 1618 loss: 7.92747869e-07
Iter: 1619 loss: 7.92466949e-07
Iter: 1620 loss: 7.91943876e-07
Iter: 1621 loss: 7.96556435e-07
Iter: 1622 loss: 7.91900675e-07
Iter: 1623 loss: 7.91380103e-07
Iter: 1624 loss: 7.94299524e-07
Iter: 1625 loss: 7.91309049e-07
Iter: 1626 loss: 7.91020057e-07
Iter: 1627 loss: 7.90723561e-07
Iter: 1628 loss: 7.90674108e-07
Iter: 1629 loss: 7.90236072e-07
Iter: 1630 loss: 7.9671662e-07
Iter: 1631 loss: 7.90240961e-07
Iter: 1632 loss: 7.89881256e-07
Iter: 1633 loss: 7.89227101e-07
Iter: 1634 loss: 7.89220394e-07
Iter: 1635 loss: 7.88630132e-07
Iter: 1636 loss: 7.91029947e-07
Iter: 1637 loss: 7.88515877e-07
Iter: 1638 loss: 7.87953923e-07
Iter: 1639 loss: 7.91200705e-07
Iter: 1640 loss: 7.87836484e-07
Iter: 1641 loss: 7.87487124e-07
Iter: 1642 loss: 7.92109176e-07
Iter: 1643 loss: 7.87498834e-07
Iter: 1644 loss: 7.87171473e-07
Iter: 1645 loss: 7.87068814e-07
Iter: 1646 loss: 7.86932901e-07
Iter: 1647 loss: 7.86596274e-07
Iter: 1648 loss: 7.87670274e-07
Iter: 1649 loss: 7.86499186e-07
Iter: 1650 loss: 7.86074111e-07
Iter: 1651 loss: 7.87024476e-07
Iter: 1652 loss: 7.85914438e-07
Iter: 1653 loss: 7.85611064e-07
Iter: 1654 loss: 7.85075486e-07
Iter: 1655 loss: 7.85089128e-07
Iter: 1656 loss: 7.84429517e-07
Iter: 1657 loss: 7.85979807e-07
Iter: 1658 loss: 7.84195663e-07
Iter: 1659 loss: 7.83616258e-07
Iter: 1660 loss: 7.83605174e-07
Iter: 1661 loss: 7.83067492e-07
Iter: 1662 loss: 7.82813174e-07
Iter: 1663 loss: 7.825588e-07
Iter: 1664 loss: 7.82162488e-07
Iter: 1665 loss: 7.85436782e-07
Iter: 1666 loss: 7.82128552e-07
Iter: 1667 loss: 7.81618496e-07
Iter: 1668 loss: 7.8196382e-07
Iter: 1669 loss: 7.81325298e-07
Iter: 1670 loss: 7.8092512e-07
Iter: 1671 loss: 7.80942457e-07
Iter: 1672 loss: 7.80613846e-07
Iter: 1673 loss: 7.80086395e-07
Iter: 1674 loss: 7.82443919e-07
Iter: 1675 loss: 7.80010282e-07
Iter: 1676 loss: 7.79479194e-07
Iter: 1677 loss: 7.82938514e-07
Iter: 1678 loss: 7.79412915e-07
Iter: 1679 loss: 7.78929575e-07
Iter: 1680 loss: 7.7953581e-07
Iter: 1681 loss: 7.78697654e-07
Iter: 1682 loss: 7.78189928e-07
Iter: 1683 loss: 7.77961532e-07
Iter: 1684 loss: 7.77701871e-07
Iter: 1685 loss: 7.77260652e-07
Iter: 1686 loss: 7.77230525e-07
Iter: 1687 loss: 7.76961826e-07
Iter: 1688 loss: 7.76330921e-07
Iter: 1689 loss: 7.85008297e-07
Iter: 1690 loss: 7.76302954e-07
Iter: 1691 loss: 7.75555691e-07
Iter: 1692 loss: 7.77033108e-07
Iter: 1693 loss: 7.752584e-07
Iter: 1694 loss: 7.74836963e-07
Iter: 1695 loss: 7.74811269e-07
Iter: 1696 loss: 7.74360956e-07
Iter: 1697 loss: 7.74927685e-07
Iter: 1698 loss: 7.74131422e-07
Iter: 1699 loss: 7.73734143e-07
Iter: 1700 loss: 7.74196e-07
Iter: 1701 loss: 7.73517058e-07
Iter: 1702 loss: 7.72818e-07
Iter: 1703 loss: 7.74187129e-07
Iter: 1704 loss: 7.72532871e-07
Iter: 1705 loss: 7.72051067e-07
Iter: 1706 loss: 7.71700115e-07
Iter: 1707 loss: 7.71522366e-07
Iter: 1708 loss: 7.70925112e-07
Iter: 1709 loss: 7.74414104e-07
Iter: 1710 loss: 7.70830184e-07
Iter: 1711 loss: 7.70531472e-07
Iter: 1712 loss: 7.70500264e-07
Iter: 1713 loss: 7.70275392e-07
Iter: 1714 loss: 7.69896474e-07
Iter: 1715 loss: 7.77750188e-07
Iter: 1716 loss: 7.69875385e-07
Iter: 1717 loss: 7.69528924e-07
Iter: 1718 loss: 7.73769727e-07
Iter: 1719 loss: 7.69524036e-07
Iter: 1720 loss: 7.6916217e-07
Iter: 1721 loss: 7.68985387e-07
Iter: 1722 loss: 7.68786208e-07
Iter: 1723 loss: 7.68253358e-07
Iter: 1724 loss: 7.69467874e-07
Iter: 1725 loss: 7.68077712e-07
Iter: 1726 loss: 7.67497966e-07
Iter: 1727 loss: 7.67763481e-07
Iter: 1728 loss: 7.67145e-07
Iter: 1729 loss: 7.66445226e-07
Iter: 1730 loss: 7.66889286e-07
Iter: 1731 loss: 7.6600503e-07
Iter: 1732 loss: 7.65561254e-07
Iter: 1733 loss: 7.65496623e-07
Iter: 1734 loss: 7.65096843e-07
Iter: 1735 loss: 7.65587174e-07
Iter: 1736 loss: 7.64893286e-07
Iter: 1737 loss: 7.64600088e-07
Iter: 1738 loss: 7.64965534e-07
Iter: 1739 loss: 7.64405797e-07
Iter: 1740 loss: 7.63894491e-07
Iter: 1741 loss: 7.64382321e-07
Iter: 1742 loss: 7.63568892e-07
Iter: 1743 loss: 7.63063554e-07
Iter: 1744 loss: 7.62624836e-07
Iter: 1745 loss: 7.62490117e-07
Iter: 1746 loss: 7.62388936e-07
Iter: 1747 loss: 7.62137915e-07
Iter: 1748 loss: 7.61796059e-07
Iter: 1749 loss: 7.61074602e-07
Iter: 1750 loss: 7.72692033e-07
Iter: 1751 loss: 7.61053229e-07
Iter: 1752 loss: 7.60495084e-07
Iter: 1753 loss: 7.65568529e-07
Iter: 1754 loss: 7.60467742e-07
Iter: 1755 loss: 7.59966269e-07
Iter: 1756 loss: 7.63062303e-07
Iter: 1757 loss: 7.59894533e-07
Iter: 1758 loss: 7.59629302e-07
Iter: 1759 loss: 7.58931719e-07
Iter: 1760 loss: 7.67404515e-07
Iter: 1761 loss: 7.5891711e-07
Iter: 1762 loss: 7.58225383e-07
Iter: 1763 loss: 7.61487968e-07
Iter: 1764 loss: 7.58111e-07
Iter: 1765 loss: 7.57477437e-07
Iter: 1766 loss: 7.59577688e-07
Iter: 1767 loss: 7.57293662e-07
Iter: 1768 loss: 7.56776217e-07
Iter: 1769 loss: 7.65112304e-07
Iter: 1770 loss: 7.56774057e-07
Iter: 1771 loss: 7.56379393e-07
Iter: 1772 loss: 7.56265763e-07
Iter: 1773 loss: 7.56026566e-07
Iter: 1774 loss: 7.55472627e-07
Iter: 1775 loss: 7.57674627e-07
Iter: 1776 loss: 7.55352971e-07
Iter: 1777 loss: 7.54992755e-07
Iter: 1778 loss: 7.57321459e-07
Iter: 1779 loss: 7.54939606e-07
Iter: 1780 loss: 7.54595931e-07
Iter: 1781 loss: 7.54863606e-07
Iter: 1782 loss: 7.54373048e-07
Iter: 1783 loss: 7.54013513e-07
Iter: 1784 loss: 7.55676922e-07
Iter: 1785 loss: 7.53965082e-07
Iter: 1786 loss: 7.53487768e-07
Iter: 1787 loss: 7.53806603e-07
Iter: 1788 loss: 7.53208155e-07
Iter: 1789 loss: 7.52813492e-07
Iter: 1790 loss: 7.5230696e-07
Iter: 1791 loss: 7.52284336e-07
Iter: 1792 loss: 7.51967718e-07
Iter: 1793 loss: 7.51879611e-07
Iter: 1794 loss: 7.51560492e-07
Iter: 1795 loss: 7.50904519e-07
Iter: 1796 loss: 7.61976708e-07
Iter: 1797 loss: 7.50903268e-07
Iter: 1798 loss: 7.50351887e-07
Iter: 1799 loss: 7.51945095e-07
Iter: 1800 loss: 7.50144522e-07
Iter: 1801 loss: 7.49613719e-07
Iter: 1802 loss: 7.49981155e-07
Iter: 1803 loss: 7.49268906e-07
Iter: 1804 loss: 7.49125e-07
Iter: 1805 loss: 7.4894848e-07
Iter: 1806 loss: 7.48695641e-07
Iter: 1807 loss: 7.48501407e-07
Iter: 1808 loss: 7.48413527e-07
Iter: 1809 loss: 7.47987826e-07
Iter: 1810 loss: 7.4885628e-07
Iter: 1811 loss: 7.47856518e-07
Iter: 1812 loss: 7.47416095e-07
Iter: 1813 loss: 7.49516175e-07
Iter: 1814 loss: 7.47308206e-07
Iter: 1815 loss: 7.4696294e-07
Iter: 1816 loss: 7.46729597e-07
Iter: 1817 loss: 7.46605394e-07
Iter: 1818 loss: 7.46235e-07
Iter: 1819 loss: 7.4620641e-07
Iter: 1820 loss: 7.46018429e-07
Iter: 1821 loss: 7.45590512e-07
Iter: 1822 loss: 7.51563107e-07
Iter: 1823 loss: 7.45540262e-07
Iter: 1824 loss: 7.45107741e-07
Iter: 1825 loss: 7.49677895e-07
Iter: 1826 loss: 7.45114733e-07
Iter: 1827 loss: 7.44760882e-07
Iter: 1828 loss: 7.45502859e-07
Iter: 1829 loss: 7.44586373e-07
Iter: 1830 loss: 7.44177896e-07
Iter: 1831 loss: 7.44492695e-07
Iter: 1832 loss: 7.43917951e-07
Iter: 1833 loss: 7.43441831e-07
Iter: 1834 loss: 7.43054784e-07
Iter: 1835 loss: 7.42927568e-07
Iter: 1836 loss: 7.42213615e-07
Iter: 1837 loss: 7.4561666e-07
Iter: 1838 loss: 7.42083e-07
Iter: 1839 loss: 7.41717713e-07
Iter: 1840 loss: 7.41699694e-07
Iter: 1841 loss: 7.41350107e-07
Iter: 1842 loss: 7.41760061e-07
Iter: 1843 loss: 7.41186909e-07
Iter: 1844 loss: 7.40954931e-07
Iter: 1845 loss: 7.41416557e-07
Iter: 1846 loss: 7.40862902e-07
Iter: 1847 loss: 7.40547137e-07
Iter: 1848 loss: 7.41349197e-07
Iter: 1849 loss: 7.40441e-07
Iter: 1850 loss: 7.40132634e-07
Iter: 1851 loss: 7.40077212e-07
Iter: 1852 loss: 7.39854556e-07
Iter: 1853 loss: 7.39496727e-07
Iter: 1854 loss: 7.39482516e-07
Iter: 1855 loss: 7.39258439e-07
Iter: 1856 loss: 7.38710696e-07
Iter: 1857 loss: 7.46128194e-07
Iter: 1858 loss: 7.3867875e-07
Iter: 1859 loss: 7.38297842e-07
Iter: 1860 loss: 7.38285735e-07
Iter: 1861 loss: 7.37910341e-07
Iter: 1862 loss: 7.38411416e-07
Iter: 1863 loss: 7.37730716e-07
Iter: 1864 loss: 7.37461278e-07
Iter: 1865 loss: 7.38580411e-07
Iter: 1866 loss: 7.37379e-07
Iter: 1867 loss: 7.37102312e-07
Iter: 1868 loss: 7.36973391e-07
Iter: 1869 loss: 7.36831e-07
Iter: 1870 loss: 7.36413199e-07
Iter: 1871 loss: 7.36725042e-07
Iter: 1872 loss: 7.3613171e-07
Iter: 1873 loss: 7.35678e-07
Iter: 1874 loss: 7.37622713e-07
Iter: 1875 loss: 7.35566516e-07
Iter: 1876 loss: 7.35010417e-07
Iter: 1877 loss: 7.39995e-07
Iter: 1878 loss: 7.35002e-07
Iter: 1879 loss: 7.34714604e-07
Iter: 1880 loss: 7.34188575e-07
Iter: 1881 loss: 7.44220699e-07
Iter: 1882 loss: 7.34183743e-07
Iter: 1883 loss: 7.33895263e-07
Iter: 1884 loss: 7.33829893e-07
Iter: 1885 loss: 7.33605e-07
Iter: 1886 loss: 7.33581601e-07
Iter: 1887 loss: 7.33391e-07
Iter: 1888 loss: 7.33145839e-07
Iter: 1889 loss: 7.33139359e-07
Iter: 1890 loss: 7.32952515e-07
Iter: 1891 loss: 7.32533067e-07
Iter: 1892 loss: 7.37424557e-07
Iter: 1893 loss: 7.32476963e-07
Iter: 1894 loss: 7.32010392e-07
Iter: 1895 loss: 7.34031971e-07
Iter: 1896 loss: 7.31929504e-07
Iter: 1897 loss: 7.31449632e-07
Iter: 1898 loss: 7.35960953e-07
Iter: 1899 loss: 7.31401656e-07
Iter: 1900 loss: 7.3113614e-07
Iter: 1901 loss: 7.30784109e-07
Iter: 1902 loss: 7.30753754e-07
Iter: 1903 loss: 7.3021215e-07
Iter: 1904 loss: 7.32923695e-07
Iter: 1905 loss: 7.30134047e-07
Iter: 1906 loss: 7.29724718e-07
Iter: 1907 loss: 7.30380521e-07
Iter: 1908 loss: 7.29537874e-07
Iter: 1909 loss: 7.29118085e-07
Iter: 1910 loss: 7.29428052e-07
Iter: 1911 loss: 7.28870873e-07
Iter: 1912 loss: 7.28622695e-07
Iter: 1913 loss: 7.28589725e-07
Iter: 1914 loss: 7.28338762e-07
Iter: 1915 loss: 7.28303576e-07
Iter: 1916 loss: 7.28104624e-07
Iter: 1917 loss: 7.27796078e-07
Iter: 1918 loss: 7.27175461e-07
Iter: 1919 loss: 7.40926112e-07
Iter: 1920 loss: 7.27174324e-07
Iter: 1921 loss: 7.26833264e-07
Iter: 1922 loss: 7.26784151e-07
Iter: 1923 loss: 7.26398525e-07
Iter: 1924 loss: 7.27555346e-07
Iter: 1925 loss: 7.26239819e-07
Iter: 1926 loss: 7.2596174e-07
Iter: 1927 loss: 7.25774953e-07
Iter: 1928 loss: 7.25642622e-07
Iter: 1929 loss: 7.25385917e-07
Iter: 1930 loss: 7.25321172e-07
Iter: 1931 loss: 7.25169571e-07
Iter: 1932 loss: 7.24824645e-07
Iter: 1933 loss: 7.32046715e-07
Iter: 1934 loss: 7.24803385e-07
Iter: 1935 loss: 7.24586926e-07
Iter: 1936 loss: 7.2457965e-07
Iter: 1937 loss: 7.2433e-07
Iter: 1938 loss: 7.23896505e-07
Iter: 1939 loss: 7.23891958e-07
Iter: 1940 loss: 7.23429594e-07
Iter: 1941 loss: 7.25010182e-07
Iter: 1942 loss: 7.23302492e-07
Iter: 1943 loss: 7.22830805e-07
Iter: 1944 loss: 7.23430048e-07
Iter: 1945 loss: 7.22570121e-07
Iter: 1946 loss: 7.2210355e-07
Iter: 1947 loss: 7.232548e-07
Iter: 1948 loss: 7.21936658e-07
Iter: 1949 loss: 7.21740491e-07
Iter: 1950 loss: 7.21705248e-07
Iter: 1951 loss: 7.21506353e-07
Iter: 1952 loss: 7.21093102e-07
Iter: 1953 loss: 7.29219096e-07
Iter: 1954 loss: 7.21107824e-07
Iter: 1955 loss: 7.207301e-07
Iter: 1956 loss: 7.21479864e-07
Iter: 1957 loss: 7.20606067e-07
Iter: 1958 loss: 7.20260459e-07
Iter: 1959 loss: 7.24562653e-07
Iter: 1960 loss: 7.20255912e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6
+ date
Wed Oct 21 10:54:04 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1 --function f1 --psi -2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4a2d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4ab8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4ab8b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc497e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc497e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4a360d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc49e7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48fb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48fb2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc49468c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc494f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48de730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48dee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4868510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4858840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48ab6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48ab510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc15f5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc15a8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc15c2f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc47ff840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc47fff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc47b7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14d0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14d0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14d02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc1497950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14ac598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14ac488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14486a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc1488f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc13a01e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc1398620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc13989d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc1517268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc13cf8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.024691213
test_loss: 0.0237323
train_loss: 0.010842392
test_loss: 0.011419016
train_loss: 0.007116771
test_loss: 0.007922669
train_loss: 0.0064131664
test_loss: 0.006636905
train_loss: 0.005281979
test_loss: 0.006147934
train_loss: 0.0047164983
test_loss: 0.0057614245
train_loss: 0.004674913
test_loss: 0.005322989
train_loss: 0.0045824
test_loss: 0.005414203
train_loss: 0.0041051935
test_loss: 0.0052017383
train_loss: 0.004320536
test_loss: 0.005181114
train_loss: 0.004142129
test_loss: 0.0050392495
train_loss: 0.004244495
test_loss: 0.0049497406
train_loss: 0.0042857896
test_loss: 0.0050960793
train_loss: 0.0042080595
test_loss: 0.0051698475
train_loss: 0.0043510217
test_loss: 0.0050986395
train_loss: 0.004027813
test_loss: 0.0049993377
train_loss: 0.0038732968
test_loss: 0.004940592
train_loss: 0.0040004146
test_loss: 0.0049524987
train_loss: 0.0039799362
test_loss: 0.005197263
train_loss: 0.004079961
test_loss: 0.0051902323
train_loss: 0.0038258857
test_loss: 0.0048476877
train_loss: 0.0037381086
test_loss: 0.0051992675
train_loss: 0.0039823186
test_loss: 0.0051554856
train_loss: 0.0038241805
test_loss: 0.004694985
train_loss: 0.004007274
test_loss: 0.004820642
train_loss: 0.0039051543
test_loss: 0.0050420915
train_loss: 0.00395396
test_loss: 0.004856654
train_loss: 0.0035287442
test_loss: 0.004927245
train_loss: 0.00350927
test_loss: 0.0046581747
train_loss: 0.00418158
test_loss: 0.005022465
train_loss: 0.0038871085
test_loss: 0.004700234
train_loss: 0.0038000366
test_loss: 0.004685028
train_loss: 0.003844186
test_loss: 0.0046972265
train_loss: 0.0038138689
test_loss: 0.004952
train_loss: 0.0035941121
test_loss: 0.0049776454
train_loss: 0.003615374
test_loss: 0.004712863
train_loss: 0.0039812727
test_loss: 0.0047391746
train_loss: 0.0037626356
test_loss: 0.0047305925
train_loss: 0.003762607
test_loss: 0.004675068
train_loss: 0.0037230016
test_loss: 0.004727916
train_loss: 0.003724188
test_loss: 0.0046702963
train_loss: 0.0039138356
test_loss: 0.0050081788
train_loss: 0.0036303664
test_loss: 0.004895965
train_loss: 0.003679594
test_loss: 0.0047841244
train_loss: 0.0034801103
test_loss: 0.004692602
train_loss: 0.0034944648
test_loss: 0.004569122
train_loss: 0.0039518345
test_loss: 0.004640182
train_loss: 0.003487298
test_loss: 0.004734398
train_loss: 0.004106274
test_loss: 0.005033981
train_loss: 0.003713897
test_loss: 0.004684965
train_loss: 0.0037537552
test_loss: 0.004660607
train_loss: 0.0034680252
test_loss: 0.004712301
train_loss: 0.0035195854
test_loss: 0.0046598194
train_loss: 0.00362779
test_loss: 0.0048129107
train_loss: 0.003804081
test_loss: 0.004764039
train_loss: 0.0036958647
test_loss: 0.0045626042
train_loss: 0.003851566
test_loss: 0.0048199533
train_loss: 0.0036141388
test_loss: 0.0045647663
train_loss: 0.0033203545
test_loss: 0.0046149786
train_loss: 0.0035452119
test_loss: 0.0047885654
train_loss: 0.0036022621
test_loss: 0.0045928126
train_loss: 0.0037102306
test_loss: 0.0045405896
train_loss: 0.0037369826
test_loss: 0.004641317
train_loss: 0.0034051647
test_loss: 0.004583089
train_loss: 0.0035641866
test_loss: 0.0045976713
train_loss: 0.0034342837
test_loss: 0.0046813004
train_loss: 0.0035445197
test_loss: 0.004801152
train_loss: 0.0035137886
test_loss: 0.0047444855
train_loss: 0.003620115
test_loss: 0.0046300823
train_loss: 0.0036195056
test_loss: 0.0047449097
train_loss: 0.0037288738
test_loss: 0.004566075
train_loss: 0.0035948516
test_loss: 0.004673381
train_loss: 0.0037011078
test_loss: 0.0047536506
train_loss: 0.003648867
test_loss: 0.0045618773
train_loss: 0.0034338206
test_loss: 0.004595958
train_loss: 0.0033270912
test_loss: 0.0046496405
train_loss: 0.0036668035
test_loss: 0.0044393134
train_loss: 0.003586249
test_loss: 0.004757923
train_loss: 0.0035798892
test_loss: 0.0044870046
train_loss: 0.0037309397
test_loss: 0.004747658
train_loss: 0.003694287
test_loss: 0.004675271
train_loss: 0.00357808
test_loss: 0.004722557
train_loss: 0.004040352
test_loss: 0.0046410384
train_loss: 0.0035997597
test_loss: 0.0045483923
train_loss: 0.003485582
test_loss: 0.004665477
train_loss: 0.003309299
test_loss: 0.0045024767
train_loss: 0.0036919415
test_loss: 0.0046965927
train_loss: 0.0034668879
test_loss: 0.004543792
train_loss: 0.003545037
test_loss: 0.004533045
train_loss: 0.0037685272
test_loss: 0.004732998
train_loss: 0.0037402278
test_loss: 0.004585319
train_loss: 0.0041473396
test_loss: 0.004862735
train_loss: 0.0037072808
test_loss: 0.004615098
train_loss: 0.0036835764
test_loss: 0.0046203835
train_loss: 0.0033572377
test_loss: 0.004481441
train_loss: 0.0035815062
test_loss: 0.004567552
train_loss: 0.0037262528
test_loss: 0.004621657
train_loss: 0.003776738
test_loss: 0.004882351
train_loss: 0.0035449623
test_loss: 0.004597182
train_loss: 0.0034089475
test_loss: 0.004543895
train_loss: 0.0032098396
test_loss: 0.0042779325
train_loss: 0.0033442576
test_loss: 0.0045264564
train_loss: 0.003332823
test_loss: 0.0046022004
train_loss: 0.0030668227
test_loss: 0.0043513263
train_loss: 0.0038594585
test_loss: 0.0044974703
train_loss: 0.0044911033
test_loss: 0.005063654
train_loss: 0.0034529855
test_loss: 0.0047582258
train_loss: 0.0034862724
test_loss: 0.004475605
train_loss: 0.003520229
test_loss: 0.0045257397
train_loss: 0.0034551537
test_loss: 0.0044991365
train_loss: 0.0034477445
test_loss: 0.0044286423
train_loss: 0.0037994375
test_loss: 0.004737329
train_loss: 0.0036487337
test_loss: 0.0046095243
train_loss: 0.003500712
test_loss: 0.004721702
train_loss: 0.0033969027
test_loss: 0.004482158
train_loss: 0.003571251
test_loss: 0.0045729843
train_loss: 0.003546694
test_loss: 0.0044388846
train_loss: 0.0032294015
test_loss: 0.0045479117
train_loss: 0.003600503
test_loss: 0.0046392516
train_loss: 0.0037136697
test_loss: 0.0046531297
train_loss: 0.0031953603
test_loss: 0.004419859
train_loss: 0.0035659834
test_loss: 0.004576433
train_loss: 0.0034946767
test_loss: 0.0045767007
train_loss: 0.0034131955
test_loss: 0.0044750194
train_loss: 0.0034713657
test_loss: 0.004516781
train_loss: 0.003486358
test_loss: 0.0044710836
train_loss: 0.003476811
test_loss: 0.0045201695
train_loss: 0.0033427551
test_loss: 0.004725129
train_loss: 0.0034218347
test_loss: 0.0045927796
train_loss: 0.0037656561
test_loss: 0.0046469034
train_loss: 0.0034300836
test_loss: 0.0047430396
train_loss: 0.00374057
test_loss: 0.004963045
train_loss: 0.0035538687
test_loss: 0.004936328
train_loss: 0.0033708422
test_loss: 0.0044534416
train_loss: 0.0033620833
test_loss: 0.004463273
train_loss: 0.0031597614
test_loss: 0.0044799014
train_loss: 0.0034665735
test_loss: 0.004650817
train_loss: 0.0033823787
test_loss: 0.004686303
train_loss: 0.0030904058
test_loss: 0.0044926507
train_loss: 0.003257215
test_loss: 0.004431937
train_loss: 0.003704973
test_loss: 0.004363583
train_loss: 0.0033866125
test_loss: 0.004856785
train_loss: 0.0037239569
test_loss: 0.0047267633
train_loss: 0.0035857032
test_loss: 0.004709822
train_loss: 0.0034022275
test_loss: 0.0045679267
train_loss: 0.0036118417
test_loss: 0.004500926
train_loss: 0.0034407575
test_loss: 0.004474888
train_loss: 0.0033970731
test_loss: 0.0045028604
train_loss: 0.0035017228
test_loss: 0.0044638696
train_loss: 0.0036691641
test_loss: 0.004441474
train_loss: 0.0037063167
test_loss: 0.0047729043
train_loss: 0.00347918
test_loss: 0.0043853167
train_loss: 0.0037614824
test_loss: 0.0045680236
train_loss: 0.0031794915
test_loss: 0.0043530636
train_loss: 0.0035371587
test_loss: 0.0043390486
train_loss: 0.003328125
test_loss: 0.004464878
train_loss: 0.0036568306
test_loss: 0.004563007
train_loss: 0.0035900294
test_loss: 0.0044642
train_loss: 0.0032377755
test_loss: 0.0043934872
train_loss: 0.0034197825
test_loss: 0.004508509
train_loss: 0.003368649
test_loss: 0.0045834044
train_loss: 0.003434019
test_loss: 0.004531462
train_loss: 0.0033439202
test_loss: 0.0043636747
train_loss: 0.003439004
test_loss: 0.004546827
train_loss: 0.0035141464
test_loss: 0.004440849
train_loss: 0.0035718041
test_loss: 0.0046576257
train_loss: 0.0033899602
test_loss: 0.0044634305
train_loss: 0.0033793459
test_loss: 0.0045467657
train_loss: 0.0033908514
test_loss: 0.0044389884
train_loss: 0.0031990306
test_loss: 0.0043796278
train_loss: 0.0037099319
test_loss: 0.0045250338
train_loss: 0.0032523228
test_loss: 0.0045351842
train_loss: 0.0032755989
test_loss: 0.0045580715
train_loss: 0.0036443886
test_loss: 0.004693937
train_loss: 0.0034042904
test_loss: 0.0045825047
train_loss: 0.0033670552
test_loss: 0.0044667223
train_loss: 0.0033369423
test_loss: 0.004511112
train_loss: 0.003602397
test_loss: 0.0044807475
train_loss: 0.0032811828
test_loss: 0.004514674
train_loss: 0.00334813
test_loss: 0.0043689488
train_loss: 0.003381327
test_loss: 0.004515013
train_loss: 0.003414614
test_loss: 0.004354102
train_loss: 0.003151475
test_loss: 0.004437218
train_loss: 0.0034188707
test_loss: 0.0045097107
train_loss: 0.003377698
test_loss: 0.0045504225
train_loss: 0.003517579
test_loss: 0.0045546093
train_loss: 0.0032312565
test_loss: 0.0043605026
train_loss: 0.0032417122
test_loss: 0.0042808326
train_loss: 0.0032011836
test_loss: 0.004271913
train_loss: 0.0032397276
test_loss: 0.004363818
train_loss: 0.0033318854
test_loss: 0.004442145
train_loss: 0.0032485146
test_loss: 0.004560732
train_loss: 0.0032873575
test_loss: 0.0046256706
train_loss: 0.003168401
test_loss: 0.004414708
train_loss: 0.0035388963
test_loss: 0.0043489733
train_loss: 0.0032866579
test_loss: 0.0045303195
train_loss: 0.0033779806
test_loss: 0.00448279
train_loss: 0.0033298742
test_loss: 0.0044584307
train_loss: 0.0035403608
test_loss: 0.004532972
train_loss: 0.0031599747
test_loss: 0.0043939794
train_loss: 0.0032386337
test_loss: 0.004452039
train_loss: 0.0029335984
test_loss: 0.0043169702
train_loss: 0.0032464436
test_loss: 0.004589615
train_loss: 0.0033866586
test_loss: 0.0046517793
train_loss: 0.0033338917
test_loss: 0.004441811
train_loss: 0.003523273
test_loss: 0.0044233175
train_loss: 0.0036729886
test_loss: 0.004412025
train_loss: 0.0034174793
test_loss: 0.004486136
train_loss: 0.003684173
test_loss: 0.004758312
train_loss: 0.0034236317
test_loss: 0.004430308
train_loss: 0.0030607684
test_loss: 0.0043634526
train_loss: 0.0033212383
test_loss: 0.0043636104
train_loss: 0.0029505142
test_loss: 0.0042684353
train_loss: 0.0035295049
test_loss: 0.004458062
train_loss: 0.0033404818
test_loss: 0.0044617453
train_loss: 0.0032464005
test_loss: 0.0043629208
train_loss: 0.0038662653
test_loss: 0.004457114
train_loss: 0.0034303372
test_loss: 0.0044924035
train_loss: 0.0031030457
test_loss: 0.004342556
train_loss: 0.0030249911
test_loss: 0.004328059
train_loss: 0.0031451632
test_loss: 0.00445109
train_loss: 0.003182645
test_loss: 0.004452281
train_loss: 0.0034402562
test_loss: 0.00449754
train_loss: 0.003314009
test_loss: 0.0044927457
train_loss: 0.0030280808
test_loss: 0.0043660123
train_loss: 0.0032170564
test_loss: 0.0043610893
train_loss: 0.0034488146
test_loss: 0.0044721887
train_loss: 0.0031806445
test_loss: 0.004431776
train_loss: 0.0036812655
test_loss: 0.004450867
train_loss: 0.0033457717
test_loss: 0.0044553126
train_loss: 0.003580818
test_loss: 0.004359486
train_loss: 0.003229715
test_loss: 0.004351953
train_loss: 0.0031011696
test_loss: 0.004388447
train_loss: 0.003259922
test_loss: 0.004274031
train_loss: 0.0031639389
test_loss: 0.004386456
train_loss: 0.0033826027
test_loss: 0.0046628215
train_loss: 0.0034596696
test_loss: 0.0043759057
train_loss: 0.0035228105
test_loss: 0.0047271117
train_loss: 0.0030963148
test_loss: 0.004585856
train_loss: 0.003180412
test_loss: 0.004460032
train_loss: 0.0031594275
test_loss: 0.004477412
train_loss: 0.003115518
test_loss: 0.0044995523
train_loss: 0.0033016976
test_loss: 0.004388013
train_loss: 0.003888272
test_loss: 0.0046409434
train_loss: 0.0035182806
test_loss: 0.004357657
train_loss: 0.003279976
test_loss: 0.0043359064
train_loss: 0.003398064
test_loss: 0.004757242
train_loss: 0.0031279863
test_loss: 0.004400679
train_loss: 0.0031288732
test_loss: 0.0043286704
train_loss: 0.0033937655
test_loss: 0.0044362475
train_loss: 0.003189955
test_loss: 0.0044850465
train_loss: 0.0030794996
test_loss: 0.0042056083
train_loss: 0.0036669336
test_loss: 0.0044125626
train_loss: 0.0032845526
test_loss: 0.0043911454
train_loss: 0.0034313106
test_loss: 0.004533366
train_loss: 0.0032321485
test_loss: 0.0044143936
train_loss: 0.0031686223
test_loss: 0.004323725
train_loss: 0.003029955
test_loss: 0.004335937
train_loss: 0.003309945
test_loss: 0.0043357145
train_loss: 0.003185107
test_loss: 0.004308714
train_loss: 0.0032712673
test_loss: 0.0044168537
train_loss: 0.0031734349
test_loss: 0.0043065916
train_loss: 0.0030139084
test_loss: 0.0042188615
train_loss: 0.0032204106
test_loss: 0.004456125
train_loss: 0.0032443882
test_loss: 0.004499923
train_loss: 0.0033230204
test_loss: 0.0044432417
train_loss: 0.003597279
test_loss: 0.004658228
train_loss: 0.0035615508
test_loss: 0.004280226
train_loss: 0.0032108729
test_loss: 0.004457251
train_loss: 0.003293795
test_loss: 0.0044929185
train_loss: 0.0034180488
test_loss: 0.0044009425
train_loss: 0.0031194056
test_loss: 0.0043622297
train_loss: 0.003173777
test_loss: 0.0043848283
train_loss: 0.0031567405
test_loss: 0.004299457
train_loss: 0.0030676292
test_loss: 0.0044937753
train_loss: 0.003108688
test_loss: 0.0045253304
train_loss: 0.0034779743
test_loss: 0.0045218305
train_loss: 0.003334722
test_loss: 0.0045210677
train_loss: 0.0031929535
test_loss: 0.0045980155
train_loss: 0.0032969334
test_loss: 0.00459212
train_loss: 0.0032489998
test_loss: 0.0047427476
train_loss: 0.0034404623
test_loss: 0.00444502
train_loss: 0.003380399
test_loss: 0.004445912
train_loss: 0.0030915826
test_loss: 0.0044275667
train_loss: 0.003262601
test_loss: 0.004415689
train_loss: 0.003049138
test_loss: 0.004456826
train_loss: 0.0030754884
test_loss: 0.004475861
train_loss: 0.0032177006
test_loss: 0.004405274
train_loss: 0.003374644
test_loss: 0.004519828
train_loss: 0.0029985963
test_loss: 0.004216505
train_loss: 0.0033433079
test_loss: 0.004373676
train_loss: 0.003111925
test_loss: 0.004420058
train_loss: 0.0031948544
test_loss: 0.0043108515
train_loss: 0.00312158
test_loss: 0.004215969
train_loss: 0.0030039032
test_loss: 0.004311111
train_loss: 0.0029668573
test_loss: 0.00430489
train_loss: 0.0029919643
test_loss: 0.0041931593
train_loss: 0.0032226965
test_loss: 0.0044636317
train_loss: 0.003284985
test_loss: 0.004408878
train_loss: 0.003293979
test_loss: 0.0043603564
train_loss: 0.0032465956
test_loss: 0.004475528
train_loss: 0.0031027254
test_loss: 0.0044361865
train_loss: 0.003199329
test_loss: 0.00434448
train_loss: 0.002985155
test_loss: 0.0043114317
train_loss: 0.003013107
test_loss: 0.004247547
train_loss: 0.0033270782
test_loss: 0.0045116786
train_loss: 0.0033935665
test_loss: 0.0044587683
train_loss: 0.0032178713
test_loss: 0.0043309904
train_loss: 0.002773102
test_loss: 0.0043133656
train_loss: 0.0030989968
test_loss: 0.0042394316
train_loss: 0.0031192906
test_loss: 0.0042474084
train_loss: 0.0032310057
test_loss: 0.004331489
train_loss: 0.003114086
test_loss: 0.004316708
train_loss: 0.0031034695
test_loss: 0.004409448
train_loss: 0.0031988183
test_loss: 0.0044332957
train_loss: 0.0032314444
test_loss: 0.0042081424
train_loss: 0.0032333392
test_loss: 0.004281706
train_loss: 0.0033002605
test_loss: 0.0045472626
train_loss: 0.0032377145
test_loss: 0.0043928437
train_loss: 0.0033585131
test_loss: 0.00451002
train_loss: 0.0031728386
test_loss: 0.004616834
train_loss: 0.0033251531
test_loss: 0.0044395425
train_loss: 0.0033931672
test_loss: 0.0044199964
train_loss: 0.0031607407
test_loss: 0.004336541
train_loss: 0.0031534368
test_loss: 0.004300084
train_loss: 0.0027983708
test_loss: 0.004162248
train_loss: 0.0030502675
test_loss: 0.004351177
train_loss: 0.0032296972
test_loss: 0.0044210544
train_loss: 0.0032946747
test_loss: 0.004387146
train_loss: 0.003315806
test_loss: 0.004334142
train_loss: 0.0034196062
test_loss: 0.004412837
train_loss: 0.0033473955
test_loss: 0.004404942
train_loss: 0.0034963456
test_loss: 0.004406935
train_loss: 0.0031945943
test_loss: 0.004294523
train_loss: 0.0032924968
test_loss: 0.0045336615
train_loss: 0.0035573153
test_loss: 0.0045792013
train_loss: 0.0031961189
test_loss: 0.0047098445
train_loss: 0.0031044031
test_loss: 0.0042223893
train_loss: 0.0031002406
test_loss: 0.004308727
train_loss: 0.0032134827
test_loss: 0.004297421
train_loss: 0.003192247
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.004220984
train_loss: 0.0032130983
test_loss: 0.0044713975
train_loss: 0.0031258203
test_loss: 0.004354408
train_loss: 0.0033856444
test_loss: 0.004315749
train_loss: 0.003667175
test_loss: 0.004385635
train_loss: 0.00335218
test_loss: 0.0044895713
train_loss: 0.003109778
test_loss: 0.0043935333
train_loss: 0.0030193536
test_loss: 0.004434574
train_loss: 0.003221001
test_loss: 0.004516915
train_loss: 0.0029960098
test_loss: 0.0042094174
train_loss: 0.003278064
test_loss: 0.004499007
train_loss: 0.003360292
test_loss: 0.0046540345
train_loss: 0.0032098615
test_loss: 0.004363082
train_loss: 0.003188868
test_loss: 0.0044643898
train_loss: 0.0030329013
test_loss: 0.0044411444
train_loss: 0.003287295
test_loss: 0.0044974443
train_loss: 0.0034579444
test_loss: 0.004306635
train_loss: 0.0032404228
test_loss: 0.004347677
train_loss: 0.0029187899
test_loss: 0.004346709
train_loss: 0.0031734537
test_loss: 0.004233609
train_loss: 0.0031982618
test_loss: 0.00434386
train_loss: 0.0032521244
test_loss: 0.0044713337
train_loss: 0.0031515583
test_loss: 0.0044054654
train_loss: 0.0030851113
test_loss: 0.0044103954
train_loss: 0.003237112
test_loss: 0.004272363
train_loss: 0.0030191333
test_loss: 0.0044060033
train_loss: 0.0033662484
test_loss: 0.0043959892
train_loss: 0.0032823412
test_loss: 0.0042223046
train_loss: 0.0033197086
test_loss: 0.004428048
train_loss: 0.0030213157
test_loss: 0.0042828196
train_loss: 0.0028332118
test_loss: 0.004234043
train_loss: 0.0030190148
test_loss: 0.004360548
train_loss: 0.0032050372
test_loss: 0.0043635233
train_loss: 0.0030885083
test_loss: 0.004407821
train_loss: 0.00306018
test_loss: 0.00427948
train_loss: 0.0030899309
test_loss: 0.004287477
train_loss: 0.002999032
test_loss: 0.0041901134
train_loss: 0.002943871
test_loss: 0.004337223
train_loss: 0.0032563517
test_loss: 0.0043231044
train_loss: 0.0032949324
test_loss: 0.004430643
train_loss: 0.003218709
test_loss: 0.0044485154
train_loss: 0.003178105
test_loss: 0.004460796
train_loss: 0.003218939
test_loss: 0.0043646125
train_loss: 0.0030770663
test_loss: 0.004374814
train_loss: 0.0035135213
test_loss: 0.0043635243
train_loss: 0.0031337421
test_loss: 0.0043894965
train_loss: 0.0030213739
test_loss: 0.0042543467
train_loss: 0.0028966474
test_loss: 0.004090215
train_loss: 0.0030702222
test_loss: 0.004393413
train_loss: 0.003385645
test_loss: 0.0044753025
train_loss: 0.0032099597
test_loss: 0.004235666
train_loss: 0.0030644117
test_loss: 0.0042904015
train_loss: 0.0031997936
test_loss: 0.004487652
train_loss: 0.0033540637
test_loss: 0.0046996763
train_loss: 0.0034000315
test_loss: 0.0043099113
train_loss: 0.0031942888
test_loss: 0.0045633027
train_loss: 0.0032157637
test_loss: 0.0042949896
train_loss: 0.0030167229
test_loss: 0.004215023
train_loss: 0.003238359
test_loss: 0.0042347335
train_loss: 0.0033149645
test_loss: 0.0043318234
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a57e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a644158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a644268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a5da1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a5dae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a53ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a52f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a4dc840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a4dc400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a4dc510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a47a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a454c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a4642f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a464d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a3b67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a3fec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a3e9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a3b1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a3699d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a35df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a30a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a2c2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a2e1ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a29d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a28d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a28d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f24808609d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2480876620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2480876158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248081c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f24807cbae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f24807f6268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248079e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f24807b3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2480766598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2480766f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.11559127e-05
Iter: 2 loss: 1.69474697e-05
Iter: 3 loss: 1.691133e-05
Iter: 4 loss: 1.56433671e-05
Iter: 5 loss: 3.11243421e-05
Iter: 6 loss: 1.56291208e-05
Iter: 7 loss: 1.49807529e-05
Iter: 8 loss: 1.43349716e-05
Iter: 9 loss: 1.42010213e-05
Iter: 10 loss: 1.30425178e-05
Iter: 11 loss: 1.74206343e-05
Iter: 12 loss: 1.27623352e-05
Iter: 13 loss: 1.16931651e-05
Iter: 14 loss: 1.38203459e-05
Iter: 15 loss: 1.12562939e-05
Iter: 16 loss: 1.0315799e-05
Iter: 17 loss: 1.3597195e-05
Iter: 18 loss: 1.00699162e-05
Iter: 19 loss: 9.28332338e-06
Iter: 20 loss: 1.26214291e-05
Iter: 21 loss: 9.11604366e-06
Iter: 22 loss: 8.666555e-06
Iter: 23 loss: 8.35221272e-06
Iter: 24 loss: 8.192118e-06
Iter: 25 loss: 7.94567222e-06
Iter: 26 loss: 7.89119713e-06
Iter: 27 loss: 7.61131378e-06
Iter: 28 loss: 7.73434749e-06
Iter: 29 loss: 7.42058273e-06
Iter: 30 loss: 7.04366039e-06
Iter: 31 loss: 6.73169188e-06
Iter: 32 loss: 6.62310458e-06
Iter: 33 loss: 6.1686942e-06
Iter: 34 loss: 7.91051934e-06
Iter: 35 loss: 6.06141612e-06
Iter: 36 loss: 5.66041308e-06
Iter: 37 loss: 6.85885e-06
Iter: 38 loss: 5.53900418e-06
Iter: 39 loss: 5.43644364e-06
Iter: 40 loss: 5.3738986e-06
Iter: 41 loss: 5.20881076e-06
Iter: 42 loss: 5.46212232e-06
Iter: 43 loss: 5.13060104e-06
Iter: 44 loss: 5.00758233e-06
Iter: 45 loss: 4.95069071e-06
Iter: 46 loss: 4.89015792e-06
Iter: 47 loss: 4.7399e-06
Iter: 48 loss: 4.7398571e-06
Iter: 49 loss: 4.65166431e-06
Iter: 50 loss: 4.48943956e-06
Iter: 51 loss: 8.23426581e-06
Iter: 52 loss: 4.48912442e-06
Iter: 53 loss: 4.33726564e-06
Iter: 54 loss: 6.51709206e-06
Iter: 55 loss: 4.33696869e-06
Iter: 56 loss: 4.2239817e-06
Iter: 57 loss: 4.1603239e-06
Iter: 58 loss: 4.11116616e-06
Iter: 59 loss: 3.95849838e-06
Iter: 60 loss: 5.25052747e-06
Iter: 61 loss: 3.94950439e-06
Iter: 62 loss: 3.85799149e-06
Iter: 63 loss: 4.0302084e-06
Iter: 64 loss: 3.81932477e-06
Iter: 65 loss: 3.70257749e-06
Iter: 66 loss: 4.26825272e-06
Iter: 67 loss: 3.68195583e-06
Iter: 68 loss: 3.62575952e-06
Iter: 69 loss: 3.6352435e-06
Iter: 70 loss: 3.58353964e-06
Iter: 71 loss: 3.50114738e-06
Iter: 72 loss: 3.47552714e-06
Iter: 73 loss: 3.42698559e-06
Iter: 74 loss: 3.32642685e-06
Iter: 75 loss: 3.84932e-06
Iter: 76 loss: 3.31057663e-06
Iter: 77 loss: 3.2543951e-06
Iter: 78 loss: 3.25357837e-06
Iter: 79 loss: 3.19209016e-06
Iter: 80 loss: 3.29675117e-06
Iter: 81 loss: 3.16451064e-06
Iter: 82 loss: 3.12444035e-06
Iter: 83 loss: 3.05814319e-06
Iter: 84 loss: 3.05792628e-06
Iter: 85 loss: 3.00068723e-06
Iter: 86 loss: 2.99687463e-06
Iter: 87 loss: 2.96517032e-06
Iter: 88 loss: 2.90748449e-06
Iter: 89 loss: 4.26891256e-06
Iter: 90 loss: 2.90745334e-06
Iter: 91 loss: 2.8559748e-06
Iter: 92 loss: 3.49023367e-06
Iter: 93 loss: 2.85556871e-06
Iter: 94 loss: 2.81958273e-06
Iter: 95 loss: 2.91766264e-06
Iter: 96 loss: 2.80779159e-06
Iter: 97 loss: 2.77191907e-06
Iter: 98 loss: 2.78083235e-06
Iter: 99 loss: 2.74578497e-06
Iter: 100 loss: 2.7030178e-06
Iter: 101 loss: 3.06072457e-06
Iter: 102 loss: 2.70054488e-06
Iter: 103 loss: 2.67013888e-06
Iter: 104 loss: 2.78739526e-06
Iter: 105 loss: 2.6629823e-06
Iter: 106 loss: 2.63124207e-06
Iter: 107 loss: 2.62661069e-06
Iter: 108 loss: 2.60444062e-06
Iter: 109 loss: 2.56681869e-06
Iter: 110 loss: 2.5899426e-06
Iter: 111 loss: 2.54269025e-06
Iter: 112 loss: 2.50173116e-06
Iter: 113 loss: 2.64714072e-06
Iter: 114 loss: 2.49119034e-06
Iter: 115 loss: 2.47782555e-06
Iter: 116 loss: 2.46878926e-06
Iter: 117 loss: 2.45446313e-06
Iter: 118 loss: 2.42370174e-06
Iter: 119 loss: 2.90246771e-06
Iter: 120 loss: 2.42255874e-06
Iter: 121 loss: 2.39636347e-06
Iter: 122 loss: 2.48924312e-06
Iter: 123 loss: 2.38968573e-06
Iter: 124 loss: 2.36673759e-06
Iter: 125 loss: 2.71092927e-06
Iter: 126 loss: 2.36674168e-06
Iter: 127 loss: 2.35413881e-06
Iter: 128 loss: 2.32188859e-06
Iter: 129 loss: 2.60230627e-06
Iter: 130 loss: 2.31672493e-06
Iter: 131 loss: 2.28430576e-06
Iter: 132 loss: 2.63987386e-06
Iter: 133 loss: 2.28353474e-06
Iter: 134 loss: 2.25690974e-06
Iter: 135 loss: 2.40669488e-06
Iter: 136 loss: 2.25327403e-06
Iter: 137 loss: 2.23353118e-06
Iter: 138 loss: 2.25788654e-06
Iter: 139 loss: 2.22318135e-06
Iter: 140 loss: 2.20804873e-06
Iter: 141 loss: 2.35375e-06
Iter: 142 loss: 2.20752327e-06
Iter: 143 loss: 2.19102913e-06
Iter: 144 loss: 2.17194906e-06
Iter: 145 loss: 2.16975059e-06
Iter: 146 loss: 2.15348336e-06
Iter: 147 loss: 2.16104377e-06
Iter: 148 loss: 2.14247939e-06
Iter: 149 loss: 2.11858764e-06
Iter: 150 loss: 2.2643419e-06
Iter: 151 loss: 2.1157739e-06
Iter: 152 loss: 2.10544454e-06
Iter: 153 loss: 2.10350163e-06
Iter: 154 loss: 2.09586096e-06
Iter: 155 loss: 2.07589437e-06
Iter: 156 loss: 2.23205393e-06
Iter: 157 loss: 2.07205312e-06
Iter: 158 loss: 2.05272272e-06
Iter: 159 loss: 2.28657041e-06
Iter: 160 loss: 2.05252081e-06
Iter: 161 loss: 2.04208504e-06
Iter: 162 loss: 2.10408825e-06
Iter: 163 loss: 2.04073808e-06
Iter: 164 loss: 2.03002355e-06
Iter: 165 loss: 2.04432263e-06
Iter: 166 loss: 2.02454248e-06
Iter: 167 loss: 2.01545731e-06
Iter: 168 loss: 2.00014324e-06
Iter: 169 loss: 2.00017189e-06
Iter: 170 loss: 1.98069301e-06
Iter: 171 loss: 2.10068242e-06
Iter: 172 loss: 1.97831537e-06
Iter: 173 loss: 1.9588656e-06
Iter: 174 loss: 2.09116888e-06
Iter: 175 loss: 1.95702933e-06
Iter: 176 loss: 1.94688073e-06
Iter: 177 loss: 1.95068719e-06
Iter: 178 loss: 1.93973574e-06
Iter: 179 loss: 1.92479865e-06
Iter: 180 loss: 2.01936291e-06
Iter: 181 loss: 1.92309335e-06
Iter: 182 loss: 1.91264303e-06
Iter: 183 loss: 1.91543768e-06
Iter: 184 loss: 1.90515027e-06
Iter: 185 loss: 1.8946165e-06
Iter: 186 loss: 1.88297e-06
Iter: 187 loss: 1.88137199e-06
Iter: 188 loss: 1.88770559e-06
Iter: 189 loss: 1.87387536e-06
Iter: 190 loss: 1.86858256e-06
Iter: 191 loss: 1.86004991e-06
Iter: 192 loss: 1.85997612e-06
Iter: 193 loss: 1.8511837e-06
Iter: 194 loss: 1.84887676e-06
Iter: 195 loss: 1.84337296e-06
Iter: 196 loss: 1.83651696e-06
Iter: 197 loss: 1.83631403e-06
Iter: 198 loss: 1.82821543e-06
Iter: 199 loss: 1.81718372e-06
Iter: 200 loss: 1.81666485e-06
Iter: 201 loss: 1.80442748e-06
Iter: 202 loss: 1.812101e-06
Iter: 203 loss: 1.79664914e-06
Iter: 204 loss: 1.78264224e-06
Iter: 205 loss: 1.86848e-06
Iter: 206 loss: 1.78093512e-06
Iter: 207 loss: 1.7719974e-06
Iter: 208 loss: 1.90930291e-06
Iter: 209 loss: 1.77200843e-06
Iter: 210 loss: 1.76716924e-06
Iter: 211 loss: 1.76656533e-06
Iter: 212 loss: 1.76302672e-06
Iter: 213 loss: 1.75320599e-06
Iter: 214 loss: 1.75422383e-06
Iter: 215 loss: 1.74561865e-06
Iter: 216 loss: 1.73564649e-06
Iter: 217 loss: 1.86715988e-06
Iter: 218 loss: 1.73560829e-06
Iter: 219 loss: 1.72950831e-06
Iter: 220 loss: 1.72218802e-06
Iter: 221 loss: 1.72143768e-06
Iter: 222 loss: 1.71195245e-06
Iter: 223 loss: 1.77644449e-06
Iter: 224 loss: 1.71104932e-06
Iter: 225 loss: 1.70325143e-06
Iter: 226 loss: 1.8072426e-06
Iter: 227 loss: 1.70320516e-06
Iter: 228 loss: 1.69965278e-06
Iter: 229 loss: 1.69126463e-06
Iter: 230 loss: 1.78797814e-06
Iter: 231 loss: 1.69049213e-06
Iter: 232 loss: 1.68068266e-06
Iter: 233 loss: 1.69305474e-06
Iter: 234 loss: 1.67563985e-06
Iter: 235 loss: 1.67252017e-06
Iter: 236 loss: 1.66972075e-06
Iter: 237 loss: 1.66595282e-06
Iter: 238 loss: 1.65915071e-06
Iter: 239 loss: 1.65914912e-06
Iter: 240 loss: 1.65152699e-06
Iter: 241 loss: 1.65289578e-06
Iter: 242 loss: 1.64582048e-06
Iter: 243 loss: 1.63545042e-06
Iter: 244 loss: 1.69460441e-06
Iter: 245 loss: 1.63395805e-06
Iter: 246 loss: 1.62801484e-06
Iter: 247 loss: 1.62797369e-06
Iter: 248 loss: 1.62366598e-06
Iter: 249 loss: 1.61622268e-06
Iter: 250 loss: 1.61626906e-06
Iter: 251 loss: 1.61037769e-06
Iter: 252 loss: 1.61020375e-06
Iter: 253 loss: 1.60630611e-06
Iter: 254 loss: 1.60134732e-06
Iter: 255 loss: 1.60092895e-06
Iter: 256 loss: 1.59451724e-06
Iter: 257 loss: 1.63238587e-06
Iter: 258 loss: 1.59370416e-06
Iter: 259 loss: 1.58799264e-06
Iter: 260 loss: 1.65205927e-06
Iter: 261 loss: 1.58788885e-06
Iter: 262 loss: 1.58465605e-06
Iter: 263 loss: 1.57817715e-06
Iter: 264 loss: 1.69902864e-06
Iter: 265 loss: 1.57810882e-06
Iter: 266 loss: 1.57200066e-06
Iter: 267 loss: 1.61877892e-06
Iter: 268 loss: 1.57153852e-06
Iter: 269 loss: 1.56642466e-06
Iter: 270 loss: 1.56234569e-06
Iter: 271 loss: 1.56082069e-06
Iter: 272 loss: 1.55901546e-06
Iter: 273 loss: 1.55667249e-06
Iter: 274 loss: 1.55476073e-06
Iter: 275 loss: 1.54928625e-06
Iter: 276 loss: 1.57422301e-06
Iter: 277 loss: 1.54730162e-06
Iter: 278 loss: 1.53860333e-06
Iter: 279 loss: 1.57983777e-06
Iter: 280 loss: 1.53709084e-06
Iter: 281 loss: 1.53135022e-06
Iter: 282 loss: 1.52891789e-06
Iter: 283 loss: 1.52591042e-06
Iter: 284 loss: 1.51735355e-06
Iter: 285 loss: 1.57435227e-06
Iter: 286 loss: 1.51645713e-06
Iter: 287 loss: 1.51299218e-06
Iter: 288 loss: 1.51241318e-06
Iter: 289 loss: 1.50943322e-06
Iter: 290 loss: 1.50557184e-06
Iter: 291 loss: 1.50530764e-06
Iter: 292 loss: 1.50108849e-06
Iter: 293 loss: 1.51039433e-06
Iter: 294 loss: 1.49956986e-06
Iter: 295 loss: 1.49522521e-06
Iter: 296 loss: 1.54597387e-06
Iter: 297 loss: 1.49512789e-06
Iter: 298 loss: 1.49213031e-06
Iter: 299 loss: 1.50269693e-06
Iter: 300 loss: 1.491381e-06
Iter: 301 loss: 1.48783386e-06
Iter: 302 loss: 1.49002653e-06
Iter: 303 loss: 1.48554147e-06
Iter: 304 loss: 1.48192964e-06
Iter: 305 loss: 1.47660512e-06
Iter: 306 loss: 1.47650405e-06
Iter: 307 loss: 1.47096955e-06
Iter: 308 loss: 1.51187123e-06
Iter: 309 loss: 1.47051981e-06
Iter: 310 loss: 1.46654065e-06
Iter: 311 loss: 1.48124013e-06
Iter: 312 loss: 1.46548768e-06
Iter: 313 loss: 1.46118964e-06
Iter: 314 loss: 1.48985873e-06
Iter: 315 loss: 1.46077991e-06
Iter: 316 loss: 1.45739159e-06
Iter: 317 loss: 1.46671596e-06
Iter: 318 loss: 1.45627337e-06
Iter: 319 loss: 1.45340118e-06
Iter: 320 loss: 1.44865169e-06
Iter: 321 loss: 1.44864134e-06
Iter: 322 loss: 1.44308115e-06
Iter: 323 loss: 1.46542857e-06
Iter: 324 loss: 1.44186492e-06
Iter: 325 loss: 1.43624504e-06
Iter: 326 loss: 1.44528292e-06
Iter: 327 loss: 1.4336689e-06
Iter: 328 loss: 1.43069087e-06
Iter: 329 loss: 1.43057173e-06
Iter: 330 loss: 1.42702459e-06
Iter: 331 loss: 1.42575993e-06
Iter: 332 loss: 1.42379656e-06
Iter: 333 loss: 1.42044223e-06
Iter: 334 loss: 1.42882038e-06
Iter: 335 loss: 1.4192583e-06
Iter: 336 loss: 1.41584974e-06
Iter: 337 loss: 1.46499849e-06
Iter: 338 loss: 1.41587e-06
Iter: 339 loss: 1.4135087e-06
Iter: 340 loss: 1.40989846e-06
Iter: 341 loss: 1.40981e-06
Iter: 342 loss: 1.40702798e-06
Iter: 343 loss: 1.44662329e-06
Iter: 344 loss: 1.40700308e-06
Iter: 345 loss: 1.40466148e-06
Iter: 346 loss: 1.3993108e-06
Iter: 347 loss: 1.47030073e-06
Iter: 348 loss: 1.3989885e-06
Iter: 349 loss: 1.39478516e-06
Iter: 350 loss: 1.43459692e-06
Iter: 351 loss: 1.39460155e-06
Iter: 352 loss: 1.39081715e-06
Iter: 353 loss: 1.42964598e-06
Iter: 354 loss: 1.3906747e-06
Iter: 355 loss: 1.3890201e-06
Iter: 356 loss: 1.38484029e-06
Iter: 357 loss: 1.42986778e-06
Iter: 358 loss: 1.38439964e-06
Iter: 359 loss: 1.38001246e-06
Iter: 360 loss: 1.42577835e-06
Iter: 361 loss: 1.37995448e-06
Iter: 362 loss: 1.37664051e-06
Iter: 363 loss: 1.37973666e-06
Iter: 364 loss: 1.37478912e-06
Iter: 365 loss: 1.37037841e-06
Iter: 366 loss: 1.38612677e-06
Iter: 367 loss: 1.36921926e-06
Iter: 368 loss: 1.36649828e-06
Iter: 369 loss: 1.39521444e-06
Iter: 370 loss: 1.36639551e-06
Iter: 371 loss: 1.36368783e-06
Iter: 372 loss: 1.36630933e-06
Iter: 373 loss: 1.36217795e-06
Iter: 374 loss: 1.35973266e-06
Iter: 375 loss: 1.367918e-06
Iter: 376 loss: 1.35907158e-06
Iter: 377 loss: 1.35608468e-06
Iter: 378 loss: 1.37170377e-06
Iter: 379 loss: 1.35560413e-06
Iter: 380 loss: 1.3541063e-06
Iter: 381 loss: 1.35119194e-06
Iter: 382 loss: 1.41511828e-06
Iter: 383 loss: 1.35120933e-06
Iter: 384 loss: 1.34834636e-06
Iter: 385 loss: 1.37436928e-06
Iter: 386 loss: 1.34816025e-06
Iter: 387 loss: 1.3450765e-06
Iter: 388 loss: 1.34493575e-06
Iter: 389 loss: 1.34254515e-06
Iter: 390 loss: 1.33904689e-06
Iter: 391 loss: 1.34272318e-06
Iter: 392 loss: 1.33711e-06
Iter: 393 loss: 1.33366734e-06
Iter: 394 loss: 1.33745971e-06
Iter: 395 loss: 1.33179901e-06
Iter: 396 loss: 1.32794958e-06
Iter: 397 loss: 1.35040455e-06
Iter: 398 loss: 1.32746982e-06
Iter: 399 loss: 1.32527009e-06
Iter: 400 loss: 1.32520245e-06
Iter: 401 loss: 1.32294895e-06
Iter: 402 loss: 1.31864147e-06
Iter: 403 loss: 1.41042517e-06
Iter: 404 loss: 1.31863374e-06
Iter: 405 loss: 1.31550212e-06
Iter: 406 loss: 1.31943557e-06
Iter: 407 loss: 1.31390459e-06
Iter: 408 loss: 1.31037041e-06
Iter: 409 loss: 1.338733e-06
Iter: 410 loss: 1.31011711e-06
Iter: 411 loss: 1.30747549e-06
Iter: 412 loss: 1.32441141e-06
Iter: 413 loss: 1.30716546e-06
Iter: 414 loss: 1.30446097e-06
Iter: 415 loss: 1.31906654e-06
Iter: 416 loss: 1.30403339e-06
Iter: 417 loss: 1.30238197e-06
Iter: 418 loss: 1.30080889e-06
Iter: 419 loss: 1.3004169e-06
Iter: 420 loss: 1.29763475e-06
Iter: 421 loss: 1.29755836e-06
Iter: 422 loss: 1.29532759e-06
Iter: 423 loss: 1.29285991e-06
Iter: 424 loss: 1.29267653e-06
Iter: 425 loss: 1.29124908e-06
Iter: 426 loss: 1.28830243e-06
Iter: 427 loss: 1.33605454e-06
Iter: 428 loss: 1.28819715e-06
Iter: 429 loss: 1.28608394e-06
Iter: 430 loss: 1.28593e-06
Iter: 431 loss: 1.28432885e-06
Iter: 432 loss: 1.28166153e-06
Iter: 433 loss: 1.28163583e-06
Iter: 434 loss: 1.27825979e-06
Iter: 435 loss: 1.28730335e-06
Iter: 436 loss: 1.27715202e-06
Iter: 437 loss: 1.27474402e-06
Iter: 438 loss: 1.30075318e-06
Iter: 439 loss: 1.27466649e-06
Iter: 440 loss: 1.27188332e-06
Iter: 441 loss: 1.27298e-06
Iter: 442 loss: 1.26989994e-06
Iter: 443 loss: 1.26782015e-06
Iter: 444 loss: 1.26596478e-06
Iter: 445 loss: 1.26544478e-06
Iter: 446 loss: 1.26194811e-06
Iter: 447 loss: 1.28205068e-06
Iter: 448 loss: 1.26147404e-06
Iter: 449 loss: 1.26055465e-06
Iter: 450 loss: 1.26016903e-06
Iter: 451 loss: 1.25882252e-06
Iter: 452 loss: 1.25626354e-06
Iter: 453 loss: 1.31453362e-06
Iter: 454 loss: 1.25624092e-06
Iter: 455 loss: 1.25411088e-06
Iter: 456 loss: 1.25688962e-06
Iter: 457 loss: 1.2530021e-06
Iter: 458 loss: 1.25059455e-06
Iter: 459 loss: 1.26506029e-06
Iter: 460 loss: 1.25033819e-06
Iter: 461 loss: 1.24823487e-06
Iter: 462 loss: 1.25221959e-06
Iter: 463 loss: 1.24735584e-06
Iter: 464 loss: 1.24509074e-06
Iter: 465 loss: 1.2638601e-06
Iter: 466 loss: 1.24495341e-06
Iter: 467 loss: 1.24348139e-06
Iter: 468 loss: 1.2409555e-06
Iter: 469 loss: 1.24092958e-06
Iter: 470 loss: 1.2389653e-06
Iter: 471 loss: 1.23887071e-06
Iter: 472 loss: 1.23735811e-06
Iter: 473 loss: 1.23710424e-06
Iter: 474 loss: 1.23604673e-06
Iter: 475 loss: 1.23383961e-06
Iter: 476 loss: 1.23158406e-06
Iter: 477 loss: 1.23117286e-06
Iter: 478 loss: 1.22783695e-06
Iter: 479 loss: 1.24799112e-06
Iter: 480 loss: 1.22742517e-06
Iter: 481 loss: 1.22447e-06
Iter: 482 loss: 1.23578548e-06
Iter: 483 loss: 1.22379276e-06
Iter: 484 loss: 1.2220944e-06
Iter: 485 loss: 1.2219532e-06
Iter: 486 loss: 1.2209124e-06
Iter: 487 loss: 1.21856556e-06
Iter: 488 loss: 1.25392069e-06
Iter: 489 loss: 1.21849348e-06
Iter: 490 loss: 1.21843027e-06
Iter: 491 loss: 1.21727408e-06
Iter: 492 loss: 1.21659377e-06
Iter: 493 loss: 1.2145847e-06
Iter: 494 loss: 1.22022379e-06
Iter: 495 loss: 1.21343237e-06
Iter: 496 loss: 1.2104523e-06
Iter: 497 loss: 1.22768745e-06
Iter: 498 loss: 1.21010896e-06
Iter: 499 loss: 1.20808966e-06
Iter: 500 loss: 1.22037625e-06
Iter: 501 loss: 1.20781431e-06
Iter: 502 loss: 1.20554068e-06
Iter: 503 loss: 1.20952745e-06
Iter: 504 loss: 1.20449351e-06
Iter: 505 loss: 1.20221489e-06
Iter: 506 loss: 1.21252447e-06
Iter: 507 loss: 1.20171057e-06
Iter: 508 loss: 1.19980223e-06
Iter: 509 loss: 1.19837296e-06
Iter: 510 loss: 1.19772028e-06
Iter: 511 loss: 1.1958507e-06
Iter: 512 loss: 1.19583137e-06
Iter: 513 loss: 1.19413221e-06
Iter: 514 loss: 1.19238712e-06
Iter: 515 loss: 1.19207846e-06
Iter: 516 loss: 1.19009371e-06
Iter: 517 loss: 1.19653464e-06
Iter: 518 loss: 1.18950015e-06
Iter: 519 loss: 1.18764012e-06
Iter: 520 loss: 1.19540618e-06
Iter: 521 loss: 1.18726746e-06
Iter: 522 loss: 1.18488629e-06
Iter: 523 loss: 1.1954229e-06
Iter: 524 loss: 1.18435889e-06
Iter: 525 loss: 1.18309458e-06
Iter: 526 loss: 1.1844545e-06
Iter: 527 loss: 1.18240746e-06
Iter: 528 loss: 1.18030653e-06
Iter: 529 loss: 1.18913181e-06
Iter: 530 loss: 1.17987099e-06
Iter: 531 loss: 1.17860498e-06
Iter: 532 loss: 1.17651848e-06
Iter: 533 loss: 1.176502e-06
Iter: 534 loss: 1.17436139e-06
Iter: 535 loss: 1.17773379e-06
Iter: 536 loss: 1.17338845e-06
Iter: 537 loss: 1.17168361e-06
Iter: 538 loss: 1.17161824e-06
Iter: 539 loss: 1.17010404e-06
Iter: 540 loss: 1.16904789e-06
Iter: 541 loss: 1.16853175e-06
Iter: 542 loss: 1.16629337e-06
Iter: 543 loss: 1.17655225e-06
Iter: 544 loss: 1.16589945e-06
Iter: 545 loss: 1.16414583e-06
Iter: 546 loss: 1.16694605e-06
Iter: 547 loss: 1.16330921e-06
Iter: 548 loss: 1.16149067e-06
Iter: 549 loss: 1.17447337e-06
Iter: 550 loss: 1.16135516e-06
Iter: 551 loss: 1.15980697e-06
Iter: 552 loss: 1.1597831e-06
Iter: 553 loss: 1.15860132e-06
Iter: 554 loss: 1.1568743e-06
Iter: 555 loss: 1.15841988e-06
Iter: 556 loss: 1.1558858e-06
Iter: 557 loss: 1.15437842e-06
Iter: 558 loss: 1.15436046e-06
Iter: 559 loss: 1.15292278e-06
Iter: 560 loss: 1.15325201e-06
Iter: 561 loss: 1.15184537e-06
Iter: 562 loss: 1.15101761e-06
Iter: 563 loss: 1.15104126e-06
Iter: 564 loss: 1.15009334e-06
Iter: 565 loss: 1.14796023e-06
Iter: 566 loss: 1.1725607e-06
Iter: 567 loss: 1.1477207e-06
Iter: 568 loss: 1.14595991e-06
Iter: 569 loss: 1.14900035e-06
Iter: 570 loss: 1.14522675e-06
Iter: 571 loss: 1.14296085e-06
Iter: 572 loss: 1.1470629e-06
Iter: 573 loss: 1.14203942e-06
Iter: 574 loss: 1.14042246e-06
Iter: 575 loss: 1.14040461e-06
Iter: 576 loss: 1.13896158e-06
Iter: 577 loss: 1.14088323e-06
Iter: 578 loss: 1.13823296e-06
Iter: 579 loss: 1.13697752e-06
Iter: 580 loss: 1.13698161e-06
Iter: 581 loss: 1.13596832e-06
Iter: 582 loss: 1.13406804e-06
Iter: 583 loss: 1.1426323e-06
Iter: 584 loss: 1.13373687e-06
Iter: 585 loss: 1.13212445e-06
Iter: 586 loss: 1.13856845e-06
Iter: 587 loss: 1.13172098e-06
Iter: 588 loss: 1.13029034e-06
Iter: 589 loss: 1.13445606e-06
Iter: 590 loss: 1.12984299e-06
Iter: 591 loss: 1.1283521e-06
Iter: 592 loss: 1.12728435e-06
Iter: 593 loss: 1.12677344e-06
Iter: 594 loss: 1.1261784e-06
Iter: 595 loss: 1.1259026e-06
Iter: 596 loss: 1.12496355e-06
Iter: 597 loss: 1.12357031e-06
Iter: 598 loss: 1.12357225e-06
Iter: 599 loss: 1.12268992e-06
Iter: 600 loss: 1.12260022e-06
Iter: 601 loss: 1.12178543e-06
Iter: 602 loss: 1.12062196e-06
Iter: 603 loss: 1.1205608e-06
Iter: 604 loss: 1.11918825e-06
Iter: 605 loss: 1.11787972e-06
Iter: 606 loss: 1.11753354e-06
Iter: 607 loss: 1.11568011e-06
Iter: 608 loss: 1.12611428e-06
Iter: 609 loss: 1.11539282e-06
Iter: 610 loss: 1.11338954e-06
Iter: 611 loss: 1.13030148e-06
Iter: 612 loss: 1.11328018e-06
Iter: 613 loss: 1.11242434e-06
Iter: 614 loss: 1.11127633e-06
Iter: 615 loss: 1.11120346e-06
Iter: 616 loss: 1.10946758e-06
Iter: 617 loss: 1.11598752e-06
Iter: 618 loss: 1.1090774e-06
Iter: 619 loss: 1.10781605e-06
Iter: 620 loss: 1.12349949e-06
Iter: 621 loss: 1.10781696e-06
Iter: 622 loss: 1.10684027e-06
Iter: 623 loss: 1.1055165e-06
Iter: 624 loss: 1.10546807e-06
Iter: 625 loss: 1.10372366e-06
Iter: 626 loss: 1.11786028e-06
Iter: 627 loss: 1.10361907e-06
Iter: 628 loss: 1.10236545e-06
Iter: 629 loss: 1.10545989e-06
Iter: 630 loss: 1.10192605e-06
Iter: 631 loss: 1.1004314e-06
Iter: 632 loss: 1.11009626e-06
Iter: 633 loss: 1.10030885e-06
Iter: 634 loss: 1.09949747e-06
Iter: 635 loss: 1.10123324e-06
Iter: 636 loss: 1.09915709e-06
Iter: 637 loss: 1.09792563e-06
Iter: 638 loss: 1.09824782e-06
Iter: 639 loss: 1.09698385e-06
Iter: 640 loss: 1.09587972e-06
Iter: 641 loss: 1.09516623e-06
Iter: 642 loss: 1.09471489e-06
Iter: 643 loss: 1.0931625e-06
Iter: 644 loss: 1.0962176e-06
Iter: 645 loss: 1.09251152e-06
Iter: 646 loss: 1.09076529e-06
Iter: 647 loss: 1.09360303e-06
Iter: 648 loss: 1.08995869e-06
Iter: 649 loss: 1.08802931e-06
Iter: 650 loss: 1.0969984e-06
Iter: 651 loss: 1.0876596e-06
Iter: 652 loss: 1.08649579e-06
Iter: 653 loss: 1.08645486e-06
Iter: 654 loss: 1.08563734e-06
Iter: 655 loss: 1.08376298e-06
Iter: 656 loss: 1.10710789e-06
Iter: 657 loss: 1.08362701e-06
Iter: 658 loss: 1.08168649e-06
Iter: 659 loss: 1.09322332e-06
Iter: 660 loss: 1.08144195e-06
Iter: 661 loss: 1.08015956e-06
Iter: 662 loss: 1.09508414e-06
Iter: 663 loss: 1.08011727e-06
Iter: 664 loss: 1.07914354e-06
Iter: 665 loss: 1.08092638e-06
Iter: 666 loss: 1.07869892e-06
Iter: 667 loss: 1.07755886e-06
Iter: 668 loss: 1.08315919e-06
Iter: 669 loss: 1.07735514e-06
Iter: 670 loss: 1.07641108e-06
Iter: 671 loss: 1.08081917e-06
Iter: 672 loss: 1.07620622e-06
Iter: 673 loss: 1.07541177e-06
Iter: 674 loss: 1.07599499e-06
Iter: 675 loss: 1.07493599e-06
Iter: 676 loss: 1.07384221e-06
Iter: 677 loss: 1.07637197e-06
Iter: 678 loss: 1.07341066e-06
Iter: 679 loss: 1.0723702e-06
Iter: 680 loss: 1.07226026e-06
Iter: 681 loss: 1.07151072e-06
Iter: 682 loss: 1.07007281e-06
Iter: 683 loss: 1.06873335e-06
Iter: 684 loss: 1.06837172e-06
Iter: 685 loss: 1.06632785e-06
Iter: 686 loss: 1.07150504e-06
Iter: 687 loss: 1.06560913e-06
Iter: 688 loss: 1.06384118e-06
Iter: 689 loss: 1.09151e-06
Iter: 690 loss: 1.06383868e-06
Iter: 691 loss: 1.0626884e-06
Iter: 692 loss: 1.07514472e-06
Iter: 693 loss: 1.06264565e-06
Iter: 694 loss: 1.06178766e-06
Iter: 695 loss: 1.05982963e-06
Iter: 696 loss: 1.08545555e-06
Iter: 697 loss: 1.05971253e-06
Iter: 698 loss: 1.05788206e-06
Iter: 699 loss: 1.06568621e-06
Iter: 700 loss: 1.05753793e-06
Iter: 701 loss: 1.05689855e-06
Iter: 702 loss: 1.05669596e-06
Iter: 703 loss: 1.05595655e-06
Iter: 704 loss: 1.05617892e-06
Iter: 705 loss: 1.05537583e-06
Iter: 706 loss: 1.05428649e-06
Iter: 707 loss: 1.05827201e-06
Iter: 708 loss: 1.054005e-06
Iter: 709 loss: 1.05290928e-06
Iter: 710 loss: 1.05402e-06
Iter: 711 loss: 1.05230424e-06
Iter: 712 loss: 1.05131448e-06
Iter: 713 loss: 1.05630488e-06
Iter: 714 loss: 1.05113736e-06
Iter: 715 loss: 1.05009713e-06
Iter: 716 loss: 1.04860737e-06
Iter: 717 loss: 1.04854939e-06
Iter: 718 loss: 1.04703304e-06
Iter: 719 loss: 1.06129266e-06
Iter: 720 loss: 1.04701917e-06
Iter: 721 loss: 1.04580079e-06
Iter: 722 loss: 1.04580135e-06
Iter: 723 loss: 1.04486435e-06
Iter: 724 loss: 1.04325807e-06
Iter: 725 loss: 1.04569847e-06
Iter: 726 loss: 1.04248875e-06
Iter: 727 loss: 1.04089474e-06
Iter: 728 loss: 1.04958224e-06
Iter: 729 loss: 1.04067362e-06
Iter: 730 loss: 1.03953573e-06
Iter: 731 loss: 1.03954665e-06
Iter: 732 loss: 1.03897219e-06
Iter: 733 loss: 1.03740535e-06
Iter: 734 loss: 1.04566993e-06
Iter: 735 loss: 1.03690104e-06
Iter: 736 loss: 1.03638547e-06
Iter: 737 loss: 1.03603145e-06
Iter: 738 loss: 1.03506557e-06
Iter: 739 loss: 1.03600223e-06
Iter: 740 loss: 1.03446246e-06
Iter: 741 loss: 1.033422e-06
Iter: 742 loss: 1.037841e-06
Iter: 743 loss: 1.03317291e-06
Iter: 744 loss: 1.03218304e-06
Iter: 745 loss: 1.03281104e-06
Iter: 746 loss: 1.03150273e-06
Iter: 747 loss: 1.03048365e-06
Iter: 748 loss: 1.03456364e-06
Iter: 749 loss: 1.0302623e-06
Iter: 750 loss: 1.02920785e-06
Iter: 751 loss: 1.03021716e-06
Iter: 752 loss: 1.02857405e-06
Iter: 753 loss: 1.02746606e-06
Iter: 754 loss: 1.02809236e-06
Iter: 755 loss: 1.02682293e-06
Iter: 756 loss: 1.02556737e-06
Iter: 757 loss: 1.03162176e-06
Iter: 758 loss: 1.02529066e-06
Iter: 759 loss: 1.02399645e-06
Iter: 760 loss: 1.02384411e-06
Iter: 761 loss: 1.02290528e-06
Iter: 762 loss: 1.02139461e-06
Iter: 763 loss: 1.02869103e-06
Iter: 764 loss: 1.02110835e-06
Iter: 765 loss: 1.01991259e-06
Iter: 766 loss: 1.01990918e-06
Iter: 767 loss: 1.01904834e-06
Iter: 768 loss: 1.01776095e-06
Iter: 769 loss: 1.01774685e-06
Iter: 770 loss: 1.01627677e-06
Iter: 771 loss: 1.01847547e-06
Iter: 772 loss: 1.01549426e-06
Iter: 773 loss: 1.01448927e-06
Iter: 774 loss: 1.02749573e-06
Iter: 775 loss: 1.01448722e-06
Iter: 776 loss: 1.01326418e-06
Iter: 777 loss: 1.01559704e-06
Iter: 778 loss: 1.01279306e-06
Iter: 779 loss: 1.01200965e-06
Iter: 780 loss: 1.01533738e-06
Iter: 781 loss: 1.01183036e-06
Iter: 782 loss: 1.01099272e-06
Iter: 783 loss: 1.01149953e-06
Iter: 784 loss: 1.01042747e-06
Iter: 785 loss: 1.00955015e-06
Iter: 786 loss: 1.01108787e-06
Iter: 787 loss: 1.00913712e-06
Iter: 788 loss: 1.00795e-06
Iter: 789 loss: 1.01012802e-06
Iter: 790 loss: 1.00739942e-06
Iter: 791 loss: 1.00643979e-06
Iter: 792 loss: 1.00644752e-06
Iter: 793 loss: 1.00568491e-06
Iter: 794 loss: 1.00459533e-06
Iter: 795 loss: 1.01235582e-06
Iter: 796 loss: 1.00450529e-06
Iter: 797 loss: 1.00344539e-06
Iter: 798 loss: 1.00325235e-06
Iter: 799 loss: 1.00255045e-06
Iter: 800 loss: 1.00108991e-06
Iter: 801 loss: 1.00783654e-06
Iter: 802 loss: 1.00081343e-06
Iter: 803 loss: 9.99531721e-07
Iter: 804 loss: 1.00115199e-06
Iter: 805 loss: 9.9882925e-07
Iter: 806 loss: 9.97237066e-07
Iter: 807 loss: 1.01838668e-06
Iter: 808 loss: 9.97215238e-07
Iter: 809 loss: 9.96636231e-07
Iter: 810 loss: 9.95009486e-07
Iter: 811 loss: 1.00549983e-06
Iter: 812 loss: 9.94621246e-07
Iter: 813 loss: 9.9365252e-07
Iter: 814 loss: 9.93497906e-07
Iter: 815 loss: 9.92644686e-07
Iter: 816 loss: 1.00071907e-06
Iter: 817 loss: 9.92593641e-07
Iter: 818 loss: 9.91999741e-07
Iter: 819 loss: 9.9130034e-07
Iter: 820 loss: 9.91213938e-07
Iter: 821 loss: 9.90301714e-07
Iter: 822 loss: 1.0012177e-06
Iter: 823 loss: 9.90304e-07
Iter: 824 loss: 9.89788077e-07
Iter: 825 loss: 9.89278078e-07
Iter: 826 loss: 9.89208e-07
Iter: 827 loss: 9.88131433e-07
Iter: 828 loss: 9.92366267e-07
Iter: 829 loss: 9.87891667e-07
Iter: 830 loss: 9.87079602e-07
Iter: 831 loss: 9.85877e-07
Iter: 832 loss: 9.85857696e-07
Iter: 833 loss: 9.8409555e-07
Iter: 834 loss: 9.90604576e-07
Iter: 835 loss: 9.83695827e-07
Iter: 836 loss: 9.82361144e-07
Iter: 837 loss: 9.91610932e-07
Iter: 838 loss: 9.82276333e-07
Iter: 839 loss: 9.81193125e-07
Iter: 840 loss: 9.82875576e-07
Iter: 841 loss: 9.80701543e-07
Iter: 842 loss: 9.79490324e-07
Iter: 843 loss: 9.80321374e-07
Iter: 844 loss: 9.78716457e-07
Iter: 845 loss: 9.77871e-07
Iter: 846 loss: 9.7780719e-07
Iter: 847 loss: 9.76970455e-07
Iter: 848 loss: 9.77143372e-07
Iter: 849 loss: 9.76371552e-07
Iter: 850 loss: 9.75483772e-07
Iter: 851 loss: 9.78231e-07
Iter: 852 loss: 9.75251e-07
Iter: 853 loss: 9.74206614e-07
Iter: 854 loss: 9.80813411e-07
Iter: 855 loss: 9.74103386e-07
Iter: 856 loss: 9.73460942e-07
Iter: 857 loss: 9.72262796e-07
Iter: 858 loss: 9.95150572e-07
Iter: 859 loss: 9.72272e-07
Iter: 860 loss: 9.71491431e-07
Iter: 861 loss: 9.71458348e-07
Iter: 862 loss: 9.70624e-07
Iter: 863 loss: 9.69540338e-07
Iter: 864 loss: 9.69440293e-07
Iter: 865 loss: 9.68268751e-07
Iter: 866 loss: 9.68427457e-07
Iter: 867 loss: 9.67346e-07
Iter: 868 loss: 9.66144626e-07
Iter: 869 loss: 9.82878873e-07
Iter: 870 loss: 9.6613735e-07
Iter: 871 loss: 9.65085292e-07
Iter: 872 loss: 9.68756126e-07
Iter: 873 loss: 9.64836318e-07
Iter: 874 loss: 9.63965249e-07
Iter: 875 loss: 9.63118737e-07
Iter: 876 loss: 9.6296526e-07
Iter: 877 loss: 9.61859541e-07
Iter: 878 loss: 9.74429e-07
Iter: 879 loss: 9.6182157e-07
Iter: 880 loss: 9.60816465e-07
Iter: 881 loss: 9.60308398e-07
Iter: 882 loss: 9.59833756e-07
Iter: 883 loss: 9.59009867e-07
Iter: 884 loss: 9.58927558e-07
Iter: 885 loss: 9.58314104e-07
Iter: 886 loss: 9.58790906e-07
Iter: 887 loss: 9.57911539e-07
Iter: 888 loss: 9.57217253e-07
Iter: 889 loss: 9.62335e-07
Iter: 890 loss: 9.57166321e-07
Iter: 891 loss: 9.56461e-07
Iter: 892 loss: 9.55383825e-07
Iter: 893 loss: 9.55354608e-07
Iter: 894 loss: 9.54449092e-07
Iter: 895 loss: 9.54511506e-07
Iter: 896 loss: 9.53740937e-07
Iter: 897 loss: 9.53361337e-07
Iter: 898 loss: 9.53030622e-07
Iter: 899 loss: 9.52608616e-07
Iter: 900 loss: 9.51515631e-07
Iter: 901 loss: 9.63117145e-07
Iter: 902 loss: 9.51441393e-07
Iter: 903 loss: 9.50131607e-07
Iter: 904 loss: 9.56762733e-07
Iter: 905 loss: 9.49921287e-07
Iter: 906 loss: 9.49307548e-07
Iter: 907 loss: 9.49243315e-07
Iter: 908 loss: 9.48730644e-07
Iter: 909 loss: 9.4761225e-07
Iter: 910 loss: 9.65566414e-07
Iter: 911 loss: 9.4760162e-07
Iter: 912 loss: 9.46311445e-07
Iter: 913 loss: 9.49723244e-07
Iter: 914 loss: 9.45859654e-07
Iter: 915 loss: 9.44751207e-07
Iter: 916 loss: 9.58739065e-07
Iter: 917 loss: 9.44754902e-07
Iter: 918 loss: 9.43911346e-07
Iter: 919 loss: 9.46070486e-07
Iter: 920 loss: 9.43632472e-07
Iter: 921 loss: 9.42717634e-07
Iter: 922 loss: 9.46667626e-07
Iter: 923 loss: 9.42538179e-07
Iter: 924 loss: 9.41689507e-07
Iter: 925 loss: 9.45597435e-07
Iter: 926 loss: 9.41560302e-07
Iter: 927 loss: 9.40930306e-07
Iter: 928 loss: 9.40485904e-07
Iter: 929 loss: 9.40257735e-07
Iter: 930 loss: 9.39322092e-07
Iter: 931 loss: 9.39195616e-07
Iter: 932 loss: 9.38506957e-07
Iter: 933 loss: 9.37887876e-07
Iter: 934 loss: 9.37742129e-07
Iter: 935 loss: 9.372053e-07
Iter: 936 loss: 9.36095432e-07
Iter: 937 loss: 9.55427709e-07
Iter: 938 loss: 9.36044273e-07
Iter: 939 loss: 9.34849879e-07
Iter: 940 loss: 9.36111178e-07
Iter: 941 loss: 9.34173841e-07
Iter: 942 loss: 9.33087904e-07
Iter: 943 loss: 9.49562093e-07
Iter: 944 loss: 9.33089893e-07
Iter: 945 loss: 9.32180967e-07
Iter: 946 loss: 9.3385853e-07
Iter: 947 loss: 9.31776469e-07
Iter: 948 loss: 9.30831618e-07
Iter: 949 loss: 9.29940938e-07
Iter: 950 loss: 9.29701912e-07
Iter: 951 loss: 9.28560439e-07
Iter: 952 loss: 9.42535905e-07
Iter: 953 loss: 9.28551628e-07
Iter: 954 loss: 9.27728e-07
Iter: 955 loss: 9.36388574e-07
Iter: 956 loss: 9.27719668e-07
Iter: 957 loss: 9.27197561e-07
Iter: 958 loss: 9.28310953e-07
Iter: 959 loss: 9.26974337e-07
Iter: 960 loss: 9.26239863e-07
Iter: 961 loss: 9.27123153e-07
Iter: 962 loss: 9.25911422e-07
Iter: 963 loss: 9.25199686e-07
Iter: 964 loss: 9.24942128e-07
Iter: 965 loss: 9.24570031e-07
Iter: 966 loss: 9.23593859e-07
Iter: 967 loss: 9.28286113e-07
Iter: 968 loss: 9.23433163e-07
Iter: 969 loss: 9.2274297e-07
Iter: 970 loss: 9.27284304e-07
Iter: 971 loss: 9.22647814e-07
Iter: 972 loss: 9.21838591e-07
Iter: 973 loss: 9.21224569e-07
Iter: 974 loss: 9.20949844e-07
Iter: 975 loss: 9.20147841e-07
Iter: 976 loss: 9.19459922e-07
Iter: 977 loss: 9.19265574e-07
Iter: 978 loss: 9.1825018e-07
Iter: 979 loss: 9.34368245e-07
Iter: 980 loss: 9.18261662e-07
Iter: 981 loss: 9.17444481e-07
Iter: 982 loss: 9.20943876e-07
Iter: 983 loss: 9.17267e-07
Iter: 984 loss: 9.16558406e-07
Iter: 985 loss: 9.1641806e-07
Iter: 986 loss: 9.15935061e-07
Iter: 987 loss: 9.14881241e-07
Iter: 988 loss: 9.15378962e-07
Iter: 989 loss: 9.142218e-07
Iter: 990 loss: 9.13592658e-07
Iter: 991 loss: 9.13401777e-07
Iter: 992 loss: 9.12833343e-07
Iter: 993 loss: 9.1282692e-07
Iter: 994 loss: 9.12388316e-07
Iter: 995 loss: 9.11422262e-07
Iter: 996 loss: 9.14034388e-07
Iter: 997 loss: 9.1111724e-07
Iter: 998 loss: 9.10514814e-07
Iter: 999 loss: 9.10320182e-07
Iter: 1000 loss: 9.09953883e-07
Iter: 1001 loss: 9.0916825e-07
Iter: 1002 loss: 9.13386316e-07
Iter: 1003 loss: 9.09091113e-07
Iter: 1004 loss: 9.08348511e-07
Iter: 1005 loss: 9.1135189e-07
Iter: 1006 loss: 9.08204242e-07
Iter: 1007 loss: 9.07498929e-07
Iter: 1008 loss: 9.08087713e-07
Iter: 1009 loss: 9.07003084e-07
Iter: 1010 loss: 9.06315108e-07
Iter: 1011 loss: 9.05798743e-07
Iter: 1012 loss: 9.05560569e-07
Iter: 1013 loss: 9.04408694e-07
Iter: 1014 loss: 9.06148216e-07
Iter: 1015 loss: 9.03888917e-07
Iter: 1016 loss: 9.02895806e-07
Iter: 1017 loss: 9.02883812e-07
Iter: 1018 loss: 9.02200838e-07
Iter: 1019 loss: 9.01808448e-07
Iter: 1020 loss: 9.01532303e-07
Iter: 1021 loss: 9.00658733e-07
Iter: 1022 loss: 9.02583054e-07
Iter: 1023 loss: 9.00325517e-07
Iter: 1024 loss: 8.99738268e-07
Iter: 1025 loss: 8.99718202e-07
Iter: 1026 loss: 8.9924157e-07
Iter: 1027 loss: 8.98957751e-07
Iter: 1028 loss: 8.98747146e-07
Iter: 1029 loss: 8.98079577e-07
Iter: 1030 loss: 9.0344065e-07
Iter: 1031 loss: 8.9803126e-07
Iter: 1032 loss: 8.97642735e-07
Iter: 1033 loss: 8.96760355e-07
Iter: 1034 loss: 9.07443564e-07
Iter: 1035 loss: 8.96665483e-07
Iter: 1036 loss: 8.95639801e-07
Iter: 1037 loss: 9.07472e-07
Iter: 1038 loss: 8.95624225e-07
Iter: 1039 loss: 8.9490527e-07
Iter: 1040 loss: 9.00033683e-07
Iter: 1041 loss: 8.94878497e-07
Iter: 1042 loss: 8.9432524e-07
Iter: 1043 loss: 8.93749302e-07
Iter: 1044 loss: 8.93676088e-07
Iter: 1045 loss: 8.92622438e-07
Iter: 1046 loss: 8.95247751e-07
Iter: 1047 loss: 8.92226467e-07
Iter: 1048 loss: 8.91415709e-07
Iter: 1049 loss: 8.91598802e-07
Iter: 1050 loss: 8.90760418e-07
Iter: 1051 loss: 8.90080514e-07
Iter: 1052 loss: 8.90078468e-07
Iter: 1053 loss: 8.89341777e-07
Iter: 1054 loss: 8.89227294e-07
Iter: 1055 loss: 8.88727868e-07
Iter: 1056 loss: 8.87957412e-07
Iter: 1057 loss: 8.88956606e-07
Iter: 1058 loss: 8.87570138e-07
Iter: 1059 loss: 8.87000397e-07
Iter: 1060 loss: 8.86955945e-07
Iter: 1061 loss: 8.86445775e-07
Iter: 1062 loss: 8.8569459e-07
Iter: 1063 loss: 8.85704708e-07
Iter: 1064 loss: 8.84817894e-07
Iter: 1065 loss: 8.93982531e-07
Iter: 1066 loss: 8.84775091e-07
Iter: 1067 loss: 8.84323072e-07
Iter: 1068 loss: 8.83754e-07
Iter: 1069 loss: 8.83689097e-07
Iter: 1070 loss: 8.83023688e-07
Iter: 1071 loss: 8.87248e-07
Iter: 1072 loss: 8.82923871e-07
Iter: 1073 loss: 8.82118741e-07
Iter: 1074 loss: 8.84416067e-07
Iter: 1075 loss: 8.81878e-07
Iter: 1076 loss: 8.81193046e-07
Iter: 1077 loss: 8.81528422e-07
Iter: 1078 loss: 8.80750633e-07
Iter: 1079 loss: 8.79929189e-07
Iter: 1080 loss: 8.81561789e-07
Iter: 1081 loss: 8.79622689e-07
Iter: 1082 loss: 8.78634637e-07
Iter: 1083 loss: 8.79137e-07
Iter: 1084 loss: 8.77936429e-07
Iter: 1085 loss: 8.77146817e-07
Iter: 1086 loss: 8.8921297e-07
Iter: 1087 loss: 8.77141758e-07
Iter: 1088 loss: 8.76402623e-07
Iter: 1089 loss: 8.77017783e-07
Iter: 1090 loss: 8.75968681e-07
Iter: 1091 loss: 8.75282524e-07
Iter: 1092 loss: 8.75755291e-07
Iter: 1093 loss: 8.74833518e-07
Iter: 1094 loss: 8.73979729e-07
Iter: 1095 loss: 8.73986664e-07
Iter: 1096 loss: 8.73642875e-07
Iter: 1097 loss: 8.73479166e-07
Iter: 1098 loss: 8.7333e-07
Iter: 1099 loss: 8.72614294e-07
Iter: 1100 loss: 8.73333079e-07
Iter: 1101 loss: 8.72204396e-07
Iter: 1102 loss: 8.71456052e-07
Iter: 1103 loss: 8.7146168e-07
Iter: 1104 loss: 8.70834697e-07
Iter: 1105 loss: 8.70141832e-07
Iter: 1106 loss: 8.79385652e-07
Iter: 1107 loss: 8.70154e-07
Iter: 1108 loss: 8.69420774e-07
Iter: 1109 loss: 8.69096255e-07
Iter: 1110 loss: 8.68671236e-07
Iter: 1111 loss: 8.67799201e-07
Iter: 1112 loss: 8.69245582e-07
Iter: 1113 loss: 8.67387143e-07
Iter: 1114 loss: 8.66445e-07
Iter: 1115 loss: 8.70214421e-07
Iter: 1116 loss: 8.66182347e-07
Iter: 1117 loss: 8.65468962e-07
Iter: 1118 loss: 8.6752334e-07
Iter: 1119 loss: 8.65239258e-07
Iter: 1120 loss: 8.64488356e-07
Iter: 1121 loss: 8.67031304e-07
Iter: 1122 loss: 8.64338404e-07
Iter: 1123 loss: 8.63360469e-07
Iter: 1124 loss: 8.64009621e-07
Iter: 1125 loss: 8.62731326e-07
Iter: 1126 loss: 8.62082629e-07
Iter: 1127 loss: 8.69828398e-07
Iter: 1128 loss: 8.62090246e-07
Iter: 1129 loss: 8.61395392e-07
Iter: 1130 loss: 8.62771174e-07
Iter: 1131 loss: 8.61143e-07
Iter: 1132 loss: 8.60626074e-07
Iter: 1133 loss: 8.60710543e-07
Iter: 1134 loss: 8.60251305e-07
Iter: 1135 loss: 8.59375518e-07
Iter: 1136 loss: 8.61559158e-07
Iter: 1137 loss: 8.59050317e-07
Iter: 1138 loss: 8.58352166e-07
Iter: 1139 loss: 8.58177827e-07
Iter: 1140 loss: 8.57779e-07
Iter: 1141 loss: 8.57145949e-07
Iter: 1142 loss: 8.57147711e-07
Iter: 1143 loss: 8.56546421e-07
Iter: 1144 loss: 8.56230372e-07
Iter: 1145 loss: 8.55972928e-07
Iter: 1146 loss: 8.55327414e-07
Iter: 1147 loss: 8.55228336e-07
Iter: 1148 loss: 8.54731184e-07
Iter: 1149 loss: 8.53768142e-07
Iter: 1150 loss: 8.60552518e-07
Iter: 1151 loss: 8.53654e-07
Iter: 1152 loss: 8.52911967e-07
Iter: 1153 loss: 8.54031157e-07
Iter: 1154 loss: 8.52564938e-07
Iter: 1155 loss: 8.5172735e-07
Iter: 1156 loss: 8.57616556e-07
Iter: 1157 loss: 8.51678692e-07
Iter: 1158 loss: 8.50979859e-07
Iter: 1159 loss: 8.52182666e-07
Iter: 1160 loss: 8.50684e-07
Iter: 1161 loss: 8.50240212e-07
Iter: 1162 loss: 8.50231515e-07
Iter: 1163 loss: 8.49848107e-07
Iter: 1164 loss: 8.4934652e-07
Iter: 1165 loss: 8.492907e-07
Iter: 1166 loss: 8.4878036e-07
Iter: 1167 loss: 8.51279651e-07
Iter: 1168 loss: 8.48708851e-07
Iter: 1169 loss: 8.48110801e-07
Iter: 1170 loss: 8.47885588e-07
Iter: 1171 loss: 8.47531112e-07
Iter: 1172 loss: 8.46820626e-07
Iter: 1173 loss: 8.47146453e-07
Iter: 1174 loss: 8.46327e-07
Iter: 1175 loss: 8.45528575e-07
Iter: 1176 loss: 8.45554041e-07
Iter: 1177 loss: 8.45000727e-07
Iter: 1178 loss: 8.44587134e-07
Iter: 1179 loss: 8.44382498e-07
Iter: 1180 loss: 8.43675934e-07
Iter: 1181 loss: 8.43428438e-07
Iter: 1182 loss: 8.43042415e-07
Iter: 1183 loss: 8.42111263e-07
Iter: 1184 loss: 8.5322921e-07
Iter: 1185 loss: 8.42095e-07
Iter: 1186 loss: 8.41532142e-07
Iter: 1187 loss: 8.43093687e-07
Iter: 1188 loss: 8.41336771e-07
Iter: 1189 loss: 8.40761061e-07
Iter: 1190 loss: 8.4330145e-07
Iter: 1191 loss: 8.40614689e-07
Iter: 1192 loss: 8.40058419e-07
Iter: 1193 loss: 8.41506449e-07
Iter: 1194 loss: 8.3987635e-07
Iter: 1195 loss: 8.39352538e-07
Iter: 1196 loss: 8.44917963e-07
Iter: 1197 loss: 8.39358563e-07
Iter: 1198 loss: 8.38983055e-07
Iter: 1199 loss: 8.38230278e-07
Iter: 1200 loss: 8.51409709e-07
Iter: 1201 loss: 8.38189408e-07
Iter: 1202 loss: 8.37574873e-07
Iter: 1203 loss: 8.46765829e-07
Iter: 1204 loss: 8.37559071e-07
Iter: 1205 loss: 8.36931633e-07
Iter: 1206 loss: 8.36317952e-07
Iter: 1207 loss: 8.36174934e-07
Iter: 1208 loss: 8.35417836e-07
Iter: 1209 loss: 8.36751838e-07
Iter: 1210 loss: 8.35111507e-07
Iter: 1211 loss: 8.34678872e-07
Iter: 1212 loss: 8.3461606e-07
Iter: 1213 loss: 8.34281821e-07
Iter: 1214 loss: 8.33644833e-07
Iter: 1215 loss: 8.45849513e-07
Iter: 1216 loss: 8.33633862e-07
Iter: 1217 loss: 8.32805e-07
Iter: 1218 loss: 8.3404916e-07
Iter: 1219 loss: 8.32463797e-07
Iter: 1220 loss: 8.31773e-07
Iter: 1221 loss: 8.36332674e-07
Iter: 1222 loss: 8.3165105e-07
Iter: 1223 loss: 8.30894e-07
Iter: 1224 loss: 8.32687817e-07
Iter: 1225 loss: 8.30614624e-07
Iter: 1226 loss: 8.300118e-07
Iter: 1227 loss: 8.34438936e-07
Iter: 1228 loss: 8.29937562e-07
Iter: 1229 loss: 8.29408e-07
Iter: 1230 loss: 8.31314821e-07
Iter: 1231 loss: 8.29281817e-07
Iter: 1232 loss: 8.2866876e-07
Iter: 1233 loss: 8.31010766e-07
Iter: 1234 loss: 8.28561042e-07
Iter: 1235 loss: 8.28192526e-07
Iter: 1236 loss: 8.27738177e-07
Iter: 1237 loss: 8.27718111e-07
Iter: 1238 loss: 8.27141946e-07
Iter: 1239 loss: 8.34125728e-07
Iter: 1240 loss: 8.27158772e-07
Iter: 1241 loss: 8.26741e-07
Iter: 1242 loss: 8.26017413e-07
Iter: 1243 loss: 8.26033e-07
Iter: 1244 loss: 8.25148788e-07
Iter: 1245 loss: 8.25871325e-07
Iter: 1246 loss: 8.24653796e-07
Iter: 1247 loss: 8.24488154e-07
Iter: 1248 loss: 8.24194558e-07
Iter: 1249 loss: 8.23824109e-07
Iter: 1250 loss: 8.23027e-07
Iter: 1251 loss: 8.37184416e-07
Iter: 1252 loss: 8.23012613e-07
Iter: 1253 loss: 8.2227325e-07
Iter: 1254 loss: 8.24060635e-07
Iter: 1255 loss: 8.21958e-07
Iter: 1256 loss: 8.21241e-07
Iter: 1257 loss: 8.23245045e-07
Iter: 1258 loss: 8.20981086e-07
Iter: 1259 loss: 8.20372179e-07
Iter: 1260 loss: 8.27925589e-07
Iter: 1261 loss: 8.20342052e-07
Iter: 1262 loss: 8.19864852e-07
Iter: 1263 loss: 8.21054527e-07
Iter: 1264 loss: 8.1971541e-07
Iter: 1265 loss: 8.19186766e-07
Iter: 1266 loss: 8.22386937e-07
Iter: 1267 loss: 8.19128218e-07
Iter: 1268 loss: 8.18666479e-07
Iter: 1269 loss: 8.19002253e-07
Iter: 1270 loss: 8.18426713e-07
Iter: 1271 loss: 8.1789284e-07
Iter: 1272 loss: 8.17621697e-07
Iter: 1273 loss: 8.17387672e-07
Iter: 1274 loss: 8.16786269e-07
Iter: 1275 loss: 8.25253835e-07
Iter: 1276 loss: 8.1677581e-07
Iter: 1277 loss: 8.1630003e-07
Iter: 1278 loss: 8.16307761e-07
Iter: 1279 loss: 8.15964825e-07
Iter: 1280 loss: 8.15399289e-07
Iter: 1281 loss: 8.15008207e-07
Iter: 1282 loss: 8.1480448e-07
Iter: 1283 loss: 8.14150951e-07
Iter: 1284 loss: 8.23571895e-07
Iter: 1285 loss: 8.14134239e-07
Iter: 1286 loss: 8.13473378e-07
Iter: 1287 loss: 8.15034639e-07
Iter: 1288 loss: 8.13218e-07
Iter: 1289 loss: 8.12836106e-07
Iter: 1290 loss: 8.11856921e-07
Iter: 1291 loss: 8.23599748e-07
Iter: 1292 loss: 8.11781433e-07
Iter: 1293 loss: 8.10908773e-07
Iter: 1294 loss: 8.22378297e-07
Iter: 1295 loss: 8.10910365e-07
Iter: 1296 loss: 8.10244273e-07
Iter: 1297 loss: 8.13982695e-07
Iter: 1298 loss: 8.10194194e-07
Iter: 1299 loss: 8.09569599e-07
Iter: 1300 loss: 8.129457e-07
Iter: 1301 loss: 8.09476717e-07
Iter: 1302 loss: 8.08999403e-07
Iter: 1303 loss: 8.11894665e-07
Iter: 1304 loss: 8.0894165e-07
Iter: 1305 loss: 8.08546417e-07
Iter: 1306 loss: 8.08074731e-07
Iter: 1307 loss: 8.08027949e-07
Iter: 1308 loss: 8.07473668e-07
Iter: 1309 loss: 8.10034919e-07
Iter: 1310 loss: 8.07324852e-07
Iter: 1311 loss: 8.06802404e-07
Iter: 1312 loss: 8.08302843e-07
Iter: 1313 loss: 8.06611638e-07
Iter: 1314 loss: 8.06014782e-07
Iter: 1315 loss: 8.08222921e-07
Iter: 1316 loss: 8.05890238e-07
Iter: 1317 loss: 8.05502339e-07
Iter: 1318 loss: 8.05103696e-07
Iter: 1319 loss: 8.05015361e-07
Iter: 1320 loss: 8.04328863e-07
Iter: 1321 loss: 8.08425568e-07
Iter: 1322 loss: 8.04287936e-07
Iter: 1323 loss: 8.03566081e-07
Iter: 1324 loss: 8.07751917e-07
Iter: 1325 loss: 8.0348542e-07
Iter: 1326 loss: 8.03085413e-07
Iter: 1327 loss: 8.02488159e-07
Iter: 1328 loss: 8.02493e-07
Iter: 1329 loss: 8.01682518e-07
Iter: 1330 loss: 8.02744751e-07
Iter: 1331 loss: 8.01281431e-07
Iter: 1332 loss: 8.00413e-07
Iter: 1333 loss: 8.08024538e-07
Iter: 1334 loss: 8.00360624e-07
Iter: 1335 loss: 7.9996596e-07
Iter: 1336 loss: 7.99929353e-07
Iter: 1337 loss: 7.99630868e-07
Iter: 1338 loss: 7.99669124e-07
Iter: 1339 loss: 7.99430154e-07
Iter: 1340 loss: 7.98941e-07
Iter: 1341 loss: 7.98851033e-07
Iter: 1342 loss: 7.98528845e-07
Iter: 1343 loss: 7.98021745e-07
Iter: 1344 loss: 7.98888891e-07
Iter: 1345 loss: 7.97816597e-07
Iter: 1346 loss: 7.97238386e-07
Iter: 1347 loss: 8.01474471e-07
Iter: 1348 loss: 7.97178586e-07
Iter: 1349 loss: 7.96689619e-07
Iter: 1350 loss: 7.96887207e-07
Iter: 1351 loss: 7.96353675e-07
Iter: 1352 loss: 7.9568872e-07
Iter: 1353 loss: 7.96943141e-07
Iter: 1354 loss: 7.95409392e-07
Iter: 1355 loss: 7.94913035e-07
Iter: 1356 loss: 7.96955646e-07
Iter: 1357 loss: 7.94763366e-07
Iter: 1358 loss: 7.94270136e-07
Iter: 1359 loss: 7.98378494e-07
Iter: 1360 loss: 7.94211132e-07
Iter: 1361 loss: 7.93784693e-07
Iter: 1362 loss: 7.93035554e-07
Iter: 1363 loss: 7.93034303e-07
Iter: 1364 loss: 7.92323135e-07
Iter: 1365 loss: 7.92658454e-07
Iter: 1366 loss: 7.91888056e-07
Iter: 1367 loss: 7.91171374e-07
Iter: 1368 loss: 7.98819315e-07
Iter: 1369 loss: 7.91126581e-07
Iter: 1370 loss: 7.90508921e-07
Iter: 1371 loss: 7.98011797e-07
Iter: 1372 loss: 7.90496699e-07
Iter: 1373 loss: 7.90052411e-07
Iter: 1374 loss: 7.90002844e-07
Iter: 1375 loss: 7.897562e-07
Iter: 1376 loss: 7.89255751e-07
Iter: 1377 loss: 7.91214916e-07
Iter: 1378 loss: 7.89147066e-07
Iter: 1379 loss: 7.88706416e-07
Iter: 1380 loss: 7.88203e-07
Iter: 1381 loss: 7.88143382e-07
Iter: 1382 loss: 7.87737747e-07
Iter: 1383 loss: 7.87706881e-07
Iter: 1384 loss: 7.87326485e-07
Iter: 1385 loss: 7.86884357e-07
Iter: 1386 loss: 7.86836381e-07
Iter: 1387 loss: 7.86284659e-07
Iter: 1388 loss: 7.89109322e-07
Iter: 1389 loss: 7.86183705e-07
Iter: 1390 loss: 7.85648808e-07
Iter: 1391 loss: 7.86318253e-07
Iter: 1392 loss: 7.85339694e-07
Iter: 1393 loss: 7.8483373e-07
Iter: 1394 loss: 7.8484004e-07
Iter: 1395 loss: 7.84510689e-07
Iter: 1396 loss: 7.83853409e-07
Iter: 1397 loss: 7.93594722e-07
Iter: 1398 loss: 7.83784e-07
Iter: 1399 loss: 7.82967959e-07
Iter: 1400 loss: 7.85735097e-07
Iter: 1401 loss: 7.82706593e-07
Iter: 1402 loss: 7.82174652e-07
Iter: 1403 loss: 7.87379577e-07
Iter: 1404 loss: 7.82144298e-07
Iter: 1405 loss: 7.81599738e-07
Iter: 1406 loss: 7.84333793e-07
Iter: 1407 loss: 7.81487188e-07
Iter: 1408 loss: 7.81138e-07
Iter: 1409 loss: 7.8113294e-07
Iter: 1410 loss: 7.80880498e-07
Iter: 1411 loss: 7.80298308e-07
Iter: 1412 loss: 7.81817846e-07
Iter: 1413 loss: 7.80122832e-07
Iter: 1414 loss: 7.79649838e-07
Iter: 1415 loss: 7.80188884e-07
Iter: 1416 loss: 7.79450431e-07
Iter: 1417 loss: 7.79034337e-07
Iter: 1418 loss: 7.83385701e-07
Iter: 1419 loss: 7.79033428e-07
Iter: 1420 loss: 7.78657238e-07
Iter: 1421 loss: 7.78139679e-07
Iter: 1422 loss: 7.78106e-07
Iter: 1423 loss: 7.77532762e-07
Iter: 1424 loss: 7.79956736e-07
Iter: 1425 loss: 7.77350863e-07
Iter: 1426 loss: 7.76803063e-07
Iter: 1427 loss: 7.79637617e-07
Iter: 1428 loss: 7.76734851e-07
Iter: 1429 loss: 7.76212232e-07
Iter: 1430 loss: 7.78138144e-07
Iter: 1431 loss: 7.76071829e-07
Iter: 1432 loss: 7.75605315e-07
Iter: 1433 loss: 7.75510216e-07
Iter: 1434 loss: 7.75179899e-07
Iter: 1435 loss: 7.74608452e-07
Iter: 1436 loss: 7.75536591e-07
Iter: 1437 loss: 7.74348166e-07
Iter: 1438 loss: 7.73979878e-07
Iter: 1439 loss: 7.73982606e-07
Iter: 1440 loss: 7.73565603e-07
Iter: 1441 loss: 7.72928729e-07
Iter: 1442 loss: 7.72922363e-07
Iter: 1443 loss: 7.72329486e-07
Iter: 1444 loss: 7.76375146e-07
Iter: 1445 loss: 7.72279691e-07
Iter: 1446 loss: 7.71787484e-07
Iter: 1447 loss: 7.72577948e-07
Iter: 1448 loss: 7.7154607e-07
Iter: 1449 loss: 7.71094051e-07
Iter: 1450 loss: 7.7144324e-07
Iter: 1451 loss: 7.70825466e-07
Iter: 1452 loss: 7.70278916e-07
Iter: 1453 loss: 7.76384184e-07
Iter: 1454 loss: 7.70277722e-07
Iter: 1455 loss: 7.69897554e-07
Iter: 1456 loss: 7.69380335e-07
Iter: 1457 loss: 7.69368683e-07
Iter: 1458 loss: 7.68717655e-07
Iter: 1459 loss: 7.72524686e-07
Iter: 1460 loss: 7.68649159e-07
Iter: 1461 loss: 7.68080213e-07
Iter: 1462 loss: 7.71535667e-07
Iter: 1463 loss: 7.68013308e-07
Iter: 1464 loss: 7.67641382e-07
Iter: 1465 loss: 7.68890118e-07
Iter: 1466 loss: 7.67514393e-07
Iter: 1467 loss: 7.67159236e-07
Iter: 1468 loss: 7.66747462e-07
Iter: 1469 loss: 7.66686469e-07
Iter: 1470 loss: 7.66125e-07
Iter: 1471 loss: 7.70920167e-07
Iter: 1472 loss: 7.66094104e-07
Iter: 1473 loss: 7.65669142e-07
Iter: 1474 loss: 7.70822908e-07
Iter: 1475 loss: 7.65648167e-07
Iter: 1476 loss: 7.6539186e-07
Iter: 1477 loss: 7.64788183e-07
Iter: 1478 loss: 7.72635303e-07
Iter: 1479 loss: 7.64743561e-07
Iter: 1480 loss: 7.642642e-07
Iter: 1481 loss: 7.64259653e-07
Iter: 1482 loss: 7.63877438e-07
Iter: 1483 loss: 7.63451851e-07
Iter: 1484 loss: 7.63423714e-07
Iter: 1485 loss: 7.62877335e-07
Iter: 1486 loss: 7.68192706e-07
Iter: 1487 loss: 7.62882223e-07
Iter: 1488 loss: 7.62399054e-07
Iter: 1489 loss: 7.63078788e-07
Iter: 1490 loss: 7.62156276e-07
Iter: 1491 loss: 7.61695162e-07
Iter: 1492 loss: 7.62022864e-07
Iter: 1493 loss: 7.61449201e-07
Iter: 1494 loss: 7.61108595e-07
Iter: 1495 loss: 7.61107458e-07
Iter: 1496 loss: 7.60792148e-07
Iter: 1497 loss: 7.6068261e-07
Iter: 1498 loss: 7.6050344e-07
Iter: 1499 loss: 7.59981276e-07
Iter: 1500 loss: 7.61384968e-07
Iter: 1501 loss: 7.59850082e-07
Iter: 1502 loss: 7.59451041e-07
Iter: 1503 loss: 7.59616967e-07
Iter: 1504 loss: 7.59178e-07
Iter: 1505 loss: 7.58902331e-07
Iter: 1506 loss: 7.58863166e-07
Iter: 1507 loss: 7.58593501e-07
Iter: 1508 loss: 7.58193096e-07
Iter: 1509 loss: 7.58177805e-07
Iter: 1510 loss: 7.5768952e-07
Iter: 1511 loss: 7.58053602e-07
Iter: 1512 loss: 7.57384214e-07
Iter: 1513 loss: 7.56782583e-07
Iter: 1514 loss: 7.62233e-07
Iter: 1515 loss: 7.5676445e-07
Iter: 1516 loss: 7.56440386e-07
Iter: 1517 loss: 7.56117117e-07
Iter: 1518 loss: 7.56053169e-07
Iter: 1519 loss: 7.55575854e-07
Iter: 1520 loss: 7.62003765e-07
Iter: 1521 loss: 7.5557881e-07
Iter: 1522 loss: 7.55165956e-07
Iter: 1523 loss: 7.54975531e-07
Iter: 1524 loss: 7.5479e-07
Iter: 1525 loss: 7.54254756e-07
Iter: 1526 loss: 7.54866164e-07
Iter: 1527 loss: 7.53991287e-07
Iter: 1528 loss: 7.53470658e-07
Iter: 1529 loss: 7.53485722e-07
Iter: 1530 loss: 7.53149152e-07
Iter: 1531 loss: 7.53121526e-07
Iter: 1532 loss: 7.52864707e-07
Iter: 1533 loss: 7.5238205e-07
Iter: 1534 loss: 7.53071674e-07
Iter: 1535 loss: 7.5215678e-07
Iter: 1536 loss: 7.51683046e-07
Iter: 1537 loss: 7.53740267e-07
Iter: 1538 loss: 7.51585617e-07
Iter: 1539 loss: 7.5104424e-07
Iter: 1540 loss: 7.54719053e-07
Iter: 1541 loss: 7.51033e-07
Iter: 1542 loss: 7.50706931e-07
Iter: 1543 loss: 7.5027674e-07
Iter: 1544 loss: 7.50246159e-07
Iter: 1545 loss: 7.49828928e-07
Iter: 1546 loss: 7.53963945e-07
Iter: 1547 loss: 7.49823812e-07
Iter: 1548 loss: 7.49415619e-07
Iter: 1549 loss: 7.49165e-07
Iter: 1550 loss: 7.48979517e-07
Iter: 1551 loss: 7.48532102e-07
Iter: 1552 loss: 7.51875746e-07
Iter: 1553 loss: 7.48478271e-07
Iter: 1554 loss: 7.48073376e-07
Iter: 1555 loss: 7.49471837e-07
Iter: 1556 loss: 7.47941385e-07
Iter: 1557 loss: 7.47569061e-07
Iter: 1558 loss: 7.47227773e-07
Iter: 1559 loss: 7.47130741e-07
Iter: 1560 loss: 7.46597379e-07
Iter: 1561 loss: 7.49131971e-07
Iter: 1562 loss: 7.46492674e-07
Iter: 1563 loss: 7.4586103e-07
Iter: 1564 loss: 7.48496063e-07
Iter: 1565 loss: 7.45670491e-07
Iter: 1566 loss: 7.45241e-07
Iter: 1567 loss: 7.45493e-07
Iter: 1568 loss: 7.44964836e-07
Iter: 1569 loss: 7.44405781e-07
Iter: 1570 loss: 7.45449086e-07
Iter: 1571 loss: 7.44163174e-07
Iter: 1572 loss: 7.43827457e-07
Iter: 1573 loss: 7.43820863e-07
Iter: 1574 loss: 7.43492e-07
Iter: 1575 loss: 7.434802e-07
Iter: 1576 loss: 7.43213548e-07
Iter: 1577 loss: 7.42862312e-07
Iter: 1578 loss: 7.42682971e-07
Iter: 1579 loss: 7.42492375e-07
Iter: 1580 loss: 7.42005852e-07
Iter: 1581 loss: 7.46525757e-07
Iter: 1582 loss: 7.41981921e-07
Iter: 1583 loss: 7.41583449e-07
Iter: 1584 loss: 7.41416898e-07
Iter: 1585 loss: 7.41184863e-07
Iter: 1586 loss: 7.40732844e-07
Iter: 1587 loss: 7.44779072e-07
Iter: 1588 loss: 7.4070482e-07
Iter: 1589 loss: 7.40256894e-07
Iter: 1590 loss: 7.40611256e-07
Iter: 1591 loss: 7.39986035e-07
Iter: 1592 loss: 7.39562e-07
Iter: 1593 loss: 7.3955664e-07
Iter: 1594 loss: 7.39175107e-07
Iter: 1595 loss: 7.38705126e-07
Iter: 1596 loss: 7.45475802e-07
Iter: 1597 loss: 7.38719052e-07
Iter: 1598 loss: 7.38281528e-07
Iter: 1599 loss: 7.3886639e-07
Iter: 1600 loss: 7.38043184e-07
Iter: 1601 loss: 7.37650794e-07
Iter: 1602 loss: 7.37185246e-07
Iter: 1603 loss: 7.37118171e-07
Iter: 1604 loss: 7.36451625e-07
Iter: 1605 loss: 7.43252826e-07
Iter: 1606 loss: 7.36421839e-07
Iter: 1607 loss: 7.3616593e-07
Iter: 1608 loss: 7.36152856e-07
Iter: 1609 loss: 7.35938443e-07
Iter: 1610 loss: 7.35524679e-07
Iter: 1611 loss: 7.425636e-07
Iter: 1612 loss: 7.35493472e-07
Iter: 1613 loss: 7.35090794e-07
Iter: 1614 loss: 7.36188554e-07
Iter: 1615 loss: 7.34928335e-07
Iter: 1616 loss: 7.34445166e-07
Iter: 1617 loss: 7.37335313e-07
Iter: 1618 loss: 7.34384685e-07
Iter: 1619 loss: 7.33927209e-07
Iter: 1620 loss: 7.33945512e-07
Iter: 1621 loss: 7.33555339e-07
Iter: 1622 loss: 7.33093259e-07
Iter: 1623 loss: 7.37092535e-07
Iter: 1624 loss: 7.33060574e-07
Iter: 1625 loss: 7.32537103e-07
Iter: 1626 loss: 7.32064905e-07
Iter: 1627 loss: 7.31915179e-07
Iter: 1628 loss: 7.31379146e-07
Iter: 1629 loss: 7.34055902e-07
Iter: 1630 loss: 7.31291777e-07
Iter: 1631 loss: 7.30841123e-07
Iter: 1632 loss: 7.35181061e-07
Iter: 1633 loss: 7.30829868e-07
Iter: 1634 loss: 7.30447709e-07
Iter: 1635 loss: 7.30341696e-07
Iter: 1636 loss: 7.301187e-07
Iter: 1637 loss: 7.29683734e-07
Iter: 1638 loss: 7.29974e-07
Iter: 1639 loss: 7.29440615e-07
Iter: 1640 loss: 7.29004e-07
Iter: 1641 loss: 7.35626486e-07
Iter: 1642 loss: 7.28992177e-07
Iter: 1643 loss: 7.28663792e-07
Iter: 1644 loss: 7.30203e-07
Iter: 1645 loss: 7.28581426e-07
Iter: 1646 loss: 7.28264638e-07
Iter: 1647 loss: 7.275803e-07
Iter: 1648 loss: 7.37832465e-07
Iter: 1649 loss: 7.27525162e-07
Iter: 1650 loss: 7.27044494e-07
Iter: 1651 loss: 7.33987349e-07
Iter: 1652 loss: 7.27050065e-07
Iter: 1653 loss: 7.26621238e-07
Iter: 1654 loss: 7.27711154e-07
Iter: 1655 loss: 7.26514372e-07
Iter: 1656 loss: 7.26031772e-07
Iter: 1657 loss: 7.26317808e-07
Iter: 1658 loss: 7.25756649e-07
Iter: 1659 loss: 7.2538603e-07
Iter: 1660 loss: 7.25389555e-07
Iter: 1661 loss: 7.25147856e-07
Iter: 1662 loss: 7.24652523e-07
Iter: 1663 loss: 7.34737e-07
Iter: 1664 loss: 7.24662641e-07
Iter: 1665 loss: 7.24181518e-07
Iter: 1666 loss: 7.28562839e-07
Iter: 1667 loss: 7.24151107e-07
Iter: 1668 loss: 7.23653443e-07
Iter: 1669 loss: 7.25323162e-07
Iter: 1670 loss: 7.23520884e-07
Iter: 1671 loss: 7.2312497e-07
Iter: 1672 loss: 7.2276066e-07
Iter: 1673 loss: 7.22662776e-07
Iter: 1674 loss: 7.22015216e-07
Iter: 1675 loss: 7.24380129e-07
Iter: 1676 loss: 7.21870208e-07
Iter: 1677 loss: 7.21509593e-07
Iter: 1678 loss: 7.21512265e-07
Iter: 1679 loss: 7.21222534e-07
Iter: 1680 loss: 7.21267497e-07
Iter: 1681 loss: 7.21024435e-07
Iter: 1682 loss: 7.20720664e-07
Iter: 1683 loss: 7.2032924e-07
Iter: 1684 loss: 7.20319917e-07
Iter: 1685 loss: 7.19672869e-07
Iter: 1686 loss: 7.23011226e-07
Iter: 1687 loss: 7.19593118e-07
Iter: 1688 loss: 7.19080788e-07
Iter: 1689 loss: 7.22568927e-07
Iter: 1690 loss: 7.18995238e-07
Iter: 1691 loss: 7.18643491e-07
Iter: 1692 loss: 7.18759395e-07
Iter: 1693 loss: 7.18410888e-07
Iter: 1694 loss: 7.17890202e-07
Iter: 1695 loss: 7.21242372e-07
Iter: 1696 loss: 7.17828812e-07
Iter: 1697 loss: 7.17470812e-07
Iter: 1698 loss: 7.16886234e-07
Iter: 1699 loss: 7.16881e-07
Iter: 1700 loss: 7.16448881e-07
Iter: 1701 loss: 7.16426143e-07
Iter: 1702 loss: 7.16041e-07
Iter: 1703 loss: 7.1634247e-07
Iter: 1704 loss: 7.15821557e-07
Iter: 1705 loss: 7.15396368e-07
Iter: 1706 loss: 7.1484385e-07
Iter: 1707 loss: 7.14813041e-07
Iter: 1708 loss: 7.14621137e-07
Iter: 1709 loss: 7.14493524e-07
Iter: 1710 loss: 7.14254725e-07
Iter: 1711 loss: 7.15319857e-07
Iter: 1712 loss: 7.14169e-07
Iter: 1713 loss: 7.13915369e-07
Iter: 1714 loss: 7.13627742e-07
Iter: 1715 loss: 7.13630357e-07
Iter: 1716 loss: 7.13229838e-07
Iter: 1717 loss: 7.13796851e-07
Iter: 1718 loss: 7.13051236e-07
Iter: 1719 loss: 7.12566816e-07
Iter: 1720 loss: 7.15004035e-07
Iter: 1721 loss: 7.12476208e-07
Iter: 1722 loss: 7.11986445e-07
Iter: 1723 loss: 7.12966653e-07
Iter: 1724 loss: 7.11786868e-07
Iter: 1725 loss: 7.11337407e-07
Iter: 1726 loss: 7.12319093e-07
Iter: 1727 loss: 7.1114647e-07
Iter: 1728 loss: 7.1060839e-07
Iter: 1729 loss: 7.13115583e-07
Iter: 1730 loss: 7.10514087e-07
Iter: 1731 loss: 7.10156087e-07
Iter: 1732 loss: 7.0972942e-07
Iter: 1733 loss: 7.09717938e-07
Iter: 1734 loss: 7.09316282e-07
Iter: 1735 loss: 7.09272911e-07
Iter: 1736 loss: 7.0896931e-07
Iter: 1737 loss: 7.08648258e-07
Iter: 1738 loss: 7.08585333e-07
Iter: 1739 loss: 7.08110861e-07
Iter: 1740 loss: 7.0816742e-07
Iter: 1741 loss: 7.0777e-07
Iter: 1742 loss: 7.07597e-07
Iter: 1743 loss: 7.07481036e-07
Iter: 1744 loss: 7.07218703e-07
Iter: 1745 loss: 7.07065681e-07
Iter: 1746 loss: 7.06951937e-07
Iter: 1747 loss: 7.06602805e-07
Iter: 1748 loss: 7.06865592e-07
Iter: 1749 loss: 7.06396804e-07
Iter: 1750 loss: 7.0597639e-07
Iter: 1751 loss: 7.06195237e-07
Iter: 1752 loss: 7.05710306e-07
Iter: 1753 loss: 7.05289438e-07
Iter: 1754 loss: 7.10741915e-07
Iter: 1755 loss: 7.05292e-07
Iter: 1756 loss: 7.04960144e-07
Iter: 1757 loss: 7.05224465e-07
Iter: 1758 loss: 7.04745787e-07
Iter: 1759 loss: 7.04356921e-07
Iter: 1760 loss: 7.05626462e-07
Iter: 1761 loss: 7.04265517e-07
Iter: 1762 loss: 7.03812361e-07
Iter: 1763 loss: 7.04323952e-07
Iter: 1764 loss: 7.03586579e-07
Iter: 1765 loss: 7.03061289e-07
Iter: 1766 loss: 7.03090677e-07
Iter: 1767 loss: 7.02637067e-07
Iter: 1768 loss: 7.02380817e-07
Iter: 1769 loss: 7.02328066e-07
Iter: 1770 loss: 7.02013324e-07
Iter: 1771 loss: 7.0138492e-07
Iter: 1772 loss: 7.10990548e-07
Iter: 1773 loss: 7.01364456e-07
Iter: 1774 loss: 7.00865144e-07
Iter: 1775 loss: 7.04724e-07
Iter: 1776 loss: 7.00803298e-07
Iter: 1777 loss: 7.00468036e-07
Iter: 1778 loss: 7.00459282e-07
Iter: 1779 loss: 7.00198711e-07
Iter: 1780 loss: 6.99751126e-07
Iter: 1781 loss: 6.99741918e-07
Iter: 1782 loss: 6.99326165e-07
Iter: 1783 loss: 7.00614e-07
Iter: 1784 loss: 6.99203667e-07
Iter: 1785 loss: 6.98766826e-07
Iter: 1786 loss: 6.99080886e-07
Iter: 1787 loss: 6.9846476e-07
Iter: 1788 loss: 6.97997621e-07
Iter: 1789 loss: 7.03356193e-07
Iter: 1790 loss: 6.9799637e-07
Iter: 1791 loss: 6.97606367e-07
Iter: 1792 loss: 6.97542418e-07
Iter: 1793 loss: 6.97282417e-07
Iter: 1794 loss: 6.96875361e-07
Iter: 1795 loss: 7.01573924e-07
Iter: 1796 loss: 6.96874054e-07
Iter: 1797 loss: 6.96547659e-07
Iter: 1798 loss: 6.96307211e-07
Iter: 1799 loss: 6.96216205e-07
Iter: 1800 loss: 6.95667381e-07
Iter: 1801 loss: 6.96889742e-07
Iter: 1802 loss: 6.95494577e-07
Iter: 1803 loss: 6.95181143e-07
Iter: 1804 loss: 6.95170343e-07
Iter: 1805 loss: 6.94939e-07
Iter: 1806 loss: 6.94448488e-07
Iter: 1807 loss: 7.00690691e-07
Iter: 1808 loss: 6.94424955e-07
Iter: 1809 loss: 6.94048e-07
Iter: 1810 loss: 6.94059622e-07
Iter: 1811 loss: 6.93685593e-07
Iter: 1812 loss: 6.948286e-07
Iter: 1813 loss: 6.93587594e-07
Iter: 1814 loss: 6.93258642e-07
Iter: 1815 loss: 6.92904564e-07
Iter: 1816 loss: 6.92864546e-07
Iter: 1817 loss: 6.9237592e-07
Iter: 1818 loss: 6.93655124e-07
Iter: 1819 loss: 6.92219942e-07
Iter: 1820 loss: 6.91595574e-07
Iter: 1821 loss: 6.92943331e-07
Iter: 1822 loss: 6.91412e-07
Iter: 1823 loss: 6.90948809e-07
Iter: 1824 loss: 6.97244218e-07
Iter: 1825 loss: 6.90967113e-07
Iter: 1826 loss: 6.90667434e-07
Iter: 1827 loss: 6.90687841e-07
Iter: 1828 loss: 6.90428806e-07
Iter: 1829 loss: 6.9004227e-07
Iter: 1830 loss: 6.91990181e-07
Iter: 1831 loss: 6.89968772e-07
Iter: 1832 loss: 6.89645503e-07
Iter: 1833 loss: 6.89646527e-07
Iter: 1834 loss: 6.89388798e-07
Iter: 1835 loss: 6.8900107e-07
Iter: 1836 loss: 6.91264631e-07
Iter: 1837 loss: 6.88964406e-07
Iter: 1838 loss: 6.88570537e-07
Iter: 1839 loss: 6.89668468e-07
Iter: 1840 loss: 6.88432692e-07
Iter: 1841 loss: 6.88084242e-07
Iter: 1842 loss: 6.87677414e-07
Iter: 1843 loss: 6.8764831e-07
Iter: 1844 loss: 6.87449869e-07
Iter: 1845 loss: 6.87346073e-07
Iter: 1846 loss: 6.87075215e-07
Iter: 1847 loss: 6.86784801e-07
Iter: 1848 loss: 6.86725571e-07
Iter: 1849 loss: 6.86389058e-07
Iter: 1850 loss: 6.86433509e-07
Iter: 1851 loss: 6.86096e-07
Iter: 1852 loss: 6.85667374e-07
Iter: 1853 loss: 6.88877776e-07
Iter: 1854 loss: 6.85658449e-07
Iter: 1855 loss: 6.85305963e-07
Iter: 1856 loss: 6.86434248e-07
Iter: 1857 loss: 6.85228201e-07
Iter: 1858 loss: 6.848922e-07
Iter: 1859 loss: 6.85737689e-07
Iter: 1860 loss: 6.84741678e-07
Iter: 1861 loss: 6.84414886e-07
Iter: 1862 loss: 6.84757254e-07
Iter: 1863 loss: 6.84199108e-07
Iter: 1864 loss: 6.83722419e-07
Iter: 1865 loss: 6.8581835e-07
Iter: 1866 loss: 6.83635562e-07
Iter: 1867 loss: 6.83225892e-07
Iter: 1868 loss: 6.83317353e-07
Iter: 1869 loss: 6.82966686e-07
Iter: 1870 loss: 6.8260988e-07
Iter: 1871 loss: 6.86850171e-07
Iter: 1872 loss: 6.82598568e-07
Iter: 1873 loss: 6.82219763e-07
Iter: 1874 loss: 6.82229881e-07
Iter: 1875 loss: 6.81950723e-07
Iter: 1876 loss: 6.81523716e-07
Iter: 1877 loss: 6.81522181e-07
Iter: 1878 loss: 6.81185e-07
Iter: 1879 loss: 6.81146162e-07
Iter: 1880 loss: 6.80944879e-07
Iter: 1881 loss: 6.80769745e-07
Iter: 1882 loss: 6.80305675e-07
Iter: 1883 loss: 6.85114401e-07
Iter: 1884 loss: 6.80241214e-07
Iter: 1885 loss: 6.79689833e-07
Iter: 1886 loss: 6.80386165e-07
Iter: 1887 loss: 6.79379355e-07
Iter: 1888 loss: 6.78872539e-07
Iter: 1889 loss: 6.82225959e-07
Iter: 1890 loss: 6.78814445e-07
Iter: 1891 loss: 6.78291542e-07
Iter: 1892 loss: 6.80285439e-07
Iter: 1893 loss: 6.78193601e-07
Iter: 1894 loss: 6.77780577e-07
Iter: 1895 loss: 6.79692107e-07
Iter: 1896 loss: 6.77699745e-07
Iter: 1897 loss: 6.77320941e-07
Iter: 1898 loss: 6.77361527e-07
Iter: 1899 loss: 6.77015919e-07
Iter: 1900 loss: 6.76613922e-07
Iter: 1901 loss: 6.81440042e-07
Iter: 1902 loss: 6.76628702e-07
Iter: 1903 loss: 6.7639138e-07
Iter: 1904 loss: 6.76146726e-07
Iter: 1905 loss: 6.76068282e-07
Iter: 1906 loss: 6.75648721e-07
Iter: 1907 loss: 6.79470475e-07
Iter: 1908 loss: 6.75623369e-07
Iter: 1909 loss: 6.75237743e-07
Iter: 1910 loss: 6.75879392e-07
Iter: 1911 loss: 6.75078923e-07
Iter: 1912 loss: 6.74786463e-07
Iter: 1913 loss: 6.74409876e-07
Iter: 1914 loss: 6.74389071e-07
Iter: 1915 loss: 6.73946204e-07
Iter: 1916 loss: 6.73924319e-07
Iter: 1917 loss: 6.73714851e-07
Iter: 1918 loss: 6.73217414e-07
Iter: 1919 loss: 6.80251333e-07
Iter: 1920 loss: 6.73190925e-07
Iter: 1921 loss: 6.72654778e-07
Iter: 1922 loss: 6.73745e-07
Iter: 1923 loss: 6.72442638e-07
Iter: 1924 loss: 6.71951966e-07
Iter: 1925 loss: 6.75551519e-07
Iter: 1926 loss: 6.71906605e-07
Iter: 1927 loss: 6.71449584e-07
Iter: 1928 loss: 6.7289352e-07
Iter: 1929 loss: 6.71280873e-07
Iter: 1930 loss: 6.70952318e-07
Iter: 1931 loss: 6.71749262e-07
Iter: 1932 loss: 6.70811119e-07
Iter: 1933 loss: 6.70407132e-07
Iter: 1934 loss: 6.70402869e-07
Iter: 1935 loss: 6.7009546e-07
Iter: 1936 loss: 6.69501446e-07
Iter: 1937 loss: 6.74952332e-07
Iter: 1938 loss: 6.69485303e-07
Iter: 1939 loss: 6.69193696e-07
Iter: 1940 loss: 6.69207679e-07
Iter: 1941 loss: 6.68951429e-07
Iter: 1942 loss: 6.68557107e-07
Iter: 1943 loss: 6.72157853e-07
Iter: 1944 loss: 6.68544772e-07
Iter: 1945 loss: 6.68204848e-07
Iter: 1946 loss: 6.68029656e-07
Iter: 1947 loss: 6.67870381e-07
Iter: 1948 loss: 6.67502604e-07
Iter: 1949 loss: 6.69005658e-07
Iter: 1950 loss: 6.67437462e-07
Iter: 1951 loss: 6.6695452e-07
Iter: 1952 loss: 6.69996894e-07
Iter: 1953 loss: 6.66905464e-07
Iter: 1954 loss: 6.66689061e-07
Iter: 1955 loss: 6.66197934e-07
Iter: 1956 loss: 6.73685804e-07
Iter: 1957 loss: 6.66185883e-07
Iter: 1958 loss: 6.65568678e-07
Iter: 1959 loss: 6.67181325e-07
Iter: 1960 loss: 6.65328912e-07
Iter: 1961 loss: 6.64892923e-07
Iter: 1962 loss: 6.67654717e-07
Iter: 1963 loss: 6.64847903e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2
+ date
Wed Oct 21 12:31:30 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/300_300_300_1 --function f1 --psi -2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589fae59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f962488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589fa31268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589fa31f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f99e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f898598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f8456a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f911bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f9118c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f8dff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f8ef400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f829730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f829bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f829400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f75ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f829620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f7e11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899d3bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f8292f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899d58f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f799a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f77d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899cd56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899c8e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899c8e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899cc17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899c8e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899c35950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899c35510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899c84378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899ad7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899b8f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899b8f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899bbe1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899b76950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899b1e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.050043546
test_loss: 0.0499618
train_loss: 0.0262179
test_loss: 0.025693156
train_loss: 0.019542407
test_loss: 0.017153224
train_loss: 0.013030596
test_loss: 0.013743249
train_loss: 0.010006244
test_loss: 0.012531781
train_loss: 0.008884949
test_loss: 0.011568857
train_loss: 0.014940969
test_loss: 0.011783932
train_loss: 0.010879952
test_loss: 0.011501472
train_loss: 0.010051995
test_loss: 0.01064819
train_loss: 0.0088078305
test_loss: 0.011446695
train_loss: 0.008236829
test_loss: 0.011172807
train_loss: 0.008384081
test_loss: 0.010574526
train_loss: 0.008493668
test_loss: 0.010042575
train_loss: 0.010806862
test_loss: 0.010276284
train_loss: 0.008018864
test_loss: 0.009466721
train_loss: 0.008145303
test_loss: 0.01003703
train_loss: 0.0073255715
test_loss: 0.009675553
train_loss: 0.008465951
test_loss: 0.009824849
train_loss: 0.0070811315
test_loss: 0.009714718
train_loss: 0.007447865
test_loss: 0.010090827
train_loss: 0.0068651084
test_loss: 0.010068132
train_loss: 0.007595261
test_loss: 0.010153638
train_loss: 0.007680636
test_loss: 0.009500552
train_loss: 0.0065270206
test_loss: 0.009314965
train_loss: 0.0068450714
test_loss: 0.00989657
train_loss: 0.0075542293
test_loss: 0.009743773
train_loss: 0.009740478
test_loss: 0.011873118
train_loss: 0.0074697705
test_loss: 0.010086211
train_loss: 0.0066032032
test_loss: 0.009435877
train_loss: 0.0074260016
test_loss: 0.008859606
train_loss: 0.006246
test_loss: 0.009364789
train_loss: 0.0061869277
test_loss: 0.00903333
train_loss: 0.0060102185
test_loss: 0.008623478
train_loss: 0.005996617
test_loss: 0.009073707
train_loss: 0.009678266
test_loss: 0.009247443
train_loss: 0.007051659
test_loss: 0.009914247
train_loss: 0.009676376
test_loss: 0.008895598
train_loss: 0.006383393
test_loss: 0.009101132
train_loss: 0.0064155855
test_loss: 0.009092918
train_loss: 0.0062415544
test_loss: 0.009149107
train_loss: 0.006745995
test_loss: 0.00980623
train_loss: 0.0061894087
test_loss: 0.009367041
train_loss: 0.0064885225
test_loss: 0.009584311
train_loss: 0.010945379
test_loss: 0.013904575
train_loss: 0.0081078475
test_loss: 0.011402613
train_loss: 0.007087452
test_loss: 0.009788226
train_loss: 0.0073019173
test_loss: 0.009926218
train_loss: 0.010269989
test_loss: 0.010601834
train_loss: 0.00707687
test_loss: 0.01021831
train_loss: 0.011226945
test_loss: 0.009698558
train_loss: 0.009791013
test_loss: 0.008894682
train_loss: 0.006578591
test_loss: 0.009655129
train_loss: 0.0086033875
test_loss: 0.0096505
train_loss: 0.0068919044
test_loss: 0.00911069
train_loss: 0.0059639136
test_loss: 0.008989156
train_loss: 0.007966776
test_loss: 0.008728111
train_loss: 0.0061530075
test_loss: 0.009319002
train_loss: 0.0059794993
test_loss: 0.009187589
train_loss: 0.0056473734
test_loss: 0.008718657
train_loss: 0.0067576664
test_loss: 0.009328187
train_loss: 0.008358303
test_loss: 0.009677463
train_loss: 0.0071546137
test_loss: 0.009183617
train_loss: 0.006415258
test_loss: 0.008608787
train_loss: 0.0050064716
test_loss: 0.008759444
train_loss: 0.006142479
test_loss: 0.0091204615
train_loss: 0.005491125
test_loss: 0.008790632
train_loss: 0.0063625155
test_loss: 0.008947091
train_loss: 0.006020316
test_loss: 0.008968093
train_loss: 0.006076336
test_loss: 0.008664951
train_loss: 0.0070198835
test_loss: 0.009536443
train_loss: 0.007002934
test_loss: 0.009131103
train_loss: 0.008021107
test_loss: 0.008908637
train_loss: 0.005521134
test_loss: 0.008943123
train_loss: 0.0062464215
test_loss: 0.00934381
train_loss: 0.0055682687
test_loss: 0.00864873
train_loss: 0.0065136272
test_loss: 0.009119821
train_loss: 0.006477314
test_loss: 0.009038653
train_loss: 0.008430082
test_loss: 0.009636292
train_loss: 0.005647947
test_loss: 0.008822879
train_loss: 0.0058868825
test_loss: 0.008879621
train_loss: 0.0059139146
test_loss: 0.009111292
train_loss: 0.0068320883
test_loss: 0.009731942
train_loss: 0.0057409173
test_loss: 0.009249394
train_loss: 0.0061185276
test_loss: 0.008443847
train_loss: 0.006318692
test_loss: 0.008991077
train_loss: 0.005161242
test_loss: 0.008311207
train_loss: 0.006516774
test_loss: 0.009228862
train_loss: 0.006617378
test_loss: 0.008675475
train_loss: 0.0050360826
test_loss: 0.008738025
train_loss: 0.005028025
test_loss: 0.008527523
train_loss: 0.005985259
test_loss: 0.008573605
train_loss: 0.0050664493
test_loss: 0.008535495
train_loss: 0.0047559366
test_loss: 0.008408051
train_loss: 0.0050586993
test_loss: 0.008437388
train_loss: 0.0063958317
test_loss: 0.0087866755
train_loss: 0.0073227407
test_loss: 0.008744193
train_loss: 0.0062225154
test_loss: 0.008924329
train_loss: 0.005376351
test_loss: 0.009053749
train_loss: 0.0051038694
test_loss: 0.008367749
train_loss: 0.005612429
test_loss: 0.008432706
train_loss: 0.0053617954
test_loss: 0.008493533
train_loss: 0.005971259
test_loss: 0.00873406
train_loss: 0.0061469595
test_loss: 0.008932582
train_loss: 0.005610143
test_loss: 0.008675072
train_loss: 0.005819489
test_loss: 0.009049453
train_loss: 0.00577259
test_loss: 0.008790839
train_loss: 0.0051276973
test_loss: 0.008578554
train_loss: 0.005239885
test_loss: 0.009026284
train_loss: 0.0053524883
test_loss: 0.008392081
train_loss: 0.007049241
test_loss: 0.008807589
train_loss: 0.0055155237
test_loss: 0.008537044
train_loss: 0.0052071856
test_loss: 0.008786465
train_loss: 0.0052252063
test_loss: 0.008655481
train_loss: 0.008631231
test_loss: 0.008469388
train_loss: 0.0061209733
test_loss: 0.009314625
train_loss: 0.0061542145
test_loss: 0.00841416
train_loss: 0.005566385
test_loss: 0.008748815
train_loss: 0.005381461
test_loss: 0.008681851
train_loss: 0.0064646094
test_loss: 0.008466603
train_loss: 0.005308219
test_loss: 0.008569089
train_loss: 0.005489696
test_loss: 0.008860148
train_loss: 0.005673724
test_loss: 0.008647938
train_loss: 0.005229859
test_loss: 0.008528169
train_loss: 0.005710928
test_loss: 0.008778304
train_loss: 0.005934763
test_loss: 0.008656575
train_loss: 0.005641202
test_loss: 0.008359705
train_loss: 0.005910109
test_loss: 0.008351236
train_loss: 0.0057753623
test_loss: 0.0086484235
train_loss: 0.0052925413
test_loss: 0.008954381
train_loss: 0.0058555994
test_loss: 0.008393559
train_loss: 0.0047876057
test_loss: 0.0083520925
train_loss: 0.0056042764
test_loss: 0.008766086
train_loss: 0.0064530517
test_loss: 0.0092883995
train_loss: 0.0055662026
test_loss: 0.008645588
train_loss: 0.0052012447
test_loss: 0.008690383
train_loss: 0.0051973457
test_loss: 0.008302862
train_loss: 0.005042499
test_loss: 0.008184846
train_loss: 0.005219187
test_loss: 0.008432414
train_loss: 0.004941081
test_loss: 0.008190497
train_loss: 0.007820972
test_loss: 0.009270821
train_loss: 0.0056028683
test_loss: 0.009307946
train_loss: 0.0048786993
test_loss: 0.0084418245
train_loss: 0.0052300906
test_loss: 0.008518425
train_loss: 0.0052258777
test_loss: 0.008619128
train_loss: 0.0052758018
test_loss: 0.008556733
train_loss: 0.0052032056
test_loss: 0.008884018
train_loss: 0.005000285
test_loss: 0.008697254
train_loss: 0.0058641927
test_loss: 0.008509557
train_loss: 0.005353508
test_loss: 0.008864815
train_loss: 0.006312463
test_loss: 0.0088063935
train_loss: 0.005905087
test_loss: 0.008567017
train_loss: 0.0064942483
test_loss: 0.008545364
train_loss: 0.0048789843
test_loss: 0.008773544
train_loss: 0.005244778
test_loss: 0.008646664
train_loss: 0.006209596
test_loss: 0.009097163
train_loss: 0.00543617
test_loss: 0.008808408
train_loss: 0.005087274
test_loss: 0.008467016
train_loss: 0.005465638
test_loss: 0.0085461335
train_loss: 0.005616282
test_loss: 0.0087552825
train_loss: 0.0056625577
test_loss: 0.008463511
train_loss: 0.0060399314
test_loss: 0.00884482
train_loss: 0.0049269143
test_loss: 0.0083654625
train_loss: 0.005112488
test_loss: 0.008450875
train_loss: 0.005703791
test_loss: 0.008581387
train_loss: 0.0059620948
test_loss: 0.008671029
train_loss: 0.004686119
test_loss: 0.0079852
train_loss: 0.0050704805
test_loss: 0.00868267
train_loss: 0.005674602
test_loss: 0.008183864
train_loss: 0.0057120016
test_loss: 0.008294096
train_loss: 0.0052107386
test_loss: 0.0085108
train_loss: 0.005150942
test_loss: 0.00860878
train_loss: 0.0061292723
test_loss: 0.008866642
train_loss: 0.0070467507
test_loss: 0.008551676
train_loss: 0.0049591325
test_loss: 0.008629773
train_loss: 0.0058657476
test_loss: 0.008898959
train_loss: 0.0060336
test_loss: 0.008763593
train_loss: 0.005633477
test_loss: 0.008989688
train_loss: 0.00601344
test_loss: 0.008562296
train_loss: 0.004575559
test_loss: 0.00834771
train_loss: 0.00522154
test_loss: 0.008469037
train_loss: 0.005196122
test_loss: 0.008822078
train_loss: 0.005379827
test_loss: 0.0087838285
train_loss: 0.005367908
test_loss: 0.008719564
train_loss: 0.0069455924
test_loss: 0.008790774
train_loss: 0.0057116663
test_loss: 0.008850418
train_loss: 0.0053633936
test_loss: 0.008758333
train_loss: 0.0050774487
test_loss: 0.008424579
train_loss: 0.005237001
test_loss: 0.008570116
train_loss: 0.0048326463
test_loss: 0.008329969
train_loss: 0.005623441
test_loss: 0.008810975
train_loss: 0.005827762
test_loss: 0.008456841
train_loss: 0.0060846442
test_loss: 0.008589669
train_loss: 0.0057127276
test_loss: 0.008971602
train_loss: 0.0050864695
test_loss: 0.008342478
train_loss: 0.0048724646
test_loss: 0.008451965
train_loss: 0.00642418
test_loss: 0.008547477
train_loss: 0.005859491
test_loss: 0.008750013
train_loss: 0.0055204816
test_loss: 0.008554579
train_loss: 0.0052378536
test_loss: 0.008643694
train_loss: 0.0053973687
test_loss: 0.008421706
train_loss: 0.0048373938
test_loss: 0.008494021
train_loss: 0.0050023007
test_loss: 0.008531231
train_loss: 0.005478929
test_loss: 0.008885725
train_loss: 0.005449741
test_loss: 0.008695953
train_loss: 0.006005232
test_loss: 0.008512065
train_loss: 0.0054306523
test_loss: 0.009115203
train_loss: 0.005067946
test_loss: 0.008420281
train_loss: 0.004903917
test_loss: 0.008491499
train_loss: 0.0043735215
test_loss: 0.008527052
train_loss: 0.005292335
test_loss: 0.0084172385
train_loss: 0.004516464
test_loss: 0.008355952
train_loss: 0.0053772302
test_loss: 0.008075659
train_loss: 0.0047073113
test_loss: 0.00847122
train_loss: 0.005256018
test_loss: 0.008936458
train_loss: 0.0051717195
test_loss: 0.0090055745
train_loss: 0.00465203
test_loss: 0.008590114
train_loss: 0.0046759136
test_loss: 0.00859286
train_loss: 0.0050529037
test_loss: 0.008461043
train_loss: 0.005718425
test_loss: 0.008136911
train_loss: 0.00494825
test_loss: 0.008874225
train_loss: 0.005444044
test_loss: 0.008988182
train_loss: 0.005223062
test_loss: 0.008571806
train_loss: 0.004983433
test_loss: 0.008622017
train_loss: 0.006029168
test_loss: 0.008662759
train_loss: 0.0046476778
test_loss: 0.008299244
train_loss: 0.005147485
test_loss: 0.008225387
train_loss: 0.005969726
test_loss: 0.008444352
train_loss: 0.0050728046
test_loss: 0.009067145
train_loss: 0.0059215203
test_loss: 0.008302887
train_loss: 0.0048519867
test_loss: 0.008311285
train_loss: 0.0059731426
test_loss: 0.009118395
train_loss: 0.0048697493
test_loss: 0.0083316825
train_loss: 0.005504085
test_loss: 0.008426429
train_loss: 0.0051028915
test_loss: 0.008589592
train_loss: 0.005140318
test_loss: 0.008524462
train_loss: 0.005157962
test_loss: 0.008287616
train_loss: 0.0050393986
test_loss: 0.008580097
train_loss: 0.006400609
test_loss: 0.00871479
train_loss: 0.0056531997
test_loss: 0.008296926
train_loss: 0.005769291
test_loss: 0.008681477
train_loss: 0.005214966
test_loss: 0.008646138
train_loss: 0.0048397817
test_loss: 0.00879184
train_loss: 0.005315818
test_loss: 0.008828803
train_loss: 0.0053990297
test_loss: 0.008809192
train_loss: 0.0053463415
test_loss: 0.00847544
train_loss: 0.0050571617
test_loss: 0.008515568
train_loss: 0.005119672
test_loss: 0.008451836
train_loss: 0.0050438736
test_loss: 0.008494225
train_loss: 0.0053456808
test_loss: 0.008709656
train_loss: 0.005242686
test_loss: 0.00869836
train_loss: 0.0048401677
test_loss: 0.008434704
train_loss: 0.0048749573
test_loss: 0.008381662
train_loss: 0.004978195
test_loss: 0.008810642
train_loss: 0.005694021
test_loss: 0.008829179
train_loss: 0.0052393735
test_loss: 0.008651618
train_loss: 0.004444681
test_loss: 0.008296508
train_loss: 0.0048920442
test_loss: 0.008249275
train_loss: 0.0073433667
test_loss: 0.008952831
train_loss: 0.006336865
test_loss: 0.00863881
train_loss: 0.004992828
test_loss: 0.008479453
train_loss: 0.005344631
test_loss: 0.0086108
train_loss: 0.004972728
test_loss: 0.0081522055
train_loss: 0.005136261
test_loss: 0.008456288
train_loss: 0.0047097
test_loss: 0.008582455
train_loss: 0.0050320365
test_loss: 0.008212205
train_loss: 0.0046191504
test_loss: 0.008506331
train_loss: 0.0073323417
test_loss: 0.008378182
train_loss: 0.004577483
test_loss: 0.008444258
train_loss: 0.006228633
test_loss: 0.008750029
train_loss: 0.006732859
test_loss: 0.008504396
train_loss: 0.0059176087
test_loss: 0.00921098
train_loss: 0.004392335
test_loss: 0.008450075
train_loss: 0.005660718
test_loss: 0.008998837
train_loss: 0.0047487444
test_loss: 0.008545544
train_loss: 0.0048388997
test_loss: 0.008370681
train_loss: 0.0073423125
test_loss: 0.008589662
train_loss: 0.0053474596
test_loss: 0.008623122
train_loss: 0.0050659017
test_loss: 0.008446926
train_loss: 0.0054130033
test_loss: 0.008113017
train_loss: 0.004796046
test_loss: 0.00847576
train_loss: 0.0049190195
test_loss: 0.008596229
train_loss: 0.0055139996
test_loss: 0.008344154
train_loss: 0.0059598023
test_loss: 0.009584672
train_loss: 0.006315364
test_loss: 0.009323989
train_loss: 0.0055425065
test_loss: 0.008864247
train_loss: 0.005664219
test_loss: 0.008509733
train_loss: 0.0052028727
test_loss: 0.008677802
train_loss: 0.006204726
test_loss: 0.008224393
train_loss: 0.004661921
test_loss: 0.008175371
train_loss: 0.0050330213
test_loss: 0.008295251
train_loss: 0.0047950656
test_loss: 0.008388743
train_loss: 0.005838955
test_loss: 0.008832601
train_loss: 0.00506225
test_loss: 0.00860442
train_loss: 0.0052340943
test_loss: 0.008836776
train_loss: 0.0055069765
test_loss: 0.008476995
train_loss: 0.0047523393
test_loss: 0.008444362
train_loss: 0.0046203383
test_loss: 0.008424825
train_loss: 0.00556737
test_loss: 0.008583657
train_loss: 0.0055671255
test_loss: 0.008486984
train_loss: 0.0051750867
test_loss: 0.0088990275
train_loss: 0.0049218927
test_loss: 0.008399447
train_loss: 0.005394607
test_loss: 0.00928294
train_loss: 0.004761643
test_loss: 0.008371356
train_loss: 0.00463956
test_loss: 0.008509347
train_loss: 0.0051847138
test_loss: 0.008558077
train_loss: 0.005575275
test_loss: 0.00894982
train_loss: 0.0047979355
test_loss: 0.008413944
train_loss: 0.0047604716
test_loss: 0.008603718
train_loss: 0.0053270725
test_loss: 0.008697009
train_loss: 0.0050423373
test_loss: 0.008490141
train_loss: 0.0049396716
test_loss: 0.008514585
train_loss: 0.0047342507
test_loss: 0.008708176
train_loss: 0.0050566574
test_loss: 0.008481774
train_loss: 0.005589677
test_loss: 0.009463056
train_loss: 0.00500022
test_loss: 0.008656995
train_loss: 0.005564563
test_loss: 0.008616173
train_loss: 0.005917595
test_loss: 0.008527248
train_loss: 0.0047518048
test_loss: 0.008647836
train_loss: 0.005166334
test_loss: 0.008403051
train_loss: 0.0044931257
test_loss: 0.008101444
train_loss: 0.0055245003
test_loss: 0.008326875
train_loss: 0.0049554985
test_loss: 0.008368939
train_loss: 0.004503972
test_loss: 0.0083287405
train_loss: 0.004663655
test_loss: 0.008426112
train_loss: 0.005194594
test_loss: 0.008569153
train_loss: 0.0050218473
test_loss: 0.008668359
train_loss: 0.0050846934
test_loss: 0.0085231485
train_loss: 0.0054569035
test_loss: 0.008536366
train_loss: 0.005590699
test_loss: 0.008746689
train_loss: 0.004763295
test_loss: 0.008854427
train_loss: 0.005140764
test_loss: 0.0083727855
train_loss: 0.0052523063
test_loss: 0.008651461
train_loss: 0.0050451793
test_loss: 0.008706667
train_loss: 0.0051910263
test_loss: 0.008550896
train_loss: 0.0046728095
test_loss: 0.00871108
train_loss: 0.0063577266
test_loss: 0.008512414
train_loss: 0.006235918
test_loss: 0.009164872
train_loss: 0.004893034
test_loss: 0.008427763
train_loss: 0.0059825527
test_loss: 0.0088713495
train_loss: 0.0046846676
test_loss: 0.0082320515
train_loss: 0.0046711485
test_loss: 0.008501854
train_loss: 0.0050066914
test_loss: 0.008247166
train_loss: 0.005091503
test_loss: 0.008590966
train_loss: 0.0048246323
test_loss: 0.008645905
train_loss: 0.0048612338
test_loss: 0.008283463
train_loss: 0.0046885535
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.008231613
train_loss: 0.0051031495
test_loss: 0.008286571
train_loss: 0.005627568
test_loss: 0.008391442
train_loss: 0.004571888
test_loss: 0.008834891
train_loss: 0.0052724564
test_loss: 0.008468631
train_loss: 0.0054309587
test_loss: 0.008766597
train_loss: 0.0047420175
test_loss: 0.008585917
train_loss: 0.0057245055
test_loss: 0.008823855
train_loss: 0.0065680044
test_loss: 0.009079291
train_loss: 0.004843034
test_loss: 0.008566299
train_loss: 0.0045029595
test_loss: 0.008502738
train_loss: 0.00531865
test_loss: 0.00853691
train_loss: 0.0045718644
test_loss: 0.008425758
train_loss: 0.005167241
test_loss: 0.0091555705
train_loss: 0.004830151
test_loss: 0.008669745
train_loss: 0.004871173
test_loss: 0.008617743
train_loss: 0.004051141
test_loss: 0.008319004
train_loss: 0.0042291456
test_loss: 0.008357557
train_loss: 0.0057490594
test_loss: 0.008547602
train_loss: 0.00537199
test_loss: 0.008635538
train_loss: 0.0047825794
test_loss: 0.0087101385
train_loss: 0.005137441
test_loss: 0.008648236
train_loss: 0.005401969
test_loss: 0.008311226
train_loss: 0.005908214
test_loss: 0.008359598
train_loss: 0.004542099
test_loss: 0.008634405
train_loss: 0.005256452
test_loss: 0.0087124035
train_loss: 0.007987684
test_loss: 0.008555093
train_loss: 0.0050379834
test_loss: 0.008800316
train_loss: 0.0049988097
test_loss: 0.008636819
train_loss: 0.0052172313
test_loss: 0.008951023
train_loss: 0.0057501746
test_loss: 0.008984037
train_loss: 0.0048638294
test_loss: 0.008824032
train_loss: 0.005060695
test_loss: 0.008757776
train_loss: 0.005275072
test_loss: 0.008515802
train_loss: 0.0044124112
test_loss: 0.0088006165
train_loss: 0.004840943
test_loss: 0.008722005
train_loss: 0.004571444
test_loss: 0.008450874
train_loss: 0.004756568
test_loss: 0.008446919
train_loss: 0.0043371543
test_loss: 0.008362511
train_loss: 0.0053226766
test_loss: 0.008647725
train_loss: 0.004447774
test_loss: 0.008456198
train_loss: 0.005122121
test_loss: 0.008691048
train_loss: 0.005546371
test_loss: 0.008253308
train_loss: 0.006122293
test_loss: 0.008733108
train_loss: 0.0049747024
test_loss: 0.008939519
train_loss: 0.004920072
test_loss: 0.008424333
train_loss: 0.004804892
test_loss: 0.0083987815
train_loss: 0.005202306
test_loss: 0.008509027
train_loss: 0.004957096
test_loss: 0.008745718
train_loss: 0.0043055667
test_loss: 0.0083079655
train_loss: 0.005744329
test_loss: 0.008512457
train_loss: 0.0064566727
test_loss: 0.008466275
train_loss: 0.0050116424
test_loss: 0.008938345
train_loss: 0.0048714275
test_loss: 0.008628668
train_loss: 0.0051370286
test_loss: 0.008860363
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85eae048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ffb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85eae598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ef5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85f0b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85f0b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85f0bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ddbe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ddb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85d9cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85db8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85d6d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85d6dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85d3a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85d3aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85cd6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85d0e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ccdc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ccdf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ccd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ccd950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c49744598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c49773598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c4971f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c497336a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c49733b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c4971f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c496b12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c496b1378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c247676a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c2470d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c24728400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c246b9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c246de620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c24684620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c24684e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000311927608
Iter: 2 loss: 0.000233899991
Iter: 3 loss: 0.000233852043
Iter: 4 loss: 0.000191860163
Iter: 5 loss: 0.000639762788
Iter: 6 loss: 0.000190432678
Iter: 7 loss: 0.000160995289
Iter: 8 loss: 0.000120822755
Iter: 9 loss: 0.000118343843
Iter: 10 loss: 7.81342387e-05
Iter: 11 loss: 0.000513641047
Iter: 12 loss: 7.68964092e-05
Iter: 13 loss: 5.77944738e-05
Iter: 14 loss: 0.000268592412
Iter: 15 loss: 5.7411904e-05
Iter: 16 loss: 4.51580454e-05
Iter: 17 loss: 9.12167598e-05
Iter: 18 loss: 4.20874858e-05
Iter: 19 loss: 3.58998186e-05
Iter: 20 loss: 0.000100645586
Iter: 21 loss: 3.57467979e-05
Iter: 22 loss: 3.29760078e-05
Iter: 23 loss: 3.28947863e-05
Iter: 24 loss: 3.07212e-05
Iter: 25 loss: 2.74758386e-05
Iter: 26 loss: 6.72186434e-05
Iter: 27 loss: 2.74432787e-05
Iter: 28 loss: 2.52505561e-05
Iter: 29 loss: 3.7690781e-05
Iter: 30 loss: 2.4943989e-05
Iter: 31 loss: 2.36060296e-05
Iter: 32 loss: 2.17106935e-05
Iter: 33 loss: 2.16435583e-05
Iter: 34 loss: 2.03483014e-05
Iter: 35 loss: 2.03044256e-05
Iter: 36 loss: 1.89849652e-05
Iter: 37 loss: 2.11972474e-05
Iter: 38 loss: 1.83904813e-05
Iter: 39 loss: 1.71810534e-05
Iter: 40 loss: 2.75035618e-05
Iter: 41 loss: 1.71136653e-05
Iter: 42 loss: 1.66521058e-05
Iter: 43 loss: 1.65893962e-05
Iter: 44 loss: 1.62305478e-05
Iter: 45 loss: 1.5283309e-05
Iter: 46 loss: 2.2205606e-05
Iter: 47 loss: 1.50823826e-05
Iter: 48 loss: 1.42062145e-05
Iter: 49 loss: 2.41417438e-05
Iter: 50 loss: 1.41905548e-05
Iter: 51 loss: 1.37691968e-05
Iter: 52 loss: 1.88885224e-05
Iter: 53 loss: 1.37643956e-05
Iter: 54 loss: 1.34068305e-05
Iter: 55 loss: 1.30940443e-05
Iter: 56 loss: 1.29996442e-05
Iter: 57 loss: 1.2524265e-05
Iter: 58 loss: 1.57400391e-05
Iter: 59 loss: 1.24772969e-05
Iter: 60 loss: 1.21139437e-05
Iter: 61 loss: 1.22659094e-05
Iter: 62 loss: 1.18645676e-05
Iter: 63 loss: 1.13566557e-05
Iter: 64 loss: 1.23503942e-05
Iter: 65 loss: 1.11468098e-05
Iter: 66 loss: 1.06714651e-05
Iter: 67 loss: 1.37740135e-05
Iter: 68 loss: 1.06202097e-05
Iter: 69 loss: 1.02607519e-05
Iter: 70 loss: 1.30279241e-05
Iter: 71 loss: 1.02341819e-05
Iter: 72 loss: 1.00525667e-05
Iter: 73 loss: 1.07452961e-05
Iter: 74 loss: 1.00094639e-05
Iter: 75 loss: 9.80264394e-06
Iter: 76 loss: 9.86953728e-06
Iter: 77 loss: 9.65602e-06
Iter: 78 loss: 9.46376895e-06
Iter: 79 loss: 9.46375712e-06
Iter: 80 loss: 9.34147829e-06
Iter: 81 loss: 9.07081085e-06
Iter: 82 loss: 1.29502941e-05
Iter: 83 loss: 9.05768e-06
Iter: 84 loss: 8.75480146e-06
Iter: 85 loss: 9.75009152e-06
Iter: 86 loss: 8.67086237e-06
Iter: 87 loss: 8.50827109e-06
Iter: 88 loss: 8.50250581e-06
Iter: 89 loss: 8.35468109e-06
Iter: 90 loss: 8.14722807e-06
Iter: 91 loss: 8.13915e-06
Iter: 92 loss: 7.94337666e-06
Iter: 93 loss: 9.57358679e-06
Iter: 94 loss: 7.93191066e-06
Iter: 95 loss: 7.76128491e-06
Iter: 96 loss: 8.02382783e-06
Iter: 97 loss: 7.68094833e-06
Iter: 98 loss: 7.51738935e-06
Iter: 99 loss: 7.62729951e-06
Iter: 100 loss: 7.41453096e-06
Iter: 101 loss: 7.23350968e-06
Iter: 102 loss: 8.94100776e-06
Iter: 103 loss: 7.22625646e-06
Iter: 104 loss: 7.10748463e-06
Iter: 105 loss: 7.84699296e-06
Iter: 106 loss: 7.09365941e-06
Iter: 107 loss: 6.98784243e-06
Iter: 108 loss: 7.15046e-06
Iter: 109 loss: 6.93764e-06
Iter: 110 loss: 6.82888185e-06
Iter: 111 loss: 7.54854591e-06
Iter: 112 loss: 6.81745132e-06
Iter: 113 loss: 6.73256181e-06
Iter: 114 loss: 6.87287275e-06
Iter: 115 loss: 6.69389465e-06
Iter: 116 loss: 6.59317766e-06
Iter: 117 loss: 6.54397445e-06
Iter: 118 loss: 6.49550066e-06
Iter: 119 loss: 6.39224e-06
Iter: 120 loss: 6.55191479e-06
Iter: 121 loss: 6.34319531e-06
Iter: 122 loss: 6.24241511e-06
Iter: 123 loss: 6.85860869e-06
Iter: 124 loss: 6.23022697e-06
Iter: 125 loss: 6.13742486e-06
Iter: 126 loss: 6.91181958e-06
Iter: 127 loss: 6.13191605e-06
Iter: 128 loss: 6.06889489e-06
Iter: 129 loss: 5.92985e-06
Iter: 130 loss: 7.94036714e-06
Iter: 131 loss: 5.92331162e-06
Iter: 132 loss: 5.7913644e-06
Iter: 133 loss: 7.25703376e-06
Iter: 134 loss: 5.78886556e-06
Iter: 135 loss: 5.6917479e-06
Iter: 136 loss: 6.17085334e-06
Iter: 137 loss: 5.67509778e-06
Iter: 138 loss: 5.59305045e-06
Iter: 139 loss: 5.83773181e-06
Iter: 140 loss: 5.56815075e-06
Iter: 141 loss: 5.52223082e-06
Iter: 142 loss: 6.15485533e-06
Iter: 143 loss: 5.52213169e-06
Iter: 144 loss: 5.47993932e-06
Iter: 145 loss: 5.44328122e-06
Iter: 146 loss: 5.43211536e-06
Iter: 147 loss: 5.35614072e-06
Iter: 148 loss: 5.91223215e-06
Iter: 149 loss: 5.34982883e-06
Iter: 150 loss: 5.3138283e-06
Iter: 151 loss: 5.38574113e-06
Iter: 152 loss: 5.29905674e-06
Iter: 153 loss: 5.25319228e-06
Iter: 154 loss: 5.19576452e-06
Iter: 155 loss: 5.19111563e-06
Iter: 156 loss: 5.12369797e-06
Iter: 157 loss: 5.4884913e-06
Iter: 158 loss: 5.11370808e-06
Iter: 159 loss: 5.06445531e-06
Iter: 160 loss: 5.11338931e-06
Iter: 161 loss: 5.03671e-06
Iter: 162 loss: 4.96828034e-06
Iter: 163 loss: 5.63724689e-06
Iter: 164 loss: 4.96587745e-06
Iter: 165 loss: 4.92554227e-06
Iter: 166 loss: 4.89020567e-06
Iter: 167 loss: 4.87952047e-06
Iter: 168 loss: 4.81858569e-06
Iter: 169 loss: 4.94607411e-06
Iter: 170 loss: 4.79448545e-06
Iter: 171 loss: 4.7435633e-06
Iter: 172 loss: 5.11035933e-06
Iter: 173 loss: 4.73922682e-06
Iter: 174 loss: 4.69635961e-06
Iter: 175 loss: 4.97265955e-06
Iter: 176 loss: 4.69150655e-06
Iter: 177 loss: 4.65461108e-06
Iter: 178 loss: 4.77937692e-06
Iter: 179 loss: 4.64470486e-06
Iter: 180 loss: 4.61063246e-06
Iter: 181 loss: 4.74592434e-06
Iter: 182 loss: 4.60283127e-06
Iter: 183 loss: 4.57420629e-06
Iter: 184 loss: 4.60211959e-06
Iter: 185 loss: 4.5579427e-06
Iter: 186 loss: 4.52265886e-06
Iter: 187 loss: 4.60942056e-06
Iter: 188 loss: 4.51023425e-06
Iter: 189 loss: 4.47886441e-06
Iter: 190 loss: 4.44837e-06
Iter: 191 loss: 4.44163743e-06
Iter: 192 loss: 4.40084e-06
Iter: 193 loss: 5.00401666e-06
Iter: 194 loss: 4.40076383e-06
Iter: 195 loss: 4.37109884e-06
Iter: 196 loss: 4.33753758e-06
Iter: 197 loss: 4.33312789e-06
Iter: 198 loss: 4.28295198e-06
Iter: 199 loss: 4.4930448e-06
Iter: 200 loss: 4.27226314e-06
Iter: 201 loss: 4.23931306e-06
Iter: 202 loss: 4.53957364e-06
Iter: 203 loss: 4.23779193e-06
Iter: 204 loss: 4.20243623e-06
Iter: 205 loss: 4.28264684e-06
Iter: 206 loss: 4.18909121e-06
Iter: 207 loss: 4.1638059e-06
Iter: 208 loss: 4.15344903e-06
Iter: 209 loss: 4.14023089e-06
Iter: 210 loss: 4.11769815e-06
Iter: 211 loss: 4.11730798e-06
Iter: 212 loss: 4.09422864e-06
Iter: 213 loss: 4.12163536e-06
Iter: 214 loss: 4.08214964e-06
Iter: 215 loss: 4.05607898e-06
Iter: 216 loss: 4.10327721e-06
Iter: 217 loss: 4.04478942e-06
Iter: 218 loss: 4.01858188e-06
Iter: 219 loss: 4.06747722e-06
Iter: 220 loss: 4.0075729e-06
Iter: 221 loss: 3.98119528e-06
Iter: 222 loss: 4.03633476e-06
Iter: 223 loss: 3.97071335e-06
Iter: 224 loss: 3.94336894e-06
Iter: 225 loss: 4.04569892e-06
Iter: 226 loss: 3.93669143e-06
Iter: 227 loss: 3.91410322e-06
Iter: 228 loss: 3.89108118e-06
Iter: 229 loss: 3.88670333e-06
Iter: 230 loss: 3.85313115e-06
Iter: 231 loss: 4.09673839e-06
Iter: 232 loss: 3.85033945e-06
Iter: 233 loss: 3.82530561e-06
Iter: 234 loss: 3.97049098e-06
Iter: 235 loss: 3.82210237e-06
Iter: 236 loss: 3.80104757e-06
Iter: 237 loss: 3.82634698e-06
Iter: 238 loss: 3.7900254e-06
Iter: 239 loss: 3.7711784e-06
Iter: 240 loss: 3.96183077e-06
Iter: 241 loss: 3.77072365e-06
Iter: 242 loss: 3.75092964e-06
Iter: 243 loss: 3.74406454e-06
Iter: 244 loss: 3.73295234e-06
Iter: 245 loss: 3.71952706e-06
Iter: 246 loss: 3.71921669e-06
Iter: 247 loss: 3.70570888e-06
Iter: 248 loss: 3.70705402e-06
Iter: 249 loss: 3.69543341e-06
Iter: 250 loss: 3.67973507e-06
Iter: 251 loss: 3.68807673e-06
Iter: 252 loss: 3.66931044e-06
Iter: 253 loss: 3.64760263e-06
Iter: 254 loss: 3.72307431e-06
Iter: 255 loss: 3.64204107e-06
Iter: 256 loss: 3.62725041e-06
Iter: 257 loss: 3.640881e-06
Iter: 258 loss: 3.61863795e-06
Iter: 259 loss: 3.59449496e-06
Iter: 260 loss: 3.64531206e-06
Iter: 261 loss: 3.58479565e-06
Iter: 262 loss: 3.56877854e-06
Iter: 263 loss: 3.57513682e-06
Iter: 264 loss: 3.55759016e-06
Iter: 265 loss: 3.53481255e-06
Iter: 266 loss: 3.61906314e-06
Iter: 267 loss: 3.52933148e-06
Iter: 268 loss: 3.51325616e-06
Iter: 269 loss: 3.61400112e-06
Iter: 270 loss: 3.51146718e-06
Iter: 271 loss: 3.49479683e-06
Iter: 272 loss: 3.51541803e-06
Iter: 273 loss: 3.48618414e-06
Iter: 274 loss: 3.46991578e-06
Iter: 275 loss: 3.64157427e-06
Iter: 276 loss: 3.46949673e-06
Iter: 277 loss: 3.45721105e-06
Iter: 278 loss: 3.46956313e-06
Iter: 279 loss: 3.45029775e-06
Iter: 280 loss: 3.43876127e-06
Iter: 281 loss: 3.59259275e-06
Iter: 282 loss: 3.43872375e-06
Iter: 283 loss: 3.42959379e-06
Iter: 284 loss: 3.41490363e-06
Iter: 285 loss: 3.41470059e-06
Iter: 286 loss: 3.39773442e-06
Iter: 287 loss: 3.45314174e-06
Iter: 288 loss: 3.39294775e-06
Iter: 289 loss: 3.37612846e-06
Iter: 290 loss: 3.43638453e-06
Iter: 291 loss: 3.37187203e-06
Iter: 292 loss: 3.35738423e-06
Iter: 293 loss: 3.36449921e-06
Iter: 294 loss: 3.34776405e-06
Iter: 295 loss: 3.3324759e-06
Iter: 296 loss: 3.47904597e-06
Iter: 297 loss: 3.33182084e-06
Iter: 298 loss: 3.32014361e-06
Iter: 299 loss: 3.31730894e-06
Iter: 300 loss: 3.30980129e-06
Iter: 301 loss: 3.29409272e-06
Iter: 302 loss: 3.30763169e-06
Iter: 303 loss: 3.28474e-06
Iter: 304 loss: 3.26784016e-06
Iter: 305 loss: 3.38070322e-06
Iter: 306 loss: 3.2660962e-06
Iter: 307 loss: 3.25149472e-06
Iter: 308 loss: 3.36901462e-06
Iter: 309 loss: 3.25061797e-06
Iter: 310 loss: 3.23968e-06
Iter: 311 loss: 3.2642065e-06
Iter: 312 loss: 3.23544691e-06
Iter: 313 loss: 3.22344522e-06
Iter: 314 loss: 3.28087481e-06
Iter: 315 loss: 3.22112965e-06
Iter: 316 loss: 3.21079187e-06
Iter: 317 loss: 3.25404e-06
Iter: 318 loss: 3.20860909e-06
Iter: 319 loss: 3.20075196e-06
Iter: 320 loss: 3.18698972e-06
Iter: 321 loss: 3.18702223e-06
Iter: 322 loss: 3.17324157e-06
Iter: 323 loss: 3.31363685e-06
Iter: 324 loss: 3.17287254e-06
Iter: 325 loss: 3.16233672e-06
Iter: 326 loss: 3.17523518e-06
Iter: 327 loss: 3.15686157e-06
Iter: 328 loss: 3.14594058e-06
Iter: 329 loss: 3.15744433e-06
Iter: 330 loss: 3.13985402e-06
Iter: 331 loss: 3.12730185e-06
Iter: 332 loss: 3.24229177e-06
Iter: 333 loss: 3.12674797e-06
Iter: 334 loss: 3.1188606e-06
Iter: 335 loss: 3.10732048e-06
Iter: 336 loss: 3.1069585e-06
Iter: 337 loss: 3.09221628e-06
Iter: 338 loss: 3.17110653e-06
Iter: 339 loss: 3.0900685e-06
Iter: 340 loss: 3.07781193e-06
Iter: 341 loss: 3.14446538e-06
Iter: 342 loss: 3.07592518e-06
Iter: 343 loss: 3.06461015e-06
Iter: 344 loss: 3.14314184e-06
Iter: 345 loss: 3.06351649e-06
Iter: 346 loss: 3.05608819e-06
Iter: 347 loss: 3.08372546e-06
Iter: 348 loss: 3.05429103e-06
Iter: 349 loss: 3.0456863e-06
Iter: 350 loss: 3.05841149e-06
Iter: 351 loss: 3.04151627e-06
Iter: 352 loss: 3.03208253e-06
Iter: 353 loss: 3.04270225e-06
Iter: 354 loss: 3.02697663e-06
Iter: 355 loss: 3.01925547e-06
Iter: 356 loss: 3.03527304e-06
Iter: 357 loss: 3.01620366e-06
Iter: 358 loss: 3.00717647e-06
Iter: 359 loss: 3.02160515e-06
Iter: 360 loss: 3.0030169e-06
Iter: 361 loss: 2.99254202e-06
Iter: 362 loss: 3.03345132e-06
Iter: 363 loss: 2.99027397e-06
Iter: 364 loss: 2.98235523e-06
Iter: 365 loss: 2.98321811e-06
Iter: 366 loss: 2.97625729e-06
Iter: 367 loss: 2.963631e-06
Iter: 368 loss: 3.0302042e-06
Iter: 369 loss: 2.96176768e-06
Iter: 370 loss: 2.95198606e-06
Iter: 371 loss: 2.96415601e-06
Iter: 372 loss: 2.94679035e-06
Iter: 373 loss: 2.93702533e-06
Iter: 374 loss: 2.93853918e-06
Iter: 375 loss: 2.92977143e-06
Iter: 376 loss: 2.92168602e-06
Iter: 377 loss: 2.92106324e-06
Iter: 378 loss: 2.91390734e-06
Iter: 379 loss: 2.92151435e-06
Iter: 380 loss: 2.90994217e-06
Iter: 381 loss: 2.90200182e-06
Iter: 382 loss: 2.96115786e-06
Iter: 383 loss: 2.90129719e-06
Iter: 384 loss: 2.89473155e-06
Iter: 385 loss: 2.88780666e-06
Iter: 386 loss: 2.88669116e-06
Iter: 387 loss: 2.87723583e-06
Iter: 388 loss: 2.93120183e-06
Iter: 389 loss: 2.87603962e-06
Iter: 390 loss: 2.86882869e-06
Iter: 391 loss: 2.86819522e-06
Iter: 392 loss: 2.86280056e-06
Iter: 393 loss: 2.85276428e-06
Iter: 394 loss: 2.92371169e-06
Iter: 395 loss: 2.85187207e-06
Iter: 396 loss: 2.84511634e-06
Iter: 397 loss: 2.85246733e-06
Iter: 398 loss: 2.84132739e-06
Iter: 399 loss: 2.83360259e-06
Iter: 400 loss: 2.84445264e-06
Iter: 401 loss: 2.82963606e-06
Iter: 402 loss: 2.81952748e-06
Iter: 403 loss: 2.87748344e-06
Iter: 404 loss: 2.81817438e-06
Iter: 405 loss: 2.81188159e-06
Iter: 406 loss: 2.80128552e-06
Iter: 407 loss: 2.80125846e-06
Iter: 408 loss: 2.7941137e-06
Iter: 409 loss: 2.79343044e-06
Iter: 410 loss: 2.78591347e-06
Iter: 411 loss: 2.80330505e-06
Iter: 412 loss: 2.78306857e-06
Iter: 413 loss: 2.77638514e-06
Iter: 414 loss: 2.82378551e-06
Iter: 415 loss: 2.77581785e-06
Iter: 416 loss: 2.77042159e-06
Iter: 417 loss: 2.7706576e-06
Iter: 418 loss: 2.76616e-06
Iter: 419 loss: 2.75990669e-06
Iter: 420 loss: 2.76551918e-06
Iter: 421 loss: 2.75616867e-06
Iter: 422 loss: 2.7479573e-06
Iter: 423 loss: 2.77052641e-06
Iter: 424 loss: 2.7452852e-06
Iter: 425 loss: 2.73915043e-06
Iter: 426 loss: 2.76965943e-06
Iter: 427 loss: 2.73807905e-06
Iter: 428 loss: 2.73167234e-06
Iter: 429 loss: 2.73167734e-06
Iter: 430 loss: 2.72661828e-06
Iter: 431 loss: 2.71792123e-06
Iter: 432 loss: 2.74707622e-06
Iter: 433 loss: 2.71554063e-06
Iter: 434 loss: 2.70842565e-06
Iter: 435 loss: 2.74496597e-06
Iter: 436 loss: 2.70721057e-06
Iter: 437 loss: 2.70028863e-06
Iter: 438 loss: 2.70604778e-06
Iter: 439 loss: 2.69611655e-06
Iter: 440 loss: 2.68869121e-06
Iter: 441 loss: 2.7088895e-06
Iter: 442 loss: 2.68626559e-06
Iter: 443 loss: 2.6808043e-06
Iter: 444 loss: 2.7661049e-06
Iter: 445 loss: 2.68080157e-06
Iter: 446 loss: 2.67551445e-06
Iter: 447 loss: 2.68086478e-06
Iter: 448 loss: 2.67257474e-06
Iter: 449 loss: 2.66678762e-06
Iter: 450 loss: 2.69997145e-06
Iter: 451 loss: 2.66605321e-06
Iter: 452 loss: 2.66231564e-06
Iter: 453 loss: 2.65630479e-06
Iter: 454 loss: 2.65621838e-06
Iter: 455 loss: 2.64883033e-06
Iter: 456 loss: 2.69421957e-06
Iter: 457 loss: 2.64792243e-06
Iter: 458 loss: 2.64099958e-06
Iter: 459 loss: 2.64551136e-06
Iter: 460 loss: 2.63656102e-06
Iter: 461 loss: 2.62963749e-06
Iter: 462 loss: 2.67678456e-06
Iter: 463 loss: 2.62900494e-06
Iter: 464 loss: 2.62251206e-06
Iter: 465 loss: 2.62286221e-06
Iter: 466 loss: 2.61742252e-06
Iter: 467 loss: 2.6103985e-06
Iter: 468 loss: 2.64999221e-06
Iter: 469 loss: 2.60935758e-06
Iter: 470 loss: 2.60295906e-06
Iter: 471 loss: 2.6210414e-06
Iter: 472 loss: 2.60082879e-06
Iter: 473 loss: 2.59445096e-06
Iter: 474 loss: 2.60127808e-06
Iter: 475 loss: 2.59089779e-06
Iter: 476 loss: 2.58447926e-06
Iter: 477 loss: 2.62648564e-06
Iter: 478 loss: 2.58376531e-06
Iter: 479 loss: 2.57785382e-06
Iter: 480 loss: 2.62608228e-06
Iter: 481 loss: 2.57744023e-06
Iter: 482 loss: 2.57407987e-06
Iter: 483 loss: 2.58417845e-06
Iter: 484 loss: 2.57298825e-06
Iter: 485 loss: 2.56908606e-06
Iter: 486 loss: 2.56180283e-06
Iter: 487 loss: 2.72394027e-06
Iter: 488 loss: 2.56179283e-06
Iter: 489 loss: 2.55475652e-06
Iter: 490 loss: 2.61111745e-06
Iter: 491 loss: 2.55422492e-06
Iter: 492 loss: 2.54912152e-06
Iter: 493 loss: 2.56242265e-06
Iter: 494 loss: 2.54735824e-06
Iter: 495 loss: 2.54133e-06
Iter: 496 loss: 2.54532733e-06
Iter: 497 loss: 2.53760822e-06
Iter: 498 loss: 2.5317845e-06
Iter: 499 loss: 2.57090596e-06
Iter: 500 loss: 2.5312404e-06
Iter: 501 loss: 2.5256727e-06
Iter: 502 loss: 2.52245923e-06
Iter: 503 loss: 2.51995289e-06
Iter: 504 loss: 2.51356346e-06
Iter: 505 loss: 2.57647571e-06
Iter: 506 loss: 2.51338088e-06
Iter: 507 loss: 2.50806102e-06
Iter: 508 loss: 2.5174686e-06
Iter: 509 loss: 2.50570884e-06
Iter: 510 loss: 2.50072571e-06
Iter: 511 loss: 2.51025267e-06
Iter: 512 loss: 2.49855793e-06
Iter: 513 loss: 2.4952833e-06
Iter: 514 loss: 2.49494769e-06
Iter: 515 loss: 2.49257573e-06
Iter: 516 loss: 2.4911933e-06
Iter: 517 loss: 2.49023651e-06
Iter: 518 loss: 2.48539823e-06
Iter: 519 loss: 2.48713718e-06
Iter: 520 loss: 2.48196147e-06
Iter: 521 loss: 2.47726257e-06
Iter: 522 loss: 2.47920411e-06
Iter: 523 loss: 2.47404773e-06
Iter: 524 loss: 2.4672272e-06
Iter: 525 loss: 2.49664276e-06
Iter: 526 loss: 2.46576928e-06
Iter: 527 loss: 2.45989349e-06
Iter: 528 loss: 2.47677644e-06
Iter: 529 loss: 2.45795695e-06
Iter: 530 loss: 2.452525e-06
Iter: 531 loss: 2.45877914e-06
Iter: 532 loss: 2.44958915e-06
Iter: 533 loss: 2.44256626e-06
Iter: 534 loss: 2.46944774e-06
Iter: 535 loss: 2.44093303e-06
Iter: 536 loss: 2.43596378e-06
Iter: 537 loss: 2.44994544e-06
Iter: 538 loss: 2.4342803e-06
Iter: 539 loss: 2.42924807e-06
Iter: 540 loss: 2.44581315e-06
Iter: 541 loss: 2.42785472e-06
Iter: 542 loss: 2.42282294e-06
Iter: 543 loss: 2.43363252e-06
Iter: 544 loss: 2.42081796e-06
Iter: 545 loss: 2.4180963e-06
Iter: 546 loss: 2.41796647e-06
Iter: 547 loss: 2.41525231e-06
Iter: 548 loss: 2.41538942e-06
Iter: 549 loss: 2.41313273e-06
Iter: 550 loss: 2.40952568e-06
Iter: 551 loss: 2.41735916e-06
Iter: 552 loss: 2.40802115e-06
Iter: 553 loss: 2.40394775e-06
Iter: 554 loss: 2.40183613e-06
Iter: 555 loss: 2.3999155e-06
Iter: 556 loss: 2.39500309e-06
Iter: 557 loss: 2.40641702e-06
Iter: 558 loss: 2.39319911e-06
Iter: 559 loss: 2.38709049e-06
Iter: 560 loss: 2.41928615e-06
Iter: 561 loss: 2.38614621e-06
Iter: 562 loss: 2.38229268e-06
Iter: 563 loss: 2.38627945e-06
Iter: 564 loss: 2.38008738e-06
Iter: 565 loss: 2.3748903e-06
Iter: 566 loss: 2.38569328e-06
Iter: 567 loss: 2.37294853e-06
Iter: 568 loss: 2.36736423e-06
Iter: 569 loss: 2.38662619e-06
Iter: 570 loss: 2.3657708e-06
Iter: 571 loss: 2.36140113e-06
Iter: 572 loss: 2.37104e-06
Iter: 573 loss: 2.35979405e-06
Iter: 574 loss: 2.35449897e-06
Iter: 575 loss: 2.37217864e-06
Iter: 576 loss: 2.35301923e-06
Iter: 577 loss: 2.34949675e-06
Iter: 578 loss: 2.37737777e-06
Iter: 579 loss: 2.34929712e-06
Iter: 580 loss: 2.34556569e-06
Iter: 581 loss: 2.35458447e-06
Iter: 582 loss: 2.34411527e-06
Iter: 583 loss: 2.34072741e-06
Iter: 584 loss: 2.34747063e-06
Iter: 585 loss: 2.33947299e-06
Iter: 586 loss: 2.33582796e-06
Iter: 587 loss: 2.33614401e-06
Iter: 588 loss: 2.33309697e-06
Iter: 589 loss: 2.3286475e-06
Iter: 590 loss: 2.33068795e-06
Iter: 591 loss: 2.32556522e-06
Iter: 592 loss: 2.32143839e-06
Iter: 593 loss: 2.37971926e-06
Iter: 594 loss: 2.32142747e-06
Iter: 595 loss: 2.31821514e-06
Iter: 596 loss: 2.31812101e-06
Iter: 597 loss: 2.31558192e-06
Iter: 598 loss: 2.31085596e-06
Iter: 599 loss: 2.31949957e-06
Iter: 600 loss: 2.30885644e-06
Iter: 601 loss: 2.3042046e-06
Iter: 602 loss: 2.33501123e-06
Iter: 603 loss: 2.3038458e-06
Iter: 604 loss: 2.29983425e-06
Iter: 605 loss: 2.29877469e-06
Iter: 606 loss: 2.29636817e-06
Iter: 607 loss: 2.29203715e-06
Iter: 608 loss: 2.34216509e-06
Iter: 609 loss: 2.29201441e-06
Iter: 610 loss: 2.28867748e-06
Iter: 611 loss: 2.29218904e-06
Iter: 612 loss: 2.28672661e-06
Iter: 613 loss: 2.28327531e-06
Iter: 614 loss: 2.28328349e-06
Iter: 615 loss: 2.28104795e-06
Iter: 616 loss: 2.27946475e-06
Iter: 617 loss: 2.27870305e-06
Iter: 618 loss: 2.27487362e-06
Iter: 619 loss: 2.28352405e-06
Iter: 620 loss: 2.27350392e-06
Iter: 621 loss: 2.2699187e-06
Iter: 622 loss: 2.27008491e-06
Iter: 623 loss: 2.26717316e-06
Iter: 624 loss: 2.26270822e-06
Iter: 625 loss: 2.27595206e-06
Iter: 626 loss: 2.26128691e-06
Iter: 627 loss: 2.25633767e-06
Iter: 628 loss: 2.28496765e-06
Iter: 629 loss: 2.25566282e-06
Iter: 630 loss: 2.25225131e-06
Iter: 631 loss: 2.25245367e-06
Iter: 632 loss: 2.24957967e-06
Iter: 633 loss: 2.24506675e-06
Iter: 634 loss: 2.27268811e-06
Iter: 635 loss: 2.24443397e-06
Iter: 636 loss: 2.24049199e-06
Iter: 637 loss: 2.24664745e-06
Iter: 638 loss: 2.23872075e-06
Iter: 639 loss: 2.23498137e-06
Iter: 640 loss: 2.24923974e-06
Iter: 641 loss: 2.23404186e-06
Iter: 642 loss: 2.23025199e-06
Iter: 643 loss: 2.24174391e-06
Iter: 644 loss: 2.22908466e-06
Iter: 645 loss: 2.22574317e-06
Iter: 646 loss: 2.26489851e-06
Iter: 647 loss: 2.22569406e-06
Iter: 648 loss: 2.22297695e-06
Iter: 649 loss: 2.22488529e-06
Iter: 650 loss: 2.22123936e-06
Iter: 651 loss: 2.21863434e-06
Iter: 652 loss: 2.22224753e-06
Iter: 653 loss: 2.21725941e-06
Iter: 654 loss: 2.2137574e-06
Iter: 655 loss: 2.21381674e-06
Iter: 656 loss: 2.21088476e-06
Iter: 657 loss: 2.20723746e-06
Iter: 658 loss: 2.21327446e-06
Iter: 659 loss: 2.2055774e-06
Iter: 660 loss: 2.20129186e-06
Iter: 661 loss: 2.23121606e-06
Iter: 662 loss: 2.20090192e-06
Iter: 663 loss: 2.19698359e-06
Iter: 664 loss: 2.19870276e-06
Iter: 665 loss: 2.1941928e-06
Iter: 666 loss: 2.19063372e-06
Iter: 667 loss: 2.20304355e-06
Iter: 668 loss: 2.18969353e-06
Iter: 669 loss: 2.18553737e-06
Iter: 670 loss: 2.19953517e-06
Iter: 671 loss: 2.184305e-06
Iter: 672 loss: 2.1812823e-06
Iter: 673 loss: 2.18740161e-06
Iter: 674 loss: 2.1800638e-06
Iter: 675 loss: 2.17629713e-06
Iter: 676 loss: 2.18866762e-06
Iter: 677 loss: 2.17520369e-06
Iter: 678 loss: 2.17261595e-06
Iter: 679 loss: 2.21432447e-06
Iter: 680 loss: 2.17259867e-06
Iter: 681 loss: 2.17053685e-06
Iter: 682 loss: 2.17189063e-06
Iter: 683 loss: 2.16912099e-06
Iter: 684 loss: 2.1665237e-06
Iter: 685 loss: 2.16707099e-06
Iter: 686 loss: 2.16466606e-06
Iter: 687 loss: 2.1613821e-06
Iter: 688 loss: 2.17437673e-06
Iter: 689 loss: 2.16061881e-06
Iter: 690 loss: 2.1582423e-06
Iter: 691 loss: 2.15548926e-06
Iter: 692 loss: 2.15512864e-06
Iter: 693 loss: 2.15154887e-06
Iter: 694 loss: 2.19932417e-06
Iter: 695 loss: 2.15157752e-06
Iter: 696 loss: 2.14869169e-06
Iter: 697 loss: 2.15266755e-06
Iter: 698 loss: 2.14726106e-06
Iter: 699 loss: 2.14407692e-06
Iter: 700 loss: 2.14430042e-06
Iter: 701 loss: 2.14157853e-06
Iter: 702 loss: 2.13816611e-06
Iter: 703 loss: 2.18030573e-06
Iter: 704 loss: 2.13813246e-06
Iter: 705 loss: 2.13574549e-06
Iter: 706 loss: 2.13581961e-06
Iter: 707 loss: 2.13383919e-06
Iter: 708 loss: 2.1302144e-06
Iter: 709 loss: 2.14311331e-06
Iter: 710 loss: 2.12932582e-06
Iter: 711 loss: 2.12633449e-06
Iter: 712 loss: 2.16398462e-06
Iter: 713 loss: 2.12635609e-06
Iter: 714 loss: 2.12398709e-06
Iter: 715 loss: 2.13085536e-06
Iter: 716 loss: 2.12331247e-06
Iter: 717 loss: 2.12123609e-06
Iter: 718 loss: 2.11866154e-06
Iter: 719 loss: 2.11841e-06
Iter: 720 loss: 2.11465454e-06
Iter: 721 loss: 2.13448425e-06
Iter: 722 loss: 2.11401925e-06
Iter: 723 loss: 2.11117367e-06
Iter: 724 loss: 2.11081442e-06
Iter: 725 loss: 2.10886697e-06
Iter: 726 loss: 2.10536291e-06
Iter: 727 loss: 2.11675388e-06
Iter: 728 loss: 2.10439839e-06
Iter: 729 loss: 2.10069311e-06
Iter: 730 loss: 2.12856389e-06
Iter: 731 loss: 2.10045914e-06
Iter: 732 loss: 2.09806922e-06
Iter: 733 loss: 2.09566724e-06
Iter: 734 loss: 2.0951843e-06
Iter: 735 loss: 2.09107611e-06
Iter: 736 loss: 2.11858355e-06
Iter: 737 loss: 2.09059317e-06
Iter: 738 loss: 2.08772462e-06
Iter: 739 loss: 2.10002418e-06
Iter: 740 loss: 2.08712868e-06
Iter: 741 loss: 2.08435404e-06
Iter: 742 loss: 2.08547817e-06
Iter: 743 loss: 2.08248525e-06
Iter: 744 loss: 2.08018355e-06
Iter: 745 loss: 2.08014785e-06
Iter: 746 loss: 2.07821085e-06
Iter: 747 loss: 2.08294318e-06
Iter: 748 loss: 2.077506e-06
Iter: 749 loss: 2.07532389e-06
Iter: 750 loss: 2.07354447e-06
Iter: 751 loss: 2.07287576e-06
Iter: 752 loss: 2.06992718e-06
Iter: 753 loss: 2.08610982e-06
Iter: 754 loss: 2.06952382e-06
Iter: 755 loss: 2.06711229e-06
Iter: 756 loss: 2.06745153e-06
Iter: 757 loss: 2.06520849e-06
Iter: 758 loss: 2.06219602e-06
Iter: 759 loss: 2.06857021e-06
Iter: 760 loss: 2.06098912e-06
Iter: 761 loss: 2.05856168e-06
Iter: 762 loss: 2.08489701e-06
Iter: 763 loss: 2.05850529e-06
Iter: 764 loss: 2.05618949e-06
Iter: 765 loss: 2.05323749e-06
Iter: 766 loss: 2.05303036e-06
Iter: 767 loss: 2.04933576e-06
Iter: 768 loss: 2.07027711e-06
Iter: 769 loss: 2.04886874e-06
Iter: 770 loss: 2.04588696e-06
Iter: 771 loss: 2.06134087e-06
Iter: 772 loss: 2.04547723e-06
Iter: 773 loss: 2.04282446e-06
Iter: 774 loss: 2.04442267e-06
Iter: 775 loss: 2.04097296e-06
Iter: 776 loss: 2.03865784e-06
Iter: 777 loss: 2.06998084e-06
Iter: 778 loss: 2.03864079e-06
Iter: 779 loss: 2.03656145e-06
Iter: 780 loss: 2.04415528e-06
Iter: 781 loss: 2.03607442e-06
Iter: 782 loss: 2.03394416e-06
Iter: 783 loss: 2.03429386e-06
Iter: 784 loss: 2.03239529e-06
Iter: 785 loss: 2.03001241e-06
Iter: 786 loss: 2.03386116e-06
Iter: 787 loss: 2.02894739e-06
Iter: 788 loss: 2.02600904e-06
Iter: 789 loss: 2.03119725e-06
Iter: 790 loss: 2.02470869e-06
Iter: 791 loss: 2.02215e-06
Iter: 792 loss: 2.02480192e-06
Iter: 793 loss: 2.0206935e-06
Iter: 794 loss: 2.01767534e-06
Iter: 795 loss: 2.03393529e-06
Iter: 796 loss: 2.0171974e-06
Iter: 797 loss: 2.01457397e-06
Iter: 798 loss: 2.02571914e-06
Iter: 799 loss: 2.01403827e-06
Iter: 800 loss: 2.01161106e-06
Iter: 801 loss: 2.00987074e-06
Iter: 802 loss: 2.00908335e-06
Iter: 803 loss: 2.00576051e-06
Iter: 804 loss: 2.02465526e-06
Iter: 805 loss: 2.00527256e-06
Iter: 806 loss: 2.00224099e-06
Iter: 807 loss: 2.01750754e-06
Iter: 808 loss: 2.00170916e-06
Iter: 809 loss: 1.99959936e-06
Iter: 810 loss: 2.00244108e-06
Iter: 811 loss: 1.99851956e-06
Iter: 812 loss: 1.99590067e-06
Iter: 813 loss: 2.02487104e-06
Iter: 814 loss: 1.99587248e-06
Iter: 815 loss: 1.9940444e-06
Iter: 816 loss: 1.99460328e-06
Iter: 817 loss: 1.9926897e-06
Iter: 818 loss: 1.99035048e-06
Iter: 819 loss: 1.99152191e-06
Iter: 820 loss: 1.9887425e-06
Iter: 821 loss: 1.98595308e-06
Iter: 822 loss: 1.99375222e-06
Iter: 823 loss: 1.98511179e-06
Iter: 824 loss: 1.98241105e-06
Iter: 825 loss: 1.98887892e-06
Iter: 826 loss: 1.98145722e-06
Iter: 827 loss: 1.97918689e-06
Iter: 828 loss: 1.9799611e-06
Iter: 829 loss: 1.97760892e-06
Iter: 830 loss: 1.97448162e-06
Iter: 831 loss: 2.00218301e-06
Iter: 832 loss: 1.97433155e-06
Iter: 833 loss: 1.97187364e-06
Iter: 834 loss: 1.97280906e-06
Iter: 835 loss: 1.97019472e-06
Iter: 836 loss: 1.96746123e-06
Iter: 837 loss: 1.97292229e-06
Iter: 838 loss: 1.96636393e-06
Iter: 839 loss: 1.9642298e-06
Iter: 840 loss: 1.99156693e-06
Iter: 841 loss: 1.96418659e-06
Iter: 842 loss: 1.96234714e-06
Iter: 843 loss: 1.96088968e-06
Iter: 844 loss: 1.96027509e-06
Iter: 845 loss: 1.95871462e-06
Iter: 846 loss: 1.95844518e-06
Iter: 847 loss: 1.95710891e-06
Iter: 848 loss: 1.95676103e-06
Iter: 849 loss: 1.95585699e-06
Iter: 850 loss: 1.95391021e-06
Iter: 851 loss: 1.95505163e-06
Iter: 852 loss: 1.95256689e-06
Iter: 853 loss: 1.95026678e-06
Iter: 854 loss: 1.95778512e-06
Iter: 855 loss: 1.94955464e-06
Iter: 856 loss: 1.94779136e-06
Iter: 857 loss: 1.95200596e-06
Iter: 858 loss: 1.94715403e-06
Iter: 859 loss: 1.94492031e-06
Iter: 860 loss: 1.94423706e-06
Iter: 861 loss: 1.94296285e-06
Iter: 862 loss: 1.94012273e-06
Iter: 863 loss: 1.95938674e-06
Iter: 864 loss: 1.93987444e-06
Iter: 865 loss: 1.93746564e-06
Iter: 866 loss: 1.94848167e-06
Iter: 867 loss: 1.93695109e-06
Iter: 868 loss: 1.93488e-06
Iter: 869 loss: 1.93372261e-06
Iter: 870 loss: 1.93275719e-06
Iter: 871 loss: 1.93030678e-06
Iter: 872 loss: 1.94707422e-06
Iter: 873 loss: 1.93007736e-06
Iter: 874 loss: 1.92776429e-06
Iter: 875 loss: 1.93631922e-06
Iter: 876 loss: 1.92717016e-06
Iter: 877 loss: 1.92510629e-06
Iter: 878 loss: 1.93606138e-06
Iter: 879 loss: 1.92473317e-06
Iter: 880 loss: 1.92246875e-06
Iter: 881 loss: 1.93269716e-06
Iter: 882 loss: 1.9220015e-06
Iter: 883 loss: 1.92066636e-06
Iter: 884 loss: 1.92019388e-06
Iter: 885 loss: 1.91947925e-06
Iter: 886 loss: 1.91712547e-06
Iter: 887 loss: 1.9199008e-06
Iter: 888 loss: 1.91581489e-06
Iter: 889 loss: 1.91387971e-06
Iter: 890 loss: 1.92148e-06
Iter: 891 loss: 1.91341064e-06
Iter: 892 loss: 1.91127128e-06
Iter: 893 loss: 1.91279059e-06
Iter: 894 loss: 1.91003232e-06
Iter: 895 loss: 1.90718038e-06
Iter: 896 loss: 1.90979017e-06
Iter: 897 loss: 1.90555011e-06
Iter: 898 loss: 1.90320816e-06
Iter: 899 loss: 1.90321e-06
Iter: 900 loss: 1.90126627e-06
Iter: 901 loss: 1.89967454e-06
Iter: 902 loss: 1.89910099e-06
Iter: 903 loss: 1.89695231e-06
Iter: 904 loss: 1.90645187e-06
Iter: 905 loss: 1.89657794e-06
Iter: 906 loss: 1.89444791e-06
Iter: 907 loss: 1.90745914e-06
Iter: 908 loss: 1.8942585e-06
Iter: 909 loss: 1.89267791e-06
Iter: 910 loss: 1.90122523e-06
Iter: 911 loss: 1.89240916e-06
Iter: 912 loss: 1.89096954e-06
Iter: 913 loss: 1.89943762e-06
Iter: 914 loss: 1.89081607e-06
Iter: 915 loss: 1.88960246e-06
Iter: 916 loss: 1.88744241e-06
Iter: 917 loss: 1.88745912e-06
Iter: 918 loss: 1.88501451e-06
Iter: 919 loss: 1.90328183e-06
Iter: 920 loss: 1.88484523e-06
Iter: 921 loss: 1.88333502e-06
Iter: 922 loss: 1.88349031e-06
Iter: 923 loss: 1.88216723e-06
Iter: 924 loss: 1.87984472e-06
Iter: 925 loss: 1.88436502e-06
Iter: 926 loss: 1.8788561e-06
Iter: 927 loss: 1.87649459e-06
Iter: 928 loss: 1.88580816e-06
Iter: 929 loss: 1.87590888e-06
Iter: 930 loss: 1.87385422e-06
Iter: 931 loss: 1.87779949e-06
Iter: 932 loss: 1.87294165e-06
Iter: 933 loss: 1.87052035e-06
Iter: 934 loss: 1.88476054e-06
Iter: 935 loss: 1.87027774e-06
Iter: 936 loss: 1.8685646e-06
Iter: 937 loss: 1.86920761e-06
Iter: 938 loss: 1.86743443e-06
Iter: 939 loss: 1.86531497e-06
Iter: 940 loss: 1.8668004e-06
Iter: 941 loss: 1.86403042e-06
Iter: 942 loss: 1.8621223e-06
Iter: 943 loss: 1.8620417e-06
Iter: 944 loss: 1.86085549e-06
Iter: 945 loss: 1.86787861e-06
Iter: 946 loss: 1.86074283e-06
Iter: 947 loss: 1.85949136e-06
Iter: 948 loss: 1.85784279e-06
Iter: 949 loss: 1.8577241e-06
Iter: 950 loss: 1.85563192e-06
Iter: 951 loss: 1.86001762e-06
Iter: 952 loss: 1.85481952e-06
Iter: 953 loss: 1.8526539e-06
Iter: 954 loss: 1.86227282e-06
Iter: 955 loss: 1.85216709e-06
Iter: 956 loss: 1.85046451e-06
Iter: 957 loss: 1.84874466e-06
Iter: 958 loss: 1.84840746e-06
Iter: 959 loss: 1.84605437e-06
Iter: 960 loss: 1.87754949e-06
Iter: 961 loss: 1.8460521e-06
Iter: 962 loss: 1.84434043e-06
Iter: 963 loss: 1.84466842e-06
Iter: 964 loss: 1.84309874e-06
Iter: 965 loss: 1.84079329e-06
Iter: 966 loss: 1.85223894e-06
Iter: 967 loss: 1.84045246e-06
Iter: 968 loss: 1.83831537e-06
Iter: 969 loss: 1.84250439e-06
Iter: 970 loss: 1.83740485e-06
Iter: 971 loss: 1.8354292e-06
Iter: 972 loss: 1.83496297e-06
Iter: 973 loss: 1.83377642e-06
Iter: 974 loss: 1.83238092e-06
Iter: 975 loss: 1.832267e-06
Iter: 976 loss: 1.83088559e-06
Iter: 977 loss: 1.83402358e-06
Iter: 978 loss: 1.8304072e-06
Iter: 979 loss: 1.82863414e-06
Iter: 980 loss: 1.83189889e-06
Iter: 981 loss: 1.82788131e-06
Iter: 982 loss: 1.82652343e-06
Iter: 983 loss: 1.82618726e-06
Iter: 984 loss: 1.82532665e-06
Iter: 985 loss: 1.82316842e-06
Iter: 986 loss: 1.83062946e-06
Iter: 987 loss: 1.82261147e-06
Iter: 988 loss: 1.82049985e-06
Iter: 989 loss: 1.82054328e-06
Iter: 990 loss: 1.81884161e-06
Iter: 991 loss: 1.81671783e-06
Iter: 992 loss: 1.82833242e-06
Iter: 993 loss: 1.8164028e-06
Iter: 994 loss: 1.81422524e-06
Iter: 995 loss: 1.81986093e-06
Iter: 996 loss: 1.81349697e-06
Iter: 997 loss: 1.81142411e-06
Iter: 998 loss: 1.81650421e-06
Iter: 999 loss: 1.81072141e-06
Iter: 1000 loss: 1.80864811e-06
Iter: 1001 loss: 1.81893643e-06
Iter: 1002 loss: 1.80831967e-06
Iter: 1003 loss: 1.80660902e-06
Iter: 1004 loss: 1.80501445e-06
Iter: 1005 loss: 1.8046469e-06
Iter: 1006 loss: 1.80262782e-06
Iter: 1007 loss: 1.82854524e-06
Iter: 1008 loss: 1.80265465e-06
Iter: 1009 loss: 1.80111488e-06
Iter: 1010 loss: 1.81515668e-06
Iter: 1011 loss: 1.80101654e-06
Iter: 1012 loss: 1.7998226e-06
Iter: 1013 loss: 1.802322e-06
Iter: 1014 loss: 1.79933954e-06
Iter: 1015 loss: 1.7980949e-06
Iter: 1016 loss: 1.79759775e-06
Iter: 1017 loss: 1.79695451e-06
Iter: 1018 loss: 1.79522897e-06
Iter: 1019 loss: 1.79959898e-06
Iter: 1020 loss: 1.79463427e-06
Iter: 1021 loss: 1.79272934e-06
Iter: 1022 loss: 1.79766971e-06
Iter: 1023 loss: 1.79207905e-06
Iter: 1024 loss: 1.79033123e-06
Iter: 1025 loss: 1.79038489e-06
Iter: 1026 loss: 1.78892788e-06
Iter: 1027 loss: 1.78685855e-06
Iter: 1028 loss: 1.80211975e-06
Iter: 1029 loss: 1.78672838e-06
Iter: 1030 loss: 1.78507025e-06
Iter: 1031 loss: 1.79088659e-06
Iter: 1032 loss: 1.78462858e-06
Iter: 1033 loss: 1.78291771e-06
Iter: 1034 loss: 1.78414507e-06
Iter: 1035 loss: 1.78192954e-06
Iter: 1036 loss: 1.77990273e-06
Iter: 1037 loss: 1.79224696e-06
Iter: 1038 loss: 1.7796732e-06
Iter: 1039 loss: 1.77827803e-06
Iter: 1040 loss: 1.77771119e-06
Iter: 1041 loss: 1.77696779e-06
Iter: 1042 loss: 1.7754237e-06
Iter: 1043 loss: 1.77542779e-06
Iter: 1044 loss: 1.77392337e-06
Iter: 1045 loss: 1.77901961e-06
Iter: 1046 loss: 1.77357481e-06
Iter: 1047 loss: 1.77255401e-06
Iter: 1048 loss: 1.7732948e-06
Iter: 1049 loss: 1.7718844e-06
Iter: 1050 loss: 1.77031734e-06
Iter: 1051 loss: 1.76889421e-06
Iter: 1052 loss: 1.76851336e-06
Iter: 1053 loss: 1.76687627e-06
Iter: 1054 loss: 1.78610526e-06
Iter: 1055 loss: 1.76686763e-06
Iter: 1056 loss: 1.76537947e-06
Iter: 1057 loss: 1.76388062e-06
Iter: 1058 loss: 1.76360345e-06
Iter: 1059 loss: 1.76137962e-06
Iter: 1060 loss: 1.76803246e-06
Iter: 1061 loss: 1.76078561e-06
Iter: 1062 loss: 1.75877324e-06
Iter: 1063 loss: 1.77388563e-06
Iter: 1064 loss: 1.75859907e-06
Iter: 1065 loss: 1.75693481e-06
Iter: 1066 loss: 1.75963692e-06
Iter: 1067 loss: 1.75614662e-06
Iter: 1068 loss: 1.7545799e-06
Iter: 1069 loss: 1.76003198e-06
Iter: 1070 loss: 1.75409195e-06
Iter: 1071 loss: 1.7523821e-06
Iter: 1072 loss: 1.75340199e-06
Iter: 1073 loss: 1.75126229e-06
Iter: 1074 loss: 1.74937259e-06
Iter: 1075 loss: 1.75800847e-06
Iter: 1076 loss: 1.74895717e-06
Iter: 1077 loss: 1.74749107e-06
Iter: 1078 loss: 1.74749539e-06
Iter: 1079 loss: 1.74665502e-06
Iter: 1080 loss: 1.74553e-06
Iter: 1081 loss: 1.74552747e-06
Iter: 1082 loss: 1.7436746e-06
Iter: 1083 loss: 1.74867284e-06
Iter: 1084 loss: 1.74307081e-06
Iter: 1085 loss: 1.74179195e-06
Iter: 1086 loss: 1.74230559e-06
Iter: 1087 loss: 1.74098773e-06
Iter: 1088 loss: 1.73885155e-06
Iter: 1089 loss: 1.74453373e-06
Iter: 1090 loss: 1.73813e-06
Iter: 1091 loss: 1.73658213e-06
Iter: 1092 loss: 1.73951616e-06
Iter: 1093 loss: 1.73590513e-06
Iter: 1094 loss: 1.73437752e-06
Iter: 1095 loss: 1.73868648e-06
Iter: 1096 loss: 1.73389026e-06
Iter: 1097 loss: 1.73222179e-06
Iter: 1098 loss: 1.74296235e-06
Iter: 1099 loss: 1.73202852e-06
Iter: 1100 loss: 1.7308862e-06
Iter: 1101 loss: 1.73167643e-06
Iter: 1102 loss: 1.73010324e-06
Iter: 1103 loss: 1.72833836e-06
Iter: 1104 loss: 1.72920852e-06
Iter: 1105 loss: 1.72712214e-06
Iter: 1106 loss: 1.72526325e-06
Iter: 1107 loss: 1.74206878e-06
Iter: 1108 loss: 1.72517775e-06
Iter: 1109 loss: 1.72433874e-06
Iter: 1110 loss: 1.7242761e-06
Iter: 1111 loss: 1.72340663e-06
Iter: 1112 loss: 1.72159469e-06
Iter: 1113 loss: 1.75366938e-06
Iter: 1114 loss: 1.72154807e-06
Iter: 1115 loss: 1.71977194e-06
Iter: 1116 loss: 1.73521789e-06
Iter: 1117 loss: 1.71966178e-06
Iter: 1118 loss: 1.71833949e-06
Iter: 1119 loss: 1.71858505e-06
Iter: 1120 loss: 1.71731904e-06
Iter: 1121 loss: 1.71563329e-06
Iter: 1122 loss: 1.71870261e-06
Iter: 1123 loss: 1.71482225e-06
Iter: 1124 loss: 1.713149e-06
Iter: 1125 loss: 1.72397745e-06
Iter: 1126 loss: 1.71288684e-06
Iter: 1127 loss: 1.71144791e-06
Iter: 1128 loss: 1.71053784e-06
Iter: 1129 loss: 1.70999397e-06
Iter: 1130 loss: 1.70808869e-06
Iter: 1131 loss: 1.71827332e-06
Iter: 1132 loss: 1.70784506e-06
Iter: 1133 loss: 1.70614339e-06
Iter: 1134 loss: 1.71917588e-06
Iter: 1135 loss: 1.7059773e-06
Iter: 1136 loss: 1.70477801e-06
Iter: 1137 loss: 1.70349176e-06
Iter: 1138 loss: 1.70328076e-06
Iter: 1139 loss: 1.70154124e-06
Iter: 1140 loss: 1.71539978e-06
Iter: 1141 loss: 1.70139606e-06
Iter: 1142 loss: 1.7000566e-06
Iter: 1143 loss: 1.70996884e-06
Iter: 1144 loss: 1.69988255e-06
Iter: 1145 loss: 1.6984651e-06
Iter: 1146 loss: 1.7034813e-06
Iter: 1147 loss: 1.69806435e-06
Iter: 1148 loss: 1.69714406e-06
Iter: 1149 loss: 1.69669011e-06
Iter: 1150 loss: 1.69619682e-06
Iter: 1151 loss: 1.69494297e-06
Iter: 1152 loss: 1.70246585e-06
Iter: 1153 loss: 1.69487544e-06
Iter: 1154 loss: 1.69365171e-06
Iter: 1155 loss: 1.6923791e-06
Iter: 1156 loss: 1.69211876e-06
Iter: 1157 loss: 1.69054783e-06
Iter: 1158 loss: 1.70144028e-06
Iter: 1159 loss: 1.69037401e-06
Iter: 1160 loss: 1.68878296e-06
Iter: 1161 loss: 1.68949714e-06
Iter: 1162 loss: 1.6876478e-06
Iter: 1163 loss: 1.68618158e-06
Iter: 1164 loss: 1.69063401e-06
Iter: 1165 loss: 1.68574024e-06
Iter: 1166 loss: 1.68413339e-06
Iter: 1167 loss: 1.68800716e-06
Iter: 1168 loss: 1.68357178e-06
Iter: 1169 loss: 1.68143822e-06
Iter: 1170 loss: 1.68646227e-06
Iter: 1171 loss: 1.68065355e-06
Iter: 1172 loss: 1.67910389e-06
Iter: 1173 loss: 1.68261886e-06
Iter: 1174 loss: 1.67850021e-06
Iter: 1175 loss: 1.67709572e-06
Iter: 1176 loss: 1.68538861e-06
Iter: 1177 loss: 1.67692e-06
Iter: 1178 loss: 1.67559745e-06
Iter: 1179 loss: 1.69065015e-06
Iter: 1180 loss: 1.67560052e-06
Iter: 1181 loss: 1.67492612e-06
Iter: 1182 loss: 1.67436497e-06
Iter: 1183 loss: 1.6741235e-06
Iter: 1184 loss: 1.67303e-06
Iter: 1185 loss: 1.67391568e-06
Iter: 1186 loss: 1.67229325e-06
Iter: 1187 loss: 1.67077076e-06
Iter: 1188 loss: 1.67783492e-06
Iter: 1189 loss: 1.67046323e-06
Iter: 1190 loss: 1.66916254e-06
Iter: 1191 loss: 1.67023677e-06
Iter: 1192 loss: 1.66830023e-06
Iter: 1193 loss: 1.66702227e-06
Iter: 1194 loss: 1.67004259e-06
Iter: 1195 loss: 1.66655298e-06
Iter: 1196 loss: 1.6650381e-06
Iter: 1197 loss: 1.67098176e-06
Iter: 1198 loss: 1.66469715e-06
Iter: 1199 loss: 1.66322036e-06
Iter: 1200 loss: 1.66291989e-06
Iter: 1201 loss: 1.66193638e-06
Iter: 1202 loss: 1.66037853e-06
Iter: 1203 loss: 1.67309827e-06
Iter: 1204 loss: 1.66027849e-06
Iter: 1205 loss: 1.65880488e-06
Iter: 1206 loss: 1.66532186e-06
Iter: 1207 loss: 1.65849315e-06
Iter: 1208 loss: 1.65735196e-06
Iter: 1209 loss: 1.65587426e-06
Iter: 1210 loss: 1.65582014e-06
Iter: 1211 loss: 1.65579502e-06
Iter: 1212 loss: 1.65504866e-06
Iter: 1213 loss: 1.65419442e-06
Iter: 1214 loss: 1.6529093e-06
Iter: 1215 loss: 1.65290021e-06
Iter: 1216 loss: 1.65159099e-06
Iter: 1217 loss: 1.65345546e-06
Iter: 1218 loss: 1.65095571e-06
Iter: 1219 loss: 1.64977246e-06
Iter: 1220 loss: 1.66137193e-06
Iter: 1221 loss: 1.64978144e-06
Iter: 1222 loss: 1.6487661e-06
Iter: 1223 loss: 1.64816879e-06
Iter: 1224 loss: 1.64772155e-06
Iter: 1225 loss: 1.64641528e-06
Iter: 1226 loss: 1.65443612e-06
Iter: 1227 loss: 1.64625658e-06
Iter: 1228 loss: 1.64510197e-06
Iter: 1229 loss: 1.6450897e-06
Iter: 1230 loss: 1.64418702e-06
Iter: 1231 loss: 1.64270182e-06
Iter: 1232 loss: 1.64958476e-06
Iter: 1233 loss: 1.64239145e-06
Iter: 1234 loss: 1.64122071e-06
Iter: 1235 loss: 1.64545486e-06
Iter: 1236 loss: 1.64089249e-06
Iter: 1237 loss: 1.63961033e-06
Iter: 1238 loss: 1.64297069e-06
Iter: 1239 loss: 1.63918628e-06
Iter: 1240 loss: 1.63812615e-06
Iter: 1241 loss: 1.64074879e-06
Iter: 1242 loss: 1.63769914e-06
Iter: 1243 loss: 1.63630307e-06
Iter: 1244 loss: 1.63933908e-06
Iter: 1245 loss: 1.63576169e-06
Iter: 1246 loss: 1.63456025e-06
Iter: 1247 loss: 1.63688424e-06
Iter: 1248 loss: 1.63405048e-06
Iter: 1249 loss: 1.63327331e-06
Iter: 1250 loss: 1.6331694e-06
Iter: 1251 loss: 1.63257801e-06
Iter: 1252 loss: 1.63104676e-06
Iter: 1253 loss: 1.64263065e-06
Iter: 1254 loss: 1.63075765e-06
Iter: 1255 loss: 1.62945128e-06
Iter: 1256 loss: 1.6327765e-06
Iter: 1257 loss: 1.62901642e-06
Iter: 1258 loss: 1.62751235e-06
Iter: 1259 loss: 1.63381412e-06
Iter: 1260 loss: 1.62713206e-06
Iter: 1261 loss: 1.62589583e-06
Iter: 1262 loss: 1.63734717e-06
Iter: 1263 loss: 1.62583819e-06
Iter: 1264 loss: 1.6250699e-06
Iter: 1265 loss: 1.62602578e-06
Iter: 1266 loss: 1.6246629e-06
Iter: 1267 loss: 1.62354968e-06
Iter: 1268 loss: 1.62473e-06
Iter: 1269 loss: 1.62297715e-06
Iter: 1270 loss: 1.62184108e-06
Iter: 1271 loss: 1.6254819e-06
Iter: 1272 loss: 1.62153265e-06
Iter: 1273 loss: 1.62055483e-06
Iter: 1274 loss: 1.62234426e-06
Iter: 1275 loss: 1.62019501e-06
Iter: 1276 loss: 1.61889579e-06
Iter: 1277 loss: 1.62178617e-06
Iter: 1278 loss: 1.61841069e-06
Iter: 1279 loss: 1.61733647e-06
Iter: 1280 loss: 1.62360197e-06
Iter: 1281 loss: 1.61716707e-06
Iter: 1282 loss: 1.61651712e-06
Iter: 1283 loss: 1.62280037e-06
Iter: 1284 loss: 1.61649552e-06
Iter: 1285 loss: 1.61568789e-06
Iter: 1286 loss: 1.61683045e-06
Iter: 1287 loss: 1.61538935e-06
Iter: 1288 loss: 1.61458081e-06
Iter: 1289 loss: 1.6158881e-06
Iter: 1290 loss: 1.61428011e-06
Iter: 1291 loss: 1.61362834e-06
Iter: 1292 loss: 1.61232754e-06
Iter: 1293 loss: 1.63375182e-06
Iter: 1294 loss: 1.61227888e-06
Iter: 1295 loss: 1.6108454e-06
Iter: 1296 loss: 1.62205572e-06
Iter: 1297 loss: 1.61079618e-06
Iter: 1298 loss: 1.6096875e-06
Iter: 1299 loss: 1.61673381e-06
Iter: 1300 loss: 1.6095828e-06
Iter: 1301 loss: 1.60849731e-06
Iter: 1302 loss: 1.61126684e-06
Iter: 1303 loss: 1.60811533e-06
Iter: 1304 loss: 1.60737784e-06
Iter: 1305 loss: 1.60714671e-06
Iter: 1306 loss: 1.60667685e-06
Iter: 1307 loss: 1.60550121e-06
Iter: 1308 loss: 1.61092225e-06
Iter: 1309 loss: 1.60526088e-06
Iter: 1310 loss: 1.6041771e-06
Iter: 1311 loss: 1.60796367e-06
Iter: 1312 loss: 1.6038656e-06
Iter: 1313 loss: 1.60288027e-06
Iter: 1314 loss: 1.60373986e-06
Iter: 1315 loss: 1.6023198e-06
Iter: 1316 loss: 1.60145123e-06
Iter: 1317 loss: 1.61209834e-06
Iter: 1318 loss: 1.6014535e-06
Iter: 1319 loss: 1.60065701e-06
Iter: 1320 loss: 1.60414879e-06
Iter: 1321 loss: 1.60053412e-06
Iter: 1322 loss: 1.59985461e-06
Iter: 1323 loss: 1.5999367e-06
Iter: 1324 loss: 1.59936167e-06
Iter: 1325 loss: 1.59843842e-06
Iter: 1326 loss: 1.5981575e-06
Iter: 1327 loss: 1.59761692e-06
Iter: 1328 loss: 1.59642e-06
Iter: 1329 loss: 1.59629644e-06
Iter: 1330 loss: 1.59541696e-06
Iter: 1331 loss: 1.59388787e-06
Iter: 1332 loss: 1.60608602e-06
Iter: 1333 loss: 1.593722e-06
Iter: 1334 loss: 1.59296053e-06
Iter: 1335 loss: 1.59294655e-06
Iter: 1336 loss: 1.59236242e-06
Iter: 1337 loss: 1.59146896e-06
Iter: 1338 loss: 1.59142e-06
Iter: 1339 loss: 1.59024762e-06
Iter: 1340 loss: 1.59657179e-06
Iter: 1341 loss: 1.59008903e-06
Iter: 1342 loss: 1.58909074e-06
Iter: 1343 loss: 1.59149863e-06
Iter: 1344 loss: 1.58873183e-06
Iter: 1345 loss: 1.58778448e-06
Iter: 1346 loss: 1.58976025e-06
Iter: 1347 loss: 1.58741068e-06
Iter: 1348 loss: 1.58640114e-06
Iter: 1349 loss: 1.59234276e-06
Iter: 1350 loss: 1.5862513e-06
Iter: 1351 loss: 1.58557646e-06
Iter: 1352 loss: 1.59430238e-06
Iter: 1353 loss: 1.58555736e-06
Iter: 1354 loss: 1.58494822e-06
Iter: 1355 loss: 1.58438922e-06
Iter: 1356 loss: 1.58419743e-06
Iter: 1357 loss: 1.58327816e-06
Iter: 1358 loss: 1.58438684e-06
Iter: 1359 loss: 1.5827768e-06
Iter: 1360 loss: 1.58180183e-06
Iter: 1361 loss: 1.58563182e-06
Iter: 1362 loss: 1.58156672e-06
Iter: 1363 loss: 1.58069315e-06
Iter: 1364 loss: 1.5816463e-06
Iter: 1365 loss: 1.58012676e-06
Iter: 1366 loss: 1.57910915e-06
Iter: 1367 loss: 1.5790074e-06
Iter: 1368 loss: 1.57826275e-06
Iter: 1369 loss: 1.57710599e-06
Iter: 1370 loss: 1.57709724e-06
Iter: 1371 loss: 1.57616114e-06
Iter: 1372 loss: 1.57652335e-06
Iter: 1373 loss: 1.57549334e-06
Iter: 1374 loss: 1.57440445e-06
Iter: 1375 loss: 1.57411057e-06
Iter: 1376 loss: 1.57336274e-06
Iter: 1377 loss: 1.57217369e-06
Iter: 1378 loss: 1.57918782e-06
Iter: 1379 loss: 1.57202885e-06
Iter: 1380 loss: 1.57066893e-06
Iter: 1381 loss: 1.57518502e-06
Iter: 1382 loss: 1.5702974e-06
Iter: 1383 loss: 1.5694219e-06
Iter: 1384 loss: 1.58180364e-06
Iter: 1385 loss: 1.56940791e-06
Iter: 1386 loss: 1.56866872e-06
Iter: 1387 loss: 1.56970179e-06
Iter: 1388 loss: 1.56823512e-06
Iter: 1389 loss: 1.56757096e-06
Iter: 1390 loss: 1.56698513e-06
Iter: 1391 loss: 1.56679221e-06
Iter: 1392 loss: 1.56553324e-06
Iter: 1393 loss: 1.57026238e-06
Iter: 1394 loss: 1.56523538e-06
Iter: 1395 loss: 1.56418923e-06
Iter: 1396 loss: 1.56586714e-06
Iter: 1397 loss: 1.56369856e-06
Iter: 1398 loss: 1.56254487e-06
Iter: 1399 loss: 1.56467104e-06
Iter: 1400 loss: 1.56201156e-06
Iter: 1401 loss: 1.56099622e-06
Iter: 1402 loss: 1.56904434e-06
Iter: 1403 loss: 1.56090164e-06
Iter: 1404 loss: 1.56010378e-06
Iter: 1405 loss: 1.56162764e-06
Iter: 1406 loss: 1.55976818e-06
Iter: 1407 loss: 1.55871419e-06
Iter: 1408 loss: 1.56093847e-06
Iter: 1409 loss: 1.5582882e-06
Iter: 1410 loss: 1.55728856e-06
Iter: 1411 loss: 1.55763951e-06
Iter: 1412 loss: 1.55655289e-06
Iter: 1413 loss: 1.55531075e-06
Iter: 1414 loss: 1.55699649e-06
Iter: 1415 loss: 1.55476982e-06
Iter: 1416 loss: 1.55378029e-06
Iter: 1417 loss: 1.55379632e-06
Iter: 1418 loss: 1.55279633e-06
Iter: 1419 loss: 1.55708926e-06
Iter: 1420 loss: 1.55261273e-06
Iter: 1421 loss: 1.55194584e-06
Iter: 1422 loss: 1.55163104e-06
Iter: 1423 loss: 1.55123712e-06
Iter: 1424 loss: 1.55011799e-06
Iter: 1425 loss: 1.55107625e-06
Iter: 1426 loss: 1.5493888e-06
Iter: 1427 loss: 1.54827626e-06
Iter: 1428 loss: 1.55049315e-06
Iter: 1429 loss: 1.54778104e-06
Iter: 1430 loss: 1.54654629e-06
Iter: 1431 loss: 1.55581688e-06
Iter: 1432 loss: 1.54640725e-06
Iter: 1433 loss: 1.54567101e-06
Iter: 1434 loss: 1.5456568e-06
Iter: 1435 loss: 1.54494671e-06
Iter: 1436 loss: 1.54358304e-06
Iter: 1437 loss: 1.54489248e-06
Iter: 1438 loss: 1.54274358e-06
Iter: 1439 loss: 1.54148643e-06
Iter: 1440 loss: 1.54145971e-06
Iter: 1441 loss: 1.54062889e-06
Iter: 1442 loss: 1.53927704e-06
Iter: 1443 loss: 1.53928954e-06
Iter: 1444 loss: 1.53769486e-06
Iter: 1445 loss: 1.54444274e-06
Iter: 1446 loss: 1.53740439e-06
Iter: 1447 loss: 1.53628e-06
Iter: 1448 loss: 1.54160728e-06
Iter: 1449 loss: 1.53604196e-06
Iter: 1450 loss: 1.53505368e-06
Iter: 1451 loss: 1.54994291e-06
Iter: 1452 loss: 1.53503447e-06
Iter: 1453 loss: 1.53427482e-06
Iter: 1454 loss: 1.53386191e-06
Iter: 1455 loss: 1.53352335e-06
Iter: 1456 loss: 1.53254814e-06
Iter: 1457 loss: 1.53278086e-06
Iter: 1458 loss: 1.53175756e-06
Iter: 1459 loss: 1.53054839e-06
Iter: 1460 loss: 1.53480164e-06
Iter: 1461 loss: 1.53027258e-06
Iter: 1462 loss: 1.52895041e-06
Iter: 1463 loss: 1.53239228e-06
Iter: 1464 loss: 1.52850487e-06
Iter: 1465 loss: 1.52752932e-06
Iter: 1466 loss: 1.52896314e-06
Iter: 1467 loss: 1.52709708e-06
Iter: 1468 loss: 1.52584039e-06
Iter: 1469 loss: 1.53146743e-06
Iter: 1470 loss: 1.525578e-06
Iter: 1471 loss: 1.52450582e-06
Iter: 1472 loss: 1.52681253e-06
Iter: 1473 loss: 1.52408165e-06
Iter: 1474 loss: 1.52289124e-06
Iter: 1475 loss: 1.52716166e-06
Iter: 1476 loss: 1.52264033e-06
Iter: 1477 loss: 1.52140865e-06
Iter: 1478 loss: 1.52322491e-06
Iter: 1479 loss: 1.52082794e-06
Iter: 1480 loss: 1.51968902e-06
Iter: 1481 loss: 1.52111807e-06
Iter: 1482 loss: 1.51911991e-06
Iter: 1483 loss: 1.51808786e-06
Iter: 1484 loss: 1.5268779e-06
Iter: 1485 loss: 1.51800702e-06
Iter: 1486 loss: 1.51676772e-06
Iter: 1487 loss: 1.52153621e-06
Iter: 1488 loss: 1.5164843e-06
Iter: 1489 loss: 1.51583617e-06
Iter: 1490 loss: 1.51438155e-06
Iter: 1491 loss: 1.53938959e-06
Iter: 1492 loss: 1.51427832e-06
Iter: 1493 loss: 1.51296035e-06
Iter: 1494 loss: 1.5284063e-06
Iter: 1495 loss: 1.51289623e-06
Iter: 1496 loss: 1.51181086e-06
Iter: 1497 loss: 1.51309132e-06
Iter: 1498 loss: 1.51127824e-06
Iter: 1499 loss: 1.51002644e-06
Iter: 1500 loss: 1.51101267e-06
Iter: 1501 loss: 1.50934852e-06
Iter: 1502 loss: 1.50808091e-06
Iter: 1503 loss: 1.51992617e-06
Iter: 1504 loss: 1.50800338e-06
Iter: 1505 loss: 1.50695064e-06
Iter: 1506 loss: 1.5060607e-06
Iter: 1507 loss: 1.50576761e-06
Iter: 1508 loss: 1.50413302e-06
Iter: 1509 loss: 1.52349e-06
Iter: 1510 loss: 1.50408971e-06
Iter: 1511 loss: 1.50304072e-06
Iter: 1512 loss: 1.50467849e-06
Iter: 1513 loss: 1.5025912e-06
Iter: 1514 loss: 1.5015496e-06
Iter: 1515 loss: 1.50373376e-06
Iter: 1516 loss: 1.5011376e-06
Iter: 1517 loss: 1.50010351e-06
Iter: 1518 loss: 1.50335825e-06
Iter: 1519 loss: 1.4997247e-06
Iter: 1520 loss: 1.49894345e-06
Iter: 1521 loss: 1.4989389e-06
Iter: 1522 loss: 1.49826906e-06
Iter: 1523 loss: 1.49711605e-06
Iter: 1524 loss: 1.49710911e-06
Iter: 1525 loss: 1.49596849e-06
Iter: 1526 loss: 1.49629898e-06
Iter: 1527 loss: 1.49516131e-06
Iter: 1528 loss: 1.49370067e-06
Iter: 1529 loss: 1.50062397e-06
Iter: 1530 loss: 1.49343327e-06
Iter: 1531 loss: 1.49221592e-06
Iter: 1532 loss: 1.50047481e-06
Iter: 1533 loss: 1.49212462e-06
Iter: 1534 loss: 1.49121479e-06
Iter: 1535 loss: 1.49032167e-06
Iter: 1536 loss: 1.49012931e-06
Iter: 1537 loss: 1.48862978e-06
Iter: 1538 loss: 1.49573179e-06
Iter: 1539 loss: 1.48831964e-06
Iter: 1540 loss: 1.48696245e-06
Iter: 1541 loss: 1.49837251e-06
Iter: 1542 loss: 1.48684217e-06
Iter: 1543 loss: 1.48594199e-06
Iter: 1544 loss: 1.48583899e-06
Iter: 1545 loss: 1.4851297e-06
Iter: 1546 loss: 1.48381469e-06
Iter: 1547 loss: 1.49351695e-06
Iter: 1548 loss: 1.48366917e-06
Iter: 1549 loss: 1.48280174e-06
Iter: 1550 loss: 1.48233494e-06
Iter: 1551 loss: 1.48194886e-06
Iter: 1552 loss: 1.4809641e-06
Iter: 1553 loss: 1.48098729e-06
Iter: 1554 loss: 1.47989613e-06
Iter: 1555 loss: 1.4797472e-06
Iter: 1556 loss: 1.47896253e-06
Iter: 1557 loss: 1.47792775e-06
Iter: 1558 loss: 1.47895207e-06
Iter: 1559 loss: 1.47736785e-06
Iter: 1560 loss: 1.47609558e-06
Iter: 1561 loss: 1.47608409e-06
Iter: 1562 loss: 1.4751032e-06
Iter: 1563 loss: 1.47378955e-06
Iter: 1564 loss: 1.48545701e-06
Iter: 1565 loss: 1.47374521e-06
Iter: 1566 loss: 1.47267542e-06
Iter: 1567 loss: 1.47499088e-06
Iter: 1568 loss: 1.47223523e-06
Iter: 1569 loss: 1.47089781e-06
Iter: 1570 loss: 1.47151457e-06
Iter: 1571 loss: 1.46993796e-06
Iter: 1572 loss: 1.46853813e-06
Iter: 1573 loss: 1.47529465e-06
Iter: 1574 loss: 1.46831439e-06
Iter: 1575 loss: 1.46703655e-06
Iter: 1576 loss: 1.47363403e-06
Iter: 1577 loss: 1.46684272e-06
Iter: 1578 loss: 1.46560762e-06
Iter: 1579 loss: 1.46615866e-06
Iter: 1580 loss: 1.46474383e-06
Iter: 1581 loss: 1.46345906e-06
Iter: 1582 loss: 1.46720572e-06
Iter: 1583 loss: 1.46301704e-06
Iter: 1584 loss: 1.46152217e-06
Iter: 1585 loss: 1.46907723e-06
Iter: 1586 loss: 1.46132152e-06
Iter: 1587 loss: 1.46052651e-06
Iter: 1588 loss: 1.46048933e-06
Iter: 1589 loss: 1.45994136e-06
Iter: 1590 loss: 1.45849117e-06
Iter: 1591 loss: 1.46956904e-06
Iter: 1592 loss: 1.45823685e-06
Iter: 1593 loss: 1.45678064e-06
Iter: 1594 loss: 1.46224761e-06
Iter: 1595 loss: 1.45643378e-06
Iter: 1596 loss: 1.45517515e-06
Iter: 1597 loss: 1.46282184e-06
Iter: 1598 loss: 1.45503316e-06
Iter: 1599 loss: 1.45382216e-06
Iter: 1600 loss: 1.4547245e-06
Iter: 1601 loss: 1.45310696e-06
Iter: 1602 loss: 1.45186345e-06
Iter: 1603 loss: 1.46000298e-06
Iter: 1604 loss: 1.4517193e-06
Iter: 1605 loss: 1.45057379e-06
Iter: 1606 loss: 1.45176682e-06
Iter: 1607 loss: 1.44996318e-06
Iter: 1608 loss: 1.44866044e-06
Iter: 1609 loss: 1.45136266e-06
Iter: 1610 loss: 1.44815101e-06
Iter: 1611 loss: 1.4469806e-06
Iter: 1612 loss: 1.46002935e-06
Iter: 1613 loss: 1.44695809e-06
Iter: 1614 loss: 1.44618298e-06
Iter: 1615 loss: 1.44480691e-06
Iter: 1616 loss: 1.44482885e-06
Iter: 1617 loss: 1.44350065e-06
Iter: 1618 loss: 1.4631371e-06
Iter: 1619 loss: 1.44349781e-06
Iter: 1620 loss: 1.44276328e-06
Iter: 1621 loss: 1.45088757e-06
Iter: 1622 loss: 1.44276851e-06
Iter: 1623 loss: 1.44208036e-06
Iter: 1624 loss: 1.4413821e-06
Iter: 1625 loss: 1.44123123e-06
Iter: 1626 loss: 1.43999205e-06
Iter: 1627 loss: 1.44029764e-06
Iter: 1628 loss: 1.43916475e-06
Iter: 1629 loss: 1.43807551e-06
Iter: 1630 loss: 1.4388288e-06
Iter: 1631 loss: 1.43740567e-06
Iter: 1632 loss: 1.43583418e-06
Iter: 1633 loss: 1.43857574e-06
Iter: 1634 loss: 1.43511727e-06
Iter: 1635 loss: 1.43373745e-06
Iter: 1636 loss: 1.45459308e-06
Iter: 1637 loss: 1.43375291e-06
Iter: 1638 loss: 1.43275156e-06
Iter: 1639 loss: 1.43203579e-06
Iter: 1640 loss: 1.43160491e-06
Iter: 1641 loss: 1.43026045e-06
Iter: 1642 loss: 1.44232035e-06
Iter: 1643 loss: 1.43016177e-06
Iter: 1644 loss: 1.42912791e-06
Iter: 1645 loss: 1.43117643e-06
Iter: 1646 loss: 1.42873239e-06
Iter: 1647 loss: 1.42760405e-06
Iter: 1648 loss: 1.43057798e-06
Iter: 1649 loss: 1.42725435e-06
Iter: 1650 loss: 1.4260329e-06
Iter: 1651 loss: 1.42781164e-06
Iter: 1652 loss: 1.42535669e-06
Iter: 1653 loss: 1.42440456e-06
Iter: 1654 loss: 1.43659713e-06
Iter: 1655 loss: 1.42440012e-06
Iter: 1656 loss: 1.42334807e-06
Iter: 1657 loss: 1.42540546e-06
Iter: 1658 loss: 1.42296631e-06
Iter: 1659 loss: 1.42227123e-06
Iter: 1660 loss: 1.42234421e-06
Iter: 1661 loss: 1.42175259e-06
Iter: 1662 loss: 1.42074953e-06
Iter: 1663 loss: 1.42138197e-06
Iter: 1664 loss: 1.42009867e-06
Iter: 1665 loss: 1.4189302e-06
Iter: 1666 loss: 1.41997953e-06
Iter: 1667 loss: 1.41826422e-06
Iter: 1668 loss: 1.41682062e-06
Iter: 1669 loss: 1.42124782e-06
Iter: 1670 loss: 1.41642818e-06
Iter: 1671 loss: 1.41507326e-06
Iter: 1672 loss: 1.42149815e-06
Iter: 1673 loss: 1.41485611e-06
Iter: 1674 loss: 1.41376518e-06
Iter: 1675 loss: 1.42262911e-06
Iter: 1676 loss: 1.41367582e-06
Iter: 1677 loss: 1.41289956e-06
Iter: 1678 loss: 1.41195756e-06
Iter: 1679 loss: 1.41188707e-06
Iter: 1680 loss: 1.41077908e-06
Iter: 1681 loss: 1.41075941e-06
Iter: 1682 loss: 1.4099312e-06
Iter: 1683 loss: 1.40948782e-06
Iter: 1684 loss: 1.40906081e-06
Iter: 1685 loss: 1.40794134e-06
Iter: 1686 loss: 1.41543569e-06
Iter: 1687 loss: 1.40785369e-06
Iter: 1688 loss: 1.40688189e-06
Iter: 1689 loss: 1.41464432e-06
Iter: 1690 loss: 1.40680936e-06
Iter: 1691 loss: 1.40612724e-06
Iter: 1692 loss: 1.40596148e-06
Iter: 1693 loss: 1.40557427e-06
Iter: 1694 loss: 1.40461964e-06
Iter: 1695 loss: 1.40414159e-06
Iter: 1696 loss: 1.4036666e-06
Iter: 1697 loss: 1.40251041e-06
Iter: 1698 loss: 1.40932138e-06
Iter: 1699 loss: 1.40237762e-06
Iter: 1700 loss: 1.40114457e-06
Iter: 1701 loss: 1.40174609e-06
Iter: 1702 loss: 1.40036445e-06
Iter: 1703 loss: 1.39901908e-06
Iter: 1704 loss: 1.40197835e-06
Iter: 1705 loss: 1.39842655e-06
Iter: 1706 loss: 1.39721158e-06
Iter: 1707 loss: 1.40644624e-06
Iter: 1708 loss: 1.39711824e-06
Iter: 1709 loss: 1.39614349e-06
Iter: 1710 loss: 1.4004595e-06
Iter: 1711 loss: 1.39587314e-06
Iter: 1712 loss: 1.39509211e-06
Iter: 1713 loss: 1.39477061e-06
Iter: 1714 loss: 1.39438725e-06
Iter: 1715 loss: 1.39303506e-06
Iter: 1716 loss: 1.40069164e-06
Iter: 1717 loss: 1.39278063e-06
Iter: 1718 loss: 1.39170368e-06
Iter: 1719 loss: 1.39555596e-06
Iter: 1720 loss: 1.3913816e-06
Iter: 1721 loss: 1.39081612e-06
Iter: 1722 loss: 1.39081419e-06
Iter: 1723 loss: 1.39023268e-06
Iter: 1724 loss: 1.38919813e-06
Iter: 1725 loss: 1.38918745e-06
Iter: 1726 loss: 1.38825487e-06
Iter: 1727 loss: 1.39113877e-06
Iter: 1728 loss: 1.38793882e-06
Iter: 1729 loss: 1.38699045e-06
Iter: 1730 loss: 1.38874236e-06
Iter: 1731 loss: 1.38657765e-06
Iter: 1732 loss: 1.38533551e-06
Iter: 1733 loss: 1.38448797e-06
Iter: 1734 loss: 1.38412372e-06
Iter: 1735 loss: 1.38269297e-06
Iter: 1736 loss: 1.40309487e-06
Iter: 1737 loss: 1.38268501e-06
Iter: 1738 loss: 1.38184305e-06
Iter: 1739 loss: 1.3831941e-06
Iter: 1740 loss: 1.38139376e-06
Iter: 1741 loss: 1.38031453e-06
Iter: 1742 loss: 1.38148891e-06
Iter: 1743 loss: 1.37972586e-06
Iter: 1744 loss: 1.37857e-06
Iter: 1745 loss: 1.39033057e-06
Iter: 1746 loss: 1.37854e-06
Iter: 1747 loss: 1.37763527e-06
Iter: 1748 loss: 1.37653933e-06
Iter: 1749 loss: 1.37641985e-06
Iter: 1750 loss: 1.37557868e-06
Iter: 1751 loss: 1.37553366e-06
Iter: 1752 loss: 1.3749011e-06
Iter: 1753 loss: 1.37542395e-06
Iter: 1754 loss: 1.37452412e-06
Iter: 1755 loss: 1.37351458e-06
Iter: 1756 loss: 1.37821382e-06
Iter: 1757 loss: 1.37337e-06
Iter: 1758 loss: 1.37263578e-06
Iter: 1759 loss: 1.37137749e-06
Iter: 1760 loss: 1.37138704e-06
Iter: 1761 loss: 1.37026962e-06
Iter: 1762 loss: 1.37591428e-06
Iter: 1763 loss: 1.37005793e-06
Iter: 1764 loss: 1.36899928e-06
Iter: 1765 loss: 1.37182292e-06
Iter: 1766 loss: 1.36863014e-06
Iter: 1767 loss: 1.36768017e-06
Iter: 1768 loss: 1.36885762e-06
Iter: 1769 loss: 1.3672352e-06
Iter: 1770 loss: 1.36609765e-06
Iter: 1771 loss: 1.36833751e-06
Iter: 1772 loss: 1.36557105e-06
Iter: 1773 loss: 1.36454844e-06
Iter: 1774 loss: 1.37180223e-06
Iter: 1775 loss: 1.36442168e-06
Iter: 1776 loss: 1.36347228e-06
Iter: 1777 loss: 1.36438859e-06
Iter: 1778 loss: 1.36286644e-06
Iter: 1779 loss: 1.36186759e-06
Iter: 1780 loss: 1.36945982e-06
Iter: 1781 loss: 1.36179756e-06
Iter: 1782 loss: 1.36095377e-06
Iter: 1783 loss: 1.36160543e-06
Iter: 1784 loss: 1.36045458e-06
Iter: 1785 loss: 1.35944538e-06
Iter: 1786 loss: 1.36528377e-06
Iter: 1787 loss: 1.35931725e-06
Iter: 1788 loss: 1.35837558e-06
Iter: 1789 loss: 1.36464541e-06
Iter: 1790 loss: 1.35829282e-06
Iter: 1791 loss: 1.35758035e-06
Iter: 1792 loss: 1.35868072e-06
Iter: 1793 loss: 1.35725031e-06
Iter: 1794 loss: 1.35672872e-06
Iter: 1795 loss: 1.35568e-06
Iter: 1796 loss: 1.37516531e-06
Iter: 1797 loss: 1.35562482e-06
Iter: 1798 loss: 1.35436562e-06
Iter: 1799 loss: 1.36155e-06
Iter: 1800 loss: 1.35421078e-06
Iter: 1801 loss: 1.35309801e-06
Iter: 1802 loss: 1.36027336e-06
Iter: 1803 loss: 1.352975e-06
Iter: 1804 loss: 1.35217329e-06
Iter: 1805 loss: 1.35121263e-06
Iter: 1806 loss: 1.35110929e-06
Iter: 1807 loss: 1.34976926e-06
Iter: 1808 loss: 1.36039193e-06
Iter: 1809 loss: 1.34967695e-06
Iter: 1810 loss: 1.34864683e-06
Iter: 1811 loss: 1.35383675e-06
Iter: 1812 loss: 1.34850268e-06
Iter: 1813 loss: 1.34750212e-06
Iter: 1814 loss: 1.34838479e-06
Iter: 1815 loss: 1.34693437e-06
Iter: 1816 loss: 1.34593324e-06
Iter: 1817 loss: 1.3538405e-06
Iter: 1818 loss: 1.34589118e-06
Iter: 1819 loss: 1.34505922e-06
Iter: 1820 loss: 1.34561969e-06
Iter: 1821 loss: 1.34455536e-06
Iter: 1822 loss: 1.34380412e-06
Iter: 1823 loss: 1.34380025e-06
Iter: 1824 loss: 1.34328604e-06
Iter: 1825 loss: 1.34362756e-06
Iter: 1826 loss: 1.34300467e-06
Iter: 1827 loss: 1.34233574e-06
Iter: 1828 loss: 1.34140032e-06
Iter: 1829 loss: 1.34132711e-06
Iter: 1830 loss: 1.34037202e-06
Iter: 1831 loss: 1.34330844e-06
Iter: 1832 loss: 1.34005109e-06
Iter: 1833 loss: 1.33906224e-06
Iter: 1834 loss: 1.34277684e-06
Iter: 1835 loss: 1.33881781e-06
Iter: 1836 loss: 1.33776939e-06
Iter: 1837 loss: 1.34133757e-06
Iter: 1838 loss: 1.33751075e-06
Iter: 1839 loss: 1.33658818e-06
Iter: 1840 loss: 1.33660865e-06
Iter: 1841 loss: 1.33580374e-06
Iter: 1842 loss: 1.33464914e-06
Iter: 1843 loss: 1.34118579e-06
Iter: 1844 loss: 1.33446906e-06
Iter: 1845 loss: 1.33361391e-06
Iter: 1846 loss: 1.33998674e-06
Iter: 1847 loss: 1.33358208e-06
Iter: 1848 loss: 1.33278115e-06
Iter: 1849 loss: 1.33232516e-06
Iter: 1850 loss: 1.33190099e-06
Iter: 1851 loss: 1.33086178e-06
Iter: 1852 loss: 1.34087963e-06
Iter: 1853 loss: 1.33082699e-06
Iter: 1854 loss: 1.33008598e-06
Iter: 1855 loss: 1.33436856e-06
Iter: 1856 loss: 1.33000117e-06
Iter: 1857 loss: 1.32918944e-06
Iter: 1858 loss: 1.3305322e-06
Iter: 1859 loss: 1.32876812e-06
Iter: 1860 loss: 1.32809009e-06
Iter: 1861 loss: 1.32803495e-06
Iter: 1862 loss: 1.32748414e-06
Iter: 1863 loss: 1.32657942e-06
Iter: 1864 loss: 1.32932541e-06
Iter: 1865 loss: 1.32631567e-06
Iter: 1866 loss: 1.3254961e-06
Iter: 1867 loss: 1.32514674e-06
Iter: 1868 loss: 1.32469381e-06
Iter: 1869 loss: 1.32347748e-06
Iter: 1870 loss: 1.32699006e-06
Iter: 1871 loss: 1.32313141e-06
Iter: 1872 loss: 1.32195919e-06
Iter: 1873 loss: 1.33542062e-06
Iter: 1874 loss: 1.32193827e-06
Iter: 1875 loss: 1.32119681e-06
Iter: 1876 loss: 1.32113757e-06
Iter: 1877 loss: 1.32064201e-06
Iter: 1878 loss: 1.31979527e-06
Iter: 1879 loss: 1.32251125e-06
Iter: 1880 loss: 1.3195538e-06
Iter: 1881 loss: 1.31844786e-06
Iter: 1882 loss: 1.32348168e-06
Iter: 1883 loss: 1.31830257e-06
Iter: 1884 loss: 1.31742195e-06
Iter: 1885 loss: 1.31923139e-06
Iter: 1886 loss: 1.31710988e-06
Iter: 1887 loss: 1.31632896e-06
Iter: 1888 loss: 1.32169055e-06
Iter: 1889 loss: 1.31624506e-06
Iter: 1890 loss: 1.31548938e-06
Iter: 1891 loss: 1.31852357e-06
Iter: 1892 loss: 1.31530453e-06
Iter: 1893 loss: 1.3147046e-06
Iter: 1894 loss: 1.31499769e-06
Iter: 1895 loss: 1.31422894e-06
Iter: 1896 loss: 1.3136314e-06
Iter: 1897 loss: 1.31375202e-06
Iter: 1898 loss: 1.313192e-06
Iter: 1899 loss: 1.3121437e-06
Iter: 1900 loss: 1.31321349e-06
Iter: 1901 loss: 1.31153547e-06
Iter: 1902 loss: 1.31053093e-06
Iter: 1903 loss: 1.3143e-06
Iter: 1904 loss: 1.31028355e-06
Iter: 1905 loss: 1.30927606e-06
Iter: 1906 loss: 1.30976355e-06
Iter: 1907 loss: 1.30854949e-06
Iter: 1908 loss: 1.30751118e-06
Iter: 1909 loss: 1.3208919e-06
Iter: 1910 loss: 1.3075213e-06
Iter: 1911 loss: 1.30661761e-06
Iter: 1912 loss: 1.30657338e-06
Iter: 1913 loss: 1.3059921e-06
Iter: 1914 loss: 1.30480839e-06
Iter: 1915 loss: 1.30777175e-06
Iter: 1916 loss: 1.30438775e-06
Iter: 1917 loss: 1.30356079e-06
Iter: 1918 loss: 1.31623256e-06
Iter: 1919 loss: 1.30355136e-06
Iter: 1920 loss: 1.30285514e-06
Iter: 1921 loss: 1.30202829e-06
Iter: 1922 loss: 1.30194155e-06
Iter: 1923 loss: 1.30129138e-06
Iter: 1924 loss: 1.30115814e-06
Iter: 1925 loss: 1.30060766e-06
Iter: 1926 loss: 1.29993509e-06
Iter: 1927 loss: 1.29987257e-06
Iter: 1928 loss: 1.29899308e-06
Iter: 1929 loss: 1.30200499e-06
Iter: 1930 loss: 1.2987291e-06
Iter: 1931 loss: 1.29799264e-06
Iter: 1932 loss: 1.29765169e-06
Iter: 1933 loss: 1.29724901e-06
Iter: 1934 loss: 1.29624e-06
Iter: 1935 loss: 1.30122339e-06
Iter: 1936 loss: 1.2960445e-06
Iter: 1937 loss: 1.29503644e-06
Iter: 1938 loss: 1.29818602e-06
Iter: 1939 loss: 1.29478212e-06
Iter: 1940 loss: 1.29388104e-06
Iter: 1941 loss: 1.29570674e-06
Iter: 1942 loss: 1.29350951e-06
Iter: 1943 loss: 1.29262889e-06
Iter: 1944 loss: 1.29592672e-06
Iter: 1945 loss: 1.29238492e-06
Iter: 1946 loss: 1.29151169e-06
Iter: 1947 loss: 1.29429179e-06
Iter: 1948 loss: 1.29123407e-06
Iter: 1949 loss: 1.29052091e-06
Iter: 1950 loss: 1.29175191e-06
Iter: 1951 loss: 1.29017781e-06
Iter: 1952 loss: 1.28922511e-06
Iter: 1953 loss: 1.2937694e-06
Iter: 1954 loss: 1.28902923e-06
Iter: 1955 loss: 1.28828037e-06
Iter: 1956 loss: 1.29245984e-06
Iter: 1957 loss: 1.28815873e-06
Iter: 1958 loss: 1.28737918e-06
Iter: 1959 loss: 1.29096304e-06
Iter: 1960 loss: 1.28728675e-06
Iter: 1961 loss: 1.28673264e-06
Iter: 1962 loss: 1.2856442e-06
Iter: 1963 loss: 1.30394233e-06
Iter: 1964 loss: 1.2855628e-06
Iter: 1965 loss: 1.28452712e-06
Iter: 1966 loss: 1.29868511e-06
Iter: 1967 loss: 1.28451927e-06
Iter: 1968 loss: 1.28386694e-06
Iter: 1969 loss: 1.28362785e-06
Iter: 1970 loss: 1.28319971e-06
Iter: 1971 loss: 1.28222632e-06
Iter: 1972 loss: 1.28342867e-06
Iter: 1973 loss: 1.28166914e-06
Iter: 1974 loss: 1.28064619e-06
Iter: 1975 loss: 1.28720797e-06
Iter: 1976 loss: 1.28051408e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.4
+ date
Wed Oct 21 14:07:24 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/300_300_300_1 --function f1 --psi -2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a6d3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a52b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a6d32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a52b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a561730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a561bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95f3fae620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95f3fe1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a4d97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95f3fa88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95f3fa39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc7acd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc7d6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc7d6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95f3f597b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc7a99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95f3f20510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc71bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc6c9488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc6aef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc738598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc738048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc66c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc684f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc684d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc61fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc5987b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc59e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc59e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc5b8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc5b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc54e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc54e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc554488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc4bd1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc4f7158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.10622512
test_loss: 0.1054124
train_loss: 0.060843967
test_loss: 0.070803285
train_loss: 0.08660334
test_loss: 0.11036842
train_loss: 0.055990458
test_loss: 0.08501481
train_loss: 0.046906684
test_loss: 0.06362656
train_loss: 0.0421164
test_loss: 0.051937632
train_loss: 0.034925275
test_loss: 0.041813143
train_loss: 0.023005312
test_loss: 0.037864752
train_loss: 0.02515577
test_loss: 0.032818504
train_loss: 0.02008786
test_loss: 0.029540552
train_loss: 0.02170552
test_loss: 0.028538324
train_loss: 0.015656255
test_loss: 0.028249161
train_loss: 0.014189102
test_loss: 0.024864724
train_loss: 0.01412234
test_loss: 0.025804887
train_loss: 0.01262017
test_loss: 0.023093646
train_loss: 0.017712975
test_loss: 0.022703426
train_loss: 0.012144958
test_loss: 0.021757761
train_loss: 0.012875065
test_loss: 0.020570345
train_loss: 0.011652782
test_loss: 0.020204034
train_loss: 0.012062667
test_loss: 0.020165559
train_loss: 0.011888095
test_loss: 0.019205019
train_loss: 0.012665361
test_loss: 0.018025424
train_loss: 0.010410462
test_loss: 0.019161675
train_loss: 0.010595595
test_loss: 0.017420897
train_loss: 0.009666943
test_loss: 0.01861016
train_loss: 0.009849001
test_loss: 0.016526086
train_loss: 0.011972663
test_loss: 0.019081589
train_loss: 0.010022854
test_loss: 0.016800918
train_loss: 0.009638453
test_loss: 0.018147899
train_loss: 0.012546624
test_loss: 0.017902592
train_loss: 0.0100402655
test_loss: 0.01766914
train_loss: 0.010213756
test_loss: 0.01618616
train_loss: 0.010342436
test_loss: 0.0171504
train_loss: 0.010120853
test_loss: 0.016392205
train_loss: 0.0090710055
test_loss: 0.017709712
train_loss: 0.011122284
test_loss: 0.017103016
train_loss: 0.0102732275
test_loss: 0.016820755
train_loss: 0.008897977
test_loss: 0.018087707
train_loss: 0.008953924
test_loss: 0.015853466
train_loss: 0.00828525
test_loss: 0.016549358
train_loss: 0.008188896
test_loss: 0.015208312
train_loss: 0.010277955
test_loss: 0.016629567
train_loss: 0.008697009
test_loss: 0.017050026
train_loss: 0.00854217
test_loss: 0.014994767
train_loss: 0.008129739
test_loss: 0.016071672
train_loss: 0.00948379
test_loss: 0.015998762
train_loss: 0.01034376
test_loss: 0.015705278
train_loss: 0.008351614
test_loss: 0.015050927
train_loss: 0.00889853
test_loss: 0.017073838
train_loss: 0.007504915
test_loss: 0.014493339
train_loss: 0.008126963
test_loss: 0.016812636
train_loss: 0.009014567
test_loss: 0.0157623
train_loss: 0.007796806
test_loss: 0.017045919
train_loss: 0.008257623
test_loss: 0.014520288
train_loss: 0.0081049185
test_loss: 0.015238249
train_loss: 0.007765674
test_loss: 0.0154455565
train_loss: 0.0102334
test_loss: 0.014778631
train_loss: 0.0077662994
test_loss: 0.015957784
train_loss: 0.008811289
test_loss: 0.014909925
train_loss: 0.011297748
test_loss: 0.015145184
train_loss: 0.0077361884
test_loss: 0.015840972
train_loss: 0.009561706
test_loss: 0.015622998
train_loss: 0.008036469
test_loss: 0.016009534
train_loss: 0.008428907
test_loss: 0.014936887
train_loss: 0.008427739
test_loss: 0.015389586
train_loss: 0.008159206
test_loss: 0.0151876
train_loss: 0.007614473
test_loss: 0.015140096
train_loss: 0.009573889
test_loss: 0.014038871
train_loss: 0.008148459
test_loss: 0.015633535
train_loss: 0.008530054
test_loss: 0.0152420215
train_loss: 0.00904616
test_loss: 0.015109802
train_loss: 0.009589218
test_loss: 0.014653728
train_loss: 0.007468838
test_loss: 0.015307199
train_loss: 0.0077726133
test_loss: 0.014585063
train_loss: 0.00815543
test_loss: 0.01427355
train_loss: 0.0079052355
test_loss: 0.014793486
train_loss: 0.007413677
test_loss: 0.015023416
train_loss: 0.0068543013
test_loss: 0.015050396
train_loss: 0.0070349956
test_loss: 0.013989323
train_loss: 0.006941982
test_loss: 0.015002756
train_loss: 0.007562205
test_loss: 0.014359631
train_loss: 0.0074769417
test_loss: 0.0150725795
train_loss: 0.009478056
test_loss: 0.015473176
train_loss: 0.008322143
test_loss: 0.013650967
train_loss: 0.0071454765
test_loss: 0.013840887
train_loss: 0.007953056
test_loss: 0.015750306
train_loss: 0.007688518
test_loss: 0.014514995
train_loss: 0.007950798
test_loss: 0.015465546
train_loss: 0.0075067035
test_loss: 0.013365185
train_loss: 0.0096036745
test_loss: 0.015730612
train_loss: 0.008181828
test_loss: 0.013954382
train_loss: 0.0073433304
test_loss: 0.014717017
train_loss: 0.009433495
test_loss: 0.014756683
train_loss: 0.007937161
test_loss: 0.013690628
train_loss: 0.006719731
test_loss: 0.015269392
train_loss: 0.0072930716
test_loss: 0.015029492
train_loss: 0.008990198
test_loss: 0.01426134
train_loss: 0.0070321998
test_loss: 0.013146011
train_loss: 0.007152573
test_loss: 0.0144700175
train_loss: 0.008056419
test_loss: 0.013944036
train_loss: 0.0068675177
test_loss: 0.014330516
train_loss: 0.008095032
test_loss: 0.013927697
train_loss: 0.007141103
test_loss: 0.013015523
train_loss: 0.0075854543
test_loss: 0.014529193
train_loss: 0.0069906805
test_loss: 0.014804585
train_loss: 0.0074411742
test_loss: 0.013370193
train_loss: 0.007957475
test_loss: 0.014508561
train_loss: 0.006762111
test_loss: 0.013656108
train_loss: 0.008537998
test_loss: 0.0140172355
train_loss: 0.007188554
test_loss: 0.01353435
train_loss: 0.0070410506
test_loss: 0.014189994
train_loss: 0.007792425
test_loss: 0.013724717
train_loss: 0.0068336898
test_loss: 0.013716068
train_loss: 0.007027399
test_loss: 0.013799424
train_loss: 0.0071208486
test_loss: 0.013023526
train_loss: 0.0070313658
test_loss: 0.014530161
train_loss: 0.006463458
test_loss: 0.013790369
train_loss: 0.0073744366
test_loss: 0.01509656
train_loss: 0.0063789585
test_loss: 0.014127315
train_loss: 0.007638534
test_loss: 0.014109442
train_loss: 0.0068330183
test_loss: 0.014576021
train_loss: 0.0076057604
test_loss: 0.01405233
train_loss: 0.0070470334
test_loss: 0.013494164
train_loss: 0.006894907
test_loss: 0.012712157
train_loss: 0.0069831903
test_loss: 0.014761709
train_loss: 0.007756929
test_loss: 0.013046447
train_loss: 0.007675496
test_loss: 0.014033285
train_loss: 0.00903921
test_loss: 0.013603747
train_loss: 0.00843498
test_loss: 0.014086216
train_loss: 0.008732662
test_loss: 0.013291599
train_loss: 0.007987659
test_loss: 0.0137511
train_loss: 0.0073059835
test_loss: 0.012665123
train_loss: 0.007290332
test_loss: 0.013189368
train_loss: 0.0068945354
test_loss: 0.012793215
train_loss: 0.0072628506
test_loss: 0.012959612
train_loss: 0.006866617
test_loss: 0.013159985
train_loss: 0.007082413
test_loss: 0.013383208
train_loss: 0.0065017925
test_loss: 0.012713986
train_loss: 0.006379719
test_loss: 0.013836457
train_loss: 0.0075203027
test_loss: 0.014266946
train_loss: 0.0074615986
test_loss: 0.013554228
train_loss: 0.0068692397
test_loss: 0.01326682
train_loss: 0.0071232393
test_loss: 0.014436556
train_loss: 0.0077867196
test_loss: 0.013784452
train_loss: 0.006841394
test_loss: 0.014946657
train_loss: 0.008956158
test_loss: 0.014255816
train_loss: 0.008326651
test_loss: 0.0145568205
train_loss: 0.008210202
test_loss: 0.014127521
train_loss: 0.007153739
test_loss: 0.014102247
train_loss: 0.008415091
test_loss: 0.014774076
train_loss: 0.0075012324
test_loss: 0.013172777
train_loss: 0.0067913397
test_loss: 0.013212955
train_loss: 0.007887347
test_loss: 0.013836282
train_loss: 0.0068066157
test_loss: 0.013383874
train_loss: 0.006929106
test_loss: 0.013191796
train_loss: 0.0070372396
test_loss: 0.013392268
train_loss: 0.008320022
test_loss: 0.014681781
train_loss: 0.0075138104
test_loss: 0.013245788
train_loss: 0.007279715
test_loss: 0.0135158645
train_loss: 0.0073900195
test_loss: 0.01393926
train_loss: 0.007541483
test_loss: 0.01282383
train_loss: 0.007486009
test_loss: 0.013809836
train_loss: 0.0064190663
test_loss: 0.013539639
train_loss: 0.0066652526
test_loss: 0.012918426
train_loss: 0.007447442
test_loss: 0.013919646
train_loss: 0.006825585
test_loss: 0.013084095
train_loss: 0.0075995987
test_loss: 0.013519081
train_loss: 0.007464643
test_loss: 0.013599135
train_loss: 0.006712083
test_loss: 0.013065188
train_loss: 0.007906849
test_loss: 0.013070265
train_loss: 0.008531282
test_loss: 0.015408697
train_loss: 0.006819579
test_loss: 0.014349969
train_loss: 0.008726707
test_loss: 0.012525769
train_loss: 0.007049748
test_loss: 0.012883831
train_loss: 0.00714423
test_loss: 0.013371349
train_loss: 0.0073200087
test_loss: 0.013204717
train_loss: 0.0071826233
test_loss: 0.013960093
train_loss: 0.007136826
test_loss: 0.015152343
train_loss: 0.007790726
test_loss: 0.013854345
train_loss: 0.006816272
test_loss: 0.01286142
train_loss: 0.0064816917
test_loss: 0.014529896
train_loss: 0.007156862
test_loss: 0.013595201
train_loss: 0.0065755043
test_loss: 0.01306566
train_loss: 0.0069959555
test_loss: 0.01308
train_loss: 0.006479547
test_loss: 0.012889461
train_loss: 0.006979091
test_loss: 0.013675934
train_loss: 0.0074888207
test_loss: 0.013755885
train_loss: 0.0070544286
test_loss: 0.015420725
train_loss: 0.006563493
test_loss: 0.01290808
train_loss: 0.007298574
test_loss: 0.013063772
train_loss: 0.008024707
test_loss: 0.013066832
train_loss: 0.008405253
test_loss: 0.014131837
train_loss: 0.0072225067
test_loss: 0.014169848
train_loss: 0.0073129656
test_loss: 0.012718847
train_loss: 0.008040775
test_loss: 0.013251326
train_loss: 0.0078934515
test_loss: 0.013406105
train_loss: 0.0071883113
test_loss: 0.013711931
train_loss: 0.007284062
test_loss: 0.012625588
train_loss: 0.0067990427
test_loss: 0.013954138
train_loss: 0.008059439
test_loss: 0.013635968
train_loss: 0.0070901616
test_loss: 0.014601447
train_loss: 0.007144396
test_loss: 0.013350934
train_loss: 0.007246404
test_loss: 0.013154106
train_loss: 0.0063379873
test_loss: 0.013460098
train_loss: 0.006818942
test_loss: 0.012710721
train_loss: 0.008286962
test_loss: 0.012959081
train_loss: 0.008018958
test_loss: 0.012890159
train_loss: 0.006739039
test_loss: 0.013519378
train_loss: 0.006950251
test_loss: 0.013636265
train_loss: 0.008376471
test_loss: 0.013220209
train_loss: 0.0062851883
test_loss: 0.013261292
train_loss: 0.0072872597
test_loss: 0.012327876
train_loss: 0.0071696597
test_loss: 0.013796275
train_loss: 0.0070632277
test_loss: 0.012957433
train_loss: 0.008362494
test_loss: 0.014077964
train_loss: 0.0068721105
test_loss: 0.015643207
train_loss: 0.0067053353
test_loss: 0.014572161
train_loss: 0.0064007407
test_loss: 0.013290183
train_loss: 0.008047486
test_loss: 0.013788073
train_loss: 0.006115148
test_loss: 0.013228743
train_loss: 0.006533832
test_loss: 0.012905218
train_loss: 0.0070843874
test_loss: 0.012823644
train_loss: 0.0062477477
test_loss: 0.013140696
train_loss: 0.008097921
test_loss: 0.012457385
train_loss: 0.007864963
test_loss: 0.013339053
train_loss: 0.006977409
test_loss: 0.013588096
train_loss: 0.0068157576
test_loss: 0.013014785
train_loss: 0.007084271
test_loss: 0.012858701
train_loss: 0.007032792
test_loss: 0.013472513
train_loss: 0.007544146
test_loss: 0.013108473
train_loss: 0.0071220556
test_loss: 0.013338141
train_loss: 0.007336319
test_loss: 0.013741424
train_loss: 0.006501521
test_loss: 0.014193973
train_loss: 0.0071521183
test_loss: 0.012901695
train_loss: 0.007537069
test_loss: 0.013084007
train_loss: 0.007031664
test_loss: 0.013160002
train_loss: 0.0070291073
test_loss: 0.01306664
train_loss: 0.0072409646
test_loss: 0.01335611
train_loss: 0.008725357
test_loss: 0.015180801
train_loss: 0.00685474
test_loss: 0.0137966825
train_loss: 0.0074885404
test_loss: 0.013641214
train_loss: 0.0068945666
test_loss: 0.014383987
train_loss: 0.0077488264
test_loss: 0.014310177
train_loss: 0.006793842
test_loss: 0.0135598425
train_loss: 0.008169783
test_loss: 0.014733291
train_loss: 0.0066158404
test_loss: 0.01490575
train_loss: 0.0061956714
test_loss: 0.013708752
train_loss: 0.0064798403
test_loss: 0.014017431
train_loss: 0.006996492
test_loss: 0.012833678
train_loss: 0.006984476
test_loss: 0.013749848
train_loss: 0.0071637565
test_loss: 0.013380148
train_loss: 0.0070000277
test_loss: 0.013421548
train_loss: 0.006540717
test_loss: 0.0135912765
train_loss: 0.008286697
test_loss: 0.014467702
train_loss: 0.008792814
test_loss: 0.013650185
train_loss: 0.0066393586
test_loss: 0.013151462
train_loss: 0.0071283136
test_loss: 0.013126173
train_loss: 0.007158837
test_loss: 0.013159644
train_loss: 0.007175704
test_loss: 0.013964227
train_loss: 0.0068922453
test_loss: 0.013392389
train_loss: 0.0066957255
test_loss: 0.013069458
train_loss: 0.0072667273
test_loss: 0.014416634
train_loss: 0.007438856
test_loss: 0.013259516
train_loss: 0.006856734
test_loss: 0.013119612
train_loss: 0.006495845
test_loss: 0.014396808
train_loss: 0.0066994373
test_loss: 0.0129245715
train_loss: 0.0063335863
test_loss: 0.013213676
train_loss: 0.00696302
test_loss: 0.013976373
train_loss: 0.00787376
test_loss: 0.012519846
train_loss: 0.0073152194
test_loss: 0.013244449
train_loss: 0.0075076837
test_loss: 0.01327942
train_loss: 0.007606797
test_loss: 0.013356891
train_loss: 0.0072404654
test_loss: 0.013008799
train_loss: 0.00687326
test_loss: 0.014224182
train_loss: 0.008507699
test_loss: 0.015078872
train_loss: 0.007387992
test_loss: 0.013166866
train_loss: 0.0069422233
test_loss: 0.01325474
train_loss: 0.006651589
test_loss: 0.012678852
train_loss: 0.0074417423
test_loss: 0.0132487025
train_loss: 0.008185318
test_loss: 0.013959607
train_loss: 0.006384896
test_loss: 0.013215164
train_loss: 0.0073627317
test_loss: 0.012732743
train_loss: 0.0067633362
test_loss: 0.013445081
train_loss: 0.0066507654
test_loss: 0.013771854
train_loss: 0.006145583
test_loss: 0.013306835
train_loss: 0.007516003
test_loss: 0.013460516
train_loss: 0.0075185103
test_loss: 0.01519051
train_loss: 0.0084697595
test_loss: 0.013599991
train_loss: 0.0078563085
test_loss: 0.013598842
train_loss: 0.006682516
test_loss: 0.013185641
train_loss: 0.006420429
test_loss: 0.014931078
train_loss: 0.007223492
test_loss: 0.012896059
train_loss: 0.0074445773
test_loss: 0.01368101
train_loss: 0.008291654
test_loss: 0.013078876
train_loss: 0.0077202944
test_loss: 0.013183188
train_loss: 0.006719646
test_loss: 0.0143584795
train_loss: 0.0065825046
test_loss: 0.013108858
train_loss: 0.0065959725
test_loss: 0.014055389
train_loss: 0.0077089258
test_loss: 0.013573189
train_loss: 0.007889413
test_loss: 0.013994717
train_loss: 0.0072039617
test_loss: 0.013929146
train_loss: 0.007406459
test_loss: 0.0138263805
train_loss: 0.0062740953
test_loss: 0.012797564
train_loss: 0.007386719
test_loss: 0.013199761
train_loss: 0.0071625253
test_loss: 0.014637018
train_loss: 0.0077305036
test_loss: 0.012658384
train_loss: 0.0075282576
test_loss: 0.013556966
train_loss: 0.006270229
test_loss: 0.012911273
train_loss: 0.0065185353
test_loss: 0.01334858
train_loss: 0.007296478
test_loss: 0.013777812
train_loss: 0.006248572
test_loss: 0.013328387
train_loss: 0.0065561174
test_loss: 0.012666497
train_loss: 0.006501175
test_loss: 0.0141147915
train_loss: 0.0072925477
test_loss: 0.0137859555
train_loss: 0.008540085
test_loss: 0.014544967
train_loss: 0.0065688198
test_loss: 0.013088176
train_loss: 0.0074970494
test_loss: 0.013537355
train_loss: 0.0073519032
test_loss: 0.013805743
train_loss: 0.0068864254
test_loss: 0.013964513
train_loss: 0.0064228624
test_loss: 0.0132244965
train_loss: 0.005971347
test_loss: 0.012730577
train_loss: 0.0060431985
test_loss: 0.013619527
train_loss: 0.006776454
test_loss: 0.012818918
train_loss: 0.006920689
test_loss: 0.01659189
train_loss: 0.0065322043
test_loss: 0.014036786
train_loss: 0.007957127
test_loss: 0.014110775
train_loss: 0.0062316954
test_loss: 0.0133472225
train_loss: 0.005517238
test_loss: 0.012496748
train_loss: 0.0062834877
test_loss: 0.012849605
train_loss: 0.0065237023
test_loss: 0.013900327
train_loss: 0.006820817
test_loss: 0.012704404
train_loss: 0.0066006775
test_loss: 0.01271593
train_loss: 0.0061726556
test_loss: 0.012888194
train_loss: 0.0063330512
test_loss: 0.013913719
train_loss: 0.006306406
test_loss: 0.012848559
train_loss: 0.005743339
test_loss: 0.012920488
train_loss: 0.006726429
test_loss: 0.01355095
train_loss: 0.006552032
test_loss: 0.013068956
train_loss: 0.0067229783
test_loss: 0.013854701
train_loss: 0.0064673712
test_loss: 0.01327272
train_loss: 0.006176032
test_loss: 0.01339506
train_loss: 0.0068688253
test_loss: 0.014382349
train_loss: 0.0064736367
test_loss: 0.014758248
train_loss: 0.007436396
test_loss: 0.014497358
train_loss: 0.008210858
test_loss: 0.014825672
train_loss: 0.0077063213
test_loss: 0.014396463
train_loss: 0.0066421404
test_loss: 0.012537386
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.00763472
test_loss: 0.012716201
train_loss: 0.0063552996
test_loss: 0.013176411
train_loss: 0.006876952
test_loss: 0.013755707
train_loss: 0.0075000664
test_loss: 0.013088352
train_loss: 0.0056988536
test_loss: 0.012792876
train_loss: 0.0062871794
test_loss: 0.014129272
train_loss: 0.006143172
test_loss: 0.012934693
train_loss: 0.005854078
test_loss: 0.012577549
train_loss: 0.006149863
test_loss: 0.013646774
train_loss: 0.007005107
test_loss: 0.0138794575
train_loss: 0.006953869
test_loss: 0.01363937
train_loss: 0.007081942
test_loss: 0.0137534505
train_loss: 0.006515681
test_loss: 0.014210917
train_loss: 0.007661179
test_loss: 0.01340737
train_loss: 0.007230758
test_loss: 0.013880862
train_loss: 0.007021893
test_loss: 0.014045844
train_loss: 0.006826963
test_loss: 0.012964799
train_loss: 0.0064088595
test_loss: 0.012615687
train_loss: 0.007260978
test_loss: 0.015909111
train_loss: 0.0063080136
test_loss: 0.013502585
train_loss: 0.009222415
test_loss: 0.013191428
train_loss: 0.0072674733
test_loss: 0.014512798
train_loss: 0.0066173123
test_loss: 0.013741083
train_loss: 0.007120877
test_loss: 0.013577976
train_loss: 0.0076444168
test_loss: 0.013490554
train_loss: 0.007139144
test_loss: 0.013377485
train_loss: 0.006401321
test_loss: 0.013134999
train_loss: 0.0069445637
test_loss: 0.014747001
train_loss: 0.007915658
test_loss: 0.014289133
train_loss: 0.007186775
test_loss: 0.013362779
train_loss: 0.007336653
test_loss: 0.01310999
train_loss: 0.0070268605
test_loss: 0.014061072
train_loss: 0.0072981827
test_loss: 0.013189816
train_loss: 0.0069720997
test_loss: 0.013494198
train_loss: 0.006501244
test_loss: 0.012971801
train_loss: 0.0065392125
test_loss: 0.012827451
train_loss: 0.006052318
test_loss: 0.013794068
train_loss: 0.0063557047
test_loss: 0.013668842
train_loss: 0.0064591644
test_loss: 0.013816509
train_loss: 0.0069793467
test_loss: 0.014459898
train_loss: 0.0076274844
test_loss: 0.01360802
train_loss: 0.005643484
test_loss: 0.01311124
train_loss: 0.006369385
test_loss: 0.013360078
train_loss: 0.0070928447
test_loss: 0.014341316
train_loss: 0.0066504045
test_loss: 0.013947941
train_loss: 0.0065226844
test_loss: 0.013672516
train_loss: 0.0065139085
test_loss: 0.014766279
train_loss: 0.0070461533
test_loss: 0.013836942
train_loss: 0.007035627
test_loss: 0.013054417
train_loss: 0.006985497
test_loss: 0.014089587
train_loss: 0.0066852523
test_loss: 0.013883483
train_loss: 0.007776304
test_loss: 0.01629413
train_loss: 0.008065809
test_loss: 0.015310072
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349ea6d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349eb39598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349ec0eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349ec0ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349eadd2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349eadd510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e9f7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e9ba6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e9d5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e9d56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e9d5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e953ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e953b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e90e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e8b1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e8b16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e86e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e8a2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e85c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e85df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474b8d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474bae400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474afa620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474afa840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474b230d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474afa7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474acf9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474aa8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474aa8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474b23730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3450307a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f345032f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34502b8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34502e4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3450293598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3450245400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000144741527
Iter: 2 loss: 0.00112465408
Iter: 3 loss: 0.0001041824
Iter: 4 loss: 9.50007598e-05
Iter: 5 loss: 8.96542188e-05
Iter: 6 loss: 8.57426494e-05
Iter: 7 loss: 7.47095255e-05
Iter: 8 loss: 0.000100823519
Iter: 9 loss: 7.06791761e-05
Iter: 10 loss: 6.702666e-05
Iter: 11 loss: 6.09734561e-05
Iter: 12 loss: 6.0950988e-05
Iter: 13 loss: 5.56910054e-05
Iter: 14 loss: 6.54631149e-05
Iter: 15 loss: 5.34475366e-05
Iter: 16 loss: 4.97501387e-05
Iter: 17 loss: 6.14177479e-05
Iter: 18 loss: 4.86776225e-05
Iter: 19 loss: 4.577047e-05
Iter: 20 loss: 4.33213427e-05
Iter: 21 loss: 4.25054022e-05
Iter: 22 loss: 3.84971172e-05
Iter: 23 loss: 4.76195492e-05
Iter: 24 loss: 3.69844565e-05
Iter: 25 loss: 3.3549095e-05
Iter: 26 loss: 3.98569027e-05
Iter: 27 loss: 3.20702493e-05
Iter: 28 loss: 2.94664751e-05
Iter: 29 loss: 5.11277904e-05
Iter: 30 loss: 2.93092726e-05
Iter: 31 loss: 2.78339248e-05
Iter: 32 loss: 2.87039184e-05
Iter: 33 loss: 2.6881411e-05
Iter: 34 loss: 2.46653362e-05
Iter: 35 loss: 2.95090558e-05
Iter: 36 loss: 2.38125394e-05
Iter: 37 loss: 2.24936484e-05
Iter: 38 loss: 2.90065036e-05
Iter: 39 loss: 2.22683702e-05
Iter: 40 loss: 2.12257219e-05
Iter: 41 loss: 3.12094307e-05
Iter: 42 loss: 2.11859842e-05
Iter: 43 loss: 2.01940784e-05
Iter: 44 loss: 3.07523769e-05
Iter: 45 loss: 2.01707626e-05
Iter: 46 loss: 1.98736707e-05
Iter: 47 loss: 2.00999275e-05
Iter: 48 loss: 1.96917463e-05
Iter: 49 loss: 1.93031246e-05
Iter: 50 loss: 1.97336121e-05
Iter: 51 loss: 1.90911978e-05
Iter: 52 loss: 1.85177742e-05
Iter: 53 loss: 1.90241153e-05
Iter: 54 loss: 1.81828473e-05
Iter: 55 loss: 1.76861304e-05
Iter: 56 loss: 1.80520474e-05
Iter: 57 loss: 1.73802e-05
Iter: 58 loss: 1.67928119e-05
Iter: 59 loss: 1.77924776e-05
Iter: 60 loss: 1.65306192e-05
Iter: 61 loss: 1.58965267e-05
Iter: 62 loss: 1.78735554e-05
Iter: 63 loss: 1.5711732e-05
Iter: 64 loss: 1.51737322e-05
Iter: 65 loss: 1.57091708e-05
Iter: 66 loss: 1.48697354e-05
Iter: 67 loss: 1.42780364e-05
Iter: 68 loss: 1.88073409e-05
Iter: 69 loss: 1.42336339e-05
Iter: 70 loss: 1.38875075e-05
Iter: 71 loss: 1.40922493e-05
Iter: 72 loss: 1.36637591e-05
Iter: 73 loss: 1.31662819e-05
Iter: 74 loss: 1.50733986e-05
Iter: 75 loss: 1.30479184e-05
Iter: 76 loss: 1.34965176e-05
Iter: 77 loss: 1.29179407e-05
Iter: 78 loss: 1.28524671e-05
Iter: 79 loss: 1.26684099e-05
Iter: 80 loss: 1.36259214e-05
Iter: 81 loss: 1.26083442e-05
Iter: 82 loss: 1.23289501e-05
Iter: 83 loss: 1.42114286e-05
Iter: 84 loss: 1.23011159e-05
Iter: 85 loss: 1.20544801e-05
Iter: 86 loss: 1.27851181e-05
Iter: 87 loss: 1.19793622e-05
Iter: 88 loss: 1.17630625e-05
Iter: 89 loss: 1.16813862e-05
Iter: 90 loss: 1.15627463e-05
Iter: 91 loss: 1.13093101e-05
Iter: 92 loss: 1.16058209e-05
Iter: 93 loss: 1.11739637e-05
Iter: 94 loss: 1.09416033e-05
Iter: 95 loss: 1.25594433e-05
Iter: 96 loss: 1.09197263e-05
Iter: 97 loss: 1.06973093e-05
Iter: 98 loss: 1.06978196e-05
Iter: 99 loss: 1.05190056e-05
Iter: 100 loss: 1.03023012e-05
Iter: 101 loss: 1.1776463e-05
Iter: 102 loss: 1.02813192e-05
Iter: 103 loss: 1.00940561e-05
Iter: 104 loss: 1.00991529e-05
Iter: 105 loss: 9.94563743e-06
Iter: 106 loss: 9.76166848e-06
Iter: 107 loss: 1.25149691e-05
Iter: 108 loss: 9.76162482e-06
Iter: 109 loss: 9.66170228e-06
Iter: 110 loss: 1.01285586e-05
Iter: 111 loss: 9.64286664e-06
Iter: 112 loss: 9.49148671e-06
Iter: 113 loss: 9.93468e-06
Iter: 114 loss: 9.44408566e-06
Iter: 115 loss: 9.39198253e-06
Iter: 116 loss: 9.45533066e-06
Iter: 117 loss: 9.36473316e-06
Iter: 118 loss: 9.29729686e-06
Iter: 119 loss: 9.52824848e-06
Iter: 120 loss: 9.2794362e-06
Iter: 121 loss: 9.2090022e-06
Iter: 122 loss: 9.16517638e-06
Iter: 123 loss: 9.13663462e-06
Iter: 124 loss: 9.02114698e-06
Iter: 125 loss: 9.08206766e-06
Iter: 126 loss: 8.94411733e-06
Iter: 127 loss: 8.82452514e-06
Iter: 128 loss: 8.96092206e-06
Iter: 129 loss: 8.75964633e-06
Iter: 130 loss: 8.62799243e-06
Iter: 131 loss: 9.77453237e-06
Iter: 132 loss: 8.62114939e-06
Iter: 133 loss: 8.53175879e-06
Iter: 134 loss: 8.56171391e-06
Iter: 135 loss: 8.46902185e-06
Iter: 136 loss: 8.34866751e-06
Iter: 137 loss: 8.67223844e-06
Iter: 138 loss: 8.30886893e-06
Iter: 139 loss: 8.21372851e-06
Iter: 140 loss: 8.46097464e-06
Iter: 141 loss: 8.18100398e-06
Iter: 142 loss: 8.056566e-06
Iter: 143 loss: 8.2899187e-06
Iter: 144 loss: 8.00348789e-06
Iter: 145 loss: 8.05668606e-06
Iter: 146 loss: 7.97077701e-06
Iter: 147 loss: 7.93546769e-06
Iter: 148 loss: 7.88161742e-06
Iter: 149 loss: 7.88056059e-06
Iter: 150 loss: 7.82820916e-06
Iter: 151 loss: 8.05052423e-06
Iter: 152 loss: 7.81724066e-06
Iter: 153 loss: 7.75808439e-06
Iter: 154 loss: 7.80755727e-06
Iter: 155 loss: 7.7226e-06
Iter: 156 loss: 7.67298843e-06
Iter: 157 loss: 7.82433563e-06
Iter: 158 loss: 7.65783534e-06
Iter: 159 loss: 7.60353396e-06
Iter: 160 loss: 7.52681171e-06
Iter: 161 loss: 7.52383039e-06
Iter: 162 loss: 7.4433e-06
Iter: 163 loss: 7.90116e-06
Iter: 164 loss: 7.43259307e-06
Iter: 165 loss: 7.35804224e-06
Iter: 166 loss: 7.59236354e-06
Iter: 167 loss: 7.33656179e-06
Iter: 168 loss: 7.26851613e-06
Iter: 169 loss: 7.49826586e-06
Iter: 170 loss: 7.24987149e-06
Iter: 171 loss: 7.19379659e-06
Iter: 172 loss: 7.13960253e-06
Iter: 173 loss: 7.12737801e-06
Iter: 174 loss: 7.03314618e-06
Iter: 175 loss: 7.97528719e-06
Iter: 176 loss: 7.03020169e-06
Iter: 177 loss: 6.98365147e-06
Iter: 178 loss: 7.07778145e-06
Iter: 179 loss: 6.96447296e-06
Iter: 180 loss: 6.92446429e-06
Iter: 181 loss: 6.92072035e-06
Iter: 182 loss: 6.89805256e-06
Iter: 183 loss: 6.85668601e-06
Iter: 184 loss: 7.82328152e-06
Iter: 185 loss: 6.85627856e-06
Iter: 186 loss: 6.81897336e-06
Iter: 187 loss: 7.27695669e-06
Iter: 188 loss: 6.81854135e-06
Iter: 189 loss: 6.79142522e-06
Iter: 190 loss: 6.75023148e-06
Iter: 191 loss: 6.74958437e-06
Iter: 192 loss: 6.70213649e-06
Iter: 193 loss: 7.07357503e-06
Iter: 194 loss: 6.69889687e-06
Iter: 195 loss: 6.66801589e-06
Iter: 196 loss: 6.64362869e-06
Iter: 197 loss: 6.63415676e-06
Iter: 198 loss: 6.57740111e-06
Iter: 199 loss: 6.65865355e-06
Iter: 200 loss: 6.54963469e-06
Iter: 201 loss: 6.50141e-06
Iter: 202 loss: 6.79671211e-06
Iter: 203 loss: 6.49578124e-06
Iter: 204 loss: 6.44137708e-06
Iter: 205 loss: 6.53372763e-06
Iter: 206 loss: 6.41708266e-06
Iter: 207 loss: 6.37814446e-06
Iter: 208 loss: 6.49210642e-06
Iter: 209 loss: 6.36644063e-06
Iter: 210 loss: 6.31752573e-06
Iter: 211 loss: 6.31730973e-06
Iter: 212 loss: 6.27834243e-06
Iter: 213 loss: 6.2353156e-06
Iter: 214 loss: 6.61058812e-06
Iter: 215 loss: 6.23312735e-06
Iter: 216 loss: 6.23987171e-06
Iter: 217 loss: 6.21694835e-06
Iter: 218 loss: 6.20741685e-06
Iter: 219 loss: 6.1799883e-06
Iter: 220 loss: 6.30784871e-06
Iter: 221 loss: 6.17047135e-06
Iter: 222 loss: 6.14330384e-06
Iter: 223 loss: 6.14274131e-06
Iter: 224 loss: 6.131193e-06
Iter: 225 loss: 6.11103405e-06
Iter: 226 loss: 6.11119731e-06
Iter: 227 loss: 6.0788725e-06
Iter: 228 loss: 6.16142052e-06
Iter: 229 loss: 6.06785079e-06
Iter: 230 loss: 6.03648277e-06
Iter: 231 loss: 6.08489e-06
Iter: 232 loss: 6.0218681e-06
Iter: 233 loss: 5.98963425e-06
Iter: 234 loss: 6.04918887e-06
Iter: 235 loss: 5.97600956e-06
Iter: 236 loss: 5.94787525e-06
Iter: 237 loss: 6.20223864e-06
Iter: 238 loss: 5.94645689e-06
Iter: 239 loss: 5.92295964e-06
Iter: 240 loss: 5.91969638e-06
Iter: 241 loss: 5.90294076e-06
Iter: 242 loss: 5.87200338e-06
Iter: 243 loss: 6.11758378e-06
Iter: 244 loss: 5.87005252e-06
Iter: 245 loss: 5.84553391e-06
Iter: 246 loss: 5.80926599e-06
Iter: 247 loss: 5.80841879e-06
Iter: 248 loss: 5.76987395e-06
Iter: 249 loss: 6.16284342e-06
Iter: 250 loss: 5.76876892e-06
Iter: 251 loss: 5.76033381e-06
Iter: 252 loss: 5.75638751e-06
Iter: 253 loss: 5.73933539e-06
Iter: 254 loss: 5.71391411e-06
Iter: 255 loss: 5.71313649e-06
Iter: 256 loss: 5.69693839e-06
Iter: 257 loss: 5.81547192e-06
Iter: 258 loss: 5.69547774e-06
Iter: 259 loss: 5.67994084e-06
Iter: 260 loss: 5.66477229e-06
Iter: 261 loss: 5.6612871e-06
Iter: 262 loss: 5.63778076e-06
Iter: 263 loss: 5.64697166e-06
Iter: 264 loss: 5.62123842e-06
Iter: 265 loss: 5.59831096e-06
Iter: 266 loss: 5.61912293e-06
Iter: 267 loss: 5.58505599e-06
Iter: 268 loss: 5.55857696e-06
Iter: 269 loss: 5.90554691e-06
Iter: 270 loss: 5.55849101e-06
Iter: 271 loss: 5.539624e-06
Iter: 272 loss: 5.58136571e-06
Iter: 273 loss: 5.53230575e-06
Iter: 274 loss: 5.51450739e-06
Iter: 275 loss: 5.50071672e-06
Iter: 276 loss: 5.49526567e-06
Iter: 277 loss: 5.46298861e-06
Iter: 278 loss: 5.65175696e-06
Iter: 279 loss: 5.45887724e-06
Iter: 280 loss: 5.44076556e-06
Iter: 281 loss: 5.48406206e-06
Iter: 282 loss: 5.43401893e-06
Iter: 283 loss: 5.40965e-06
Iter: 284 loss: 5.43656597e-06
Iter: 285 loss: 5.39623397e-06
Iter: 286 loss: 5.43242459e-06
Iter: 287 loss: 5.39040775e-06
Iter: 288 loss: 5.38612903e-06
Iter: 289 loss: 5.37366122e-06
Iter: 290 loss: 5.4312377e-06
Iter: 291 loss: 5.36909465e-06
Iter: 292 loss: 5.35282197e-06
Iter: 293 loss: 5.42922226e-06
Iter: 294 loss: 5.3498984e-06
Iter: 295 loss: 5.33089133e-06
Iter: 296 loss: 5.34647916e-06
Iter: 297 loss: 5.31987553e-06
Iter: 298 loss: 5.30782609e-06
Iter: 299 loss: 5.28934106e-06
Iter: 300 loss: 5.28924102e-06
Iter: 301 loss: 5.26259646e-06
Iter: 302 loss: 5.43660281e-06
Iter: 303 loss: 5.2597884e-06
Iter: 304 loss: 5.2443761e-06
Iter: 305 loss: 5.40992187e-06
Iter: 306 loss: 5.24401366e-06
Iter: 307 loss: 5.22859364e-06
Iter: 308 loss: 5.2142791e-06
Iter: 309 loss: 5.21076163e-06
Iter: 310 loss: 5.19366176e-06
Iter: 311 loss: 5.30624766e-06
Iter: 312 loss: 5.19171681e-06
Iter: 313 loss: 5.17593162e-06
Iter: 314 loss: 5.19150854e-06
Iter: 315 loss: 5.16704e-06
Iter: 316 loss: 5.14926705e-06
Iter: 317 loss: 5.22119444e-06
Iter: 318 loss: 5.14526391e-06
Iter: 319 loss: 5.13002942e-06
Iter: 320 loss: 5.26313579e-06
Iter: 321 loss: 5.12935912e-06
Iter: 322 loss: 5.11194276e-06
Iter: 323 loss: 5.20123e-06
Iter: 324 loss: 5.10919199e-06
Iter: 325 loss: 5.10392829e-06
Iter: 326 loss: 5.09464508e-06
Iter: 327 loss: 5.32263584e-06
Iter: 328 loss: 5.09463734e-06
Iter: 329 loss: 5.08266839e-06
Iter: 330 loss: 5.15225e-06
Iter: 331 loss: 5.08085031e-06
Iter: 332 loss: 5.06869856e-06
Iter: 333 loss: 5.04744457e-06
Iter: 334 loss: 5.04729e-06
Iter: 335 loss: 5.03010688e-06
Iter: 336 loss: 5.12410679e-06
Iter: 337 loss: 5.02729154e-06
Iter: 338 loss: 5.01264685e-06
Iter: 339 loss: 5.00475562e-06
Iter: 340 loss: 4.99821408e-06
Iter: 341 loss: 4.98496047e-06
Iter: 342 loss: 4.98335521e-06
Iter: 343 loss: 4.97293286e-06
Iter: 344 loss: 4.96901976e-06
Iter: 345 loss: 4.96344273e-06
Iter: 346 loss: 4.948678e-06
Iter: 347 loss: 4.97119709e-06
Iter: 348 loss: 4.94158212e-06
Iter: 349 loss: 4.92914933e-06
Iter: 350 loss: 4.96308076e-06
Iter: 351 loss: 4.92497384e-06
Iter: 352 loss: 4.9121868e-06
Iter: 353 loss: 5.04331638e-06
Iter: 354 loss: 4.9115697e-06
Iter: 355 loss: 4.90847697e-06
Iter: 356 loss: 4.90673892e-06
Iter: 357 loss: 4.90453158e-06
Iter: 358 loss: 4.89780086e-06
Iter: 359 loss: 4.91953642e-06
Iter: 360 loss: 4.89463e-06
Iter: 361 loss: 4.88442174e-06
Iter: 362 loss: 4.93978496e-06
Iter: 363 loss: 4.88289788e-06
Iter: 364 loss: 4.87318357e-06
Iter: 365 loss: 4.90519869e-06
Iter: 366 loss: 4.87045963e-06
Iter: 367 loss: 4.8641441e-06
Iter: 368 loss: 4.85056216e-06
Iter: 369 loss: 5.0515e-06
Iter: 370 loss: 4.84986776e-06
Iter: 371 loss: 4.83135955e-06
Iter: 372 loss: 4.90985894e-06
Iter: 373 loss: 4.82760424e-06
Iter: 374 loss: 4.81525512e-06
Iter: 375 loss: 4.8933357e-06
Iter: 376 loss: 4.81372672e-06
Iter: 377 loss: 4.80218932e-06
Iter: 378 loss: 4.86400404e-06
Iter: 379 loss: 4.8003194e-06
Iter: 380 loss: 4.79024357e-06
Iter: 381 loss: 4.79241407e-06
Iter: 382 loss: 4.78261791e-06
Iter: 383 loss: 4.77080312e-06
Iter: 384 loss: 4.77188769e-06
Iter: 385 loss: 4.76154946e-06
Iter: 386 loss: 4.74867056e-06
Iter: 387 loss: 4.8788e-06
Iter: 388 loss: 4.74830676e-06
Iter: 389 loss: 4.74522767e-06
Iter: 390 loss: 4.74272565e-06
Iter: 391 loss: 4.73769e-06
Iter: 392 loss: 4.72657212e-06
Iter: 393 loss: 4.8881484e-06
Iter: 394 loss: 4.72610282e-06
Iter: 395 loss: 4.71813382e-06
Iter: 396 loss: 4.74309581e-06
Iter: 397 loss: 4.71563908e-06
Iter: 398 loss: 4.70784562e-06
Iter: 399 loss: 4.75905154e-06
Iter: 400 loss: 4.70715077e-06
Iter: 401 loss: 4.69988572e-06
Iter: 402 loss: 4.68784765e-06
Iter: 403 loss: 4.68792769e-06
Iter: 404 loss: 4.67733526e-06
Iter: 405 loss: 4.73719774e-06
Iter: 406 loss: 4.6758023e-06
Iter: 407 loss: 4.66637948e-06
Iter: 408 loss: 4.66007123e-06
Iter: 409 loss: 4.6564237e-06
Iter: 410 loss: 4.6451637e-06
Iter: 411 loss: 4.64506638e-06
Iter: 412 loss: 4.63884135e-06
Iter: 413 loss: 4.67194877e-06
Iter: 414 loss: 4.63795823e-06
Iter: 415 loss: 4.63204196e-06
Iter: 416 loss: 4.61782611e-06
Iter: 417 loss: 4.79389564e-06
Iter: 418 loss: 4.61668333e-06
Iter: 419 loss: 4.60368665e-06
Iter: 420 loss: 4.70017494e-06
Iter: 421 loss: 4.60260071e-06
Iter: 422 loss: 4.59850889e-06
Iter: 423 loss: 4.59702915e-06
Iter: 424 loss: 4.58995873e-06
Iter: 425 loss: 4.59089279e-06
Iter: 426 loss: 4.58484283e-06
Iter: 427 loss: 4.5798588e-06
Iter: 428 loss: 4.57939677e-06
Iter: 429 loss: 4.57574242e-06
Iter: 430 loss: 4.56960788e-06
Iter: 431 loss: 4.59354078e-06
Iter: 432 loss: 4.568441e-06
Iter: 433 loss: 4.56017506e-06
Iter: 434 loss: 4.55641475e-06
Iter: 435 loss: 4.55229883e-06
Iter: 436 loss: 4.54376277e-06
Iter: 437 loss: 4.56299495e-06
Iter: 438 loss: 4.5403076e-06
Iter: 439 loss: 4.5306133e-06
Iter: 440 loss: 4.5271845e-06
Iter: 441 loss: 4.52171116e-06
Iter: 442 loss: 4.50995776e-06
Iter: 443 loss: 4.62136404e-06
Iter: 444 loss: 4.5093866e-06
Iter: 445 loss: 4.50039533e-06
Iter: 446 loss: 4.53669782e-06
Iter: 447 loss: 4.49833578e-06
Iter: 448 loss: 4.48909077e-06
Iter: 449 loss: 4.52435688e-06
Iter: 450 loss: 4.48702031e-06
Iter: 451 loss: 4.48118544e-06
Iter: 452 loss: 4.47858e-06
Iter: 453 loss: 4.47575e-06
Iter: 454 loss: 4.4665876e-06
Iter: 455 loss: 4.50736707e-06
Iter: 456 loss: 4.46482227e-06
Iter: 457 loss: 4.46665854e-06
Iter: 458 loss: 4.46107424e-06
Iter: 459 loss: 4.45959449e-06
Iter: 460 loss: 4.45456089e-06
Iter: 461 loss: 4.4549779e-06
Iter: 462 loss: 4.44918078e-06
Iter: 463 loss: 4.44240823e-06
Iter: 464 loss: 4.44242687e-06
Iter: 465 loss: 4.43850149e-06
Iter: 466 loss: 4.44888065e-06
Iter: 467 loss: 4.43717136e-06
Iter: 468 loss: 4.43189492e-06
Iter: 469 loss: 4.41975408e-06
Iter: 470 loss: 4.57191936e-06
Iter: 471 loss: 4.4189992e-06
Iter: 472 loss: 4.41040629e-06
Iter: 473 loss: 4.51709366e-06
Iter: 474 loss: 4.41020575e-06
Iter: 475 loss: 4.4026765e-06
Iter: 476 loss: 4.39755331e-06
Iter: 477 loss: 4.39462701e-06
Iter: 478 loss: 4.38698225e-06
Iter: 479 loss: 4.38706866e-06
Iter: 480 loss: 4.38054576e-06
Iter: 481 loss: 4.39080668e-06
Iter: 482 loss: 4.37752624e-06
Iter: 483 loss: 4.36868959e-06
Iter: 484 loss: 4.37415747e-06
Iter: 485 loss: 4.36305891e-06
Iter: 486 loss: 4.3554237e-06
Iter: 487 loss: 4.35800575e-06
Iter: 488 loss: 4.35013635e-06
Iter: 489 loss: 4.35310312e-06
Iter: 490 loss: 4.34666208e-06
Iter: 491 loss: 4.3423629e-06
Iter: 492 loss: 4.33414471e-06
Iter: 493 loss: 4.5165707e-06
Iter: 494 loss: 4.33411515e-06
Iter: 495 loss: 4.33029072e-06
Iter: 496 loss: 4.32710385e-06
Iter: 497 loss: 4.32585693e-06
Iter: 498 loss: 4.31864464e-06
Iter: 499 loss: 4.3549353e-06
Iter: 500 loss: 4.31744184e-06
Iter: 501 loss: 4.31076205e-06
Iter: 502 loss: 4.36706523e-06
Iter: 503 loss: 4.31031503e-06
Iter: 504 loss: 4.30657383e-06
Iter: 505 loss: 4.30374121e-06
Iter: 506 loss: 4.30259115e-06
Iter: 507 loss: 4.29618922e-06
Iter: 508 loss: 4.29769534e-06
Iter: 509 loss: 4.29153852e-06
Iter: 510 loss: 4.28472777e-06
Iter: 511 loss: 4.30095815e-06
Iter: 512 loss: 4.28248495e-06
Iter: 513 loss: 4.2735137e-06
Iter: 514 loss: 4.29850752e-06
Iter: 515 loss: 4.27054056e-06
Iter: 516 loss: 4.26576662e-06
Iter: 517 loss: 4.26562792e-06
Iter: 518 loss: 4.26133465e-06
Iter: 519 loss: 4.25454482e-06
Iter: 520 loss: 4.25439612e-06
Iter: 521 loss: 4.24699738e-06
Iter: 522 loss: 4.31195531e-06
Iter: 523 loss: 4.2466886e-06
Iter: 524 loss: 4.24254586e-06
Iter: 525 loss: 4.24744849e-06
Iter: 526 loss: 4.24033124e-06
Iter: 527 loss: 4.23650135e-06
Iter: 528 loss: 4.23609936e-06
Iter: 529 loss: 4.23454367e-06
Iter: 530 loss: 4.22994071e-06
Iter: 531 loss: 4.25233793e-06
Iter: 532 loss: 4.2283009e-06
Iter: 533 loss: 4.2230713e-06
Iter: 534 loss: 4.22822768e-06
Iter: 535 loss: 4.21996583e-06
Iter: 536 loss: 4.21205368e-06
Iter: 537 loss: 4.26279121e-06
Iter: 538 loss: 4.2112606e-06
Iter: 539 loss: 4.20619835e-06
Iter: 540 loss: 4.20822289e-06
Iter: 541 loss: 4.20259948e-06
Iter: 542 loss: 4.19733078e-06
Iter: 543 loss: 4.2002157e-06
Iter: 544 loss: 4.19400067e-06
Iter: 545 loss: 4.18723721e-06
Iter: 546 loss: 4.2082047e-06
Iter: 547 loss: 4.18537229e-06
Iter: 548 loss: 4.17920546e-06
Iter: 549 loss: 4.18193349e-06
Iter: 550 loss: 4.17510091e-06
Iter: 551 loss: 4.16881949e-06
Iter: 552 loss: 4.16879811e-06
Iter: 553 loss: 4.16427974e-06
Iter: 554 loss: 4.15806244e-06
Iter: 555 loss: 4.15771774e-06
Iter: 556 loss: 4.14985789e-06
Iter: 557 loss: 4.18525906e-06
Iter: 558 loss: 4.14815167e-06
Iter: 559 loss: 4.14849092e-06
Iter: 560 loss: 4.14613442e-06
Iter: 561 loss: 4.14365741e-06
Iter: 562 loss: 4.13840417e-06
Iter: 563 loss: 4.21830509e-06
Iter: 564 loss: 4.13815e-06
Iter: 565 loss: 4.1332928e-06
Iter: 566 loss: 4.1307494e-06
Iter: 567 loss: 4.12852432e-06
Iter: 568 loss: 4.12340114e-06
Iter: 569 loss: 4.16768489e-06
Iter: 570 loss: 4.12308236e-06
Iter: 571 loss: 4.11738256e-06
Iter: 572 loss: 4.13360613e-06
Iter: 573 loss: 4.11558358e-06
Iter: 574 loss: 4.11046267e-06
Iter: 575 loss: 4.10804478e-06
Iter: 576 loss: 4.10552457e-06
Iter: 577 loss: 4.10049324e-06
Iter: 578 loss: 4.10610392e-06
Iter: 579 loss: 4.09768336e-06
Iter: 580 loss: 4.09080621e-06
Iter: 581 loss: 4.13535417e-06
Iter: 582 loss: 4.09005088e-06
Iter: 583 loss: 4.08582218e-06
Iter: 584 loss: 4.08125197e-06
Iter: 585 loss: 4.08055212e-06
Iter: 586 loss: 4.07480184e-06
Iter: 587 loss: 4.0746545e-06
Iter: 588 loss: 4.07025527e-06
Iter: 589 loss: 4.07141442e-06
Iter: 590 loss: 4.06679465e-06
Iter: 591 loss: 4.06010577e-06
Iter: 592 loss: 4.05131641e-06
Iter: 593 loss: 4.05076753e-06
Iter: 594 loss: 4.08080632e-06
Iter: 595 loss: 4.04911179e-06
Iter: 596 loss: 4.04758839e-06
Iter: 597 loss: 4.04373486e-06
Iter: 598 loss: 4.07732432e-06
Iter: 599 loss: 4.04289767e-06
Iter: 600 loss: 4.03870399e-06
Iter: 601 loss: 4.03204558e-06
Iter: 602 loss: 4.03197282e-06
Iter: 603 loss: 4.02561454e-06
Iter: 604 loss: 4.05494575e-06
Iter: 605 loss: 4.0242935e-06
Iter: 606 loss: 4.01956913e-06
Iter: 607 loss: 4.01953503e-06
Iter: 608 loss: 4.01506895e-06
Iter: 609 loss: 4.0155669e-06
Iter: 610 loss: 4.01170109e-06
Iter: 611 loss: 4.00901263e-06
Iter: 612 loss: 4.00830413e-06
Iter: 613 loss: 4.00639328e-06
Iter: 614 loss: 4.00088629e-06
Iter: 615 loss: 4.00602767e-06
Iter: 616 loss: 3.99757573e-06
Iter: 617 loss: 3.993061e-06
Iter: 618 loss: 4.0313339e-06
Iter: 619 loss: 3.99286409e-06
Iter: 620 loss: 3.9892584e-06
Iter: 621 loss: 3.989795e-06
Iter: 622 loss: 3.98644625e-06
Iter: 623 loss: 3.982213e-06
Iter: 624 loss: 4.0463965e-06
Iter: 625 loss: 3.98219436e-06
Iter: 626 loss: 3.97941358e-06
Iter: 627 loss: 3.97501071e-06
Iter: 628 loss: 3.97493841e-06
Iter: 629 loss: 3.97583608e-06
Iter: 630 loss: 3.97238682e-06
Iter: 631 loss: 3.97001168e-06
Iter: 632 loss: 3.96518e-06
Iter: 633 loss: 4.04337379e-06
Iter: 634 loss: 3.96500627e-06
Iter: 635 loss: 3.9619e-06
Iter: 636 loss: 3.95828192e-06
Iter: 637 loss: 3.9577526e-06
Iter: 638 loss: 3.9515e-06
Iter: 639 loss: 3.96778023e-06
Iter: 640 loss: 3.94925519e-06
Iter: 641 loss: 3.94409381e-06
Iter: 642 loss: 3.95414281e-06
Iter: 643 loss: 3.94212657e-06
Iter: 644 loss: 3.93802657e-06
Iter: 645 loss: 3.93766913e-06
Iter: 646 loss: 3.93525534e-06
Iter: 647 loss: 3.93301707e-06
Iter: 648 loss: 3.93242499e-06
Iter: 649 loss: 3.92874244e-06
Iter: 650 loss: 3.92407583e-06
Iter: 651 loss: 3.92364109e-06
Iter: 652 loss: 3.91745971e-06
Iter: 653 loss: 3.96301493e-06
Iter: 654 loss: 3.91693447e-06
Iter: 655 loss: 3.91213962e-06
Iter: 656 loss: 3.93092159e-06
Iter: 657 loss: 3.91105186e-06
Iter: 658 loss: 3.90634932e-06
Iter: 659 loss: 3.9307929e-06
Iter: 660 loss: 3.90550804e-06
Iter: 661 loss: 3.90276728e-06
Iter: 662 loss: 3.92832862e-06
Iter: 663 loss: 3.90274226e-06
Iter: 664 loss: 3.90000605e-06
Iter: 665 loss: 3.91798176e-06
Iter: 666 loss: 3.89974821e-06
Iter: 667 loss: 3.89819161e-06
Iter: 668 loss: 3.8935068e-06
Iter: 669 loss: 3.91533831e-06
Iter: 670 loss: 3.8919e-06
Iter: 671 loss: 3.88678336e-06
Iter: 672 loss: 3.91763024e-06
Iter: 673 loss: 3.88599619e-06
Iter: 674 loss: 3.88173794e-06
Iter: 675 loss: 3.8850385e-06
Iter: 676 loss: 3.87925e-06
Iter: 677 loss: 3.87387354e-06
Iter: 678 loss: 3.8938565e-06
Iter: 679 loss: 3.87269529e-06
Iter: 680 loss: 3.86913689e-06
Iter: 681 loss: 3.90229661e-06
Iter: 682 loss: 3.86897591e-06
Iter: 683 loss: 3.86486499e-06
Iter: 684 loss: 3.86851207e-06
Iter: 685 loss: 3.8622893e-06
Iter: 686 loss: 3.85921794e-06
Iter: 687 loss: 3.85562453e-06
Iter: 688 loss: 3.85516705e-06
Iter: 689 loss: 3.8489934e-06
Iter: 690 loss: 3.86507463e-06
Iter: 691 loss: 3.8468288e-06
Iter: 692 loss: 3.84129271e-06
Iter: 693 loss: 3.85637532e-06
Iter: 694 loss: 3.83960378e-06
Iter: 695 loss: 3.83370934e-06
Iter: 696 loss: 3.88807803e-06
Iter: 697 loss: 3.83357929e-06
Iter: 698 loss: 3.83156248e-06
Iter: 699 loss: 3.83074666e-06
Iter: 700 loss: 3.82878534e-06
Iter: 701 loss: 3.82599092e-06
Iter: 702 loss: 3.82601502e-06
Iter: 703 loss: 3.82351664e-06
Iter: 704 loss: 3.81912514e-06
Iter: 705 loss: 3.81915379e-06
Iter: 706 loss: 3.81426798e-06
Iter: 707 loss: 3.85601606e-06
Iter: 708 loss: 3.81396012e-06
Iter: 709 loss: 3.81068958e-06
Iter: 710 loss: 3.80781512e-06
Iter: 711 loss: 3.80699134e-06
Iter: 712 loss: 3.80205688e-06
Iter: 713 loss: 3.86722877e-06
Iter: 714 loss: 3.80207189e-06
Iter: 715 loss: 3.79997277e-06
Iter: 716 loss: 3.79988592e-06
Iter: 717 loss: 3.79801304e-06
Iter: 718 loss: 3.793514e-06
Iter: 719 loss: 3.84488476e-06
Iter: 720 loss: 3.79310268e-06
Iter: 721 loss: 3.78900222e-06
Iter: 722 loss: 3.81387099e-06
Iter: 723 loss: 3.78857158e-06
Iter: 724 loss: 3.78542131e-06
Iter: 725 loss: 3.78068512e-06
Iter: 726 loss: 3.78046252e-06
Iter: 727 loss: 3.77511856e-06
Iter: 728 loss: 3.80357073e-06
Iter: 729 loss: 3.77433e-06
Iter: 730 loss: 3.77085098e-06
Iter: 731 loss: 3.77002652e-06
Iter: 732 loss: 3.76771436e-06
Iter: 733 loss: 3.76869798e-06
Iter: 734 loss: 3.76554635e-06
Iter: 735 loss: 3.76350931e-06
Iter: 736 loss: 3.76360458e-06
Iter: 737 loss: 3.76203684e-06
Iter: 738 loss: 3.76027833e-06
Iter: 739 loss: 3.75767e-06
Iter: 740 loss: 3.75771742e-06
Iter: 741 loss: 3.7536056e-06
Iter: 742 loss: 3.75320269e-06
Iter: 743 loss: 3.75020181e-06
Iter: 744 loss: 3.74646811e-06
Iter: 745 loss: 3.78083655e-06
Iter: 746 loss: 3.74636e-06
Iter: 747 loss: 3.74258343e-06
Iter: 748 loss: 3.73800844e-06
Iter: 749 loss: 3.73772809e-06
Iter: 750 loss: 3.73360558e-06
Iter: 751 loss: 3.7335426e-06
Iter: 752 loss: 3.73084981e-06
Iter: 753 loss: 3.75566196e-06
Iter: 754 loss: 3.73071066e-06
Iter: 755 loss: 3.72832733e-06
Iter: 756 loss: 3.72401792e-06
Iter: 757 loss: 3.72404793e-06
Iter: 758 loss: 3.71964757e-06
Iter: 759 loss: 3.73038733e-06
Iter: 760 loss: 3.71818669e-06
Iter: 761 loss: 3.71344277e-06
Iter: 762 loss: 3.71645365e-06
Iter: 763 loss: 3.71033411e-06
Iter: 764 loss: 3.7060513e-06
Iter: 765 loss: 3.75934724e-06
Iter: 766 loss: 3.70603107e-06
Iter: 767 loss: 3.70464295e-06
Iter: 768 loss: 3.70391263e-06
Iter: 769 loss: 3.70295925e-06
Iter: 770 loss: 3.69986856e-06
Iter: 771 loss: 3.70561293e-06
Iter: 772 loss: 3.69787631e-06
Iter: 773 loss: 3.69428972e-06
Iter: 774 loss: 3.74506885e-06
Iter: 775 loss: 3.69424106e-06
Iter: 776 loss: 3.69184136e-06
Iter: 777 loss: 3.6888066e-06
Iter: 778 loss: 3.68860174e-06
Iter: 779 loss: 3.68402107e-06
Iter: 780 loss: 3.69482314e-06
Iter: 781 loss: 3.6822687e-06
Iter: 782 loss: 3.67784219e-06
Iter: 783 loss: 3.68458859e-06
Iter: 784 loss: 3.67573034e-06
Iter: 785 loss: 3.67244502e-06
Iter: 786 loss: 3.67243274e-06
Iter: 787 loss: 3.66950644e-06
Iter: 788 loss: 3.68319525e-06
Iter: 789 loss: 3.66893141e-06
Iter: 790 loss: 3.6667077e-06
Iter: 791 loss: 3.66175982e-06
Iter: 792 loss: 3.73059493e-06
Iter: 793 loss: 3.66147151e-06
Iter: 794 loss: 3.65755068e-06
Iter: 795 loss: 3.70534144e-06
Iter: 796 loss: 3.65751953e-06
Iter: 797 loss: 3.65438063e-06
Iter: 798 loss: 3.65198684e-06
Iter: 799 loss: 3.6510221e-06
Iter: 800 loss: 3.65124015e-06
Iter: 801 loss: 3.64875018e-06
Iter: 802 loss: 3.64712946e-06
Iter: 803 loss: 3.64370317e-06
Iter: 804 loss: 3.70226098e-06
Iter: 805 loss: 3.64356447e-06
Iter: 806 loss: 3.64104426e-06
Iter: 807 loss: 3.640253e-06
Iter: 808 loss: 3.63883578e-06
Iter: 809 loss: 3.63419258e-06
Iter: 810 loss: 3.64952848e-06
Iter: 811 loss: 3.63283607e-06
Iter: 812 loss: 3.62936021e-06
Iter: 813 loss: 3.65006508e-06
Iter: 814 loss: 3.62883725e-06
Iter: 815 loss: 3.62620699e-06
Iter: 816 loss: 3.62263836e-06
Iter: 817 loss: 3.62260926e-06
Iter: 818 loss: 3.61839898e-06
Iter: 819 loss: 3.67348275e-06
Iter: 820 loss: 3.61845059e-06
Iter: 821 loss: 3.61636194e-06
Iter: 822 loss: 3.64210109e-06
Iter: 823 loss: 3.61643424e-06
Iter: 824 loss: 3.61406956e-06
Iter: 825 loss: 3.61013826e-06
Iter: 826 loss: 3.7112452e-06
Iter: 827 loss: 3.61009597e-06
Iter: 828 loss: 3.60695822e-06
Iter: 829 loss: 3.61333059e-06
Iter: 830 loss: 3.60570448e-06
Iter: 831 loss: 3.60165654e-06
Iter: 832 loss: 3.60928425e-06
Iter: 833 loss: 3.60021022e-06
Iter: 834 loss: 3.59896421e-06
Iter: 835 loss: 3.59823525e-06
Iter: 836 loss: 3.59612727e-06
Iter: 837 loss: 3.59773958e-06
Iter: 838 loss: 3.59475962e-06
Iter: 839 loss: 3.59323985e-06
Iter: 840 loss: 3.58946886e-06
Iter: 841 loss: 3.62271885e-06
Iter: 842 loss: 3.58891793e-06
Iter: 843 loss: 3.58515354e-06
Iter: 844 loss: 3.63294203e-06
Iter: 845 loss: 3.58516263e-06
Iter: 846 loss: 3.58305465e-06
Iter: 847 loss: 3.58647503e-06
Iter: 848 loss: 3.58200441e-06
Iter: 849 loss: 3.57899694e-06
Iter: 850 loss: 3.57737486e-06
Iter: 851 loss: 3.57604608e-06
Iter: 852 loss: 3.57264753e-06
Iter: 853 loss: 3.60644185e-06
Iter: 854 loss: 3.57248132e-06
Iter: 855 loss: 3.57023237e-06
Iter: 856 loss: 3.56649457e-06
Iter: 857 loss: 3.56647934e-06
Iter: 858 loss: 3.56731516e-06
Iter: 859 loss: 3.56479086e-06
Iter: 860 loss: 3.56307646e-06
Iter: 861 loss: 3.55993848e-06
Iter: 862 loss: 3.62819651e-06
Iter: 863 loss: 3.55997577e-06
Iter: 864 loss: 3.55630164e-06
Iter: 865 loss: 3.56032433e-06
Iter: 866 loss: 3.55430188e-06
Iter: 867 loss: 3.55074985e-06
Iter: 868 loss: 3.56258352e-06
Iter: 869 loss: 3.54998747e-06
Iter: 870 loss: 3.54970325e-06
Iter: 871 loss: 3.54858616e-06
Iter: 872 loss: 3.54682061e-06
Iter: 873 loss: 3.5438436e-06
Iter: 874 loss: 3.54378608e-06
Iter: 875 loss: 3.54226768e-06
Iter: 876 loss: 3.54049689e-06
Iter: 877 loss: 3.54038275e-06
Iter: 878 loss: 3.53740575e-06
Iter: 879 loss: 3.5478497e-06
Iter: 880 loss: 3.53676251e-06
Iter: 881 loss: 3.53474206e-06
Iter: 882 loss: 3.54754161e-06
Iter: 883 loss: 3.53459131e-06
Iter: 884 loss: 3.5328128e-06
Iter: 885 loss: 3.53229939e-06
Iter: 886 loss: 3.53126097e-06
Iter: 887 loss: 3.52860434e-06
Iter: 888 loss: 3.54906115e-06
Iter: 889 loss: 3.52842289e-06
Iter: 890 loss: 3.52661891e-06
Iter: 891 loss: 3.53024757e-06
Iter: 892 loss: 3.52579195e-06
Iter: 893 loss: 3.52385337e-06
Iter: 894 loss: 3.53017208e-06
Iter: 895 loss: 3.5234234e-06
Iter: 896 loss: 3.52078223e-06
Iter: 897 loss: 3.52932125e-06
Iter: 898 loss: 3.52015468e-06
Iter: 899 loss: 3.51857307e-06
Iter: 900 loss: 3.51646122e-06
Iter: 901 loss: 3.51633594e-06
Iter: 902 loss: 3.51409585e-06
Iter: 903 loss: 3.52956658e-06
Iter: 904 loss: 3.51391122e-06
Iter: 905 loss: 3.51444623e-06
Iter: 906 loss: 3.51301128e-06
Iter: 907 loss: 3.51259405e-06
Iter: 908 loss: 3.51110702e-06
Iter: 909 loss: 3.50900427e-06
Iter: 910 loss: 3.50865798e-06
Iter: 911 loss: 3.50670052e-06
Iter: 912 loss: 3.53824157e-06
Iter: 913 loss: 3.50665164e-06
Iter: 914 loss: 3.50502319e-06
Iter: 915 loss: 3.50400614e-06
Iter: 916 loss: 3.5033263e-06
Iter: 917 loss: 3.50042433e-06
Iter: 918 loss: 3.52088819e-06
Iter: 919 loss: 3.50012556e-06
Iter: 920 loss: 3.49869333e-06
Iter: 921 loss: 3.50456253e-06
Iter: 922 loss: 3.49830589e-06
Iter: 923 loss: 3.49670563e-06
Iter: 924 loss: 3.49695529e-06
Iter: 925 loss: 3.49534048e-06
Iter: 926 loss: 3.49318202e-06
Iter: 927 loss: 3.50682603e-06
Iter: 928 loss: 3.49283982e-06
Iter: 929 loss: 3.49140328e-06
Iter: 930 loss: 3.50343021e-06
Iter: 931 loss: 3.49139782e-06
Iter: 932 loss: 3.49008974e-06
Iter: 933 loss: 3.48870162e-06
Iter: 934 loss: 3.48840967e-06
Iter: 935 loss: 3.48624985e-06
Iter: 936 loss: 3.49161246e-06
Iter: 937 loss: 3.48550111e-06
Iter: 938 loss: 3.48467984e-06
Iter: 939 loss: 3.48456251e-06
Iter: 940 loss: 3.48341814e-06
Iter: 941 loss: 3.481e-06
Iter: 942 loss: 3.53189876e-06
Iter: 943 loss: 3.48100934e-06
Iter: 944 loss: 3.47920241e-06
Iter: 945 loss: 3.47708556e-06
Iter: 946 loss: 3.47691184e-06
Iter: 947 loss: 3.47434252e-06
Iter: 948 loss: 3.48037565e-06
Iter: 949 loss: 3.4733539e-06
Iter: 950 loss: 3.47054311e-06
Iter: 951 loss: 3.48860362e-06
Iter: 952 loss: 3.47036939e-06
Iter: 953 loss: 3.46847378e-06
Iter: 954 loss: 3.47607738e-06
Iter: 955 loss: 3.4679324e-06
Iter: 956 loss: 3.46597471e-06
Iter: 957 loss: 3.46573438e-06
Iter: 958 loss: 3.46427692e-06
Iter: 959 loss: 3.46146021e-06
Iter: 960 loss: 3.48363255e-06
Iter: 961 loss: 3.46137e-06
Iter: 962 loss: 3.45960461e-06
Iter: 963 loss: 3.46656861e-06
Iter: 964 loss: 3.45918897e-06
Iter: 965 loss: 3.45736839e-06
Iter: 966 loss: 3.46418096e-06
Iter: 967 loss: 3.45695162e-06
Iter: 968 loss: 3.45513945e-06
Iter: 969 loss: 3.45471472e-06
Iter: 970 loss: 3.45360309e-06
Iter: 971 loss: 3.4516795e-06
Iter: 972 loss: 3.45252147e-06
Iter: 973 loss: 3.45023886e-06
Iter: 974 loss: 3.45117405e-06
Iter: 975 loss: 3.44922023e-06
Iter: 976 loss: 3.44825071e-06
Iter: 977 loss: 3.44558089e-06
Iter: 978 loss: 3.46921638e-06
Iter: 979 loss: 3.44520572e-06
Iter: 980 loss: 3.44344267e-06
Iter: 981 loss: 3.4409411e-06
Iter: 982 loss: 3.44086811e-06
Iter: 983 loss: 3.43723855e-06
Iter: 984 loss: 3.4556806e-06
Iter: 985 loss: 3.43670445e-06
Iter: 986 loss: 3.43407737e-06
Iter: 987 loss: 3.44949626e-06
Iter: 988 loss: 3.43375086e-06
Iter: 989 loss: 3.43136298e-06
Iter: 990 loss: 3.43834745e-06
Iter: 991 loss: 3.43061924e-06
Iter: 992 loss: 3.42857652e-06
Iter: 993 loss: 3.42957082e-06
Iter: 994 loss: 3.42721955e-06
Iter: 995 loss: 3.42460521e-06
Iter: 996 loss: 3.43919896e-06
Iter: 997 loss: 3.42410704e-06
Iter: 998 loss: 3.42234307e-06
Iter: 999 loss: 3.44300338e-06
Iter: 1000 loss: 3.42232715e-06
Iter: 1001 loss: 3.42072303e-06
Iter: 1002 loss: 3.42396311e-06
Iter: 1003 loss: 3.42002932e-06
Iter: 1004 loss: 3.41829491e-06
Iter: 1005 loss: 3.41690497e-06
Iter: 1006 loss: 3.41643226e-06
Iter: 1007 loss: 3.41468717e-06
Iter: 1008 loss: 3.42852491e-06
Iter: 1009 loss: 3.41454142e-06
Iter: 1010 loss: 3.41386885e-06
Iter: 1011 loss: 3.41359782e-06
Iter: 1012 loss: 3.41309487e-06
Iter: 1013 loss: 3.41139548e-06
Iter: 1014 loss: 3.40991e-06
Iter: 1015 loss: 3.40926863e-06
Iter: 1016 loss: 3.4058512e-06
Iter: 1017 loss: 3.42091698e-06
Iter: 1018 loss: 3.40541e-06
Iter: 1019 loss: 3.40293377e-06
Iter: 1020 loss: 3.40532824e-06
Iter: 1021 loss: 3.40156726e-06
Iter: 1022 loss: 3.39905546e-06
Iter: 1023 loss: 3.42807834e-06
Iter: 1024 loss: 3.39903227e-06
Iter: 1025 loss: 3.39708595e-06
Iter: 1026 loss: 3.4006398e-06
Iter: 1027 loss: 3.39643566e-06
Iter: 1028 loss: 3.39400913e-06
Iter: 1029 loss: 3.3961478e-06
Iter: 1030 loss: 3.39247572e-06
Iter: 1031 loss: 3.3906565e-06
Iter: 1032 loss: 3.40954443e-06
Iter: 1033 loss: 3.39057169e-06
Iter: 1034 loss: 3.38874861e-06
Iter: 1035 loss: 3.39347571e-06
Iter: 1036 loss: 3.38809969e-06
Iter: 1037 loss: 3.38611358e-06
Iter: 1038 loss: 3.39219332e-06
Iter: 1039 loss: 3.38551126e-06
Iter: 1040 loss: 3.38424843e-06
Iter: 1041 loss: 3.38497512e-06
Iter: 1042 loss: 3.38339078e-06
Iter: 1043 loss: 3.38290101e-06
Iter: 1044 loss: 3.38263976e-06
Iter: 1045 loss: 3.38165364e-06
Iter: 1046 loss: 3.37978167e-06
Iter: 1047 loss: 3.41595774e-06
Iter: 1048 loss: 3.37972915e-06
Iter: 1049 loss: 3.37855e-06
Iter: 1050 loss: 3.37746633e-06
Iter: 1051 loss: 3.37720098e-06
Iter: 1052 loss: 3.37507231e-06
Iter: 1053 loss: 3.37504707e-06
Iter: 1054 loss: 3.37339907e-06
Iter: 1055 loss: 3.37034339e-06
Iter: 1056 loss: 3.38031214e-06
Iter: 1057 loss: 3.36950052e-06
Iter: 1058 loss: 3.36690368e-06
Iter: 1059 loss: 3.38340601e-06
Iter: 1060 loss: 3.36666267e-06
Iter: 1061 loss: 3.36461812e-06
Iter: 1062 loss: 3.37639131e-06
Iter: 1063 loss: 3.36433959e-06
Iter: 1064 loss: 3.36253174e-06
Iter: 1065 loss: 3.36247786e-06
Iter: 1066 loss: 3.36105268e-06
Iter: 1067 loss: 3.3587794e-06
Iter: 1068 loss: 3.37275515e-06
Iter: 1069 loss: 3.35847449e-06
Iter: 1070 loss: 3.35662753e-06
Iter: 1071 loss: 3.37465349e-06
Iter: 1072 loss: 3.35651066e-06
Iter: 1073 loss: 3.35508275e-06
Iter: 1074 loss: 3.35482082e-06
Iter: 1075 loss: 3.35380309e-06
Iter: 1076 loss: 3.35218328e-06
Iter: 1077 loss: 3.35928371e-06
Iter: 1078 loss: 3.35183722e-06
Iter: 1079 loss: 3.35176537e-06
Iter: 1080 loss: 3.35120058e-06
Iter: 1081 loss: 3.35072764e-06
Iter: 1082 loss: 3.34932656e-06
Iter: 1083 loss: 3.34946299e-06
Iter: 1084 loss: 3.34789502e-06
Iter: 1085 loss: 3.34533661e-06
Iter: 1086 loss: 3.35058326e-06
Iter: 1087 loss: 3.34416109e-06
Iter: 1088 loss: 3.34199694e-06
Iter: 1089 loss: 3.34372771e-06
Iter: 1090 loss: 3.34086053e-06
Iter: 1091 loss: 3.33747357e-06
Iter: 1092 loss: 3.34286392e-06
Iter: 1093 loss: 3.33582693e-06
Iter: 1094 loss: 3.33334856e-06
Iter: 1095 loss: 3.34248671e-06
Iter: 1096 loss: 3.33265371e-06
Iter: 1097 loss: 3.33037588e-06
Iter: 1098 loss: 3.34911033e-06
Iter: 1099 loss: 3.33020034e-06
Iter: 1100 loss: 3.32826789e-06
Iter: 1101 loss: 3.33611024e-06
Iter: 1102 loss: 3.32785294e-06
Iter: 1103 loss: 3.32621016e-06
Iter: 1104 loss: 3.32668105e-06
Iter: 1105 loss: 3.3250185e-06
Iter: 1106 loss: 3.32357808e-06
Iter: 1107 loss: 3.32358377e-06
Iter: 1108 loss: 3.32235641e-06
Iter: 1109 loss: 3.32271043e-06
Iter: 1110 loss: 3.32138234e-06
Iter: 1111 loss: 3.32003856e-06
Iter: 1112 loss: 3.32790387e-06
Iter: 1113 loss: 3.3199135e-06
Iter: 1114 loss: 3.3185338e-06
Iter: 1115 loss: 3.33268508e-06
Iter: 1116 loss: 3.31846263e-06
Iter: 1117 loss: 3.31794126e-06
Iter: 1118 loss: 3.31624869e-06
Iter: 1119 loss: 3.32430454e-06
Iter: 1120 loss: 3.31565752e-06
Iter: 1121 loss: 3.31364458e-06
Iter: 1122 loss: 3.3184956e-06
Iter: 1123 loss: 3.31295223e-06
Iter: 1124 loss: 3.31092906e-06
Iter: 1125 loss: 3.31630895e-06
Iter: 1126 loss: 3.31034539e-06
Iter: 1127 loss: 3.30787771e-06
Iter: 1128 loss: 3.30736816e-06
Iter: 1129 loss: 3.30584226e-06
Iter: 1130 loss: 3.30299054e-06
Iter: 1131 loss: 3.32643049e-06
Iter: 1132 loss: 3.30281591e-06
Iter: 1133 loss: 3.30053695e-06
Iter: 1134 loss: 3.30132866e-06
Iter: 1135 loss: 3.29890918e-06
Iter: 1136 loss: 3.29702061e-06
Iter: 1137 loss: 3.29685759e-06
Iter: 1138 loss: 3.29535192e-06
Iter: 1139 loss: 3.29341401e-06
Iter: 1140 loss: 3.29317027e-06
Iter: 1141 loss: 3.2911089e-06
Iter: 1142 loss: 3.29112504e-06
Iter: 1143 loss: 3.28942133e-06
Iter: 1144 loss: 3.29057821e-06
Iter: 1145 loss: 3.2883645e-06
Iter: 1146 loss: 3.28795522e-06
Iter: 1147 loss: 3.28754481e-06
Iter: 1148 loss: 3.28692977e-06
Iter: 1149 loss: 3.28501733e-06
Iter: 1150 loss: 3.29759837e-06
Iter: 1151 loss: 3.28465785e-06
Iter: 1152 loss: 3.28220585e-06
Iter: 1153 loss: 3.28469537e-06
Iter: 1154 loss: 3.28071496e-06
Iter: 1155 loss: 3.27887165e-06
Iter: 1156 loss: 3.28282636e-06
Iter: 1157 loss: 3.27803104e-06
Iter: 1158 loss: 3.27519274e-06
Iter: 1159 loss: 3.27603675e-06
Iter: 1160 loss: 3.27329644e-06
Iter: 1161 loss: 3.27004591e-06
Iter: 1162 loss: 3.28639226e-06
Iter: 1163 loss: 3.26939676e-06
Iter: 1164 loss: 3.26657619e-06
Iter: 1165 loss: 3.26815029e-06
Iter: 1166 loss: 3.26460668e-06
Iter: 1167 loss: 3.26132249e-06
Iter: 1168 loss: 3.27521752e-06
Iter: 1169 loss: 3.2604562e-06
Iter: 1170 loss: 3.25769315e-06
Iter: 1171 loss: 3.27286375e-06
Iter: 1172 loss: 3.25734186e-06
Iter: 1173 loss: 3.2551693e-06
Iter: 1174 loss: 3.25501696e-06
Iter: 1175 loss: 3.25323344e-06
Iter: 1176 loss: 3.25074097e-06
Iter: 1177 loss: 3.25064752e-06
Iter: 1178 loss: 3.24924304e-06
Iter: 1179 loss: 3.25839619e-06
Iter: 1180 loss: 3.24907978e-06
Iter: 1181 loss: 3.24762641e-06
Iter: 1182 loss: 3.25607266e-06
Iter: 1183 loss: 3.24738312e-06
Iter: 1184 loss: 3.24605435e-06
Iter: 1185 loss: 3.24574853e-06
Iter: 1186 loss: 3.24484949e-06
Iter: 1187 loss: 3.2431758e-06
Iter: 1188 loss: 3.24117332e-06
Iter: 1189 loss: 3.24089433e-06
Iter: 1190 loss: 3.23884433e-06
Iter: 1191 loss: 3.24169446e-06
Iter: 1192 loss: 3.23783456e-06
Iter: 1193 loss: 3.23497306e-06
Iter: 1194 loss: 3.24331e-06
Iter: 1195 loss: 3.23408358e-06
Iter: 1196 loss: 3.23148834e-06
Iter: 1197 loss: 3.23866698e-06
Iter: 1198 loss: 3.23045492e-06
Iter: 1199 loss: 3.22761593e-06
Iter: 1200 loss: 3.22505252e-06
Iter: 1201 loss: 3.22435881e-06
Iter: 1202 loss: 3.22024925e-06
Iter: 1203 loss: 3.26816416e-06
Iter: 1204 loss: 3.22028836e-06
Iter: 1205 loss: 3.21736525e-06
Iter: 1206 loss: 3.21821676e-06
Iter: 1207 loss: 3.21526522e-06
Iter: 1208 loss: 3.21165135e-06
Iter: 1209 loss: 3.22815094e-06
Iter: 1210 loss: 3.21098514e-06
Iter: 1211 loss: 3.20772642e-06
Iter: 1212 loss: 3.22097731e-06
Iter: 1213 loss: 3.20696154e-06
Iter: 1214 loss: 3.20511276e-06
Iter: 1215 loss: 3.20494973e-06
Iter: 1216 loss: 3.20334107e-06
Iter: 1217 loss: 3.20698859e-06
Iter: 1218 loss: 3.20265394e-06
Iter: 1219 loss: 3.20141316e-06
Iter: 1220 loss: 3.20004642e-06
Iter: 1221 loss: 3.19981427e-06
Iter: 1222 loss: 3.19722812e-06
Iter: 1223 loss: 3.19768219e-06
Iter: 1224 loss: 3.19516539e-06
Iter: 1225 loss: 3.19299124e-06
Iter: 1226 loss: 3.19455285e-06
Iter: 1227 loss: 3.19153787e-06
Iter: 1228 loss: 3.18836646e-06
Iter: 1229 loss: 3.19373476e-06
Iter: 1230 loss: 3.18694e-06
Iter: 1231 loss: 3.18419825e-06
Iter: 1232 loss: 3.21032212e-06
Iter: 1233 loss: 3.18415505e-06
Iter: 1234 loss: 3.18181787e-06
Iter: 1235 loss: 3.18200046e-06
Iter: 1236 loss: 3.17995273e-06
Iter: 1237 loss: 3.17699073e-06
Iter: 1238 loss: 3.18583761e-06
Iter: 1239 loss: 3.17612603e-06
Iter: 1240 loss: 3.1733955e-06
Iter: 1241 loss: 3.17309832e-06
Iter: 1242 loss: 3.1712309e-06
Iter: 1243 loss: 3.16758769e-06
Iter: 1244 loss: 3.19890478e-06
Iter: 1245 loss: 3.16741398e-06
Iter: 1246 loss: 3.16488604e-06
Iter: 1247 loss: 3.18025423e-06
Iter: 1248 loss: 3.1647287e-06
Iter: 1249 loss: 3.16155956e-06
Iter: 1250 loss: 3.18015373e-06
Iter: 1251 loss: 3.16127944e-06
Iter: 1252 loss: 3.15908164e-06
Iter: 1253 loss: 3.16154365e-06
Iter: 1254 loss: 3.15771558e-06
Iter: 1255 loss: 3.15618217e-06
Iter: 1256 loss: 3.15713896e-06
Iter: 1257 loss: 3.15513353e-06
Iter: 1258 loss: 3.1528798e-06
Iter: 1259 loss: 3.15157786e-06
Iter: 1260 loss: 3.15061084e-06
Iter: 1261 loss: 3.14758654e-06
Iter: 1262 loss: 3.15248212e-06
Iter: 1263 loss: 3.14622139e-06
Iter: 1264 loss: 3.14307226e-06
Iter: 1265 loss: 3.15162697e-06
Iter: 1266 loss: 3.14197223e-06
Iter: 1267 loss: 3.13865121e-06
Iter: 1268 loss: 3.14871204e-06
Iter: 1269 loss: 3.13766577e-06
Iter: 1270 loss: 3.13399664e-06
Iter: 1271 loss: 3.14021963e-06
Iter: 1272 loss: 3.13223973e-06
Iter: 1273 loss: 3.12867314e-06
Iter: 1274 loss: 3.15011357e-06
Iter: 1275 loss: 3.12819157e-06
Iter: 1276 loss: 3.12557449e-06
Iter: 1277 loss: 3.12588622e-06
Iter: 1278 loss: 3.12358907e-06
Iter: 1279 loss: 3.11942676e-06
Iter: 1280 loss: 3.13319606e-06
Iter: 1281 loss: 3.11826489e-06
Iter: 1282 loss: 3.11602707e-06
Iter: 1283 loss: 3.1400823e-06
Iter: 1284 loss: 3.11601934e-06
Iter: 1285 loss: 3.11348163e-06
Iter: 1286 loss: 3.12864677e-06
Iter: 1287 loss: 3.11305712e-06
Iter: 1288 loss: 3.11176836e-06
Iter: 1289 loss: 3.11021086e-06
Iter: 1290 loss: 3.10997666e-06
Iter: 1291 loss: 3.10745418e-06
Iter: 1292 loss: 3.11424697e-06
Iter: 1293 loss: 3.10660789e-06
Iter: 1294 loss: 3.10486575e-06
Iter: 1295 loss: 3.10181713e-06
Iter: 1296 loss: 3.1017737e-06
Iter: 1297 loss: 3.09802454e-06
Iter: 1298 loss: 3.13664736e-06
Iter: 1299 loss: 3.097844e-06
Iter: 1300 loss: 3.09551115e-06
Iter: 1301 loss: 3.09084089e-06
Iter: 1302 loss: 3.19600758e-06
Iter: 1303 loss: 3.09080838e-06
Iter: 1304 loss: 3.08558788e-06
Iter: 1305 loss: 3.12845509e-06
Iter: 1306 loss: 3.08526523e-06
Iter: 1307 loss: 3.08100289e-06
Iter: 1308 loss: 3.09858956e-06
Iter: 1309 loss: 3.08012022e-06
Iter: 1310 loss: 3.07619757e-06
Iter: 1311 loss: 3.08573544e-06
Iter: 1312 loss: 3.0746894e-06
Iter: 1313 loss: 3.07158598e-06
Iter: 1314 loss: 3.07907885e-06
Iter: 1315 loss: 3.07036726e-06
Iter: 1316 loss: 3.06697075e-06
Iter: 1317 loss: 3.0687745e-06
Iter: 1318 loss: 3.06459151e-06
Iter: 1319 loss: 3.06103175e-06
Iter: 1320 loss: 3.09124152e-06
Iter: 1321 loss: 3.06077982e-06
Iter: 1322 loss: 3.06100674e-06
Iter: 1323 loss: 3.05937942e-06
Iter: 1324 loss: 3.05844e-06
Iter: 1325 loss: 3.05560297e-06
Iter: 1326 loss: 3.06968286e-06
Iter: 1327 loss: 3.05468711e-06
Iter: 1328 loss: 3.0528081e-06
Iter: 1329 loss: 3.05279445e-06
Iter: 1330 loss: 3.05089316e-06
Iter: 1331 loss: 3.04734931e-06
Iter: 1332 loss: 3.1256036e-06
Iter: 1333 loss: 3.04731793e-06
Iter: 1334 loss: 3.04378682e-06
Iter: 1335 loss: 3.05114418e-06
Iter: 1336 loss: 3.04257e-06
Iter: 1337 loss: 3.03860497e-06
Iter: 1338 loss: 3.04283958e-06
Iter: 1339 loss: 3.03663796e-06
Iter: 1340 loss: 3.03373622e-06
Iter: 1341 loss: 3.07536357e-06
Iter: 1342 loss: 3.03373281e-06
Iter: 1343 loss: 3.0313181e-06
Iter: 1344 loss: 3.03427169e-06
Iter: 1345 loss: 3.0298786e-06
Iter: 1346 loss: 3.02670333e-06
Iter: 1347 loss: 3.03257593e-06
Iter: 1348 loss: 3.02525723e-06
Iter: 1349 loss: 3.02256217e-06
Iter: 1350 loss: 3.03182492e-06
Iter: 1351 loss: 3.02206126e-06
Iter: 1352 loss: 3.01897057e-06
Iter: 1353 loss: 3.02029639e-06
Iter: 1354 loss: 3.01684486e-06
Iter: 1355 loss: 3.01366026e-06
Iter: 1356 loss: 3.02157741e-06
Iter: 1357 loss: 3.0125675e-06
Iter: 1358 loss: 3.01529985e-06
Iter: 1359 loss: 3.01147361e-06
Iter: 1360 loss: 3.0105839e-06
Iter: 1361 loss: 3.00811371e-06
Iter: 1362 loss: 3.02052058e-06
Iter: 1363 loss: 3.00717738e-06
Iter: 1364 loss: 3.00517559e-06
Iter: 1365 loss: 3.0258027e-06
Iter: 1366 loss: 3.00511897e-06
Iter: 1367 loss: 3.00311126e-06
Iter: 1368 loss: 3.00290822e-06
Iter: 1369 loss: 3.00141892e-06
Iter: 1370 loss: 2.99934527e-06
Iter: 1371 loss: 3.00049805e-06
Iter: 1372 loss: 2.99799899e-06
Iter: 1373 loss: 2.99497788e-06
Iter: 1374 loss: 2.99575458e-06
Iter: 1375 loss: 2.9926664e-06
Iter: 1376 loss: 2.98963096e-06
Iter: 1377 loss: 3.00956663e-06
Iter: 1378 loss: 2.9893281e-06
Iter: 1379 loss: 2.98661e-06
Iter: 1380 loss: 2.98697432e-06
Iter: 1381 loss: 2.98454961e-06
Iter: 1382 loss: 2.9817179e-06
Iter: 1383 loss: 3.00033435e-06
Iter: 1384 loss: 2.98140048e-06
Iter: 1385 loss: 2.97872066e-06
Iter: 1386 loss: 2.99412022e-06
Iter: 1387 loss: 2.97844963e-06
Iter: 1388 loss: 2.97611314e-06
Iter: 1389 loss: 2.98358805e-06
Iter: 1390 loss: 2.97550923e-06
Iter: 1391 loss: 2.97382257e-06
Iter: 1392 loss: 2.97754013e-06
Iter: 1393 loss: 2.97321685e-06
Iter: 1394 loss: 2.9703956e-06
Iter: 1395 loss: 2.98506529e-06
Iter: 1396 loss: 2.96995836e-06
Iter: 1397 loss: 2.96910457e-06
Iter: 1398 loss: 2.96694907e-06
Iter: 1399 loss: 2.99619114e-06
Iter: 1400 loss: 2.9669136e-06
Iter: 1401 loss: 2.9652374e-06
Iter: 1402 loss: 2.96524468e-06
Iter: 1403 loss: 2.96387771e-06
Iter: 1404 loss: 2.96089684e-06
Iter: 1405 loss: 3.01221189e-06
Iter: 1406 loss: 2.96083249e-06
Iter: 1407 loss: 2.9579885e-06
Iter: 1408 loss: 2.96742814e-06
Iter: 1409 loss: 2.95716677e-06
Iter: 1410 loss: 2.95460586e-06
Iter: 1411 loss: 2.96529925e-06
Iter: 1412 loss: 2.95408654e-06
Iter: 1413 loss: 2.95177597e-06
Iter: 1414 loss: 2.95267e-06
Iter: 1415 loss: 2.95016343e-06
Iter: 1416 loss: 2.94740425e-06
Iter: 1417 loss: 2.95827203e-06
Iter: 1418 loss: 2.94682286e-06
Iter: 1419 loss: 2.94464871e-06
Iter: 1420 loss: 2.95329414e-06
Iter: 1421 loss: 2.94405731e-06
Iter: 1422 loss: 2.94164511e-06
Iter: 1423 loss: 2.94121196e-06
Iter: 1424 loss: 2.93965286e-06
Iter: 1425 loss: 2.93790072e-06
Iter: 1426 loss: 2.93794665e-06
Iter: 1427 loss: 2.93678613e-06
Iter: 1428 loss: 2.93677067e-06
Iter: 1429 loss: 2.93564381e-06
Iter: 1430 loss: 2.93588892e-06
Iter: 1431 loss: 2.93488529e-06
Iter: 1432 loss: 2.93387e-06
Iter: 1433 loss: 2.93196854e-06
Iter: 1434 loss: 2.9721873e-06
Iter: 1435 loss: 2.93194739e-06
Iter: 1436 loss: 2.93013454e-06
Iter: 1437 loss: 2.94430401e-06
Iter: 1438 loss: 2.93002677e-06
Iter: 1439 loss: 2.92801292e-06
Iter: 1440 loss: 2.93008975e-06
Iter: 1441 loss: 2.9269595e-06
Iter: 1442 loss: 2.925329e-06
Iter: 1443 loss: 2.92466666e-06
Iter: 1444 loss: 2.92396e-06
Iter: 1445 loss: 2.92153709e-06
Iter: 1446 loss: 2.92245886e-06
Iter: 1447 loss: 2.91989159e-06
Iter: 1448 loss: 2.91793867e-06
Iter: 1449 loss: 2.91797733e-06
Iter: 1450 loss: 2.91627293e-06
Iter: 1451 loss: 2.91618471e-06
Iter: 1452 loss: 2.91496463e-06
Iter: 1453 loss: 2.91268907e-06
Iter: 1454 loss: 2.92432628e-06
Iter: 1455 loss: 2.91215565e-06
Iter: 1456 loss: 2.91064134e-06
Iter: 1457 loss: 2.90945718e-06
Iter: 1458 loss: 2.9087339e-06
Iter: 1459 loss: 2.90635762e-06
Iter: 1460 loss: 2.9301043e-06
Iter: 1461 loss: 2.90624757e-06
Iter: 1462 loss: 2.90632761e-06
Iter: 1463 loss: 2.90544858e-06
Iter: 1464 loss: 2.90469279e-06
Iter: 1465 loss: 2.90282355e-06
Iter: 1466 loss: 2.92532923e-06
Iter: 1467 loss: 2.90271873e-06
Iter: 1468 loss: 2.90146727e-06
Iter: 1469 loss: 2.90483786e-06
Iter: 1470 loss: 2.90104754e-06
Iter: 1471 loss: 2.89944228e-06
Iter: 1472 loss: 2.89962645e-06
Iter: 1473 loss: 2.89821355e-06
Iter: 1474 loss: 2.89664467e-06
Iter: 1475 loss: 2.89664831e-06
Iter: 1476 loss: 2.89525315e-06
Iter: 1477 loss: 2.89425702e-06
Iter: 1478 loss: 2.89390937e-06
Iter: 1479 loss: 2.89218451e-06
Iter: 1480 loss: 2.89042509e-06
Iter: 1481 loss: 2.88996171e-06
Iter: 1482 loss: 2.88755609e-06
Iter: 1483 loss: 2.90368462e-06
Iter: 1484 loss: 2.88738329e-06
Iter: 1485 loss: 2.88509273e-06
Iter: 1486 loss: 2.88536467e-06
Iter: 1487 loss: 2.88334058e-06
Iter: 1488 loss: 2.88086881e-06
Iter: 1489 loss: 2.89433137e-06
Iter: 1490 loss: 2.88045612e-06
Iter: 1491 loss: 2.87826242e-06
Iter: 1492 loss: 2.88178262e-06
Iter: 1493 loss: 2.87720468e-06
Iter: 1494 loss: 2.8783852e-06
Iter: 1495 loss: 2.8765437e-06
Iter: 1496 loss: 2.87606122e-06
Iter: 1497 loss: 2.87533317e-06
Iter: 1498 loss: 2.87522926e-06
Iter: 1499 loss: 2.87422745e-06
Iter: 1500 loss: 2.87232933e-06
Iter: 1501 loss: 2.91592346e-06
Iter: 1502 loss: 2.87233661e-06
Iter: 1503 loss: 2.87044168e-06
Iter: 1504 loss: 2.89140985e-06
Iter: 1505 loss: 2.87028297e-06
Iter: 1506 loss: 2.86900013e-06
Iter: 1507 loss: 2.87480793e-06
Iter: 1508 loss: 2.86864679e-06
Iter: 1509 loss: 2.86707473e-06
Iter: 1510 loss: 2.86900695e-06
Iter: 1511 loss: 2.86620707e-06
Iter: 1512 loss: 2.86447676e-06
Iter: 1513 loss: 2.86676163e-06
Iter: 1514 loss: 2.86343356e-06
Iter: 1515 loss: 2.86160684e-06
Iter: 1516 loss: 2.86044929e-06
Iter: 1517 loss: 2.85970555e-06
Iter: 1518 loss: 2.85743658e-06
Iter: 1519 loss: 2.86542445e-06
Iter: 1520 loss: 2.85678402e-06
Iter: 1521 loss: 2.85444617e-06
Iter: 1522 loss: 2.86674231e-06
Iter: 1523 loss: 2.85409669e-06
Iter: 1524 loss: 2.85216902e-06
Iter: 1525 loss: 2.85664828e-06
Iter: 1526 loss: 2.85152873e-06
Iter: 1527 loss: 2.84959151e-06
Iter: 1528 loss: 2.85853912e-06
Iter: 1529 loss: 2.84934276e-06
Iter: 1530 loss: 2.84795283e-06
Iter: 1531 loss: 2.84792441e-06
Iter: 1532 loss: 2.84734188e-06
Iter: 1533 loss: 2.84569455e-06
Iter: 1534 loss: 2.84965427e-06
Iter: 1535 loss: 2.84469525e-06
Iter: 1536 loss: 2.84208022e-06
Iter: 1537 loss: 2.85347551e-06
Iter: 1538 loss: 2.84161479e-06
Iter: 1539 loss: 2.83981399e-06
Iter: 1540 loss: 2.8585946e-06
Iter: 1541 loss: 2.83977442e-06
Iter: 1542 loss: 2.8383547e-06
Iter: 1543 loss: 2.84171438e-06
Iter: 1544 loss: 2.8378829e-06
Iter: 1545 loss: 2.83631607e-06
Iter: 1546 loss: 2.83758686e-06
Iter: 1547 loss: 2.83536929e-06
Iter: 1548 loss: 2.8336874e-06
Iter: 1549 loss: 2.83755207e-06
Iter: 1550 loss: 2.83293e-06
Iter: 1551 loss: 2.83134978e-06
Iter: 1552 loss: 2.82956535e-06
Iter: 1553 loss: 2.82913038e-06
Iter: 1554 loss: 2.82662882e-06
Iter: 1555 loss: 2.84586417e-06
Iter: 1556 loss: 2.82646511e-06
Iter: 1557 loss: 2.82457404e-06
Iter: 1558 loss: 2.82445035e-06
Iter: 1559 loss: 2.82304973e-06
Iter: 1560 loss: 2.82073393e-06
Iter: 1561 loss: 2.84090265e-06
Iter: 1562 loss: 2.82053429e-06
Iter: 1563 loss: 2.82076917e-06
Iter: 1564 loss: 2.81976327e-06
Iter: 1565 loss: 2.81894154e-06
Iter: 1566 loss: 2.8172326e-06
Iter: 1567 loss: 2.85091846e-06
Iter: 1568 loss: 2.81715688e-06
Iter: 1569 loss: 2.81608163e-06
Iter: 1570 loss: 2.81466555e-06
Iter: 1571 loss: 2.81446501e-06
Iter: 1572 loss: 2.81191024e-06
Iter: 1573 loss: 2.81866596e-06
Iter: 1574 loss: 2.81108305e-06
Iter: 1575 loss: 2.80911445e-06
Iter: 1576 loss: 2.80923905e-06
Iter: 1577 loss: 2.80809627e-06
Iter: 1578 loss: 2.80923791e-06
Iter: 1579 loss: 2.80749464e-06
Iter: 1580 loss: 2.80595623e-06
Iter: 1581 loss: 2.80458835e-06
Iter: 1582 loss: 2.80414042e-06
Iter: 1583 loss: 2.80208246e-06
Iter: 1584 loss: 2.82032443e-06
Iter: 1585 loss: 2.80203858e-06
Iter: 1586 loss: 2.80053291e-06
Iter: 1587 loss: 2.7982469e-06
Iter: 1588 loss: 2.79822689e-06
Iter: 1589 loss: 2.79565938e-06
Iter: 1590 loss: 2.80818e-06
Iter: 1591 loss: 2.7953779e-06
Iter: 1592 loss: 2.79252845e-06
Iter: 1593 loss: 2.79501592e-06
Iter: 1594 loss: 2.79080837e-06
Iter: 1595 loss: 2.78842458e-06
Iter: 1596 loss: 2.79359529e-06
Iter: 1597 loss: 2.78747893e-06
Iter: 1598 loss: 2.79046299e-06
Iter: 1599 loss: 2.78660855e-06
Iter: 1600 loss: 2.78599282e-06
Iter: 1601 loss: 2.78433936e-06
Iter: 1602 loss: 2.79613664e-06
Iter: 1603 loss: 2.78396601e-06
Iter: 1604 loss: 2.78255357e-06
Iter: 1605 loss: 2.78452194e-06
Iter: 1606 loss: 2.78178345e-06
Iter: 1607 loss: 2.77994423e-06
Iter: 1608 loss: 2.78414291e-06
Iter: 1609 loss: 2.77926119e-06
Iter: 1610 loss: 2.77730396e-06
Iter: 1611 loss: 2.79955862e-06
Iter: 1612 loss: 2.77730078e-06
Iter: 1613 loss: 2.77610798e-06
Iter: 1614 loss: 2.77676645e-06
Iter: 1615 loss: 2.77542904e-06
Iter: 1616 loss: 2.77387085e-06
Iter: 1617 loss: 2.77398863e-06
Iter: 1618 loss: 2.77270465e-06
Iter: 1619 loss: 2.77048366e-06
Iter: 1620 loss: 2.77512845e-06
Iter: 1621 loss: 2.76949822e-06
Iter: 1622 loss: 2.76787068e-06
Iter: 1623 loss: 2.77029358e-06
Iter: 1624 loss: 2.76688434e-06
Iter: 1625 loss: 2.76485116e-06
Iter: 1626 loss: 2.77409981e-06
Iter: 1627 loss: 2.76434662e-06
Iter: 1628 loss: 2.76268815e-06
Iter: 1629 loss: 2.76043147e-06
Iter: 1630 loss: 2.76018818e-06
Iter: 1631 loss: 2.76274568e-06
Iter: 1632 loss: 2.7595845e-06
Iter: 1633 loss: 2.75874527e-06
Iter: 1634 loss: 2.75751063e-06
Iter: 1635 loss: 2.75747016e-06
Iter: 1636 loss: 2.75611615e-06
Iter: 1637 loss: 2.754246e-06
Iter: 1638 loss: 2.75426964e-06
Iter: 1639 loss: 2.75232446e-06
Iter: 1640 loss: 2.76156743e-06
Iter: 1641 loss: 2.75204093e-06
Iter: 1642 loss: 2.75008415e-06
Iter: 1643 loss: 2.75248385e-06
Iter: 1644 loss: 2.74913532e-06
Iter: 1645 loss: 2.74760987e-06
Iter: 1646 loss: 2.7476076e-06
Iter: 1647 loss: 2.74676904e-06
Iter: 1648 loss: 2.74579497e-06
Iter: 1649 loss: 2.74570402e-06
Iter: 1650 loss: 2.74415379e-06
Iter: 1651 loss: 2.74634522e-06
Iter: 1652 loss: 2.74330773e-06
Iter: 1653 loss: 2.74161357e-06
Iter: 1654 loss: 2.75327307e-06
Iter: 1655 loss: 2.74137824e-06
Iter: 1656 loss: 2.74017657e-06
Iter: 1657 loss: 2.73828846e-06
Iter: 1658 loss: 2.73824435e-06
Iter: 1659 loss: 2.7360943e-06
Iter: 1660 loss: 2.76416313e-06
Iter: 1661 loss: 2.73608384e-06
Iter: 1662 loss: 2.73484329e-06
Iter: 1663 loss: 2.73485057e-06
Iter: 1664 loss: 2.73373462e-06
Iter: 1665 loss: 2.73488649e-06
Iter: 1666 loss: 2.73304386e-06
Iter: 1667 loss: 2.7325e-06
Iter: 1668 loss: 2.73101477e-06
Iter: 1669 loss: 2.73886735e-06
Iter: 1670 loss: 2.73037858e-06
Iter: 1671 loss: 2.72892248e-06
Iter: 1672 loss: 2.73098294e-06
Iter: 1673 loss: 2.7281983e-06
Iter: 1674 loss: 2.72594457e-06
Iter: 1675 loss: 2.74012723e-06
Iter: 1676 loss: 2.72575835e-06
Iter: 1677 loss: 2.72432362e-06
Iter: 1678 loss: 2.74035574e-06
Iter: 1679 loss: 2.72436228e-06
Iter: 1680 loss: 2.7229994e-06
Iter: 1681 loss: 2.72350644e-06
Iter: 1682 loss: 2.72218199e-06
Iter: 1683 loss: 2.72074658e-06
Iter: 1684 loss: 2.72189959e-06
Iter: 1685 loss: 2.71981457e-06
Iter: 1686 loss: 2.7184542e-06
Iter: 1687 loss: 2.72547732e-06
Iter: 1688 loss: 2.71817044e-06
Iter: 1689 loss: 2.71682097e-06
Iter: 1690 loss: 2.71823e-06
Iter: 1691 loss: 2.71614567e-06
Iter: 1692 loss: 2.71453155e-06
Iter: 1693 loss: 2.71334829e-06
Iter: 1694 loss: 2.71279509e-06
Iter: 1695 loss: 2.71047702e-06
Iter: 1696 loss: 2.73481828e-06
Iter: 1697 loss: 2.71035242e-06
Iter: 1698 loss: 2.70905e-06
Iter: 1699 loss: 2.71826093e-06
Iter: 1700 loss: 2.70891019e-06
Iter: 1701 loss: 2.70780629e-06
Iter: 1702 loss: 2.72568036e-06
Iter: 1703 loss: 2.7078213e-06
Iter: 1704 loss: 2.70734699e-06
Iter: 1705 loss: 2.70577175e-06
Iter: 1706 loss: 2.70932469e-06
Iter: 1707 loss: 2.70489045e-06
Iter: 1708 loss: 2.70343276e-06
Iter: 1709 loss: 2.7068204e-06
Iter: 1710 loss: 2.70294981e-06
Iter: 1711 loss: 2.70122155e-06
Iter: 1712 loss: 2.70385453e-06
Iter: 1713 loss: 2.7002061e-06
Iter: 1714 loss: 2.69954e-06
Iter: 1715 loss: 2.69931434e-06
Iter: 1716 loss: 2.69843554e-06
Iter: 1717 loss: 2.69854513e-06
Iter: 1718 loss: 2.69769885e-06
Iter: 1719 loss: 2.69653606e-06
Iter: 1720 loss: 2.69942984e-06
Iter: 1721 loss: 2.69605198e-06
Iter: 1722 loss: 2.6948228e-06
Iter: 1723 loss: 2.69792486e-06
Iter: 1724 loss: 2.69451129e-06
Iter: 1725 loss: 2.69337283e-06
Iter: 1726 loss: 2.69616021e-06
Iter: 1727 loss: 2.69301563e-06
Iter: 1728 loss: 2.69203042e-06
Iter: 1729 loss: 2.69335442e-06
Iter: 1730 loss: 2.69149223e-06
Iter: 1731 loss: 2.6902942e-06
Iter: 1732 loss: 2.6907453e-06
Iter: 1733 loss: 2.68955455e-06
Iter: 1734 loss: 2.69044494e-06
Iter: 1735 loss: 2.68899043e-06
Iter: 1736 loss: 2.68854046e-06
Iter: 1737 loss: 2.68792837e-06
Iter: 1738 loss: 2.70302417e-06
Iter: 1739 loss: 2.68799067e-06
Iter: 1740 loss: 2.68738358e-06
Iter: 1741 loss: 2.68606618e-06
Iter: 1742 loss: 2.70104692e-06
Iter: 1743 loss: 2.68601138e-06
Iter: 1744 loss: 2.68479516e-06
Iter: 1745 loss: 2.6894013e-06
Iter: 1746 loss: 2.68451322e-06
Iter: 1747 loss: 2.68342228e-06
Iter: 1748 loss: 2.68180656e-06
Iter: 1749 loss: 2.68174381e-06
Iter: 1750 loss: 2.68007648e-06
Iter: 1751 loss: 2.69500288e-06
Iter: 1752 loss: 2.67992505e-06
Iter: 1753 loss: 2.67910286e-06
Iter: 1754 loss: 2.68876556e-06
Iter: 1755 loss: 2.67904284e-06
Iter: 1756 loss: 2.67814e-06
Iter: 1757 loss: 2.67679775e-06
Iter: 1758 loss: 2.67671544e-06
Iter: 1759 loss: 2.67531959e-06
Iter: 1760 loss: 2.68309645e-06
Iter: 1761 loss: 2.67515111e-06
Iter: 1762 loss: 2.67421547e-06
Iter: 1763 loss: 2.67391761e-06
Iter: 1764 loss: 2.67332507e-06
Iter: 1765 loss: 2.67161022e-06
Iter: 1766 loss: 2.67841347e-06
Iter: 1767 loss: 2.67128075e-06
Iter: 1768 loss: 2.67063706e-06
Iter: 1769 loss: 2.67057862e-06
Iter: 1770 loss: 2.66979146e-06
Iter: 1771 loss: 2.67344808e-06
Iter: 1772 loss: 2.66971983e-06
Iter: 1773 loss: 2.66916959e-06
Iter: 1774 loss: 2.6680591e-06
Iter: 1775 loss: 2.6789503e-06
Iter: 1776 loss: 2.66794223e-06
Iter: 1777 loss: 2.66685151e-06
Iter: 1778 loss: 2.66567213e-06
Iter: 1779 loss: 2.66549932e-06
Iter: 1780 loss: 2.66369807e-06
Iter: 1781 loss: 2.67962741e-06
Iter: 1782 loss: 2.66358916e-06
Iter: 1783 loss: 2.66228767e-06
Iter: 1784 loss: 2.66406096e-06
Iter: 1785 loss: 2.66158054e-06
Iter: 1786 loss: 2.66010238e-06
Iter: 1787 loss: 2.67111136e-06
Iter: 1788 loss: 2.66003872e-06
Iter: 1789 loss: 2.6589787e-06
Iter: 1790 loss: 2.66806728e-06
Iter: 1791 loss: 2.65889548e-06
Iter: 1792 loss: 2.65814879e-06
Iter: 1793 loss: 2.65695667e-06
Iter: 1794 loss: 2.65695326e-06
Iter: 1795 loss: 2.65535868e-06
Iter: 1796 loss: 2.65751623e-06
Iter: 1797 loss: 2.65453491e-06
Iter: 1798 loss: 2.65318363e-06
Iter: 1799 loss: 2.65582366e-06
Iter: 1800 loss: 2.65251538e-06
Iter: 1801 loss: 2.65102858e-06
Iter: 1802 loss: 2.66300162e-06
Iter: 1803 loss: 2.65087556e-06
Iter: 1804 loss: 2.65100198e-06
Iter: 1805 loss: 2.65044605e-06
Iter: 1806 loss: 2.65002109e-06
Iter: 1807 loss: 2.64910591e-06
Iter: 1808 loss: 2.66943789e-06
Iter: 1809 loss: 2.64909022e-06
Iter: 1810 loss: 2.64816e-06
Iter: 1811 loss: 2.64662867e-06
Iter: 1812 loss: 2.6466023e-06
Iter: 1813 loss: 2.64514347e-06
Iter: 1814 loss: 2.65054314e-06
Iter: 1815 loss: 2.64480786e-06
Iter: 1816 loss: 2.64328537e-06
Iter: 1817 loss: 2.64264895e-06
Iter: 1818 loss: 2.6418486e-06
Iter: 1819 loss: 2.64031473e-06
Iter: 1820 loss: 2.65446988e-06
Iter: 1821 loss: 2.64026767e-06
Iter: 1822 loss: 2.63881884e-06
Iter: 1823 loss: 2.64329969e-06
Iter: 1824 loss: 2.63846414e-06
Iter: 1825 loss: 2.63704783e-06
Iter: 1826 loss: 2.64779692e-06
Iter: 1827 loss: 2.6369371e-06
Iter: 1828 loss: 2.63594711e-06
Iter: 1829 loss: 2.63717038e-06
Iter: 1830 loss: 2.63551101e-06
Iter: 1831 loss: 2.63444463e-06
Iter: 1832 loss: 2.63266656e-06
Iter: 1833 loss: 2.67456517e-06
Iter: 1834 loss: 2.6325913e-06
Iter: 1835 loss: 2.63106949e-06
Iter: 1836 loss: 2.64731034e-06
Iter: 1837 loss: 2.63101242e-06
Iter: 1838 loss: 2.6295761e-06
Iter: 1839 loss: 2.63031416e-06
Iter: 1840 loss: 2.62865842e-06
Iter: 1841 loss: 2.62998333e-06
Iter: 1842 loss: 2.62801905e-06
Iter: 1843 loss: 2.62749927e-06
Iter: 1844 loss: 2.62651065e-06
Iter: 1845 loss: 2.64727123e-06
Iter: 1846 loss: 2.62647495e-06
Iter: 1847 loss: 2.62530739e-06
Iter: 1848 loss: 2.62563503e-06
Iter: 1849 loss: 2.62456183e-06
Iter: 1850 loss: 2.62309777e-06
Iter: 1851 loss: 2.62637559e-06
Iter: 1852 loss: 2.62259618e-06
Iter: 1853 loss: 2.62146159e-06
Iter: 1854 loss: 2.6213711e-06
Iter: 1855 loss: 2.62036019e-06
Iter: 1856 loss: 2.61878563e-06
Iter: 1857 loss: 2.62632511e-06
Iter: 1858 loss: 2.61843661e-06
Iter: 1859 loss: 2.6176715e-06
Iter: 1860 loss: 2.61754599e-06
Iter: 1861 loss: 2.61665218e-06
Iter: 1862 loss: 2.61607511e-06
Iter: 1863 loss: 2.61585683e-06
Iter: 1864 loss: 2.61477294e-06
Iter: 1865 loss: 2.61801642e-06
Iter: 1866 loss: 2.61445552e-06
Iter: 1867 loss: 2.61360265e-06
Iter: 1868 loss: 2.61498326e-06
Iter: 1869 loss: 2.61322884e-06
Iter: 1870 loss: 2.6120515e-06
Iter: 1871 loss: 2.61193236e-06
Iter: 1872 loss: 2.61098785e-06
Iter: 1873 loss: 2.60966408e-06
Iter: 1874 loss: 2.61328864e-06
Iter: 1875 loss: 2.60932848e-06
Iter: 1876 loss: 2.60843535e-06
Iter: 1877 loss: 2.60832712e-06
Iter: 1878 loss: 2.60795559e-06
Iter: 1879 loss: 2.60720344e-06
Iter: 1880 loss: 2.61202695e-06
Iter: 1881 loss: 2.60697584e-06
Iter: 1882 loss: 2.60614979e-06
Iter: 1883 loss: 2.60848356e-06
Iter: 1884 loss: 2.60576167e-06
Iter: 1885 loss: 2.60470347e-06
Iter: 1886 loss: 2.60693741e-06
Iter: 1887 loss: 2.60422166e-06
Iter: 1888 loss: 2.60329534e-06
Iter: 1889 loss: 2.60253296e-06
Iter: 1890 loss: 2.60239085e-06
Iter: 1891 loss: 2.6008106e-06
Iter: 1892 loss: 2.61347986e-06
Iter: 1893 loss: 2.60070715e-06
Iter: 1894 loss: 2.59947774e-06
Iter: 1895 loss: 2.61258629e-06
Iter: 1896 loss: 2.59948183e-06
Iter: 1897 loss: 2.59880608e-06
Iter: 1898 loss: 2.59826311e-06
Iter: 1899 loss: 2.59818898e-06
Iter: 1900 loss: 2.59687886e-06
Iter: 1901 loss: 2.59569561e-06
Iter: 1902 loss: 2.59544754e-06
Iter: 1903 loss: 2.59441572e-06
Iter: 1904 loss: 2.59434819e-06
Iter: 1905 loss: 2.59340231e-06
Iter: 1906 loss: 2.59248236e-06
Iter: 1907 loss: 2.59234844e-06
Iter: 1908 loss: 2.59125954e-06
Iter: 1909 loss: 2.60614479e-06
Iter: 1910 loss: 2.59125682e-06
Iter: 1911 loss: 2.59034209e-06
Iter: 1912 loss: 2.60344154e-06
Iter: 1913 loss: 2.59039916e-06
Iter: 1914 loss: 2.59006288e-06
Iter: 1915 loss: 2.58886166e-06
Iter: 1916 loss: 2.59060107e-06
Iter: 1917 loss: 2.58798013e-06
Iter: 1918 loss: 2.58683804e-06
Iter: 1919 loss: 2.60076376e-06
Iter: 1920 loss: 2.58683781e-06
Iter: 1921 loss: 2.58576029e-06
Iter: 1922 loss: 2.58615e-06
Iter: 1923 loss: 2.58497744e-06
Iter: 1924 loss: 2.58358614e-06
Iter: 1925 loss: 2.58694854e-06
Iter: 1926 loss: 2.58303362e-06
Iter: 1927 loss: 2.58189493e-06
Iter: 1928 loss: 2.58104e-06
Iter: 1929 loss: 2.58068144e-06
Iter: 1930 loss: 2.57978854e-06
Iter: 1931 loss: 2.57964484e-06
Iter: 1932 loss: 2.57846659e-06
Iter: 1933 loss: 2.5808838e-06
Iter: 1934 loss: 2.57803822e-06
Iter: 1935 loss: 2.57711054e-06
Iter: 1936 loss: 2.57587476e-06
Iter: 1937 loss: 2.57586453e-06
Iter: 1938 loss: 2.57430497e-06
Iter: 1939 loss: 2.57902548e-06
Iter: 1940 loss: 2.57394504e-06
Iter: 1941 loss: 2.57247029e-06
Iter: 1942 loss: 2.57708734e-06
Iter: 1943 loss: 2.57195e-06
Iter: 1944 loss: 2.57343982e-06
Iter: 1945 loss: 2.57152897e-06
Iter: 1946 loss: 2.57118563e-06
Iter: 1947 loss: 2.57016882e-06
Iter: 1948 loss: 2.57645297e-06
Iter: 1949 loss: 2.56994804e-06
Iter: 1950 loss: 2.56898397e-06
Iter: 1951 loss: 2.56817111e-06
Iter: 1952 loss: 2.56789963e-06
Iter: 1953 loss: 2.5667141e-06
Iter: 1954 loss: 2.58071691e-06
Iter: 1955 loss: 2.56669432e-06
Iter: 1956 loss: 2.5654781e-06
Iter: 1957 loss: 2.56497287e-06
Iter: 1958 loss: 2.56443491e-06
Iter: 1959 loss: 2.56299495e-06
Iter: 1960 loss: 2.57204147e-06
Iter: 1961 loss: 2.56288081e-06
Iter: 1962 loss: 2.56179101e-06
Iter: 1963 loss: 2.56237672e-06
Iter: 1964 loss: 2.56113481e-06
Iter: 1965 loss: 2.55932446e-06
Iter: 1966 loss: 2.56628778e-06
Iter: 1967 loss: 2.55899522e-06
Iter: 1968 loss: 2.55814302e-06
Iter: 1969 loss: 2.56419435e-06
Iter: 1970 loss: 2.55805344e-06
Iter: 1971 loss: 2.55722921e-06
Iter: 1972 loss: 2.56185695e-06
Iter: 1973 loss: 2.55706937e-06
Iter: 1974 loss: 2.55635746e-06
Iter: 1975 loss: 2.55492114e-06
Iter: 1976 loss: 2.58695945e-06
Iter: 1977 loss: 2.55490909e-06
Iter: 1978 loss: 2.55387886e-06
Iter: 1979 loss: 2.5573238e-06
Iter: 1980 loss: 2.55361829e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.8
+ date
Wed Oct 21 15:46:49 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/300_300_300_1 --function f1 --psi -2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cfe9d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8d065488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8d0548c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cfaf730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cfaf620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cf98c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ced1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cf457b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cf2d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cdaa510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ce68400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ce7e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ce7eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cde0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cde0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cde0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b8e7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b8c3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b8eb158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b80eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b83b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b83b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ce18840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ce51598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ce51510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44714ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44684730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d446c1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d446c18c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b7ce7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d445f98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44681158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44646620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44652488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44610840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44558730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.101940334
test_loss: 0.09406038
train_loss: 0.06699291
test_loss: 0.06713697
train_loss: 0.039990783
test_loss: 0.044179928
train_loss: 0.029798746
test_loss: 0.031307947
train_loss: 0.026003132
test_loss: 0.027690325
train_loss: 0.02056898
test_loss: 0.023340937
train_loss: 0.019786207
test_loss: 0.023322588
train_loss: 0.018373638
test_loss: 0.020428736
train_loss: 0.016311305
test_loss: 0.018775418
train_loss: 0.014862297
test_loss: 0.017291034
train_loss: 0.015425305
test_loss: 0.017462777
train_loss: 0.015332253
test_loss: 0.016851079
train_loss: 0.015965234
test_loss: 0.017112073
train_loss: 0.013723678
test_loss: 0.015952686
train_loss: 0.0138883535
test_loss: 0.016257724
train_loss: 0.012701608
test_loss: 0.015339072
train_loss: 0.013845034
test_loss: 0.0150267985
train_loss: 0.012176029
test_loss: 0.014810327
train_loss: 0.012058962
test_loss: 0.014584965
train_loss: 0.012376024
test_loss: 0.01425511
train_loss: 0.013054087
test_loss: 0.014078218
train_loss: 0.012158741
test_loss: 0.013824467
train_loss: 0.011234304
test_loss: 0.013592578
train_loss: 0.011233607
test_loss: 0.013557482
train_loss: 0.011102913
test_loss: 0.013096092
train_loss: 0.011164635
test_loss: 0.0130320545
train_loss: 0.010633839
test_loss: 0.013369295
train_loss: 0.0106830085
test_loss: 0.01334831
train_loss: 0.009895794
test_loss: 0.01270021
train_loss: 0.010662867
test_loss: 0.013249732
train_loss: 0.009791948
test_loss: 0.0126400795
train_loss: 0.009646252
test_loss: 0.012177275
train_loss: 0.01065975
test_loss: 0.012624376
train_loss: 0.009871649
test_loss: 0.01301721
train_loss: 0.010127351
test_loss: 0.012300397
train_loss: 0.010592316
test_loss: 0.012245706
train_loss: 0.010358607
test_loss: 0.012337129
train_loss: 0.010108441
test_loss: 0.012144786
train_loss: 0.011254587
test_loss: 0.012436244
train_loss: 0.010424375
test_loss: 0.01231458
train_loss: 0.008953281
test_loss: 0.012401982
train_loss: 0.009552434
test_loss: 0.012348376
train_loss: 0.009804811
test_loss: 0.01265115
train_loss: 0.0093177315
test_loss: 0.011788763
train_loss: 0.009609973
test_loss: 0.01213573
train_loss: 0.009797441
test_loss: 0.012169466
train_loss: 0.009685341
test_loss: 0.012153394
train_loss: 0.009316371
test_loss: 0.012034784
train_loss: 0.008592932
test_loss: 0.012247793
train_loss: 0.009123341
test_loss: 0.011585658
train_loss: 0.009087247
test_loss: 0.011650029
train_loss: 0.008410418
test_loss: 0.012453419
train_loss: 0.009514812
test_loss: 0.011516352
train_loss: 0.009831285
test_loss: 0.011702307
train_loss: 0.008676462
test_loss: 0.01128046
train_loss: 0.008932266
test_loss: 0.011793871
train_loss: 0.009231817
test_loss: 0.011281791
train_loss: 0.009018764
test_loss: 0.012048696
train_loss: 0.00871111
test_loss: 0.011250249
train_loss: 0.009143414
test_loss: 0.01167229
train_loss: 0.008725926
test_loss: 0.011173091
train_loss: 0.00899685
test_loss: 0.011390184
train_loss: 0.009006907
test_loss: 0.012220733
train_loss: 0.00833286
test_loss: 0.011675815
train_loss: 0.009130987
test_loss: 0.012486127
train_loss: 0.008728342
test_loss: 0.011085228
train_loss: 0.009339256
test_loss: 0.0107401265
train_loss: 0.008943785
test_loss: 0.011094945
train_loss: 0.008455986
test_loss: 0.01056822
train_loss: 0.008288363
test_loss: 0.010934363
train_loss: 0.008882955
test_loss: 0.010889544
train_loss: 0.0077009695
test_loss: 0.010633734
train_loss: 0.008456395
test_loss: 0.010987772
train_loss: 0.008794616
test_loss: 0.010679627
train_loss: 0.008235987
test_loss: 0.01019491
train_loss: 0.008337935
test_loss: 0.010477391
train_loss: 0.0078118993
test_loss: 0.010093426
train_loss: 0.008313716
test_loss: 0.010185201
train_loss: 0.007616323
test_loss: 0.010466024
train_loss: 0.008647803
test_loss: 0.010412803
train_loss: 0.007862482
test_loss: 0.010170863
train_loss: 0.008116041
test_loss: 0.010699199
train_loss: 0.007569584
test_loss: 0.010444587
train_loss: 0.0075962865
test_loss: 0.010680532
train_loss: 0.007332377
test_loss: 0.010517921
train_loss: 0.008133754
test_loss: 0.010335879
train_loss: 0.007684555
test_loss: 0.009891362
train_loss: 0.008045642
test_loss: 0.010516614
train_loss: 0.007390458
test_loss: 0.010791573
train_loss: 0.008275092
test_loss: 0.010957189
train_loss: 0.008071478
test_loss: 0.010876506
train_loss: 0.007946684
test_loss: 0.010338484
train_loss: 0.0075164894
test_loss: 0.009600406
train_loss: 0.007489129
test_loss: 0.010352085
train_loss: 0.00872009
test_loss: 0.011342245
train_loss: 0.0078242235
test_loss: 0.0102718845
train_loss: 0.0067081293
test_loss: 0.010086193
train_loss: 0.0073970445
test_loss: 0.010342816
train_loss: 0.008003519
test_loss: 0.010690695
train_loss: 0.008030211
test_loss: 0.01055059
train_loss: 0.00872546
test_loss: 0.010771287
train_loss: 0.007258876
test_loss: 0.010043505
train_loss: 0.007198897
test_loss: 0.010067446
train_loss: 0.0077773416
test_loss: 0.01050147
train_loss: 0.0077863
test_loss: 0.010194529
train_loss: 0.0078100525
test_loss: 0.0104348175
train_loss: 0.0077229906
test_loss: 0.009954258
train_loss: 0.0069520744
test_loss: 0.009762345
train_loss: 0.0074198283
test_loss: 0.0096507985
train_loss: 0.0080428915
test_loss: 0.010544243
train_loss: 0.0076095704
test_loss: 0.0101400865
train_loss: 0.0075030383
test_loss: 0.010100421
train_loss: 0.007763434
test_loss: 0.010276282
train_loss: 0.0074439645
test_loss: 0.01026924
train_loss: 0.007136781
test_loss: 0.009915245
train_loss: 0.006917459
test_loss: 0.010180024
train_loss: 0.007365507
test_loss: 0.009771795
train_loss: 0.0072815507
test_loss: 0.010108751
train_loss: 0.0075753056
test_loss: 0.009840333
train_loss: 0.007768835
test_loss: 0.00979922
train_loss: 0.0072787446
test_loss: 0.009906357
train_loss: 0.0072201677
test_loss: 0.010032339
train_loss: 0.0076529216
test_loss: 0.010106063
train_loss: 0.007493941
test_loss: 0.010478448
train_loss: 0.007254348
test_loss: 0.010909882
train_loss: 0.0073475363
test_loss: 0.009715567
train_loss: 0.0069643697
test_loss: 0.009833313
train_loss: 0.00753288
test_loss: 0.010684609
train_loss: 0.007410055
test_loss: 0.010042909
train_loss: 0.0067669153
test_loss: 0.009586128
train_loss: 0.008272234
test_loss: 0.010258303
train_loss: 0.007028531
test_loss: 0.009903289
train_loss: 0.0075984932
test_loss: 0.009829041
train_loss: 0.0076056467
test_loss: 0.0100541245
train_loss: 0.007443222
test_loss: 0.00965517
train_loss: 0.0067486567
test_loss: 0.009957508
train_loss: 0.00768905
test_loss: 0.009672885
train_loss: 0.006964313
test_loss: 0.009585274
train_loss: 0.0068865083
test_loss: 0.0097935395
train_loss: 0.0069351126
test_loss: 0.009499936
train_loss: 0.0065349503
test_loss: 0.009588798
train_loss: 0.0073200488
test_loss: 0.010096139
train_loss: 0.006776858
test_loss: 0.009670371
train_loss: 0.007868008
test_loss: 0.0101951845
train_loss: 0.0073569175
test_loss: 0.009316683
train_loss: 0.0068277754
test_loss: 0.009216818
train_loss: 0.0066062203
test_loss: 0.009824175
train_loss: 0.006525171
test_loss: 0.00962179
train_loss: 0.0071072965
test_loss: 0.009332378
train_loss: 0.007939655
test_loss: 0.009912159
train_loss: 0.007376791
test_loss: 0.010722447
train_loss: 0.007601816
test_loss: 0.010083927
train_loss: 0.0073688775
test_loss: 0.009606328
train_loss: 0.0074292263
test_loss: 0.009869827
train_loss: 0.007723804
test_loss: 0.010082073
train_loss: 0.0066383113
test_loss: 0.009726578
train_loss: 0.006526801
test_loss: 0.00949954
train_loss: 0.0068208496
test_loss: 0.009427342
train_loss: 0.0071033956
test_loss: 0.009353057
train_loss: 0.006941296
test_loss: 0.009521664
train_loss: 0.006906598
test_loss: 0.009531118
train_loss: 0.0063717742
test_loss: 0.009311951
train_loss: 0.0075080674
test_loss: 0.010385305
train_loss: 0.0075852117
test_loss: 0.009451841
train_loss: 0.006989811
test_loss: 0.009354857
train_loss: 0.0073247543
test_loss: 0.009715119
train_loss: 0.006187454
test_loss: 0.009013365
train_loss: 0.006865436
test_loss: 0.009412945
train_loss: 0.006995837
test_loss: 0.009465992
train_loss: 0.0068046018
test_loss: 0.009452257
train_loss: 0.0062152212
test_loss: 0.009230156
train_loss: 0.0069399336
test_loss: 0.009332431
train_loss: 0.0065894537
test_loss: 0.0093528
train_loss: 0.0076062903
test_loss: 0.009718449
train_loss: 0.00693364
test_loss: 0.009717692
train_loss: 0.0071574245
test_loss: 0.009624041
train_loss: 0.0069578225
test_loss: 0.009137492
train_loss: 0.006667388
test_loss: 0.009550767
train_loss: 0.0071409116
test_loss: 0.009457457
train_loss: 0.0067136306
test_loss: 0.009773048
train_loss: 0.006790203
test_loss: 0.009586851
train_loss: 0.0068135206
test_loss: 0.009416529
train_loss: 0.0076360805
test_loss: 0.010104328
train_loss: 0.006346128
test_loss: 0.009008385
train_loss: 0.0073038572
test_loss: 0.009211737
train_loss: 0.006248554
test_loss: 0.009295114
train_loss: 0.0061353324
test_loss: 0.009151025
train_loss: 0.0061744917
test_loss: 0.008851836
train_loss: 0.006616486
test_loss: 0.009117237
train_loss: 0.0067437347
test_loss: 0.009270993
train_loss: 0.0066422457
test_loss: 0.009239948
train_loss: 0.0073534218
test_loss: 0.009347198
train_loss: 0.0066110063
test_loss: 0.009132133
train_loss: 0.006642648
test_loss: 0.00903149
train_loss: 0.0059692347
test_loss: 0.009200397
train_loss: 0.0071738465
test_loss: 0.009488222
train_loss: 0.006680232
test_loss: 0.00924143
train_loss: 0.0062253363
test_loss: 0.009358086
train_loss: 0.0061352476
test_loss: 0.009452624
train_loss: 0.006521652
test_loss: 0.009156775
train_loss: 0.0072045317
test_loss: 0.009321129
train_loss: 0.0060488973
test_loss: 0.008945506
train_loss: 0.0069785924
test_loss: 0.010009489
train_loss: 0.0065380246
test_loss: 0.009298287
train_loss: 0.006986236
test_loss: 0.009180611
train_loss: 0.0067532007
test_loss: 0.009157416
train_loss: 0.006132186
test_loss: 0.0090804715
train_loss: 0.007133308
test_loss: 0.0092614265
train_loss: 0.0061164224
test_loss: 0.008785844
train_loss: 0.00576713
test_loss: 0.008652684
train_loss: 0.0062067225
test_loss: 0.009095356
train_loss: 0.0060169734
test_loss: 0.009011681
train_loss: 0.0060570184
test_loss: 0.008979242
train_loss: 0.006132254
test_loss: 0.00870491
train_loss: 0.0061386046
test_loss: 0.008742355
train_loss: 0.0065747304
test_loss: 0.009045409
train_loss: 0.0060335966
test_loss: 0.00928543
train_loss: 0.0066404776
test_loss: 0.009368706
train_loss: 0.006090119
test_loss: 0.009476289
train_loss: 0.006804442
test_loss: 0.0093475
train_loss: 0.0068926727
test_loss: 0.0091284495
train_loss: 0.006092257
test_loss: 0.009089554
train_loss: 0.0063499734
test_loss: 0.009085131
train_loss: 0.0063411724
test_loss: 0.0090730265
train_loss: 0.0069418773
test_loss: 0.00949561
train_loss: 0.006574486
test_loss: 0.008927081
train_loss: 0.0059778313
test_loss: 0.009314754
train_loss: 0.006787768
test_loss: 0.009027408
train_loss: 0.0064293374
test_loss: 0.00915431
train_loss: 0.0062668016
test_loss: 0.008865486
train_loss: 0.006489163
test_loss: 0.008766531
train_loss: 0.006245673
test_loss: 0.008955217
train_loss: 0.006740639
test_loss: 0.008719095
train_loss: 0.0057463
test_loss: 0.00889993
train_loss: 0.0061963573
test_loss: 0.008936504
train_loss: 0.0056756604
test_loss: 0.00905138
train_loss: 0.0064443457
test_loss: 0.008706506
train_loss: 0.006155492
test_loss: 0.008702598
train_loss: 0.00652685
test_loss: 0.008971869
train_loss: 0.00598345
test_loss: 0.009152673
train_loss: 0.006286473
test_loss: 0.009070307
train_loss: 0.0070054377
test_loss: 0.00898759
train_loss: 0.007188137
test_loss: 0.008952663
train_loss: 0.0060333437
test_loss: 0.009184958
train_loss: 0.0068556718
test_loss: 0.0094704665
train_loss: 0.00625059
test_loss: 0.009226853
train_loss: 0.006432168
test_loss: 0.009510407
train_loss: 0.006530828
test_loss: 0.009547519
train_loss: 0.006183966
test_loss: 0.008779404
train_loss: 0.0061249533
test_loss: 0.0089642145
train_loss: 0.006563969
test_loss: 0.008803482
train_loss: 0.0074701873
test_loss: 0.009620454
train_loss: 0.0062605115
test_loss: 0.0095687695
train_loss: 0.0061356844
test_loss: 0.009188505
train_loss: 0.0062919995
test_loss: 0.008707098
train_loss: 0.006313711
test_loss: 0.008796233
train_loss: 0.006124203
test_loss: 0.008760026
train_loss: 0.0057743047
test_loss: 0.008613997
train_loss: 0.0060550496
test_loss: 0.008893723
train_loss: 0.005911154
test_loss: 0.008990231
train_loss: 0.0065676505
test_loss: 0.00878989
train_loss: 0.005506973
test_loss: 0.008546701
train_loss: 0.0064740274
test_loss: 0.008824478
train_loss: 0.0060414416
test_loss: 0.008805513
train_loss: 0.006349353
test_loss: 0.008872683
train_loss: 0.0057936837
test_loss: 0.00850477
train_loss: 0.006136001
test_loss: 0.008553796
train_loss: 0.0059994077
test_loss: 0.008813563
train_loss: 0.006450681
test_loss: 0.009058378
train_loss: 0.0067142365
test_loss: 0.009023553
train_loss: 0.006307355
test_loss: 0.008887343
train_loss: 0.006248484
test_loss: 0.008690629
train_loss: 0.006162678
test_loss: 0.008659625
train_loss: 0.0059408434
test_loss: 0.008778656
train_loss: 0.006369517
test_loss: 0.009160182
train_loss: 0.005687443
test_loss: 0.008631754
train_loss: 0.0059553464
test_loss: 0.0089195175
train_loss: 0.0059704534
test_loss: 0.008695089
train_loss: 0.005820697
test_loss: 0.008553374
train_loss: 0.006272242
test_loss: 0.008674969
train_loss: 0.006498786
test_loss: 0.008853063
train_loss: 0.0063316296
test_loss: 0.009219066
train_loss: 0.005744132
test_loss: 0.008626024
train_loss: 0.006024059
test_loss: 0.009006683
train_loss: 0.0062342873
test_loss: 0.009261283
train_loss: 0.006217234
test_loss: 0.008343696
train_loss: 0.0070811547
test_loss: 0.0095900465
train_loss: 0.006196564
test_loss: 0.008831095
train_loss: 0.006598993
test_loss: 0.008948585
train_loss: 0.006305485
test_loss: 0.008792909
train_loss: 0.005091831
test_loss: 0.008479866
train_loss: 0.0058731884
test_loss: 0.008968142
train_loss: 0.0063242824
test_loss: 0.008835023
train_loss: 0.0058654114
test_loss: 0.008681006
train_loss: 0.005617827
test_loss: 0.008782026
train_loss: 0.00554455
test_loss: 0.008869748
train_loss: 0.005819194
test_loss: 0.008781723
train_loss: 0.006348823
test_loss: 0.008579756
train_loss: 0.0058449423
test_loss: 0.008429204
train_loss: 0.0057693096
test_loss: 0.008760273
train_loss: 0.0054276157
test_loss: 0.008672379
train_loss: 0.0060678883
test_loss: 0.009056778
train_loss: 0.005907786
test_loss: 0.008729611
train_loss: 0.005396818
test_loss: 0.008568041
train_loss: 0.006002466
test_loss: 0.008807744
train_loss: 0.0059665353
test_loss: 0.008431489
train_loss: 0.006756477
test_loss: 0.008852644
train_loss: 0.0061772633
test_loss: 0.008433627
train_loss: 0.005878495
test_loss: 0.0090267
train_loss: 0.006118609
test_loss: 0.008668491
train_loss: 0.0060333484
test_loss: 0.008746898
train_loss: 0.0060617453
test_loss: 0.00888042
train_loss: 0.0065574613
test_loss: 0.0085716685
train_loss: 0.005998835
test_loss: 0.008508604
train_loss: 0.006864883
test_loss: 0.008469765
train_loss: 0.0064855907
test_loss: 0.0088581685
train_loss: 0.005228402
test_loss: 0.008431533
train_loss: 0.0065782648
test_loss: 0.009259668
train_loss: 0.0057519544
test_loss: 0.008474271
train_loss: 0.0059804204
test_loss: 0.0087042
train_loss: 0.005599035
test_loss: 0.008609863
train_loss: 0.0064710695
test_loss: 0.008447189
train_loss: 0.0052842824
test_loss: 0.008409438
train_loss: 0.005654199
test_loss: 0.00883739
train_loss: 0.0054535447
test_loss: 0.008255082
train_loss: 0.005480556
test_loss: 0.008444167
train_loss: 0.0055017713
test_loss: 0.008349704
train_loss: 0.0057279207
test_loss: 0.008748122
train_loss: 0.006224776
test_loss: 0.008918302
train_loss: 0.0057886983
test_loss: 0.008557964
train_loss: 0.0053062015
test_loss: 0.008523733
train_loss: 0.0058198385
test_loss: 0.00845663
train_loss: 0.006003574
test_loss: 0.008683331
train_loss: 0.006620031
test_loss: 0.008711707
train_loss: 0.0058307843
test_loss: 0.008229097
train_loss: 0.006025264
test_loss: 0.00832938
train_loss: 0.006343971
test_loss: 0.008704577
train_loss: 0.005669601
test_loss: 0.009250697
train_loss: 0.005784434
test_loss: 0.008640822
train_loss: 0.0059149396
test_loss: 0.008524572
train_loss: 0.006339283
test_loss: 0.008594095
train_loss: 0.0055780713
test_loss: 0.008230834
train_loss: 0.006046825
test_loss: 0.008173601
train_loss: 0.0054618116
test_loss: 0.0085590165
train_loss: 0.005408686
test_loss: 0.008552199
train_loss: 0.005281465
test_loss: 0.008173424
train_loss: 0.0057324125/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.008546352
train_loss: 0.0054514427
test_loss: 0.008320928
train_loss: 0.0052454425
test_loss: 0.008846264
train_loss: 0.0057261074
test_loss: 0.008467323
train_loss: 0.0062651895
test_loss: 0.008644069
train_loss: 0.0056824945
test_loss: 0.00852375
train_loss: 0.0052933805
test_loss: 0.008534162
train_loss: 0.0068738265
test_loss: 0.008890667
train_loss: 0.005905759
test_loss: 0.0090915
train_loss: 0.006177573
test_loss: 0.008603925
train_loss: 0.0063326433
test_loss: 0.008498497
train_loss: 0.006393709
test_loss: 0.0087312795
train_loss: 0.0062805987
test_loss: 0.008459111
train_loss: 0.0058162427
test_loss: 0.008570247
train_loss: 0.006070316
test_loss: 0.008830937
train_loss: 0.0065665976
test_loss: 0.009031929
train_loss: 0.005634154
test_loss: 0.008306411
train_loss: 0.005451601
test_loss: 0.008446029
train_loss: 0.006436924
test_loss: 0.008657725
train_loss: 0.006032289
test_loss: 0.008412699
train_loss: 0.006229967
test_loss: 0.008726615
train_loss: 0.0056123342
test_loss: 0.008730119
train_loss: 0.0056766314
test_loss: 0.008504941
train_loss: 0.0058987467
test_loss: 0.008570024
train_loss: 0.006112652
test_loss: 0.008817102
train_loss: 0.0052843583
test_loss: 0.008446054
train_loss: 0.0051360354
test_loss: 0.008264504
train_loss: 0.005631273
test_loss: 0.008360532
train_loss: 0.005612881
test_loss: 0.008709849
train_loss: 0.0057916823
test_loss: 0.008672105
train_loss: 0.006401022
test_loss: 0.008221868
train_loss: 0.005827829
test_loss: 0.008477629
train_loss: 0.0060884184
test_loss: 0.008184699
train_loss: 0.006091518
test_loss: 0.008616837
train_loss: 0.005597749
test_loss: 0.008395004
train_loss: 0.0056823995
test_loss: 0.008492075
train_loss: 0.005955821
test_loss: 0.008201677
train_loss: 0.0060473913
test_loss: 0.008842261
train_loss: 0.0056882245
test_loss: 0.008796307
train_loss: 0.0053017447
test_loss: 0.00842267
train_loss: 0.0063603255
test_loss: 0.008356867
train_loss: 0.0054061688
test_loss: 0.0083926385
train_loss: 0.005485276
test_loss: 0.008105549
train_loss: 0.0059360582
test_loss: 0.008316243
train_loss: 0.0052006207
test_loss: 0.00802707
train_loss: 0.0061047864
test_loss: 0.008364086
train_loss: 0.0053711473
test_loss: 0.008404155
train_loss: 0.006269027
test_loss: 0.008282357
train_loss: 0.0052144774
test_loss: 0.008417996
train_loss: 0.0055143777
test_loss: 0.008549158
train_loss: 0.0055110017
test_loss: 0.008361394
train_loss: 0.005536807
test_loss: 0.008553802
train_loss: 0.005580479
test_loss: 0.00817371
train_loss: 0.0055391975
test_loss: 0.008444349
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53e99620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53d5a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53d5aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53da3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53da3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53cf7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53cdf8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53cf7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53cf7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53c2c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53c15ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53c2c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53c0f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53c0f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53b749d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53b74840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53ba7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53b67840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53b14730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53b26f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53ac6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53adabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53aa6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd428df730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd428df620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd428a8d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd428a8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd42868730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd42868378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd42815378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd427e5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd42801158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd427a37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd427b6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd4274f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd427179d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.1758692e-05
Iter: 2 loss: 4.96110733e-05
Iter: 3 loss: 4.95999448e-05
Iter: 4 loss: 4.54168585e-05
Iter: 5 loss: 4.50496227e-05
Iter: 6 loss: 4.35034672e-05
Iter: 7 loss: 3.90978348e-05
Iter: 8 loss: 5.94242811e-05
Iter: 9 loss: 3.75196323e-05
Iter: 10 loss: 3.20529e-05
Iter: 11 loss: 7.62416457e-05
Iter: 12 loss: 3.16894548e-05
Iter: 13 loss: 2.89589207e-05
Iter: 14 loss: 3.04274909e-05
Iter: 15 loss: 2.71635017e-05
Iter: 16 loss: 2.36487722e-05
Iter: 17 loss: 3.97761833e-05
Iter: 18 loss: 2.29941161e-05
Iter: 19 loss: 2.12500308e-05
Iter: 20 loss: 3.31771043e-05
Iter: 21 loss: 2.10748258e-05
Iter: 22 loss: 1.97359514e-05
Iter: 23 loss: 2.10581602e-05
Iter: 24 loss: 1.89785242e-05
Iter: 25 loss: 1.76203012e-05
Iter: 26 loss: 2.40441295e-05
Iter: 27 loss: 1.73738135e-05
Iter: 28 loss: 1.64284938e-05
Iter: 29 loss: 2.02885421e-05
Iter: 30 loss: 1.62217293e-05
Iter: 31 loss: 1.55023299e-05
Iter: 32 loss: 1.68040569e-05
Iter: 33 loss: 1.51901459e-05
Iter: 34 loss: 1.43376137e-05
Iter: 35 loss: 1.66374921e-05
Iter: 36 loss: 1.40550565e-05
Iter: 37 loss: 1.35614628e-05
Iter: 38 loss: 1.84331439e-05
Iter: 39 loss: 1.35446962e-05
Iter: 40 loss: 1.33397498e-05
Iter: 41 loss: 1.32963078e-05
Iter: 42 loss: 1.3083768e-05
Iter: 43 loss: 1.25343322e-05
Iter: 44 loss: 1.70362327e-05
Iter: 45 loss: 1.24368325e-05
Iter: 46 loss: 1.20433178e-05
Iter: 47 loss: 1.17120653e-05
Iter: 48 loss: 1.16014944e-05
Iter: 49 loss: 1.10816263e-05
Iter: 50 loss: 1.10812234e-05
Iter: 51 loss: 1.08482946e-05
Iter: 52 loss: 1.07912429e-05
Iter: 53 loss: 1.06437892e-05
Iter: 54 loss: 1.02844751e-05
Iter: 55 loss: 1.34428883e-05
Iter: 56 loss: 1.02663171e-05
Iter: 57 loss: 1.00688203e-05
Iter: 58 loss: 1.10508681e-05
Iter: 59 loss: 1.00355328e-05
Iter: 60 loss: 9.85285624e-06
Iter: 61 loss: 9.54383813e-06
Iter: 62 loss: 9.54342795e-06
Iter: 63 loss: 9.28193913e-06
Iter: 64 loss: 1.19568567e-05
Iter: 65 loss: 9.27429664e-06
Iter: 66 loss: 9.04470926e-06
Iter: 67 loss: 9.55235191e-06
Iter: 68 loss: 8.95678477e-06
Iter: 69 loss: 8.80361222e-06
Iter: 70 loss: 1.02109625e-05
Iter: 71 loss: 8.79689469e-06
Iter: 72 loss: 8.67557537e-06
Iter: 73 loss: 9.07504182e-06
Iter: 74 loss: 8.64186222e-06
Iter: 75 loss: 8.5616357e-06
Iter: 76 loss: 8.55317194e-06
Iter: 77 loss: 8.50909601e-06
Iter: 78 loss: 8.37624248e-06
Iter: 79 loss: 8.77863567e-06
Iter: 80 loss: 8.30990757e-06
Iter: 81 loss: 8.12254621e-06
Iter: 82 loss: 9.2830187e-06
Iter: 83 loss: 8.10030724e-06
Iter: 84 loss: 7.99746e-06
Iter: 85 loss: 8.00118687e-06
Iter: 86 loss: 7.91594175e-06
Iter: 87 loss: 7.79131915e-06
Iter: 88 loss: 8.89927469e-06
Iter: 89 loss: 7.78514277e-06
Iter: 90 loss: 7.69100188e-06
Iter: 91 loss: 7.67539495e-06
Iter: 92 loss: 7.61067076e-06
Iter: 93 loss: 7.49766332e-06
Iter: 94 loss: 8.67483232e-06
Iter: 95 loss: 7.49477113e-06
Iter: 96 loss: 7.41540953e-06
Iter: 97 loss: 7.48336561e-06
Iter: 98 loss: 7.36877337e-06
Iter: 99 loss: 7.24797928e-06
Iter: 100 loss: 7.67417805e-06
Iter: 101 loss: 7.21665538e-06
Iter: 102 loss: 7.14994894e-06
Iter: 103 loss: 7.55683413e-06
Iter: 104 loss: 7.14176758e-06
Iter: 105 loss: 7.07788286e-06
Iter: 106 loss: 6.97351607e-06
Iter: 107 loss: 6.97317955e-06
Iter: 108 loss: 6.90545357e-06
Iter: 109 loss: 6.90470324e-06
Iter: 110 loss: 6.87789543e-06
Iter: 111 loss: 6.87308238e-06
Iter: 112 loss: 6.83854068e-06
Iter: 113 loss: 6.76024956e-06
Iter: 114 loss: 7.81809831e-06
Iter: 115 loss: 6.75559522e-06
Iter: 116 loss: 6.71178623e-06
Iter: 117 loss: 6.8553968e-06
Iter: 118 loss: 6.69958217e-06
Iter: 119 loss: 6.64877e-06
Iter: 120 loss: 6.62566e-06
Iter: 121 loss: 6.60004343e-06
Iter: 122 loss: 6.53599591e-06
Iter: 123 loss: 7.00758028e-06
Iter: 124 loss: 6.53066627e-06
Iter: 125 loss: 6.48068954e-06
Iter: 126 loss: 6.50515585e-06
Iter: 127 loss: 6.44724969e-06
Iter: 128 loss: 6.38921119e-06
Iter: 129 loss: 6.62147158e-06
Iter: 130 loss: 6.37617086e-06
Iter: 131 loss: 6.31750845e-06
Iter: 132 loss: 6.53233747e-06
Iter: 133 loss: 6.30283512e-06
Iter: 134 loss: 6.26101382e-06
Iter: 135 loss: 6.23616779e-06
Iter: 136 loss: 6.21883919e-06
Iter: 137 loss: 6.15674116e-06
Iter: 138 loss: 6.88332602e-06
Iter: 139 loss: 6.15586487e-06
Iter: 140 loss: 6.11363339e-06
Iter: 141 loss: 6.09131439e-06
Iter: 142 loss: 6.07217089e-06
Iter: 143 loss: 6.02916134e-06
Iter: 144 loss: 6.02874024e-06
Iter: 145 loss: 6.02708133e-06
Iter: 146 loss: 6.01510965e-06
Iter: 147 loss: 6.00701469e-06
Iter: 148 loss: 5.98075439e-06
Iter: 149 loss: 6.01192141e-06
Iter: 150 loss: 5.96078507e-06
Iter: 151 loss: 5.91206481e-06
Iter: 152 loss: 6.17698288e-06
Iter: 153 loss: 5.90511627e-06
Iter: 154 loss: 5.88007515e-06
Iter: 155 loss: 5.95377287e-06
Iter: 156 loss: 5.87236809e-06
Iter: 157 loss: 5.84053896e-06
Iter: 158 loss: 5.81040331e-06
Iter: 159 loss: 5.80310643e-06
Iter: 160 loss: 5.77211449e-06
Iter: 161 loss: 6.2056929e-06
Iter: 162 loss: 5.77200944e-06
Iter: 163 loss: 5.74530259e-06
Iter: 164 loss: 5.73783427e-06
Iter: 165 loss: 5.72178033e-06
Iter: 166 loss: 5.6844674e-06
Iter: 167 loss: 6.0012685e-06
Iter: 168 loss: 5.68243604e-06
Iter: 169 loss: 5.65913933e-06
Iter: 170 loss: 5.63307185e-06
Iter: 171 loss: 5.62960622e-06
Iter: 172 loss: 5.58562078e-06
Iter: 173 loss: 5.87988643e-06
Iter: 174 loss: 5.58133888e-06
Iter: 175 loss: 5.55140468e-06
Iter: 176 loss: 5.60310309e-06
Iter: 177 loss: 5.53820473e-06
Iter: 178 loss: 5.51006724e-06
Iter: 179 loss: 5.68808809e-06
Iter: 180 loss: 5.50690675e-06
Iter: 181 loss: 5.4970269e-06
Iter: 182 loss: 5.49380184e-06
Iter: 183 loss: 5.47978925e-06
Iter: 184 loss: 5.45956527e-06
Iter: 185 loss: 5.45902139e-06
Iter: 186 loss: 5.44762679e-06
Iter: 187 loss: 5.43244596e-06
Iter: 188 loss: 5.43155966e-06
Iter: 189 loss: 5.40426026e-06
Iter: 190 loss: 5.46005322e-06
Iter: 191 loss: 5.39342545e-06
Iter: 192 loss: 5.37254164e-06
Iter: 193 loss: 5.45688818e-06
Iter: 194 loss: 5.36807647e-06
Iter: 195 loss: 5.34056926e-06
Iter: 196 loss: 5.37526739e-06
Iter: 197 loss: 5.32638478e-06
Iter: 198 loss: 5.31000114e-06
Iter: 199 loss: 5.54562757e-06
Iter: 200 loss: 5.3099543e-06
Iter: 201 loss: 5.2970563e-06
Iter: 202 loss: 5.27072098e-06
Iter: 203 loss: 5.75834747e-06
Iter: 204 loss: 5.27038219e-06
Iter: 205 loss: 5.24851657e-06
Iter: 206 loss: 5.58830652e-06
Iter: 207 loss: 5.24852612e-06
Iter: 208 loss: 5.22881828e-06
Iter: 209 loss: 5.22206392e-06
Iter: 210 loss: 5.21080756e-06
Iter: 211 loss: 5.18483193e-06
Iter: 212 loss: 5.40174324e-06
Iter: 213 loss: 5.18324623e-06
Iter: 214 loss: 5.16747332e-06
Iter: 215 loss: 5.1628781e-06
Iter: 216 loss: 5.1532279e-06
Iter: 217 loss: 5.14299518e-06
Iter: 218 loss: 5.14015392e-06
Iter: 219 loss: 5.12837096e-06
Iter: 220 loss: 5.19804871e-06
Iter: 221 loss: 5.12689621e-06
Iter: 222 loss: 5.12204861e-06
Iter: 223 loss: 5.10661403e-06
Iter: 224 loss: 5.126426e-06
Iter: 225 loss: 5.09522533e-06
Iter: 226 loss: 5.0733961e-06
Iter: 227 loss: 5.30326133e-06
Iter: 228 loss: 5.0728413e-06
Iter: 229 loss: 5.05655044e-06
Iter: 230 loss: 5.09024858e-06
Iter: 231 loss: 5.04989839e-06
Iter: 232 loss: 5.03334104e-06
Iter: 233 loss: 5.07617096e-06
Iter: 234 loss: 5.02752664e-06
Iter: 235 loss: 5.01161276e-06
Iter: 236 loss: 5.05066419e-06
Iter: 237 loss: 5.00588703e-06
Iter: 238 loss: 4.99162161e-06
Iter: 239 loss: 5.14085923e-06
Iter: 240 loss: 4.99117141e-06
Iter: 241 loss: 4.98044847e-06
Iter: 242 loss: 4.97051542e-06
Iter: 243 loss: 4.96783287e-06
Iter: 244 loss: 4.95066615e-06
Iter: 245 loss: 5.07423738e-06
Iter: 246 loss: 4.94914e-06
Iter: 247 loss: 4.93519474e-06
Iter: 248 loss: 4.92303343e-06
Iter: 249 loss: 4.91944957e-06
Iter: 250 loss: 4.90090679e-06
Iter: 251 loss: 5.18983688e-06
Iter: 252 loss: 4.90097318e-06
Iter: 253 loss: 4.89695049e-06
Iter: 254 loss: 4.8955817e-06
Iter: 255 loss: 4.88974092e-06
Iter: 256 loss: 4.88092701e-06
Iter: 257 loss: 4.88073874e-06
Iter: 258 loss: 4.87028228e-06
Iter: 259 loss: 4.86204863e-06
Iter: 260 loss: 4.85881e-06
Iter: 261 loss: 4.84665225e-06
Iter: 262 loss: 4.83074382e-06
Iter: 263 loss: 4.82967971e-06
Iter: 264 loss: 4.82116e-06
Iter: 265 loss: 4.81803363e-06
Iter: 266 loss: 4.80880408e-06
Iter: 267 loss: 4.79005848e-06
Iter: 268 loss: 5.11435155e-06
Iter: 269 loss: 4.78962966e-06
Iter: 270 loss: 4.77704771e-06
Iter: 271 loss: 4.77613321e-06
Iter: 272 loss: 4.76627793e-06
Iter: 273 loss: 4.75729257e-06
Iter: 274 loss: 4.75506522e-06
Iter: 275 loss: 4.73922046e-06
Iter: 276 loss: 4.92618801e-06
Iter: 277 loss: 4.73906357e-06
Iter: 278 loss: 4.73125692e-06
Iter: 279 loss: 4.74686658e-06
Iter: 280 loss: 4.7280605e-06
Iter: 281 loss: 4.71871954e-06
Iter: 282 loss: 4.71117073e-06
Iter: 283 loss: 4.7083272e-06
Iter: 284 loss: 4.69348561e-06
Iter: 285 loss: 4.76156629e-06
Iter: 286 loss: 4.69089082e-06
Iter: 287 loss: 4.684543e-06
Iter: 288 loss: 4.68350572e-06
Iter: 289 loss: 4.67608061e-06
Iter: 290 loss: 4.69798e-06
Iter: 291 loss: 4.67381415e-06
Iter: 292 loss: 4.6691448e-06
Iter: 293 loss: 4.65813537e-06
Iter: 294 loss: 4.81192319e-06
Iter: 295 loss: 4.65762514e-06
Iter: 296 loss: 4.64579489e-06
Iter: 297 loss: 4.69161387e-06
Iter: 298 loss: 4.64321965e-06
Iter: 299 loss: 4.63340575e-06
Iter: 300 loss: 4.63244123e-06
Iter: 301 loss: 4.62515754e-06
Iter: 302 loss: 4.6093578e-06
Iter: 303 loss: 4.69448287e-06
Iter: 304 loss: 4.60699721e-06
Iter: 305 loss: 4.59745206e-06
Iter: 306 loss: 4.60519e-06
Iter: 307 loss: 4.59182729e-06
Iter: 308 loss: 4.57777332e-06
Iter: 309 loss: 4.63214838e-06
Iter: 310 loss: 4.57462511e-06
Iter: 311 loss: 4.56499856e-06
Iter: 312 loss: 4.60467936e-06
Iter: 313 loss: 4.56304133e-06
Iter: 314 loss: 4.55135296e-06
Iter: 315 loss: 4.56536054e-06
Iter: 316 loss: 4.54518295e-06
Iter: 317 loss: 4.53522898e-06
Iter: 318 loss: 4.60990486e-06
Iter: 319 loss: 4.53438088e-06
Iter: 320 loss: 4.5258721e-06
Iter: 321 loss: 4.51768028e-06
Iter: 322 loss: 4.5157758e-06
Iter: 323 loss: 4.51741653e-06
Iter: 324 loss: 4.51040023e-06
Iter: 325 loss: 4.50530615e-06
Iter: 326 loss: 4.51049527e-06
Iter: 327 loss: 4.50252537e-06
Iter: 328 loss: 4.49853542e-06
Iter: 329 loss: 4.4884664e-06
Iter: 330 loss: 4.55947975e-06
Iter: 331 loss: 4.48624041e-06
Iter: 332 loss: 4.47832736e-06
Iter: 333 loss: 4.47814364e-06
Iter: 334 loss: 4.47227421e-06
Iter: 335 loss: 4.45978367e-06
Iter: 336 loss: 4.67871587e-06
Iter: 337 loss: 4.45968635e-06
Iter: 338 loss: 4.44987199e-06
Iter: 339 loss: 4.44987245e-06
Iter: 340 loss: 4.44206125e-06
Iter: 341 loss: 4.43386762e-06
Iter: 342 loss: 4.43241788e-06
Iter: 343 loss: 4.42020928e-06
Iter: 344 loss: 4.49924573e-06
Iter: 345 loss: 4.41897782e-06
Iter: 346 loss: 4.40948816e-06
Iter: 347 loss: 4.44396255e-06
Iter: 348 loss: 4.40704025e-06
Iter: 349 loss: 4.39771793e-06
Iter: 350 loss: 4.39853829e-06
Iter: 351 loss: 4.39045652e-06
Iter: 352 loss: 4.38190818e-06
Iter: 353 loss: 4.38155166e-06
Iter: 354 loss: 4.37646895e-06
Iter: 355 loss: 4.37977224e-06
Iter: 356 loss: 4.37316294e-06
Iter: 357 loss: 4.36763321e-06
Iter: 358 loss: 4.36772098e-06
Iter: 359 loss: 4.36129631e-06
Iter: 360 loss: 4.35751554e-06
Iter: 361 loss: 4.35489e-06
Iter: 362 loss: 4.34999856e-06
Iter: 363 loss: 4.34547246e-06
Iter: 364 loss: 4.34432695e-06
Iter: 365 loss: 4.33540026e-06
Iter: 366 loss: 4.34323601e-06
Iter: 367 loss: 4.33005152e-06
Iter: 368 loss: 4.32249908e-06
Iter: 369 loss: 4.41159045e-06
Iter: 370 loss: 4.32246043e-06
Iter: 371 loss: 4.31591661e-06
Iter: 372 loss: 4.30589898e-06
Iter: 373 loss: 4.30572572e-06
Iter: 374 loss: 4.29891497e-06
Iter: 375 loss: 4.298563e-06
Iter: 376 loss: 4.29274633e-06
Iter: 377 loss: 4.28200292e-06
Iter: 378 loss: 4.52097083e-06
Iter: 379 loss: 4.28199428e-06
Iter: 380 loss: 4.27444957e-06
Iter: 381 loss: 4.27406667e-06
Iter: 382 loss: 4.26755e-06
Iter: 383 loss: 4.25720827e-06
Iter: 384 loss: 4.25699091e-06
Iter: 385 loss: 4.24981772e-06
Iter: 386 loss: 4.2496913e-06
Iter: 387 loss: 4.24295968e-06
Iter: 388 loss: 4.24245218e-06
Iter: 389 loss: 4.23760048e-06
Iter: 390 loss: 4.23127403e-06
Iter: 391 loss: 4.23134406e-06
Iter: 392 loss: 4.22817448e-06
Iter: 393 loss: 4.22819085e-06
Iter: 394 loss: 4.2245365e-06
Iter: 395 loss: 4.22008725e-06
Iter: 396 loss: 4.21963432e-06
Iter: 397 loss: 4.21486038e-06
Iter: 398 loss: 4.21085088e-06
Iter: 399 loss: 4.20955621e-06
Iter: 400 loss: 4.20296692e-06
Iter: 401 loss: 4.20684319e-06
Iter: 402 loss: 4.19861635e-06
Iter: 403 loss: 4.19063872e-06
Iter: 404 loss: 4.22933317e-06
Iter: 405 loss: 4.18934633e-06
Iter: 406 loss: 4.18149966e-06
Iter: 407 loss: 4.21131335e-06
Iter: 408 loss: 4.17962565e-06
Iter: 409 loss: 4.17339243e-06
Iter: 410 loss: 4.17465753e-06
Iter: 411 loss: 4.16873718e-06
Iter: 412 loss: 4.15891191e-06
Iter: 413 loss: 4.20797505e-06
Iter: 414 loss: 4.15729028e-06
Iter: 415 loss: 4.15135673e-06
Iter: 416 loss: 4.17075489e-06
Iter: 417 loss: 4.14979831e-06
Iter: 418 loss: 4.14243095e-06
Iter: 419 loss: 4.14438728e-06
Iter: 420 loss: 4.13710814e-06
Iter: 421 loss: 4.13066391e-06
Iter: 422 loss: 4.21274217e-06
Iter: 423 loss: 4.13066118e-06
Iter: 424 loss: 4.12531108e-06
Iter: 425 loss: 4.12484451e-06
Iter: 426 loss: 4.12085683e-06
Iter: 427 loss: 4.12039481e-06
Iter: 428 loss: 4.11698602e-06
Iter: 429 loss: 4.11504e-06
Iter: 430 loss: 4.11082419e-06
Iter: 431 loss: 4.17640513e-06
Iter: 432 loss: 4.11062501e-06
Iter: 433 loss: 4.10609346e-06
Iter: 434 loss: 4.10125904e-06
Iter: 435 loss: 4.10038047e-06
Iter: 436 loss: 4.09130553e-06
Iter: 437 loss: 4.15073646e-06
Iter: 438 loss: 4.0904356e-06
Iter: 439 loss: 4.08614687e-06
Iter: 440 loss: 4.08957658e-06
Iter: 441 loss: 4.08364758e-06
Iter: 442 loss: 4.07663265e-06
Iter: 443 loss: 4.0799132e-06
Iter: 444 loss: 4.07195057e-06
Iter: 445 loss: 4.06600702e-06
Iter: 446 loss: 4.08861433e-06
Iter: 447 loss: 4.06465733e-06
Iter: 448 loss: 4.05908577e-06
Iter: 449 loss: 4.08508822e-06
Iter: 450 loss: 4.05804121e-06
Iter: 451 loss: 4.0524792e-06
Iter: 452 loss: 4.05268747e-06
Iter: 453 loss: 4.0483028e-06
Iter: 454 loss: 4.04157845e-06
Iter: 455 loss: 4.10077519e-06
Iter: 456 loss: 4.0412724e-06
Iter: 457 loss: 4.03548893e-06
Iter: 458 loss: 4.04304774e-06
Iter: 459 loss: 4.03267586e-06
Iter: 460 loss: 4.03087461e-06
Iter: 461 loss: 4.02984e-06
Iter: 462 loss: 4.02679325e-06
Iter: 463 loss: 4.02774867e-06
Iter: 464 loss: 4.02465594e-06
Iter: 465 loss: 4.02171554e-06
Iter: 466 loss: 4.01529951e-06
Iter: 467 loss: 4.11221117e-06
Iter: 468 loss: 4.01499801e-06
Iter: 469 loss: 4.00900444e-06
Iter: 470 loss: 4.07644529e-06
Iter: 471 loss: 4.00879389e-06
Iter: 472 loss: 4.00467297e-06
Iter: 473 loss: 4.00328645e-06
Iter: 474 loss: 4.00094177e-06
Iter: 475 loss: 3.99380224e-06
Iter: 476 loss: 4.0176692e-06
Iter: 477 loss: 3.99179225e-06
Iter: 478 loss: 3.98679549e-06
Iter: 479 loss: 3.99992314e-06
Iter: 480 loss: 3.98529801e-06
Iter: 481 loss: 3.9790084e-06
Iter: 482 loss: 3.98140401e-06
Iter: 483 loss: 3.97472286e-06
Iter: 484 loss: 3.96891119e-06
Iter: 485 loss: 4.00465433e-06
Iter: 486 loss: 3.96833912e-06
Iter: 487 loss: 3.9621068e-06
Iter: 488 loss: 3.96067117e-06
Iter: 489 loss: 3.95657707e-06
Iter: 490 loss: 3.95090319e-06
Iter: 491 loss: 3.98845168e-06
Iter: 492 loss: 3.95041025e-06
Iter: 493 loss: 3.9448546e-06
Iter: 494 loss: 3.97544818e-06
Iter: 495 loss: 3.94403196e-06
Iter: 496 loss: 3.94226254e-06
Iter: 497 loss: 3.94128e-06
Iter: 498 loss: 3.93972869e-06
Iter: 499 loss: 3.93590926e-06
Iter: 500 loss: 3.96649148e-06
Iter: 501 loss: 3.93524169e-06
Iter: 502 loss: 3.93026949e-06
Iter: 503 loss: 3.93171103e-06
Iter: 504 loss: 3.92660331e-06
Iter: 505 loss: 3.92143602e-06
Iter: 506 loss: 3.97149051e-06
Iter: 507 loss: 3.92133279e-06
Iter: 508 loss: 3.91776348e-06
Iter: 509 loss: 3.92151924e-06
Iter: 510 loss: 3.91581216e-06
Iter: 511 loss: 3.91162484e-06
Iter: 512 loss: 3.92672791e-06
Iter: 513 loss: 3.91049252e-06
Iter: 514 loss: 3.9061897e-06
Iter: 515 loss: 3.90402965e-06
Iter: 516 loss: 3.90194418e-06
Iter: 517 loss: 3.89716479e-06
Iter: 518 loss: 3.91545063e-06
Iter: 519 loss: 3.89596426e-06
Iter: 520 loss: 3.88923308e-06
Iter: 521 loss: 3.89591241e-06
Iter: 522 loss: 3.88564058e-06
Iter: 523 loss: 3.88042918e-06
Iter: 524 loss: 3.90099831e-06
Iter: 525 loss: 3.87934233e-06
Iter: 526 loss: 3.87339333e-06
Iter: 527 loss: 3.88466515e-06
Iter: 528 loss: 3.87093132e-06
Iter: 529 loss: 3.87197815e-06
Iter: 530 loss: 3.86951433e-06
Iter: 531 loss: 3.86763668e-06
Iter: 532 loss: 3.86229522e-06
Iter: 533 loss: 3.89301204e-06
Iter: 534 loss: 3.86076954e-06
Iter: 535 loss: 3.85493513e-06
Iter: 536 loss: 3.88262833e-06
Iter: 537 loss: 3.85386511e-06
Iter: 538 loss: 3.85051862e-06
Iter: 539 loss: 3.85497196e-06
Iter: 540 loss: 3.84878877e-06
Iter: 541 loss: 3.84381747e-06
Iter: 542 loss: 3.84552413e-06
Iter: 543 loss: 3.84017767e-06
Iter: 544 loss: 3.83520546e-06
Iter: 545 loss: 3.86750753e-06
Iter: 546 loss: 3.83456972e-06
Iter: 547 loss: 3.82937924e-06
Iter: 548 loss: 3.83593579e-06
Iter: 549 loss: 3.8266553e-06
Iter: 550 loss: 3.82217286e-06
Iter: 551 loss: 3.83801216e-06
Iter: 552 loss: 3.82098779e-06
Iter: 553 loss: 3.81605878e-06
Iter: 554 loss: 3.81705195e-06
Iter: 555 loss: 3.81231121e-06
Iter: 556 loss: 3.80640131e-06
Iter: 557 loss: 3.82407052e-06
Iter: 558 loss: 3.80452366e-06
Iter: 559 loss: 3.7998484e-06
Iter: 560 loss: 3.83171027e-06
Iter: 561 loss: 3.79919493e-06
Iter: 562 loss: 3.79456333e-06
Iter: 563 loss: 3.7962277e-06
Iter: 564 loss: 3.79137191e-06
Iter: 565 loss: 3.79036874e-06
Iter: 566 loss: 3.7881241e-06
Iter: 567 loss: 3.78573736e-06
Iter: 568 loss: 3.78148798e-06
Iter: 569 loss: 3.78155482e-06
Iter: 570 loss: 3.77845481e-06
Iter: 571 loss: 3.77161405e-06
Iter: 572 loss: 3.86614829e-06
Iter: 573 loss: 3.77115566e-06
Iter: 574 loss: 3.76592698e-06
Iter: 575 loss: 3.76566913e-06
Iter: 576 loss: 3.76189882e-06
Iter: 577 loss: 3.7602249e-06
Iter: 578 loss: 3.75829882e-06
Iter: 579 loss: 3.75223226e-06
Iter: 580 loss: 3.78834329e-06
Iter: 581 loss: 3.7515365e-06
Iter: 582 loss: 3.74774208e-06
Iter: 583 loss: 3.75355694e-06
Iter: 584 loss: 3.74605497e-06
Iter: 585 loss: 3.74057e-06
Iter: 586 loss: 3.74644833e-06
Iter: 587 loss: 3.73761804e-06
Iter: 588 loss: 3.73281227e-06
Iter: 589 loss: 3.77783e-06
Iter: 590 loss: 3.73250032e-06
Iter: 591 loss: 3.72926115e-06
Iter: 592 loss: 3.72205523e-06
Iter: 593 loss: 3.83551833e-06
Iter: 594 loss: 3.72158092e-06
Iter: 595 loss: 3.71723354e-06
Iter: 596 loss: 3.71670649e-06
Iter: 597 loss: 3.71369e-06
Iter: 598 loss: 3.71503143e-06
Iter: 599 loss: 3.7116813e-06
Iter: 600 loss: 3.70728185e-06
Iter: 601 loss: 3.7722557e-06
Iter: 602 loss: 3.70726093e-06
Iter: 603 loss: 3.70566136e-06
Iter: 604 loss: 3.70229554e-06
Iter: 605 loss: 3.76125195e-06
Iter: 606 loss: 3.70206862e-06
Iter: 607 loss: 3.69849568e-06
Iter: 608 loss: 3.69698432e-06
Iter: 609 loss: 3.69514646e-06
Iter: 610 loss: 3.69018812e-06
Iter: 611 loss: 3.72085333e-06
Iter: 612 loss: 3.68952897e-06
Iter: 613 loss: 3.68532619e-06
Iter: 614 loss: 3.70391717e-06
Iter: 615 loss: 3.68447627e-06
Iter: 616 loss: 3.68137e-06
Iter: 617 loss: 3.69021154e-06
Iter: 618 loss: 3.68049564e-06
Iter: 619 loss: 3.67631128e-06
Iter: 620 loss: 3.67683788e-06
Iter: 621 loss: 3.67313532e-06
Iter: 622 loss: 3.66923041e-06
Iter: 623 loss: 3.7040686e-06
Iter: 624 loss: 3.66915515e-06
Iter: 625 loss: 3.66570475e-06
Iter: 626 loss: 3.66145014e-06
Iter: 627 loss: 3.661145e-06
Iter: 628 loss: 3.65574715e-06
Iter: 629 loss: 3.72217892e-06
Iter: 630 loss: 3.65568349e-06
Iter: 631 loss: 3.65234155e-06
Iter: 632 loss: 3.64788684e-06
Iter: 633 loss: 3.6476049e-06
Iter: 634 loss: 3.64632115e-06
Iter: 635 loss: 3.64461857e-06
Iter: 636 loss: 3.64163429e-06
Iter: 637 loss: 3.65352412e-06
Iter: 638 loss: 3.64084144e-06
Iter: 639 loss: 3.63965682e-06
Iter: 640 loss: 3.63554818e-06
Iter: 641 loss: 3.64092057e-06
Iter: 642 loss: 3.63245886e-06
Iter: 643 loss: 3.6278534e-06
Iter: 644 loss: 3.67811253e-06
Iter: 645 loss: 3.62769833e-06
Iter: 646 loss: 3.62291212e-06
Iter: 647 loss: 3.63124923e-06
Iter: 648 loss: 3.62076412e-06
Iter: 649 loss: 3.61695675e-06
Iter: 650 loss: 3.6548613e-06
Iter: 651 loss: 3.61677394e-06
Iter: 652 loss: 3.61346088e-06
Iter: 653 loss: 3.61132925e-06
Iter: 654 loss: 3.61009825e-06
Iter: 655 loss: 3.60548825e-06
Iter: 656 loss: 3.63395952e-06
Iter: 657 loss: 3.60500076e-06
Iter: 658 loss: 3.60070317e-06
Iter: 659 loss: 3.6077945e-06
Iter: 660 loss: 3.59866613e-06
Iter: 661 loss: 3.59484397e-06
Iter: 662 loss: 3.61896e-06
Iter: 663 loss: 3.59440764e-06
Iter: 664 loss: 3.59081923e-06
Iter: 665 loss: 3.58977672e-06
Iter: 666 loss: 3.58760599e-06
Iter: 667 loss: 3.58360262e-06
Iter: 668 loss: 3.61291222e-06
Iter: 669 loss: 3.58316174e-06
Iter: 670 loss: 3.58053762e-06
Iter: 671 loss: 3.61566458e-06
Iter: 672 loss: 3.58047168e-06
Iter: 673 loss: 3.577391e-06
Iter: 674 loss: 3.57620343e-06
Iter: 675 loss: 3.57434078e-06
Iter: 676 loss: 3.57200724e-06
Iter: 677 loss: 3.5698979e-06
Iter: 678 loss: 3.56931037e-06
Iter: 679 loss: 3.56516671e-06
Iter: 680 loss: 3.56420878e-06
Iter: 681 loss: 3.56159035e-06
Iter: 682 loss: 3.55780548e-06
Iter: 683 loss: 3.59898559e-06
Iter: 684 loss: 3.55765769e-06
Iter: 685 loss: 3.55373049e-06
Iter: 686 loss: 3.55232146e-06
Iter: 687 loss: 3.55008297e-06
Iter: 688 loss: 3.54621238e-06
Iter: 689 loss: 3.57373256e-06
Iter: 690 loss: 3.54594818e-06
Iter: 691 loss: 3.54173517e-06
Iter: 692 loss: 3.54265353e-06
Iter: 693 loss: 3.53875203e-06
Iter: 694 loss: 3.5350231e-06
Iter: 695 loss: 3.53498399e-06
Iter: 696 loss: 3.53177779e-06
Iter: 697 loss: 3.52882216e-06
Iter: 698 loss: 3.52798861e-06
Iter: 699 loss: 3.52233928e-06
Iter: 700 loss: 3.55131806e-06
Iter: 701 loss: 3.52139068e-06
Iter: 702 loss: 3.51814674e-06
Iter: 703 loss: 3.51961398e-06
Iter: 704 loss: 3.51593599e-06
Iter: 705 loss: 3.51504559e-06
Iter: 706 loss: 3.51358e-06
Iter: 707 loss: 3.51165954e-06
Iter: 708 loss: 3.50987602e-06
Iter: 709 loss: 3.50932828e-06
Iter: 710 loss: 3.50741357e-06
Iter: 711 loss: 3.50567416e-06
Iter: 712 loss: 3.50533833e-06
Iter: 713 loss: 3.50169512e-06
Iter: 714 loss: 3.50474875e-06
Iter: 715 loss: 3.49945776e-06
Iter: 716 loss: 3.49506877e-06
Iter: 717 loss: 3.50175719e-06
Iter: 718 loss: 3.49308721e-06
Iter: 719 loss: 3.48915137e-06
Iter: 720 loss: 3.52131974e-06
Iter: 721 loss: 3.48882736e-06
Iter: 722 loss: 3.48555704e-06
Iter: 723 loss: 3.48337426e-06
Iter: 724 loss: 3.48223148e-06
Iter: 725 loss: 3.47825517e-06
Iter: 726 loss: 3.52508732e-06
Iter: 727 loss: 3.47817968e-06
Iter: 728 loss: 3.47475907e-06
Iter: 729 loss: 3.47576088e-06
Iter: 730 loss: 3.47216496e-06
Iter: 731 loss: 3.46865181e-06
Iter: 732 loss: 3.49547054e-06
Iter: 733 loss: 3.4683253e-06
Iter: 734 loss: 3.46462411e-06
Iter: 735 loss: 3.46349202e-06
Iter: 736 loss: 3.46126944e-06
Iter: 737 loss: 3.45739227e-06
Iter: 738 loss: 3.51340896e-06
Iter: 739 loss: 3.45739363e-06
Iter: 740 loss: 3.45704825e-06
Iter: 741 loss: 3.45630019e-06
Iter: 742 loss: 3.45535273e-06
Iter: 743 loss: 3.4526015e-06
Iter: 744 loss: 3.45198214e-06
Iter: 745 loss: 3.44937e-06
Iter: 746 loss: 3.44422756e-06
Iter: 747 loss: 3.4913644e-06
Iter: 748 loss: 3.44396562e-06
Iter: 749 loss: 3.44131195e-06
Iter: 750 loss: 3.44384625e-06
Iter: 751 loss: 3.43968986e-06
Iter: 752 loss: 3.4356276e-06
Iter: 753 loss: 3.44696264e-06
Iter: 754 loss: 3.43418901e-06
Iter: 755 loss: 3.43148258e-06
Iter: 756 loss: 3.43932788e-06
Iter: 757 loss: 3.43072429e-06
Iter: 758 loss: 3.42696012e-06
Iter: 759 loss: 3.42379553e-06
Iter: 760 loss: 3.42280532e-06
Iter: 761 loss: 3.4184493e-06
Iter: 762 loss: 3.44471505e-06
Iter: 763 loss: 3.4180182e-06
Iter: 764 loss: 3.41333521e-06
Iter: 765 loss: 3.42186581e-06
Iter: 766 loss: 3.41126815e-06
Iter: 767 loss: 3.40811516e-06
Iter: 768 loss: 3.41834311e-06
Iter: 769 loss: 3.40730048e-06
Iter: 770 loss: 3.40314546e-06
Iter: 771 loss: 3.41392206e-06
Iter: 772 loss: 3.40160614e-06
Iter: 773 loss: 3.39866756e-06
Iter: 774 loss: 3.42666226e-06
Iter: 775 loss: 3.39858821e-06
Iter: 776 loss: 3.39721441e-06
Iter: 777 loss: 3.3971196e-06
Iter: 778 loss: 3.39573762e-06
Iter: 779 loss: 3.39150415e-06
Iter: 780 loss: 3.41065515e-06
Iter: 781 loss: 3.38993459e-06
Iter: 782 loss: 3.38701898e-06
Iter: 783 loss: 3.39549888e-06
Iter: 784 loss: 3.38602922e-06
Iter: 785 loss: 3.38250175e-06
Iter: 786 loss: 3.38703421e-06
Iter: 787 loss: 3.38068071e-06
Iter: 788 loss: 3.37717825e-06
Iter: 789 loss: 3.407132e-06
Iter: 790 loss: 3.3769993e-06
Iter: 791 loss: 3.37448228e-06
Iter: 792 loss: 3.37566894e-06
Iter: 793 loss: 3.37277515e-06
Iter: 794 loss: 3.36945595e-06
Iter: 795 loss: 3.37967276e-06
Iter: 796 loss: 3.36845574e-06
Iter: 797 loss: 3.36500079e-06
Iter: 798 loss: 3.36945232e-06
Iter: 799 loss: 3.36306312e-06
Iter: 800 loss: 3.35981758e-06
Iter: 801 loss: 3.37262509e-06
Iter: 802 loss: 3.3590868e-06
Iter: 803 loss: 3.35560821e-06
Iter: 804 loss: 3.36232165e-06
Iter: 805 loss: 3.35419031e-06
Iter: 806 loss: 3.35108143e-06
Iter: 807 loss: 3.36726634e-06
Iter: 808 loss: 3.35066306e-06
Iter: 809 loss: 3.34704214e-06
Iter: 810 loss: 3.34374954e-06
Iter: 811 loss: 3.34291371e-06
Iter: 812 loss: 3.34049787e-06
Iter: 813 loss: 3.34025663e-06
Iter: 814 loss: 3.33860567e-06
Iter: 815 loss: 3.33860021e-06
Iter: 816 loss: 3.33693265e-06
Iter: 817 loss: 3.33409616e-06
Iter: 818 loss: 3.33405683e-06
Iter: 819 loss: 3.33209755e-06
Iter: 820 loss: 3.3372753e-06
Iter: 821 loss: 3.33139906e-06
Iter: 822 loss: 3.32931859e-06
Iter: 823 loss: 3.32517607e-06
Iter: 824 loss: 3.40874772e-06
Iter: 825 loss: 3.32514128e-06
Iter: 826 loss: 3.32170566e-06
Iter: 827 loss: 3.32169566e-06
Iter: 828 loss: 3.31863e-06
Iter: 829 loss: 3.31649176e-06
Iter: 830 loss: 3.31541014e-06
Iter: 831 loss: 3.31312458e-06
Iter: 832 loss: 3.31303318e-06
Iter: 833 loss: 3.31077445e-06
Iter: 834 loss: 3.3071758e-06
Iter: 835 loss: 3.3071035e-06
Iter: 836 loss: 3.30379362e-06
Iter: 837 loss: 3.33315347e-06
Iter: 838 loss: 3.30374178e-06
Iter: 839 loss: 3.30039802e-06
Iter: 840 loss: 3.29799605e-06
Iter: 841 loss: 3.29685554e-06
Iter: 842 loss: 3.2927187e-06
Iter: 843 loss: 3.3368342e-06
Iter: 844 loss: 3.29267391e-06
Iter: 845 loss: 3.2896105e-06
Iter: 846 loss: 3.29403247e-06
Iter: 847 loss: 3.28813098e-06
Iter: 848 loss: 3.28641227e-06
Iter: 849 loss: 3.28620854e-06
Iter: 850 loss: 3.28416854e-06
Iter: 851 loss: 3.28869055e-06
Iter: 852 loss: 3.2833359e-06
Iter: 853 loss: 3.28164606e-06
Iter: 854 loss: 3.2788339e-06
Iter: 855 loss: 3.27883117e-06
Iter: 856 loss: 3.27583143e-06
Iter: 857 loss: 3.28185024e-06
Iter: 858 loss: 3.2747746e-06
Iter: 859 loss: 3.27090856e-06
Iter: 860 loss: 3.28099463e-06
Iter: 861 loss: 3.26944337e-06
Iter: 862 loss: 3.26687859e-06
Iter: 863 loss: 3.26993313e-06
Iter: 864 loss: 3.26548366e-06
Iter: 865 loss: 3.26161171e-06
Iter: 866 loss: 3.2673197e-06
Iter: 867 loss: 3.25990777e-06
Iter: 868 loss: 3.25706719e-06
Iter: 869 loss: 3.27597354e-06
Iter: 870 loss: 3.25680594e-06
Iter: 871 loss: 3.25357496e-06
Iter: 872 loss: 3.25311157e-06
Iter: 873 loss: 3.25074916e-06
Iter: 874 loss: 3.24773237e-06
Iter: 875 loss: 3.28826241e-06
Iter: 876 loss: 3.24774101e-06
Iter: 877 loss: 3.24553503e-06
Iter: 878 loss: 3.24264784e-06
Iter: 879 loss: 3.24254597e-06
Iter: 880 loss: 3.23903259e-06
Iter: 881 loss: 3.26996428e-06
Iter: 882 loss: 3.23872746e-06
Iter: 883 loss: 3.23632867e-06
Iter: 884 loss: 3.24498524e-06
Iter: 885 loss: 3.23574977e-06
Iter: 886 loss: 3.23269978e-06
Iter: 887 loss: 3.26527061e-06
Iter: 888 loss: 3.23270342e-06
Iter: 889 loss: 3.2316791e-06
Iter: 890 loss: 3.22952405e-06
Iter: 891 loss: 3.2630719e-06
Iter: 892 loss: 3.22941014e-06
Iter: 893 loss: 3.22687583e-06
Iter: 894 loss: 3.22805818e-06
Iter: 895 loss: 3.22502069e-06
Iter: 896 loss: 3.22190726e-06
Iter: 897 loss: 3.23985705e-06
Iter: 898 loss: 3.22147253e-06
Iter: 899 loss: 3.21877928e-06
Iter: 900 loss: 3.22406049e-06
Iter: 901 loss: 3.21768675e-06
Iter: 902 loss: 3.21487505e-06
Iter: 903 loss: 3.22456799e-06
Iter: 904 loss: 3.21406787e-06
Iter: 905 loss: 3.21100606e-06
Iter: 906 loss: 3.20760819e-06
Iter: 907 loss: 3.20716913e-06
Iter: 908 loss: 3.20348317e-06
Iter: 909 loss: 3.25011547e-06
Iter: 910 loss: 3.20355275e-06
Iter: 911 loss: 3.20001845e-06
Iter: 912 loss: 3.20179834e-06
Iter: 913 loss: 3.19782021e-06
Iter: 914 loss: 3.19497713e-06
Iter: 915 loss: 3.22349797e-06
Iter: 916 loss: 3.1949312e-06
Iter: 917 loss: 3.19265473e-06
Iter: 918 loss: 3.18876732e-06
Iter: 919 loss: 3.18873435e-06
Iter: 920 loss: 3.19041101e-06
Iter: 921 loss: 3.1874265e-06
Iter: 922 loss: 3.18597267e-06
Iter: 923 loss: 3.18717412e-06
Iter: 924 loss: 3.18502862e-06
Iter: 925 loss: 3.18372872e-06
Iter: 926 loss: 3.1807117e-06
Iter: 927 loss: 3.21469201e-06
Iter: 928 loss: 3.18045227e-06
Iter: 929 loss: 3.17763761e-06
Iter: 930 loss: 3.1932077e-06
Iter: 931 loss: 3.17731201e-06
Iter: 932 loss: 3.1746647e-06
Iter: 933 loss: 3.17361355e-06
Iter: 934 loss: 3.17220588e-06
Iter: 935 loss: 3.16960723e-06
Iter: 936 loss: 3.20621325e-06
Iter: 937 loss: 3.16957221e-06
Iter: 938 loss: 3.16711589e-06
Iter: 939 loss: 3.1671359e-06
Iter: 940 loss: 3.16503611e-06
Iter: 941 loss: 3.16166e-06
Iter: 942 loss: 3.17901277e-06
Iter: 943 loss: 3.16118894e-06
Iter: 944 loss: 3.15837565e-06
Iter: 945 loss: 3.15957436e-06
Iter: 946 loss: 3.15661646e-06
Iter: 947 loss: 3.15345687e-06
Iter: 948 loss: 3.16807791e-06
Iter: 949 loss: 3.15277521e-06
Iter: 950 loss: 3.14981276e-06
Iter: 951 loss: 3.15207762e-06
Iter: 952 loss: 3.14803e-06
Iter: 953 loss: 3.14581712e-06
Iter: 954 loss: 3.14592239e-06
Iter: 955 loss: 3.14445037e-06
Iter: 956 loss: 3.16521664e-06
Iter: 957 loss: 3.14442423e-06
Iter: 958 loss: 3.14323802e-06
Iter: 959 loss: 3.1403797e-06
Iter: 960 loss: 3.16335263e-06
Iter: 961 loss: 3.13985265e-06
Iter: 962 loss: 3.13764485e-06
Iter: 963 loss: 3.14242129e-06
Iter: 964 loss: 3.13661621e-06
Iter: 965 loss: 3.13337296e-06
Iter: 966 loss: 3.13809483e-06
Iter: 967 loss: 3.13176611e-06
Iter: 968 loss: 3.12898692e-06
Iter: 969 loss: 3.14033082e-06
Iter: 970 loss: 3.12847419e-06
Iter: 971 loss: 3.12522388e-06
Iter: 972 loss: 3.12676593e-06
Iter: 973 loss: 3.12310954e-06
Iter: 974 loss: 3.12042198e-06
Iter: 975 loss: 3.14643421e-06
Iter: 976 loss: 3.12029965e-06
Iter: 977 loss: 3.11815757e-06
Iter: 978 loss: 3.12105567e-06
Iter: 979 loss: 3.11707936e-06
Iter: 980 loss: 3.11459144e-06
Iter: 981 loss: 3.12659608e-06
Iter: 982 loss: 3.11403051e-06
Iter: 983 loss: 3.11197482e-06
Iter: 984 loss: 3.11002236e-06
Iter: 985 loss: 3.10945461e-06
Iter: 986 loss: 3.1067807e-06
Iter: 987 loss: 3.1484e-06
Iter: 988 loss: 3.10676296e-06
Iter: 989 loss: 3.10634141e-06
Iter: 990 loss: 3.10581322e-06
Iter: 991 loss: 3.10509722e-06
Iter: 992 loss: 3.10323594e-06
Iter: 993 loss: 3.12154134e-06
Iter: 994 loss: 3.10303312e-06
Iter: 995 loss: 3.10104838e-06
Iter: 996 loss: 3.10247242e-06
Iter: 997 loss: 3.09980896e-06
Iter: 998 loss: 3.09711e-06
Iter: 999 loss: 3.10273981e-06
Iter: 1000 loss: 3.09602e-06
Iter: 1001 loss: 3.09361576e-06
Iter: 1002 loss: 3.10432256e-06
Iter: 1003 loss: 3.09306279e-06
Iter: 1004 loss: 3.09066763e-06
Iter: 1005 loss: 3.09034726e-06
Iter: 1006 loss: 3.08861922e-06
Iter: 1007 loss: 3.08619838e-06
Iter: 1008 loss: 3.1216864e-06
Iter: 1009 loss: 3.08625658e-06
Iter: 1010 loss: 3.08436688e-06
Iter: 1011 loss: 3.08250719e-06
Iter: 1012 loss: 3.08212748e-06
Iter: 1013 loss: 3.07960363e-06
Iter: 1014 loss: 3.11664894e-06
Iter: 1015 loss: 3.07957635e-06
Iter: 1016 loss: 3.07781647e-06
Iter: 1017 loss: 3.07838036e-06
Iter: 1018 loss: 3.07664095e-06
Iter: 1019 loss: 3.07386608e-06
Iter: 1020 loss: 3.07633377e-06
Iter: 1021 loss: 3.07217397e-06
Iter: 1022 loss: 3.07203368e-06
Iter: 1023 loss: 3.07129721e-06
Iter: 1024 loss: 3.06998982e-06
Iter: 1025 loss: 3.06852462e-06
Iter: 1026 loss: 3.06843708e-06
Iter: 1027 loss: 3.06681704e-06
Iter: 1028 loss: 3.06476932e-06
Iter: 1029 loss: 3.06451761e-06
Iter: 1030 loss: 3.06194033e-06
Iter: 1031 loss: 3.07169216e-06
Iter: 1032 loss: 3.06134461e-06
Iter: 1033 loss: 3.05844742e-06
Iter: 1034 loss: 3.06292759e-06
Iter: 1035 loss: 3.05708704e-06
Iter: 1036 loss: 3.05417871e-06
Iter: 1037 loss: 3.0614076e-06
Iter: 1038 loss: 3.05320509e-06
Iter: 1039 loss: 3.05058597e-06
Iter: 1040 loss: 3.07056575e-06
Iter: 1041 loss: 3.05038179e-06
Iter: 1042 loss: 3.04814e-06
Iter: 1043 loss: 3.04666401e-06
Iter: 1044 loss: 3.04581818e-06
Iter: 1045 loss: 3.04303376e-06
Iter: 1046 loss: 3.08046106e-06
Iter: 1047 loss: 3.04301875e-06
Iter: 1048 loss: 3.04138166e-06
Iter: 1049 loss: 3.04346668e-06
Iter: 1050 loss: 3.04050309e-06
Iter: 1051 loss: 3.03800834e-06
Iter: 1052 loss: 3.03881939e-06
Iter: 1053 loss: 3.03606475e-06
Iter: 1054 loss: 3.03404386e-06
Iter: 1055 loss: 3.06633774e-06
Iter: 1056 loss: 3.03406364e-06
Iter: 1057 loss: 3.03262414e-06
Iter: 1058 loss: 3.05315621e-06
Iter: 1059 loss: 3.03261118e-06
Iter: 1060 loss: 3.03157981e-06
Iter: 1061 loss: 3.02893022e-06
Iter: 1062 loss: 3.04020909e-06
Iter: 1063 loss: 3.02787839e-06
Iter: 1064 loss: 3.02544254e-06
Iter: 1065 loss: 3.03311663e-06
Iter: 1066 loss: 3.02478429e-06
Iter: 1067 loss: 3.02155672e-06
Iter: 1068 loss: 3.02629e-06
Iter: 1069 loss: 3.02005105e-06
Iter: 1070 loss: 3.01723458e-06
Iter: 1071 loss: 3.03717457e-06
Iter: 1072 loss: 3.01700175e-06
Iter: 1073 loss: 3.01484624e-06
Iter: 1074 loss: 3.01729779e-06
Iter: 1075 loss: 3.0136282e-06
Iter: 1076 loss: 3.01104251e-06
Iter: 1077 loss: 3.01800287e-06
Iter: 1078 loss: 3.01018918e-06
Iter: 1079 loss: 3.00747229e-06
Iter: 1080 loss: 3.01580508e-06
Iter: 1081 loss: 3.00677107e-06
Iter: 1082 loss: 3.00462671e-06
Iter: 1083 loss: 3.01358114e-06
Iter: 1084 loss: 3.00418924e-06
Iter: 1085 loss: 3.0018025e-06
Iter: 1086 loss: 3.00429656e-06
Iter: 1087 loss: 3.00049123e-06
Iter: 1088 loss: 2.99828253e-06
Iter: 1089 loss: 3.00958436e-06
Iter: 1090 loss: 2.99788121e-06
Iter: 1091 loss: 2.9968669e-06
Iter: 1092 loss: 2.99660405e-06
Iter: 1093 loss: 2.9953776e-06
Iter: 1094 loss: 2.99231465e-06
Iter: 1095 loss: 3.01772252e-06
Iter: 1096 loss: 2.99173325e-06
Iter: 1097 loss: 2.98955365e-06
Iter: 1098 loss: 2.9971884e-06
Iter: 1099 loss: 2.98915484e-06
Iter: 1100 loss: 2.98654959e-06
Iter: 1101 loss: 2.98421901e-06
Iter: 1102 loss: 2.98359055e-06
Iter: 1103 loss: 2.980933e-06
Iter: 1104 loss: 3.01457385e-06
Iter: 1105 loss: 2.98094778e-06
Iter: 1106 loss: 2.9782741e-06
Iter: 1107 loss: 2.97886845e-06
Iter: 1108 loss: 2.97627298e-06
Iter: 1109 loss: 2.97339557e-06
Iter: 1110 loss: 2.99159319e-06
Iter: 1111 loss: 2.97309339e-06
Iter: 1112 loss: 2.97059819e-06
Iter: 1113 loss: 2.9728094e-06
Iter: 1114 loss: 2.96918506e-06
Iter: 1115 loss: 2.96642042e-06
Iter: 1116 loss: 2.980923e-06
Iter: 1117 loss: 2.9658795e-06
Iter: 1118 loss: 2.96341477e-06
Iter: 1119 loss: 2.97030533e-06
Iter: 1120 loss: 2.96265807e-06
Iter: 1121 loss: 2.96060261e-06
Iter: 1122 loss: 2.96715643e-06
Iter: 1123 loss: 2.95991845e-06
Iter: 1124 loss: 2.95830159e-06
Iter: 1125 loss: 2.98245436e-06
Iter: 1126 loss: 2.95825339e-06
Iter: 1127 loss: 2.95619202e-06
Iter: 1128 loss: 2.95656491e-06
Iter: 1129 loss: 2.95461905e-06
Iter: 1130 loss: 2.95352652e-06
Iter: 1131 loss: 2.95219206e-06
Iter: 1132 loss: 2.95208042e-06
Iter: 1133 loss: 2.94967e-06
Iter: 1134 loss: 2.94912911e-06
Iter: 1135 loss: 2.94737947e-06
Iter: 1136 loss: 2.94508118e-06
Iter: 1137 loss: 2.9633843e-06
Iter: 1138 loss: 2.94485835e-06
Iter: 1139 loss: 2.94240749e-06
Iter: 1140 loss: 2.94307802e-06
Iter: 1141 loss: 2.94072515e-06
Iter: 1142 loss: 2.93819653e-06
Iter: 1143 loss: 2.96499366e-06
Iter: 1144 loss: 2.93807261e-06
Iter: 1145 loss: 2.93589528e-06
Iter: 1146 loss: 2.93423022e-06
Iter: 1147 loss: 2.93355833e-06
Iter: 1148 loss: 2.93094308e-06
Iter: 1149 loss: 2.9620262e-06
Iter: 1150 loss: 2.93092262e-06
Iter: 1151 loss: 2.92907907e-06
Iter: 1152 loss: 2.92993832e-06
Iter: 1153 loss: 2.92795266e-06
Iter: 1154 loss: 2.92478899e-06
Iter: 1155 loss: 2.92977893e-06
Iter: 1156 loss: 2.92342793e-06
Iter: 1157 loss: 2.92092636e-06
Iter: 1158 loss: 2.95394102e-06
Iter: 1159 loss: 2.92092454e-06
Iter: 1160 loss: 2.91899596e-06
Iter: 1161 loss: 2.93943754e-06
Iter: 1162 loss: 2.91889546e-06
Iter: 1163 loss: 2.91802417e-06
Iter: 1164 loss: 2.91619153e-06
Iter: 1165 loss: 2.94650681e-06
Iter: 1166 loss: 2.91610422e-06
Iter: 1167 loss: 2.91402557e-06
Iter: 1168 loss: 2.91256265e-06
Iter: 1169 loss: 2.91188462e-06
Iter: 1170 loss: 2.9089997e-06
Iter: 1171 loss: 2.93342578e-06
Iter: 1172 loss: 2.90871776e-06
Iter: 1173 loss: 2.90645585e-06
Iter: 1174 loss: 2.90641856e-06
Iter: 1175 loss: 2.90466051e-06
Iter: 1176 loss: 2.90182697e-06
Iter: 1177 loss: 2.92833101e-06
Iter: 1178 loss: 2.90165394e-06
Iter: 1179 loss: 2.89973309e-06
Iter: 1180 loss: 2.89967738e-06
Iter: 1181 loss: 2.89818172e-06
Iter: 1182 loss: 2.89501259e-06
Iter: 1183 loss: 2.91174592e-06
Iter: 1184 loss: 2.89452078e-06
Iter: 1185 loss: 2.89258969e-06
Iter: 1186 loss: 2.89588888e-06
Iter: 1187 loss: 2.89197442e-06
Iter: 1188 loss: 2.88920751e-06
Iter: 1189 loss: 2.89138688e-06
Iter: 1190 loss: 2.88766887e-06
Iter: 1191 loss: 2.88589e-06
Iter: 1192 loss: 2.88588421e-06
Iter: 1193 loss: 2.88480032e-06
Iter: 1194 loss: 2.89923855e-06
Iter: 1195 loss: 2.8848026e-06
Iter: 1196 loss: 2.88388242e-06
Iter: 1197 loss: 2.88138949e-06
Iter: 1198 loss: 2.90083108e-06
Iter: 1199 loss: 2.88087404e-06
Iter: 1200 loss: 2.87896091e-06
Iter: 1201 loss: 2.88755382e-06
Iter: 1202 loss: 2.87859029e-06
Iter: 1203 loss: 2.87670036e-06
Iter: 1204 loss: 2.87717921e-06
Iter: 1205 loss: 2.87531e-06
Iter: 1206 loss: 2.87273087e-06
Iter: 1207 loss: 2.87964599e-06
Iter: 1208 loss: 2.87205557e-06
Iter: 1209 loss: 2.86973227e-06
Iter: 1210 loss: 2.88509136e-06
Iter: 1211 loss: 2.86951013e-06
Iter: 1212 loss: 2.867896e-06
Iter: 1213 loss: 2.86711384e-06
Iter: 1214 loss: 2.86623322e-06
Iter: 1215 loss: 2.8635327e-06
Iter: 1216 loss: 2.88372212e-06
Iter: 1217 loss: 2.86332738e-06
Iter: 1218 loss: 2.86143973e-06
Iter: 1219 loss: 2.86411819e-06
Iter: 1220 loss: 2.86055365e-06
Iter: 1221 loss: 2.85788201e-06
Iter: 1222 loss: 2.86015575e-06
Iter: 1223 loss: 2.8563079e-06
Iter: 1224 loss: 2.85451779e-06
Iter: 1225 loss: 2.88045135e-06
Iter: 1226 loss: 2.85448095e-06
Iter: 1227 loss: 2.85315218e-06
Iter: 1228 loss: 2.86842351e-06
Iter: 1229 loss: 2.85316128e-06
Iter: 1230 loss: 2.8517411e-06
Iter: 1231 loss: 2.84973794e-06
Iter: 1232 loss: 2.84965699e-06
Iter: 1233 loss: 2.84825819e-06
Iter: 1234 loss: 2.84924658e-06
Iter: 1235 loss: 2.84740622e-06
Iter: 1236 loss: 2.84551425e-06
Iter: 1237 loss: 2.84386169e-06
Iter: 1238 loss: 2.8432396e-06
Iter: 1239 loss: 2.84052476e-06
Iter: 1240 loss: 2.86174031e-06
Iter: 1241 loss: 2.84039652e-06
Iter: 1242 loss: 2.83806094e-06
Iter: 1243 loss: 2.83951704e-06
Iter: 1244 loss: 2.8365157e-06
Iter: 1245 loss: 2.83362351e-06
Iter: 1246 loss: 2.84276507e-06
Iter: 1247 loss: 2.83273835e-06
Iter: 1248 loss: 2.8298698e-06
Iter: 1249 loss: 2.84562611e-06
Iter: 1250 loss: 2.8294935e-06
Iter: 1251 loss: 2.82750625e-06
Iter: 1252 loss: 2.830217e-06
Iter: 1253 loss: 2.82651217e-06
Iter: 1254 loss: 2.82372685e-06
Iter: 1255 loss: 2.83294321e-06
Iter: 1256 loss: 2.82306064e-06
Iter: 1257 loss: 2.82103633e-06
Iter: 1258 loss: 2.82724159e-06
Iter: 1259 loss: 2.8203126e-06
Iter: 1260 loss: 2.81870393e-06
Iter: 1261 loss: 2.8423965e-06
Iter: 1262 loss: 2.81872644e-06
Iter: 1263 loss: 2.81685925e-06
Iter: 1264 loss: 2.82064457e-06
Iter: 1265 loss: 2.81607436e-06
Iter: 1266 loss: 2.81506823e-06
Iter: 1267 loss: 2.81466e-06
Iter: 1268 loss: 2.81414896e-06
Iter: 1269 loss: 2.81264533e-06
Iter: 1270 loss: 2.80969107e-06
Iter: 1271 loss: 2.86882187e-06
Iter: 1272 loss: 2.80964696e-06
Iter: 1273 loss: 2.80745644e-06
Iter: 1274 loss: 2.8074669e-06
Iter: 1275 loss: 2.80564677e-06
Iter: 1276 loss: 2.80456698e-06
Iter: 1277 loss: 2.80386712e-06
Iter: 1278 loss: 2.80166932e-06
Iter: 1279 loss: 2.81745793e-06
Iter: 1280 loss: 2.80136987e-06
Iter: 1281 loss: 2.79933465e-06
Iter: 1282 loss: 2.80230324e-06
Iter: 1283 loss: 2.79826281e-06
Iter: 1284 loss: 2.79594565e-06
Iter: 1285 loss: 2.80424956e-06
Iter: 1286 loss: 2.79531105e-06
Iter: 1287 loss: 2.79313599e-06
Iter: 1288 loss: 2.80108293e-06
Iter: 1289 loss: 2.79245887e-06
Iter: 1290 loss: 2.79063443e-06
Iter: 1291 loss: 2.79676487e-06
Iter: 1292 loss: 2.79005599e-06
Iter: 1293 loss: 2.78831135e-06
Iter: 1294 loss: 2.79931305e-06
Iter: 1295 loss: 2.7881124e-06
Iter: 1296 loss: 2.78655261e-06
Iter: 1297 loss: 2.80958375e-06
Iter: 1298 loss: 2.78648554e-06
Iter: 1299 loss: 2.78588959e-06
Iter: 1300 loss: 2.78461494e-06
Iter: 1301 loss: 2.807084e-06
Iter: 1302 loss: 2.78459856e-06
Iter: 1303 loss: 2.78301059e-06
Iter: 1304 loss: 2.78157199e-06
Iter: 1305 loss: 2.78112043e-06
Iter: 1306 loss: 2.77854861e-06
Iter: 1307 loss: 2.79350274e-06
Iter: 1308 loss: 2.77820186e-06
Iter: 1309 loss: 2.77617437e-06
Iter: 1310 loss: 2.77515505e-06
Iter: 1311 loss: 2.77414028e-06
Iter: 1312 loss: 2.77154095e-06
Iter: 1313 loss: 2.80546283e-06
Iter: 1314 loss: 2.77151594e-06
Iter: 1315 loss: 2.76999708e-06
Iter: 1316 loss: 2.76963442e-06
Iter: 1317 loss: 2.76855417e-06
Iter: 1318 loss: 2.76597848e-06
Iter: 1319 loss: 2.7789315e-06
Iter: 1320 loss: 2.76557194e-06
Iter: 1321 loss: 2.76369587e-06
Iter: 1322 loss: 2.76777314e-06
Iter: 1323 loss: 2.76289029e-06
Iter: 1324 loss: 2.76079368e-06
Iter: 1325 loss: 2.7670435e-06
Iter: 1326 loss: 2.76005949e-06
Iter: 1327 loss: 2.75816592e-06
Iter: 1328 loss: 2.76980791e-06
Iter: 1329 loss: 2.75793968e-06
Iter: 1330 loss: 2.75677053e-06
Iter: 1331 loss: 2.75672573e-06
Iter: 1332 loss: 2.75588673e-06
Iter: 1333 loss: 2.75415346e-06
Iter: 1334 loss: 2.78297853e-06
Iter: 1335 loss: 2.75415914e-06
Iter: 1336 loss: 2.75251705e-06
Iter: 1337 loss: 2.75198818e-06
Iter: 1338 loss: 2.75101229e-06
Iter: 1339 loss: 2.74848617e-06
Iter: 1340 loss: 2.76238848e-06
Iter: 1341 loss: 2.7480412e-06
Iter: 1342 loss: 2.74623471e-06
Iter: 1343 loss: 2.74357e-06
Iter: 1344 loss: 2.74358422e-06
Iter: 1345 loss: 2.74104809e-06
Iter: 1346 loss: 2.74101058e-06
Iter: 1347 loss: 2.7394758e-06
Iter: 1348 loss: 2.73783758e-06
Iter: 1349 loss: 2.7375047e-06
Iter: 1350 loss: 2.73478145e-06
Iter: 1351 loss: 2.75775096e-06
Iter: 1352 loss: 2.73462024e-06
Iter: 1353 loss: 2.73270598e-06
Iter: 1354 loss: 2.73283467e-06
Iter: 1355 loss: 2.73121123e-06
Iter: 1356 loss: 2.72875695e-06
Iter: 1357 loss: 2.74474587e-06
Iter: 1358 loss: 2.72836e-06
Iter: 1359 loss: 2.72649345e-06
Iter: 1360 loss: 2.74038098e-06
Iter: 1361 loss: 2.72631542e-06
Iter: 1362 loss: 2.72556463e-06
Iter: 1363 loss: 2.7254232e-06
Iter: 1364 loss: 2.72473562e-06
Iter: 1365 loss: 2.7234837e-06
Iter: 1366 loss: 2.75166781e-06
Iter: 1367 loss: 2.72358557e-06
Iter: 1368 loss: 2.72209036e-06
Iter: 1369 loss: 2.72153602e-06
Iter: 1370 loss: 2.72092711e-06
Iter: 1371 loss: 2.71880754e-06
Iter: 1372 loss: 2.7305191e-06
Iter: 1373 loss: 2.7185115e-06
Iter: 1374 loss: 2.71695217e-06
Iter: 1375 loss: 2.71608178e-06
Iter: 1376 loss: 2.71530917e-06
Iter: 1377 loss: 2.71306089e-06
Iter: 1378 loss: 2.73092678e-06
Iter: 1379 loss: 2.71292765e-06
Iter: 1380 loss: 2.71139197e-06
Iter: 1381 loss: 2.71143131e-06
Iter: 1382 loss: 2.7102667e-06
Iter: 1383 loss: 2.70795317e-06
Iter: 1384 loss: 2.7196179e-06
Iter: 1385 loss: 2.70765872e-06
Iter: 1386 loss: 2.70602982e-06
Iter: 1387 loss: 2.71001409e-06
Iter: 1388 loss: 2.70549458e-06
Iter: 1389 loss: 2.7035228e-06
Iter: 1390 loss: 2.7072665e-06
Iter: 1391 loss: 2.70269e-06
Iter: 1392 loss: 2.70120199e-06
Iter: 1393 loss: 2.72006e-06
Iter: 1394 loss: 2.70122155e-06
Iter: 1395 loss: 2.70047144e-06
Iter: 1396 loss: 2.70045143e-06
Iter: 1397 loss: 2.69959082e-06
Iter: 1398 loss: 2.69824477e-06
Iter: 1399 loss: 2.69824454e-06
Iter: 1400 loss: 2.69691282e-06
Iter: 1401 loss: 2.6975913e-06
Iter: 1402 loss: 2.69595012e-06
Iter: 1403 loss: 2.69446173e-06
Iter: 1404 loss: 2.69866132e-06
Iter: 1405 loss: 2.69407633e-06
Iter: 1406 loss: 2.69237421e-06
Iter: 1407 loss: 2.69367047e-06
Iter: 1408 loss: 2.69144493e-06
Iter: 1409 loss: 2.68932536e-06
Iter: 1410 loss: 2.69440602e-06
Iter: 1411 loss: 2.68844315e-06
Iter: 1412 loss: 2.68651615e-06
Iter: 1413 loss: 2.69209e-06
Iter: 1414 loss: 2.68584677e-06
Iter: 1415 loss: 2.68376334e-06
Iter: 1416 loss: 2.68682766e-06
Iter: 1417 loss: 2.68273288e-06
Iter: 1418 loss: 2.6808284e-06
Iter: 1419 loss: 2.68750364e-06
Iter: 1420 loss: 2.68039093e-06
Iter: 1421 loss: 2.67816063e-06
Iter: 1422 loss: 2.68346048e-06
Iter: 1423 loss: 2.67732685e-06
Iter: 1424 loss: 2.6757e-06
Iter: 1425 loss: 2.68865483e-06
Iter: 1426 loss: 2.67561018e-06
Iter: 1427 loss: 2.67466817e-06
Iter: 1428 loss: 2.68838494e-06
Iter: 1429 loss: 2.67463179e-06
Iter: 1430 loss: 2.67336327e-06
Iter: 1431 loss: 2.67256519e-06
Iter: 1432 loss: 2.67207429e-06
Iter: 1433 loss: 2.67081032e-06
Iter: 1434 loss: 2.67188534e-06
Iter: 1435 loss: 2.67014025e-06
Iter: 1436 loss: 2.66886127e-06
Iter: 1437 loss: 2.66819507e-06
Iter: 1438 loss: 2.66752841e-06
Iter: 1439 loss: 2.66545203e-06
Iter: 1440 loss: 2.67818655e-06
Iter: 1441 loss: 2.6652674e-06
Iter: 1442 loss: 2.66362576e-06
Iter: 1443 loss: 2.66426969e-06
Iter: 1444 loss: 2.66245047e-06
Iter: 1445 loss: 2.66055e-06
Iter: 1446 loss: 2.66778852e-06
Iter: 1447 loss: 2.66004804e-06
Iter: 1448 loss: 2.65812764e-06
Iter: 1449 loss: 2.66254256e-06
Iter: 1450 loss: 2.65732888e-06
Iter: 1451 loss: 2.65567496e-06
Iter: 1452 loss: 2.65681183e-06
Iter: 1453 loss: 2.65478684e-06
Iter: 1454 loss: 2.65240601e-06
Iter: 1455 loss: 2.66725669e-06
Iter: 1456 loss: 2.65225117e-06
Iter: 1457 loss: 2.65075732e-06
Iter: 1458 loss: 2.65587687e-06
Iter: 1459 loss: 2.65039262e-06
Iter: 1460 loss: 2.64901701e-06
Iter: 1461 loss: 2.66064762e-06
Iter: 1462 loss: 2.64891e-06
Iter: 1463 loss: 2.6472926e-06
Iter: 1464 loss: 2.65281551e-06
Iter: 1465 loss: 2.64693881e-06
Iter: 1466 loss: 2.64623532e-06
Iter: 1467 loss: 2.64549567e-06
Iter: 1468 loss: 2.64535288e-06
Iter: 1469 loss: 2.6441171e-06
Iter: 1470 loss: 2.64281e-06
Iter: 1471 loss: 2.64249957e-06
Iter: 1472 loss: 2.64044479e-06
Iter: 1473 loss: 2.65630842e-06
Iter: 1474 loss: 2.64030064e-06
Iter: 1475 loss: 2.63867628e-06
Iter: 1476 loss: 2.63966513e-06
Iter: 1477 loss: 2.63766469e-06
Iter: 1478 loss: 2.63565744e-06
Iter: 1479 loss: 2.64196933e-06
Iter: 1480 loss: 2.63515858e-06
Iter: 1481 loss: 2.63341826e-06
Iter: 1482 loss: 2.63892616e-06
Iter: 1483 loss: 2.63293668e-06
Iter: 1484 loss: 2.6313121e-06
Iter: 1485 loss: 2.63215725e-06
Iter: 1486 loss: 2.6303228e-06
Iter: 1487 loss: 2.62811955e-06
Iter: 1488 loss: 2.63890865e-06
Iter: 1489 loss: 2.62769845e-06
Iter: 1490 loss: 2.62604385e-06
Iter: 1491 loss: 2.63436596e-06
Iter: 1492 loss: 2.62585081e-06
Iter: 1493 loss: 2.62443018e-06
Iter: 1494 loss: 2.63031097e-06
Iter: 1495 loss: 2.6241014e-06
Iter: 1496 loss: 2.62257322e-06
Iter: 1497 loss: 2.64213531e-06
Iter: 1498 loss: 2.62252047e-06
Iter: 1499 loss: 2.62190861e-06
Iter: 1500 loss: 2.62101662e-06
Iter: 1501 loss: 2.62099366e-06
Iter: 1502 loss: 2.61977129e-06
Iter: 1503 loss: 2.61835521e-06
Iter: 1504 loss: 2.618179e-06
Iter: 1505 loss: 2.61628725e-06
Iter: 1506 loss: 2.6394714e-06
Iter: 1507 loss: 2.61627474e-06
Iter: 1508 loss: 2.61503146e-06
Iter: 1509 loss: 2.6140865e-06
Iter: 1510 loss: 2.61372429e-06
Iter: 1511 loss: 2.61150399e-06
Iter: 1512 loss: 2.62212643e-06
Iter: 1513 loss: 2.61107198e-06
Iter: 1514 loss: 2.60948582e-06
Iter: 1515 loss: 2.61392825e-06
Iter: 1516 loss: 2.60890238e-06
Iter: 1517 loss: 2.60716206e-06
Iter: 1518 loss: 2.60971751e-06
Iter: 1519 loss: 2.6062869e-06
Iter: 1520 loss: 2.60415209e-06
Iter: 1521 loss: 2.60840466e-06
Iter: 1522 loss: 2.60327852e-06
Iter: 1523 loss: 2.6011212e-06
Iter: 1524 loss: 2.61526725e-06
Iter: 1525 loss: 2.60090587e-06
Iter: 1526 loss: 2.59919079e-06
Iter: 1527 loss: 2.60078923e-06
Iter: 1528 loss: 2.59822195e-06
Iter: 1529 loss: 2.59822264e-06
Iter: 1530 loss: 2.59722265e-06
Iter: 1531 loss: 2.59671833e-06
Iter: 1532 loss: 2.59580838e-06
Iter: 1533 loss: 2.61664081e-06
Iter: 1534 loss: 2.59578e-06
Iter: 1535 loss: 2.59456147e-06
Iter: 1536 loss: 2.59264516e-06
Iter: 1537 loss: 2.59264061e-06
Iter: 1538 loss: 2.59094622e-06
Iter: 1539 loss: 2.59100034e-06
Iter: 1540 loss: 2.58977411e-06
Iter: 1541 loss: 2.58915429e-06
Iter: 1542 loss: 2.58859063e-06
Iter: 1543 loss: 2.5865379e-06
Iter: 1544 loss: 2.59312128e-06
Iter: 1545 loss: 2.58594946e-06
Iter: 1546 loss: 2.58442333e-06
Iter: 1547 loss: 2.58804789e-06
Iter: 1548 loss: 2.58373916e-06
Iter: 1549 loss: 2.58166983e-06
Iter: 1550 loss: 2.58308251e-06
Iter: 1551 loss: 2.58029104e-06
Iter: 1552 loss: 2.57810234e-06
Iter: 1553 loss: 2.58556111e-06
Iter: 1554 loss: 2.57742795e-06
Iter: 1555 loss: 2.57531974e-06
Iter: 1556 loss: 2.58567752e-06
Iter: 1557 loss: 2.57507031e-06
Iter: 1558 loss: 2.57304964e-06
Iter: 1559 loss: 2.57726469e-06
Iter: 1560 loss: 2.57228703e-06
Iter: 1561 loss: 2.57283318e-06
Iter: 1562 loss: 2.57150668e-06
Iter: 1563 loss: 2.5710151e-06
Iter: 1564 loss: 2.56999829e-06
Iter: 1565 loss: 2.58925866e-06
Iter: 1566 loss: 2.56992462e-06
Iter: 1567 loss: 2.56877865e-06
Iter: 1568 loss: 2.56674343e-06
Iter: 1569 loss: 2.56672388e-06
Iter: 1570 loss: 2.56499925e-06
Iter: 1571 loss: 2.58786713e-06
Iter: 1572 loss: 2.56493513e-06
Iter: 1573 loss: 2.56365752e-06
Iter: 1574 loss: 2.56348721e-06
Iter: 1575 loss: 2.56251633e-06
Iter: 1576 loss: 2.56046633e-06
Iter: 1577 loss: 2.56898807e-06
Iter: 1578 loss: 2.56008275e-06
Iter: 1579 loss: 2.55850296e-06
Iter: 1580 loss: 2.56230851e-06
Iter: 1581 loss: 2.55796022e-06
Iter: 1582 loss: 2.55617761e-06
Iter: 1583 loss: 2.55987061e-06
Iter: 1584 loss: 2.55543955e-06
Iter: 1585 loss: 2.5538875e-06
Iter: 1586 loss: 2.55841633e-06
Iter: 1587 loss: 2.55348505e-06
Iter: 1588 loss: 2.55181294e-06
Iter: 1589 loss: 2.55406508e-06
Iter: 1590 loss: 2.55092823e-06
Iter: 1591 loss: 2.54894508e-06
Iter: 1592 loss: 2.55581199e-06
Iter: 1593 loss: 2.5485092e-06
Iter: 1594 loss: 2.54850829e-06
Iter: 1595 loss: 2.54770498e-06
Iter: 1596 loss: 2.54689257e-06
Iter: 1597 loss: 2.54581619e-06
Iter: 1598 loss: 2.54579891e-06
Iter: 1599 loss: 2.54450265e-06
Iter: 1600 loss: 2.54303245e-06
Iter: 1601 loss: 2.54271185e-06
Iter: 1602 loss: 2.54091287e-06
Iter: 1603 loss: 2.55403484e-06
Iter: 1604 loss: 2.54077236e-06
Iter: 1605 loss: 2.53932421e-06
Iter: 1606 loss: 2.53971029e-06
Iter: 1607 loss: 2.53833832e-06
Iter: 1608 loss: 2.53616281e-06
Iter: 1609 loss: 2.54284487e-06
Iter: 1610 loss: 2.53538269e-06
Iter: 1611 loss: 2.53376e-06
Iter: 1612 loss: 2.53652843e-06
Iter: 1613 loss: 2.53305757e-06
Iter: 1614 loss: 2.53100029e-06
Iter: 1615 loss: 2.53876988e-06
Iter: 1616 loss: 2.53058715e-06
Iter: 1617 loss: 2.52898894e-06
Iter: 1618 loss: 2.53458484e-06
Iter: 1619 loss: 2.52858172e-06
Iter: 1620 loss: 2.52698169e-06
Iter: 1621 loss: 2.5267268e-06
Iter: 1622 loss: 2.52559494e-06
Iter: 1623 loss: 2.52357154e-06
Iter: 1624 loss: 2.54118777e-06
Iter: 1625 loss: 2.52351151e-06
Iter: 1626 loss: 2.52263726e-06
Iter: 1627 loss: 2.52258315e-06
Iter: 1628 loss: 2.52156315e-06
Iter: 1629 loss: 2.52177415e-06
Iter: 1630 loss: 2.52070367e-06
Iter: 1631 loss: 2.51974893e-06
Iter: 1632 loss: 2.5185409e-06
Iter: 1633 loss: 2.51853794e-06
Iter: 1634 loss: 2.51688152e-06
Iter: 1635 loss: 2.52478185e-06
Iter: 1636 loss: 2.51663414e-06
Iter: 1637 loss: 2.51542201e-06
Iter: 1638 loss: 2.51485199e-06
Iter: 1639 loss: 2.51423376e-06
Iter: 1640 loss: 2.51214715e-06
Iter: 1641 loss: 2.52608947e-06
Iter: 1642 loss: 2.51191068e-06
Iter: 1643 loss: 2.51061556e-06
Iter: 1644 loss: 2.5108825e-06
Iter: 1645 loss: 2.5096972e-06
Iter: 1646 loss: 2.50746189e-06
Iter: 1647 loss: 2.51031679e-06
Iter: 1648 loss: 2.50632388e-06
Iter: 1649 loss: 2.50445441e-06
Iter: 1650 loss: 2.51508845e-06
Iter: 1651 loss: 2.50425592e-06
Iter: 1652 loss: 2.50245694e-06
Iter: 1653 loss: 2.50387779e-06
Iter: 1654 loss: 2.50130438e-06
Iter: 1655 loss: 2.49960885e-06
Iter: 1656 loss: 2.51069127e-06
Iter: 1657 loss: 2.49943355e-06
Iter: 1658 loss: 2.49831601e-06
Iter: 1659 loss: 2.51388292e-06
Iter: 1660 loss: 2.49829827e-06
Iter: 1661 loss: 2.49701202e-06
Iter: 1662 loss: 2.50025096e-06
Iter: 1663 loss: 2.49651634e-06
Iter: 1664 loss: 2.49564732e-06
Iter: 1665 loss: 2.49433788e-06
Iter: 1666 loss: 2.49431287e-06
Iter: 1667 loss: 2.49283494e-06
Iter: 1668 loss: 2.49894583e-06
Iter: 1669 loss: 2.49247728e-06
Iter: 1670 loss: 2.49111326e-06
Iter: 1671 loss: 2.49134473e-06
Iter: 1672 loss: 2.49010486e-06
Iter: 1673 loss: 2.48809329e-06
Iter: 1674 loss: 2.4988085e-06
Iter: 1675 loss: 2.48784863e-06
Iter: 1676 loss: 2.486413e-06
Iter: 1677 loss: 2.48776746e-06
Iter: 1678 loss: 2.48565675e-06
Iter: 1679 loss: 2.48364154e-06
Iter: 1680 loss: 2.48677952e-06
Iter: 1681 loss: 2.48262745e-06
Iter: 1682 loss: 2.48117544e-06
Iter: 1683 loss: 2.49193727e-06
Iter: 1684 loss: 2.48101014e-06
Iter: 1685 loss: 2.47954949e-06
Iter: 1686 loss: 2.47850971e-06
Iter: 1687 loss: 2.47800017e-06
Iter: 1688 loss: 2.4762985e-06
Iter: 1689 loss: 2.49446839e-06
Iter: 1690 loss: 2.47632488e-06
Iter: 1691 loss: 2.47498588e-06
Iter: 1692 loss: 2.48009724e-06
Iter: 1693 loss: 2.47459911e-06
Iter: 1694 loss: 2.47310163e-06
Iter: 1695 loss: 2.49197728e-06
Iter: 1696 loss: 2.47306957e-06
Iter: 1697 loss: 2.47252046e-06
Iter: 1698 loss: 2.47124399e-06
Iter: 1699 loss: 2.48551532e-06
Iter: 1700 loss: 2.47105731e-06
Iter: 1701 loss: 2.46962827e-06
Iter: 1702 loss: 2.47679054e-06
Iter: 1703 loss: 2.46934451e-06
Iter: 1704 loss: 2.467998e-06
Iter: 1705 loss: 2.46792501e-06
Iter: 1706 loss: 2.46692571e-06
Iter: 1707 loss: 2.4654878e-06
Iter: 1708 loss: 2.47935645e-06
Iter: 1709 loss: 2.46530817e-06
Iter: 1710 loss: 2.46418108e-06
Iter: 1711 loss: 2.46412037e-06
Iter: 1712 loss: 2.46323316e-06
Iter: 1713 loss: 2.46135096e-06
Iter: 1714 loss: 2.46596755e-06
Iter: 1715 loss: 2.46053787e-06
Iter: 1716 loss: 2.45906e-06
Iter: 1717 loss: 2.46437935e-06
Iter: 1718 loss: 2.458675e-06
Iter: 1719 loss: 2.45684487e-06
Iter: 1720 loss: 2.4556557e-06
Iter: 1721 loss: 2.45495266e-06
Iter: 1722 loss: 2.45313254e-06
Iter: 1723 loss: 2.47707021e-06
Iter: 1724 loss: 2.4531173e-06
Iter: 1725 loss: 2.45171645e-06
Iter: 1726 loss: 2.45446063e-06
Iter: 1727 loss: 2.45114e-06
Iter: 1728 loss: 2.45020419e-06
Iter: 1729 loss: 2.45000956e-06
Iter: 1730 loss: 2.4495157e-06
Iter: 1731 loss: 2.44832563e-06
Iter: 1732 loss: 2.46121886e-06
Iter: 1733 loss: 2.44813737e-06
Iter: 1734 loss: 2.4467688e-06
Iter: 1735 loss: 2.44795137e-06
Iter: 1736 loss: 2.4459157e-06
Iter: 1737 loss: 2.44434545e-06
Iter: 1738 loss: 2.45153774e-06
Iter: 1739 loss: 2.44395255e-06
Iter: 1740 loss: 2.44259036e-06
Iter: 1741 loss: 2.44444163e-06
Iter: 1742 loss: 2.44182047e-06
Iter: 1743 loss: 2.44031958e-06
Iter: 1744 loss: 2.44377384e-06
Iter: 1745 loss: 2.43965815e-06
Iter: 1746 loss: 2.43785325e-06
Iter: 1747 loss: 2.44255352e-06
Iter: 1748 loss: 2.43712748e-06
Iter: 1749 loss: 2.4356807e-06
Iter: 1750 loss: 2.43863497e-06
Iter: 1751 loss: 2.43507475e-06
Iter: 1752 loss: 2.43319914e-06
Iter: 1753 loss: 2.43601767e-06
Iter: 1754 loss: 2.43221348e-06
Iter: 1755 loss: 2.43082377e-06
Iter: 1756 loss: 2.44016019e-06
Iter: 1757 loss: 2.43065824e-06
Iter: 1758 loss: 2.42923352e-06
Iter: 1759 loss: 2.42892656e-06
Iter: 1760 loss: 2.42806027e-06
Iter: 1761 loss: 2.4291403e-06
Iter: 1762 loss: 2.42717738e-06
Iter: 1763 loss: 2.42674923e-06
Iter: 1764 loss: 2.42572901e-06
Iter: 1765 loss: 2.43985187e-06
Iter: 1766 loss: 2.42570059e-06
Iter: 1767 loss: 2.4246217e-06
Iter: 1768 loss: 2.42354849e-06
Iter: 1769 loss: 2.4232495e-06
Iter: 1770 loss: 2.42144915e-06
Iter: 1771 loss: 2.43774844e-06
Iter: 1772 loss: 2.42137139e-06
Iter: 1773 loss: 2.42011447e-06
Iter: 1774 loss: 2.41998259e-06
Iter: 1775 loss: 2.41907674e-06
Iter: 1776 loss: 2.41732914e-06
Iter: 1777 loss: 2.42609553e-06
Iter: 1778 loss: 2.41711609e-06
Iter: 1779 loss: 2.41569069e-06
Iter: 1780 loss: 2.42253418e-06
Iter: 1781 loss: 2.4154424e-06
Iter: 1782 loss: 2.41435919e-06
Iter: 1783 loss: 2.41511862e-06
Iter: 1784 loss: 2.41368889e-06
Iter: 1785 loss: 2.41221392e-06
Iter: 1786 loss: 2.41418911e-06
Iter: 1787 loss: 2.41134694e-06
Iter: 1788 loss: 2.40998042e-06
Iter: 1789 loss: 2.41656653e-06
Iter: 1790 loss: 2.40974509e-06
Iter: 1791 loss: 2.40819872e-06
Iter: 1792 loss: 2.40863847e-06
Iter: 1793 loss: 2.40719896e-06
Iter: 1794 loss: 2.4092642e-06
Iter: 1795 loss: 2.40676445e-06
Iter: 1796 loss: 2.40634836e-06
Iter: 1797 loss: 2.40563054e-06
Iter: 1798 loss: 2.42207807e-06
Iter: 1799 loss: 2.40562122e-06
Iter: 1800 loss: 2.40457416e-06
Iter: 1801 loss: 2.40267696e-06
Iter: 1802 loss: 2.44071e-06
Iter: 1803 loss: 2.4027006e-06
Iter: 1804 loss: 2.40117606e-06
Iter: 1805 loss: 2.40116287e-06
Iter: 1806 loss: 2.39999031e-06
Iter: 1807 loss: 2.40076e-06
Iter: 1808 loss: 2.39934525e-06
Iter: 1809 loss: 2.39780138e-06
Iter: 1810 loss: 2.40128929e-06
Iter: 1811 loss: 2.39722658e-06
Iter: 1812 loss: 2.39597398e-06
Iter: 1813 loss: 2.40104487e-06
Iter: 1814 loss: 2.39571432e-06
Iter: 1815 loss: 2.39432666e-06
Iter: 1816 loss: 2.39756628e-06
Iter: 1817 loss: 2.39392466e-06
Iter: 1818 loss: 2.39235806e-06
Iter: 1819 loss: 2.39316159e-06
Iter: 1820 loss: 2.39135875e-06
Iter: 1821 loss: 2.38982761e-06
Iter: 1822 loss: 2.39922474e-06
Iter: 1823 loss: 2.38964731e-06
Iter: 1824 loss: 2.3880234e-06
Iter: 1825 loss: 2.38761959e-06
Iter: 1826 loss: 2.38663461e-06
Iter: 1827 loss: 2.38827715e-06
Iter: 1828 loss: 2.38611938e-06
Iter: 1829 loss: 2.3856087e-06
Iter: 1830 loss: 2.38498887e-06
Iter: 1831 loss: 2.38492703e-06
Iter: 1832 loss: 2.38406301e-06
Iter: 1833 loss: 2.38252414e-06
Iter: 1834 loss: 2.38254415e-06
Iter: 1835 loss: 2.3812172e-06
Iter: 1836 loss: 2.39459132e-06
Iter: 1837 loss: 2.38115126e-06
Iter: 1838 loss: 2.38001894e-06
Iter: 1839 loss: 2.38076927e-06
Iter: 1840 loss: 2.37935546e-06
Iter: 1841 loss: 2.37772929e-06
Iter: 1842 loss: 2.38190955e-06
Iter: 1843 loss: 2.37723862e-06
Iter: 1844 loss: 2.37611243e-06
Iter: 1845 loss: 2.37747145e-06
Iter: 1846 loss: 2.37552968e-06
Iter: 1847 loss: 2.37388326e-06
Iter: 1848 loss: 2.37624454e-06
Iter: 1849 loss: 2.37311792e-06
Iter: 1850 loss: 2.37175209e-06
Iter: 1851 loss: 2.3831235e-06
Iter: 1852 loss: 2.37170707e-06
Iter: 1853 loss: 2.37058384e-06
Iter: 1854 loss: 2.37109771e-06
Iter: 1855 loss: 2.36980213e-06
Iter: 1856 loss: 2.36832329e-06
Iter: 1857 loss: 2.36860865e-06
Iter: 1858 loss: 2.36724236e-06
Iter: 1859 loss: 2.36605547e-06
Iter: 1860 loss: 2.36602637e-06
Iter: 1861 loss: 2.36510095e-06
Iter: 1862 loss: 2.37906465e-06
Iter: 1863 loss: 2.36505457e-06
Iter: 1864 loss: 2.36441974e-06
Iter: 1865 loss: 2.36302139e-06
Iter: 1866 loss: 2.3753264e-06
Iter: 1867 loss: 2.36269489e-06
Iter: 1868 loss: 2.36143205e-06
Iter: 1869 loss: 2.36553569e-06
Iter: 1870 loss: 2.36102983e-06
Iter: 1871 loss: 2.3595444e-06
Iter: 1872 loss: 2.36197366e-06
Iter: 1873 loss: 2.35885955e-06
Iter: 1874 loss: 2.35760717e-06
Iter: 1875 loss: 2.37197628e-06
Iter: 1876 loss: 2.35758239e-06
Iter: 1877 loss: 2.35676021e-06
Iter: 1878 loss: 2.35633865e-06
Iter: 1879 loss: 2.35607035e-06
Iter: 1880 loss: 2.35461675e-06
Iter: 1881 loss: 2.35947664e-06
Iter: 1882 loss: 2.35423113e-06
Iter: 1883 loss: 2.3532773e-06
Iter: 1884 loss: 2.35527978e-06
Iter: 1885 loss: 2.35286529e-06
Iter: 1886 loss: 2.35152902e-06
Iter: 1887 loss: 2.35379139e-06
Iter: 1888 loss: 2.35084622e-06
Iter: 1889 loss: 2.34979962e-06
Iter: 1890 loss: 2.35577272e-06
Iter: 1891 loss: 2.34960089e-06
Iter: 1892 loss: 2.348625e-06
Iter: 1893 loss: 2.34993468e-06
Iter: 1894 loss: 2.34806453e-06
Iter: 1895 loss: 2.34800336e-06
Iter: 1896 loss: 2.34750041e-06
Iter: 1897 loss: 2.34702475e-06
Iter: 1898 loss: 2.34564413e-06
Iter: 1899 loss: 2.3569678e-06
Iter: 1900 loss: 2.34538584e-06
Iter: 1901 loss: 2.34409663e-06
Iter: 1902 loss: 2.34769686e-06
Iter: 1903 loss: 2.34368235e-06
Iter: 1904 loss: 2.34250956e-06
Iter: 1905 loss: 2.3425946e-06
Iter: 1906 loss: 2.34163372e-06
Iter: 1907 loss: 2.34072786e-06
Iter: 1908 loss: 2.3407149e-06
Iter: 1909 loss: 2.33991886e-06
Iter: 1910 loss: 2.33906508e-06
Iter: 1911 loss: 2.3389432e-06
Iter: 1912 loss: 2.33746505e-06
Iter: 1913 loss: 2.34613572e-06
Iter: 1914 loss: 2.3372852e-06
Iter: 1915 loss: 2.33640094e-06
Iter: 1916 loss: 2.3366681e-06
Iter: 1917 loss: 2.33579976e-06
Iter: 1918 loss: 2.33445962e-06
Iter: 1919 loss: 2.34093886e-06
Iter: 1920 loss: 2.33423975e-06
Iter: 1921 loss: 2.33335231e-06
Iter: 1922 loss: 2.33613764e-06
Iter: 1923 loss: 2.3330731e-06
Iter: 1924 loss: 2.33198944e-06
Iter: 1925 loss: 2.33125365e-06
Iter: 1926 loss: 2.33082505e-06
Iter: 1927 loss: 2.33069773e-06
Iter: 1928 loss: 2.33022706e-06
Iter: 1929 loss: 2.32956472e-06
Iter: 1930 loss: 2.33086826e-06
Iter: 1931 loss: 2.32921e-06
Iter: 1932 loss: 2.32867069e-06
Iter: 1933 loss: 2.32743082e-06
Iter: 1934 loss: 2.34379968e-06
Iter: 1935 loss: 2.32737921e-06
Iter: 1936 loss: 2.32587809e-06
Iter: 1937 loss: 2.32786579e-06
Iter: 1938 loss: 2.32516231e-06
Iter: 1939 loss: 2.32391631e-06
Iter: 1940 loss: 2.33322135e-06
Iter: 1941 loss: 2.32373077e-06
Iter: 1942 loss: 2.32266484e-06
Iter: 1943 loss: 2.32583625e-06
Iter: 1944 loss: 2.32240382e-06
Iter: 1945 loss: 2.32119055e-06
Iter: 1946 loss: 2.32675484e-06
Iter: 1947 loss: 2.32092043e-06
Iter: 1948 loss: 2.32017101e-06
Iter: 1949 loss: 2.31935451e-06
Iter: 1950 loss: 2.31914964e-06
Iter: 1951 loss: 2.3176442e-06
Iter: 1952 loss: 2.32852358e-06
Iter: 1953 loss: 2.31745184e-06
Iter: 1954 loss: 2.31647778e-06
Iter: 1955 loss: 2.31783361e-06
Iter: 1956 loss: 2.31594413e-06
Iter: 1957 loss: 2.31456306e-06
Iter: 1958 loss: 2.31677836e-06
Iter: 1959 loss: 2.31396052e-06
Iter: 1960 loss: 2.31305421e-06
Iter: 1961 loss: 2.31302715e-06
Iter: 1962 loss: 2.31256627e-06
Iter: 1963 loss: 2.31253489e-06
Iter: 1964 loss: 2.31218291e-06
Iter: 1965 loss: 2.31109152e-06
Iter: 1966 loss: 2.31322429e-06
Iter: 1967 loss: 2.31025388e-06
Iter: 1968 loss: 2.30896899e-06
Iter: 1969 loss: 2.32285402e-06
Iter: 1970 loss: 2.30897899e-06
Iter: 1971 loss: 2.30793444e-06
Iter: 1972 loss: 2.3067139e-06
Iter: 1973 loss: 2.30664091e-06
Iter: 1974 loss: 2.30544492e-06
Iter: 1975 loss: 2.30537648e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi3
+ date
Wed Oct 21 17:22:55 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/300_300_300_1 --function f1 --psi -2 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c586e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c5816a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c5a82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c5a8d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c4d7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c4d7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c3dbc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c476950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c476488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c45f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c3bea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c31de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c317488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c317c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c3868c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c2ea730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c2ea400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c28bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5149068950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5149093f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5149053620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f51490456a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c2668c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148fc4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148fca598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148fcabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f51490219d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5149009950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f51490096a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148f10620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148f10598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148e707b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148e70378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148f36510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148e52a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148e1e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.023274278
test_loss: 0.022741927
train_loss: 0.011016102
test_loss: 0.011131989
train_loss: 0.007237138
test_loss: 0.00863787
train_loss: 0.00669277
test_loss: 0.00827559
train_loss: 0.0063235504
test_loss: 0.0075984863
train_loss: 0.0054894416
test_loss: 0.007980763
train_loss: 0.005630607
test_loss: 0.00738827
train_loss: 0.0055614808
test_loss: 0.0075218137
train_loss: 0.0049719648
test_loss: 0.007246999
train_loss: 0.005494129
test_loss: 0.0069656675
train_loss: 0.0054546795
test_loss: 0.007325562
train_loss: 0.0057539605
test_loss: 0.0073487526
train_loss: 0.0052466495
test_loss: 0.007100255
train_loss: 0.0054461746
test_loss: 0.0070541385
train_loss: 0.005377919
test_loss: 0.007025024
train_loss: 0.005231712
test_loss: 0.007183807
train_loss: 0.005137408
test_loss: 0.0070444113
train_loss: 0.0052017514
test_loss: 0.007292895
train_loss: 0.00515196
test_loss: 0.007022777
train_loss: 0.004486874
test_loss: 0.0068863295
train_loss: 0.005010705
test_loss: 0.006937164
train_loss: 0.00467307
test_loss: 0.0068160202
train_loss: 0.005416968
test_loss: 0.0068338783
train_loss: 0.0051068715
test_loss: 0.0073070433
train_loss: 0.005181445
test_loss: 0.0074662496
train_loss: 0.004660772
test_loss: 0.0069534257
train_loss: 0.0052830707
test_loss: 0.0067651607
train_loss: 0.0049985754
test_loss: 0.0068351687
train_loss: 0.005303207
test_loss: 0.0069389516
train_loss: 0.004712505
test_loss: 0.0067591933
train_loss: 0.0050300863
test_loss: 0.006794292
train_loss: 0.0047129192
test_loss: 0.0068841903
train_loss: 0.004781775
test_loss: 0.0068014115
train_loss: 0.004872055
test_loss: 0.0064585614
train_loss: 0.005119782
test_loss: 0.006602534
train_loss: 0.0045707575
test_loss: 0.006790493
train_loss: 0.004706994
test_loss: 0.00673415
train_loss: 0.004963182
test_loss: 0.007206591
train_loss: 0.004844828
test_loss: 0.0068671573
train_loss: 0.004461536
test_loss: 0.006575636
train_loss: 0.0046101385
test_loss: 0.0067959703
train_loss: 0.0047103344
test_loss: 0.0065554637
train_loss: 0.0043342984
test_loss: 0.0066061313
train_loss: 0.004813781
test_loss: 0.00659571
train_loss: 0.005066055
test_loss: 0.006623543
train_loss: 0.0044940803
test_loss: 0.006463256
train_loss: 0.0043308455
test_loss: 0.0067194174
train_loss: 0.004372559
test_loss: 0.0064746984
train_loss: 0.005247499
test_loss: 0.006981028
train_loss: 0.0046036066
test_loss: 0.006463317
train_loss: 0.0046578846
test_loss: 0.0065774303
train_loss: 0.004854331
test_loss: 0.0066086967
train_loss: 0.0045374893
test_loss: 0.0065676332
train_loss: 0.004620526
test_loss: 0.006793562
train_loss: 0.0047090263
test_loss: 0.0066919453
train_loss: 0.0045049977
test_loss: 0.006646032
train_loss: 0.004637533
test_loss: 0.006766746
train_loss: 0.0048882626
test_loss: 0.0066429763
train_loss: 0.0042773015
test_loss: 0.0065557845
train_loss: 0.004458382
test_loss: 0.006498665
train_loss: 0.004487479
test_loss: 0.0063764127
train_loss: 0.0044759153
test_loss: 0.0064521357
train_loss: 0.0042723706
test_loss: 0.0063320743
train_loss: 0.0046503083
test_loss: 0.0064803297
train_loss: 0.004481983
test_loss: 0.006364342
train_loss: 0.004561622
test_loss: 0.0065209754
train_loss: 0.004916268
test_loss: 0.0066744005
train_loss: 0.004710653
test_loss: 0.00646358
train_loss: 0.0046501067
test_loss: 0.0064538666
train_loss: 0.0048516276
test_loss: 0.0064590233
train_loss: 0.0049347593
test_loss: 0.0065804627
train_loss: 0.0046203705
test_loss: 0.006448767
train_loss: 0.004526497
test_loss: 0.0066846106
train_loss: 0.0043534073
test_loss: 0.006361062
train_loss: 0.004646114
test_loss: 0.006458958
train_loss: 0.0044172783
test_loss: 0.006430609
train_loss: 0.004500125
test_loss: 0.006376879
train_loss: 0.004466825
test_loss: 0.0066218185
train_loss: 0.0046188557
test_loss: 0.0064380853
train_loss: 0.0046704207
test_loss: 0.006505174
train_loss: 0.004402871
test_loss: 0.006397037
train_loss: 0.0044437884
test_loss: 0.0066409768
train_loss: 0.0050171595
test_loss: 0.006668797
train_loss: 0.0045522745
test_loss: 0.0066104885
train_loss: 0.0046144733
test_loss: 0.006511345
train_loss: 0.0044273306
test_loss: 0.006413293
train_loss: 0.004712229
test_loss: 0.0065944297
train_loss: 0.0042547355
test_loss: 0.006511032
train_loss: 0.0045766253
test_loss: 0.0063646105
train_loss: 0.004661373
test_loss: 0.0064931316
train_loss: 0.0045243865
test_loss: 0.0065061445
train_loss: 0.004556702
test_loss: 0.0062382105
train_loss: 0.0047745686
test_loss: 0.006302165
train_loss: 0.0045129787
test_loss: 0.006441755
train_loss: 0.004455127
test_loss: 0.00632644
train_loss: 0.0048283357
test_loss: 0.0064420793
train_loss: 0.0043621208
test_loss: 0.0064337626
train_loss: 0.004557842
test_loss: 0.0064302706
train_loss: 0.005098705
test_loss: 0.006634989
train_loss: 0.004756579
test_loss: 0.0065042395
train_loss: 0.0050234376
test_loss: 0.006672005
train_loss: 0.0048414716
test_loss: 0.006437649
train_loss: 0.004553241
test_loss: 0.0063773924
train_loss: 0.0042534145
test_loss: 0.0064903875
train_loss: 0.0044770716
test_loss: 0.0066803405
train_loss: 0.004615255
test_loss: 0.006523812
train_loss: 0.004474621
test_loss: 0.00640475
train_loss: 0.0044237636
test_loss: 0.006371678
train_loss: 0.0039243856
test_loss: 0.0061546103
train_loss: 0.0041540675
test_loss: 0.006368658
train_loss: 0.0041556368
test_loss: 0.006204418
train_loss: 0.0046618865
test_loss: 0.006446108
train_loss: 0.0047622225
test_loss: 0.006764238
train_loss: 0.0044064373
test_loss: 0.00665504
train_loss: 0.0044547617
test_loss: 0.0064585255
train_loss: 0.0043722484
test_loss: 0.0065215235
train_loss: 0.004416002
test_loss: 0.006228968
train_loss: 0.0042170417
test_loss: 0.006488044
train_loss: 0.0044837166
test_loss: 0.006409122
train_loss: 0.004241586
test_loss: 0.0063505895
train_loss: 0.0045800563
test_loss: 0.006217993
train_loss: 0.004341623
test_loss: 0.0063992455
train_loss: 0.0041961162
test_loss: 0.0063288347
train_loss: 0.004318118
test_loss: 0.006037308
train_loss: 0.004680037
test_loss: 0.006326096
train_loss: 0.0040618377
test_loss: 0.0061345547
train_loss: 0.0040185945
test_loss: 0.006136968
train_loss: 0.0042478074
test_loss: 0.0061988267
train_loss: 0.0044495636
test_loss: 0.006312143
train_loss: 0.0040871636
test_loss: 0.006139549
train_loss: 0.0047191717
test_loss: 0.0062272977
train_loss: 0.004378955
test_loss: 0.0062422957
train_loss: 0.0045093633
test_loss: 0.0069970023
train_loss: 0.0045594205
test_loss: 0.0064982534
train_loss: 0.004786635
test_loss: 0.0063172174
train_loss: 0.004951894
test_loss: 0.006377333
train_loss: 0.004421142
test_loss: 0.006406465
train_loss: 0.0041079554
test_loss: 0.006406048
train_loss: 0.0042857514
test_loss: 0.0063897897
train_loss: 0.0048244726
test_loss: 0.006499662
train_loss: 0.003888708
test_loss: 0.00608995
train_loss: 0.0046189027
test_loss: 0.0064933533
train_loss: 0.0045200055
test_loss: 0.006258481
train_loss: 0.004076706
test_loss: 0.0060041933
train_loss: 0.004146575
test_loss: 0.00597734
train_loss: 0.004411599
test_loss: 0.0062458892
train_loss: 0.004559106
test_loss: 0.006305991
train_loss: 0.0040500164
test_loss: 0.006108821
train_loss: 0.004393909
test_loss: 0.006094694
train_loss: 0.004432248
test_loss: 0.006318965
train_loss: 0.004290299
test_loss: 0.0061243847
train_loss: 0.0044494844
test_loss: 0.0067322515
train_loss: 0.0044277017
test_loss: 0.006309947
train_loss: 0.004097999
test_loss: 0.0063618743
train_loss: 0.004406574
test_loss: 0.0061933417
train_loss: 0.0045464444
test_loss: 0.0063677523
train_loss: 0.0043147057
test_loss: 0.006323362
train_loss: 0.004258401
test_loss: 0.0064820987
train_loss: 0.0042521944
test_loss: 0.0061981063
train_loss: 0.004007277
test_loss: 0.0060884343
train_loss: 0.0040784674
test_loss: 0.006343145
train_loss: 0.004330186
test_loss: 0.006291225
train_loss: 0.004426053
test_loss: 0.006380592
train_loss: 0.0046434435
test_loss: 0.006265145
train_loss: 0.0044252556
test_loss: 0.006589593
train_loss: 0.0043629175
test_loss: 0.0062448587
train_loss: 0.004640965
test_loss: 0.006360286
train_loss: 0.004363115
test_loss: 0.0062743695
train_loss: 0.0041580987
test_loss: 0.0061076507
train_loss: 0.0041136392
test_loss: 0.006499244
train_loss: 0.0043342314
test_loss: 0.0063045467
train_loss: 0.0046752114
test_loss: 0.0065598744
train_loss: 0.004434381
test_loss: 0.006503231
train_loss: 0.004314828
test_loss: 0.0059007304
train_loss: 0.004587422
test_loss: 0.0062365965
train_loss: 0.0042390567
test_loss: 0.0061030025
train_loss: 0.0040416718
test_loss: 0.006188798
train_loss: 0.0042457753
test_loss: 0.006303213
train_loss: 0.004217393
test_loss: 0.006037131
train_loss: 0.004131618
test_loss: 0.0061463173
train_loss: 0.0038752172
test_loss: 0.006038052
train_loss: 0.004332521
test_loss: 0.005950358
train_loss: 0.0042489953
test_loss: 0.0064867036
train_loss: 0.004246288
test_loss: 0.0061615203
train_loss: 0.004632417
test_loss: 0.0062580234
train_loss: 0.003999859
test_loss: 0.0061131986
train_loss: 0.0038962855
test_loss: 0.0060268124
train_loss: 0.0043012933
test_loss: 0.0060378984
train_loss: 0.0043346174
test_loss: 0.00596728
train_loss: 0.003835732
test_loss: 0.006071195
train_loss: 0.0044549396
test_loss: 0.0061676055
train_loss: 0.004627785
test_loss: 0.0060868845
train_loss: 0.004309618
test_loss: 0.006231551
train_loss: 0.004485209
test_loss: 0.006375782
train_loss: 0.0042976122
test_loss: 0.006226824
train_loss: 0.0043945694
test_loss: 0.006324826
train_loss: 0.0043114237
test_loss: 0.0063206665
train_loss: 0.0046308674
test_loss: 0.006158893
train_loss: 0.0041257935
test_loss: 0.006025042
train_loss: 0.0042607062
test_loss: 0.0059560225
train_loss: 0.0041487394
test_loss: 0.006008136
train_loss: 0.004327031
test_loss: 0.0062326565
train_loss: 0.004211935
test_loss: 0.006253518
train_loss: 0.004190131
test_loss: 0.0060115145
train_loss: 0.004254572
test_loss: 0.0059942147
train_loss: 0.0041442355
test_loss: 0.00622698
train_loss: 0.0040874993
test_loss: 0.0061171316
train_loss: 0.0041730916
test_loss: 0.006054401
train_loss: 0.0041804416
test_loss: 0.0060066986
train_loss: 0.004375591
test_loss: 0.0060012867
train_loss: 0.004568474
test_loss: 0.0061228205
train_loss: 0.0049721436
test_loss: 0.006270292
train_loss: 0.00397065
test_loss: 0.0059287064
train_loss: 0.0042081173
test_loss: 0.006226411
train_loss: 0.0045294934
test_loss: 0.0062728943
train_loss: 0.0038017426
test_loss: 0.006029176
train_loss: 0.0044865035
test_loss: 0.0062164613
train_loss: 0.0039739637
test_loss: 0.0058133868
train_loss: 0.0038373496
test_loss: 0.0060193273
train_loss: 0.004134811
test_loss: 0.006133853
train_loss: 0.004144586
test_loss: 0.0063308706
train_loss: 0.0049006026
test_loss: 0.0060995533
train_loss: 0.004210491
test_loss: 0.0063182283
train_loss: 0.003879358
test_loss: 0.0060218107
train_loss: 0.0038361552
test_loss: 0.00601039
train_loss: 0.00398235
test_loss: 0.0060054804
train_loss: 0.004063302
test_loss: 0.005919123
train_loss: 0.004019488
test_loss: 0.006133278
train_loss: 0.003930576
test_loss: 0.0061373035
train_loss: 0.004295891
test_loss: 0.0060498663
train_loss: 0.0040527936
test_loss: 0.0060283546
train_loss: 0.0038701007
test_loss: 0.006002959
train_loss: 0.0041414397
test_loss: 0.005915034
train_loss: 0.0042049913
test_loss: 0.005911438
train_loss: 0.0039477777
test_loss: 0.0059884526
train_loss: 0.0043564797
test_loss: 0.0063728117
train_loss: 0.003818916
test_loss: 0.006118107
train_loss: 0.004101805
test_loss: 0.005883738
train_loss: 0.0038680218
test_loss: 0.006076495
train_loss: 0.004051719
test_loss: 0.0060325987
train_loss: 0.004240302
test_loss: 0.0059546637
train_loss: 0.004111926
test_loss: 0.00588666
train_loss: 0.0038394667
test_loss: 0.0059240665
train_loss: 0.0042701764
test_loss: 0.0058566486
train_loss: 0.0039451527
test_loss: 0.0058422796
train_loss: 0.0042222114
test_loss: 0.0059994333
train_loss: 0.004381518
test_loss: 0.0063201627
train_loss: 0.0043047406
test_loss: 0.00599874
train_loss: 0.004184584
test_loss: 0.005708127
train_loss: 0.0038933172
test_loss: 0.0059247646
train_loss: 0.004256942
test_loss: 0.0059679537
train_loss: 0.0038825325
test_loss: 0.006148785
train_loss: 0.0038400888
test_loss: 0.0059298333
train_loss: 0.00370689
test_loss: 0.005775947
train_loss: 0.004238594
test_loss: 0.0058232704
train_loss: 0.0036363448
test_loss: 0.005835651
train_loss: 0.0038101063
test_loss: 0.0058332128
train_loss: 0.0041655595
test_loss: 0.0059060897
train_loss: 0.003899739
test_loss: 0.0057897517
train_loss: 0.004076987
test_loss: 0.0059125004
train_loss: 0.0039868075
test_loss: 0.0059496504
train_loss: 0.0045133866
test_loss: 0.0067509986
train_loss: 0.004984576
test_loss: 0.0063970145
train_loss: 0.0039035275
test_loss: 0.00604958
train_loss: 0.0041823536
test_loss: 0.0060149026
train_loss: 0.004244111
test_loss: 0.0059183734
train_loss: 0.0043804427
test_loss: 0.0059089046
train_loss: 0.004262109
test_loss: 0.0059868465
train_loss: 0.0041180486
test_loss: 0.006088058
train_loss: 0.00394344
test_loss: 0.005899329
train_loss: 0.0039306907
test_loss: 0.0059667085
train_loss: 0.0040308395
test_loss: 0.006053438
train_loss: 0.0043964433
test_loss: 0.0061469013
train_loss: 0.0039885994
test_loss: 0.006012313
train_loss: 0.0038977186
test_loss: 0.006051822
train_loss: 0.004106966
test_loss: 0.006002873
train_loss: 0.003934674
test_loss: 0.0060960036
train_loss: 0.0045544864
test_loss: 0.0062145097
train_loss: 0.004061976
test_loss: 0.006034311
train_loss: 0.0040088836
test_loss: 0.0058977045
train_loss: 0.0039155623
test_loss: 0.0062123
train_loss: 0.0042668805
test_loss: 0.0057946476
train_loss: 0.004113981
test_loss: 0.006120807
train_loss: 0.0039295433
test_loss: 0.005762223
train_loss: 0.0038385172
test_loss: 0.005783903
train_loss: 0.0041552065
test_loss: 0.005783073
train_loss: 0.003909076
test_loss: 0.005780621
train_loss: 0.0040505086
test_loss: 0.006062382
train_loss: 0.0041383086
test_loss: 0.006219194
train_loss: 0.0040889597
test_loss: 0.006148821
train_loss: 0.0039259624
test_loss: 0.0057097147
train_loss: 0.0041821906
test_loss: 0.005928602
train_loss: 0.0036684056
test_loss: 0.0057617463
train_loss: 0.0041007414
test_loss: 0.0058491407
train_loss: 0.004103421
test_loss: 0.005874634
train_loss: 0.0038014606
test_loss: 0.0060663493
train_loss: 0.0038913202
test_loss: 0.0058849053
train_loss: 0.0037313648
test_loss: 0.006165075
train_loss: 0.0039828233
test_loss: 0.005926967
train_loss: 0.0035208047
test_loss: 0.005836604
train_loss: 0.0043361587
test_loss: 0.005990328
train_loss: 0.003952171
test_loss: 0.0060878186
train_loss: 0.004141991
test_loss: 0.005995923
train_loss: 0.0044777826
test_loss: 0.0060110274
train_loss: 0.0040074536
test_loss: 0.005959775
train_loss: 0.004283443
test_loss: 0.00587918
train_loss: 0.0043872697
test_loss: 0.0059535517
train_loss: 0.0040059756
test_loss: 0.0058733076
train_loss: 0.0038946318
test_loss: 0.0060927235
train_loss: 0.004273352
test_loss: 0.005984653
train_loss: 0.003870892
test_loss: 0.0059533934
train_loss: 0.0040592826
test_loss: 0.0059955707
train_loss: 0.0037760418
test_loss: 0.0057921223
train_loss: 0.0039026546
test_loss: 0.0058912314
train_loss: 0.004454852
test_loss: 0.0058740606
train_loss: 0.0042309538
test_loss: 0.005995061
train_loss: 0.004103891
test_loss: 0.0058413963
train_loss: 0.004028093
test_loss: 0.005774052
train_loss: 0.004016461
test_loss: 0.0057736686
train_loss: 0.0036622705
test_loss: 0.005700265
train_loss: 0.0041208426
test_loss: 0.0062984517
train_loss: 0.004515762
test_loss: 0.006544991
train_loss: 0.004288921
test_loss: 0.0059803394
train_loss: 0.0040905178
test_loss: 0.00611739
train_loss: 0.0039065615
test_loss: 0.005943153
train_loss: 0.004164214
test_loss: 0.006247753
train_loss: 0.0041499287
test_loss: 0.00631256
train_loss: 0.003936021
test_loss: 0.0056389817
train_loss: 0.0046825903
test_loss: 0.005869362
train_loss: 0.0040363586
test_loss: 0.006106693
train_loss: 0.004233453
test_loss: 0.0058728573
train_loss: 0.0040657218
test_loss: 0.0058809714
train_loss: 0.0041701132
test_loss: 0.005837299
train_loss: 0.0038312767
test_loss: 0.006003352
train_loss: 0.0037086504
test_loss: 0.0056173825
train_loss: 0.0036482757
test_loss: 0.005992506
train_loss: 0.0038365023
test_loss: 0.0059750103
train_loss: 0.004015653
test_loss: 0.006119553
train_loss: 0.0042375405
test_loss: 0.0060779136
train_loss: 0.004181445
test_loss: 0.0059459987
train_loss: 0.0039259256
test_loss: 0.0060756323
train_loss: 0.0042818836/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0060788924
train_loss: 0.0037921006
test_loss: 0.005588461
train_loss: 0.004213415
test_loss: 0.0061511593
train_loss: 0.0044971425
test_loss: 0.0059810244
train_loss: 0.0038983105
test_loss: 0.0058858725
train_loss: 0.003746882
test_loss: 0.0057601365
train_loss: 0.003954489
test_loss: 0.0064087803
train_loss: 0.0039617177
test_loss: 0.005827063
train_loss: 0.0037138886
test_loss: 0.005895002
train_loss: 0.004028758
test_loss: 0.0057514636
train_loss: 0.0047282055
test_loss: 0.006102538
train_loss: 0.0037240651
test_loss: 0.0057686716
train_loss: 0.004064376
test_loss: 0.005579942
train_loss: 0.0038553746
test_loss: 0.005866141
train_loss: 0.00420453
test_loss: 0.005842514
train_loss: 0.003913365
test_loss: 0.0057246117
train_loss: 0.003897862
test_loss: 0.0057510505
train_loss: 0.0038156093
test_loss: 0.005723284
train_loss: 0.003960836
test_loss: 0.005815154
train_loss: 0.0038023097
test_loss: 0.005725096
train_loss: 0.004456153
test_loss: 0.0057274117
train_loss: 0.003907612
test_loss: 0.005931499
train_loss: 0.0038707058
test_loss: 0.0057113254
train_loss: 0.0040185517
test_loss: 0.006092697
train_loss: 0.004075916
test_loss: 0.0060234345
train_loss: 0.003535943
test_loss: 0.0059057525
train_loss: 0.0037525787
test_loss: 0.0060548116
train_loss: 0.003885187
test_loss: 0.005802465
train_loss: 0.0039900267
test_loss: 0.0059601646
train_loss: 0.003859731
test_loss: 0.0058938446
train_loss: 0.0036574272
test_loss: 0.0058618663
train_loss: 0.0037531955
test_loss: 0.005879546
train_loss: 0.0041510863
test_loss: 0.005647965
train_loss: 0.0037385807
test_loss: 0.005968981
train_loss: 0.004129305
test_loss: 0.005900088
train_loss: 0.0039458424
test_loss: 0.005858778
train_loss: 0.00371265
test_loss: 0.0058314097
train_loss: 0.003814601
test_loss: 0.005745034
train_loss: 0.004059337
test_loss: 0.0058571827
train_loss: 0.0034215723
test_loss: 0.005597347
train_loss: 0.0038604466
test_loss: 0.0057204957
train_loss: 0.0041886074
test_loss: 0.005880706
train_loss: 0.003749387
test_loss: 0.005785676
train_loss: 0.0037365698
test_loss: 0.005961411
train_loss: 0.003514304
test_loss: 0.0059559913
train_loss: 0.0041860403
test_loss: 0.0058443043
train_loss: 0.004007383
test_loss: 0.0058370577
train_loss: 0.0046030986
test_loss: 0.005848021
train_loss: 0.0042375275
test_loss: 0.005972908
train_loss: 0.0041747894
test_loss: 0.005984566
train_loss: 0.00392819
test_loss: 0.0058893617
train_loss: 0.0039774114
test_loss: 0.00583842
train_loss: 0.004034701
test_loss: 0.00623622
train_loss: 0.0039224057
test_loss: 0.0060779
train_loss: 0.003661328
test_loss: 0.00606903
train_loss: 0.0035577817
test_loss: 0.005498944
train_loss: 0.0035283598
test_loss: 0.0055509615
train_loss: 0.0041844305
test_loss: 0.005940107
train_loss: 0.0038530626
test_loss: 0.0058069057
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi3/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49bbb4e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49bb966a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49babb1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49bb96620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49bb27730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49bb27ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49ba4f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49ba20bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49ba20c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd49b9d5c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd44f4eb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd44f503f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd44f503a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd44f4c1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd44f4c1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd44f464510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd44f49d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd44f45dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd44f407950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd44f40af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4284a08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd428460598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd42848f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd428442840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd42848f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd428421840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4283ef2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4283d39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4283d37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd428384378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd428332378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4282d8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4282d8268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4282d8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4282969d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd4282fc6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.56017393e-05
Iter: 2 loss: 2.18293662e-05
Iter: 3 loss: 5.67333336e-05
Iter: 4 loss: 2.16709759e-05
Iter: 5 loss: 2.00288086e-05
Iter: 6 loss: 2.00242484e-05
Iter: 7 loss: 1.92434964e-05
Iter: 8 loss: 1.73657318e-05
Iter: 9 loss: 3.77617544e-05
Iter: 10 loss: 1.7170245e-05
Iter: 11 loss: 1.52321572e-05
Iter: 12 loss: 3.02391272e-05
Iter: 13 loss: 1.50906026e-05
Iter: 14 loss: 1.40475604e-05
Iter: 15 loss: 1.64486901e-05
Iter: 16 loss: 1.36582112e-05
Iter: 17 loss: 1.25920424e-05
Iter: 18 loss: 1.36209965e-05
Iter: 19 loss: 1.19857377e-05
Iter: 20 loss: 1.13273527e-05
Iter: 21 loss: 1.99336355e-05
Iter: 22 loss: 1.13234055e-05
Iter: 23 loss: 1.07299365e-05
Iter: 24 loss: 1.01830801e-05
Iter: 25 loss: 1.00395537e-05
Iter: 26 loss: 9.44326803e-06
Iter: 27 loss: 1.75221849e-05
Iter: 28 loss: 9.44083604e-06
Iter: 29 loss: 9.00938085e-06
Iter: 30 loss: 8.4566318e-06
Iter: 31 loss: 8.41695055e-06
Iter: 32 loss: 8.07959441e-06
Iter: 33 loss: 8.02356e-06
Iter: 34 loss: 7.80304163e-06
Iter: 35 loss: 7.37988648e-06
Iter: 36 loss: 1.62546603e-05
Iter: 37 loss: 7.37764412e-06
Iter: 38 loss: 7.49236915e-06
Iter: 39 loss: 7.21719425e-06
Iter: 40 loss: 7.05639513e-06
Iter: 41 loss: 7.12043084e-06
Iter: 42 loss: 6.94515347e-06
Iter: 43 loss: 6.78076958e-06
Iter: 44 loss: 6.53176812e-06
Iter: 45 loss: 6.52716335e-06
Iter: 46 loss: 6.28831549e-06
Iter: 47 loss: 8.54265727e-06
Iter: 48 loss: 6.27889494e-06
Iter: 49 loss: 6.08072924e-06
Iter: 50 loss: 6.75344882e-06
Iter: 51 loss: 6.02747832e-06
Iter: 52 loss: 5.87859267e-06
Iter: 53 loss: 6.84865836e-06
Iter: 54 loss: 5.86246233e-06
Iter: 55 loss: 5.74126534e-06
Iter: 56 loss: 6.26282872e-06
Iter: 57 loss: 5.71609871e-06
Iter: 58 loss: 5.62754667e-06
Iter: 59 loss: 5.65581649e-06
Iter: 60 loss: 5.5644764e-06
Iter: 61 loss: 5.42064572e-06
Iter: 62 loss: 5.59433647e-06
Iter: 63 loss: 5.345315e-06
Iter: 64 loss: 5.21736729e-06
Iter: 65 loss: 6.35530432e-06
Iter: 66 loss: 5.21122865e-06
Iter: 67 loss: 5.10398968e-06
Iter: 68 loss: 5.04288164e-06
Iter: 69 loss: 4.99683802e-06
Iter: 70 loss: 4.89016475e-06
Iter: 71 loss: 6.37114545e-06
Iter: 72 loss: 4.89001195e-06
Iter: 73 loss: 4.85767077e-06
Iter: 74 loss: 4.85342798e-06
Iter: 75 loss: 4.81134975e-06
Iter: 76 loss: 4.70942905e-06
Iter: 77 loss: 5.80866072e-06
Iter: 78 loss: 4.69846054e-06
Iter: 79 loss: 4.62296748e-06
Iter: 80 loss: 5.07692357e-06
Iter: 81 loss: 4.61349691e-06
Iter: 82 loss: 4.55720965e-06
Iter: 83 loss: 4.46488957e-06
Iter: 84 loss: 4.46433205e-06
Iter: 85 loss: 4.39028372e-06
Iter: 86 loss: 5.19063315e-06
Iter: 87 loss: 4.38853885e-06
Iter: 88 loss: 4.32057323e-06
Iter: 89 loss: 4.74379885e-06
Iter: 90 loss: 4.31264925e-06
Iter: 91 loss: 4.26950646e-06
Iter: 92 loss: 4.53512257e-06
Iter: 93 loss: 4.26413499e-06
Iter: 94 loss: 4.22370522e-06
Iter: 95 loss: 4.15913155e-06
Iter: 96 loss: 4.15861132e-06
Iter: 97 loss: 4.09252243e-06
Iter: 98 loss: 4.8637803e-06
Iter: 99 loss: 4.09175e-06
Iter: 100 loss: 4.04706589e-06
Iter: 101 loss: 4.14489341e-06
Iter: 102 loss: 4.03013564e-06
Iter: 103 loss: 3.9894353e-06
Iter: 104 loss: 4.11640303e-06
Iter: 105 loss: 3.97775329e-06
Iter: 106 loss: 3.93457549e-06
Iter: 107 loss: 4.06719391e-06
Iter: 108 loss: 3.92195579e-06
Iter: 109 loss: 3.88534772e-06
Iter: 110 loss: 3.8847229e-06
Iter: 111 loss: 3.87370437e-06
Iter: 112 loss: 3.8410235e-06
Iter: 113 loss: 3.96030737e-06
Iter: 114 loss: 3.82672715e-06
Iter: 115 loss: 3.77583092e-06
Iter: 116 loss: 4.018651e-06
Iter: 117 loss: 3.76693561e-06
Iter: 118 loss: 3.73353669e-06
Iter: 119 loss: 3.73715875e-06
Iter: 120 loss: 3.70774364e-06
Iter: 121 loss: 3.67212397e-06
Iter: 122 loss: 4.21664663e-06
Iter: 123 loss: 3.67202915e-06
Iter: 124 loss: 3.65139977e-06
Iter: 125 loss: 3.71133206e-06
Iter: 126 loss: 3.64504854e-06
Iter: 127 loss: 3.615688e-06
Iter: 128 loss: 3.63194545e-06
Iter: 129 loss: 3.5964938e-06
Iter: 130 loss: 3.57140857e-06
Iter: 131 loss: 3.64508401e-06
Iter: 132 loss: 3.56382679e-06
Iter: 133 loss: 3.53317137e-06
Iter: 134 loss: 3.56235046e-06
Iter: 135 loss: 3.51575022e-06
Iter: 136 loss: 3.48867661e-06
Iter: 137 loss: 3.76249386e-06
Iter: 138 loss: 3.48778849e-06
Iter: 139 loss: 3.46992215e-06
Iter: 140 loss: 3.47754599e-06
Iter: 141 loss: 3.45763351e-06
Iter: 142 loss: 3.44075602e-06
Iter: 143 loss: 3.43948568e-06
Iter: 144 loss: 3.42609428e-06
Iter: 145 loss: 3.40744691e-06
Iter: 146 loss: 3.40678207e-06
Iter: 147 loss: 3.38945824e-06
Iter: 148 loss: 3.36294261e-06
Iter: 149 loss: 3.36236735e-06
Iter: 150 loss: 3.33232606e-06
Iter: 151 loss: 3.59632145e-06
Iter: 152 loss: 3.33067032e-06
Iter: 153 loss: 3.30819466e-06
Iter: 154 loss: 3.35923278e-06
Iter: 155 loss: 3.29977661e-06
Iter: 156 loss: 3.27838734e-06
Iter: 157 loss: 3.30303988e-06
Iter: 158 loss: 3.2670066e-06
Iter: 159 loss: 3.2402138e-06
Iter: 160 loss: 3.51638323e-06
Iter: 161 loss: 3.2394023e-06
Iter: 162 loss: 3.22450956e-06
Iter: 163 loss: 3.25947826e-06
Iter: 164 loss: 3.21917241e-06
Iter: 165 loss: 3.20279287e-06
Iter: 166 loss: 3.18607545e-06
Iter: 167 loss: 3.18295884e-06
Iter: 168 loss: 3.16663522e-06
Iter: 169 loss: 3.16659089e-06
Iter: 170 loss: 3.15082639e-06
Iter: 171 loss: 3.13173132e-06
Iter: 172 loss: 3.12987686e-06
Iter: 173 loss: 3.13366672e-06
Iter: 174 loss: 3.11993836e-06
Iter: 175 loss: 3.11141662e-06
Iter: 176 loss: 3.11620761e-06
Iter: 177 loss: 3.10575797e-06
Iter: 178 loss: 3.09709912e-06
Iter: 179 loss: 3.07472237e-06
Iter: 180 loss: 3.25351743e-06
Iter: 181 loss: 3.07041273e-06
Iter: 182 loss: 3.05611297e-06
Iter: 183 loss: 3.05580807e-06
Iter: 184 loss: 3.04453943e-06
Iter: 185 loss: 3.03035131e-06
Iter: 186 loss: 3.02938906e-06
Iter: 187 loss: 3.01234013e-06
Iter: 188 loss: 3.19027322e-06
Iter: 189 loss: 3.01196951e-06
Iter: 190 loss: 2.99794374e-06
Iter: 191 loss: 3.04427567e-06
Iter: 192 loss: 2.99399608e-06
Iter: 193 loss: 2.98113946e-06
Iter: 194 loss: 3.06422726e-06
Iter: 195 loss: 2.97983752e-06
Iter: 196 loss: 2.96847884e-06
Iter: 197 loss: 2.95738391e-06
Iter: 198 loss: 2.95486734e-06
Iter: 199 loss: 2.94089136e-06
Iter: 200 loss: 3.02550802e-06
Iter: 201 loss: 2.93912285e-06
Iter: 202 loss: 2.92515233e-06
Iter: 203 loss: 2.95211476e-06
Iter: 204 loss: 2.91941569e-06
Iter: 205 loss: 2.91085098e-06
Iter: 206 loss: 2.91076526e-06
Iter: 207 loss: 2.90444495e-06
Iter: 208 loss: 2.96111375e-06
Iter: 209 loss: 2.90412777e-06
Iter: 210 loss: 2.89887021e-06
Iter: 211 loss: 2.88356296e-06
Iter: 212 loss: 2.94674237e-06
Iter: 213 loss: 2.87761645e-06
Iter: 214 loss: 2.86640829e-06
Iter: 215 loss: 2.99531371e-06
Iter: 216 loss: 2.86615568e-06
Iter: 217 loss: 2.85551414e-06
Iter: 218 loss: 2.84348152e-06
Iter: 219 loss: 2.84199768e-06
Iter: 220 loss: 2.82888959e-06
Iter: 221 loss: 2.95233622e-06
Iter: 222 loss: 2.82836118e-06
Iter: 223 loss: 2.81553457e-06
Iter: 224 loss: 2.83686222e-06
Iter: 225 loss: 2.80983704e-06
Iter: 226 loss: 2.80070844e-06
Iter: 227 loss: 2.80071299e-06
Iter: 228 loss: 2.79362462e-06
Iter: 229 loss: 2.79400524e-06
Iter: 230 loss: 2.78826383e-06
Iter: 231 loss: 2.77784284e-06
Iter: 232 loss: 2.79150663e-06
Iter: 233 loss: 2.77254435e-06
Iter: 234 loss: 2.76385026e-06
Iter: 235 loss: 2.78140396e-06
Iter: 236 loss: 2.76025116e-06
Iter: 237 loss: 2.75084949e-06
Iter: 238 loss: 2.82926203e-06
Iter: 239 loss: 2.75027378e-06
Iter: 240 loss: 2.74548438e-06
Iter: 241 loss: 2.74532385e-06
Iter: 242 loss: 2.74082686e-06
Iter: 243 loss: 2.73268756e-06
Iter: 244 loss: 2.73260798e-06
Iter: 245 loss: 2.72572015e-06
Iter: 246 loss: 2.73446312e-06
Iter: 247 loss: 2.72220382e-06
Iter: 248 loss: 2.7135186e-06
Iter: 249 loss: 2.70665328e-06
Iter: 250 loss: 2.70403598e-06
Iter: 251 loss: 2.69482553e-06
Iter: 252 loss: 2.82317546e-06
Iter: 253 loss: 2.69485167e-06
Iter: 254 loss: 2.686578e-06
Iter: 255 loss: 2.68498661e-06
Iter: 256 loss: 2.67956239e-06
Iter: 257 loss: 2.67212954e-06
Iter: 258 loss: 2.76297624e-06
Iter: 259 loss: 2.67197083e-06
Iter: 260 loss: 2.66411553e-06
Iter: 261 loss: 2.66805569e-06
Iter: 262 loss: 2.65879362e-06
Iter: 263 loss: 2.6501923e-06
Iter: 264 loss: 2.70927353e-06
Iter: 265 loss: 2.649409e-06
Iter: 266 loss: 2.64402183e-06
Iter: 267 loss: 2.6351463e-06
Iter: 268 loss: 2.63508582e-06
Iter: 269 loss: 2.62629874e-06
Iter: 270 loss: 2.62631829e-06
Iter: 271 loss: 2.62229219e-06
Iter: 272 loss: 2.62224853e-06
Iter: 273 loss: 2.61751347e-06
Iter: 274 loss: 2.61273544e-06
Iter: 275 loss: 2.61173295e-06
Iter: 276 loss: 2.6051639e-06
Iter: 277 loss: 2.61775062e-06
Iter: 278 loss: 2.60254637e-06
Iter: 279 loss: 2.5979507e-06
Iter: 280 loss: 2.59156172e-06
Iter: 281 loss: 2.5913439e-06
Iter: 282 loss: 2.58281261e-06
Iter: 283 loss: 2.63740458e-06
Iter: 284 loss: 2.5818315e-06
Iter: 285 loss: 2.5746283e-06
Iter: 286 loss: 2.60467414e-06
Iter: 287 loss: 2.57315196e-06
Iter: 288 loss: 2.56788826e-06
Iter: 289 loss: 2.57286683e-06
Iter: 290 loss: 2.56496833e-06
Iter: 291 loss: 2.55825e-06
Iter: 292 loss: 2.61650575e-06
Iter: 293 loss: 2.55773534e-06
Iter: 294 loss: 2.55325176e-06
Iter: 295 loss: 2.57105148e-06
Iter: 296 loss: 2.55224813e-06
Iter: 297 loss: 2.54802785e-06
Iter: 298 loss: 2.54322072e-06
Iter: 299 loss: 2.54245288e-06
Iter: 300 loss: 2.53718918e-06
Iter: 301 loss: 2.58239606e-06
Iter: 302 loss: 2.53681719e-06
Iter: 303 loss: 2.5313866e-06
Iter: 304 loss: 2.53830422e-06
Iter: 305 loss: 2.52865038e-06
Iter: 306 loss: 2.522165e-06
Iter: 307 loss: 2.62493313e-06
Iter: 308 loss: 2.52217706e-06
Iter: 309 loss: 2.52000223e-06
Iter: 310 loss: 2.51803772e-06
Iter: 311 loss: 2.51751817e-06
Iter: 312 loss: 2.51354095e-06
Iter: 313 loss: 2.50556923e-06
Iter: 314 loss: 2.66584811e-06
Iter: 315 loss: 2.5055142e-06
Iter: 316 loss: 2.49849791e-06
Iter: 317 loss: 2.55932719e-06
Iter: 318 loss: 2.49810455e-06
Iter: 319 loss: 2.49166715e-06
Iter: 320 loss: 2.49898312e-06
Iter: 321 loss: 2.48818833e-06
Iter: 322 loss: 2.48269362e-06
Iter: 323 loss: 2.51844722e-06
Iter: 324 loss: 2.48203219e-06
Iter: 325 loss: 2.47671187e-06
Iter: 326 loss: 2.48065135e-06
Iter: 327 loss: 2.47331695e-06
Iter: 328 loss: 2.46735817e-06
Iter: 329 loss: 2.5445629e-06
Iter: 330 loss: 2.4673086e-06
Iter: 331 loss: 2.46335276e-06
Iter: 332 loss: 2.46384025e-06
Iter: 333 loss: 2.46030163e-06
Iter: 334 loss: 2.45570618e-06
Iter: 335 loss: 2.4635433e-06
Iter: 336 loss: 2.45366618e-06
Iter: 337 loss: 2.44767784e-06
Iter: 338 loss: 2.45383626e-06
Iter: 339 loss: 2.44435478e-06
Iter: 340 loss: 2.44547255e-06
Iter: 341 loss: 2.44151101e-06
Iter: 342 loss: 2.4395772e-06
Iter: 343 loss: 2.43649333e-06
Iter: 344 loss: 2.43654085e-06
Iter: 345 loss: 2.43252089e-06
Iter: 346 loss: 2.42769829e-06
Iter: 347 loss: 2.42726628e-06
Iter: 348 loss: 2.42238502e-06
Iter: 349 loss: 2.48839069e-06
Iter: 350 loss: 2.42229407e-06
Iter: 351 loss: 2.41880844e-06
Iter: 352 loss: 2.412512e-06
Iter: 353 loss: 2.56552e-06
Iter: 354 loss: 2.41248972e-06
Iter: 355 loss: 2.4075207e-06
Iter: 356 loss: 2.40745567e-06
Iter: 357 loss: 2.40331315e-06
Iter: 358 loss: 2.39747578e-06
Iter: 359 loss: 2.3972907e-06
Iter: 360 loss: 2.39402971e-06
Iter: 361 loss: 2.39329984e-06
Iter: 362 loss: 2.39004748e-06
Iter: 363 loss: 2.39099722e-06
Iter: 364 loss: 2.3878049e-06
Iter: 365 loss: 2.38341954e-06
Iter: 366 loss: 2.39237079e-06
Iter: 367 loss: 2.38166194e-06
Iter: 368 loss: 2.37748236e-06
Iter: 369 loss: 2.38412508e-06
Iter: 370 loss: 2.37542781e-06
Iter: 371 loss: 2.37349104e-06
Iter: 372 loss: 2.37269705e-06
Iter: 373 loss: 2.37031963e-06
Iter: 374 loss: 2.36934511e-06
Iter: 375 loss: 2.36801407e-06
Iter: 376 loss: 2.36506776e-06
Iter: 377 loss: 2.36245205e-06
Iter: 378 loss: 2.36166102e-06
Iter: 379 loss: 2.35712423e-06
Iter: 380 loss: 2.37081076e-06
Iter: 381 loss: 2.35579e-06
Iter: 382 loss: 2.3505645e-06
Iter: 383 loss: 2.35077482e-06
Iter: 384 loss: 2.34668119e-06
Iter: 385 loss: 2.34149775e-06
Iter: 386 loss: 2.39766291e-06
Iter: 387 loss: 2.34138315e-06
Iter: 388 loss: 2.33760829e-06
Iter: 389 loss: 2.33260675e-06
Iter: 390 loss: 2.3323762e-06
Iter: 391 loss: 2.32721663e-06
Iter: 392 loss: 2.3271557e-06
Iter: 393 loss: 2.32408547e-06
Iter: 394 loss: 2.33266269e-06
Iter: 395 loss: 2.32303182e-06
Iter: 396 loss: 2.31889953e-06
Iter: 397 loss: 2.32347475e-06
Iter: 398 loss: 2.31676449e-06
Iter: 399 loss: 2.31365152e-06
Iter: 400 loss: 2.32112075e-06
Iter: 401 loss: 2.31252352e-06
Iter: 402 loss: 2.30910723e-06
Iter: 403 loss: 2.3237244e-06
Iter: 404 loss: 2.30817341e-06
Iter: 405 loss: 2.30390106e-06
Iter: 406 loss: 2.33586616e-06
Iter: 407 loss: 2.30361888e-06
Iter: 408 loss: 2.30224259e-06
Iter: 409 loss: 2.30052092e-06
Iter: 410 loss: 2.30030855e-06
Iter: 411 loss: 2.29724765e-06
Iter: 412 loss: 2.29471016e-06
Iter: 413 loss: 2.29388024e-06
Iter: 414 loss: 2.28996078e-06
Iter: 415 loss: 2.32070261e-06
Iter: 416 loss: 2.28977478e-06
Iter: 417 loss: 2.28618933e-06
Iter: 418 loss: 2.28513932e-06
Iter: 419 loss: 2.28293379e-06
Iter: 420 loss: 2.27825717e-06
Iter: 421 loss: 2.29763873e-06
Iter: 422 loss: 2.27726696e-06
Iter: 423 loss: 2.27288865e-06
Iter: 424 loss: 2.28880572e-06
Iter: 425 loss: 2.27175724e-06
Iter: 426 loss: 2.26812062e-06
Iter: 427 loss: 2.28173394e-06
Iter: 428 loss: 2.26719385e-06
Iter: 429 loss: 2.26289785e-06
Iter: 430 loss: 2.27997293e-06
Iter: 431 loss: 2.26195516e-06
Iter: 432 loss: 2.25899657e-06
Iter: 433 loss: 2.26395377e-06
Iter: 434 loss: 2.25778308e-06
Iter: 435 loss: 2.25411463e-06
Iter: 436 loss: 2.25616077e-06
Iter: 437 loss: 2.25184476e-06
Iter: 438 loss: 2.25148688e-06
Iter: 439 loss: 2.24965197e-06
Iter: 440 loss: 2.24860196e-06
Iter: 441 loss: 2.24608948e-06
Iter: 442 loss: 2.27150622e-06
Iter: 443 loss: 2.24578389e-06
Iter: 444 loss: 2.24232031e-06
Iter: 445 loss: 2.24261589e-06
Iter: 446 loss: 2.23955931e-06
Iter: 447 loss: 2.23605707e-06
Iter: 448 loss: 2.27579335e-06
Iter: 449 loss: 2.23592633e-06
Iter: 450 loss: 2.23342863e-06
Iter: 451 loss: 2.2299605e-06
Iter: 452 loss: 2.22984431e-06
Iter: 453 loss: 2.22528092e-06
Iter: 454 loss: 2.27271221e-06
Iter: 455 loss: 2.22510607e-06
Iter: 456 loss: 2.22229255e-06
Iter: 457 loss: 2.21849632e-06
Iter: 458 loss: 2.2183458e-06
Iter: 459 loss: 2.21364667e-06
Iter: 460 loss: 2.21364053e-06
Iter: 461 loss: 2.21070809e-06
Iter: 462 loss: 2.2248496e-06
Iter: 463 loss: 2.21016353e-06
Iter: 464 loss: 2.20719835e-06
Iter: 465 loss: 2.20886886e-06
Iter: 466 loss: 2.20519269e-06
Iter: 467 loss: 2.20181028e-06
Iter: 468 loss: 2.20735637e-06
Iter: 469 loss: 2.20020502e-06
Iter: 470 loss: 2.19932826e-06
Iter: 471 loss: 2.19838876e-06
Iter: 472 loss: 2.19690605e-06
Iter: 473 loss: 2.19468325e-06
Iter: 474 loss: 2.19463573e-06
Iter: 475 loss: 2.19239723e-06
Iter: 476 loss: 2.19048229e-06
Iter: 477 loss: 2.18986565e-06
Iter: 478 loss: 2.18621494e-06
Iter: 479 loss: 2.21351343e-06
Iter: 480 loss: 2.18582318e-06
Iter: 481 loss: 2.18300306e-06
Iter: 482 loss: 2.18079344e-06
Iter: 483 loss: 2.17986758e-06
Iter: 484 loss: 2.17545221e-06
Iter: 485 loss: 2.21285973e-06
Iter: 486 loss: 2.17514571e-06
Iter: 487 loss: 2.17233878e-06
Iter: 488 loss: 2.16967965e-06
Iter: 489 loss: 2.16902072e-06
Iter: 490 loss: 2.16414423e-06
Iter: 491 loss: 2.2047102e-06
Iter: 492 loss: 2.16384092e-06
Iter: 493 loss: 2.16103035e-06
Iter: 494 loss: 2.16577473e-06
Iter: 495 loss: 2.15976365e-06
Iter: 496 loss: 2.15563341e-06
Iter: 497 loss: 2.16776562e-06
Iter: 498 loss: 2.15440059e-06
Iter: 499 loss: 2.15163254e-06
Iter: 500 loss: 2.16431135e-06
Iter: 501 loss: 2.15102091e-06
Iter: 502 loss: 2.14902752e-06
Iter: 503 loss: 2.16210447e-06
Iter: 504 loss: 2.14890497e-06
Iter: 505 loss: 2.14613351e-06
Iter: 506 loss: 2.1462497e-06
Iter: 507 loss: 2.14402735e-06
Iter: 508 loss: 2.14199827e-06
Iter: 509 loss: 2.14202419e-06
Iter: 510 loss: 2.14043962e-06
Iter: 511 loss: 2.13737962e-06
Iter: 512 loss: 2.13695193e-06
Iter: 513 loss: 2.13477e-06
Iter: 514 loss: 2.13111571e-06
Iter: 515 loss: 2.16944863e-06
Iter: 516 loss: 2.13106046e-06
Iter: 517 loss: 2.12844088e-06
Iter: 518 loss: 2.12557688e-06
Iter: 519 loss: 2.12516352e-06
Iter: 520 loss: 2.12119335e-06
Iter: 521 loss: 2.16264516e-06
Iter: 522 loss: 2.12102123e-06
Iter: 523 loss: 2.11838551e-06
Iter: 524 loss: 2.11949805e-06
Iter: 525 loss: 2.11645329e-06
Iter: 526 loss: 2.11351744e-06
Iter: 527 loss: 2.12816872e-06
Iter: 528 loss: 2.11298266e-06
Iter: 529 loss: 2.10983899e-06
Iter: 530 loss: 2.12638861e-06
Iter: 531 loss: 2.10929852e-06
Iter: 532 loss: 2.10686949e-06
Iter: 533 loss: 2.11481824e-06
Iter: 534 loss: 2.10623239e-06
Iter: 535 loss: 2.10410826e-06
Iter: 536 loss: 2.10624785e-06
Iter: 537 loss: 2.10297526e-06
Iter: 538 loss: 2.10085636e-06
Iter: 539 loss: 2.10077064e-06
Iter: 540 loss: 2.09979225e-06
Iter: 541 loss: 2.09779728e-06
Iter: 542 loss: 2.13052749e-06
Iter: 543 loss: 2.09770315e-06
Iter: 544 loss: 2.09517543e-06
Iter: 545 loss: 2.09217251e-06
Iter: 546 loss: 2.09184236e-06
Iter: 547 loss: 2.08806796e-06
Iter: 548 loss: 2.08809683e-06
Iter: 549 loss: 2.08590859e-06
Iter: 550 loss: 2.08394817e-06
Iter: 551 loss: 2.08339361e-06
Iter: 552 loss: 2.07980975e-06
Iter: 553 loss: 2.1075582e-06
Iter: 554 loss: 2.0796042e-06
Iter: 555 loss: 2.07748599e-06
Iter: 556 loss: 2.07669109e-06
Iter: 557 loss: 2.07562653e-06
Iter: 558 loss: 2.07200469e-06
Iter: 559 loss: 2.08847405e-06
Iter: 560 loss: 2.07126072e-06
Iter: 561 loss: 2.06906361e-06
Iter: 562 loss: 2.08292249e-06
Iter: 563 loss: 2.06879167e-06
Iter: 564 loss: 2.06621598e-06
Iter: 565 loss: 2.07247876e-06
Iter: 566 loss: 2.06531467e-06
Iter: 567 loss: 2.06342088e-06
Iter: 568 loss: 2.07636776e-06
Iter: 569 loss: 2.06329969e-06
Iter: 570 loss: 2.06229561e-06
Iter: 571 loss: 2.07752646e-06
Iter: 572 loss: 2.06237723e-06
Iter: 573 loss: 2.06139066e-06
Iter: 574 loss: 2.05896231e-06
Iter: 575 loss: 2.07810081e-06
Iter: 576 loss: 2.05854371e-06
Iter: 577 loss: 2.056021e-06
Iter: 578 loss: 2.06039113e-06
Iter: 579 loss: 2.05500965e-06
Iter: 580 loss: 2.05215883e-06
Iter: 581 loss: 2.06767618e-06
Iter: 582 loss: 2.0518105e-06
Iter: 583 loss: 2.04944081e-06
Iter: 584 loss: 2.05330571e-06
Iter: 585 loss: 2.04830167e-06
Iter: 586 loss: 2.04555431e-06
Iter: 587 loss: 2.05274455e-06
Iter: 588 loss: 2.04468824e-06
Iter: 589 loss: 2.04251364e-06
Iter: 590 loss: 2.0454836e-06
Iter: 591 loss: 2.0414866e-06
Iter: 592 loss: 2.03831587e-06
Iter: 593 loss: 2.04490243e-06
Iter: 594 loss: 2.03712307e-06
Iter: 595 loss: 2.0346356e-06
Iter: 596 loss: 2.04436083e-06
Iter: 597 loss: 2.0340467e-06
Iter: 598 loss: 2.03106401e-06
Iter: 599 loss: 2.04057642e-06
Iter: 600 loss: 2.03017976e-06
Iter: 601 loss: 2.02775391e-06
Iter: 602 loss: 2.0434984e-06
Iter: 603 loss: 2.02746696e-06
Iter: 604 loss: 2.02652632e-06
Iter: 605 loss: 2.02650904e-06
Iter: 606 loss: 2.02551109e-06
Iter: 607 loss: 2.02310957e-06
Iter: 608 loss: 2.05084666e-06
Iter: 609 loss: 2.02281763e-06
Iter: 610 loss: 2.02048523e-06
Iter: 611 loss: 2.02673209e-06
Iter: 612 loss: 2.01960643e-06
Iter: 613 loss: 2.017214e-06
Iter: 614 loss: 2.01580178e-06
Iter: 615 loss: 2.01474677e-06
Iter: 616 loss: 2.01207172e-06
Iter: 617 loss: 2.01201601e-06
Iter: 618 loss: 2.01044577e-06
Iter: 619 loss: 2.0082116e-06
Iter: 620 loss: 2.0081568e-06
Iter: 621 loss: 2.00490331e-06
Iter: 622 loss: 2.02591286e-06
Iter: 623 loss: 2.00457725e-06
Iter: 624 loss: 2.0021871e-06
Iter: 625 loss: 2.00377e-06
Iter: 626 loss: 2.00061959e-06
Iter: 627 loss: 1.99783699e-06
Iter: 628 loss: 2.01479656e-06
Iter: 629 loss: 1.99748251e-06
Iter: 630 loss: 1.99501824e-06
Iter: 631 loss: 2.00456839e-06
Iter: 632 loss: 1.99435976e-06
Iter: 633 loss: 1.9919753e-06
Iter: 634 loss: 2.00501813e-06
Iter: 635 loss: 1.99158421e-06
Iter: 636 loss: 1.99032138e-06
Iter: 637 loss: 2.00316481e-06
Iter: 638 loss: 1.99026272e-06
Iter: 639 loss: 1.98866474e-06
Iter: 640 loss: 1.98856696e-06
Iter: 641 loss: 1.98738417e-06
Iter: 642 loss: 1.98590624e-06
Iter: 643 loss: 1.98555131e-06
Iter: 644 loss: 1.98465591e-06
Iter: 645 loss: 1.98235239e-06
Iter: 646 loss: 1.97964482e-06
Iter: 647 loss: 1.97934014e-06
Iter: 648 loss: 1.97645431e-06
Iter: 649 loss: 1.97642294e-06
Iter: 650 loss: 1.97476902e-06
Iter: 651 loss: 1.97534155e-06
Iter: 652 loss: 1.97357758e-06
Iter: 653 loss: 1.97079703e-06
Iter: 654 loss: 1.97265535e-06
Iter: 655 loss: 1.96911333e-06
Iter: 656 loss: 1.96651195e-06
Iter: 657 loss: 1.97524946e-06
Iter: 658 loss: 1.96578412e-06
Iter: 659 loss: 1.96268866e-06
Iter: 660 loss: 1.97048985e-06
Iter: 661 loss: 1.96165661e-06
Iter: 662 loss: 1.95950429e-06
Iter: 663 loss: 1.97210761e-06
Iter: 664 loss: 1.95925918e-06
Iter: 665 loss: 1.95697794e-06
Iter: 666 loss: 1.9620561e-06
Iter: 667 loss: 1.95616281e-06
Iter: 668 loss: 1.95438929e-06
Iter: 669 loss: 1.97273243e-06
Iter: 670 loss: 1.95436746e-06
Iter: 671 loss: 1.95288931e-06
Iter: 672 loss: 1.96008477e-06
Iter: 673 loss: 1.95259463e-06
Iter: 674 loss: 1.95159964e-06
Iter: 675 loss: 1.949129e-06
Iter: 676 loss: 1.97375493e-06
Iter: 677 loss: 1.94877589e-06
Iter: 678 loss: 1.9459992e-06
Iter: 679 loss: 1.95389521e-06
Iter: 680 loss: 1.94510585e-06
Iter: 681 loss: 1.94244899e-06
Iter: 682 loss: 1.95811e-06
Iter: 683 loss: 1.94201311e-06
Iter: 684 loss: 1.94020504e-06
Iter: 685 loss: 1.94504446e-06
Iter: 686 loss: 1.93958658e-06
Iter: 687 loss: 1.93725214e-06
Iter: 688 loss: 1.94020708e-06
Iter: 689 loss: 1.93606525e-06
Iter: 690 loss: 1.93403321e-06
Iter: 691 loss: 1.93501819e-06
Iter: 692 loss: 1.93265259e-06
Iter: 693 loss: 1.92953576e-06
Iter: 694 loss: 1.94388485e-06
Iter: 695 loss: 1.92891957e-06
Iter: 696 loss: 1.92693778e-06
Iter: 697 loss: 1.93206279e-06
Iter: 698 loss: 1.92622156e-06
Iter: 699 loss: 1.9237209e-06
Iter: 700 loss: 1.93654137e-06
Iter: 701 loss: 1.92324956e-06
Iter: 702 loss: 1.92171115e-06
Iter: 703 loss: 1.93729647e-06
Iter: 704 loss: 1.92166317e-06
Iter: 705 loss: 1.92070092e-06
Iter: 706 loss: 1.93337473e-06
Iter: 707 loss: 1.92069319e-06
Iter: 708 loss: 1.91999334e-06
Iter: 709 loss: 1.91800655e-06
Iter: 710 loss: 1.92594644e-06
Iter: 711 loss: 1.91720164e-06
Iter: 712 loss: 1.91493245e-06
Iter: 713 loss: 1.9283043e-06
Iter: 714 loss: 1.91465097e-06
Iter: 715 loss: 1.91246636e-06
Iter: 716 loss: 1.91555273e-06
Iter: 717 loss: 1.91149411e-06
Iter: 718 loss: 1.90954734e-06
Iter: 719 loss: 1.92495099e-06
Iter: 720 loss: 1.90936407e-06
Iter: 721 loss: 1.90777064e-06
Iter: 722 loss: 1.90888022e-06
Iter: 723 loss: 1.90672336e-06
Iter: 724 loss: 1.90456853e-06
Iter: 725 loss: 1.90778087e-06
Iter: 726 loss: 1.90357514e-06
Iter: 727 loss: 1.90087189e-06
Iter: 728 loss: 1.90510434e-06
Iter: 729 loss: 1.89977209e-06
Iter: 730 loss: 1.89750381e-06
Iter: 731 loss: 1.90845458e-06
Iter: 732 loss: 1.89714399e-06
Iter: 733 loss: 1.89478271e-06
Iter: 734 loss: 1.90066498e-06
Iter: 735 loss: 1.89385696e-06
Iter: 736 loss: 1.89221078e-06
Iter: 737 loss: 1.89217565e-06
Iter: 738 loss: 1.89132675e-06
Iter: 739 loss: 1.89916216e-06
Iter: 740 loss: 1.89128616e-06
Iter: 741 loss: 1.89034336e-06
Iter: 742 loss: 1.88807644e-06
Iter: 743 loss: 1.9091492e-06
Iter: 744 loss: 1.88772356e-06
Iter: 745 loss: 1.88553224e-06
Iter: 746 loss: 1.89551292e-06
Iter: 747 loss: 1.88511308e-06
Iter: 748 loss: 1.8832817e-06
Iter: 749 loss: 1.88134823e-06
Iter: 750 loss: 1.88097169e-06
Iter: 751 loss: 1.87902947e-06
Iter: 752 loss: 1.87894966e-06
Iter: 753 loss: 1.87758292e-06
Iter: 754 loss: 1.87683167e-06
Iter: 755 loss: 1.87626915e-06
Iter: 756 loss: 1.87383068e-06
Iter: 757 loss: 1.88403055e-06
Iter: 758 loss: 1.87335115e-06
Iter: 759 loss: 1.87150079e-06
Iter: 760 loss: 1.87138858e-06
Iter: 761 loss: 1.86999273e-06
Iter: 762 loss: 1.86761383e-06
Iter: 763 loss: 1.88277227e-06
Iter: 764 loss: 1.86738987e-06
Iter: 765 loss: 1.86541615e-06
Iter: 766 loss: 1.87427452e-06
Iter: 767 loss: 1.8649298e-06
Iter: 768 loss: 1.86364059e-06
Iter: 769 loss: 1.88184094e-06
Iter: 770 loss: 1.86359068e-06
Iter: 771 loss: 1.86261298e-06
Iter: 772 loss: 1.86920454e-06
Iter: 773 loss: 1.86255807e-06
Iter: 774 loss: 1.86140574e-06
Iter: 775 loss: 1.85982378e-06
Iter: 776 loss: 1.8598098e-06
Iter: 777 loss: 1.85815543e-06
Iter: 778 loss: 1.86007583e-06
Iter: 779 loss: 1.85735144e-06
Iter: 780 loss: 1.85563317e-06
Iter: 781 loss: 1.85319959e-06
Iter: 782 loss: 1.85309887e-06
Iter: 783 loss: 1.8507244e-06
Iter: 784 loss: 1.85071099e-06
Iter: 785 loss: 1.84905912e-06
Iter: 786 loss: 1.85028466e-06
Iter: 787 loss: 1.8481195e-06
Iter: 788 loss: 1.8459973e-06
Iter: 789 loss: 1.85473891e-06
Iter: 790 loss: 1.84547594e-06
Iter: 791 loss: 1.84399039e-06
Iter: 792 loss: 1.84438716e-06
Iter: 793 loss: 1.84295777e-06
Iter: 794 loss: 1.84044825e-06
Iter: 795 loss: 1.84515739e-06
Iter: 796 loss: 1.83936493e-06
Iter: 797 loss: 1.83744578e-06
Iter: 798 loss: 1.84493979e-06
Iter: 799 loss: 1.83693658e-06
Iter: 800 loss: 1.8351609e-06
Iter: 801 loss: 1.85430713e-06
Iter: 802 loss: 1.83511736e-06
Iter: 803 loss: 1.83417319e-06
Iter: 804 loss: 1.8460621e-06
Iter: 805 loss: 1.83410179e-06
Iter: 806 loss: 1.83320071e-06
Iter: 807 loss: 1.83317172e-06
Iter: 808 loss: 1.8324472e-06
Iter: 809 loss: 1.83132329e-06
Iter: 810 loss: 1.82975668e-06
Iter: 811 loss: 1.82969438e-06
Iter: 812 loss: 1.82773863e-06
Iter: 813 loss: 1.82771487e-06
Iter: 814 loss: 1.82617418e-06
Iter: 815 loss: 1.82432336e-06
Iter: 816 loss: 1.85398687e-06
Iter: 817 loss: 1.82430517e-06
Iter: 818 loss: 1.8229166e-06
Iter: 819 loss: 1.8223e-06
Iter: 820 loss: 1.821517e-06
Iter: 821 loss: 1.81958603e-06
Iter: 822 loss: 1.84130295e-06
Iter: 823 loss: 1.81955932e-06
Iter: 824 loss: 1.81829478e-06
Iter: 825 loss: 1.8166977e-06
Iter: 826 loss: 1.81660596e-06
Iter: 827 loss: 1.81439236e-06
Iter: 828 loss: 1.83545103e-06
Iter: 829 loss: 1.81430948e-06
Iter: 830 loss: 1.81293967e-06
Iter: 831 loss: 1.81212033e-06
Iter: 832 loss: 1.81152541e-06
Iter: 833 loss: 1.80986535e-06
Iter: 834 loss: 1.80980692e-06
Iter: 835 loss: 1.80893494e-06
Iter: 836 loss: 1.81499183e-06
Iter: 837 loss: 1.80881511e-06
Iter: 838 loss: 1.80789891e-06
Iter: 839 loss: 1.8091132e-06
Iter: 840 loss: 1.80743859e-06
Iter: 841 loss: 1.80634254e-06
Iter: 842 loss: 1.80456698e-06
Iter: 843 loss: 1.80457516e-06
Iter: 844 loss: 1.80266693e-06
Iter: 845 loss: 1.8066919e-06
Iter: 846 loss: 1.80196082e-06
Iter: 847 loss: 1.79997062e-06
Iter: 848 loss: 1.80794677e-06
Iter: 849 loss: 1.79949711e-06
Iter: 850 loss: 1.79777749e-06
Iter: 851 loss: 1.80165603e-06
Iter: 852 loss: 1.79719768e-06
Iter: 853 loss: 1.79514643e-06
Iter: 854 loss: 1.80298753e-06
Iter: 855 loss: 1.79470089e-06
Iter: 856 loss: 1.7928985e-06
Iter: 857 loss: 1.79497385e-06
Iter: 858 loss: 1.79201948e-06
Iter: 859 loss: 1.78994469e-06
Iter: 860 loss: 1.79784e-06
Iter: 861 loss: 1.78947118e-06
Iter: 862 loss: 1.78791561e-06
Iter: 863 loss: 1.78813434e-06
Iter: 864 loss: 1.78683194e-06
Iter: 865 loss: 1.78462278e-06
Iter: 866 loss: 1.80611551e-06
Iter: 867 loss: 1.78458743e-06
Iter: 868 loss: 1.78375296e-06
Iter: 869 loss: 1.78368498e-06
Iter: 870 loss: 1.78301798e-06
Iter: 871 loss: 1.78423522e-06
Iter: 872 loss: 1.78280811e-06
Iter: 873 loss: 1.78192477e-06
Iter: 874 loss: 1.77979018e-06
Iter: 875 loss: 1.80283041e-06
Iter: 876 loss: 1.77956451e-06
Iter: 877 loss: 1.77781908e-06
Iter: 878 loss: 1.79047902e-06
Iter: 879 loss: 1.77769948e-06
Iter: 880 loss: 1.77604795e-06
Iter: 881 loss: 1.77546235e-06
Iter: 882 loss: 1.77460902e-06
Iter: 883 loss: 1.77294169e-06
Iter: 884 loss: 1.79250333e-06
Iter: 885 loss: 1.77292395e-06
Iter: 886 loss: 1.77147831e-06
Iter: 887 loss: 1.77158336e-06
Iter: 888 loss: 1.77036497e-06
Iter: 889 loss: 1.7684381e-06
Iter: 890 loss: 1.77955678e-06
Iter: 891 loss: 1.76828473e-06
Iter: 892 loss: 1.76671642e-06
Iter: 893 loss: 1.76838762e-06
Iter: 894 loss: 1.76580727e-06
Iter: 895 loss: 1.76423634e-06
Iter: 896 loss: 1.76753622e-06
Iter: 897 loss: 1.76357651e-06
Iter: 898 loss: 1.76156414e-06
Iter: 899 loss: 1.77043898e-06
Iter: 900 loss: 1.76123206e-06
Iter: 901 loss: 1.76056915e-06
Iter: 902 loss: 1.76045808e-06
Iter: 903 loss: 1.75979972e-06
Iter: 904 loss: 1.76034087e-06
Iter: 905 loss: 1.75939408e-06
Iter: 906 loss: 1.75836249e-06
Iter: 907 loss: 1.75615492e-06
Iter: 908 loss: 1.79422818e-06
Iter: 909 loss: 1.75611422e-06
Iter: 910 loss: 1.75429477e-06
Iter: 911 loss: 1.76579624e-06
Iter: 912 loss: 1.75409377e-06
Iter: 913 loss: 1.75265973e-06
Iter: 914 loss: 1.7515672e-06
Iter: 915 loss: 1.75113018e-06
Iter: 916 loss: 1.74922275e-06
Iter: 917 loss: 1.77033485e-06
Iter: 918 loss: 1.74920763e-06
Iter: 919 loss: 1.74783781e-06
Iter: 920 loss: 1.74812953e-06
Iter: 921 loss: 1.74672641e-06
Iter: 922 loss: 1.74476224e-06
Iter: 923 loss: 1.75758009e-06
Iter: 924 loss: 1.74454613e-06
Iter: 925 loss: 1.74322508e-06
Iter: 926 loss: 1.74435468e-06
Iter: 927 loss: 1.74245201e-06
Iter: 928 loss: 1.74074694e-06
Iter: 929 loss: 1.74390289e-06
Iter: 930 loss: 1.74001298e-06
Iter: 931 loss: 1.73840772e-06
Iter: 932 loss: 1.74610784e-06
Iter: 933 loss: 1.73801527e-06
Iter: 934 loss: 1.7370935e-06
Iter: 935 loss: 1.73708065e-06
Iter: 936 loss: 1.73628916e-06
Iter: 937 loss: 1.73880687e-06
Iter: 938 loss: 1.73613307e-06
Iter: 939 loss: 1.73531578e-06
Iter: 940 loss: 1.7342519e-06
Iter: 941 loss: 1.73417561e-06
Iter: 942 loss: 1.73283797e-06
Iter: 943 loss: 1.73417948e-06
Iter: 944 loss: 1.73210708e-06
Iter: 945 loss: 1.73074875e-06
Iter: 946 loss: 1.73022272e-06
Iter: 947 loss: 1.7293869e-06
Iter: 948 loss: 1.72767091e-06
Iter: 949 loss: 1.74957256e-06
Iter: 950 loss: 1.72756984e-06
Iter: 951 loss: 1.72648924e-06
Iter: 952 loss: 1.72551472e-06
Iter: 953 loss: 1.72522903e-06
Iter: 954 loss: 1.72310797e-06
Iter: 955 loss: 1.74175636e-06
Iter: 956 loss: 1.72298803e-06
Iter: 957 loss: 1.72186606e-06
Iter: 958 loss: 1.72249872e-06
Iter: 959 loss: 1.72112709e-06
Iter: 960 loss: 1.7194991e-06
Iter: 961 loss: 1.72346984e-06
Iter: 962 loss: 1.71890088e-06
Iter: 963 loss: 1.71748718e-06
Iter: 964 loss: 1.72352929e-06
Iter: 965 loss: 1.71728175e-06
Iter: 966 loss: 1.71617091e-06
Iter: 967 loss: 1.72877481e-06
Iter: 968 loss: 1.71613374e-06
Iter: 969 loss: 1.71522754e-06
Iter: 970 loss: 1.71999102e-06
Iter: 971 loss: 1.71512897e-06
Iter: 972 loss: 1.71433271e-06
Iter: 973 loss: 1.71402371e-06
Iter: 974 loss: 1.71360443e-06
Iter: 975 loss: 1.71256011e-06
Iter: 976 loss: 1.71241152e-06
Iter: 977 loss: 1.71168494e-06
Iter: 978 loss: 1.71025272e-06
Iter: 979 loss: 1.70991939e-06
Iter: 980 loss: 1.70903149e-06
Iter: 981 loss: 1.70710302e-06
Iter: 982 loss: 1.7249431e-06
Iter: 983 loss: 1.70704288e-06
Iter: 984 loss: 1.70573071e-06
Iter: 985 loss: 1.70513931e-06
Iter: 986 loss: 1.70451358e-06
Iter: 987 loss: 1.70236342e-06
Iter: 988 loss: 1.72101784e-06
Iter: 989 loss: 1.70228952e-06
Iter: 990 loss: 1.70105739e-06
Iter: 991 loss: 1.7038335e-06
Iter: 992 loss: 1.70059843e-06
Iter: 993 loss: 1.69915256e-06
Iter: 994 loss: 1.69933605e-06
Iter: 995 loss: 1.69796056e-06
Iter: 996 loss: 1.69657926e-06
Iter: 997 loss: 1.70823682e-06
Iter: 998 loss: 1.69658028e-06
Iter: 999 loss: 1.6953843e-06
Iter: 1000 loss: 1.698353e-06
Iter: 1001 loss: 1.69499594e-06
Iter: 1002 loss: 1.69371947e-06
Iter: 1003 loss: 1.70850694e-06
Iter: 1004 loss: 1.69374596e-06
Iter: 1005 loss: 1.69292491e-06
Iter: 1006 loss: 1.69339273e-06
Iter: 1007 loss: 1.69240423e-06
Iter: 1008 loss: 1.69153202e-06
Iter: 1009 loss: 1.69011582e-06
Iter: 1010 loss: 1.69013651e-06
Iter: 1011 loss: 1.68834e-06
Iter: 1012 loss: 1.69397902e-06
Iter: 1013 loss: 1.6878621e-06
Iter: 1014 loss: 1.68623751e-06
Iter: 1015 loss: 1.69001828e-06
Iter: 1016 loss: 1.68566237e-06
Iter: 1017 loss: 1.68418376e-06
Iter: 1018 loss: 1.6883123e-06
Iter: 1019 loss: 1.68363613e-06
Iter: 1020 loss: 1.68188797e-06
Iter: 1021 loss: 1.68849124e-06
Iter: 1022 loss: 1.68150268e-06
Iter: 1023 loss: 1.68012662e-06
Iter: 1024 loss: 1.68594568e-06
Iter: 1025 loss: 1.67984217e-06
Iter: 1026 loss: 1.67845621e-06
Iter: 1027 loss: 1.67794963e-06
Iter: 1028 loss: 1.67719509e-06
Iter: 1029 loss: 1.6758886e-06
Iter: 1030 loss: 1.68614054e-06
Iter: 1031 loss: 1.6758064e-06
Iter: 1032 loss: 1.67446638e-06
Iter: 1033 loss: 1.67654048e-06
Iter: 1034 loss: 1.67388066e-06
Iter: 1035 loss: 1.67275607e-06
Iter: 1036 loss: 1.67269695e-06
Iter: 1037 loss: 1.67204962e-06
Iter: 1038 loss: 1.67224721e-06
Iter: 1039 loss: 1.67161738e-06
Iter: 1040 loss: 1.67079509e-06
Iter: 1041 loss: 1.6690982e-06
Iter: 1042 loss: 1.69895031e-06
Iter: 1043 loss: 1.66908671e-06
Iter: 1044 loss: 1.66739574e-06
Iter: 1045 loss: 1.67734197e-06
Iter: 1046 loss: 1.66714267e-06
Iter: 1047 loss: 1.66576649e-06
Iter: 1048 loss: 1.66589894e-06
Iter: 1049 loss: 1.66464588e-06
Iter: 1050 loss: 1.66297696e-06
Iter: 1051 loss: 1.67515452e-06
Iter: 1052 loss: 1.66282189e-06
Iter: 1053 loss: 1.66138693e-06
Iter: 1054 loss: 1.66541474e-06
Iter: 1055 loss: 1.6610079e-06
Iter: 1056 loss: 1.65968663e-06
Iter: 1057 loss: 1.66435723e-06
Iter: 1058 loss: 1.65933466e-06
Iter: 1059 loss: 1.65783786e-06
Iter: 1060 loss: 1.65919027e-06
Iter: 1061 loss: 1.65701863e-06
Iter: 1062 loss: 1.6557683e-06
Iter: 1063 loss: 1.65983352e-06
Iter: 1064 loss: 1.65530105e-06
Iter: 1065 loss: 1.65373126e-06
Iter: 1066 loss: 1.65673237e-06
Iter: 1067 loss: 1.65305698e-06
Iter: 1068 loss: 1.65271013e-06
Iter: 1069 loss: 1.65224753e-06
Iter: 1070 loss: 1.65175902e-06
Iter: 1071 loss: 1.65160429e-06
Iter: 1072 loss: 1.65128404e-06
Iter: 1073 loss: 1.65050562e-06
Iter: 1074 loss: 1.64930179e-06
Iter: 1075 loss: 1.64924e-06
Iter: 1076 loss: 1.64776293e-06
Iter: 1077 loss: 1.65304095e-06
Iter: 1078 loss: 1.64732785e-06
Iter: 1079 loss: 1.64616949e-06
Iter: 1080 loss: 1.64515643e-06
Iter: 1081 loss: 1.64483799e-06
Iter: 1082 loss: 1.64311029e-06
Iter: 1083 loss: 1.665019e-06
Iter: 1084 loss: 1.64307903e-06
Iter: 1085 loss: 1.64199309e-06
Iter: 1086 loss: 1.64231324e-06
Iter: 1087 loss: 1.64118251e-06
Iter: 1088 loss: 1.63963432e-06
Iter: 1089 loss: 1.64897415e-06
Iter: 1090 loss: 1.6394323e-06
Iter: 1091 loss: 1.63831965e-06
Iter: 1092 loss: 1.63945e-06
Iter: 1093 loss: 1.63770051e-06
Iter: 1094 loss: 1.63628147e-06
Iter: 1095 loss: 1.63994378e-06
Iter: 1096 loss: 1.63576794e-06
Iter: 1097 loss: 1.6343572e-06
Iter: 1098 loss: 1.63643358e-06
Iter: 1099 loss: 1.63368964e-06
Iter: 1100 loss: 1.63395077e-06
Iter: 1101 loss: 1.63307254e-06
Iter: 1102 loss: 1.63269192e-06
Iter: 1103 loss: 1.63238747e-06
Iter: 1104 loss: 1.63225104e-06
Iter: 1105 loss: 1.63152288e-06
Iter: 1106 loss: 1.63032951e-06
Iter: 1107 loss: 1.63028744e-06
Iter: 1108 loss: 1.62900449e-06
Iter: 1109 loss: 1.63865934e-06
Iter: 1110 loss: 1.6289581e-06
Iter: 1111 loss: 1.62792435e-06
Iter: 1112 loss: 1.62618437e-06
Iter: 1113 loss: 1.66698896e-06
Iter: 1114 loss: 1.62616823e-06
Iter: 1115 loss: 1.62456047e-06
Iter: 1116 loss: 1.62456092e-06
Iter: 1117 loss: 1.62331037e-06
Iter: 1118 loss: 1.62317474e-06
Iter: 1119 loss: 1.62223512e-06
Iter: 1120 loss: 1.62077868e-06
Iter: 1121 loss: 1.6390145e-06
Iter: 1122 loss: 1.62076913e-06
Iter: 1123 loss: 1.61981802e-06
Iter: 1124 loss: 1.62034723e-06
Iter: 1125 loss: 1.61915705e-06
Iter: 1126 loss: 1.61758885e-06
Iter: 1127 loss: 1.61922776e-06
Iter: 1128 loss: 1.61674689e-06
Iter: 1129 loss: 1.61515425e-06
Iter: 1130 loss: 1.62464153e-06
Iter: 1131 loss: 1.61494438e-06
Iter: 1132 loss: 1.61444655e-06
Iter: 1133 loss: 1.61425032e-06
Iter: 1134 loss: 1.61360163e-06
Iter: 1135 loss: 1.61356922e-06
Iter: 1136 loss: 1.61306571e-06
Iter: 1137 loss: 1.61219759e-06
Iter: 1138 loss: 1.61221988e-06
Iter: 1139 loss: 1.61154048e-06
Iter: 1140 loss: 1.61050139e-06
Iter: 1141 loss: 1.61280332e-06
Iter: 1142 loss: 1.6101194e-06
Iter: 1143 loss: 1.60907587e-06
Iter: 1144 loss: 1.60806621e-06
Iter: 1145 loss: 1.60783225e-06
Iter: 1146 loss: 1.60621323e-06
Iter: 1147 loss: 1.62224569e-06
Iter: 1148 loss: 1.60619106e-06
Iter: 1149 loss: 1.60522677e-06
Iter: 1150 loss: 1.60434786e-06
Iter: 1151 loss: 1.60405762e-06
Iter: 1152 loss: 1.60280126e-06
Iter: 1153 loss: 1.60282468e-06
Iter: 1154 loss: 1.6020017e-06
Iter: 1155 loss: 1.60158606e-06
Iter: 1156 loss: 1.60118225e-06
Iter: 1157 loss: 1.5996909e-06
Iter: 1158 loss: 1.60647755e-06
Iter: 1159 loss: 1.59939646e-06
Iter: 1160 loss: 1.59851061e-06
Iter: 1161 loss: 1.59818842e-06
Iter: 1162 loss: 1.59760395e-06
Iter: 1163 loss: 1.59689114e-06
Iter: 1164 loss: 1.59678814e-06
Iter: 1165 loss: 1.59599335e-06
Iter: 1166 loss: 1.6001394e-06
Iter: 1167 loss: 1.59583374e-06
Iter: 1168 loss: 1.59526314e-06
Iter: 1169 loss: 1.59471392e-06
Iter: 1170 loss: 1.59458796e-06
Iter: 1171 loss: 1.59375759e-06
Iter: 1172 loss: 1.59532942e-06
Iter: 1173 loss: 1.59340027e-06
Iter: 1174 loss: 1.59238721e-06
Iter: 1175 loss: 1.59273941e-06
Iter: 1176 loss: 1.59169076e-06
Iter: 1177 loss: 1.59083095e-06
Iter: 1178 loss: 1.59668093e-06
Iter: 1179 loss: 1.59080116e-06
Iter: 1180 loss: 1.58998762e-06
Iter: 1181 loss: 1.58959949e-06
Iter: 1182 loss: 1.58921898e-06
Iter: 1183 loss: 1.5881966e-06
Iter: 1184 loss: 1.59928243e-06
Iter: 1185 loss: 1.58819466e-06
Iter: 1186 loss: 1.58741409e-06
Iter: 1187 loss: 1.58819137e-06
Iter: 1188 loss: 1.58696901e-06
Iter: 1189 loss: 1.58596504e-06
Iter: 1190 loss: 1.59067895e-06
Iter: 1191 loss: 1.58584419e-06
Iter: 1192 loss: 1.58509829e-06
Iter: 1193 loss: 1.58481657e-06
Iter: 1194 loss: 1.58447961e-06
Iter: 1195 loss: 1.58335024e-06
Iter: 1196 loss: 1.59237845e-06
Iter: 1197 loss: 1.58329738e-06
Iter: 1198 loss: 1.58324701e-06
Iter: 1199 loss: 1.58294711e-06
Iter: 1200 loss: 1.58276202e-06
Iter: 1201 loss: 1.58212777e-06
Iter: 1202 loss: 1.58415855e-06
Iter: 1203 loss: 1.58186469e-06
Iter: 1204 loss: 1.58087721e-06
Iter: 1205 loss: 1.58677483e-06
Iter: 1206 loss: 1.58076273e-06
Iter: 1207 loss: 1.58020964e-06
Iter: 1208 loss: 1.58059038e-06
Iter: 1209 loss: 1.57988245e-06
Iter: 1210 loss: 1.57912621e-06
Iter: 1211 loss: 1.58000125e-06
Iter: 1212 loss: 1.57877753e-06
Iter: 1213 loss: 1.57798706e-06
Iter: 1214 loss: 1.58011403e-06
Iter: 1215 loss: 1.57771274e-06
Iter: 1216 loss: 1.57691238e-06
Iter: 1217 loss: 1.58141506e-06
Iter: 1218 loss: 1.57682393e-06
Iter: 1219 loss: 1.576155e-06
Iter: 1220 loss: 1.57656518e-06
Iter: 1221 loss: 1.57573254e-06
Iter: 1222 loss: 1.57518627e-06
Iter: 1223 loss: 1.57517866e-06
Iter: 1224 loss: 1.57471618e-06
Iter: 1225 loss: 1.5739804e-06
Iter: 1226 loss: 1.57400314e-06
Iter: 1227 loss: 1.57318095e-06
Iter: 1228 loss: 1.58165801e-06
Iter: 1229 loss: 1.57314275e-06
Iter: 1230 loss: 1.57283739e-06
Iter: 1231 loss: 1.57279385e-06
Iter: 1232 loss: 1.57239413e-06
Iter: 1233 loss: 1.57146451e-06
Iter: 1234 loss: 1.58329317e-06
Iter: 1235 loss: 1.57141119e-06
Iter: 1236 loss: 1.57076533e-06
Iter: 1237 loss: 1.57616387e-06
Iter: 1238 loss: 1.57071531e-06
Iter: 1239 loss: 1.57023146e-06
Iter: 1240 loss: 1.57085981e-06
Iter: 1241 loss: 1.57000454e-06
Iter: 1242 loss: 1.56938586e-06
Iter: 1243 loss: 1.56915303e-06
Iter: 1244 loss: 1.56878446e-06
Iter: 1245 loss: 1.56814292e-06
Iter: 1246 loss: 1.57106069e-06
Iter: 1247 loss: 1.56798728e-06
Iter: 1248 loss: 1.56737781e-06
Iter: 1249 loss: 1.56847045e-06
Iter: 1250 loss: 1.56709677e-06
Iter: 1251 loss: 1.56647286e-06
Iter: 1252 loss: 1.5681951e-06
Iter: 1253 loss: 1.56623901e-06
Iter: 1254 loss: 1.56567694e-06
Iter: 1255 loss: 1.57050454e-06
Iter: 1256 loss: 1.56563055e-06
Iter: 1257 loss: 1.56508872e-06
Iter: 1258 loss: 1.56502688e-06
Iter: 1259 loss: 1.56467991e-06
Iter: 1260 loss: 1.5639497e-06
Iter: 1261 loss: 1.56879196e-06
Iter: 1262 loss: 1.56393185e-06
Iter: 1263 loss: 1.5634788e-06
Iter: 1264 loss: 1.56567694e-06
Iter: 1265 loss: 1.56343629e-06
Iter: 1266 loss: 1.56285989e-06
Iter: 1267 loss: 1.5643684e-06
Iter: 1268 loss: 1.56264855e-06
Iter: 1269 loss: 1.56238593e-06
Iter: 1270 loss: 1.56223143e-06
Iter: 1271 loss: 1.56209046e-06
Iter: 1272 loss: 1.56164947e-06
Iter: 1273 loss: 1.56179499e-06
Iter: 1274 loss: 1.56131205e-06
Iter: 1275 loss: 1.56067688e-06
Iter: 1276 loss: 1.56394208e-06
Iter: 1277 loss: 1.56059855e-06
Iter: 1278 loss: 1.56018768e-06
Iter: 1279 loss: 1.55969019e-06
Iter: 1280 loss: 1.55965949e-06
Iter: 1281 loss: 1.5587309e-06
Iter: 1282 loss: 1.56290605e-06
Iter: 1283 loss: 1.55858038e-06
Iter: 1284 loss: 1.55803923e-06
Iter: 1285 loss: 1.5582574e-06
Iter: 1286 loss: 1.55753514e-06
Iter: 1287 loss: 1.55676798e-06
Iter: 1288 loss: 1.56286683e-06
Iter: 1289 loss: 1.55672456e-06
Iter: 1290 loss: 1.55616e-06
Iter: 1291 loss: 1.55822659e-06
Iter: 1292 loss: 1.55601833e-06
Iter: 1293 loss: 1.55539828e-06
Iter: 1294 loss: 1.55637531e-06
Iter: 1295 loss: 1.55520343e-06
Iter: 1296 loss: 1.55463249e-06
Iter: 1297 loss: 1.55703162e-06
Iter: 1298 loss: 1.55447492e-06
Iter: 1299 loss: 1.55415114e-06
Iter: 1300 loss: 1.55413886e-06
Iter: 1301 loss: 1.55393491e-06
Iter: 1302 loss: 1.55343514e-06
Iter: 1303 loss: 1.56024021e-06
Iter: 1304 loss: 1.55341922e-06
Iter: 1305 loss: 1.55288114e-06
Iter: 1306 loss: 1.55311238e-06
Iter: 1307 loss: 1.55249029e-06
Iter: 1308 loss: 1.55193129e-06
Iter: 1309 loss: 1.55986754e-06
Iter: 1310 loss: 1.55190071e-06
Iter: 1311 loss: 1.55152861e-06
Iter: 1312 loss: 1.55089651e-06
Iter: 1313 loss: 1.56639442e-06
Iter: 1314 loss: 1.55091948e-06
Iter: 1315 loss: 1.55017847e-06
Iter: 1316 loss: 1.55660825e-06
Iter: 1317 loss: 1.55013822e-06
Iter: 1318 loss: 1.54965596e-06
Iter: 1319 loss: 1.54889472e-06
Iter: 1320 loss: 1.54891222e-06
Iter: 1321 loss: 1.54799477e-06
Iter: 1322 loss: 1.56064152e-06
Iter: 1323 loss: 1.54802456e-06
Iter: 1324 loss: 1.54746795e-06
Iter: 1325 loss: 1.54789961e-06
Iter: 1326 loss: 1.54713621e-06
Iter: 1327 loss: 1.54629242e-06
Iter: 1328 loss: 1.54994871e-06
Iter: 1329 loss: 1.54617169e-06
Iter: 1330 loss: 1.54558882e-06
Iter: 1331 loss: 1.54741576e-06
Iter: 1332 loss: 1.54549639e-06
Iter: 1333 loss: 1.54510758e-06
Iter: 1334 loss: 1.54509826e-06
Iter: 1335 loss: 1.54468523e-06
Iter: 1336 loss: 1.5441899e-06
Iter: 1337 loss: 1.54416807e-06
Iter: 1338 loss: 1.54361589e-06
Iter: 1339 loss: 1.5430403e-06
Iter: 1340 loss: 1.54291365e-06
Iter: 1341 loss: 1.54212853e-06
Iter: 1342 loss: 1.54995507e-06
Iter: 1343 loss: 1.54213353e-06
Iter: 1344 loss: 1.54143504e-06
Iter: 1345 loss: 1.54089241e-06
Iter: 1346 loss: 1.54068118e-06
Iter: 1347 loss: 1.53991414e-06
Iter: 1348 loss: 1.54728718e-06
Iter: 1349 loss: 1.53986298e-06
Iter: 1350 loss: 1.53923156e-06
Iter: 1351 loss: 1.53823908e-06
Iter: 1352 loss: 1.53820224e-06
Iter: 1353 loss: 1.53718065e-06
Iter: 1354 loss: 1.53717406e-06
Iter: 1355 loss: 1.53643919e-06
Iter: 1356 loss: 1.53545272e-06
Iter: 1357 loss: 1.53537258e-06
Iter: 1358 loss: 1.53443511e-06
Iter: 1359 loss: 1.5397527e-06
Iter: 1360 loss: 1.534273e-06
Iter: 1361 loss: 1.53293604e-06
Iter: 1362 loss: 1.53659562e-06
Iter: 1363 loss: 1.53257838e-06
Iter: 1364 loss: 1.53169412e-06
Iter: 1365 loss: 1.53872e-06
Iter: 1366 loss: 1.53158305e-06
Iter: 1367 loss: 1.53115934e-06
Iter: 1368 loss: 1.53113251e-06
Iter: 1369 loss: 1.5307179e-06
Iter: 1370 loss: 1.52978612e-06
Iter: 1371 loss: 1.53892461e-06
Iter: 1372 loss: 1.52966913e-06
Iter: 1373 loss: 1.52917153e-06
Iter: 1374 loss: 1.53093379e-06
Iter: 1375 loss: 1.5289969e-06
Iter: 1376 loss: 1.5284752e-06
Iter: 1377 loss: 1.53049882e-06
Iter: 1378 loss: 1.52829398e-06
Iter: 1379 loss: 1.52769883e-06
Iter: 1380 loss: 1.52860844e-06
Iter: 1381 loss: 1.52734469e-06
Iter: 1382 loss: 1.52668736e-06
Iter: 1383 loss: 1.52687289e-06
Iter: 1384 loss: 1.52616894e-06
Iter: 1385 loss: 1.52531925e-06
Iter: 1386 loss: 1.52941163e-06
Iter: 1387 loss: 1.52511234e-06
Iter: 1388 loss: 1.52426423e-06
Iter: 1389 loss: 1.52486984e-06
Iter: 1390 loss: 1.52379619e-06
Iter: 1391 loss: 1.5229557e-06
Iter: 1392 loss: 1.52671396e-06
Iter: 1393 loss: 1.52280211e-06
Iter: 1394 loss: 1.52195616e-06
Iter: 1395 loss: 1.52354437e-06
Iter: 1396 loss: 1.52159623e-06
Iter: 1397 loss: 1.52087932e-06
Iter: 1398 loss: 1.52200278e-06
Iter: 1399 loss: 1.52055645e-06
Iter: 1400 loss: 1.51981317e-06
Iter: 1401 loss: 1.52887947e-06
Iter: 1402 loss: 1.51983613e-06
Iter: 1403 loss: 1.51957852e-06
Iter: 1404 loss: 1.51956988e-06
Iter: 1405 loss: 1.51926133e-06
Iter: 1406 loss: 1.51872564e-06
Iter: 1407 loss: 1.53099529e-06
Iter: 1408 loss: 1.51871018e-06
Iter: 1409 loss: 1.51810434e-06
Iter: 1410 loss: 1.51816323e-06
Iter: 1411 loss: 1.51764937e-06
Iter: 1412 loss: 1.5171513e-06
Iter: 1413 loss: 1.51672702e-06
Iter: 1414 loss: 1.51662948e-06
Iter: 1415 loss: 1.51588699e-06
Iter: 1416 loss: 1.51592258e-06
Iter: 1417 loss: 1.51550648e-06
Iter: 1418 loss: 1.51679319e-06
Iter: 1419 loss: 1.51541951e-06
Iter: 1420 loss: 1.51490769e-06
Iter: 1421 loss: 1.51439406e-06
Iter: 1422 loss: 1.51430402e-06
Iter: 1423 loss: 1.51365043e-06
Iter: 1424 loss: 1.51535301e-06
Iter: 1425 loss: 1.51340237e-06
Iter: 1426 loss: 1.51254267e-06
Iter: 1427 loss: 1.51511153e-06
Iter: 1428 loss: 1.51228335e-06
Iter: 1429 loss: 1.51167797e-06
Iter: 1430 loss: 1.51311735e-06
Iter: 1431 loss: 1.5114739e-06
Iter: 1432 loss: 1.5108767e-06
Iter: 1433 loss: 1.51327686e-06
Iter: 1434 loss: 1.51073345e-06
Iter: 1435 loss: 1.51014819e-06
Iter: 1436 loss: 1.51095878e-06
Iter: 1437 loss: 1.50998846e-06
Iter: 1438 loss: 1.5096191e-06
Iter: 1439 loss: 1.50957544e-06
Iter: 1440 loss: 1.50917981e-06
Iter: 1441 loss: 1.50964297e-06
Iter: 1442 loss: 1.50893732e-06
Iter: 1443 loss: 1.50856056e-06
Iter: 1444 loss: 1.50779601e-06
Iter: 1445 loss: 1.5262807e-06
Iter: 1446 loss: 1.50779078e-06
Iter: 1447 loss: 1.50717824e-06
Iter: 1448 loss: 1.51067059e-06
Iter: 1449 loss: 1.50706433e-06
Iter: 1450 loss: 1.50648486e-06
Iter: 1451 loss: 1.50609117e-06
Iter: 1452 loss: 1.50586141e-06
Iter: 1453 loss: 1.50533469e-06
Iter: 1454 loss: 1.50529195e-06
Iter: 1455 loss: 1.50486744e-06
Iter: 1456 loss: 1.50501478e-06
Iter: 1457 loss: 1.50457413e-06
Iter: 1458 loss: 1.50383835e-06
Iter: 1459 loss: 1.50452149e-06
Iter: 1460 loss: 1.50341202e-06
Iter: 1461 loss: 1.50287269e-06
Iter: 1462 loss: 1.50358335e-06
Iter: 1463 loss: 1.50261258e-06
Iter: 1464 loss: 1.50177675e-06
Iter: 1465 loss: 1.50491508e-06
Iter: 1466 loss: 1.50164544e-06
Iter: 1467 loss: 1.50102551e-06
Iter: 1468 loss: 1.50040592e-06
Iter: 1469 loss: 1.50031678e-06
Iter: 1470 loss: 1.49935352e-06
Iter: 1471 loss: 1.51214158e-06
Iter: 1472 loss: 1.49935784e-06
Iter: 1473 loss: 1.49907987e-06
Iter: 1474 loss: 1.49906373e-06
Iter: 1475 loss: 1.49862922e-06
Iter: 1476 loss: 1.49850496e-06
Iter: 1477 loss: 1.49828929e-06
Iter: 1478 loss: 1.49785444e-06
Iter: 1479 loss: 1.49827929e-06
Iter: 1480 loss: 1.49753532e-06
Iter: 1481 loss: 1.49708956e-06
Iter: 1482 loss: 1.49668449e-06
Iter: 1483 loss: 1.49657114e-06
Iter: 1484 loss: 1.49570985e-06
Iter: 1485 loss: 1.5009839e-06
Iter: 1486 loss: 1.49560992e-06
Iter: 1487 loss: 1.49506855e-06
Iter: 1488 loss: 1.49484492e-06
Iter: 1489 loss: 1.49455866e-06
Iter: 1490 loss: 1.49383459e-06
Iter: 1491 loss: 1.50140613e-06
Iter: 1492 loss: 1.49380753e-06
Iter: 1493 loss: 1.49315929e-06
Iter: 1494 loss: 1.49464631e-06
Iter: 1495 loss: 1.4929692e-06
Iter: 1496 loss: 1.49228276e-06
Iter: 1497 loss: 1.49281755e-06
Iter: 1498 loss: 1.49189952e-06
Iter: 1499 loss: 1.49108655e-06
Iter: 1500 loss: 1.49042603e-06
Iter: 1501 loss: 1.49023992e-06
Iter: 1502 loss: 1.4892737e-06
Iter: 1503 loss: 1.48927688e-06
Iter: 1504 loss: 1.48855543e-06
Iter: 1505 loss: 1.48872141e-06
Iter: 1506 loss: 1.48801564e-06
Iter: 1507 loss: 1.48835431e-06
Iter: 1508 loss: 1.48760614e-06
Iter: 1509 loss: 1.4874006e-06
Iter: 1510 loss: 1.48697836e-06
Iter: 1511 loss: 1.49008122e-06
Iter: 1512 loss: 1.4868217e-06
Iter: 1513 loss: 1.48604613e-06
Iter: 1514 loss: 1.48548213e-06
Iter: 1515 loss: 1.48518257e-06
Iter: 1516 loss: 1.48422157e-06
Iter: 1517 loss: 1.49803031e-06
Iter: 1518 loss: 1.48417143e-06
Iter: 1519 loss: 1.48357037e-06
Iter: 1520 loss: 1.48271261e-06
Iter: 1521 loss: 1.4827101e-06
Iter: 1522 loss: 1.48160734e-06
Iter: 1523 loss: 1.49061214e-06
Iter: 1524 loss: 1.48152196e-06
Iter: 1525 loss: 1.48080062e-06
Iter: 1526 loss: 1.48233062e-06
Iter: 1527 loss: 1.48051595e-06
Iter: 1528 loss: 1.47951982e-06
Iter: 1529 loss: 1.48135189e-06
Iter: 1530 loss: 1.47907872e-06
Iter: 1531 loss: 1.47831145e-06
Iter: 1532 loss: 1.47907758e-06
Iter: 1533 loss: 1.47787989e-06
Iter: 1534 loss: 1.47668743e-06
Iter: 1535 loss: 1.47609467e-06
Iter: 1536 loss: 1.47554272e-06
Iter: 1537 loss: 1.47438629e-06
Iter: 1538 loss: 1.48280242e-06
Iter: 1539 loss: 1.47427022e-06
Iter: 1540 loss: 1.47304843e-06
Iter: 1541 loss: 1.47595108e-06
Iter: 1542 loss: 1.47260653e-06
Iter: 1543 loss: 1.47247613e-06
Iter: 1544 loss: 1.47219089e-06
Iter: 1545 loss: 1.47183016e-06
Iter: 1546 loss: 1.47130663e-06
Iter: 1547 loss: 1.47125729e-06
Iter: 1548 loss: 1.47056608e-06
Iter: 1549 loss: 1.46987372e-06
Iter: 1550 loss: 1.46976868e-06
Iter: 1551 loss: 1.46879529e-06
Iter: 1552 loss: 1.46963134e-06
Iter: 1553 loss: 1.46817e-06
Iter: 1554 loss: 1.4670336e-06
Iter: 1555 loss: 1.48084541e-06
Iter: 1556 loss: 1.46704178e-06
Iter: 1557 loss: 1.46625007e-06
Iter: 1558 loss: 1.46677507e-06
Iter: 1559 loss: 1.46578361e-06
Iter: 1560 loss: 1.46483239e-06
Iter: 1561 loss: 1.46637183e-06
Iter: 1562 loss: 1.46436196e-06
Iter: 1563 loss: 1.46334503e-06
Iter: 1564 loss: 1.46673619e-06
Iter: 1565 loss: 1.46310617e-06
Iter: 1566 loss: 1.46211801e-06
Iter: 1567 loss: 1.4705638e-06
Iter: 1568 loss: 1.46207321e-06
Iter: 1569 loss: 1.46137904e-06
Iter: 1570 loss: 1.4609966e-06
Iter: 1571 loss: 1.46082357e-06
Iter: 1572 loss: 1.45954596e-06
Iter: 1573 loss: 1.46268235e-06
Iter: 1574 loss: 1.45911133e-06
Iter: 1575 loss: 1.45827039e-06
Iter: 1576 loss: 1.45945546e-06
Iter: 1577 loss: 1.45787465e-06
Iter: 1578 loss: 1.45734134e-06
Iter: 1579 loss: 1.45721788e-06
Iter: 1580 loss: 1.45659931e-06
Iter: 1581 loss: 1.4563667e-06
Iter: 1582 loss: 1.45601553e-06
Iter: 1583 loss: 1.45556805e-06
Iter: 1584 loss: 1.45461763e-06
Iter: 1585 loss: 1.4709002e-06
Iter: 1586 loss: 1.45455863e-06
Iter: 1587 loss: 1.45335616e-06
Iter: 1588 loss: 1.46282844e-06
Iter: 1589 loss: 1.45326317e-06
Iter: 1590 loss: 1.45247736e-06
Iter: 1591 loss: 1.45267904e-06
Iter: 1592 loss: 1.45194417e-06
Iter: 1593 loss: 1.45070817e-06
Iter: 1594 loss: 1.45774709e-06
Iter: 1595 loss: 1.45055515e-06
Iter: 1596 loss: 1.44974433e-06
Iter: 1597 loss: 1.45304875e-06
Iter: 1598 loss: 1.44960154e-06
Iter: 1599 loss: 1.44885053e-06
Iter: 1600 loss: 1.44891794e-06
Iter: 1601 loss: 1.44827141e-06
Iter: 1602 loss: 1.44755575e-06
Iter: 1603 loss: 1.45848867e-06
Iter: 1604 loss: 1.4475703e-06
Iter: 1605 loss: 1.44698299e-06
Iter: 1606 loss: 1.44681439e-06
Iter: 1607 loss: 1.44653e-06
Iter: 1608 loss: 1.44573153e-06
Iter: 1609 loss: 1.44707928e-06
Iter: 1610 loss: 1.44538558e-06
Iter: 1611 loss: 1.44452622e-06
Iter: 1612 loss: 1.44719138e-06
Iter: 1613 loss: 1.4443217e-06
Iter: 1614 loss: 1.44427406e-06
Iter: 1615 loss: 1.44391981e-06
Iter: 1616 loss: 1.44372768e-06
Iter: 1617 loss: 1.44306216e-06
Iter: 1618 loss: 1.44663045e-06
Iter: 1619 loss: 1.44291653e-06
Iter: 1620 loss: 1.44204375e-06
Iter: 1621 loss: 1.44325054e-06
Iter: 1622 loss: 1.44165176e-06
Iter: 1623 loss: 1.44082924e-06
Iter: 1624 loss: 1.44159333e-06
Iter: 1625 loss: 1.44037313e-06
Iter: 1626 loss: 1.43935586e-06
Iter: 1627 loss: 1.44603337e-06
Iter: 1628 loss: 1.43922773e-06
Iter: 1629 loss: 1.43870875e-06
Iter: 1630 loss: 1.43821171e-06
Iter: 1631 loss: 1.43804937e-06
Iter: 1632 loss: 1.43719558e-06
Iter: 1633 loss: 1.45129729e-06
Iter: 1634 loss: 1.43720808e-06
Iter: 1635 loss: 1.4367871e-06
Iter: 1636 loss: 1.43579098e-06
Iter: 1637 loss: 1.45103e-06
Iter: 1638 loss: 1.4357704e-06
Iter: 1639 loss: 1.43496857e-06
Iter: 1640 loss: 1.4349489e-06
Iter: 1641 loss: 1.43433374e-06
Iter: 1642 loss: 1.43559907e-06
Iter: 1643 loss: 1.43407897e-06
Iter: 1644 loss: 1.43356169e-06
Iter: 1645 loss: 1.43311831e-06
Iter: 1646 loss: 1.43297825e-06
Iter: 1647 loss: 1.43283717e-06
Iter: 1648 loss: 1.43254817e-06
Iter: 1649 loss: 1.43206148e-06
Iter: 1650 loss: 1.43181842e-06
Iter: 1651 loss: 1.43162697e-06
Iter: 1652 loss: 1.43106865e-06
Iter: 1653 loss: 1.43065e-06
Iter: 1654 loss: 1.43056195e-06
Iter: 1655 loss: 1.42976899e-06
Iter: 1656 loss: 1.43005104e-06
Iter: 1657 loss: 1.4291885e-06
Iter: 1658 loss: 1.42839906e-06
Iter: 1659 loss: 1.43385785e-06
Iter: 1660 loss: 1.42825274e-06
Iter: 1661 loss: 1.42744091e-06
Iter: 1662 loss: 1.42768795e-06
Iter: 1663 loss: 1.42684644e-06
Iter: 1664 loss: 1.4262389e-06
Iter: 1665 loss: 1.43343254e-06
Iter: 1666 loss: 1.42620388e-06
Iter: 1667 loss: 1.42548436e-06
Iter: 1668 loss: 1.42627641e-06
Iter: 1669 loss: 1.42502563e-06
Iter: 1670 loss: 1.42447368e-06
Iter: 1671 loss: 1.42977046e-06
Iter: 1672 loss: 1.42443014e-06
Iter: 1673 loss: 1.42391082e-06
Iter: 1674 loss: 1.42353008e-06
Iter: 1675 loss: 1.42341355e-06
Iter: 1676 loss: 1.42254669e-06
Iter: 1677 loss: 1.42957867e-06
Iter: 1678 loss: 1.42251019e-06
Iter: 1679 loss: 1.42211e-06
Iter: 1680 loss: 1.4225584e-06
Iter: 1681 loss: 1.42190038e-06
Iter: 1682 loss: 1.42132808e-06
Iter: 1683 loss: 1.42178646e-06
Iter: 1684 loss: 1.4209545e-06
Iter: 1685 loss: 1.4208556e-06
Iter: 1686 loss: 1.42060924e-06
Iter: 1687 loss: 1.42045656e-06
Iter: 1688 loss: 1.41995724e-06
Iter: 1689 loss: 1.42216049e-06
Iter: 1690 loss: 1.41977603e-06
Iter: 1691 loss: 1.41908276e-06
Iter: 1692 loss: 1.42056842e-06
Iter: 1693 loss: 1.41874068e-06
Iter: 1694 loss: 1.41803935e-06
Iter: 1695 loss: 1.41850762e-06
Iter: 1696 loss: 1.41767032e-06
Iter: 1697 loss: 1.41669966e-06
Iter: 1698 loss: 1.42149156e-06
Iter: 1699 loss: 1.41654687e-06
Iter: 1700 loss: 1.41574196e-06
Iter: 1701 loss: 1.41583416e-06
Iter: 1702 loss: 1.41512169e-06
Iter: 1703 loss: 1.41415808e-06
Iter: 1704 loss: 1.41947078e-06
Iter: 1705 loss: 1.41393775e-06
Iter: 1706 loss: 1.41304213e-06
Iter: 1707 loss: 1.41592045e-06
Iter: 1708 loss: 1.41284636e-06
Iter: 1709 loss: 1.41198e-06
Iter: 1710 loss: 1.41278383e-06
Iter: 1711 loss: 1.41149803e-06
Iter: 1712 loss: 1.41045052e-06
Iter: 1713 loss: 1.41847636e-06
Iter: 1714 loss: 1.41035639e-06
Iter: 1715 loss: 1.40969087e-06
Iter: 1716 loss: 1.41233272e-06
Iter: 1717 loss: 1.40952284e-06
Iter: 1718 loss: 1.40878637e-06
Iter: 1719 loss: 1.40841212e-06
Iter: 1720 loss: 1.40812199e-06
Iter: 1721 loss: 1.40718021e-06
Iter: 1722 loss: 1.42002659e-06
Iter: 1723 loss: 1.40718464e-06
Iter: 1724 loss: 1.40677912e-06
Iter: 1725 loss: 1.40931161e-06
Iter: 1726 loss: 1.40672091e-06
Iter: 1727 loss: 1.40621501e-06
Iter: 1728 loss: 1.40534087e-06
Iter: 1729 loss: 1.4053345e-06
Iter: 1730 loss: 1.40456848e-06
Iter: 1731 loss: 1.40595512e-06
Iter: 1732 loss: 1.40427142e-06
Iter: 1733 loss: 1.40368968e-06
Iter: 1734 loss: 1.40327154e-06
Iter: 1735 loss: 1.40301904e-06
Iter: 1736 loss: 1.40238058e-06
Iter: 1737 loss: 1.40235386e-06
Iter: 1738 loss: 1.40181987e-06
Iter: 1739 loss: 1.40092197e-06
Iter: 1740 loss: 1.4009114e-06
Iter: 1741 loss: 1.40002578e-06
Iter: 1742 loss: 1.4073762e-06
Iter: 1743 loss: 1.39995188e-06
Iter: 1744 loss: 1.39925328e-06
Iter: 1745 loss: 1.40270049e-06
Iter: 1746 loss: 1.39916699e-06
Iter: 1747 loss: 1.39841779e-06
Iter: 1748 loss: 1.39967528e-06
Iter: 1749 loss: 1.39807889e-06
Iter: 1750 loss: 1.39726285e-06
Iter: 1751 loss: 1.40251689e-06
Iter: 1752 loss: 1.39715428e-06
Iter: 1753 loss: 1.39660449e-06
Iter: 1754 loss: 1.39695658e-06
Iter: 1755 loss: 1.39624478e-06
Iter: 1756 loss: 1.39539861e-06
Iter: 1757 loss: 1.39806525e-06
Iter: 1758 loss: 1.39511849e-06
Iter: 1759 loss: 1.39493557e-06
Iter: 1760 loss: 1.39475344e-06
Iter: 1761 loss: 1.39444751e-06
Iter: 1762 loss: 1.39401868e-06
Iter: 1763 loss: 1.40242128e-06
Iter: 1764 loss: 1.39398333e-06
Iter: 1765 loss: 1.3933028e-06
Iter: 1766 loss: 1.39280917e-06
Iter: 1767 loss: 1.39257168e-06
Iter: 1768 loss: 1.39181623e-06
Iter: 1769 loss: 1.39270253e-06
Iter: 1770 loss: 1.39138729e-06
Iter: 1771 loss: 1.39027088e-06
Iter: 1772 loss: 1.39551503e-06
Iter: 1773 loss: 1.39009126e-06
Iter: 1774 loss: 1.38944824e-06
Iter: 1775 loss: 1.38960468e-06
Iter: 1776 loss: 1.38906137e-06
Iter: 1777 loss: 1.38812857e-06
Iter: 1778 loss: 1.39392978e-06
Iter: 1779 loss: 1.38803148e-06
Iter: 1780 loss: 1.38731787e-06
Iter: 1781 loss: 1.38742053e-06
Iter: 1782 loss: 1.3868239e-06
Iter: 1783 loss: 1.38599512e-06
Iter: 1784 loss: 1.39433155e-06
Iter: 1785 loss: 1.38596658e-06
Iter: 1786 loss: 1.38529686e-06
Iter: 1787 loss: 1.3865706e-06
Iter: 1788 loss: 1.38503913e-06
Iter: 1789 loss: 1.38425912e-06
Iter: 1790 loss: 1.38656037e-06
Iter: 1791 loss: 1.38405221e-06
Iter: 1792 loss: 1.3835255e-06
Iter: 1793 loss: 1.38782491e-06
Iter: 1794 loss: 1.38347809e-06
Iter: 1795 loss: 1.38290079e-06
Iter: 1796 loss: 1.38455152e-06
Iter: 1797 loss: 1.38270366e-06
Iter: 1798 loss: 1.38233395e-06
Iter: 1799 loss: 1.38243922e-06
Iter: 1800 loss: 1.3820428e-06
Iter: 1801 loss: 1.38160283e-06
Iter: 1802 loss: 1.38064115e-06
Iter: 1803 loss: 1.40062366e-06
Iter: 1804 loss: 1.38064831e-06
Iter: 1805 loss: 1.37951508e-06
Iter: 1806 loss: 1.38738073e-06
Iter: 1807 loss: 1.3794288e-06
Iter: 1808 loss: 1.37861116e-06
Iter: 1809 loss: 1.37734128e-06
Iter: 1810 loss: 1.3772916e-06
Iter: 1811 loss: 1.37640177e-06
Iter: 1812 loss: 1.37636562e-06
Iter: 1813 loss: 1.37545578e-06
Iter: 1814 loss: 1.37474603e-06
Iter: 1815 loss: 1.37451377e-06
Iter: 1816 loss: 1.37351617e-06
Iter: 1817 loss: 1.387206e-06
Iter: 1818 loss: 1.37351117e-06
Iter: 1819 loss: 1.37265442e-06
Iter: 1820 loss: 1.37166751e-06
Iter: 1821 loss: 1.37158963e-06
Iter: 1822 loss: 1.37063739e-06
Iter: 1823 loss: 1.38323912e-06
Iter: 1824 loss: 1.37060715e-06
Iter: 1825 loss: 1.36977042e-06
Iter: 1826 loss: 1.37121822e-06
Iter: 1827 loss: 1.36939036e-06
Iter: 1828 loss: 1.3687204e-06
Iter: 1829 loss: 1.36869403e-06
Iter: 1830 loss: 1.36822098e-06
Iter: 1831 loss: 1.36774906e-06
Iter: 1832 loss: 1.36756125e-06
Iter: 1833 loss: 1.36694212e-06
Iter: 1834 loss: 1.36684275e-06
Iter: 1835 loss: 1.36635902e-06
Iter: 1836 loss: 1.36534459e-06
Iter: 1837 loss: 1.3691465e-06
Iter: 1838 loss: 1.36508322e-06
Iter: 1839 loss: 1.36408585e-06
Iter: 1840 loss: 1.36429787e-06
Iter: 1841 loss: 1.36340532e-06
Iter: 1842 loss: 1.36237622e-06
Iter: 1843 loss: 1.37070674e-06
Iter: 1844 loss: 1.36226276e-06
Iter: 1845 loss: 1.36145502e-06
Iter: 1846 loss: 1.36036772e-06
Iter: 1847 loss: 1.36033134e-06
Iter: 1848 loss: 1.35895652e-06
Iter: 1849 loss: 1.36295318e-06
Iter: 1850 loss: 1.35852542e-06
Iter: 1851 loss: 1.35714777e-06
Iter: 1852 loss: 1.3695319e-06
Iter: 1853 loss: 1.35702703e-06
Iter: 1854 loss: 1.35621372e-06
Iter: 1855 loss: 1.35513596e-06
Iter: 1856 loss: 1.35503296e-06
Iter: 1857 loss: 1.35405139e-06
Iter: 1858 loss: 1.3539634e-06
Iter: 1859 loss: 1.35327991e-06
Iter: 1860 loss: 1.35504456e-06
Iter: 1861 loss: 1.35305e-06
Iter: 1862 loss: 1.35207847e-06
Iter: 1863 loss: 1.35730238e-06
Iter: 1864 loss: 1.35197524e-06
Iter: 1865 loss: 1.35115647e-06
Iter: 1866 loss: 1.3509424e-06
Iter: 1867 loss: 1.3504507e-06
Iter: 1868 loss: 1.34974232e-06
Iter: 1869 loss: 1.34989864e-06
Iter: 1870 loss: 1.3492604e-06
Iter: 1871 loss: 1.34807135e-06
Iter: 1872 loss: 1.34931281e-06
Iter: 1873 loss: 1.34745869e-06
Iter: 1874 loss: 1.34658512e-06
Iter: 1875 loss: 1.35746404e-06
Iter: 1876 loss: 1.34655306e-06
Iter: 1877 loss: 1.34590573e-06
Iter: 1878 loss: 1.3446072e-06
Iter: 1879 loss: 1.37327572e-06
Iter: 1880 loss: 1.34458116e-06
Iter: 1881 loss: 1.34347624e-06
Iter: 1882 loss: 1.35892378e-06
Iter: 1883 loss: 1.34348988e-06
Iter: 1884 loss: 1.34253173e-06
Iter: 1885 loss: 1.34245454e-06
Iter: 1886 loss: 1.34180777e-06
Iter: 1887 loss: 1.340675e-06
Iter: 1888 loss: 1.34512743e-06
Iter: 1889 loss: 1.34046581e-06
Iter: 1890 loss: 1.33912135e-06
Iter: 1891 loss: 1.34242964e-06
Iter: 1892 loss: 1.33881531e-06
Iter: 1893 loss: 1.33781259e-06
Iter: 1894 loss: 1.34172865e-06
Iter: 1895 loss: 1.33751519e-06
Iter: 1896 loss: 1.33698563e-06
Iter: 1897 loss: 1.33696892e-06
Iter: 1898 loss: 1.33633398e-06
Iter: 1899 loss: 1.3380494e-06
Iter: 1900 loss: 1.33616595e-06
Iter: 1901 loss: 1.33569847e-06
Iter: 1902 loss: 1.33501965e-06
Iter: 1903 loss: 1.33502158e-06
Iter: 1904 loss: 1.33389972e-06
Iter: 1905 loss: 1.33670767e-06
Iter: 1906 loss: 1.33350181e-06
Iter: 1907 loss: 1.33277797e-06
Iter: 1908 loss: 1.33238791e-06
Iter: 1909 loss: 1.33203889e-06
Iter: 1910 loss: 1.33085177e-06
Iter: 1911 loss: 1.34001823e-06
Iter: 1912 loss: 1.33070455e-06
Iter: 1913 loss: 1.32998309e-06
Iter: 1914 loss: 1.33148171e-06
Iter: 1915 loss: 1.32965033e-06
Iter: 1916 loss: 1.32867081e-06
Iter: 1917 loss: 1.32835657e-06
Iter: 1918 loss: 1.32777927e-06
Iter: 1919 loss: 1.32682919e-06
Iter: 1920 loss: 1.33216577e-06
Iter: 1921 loss: 1.32665127e-06
Iter: 1922 loss: 1.32550258e-06
Iter: 1923 loss: 1.32730872e-06
Iter: 1924 loss: 1.32492073e-06
Iter: 1925 loss: 1.32395451e-06
Iter: 1926 loss: 1.32324203e-06
Iter: 1927 loss: 1.32291802e-06
Iter: 1928 loss: 1.32205685e-06
Iter: 1929 loss: 1.32201e-06
Iter: 1930 loss: 1.32156174e-06
Iter: 1931 loss: 1.32649086e-06
Iter: 1932 loss: 1.32155571e-06
Iter: 1933 loss: 1.32099285e-06
Iter: 1934 loss: 1.32019431e-06
Iter: 1935 loss: 1.32014429e-06
Iter: 1936 loss: 1.31944671e-06
Iter: 1937 loss: 1.32213859e-06
Iter: 1938 loss: 1.31925583e-06
Iter: 1939 loss: 1.31869399e-06
Iter: 1940 loss: 1.31954084e-06
Iter: 1941 loss: 1.31843683e-06
Iter: 1942 loss: 1.31755701e-06
Iter: 1943 loss: 1.31776733e-06
Iter: 1944 loss: 1.31692059e-06
Iter: 1945 loss: 1.31620777e-06
Iter: 1946 loss: 1.31777028e-06
Iter: 1947 loss: 1.31590355e-06
Iter: 1948 loss: 1.31492402e-06
Iter: 1949 loss: 1.31986826e-06
Iter: 1950 loss: 1.31477236e-06
Iter: 1951 loss: 1.31398895e-06
Iter: 1952 loss: 1.31414947e-06
Iter: 1953 loss: 1.31346189e-06
Iter: 1954 loss: 1.31236663e-06
Iter: 1955 loss: 1.31501429e-06
Iter: 1956 loss: 1.31200784e-06
Iter: 1957 loss: 1.31104559e-06
Iter: 1958 loss: 1.31366653e-06
Iter: 1959 loss: 1.31074421e-06
Iter: 1960 loss: 1.30988337e-06
Iter: 1961 loss: 1.31467618e-06
Iter: 1962 loss: 1.30968135e-06
Iter: 1963 loss: 1.30893477e-06
Iter: 1964 loss: 1.30963394e-06
Iter: 1965 loss: 1.30848116e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/300_300_300_1
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0
+ date
Wed Oct 21 18:59:36 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1 --function f1 --psi -2 --phi 0 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcdd1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcdd97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcec9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcec9ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcdf22f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcdf29d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcdf2488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcd72730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dccee158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcdda378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcdda268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcc9aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcc720d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcc21ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcc21bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcc21c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcc218c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcc21950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcc33620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d21b3b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d21b3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d20fd8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d20ea400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d20c1ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d21dd7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d21ddf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d20ace18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d20acb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49dcbcf378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d20ac620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d2046d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d1fe9378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d1fe3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d1f5e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d1f122f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f49d1f2b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.026792059
test_loss: 0.025782004
train_loss: 0.013527652
test_loss: 0.013424188
train_loss: 0.008666231
test_loss: 0.00857091
train_loss: 0.0067645563
test_loss: 0.00710811
train_loss: 0.0062166015
test_loss: 0.006598784
train_loss: 0.005838677
test_loss: 0.0056223106
train_loss: 0.0055716615
test_loss: 0.0053844163
train_loss: 0.0052991216
test_loss: 0.0065197805
train_loss: 0.0050971117
test_loss: 0.0050788494
train_loss: 0.004675249
test_loss: 0.004771722
train_loss: 0.00484319
test_loss: 0.0048280586
train_loss: 0.0058375765
test_loss: 0.0057060034
train_loss: 0.005040001
test_loss: 0.004955926
train_loss: 0.004545065
test_loss: 0.004660461
train_loss: 0.0046842005
test_loss: 0.004317403
train_loss: 0.0043464624
test_loss: 0.0043846993
train_loss: 0.004307023
test_loss: 0.0044720965
train_loss: 0.0045093168
test_loss: 0.0043159844
train_loss: 0.0041611735
test_loss: 0.004222661
train_loss: 0.0045373645
test_loss: 0.0043077143
train_loss: 0.004401282
test_loss: 0.004157212
train_loss: 0.0042802873
test_loss: 0.00417054
train_loss: 0.0038721242
test_loss: 0.0044959574
train_loss: 0.004205518
test_loss: 0.0048987935
train_loss: 0.004808737
test_loss: 0.0044188784
train_loss: 0.0043297606
test_loss: 0.0046790284
train_loss: 0.0038651084
test_loss: 0.0043148533
train_loss: 0.0042574927
test_loss: 0.0042557917
train_loss: 0.004085925
test_loss: 0.0040824725
train_loss: 0.004064997
test_loss: 0.004108996
train_loss: 0.0038508456
test_loss: 0.0039410032
train_loss: 0.0038735606
test_loss: 0.0039588627
train_loss: 0.0038033142
test_loss: 0.0045590103
train_loss: 0.0046777423
test_loss: 0.004139631
train_loss: 0.003633537
test_loss: 0.0038938727
train_loss: 0.004487706
test_loss: 0.004333681
train_loss: 0.004367527
test_loss: 0.003954869
train_loss: 0.0038010422
test_loss: 0.0037726997
train_loss: 0.0037479186
test_loss: 0.0037222435
train_loss: 0.0036570441
test_loss: 0.0038770544
train_loss: 0.0035016004
test_loss: 0.0036871657
train_loss: 0.0034691598
test_loss: 0.0037081705
train_loss: 0.0036577033
test_loss: 0.00381748
train_loss: 0.0035241307
test_loss: 0.0037526619
train_loss: 0.003435182
test_loss: 0.0034394097
train_loss: 0.0037366608
test_loss: 0.0035490652
train_loss: 0.003711796
test_loss: 0.0038130817
train_loss: 0.0035135425
test_loss: 0.004116919
train_loss: 0.0035054055
test_loss: 0.0035957063
train_loss: 0.0040879464
test_loss: 0.0037821268
train_loss: 0.004231817
test_loss: 0.0038685303
train_loss: 0.003504015
test_loss: 0.0038200119
train_loss: 0.0036038063
test_loss: 0.003513291
train_loss: 0.004568233
test_loss: 0.0036442769
train_loss: 0.0039862525
test_loss: 0.0036921327
train_loss: 0.0035782678
test_loss: 0.0038992898
train_loss: 0.003563593
test_loss: 0.003708682
train_loss: 0.0033851108
test_loss: 0.003657131
train_loss: 0.0033808348
test_loss: 0.00429165
train_loss: 0.0037533105
test_loss: 0.004178275
train_loss: 0.0034522915
test_loss: 0.0038276466
train_loss: 0.0032899184
test_loss: 0.0036377313
train_loss: 0.0033555035
test_loss: 0.0036284279
train_loss: 0.0033460804
test_loss: 0.0035313237
train_loss: 0.0032844841
test_loss: 0.0036298742
train_loss: 0.0033424194
test_loss: 0.0035092887
train_loss: 0.003423958
test_loss: 0.0033537499
train_loss: 0.0036250597
test_loss: 0.0042267386
train_loss: 0.0032779186
test_loss: 0.0035527684
train_loss: 0.0036970421
test_loss: 0.0040769596
train_loss: 0.0034559886
test_loss: 0.0034564666
train_loss: 0.0032378938
test_loss: 0.0033977896
train_loss: 0.0033601404
test_loss: 0.0034046848
train_loss: 0.0032193218
test_loss: 0.0035624662
train_loss: 0.0031510117
test_loss: 0.0036889776
train_loss: 0.0033936072
test_loss: 0.0033786243
train_loss: 0.003339873
test_loss: 0.0035283237
train_loss: 0.0038870946
test_loss: 0.0039468254
train_loss: 0.003211133
test_loss: 0.0034827774
train_loss: 0.003254854
test_loss: 0.0036912125
train_loss: 0.003509264
test_loss: 0.0034631724
train_loss: 0.0033477854
test_loss: 0.0037575762
train_loss: 0.0035160114
test_loss: 0.0037266565
train_loss: 0.0033329623
test_loss: 0.0033756078
train_loss: 0.0031622367
test_loss: 0.0034666397
train_loss: 0.0032921308
test_loss: 0.0033941087
train_loss: 0.0030920417
test_loss: 0.0033933888
train_loss: 0.0032905636
test_loss: 0.0035204012
train_loss: 0.0033216246
test_loss: 0.004198833
train_loss: 0.0031799376
test_loss: 0.0034877753
train_loss: 0.003150436
test_loss: 0.0035840878
train_loss: 0.0034549248
test_loss: 0.003694213
train_loss: 0.0034906045
test_loss: 0.00375512
train_loss: 0.003122718
test_loss: 0.0033703723
train_loss: 0.0034774279
test_loss: 0.0036436066
train_loss: 0.0030932594
test_loss: 0.003435301
train_loss: 0.003048921
test_loss: 0.0032496
train_loss: 0.003425418
test_loss: 0.0033350517
train_loss: 0.003271909
test_loss: 0.0033585273
train_loss: 0.003413729
test_loss: 0.0034835709
train_loss: 0.003490857
test_loss: 0.003488933
train_loss: 0.003373205
test_loss: 0.0035838934
train_loss: 0.0031305142
test_loss: 0.003219028
train_loss: 0.003185783
test_loss: 0.0033958412
train_loss: 0.003000516
test_loss: 0.0033066722
train_loss: 0.0030365526
test_loss: 0.003389304
train_loss: 0.0036563063
test_loss: 0.0036172634
train_loss: 0.0030976753
test_loss: 0.0033922691
train_loss: 0.003042
test_loss: 0.003765082
train_loss: 0.0035747834
test_loss: 0.003900269
train_loss: 0.0034423121
test_loss: 0.003780158
train_loss: 0.0038683692
test_loss: 0.0039496752
train_loss: 0.0032330104
test_loss: 0.0035545612
train_loss: 0.00317814
test_loss: 0.0033985605
train_loss: 0.0029668282
test_loss: 0.0032423723
train_loss: 0.0029994135
test_loss: 0.0036707905
train_loss: 0.0030768309
test_loss: 0.0033224805
train_loss: 0.0030788067
test_loss: 0.0033609525
train_loss: 0.0035609892
test_loss: 0.0034887146
train_loss: 0.002978851
test_loss: 0.0034226996
train_loss: 0.0030836295
test_loss: 0.0032275629
train_loss: 0.0029415968
test_loss: 0.0031923882
train_loss: 0.0029294216
test_loss: 0.0035173555
train_loss: 0.002946407
test_loss: 0.0032931208
train_loss: 0.0028452661
test_loss: 0.0033785137
train_loss: 0.0033865194
test_loss: 0.003959773
train_loss: 0.0031513493
test_loss: 0.0033371167
train_loss: 0.0029173421
test_loss: 0.0032066796
train_loss: 0.0032094463
test_loss: 0.003146625
train_loss: 0.0029422643
test_loss: 0.0031440407
train_loss: 0.0027932469
test_loss: 0.0031948937
train_loss: 0.0029564356
test_loss: 0.003391618
train_loss: 0.0030763387
test_loss: 0.0033695225
train_loss: 0.00294633
test_loss: 0.0034256366
train_loss: 0.0028272336
test_loss: 0.0032129406
train_loss: 0.0030586882
test_loss: 0.0032084412
train_loss: 0.0032404754
test_loss: 0.0034303667
train_loss: 0.0029215873
test_loss: 0.0032852876
train_loss: 0.0028742694
test_loss: 0.0035610127
train_loss: 0.0036035853
test_loss: 0.0035454847
train_loss: 0.0030848507
test_loss: 0.0038441408
train_loss: 0.0029651853
test_loss: 0.0031508454
train_loss: 0.002958992
test_loss: 0.0032512662
train_loss: 0.0029274048
test_loss: 0.0031208042
train_loss: 0.002850564
test_loss: 0.0031704411
train_loss: 0.002853855
test_loss: 0.0034261919
train_loss: 0.0028536257
test_loss: 0.0032948935
train_loss: 0.0028409471
test_loss: 0.00324313
train_loss: 0.0028410824
test_loss: 0.0031307726
train_loss: 0.002711578
test_loss: 0.0034340494
train_loss: 0.0029544225
test_loss: 0.0033730154
train_loss: 0.0030364664
test_loss: 0.0033736632
train_loss: 0.0029347353
test_loss: 0.0031572517
train_loss: 0.0027579926
test_loss: 0.0033398168
train_loss: 0.0029153833
test_loss: 0.0031790438
train_loss: 0.0028592953
test_loss: 0.0032010165
train_loss: 0.0029764564
test_loss: 0.0035553416
train_loss: 0.0028434552
test_loss: 0.0033737668
train_loss: 0.0030160039
test_loss: 0.0035839374
train_loss: 0.0028108563
test_loss: 0.0034678252
train_loss: 0.0028861419
test_loss: 0.003229343
train_loss: 0.0027966117
test_loss: 0.0031730896
train_loss: 0.0028589102
test_loss: 0.0031132873
train_loss: 0.0027854107
test_loss: 0.0032275482
train_loss: 0.0028757392
test_loss: 0.0030760965
train_loss: 0.0026593064
test_loss: 0.0031945969
train_loss: 0.0030181925
test_loss: 0.0035056756
train_loss: 0.0029748036
test_loss: 0.0032804655
train_loss: 0.002807265
test_loss: 0.003214374
train_loss: 0.0027473227
test_loss: 0.0032722286
train_loss: 0.0027112365
test_loss: 0.0031684435
train_loss: 0.0028689152
test_loss: 0.0034613
train_loss: 0.0026963446
test_loss: 0.00335728
train_loss: 0.0025228052
test_loss: 0.0030810665
train_loss: 0.0029728392
test_loss: 0.003486615
train_loss: 0.0027524813
test_loss: 0.003222648
train_loss: 0.00295327
test_loss: 0.0033504716
train_loss: 0.003083478
test_loss: 0.0031977214
train_loss: 0.0026402134
test_loss: 0.0034391042
train_loss: 0.0026367898
test_loss: 0.003061022
train_loss: 0.002980527
test_loss: 0.0034240892
train_loss: 0.002874254
test_loss: 0.0036349562
train_loss: 0.002882918
test_loss: 0.0034405123
train_loss: 0.0026560046
test_loss: 0.0032944595
train_loss: 0.0026296987
test_loss: 0.0031254713
train_loss: 0.0028114072
test_loss: 0.0031881903
train_loss: 0.002953121
test_loss: 0.0032392452
train_loss: 0.0026527843
test_loss: 0.003171724
train_loss: 0.002647108
test_loss: 0.0031283111
train_loss: 0.0030134278
test_loss: 0.0033720767
train_loss: 0.0025917469
test_loss: 0.0032997297
train_loss: 0.0026196614
test_loss: 0.0035162214
train_loss: 0.0028215563
test_loss: 0.0033315502
train_loss: 0.002937922
test_loss: 0.0035784892
train_loss: 0.0029261627
test_loss: 0.0032343606
train_loss: 0.0027149238
test_loss: 0.00315076
train_loss: 0.0027527825
test_loss: 0.0033356291
train_loss: 0.0027482505
test_loss: 0.0030313898
train_loss: 0.0025865678
test_loss: 0.0031326201
train_loss: 0.0025338721
test_loss: 0.0032810287
train_loss: 0.0026022648
test_loss: 0.0030899893
train_loss: 0.0026874319
test_loss: 0.0035521341
train_loss: 0.0031106002
test_loss: 0.0031365596
train_loss: 0.0025993914
test_loss: 0.0031972365
train_loss: 0.0028269347
test_loss: 0.0031979952
train_loss: 0.0024921717
test_loss: 0.0030678278
train_loss: 0.0027923563
test_loss: 0.0033292733
train_loss: 0.0026417356
test_loss: 0.0029949103
train_loss: 0.0025022898
test_loss: 0.0029704177
train_loss: 0.0024005417
test_loss: 0.0031259616
train_loss: 0.002643465
test_loss: 0.0030566768
train_loss: 0.0026946433
test_loss: 0.0032609894
train_loss: 0.002548022
test_loss: 0.0032167283
train_loss: 0.0026385842
test_loss: 0.0030165778
train_loss: 0.0024341375
test_loss: 0.0030849262
train_loss: 0.0025519966
test_loss: 0.003082102
train_loss: 0.0025926551
test_loss: 0.0032733264
train_loss: 0.0025874746
test_loss: 0.0030937714
train_loss: 0.0024970323
test_loss: 0.0031132405
train_loss: 0.0024107634
test_loss: 0.0030557306
train_loss: 0.0027648332
test_loss: 0.0032875866
train_loss: 0.0025631313
test_loss: 0.0032279824
train_loss: 0.0024710584
test_loss: 0.002945634
train_loss: 0.002482504
test_loss: 0.0029527035
train_loss: 0.0025952274
test_loss: 0.002984218
train_loss: 0.0030314517
test_loss: 0.0031383592
train_loss: 0.0028493963
test_loss: 0.0033727835
train_loss: 0.0028355005
test_loss: 0.0033799764
train_loss: 0.0028778864
test_loss: 0.0030908068
train_loss: 0.0025330465
test_loss: 0.003020758
train_loss: 0.0024694733
test_loss: 0.0029971933
train_loss: 0.0025444464
test_loss: 0.0028753476
train_loss: 0.0024691322
test_loss: 0.0032856646
train_loss: 0.0026404993
test_loss: 0.0031508114
train_loss: 0.0024869163
test_loss: 0.0030282263
train_loss: 0.0028528837
test_loss: 0.0031367806
train_loss: 0.0028609799
test_loss: 0.0032288544
train_loss: 0.0025445642
test_loss: 0.0030198088
train_loss: 0.0026210048
test_loss: 0.0031025212
train_loss: 0.0023470884
test_loss: 0.003298087
train_loss: 0.0028196233
test_loss: 0.0031637382
train_loss: 0.0026248505
test_loss: 0.003194937
train_loss: 0.0023219017
test_loss: 0.0028994065
train_loss: 0.0025538532
test_loss: 0.0029525536
train_loss: 0.0026369025
test_loss: 0.0031065685
train_loss: 0.0027972988
test_loss: 0.0035627082
train_loss: 0.002477959
test_loss: 0.0031960185
train_loss: 0.0023951884
test_loss: 0.0030559816
train_loss: 0.002458252
test_loss: 0.0030340855
train_loss: 0.0024875407
test_loss: 0.0029689458
train_loss: 0.0026042853
test_loss: 0.003146352
train_loss: 0.0027207066
test_loss: 0.003186118
train_loss: 0.002481201
test_loss: 0.0030853983
train_loss: 0.0025414124
test_loss: 0.0032275647
train_loss: 0.0027655098
test_loss: 0.003209699
train_loss: 0.0025607524
test_loss: 0.003119102
train_loss: 0.0026407074
test_loss: 0.003083464
train_loss: 0.0025516395
test_loss: 0.0032703169
train_loss: 0.0025254674
test_loss: 0.0030518693
train_loss: 0.002394082
test_loss: 0.0029806874
train_loss: 0.0024859721
test_loss: 0.0030344636
train_loss: 0.0024002395
test_loss: 0.0030376646
train_loss: 0.0023045857
test_loss: 0.0030292554
train_loss: 0.0024462852
test_loss: 0.003056382
train_loss: 0.0024554257
test_loss: 0.0031264892
train_loss: 0.0023755664
test_loss: 0.0030670515
train_loss: 0.0024480096
test_loss: 0.0031019063
train_loss: 0.002670806
test_loss: 0.002922198
train_loss: 0.0026914054
test_loss: 0.003115822
train_loss: 0.0023145631
test_loss: 0.003194606
train_loss: 0.0024644304
test_loss: 0.0030703195
train_loss: 0.0024662572
test_loss: 0.0031998702
train_loss: 0.0025268097
test_loss: 0.0030447724
train_loss: 0.002636645
test_loss: 0.00297657
train_loss: 0.0024143942
test_loss: 0.0029862458
train_loss: 0.0023478654
test_loss: 0.0030548386
train_loss: 0.00237628
test_loss: 0.00324649
train_loss: 0.0025735789
test_loss: 0.0029869087
train_loss: 0.002344125
test_loss: 0.002943295
train_loss: 0.0026163657
test_loss: 0.0029780408
train_loss: 0.002697001
test_loss: 0.0033181868
train_loss: 0.00246205
test_loss: 0.0029528388
train_loss: 0.002498601
test_loss: 0.002927105
train_loss: 0.002401968
test_loss: 0.0029666226
train_loss: 0.00255117
test_loss: 0.0029178706
train_loss: 0.0022163903
test_loss: 0.0028579403
train_loss: 0.0026046874
test_loss: 0.0031431492
train_loss: 0.0024816187
test_loss: 0.0029944815
train_loss: 0.002489896
test_loss: 0.002910285
train_loss: 0.00243242
test_loss: 0.0029797605
train_loss: 0.0023095594
test_loss: 0.0028194834
train_loss: 0.0025269203
test_loss: 0.0030299884
train_loss: 0.0026859983
test_loss: 0.0032172136
train_loss: 0.0027759604
test_loss: 0.0034825355
train_loss: 0.0025462566
test_loss: 0.0030717514
train_loss: 0.002372968
test_loss: 0.003030965
train_loss: 0.0023099235
test_loss: 0.0029348852
train_loss: 0.0022021993
test_loss: 0.0029373951
train_loss: 0.002213671
test_loss: 0.0029072703
train_loss: 0.002187931
test_loss: 0.0027741462
train_loss: 0.002225777
test_loss: 0.0030224058
train_loss: 0.002577614
test_loss: 0.003136952
train_loss: 0.002310882
test_loss: 0.0029304796
train_loss: 0.0022913844
test_loss: 0.0029510043
train_loss: 0.0022517391
test_loss: 0.0030004887
train_loss: 0.0024565822
test_loss: 0.0029763314
train_loss: 0.002530334
test_loss: 0.0029707353
train_loss: 0.0022879746
test_loss: 0.0028071946
train_loss: 0.0025280858
test_loss: 0.0030819313
train_loss: 0.002263199
test_loss: 0.0030382301
train_loss: 0.0023570328
test_loss: 0.0029458283
train_loss: 0.0026605711
test_loss: 0.0030921756
train_loss: 0.0023273893
test_loss: 0.002892627
train_loss: 0.0025742366
test_loss: 0.00296301
train_loss: 0.0024218797
test_loss: 0.0028552054
train_loss: 0.0021988195
test_loss: 0.0028422673
train_loss: 0.002281607
test_loss: 0.0029330829
train_loss: 0.0023466318
test_loss: 0.0030418998
train_loss: 0.0021739497
test_loss: 0.0028888553
train_loss: 0.00234809
test_loss: 0.003251294
train_loss: 0.002163358
test_loss: 0.0028693757
train_loss: 0.0023977698
test_loss: 0.002942211
train_loss: 0.0022290153
test_loss: 0.0029520013
train_loss: 0.0023124744
test_loss: 0.0028692598
train_loss: 0.0023625095
test_loss: 0.0028902113
train_loss: 0.002134448
test_loss: 0.0030024494
train_loss: 0.0027492018
test_loss: 0.003270808
train_loss: 0.0024369045
test_loss: 0.003133721
train_loss: 0.0021076314
test_loss: 0.00270907
train_loss: 0.0022141354
test_loss: 0.0028811449
train_loss: 0.0021583517
test_loss: 0.002782274
train_loss: 0.002313879
test_loss: 0.0027327437
train_loss: 0.0023220878
test_loss: 0.002833608
train_loss: 0.0022408958
test_loss: 0.0028869635
train_loss: 0.0022117794
test_loss: 0.002913403
train_loss: 0.0022827233
test_loss: 0.002713201
train_loss: 0.0022988871
test_loss: 0.0028344782
train_loss: 0.0022755088
test_loss: 0.0029753123
train_loss: 0.0025368782
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.00302346
train_loss: 0.002183897
test_loss: 0.0028232594
train_loss: 0.0021490245
test_loss: 0.002766603
train_loss: 0.002489843
test_loss: 0.0028780354
train_loss: 0.0023769946
test_loss: 0.0029959476
train_loss: 0.0022250582
test_loss: 0.0028858706
train_loss: 0.0022164441
test_loss: 0.0028257454
train_loss: 0.0022593571
test_loss: 0.0027887332
train_loss: 0.002447086
test_loss: 0.002854283
train_loss: 0.002051056
test_loss: 0.002747397
train_loss: 0.00233731
test_loss: 0.002780441
train_loss: 0.002362478
test_loss: 0.003027531
train_loss: 0.0021638721
test_loss: 0.0028220708
train_loss: 0.0022298656
test_loss: 0.0028533565
train_loss: 0.0022272393
test_loss: 0.0027411215
train_loss: 0.0022915984
test_loss: 0.002789636
train_loss: 0.0021573268
test_loss: 0.0028209116
train_loss: 0.0024428384
test_loss: 0.0029212432
train_loss: 0.0023340224
test_loss: 0.0029608961
train_loss: 0.0022059544
test_loss: 0.0027545874
train_loss: 0.00215529
test_loss: 0.0028841777
train_loss: 0.0025803852
test_loss: 0.0028386982
train_loss: 0.0023561558
test_loss: 0.0027469068
train_loss: 0.0019075922
test_loss: 0.0026478902
train_loss: 0.0021568811
test_loss: 0.0028169998
train_loss: 0.00213994
test_loss: 0.002831998
train_loss: 0.002200187
test_loss: 0.0027735007
train_loss: 0.0024567111
test_loss: 0.0027928862
train_loss: 0.0022377954
test_loss: 0.0026923534
train_loss: 0.0024221384
test_loss: 0.0027640234
train_loss: 0.0021057462
test_loss: 0.0027247753
train_loss: 0.002176404
test_loss: 0.0027043452
train_loss: 0.0021318947
test_loss: 0.0028120866
train_loss: 0.0020848706
test_loss: 0.002691272
train_loss: 0.0022342647
test_loss: 0.0029856528
train_loss: 0.0024382253
test_loss: 0.0028686987
train_loss: 0.0021279035
test_loss: 0.002762458
train_loss: 0.0021853067
test_loss: 0.0026817666
train_loss: 0.0020270303
test_loss: 0.0027072963
train_loss: 0.0022335083
test_loss: 0.0027708642
train_loss: 0.002272805
test_loss: 0.002979985
train_loss: 0.0023088886
test_loss: 0.0028277822
train_loss: 0.0022721933
test_loss: 0.0028619836
train_loss: 0.0022293087
test_loss: 0.0026841925
train_loss: 0.0022368224
test_loss: 0.0027095906
train_loss: 0.0020885894
test_loss: 0.002634311
train_loss: 0.002073672
test_loss: 0.0028403567
train_loss: 0.0021649394
test_loss: 0.0028586753
train_loss: 0.0023613113
test_loss: 0.0029772327
train_loss: 0.0025467472
test_loss: 0.003136909
train_loss: 0.0024510121
test_loss: 0.0028673154
train_loss: 0.0020067706
test_loss: 0.002692316
train_loss: 0.0022107107
test_loss: 0.0027496198
train_loss: 0.0020171534
test_loss: 0.0025839647
train_loss: 0.0019081954
test_loss: 0.0026189317
train_loss: 0.0020742903
test_loss: 0.0027279493
train_loss: 0.002233011
test_loss: 0.0027467527
train_loss: 0.0023869982
test_loss: 0.002997788
train_loss: 0.0022831368
test_loss: 0.0033005234
train_loss: 0.0023007777
test_loss: 0.0028986502
train_loss: 0.0021631827
test_loss: 0.0027214081
train_loss: 0.0020900522
test_loss: 0.0027583637
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 0 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb25a7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb24c66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb25a7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb24c6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb2508598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb2508950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb2508ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb250fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb242fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb245dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb245dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb23d42f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb23e9378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb2393b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb2393d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb23e9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb2310bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb23106a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febb230cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febae112c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febae1129d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febae1128c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febae0a0b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febae0aad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febae069620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febae069730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febae051d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febae069950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febadfd1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febadf99048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febadfe5e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febadf63378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febadf63510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febadf0dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febadefa400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7febaded8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 7.58461465e-06
Iter: 2 loss: 6.08606e-06
Iter: 3 loss: 6.05192236e-06
Iter: 4 loss: 5.46696629e-06
Iter: 5 loss: 6.46061153e-06
Iter: 6 loss: 5.20366302e-06
Iter: 7 loss: 4.9360433e-06
Iter: 8 loss: 4.83796066e-06
Iter: 9 loss: 4.68954568e-06
Iter: 10 loss: 4.49185745e-06
Iter: 11 loss: 4.45903788e-06
Iter: 12 loss: 4.35037055e-06
Iter: 13 loss: 4.28213207e-06
Iter: 14 loss: 4.23903975e-06
Iter: 15 loss: 4.13135876e-06
Iter: 16 loss: 4.12586542e-06
Iter: 17 loss: 4.04346156e-06
Iter: 18 loss: 3.90034211e-06
Iter: 19 loss: 3.89975276e-06
Iter: 20 loss: 3.83954284e-06
Iter: 21 loss: 3.67138864e-06
Iter: 22 loss: 4.57444139e-06
Iter: 23 loss: 3.61938237e-06
Iter: 24 loss: 3.4676732e-06
Iter: 25 loss: 3.4590953e-06
Iter: 26 loss: 3.32818081e-06
Iter: 27 loss: 3.37744041e-06
Iter: 28 loss: 3.23689164e-06
Iter: 29 loss: 3.16475553e-06
Iter: 30 loss: 3.34847778e-06
Iter: 31 loss: 3.13972123e-06
Iter: 32 loss: 3.10133032e-06
Iter: 33 loss: 3.01026876e-06
Iter: 34 loss: 4.05120045e-06
Iter: 35 loss: 3.0016954e-06
Iter: 36 loss: 2.9367186e-06
Iter: 37 loss: 3.30146918e-06
Iter: 38 loss: 2.92767254e-06
Iter: 39 loss: 2.86807e-06
Iter: 40 loss: 2.91666174e-06
Iter: 41 loss: 2.83244299e-06
Iter: 42 loss: 2.79353958e-06
Iter: 43 loss: 2.79053575e-06
Iter: 44 loss: 2.74518811e-06
Iter: 45 loss: 2.72959073e-06
Iter: 46 loss: 2.70384453e-06
Iter: 47 loss: 2.63515494e-06
Iter: 48 loss: 2.5885688e-06
Iter: 49 loss: 2.56325029e-06
Iter: 50 loss: 2.5359534e-06
Iter: 51 loss: 2.52656878e-06
Iter: 52 loss: 2.48400602e-06
Iter: 53 loss: 2.40144891e-06
Iter: 54 loss: 4.09496351e-06
Iter: 55 loss: 2.4009089e-06
Iter: 56 loss: 2.3221678e-06
Iter: 57 loss: 2.67558789e-06
Iter: 58 loss: 2.306846e-06
Iter: 59 loss: 2.25627673e-06
Iter: 60 loss: 2.25507119e-06
Iter: 61 loss: 2.22855033e-06
Iter: 62 loss: 2.19127105e-06
Iter: 63 loss: 2.18988225e-06
Iter: 64 loss: 2.1408764e-06
Iter: 65 loss: 2.76602987e-06
Iter: 66 loss: 2.14050988e-06
Iter: 67 loss: 2.12820169e-06
Iter: 68 loss: 2.10065355e-06
Iter: 69 loss: 2.47689923e-06
Iter: 70 loss: 2.09908217e-06
Iter: 71 loss: 2.06256709e-06
Iter: 72 loss: 2.13594376e-06
Iter: 73 loss: 2.04773187e-06
Iter: 74 loss: 2.02502974e-06
Iter: 75 loss: 2.22425365e-06
Iter: 76 loss: 2.02386946e-06
Iter: 77 loss: 1.99637407e-06
Iter: 78 loss: 2.0773839e-06
Iter: 79 loss: 1.98791577e-06
Iter: 80 loss: 1.954352e-06
Iter: 81 loss: 1.91230833e-06
Iter: 82 loss: 1.90886112e-06
Iter: 83 loss: 1.87125852e-06
Iter: 84 loss: 2.09912719e-06
Iter: 85 loss: 1.8667107e-06
Iter: 86 loss: 1.83752195e-06
Iter: 87 loss: 2.27193323e-06
Iter: 88 loss: 1.83756219e-06
Iter: 89 loss: 1.82213216e-06
Iter: 90 loss: 1.8019216e-06
Iter: 91 loss: 1.80062659e-06
Iter: 92 loss: 1.78938922e-06
Iter: 93 loss: 1.7887653e-06
Iter: 94 loss: 1.77497327e-06
Iter: 95 loss: 1.75147829e-06
Iter: 96 loss: 1.75149398e-06
Iter: 97 loss: 1.75034961e-06
Iter: 98 loss: 1.74449201e-06
Iter: 99 loss: 1.73901924e-06
Iter: 100 loss: 1.72345312e-06
Iter: 101 loss: 1.80140239e-06
Iter: 102 loss: 1.71826923e-06
Iter: 103 loss: 1.69276655e-06
Iter: 104 loss: 1.69834402e-06
Iter: 105 loss: 1.67396115e-06
Iter: 106 loss: 1.6488234e-06
Iter: 107 loss: 1.72231614e-06
Iter: 108 loss: 1.64108633e-06
Iter: 109 loss: 1.62377955e-06
Iter: 110 loss: 1.62272886e-06
Iter: 111 loss: 1.60628349e-06
Iter: 112 loss: 1.63281607e-06
Iter: 113 loss: 1.5986559e-06
Iter: 114 loss: 1.5869191e-06
Iter: 115 loss: 1.57541285e-06
Iter: 116 loss: 1.57278134e-06
Iter: 117 loss: 1.56257022e-06
Iter: 118 loss: 1.56159183e-06
Iter: 119 loss: 1.55350506e-06
Iter: 120 loss: 1.57447664e-06
Iter: 121 loss: 1.5507427e-06
Iter: 122 loss: 1.54527777e-06
Iter: 123 loss: 1.53214887e-06
Iter: 124 loss: 1.67589087e-06
Iter: 125 loss: 1.53077212e-06
Iter: 126 loss: 1.53337396e-06
Iter: 127 loss: 1.52378664e-06
Iter: 128 loss: 1.52034522e-06
Iter: 129 loss: 1.51315521e-06
Iter: 130 loss: 1.63332811e-06
Iter: 131 loss: 1.51296445e-06
Iter: 132 loss: 1.50528842e-06
Iter: 133 loss: 1.59909121e-06
Iter: 134 loss: 1.50520714e-06
Iter: 135 loss: 1.4968922e-06
Iter: 136 loss: 1.47440028e-06
Iter: 137 loss: 1.62243862e-06
Iter: 138 loss: 1.46893581e-06
Iter: 139 loss: 1.45079252e-06
Iter: 140 loss: 1.47863841e-06
Iter: 141 loss: 1.44227351e-06
Iter: 142 loss: 1.4247737e-06
Iter: 143 loss: 1.57333204e-06
Iter: 144 loss: 1.42381668e-06
Iter: 145 loss: 1.41946452e-06
Iter: 146 loss: 1.4177358e-06
Iter: 147 loss: 1.41144665e-06
Iter: 148 loss: 1.39658835e-06
Iter: 149 loss: 1.56724877e-06
Iter: 150 loss: 1.39522638e-06
Iter: 151 loss: 1.38577911e-06
Iter: 152 loss: 1.39175404e-06
Iter: 153 loss: 1.37979976e-06
Iter: 154 loss: 1.36929873e-06
Iter: 155 loss: 1.49755704e-06
Iter: 156 loss: 1.36917447e-06
Iter: 157 loss: 1.3648463e-06
Iter: 158 loss: 1.36436688e-06
Iter: 159 loss: 1.36117023e-06
Iter: 160 loss: 1.35320113e-06
Iter: 161 loss: 1.42798319e-06
Iter: 162 loss: 1.35207767e-06
Iter: 163 loss: 1.34374659e-06
Iter: 164 loss: 1.38937332e-06
Iter: 165 loss: 1.34250968e-06
Iter: 166 loss: 1.33446724e-06
Iter: 167 loss: 1.34549327e-06
Iter: 168 loss: 1.33048138e-06
Iter: 169 loss: 1.3166723e-06
Iter: 170 loss: 1.37626307e-06
Iter: 171 loss: 1.31379136e-06
Iter: 172 loss: 1.30885485e-06
Iter: 173 loss: 1.30759349e-06
Iter: 174 loss: 1.30449575e-06
Iter: 175 loss: 1.29819159e-06
Iter: 176 loss: 1.29385285e-06
Iter: 177 loss: 1.29153318e-06
Iter: 178 loss: 1.30269905e-06
Iter: 179 loss: 1.28963586e-06
Iter: 180 loss: 1.2880131e-06
Iter: 181 loss: 1.28370971e-06
Iter: 182 loss: 1.31048591e-06
Iter: 183 loss: 1.28261854e-06
Iter: 184 loss: 1.27680164e-06
Iter: 185 loss: 1.28409772e-06
Iter: 186 loss: 1.27380508e-06
Iter: 187 loss: 1.27115584e-06
Iter: 188 loss: 1.27013072e-06
Iter: 189 loss: 1.26655982e-06
Iter: 190 loss: 1.25605959e-06
Iter: 191 loss: 1.28851252e-06
Iter: 192 loss: 1.25081669e-06
Iter: 193 loss: 1.24212079e-06
Iter: 194 loss: 1.32736022e-06
Iter: 195 loss: 1.24176836e-06
Iter: 196 loss: 1.23582367e-06
Iter: 197 loss: 1.23573932e-06
Iter: 198 loss: 1.23141751e-06
Iter: 199 loss: 1.22441918e-06
Iter: 200 loss: 1.22436359e-06
Iter: 201 loss: 1.22002257e-06
Iter: 202 loss: 1.2369951e-06
Iter: 203 loss: 1.21899234e-06
Iter: 204 loss: 1.21584503e-06
Iter: 205 loss: 1.21576318e-06
Iter: 206 loss: 1.21356231e-06
Iter: 207 loss: 1.21138942e-06
Iter: 208 loss: 1.21090886e-06
Iter: 209 loss: 1.20753384e-06
Iter: 210 loss: 1.20146228e-06
Iter: 211 loss: 1.35217044e-06
Iter: 212 loss: 1.20146774e-06
Iter: 213 loss: 1.20045252e-06
Iter: 214 loss: 1.19798915e-06
Iter: 215 loss: 1.19467541e-06
Iter: 216 loss: 1.19178185e-06
Iter: 217 loss: 1.19093625e-06
Iter: 218 loss: 1.18604498e-06
Iter: 219 loss: 1.17301363e-06
Iter: 220 loss: 1.26409668e-06
Iter: 221 loss: 1.17011678e-06
Iter: 222 loss: 1.17775994e-06
Iter: 223 loss: 1.16657384e-06
Iter: 224 loss: 1.1627335e-06
Iter: 225 loss: 1.16660544e-06
Iter: 226 loss: 1.16057959e-06
Iter: 227 loss: 1.15750277e-06
Iter: 228 loss: 1.15588443e-06
Iter: 229 loss: 1.15449552e-06
Iter: 230 loss: 1.1506138e-06
Iter: 231 loss: 1.15182286e-06
Iter: 232 loss: 1.14789555e-06
Iter: 233 loss: 1.14812451e-06
Iter: 234 loss: 1.14555803e-06
Iter: 235 loss: 1.1438982e-06
Iter: 236 loss: 1.14002569e-06
Iter: 237 loss: 1.18465971e-06
Iter: 238 loss: 1.13968576e-06
Iter: 239 loss: 1.13405849e-06
Iter: 240 loss: 1.12636417e-06
Iter: 241 loss: 1.12604698e-06
Iter: 242 loss: 1.13795454e-06
Iter: 243 loss: 1.12357043e-06
Iter: 244 loss: 1.12165753e-06
Iter: 245 loss: 1.11665952e-06
Iter: 246 loss: 1.15147145e-06
Iter: 247 loss: 1.1155114e-06
Iter: 248 loss: 1.10948963e-06
Iter: 249 loss: 1.13215572e-06
Iter: 250 loss: 1.10807389e-06
Iter: 251 loss: 1.10634141e-06
Iter: 252 loss: 1.10555197e-06
Iter: 253 loss: 1.10332735e-06
Iter: 254 loss: 1.09948905e-06
Iter: 255 loss: 1.09947871e-06
Iter: 256 loss: 1.09625785e-06
Iter: 257 loss: 1.11211568e-06
Iter: 258 loss: 1.09569555e-06
Iter: 259 loss: 1.0940222e-06
Iter: 260 loss: 1.10519863e-06
Iter: 261 loss: 1.0938395e-06
Iter: 262 loss: 1.09132532e-06
Iter: 263 loss: 1.08812208e-06
Iter: 264 loss: 1.08790573e-06
Iter: 265 loss: 1.08504605e-06
Iter: 266 loss: 1.08818347e-06
Iter: 267 loss: 1.08350957e-06
Iter: 268 loss: 1.07953406e-06
Iter: 269 loss: 1.07693199e-06
Iter: 270 loss: 1.07536425e-06
Iter: 271 loss: 1.07009305e-06
Iter: 272 loss: 1.06987068e-06
Iter: 273 loss: 1.06796585e-06
Iter: 274 loss: 1.06332323e-06
Iter: 275 loss: 1.10223186e-06
Iter: 276 loss: 1.06247012e-06
Iter: 277 loss: 1.06174843e-06
Iter: 278 loss: 1.06024197e-06
Iter: 279 loss: 1.05831953e-06
Iter: 280 loss: 1.06149582e-06
Iter: 281 loss: 1.057482e-06
Iter: 282 loss: 1.05620438e-06
Iter: 283 loss: 1.05297556e-06
Iter: 284 loss: 1.07957374e-06
Iter: 285 loss: 1.05238269e-06
Iter: 286 loss: 1.0549727e-06
Iter: 287 loss: 1.05101549e-06
Iter: 288 loss: 1.05020672e-06
Iter: 289 loss: 1.04785431e-06
Iter: 290 loss: 1.05474646e-06
Iter: 291 loss: 1.04664127e-06
Iter: 292 loss: 1.04281253e-06
Iter: 293 loss: 1.05989477e-06
Iter: 294 loss: 1.04204287e-06
Iter: 295 loss: 1.03949844e-06
Iter: 296 loss: 1.03938237e-06
Iter: 297 loss: 1.03790171e-06
Iter: 298 loss: 1.03387686e-06
Iter: 299 loss: 1.05968104e-06
Iter: 300 loss: 1.03281832e-06
Iter: 301 loss: 1.02907427e-06
Iter: 302 loss: 1.07186327e-06
Iter: 303 loss: 1.02903016e-06
Iter: 304 loss: 1.02636386e-06
Iter: 305 loss: 1.02787772e-06
Iter: 306 loss: 1.02463696e-06
Iter: 307 loss: 1.02165291e-06
Iter: 308 loss: 1.02164154e-06
Iter: 309 loss: 1.0199908e-06
Iter: 310 loss: 1.0194135e-06
Iter: 311 loss: 1.01850492e-06
Iter: 312 loss: 1.01689329e-06
Iter: 313 loss: 1.01587761e-06
Iter: 314 loss: 1.01523437e-06
Iter: 315 loss: 1.01206581e-06
Iter: 316 loss: 1.04920036e-06
Iter: 317 loss: 1.01206206e-06
Iter: 318 loss: 1.01129967e-06
Iter: 319 loss: 1.00990223e-06
Iter: 320 loss: 1.0385113e-06
Iter: 321 loss: 1.00987972e-06
Iter: 322 loss: 1.00852424e-06
Iter: 323 loss: 1.00852344e-06
Iter: 324 loss: 1.00700288e-06
Iter: 325 loss: 1.00431328e-06
Iter: 326 loss: 1.00433681e-06
Iter: 327 loss: 1.00224202e-06
Iter: 328 loss: 1.00979628e-06
Iter: 329 loss: 1.00169609e-06
Iter: 330 loss: 9.98398491e-07
Iter: 331 loss: 1.00045736e-06
Iter: 332 loss: 9.96283e-07
Iter: 333 loss: 9.93846925e-07
Iter: 334 loss: 9.94483116e-07
Iter: 335 loss: 9.92147079e-07
Iter: 336 loss: 9.89227829e-07
Iter: 337 loss: 9.88203283e-07
Iter: 338 loss: 9.86534815e-07
Iter: 339 loss: 9.88072202e-07
Iter: 340 loss: 9.85459678e-07
Iter: 341 loss: 9.8469468e-07
Iter: 342 loss: 9.85842462e-07
Iter: 343 loss: 9.84312578e-07
Iter: 344 loss: 9.8337e-07
Iter: 345 loss: 9.81363087e-07
Iter: 346 loss: 1.01366982e-06
Iter: 347 loss: 9.81301127e-07
Iter: 348 loss: 9.80417553e-07
Iter: 349 loss: 9.80236791e-07
Iter: 350 loss: 9.78951789e-07
Iter: 351 loss: 9.78011826e-07
Iter: 352 loss: 9.77599e-07
Iter: 353 loss: 9.75893727e-07
Iter: 354 loss: 9.75508783e-07
Iter: 355 loss: 9.7440352e-07
Iter: 356 loss: 9.73690703e-07
Iter: 357 loss: 9.731707e-07
Iter: 358 loss: 9.72206408e-07
Iter: 359 loss: 9.69147095e-07
Iter: 360 loss: 9.73809392e-07
Iter: 361 loss: 9.66991593e-07
Iter: 362 loss: 9.70437213e-07
Iter: 363 loss: 9.65984555e-07
Iter: 364 loss: 9.65244681e-07
Iter: 365 loss: 9.64819264e-07
Iter: 366 loss: 9.64482524e-07
Iter: 367 loss: 9.63447746e-07
Iter: 368 loss: 9.60983471e-07
Iter: 369 loss: 9.90027729e-07
Iter: 370 loss: 9.60743705e-07
Iter: 371 loss: 9.58696546e-07
Iter: 372 loss: 9.75245712e-07
Iter: 373 loss: 9.58543524e-07
Iter: 374 loss: 9.56793201e-07
Iter: 375 loss: 9.80528171e-07
Iter: 376 loss: 9.56802296e-07
Iter: 377 loss: 9.552449e-07
Iter: 378 loss: 9.53657377e-07
Iter: 379 loss: 9.53346387e-07
Iter: 380 loss: 9.513642e-07
Iter: 381 loss: 9.50811796e-07
Iter: 382 loss: 9.49606886e-07
Iter: 383 loss: 9.46537853e-07
Iter: 384 loss: 9.67068786e-07
Iter: 385 loss: 9.46224759e-07
Iter: 386 loss: 9.45227157e-07
Iter: 387 loss: 9.44918725e-07
Iter: 388 loss: 9.44231431e-07
Iter: 389 loss: 9.42112365e-07
Iter: 390 loss: 9.46094815e-07
Iter: 391 loss: 9.40712539e-07
Iter: 392 loss: 9.40420364e-07
Iter: 393 loss: 9.39225231e-07
Iter: 394 loss: 9.37988659e-07
Iter: 395 loss: 9.38740186e-07
Iter: 396 loss: 9.3720223e-07
Iter: 397 loss: 9.362588e-07
Iter: 398 loss: 9.33695617e-07
Iter: 399 loss: 9.53543e-07
Iter: 400 loss: 9.33241381e-07
Iter: 401 loss: 9.32558805e-07
Iter: 402 loss: 9.32002e-07
Iter: 403 loss: 9.30607257e-07
Iter: 404 loss: 9.33818399e-07
Iter: 405 loss: 9.30092824e-07
Iter: 406 loss: 9.28802649e-07
Iter: 407 loss: 9.26384246e-07
Iter: 408 loss: 9.80298637e-07
Iter: 409 loss: 9.26369182e-07
Iter: 410 loss: 9.25763175e-07
Iter: 411 loss: 9.25077757e-07
Iter: 412 loss: 9.24028882e-07
Iter: 413 loss: 9.21104913e-07
Iter: 414 loss: 9.41110386e-07
Iter: 415 loss: 9.20485661e-07
Iter: 416 loss: 9.17742398e-07
Iter: 417 loss: 9.47818194e-07
Iter: 418 loss: 9.17702607e-07
Iter: 419 loss: 9.16339275e-07
Iter: 420 loss: 9.26201892e-07
Iter: 421 loss: 9.16207114e-07
Iter: 422 loss: 9.14586849e-07
Iter: 423 loss: 9.16934596e-07
Iter: 424 loss: 9.13831741e-07
Iter: 425 loss: 9.12411792e-07
Iter: 426 loss: 9.12072323e-07
Iter: 427 loss: 9.11183179e-07
Iter: 428 loss: 9.10407607e-07
Iter: 429 loss: 9.10347183e-07
Iter: 430 loss: 9.09332073e-07
Iter: 431 loss: 9.07705044e-07
Iter: 432 loss: 9.07680146e-07
Iter: 433 loss: 9.06463811e-07
Iter: 434 loss: 9.09451956e-07
Iter: 435 loss: 9.06055675e-07
Iter: 436 loss: 9.05026241e-07
Iter: 437 loss: 9.04964054e-07
Iter: 438 loss: 9.03949854e-07
Iter: 439 loss: 9.02245688e-07
Iter: 440 loss: 9.02232159e-07
Iter: 441 loss: 9.01221256e-07
Iter: 442 loss: 9.01204089e-07
Iter: 443 loss: 9.00088253e-07
Iter: 444 loss: 8.98268695e-07
Iter: 445 loss: 8.98246697e-07
Iter: 446 loss: 8.96239044e-07
Iter: 447 loss: 9.00048065e-07
Iter: 448 loss: 8.95380765e-07
Iter: 449 loss: 8.93938704e-07
Iter: 450 loss: 9.06095e-07
Iter: 451 loss: 8.93914375e-07
Iter: 452 loss: 8.92647677e-07
Iter: 453 loss: 9.01875296e-07
Iter: 454 loss: 8.92579e-07
Iter: 455 loss: 8.91720049e-07
Iter: 456 loss: 8.91189188e-07
Iter: 457 loss: 8.90837782e-07
Iter: 458 loss: 8.89925332e-07
Iter: 459 loss: 8.93233846e-07
Iter: 460 loss: 8.89677324e-07
Iter: 461 loss: 8.88270108e-07
Iter: 462 loss: 8.90602905e-07
Iter: 463 loss: 8.87643637e-07
Iter: 464 loss: 8.86800251e-07
Iter: 465 loss: 8.86393821e-07
Iter: 466 loss: 8.85945042e-07
Iter: 467 loss: 8.84795782e-07
Iter: 468 loss: 8.93778747e-07
Iter: 469 loss: 8.84718133e-07
Iter: 470 loss: 8.83248617e-07
Iter: 471 loss: 8.83215534e-07
Iter: 472 loss: 8.82021e-07
Iter: 473 loss: 8.80742448e-07
Iter: 474 loss: 8.84486e-07
Iter: 475 loss: 8.80356424e-07
Iter: 476 loss: 8.78733431e-07
Iter: 477 loss: 8.85216821e-07
Iter: 478 loss: 8.78351216e-07
Iter: 479 loss: 8.77283355e-07
Iter: 480 loss: 8.75902288e-07
Iter: 481 loss: 8.75800083e-07
Iter: 482 loss: 8.74080115e-07
Iter: 483 loss: 8.80212554e-07
Iter: 484 loss: 8.73660042e-07
Iter: 485 loss: 8.71909947e-07
Iter: 486 loss: 8.92983792e-07
Iter: 487 loss: 8.71893803e-07
Iter: 488 loss: 8.70777853e-07
Iter: 489 loss: 8.71834686e-07
Iter: 490 loss: 8.70119948e-07
Iter: 491 loss: 8.69398889e-07
Iter: 492 loss: 8.69407643e-07
Iter: 493 loss: 8.68804705e-07
Iter: 494 loss: 8.67675453e-07
Iter: 495 loss: 8.81878e-07
Iter: 496 loss: 8.67685571e-07
Iter: 497 loss: 8.67082747e-07
Iter: 498 loss: 8.65861125e-07
Iter: 499 loss: 8.8870263e-07
Iter: 500 loss: 8.65819572e-07
Iter: 501 loss: 8.64345509e-07
Iter: 502 loss: 8.67601386e-07
Iter: 503 loss: 8.63764399e-07
Iter: 504 loss: 8.61902947e-07
Iter: 505 loss: 8.79953177e-07
Iter: 506 loss: 8.61826e-07
Iter: 507 loss: 8.61064564e-07
Iter: 508 loss: 8.60049681e-07
Iter: 509 loss: 8.59965951e-07
Iter: 510 loss: 8.58432884e-07
Iter: 511 loss: 8.74903662e-07
Iter: 512 loss: 8.58403155e-07
Iter: 513 loss: 8.57415841e-07
Iter: 514 loss: 8.56189274e-07
Iter: 515 loss: 8.56091162e-07
Iter: 516 loss: 8.55136363e-07
Iter: 517 loss: 8.60049681e-07
Iter: 518 loss: 8.54997495e-07
Iter: 519 loss: 8.54260179e-07
Iter: 520 loss: 8.59808665e-07
Iter: 521 loss: 8.54166501e-07
Iter: 522 loss: 8.53575e-07
Iter: 523 loss: 8.54043947e-07
Iter: 524 loss: 8.53179415e-07
Iter: 525 loss: 8.52434823e-07
Iter: 526 loss: 8.50948823e-07
Iter: 527 loss: 8.81671667e-07
Iter: 528 loss: 8.5095536e-07
Iter: 529 loss: 8.50868e-07
Iter: 530 loss: 8.50112713e-07
Iter: 531 loss: 8.49597939e-07
Iter: 532 loss: 8.48649393e-07
Iter: 533 loss: 8.71230895e-07
Iter: 534 loss: 8.48655247e-07
Iter: 535 loss: 8.47573631e-07
Iter: 536 loss: 8.45790851e-07
Iter: 537 loss: 8.45755949e-07
Iter: 538 loss: 8.46553803e-07
Iter: 539 loss: 8.44824399e-07
Iter: 540 loss: 8.44345038e-07
Iter: 541 loss: 8.43487726e-07
Iter: 542 loss: 8.63812261e-07
Iter: 543 loss: 8.43458224e-07
Iter: 544 loss: 8.4264309e-07
Iter: 545 loss: 8.55652956e-07
Iter: 546 loss: 8.4264309e-07
Iter: 547 loss: 8.41763779e-07
Iter: 548 loss: 8.40536359e-07
Iter: 549 loss: 8.40500377e-07
Iter: 550 loss: 8.39225152e-07
Iter: 551 loss: 8.40449161e-07
Iter: 552 loss: 8.38472374e-07
Iter: 553 loss: 8.39237202e-07
Iter: 554 loss: 8.37715504e-07
Iter: 555 loss: 8.37145762e-07
Iter: 556 loss: 8.36709e-07
Iter: 557 loss: 8.36524634e-07
Iter: 558 loss: 8.35524247e-07
Iter: 559 loss: 8.34361231e-07
Iter: 560 loss: 8.34225716e-07
Iter: 561 loss: 8.33494084e-07
Iter: 562 loss: 8.33361241e-07
Iter: 563 loss: 8.32554178e-07
Iter: 564 loss: 8.32171224e-07
Iter: 565 loss: 8.31757859e-07
Iter: 566 loss: 8.30703925e-07
Iter: 567 loss: 8.286911e-07
Iter: 568 loss: 8.72609576e-07
Iter: 569 loss: 8.28686098e-07
Iter: 570 loss: 8.29806709e-07
Iter: 571 loss: 8.28064685e-07
Iter: 572 loss: 8.27463055e-07
Iter: 573 loss: 8.26027303e-07
Iter: 574 loss: 8.423483e-07
Iter: 575 loss: 8.25889231e-07
Iter: 576 loss: 8.24803e-07
Iter: 577 loss: 8.38247274e-07
Iter: 578 loss: 8.24797894e-07
Iter: 579 loss: 8.23993162e-07
Iter: 580 loss: 8.31236889e-07
Iter: 581 loss: 8.23974062e-07
Iter: 582 loss: 8.23616801e-07
Iter: 583 loss: 8.22469701e-07
Iter: 584 loss: 8.23150799e-07
Iter: 585 loss: 8.21460503e-07
Iter: 586 loss: 8.20097682e-07
Iter: 587 loss: 8.37407811e-07
Iter: 588 loss: 8.2005937e-07
Iter: 589 loss: 8.19237187e-07
Iter: 590 loss: 8.3169931e-07
Iter: 591 loss: 8.19237925e-07
Iter: 592 loss: 8.18468834e-07
Iter: 593 loss: 8.18542105e-07
Iter: 594 loss: 8.17834348e-07
Iter: 595 loss: 8.17191449e-07
Iter: 596 loss: 8.21078629e-07
Iter: 597 loss: 8.17078899e-07
Iter: 598 loss: 8.16451347e-07
Iter: 599 loss: 8.1891244e-07
Iter: 600 loss: 8.16282068e-07
Iter: 601 loss: 8.15643034e-07
Iter: 602 loss: 8.14668113e-07
Iter: 603 loss: 8.14663281e-07
Iter: 604 loss: 8.13761176e-07
Iter: 605 loss: 8.13344059e-07
Iter: 606 loss: 8.12890733e-07
Iter: 607 loss: 8.1233128e-07
Iter: 608 loss: 8.120345e-07
Iter: 609 loss: 8.11587029e-07
Iter: 610 loss: 8.11010068e-07
Iter: 611 loss: 8.10964139e-07
Iter: 612 loss: 8.10483527e-07
Iter: 613 loss: 8.1589468e-07
Iter: 614 loss: 8.10461529e-07
Iter: 615 loss: 8.09841822e-07
Iter: 616 loss: 8.08568132e-07
Iter: 617 loss: 8.30537715e-07
Iter: 618 loss: 8.08543746e-07
Iter: 619 loss: 8.07683875e-07
Iter: 620 loss: 8.07795345e-07
Iter: 621 loss: 8.06956564e-07
Iter: 622 loss: 8.05699756e-07
Iter: 623 loss: 8.09101607e-07
Iter: 624 loss: 8.05260697e-07
Iter: 625 loss: 8.04799356e-07
Iter: 626 loss: 8.04707327e-07
Iter: 627 loss: 8.04027707e-07
Iter: 628 loss: 8.03595867e-07
Iter: 629 loss: 8.03357466e-07
Iter: 630 loss: 8.03018281e-07
Iter: 631 loss: 8.02964166e-07
Iter: 632 loss: 8.02704221e-07
Iter: 633 loss: 8.02239583e-07
Iter: 634 loss: 8.02212412e-07
Iter: 635 loss: 8.01535407e-07
Iter: 636 loss: 8.00360908e-07
Iter: 637 loss: 8.00344537e-07
Iter: 638 loss: 7.99064367e-07
Iter: 639 loss: 8.12644203e-07
Iter: 640 loss: 7.99021e-07
Iter: 641 loss: 7.98180565e-07
Iter: 642 loss: 8.08086611e-07
Iter: 643 loss: 7.98131907e-07
Iter: 644 loss: 7.97421421e-07
Iter: 645 loss: 7.9572078e-07
Iter: 646 loss: 8.13879296e-07
Iter: 647 loss: 7.95499545e-07
Iter: 648 loss: 7.95836911e-07
Iter: 649 loss: 7.9480651e-07
Iter: 650 loss: 7.94379616e-07
Iter: 651 loss: 7.93126674e-07
Iter: 652 loss: 7.98915e-07
Iter: 653 loss: 7.92686876e-07
Iter: 654 loss: 7.91533921e-07
Iter: 655 loss: 8.02561658e-07
Iter: 656 loss: 7.91483501e-07
Iter: 657 loss: 7.90738795e-07
Iter: 658 loss: 7.89640865e-07
Iter: 659 loss: 7.89616138e-07
Iter: 660 loss: 7.8883e-07
Iter: 661 loss: 7.88752345e-07
Iter: 662 loss: 7.88276907e-07
Iter: 663 loss: 7.92506398e-07
Iter: 664 loss: 7.88243369e-07
Iter: 665 loss: 7.87608769e-07
Iter: 666 loss: 7.88230636e-07
Iter: 667 loss: 7.87243778e-07
Iter: 668 loss: 7.86768055e-07
Iter: 669 loss: 7.85677344e-07
Iter: 670 loss: 7.98017254e-07
Iter: 671 loss: 7.85554789e-07
Iter: 672 loss: 7.8452365e-07
Iter: 673 loss: 7.84501822e-07
Iter: 674 loss: 7.83838459e-07
Iter: 675 loss: 7.84437077e-07
Iter: 676 loss: 7.83456358e-07
Iter: 677 loss: 7.82975462e-07
Iter: 678 loss: 7.88815214e-07
Iter: 679 loss: 7.82960683e-07
Iter: 680 loss: 7.82610869e-07
Iter: 681 loss: 7.82124516e-07
Iter: 682 loss: 7.8210843e-07
Iter: 683 loss: 7.814848e-07
Iter: 684 loss: 7.87288855e-07
Iter: 685 loss: 7.8144592e-07
Iter: 686 loss: 7.80970481e-07
Iter: 687 loss: 7.82498034e-07
Iter: 688 loss: 7.80817118e-07
Iter: 689 loss: 7.80581672e-07
Iter: 690 loss: 7.79838388e-07
Iter: 691 loss: 7.81612812e-07
Iter: 692 loss: 7.79393588e-07
Iter: 693 loss: 7.78000242e-07
Iter: 694 loss: 7.84604e-07
Iter: 695 loss: 7.77699483e-07
Iter: 696 loss: 7.76434149e-07
Iter: 697 loss: 7.76441e-07
Iter: 698 loss: 7.75431033e-07
Iter: 699 loss: 7.73801446e-07
Iter: 700 loss: 7.87704e-07
Iter: 701 loss: 7.73716e-07
Iter: 702 loss: 7.73225224e-07
Iter: 703 loss: 7.73016097e-07
Iter: 704 loss: 7.72438511e-07
Iter: 705 loss: 7.71778787e-07
Iter: 706 loss: 7.71719215e-07
Iter: 707 loss: 7.71358202e-07
Iter: 708 loss: 7.72994383e-07
Iter: 709 loss: 7.71266855e-07
Iter: 710 loss: 7.70753445e-07
Iter: 711 loss: 7.71887585e-07
Iter: 712 loss: 7.70550059e-07
Iter: 713 loss: 7.70130555e-07
Iter: 714 loss: 7.6982883e-07
Iter: 715 loss: 7.69695134e-07
Iter: 716 loss: 7.6933037e-07
Iter: 717 loss: 7.69323549e-07
Iter: 718 loss: 7.68945199e-07
Iter: 719 loss: 7.6797096e-07
Iter: 720 loss: 7.72442377e-07
Iter: 721 loss: 7.67594e-07
Iter: 722 loss: 7.67082383e-07
Iter: 723 loss: 7.66910716e-07
Iter: 724 loss: 7.66287371e-07
Iter: 725 loss: 7.66628148e-07
Iter: 726 loss: 7.65857408e-07
Iter: 727 loss: 7.65059781e-07
Iter: 728 loss: 7.64234301e-07
Iter: 729 loss: 7.64096853e-07
Iter: 730 loss: 7.63361072e-07
Iter: 731 loss: 7.69989128e-07
Iter: 732 loss: 7.63300761e-07
Iter: 733 loss: 7.62803e-07
Iter: 734 loss: 7.62118134e-07
Iter: 735 loss: 7.62074308e-07
Iter: 736 loss: 7.61522585e-07
Iter: 737 loss: 7.6517847e-07
Iter: 738 loss: 7.61452725e-07
Iter: 739 loss: 7.60939258e-07
Iter: 740 loss: 7.60970352e-07
Iter: 741 loss: 7.60567673e-07
Iter: 742 loss: 7.59745603e-07
Iter: 743 loss: 7.74768068e-07
Iter: 744 loss: 7.59731734e-07
Iter: 745 loss: 7.59450359e-07
Iter: 746 loss: 7.59334796e-07
Iter: 747 loss: 7.58983447e-07
Iter: 748 loss: 7.58063265e-07
Iter: 749 loss: 7.64468155e-07
Iter: 750 loss: 7.57811677e-07
Iter: 751 loss: 7.57120461e-07
Iter: 752 loss: 7.62289119e-07
Iter: 753 loss: 7.57035139e-07
Iter: 754 loss: 7.56564646e-07
Iter: 755 loss: 7.56562144e-07
Iter: 756 loss: 7.56190502e-07
Iter: 757 loss: 7.55273959e-07
Iter: 758 loss: 7.68631367e-07
Iter: 759 loss: 7.55239455e-07
Iter: 760 loss: 7.54698306e-07
Iter: 761 loss: 7.55628548e-07
Iter: 762 loss: 7.54469e-07
Iter: 763 loss: 7.53728386e-07
Iter: 764 loss: 7.57161274e-07
Iter: 765 loss: 7.53532504e-07
Iter: 766 loss: 7.53344352e-07
Iter: 767 loss: 7.5328154e-07
Iter: 768 loss: 7.53019776e-07
Iter: 769 loss: 7.52832307e-07
Iter: 770 loss: 7.52759433e-07
Iter: 771 loss: 7.52397341e-07
Iter: 772 loss: 7.52004325e-07
Iter: 773 loss: 7.5193384e-07
Iter: 774 loss: 7.5160483e-07
Iter: 775 loss: 7.51480513e-07
Iter: 776 loss: 7.51187201e-07
Iter: 777 loss: 7.5024127e-07
Iter: 778 loss: 7.52763299e-07
Iter: 779 loss: 7.4972445e-07
Iter: 780 loss: 7.502465e-07
Iter: 781 loss: 7.49214e-07
Iter: 782 loss: 7.48970479e-07
Iter: 783 loss: 7.48508342e-07
Iter: 784 loss: 7.48505386e-07
Iter: 785 loss: 7.48088326e-07
Iter: 786 loss: 7.48095317e-07
Iter: 787 loss: 7.47673198e-07
Iter: 788 loss: 7.47789898e-07
Iter: 789 loss: 7.47329921e-07
Iter: 790 loss: 7.47038257e-07
Iter: 791 loss: 7.46946796e-07
Iter: 792 loss: 7.46765181e-07
Iter: 793 loss: 7.46329761e-07
Iter: 794 loss: 7.45960165e-07
Iter: 795 loss: 7.45878822e-07
Iter: 796 loss: 7.45265481e-07
Iter: 797 loss: 7.46488467e-07
Iter: 798 loss: 7.45018497e-07
Iter: 799 loss: 7.44281067e-07
Iter: 800 loss: 7.48343e-07
Iter: 801 loss: 7.44196655e-07
Iter: 802 loss: 7.43318196e-07
Iter: 803 loss: 7.44024874e-07
Iter: 804 loss: 7.42786085e-07
Iter: 805 loss: 7.42184852e-07
Iter: 806 loss: 7.42928648e-07
Iter: 807 loss: 7.4189694e-07
Iter: 808 loss: 7.41379552e-07
Iter: 809 loss: 7.41391034e-07
Iter: 810 loss: 7.40866085e-07
Iter: 811 loss: 7.39875418e-07
Iter: 812 loss: 7.59909369e-07
Iter: 813 loss: 7.3989986e-07
Iter: 814 loss: 7.40014684e-07
Iter: 815 loss: 7.39629513e-07
Iter: 816 loss: 7.39468646e-07
Iter: 817 loss: 7.38849508e-07
Iter: 818 loss: 7.42716225e-07
Iter: 819 loss: 7.38695348e-07
Iter: 820 loss: 7.38041649e-07
Iter: 821 loss: 7.39097288e-07
Iter: 822 loss: 7.37740493e-07
Iter: 823 loss: 7.36801496e-07
Iter: 824 loss: 7.36383129e-07
Iter: 825 loss: 7.35894616e-07
Iter: 826 loss: 7.36648644e-07
Iter: 827 loss: 7.35532922e-07
Iter: 828 loss: 7.35112906e-07
Iter: 829 loss: 7.33994398e-07
Iter: 830 loss: 7.42794839e-07
Iter: 831 loss: 7.33778336e-07
Iter: 832 loss: 7.32130388e-07
Iter: 833 loss: 7.44574322e-07
Iter: 834 loss: 7.32012779e-07
Iter: 835 loss: 7.31463729e-07
Iter: 836 loss: 7.31422119e-07
Iter: 837 loss: 7.31059288e-07
Iter: 838 loss: 7.33750653e-07
Iter: 839 loss: 7.31025352e-07
Iter: 840 loss: 7.30683e-07
Iter: 841 loss: 7.29986766e-07
Iter: 842 loss: 7.43347243e-07
Iter: 843 loss: 7.29973749e-07
Iter: 844 loss: 7.29463864e-07
Iter: 845 loss: 7.35042875e-07
Iter: 846 loss: 7.29448288e-07
Iter: 847 loss: 7.28852456e-07
Iter: 848 loss: 7.29988358e-07
Iter: 849 loss: 7.28630823e-07
Iter: 850 loss: 7.2819364e-07
Iter: 851 loss: 7.27686825e-07
Iter: 852 loss: 7.27645897e-07
Iter: 853 loss: 7.27102588e-07
Iter: 854 loss: 7.27084512e-07
Iter: 855 loss: 7.26545409e-07
Iter: 856 loss: 7.25619259e-07
Iter: 857 loss: 7.25616189e-07
Iter: 858 loss: 7.2499131e-07
Iter: 859 loss: 7.25795303e-07
Iter: 860 loss: 7.24674578e-07
Iter: 861 loss: 7.24012523e-07
Iter: 862 loss: 7.24048334e-07
Iter: 863 loss: 7.23542257e-07
Iter: 864 loss: 7.22631569e-07
Iter: 865 loss: 7.43785677e-07
Iter: 866 loss: 7.22635718e-07
Iter: 867 loss: 7.21747256e-07
Iter: 868 loss: 7.21607421e-07
Iter: 869 loss: 7.20978e-07
Iter: 870 loss: 7.20504602e-07
Iter: 871 loss: 7.20367e-07
Iter: 872 loss: 7.20022854e-07
Iter: 873 loss: 7.20912453e-07
Iter: 874 loss: 7.19895127e-07
Iter: 875 loss: 7.19404284e-07
Iter: 876 loss: 7.21359299e-07
Iter: 877 loss: 7.19307536e-07
Iter: 878 loss: 7.18896899e-07
Iter: 879 loss: 7.18605634e-07
Iter: 880 loss: 7.18481203e-07
Iter: 881 loss: 7.18106435e-07
Iter: 882 loss: 7.18077e-07
Iter: 883 loss: 7.17780324e-07
Iter: 884 loss: 7.1688055e-07
Iter: 885 loss: 7.23864e-07
Iter: 886 loss: 7.16744751e-07
Iter: 887 loss: 7.15816896e-07
Iter: 888 loss: 7.15799331e-07
Iter: 889 loss: 7.15120677e-07
Iter: 890 loss: 7.14515238e-07
Iter: 891 loss: 7.14316798e-07
Iter: 892 loss: 7.13535314e-07
Iter: 893 loss: 7.12904409e-07
Iter: 894 loss: 7.12635256e-07
Iter: 895 loss: 7.12277597e-07
Iter: 896 loss: 7.12223198e-07
Iter: 897 loss: 7.1186173e-07
Iter: 898 loss: 7.1530178e-07
Iter: 899 loss: 7.11841096e-07
Iter: 900 loss: 7.1161611e-07
Iter: 901 loss: 7.11001803e-07
Iter: 902 loss: 7.14151724e-07
Iter: 903 loss: 7.1074669e-07
Iter: 904 loss: 7.10112e-07
Iter: 905 loss: 7.11555288e-07
Iter: 906 loss: 7.09811616e-07
Iter: 907 loss: 7.09120627e-07
Iter: 908 loss: 7.17521914e-07
Iter: 909 loss: 7.09085555e-07
Iter: 910 loss: 7.08663947e-07
Iter: 911 loss: 7.1563511e-07
Iter: 912 loss: 7.0866713e-07
Iter: 913 loss: 7.08260814e-07
Iter: 914 loss: 7.07627635e-07
Iter: 915 loss: 7.24171798e-07
Iter: 916 loss: 7.0761871e-07
Iter: 917 loss: 7.06914932e-07
Iter: 918 loss: 7.16268119e-07
Iter: 919 loss: 7.06897652e-07
Iter: 920 loss: 7.06418859e-07
Iter: 921 loss: 7.07151287e-07
Iter: 922 loss: 7.06171932e-07
Iter: 923 loss: 7.05817456e-07
Iter: 924 loss: 7.06161472e-07
Iter: 925 loss: 7.05646e-07
Iter: 926 loss: 7.04916772e-07
Iter: 927 loss: 7.05824846e-07
Iter: 928 loss: 7.04582931e-07
Iter: 929 loss: 7.04025751e-07
Iter: 930 loss: 7.04128752e-07
Iter: 931 loss: 7.03630292e-07
Iter: 932 loss: 7.0328349e-07
Iter: 933 loss: 7.0339479e-07
Iter: 934 loss: 7.0302724e-07
Iter: 935 loss: 7.02649e-07
Iter: 936 loss: 7.02620696e-07
Iter: 937 loss: 7.02432885e-07
Iter: 938 loss: 7.01936642e-07
Iter: 939 loss: 7.07830281e-07
Iter: 940 loss: 7.01897648e-07
Iter: 941 loss: 7.01239742e-07
Iter: 942 loss: 7.00553414e-07
Iter: 943 loss: 7.00414034e-07
Iter: 944 loss: 6.99592476e-07
Iter: 945 loss: 6.99594125e-07
Iter: 946 loss: 6.98762506e-07
Iter: 947 loss: 7.02623822e-07
Iter: 948 loss: 6.98615167e-07
Iter: 949 loss: 6.97935548e-07
Iter: 950 loss: 6.98694862e-07
Iter: 951 loss: 6.97553446e-07
Iter: 952 loss: 6.96955397e-07
Iter: 953 loss: 7.04942181e-07
Iter: 954 loss: 6.96948177e-07
Iter: 955 loss: 6.96662141e-07
Iter: 956 loss: 6.96405777e-07
Iter: 957 loss: 6.96360758e-07
Iter: 958 loss: 6.95868664e-07
Iter: 959 loss: 6.98525469e-07
Iter: 960 loss: 6.95795563e-07
Iter: 961 loss: 6.95289714e-07
Iter: 962 loss: 6.95587516e-07
Iter: 963 loss: 6.94918924e-07
Iter: 964 loss: 6.94512437e-07
Iter: 965 loss: 6.93940933e-07
Iter: 966 loss: 6.93930929e-07
Iter: 967 loss: 6.93237553e-07
Iter: 968 loss: 6.9993996e-07
Iter: 969 loss: 6.93245738e-07
Iter: 970 loss: 6.92647518e-07
Iter: 971 loss: 6.98533199e-07
Iter: 972 loss: 6.9264479e-07
Iter: 973 loss: 6.92367223e-07
Iter: 974 loss: 6.9147e-07
Iter: 975 loss: 6.93502102e-07
Iter: 976 loss: 6.90981835e-07
Iter: 977 loss: 6.89879528e-07
Iter: 978 loss: 7.04079923e-07
Iter: 979 loss: 6.89884871e-07
Iter: 980 loss: 6.89289891e-07
Iter: 981 loss: 6.89290914e-07
Iter: 982 loss: 6.88653813e-07
Iter: 983 loss: 6.8775978e-07
Iter: 984 loss: 6.87694126e-07
Iter: 985 loss: 6.87152578e-07
Iter: 986 loss: 6.87092e-07
Iter: 987 loss: 6.86664066e-07
Iter: 988 loss: 6.86610349e-07
Iter: 989 loss: 6.8627412e-07
Iter: 990 loss: 6.85738712e-07
Iter: 991 loss: 6.86143721e-07
Iter: 992 loss: 6.85383952e-07
Iter: 993 loss: 6.84761289e-07
Iter: 994 loss: 6.93353343e-07
Iter: 995 loss: 6.8476578e-07
Iter: 996 loss: 6.84352244e-07
Iter: 997 loss: 6.83847702e-07
Iter: 998 loss: 6.8380848e-07
Iter: 999 loss: 6.83158191e-07
Iter: 1000 loss: 6.8243736e-07
Iter: 1001 loss: 6.82359541e-07
Iter: 1002 loss: 6.8206316e-07
Iter: 1003 loss: 6.81712436e-07
Iter: 1004 loss: 6.81275708e-07
Iter: 1005 loss: 6.80516337e-07
Iter: 1006 loss: 6.8050872e-07
Iter: 1007 loss: 6.79523851e-07
Iter: 1008 loss: 6.77832645e-07
Iter: 1009 loss: 6.77842536e-07
Iter: 1010 loss: 6.7730025e-07
Iter: 1011 loss: 6.76961633e-07
Iter: 1012 loss: 6.76198511e-07
Iter: 1013 loss: 6.79942218e-07
Iter: 1014 loss: 6.76023376e-07
Iter: 1015 loss: 6.75468243e-07
Iter: 1016 loss: 6.75355523e-07
Iter: 1017 loss: 6.74973933e-07
Iter: 1018 loss: 6.74039939e-07
Iter: 1019 loss: 6.80569428e-07
Iter: 1020 loss: 6.73944612e-07
Iter: 1021 loss: 6.734615e-07
Iter: 1022 loss: 6.73241914e-07
Iter: 1023 loss: 6.72992201e-07
Iter: 1024 loss: 6.7238858e-07
Iter: 1025 loss: 6.7893626e-07
Iter: 1026 loss: 6.72355e-07
Iter: 1027 loss: 6.71805537e-07
Iter: 1028 loss: 6.72200827e-07
Iter: 1029 loss: 6.71451176e-07
Iter: 1030 loss: 6.71049463e-07
Iter: 1031 loss: 6.70595739e-07
Iter: 1032 loss: 6.70488419e-07
Iter: 1033 loss: 6.6970938e-07
Iter: 1034 loss: 6.73931822e-07
Iter: 1035 loss: 6.69589099e-07
Iter: 1036 loss: 6.68591724e-07
Iter: 1037 loss: 6.7120942e-07
Iter: 1038 loss: 6.68272548e-07
Iter: 1039 loss: 6.67671657e-07
Iter: 1040 loss: 6.66615392e-07
Iter: 1041 loss: 6.90577394e-07
Iter: 1042 loss: 6.66642848e-07
Iter: 1043 loss: 6.65070161e-07
Iter: 1044 loss: 6.67877032e-07
Iter: 1045 loss: 6.64370077e-07
Iter: 1046 loss: 6.64288336e-07
Iter: 1047 loss: 6.63671472e-07
Iter: 1048 loss: 6.63220305e-07
Iter: 1049 loss: 6.62466505e-07
Iter: 1050 loss: 6.62456046e-07
Iter: 1051 loss: 6.61842705e-07
Iter: 1052 loss: 6.61812123e-07
Iter: 1053 loss: 6.6138972e-07
Iter: 1054 loss: 6.60927299e-07
Iter: 1055 loss: 6.60883757e-07
Iter: 1056 loss: 6.60324929e-07
Iter: 1057 loss: 6.61922229e-07
Iter: 1058 loss: 6.60162698e-07
Iter: 1059 loss: 6.59523721e-07
Iter: 1060 loss: 6.64332731e-07
Iter: 1061 loss: 6.59485522e-07
Iter: 1062 loss: 6.59018724e-07
Iter: 1063 loss: 6.58742692e-07
Iter: 1064 loss: 6.58589727e-07
Iter: 1065 loss: 6.58045963e-07
Iter: 1066 loss: 6.5857364e-07
Iter: 1067 loss: 6.57762428e-07
Iter: 1068 loss: 6.56880729e-07
Iter: 1069 loss: 6.61498632e-07
Iter: 1070 loss: 6.56763746e-07
Iter: 1071 loss: 6.55982149e-07
Iter: 1072 loss: 6.54985683e-07
Iter: 1073 loss: 6.54901726e-07
Iter: 1074 loss: 6.53984273e-07
Iter: 1075 loss: 6.54651217e-07
Iter: 1076 loss: 6.53384348e-07
Iter: 1077 loss: 6.52500091e-07
Iter: 1078 loss: 6.52499295e-07
Iter: 1079 loss: 6.51634195e-07
Iter: 1080 loss: 6.53502696e-07
Iter: 1081 loss: 6.51285177e-07
Iter: 1082 loss: 6.50932122e-07
Iter: 1083 loss: 6.54466248e-07
Iter: 1084 loss: 6.50909328e-07
Iter: 1085 loss: 6.50537629e-07
Iter: 1086 loss: 6.50115908e-07
Iter: 1087 loss: 6.50037464e-07
Iter: 1088 loss: 6.4952053e-07
Iter: 1089 loss: 6.50510231e-07
Iter: 1090 loss: 6.49307708e-07
Iter: 1091 loss: 6.49000867e-07
Iter: 1092 loss: 6.48991545e-07
Iter: 1093 loss: 6.48650598e-07
Iter: 1094 loss: 6.47802267e-07
Iter: 1095 loss: 6.55750796e-07
Iter: 1096 loss: 6.47701881e-07
Iter: 1097 loss: 6.46569219e-07
Iter: 1098 loss: 6.50991808e-07
Iter: 1099 loss: 6.46312571e-07
Iter: 1100 loss: 6.45765112e-07
Iter: 1101 loss: 6.45748798e-07
Iter: 1102 loss: 6.4530127e-07
Iter: 1103 loss: 6.44345278e-07
Iter: 1104 loss: 6.65059702e-07
Iter: 1105 loss: 6.44345732e-07
Iter: 1106 loss: 6.43291969e-07
Iter: 1107 loss: 6.44403769e-07
Iter: 1108 loss: 6.42726036e-07
Iter: 1109 loss: 6.42126565e-07
Iter: 1110 loss: 6.49545768e-07
Iter: 1111 loss: 6.42132477e-07
Iter: 1112 loss: 6.41534484e-07
Iter: 1113 loss: 6.45450825e-07
Iter: 1114 loss: 6.414989e-07
Iter: 1115 loss: 6.41139422e-07
Iter: 1116 loss: 6.40932171e-07
Iter: 1117 loss: 6.40769827e-07
Iter: 1118 loss: 6.40171663e-07
Iter: 1119 loss: 6.45713556e-07
Iter: 1120 loss: 6.40132498e-07
Iter: 1121 loss: 6.39789562e-07
Iter: 1122 loss: 6.38914e-07
Iter: 1123 loss: 6.49412186e-07
Iter: 1124 loss: 6.38856e-07
Iter: 1125 loss: 6.38446124e-07
Iter: 1126 loss: 6.38373081e-07
Iter: 1127 loss: 6.37953065e-07
Iter: 1128 loss: 6.39129269e-07
Iter: 1129 loss: 6.37819539e-07
Iter: 1130 loss: 6.37452672e-07
Iter: 1131 loss: 6.36858545e-07
Iter: 1132 loss: 6.36855475e-07
Iter: 1133 loss: 6.36288519e-07
Iter: 1134 loss: 6.36296e-07
Iter: 1135 loss: 6.35872254e-07
Iter: 1136 loss: 6.36820801e-07
Iter: 1137 loss: 6.3568632e-07
Iter: 1138 loss: 6.35318145e-07
Iter: 1139 loss: 6.3449022e-07
Iter: 1140 loss: 6.45625732e-07
Iter: 1141 loss: 6.34465323e-07
Iter: 1142 loss: 6.33688558e-07
Iter: 1143 loss: 6.42166128e-07
Iter: 1144 loss: 6.33711693e-07
Iter: 1145 loss: 6.33383081e-07
Iter: 1146 loss: 6.33367392e-07
Iter: 1147 loss: 6.33048103e-07
Iter: 1148 loss: 6.32441697e-07
Iter: 1149 loss: 6.43414069e-07
Iter: 1150 loss: 6.3242851e-07
Iter: 1151 loss: 6.32293677e-07
Iter: 1152 loss: 6.32132355e-07
Iter: 1153 loss: 6.31930789e-07
Iter: 1154 loss: 6.31434887e-07
Iter: 1155 loss: 6.37898722e-07
Iter: 1156 loss: 6.31390265e-07
Iter: 1157 loss: 6.30842521e-07
Iter: 1158 loss: 6.32789295e-07
Iter: 1159 loss: 6.30682393e-07
Iter: 1160 loss: 6.30169382e-07
Iter: 1161 loss: 6.35975368e-07
Iter: 1162 loss: 6.30173872e-07
Iter: 1163 loss: 6.29892611e-07
Iter: 1164 loss: 6.29469071e-07
Iter: 1165 loss: 6.29449517e-07
Iter: 1166 loss: 6.28853e-07
Iter: 1167 loss: 6.30498221e-07
Iter: 1168 loss: 6.28615624e-07
Iter: 1169 loss: 6.27816689e-07
Iter: 1170 loss: 6.32479e-07
Iter: 1171 loss: 6.27746e-07
Iter: 1172 loss: 6.27260931e-07
Iter: 1173 loss: 6.27173279e-07
Iter: 1174 loss: 6.26903443e-07
Iter: 1175 loss: 6.263665e-07
Iter: 1176 loss: 6.25834e-07
Iter: 1177 loss: 6.25726955e-07
Iter: 1178 loss: 6.25567736e-07
Iter: 1179 loss: 6.25321832e-07
Iter: 1180 loss: 6.24934557e-07
Iter: 1181 loss: 6.25218263e-07
Iter: 1182 loss: 6.24717131e-07
Iter: 1183 loss: 6.2438022e-07
Iter: 1184 loss: 6.2542756e-07
Iter: 1185 loss: 6.24289612e-07
Iter: 1186 loss: 6.23793312e-07
Iter: 1187 loss: 6.2336153e-07
Iter: 1188 loss: 6.23222945e-07
Iter: 1189 loss: 6.22747734e-07
Iter: 1190 loss: 6.24031941e-07
Iter: 1191 loss: 6.22583457e-07
Iter: 1192 loss: 6.22073117e-07
Iter: 1193 loss: 6.26647534e-07
Iter: 1194 loss: 6.22057144e-07
Iter: 1195 loss: 6.2159836e-07
Iter: 1196 loss: 6.20840581e-07
Iter: 1197 loss: 6.20852063e-07
Iter: 1198 loss: 6.20157948e-07
Iter: 1199 loss: 6.26084272e-07
Iter: 1200 loss: 6.2012748e-07
Iter: 1201 loss: 6.1967171e-07
Iter: 1202 loss: 6.22869834e-07
Iter: 1203 loss: 6.19633511e-07
Iter: 1204 loss: 6.19178309e-07
Iter: 1205 loss: 6.18262618e-07
Iter: 1206 loss: 6.3424676e-07
Iter: 1207 loss: 6.18209128e-07
Iter: 1208 loss: 6.17332205e-07
Iter: 1209 loss: 6.2261114e-07
Iter: 1210 loss: 6.17219314e-07
Iter: 1211 loss: 6.16839372e-07
Iter: 1212 loss: 6.1981666e-07
Iter: 1213 loss: 6.16847274e-07
Iter: 1214 loss: 6.16413786e-07
Iter: 1215 loss: 6.1742719e-07
Iter: 1216 loss: 6.16257694e-07
Iter: 1217 loss: 6.1590805e-07
Iter: 1218 loss: 6.16488e-07
Iter: 1219 loss: 6.15758609e-07
Iter: 1220 loss: 6.15379918e-07
Iter: 1221 loss: 6.18834747e-07
Iter: 1222 loss: 6.15370595e-07
Iter: 1223 loss: 6.15131285e-07
Iter: 1224 loss: 6.14535e-07
Iter: 1225 loss: 6.18347144e-07
Iter: 1226 loss: 6.14384248e-07
Iter: 1227 loss: 6.1404819e-07
Iter: 1228 loss: 6.13909378e-07
Iter: 1229 loss: 6.13526254e-07
Iter: 1230 loss: 6.13331167e-07
Iter: 1231 loss: 6.13159955e-07
Iter: 1232 loss: 6.12687e-07
Iter: 1233 loss: 6.12761198e-07
Iter: 1234 loss: 6.12343513e-07
Iter: 1235 loss: 6.11991e-07
Iter: 1236 loss: 6.11946575e-07
Iter: 1237 loss: 6.11709538e-07
Iter: 1238 loss: 6.11598693e-07
Iter: 1239 loss: 6.11473581e-07
Iter: 1240 loss: 6.1110012e-07
Iter: 1241 loss: 6.10538564e-07
Iter: 1242 loss: 6.10537825e-07
Iter: 1243 loss: 6.09997528e-07
Iter: 1244 loss: 6.15422209e-07
Iter: 1245 loss: 6.09982067e-07
Iter: 1246 loss: 6.09722861e-07
Iter: 1247 loss: 6.09699384e-07
Iter: 1248 loss: 6.09452172e-07
Iter: 1249 loss: 6.08798132e-07
Iter: 1250 loss: 6.13697637e-07
Iter: 1251 loss: 6.08680807e-07
Iter: 1252 loss: 6.08423136e-07
Iter: 1253 loss: 6.08230607e-07
Iter: 1254 loss: 6.07985839e-07
Iter: 1255 loss: 6.07465211e-07
Iter: 1256 loss: 6.1795572e-07
Iter: 1257 loss: 6.07453785e-07
Iter: 1258 loss: 6.06985509e-07
Iter: 1259 loss: 6.10002417e-07
Iter: 1260 loss: 6.06943445e-07
Iter: 1261 loss: 6.06465e-07
Iter: 1262 loss: 6.08975256e-07
Iter: 1263 loss: 6.0637683e-07
Iter: 1264 loss: 6.06122626e-07
Iter: 1265 loss: 6.05753826e-07
Iter: 1266 loss: 6.0574348e-07
Iter: 1267 loss: 6.05297032e-07
Iter: 1268 loss: 6.09735594e-07
Iter: 1269 loss: 6.05256275e-07
Iter: 1270 loss: 6.04846605e-07
Iter: 1271 loss: 6.05164928e-07
Iter: 1272 loss: 6.04549825e-07
Iter: 1273 loss: 6.04183924e-07
Iter: 1274 loss: 6.04329102e-07
Iter: 1275 loss: 6.03891749e-07
Iter: 1276 loss: 6.03356568e-07
Iter: 1277 loss: 6.02534556e-07
Iter: 1278 loss: 6.02523073e-07
Iter: 1279 loss: 6.02557407e-07
Iter: 1280 loss: 6.02182809e-07
Iter: 1281 loss: 6.01846921e-07
Iter: 1282 loss: 6.01879037e-07
Iter: 1283 loss: 6.01580155e-07
Iter: 1284 loss: 6.01197542e-07
Iter: 1285 loss: 6.02868397e-07
Iter: 1286 loss: 6.0113365e-07
Iter: 1287 loss: 6.00735291e-07
Iter: 1288 loss: 6.0214586e-07
Iter: 1289 loss: 6.00604608e-07
Iter: 1290 loss: 6.00413898e-07
Iter: 1291 loss: 5.99940563e-07
Iter: 1292 loss: 6.07973448e-07
Iter: 1293 loss: 5.99931866e-07
Iter: 1294 loss: 5.99604618e-07
Iter: 1295 loss: 5.99542091e-07
Iter: 1296 loss: 5.99267e-07
Iter: 1297 loss: 5.98905785e-07
Iter: 1298 loss: 5.98895156e-07
Iter: 1299 loss: 5.98467636e-07
Iter: 1300 loss: 5.99884402e-07
Iter: 1301 loss: 5.98367535e-07
Iter: 1302 loss: 5.97919154e-07
Iter: 1303 loss: 6.00991029e-07
Iter: 1304 loss: 5.97859923e-07
Iter: 1305 loss: 5.97593726e-07
Iter: 1306 loss: 5.97798589e-07
Iter: 1307 loss: 5.97442238e-07
Iter: 1308 loss: 5.97100495e-07
Iter: 1309 loss: 5.96983057e-07
Iter: 1310 loss: 5.96800305e-07
Iter: 1311 loss: 5.96317705e-07
Iter: 1312 loss: 5.98410338e-07
Iter: 1313 loss: 5.96224822e-07
Iter: 1314 loss: 5.95834138e-07
Iter: 1315 loss: 5.95825668e-07
Iter: 1316 loss: 5.9563e-07
Iter: 1317 loss: 5.95286792e-07
Iter: 1318 loss: 5.95272127e-07
Iter: 1319 loss: 5.94827895e-07
Iter: 1320 loss: 5.99912312e-07
Iter: 1321 loss: 5.94817266e-07
Iter: 1322 loss: 5.94564654e-07
Iter: 1323 loss: 5.94094899e-07
Iter: 1324 loss: 5.94100072e-07
Iter: 1325 loss: 5.93692562e-07
Iter: 1326 loss: 5.95705046e-07
Iter: 1327 loss: 5.93680795e-07
Iter: 1328 loss: 5.93199616e-07
Iter: 1329 loss: 5.95793267e-07
Iter: 1330 loss: 5.93127538e-07
Iter: 1331 loss: 5.92970139e-07
Iter: 1332 loss: 5.92836159e-07
Iter: 1333 loss: 5.92770107e-07
Iter: 1334 loss: 5.92517722e-07
Iter: 1335 loss: 5.95025142e-07
Iter: 1336 loss: 5.92507774e-07
Iter: 1337 loss: 5.92230265e-07
Iter: 1338 loss: 5.91777621e-07
Iter: 1339 loss: 6.02655746e-07
Iter: 1340 loss: 5.91771709e-07
Iter: 1341 loss: 5.9133481e-07
Iter: 1342 loss: 5.95376719e-07
Iter: 1343 loss: 5.91308549e-07
Iter: 1344 loss: 5.91026037e-07
Iter: 1345 loss: 5.90873128e-07
Iter: 1346 loss: 5.90743298e-07
Iter: 1347 loss: 5.90279683e-07
Iter: 1348 loss: 5.95373535e-07
Iter: 1349 loss: 5.90293041e-07
Iter: 1350 loss: 5.90045715e-07
Iter: 1351 loss: 5.89715967e-07
Iter: 1352 loss: 5.89715569e-07
Iter: 1353 loss: 5.89340573e-07
Iter: 1354 loss: 5.93267032e-07
Iter: 1355 loss: 5.89332842e-07
Iter: 1356 loss: 5.88922376e-07
Iter: 1357 loss: 5.89195508e-07
Iter: 1358 loss: 5.88675107e-07
Iter: 1359 loss: 5.88343312e-07
Iter: 1360 loss: 5.87854743e-07
Iter: 1361 loss: 5.87824843e-07
Iter: 1362 loss: 5.87488103e-07
Iter: 1363 loss: 5.87458089e-07
Iter: 1364 loss: 5.8708963e-07
Iter: 1365 loss: 5.87656359e-07
Iter: 1366 loss: 5.86928877e-07
Iter: 1367 loss: 5.86712758e-07
Iter: 1368 loss: 5.86583781e-07
Iter: 1369 loss: 5.86505962e-07
Iter: 1370 loss: 5.86207193e-07
Iter: 1371 loss: 5.86203328e-07
Iter: 1372 loss: 5.86037913e-07
Iter: 1373 loss: 5.8587176e-07
Iter: 1374 loss: 5.85803377e-07
Iter: 1375 loss: 5.85513078e-07
Iter: 1376 loss: 5.85562e-07
Iter: 1377 loss: 5.85272119e-07
Iter: 1378 loss: 5.84927193e-07
Iter: 1379 loss: 5.84941176e-07
Iter: 1380 loss: 5.84631152e-07
Iter: 1381 loss: 5.85093e-07
Iter: 1382 loss: 5.84524059e-07
Iter: 1383 loss: 5.84289864e-07
Iter: 1384 loss: 5.84025202e-07
Iter: 1385 loss: 5.84025088e-07
Iter: 1386 loss: 5.83660778e-07
Iter: 1387 loss: 5.83653161e-07
Iter: 1388 loss: 5.83477402e-07
Iter: 1389 loss: 5.83253097e-07
Iter: 1390 loss: 5.83230246e-07
Iter: 1391 loss: 5.82962514e-07
Iter: 1392 loss: 5.82673e-07
Iter: 1393 loss: 5.82616622e-07
Iter: 1394 loss: 5.82222924e-07
Iter: 1395 loss: 5.82222356e-07
Iter: 1396 loss: 5.82021926e-07
Iter: 1397 loss: 5.81637551e-07
Iter: 1398 loss: 5.86242209e-07
Iter: 1399 loss: 5.81575819e-07
Iter: 1400 loss: 5.81009942e-07
Iter: 1401 loss: 5.81516474e-07
Iter: 1402 loss: 5.80696224e-07
Iter: 1403 loss: 5.80582537e-07
Iter: 1404 loss: 5.80476e-07
Iter: 1405 loss: 5.80217716e-07
Iter: 1406 loss: 5.79913035e-07
Iter: 1407 loss: 5.79871426e-07
Iter: 1408 loss: 5.79540711e-07
Iter: 1409 loss: 5.80330209e-07
Iter: 1410 loss: 5.79415769e-07
Iter: 1411 loss: 5.79056746e-07
Iter: 1412 loss: 5.7980958e-07
Iter: 1413 loss: 5.78915433e-07
Iter: 1414 loss: 5.78434879e-07
Iter: 1415 loss: 5.81060249e-07
Iter: 1416 loss: 5.7834842e-07
Iter: 1417 loss: 5.78002e-07
Iter: 1418 loss: 5.78077447e-07
Iter: 1419 loss: 5.77772596e-07
Iter: 1420 loss: 5.77584387e-07
Iter: 1421 loss: 5.77550054e-07
Iter: 1422 loss: 5.77390892e-07
Iter: 1423 loss: 5.77007427e-07
Iter: 1424 loss: 5.81219069e-07
Iter: 1425 loss: 5.76974344e-07
Iter: 1426 loss: 5.76696209e-07
Iter: 1427 loss: 5.79088919e-07
Iter: 1428 loss: 5.76677508e-07
Iter: 1429 loss: 5.76409377e-07
Iter: 1430 loss: 5.777365e-07
Iter: 1431 loss: 5.76325078e-07
Iter: 1432 loss: 5.76108903e-07
Iter: 1433 loss: 5.75750789e-07
Iter: 1434 loss: 5.75748572e-07
Iter: 1435 loss: 5.75418767e-07
Iter: 1436 loss: 5.75640229e-07
Iter: 1437 loss: 5.7523107e-07
Iter: 1438 loss: 5.74794853e-07
Iter: 1439 loss: 5.78390313e-07
Iter: 1440 loss: 5.74767e-07
Iter: 1441 loss: 5.74630235e-07
Iter: 1442 loss: 5.74618298e-07
Iter: 1443 loss: 5.74449246e-07
Iter: 1444 loss: 5.74153887e-07
Iter: 1445 loss: 5.81041832e-07
Iter: 1446 loss: 5.74157696e-07
Iter: 1447 loss: 5.73899342e-07
Iter: 1448 loss: 5.75795184e-07
Iter: 1449 loss: 5.73875923e-07
Iter: 1450 loss: 5.73603415e-07
Iter: 1451 loss: 5.74769899e-07
Iter: 1452 loss: 5.7357073e-07
Iter: 1453 loss: 5.733732e-07
Iter: 1454 loss: 5.729907e-07
Iter: 1455 loss: 5.79401785e-07
Iter: 1456 loss: 5.72960744e-07
Iter: 1457 loss: 5.72945396e-07
Iter: 1458 loss: 5.72825115e-07
Iter: 1459 loss: 5.72694887e-07
Iter: 1460 loss: 5.72565966e-07
Iter: 1461 loss: 5.72534304e-07
Iter: 1462 loss: 5.72293288e-07
Iter: 1463 loss: 5.72404701e-07
Iter: 1464 loss: 5.72110366e-07
Iter: 1465 loss: 5.71861904e-07
Iter: 1466 loss: 5.71888052e-07
Iter: 1467 loss: 5.71781413e-07
Iter: 1468 loss: 5.7145013e-07
Iter: 1469 loss: 5.71864916e-07
Iter: 1470 loss: 5.71205305e-07
Iter: 1471 loss: 5.70608734e-07
Iter: 1472 loss: 5.73007469e-07
Iter: 1473 loss: 5.70474242e-07
Iter: 1474 loss: 5.69984081e-07
Iter: 1475 loss: 5.7141574e-07
Iter: 1476 loss: 5.69845838e-07
Iter: 1477 loss: 5.69790245e-07
Iter: 1478 loss: 5.69617214e-07
Iter: 1479 loss: 5.69526378e-07
Iter: 1480 loss: 5.69375857e-07
Iter: 1481 loss: 5.72611157e-07
Iter: 1482 loss: 5.69365511e-07
Iter: 1483 loss: 5.69141207e-07
Iter: 1484 loss: 5.70558541e-07
Iter: 1485 loss: 5.69123074e-07
Iter: 1486 loss: 5.68924918e-07
Iter: 1487 loss: 5.68868472e-07
Iter: 1488 loss: 5.6877883e-07
Iter: 1489 loss: 5.68619726e-07
Iter: 1490 loss: 5.68882228e-07
Iter: 1491 loss: 5.68546966e-07
Iter: 1492 loss: 5.68301232e-07
Iter: 1493 loss: 5.69144333e-07
Iter: 1494 loss: 5.68252631e-07
Iter: 1495 loss: 5.67993879e-07
Iter: 1496 loss: 5.67640711e-07
Iter: 1497 loss: 5.67657e-07
Iter: 1498 loss: 5.67510369e-07
Iter: 1499 loss: 5.67475411e-07
Iter: 1500 loss: 5.67306813e-07
Iter: 1501 loss: 5.67382131e-07
Iter: 1502 loss: 5.67217342e-07
Iter: 1503 loss: 5.67003326e-07
Iter: 1504 loss: 5.66744461e-07
Iter: 1505 loss: 5.6673764e-07
Iter: 1506 loss: 5.6643114e-07
Iter: 1507 loss: 5.66464962e-07
Iter: 1508 loss: 5.66235826e-07
Iter: 1509 loss: 5.65795517e-07
Iter: 1510 loss: 5.692184e-07
Iter: 1511 loss: 5.65765276e-07
Iter: 1512 loss: 5.65518064e-07
Iter: 1513 loss: 5.65532105e-07
Iter: 1514 loss: 5.65399773e-07
Iter: 1515 loss: 5.6509424e-07
Iter: 1516 loss: 5.66032611e-07
Iter: 1517 loss: 5.64923084e-07
Iter: 1518 loss: 5.64818038e-07
Iter: 1519 loss: 5.64663e-07
Iter: 1520 loss: 5.64553034e-07
Iter: 1521 loss: 5.64332709e-07
Iter: 1522 loss: 5.66797e-07
Iter: 1523 loss: 5.64302752e-07
Iter: 1524 loss: 5.64095899e-07
Iter: 1525 loss: 5.66807103e-07
Iter: 1526 loss: 5.6409408e-07
Iter: 1527 loss: 5.63845731e-07
Iter: 1528 loss: 5.63923436e-07
Iter: 1529 loss: 5.6371789e-07
Iter: 1530 loss: 5.63553101e-07
Iter: 1531 loss: 5.64682125e-07
Iter: 1532 loss: 5.63523145e-07
Iter: 1533 loss: 5.63365802e-07
Iter: 1534 loss: 5.63050435e-07
Iter: 1535 loss: 5.693052e-07
Iter: 1536 loss: 5.63053447e-07
Iter: 1537 loss: 5.62681748e-07
Iter: 1538 loss: 5.65896755e-07
Iter: 1539 loss: 5.62660489e-07
Iter: 1540 loss: 5.62435105e-07
Iter: 1541 loss: 5.62553396e-07
Iter: 1542 loss: 5.62292712e-07
Iter: 1543 loss: 5.61952902e-07
Iter: 1544 loss: 5.61860645e-07
Iter: 1545 loss: 5.61659078e-07
Iter: 1546 loss: 5.61330467e-07
Iter: 1547 loss: 5.6538704e-07
Iter: 1548 loss: 5.61337401e-07
Iter: 1549 loss: 5.61066884e-07
Iter: 1550 loss: 5.6252992e-07
Iter: 1551 loss: 5.61040565e-07
Iter: 1552 loss: 5.6083536e-07
Iter: 1553 loss: 5.60535568e-07
Iter: 1554 loss: 5.60555e-07
Iter: 1555 loss: 5.60495494e-07
Iter: 1556 loss: 5.60393346e-07
Iter: 1557 loss: 5.60279e-07
Iter: 1558 loss: 5.6001096e-07
Iter: 1559 loss: 5.60942567e-07
Iter: 1560 loss: 5.599e-07
Iter: 1561 loss: 5.5984475e-07
Iter: 1562 loss: 5.59739817e-07
Iter: 1563 loss: 5.59598e-07
Iter: 1564 loss: 5.59476e-07
Iter: 1565 loss: 5.59426e-07
Iter: 1566 loss: 5.59172236e-07
Iter: 1567 loss: 5.59744649e-07
Iter: 1568 loss: 5.59069576e-07
Iter: 1569 loss: 5.58667296e-07
Iter: 1570 loss: 5.59453952e-07
Iter: 1571 loss: 5.58480679e-07
Iter: 1572 loss: 5.58135753e-07
Iter: 1573 loss: 5.59005684e-07
Iter: 1574 loss: 5.58038437e-07
Iter: 1575 loss: 5.57741487e-07
Iter: 1576 loss: 5.57238195e-07
Iter: 1577 loss: 5.57244505e-07
Iter: 1578 loss: 5.56981888e-07
Iter: 1579 loss: 5.56950567e-07
Iter: 1580 loss: 5.56785267e-07
Iter: 1581 loss: 5.58467377e-07
Iter: 1582 loss: 5.56811472e-07
Iter: 1583 loss: 5.56671807e-07
Iter: 1584 loss: 5.56416069e-07
Iter: 1585 loss: 5.61227864e-07
Iter: 1586 loss: 5.56423572e-07
Iter: 1587 loss: 5.56139526e-07
Iter: 1588 loss: 5.57826e-07
Iter: 1589 loss: 5.56112241e-07
Iter: 1590 loss: 5.55760721e-07
Iter: 1591 loss: 5.56856e-07
Iter: 1592 loss: 5.55691656e-07
Iter: 1593 loss: 5.55479687e-07
Iter: 1594 loss: 5.55093948e-07
Iter: 1595 loss: 5.64289849e-07
Iter: 1596 loss: 5.55092697e-07
Iter: 1597 loss: 5.54823202e-07
Iter: 1598 loss: 5.54768576e-07
Iter: 1599 loss: 5.54640906e-07
Iter: 1600 loss: 5.54410121e-07
Iter: 1601 loss: 5.54404892e-07
Iter: 1602 loss: 5.54124199e-07
Iter: 1603 loss: 5.56274472e-07
Iter: 1604 loss: 5.54094925e-07
Iter: 1605 loss: 5.53819632e-07
Iter: 1606 loss: 5.53629832e-07
Iter: 1607 loss: 5.5352632e-07
Iter: 1608 loss: 5.5319947e-07
Iter: 1609 loss: 5.54355324e-07
Iter: 1610 loss: 5.53113068e-07
Iter: 1611 loss: 5.52787128e-07
Iter: 1612 loss: 5.52196184e-07
Iter: 1613 loss: 5.66005269e-07
Iter: 1614 loss: 5.52204256e-07
Iter: 1615 loss: 5.5196341e-07
Iter: 1616 loss: 5.51897642e-07
Iter: 1617 loss: 5.51654693e-07
Iter: 1618 loss: 5.54015855e-07
Iter: 1619 loss: 5.51627522e-07
Iter: 1620 loss: 5.51471317e-07
Iter: 1621 loss: 5.51102289e-07
Iter: 1622 loss: 5.56412829e-07
Iter: 1623 loss: 5.51101834e-07
Iter: 1624 loss: 5.50980189e-07
Iter: 1625 loss: 5.50896175e-07
Iter: 1626 loss: 5.50707512e-07
Iter: 1627 loss: 5.50161417e-07
Iter: 1628 loss: 5.52410256e-07
Iter: 1629 loss: 5.49940466e-07
Iter: 1630 loss: 5.49669267e-07
Iter: 1631 loss: 5.49602248e-07
Iter: 1632 loss: 5.49295123e-07
Iter: 1633 loss: 5.50304549e-07
Iter: 1634 loss: 5.49205652e-07
Iter: 1635 loss: 5.48986918e-07
Iter: 1636 loss: 5.48495791e-07
Iter: 1637 loss: 5.54415578e-07
Iter: 1638 loss: 5.48446678e-07
Iter: 1639 loss: 5.4818554e-07
Iter: 1640 loss: 5.48084e-07
Iter: 1641 loss: 5.47960155e-07
Iter: 1642 loss: 5.47850107e-07
Iter: 1643 loss: 5.47821401e-07
Iter: 1644 loss: 5.4762404e-07
Iter: 1645 loss: 5.47419688e-07
Iter: 1646 loss: 5.47377681e-07
Iter: 1647 loss: 5.47137233e-07
Iter: 1648 loss: 5.46795832e-07
Iter: 1649 loss: 5.4676309e-07
Iter: 1650 loss: 5.46435615e-07
Iter: 1651 loss: 5.46769343e-07
Iter: 1652 loss: 5.46232741e-07
Iter: 1653 loss: 5.45882472e-07
Iter: 1654 loss: 5.47217837e-07
Iter: 1655 loss: 5.45796752e-07
Iter: 1656 loss: 5.4543051e-07
Iter: 1657 loss: 5.46214892e-07
Iter: 1658 loss: 5.45280727e-07
Iter: 1659 loss: 5.44911757e-07
Iter: 1660 loss: 5.44924887e-07
Iter: 1661 loss: 5.44673355e-07
Iter: 1662 loss: 5.45286412e-07
Iter: 1663 loss: 5.44557167e-07
Iter: 1664 loss: 5.44311092e-07
Iter: 1665 loss: 5.43922454e-07
Iter: 1666 loss: 5.4390415e-07
Iter: 1667 loss: 5.43501642e-07
Iter: 1668 loss: 5.43497777e-07
Iter: 1669 loss: 5.43282738e-07
Iter: 1670 loss: 5.42937414e-07
Iter: 1671 loss: 5.4294685e-07
Iter: 1672 loss: 5.4257805e-07
Iter: 1673 loss: 5.43076567e-07
Iter: 1674 loss: 5.42396378e-07
Iter: 1675 loss: 5.42157238e-07
Iter: 1676 loss: 5.45444721e-07
Iter: 1677 loss: 5.42174121e-07
Iter: 1678 loss: 5.41979887e-07
Iter: 1679 loss: 5.43311558e-07
Iter: 1680 loss: 5.4193697e-07
Iter: 1681 loss: 5.41739155e-07
Iter: 1682 loss: 5.41269458e-07
Iter: 1683 loss: 5.45915043e-07
Iter: 1684 loss: 5.41211e-07
Iter: 1685 loss: 5.40974725e-07
Iter: 1686 loss: 5.40885537e-07
Iter: 1687 loss: 5.40684539e-07
Iter: 1688 loss: 5.4019074e-07
Iter: 1689 loss: 5.4445843e-07
Iter: 1690 loss: 5.40110705e-07
Iter: 1691 loss: 5.39415623e-07
Iter: 1692 loss: 5.41889563e-07
Iter: 1693 loss: 5.39246969e-07
Iter: 1694 loss: 5.38812628e-07
Iter: 1695 loss: 5.38793302e-07
Iter: 1696 loss: 5.38534209e-07
Iter: 1697 loss: 5.38347308e-07
Iter: 1698 loss: 5.38256e-07
Iter: 1699 loss: 5.37849587e-07
Iter: 1700 loss: 5.3973713e-07
Iter: 1701 loss: 5.37803373e-07
Iter: 1702 loss: 5.37397227e-07
Iter: 1703 loss: 5.3874e-07
Iter: 1704 loss: 5.37296728e-07
Iter: 1705 loss: 5.37058327e-07
Iter: 1706 loss: 5.36617e-07
Iter: 1707 loss: 5.45946307e-07
Iter: 1708 loss: 5.3661671e-07
Iter: 1709 loss: 5.36053449e-07
Iter: 1710 loss: 5.37824292e-07
Iter: 1711 loss: 5.35891729e-07
Iter: 1712 loss: 5.35739389e-07
Iter: 1713 loss: 5.35679362e-07
Iter: 1714 loss: 5.35444e-07
Iter: 1715 loss: 5.35120478e-07
Iter: 1716 loss: 5.35107233e-07
Iter: 1717 loss: 5.34703304e-07
Iter: 1718 loss: 5.35204833e-07
Iter: 1719 loss: 5.3447252e-07
Iter: 1720 loss: 5.33992932e-07
Iter: 1721 loss: 5.39246059e-07
Iter: 1722 loss: 5.33982188e-07
Iter: 1723 loss: 5.3366557e-07
Iter: 1724 loss: 5.32996637e-07
Iter: 1725 loss: 5.42888131e-07
Iter: 1726 loss: 5.3298163e-07
Iter: 1727 loss: 5.32685817e-07
Iter: 1728 loss: 5.32623062e-07
Iter: 1729 loss: 5.32272338e-07
Iter: 1730 loss: 5.32571221e-07
Iter: 1731 loss: 5.32056e-07
Iter: 1732 loss: 5.31707656e-07
Iter: 1733 loss: 5.31974592e-07
Iter: 1734 loss: 5.31474825e-07
Iter: 1735 loss: 5.31050034e-07
Iter: 1736 loss: 5.35625418e-07
Iter: 1737 loss: 5.31005e-07
Iter: 1738 loss: 5.30753482e-07
Iter: 1739 loss: 5.30815669e-07
Iter: 1740 loss: 5.30555099e-07
Iter: 1741 loss: 5.30297825e-07
Iter: 1742 loss: 5.29684655e-07
Iter: 1743 loss: 5.3823112e-07
Iter: 1744 loss: 5.29663907e-07
Iter: 1745 loss: 5.29060912e-07
Iter: 1746 loss: 5.29080467e-07
Iter: 1747 loss: 5.28820124e-07
Iter: 1748 loss: 5.28813075e-07
Iter: 1749 loss: 5.28630721e-07
Iter: 1750 loss: 5.28148234e-07
Iter: 1751 loss: 5.30741e-07
Iter: 1752 loss: 5.27984866e-07
Iter: 1753 loss: 5.27529892e-07
Iter: 1754 loss: 5.27517159e-07
Iter: 1755 loss: 5.27162229e-07
Iter: 1756 loss: 5.26913936e-07
Iter: 1757 loss: 5.26794736e-07
Iter: 1758 loss: 5.26298322e-07
Iter: 1759 loss: 5.26104543e-07
Iter: 1760 loss: 5.25850623e-07
Iter: 1761 loss: 5.25216478e-07
Iter: 1762 loss: 5.25230689e-07
Iter: 1763 loss: 5.2495011e-07
Iter: 1764 loss: 5.24681127e-07
Iter: 1765 loss: 5.24602456e-07
Iter: 1766 loss: 5.242116e-07
Iter: 1767 loss: 5.28338489e-07
Iter: 1768 loss: 5.24205291e-07
Iter: 1769 loss: 5.23833e-07
Iter: 1770 loss: 5.24597453e-07
Iter: 1771 loss: 5.23680114e-07
Iter: 1772 loss: 5.23424035e-07
Iter: 1773 loss: 5.23745484e-07
Iter: 1774 loss: 5.23306085e-07
Iter: 1775 loss: 5.23043923e-07
Iter: 1776 loss: 5.22792561e-07
Iter: 1777 loss: 5.22734126e-07
Iter: 1778 loss: 5.22501182e-07
Iter: 1779 loss: 5.22447692e-07
Iter: 1780 loss: 5.2219707e-07
Iter: 1781 loss: 5.2184356e-07
Iter: 1782 loss: 5.21825427e-07
Iter: 1783 loss: 5.21435084e-07
Iter: 1784 loss: 5.22096e-07
Iter: 1785 loss: 5.21274444e-07
Iter: 1786 loss: 5.20644846e-07
Iter: 1787 loss: 5.23631172e-07
Iter: 1788 loss: 5.20551794e-07
Iter: 1789 loss: 5.20180834e-07
Iter: 1790 loss: 5.19839887e-07
Iter: 1791 loss: 5.1975212e-07
Iter: 1792 loss: 5.19428e-07
Iter: 1793 loss: 5.19396963e-07
Iter: 1794 loss: 5.19085518e-07
Iter: 1795 loss: 5.19044193e-07
Iter: 1796 loss: 5.18825232e-07
Iter: 1797 loss: 5.18642e-07
Iter: 1798 loss: 5.1928248e-07
Iter: 1799 loss: 5.18556476e-07
Iter: 1800 loss: 5.18315574e-07
Iter: 1801 loss: 5.19006676e-07
Iter: 1802 loss: 5.182211e-07
Iter: 1803 loss: 5.17963599e-07
Iter: 1804 loss: 5.17766409e-07
Iter: 1805 loss: 5.17664944e-07
Iter: 1806 loss: 5.17374929e-07
Iter: 1807 loss: 5.18184493e-07
Iter: 1808 loss: 5.17258172e-07
Iter: 1809 loss: 5.1691552e-07
Iter: 1810 loss: 5.17824049e-07
Iter: 1811 loss: 5.16802686e-07
Iter: 1812 loss: 5.16288878e-07
Iter: 1813 loss: 5.18222e-07
Iter: 1814 loss: 5.1614046e-07
Iter: 1815 loss: 5.15862041e-07
Iter: 1816 loss: 5.15801389e-07
Iter: 1817 loss: 5.15615284e-07
Iter: 1818 loss: 5.15353577e-07
Iter: 1819 loss: 5.15355509e-07
Iter: 1820 loss: 5.15134275e-07
Iter: 1821 loss: 5.14754447e-07
Iter: 1822 loss: 5.14735e-07
Iter: 1823 loss: 5.14507406e-07
Iter: 1824 loss: 5.14519115e-07
Iter: 1825 loss: 5.14278497e-07
Iter: 1826 loss: 5.1446932e-07
Iter: 1827 loss: 5.1413042e-07
Iter: 1828 loss: 5.13859845e-07
Iter: 1829 loss: 5.13572786e-07
Iter: 1830 loss: 5.13533678e-07
Iter: 1831 loss: 5.13547e-07
Iter: 1832 loss: 5.13366729e-07
Iter: 1833 loss: 5.13251621e-07
Iter: 1834 loss: 5.12969393e-07
Iter: 1835 loss: 5.14874216e-07
Iter: 1836 loss: 5.12886118e-07
Iter: 1837 loss: 5.12349629e-07
Iter: 1838 loss: 5.12912436e-07
Iter: 1839 loss: 5.12084966e-07
Iter: 1840 loss: 5.11625103e-07
Iter: 1841 loss: 5.17707065e-07
Iter: 1842 loss: 5.11629537e-07
Iter: 1843 loss: 5.11316784e-07
Iter: 1844 loss: 5.14421117e-07
Iter: 1845 loss: 5.11296435e-07
Iter: 1846 loss: 5.11035523e-07
Iter: 1847 loss: 5.10517339e-07
Iter: 1848 loss: 5.21553829e-07
Iter: 1849 loss: 5.10525695e-07
Iter: 1850 loss: 5.10345e-07
Iter: 1851 loss: 5.10309349e-07
Iter: 1852 loss: 5.10121765e-07
Iter: 1853 loss: 5.10126654e-07
Iter: 1854 loss: 5.0997329e-07
Iter: 1855 loss: 5.09753818e-07
Iter: 1856 loss: 5.09445044e-07
Iter: 1857 loss: 5.09413667e-07
Iter: 1858 loss: 5.09318738e-07
Iter: 1859 loss: 5.09180666e-07
Iter: 1860 loss: 5.09072265e-07
Iter: 1861 loss: 5.08687549e-07
Iter: 1862 loss: 5.11210544e-07
Iter: 1863 loss: 5.0860524e-07
Iter: 1864 loss: 5.08288849e-07
Iter: 1865 loss: 5.08275264e-07
Iter: 1866 loss: 5.08013613e-07
Iter: 1867 loss: 5.07913e-07
Iter: 1868 loss: 5.07773791e-07
Iter: 1869 loss: 5.07508901e-07
Iter: 1870 loss: 5.07336381e-07
Iter: 1871 loss: 5.07223206e-07
Iter: 1872 loss: 5.06755043e-07
Iter: 1873 loss: 5.06998049e-07
Iter: 1874 loss: 5.064569e-07
Iter: 1875 loss: 5.06628851e-07
Iter: 1876 loss: 5.06191668e-07
Iter: 1877 loss: 5.06047229e-07
Iter: 1878 loss: 5.05741696e-07
Iter: 1879 loss: 5.10504037e-07
Iter: 1880 loss: 5.05744879e-07
Iter: 1881 loss: 5.05377102e-07
Iter: 1882 loss: 5.056379e-07
Iter: 1883 loss: 5.05161097e-07
Iter: 1884 loss: 5.05016658e-07
Iter: 1885 loss: 5.04894388e-07
Iter: 1886 loss: 5.04743184e-07
Iter: 1887 loss: 5.04373e-07
Iter: 1888 loss: 5.08867e-07
Iter: 1889 loss: 5.04339937e-07
Iter: 1890 loss: 5.04098807e-07
Iter: 1891 loss: 5.04111767e-07
Iter: 1892 loss: 5.03882916e-07
Iter: 1893 loss: 5.04829529e-07
Iter: 1894 loss: 5.03825333e-07
Iter: 1895 loss: 5.03672823e-07
Iter: 1896 loss: 5.03534238e-07
Iter: 1897 loss: 5.03502235e-07
Iter: 1898 loss: 5.03120475e-07
Iter: 1899 loss: 5.03864783e-07
Iter: 1900 loss: 5.0293886e-07
Iter: 1901 loss: 5.02716603e-07
Iter: 1902 loss: 5.0294426e-07
Iter: 1903 loss: 5.02583248e-07
Iter: 1904 loss: 5.02199555e-07
Iter: 1905 loss: 5.01735087e-07
Iter: 1906 loss: 5.01721104e-07
Iter: 1907 loss: 5.01291e-07
Iter: 1908 loss: 5.01277384e-07
Iter: 1909 loss: 5.01058139e-07
Iter: 1910 loss: 5.01052455e-07
Iter: 1911 loss: 5.00912279e-07
Iter: 1912 loss: 5.00534952e-07
Iter: 1913 loss: 5.02853425e-07
Iter: 1914 loss: 5.00461624e-07
Iter: 1915 loss: 5.00217652e-07
Iter: 1916 loss: 5.00215e-07
Iter: 1917 loss: 5.0000591e-07
Iter: 1918 loss: 5.01048817e-07
Iter: 1919 loss: 4.99955092e-07
Iter: 1920 loss: 4.9975057e-07
Iter: 1921 loss: 4.99334931e-07
Iter: 1922 loss: 5.06661479e-07
Iter: 1923 loss: 4.9935403e-07
Iter: 1924 loss: 4.99358293e-07
Iter: 1925 loss: 4.99187649e-07
Iter: 1926 loss: 4.9904213e-07
Iter: 1927 loss: 4.98734721e-07
Iter: 1928 loss: 5.03604667e-07
Iter: 1929 loss: 4.98720055e-07
Iter: 1930 loss: 4.98344093e-07
Iter: 1931 loss: 4.99681846e-07
Iter: 1932 loss: 4.98254508e-07
Iter: 1933 loss: 4.97830456e-07
Iter: 1934 loss: 5.00392332e-07
Iter: 1935 loss: 4.97763779e-07
Iter: 1936 loss: 4.97576536e-07
Iter: 1937 loss: 4.97236044e-07
Iter: 1938 loss: 5.04035768e-07
Iter: 1939 loss: 4.97209612e-07
Iter: 1940 loss: 4.96800794e-07
Iter: 1941 loss: 4.99642169e-07
Iter: 1942 loss: 4.96764869e-07
Iter: 1943 loss: 4.96506061e-07
Iter: 1944 loss: 4.96763278e-07
Iter: 1945 loss: 4.96371115e-07
Iter: 1946 loss: 4.96059101e-07
Iter: 1947 loss: 5.00496355e-07
Iter: 1948 loss: 4.96051598e-07
Iter: 1949 loss: 4.95935296e-07
Iter: 1950 loss: 4.95778352e-07
Iter: 1951 loss: 4.9578864e-07
Iter: 1952 loss: 4.95634652e-07
Iter: 1953 loss: 4.9651544e-07
Iter: 1954 loss: 4.95602649e-07
Iter: 1955 loss: 4.95408869e-07
Iter: 1956 loss: 4.95066615e-07
Iter: 1957 loss: 4.95086397e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4
+ date
Wed Oct 21 21:12:33 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/500_500_500_500_1 --function f1 --psi -2 --phi 0.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d88140bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d88085e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d88140510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d88140730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d8016e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d8016e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d8020b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d800db9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d800db950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d8004bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d8007c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d801199d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d801197b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d70682510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d7069f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d7069f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d80026488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d70635ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d70601730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d705f7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d7057e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d7057e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d8008b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d8008e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d8008e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d80097ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d704999d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d70481620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d70481268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d7045c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d7045c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d7041b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d7041b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d7041b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d704cf268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2d70353158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.043631837
test_loss: 0.041984778
train_loss: 0.017171837
test_loss: 0.016842147
train_loss: 0.009840486
test_loss: 0.010154456
train_loss: 0.0074179685
test_loss: 0.007818641
train_loss: 0.006134225
test_loss: 0.0062366948
train_loss: 0.00553558
test_loss: 0.005515377
train_loss: 0.0050922027
test_loss: 0.005052847
train_loss: 0.0043370514
test_loss: 0.0043540737
train_loss: 0.00377644
test_loss: 0.0042744903
train_loss: 0.004157484
test_loss: 0.0041971295
train_loss: 0.0036510963
test_loss: 0.0040810076
train_loss: 0.0037938885
test_loss: 0.003926629
train_loss: 0.003673312
test_loss: 0.003993952
train_loss: 0.0033211866
test_loss: 0.0036974077
train_loss: 0.003318252
test_loss: 0.0037077002
train_loss: 0.003631812
test_loss: 0.0037802467
train_loss: 0.0032965909
test_loss: 0.0040153735
train_loss: 0.0031829393
test_loss: 0.003595819
train_loss: 0.0030060539
test_loss: 0.0035007263
train_loss: 0.0031435217
test_loss: 0.0036098326
train_loss: 0.0029562884
test_loss: 0.0032927848
train_loss: 0.0031357617
test_loss: 0.0033540865
train_loss: 0.0028653769
test_loss: 0.0032816394
train_loss: 0.0028095087
test_loss: 0.0033435666
train_loss: 0.0030358045
test_loss: 0.00339813
train_loss: 0.0026315309
test_loss: 0.003340562
train_loss: 0.0029237282
test_loss: 0.0032975478
train_loss: 0.0031841905
test_loss: 0.0034665498
train_loss: 0.0029980976
test_loss: 0.0035088772
train_loss: 0.0027986954
test_loss: 0.003367339
train_loss: 0.0029514951
test_loss: 0.0033127412
train_loss: 0.0026679602
test_loss: 0.0032451518
train_loss: 0.0029801545
test_loss: 0.0032540304
train_loss: 0.0027000336
test_loss: 0.0031573116
train_loss: 0.0027905372
test_loss: 0.0034094192
train_loss: 0.0027325554
test_loss: 0.003196533
train_loss: 0.0028487893
test_loss: 0.0033141363
train_loss: 0.0028230057
test_loss: 0.0032644998
train_loss: 0.002827137
test_loss: 0.0031832987
train_loss: 0.0026574163
test_loss: 0.0031236417
train_loss: 0.002638615
test_loss: 0.0030569378
train_loss: 0.0025729774
test_loss: 0.003184613
train_loss: 0.002543782
test_loss: 0.0030769957
train_loss: 0.002626385
test_loss: 0.0030980764
train_loss: 0.0025651555
test_loss: 0.0029680931
train_loss: 0.0027856794
test_loss: 0.0031826487
train_loss: 0.002712798
test_loss: 0.0032637639
train_loss: 0.0026746313
test_loss: 0.003157843
train_loss: 0.0025526076
test_loss: 0.0031300962
train_loss: 0.0024995639
test_loss: 0.0029632926
train_loss: 0.0025482539
test_loss: 0.0031083156
train_loss: 0.0023918163
test_loss: 0.0031610224
train_loss: 0.0026415272
test_loss: 0.0032503773
train_loss: 0.0027225092
test_loss: 0.0031415098
train_loss: 0.0024654213
test_loss: 0.0032734424
train_loss: 0.0025811403
test_loss: 0.0030001004
train_loss: 0.0023510791
test_loss: 0.0029906107
train_loss: 0.002815607
test_loss: 0.0031238561
train_loss: 0.0027143876
test_loss: 0.0032989958
train_loss: 0.0025635127
test_loss: 0.0030061288
train_loss: 0.0026338242
test_loss: 0.0031830708
train_loss: 0.0023572682
test_loss: 0.0028953447
train_loss: 0.0025500343
test_loss: 0.0031441937
train_loss: 0.0023554503
test_loss: 0.0030012038
train_loss: 0.0025058463
test_loss: 0.0029158655
train_loss: 0.0024504624
test_loss: 0.002955638
train_loss: 0.0025743623
test_loss: 0.003232706
train_loss: 0.0025506718
test_loss: 0.0033670352
train_loss: 0.0025927294
test_loss: 0.003120298
train_loss: 0.0025513903
test_loss: 0.0030703098
train_loss: 0.0025798
test_loss: 0.003050381
train_loss: 0.0025204082
test_loss: 0.00302837
train_loss: 0.0024987052
test_loss: 0.0029738583
train_loss: 0.0025088121
test_loss: 0.0030009942
train_loss: 0.0027320897
test_loss: 0.0031321947
train_loss: 0.0023740581
test_loss: 0.0029456406
train_loss: 0.0024580075
test_loss: 0.0031790233
train_loss: 0.002438624
test_loss: 0.0030351065
train_loss: 0.0021864194
test_loss: 0.0029003946
train_loss: 0.0026407964
test_loss: 0.003013987
train_loss: 0.0024107276
test_loss: 0.0030558312
train_loss: 0.0024748845
test_loss: 0.0029196006
train_loss: 0.002434857
test_loss: 0.0030128967
train_loss: 0.0024124014
test_loss: 0.0028515267
train_loss: 0.002295761
test_loss: 0.0030366685
train_loss: 0.00250169
test_loss: 0.0030403007
train_loss: 0.002410711
test_loss: 0.0029800364
train_loss: 0.002245478
test_loss: 0.0028206133
train_loss: 0.0024040919
test_loss: 0.0028561414
train_loss: 0.002345753
test_loss: 0.002936816
train_loss: 0.002744503
test_loss: 0.0031001437
train_loss: 0.002599648
test_loss: 0.002961905
train_loss: 0.0023764218
test_loss: 0.003000796
train_loss: 0.0025460056
test_loss: 0.0028678102
train_loss: 0.0028371632
test_loss: 0.003033717
train_loss: 0.002295189
test_loss: 0.0028671676
train_loss: 0.0023164414
test_loss: 0.002878948
train_loss: 0.0024059606
test_loss: 0.0030131121
train_loss: 0.0023785785
test_loss: 0.0029501873
train_loss: 0.002632992
test_loss: 0.003037631
train_loss: 0.002330002
test_loss: 0.002781435
train_loss: 0.0025085458
test_loss: 0.0030529024
train_loss: 0.0023088076
test_loss: 0.0029539487
train_loss: 0.0024448726
test_loss: 0.0027333843
train_loss: 0.0024090009
test_loss: 0.0031307852
train_loss: 0.0026859245
test_loss: 0.0028874995
train_loss: 0.0024175362
test_loss: 0.0027496452
train_loss: 0.0023854224
test_loss: 0.0028558332
train_loss: 0.0023661705
test_loss: 0.0030289788
train_loss: 0.0025414515
test_loss: 0.0030216645
train_loss: 0.0024279617
test_loss: 0.0028052414
train_loss: 0.002270292
test_loss: 0.0029988515
train_loss: 0.0025754115
test_loss: 0.0030168097
train_loss: 0.002368965
test_loss: 0.0029253308
train_loss: 0.002681148
test_loss: 0.0029167817
train_loss: 0.0022397446
test_loss: 0.0028390938
train_loss: 0.0022571902
test_loss: 0.0027560652
train_loss: 0.0022017553
test_loss: 0.0028494834
train_loss: 0.0022888323
test_loss: 0.0028501025
train_loss: 0.0022774602
test_loss: 0.0027101603
train_loss: 0.0022086427
test_loss: 0.002638658
train_loss: 0.0022949432
test_loss: 0.002835129
train_loss: 0.002455457
test_loss: 0.0029867669
train_loss: 0.0022996557
test_loss: 0.002871251
train_loss: 0.0024146845
test_loss: 0.0029875047
train_loss: 0.0024171362
test_loss: 0.0028665434
train_loss: 0.0024398663
test_loss: 0.0028694943
train_loss: 0.0025413195
test_loss: 0.002944345
train_loss: 0.0023093603
test_loss: 0.0026865331
train_loss: 0.0021332095
test_loss: 0.0027727843
train_loss: 0.002462998
test_loss: 0.0028566138
train_loss: 0.002133849
test_loss: 0.0026824437
train_loss: 0.0023813928
test_loss: 0.0031624637
train_loss: 0.0022571234
test_loss: 0.0028814191
train_loss: 0.002337318
test_loss: 0.0027661514
train_loss: 0.002264498
test_loss: 0.0027997128
train_loss: 0.0021542301
test_loss: 0.0027569493
train_loss: 0.002123575
test_loss: 0.002742061
train_loss: 0.0023034916
test_loss: 0.0027864056
train_loss: 0.0021301042
test_loss: 0.002843028
train_loss: 0.0021618698
test_loss: 0.0029833717
train_loss: 0.0021717648
test_loss: 0.002652792
train_loss: 0.0021810513
test_loss: 0.00265332
train_loss: 0.0022593425
test_loss: 0.0029682005
train_loss: 0.0023479925
test_loss: 0.002732965
train_loss: 0.0021839477
test_loss: 0.0027944134
train_loss: 0.002445955
test_loss: 0.0029221184
train_loss: 0.002356019
test_loss: 0.0026647395
train_loss: 0.0021507656
test_loss: 0.002696344
train_loss: 0.0024569656
test_loss: 0.002807226
train_loss: 0.0022111763
test_loss: 0.002716373
train_loss: 0.0021444634
test_loss: 0.0026947432
train_loss: 0.0021684943
test_loss: 0.0027227364
train_loss: 0.0023739678
test_loss: 0.0029065341
train_loss: 0.002478845
test_loss: 0.0028033939
train_loss: 0.0023772276
test_loss: 0.002721427
train_loss: 0.0022634328
test_loss: 0.002798349
train_loss: 0.0024891668
test_loss: 0.0029452734
train_loss: 0.0023157457
test_loss: 0.0027865877
train_loss: 0.002166543
test_loss: 0.0028509109
train_loss: 0.0023459503
test_loss: 0.0028416927
train_loss: 0.002329988
test_loss: 0.002785422
train_loss: 0.0023379624
test_loss: 0.0028259242
train_loss: 0.0021750128
test_loss: 0.0029073902
train_loss: 0.0024762705
test_loss: 0.0029146606
train_loss: 0.0020286294
test_loss: 0.0027527602
train_loss: 0.0021861836
test_loss: 0.0026810665
train_loss: 0.0021487745
test_loss: 0.00267729
train_loss: 0.0023659533
test_loss: 0.0028027464
train_loss: 0.0020116519
test_loss: 0.0026212726
train_loss: 0.002366732
test_loss: 0.0027664874
train_loss: 0.0022213173
test_loss: 0.002906718
train_loss: 0.0022979537
test_loss: 0.0029428548
train_loss: 0.0026963989
test_loss: 0.0029627027
train_loss: 0.0023586624
test_loss: 0.0029075115
train_loss: 0.0021239016
test_loss: 0.0025880197
train_loss: 0.002063363
test_loss: 0.002646251
train_loss: 0.0021792238
test_loss: 0.00266715
train_loss: 0.0023248522
test_loss: 0.0026617742
train_loss: 0.0022423903
test_loss: 0.0030743312
train_loss: 0.0022742557
test_loss: 0.0028082177
train_loss: 0.0021656728
test_loss: 0.0028230143
train_loss: 0.002237297
test_loss: 0.002704666
train_loss: 0.0021578204
test_loss: 0.002615339
train_loss: 0.002480388
test_loss: 0.0027614278
train_loss: 0.0024097678
test_loss: 0.0028033333
train_loss: 0.00222612
test_loss: 0.0026105721
train_loss: 0.00214187
test_loss: 0.0027215069
train_loss: 0.0021857866
test_loss: 0.0026963297
train_loss: 0.0021981904
test_loss: 0.0029056869
train_loss: 0.002398399
test_loss: 0.0028633997
train_loss: 0.0020903044
test_loss: 0.0027359233
train_loss: 0.002119341
test_loss: 0.0026878996
train_loss: 0.0023398781
test_loss: 0.0027946455
train_loss: 0.0021853978
test_loss: 0.0026645532
train_loss: 0.0022786101
test_loss: 0.0028113332
train_loss: 0.002272182
test_loss: 0.0026324415
train_loss: 0.0021227966
test_loss: 0.0026352396
train_loss: 0.0021651213
test_loss: 0.003020749
train_loss: 0.0025697742
test_loss: 0.003193592
train_loss: 0.0021180133
test_loss: 0.0030934263
train_loss: 0.0021431374
test_loss: 0.0026394895
train_loss: 0.0023042494
test_loss: 0.0028852485
train_loss: 0.0020877183
test_loss: 0.002566269
train_loss: 0.0022004708
test_loss: 0.0026781785
train_loss: 0.0021571894
test_loss: 0.002535172
train_loss: 0.00222032
test_loss: 0.002745323
train_loss: 0.0020577721
test_loss: 0.00265331
train_loss: 0.0019771587
test_loss: 0.002713878
train_loss: 0.0023601141
test_loss: 0.0027276592
train_loss: 0.0023087298
test_loss: 0.002770448
train_loss: 0.002083798
test_loss: 0.0026650145
train_loss: 0.0022016815
test_loss: 0.002742671
train_loss: 0.0021079963
test_loss: 0.0026630021
train_loss: 0.002041811
test_loss: 0.0026514495
train_loss: 0.0020743038
test_loss: 0.0028143316
train_loss: 0.002041108
test_loss: 0.0027302585
train_loss: 0.0020848047
test_loss: 0.0026063882
train_loss: 0.0021445614
test_loss: 0.002569034
train_loss: 0.002210381
test_loss: 0.00275264
train_loss: 0.0022805405
test_loss: 0.0026722902
train_loss: 0.0023110213
test_loss: 0.00274389
train_loss: 0.0021285117
test_loss: 0.0026269043
train_loss: 0.0022154804
test_loss: 0.002809753
train_loss: 0.0024045845
test_loss: 0.0027155485
train_loss: 0.0021506022
test_loss: 0.002653391
train_loss: 0.0021896851
test_loss: 0.0026192688
train_loss: 0.002357253
test_loss: 0.0027884473
train_loss: 0.002279245
test_loss: 0.0029048538
train_loss: 0.002227855
test_loss: 0.0029261925
train_loss: 0.0028110999
test_loss: 0.0027022888
train_loss: 0.0021321464
test_loss: 0.0025871058
train_loss: 0.0020853763
test_loss: 0.002664697
train_loss: 0.0023188735
test_loss: 0.0026611327
train_loss: 0.002123666
test_loss: 0.0026987735
train_loss: 0.0021668137
test_loss: 0.0025914158
train_loss: 0.0019997193
test_loss: 0.0025874272
train_loss: 0.0024083843
test_loss: 0.0028582509
train_loss: 0.0020692747
test_loss: 0.0026927346
train_loss: 0.0020951612
test_loss: 0.0026935607
train_loss: 0.002093006
test_loss: 0.002617778
train_loss: 0.0022389928
test_loss: 0.0027334604
train_loss: 0.0020408896
test_loss: 0.0025134243
train_loss: 0.002112952
test_loss: 0.0028871307
train_loss: 0.0022334347
test_loss: 0.0025715956
train_loss: 0.0021641962
test_loss: 0.0027288161
train_loss: 0.0020569284
test_loss: 0.0025782746
train_loss: 0.0019076865
test_loss: 0.002890307
train_loss: 0.002010394
test_loss: 0.0026329702
train_loss: 0.002232944
test_loss: 0.002614837
train_loss: 0.0021668181
test_loss: 0.002592532
train_loss: 0.002530088
test_loss: 0.002811155
train_loss: 0.0020598941
test_loss: 0.002662355
train_loss: 0.0021193193
test_loss: 0.0028200212
train_loss: 0.0021526488
test_loss: 0.0026504674
train_loss: 0.002433495
test_loss: 0.0026497194
train_loss: 0.0022172518
test_loss: 0.0027747941
train_loss: 0.0021802448
test_loss: 0.0026347237
train_loss: 0.0022733468
test_loss: 0.0026030764
train_loss: 0.0021666135
test_loss: 0.0026049556
train_loss: 0.002118021
test_loss: 0.0027292452
train_loss: 0.0020329456
test_loss: 0.00258196
train_loss: 0.002122762
test_loss: 0.0025944742
train_loss: 0.0021005946
test_loss: 0.002707515
train_loss: 0.0019967263
test_loss: 0.0026751407
train_loss: 0.0020774808
test_loss: 0.0025504048
train_loss: 0.0020351312
test_loss: 0.0028482236
train_loss: 0.0021023096
test_loss: 0.0025979653
train_loss: 0.002014657
test_loss: 0.0029137298
train_loss: 0.0022854696
test_loss: 0.0026453452
train_loss: 0.0021490408
test_loss: 0.0027113124
train_loss: 0.0019839406
test_loss: 0.002510632
train_loss: 0.0020041172
test_loss: 0.002445043
train_loss: 0.002130528
test_loss: 0.002498189
train_loss: 0.001980131
test_loss: 0.002681691
train_loss: 0.002194847
test_loss: 0.0029240253
train_loss: 0.002139262
test_loss: 0.0029672373
train_loss: 0.0021730624
test_loss: 0.0025135912
train_loss: 0.0020562485
test_loss: 0.0028266562
train_loss: 0.0020215577
test_loss: 0.002451135
train_loss: 0.0022112904
test_loss: 0.0027840433
train_loss: 0.0021851193
test_loss: 0.0025524597
train_loss: 0.002082147
test_loss: 0.002674556
train_loss: 0.0019870154
test_loss: 0.0026689405
train_loss: 0.0023366231
test_loss: 0.0029366808
train_loss: 0.002287582
test_loss: 0.0027155862
train_loss: 0.00224767
test_loss: 0.0024926825
train_loss: 0.0020496203
test_loss: 0.002741413
train_loss: 0.002114914
test_loss: 0.0026306908
train_loss: 0.002051415
test_loss: 0.0025599329
train_loss: 0.00217574
test_loss: 0.0026805836
train_loss: 0.002147117
test_loss: 0.0025925396
train_loss: 0.0020753155
test_loss: 0.0026068925
train_loss: 0.001934595
test_loss: 0.0024793362
train_loss: 0.002579846
test_loss: 0.0027386553
train_loss: 0.0020530564
test_loss: 0.0027538277
train_loss: 0.0020838045
test_loss: 0.0025699327
train_loss: 0.0020511148
test_loss: 0.0026559476
train_loss: 0.0019498518
test_loss: 0.0025863694
train_loss: 0.0021773912
test_loss: 0.0026737596
train_loss: 0.0020473632
test_loss: 0.002501268
train_loss: 0.0020908928
test_loss: 0.0026350247
train_loss: 0.0021910318
test_loss: 0.002631023
train_loss: 0.0018350171
test_loss: 0.002607866
train_loss: 0.00222982
test_loss: 0.0026520614
train_loss: 0.0020890827
test_loss: 0.0025604728
train_loss: 0.002026586
test_loss: 0.002566459
train_loss: 0.0020984823
test_loss: 0.0025171496
train_loss: 0.0019379137
test_loss: 0.0024684537
train_loss: 0.002249383
test_loss: 0.0025572511
train_loss: 0.0019975863
test_loss: 0.00275821
train_loss: 0.0020612406
test_loss: 0.0027186165
train_loss: 0.0021086282
test_loss: 0.0025912472
train_loss: 0.0018765337
test_loss: 0.0024618425
train_loss: 0.0020200508
test_loss: 0.0026670003
train_loss: 0.0019218699
test_loss: 0.002499421
train_loss: 0.0021717383
test_loss: 0.0024654267
train_loss: 0.0022145675
test_loss: 0.0026559753
train_loss: 0.0024211525
test_loss: 0.0025746066
train_loss: 0.0020882413
test_loss: 0.0026106948
train_loss: 0.002090249
test_loss: 0.0024942383
train_loss: 0.0019359121
test_loss: 0.002543015
train_loss: 0.0023286806
test_loss: 0.0024653745
train_loss: 0.002059693
test_loss: 0.0028465756
train_loss: 0.0022543243
test_loss: 0.0026487815
train_loss: 0.0021713034
test_loss: 0.0026813732
train_loss: 0.0022513485
test_loss: 0.0025833547
train_loss: 0.0019954483
test_loss: 0.0025700456
train_loss: 0.0019095896
test_loss: 0.002656391
train_loss: 0.0021234946
test_loss: 0.00265744
train_loss: 0.002131556
test_loss: 0.0024664449
train_loss: 0.0022072946
test_loss: 0.0025084594
train_loss: 0.0020383166
test_loss: 0.002615415
train_loss: 0.0020611994
test_loss: 0.0024502687
train_loss: 0.0020481262
test_loss: 0.002566256
train_loss: 0.001897013
test_loss: 0.0025199791
train_loss: 0.0019503882
test_loss: 0.0024137653
train_loss: 0.0022299117
test_loss: 0.0025522741
train_loss: 0.0022028203
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0025566935
train_loss: 0.0019609947
test_loss: 0.0024678449
train_loss: 0.0019829997
test_loss: 0.0024965638
train_loss: 0.0020216391
test_loss: 0.0025932537
train_loss: 0.0019221687
test_loss: 0.002421783
train_loss: 0.0020735455
test_loss: 0.002471317
train_loss: 0.0019636415
test_loss: 0.0025584179
train_loss: 0.0019227418
test_loss: 0.0024710523
train_loss: 0.002044385
test_loss: 0.0026124734
train_loss: 0.0019583507
test_loss: 0.0025498306
train_loss: 0.0021360188
test_loss: 0.0024169153
train_loss: 0.002036939
test_loss: 0.0026069973
train_loss: 0.0020230631
test_loss: 0.002581944
train_loss: 0.0020377121
test_loss: 0.0023793138
train_loss: 0.0020344593
test_loss: 0.0025274379
train_loss: 0.0021352235
test_loss: 0.002774974
train_loss: 0.001962028
test_loss: 0.002499779
train_loss: 0.002021094
test_loss: 0.0027089852
train_loss: 0.0022760304
test_loss: 0.0024991638
train_loss: 0.0021181027
test_loss: 0.0026675079
train_loss: 0.0018905073
test_loss: 0.0025045325
train_loss: 0.0020894494
test_loss: 0.0026880887
train_loss: 0.0019466208
test_loss: 0.002456665
train_loss: 0.0020417792
test_loss: 0.0025909564
train_loss: 0.0021594523
test_loss: 0.0024741148
train_loss: 0.0019422972
test_loss: 0.0024696602
train_loss: 0.001875947
test_loss: 0.002400801
train_loss: 0.0019264796
test_loss: 0.0024167162
train_loss: 0.0023309747
test_loss: 0.0026691828
train_loss: 0.0019634622
test_loss: 0.0025231466
train_loss: 0.002013464
test_loss: 0.0024295035
train_loss: 0.0020566254
test_loss: 0.0025472888
train_loss: 0.0019805664
test_loss: 0.002527151
train_loss: 0.0021250045
test_loss: 0.002660078
train_loss: 0.0021063376
test_loss: 0.002434936
train_loss: 0.002018813
test_loss: 0.0025073332
train_loss: 0.0021162413
test_loss: 0.0026449007
train_loss: 0.0020509753
test_loss: 0.0025139612
train_loss: 0.002290805
test_loss: 0.0025574695
train_loss: 0.0019629993
test_loss: 0.002599094
train_loss: 0.0021805998
test_loss: 0.0026463803
train_loss: 0.0022325867
test_loss: 0.0024896828
train_loss: 0.0021181337
test_loss: 0.002425504
train_loss: 0.0021317818
test_loss: 0.0025238558
train_loss: 0.0021327024
test_loss: 0.0026363884
train_loss: 0.0021824008
test_loss: 0.0026666194
train_loss: 0.0020594816
test_loss: 0.0025059935
train_loss: 0.0021766443
test_loss: 0.00297779
train_loss: 0.0022968557
test_loss: 0.0026466532
train_loss: 0.001912721
test_loss: 0.0023744483
train_loss: 0.0018780509
test_loss: 0.002402705
train_loss: 0.0019707824
test_loss: 0.0026073547
train_loss: 0.002192465
test_loss: 0.002575333
train_loss: 0.0022149822
test_loss: 0.0025257608
train_loss: 0.0023538293
test_loss: 0.0026221373
train_loss: 0.002110556
test_loss: 0.0025894048
train_loss: 0.0019979961
test_loss: 0.0026813992
train_loss: 0.0021363688
test_loss: 0.0023513348
train_loss: 0.0019141354
test_loss: 0.0023569793
train_loss: 0.0020328
test_loss: 0.0024243721
train_loss: 0.0019490378
test_loss: 0.0024398067
train_loss: 0.0019773436
test_loss: 0.002399201
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce97ded0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce97d476a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce97dfad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce97d8a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce97ce26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce97ce29d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce97cc98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce97c7b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce97cc9488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce97c752f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce97c75400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce97cd8ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce97be7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce5bf2c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce5beec6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce5bf2a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce5bebe2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce5becaea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce5be9b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce5be99f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce5be45510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce5be458c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce5be45378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce34579730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce34579510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce34525f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce344d7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce3450f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce3450f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce3450f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce3445eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce34481268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce34481378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce34435598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce343da0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fce3440e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.61387833e-06
Iter: 2 loss: 4.49355457e-06
Iter: 3 loss: 4.29112788e-06
Iter: 4 loss: 4.0252944e-06
Iter: 5 loss: 3.79536596e-06
Iter: 6 loss: 3.72406657e-06
Iter: 7 loss: 3.55384145e-06
Iter: 8 loss: 5.81017503e-06
Iter: 9 loss: 3.55293651e-06
Iter: 10 loss: 3.38388e-06
Iter: 11 loss: 3.35434083e-06
Iter: 12 loss: 3.2392411e-06
Iter: 13 loss: 3.14607132e-06
Iter: 14 loss: 3.54415693e-06
Iter: 15 loss: 3.12683437e-06
Iter: 16 loss: 3.03506931e-06
Iter: 17 loss: 3.79660878e-06
Iter: 18 loss: 3.0296178e-06
Iter: 19 loss: 2.96677308e-06
Iter: 20 loss: 2.81462599e-06
Iter: 21 loss: 4.43321505e-06
Iter: 22 loss: 2.79793767e-06
Iter: 23 loss: 2.77313529e-06
Iter: 24 loss: 2.71926569e-06
Iter: 25 loss: 2.65364406e-06
Iter: 26 loss: 2.48790161e-06
Iter: 27 loss: 3.97929398e-06
Iter: 28 loss: 2.4626429e-06
Iter: 29 loss: 2.31517811e-06
Iter: 30 loss: 3.56722762e-06
Iter: 31 loss: 2.30698924e-06
Iter: 32 loss: 2.2498466e-06
Iter: 33 loss: 2.51061374e-06
Iter: 34 loss: 2.23898405e-06
Iter: 35 loss: 2.1873343e-06
Iter: 36 loss: 2.2699794e-06
Iter: 37 loss: 2.16337753e-06
Iter: 38 loss: 2.1434098e-06
Iter: 39 loss: 2.13987e-06
Iter: 40 loss: 2.11190036e-06
Iter: 41 loss: 2.06172103e-06
Iter: 42 loss: 3.29414297e-06
Iter: 43 loss: 2.06171626e-06
Iter: 44 loss: 1.99892747e-06
Iter: 45 loss: 2.09725749e-06
Iter: 46 loss: 1.96958e-06
Iter: 47 loss: 1.90828882e-06
Iter: 48 loss: 1.90813398e-06
Iter: 49 loss: 1.87375508e-06
Iter: 50 loss: 1.77770607e-06
Iter: 51 loss: 2.29671696e-06
Iter: 52 loss: 1.74810918e-06
Iter: 53 loss: 1.79875065e-06
Iter: 54 loss: 1.70816315e-06
Iter: 55 loss: 1.68192219e-06
Iter: 56 loss: 1.64564563e-06
Iter: 57 loss: 1.64410085e-06
Iter: 58 loss: 1.61142771e-06
Iter: 59 loss: 1.77514812e-06
Iter: 60 loss: 1.60595778e-06
Iter: 61 loss: 1.56828082e-06
Iter: 62 loss: 1.72882392e-06
Iter: 63 loss: 1.5604229e-06
Iter: 64 loss: 1.54416057e-06
Iter: 65 loss: 1.51670929e-06
Iter: 66 loss: 1.51660947e-06
Iter: 67 loss: 1.47606715e-06
Iter: 68 loss: 1.5074188e-06
Iter: 69 loss: 1.45146657e-06
Iter: 70 loss: 1.41434566e-06
Iter: 71 loss: 1.68173756e-06
Iter: 72 loss: 1.41115765e-06
Iter: 73 loss: 1.38226153e-06
Iter: 74 loss: 1.61866149e-06
Iter: 75 loss: 1.38044959e-06
Iter: 76 loss: 1.35437858e-06
Iter: 77 loss: 1.53225653e-06
Iter: 78 loss: 1.35185735e-06
Iter: 79 loss: 1.33780634e-06
Iter: 80 loss: 1.31795559e-06
Iter: 81 loss: 1.31719514e-06
Iter: 82 loss: 1.29308501e-06
Iter: 83 loss: 1.63863626e-06
Iter: 84 loss: 1.29301839e-06
Iter: 85 loss: 1.27068893e-06
Iter: 86 loss: 1.23898189e-06
Iter: 87 loss: 1.23789096e-06
Iter: 88 loss: 1.22046481e-06
Iter: 89 loss: 1.41346402e-06
Iter: 90 loss: 1.22012159e-06
Iter: 91 loss: 1.19956144e-06
Iter: 92 loss: 1.21578478e-06
Iter: 93 loss: 1.18715411e-06
Iter: 94 loss: 1.17297202e-06
Iter: 95 loss: 1.20247546e-06
Iter: 96 loss: 1.16738147e-06
Iter: 97 loss: 1.15940179e-06
Iter: 98 loss: 1.15887474e-06
Iter: 99 loss: 1.15327612e-06
Iter: 100 loss: 1.13703175e-06
Iter: 101 loss: 1.20618211e-06
Iter: 102 loss: 1.13069439e-06
Iter: 103 loss: 1.11106624e-06
Iter: 104 loss: 1.1656175e-06
Iter: 105 loss: 1.10472672e-06
Iter: 106 loss: 1.07805931e-06
Iter: 107 loss: 1.10438373e-06
Iter: 108 loss: 1.06308721e-06
Iter: 109 loss: 1.06181085e-06
Iter: 110 loss: 1.05355662e-06
Iter: 111 loss: 1.04477363e-06
Iter: 112 loss: 1.05412698e-06
Iter: 113 loss: 1.03991738e-06
Iter: 114 loss: 1.03089383e-06
Iter: 115 loss: 1.03196146e-06
Iter: 116 loss: 1.02393494e-06
Iter: 117 loss: 1.01813134e-06
Iter: 118 loss: 1.01733497e-06
Iter: 119 loss: 1.01308069e-06
Iter: 120 loss: 1.00365378e-06
Iter: 121 loss: 1.13816236e-06
Iter: 122 loss: 1.00317857e-06
Iter: 123 loss: 9.92262585e-07
Iter: 124 loss: 1.03677394e-06
Iter: 125 loss: 9.89925411e-07
Iter: 126 loss: 9.79706556e-07
Iter: 127 loss: 1.10156248e-06
Iter: 128 loss: 9.79594915e-07
Iter: 129 loss: 9.73833721e-07
Iter: 130 loss: 9.59055455e-07
Iter: 131 loss: 1.08378129e-06
Iter: 132 loss: 9.56597091e-07
Iter: 133 loss: 9.49307093e-07
Iter: 134 loss: 9.46159673e-07
Iter: 135 loss: 9.39064194e-07
Iter: 136 loss: 9.32482521e-07
Iter: 137 loss: 9.30753743e-07
Iter: 138 loss: 9.23575953e-07
Iter: 139 loss: 9.13674171e-07
Iter: 140 loss: 9.13222777e-07
Iter: 141 loss: 8.99957e-07
Iter: 142 loss: 1.01491446e-06
Iter: 143 loss: 8.99243105e-07
Iter: 144 loss: 8.90731144e-07
Iter: 145 loss: 8.89027717e-07
Iter: 146 loss: 8.83376174e-07
Iter: 147 loss: 8.72986107e-07
Iter: 148 loss: 8.72407895e-07
Iter: 149 loss: 8.69242399e-07
Iter: 150 loss: 8.6716409e-07
Iter: 151 loss: 8.65906657e-07
Iter: 152 loss: 8.60951104e-07
Iter: 153 loss: 9.09519258e-07
Iter: 154 loss: 8.60822468e-07
Iter: 155 loss: 8.56942847e-07
Iter: 156 loss: 8.51980417e-07
Iter: 157 loss: 8.51649702e-07
Iter: 158 loss: 8.463482e-07
Iter: 159 loss: 8.6761429e-07
Iter: 160 loss: 8.45135276e-07
Iter: 161 loss: 8.40112307e-07
Iter: 162 loss: 8.94346442e-07
Iter: 163 loss: 8.40019084e-07
Iter: 164 loss: 8.35685682e-07
Iter: 165 loss: 8.26503481e-07
Iter: 166 loss: 9.77779564e-07
Iter: 167 loss: 8.26253029e-07
Iter: 168 loss: 8.22772108e-07
Iter: 169 loss: 8.21742674e-07
Iter: 170 loss: 8.17375565e-07
Iter: 171 loss: 8.09295216e-07
Iter: 172 loss: 9.96876565e-07
Iter: 173 loss: 8.09298399e-07
Iter: 174 loss: 7.99755071e-07
Iter: 175 loss: 8.05633476e-07
Iter: 176 loss: 7.93659297e-07
Iter: 177 loss: 7.88503485e-07
Iter: 178 loss: 8.24635663e-07
Iter: 179 loss: 7.88034185e-07
Iter: 180 loss: 7.82749964e-07
Iter: 181 loss: 7.78278547e-07
Iter: 182 loss: 7.76785e-07
Iter: 183 loss: 7.76128843e-07
Iter: 184 loss: 7.73770353e-07
Iter: 185 loss: 7.70067459e-07
Iter: 186 loss: 7.66172e-07
Iter: 187 loss: 7.65532945e-07
Iter: 188 loss: 7.60046305e-07
Iter: 189 loss: 7.70417955e-07
Iter: 190 loss: 7.57726184e-07
Iter: 191 loss: 7.53383e-07
Iter: 192 loss: 7.53295694e-07
Iter: 193 loss: 7.51187486e-07
Iter: 194 loss: 7.45317777e-07
Iter: 195 loss: 7.78595222e-07
Iter: 196 loss: 7.43611395e-07
Iter: 197 loss: 7.45200737e-07
Iter: 198 loss: 7.41166332e-07
Iter: 199 loss: 7.39238e-07
Iter: 200 loss: 7.38285166e-07
Iter: 201 loss: 7.37382607e-07
Iter: 202 loss: 7.34959201e-07
Iter: 203 loss: 7.28642533e-07
Iter: 204 loss: 7.8048663e-07
Iter: 205 loss: 7.27547558e-07
Iter: 206 loss: 7.22840241e-07
Iter: 207 loss: 7.22716209e-07
Iter: 208 loss: 7.19482955e-07
Iter: 209 loss: 7.39414531e-07
Iter: 210 loss: 7.19117281e-07
Iter: 211 loss: 7.14920645e-07
Iter: 212 loss: 7.20322419e-07
Iter: 213 loss: 7.12825056e-07
Iter: 214 loss: 7.0934351e-07
Iter: 215 loss: 7.10235383e-07
Iter: 216 loss: 7.06817332e-07
Iter: 217 loss: 7.02645593e-07
Iter: 218 loss: 7.03691853e-07
Iter: 219 loss: 6.99601969e-07
Iter: 220 loss: 6.94387552e-07
Iter: 221 loss: 7.20595e-07
Iter: 222 loss: 6.9348323e-07
Iter: 223 loss: 6.89113563e-07
Iter: 224 loss: 7.07498202e-07
Iter: 225 loss: 6.88246928e-07
Iter: 226 loss: 6.83898293e-07
Iter: 227 loss: 7.37174503e-07
Iter: 228 loss: 6.83857138e-07
Iter: 229 loss: 6.82464588e-07
Iter: 230 loss: 6.79823643e-07
Iter: 231 loss: 7.3444528e-07
Iter: 232 loss: 6.79809659e-07
Iter: 233 loss: 6.7785254e-07
Iter: 234 loss: 6.77724074e-07
Iter: 235 loss: 6.75979e-07
Iter: 236 loss: 6.73301e-07
Iter: 237 loss: 6.73228271e-07
Iter: 238 loss: 6.71146267e-07
Iter: 239 loss: 6.75576189e-07
Iter: 240 loss: 6.70330792e-07
Iter: 241 loss: 6.66505343e-07
Iter: 242 loss: 6.72269948e-07
Iter: 243 loss: 6.64709205e-07
Iter: 244 loss: 6.61702472e-07
Iter: 245 loss: 6.61760396e-07
Iter: 246 loss: 6.59289356e-07
Iter: 247 loss: 6.57196892e-07
Iter: 248 loss: 6.57002e-07
Iter: 249 loss: 6.54995574e-07
Iter: 250 loss: 6.50748916e-07
Iter: 251 loss: 7.2308336e-07
Iter: 252 loss: 6.50661036e-07
Iter: 253 loss: 6.47966e-07
Iter: 254 loss: 6.65506377e-07
Iter: 255 loss: 6.4769597e-07
Iter: 256 loss: 6.45552404e-07
Iter: 257 loss: 6.44946e-07
Iter: 258 loss: 6.43656676e-07
Iter: 259 loss: 6.40586222e-07
Iter: 260 loss: 6.68261066e-07
Iter: 261 loss: 6.40431836e-07
Iter: 262 loss: 6.39553377e-07
Iter: 263 loss: 6.39301447e-07
Iter: 264 loss: 6.38339714e-07
Iter: 265 loss: 6.35427341e-07
Iter: 266 loss: 6.43381043e-07
Iter: 267 loss: 6.33921104e-07
Iter: 268 loss: 6.31254352e-07
Iter: 269 loss: 6.31107696e-07
Iter: 270 loss: 6.28599651e-07
Iter: 271 loss: 6.34042806e-07
Iter: 272 loss: 6.2767424e-07
Iter: 273 loss: 6.25942732e-07
Iter: 274 loss: 6.22979769e-07
Iter: 275 loss: 6.22956293e-07
Iter: 276 loss: 6.23138249e-07
Iter: 277 loss: 6.21431695e-07
Iter: 278 loss: 6.20568471e-07
Iter: 279 loss: 6.18427521e-07
Iter: 280 loss: 6.40813141e-07
Iter: 281 loss: 6.18169679e-07
Iter: 282 loss: 6.16051352e-07
Iter: 283 loss: 6.39580776e-07
Iter: 284 loss: 6.16021794e-07
Iter: 285 loss: 6.13849352e-07
Iter: 286 loss: 6.17799856e-07
Iter: 287 loss: 6.12914846e-07
Iter: 288 loss: 6.11882854e-07
Iter: 289 loss: 6.09675794e-07
Iter: 290 loss: 6.45126363e-07
Iter: 291 loss: 6.0963e-07
Iter: 292 loss: 6.06604e-07
Iter: 293 loss: 6.14635951e-07
Iter: 294 loss: 6.05579658e-07
Iter: 295 loss: 6.03051205e-07
Iter: 296 loss: 6.11500809e-07
Iter: 297 loss: 6.02358455e-07
Iter: 298 loss: 6.00644626e-07
Iter: 299 loss: 6.00544865e-07
Iter: 300 loss: 5.9920194e-07
Iter: 301 loss: 5.9686829e-07
Iter: 302 loss: 5.96897621e-07
Iter: 303 loss: 5.94488e-07
Iter: 304 loss: 5.9376e-07
Iter: 305 loss: 5.92352535e-07
Iter: 306 loss: 5.89217393e-07
Iter: 307 loss: 6.23752442e-07
Iter: 308 loss: 5.89147362e-07
Iter: 309 loss: 5.87760383e-07
Iter: 310 loss: 5.92711103e-07
Iter: 311 loss: 5.87445641e-07
Iter: 312 loss: 5.85902853e-07
Iter: 313 loss: 5.99802775e-07
Iter: 314 loss: 5.85816e-07
Iter: 315 loss: 5.84879899e-07
Iter: 316 loss: 5.83721032e-07
Iter: 317 loss: 5.83624967e-07
Iter: 318 loss: 5.82170117e-07
Iter: 319 loss: 5.83441192e-07
Iter: 320 loss: 5.81306665e-07
Iter: 321 loss: 5.78493882e-07
Iter: 322 loss: 5.88752243e-07
Iter: 323 loss: 5.77798573e-07
Iter: 324 loss: 5.75972649e-07
Iter: 325 loss: 5.7486784e-07
Iter: 326 loss: 5.74167e-07
Iter: 327 loss: 5.71889188e-07
Iter: 328 loss: 5.80328731e-07
Iter: 329 loss: 5.71351393e-07
Iter: 330 loss: 5.69317933e-07
Iter: 331 loss: 5.69323959e-07
Iter: 332 loss: 5.68684356e-07
Iter: 333 loss: 5.67349389e-07
Iter: 334 loss: 5.91928767e-07
Iter: 335 loss: 5.67337509e-07
Iter: 336 loss: 5.65819448e-07
Iter: 337 loss: 5.74757564e-07
Iter: 338 loss: 5.65653863e-07
Iter: 339 loss: 5.63994945e-07
Iter: 340 loss: 5.73092848e-07
Iter: 341 loss: 5.63743924e-07
Iter: 342 loss: 5.62994273e-07
Iter: 343 loss: 5.61031413e-07
Iter: 344 loss: 5.73818511e-07
Iter: 345 loss: 5.60487138e-07
Iter: 346 loss: 5.57319254e-07
Iter: 347 loss: 5.69406438e-07
Iter: 348 loss: 5.56600128e-07
Iter: 349 loss: 5.54650342e-07
Iter: 350 loss: 5.79969083e-07
Iter: 351 loss: 5.54649432e-07
Iter: 352 loss: 5.53169e-07
Iter: 353 loss: 5.66774e-07
Iter: 354 loss: 5.53096868e-07
Iter: 355 loss: 5.52104893e-07
Iter: 356 loss: 5.51409698e-07
Iter: 357 loss: 5.51070457e-07
Iter: 358 loss: 5.50001914e-07
Iter: 359 loss: 5.49161655e-07
Iter: 360 loss: 5.48825597e-07
Iter: 361 loss: 5.47658772e-07
Iter: 362 loss: 5.47599143e-07
Iter: 363 loss: 5.46428396e-07
Iter: 364 loss: 5.47795e-07
Iter: 365 loss: 5.45782768e-07
Iter: 366 loss: 5.44772035e-07
Iter: 367 loss: 5.42560088e-07
Iter: 368 loss: 5.72350245e-07
Iter: 369 loss: 5.42438329e-07
Iter: 370 loss: 5.4242355e-07
Iter: 371 loss: 5.41350062e-07
Iter: 372 loss: 5.40380711e-07
Iter: 373 loss: 5.40619737e-07
Iter: 374 loss: 5.39661755e-07
Iter: 375 loss: 5.38770337e-07
Iter: 376 loss: 5.37800929e-07
Iter: 377 loss: 5.37649044e-07
Iter: 378 loss: 5.37181393e-07
Iter: 379 loss: 5.36785251e-07
Iter: 380 loss: 5.3631419e-07
Iter: 381 loss: 5.35091544e-07
Iter: 382 loss: 5.43118176e-07
Iter: 383 loss: 5.34809089e-07
Iter: 384 loss: 5.32869706e-07
Iter: 385 loss: 5.32398758e-07
Iter: 386 loss: 5.31137516e-07
Iter: 387 loss: 5.29186309e-07
Iter: 388 loss: 5.4491619e-07
Iter: 389 loss: 5.29078875e-07
Iter: 390 loss: 5.27560076e-07
Iter: 391 loss: 5.40854103e-07
Iter: 392 loss: 5.27492887e-07
Iter: 393 loss: 5.26182589e-07
Iter: 394 loss: 5.35157142e-07
Iter: 395 loss: 5.26058841e-07
Iter: 396 loss: 5.25565554e-07
Iter: 397 loss: 5.25075279e-07
Iter: 398 loss: 5.24951815e-07
Iter: 399 loss: 5.24109e-07
Iter: 400 loss: 5.28886517e-07
Iter: 401 loss: 5.23955805e-07
Iter: 402 loss: 5.22886126e-07
Iter: 403 loss: 5.24430675e-07
Iter: 404 loss: 5.22366236e-07
Iter: 405 loss: 5.21535355e-07
Iter: 406 loss: 5.19799926e-07
Iter: 407 loss: 5.49818196e-07
Iter: 408 loss: 5.19769856e-07
Iter: 409 loss: 5.18313527e-07
Iter: 410 loss: 5.18216837e-07
Iter: 411 loss: 5.16823775e-07
Iter: 412 loss: 5.15587203e-07
Iter: 413 loss: 5.15218915e-07
Iter: 414 loss: 5.13989221e-07
Iter: 415 loss: 5.15403315e-07
Iter: 416 loss: 5.13359964e-07
Iter: 417 loss: 5.12273743e-07
Iter: 418 loss: 5.12227416e-07
Iter: 419 loss: 5.11716621e-07
Iter: 420 loss: 5.110453e-07
Iter: 421 loss: 5.11005169e-07
Iter: 422 loss: 5.10387849e-07
Iter: 423 loss: 5.09355232e-07
Iter: 424 loss: 5.09346307e-07
Iter: 425 loss: 5.08641278e-07
Iter: 426 loss: 5.08465916e-07
Iter: 427 loss: 5.07647883e-07
Iter: 428 loss: 5.08988364e-07
Iter: 429 loss: 5.07280618e-07
Iter: 430 loss: 5.06456047e-07
Iter: 431 loss: 5.04780701e-07
Iter: 432 loss: 5.317263e-07
Iter: 433 loss: 5.04712091e-07
Iter: 434 loss: 5.03880301e-07
Iter: 435 loss: 5.03595e-07
Iter: 436 loss: 5.02595242e-07
Iter: 437 loss: 5.02677608e-07
Iter: 438 loss: 5.01836837e-07
Iter: 439 loss: 5.0082275e-07
Iter: 440 loss: 5.00261649e-07
Iter: 441 loss: 4.99826626e-07
Iter: 442 loss: 4.9953087e-07
Iter: 443 loss: 4.99145358e-07
Iter: 444 loss: 4.98595341e-07
Iter: 445 loss: 4.97862061e-07
Iter: 446 loss: 4.97816814e-07
Iter: 447 loss: 4.96897087e-07
Iter: 448 loss: 4.96512598e-07
Iter: 449 loss: 4.96038695e-07
Iter: 450 loss: 4.95590314e-07
Iter: 451 loss: 4.95224526e-07
Iter: 452 loss: 4.94894039e-07
Iter: 453 loss: 4.93990967e-07
Iter: 454 loss: 4.98467955e-07
Iter: 455 loss: 4.93675145e-07
Iter: 456 loss: 4.92404297e-07
Iter: 457 loss: 5.00350495e-07
Iter: 458 loss: 4.9225855e-07
Iter: 459 loss: 4.91919536e-07
Iter: 460 loss: 4.91677952e-07
Iter: 461 loss: 4.91297101e-07
Iter: 462 loss: 4.90221737e-07
Iter: 463 loss: 4.95726226e-07
Iter: 464 loss: 4.89898412e-07
Iter: 465 loss: 4.88410137e-07
Iter: 466 loss: 4.94210553e-07
Iter: 467 loss: 4.88043668e-07
Iter: 468 loss: 4.87711873e-07
Iter: 469 loss: 4.87455338e-07
Iter: 470 loss: 4.8694551e-07
Iter: 471 loss: 4.85716782e-07
Iter: 472 loss: 4.97428687e-07
Iter: 473 loss: 4.85516296e-07
Iter: 474 loss: 4.84456962e-07
Iter: 475 loss: 4.97483256e-07
Iter: 476 loss: 4.84426266e-07
Iter: 477 loss: 4.84013697e-07
Iter: 478 loss: 4.839751e-07
Iter: 479 loss: 4.83675308e-07
Iter: 480 loss: 4.82823793e-07
Iter: 481 loss: 4.88441515e-07
Iter: 482 loss: 4.82641781e-07
Iter: 483 loss: 4.8187178e-07
Iter: 484 loss: 4.9296392e-07
Iter: 485 loss: 4.81874679e-07
Iter: 486 loss: 4.81113091e-07
Iter: 487 loss: 4.83871418e-07
Iter: 488 loss: 4.80934e-07
Iter: 489 loss: 4.8037424e-07
Iter: 490 loss: 4.78829918e-07
Iter: 491 loss: 4.87830334e-07
Iter: 492 loss: 4.78404672e-07
Iter: 493 loss: 4.76753485e-07
Iter: 494 loss: 4.95959171e-07
Iter: 495 loss: 4.76724779e-07
Iter: 496 loss: 4.76042231e-07
Iter: 497 loss: 4.75964896e-07
Iter: 498 loss: 4.753403e-07
Iter: 499 loss: 4.74222105e-07
Iter: 500 loss: 4.7422e-07
Iter: 501 loss: 4.73599073e-07
Iter: 502 loss: 4.734718e-07
Iter: 503 loss: 4.73032571e-07
Iter: 504 loss: 4.7210429e-07
Iter: 505 loss: 4.81958864e-07
Iter: 506 loss: 4.72060947e-07
Iter: 507 loss: 4.71304929e-07
Iter: 508 loss: 4.78811103e-07
Iter: 509 loss: 4.71295266e-07
Iter: 510 loss: 4.70907139e-07
Iter: 511 loss: 4.69904194e-07
Iter: 512 loss: 4.77619494e-07
Iter: 513 loss: 4.69715047e-07
Iter: 514 loss: 4.69099945e-07
Iter: 515 loss: 4.68929841e-07
Iter: 516 loss: 4.68221401e-07
Iter: 517 loss: 4.67654814e-07
Iter: 518 loss: 4.67432926e-07
Iter: 519 loss: 4.66612676e-07
Iter: 520 loss: 4.65806323e-07
Iter: 521 loss: 4.65619451e-07
Iter: 522 loss: 4.65914439e-07
Iter: 523 loss: 4.65094161e-07
Iter: 524 loss: 4.64752247e-07
Iter: 525 loss: 4.64149252e-07
Iter: 526 loss: 4.78120967e-07
Iter: 527 loss: 4.64144875e-07
Iter: 528 loss: 4.63348954e-07
Iter: 529 loss: 4.66575102e-07
Iter: 530 loss: 4.63164099e-07
Iter: 531 loss: 4.62167861e-07
Iter: 532 loss: 4.66350343e-07
Iter: 533 loss: 4.61970188e-07
Iter: 534 loss: 4.6145567e-07
Iter: 535 loss: 4.61157555e-07
Iter: 536 loss: 4.60914265e-07
Iter: 537 loss: 4.60071192e-07
Iter: 538 loss: 4.60183401e-07
Iter: 539 loss: 4.59410046e-07
Iter: 540 loss: 4.59122361e-07
Iter: 541 loss: 4.58967492e-07
Iter: 542 loss: 4.58477473e-07
Iter: 543 loss: 4.5831365e-07
Iter: 544 loss: 4.58025795e-07
Iter: 545 loss: 4.57454234e-07
Iter: 546 loss: 4.57048657e-07
Iter: 547 loss: 4.56832538e-07
Iter: 548 loss: 4.56337744e-07
Iter: 549 loss: 4.56213968e-07
Iter: 550 loss: 4.55781e-07
Iter: 551 loss: 4.54588445e-07
Iter: 552 loss: 4.60488536e-07
Iter: 553 loss: 4.54165303e-07
Iter: 554 loss: 4.53246628e-07
Iter: 555 loss: 4.53224345e-07
Iter: 556 loss: 4.52427884e-07
Iter: 557 loss: 4.57601402e-07
Iter: 558 loss: 4.52368653e-07
Iter: 559 loss: 4.51994509e-07
Iter: 560 loss: 4.5127797e-07
Iter: 561 loss: 4.64346044e-07
Iter: 562 loss: 4.51270949e-07
Iter: 563 loss: 4.51008589e-07
Iter: 564 loss: 4.50834307e-07
Iter: 565 loss: 4.50492024e-07
Iter: 566 loss: 4.49867315e-07
Iter: 567 loss: 4.49848045e-07
Iter: 568 loss: 4.49086855e-07
Iter: 569 loss: 4.49102458e-07
Iter: 570 loss: 4.48457229e-07
Iter: 571 loss: 4.47688706e-07
Iter: 572 loss: 4.55415659e-07
Iter: 573 loss: 4.47649683e-07
Iter: 574 loss: 4.47064821e-07
Iter: 575 loss: 4.53503503e-07
Iter: 576 loss: 4.47042083e-07
Iter: 577 loss: 4.4646265e-07
Iter: 578 loss: 4.45534425e-07
Iter: 579 loss: 4.4554065e-07
Iter: 580 loss: 4.44887974e-07
Iter: 581 loss: 4.47609352e-07
Iter: 582 loss: 4.44784916e-07
Iter: 583 loss: 4.44228277e-07
Iter: 584 loss: 4.51293374e-07
Iter: 585 loss: 4.44207274e-07
Iter: 586 loss: 4.43736042e-07
Iter: 587 loss: 4.42682e-07
Iter: 588 loss: 4.57420867e-07
Iter: 589 loss: 4.42635582e-07
Iter: 590 loss: 4.41993478e-07
Iter: 591 loss: 4.41992626e-07
Iter: 592 loss: 4.41275859e-07
Iter: 593 loss: 4.42865428e-07
Iter: 594 loss: 4.40995365e-07
Iter: 595 loss: 4.40495e-07
Iter: 596 loss: 4.39733697e-07
Iter: 597 loss: 4.39726932e-07
Iter: 598 loss: 4.39249476e-07
Iter: 599 loss: 4.39249789e-07
Iter: 600 loss: 4.38693462e-07
Iter: 601 loss: 4.38395773e-07
Iter: 602 loss: 4.38128097e-07
Iter: 603 loss: 4.37612584e-07
Iter: 604 loss: 4.37897455e-07
Iter: 605 loss: 4.3729392e-07
Iter: 606 loss: 4.36564761e-07
Iter: 607 loss: 4.36036856e-07
Iter: 608 loss: 4.35755027e-07
Iter: 609 loss: 4.3532927e-07
Iter: 610 loss: 4.35087117e-07
Iter: 611 loss: 4.34576521e-07
Iter: 612 loss: 4.34518483e-07
Iter: 613 loss: 4.34125525e-07
Iter: 614 loss: 4.33566555e-07
Iter: 615 loss: 4.3369252e-07
Iter: 616 loss: 4.33149921e-07
Iter: 617 loss: 4.32479396e-07
Iter: 618 loss: 4.32484399e-07
Iter: 619 loss: 4.32203308e-07
Iter: 620 loss: 4.31992078e-07
Iter: 621 loss: 4.31922501e-07
Iter: 622 loss: 4.31584738e-07
Iter: 623 loss: 4.34213035e-07
Iter: 624 loss: 4.31543185e-07
Iter: 625 loss: 4.31135646e-07
Iter: 626 loss: 4.3117376e-07
Iter: 627 loss: 4.3080621e-07
Iter: 628 loss: 4.30459124e-07
Iter: 629 loss: 4.29795648e-07
Iter: 630 loss: 4.46163057e-07
Iter: 631 loss: 4.29778538e-07
Iter: 632 loss: 4.28812825e-07
Iter: 633 loss: 4.32995535e-07
Iter: 634 loss: 4.2862672e-07
Iter: 635 loss: 4.27964153e-07
Iter: 636 loss: 4.2794025e-07
Iter: 637 loss: 4.27689855e-07
Iter: 638 loss: 4.27006711e-07
Iter: 639 loss: 4.33719123e-07
Iter: 640 loss: 4.26901067e-07
Iter: 641 loss: 4.26063508e-07
Iter: 642 loss: 4.29484032e-07
Iter: 643 loss: 4.25868677e-07
Iter: 644 loss: 4.25800522e-07
Iter: 645 loss: 4.25618936e-07
Iter: 646 loss: 4.25386958e-07
Iter: 647 loss: 4.24857291e-07
Iter: 648 loss: 4.32170197e-07
Iter: 649 loss: 4.24859849e-07
Iter: 650 loss: 4.24246338e-07
Iter: 651 loss: 4.26197801e-07
Iter: 652 loss: 4.24060289e-07
Iter: 653 loss: 4.23633338e-07
Iter: 654 loss: 4.23631462e-07
Iter: 655 loss: 4.23353441e-07
Iter: 656 loss: 4.22772928e-07
Iter: 657 loss: 4.30664983e-07
Iter: 658 loss: 4.22713981e-07
Iter: 659 loss: 4.22253521e-07
Iter: 660 loss: 4.22235445e-07
Iter: 661 loss: 4.21817e-07
Iter: 662 loss: 4.21824524e-07
Iter: 663 loss: 4.21479143e-07
Iter: 664 loss: 4.21046707e-07
Iter: 665 loss: 4.20351171e-07
Iter: 666 loss: 4.20339774e-07
Iter: 667 loss: 4.1967894e-07
Iter: 668 loss: 4.19681612e-07
Iter: 669 loss: 4.19152343e-07
Iter: 670 loss: 4.22652164e-07
Iter: 671 loss: 4.19116162e-07
Iter: 672 loss: 4.18818615e-07
Iter: 673 loss: 4.18069533e-07
Iter: 674 loss: 4.24624801e-07
Iter: 675 loss: 4.17942033e-07
Iter: 676 loss: 4.17342136e-07
Iter: 677 loss: 4.17336565e-07
Iter: 678 loss: 4.17073181e-07
Iter: 679 loss: 4.17074403e-07
Iter: 680 loss: 4.16817358e-07
Iter: 681 loss: 4.16279306e-07
Iter: 682 loss: 4.23933358e-07
Iter: 683 loss: 4.16234883e-07
Iter: 684 loss: 4.15935801e-07
Iter: 685 loss: 4.20643744e-07
Iter: 686 loss: 4.15931964e-07
Iter: 687 loss: 4.15564074e-07
Iter: 688 loss: 4.16014643e-07
Iter: 689 loss: 4.15374274e-07
Iter: 690 loss: 4.14906651e-07
Iter: 691 loss: 4.14109167e-07
Iter: 692 loss: 4.14123548e-07
Iter: 693 loss: 4.13548122e-07
Iter: 694 loss: 4.13458679e-07
Iter: 695 loss: 4.13036474e-07
Iter: 696 loss: 4.12384111e-07
Iter: 697 loss: 4.12371548e-07
Iter: 698 loss: 4.11602684e-07
Iter: 699 loss: 4.12522269e-07
Iter: 700 loss: 4.11187159e-07
Iter: 701 loss: 4.10857837e-07
Iter: 702 loss: 4.10744917e-07
Iter: 703 loss: 4.10453765e-07
Iter: 704 loss: 4.10498956e-07
Iter: 705 loss: 4.10266097e-07
Iter: 706 loss: 4.09993447e-07
Iter: 707 loss: 4.09509681e-07
Iter: 708 loss: 4.19605044e-07
Iter: 709 loss: 4.0950647e-07
Iter: 710 loss: 4.09061528e-07
Iter: 711 loss: 4.09072186e-07
Iter: 712 loss: 4.08635458e-07
Iter: 713 loss: 4.09704569e-07
Iter: 714 loss: 4.08509095e-07
Iter: 715 loss: 4.08079728e-07
Iter: 716 loss: 4.06994729e-07
Iter: 717 loss: 4.1688898e-07
Iter: 718 loss: 4.06823801e-07
Iter: 719 loss: 4.07285881e-07
Iter: 720 loss: 4.06401313e-07
Iter: 721 loss: 4.06077788e-07
Iter: 722 loss: 4.05496962e-07
Iter: 723 loss: 4.0548e-07
Iter: 724 loss: 4.04918126e-07
Iter: 725 loss: 4.10002343e-07
Iter: 726 loss: 4.04893228e-07
Iter: 727 loss: 4.04414919e-07
Iter: 728 loss: 4.06411345e-07
Iter: 729 loss: 4.04318087e-07
Iter: 730 loss: 4.041122e-07
Iter: 731 loss: 4.03963725e-07
Iter: 732 loss: 4.03910576e-07
Iter: 733 loss: 4.0356224e-07
Iter: 734 loss: 4.04711898e-07
Iter: 735 loss: 4.03480357e-07
Iter: 736 loss: 4.03048148e-07
Iter: 737 loss: 4.03981801e-07
Iter: 738 loss: 4.02882051e-07
Iter: 739 loss: 4.02478e-07
Iter: 740 loss: 4.02022692e-07
Iter: 741 loss: 4.01982732e-07
Iter: 742 loss: 4.01140767e-07
Iter: 743 loss: 4.01946949e-07
Iter: 744 loss: 4.00689089e-07
Iter: 745 loss: 4.00513784e-07
Iter: 746 loss: 4.00241419e-07
Iter: 747 loss: 3.99941257e-07
Iter: 748 loss: 3.99421594e-07
Iter: 749 loss: 3.99415114e-07
Iter: 750 loss: 3.989108e-07
Iter: 751 loss: 3.99366229e-07
Iter: 752 loss: 3.98605778e-07
Iter: 753 loss: 3.98746522e-07
Iter: 754 loss: 3.98376073e-07
Iter: 755 loss: 3.98271425e-07
Iter: 756 loss: 3.97944234e-07
Iter: 757 loss: 3.99281447e-07
Iter: 758 loss: 3.97837624e-07
Iter: 759 loss: 3.97358747e-07
Iter: 760 loss: 4.02318022e-07
Iter: 761 loss: 3.97363721e-07
Iter: 762 loss: 3.96950725e-07
Iter: 763 loss: 3.96708344e-07
Iter: 764 loss: 3.9654438e-07
Iter: 765 loss: 3.96155485e-07
Iter: 766 loss: 3.98107545e-07
Iter: 767 loss: 3.96090229e-07
Iter: 768 loss: 3.95608225e-07
Iter: 769 loss: 3.97432757e-07
Iter: 770 loss: 3.95525376e-07
Iter: 771 loss: 3.95045788e-07
Iter: 772 loss: 3.94847092e-07
Iter: 773 loss: 3.94612584e-07
Iter: 774 loss: 3.9412447e-07
Iter: 775 loss: 3.93879162e-07
Iter: 776 loss: 3.93672536e-07
Iter: 777 loss: 3.93151737e-07
Iter: 778 loss: 3.93159496e-07
Iter: 779 loss: 3.92770175e-07
Iter: 780 loss: 3.95976855e-07
Iter: 781 loss: 3.92745051e-07
Iter: 782 loss: 3.92502e-07
Iter: 783 loss: 3.91880235e-07
Iter: 784 loss: 3.96466362e-07
Iter: 785 loss: 3.91747335e-07
Iter: 786 loss: 3.91387857e-07
Iter: 787 loss: 3.91319304e-07
Iter: 788 loss: 3.90969234e-07
Iter: 789 loss: 3.92684626e-07
Iter: 790 loss: 3.90897895e-07
Iter: 791 loss: 3.90651223e-07
Iter: 792 loss: 3.90353819e-07
Iter: 793 loss: 3.90350067e-07
Iter: 794 loss: 3.89800277e-07
Iter: 795 loss: 3.93980031e-07
Iter: 796 loss: 3.89744e-07
Iter: 797 loss: 3.89498297e-07
Iter: 798 loss: 3.89186368e-07
Iter: 799 loss: 3.8915158e-07
Iter: 800 loss: 3.8872858e-07
Iter: 801 loss: 3.92057416e-07
Iter: 802 loss: 3.88691802e-07
Iter: 803 loss: 3.88203603e-07
Iter: 804 loss: 3.88494357e-07
Iter: 805 loss: 3.87885763e-07
Iter: 806 loss: 3.87355385e-07
Iter: 807 loss: 3.87520373e-07
Iter: 808 loss: 3.86982293e-07
Iter: 809 loss: 3.86397062e-07
Iter: 810 loss: 3.86810768e-07
Iter: 811 loss: 3.86038096e-07
Iter: 812 loss: 3.86223803e-07
Iter: 813 loss: 3.85831157e-07
Iter: 814 loss: 3.85650338e-07
Iter: 815 loss: 3.85417735e-07
Iter: 816 loss: 3.8540972e-07
Iter: 817 loss: 3.85104386e-07
Iter: 818 loss: 3.85102368e-07
Iter: 819 loss: 3.84845805e-07
Iter: 820 loss: 3.84644096e-07
Iter: 821 loss: 3.84583757e-07
Iter: 822 loss: 3.84393616e-07
Iter: 823 loss: 3.83984144e-07
Iter: 824 loss: 3.90857167e-07
Iter: 825 loss: 3.83985423e-07
Iter: 826 loss: 3.83557563e-07
Iter: 827 loss: 3.88052854e-07
Iter: 828 loss: 3.835477e-07
Iter: 829 loss: 3.83112109e-07
Iter: 830 loss: 3.82562206e-07
Iter: 831 loss: 3.82529777e-07
Iter: 832 loss: 3.8202765e-07
Iter: 833 loss: 3.8310435e-07
Iter: 834 loss: 3.81816562e-07
Iter: 835 loss: 3.81250857e-07
Iter: 836 loss: 3.86110372e-07
Iter: 837 loss: 3.81217831e-07
Iter: 838 loss: 3.80882653e-07
Iter: 839 loss: 3.81087716e-07
Iter: 840 loss: 3.8065707e-07
Iter: 841 loss: 3.80359324e-07
Iter: 842 loss: 3.80249276e-07
Iter: 843 loss: 3.80056633e-07
Iter: 844 loss: 3.79517928e-07
Iter: 845 loss: 3.80891635e-07
Iter: 846 loss: 3.79326138e-07
Iter: 847 loss: 3.79155836e-07
Iter: 848 loss: 3.79056473e-07
Iter: 849 loss: 3.78860761e-07
Iter: 850 loss: 3.7836756e-07
Iter: 851 loss: 3.81102609e-07
Iter: 852 loss: 3.78188133e-07
Iter: 853 loss: 3.77783749e-07
Iter: 854 loss: 3.77746375e-07
Iter: 855 loss: 3.7737891e-07
Iter: 856 loss: 3.78018456e-07
Iter: 857 loss: 3.77177571e-07
Iter: 858 loss: 3.7694673e-07
Iter: 859 loss: 3.77987476e-07
Iter: 860 loss: 3.76912112e-07
Iter: 861 loss: 3.76644721e-07
Iter: 862 loss: 3.76526202e-07
Iter: 863 loss: 3.76366415e-07
Iter: 864 loss: 3.75926163e-07
Iter: 865 loss: 3.7610161e-07
Iter: 866 loss: 3.75634215e-07
Iter: 867 loss: 3.75314073e-07
Iter: 868 loss: 3.80453855e-07
Iter: 869 loss: 3.75304154e-07
Iter: 870 loss: 3.74966419e-07
Iter: 871 loss: 3.74284411e-07
Iter: 872 loss: 3.88994749e-07
Iter: 873 loss: 3.74279239e-07
Iter: 874 loss: 3.73551899e-07
Iter: 875 loss: 3.768223e-07
Iter: 876 loss: 3.73415929e-07
Iter: 877 loss: 3.72978946e-07
Iter: 878 loss: 3.74313316e-07
Iter: 879 loss: 3.72848405e-07
Iter: 880 loss: 3.72518087e-07
Iter: 881 loss: 3.75902346e-07
Iter: 882 loss: 3.72501944e-07
Iter: 883 loss: 3.72127289e-07
Iter: 884 loss: 3.72603211e-07
Iter: 885 loss: 3.71925552e-07
Iter: 886 loss: 3.71709149e-07
Iter: 887 loss: 3.71989671e-07
Iter: 888 loss: 3.71593728e-07
Iter: 889 loss: 3.71251389e-07
Iter: 890 loss: 3.72225941e-07
Iter: 891 loss: 3.7114421e-07
Iter: 892 loss: 3.70753867e-07
Iter: 893 loss: 3.70203622e-07
Iter: 894 loss: 3.70193902e-07
Iter: 895 loss: 3.69729719e-07
Iter: 896 loss: 3.69700558e-07
Iter: 897 loss: 3.69412533e-07
Iter: 898 loss: 3.68782622e-07
Iter: 899 loss: 3.77732135e-07
Iter: 900 loss: 3.68747379e-07
Iter: 901 loss: 3.68088251e-07
Iter: 902 loss: 3.75061944e-07
Iter: 903 loss: 3.68067163e-07
Iter: 904 loss: 3.67574614e-07
Iter: 905 loss: 3.70714218e-07
Iter: 906 loss: 3.67518339e-07
Iter: 907 loss: 3.67304324e-07
Iter: 908 loss: 3.67062228e-07
Iter: 909 loss: 3.67044265e-07
Iter: 910 loss: 3.66642439e-07
Iter: 911 loss: 3.67760464e-07
Iter: 912 loss: 3.66508061e-07
Iter: 913 loss: 3.66219e-07
Iter: 914 loss: 3.68469529e-07
Iter: 915 loss: 3.66182917e-07
Iter: 916 loss: 3.65886052e-07
Iter: 917 loss: 3.66862878e-07
Iter: 918 loss: 3.6578848e-07
Iter: 919 loss: 3.65428e-07
Iter: 920 loss: 3.6471539e-07
Iter: 921 loss: 3.77645222e-07
Iter: 922 loss: 3.64693562e-07
Iter: 923 loss: 3.64431457e-07
Iter: 924 loss: 3.64265418e-07
Iter: 925 loss: 3.63965569e-07
Iter: 926 loss: 3.63414586e-07
Iter: 927 loss: 3.75950719e-07
Iter: 928 loss: 3.63408446e-07
Iter: 929 loss: 3.62994484e-07
Iter: 930 loss: 3.62985929e-07
Iter: 931 loss: 3.62587429e-07
Iter: 932 loss: 3.62537122e-07
Iter: 933 loss: 3.62294486e-07
Iter: 934 loss: 3.61998161e-07
Iter: 935 loss: 3.62709e-07
Iter: 936 loss: 3.61900192e-07
Iter: 937 loss: 3.61597074e-07
Iter: 938 loss: 3.63297886e-07
Iter: 939 loss: 3.61543925e-07
Iter: 940 loss: 3.61302853e-07
Iter: 941 loss: 3.61073035e-07
Iter: 942 loss: 3.60991862e-07
Iter: 943 loss: 3.60629031e-07
Iter: 944 loss: 3.60520772e-07
Iter: 945 loss: 3.6026978e-07
Iter: 946 loss: 3.59592633e-07
Iter: 947 loss: 3.64335534e-07
Iter: 948 loss: 3.59532521e-07
Iter: 949 loss: 3.59285025e-07
Iter: 950 loss: 3.59269e-07
Iter: 951 loss: 3.59020333e-07
Iter: 952 loss: 3.58449483e-07
Iter: 953 loss: 3.65311e-07
Iter: 954 loss: 3.58404122e-07
Iter: 955 loss: 3.57862035e-07
Iter: 956 loss: 3.6475052e-07
Iter: 957 loss: 3.57855697e-07
Iter: 958 loss: 3.57361e-07
Iter: 959 loss: 3.58984096e-07
Iter: 960 loss: 3.5723923e-07
Iter: 961 loss: 3.57024618e-07
Iter: 962 loss: 3.57423517e-07
Iter: 963 loss: 3.56942166e-07
Iter: 964 loss: 3.56611565e-07
Iter: 965 loss: 3.56748785e-07
Iter: 966 loss: 3.5637612e-07
Iter: 967 loss: 3.56063879e-07
Iter: 968 loss: 3.56207437e-07
Iter: 969 loss: 3.55864358e-07
Iter: 970 loss: 3.55623314e-07
Iter: 971 loss: 3.55615811e-07
Iter: 972 loss: 3.55414102e-07
Iter: 973 loss: 3.55018472e-07
Iter: 974 loss: 3.62372589e-07
Iter: 975 loss: 3.55013754e-07
Iter: 976 loss: 3.54621818e-07
Iter: 977 loss: 3.56086645e-07
Iter: 978 loss: 3.54526946e-07
Iter: 979 loss: 3.54117333e-07
Iter: 980 loss: 3.5369186e-07
Iter: 981 loss: 3.53610943e-07
Iter: 982 loss: 3.53100404e-07
Iter: 983 loss: 3.53061353e-07
Iter: 984 loss: 3.52662909e-07
Iter: 985 loss: 3.549859e-07
Iter: 986 loss: 3.52602598e-07
Iter: 987 loss: 3.52316533e-07
Iter: 988 loss: 3.5166272e-07
Iter: 989 loss: 3.60133924e-07
Iter: 990 loss: 3.51615085e-07
Iter: 991 loss: 3.51532975e-07
Iter: 992 loss: 3.51241226e-07
Iter: 993 loss: 3.51082548e-07
Iter: 994 loss: 3.50875666e-07
Iter: 995 loss: 3.50828032e-07
Iter: 996 loss: 3.50660798e-07
Iter: 997 loss: 3.53469375e-07
Iter: 998 loss: 3.50658752e-07
Iter: 999 loss: 3.50457611e-07
Iter: 1000 loss: 3.50039102e-07
Iter: 1001 loss: 3.57368663e-07
Iter: 1002 loss: 3.50051323e-07
Iter: 1003 loss: 3.49751417e-07
Iter: 1004 loss: 3.49876217e-07
Iter: 1005 loss: 3.49567756e-07
Iter: 1006 loss: 3.49140748e-07
Iter: 1007 loss: 3.54602207e-07
Iter: 1008 loss: 3.49161951e-07
Iter: 1009 loss: 3.48764047e-07
Iter: 1010 loss: 3.4833613e-07
Iter: 1011 loss: 3.48255355e-07
Iter: 1012 loss: 3.47832326e-07
Iter: 1013 loss: 3.4803179e-07
Iter: 1014 loss: 3.47537025e-07
Iter: 1015 loss: 3.4701543e-07
Iter: 1016 loss: 3.48729884e-07
Iter: 1017 loss: 3.46882757e-07
Iter: 1018 loss: 3.4679465e-07
Iter: 1019 loss: 3.46712056e-07
Iter: 1020 loss: 3.46507136e-07
Iter: 1021 loss: 3.46135607e-07
Iter: 1022 loss: 3.5275886e-07
Iter: 1023 loss: 3.46118384e-07
Iter: 1024 loss: 3.45852357e-07
Iter: 1025 loss: 3.47043709e-07
Iter: 1026 loss: 3.45787839e-07
Iter: 1027 loss: 3.45600682e-07
Iter: 1028 loss: 3.45599688e-07
Iter: 1029 loss: 3.45374815e-07
Iter: 1030 loss: 3.44851287e-07
Iter: 1031 loss: 3.48176684e-07
Iter: 1032 loss: 3.4472481e-07
Iter: 1033 loss: 3.44911484e-07
Iter: 1034 loss: 3.44498e-07
Iter: 1035 loss: 3.44346915e-07
Iter: 1036 loss: 3.44009322e-07
Iter: 1037 loss: 3.4747444e-07
Iter: 1038 loss: 3.43959414e-07
Iter: 1039 loss: 3.43608235e-07
Iter: 1040 loss: 3.46268621e-07
Iter: 1041 loss: 3.43577739e-07
Iter: 1042 loss: 3.43284398e-07
Iter: 1043 loss: 3.43537181e-07
Iter: 1044 loss: 3.43089027e-07
Iter: 1045 loss: 3.42850257e-07
Iter: 1046 loss: 3.42499504e-07
Iter: 1047 loss: 3.42498453e-07
Iter: 1048 loss: 3.42153e-07
Iter: 1049 loss: 3.42098645e-07
Iter: 1050 loss: 3.41854218e-07
Iter: 1051 loss: 3.4152211e-07
Iter: 1052 loss: 3.41129805e-07
Iter: 1053 loss: 3.41075975e-07
Iter: 1054 loss: 3.40577856e-07
Iter: 1055 loss: 3.4653732e-07
Iter: 1056 loss: 3.40576321e-07
Iter: 1057 loss: 3.40264251e-07
Iter: 1058 loss: 3.40643851e-07
Iter: 1059 loss: 3.40096335e-07
Iter: 1060 loss: 3.3971844e-07
Iter: 1061 loss: 3.39596596e-07
Iter: 1062 loss: 3.39393864e-07
Iter: 1063 loss: 3.39124085e-07
Iter: 1064 loss: 3.39027736e-07
Iter: 1065 loss: 3.38869938e-07
Iter: 1066 loss: 3.38622044e-07
Iter: 1067 loss: 3.38596e-07
Iter: 1068 loss: 3.38303721e-07
Iter: 1069 loss: 3.40046199e-07
Iter: 1070 loss: 3.38235964e-07
Iter: 1071 loss: 3.3786165e-07
Iter: 1072 loss: 3.38184662e-07
Iter: 1073 loss: 3.37620151e-07
Iter: 1074 loss: 3.37324252e-07
Iter: 1075 loss: 3.38384893e-07
Iter: 1076 loss: 3.37231143e-07
Iter: 1077 loss: 3.36968554e-07
Iter: 1078 loss: 3.38785753e-07
Iter: 1079 loss: 3.36945106e-07
Iter: 1080 loss: 3.36777077e-07
Iter: 1081 loss: 3.36538392e-07
Iter: 1082 loss: 3.36546e-07
Iter: 1083 loss: 3.36404526e-07
Iter: 1084 loss: 3.36382e-07
Iter: 1085 loss: 3.36248405e-07
Iter: 1086 loss: 3.35983373e-07
Iter: 1087 loss: 3.41257163e-07
Iter: 1088 loss: 3.35981895e-07
Iter: 1089 loss: 3.35586492e-07
Iter: 1090 loss: 3.35616136e-07
Iter: 1091 loss: 3.35286757e-07
Iter: 1092 loss: 3.3481524e-07
Iter: 1093 loss: 3.36592109e-07
Iter: 1094 loss: 3.34701951e-07
Iter: 1095 loss: 3.34228844e-07
Iter: 1096 loss: 3.34011531e-07
Iter: 1097 loss: 3.3379726e-07
Iter: 1098 loss: 3.34203605e-07
Iter: 1099 loss: 3.33607431e-07
Iter: 1100 loss: 3.33459809e-07
Iter: 1101 loss: 3.33114656e-07
Iter: 1102 loss: 3.38484227e-07
Iter: 1103 loss: 3.33098257e-07
Iter: 1104 loss: 3.32754155e-07
Iter: 1105 loss: 3.33938715e-07
Iter: 1106 loss: 3.32643765e-07
Iter: 1107 loss: 3.32335844e-07
Iter: 1108 loss: 3.37358472e-07
Iter: 1109 loss: 3.32337152e-07
Iter: 1110 loss: 3.32196578e-07
Iter: 1111 loss: 3.31840027e-07
Iter: 1112 loss: 3.34978637e-07
Iter: 1113 loss: 3.31821468e-07
Iter: 1114 loss: 3.31371638e-07
Iter: 1115 loss: 3.37116688e-07
Iter: 1116 loss: 3.31381671e-07
Iter: 1117 loss: 3.31105099e-07
Iter: 1118 loss: 3.31288845e-07
Iter: 1119 loss: 3.30946051e-07
Iter: 1120 loss: 3.30712226e-07
Iter: 1121 loss: 3.31834116e-07
Iter: 1122 loss: 3.30683037e-07
Iter: 1123 loss: 3.30386911e-07
Iter: 1124 loss: 3.30276862e-07
Iter: 1125 loss: 3.30121566e-07
Iter: 1126 loss: 3.29761235e-07
Iter: 1127 loss: 3.30144701e-07
Iter: 1128 loss: 3.29573396e-07
Iter: 1129 loss: 3.29226282e-07
Iter: 1130 loss: 3.29121207e-07
Iter: 1131 loss: 3.28932629e-07
Iter: 1132 loss: 3.28327758e-07
Iter: 1133 loss: 3.30732973e-07
Iter: 1134 loss: 3.28189913e-07
Iter: 1135 loss: 3.28171382e-07
Iter: 1136 loss: 3.27986243e-07
Iter: 1137 loss: 3.27876251e-07
Iter: 1138 loss: 3.27567307e-07
Iter: 1139 loss: 3.2961043e-07
Iter: 1140 loss: 3.27465983e-07
Iter: 1141 loss: 3.27302814e-07
Iter: 1142 loss: 3.27265866e-07
Iter: 1143 loss: 3.27078737e-07
Iter: 1144 loss: 3.27174689e-07
Iter: 1145 loss: 3.26951266e-07
Iter: 1146 loss: 3.26780366e-07
Iter: 1147 loss: 3.2666054e-07
Iter: 1148 loss: 3.26611541e-07
Iter: 1149 loss: 3.26313852e-07
Iter: 1150 loss: 3.29564045e-07
Iter: 1151 loss: 3.26314193e-07
Iter: 1152 loss: 3.26173335e-07
Iter: 1153 loss: 3.25950339e-07
Iter: 1154 loss: 3.25945791e-07
Iter: 1155 loss: 3.25544306e-07
Iter: 1156 loss: 3.27240173e-07
Iter: 1157 loss: 3.25478737e-07
Iter: 1158 loss: 3.25139e-07
Iter: 1159 loss: 3.24854113e-07
Iter: 1160 loss: 3.24761118e-07
Iter: 1161 loss: 3.24434524e-07
Iter: 1162 loss: 3.2602486e-07
Iter: 1163 loss: 3.24367647e-07
Iter: 1164 loss: 3.24072744e-07
Iter: 1165 loss: 3.24086898e-07
Iter: 1166 loss: 3.2383565e-07
Iter: 1167 loss: 3.23619616e-07
Iter: 1168 loss: 3.23603928e-07
Iter: 1169 loss: 3.23374309e-07
Iter: 1170 loss: 3.23949337e-07
Iter: 1171 loss: 3.23320194e-07
Iter: 1172 loss: 3.23114818e-07
Iter: 1173 loss: 3.22729392e-07
Iter: 1174 loss: 3.30417492e-07
Iter: 1175 loss: 3.22750026e-07
Iter: 1176 loss: 3.22739879e-07
Iter: 1177 loss: 3.22582196e-07
Iter: 1178 loss: 3.22445317e-07
Iter: 1179 loss: 3.2210653e-07
Iter: 1180 loss: 3.25823834e-07
Iter: 1181 loss: 3.22059776e-07
Iter: 1182 loss: 3.21661219e-07
Iter: 1183 loss: 3.22218256e-07
Iter: 1184 loss: 3.21475e-07
Iter: 1185 loss: 3.21466757e-07
Iter: 1186 loss: 3.21320897e-07
Iter: 1187 loss: 3.21200503e-07
Iter: 1188 loss: 3.20905741e-07
Iter: 1189 loss: 3.23535232e-07
Iter: 1190 loss: 3.20850461e-07
Iter: 1191 loss: 3.20557234e-07
Iter: 1192 loss: 3.20554108e-07
Iter: 1193 loss: 3.20288734e-07
Iter: 1194 loss: 3.20822267e-07
Iter: 1195 loss: 3.20206141e-07
Iter: 1196 loss: 3.20017591e-07
Iter: 1197 loss: 3.19647938e-07
Iter: 1198 loss: 3.26610831e-07
Iter: 1199 loss: 3.19642737e-07
Iter: 1200 loss: 3.19357042e-07
Iter: 1201 loss: 3.19354911e-07
Iter: 1202 loss: 3.19245174e-07
Iter: 1203 loss: 3.19244e-07
Iter: 1204 loss: 3.19127707e-07
Iter: 1205 loss: 3.18989919e-07
Iter: 1206 loss: 3.18978664e-07
Iter: 1207 loss: 3.18803757e-07
Iter: 1208 loss: 3.18788182e-07
Iter: 1209 loss: 3.18645931e-07
Iter: 1210 loss: 3.18423815e-07
Iter: 1211 loss: 3.2055442e-07
Iter: 1212 loss: 3.18417278e-07
Iter: 1213 loss: 3.18119476e-07
Iter: 1214 loss: 3.17832956e-07
Iter: 1215 loss: 3.17773e-07
Iter: 1216 loss: 3.1747544e-07
Iter: 1217 loss: 3.18029663e-07
Iter: 1218 loss: 3.17368347e-07
Iter: 1219 loss: 3.17170361e-07
Iter: 1220 loss: 3.17170674e-07
Iter: 1221 loss: 3.16976241e-07
Iter: 1222 loss: 3.16940088e-07
Iter: 1223 loss: 3.16814408e-07
Iter: 1224 loss: 3.16691569e-07
Iter: 1225 loss: 3.17572812e-07
Iter: 1226 loss: 3.16683924e-07
Iter: 1227 loss: 3.16531043e-07
Iter: 1228 loss: 3.16575154e-07
Iter: 1229 loss: 3.16403657e-07
Iter: 1230 loss: 3.16257967e-07
Iter: 1231 loss: 3.16104291e-07
Iter: 1232 loss: 3.16055264e-07
Iter: 1233 loss: 3.15810553e-07
Iter: 1234 loss: 3.15902525e-07
Iter: 1235 loss: 3.15619047e-07
Iter: 1236 loss: 3.15256329e-07
Iter: 1237 loss: 3.20253832e-07
Iter: 1238 loss: 3.15259456e-07
Iter: 1239 loss: 3.14975068e-07
Iter: 1240 loss: 3.1485763e-07
Iter: 1241 loss: 3.14734791e-07
Iter: 1242 loss: 3.14445799e-07
Iter: 1243 loss: 3.14617893e-07
Iter: 1244 loss: 3.14295505e-07
Iter: 1245 loss: 3.13907549e-07
Iter: 1246 loss: 3.14665158e-07
Iter: 1247 loss: 3.1374563e-07
Iter: 1248 loss: 3.13847636e-07
Iter: 1249 loss: 3.13641635e-07
Iter: 1250 loss: 3.13566915e-07
Iter: 1251 loss: 3.13327121e-07
Iter: 1252 loss: 3.13895242e-07
Iter: 1253 loss: 3.13201326e-07
Iter: 1254 loss: 3.12913215e-07
Iter: 1255 loss: 3.15600232e-07
Iter: 1256 loss: 3.12892439e-07
Iter: 1257 loss: 3.12716594e-07
Iter: 1258 loss: 3.12714e-07
Iter: 1259 loss: 3.12583978e-07
Iter: 1260 loss: 3.1224792e-07
Iter: 1261 loss: 3.14003216e-07
Iter: 1262 loss: 3.12133977e-07
Iter: 1263 loss: 3.12063548e-07
Iter: 1264 loss: 3.11947815e-07
Iter: 1265 loss: 3.11796782e-07
Iter: 1266 loss: 3.11891654e-07
Iter: 1267 loss: 3.11690883e-07
Iter: 1268 loss: 3.11510519e-07
Iter: 1269 loss: 3.11134897e-07
Iter: 1270 loss: 3.16785872e-07
Iter: 1271 loss: 3.11086183e-07
Iter: 1272 loss: 3.11114775e-07
Iter: 1273 loss: 3.10920512e-07
Iter: 1274 loss: 3.10751602e-07
Iter: 1275 loss: 3.10603411e-07
Iter: 1276 loss: 3.10558136e-07
Iter: 1277 loss: 3.10313311e-07
Iter: 1278 loss: 3.100551e-07
Iter: 1279 loss: 3.10017185e-07
Iter: 1280 loss: 3.09710515e-07
Iter: 1281 loss: 3.12625048e-07
Iter: 1282 loss: 3.09686868e-07
Iter: 1283 loss: 3.09547289e-07
Iter: 1284 loss: 3.09538223e-07
Iter: 1285 loss: 3.09384063e-07
Iter: 1286 loss: 3.09181985e-07
Iter: 1287 loss: 3.09178063e-07
Iter: 1288 loss: 3.08982237e-07
Iter: 1289 loss: 3.09224049e-07
Iter: 1290 loss: 3.08890776e-07
Iter: 1291 loss: 3.08678352e-07
Iter: 1292 loss: 3.11742127e-07
Iter: 1293 loss: 3.08668518e-07
Iter: 1294 loss: 3.0848409e-07
Iter: 1295 loss: 3.080998e-07
Iter: 1296 loss: 3.15623936e-07
Iter: 1297 loss: 3.08099231e-07
Iter: 1298 loss: 3.07961812e-07
Iter: 1299 loss: 3.07906646e-07
Iter: 1300 loss: 3.07721734e-07
Iter: 1301 loss: 3.07504251e-07
Iter: 1302 loss: 3.07472931e-07
Iter: 1303 loss: 3.07197524e-07
Iter: 1304 loss: 3.07495043e-07
Iter: 1305 loss: 3.07031911e-07
Iter: 1306 loss: 3.06826962e-07
Iter: 1307 loss: 3.06806356e-07
Iter: 1308 loss: 3.06695426e-07
Iter: 1309 loss: 3.06511282e-07
Iter: 1310 loss: 3.1017882e-07
Iter: 1311 loss: 3.06492893e-07
Iter: 1312 loss: 3.06263985e-07
Iter: 1313 loss: 3.07207358e-07
Iter: 1314 loss: 3.06195062e-07
Iter: 1315 loss: 3.06056307e-07
Iter: 1316 loss: 3.06055767e-07
Iter: 1317 loss: 3.05901835e-07
Iter: 1318 loss: 3.05986362e-07
Iter: 1319 loss: 3.05822056e-07
Iter: 1320 loss: 3.05623303e-07
Iter: 1321 loss: 3.05400135e-07
Iter: 1322 loss: 3.05380439e-07
Iter: 1323 loss: 3.05255043e-07
Iter: 1324 loss: 3.05239666e-07
Iter: 1325 loss: 3.05079823e-07
Iter: 1326 loss: 3.05111371e-07
Iter: 1327 loss: 3.04968552e-07
Iter: 1328 loss: 3.04794611e-07
Iter: 1329 loss: 3.04472508e-07
Iter: 1330 loss: 3.04476771e-07
Iter: 1331 loss: 3.04567465e-07
Iter: 1332 loss: 3.0433165e-07
Iter: 1333 loss: 3.04246754e-07
Iter: 1334 loss: 3.04033733e-07
Iter: 1335 loss: 3.05892115e-07
Iter: 1336 loss: 3.03988315e-07
Iter: 1337 loss: 3.03686733e-07
Iter: 1338 loss: 3.03792e-07
Iter: 1339 loss: 3.03456346e-07
Iter: 1340 loss: 3.03617981e-07
Iter: 1341 loss: 3.03365709e-07
Iter: 1342 loss: 3.03263107e-07
Iter: 1343 loss: 3.03038064e-07
Iter: 1344 loss: 3.04882548e-07
Iter: 1345 loss: 3.03007539e-07
Iter: 1346 loss: 3.02772492e-07
Iter: 1347 loss: 3.03882189e-07
Iter: 1348 loss: 3.02731848e-07
Iter: 1349 loss: 3.02598e-07
Iter: 1350 loss: 3.02589115e-07
Iter: 1351 loss: 3.02454907e-07
Iter: 1352 loss: 3.02122089e-07
Iter: 1353 loss: 3.06350728e-07
Iter: 1354 loss: 3.02095486e-07
Iter: 1355 loss: 3.01852111e-07
Iter: 1356 loss: 3.03423974e-07
Iter: 1357 loss: 3.01794444e-07
Iter: 1358 loss: 3.01594241e-07
Iter: 1359 loss: 3.03564377e-07
Iter: 1360 loss: 3.01577757e-07
Iter: 1361 loss: 3.01337451e-07
Iter: 1362 loss: 3.01075204e-07
Iter: 1363 loss: 3.01035186e-07
Iter: 1364 loss: 3.00825945e-07
Iter: 1365 loss: 3.02371177e-07
Iter: 1366 loss: 3.00801844e-07
Iter: 1367 loss: 3.00694751e-07
Iter: 1368 loss: 3.00695717e-07
Iter: 1369 loss: 3.00606416e-07
Iter: 1370 loss: 3.00411216e-07
Iter: 1371 loss: 3.02476934e-07
Iter: 1372 loss: 3.00371852e-07
Iter: 1373 loss: 3.0015741e-07
Iter: 1374 loss: 3.00432873e-07
Iter: 1375 loss: 3.00051056e-07
Iter: 1376 loss: 2.99781163e-07
Iter: 1377 loss: 3.01690136e-07
Iter: 1378 loss: 2.99756181e-07
Iter: 1379 loss: 2.99447095e-07
Iter: 1380 loss: 3.0083271e-07
Iter: 1381 loss: 2.9936956e-07
Iter: 1382 loss: 2.9923342e-07
Iter: 1383 loss: 2.98960799e-07
Iter: 1384 loss: 3.04531341e-07
Iter: 1385 loss: 2.98962959e-07
Iter: 1386 loss: 2.98670983e-07
Iter: 1387 loss: 3.00232387e-07
Iter: 1388 loss: 2.98630596e-07
Iter: 1389 loss: 2.98560906e-07
Iter: 1390 loss: 2.98487237e-07
Iter: 1391 loss: 2.98421526e-07
Iter: 1392 loss: 2.9822661e-07
Iter: 1393 loss: 2.99637577e-07
Iter: 1394 loss: 2.98191708e-07
Iter: 1395 loss: 2.97984343e-07
Iter: 1396 loss: 3.00030479e-07
Iter: 1397 loss: 2.97974424e-07
Iter: 1398 loss: 2.97805173e-07
Iter: 1399 loss: 2.9891288e-07
Iter: 1400 loss: 2.97804547e-07
Iter: 1401 loss: 2.97673097e-07
Iter: 1402 loss: 2.97365602e-07
Iter: 1403 loss: 3.01118234e-07
Iter: 1404 loss: 2.97337664e-07
Iter: 1405 loss: 2.97143799e-07
Iter: 1406 loss: 2.97124302e-07
Iter: 1407 loss: 2.96933592e-07
Iter: 1408 loss: 2.97297049e-07
Iter: 1409 loss: 2.96866801e-07
Iter: 1410 loss: 2.96715655e-07
Iter: 1411 loss: 2.96424361e-07
Iter: 1412 loss: 3.0133009e-07
Iter: 1413 loss: 2.96413759e-07
Iter: 1414 loss: 2.96161943e-07
Iter: 1415 loss: 2.99347e-07
Iter: 1416 loss: 2.96156429e-07
Iter: 1417 loss: 2.96057806e-07
Iter: 1418 loss: 2.96051326e-07
Iter: 1419 loss: 2.95954351e-07
Iter: 1420 loss: 2.95703387e-07
Iter: 1421 loss: 2.97580868e-07
Iter: 1422 loss: 2.95649784e-07
Iter: 1423 loss: 2.95414e-07
Iter: 1424 loss: 2.95856694e-07
Iter: 1425 loss: 2.9531958e-07
Iter: 1426 loss: 2.95030617e-07
Iter: 1427 loss: 2.96214409e-07
Iter: 1428 loss: 2.95002764e-07
Iter: 1429 loss: 2.94949956e-07
Iter: 1430 loss: 2.94917612e-07
Iter: 1431 loss: 2.94798e-07
Iter: 1432 loss: 2.94647407e-07
Iter: 1433 loss: 2.94655024e-07
Iter: 1434 loss: 2.94478781e-07
Iter: 1435 loss: 2.95163971e-07
Iter: 1436 loss: 2.94458857e-07
Iter: 1437 loss: 2.94288611e-07
Iter: 1438 loss: 2.95613575e-07
Iter: 1439 loss: 2.94267664e-07
Iter: 1440 loss: 2.94166227e-07
Iter: 1441 loss: 2.93902616e-07
Iter: 1442 loss: 2.96415692e-07
Iter: 1443 loss: 2.93864105e-07
Iter: 1444 loss: 2.93560447e-07
Iter: 1445 loss: 2.97214171e-07
Iter: 1446 loss: 2.93561158e-07
Iter: 1447 loss: 2.93290469e-07
Iter: 1448 loss: 2.94323513e-07
Iter: 1449 loss: 2.93242721e-07
Iter: 1450 loss: 2.93101607e-07
Iter: 1451 loss: 2.92828162e-07
Iter: 1452 loss: 2.92825575e-07
Iter: 1453 loss: 2.92545309e-07
Iter: 1454 loss: 2.94122685e-07
Iter: 1455 loss: 2.92504637e-07
Iter: 1456 loss: 2.92398568e-07
Iter: 1457 loss: 2.92406355e-07
Iter: 1458 loss: 2.92265952e-07
Iter: 1459 loss: 2.92015443e-07
Iter: 1460 loss: 2.97845276e-07
Iter: 1461 loss: 2.92017262e-07
Iter: 1462 loss: 2.91775478e-07
Iter: 1463 loss: 2.91976789e-07
Iter: 1464 loss: 2.91640333e-07
Iter: 1465 loss: 2.91387039e-07
Iter: 1466 loss: 2.91892377e-07
Iter: 1467 loss: 2.91266218e-07
Iter: 1468 loss: 2.91022786e-07
Iter: 1469 loss: 2.91014118e-07
Iter: 1470 loss: 2.90918251e-07
Iter: 1471 loss: 2.90680958e-07
Iter: 1472 loss: 2.93984812e-07
Iter: 1473 loss: 2.90653105e-07
Iter: 1474 loss: 2.90508865e-07
Iter: 1475 loss: 2.90501561e-07
Iter: 1476 loss: 2.90358059e-07
Iter: 1477 loss: 2.90611808e-07
Iter: 1478 loss: 2.9027612e-07
Iter: 1479 loss: 2.90166867e-07
Iter: 1480 loss: 2.89967147e-07
Iter: 1481 loss: 2.89955437e-07
Iter: 1482 loss: 2.89678837e-07
Iter: 1483 loss: 2.93207677e-07
Iter: 1484 loss: 2.89670226e-07
Iter: 1485 loss: 2.89487048e-07
Iter: 1486 loss: 2.89247566e-07
Iter: 1487 loss: 2.89215109e-07
Iter: 1488 loss: 2.88959285e-07
Iter: 1489 loss: 2.89510496e-07
Iter: 1490 loss: 2.88852164e-07
Iter: 1491 loss: 2.88600887e-07
Iter: 1492 loss: 2.88607595e-07
Iter: 1493 loss: 2.88372519e-07
Iter: 1494 loss: 2.88588e-07
Iter: 1495 loss: 2.88259884e-07
Iter: 1496 loss: 2.8810905e-07
Iter: 1497 loss: 2.87901571e-07
Iter: 1498 loss: 2.87884575e-07
Iter: 1499 loss: 2.87561619e-07
Iter: 1500 loss: 2.88584602e-07
Iter: 1501 loss: 2.87486813e-07
Iter: 1502 loss: 2.87352464e-07
Iter: 1503 loss: 2.87329186e-07
Iter: 1504 loss: 2.87155132e-07
Iter: 1505 loss: 2.868141e-07
Iter: 1506 loss: 2.92613777e-07
Iter: 1507 loss: 2.86807023e-07
Iter: 1508 loss: 2.86535794e-07
Iter: 1509 loss: 2.87448188e-07
Iter: 1510 loss: 2.86465337e-07
Iter: 1511 loss: 2.86314844e-07
Iter: 1512 loss: 2.86297478e-07
Iter: 1513 loss: 2.86148918e-07
Iter: 1514 loss: 2.86038102e-07
Iter: 1515 loss: 2.85985323e-07
Iter: 1516 loss: 2.85827724e-07
Iter: 1517 loss: 2.85773581e-07
Iter: 1518 loss: 2.85663731e-07
Iter: 1519 loss: 2.85425756e-07
Iter: 1520 loss: 2.87574608e-07
Iter: 1521 loss: 2.85434623e-07
Iter: 1522 loss: 2.85160525e-07
Iter: 1523 loss: 2.85681665e-07
Iter: 1524 loss: 2.85043541e-07
Iter: 1525 loss: 2.84845584e-07
Iter: 1526 loss: 2.8461875e-07
Iter: 1527 loss: 2.84603374e-07
Iter: 1528 loss: 2.84197085e-07
Iter: 1529 loss: 2.85422374e-07
Iter: 1530 loss: 2.84095307e-07
Iter: 1531 loss: 2.84042187e-07
Iter: 1532 loss: 2.83939812e-07
Iter: 1533 loss: 2.83847e-07
Iter: 1534 loss: 2.8367333e-07
Iter: 1535 loss: 2.87052956e-07
Iter: 1536 loss: 2.83663383e-07
Iter: 1537 loss: 2.83455563e-07
Iter: 1538 loss: 2.83418672e-07
Iter: 1539 loss: 2.83274e-07
Iter: 1540 loss: 2.8327787e-07
Iter: 1541 loss: 2.83178053e-07
Iter: 1542 loss: 2.83073859e-07
Iter: 1543 loss: 2.8281616e-07
Iter: 1544 loss: 2.86191806e-07
Iter: 1545 loss: 2.82810788e-07
Iter: 1546 loss: 2.82532028e-07
Iter: 1547 loss: 2.82749284e-07
Iter: 1548 loss: 2.82367694e-07
Iter: 1549 loss: 2.82043231e-07
Iter: 1550 loss: 2.82312612e-07
Iter: 1551 loss: 2.81871223e-07
Iter: 1552 loss: 2.81770099e-07
Iter: 1553 loss: 2.81655218e-07
Iter: 1554 loss: 2.81503702e-07
Iter: 1555 loss: 2.81411559e-07
Iter: 1556 loss: 2.81310292e-07
Iter: 1557 loss: 2.81116058e-07
Iter: 1558 loss: 2.81091673e-07
Iter: 1559 loss: 2.80927054e-07
Iter: 1560 loss: 2.80774259e-07
Iter: 1561 loss: 2.80748083e-07
Iter: 1562 loss: 2.80643974e-07
Iter: 1563 loss: 2.80464462e-07
Iter: 1564 loss: 2.8047e-07
Iter: 1565 loss: 2.80279096e-07
Iter: 1566 loss: 2.82030356e-07
Iter: 1567 loss: 2.80274037e-07
Iter: 1568 loss: 2.80098845e-07
Iter: 1569 loss: 2.80805324e-07
Iter: 1570 loss: 2.80065422e-07
Iter: 1571 loss: 2.7995253e-07
Iter: 1572 loss: 2.79793937e-07
Iter: 1573 loss: 2.84019734e-07
Iter: 1574 loss: 2.79796751e-07
Iter: 1575 loss: 2.79695371e-07
Iter: 1576 loss: 2.7967576e-07
Iter: 1577 loss: 2.79562244e-07
Iter: 1578 loss: 2.79351923e-07
Iter: 1579 loss: 2.84593227e-07
Iter: 1580 loss: 2.79349933e-07
Iter: 1581 loss: 2.79156666e-07
Iter: 1582 loss: 2.78890809e-07
Iter: 1583 loss: 2.78880378e-07
Iter: 1584 loss: 2.78405707e-07
Iter: 1585 loss: 2.79908647e-07
Iter: 1586 loss: 2.78266384e-07
Iter: 1587 loss: 2.78112282e-07
Iter: 1588 loss: 2.78102e-07
Iter: 1589 loss: 2.77904121e-07
Iter: 1590 loss: 2.77890024e-07
Iter: 1591 loss: 2.77755248e-07
Iter: 1592 loss: 2.77591568e-07
Iter: 1593 loss: 2.78222188e-07
Iter: 1594 loss: 2.77567295e-07
Iter: 1595 loss: 2.77440193e-07
Iter: 1596 loss: 2.78631518e-07
Iter: 1597 loss: 2.77426551e-07
Iter: 1598 loss: 2.77347624e-07
Iter: 1599 loss: 2.77136195e-07
Iter: 1600 loss: 2.78853292e-07
Iter: 1601 loss: 2.77095211e-07
Iter: 1602 loss: 2.76990733e-07
Iter: 1603 loss: 2.76958474e-07
Iter: 1604 loss: 2.767909e-07
Iter: 1605 loss: 2.76624263e-07
Iter: 1606 loss: 2.76599849e-07
Iter: 1607 loss: 2.763434e-07
Iter: 1608 loss: 2.76678975e-07
Iter: 1609 loss: 2.76199955e-07
Iter: 1610 loss: 2.76029851e-07
Iter: 1611 loss: 2.76008763e-07
Iter: 1612 loss: 2.75878932e-07
Iter: 1613 loss: 2.7574913e-07
Iter: 1614 loss: 2.75716843e-07
Iter: 1615 loss: 2.75518687e-07
Iter: 1616 loss: 2.75361117e-07
Iter: 1617 loss: 2.75314534e-07
Iter: 1618 loss: 2.75032733e-07
Iter: 1619 loss: 2.76682613e-07
Iter: 1620 loss: 2.7499641e-07
Iter: 1621 loss: 2.7485757e-07
Iter: 1622 loss: 2.74863567e-07
Iter: 1623 loss: 2.74690507e-07
Iter: 1624 loss: 2.744695e-07
Iter: 1625 loss: 2.74461712e-07
Iter: 1626 loss: 2.74261424e-07
Iter: 1627 loss: 2.75757486e-07
Iter: 1628 loss: 2.74271542e-07
Iter: 1629 loss: 2.73999802e-07
Iter: 1630 loss: 2.73941083e-07
Iter: 1631 loss: 2.73778596e-07
Iter: 1632 loss: 2.7358152e-07
Iter: 1633 loss: 2.74194093e-07
Iter: 1634 loss: 2.73500603e-07
Iter: 1635 loss: 2.73336923e-07
Iter: 1636 loss: 2.73345393e-07
Iter: 1637 loss: 2.73242819e-07
Iter: 1638 loss: 2.72990235e-07
Iter: 1639 loss: 2.75348327e-07
Iter: 1640 loss: 2.72970112e-07
Iter: 1641 loss: 2.72834427e-07
Iter: 1642 loss: 2.72824536e-07
Iter: 1643 loss: 2.7268851e-07
Iter: 1644 loss: 2.73056457e-07
Iter: 1645 loss: 2.72626949e-07
Iter: 1646 loss: 2.72523721e-07
Iter: 1647 loss: 2.72306863e-07
Iter: 1648 loss: 2.77032029e-07
Iter: 1649 loss: 2.72309052e-07
Iter: 1650 loss: 2.72031656e-07
Iter: 1651 loss: 2.73009107e-07
Iter: 1652 loss: 2.71971317e-07
Iter: 1653 loss: 2.71770972e-07
Iter: 1654 loss: 2.72904884e-07
Iter: 1655 loss: 2.7174508e-07
Iter: 1656 loss: 2.71564488e-07
Iter: 1657 loss: 2.72993418e-07
Iter: 1658 loss: 2.71542319e-07
Iter: 1659 loss: 2.71407657e-07
Iter: 1660 loss: 2.71194835e-07
Iter: 1661 loss: 2.71178237e-07
Iter: 1662 loss: 2.70990256e-07
Iter: 1663 loss: 2.72627e-07
Iter: 1664 loss: 2.70971242e-07
Iter: 1665 loss: 2.70701094e-07
Iter: 1666 loss: 2.70737928e-07
Iter: 1667 loss: 2.70484747e-07
Iter: 1668 loss: 2.70277667e-07
Iter: 1669 loss: 2.70340337e-07
Iter: 1670 loss: 2.70120211e-07
Iter: 1671 loss: 2.69978017e-07
Iter: 1672 loss: 2.6997651e-07
Iter: 1673 loss: 2.69828178e-07
Iter: 1674 loss: 2.69891842e-07
Iter: 1675 loss: 2.69725689e-07
Iter: 1676 loss: 2.69605778e-07
Iter: 1677 loss: 2.69484303e-07
Iter: 1678 loss: 2.69457701e-07
Iter: 1679 loss: 2.69354331e-07
Iter: 1680 loss: 2.69322385e-07
Iter: 1681 loss: 2.69235755e-07
Iter: 1682 loss: 2.69047206e-07
Iter: 1683 loss: 2.72407704e-07
Iter: 1684 loss: 2.69056841e-07
Iter: 1685 loss: 2.68846037e-07
Iter: 1686 loss: 2.68925675e-07
Iter: 1687 loss: 2.68708277e-07
Iter: 1688 loss: 2.68422468e-07
Iter: 1689 loss: 2.69524264e-07
Iter: 1690 loss: 2.68351897e-07
Iter: 1691 loss: 2.6820166e-07
Iter: 1692 loss: 2.70207948e-07
Iter: 1693 loss: 2.68194526e-07
Iter: 1694 loss: 2.68040424e-07
Iter: 1695 loss: 2.68685824e-07
Iter: 1696 loss: 2.68009217e-07
Iter: 1697 loss: 2.67889504e-07
Iter: 1698 loss: 2.67699903e-07
Iter: 1699 loss: 2.67706355e-07
Iter: 1700 loss: 2.67639e-07
Iter: 1701 loss: 2.67617509e-07
Iter: 1702 loss: 2.6751286e-07
Iter: 1703 loss: 2.67344205e-07
Iter: 1704 loss: 2.67347e-07
Iter: 1705 loss: 2.67222845e-07
Iter: 1706 loss: 2.67957404e-07
Iter: 1707 loss: 2.67212243e-07
Iter: 1708 loss: 2.67084602e-07
Iter: 1709 loss: 2.6806407e-07
Iter: 1710 loss: 2.67079571e-07
Iter: 1711 loss: 2.66997546e-07
Iter: 1712 loss: 2.66784241e-07
Iter: 1713 loss: 2.68418916e-07
Iter: 1714 loss: 2.66738851e-07
Iter: 1715 loss: 2.66732314e-07
Iter: 1716 loss: 2.66636278e-07
Iter: 1717 loss: 2.66568094e-07
Iter: 1718 loss: 2.66444033e-07
Iter: 1719 loss: 2.69379058e-07
Iter: 1720 loss: 2.66433346e-07
Iter: 1721 loss: 2.66289732e-07
Iter: 1722 loss: 2.66398871e-07
Iter: 1723 loss: 2.66213e-07
Iter: 1724 loss: 2.6602703e-07
Iter: 1725 loss: 2.67399514e-07
Iter: 1726 loss: 2.66009181e-07
Iter: 1727 loss: 2.65875201e-07
Iter: 1728 loss: 2.66992373e-07
Iter: 1729 loss: 2.65859057e-07
Iter: 1730 loss: 2.65738265e-07
Iter: 1731 loss: 2.65868181e-07
Iter: 1732 loss: 2.65661953e-07
Iter: 1733 loss: 2.65540791e-07
Iter: 1734 loss: 2.6569586e-07
Iter: 1735 loss: 2.65479514e-07
Iter: 1736 loss: 2.65330812e-07
Iter: 1737 loss: 2.66793364e-07
Iter: 1738 loss: 2.65330925e-07
Iter: 1739 loss: 2.65243528e-07
Iter: 1740 loss: 2.65029371e-07
Iter: 1741 loss: 2.68246879e-07
Iter: 1742 loss: 2.65038352e-07
Iter: 1743 loss: 2.6497105e-07
Iter: 1744 loss: 2.64928161e-07
Iter: 1745 loss: 2.64824905e-07
Iter: 1746 loss: 2.64795261e-07
Iter: 1747 loss: 2.64744472e-07
Iter: 1748 loss: 2.6463357e-07
Iter: 1749 loss: 2.64863701e-07
Iter: 1750 loss: 2.64592501e-07
Iter: 1751 loss: 2.64394686e-07
Iter: 1752 loss: 2.64616773e-07
Iter: 1753 loss: 2.64327412e-07
Iter: 1754 loss: 2.64188941e-07
Iter: 1755 loss: 2.64100436e-07
Iter: 1756 loss: 2.64043422e-07
Iter: 1757 loss: 2.6384987e-07
Iter: 1758 loss: 2.64428849e-07
Iter: 1759 loss: 2.6378143e-07
Iter: 1760 loss: 2.6361559e-07
Iter: 1761 loss: 2.64047685e-07
Iter: 1762 loss: 2.63552238e-07
Iter: 1763 loss: 2.63400892e-07
Iter: 1764 loss: 2.65538972e-07
Iter: 1765 loss: 2.6340652e-07
Iter: 1766 loss: 2.63316224e-07
Iter: 1767 loss: 2.63354508e-07
Iter: 1768 loss: 2.6325128e-07
Iter: 1769 loss: 2.63165589e-07
Iter: 1770 loss: 2.6377603e-07
Iter: 1771 loss: 2.63165873e-07
Iter: 1772 loss: 2.63054346e-07
Iter: 1773 loss: 2.6297144e-07
Iter: 1774 loss: 2.62944184e-07
Iter: 1775 loss: 2.62819412e-07
Iter: 1776 loss: 2.63825967e-07
Iter: 1777 loss: 2.62814808e-07
Iter: 1778 loss: 2.62688332e-07
Iter: 1779 loss: 2.6282089e-07
Iter: 1780 loss: 2.62609205e-07
Iter: 1781 loss: 2.62489834e-07
Iter: 1782 loss: 2.62422731e-07
Iter: 1783 loss: 2.62368587e-07
Iter: 1784 loss: 2.62199762e-07
Iter: 1785 loss: 2.62206612e-07
Iter: 1786 loss: 2.62058279e-07
Iter: 1787 loss: 2.61766701e-07
Iter: 1788 loss: 2.66550188e-07
Iter: 1789 loss: 2.61768321e-07
Iter: 1790 loss: 2.61496524e-07
Iter: 1791 loss: 2.63325717e-07
Iter: 1792 loss: 2.61472934e-07
Iter: 1793 loss: 2.61287084e-07
Iter: 1794 loss: 2.6228409e-07
Iter: 1795 loss: 2.61261448e-07
Iter: 1796 loss: 2.61127866e-07
Iter: 1797 loss: 2.61738307e-07
Iter: 1798 loss: 2.61087621e-07
Iter: 1799 loss: 2.60935167e-07
Iter: 1800 loss: 2.61035041e-07
Iter: 1801 loss: 2.60842569e-07
Iter: 1802 loss: 2.60675279e-07
Iter: 1803 loss: 2.61804388e-07
Iter: 1804 loss: 2.60668969e-07
Iter: 1805 loss: 2.60526292e-07
Iter: 1806 loss: 2.60527202e-07
Iter: 1807 loss: 2.60425878e-07
Iter: 1808 loss: 2.60240881e-07
Iter: 1809 loss: 2.6058126e-07
Iter: 1810 loss: 2.60175511e-07
Iter: 1811 loss: 2.60009244e-07
Iter: 1812 loss: 2.62187569e-07
Iter: 1813 loss: 2.60008903e-07
Iter: 1814 loss: 2.59906699e-07
Iter: 1815 loss: 2.59682309e-07
Iter: 1816 loss: 2.62713911e-07
Iter: 1817 loss: 2.59672959e-07
Iter: 1818 loss: 2.5949754e-07
Iter: 1819 loss: 2.59491571e-07
Iter: 1820 loss: 2.59354238e-07
Iter: 1821 loss: 2.5927352e-07
Iter: 1822 loss: 2.59213522e-07
Iter: 1823 loss: 2.5904194e-07
Iter: 1824 loss: 2.59049727e-07
Iter: 1825 loss: 2.58924e-07
Iter: 1826 loss: 2.58665153e-07
Iter: 1827 loss: 2.5958991e-07
Iter: 1828 loss: 2.58597936e-07
Iter: 1829 loss: 2.58340037e-07
Iter: 1830 loss: 2.59065246e-07
Iter: 1831 loss: 2.58278732e-07
Iter: 1832 loss: 2.58042917e-07
Iter: 1833 loss: 2.59339402e-07
Iter: 1834 loss: 2.5801512e-07
Iter: 1835 loss: 2.57863519e-07
Iter: 1836 loss: 2.58578979e-07
Iter: 1837 loss: 2.57835865e-07
Iter: 1838 loss: 2.57699412e-07
Iter: 1839 loss: 2.57944919e-07
Iter: 1840 loss: 2.57653767e-07
Iter: 1841 loss: 2.57538375e-07
Iter: 1842 loss: 2.5763e-07
Iter: 1843 loss: 2.57480622e-07
Iter: 1844 loss: 2.57371369e-07
Iter: 1845 loss: 2.5735909e-07
Iter: 1846 loss: 2.57291902e-07
Iter: 1847 loss: 2.57128448e-07
Iter: 1848 loss: 2.60351896e-07
Iter: 1849 loss: 2.57120121e-07
Iter: 1850 loss: 2.56981195e-07
Iter: 1851 loss: 2.58441332e-07
Iter: 1852 loss: 2.56989267e-07
Iter: 1853 loss: 2.5683255e-07
Iter: 1854 loss: 2.57087549e-07
Iter: 1855 loss: 2.56767805e-07
Iter: 1856 loss: 2.56625185e-07
Iter: 1857 loss: 2.56451528e-07
Iter: 1858 loss: 2.56421856e-07
Iter: 1859 loss: 2.56188144e-07
Iter: 1860 loss: 2.58348393e-07
Iter: 1861 loss: 2.56166544e-07
Iter: 1862 loss: 2.56033815e-07
Iter: 1863 loss: 2.56807795e-07
Iter: 1864 loss: 2.55996724e-07
Iter: 1865 loss: 2.5586624e-07
Iter: 1866 loss: 2.56530626e-07
Iter: 1867 loss: 2.55840234e-07
Iter: 1868 loss: 2.55748262e-07
Iter: 1869 loss: 2.55959151e-07
Iter: 1870 loss: 2.55720636e-07
Iter: 1871 loss: 2.55605e-07
Iter: 1872 loss: 2.56020257e-07
Iter: 1873 loss: 2.55584041e-07
Iter: 1874 loss: 2.55493831e-07
Iter: 1875 loss: 2.55433775e-07
Iter: 1876 loss: 2.55398874e-07
Iter: 1877 loss: 2.55288398e-07
Iter: 1878 loss: 2.56897863e-07
Iter: 1879 loss: 2.55286636e-07
Iter: 1880 loss: 2.5517582e-07
Iter: 1881 loss: 2.55020211e-07
Iter: 1882 loss: 2.55008388e-07
Iter: 1883 loss: 2.54813443e-07
Iter: 1884 loss: 2.55242384e-07
Iter: 1885 loss: 2.54734914e-07
Iter: 1886 loss: 2.54493102e-07
Iter: 1887 loss: 2.56001925e-07
Iter: 1888 loss: 2.5446451e-07
Iter: 1889 loss: 2.54285453e-07
Iter: 1890 loss: 2.5416918e-07
Iter: 1891 loss: 2.54114127e-07
Iter: 1892 loss: 2.53934672e-07
Iter: 1893 loss: 2.55506109e-07
Iter: 1894 loss: 2.53929755e-07
Iter: 1895 loss: 2.53768633e-07
Iter: 1896 loss: 2.54599371e-07
Iter: 1897 loss: 2.53743394e-07
Iter: 1898 loss: 2.53617031e-07
Iter: 1899 loss: 2.53832866e-07
Iter: 1900 loss: 2.53568288e-07
Iter: 1901 loss: 2.53431807e-07
Iter: 1902 loss: 2.5379623e-07
Iter: 1903 loss: 2.5338349e-07
Iter: 1904 loss: 2.5327742e-07
Iter: 1905 loss: 2.54064588e-07
Iter: 1906 loss: 2.532488e-07
Iter: 1907 loss: 2.53121868e-07
Iter: 1908 loss: 2.52983682e-07
Iter: 1909 loss: 2.52975667e-07
Iter: 1910 loss: 2.52800419e-07
Iter: 1911 loss: 2.54090367e-07
Iter: 1912 loss: 2.52789107e-07
Iter: 1913 loss: 2.52565883e-07
Iter: 1914 loss: 2.52961854e-07
Iter: 1915 loss: 2.52469874e-07
Iter: 1916 loss: 2.52314123e-07
Iter: 1917 loss: 2.52275129e-07
Iter: 1918 loss: 2.52164796e-07
Iter: 1919 loss: 2.52006259e-07
Iter: 1920 loss: 2.52008732e-07
Iter: 1921 loss: 2.51881687e-07
Iter: 1922 loss: 2.51706382e-07
Iter: 1923 loss: 2.51699504e-07
Iter: 1924 loss: 2.51524909e-07
Iter: 1925 loss: 2.52159708e-07
Iter: 1926 loss: 2.5146656e-07
Iter: 1927 loss: 2.51288895e-07
Iter: 1928 loss: 2.52650636e-07
Iter: 1929 loss: 2.51279118e-07
Iter: 1930 loss: 2.51161396e-07
Iter: 1931 loss: 2.51351935e-07
Iter: 1932 loss: 2.51091961e-07
Iter: 1933 loss: 2.50966423e-07
Iter: 1934 loss: 2.51370409e-07
Iter: 1935 loss: 2.50922426e-07
Iter: 1936 loss: 2.50778953e-07
Iter: 1937 loss: 2.51599488e-07
Iter: 1938 loss: 2.50761559e-07
Iter: 1939 loss: 2.50637925e-07
Iter: 1940 loss: 2.50527393e-07
Iter: 1941 loss: 2.50501444e-07
Iter: 1942 loss: 2.50306414e-07
Iter: 1943 loss: 2.50907533e-07
Iter: 1944 loss: 2.50271853e-07
Iter: 1945 loss: 2.50063522e-07
Iter: 1946 loss: 2.5147466e-07
Iter: 1947 loss: 2.50044e-07
Iter: 1948 loss: 2.49915388e-07
Iter: 1949 loss: 2.49705153e-07
Iter: 1950 loss: 2.49703646e-07
Iter: 1951 loss: 2.49569666e-07
Iter: 1952 loss: 2.49543575e-07
Iter: 1953 loss: 2.49434947e-07
Iter: 1954 loss: 2.4924924e-07
Iter: 1955 loss: 2.4924006e-07
Iter: 1956 loss: 2.49033974e-07
Iter: 1957 loss: 2.49280134e-07
Iter: 1958 loss: 2.48939784e-07
Iter: 1959 loss: 2.4881524e-07
Iter: 1960 loss: 2.48806884e-07
Iter: 1961 loss: 2.48695585e-07
Iter: 1962 loss: 2.48732476e-07
Iter: 1963 loss: 2.48640021e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8
+ date
Wed Oct 21 23:23:31 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/500_500_500_500_1 --function f1 --psi -2 --phi 0.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c99269d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c991f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c991dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c99b8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c99b8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c9837d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c9876a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c980e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c9817048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c97d6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c96ec9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c970b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c970bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c9793378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c9749950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c9749a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c97560d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c96d6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03bec976a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03becb7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03bec1c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03bec36730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03bebd5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03bebf3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03bebf32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03beba3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03beb02950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03beaf5510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03beaf5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03beafa2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03beafa268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03bea93158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03bea7d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03bea97f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03c96926a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03bea038c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.018544601
test_loss: 0.018155389
train_loss: 0.0071488773
test_loss: 0.0070306826
train_loss: 0.004186214
test_loss: 0.0044657476
train_loss: 0.0034575996
test_loss: 0.003762505
train_loss: 0.0028773812
test_loss: 0.0033133938
train_loss: 0.0027947694
test_loss: 0.0032588271
train_loss: 0.0031822142
test_loss: 0.0033916277
train_loss: 0.0026904987
test_loss: 0.0031362132
train_loss: 0.002494314
test_loss: 0.0030773778
train_loss: 0.0025905196
test_loss: 0.0029319439
train_loss: 0.0023986015
test_loss: 0.0031445269
train_loss: 0.0025227515
test_loss: 0.0030787622
train_loss: 0.0025530965
test_loss: 0.0028325552
train_loss: 0.0026178206
test_loss: 0.0029868896
train_loss: 0.0027668397
test_loss: 0.003138929
train_loss: 0.0027387955
test_loss: 0.0032517407
train_loss: 0.0026967833
test_loss: 0.0031025563
train_loss: 0.0024234664
test_loss: 0.0031304806
train_loss: 0.0024057587
test_loss: 0.002945619
train_loss: 0.0026600803
test_loss: 0.002966871
train_loss: 0.0022880416
test_loss: 0.0030424271
train_loss: 0.0023418958
test_loss: 0.0029591802
train_loss: 0.0024306145
test_loss: 0.003086567
train_loss: 0.0024175444
test_loss: 0.0029264975
train_loss: 0.0024415182
test_loss: 0.002849105
train_loss: 0.0024516
test_loss: 0.002754978
train_loss: 0.0023409503
test_loss: 0.002750036
train_loss: 0.002305277
test_loss: 0.003036652
train_loss: 0.0022929534
test_loss: 0.002781835
train_loss: 0.0028005377
test_loss: 0.003061939
train_loss: 0.002651825
test_loss: 0.0029628214
train_loss: 0.00243376
test_loss: 0.0030090143
train_loss: 0.0022944151
test_loss: 0.0027887498
train_loss: 0.0022828004
test_loss: 0.0029178155
train_loss: 0.0022993078
test_loss: 0.003009597
train_loss: 0.0022669937
test_loss: 0.0028111206
train_loss: 0.0024555735
test_loss: 0.0028799549
train_loss: 0.0025528981
test_loss: 0.0028591752
train_loss: 0.0024329228
test_loss: 0.002939235
train_loss: 0.002299821
test_loss: 0.0027997696
train_loss: 0.0022580805
test_loss: 0.0029028729
train_loss: 0.0022478525
test_loss: 0.0028494028
train_loss: 0.0025283922
test_loss: 0.002756309
train_loss: 0.0026638263
test_loss: 0.002939393
train_loss: 0.00221886
test_loss: 0.0027023887
train_loss: 0.002137429
test_loss: 0.002917074
train_loss: 0.0024159094
test_loss: 0.0028618802
train_loss: 0.002358302
test_loss: 0.0027240277
train_loss: 0.0022181147
test_loss: 0.0026794372
train_loss: 0.002271455
test_loss: 0.0028919168
train_loss: 0.0023331414
test_loss: 0.0027138654
train_loss: 0.0022259247
test_loss: 0.0028443842
train_loss: 0.0023714725
test_loss: 0.0028187
train_loss: 0.0021636577
test_loss: 0.0026852058
train_loss: 0.0025010568
test_loss: 0.0028782855
train_loss: 0.0024403702
test_loss: 0.0029775228
train_loss: 0.00233474
test_loss: 0.002785551
train_loss: 0.0024995673
test_loss: 0.0028213386
train_loss: 0.0022156318
test_loss: 0.0026799352
train_loss: 0.002149445
test_loss: 0.0027792477
train_loss: 0.0023109058
test_loss: 0.0027007079
train_loss: 0.0021953078
test_loss: 0.002826131
train_loss: 0.0022606258
test_loss: 0.0027120477
train_loss: 0.0021743416
test_loss: 0.0027669724
train_loss: 0.0023547926
test_loss: 0.0027704812
train_loss: 0.002132137
test_loss: 0.0028397886
train_loss: 0.0025032354
test_loss: 0.0028660242
train_loss: 0.0021846122
test_loss: 0.0027939996
train_loss: 0.00224417
test_loss: 0.0029636153
train_loss: 0.0024138077
test_loss: 0.0031295908
train_loss: 0.002265814
test_loss: 0.0028193167
train_loss: 0.002358533
test_loss: 0.0027207022
train_loss: 0.002727446
test_loss: 0.0028699986
train_loss: 0.0027401107
test_loss: 0.0030566752
train_loss: 0.0021629902
test_loss: 0.0027841465
train_loss: 0.0025569256
test_loss: 0.0027988907
train_loss: 0.0024974246
test_loss: 0.0027551928
train_loss: 0.0027865125
test_loss: 0.002990136
train_loss: 0.0023813942
test_loss: 0.0027381706
train_loss: 0.0027275723
test_loss: 0.0028357892
train_loss: 0.002214733
test_loss: 0.0026825438
train_loss: 0.0022677563
test_loss: 0.0028396095
train_loss: 0.00224942
test_loss: 0.0026977875
train_loss: 0.0020919726
test_loss: 0.0027427883
train_loss: 0.0022527152
test_loss: 0.0027742048
train_loss: 0.0023395987
test_loss: 0.002756679
train_loss: 0.002368786
test_loss: 0.0029182178
train_loss: 0.0020856925
test_loss: 0.0027699799
train_loss: 0.0023146041
test_loss: 0.0027534633
train_loss: 0.0024372572
test_loss: 0.003110245
train_loss: 0.002320803
test_loss: 0.0031844531
train_loss: 0.0024733392
test_loss: 0.0028788438
train_loss: 0.0021504671
test_loss: 0.0027640453
train_loss: 0.0022227035
test_loss: 0.0029305948
train_loss: 0.0021762238
test_loss: 0.0028129753
train_loss: 0.0022910312
test_loss: 0.0027052113
train_loss: 0.002225762
test_loss: 0.0027587318
train_loss: 0.0022156145
test_loss: 0.0027484775
train_loss: 0.00234674
test_loss: 0.0027692437
train_loss: 0.0023198517
test_loss: 0.002778239
train_loss: 0.002156551
test_loss: 0.00266941
train_loss: 0.0021465104
test_loss: 0.002578297
train_loss: 0.0024136314
test_loss: 0.002769143
train_loss: 0.0020443655
test_loss: 0.0025489975
train_loss: 0.0020938378
test_loss: 0.0027288436
train_loss: 0.002342246
test_loss: 0.0028390428
train_loss: 0.0022950794
test_loss: 0.00269401
train_loss: 0.0022373265
test_loss: 0.0027666886
train_loss: 0.0022108771
test_loss: 0.0026550672
train_loss: 0.0022925106
test_loss: 0.002741828
train_loss: 0.0022368701
test_loss: 0.0027401452
train_loss: 0.0023054213
test_loss: 0.002753771
train_loss: 0.0023354418
test_loss: 0.0027543143
train_loss: 0.002286745
test_loss: 0.0027088223
train_loss: 0.0023500496
test_loss: 0.0026275653
train_loss: 0.0020434295
test_loss: 0.002644468
train_loss: 0.0023480738
test_loss: 0.0027144398
train_loss: 0.0021022963
test_loss: 0.0025747798
train_loss: 0.0021833694
test_loss: 0.0027725683
train_loss: 0.0022342666
test_loss: 0.0027865837
train_loss: 0.0022095614
test_loss: 0.002556997
train_loss: 0.0023586333
test_loss: 0.0031573588
train_loss: 0.0022440485
test_loss: 0.0027614043
train_loss: 0.0021697886
test_loss: 0.0025861037
train_loss: 0.002145226
test_loss: 0.0027883658
train_loss: 0.0021023923
test_loss: 0.0027724474
train_loss: 0.0020489956
test_loss: 0.002672224
train_loss: 0.0022156897
test_loss: 0.0026674843
train_loss: 0.0020166948
test_loss: 0.0028590406
train_loss: 0.0021716056
test_loss: 0.002763542
train_loss: 0.0022465931
test_loss: 0.002654212
train_loss: 0.0019465236
test_loss: 0.0026133887
train_loss: 0.0021509146
test_loss: 0.0028972712
train_loss: 0.0023645116
test_loss: 0.0027230699
train_loss: 0.0023134851
test_loss: 0.0027135739
train_loss: 0.0021038833
test_loss: 0.0025830218
train_loss: 0.0021720144
test_loss: 0.0027526228
train_loss: 0.002073068
test_loss: 0.002686869
train_loss: 0.002098841
test_loss: 0.0028734684
train_loss: 0.002400869
test_loss: 0.0027449261
train_loss: 0.0019068026
test_loss: 0.0026302985
train_loss: 0.0022600035
test_loss: 0.002721361
train_loss: 0.002486043
test_loss: 0.0029098473
train_loss: 0.0023744958
test_loss: 0.0027309875
train_loss: 0.0020893794
test_loss: 0.0026264198
train_loss: 0.0020869593
test_loss: 0.0025985476
train_loss: 0.002388289
test_loss: 0.0028060044
train_loss: 0.0021067795
test_loss: 0.0026150553
train_loss: 0.0021588833
test_loss: 0.0028031443
train_loss: 0.0021360375
test_loss: 0.0026410061
train_loss: 0.0021220993
test_loss: 0.0026426539
train_loss: 0.0024183192
test_loss: 0.0027252054
train_loss: 0.0023075896
test_loss: 0.0028970058
train_loss: 0.0023853746
test_loss: 0.0028966488
train_loss: 0.002204977
test_loss: 0.002895116
train_loss: 0.0021612307
test_loss: 0.002742491
train_loss: 0.0021766862
test_loss: 0.002635853
train_loss: 0.0019665123
test_loss: 0.0026858812
train_loss: 0.0023746123
test_loss: 0.0030394713
train_loss: 0.002319825
test_loss: 0.0027071899
train_loss: 0.0023198887
test_loss: 0.002967038
train_loss: 0.002199023
test_loss: 0.0025273545
train_loss: 0.0022491184
test_loss: 0.0031048518
train_loss: 0.002361281
test_loss: 0.0028675732
train_loss: 0.002248809
test_loss: 0.0028520674
train_loss: 0.0021928973
test_loss: 0.0029771172
train_loss: 0.0021344232
test_loss: 0.0028017422
train_loss: 0.0022991518
test_loss: 0.0026651826
train_loss: 0.0020308432
test_loss: 0.0026662853
train_loss: 0.0022229243
test_loss: 0.002802672
train_loss: 0.0021342211
test_loss: 0.0026785312
train_loss: 0.0021199249
test_loss: 0.0027579502
train_loss: 0.0021417239
test_loss: 0.002668096
train_loss: 0.002132366
test_loss: 0.0027916536
train_loss: 0.0020204931
test_loss: 0.0026112415
train_loss: 0.0021143258
test_loss: 0.0026778916
train_loss: 0.0022514476
test_loss: 0.0026988422
train_loss: 0.0023181
test_loss: 0.0027413466
train_loss: 0.002175788
test_loss: 0.0027413343
train_loss: 0.0020347594
test_loss: 0.0026004915
train_loss: 0.0020732845
test_loss: 0.0027627957
train_loss: 0.0021822779
test_loss: 0.0026480004
train_loss: 0.0020286927
test_loss: 0.0025712985
train_loss: 0.0021687404
test_loss: 0.0027028432
train_loss: 0.0021299168
test_loss: 0.0024935915
train_loss: 0.0021131947
test_loss: 0.0026314235
train_loss: 0.0021603298
test_loss: 0.002899911
train_loss: 0.0022515948
test_loss: 0.0026593318
train_loss: 0.0020619887
test_loss: 0.0025986398
train_loss: 0.0024788314
test_loss: 0.0028553521
train_loss: 0.002034903
test_loss: 0.0026769547
train_loss: 0.001982993
test_loss: 0.0026908775
train_loss: 0.0020551872
test_loss: 0.002653051
train_loss: 0.0023696553
test_loss: 0.0027961526
train_loss: 0.002015468
test_loss: 0.0025250472
train_loss: 0.0020931424
test_loss: 0.0027616578
train_loss: 0.0020963051
test_loss: 0.002766077
train_loss: 0.0020302748
test_loss: 0.0025747942
train_loss: 0.0020215986
test_loss: 0.0027922415
train_loss: 0.0021114473
test_loss: 0.0026868582
train_loss: 0.0022076138
test_loss: 0.0026314915
train_loss: 0.0020450496
test_loss: 0.0027358145
train_loss: 0.002094148
test_loss: 0.002650227
train_loss: 0.0019932364
test_loss: 0.0026536973
train_loss: 0.0020234822
test_loss: 0.0027216831
train_loss: 0.0021900013
test_loss: 0.0026912347
train_loss: 0.0019910245
test_loss: 0.0026401798
train_loss: 0.0021380563
test_loss: 0.0028047762
train_loss: 0.0021457945
test_loss: 0.0026379453
train_loss: 0.0022631271
test_loss: 0.0027404192
train_loss: 0.0024779
test_loss: 0.0031912802
train_loss: 0.0021365997
test_loss: 0.002772634
train_loss: 0.0020717308
test_loss: 0.002620447
train_loss: 0.0020662984
test_loss: 0.0028359247
train_loss: 0.0022095416
test_loss: 0.0026461107
train_loss: 0.0021085422
test_loss: 0.0027200473
train_loss: 0.0021773865
test_loss: 0.0025803181
train_loss: 0.0022072655
test_loss: 0.0028082007
train_loss: 0.0023788952
test_loss: 0.0026798355
train_loss: 0.002243923
test_loss: 0.002765659
train_loss: 0.0022119107
test_loss: 0.002697296
train_loss: 0.0021025576
test_loss: 0.0025565552
train_loss: 0.002038212
test_loss: 0.0025000542
train_loss: 0.0022688997
test_loss: 0.0026134425
train_loss: 0.0020959538
test_loss: 0.0026935365
train_loss: 0.0021215116
test_loss: 0.0026410145
train_loss: 0.0021976584
test_loss: 0.002785057
train_loss: 0.0020210268
test_loss: 0.0026517708
train_loss: 0.0023644764
test_loss: 0.0029223007
train_loss: 0.0021673061
test_loss: 0.0027035181
train_loss: 0.0021450464
test_loss: 0.0026112492
train_loss: 0.0021328614
test_loss: 0.0028283317
train_loss: 0.0020879412
test_loss: 0.0025911706
train_loss: 0.0020133154
test_loss: 0.0024988381
train_loss: 0.0021838206
test_loss: 0.0026115405
train_loss: 0.0020931468
test_loss: 0.0026549648
train_loss: 0.0019902189
test_loss: 0.002688851
train_loss: 0.0020936977
test_loss: 0.002643005
train_loss: 0.002166565
test_loss: 0.002658657
train_loss: 0.0020918667
test_loss: 0.0024562308
train_loss: 0.0019025325
test_loss: 0.0026710448
train_loss: 0.0020202769
test_loss: 0.00255216
train_loss: 0.002214043
test_loss: 0.0026957334
train_loss: 0.0021040894
test_loss: 0.0024368253
train_loss: 0.0019860575
test_loss: 0.0024862855
train_loss: 0.0020953722
test_loss: 0.0027013642
train_loss: 0.0021024428
test_loss: 0.002532057
train_loss: 0.002437459
test_loss: 0.0028928283
train_loss: 0.0020512156
test_loss: 0.0026808318
train_loss: 0.0019420558
test_loss: 0.0026443466
train_loss: 0.0024624236
test_loss: 0.0026426134
train_loss: 0.001994807
test_loss: 0.002597861
train_loss: 0.0020106514
test_loss: 0.0025642167
train_loss: 0.0020264012
test_loss: 0.0027906664
train_loss: 0.0020149506
test_loss: 0.0025050556
train_loss: 0.002115623
test_loss: 0.0025358412
train_loss: 0.0021709907
test_loss: 0.0025742762
train_loss: 0.002247782
test_loss: 0.002532214
train_loss: 0.0020105592
test_loss: 0.0024343508
train_loss: 0.0019363418
test_loss: 0.002547776
train_loss: 0.0019811415
test_loss: 0.0026489187
train_loss: 0.0024519034
test_loss: 0.0027514575
train_loss: 0.0021429858
test_loss: 0.0025947355
train_loss: 0.0020002471
test_loss: 0.0025402822
train_loss: 0.0023914347
test_loss: 0.0026894016
train_loss: 0.0022570426
test_loss: 0.0026966895
train_loss: 0.0020256513
test_loss: 0.0026027602
train_loss: 0.0022187044
test_loss: 0.0026126024
train_loss: 0.0021758322
test_loss: 0.002642032
train_loss: 0.0023308685
test_loss: 0.002736067
train_loss: 0.0020957834
test_loss: 0.0028113008
train_loss: 0.0020621922
test_loss: 0.0026234228
train_loss: 0.0023010238
test_loss: 0.0025080366
train_loss: 0.0022040983
test_loss: 0.0026052962
train_loss: 0.0022478031
test_loss: 0.0026695223
train_loss: 0.002406112
test_loss: 0.002821355
train_loss: 0.00207278
test_loss: 0.0027176633
train_loss: 0.0020911398
test_loss: 0.0025649574
train_loss: 0.0021944835
test_loss: 0.0026191995
train_loss: 0.0019739794
test_loss: 0.0025252725
train_loss: 0.0021743348
test_loss: 0.0025833051
train_loss: 0.0019201941
test_loss: 0.0024355645
train_loss: 0.0021671476
test_loss: 0.00261508
train_loss: 0.0021432978
test_loss: 0.0026454043
train_loss: 0.0022831094
test_loss: 0.0027380663
train_loss: 0.0023232743
test_loss: 0.0026690748
train_loss: 0.0020627326
test_loss: 0.0026182968
train_loss: 0.0020679352
test_loss: 0.0026658373
train_loss: 0.002075309
test_loss: 0.0027160873
train_loss: 0.0019947893
test_loss: 0.0026101961
train_loss: 0.0019620964
test_loss: 0.0024477418
train_loss: 0.0020148247
test_loss: 0.0024837935
train_loss: 0.0020581218
test_loss: 0.0024546762
train_loss: 0.0020196545
test_loss: 0.0025233414
train_loss: 0.0021014002
test_loss: 0.002606029
train_loss: 0.002143753
test_loss: 0.0025610547
train_loss: 0.0023071365
test_loss: 0.0028965147
train_loss: 0.002156728
test_loss: 0.002642377
train_loss: 0.0020962683
test_loss: 0.0025645886
train_loss: 0.0020178703
test_loss: 0.002494829
train_loss: 0.0018882976
test_loss: 0.0025849992
train_loss: 0.0019656455
test_loss: 0.0024603934
train_loss: 0.0019602343
test_loss: 0.0025759113
train_loss: 0.0020208003
test_loss: 0.0025693092
train_loss: 0.0017450962
test_loss: 0.0024468675
train_loss: 0.0023412986
test_loss: 0.0026383072
train_loss: 0.0021255463
test_loss: 0.0026158125
train_loss: 0.0021418517
test_loss: 0.0026356897
train_loss: 0.0019745617
test_loss: 0.002582145
train_loss: 0.0021220052
test_loss: 0.0027733706
train_loss: 0.002327783
test_loss: 0.0026331362
train_loss: 0.0019418367
test_loss: 0.002463024
train_loss: 0.0020221113
test_loss: 0.0025408696
train_loss: 0.0020050243
test_loss: 0.002473346
train_loss: 0.0020232827
test_loss: 0.002582517
train_loss: 0.0020508524
test_loss: 0.002605431
train_loss: 0.002107554
test_loss: 0.002533548
train_loss: 0.0022457936
test_loss: 0.0025135695
train_loss: 0.0023548775
test_loss: 0.0025189233
train_loss: 0.0021303515
test_loss: 0.00275058
train_loss: 0.0020983655
test_loss: 0.0026943234
train_loss: 0.0020446426
test_loss: 0.0024787462
train_loss: 0.0018200236
test_loss: 0.002419735
train_loss: 0.002077886
test_loss: 0.002491842
train_loss: 0.002498446
test_loss: 0.002650032
train_loss: 0.0021393572
test_loss: 0.002711133
train_loss: 0.0020220173
test_loss: 0.0028079038
train_loss: 0.0020452575
test_loss: 0.0025146985
train_loss: 0.0019258021
test_loss: 0.0024502205
train_loss: 0.0020086167
test_loss: 0.0027052874
train_loss: 0.0020937398
test_loss: 0.002756983
train_loss: 0.0019542295
test_loss: 0.002471923
train_loss: 0.0019606063
test_loss: 0.0024538296
train_loss: 0.001984254
test_loss: 0.0025245163
train_loss: 0.0020603803
test_loss: 0.0026125843
train_loss: 0.0019868575
test_loss: 0.0025441686
train_loss: 0.002316128
test_loss: 0.0025643066
train_loss: 0.00207044
test_loss: 0.0027620392/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

train_loss: 0.0021807551
test_loss: 0.002666119
train_loss: 0.002323283
test_loss: 0.0026140884
train_loss: 0.002271026
test_loss: 0.0025727837
train_loss: 0.0021422892
test_loss: 0.0024562413
train_loss: 0.0021367616
test_loss: 0.002525956
train_loss: 0.002383057
test_loss: 0.0025901692
train_loss: 0.002259324
test_loss: 0.0025887506
train_loss: 0.0022284081
test_loss: 0.0028118526
train_loss: 0.0022630838
test_loss: 0.0027107971
train_loss: 0.0022686268
test_loss: 0.002797709
train_loss: 0.0024200666
test_loss: 0.00275529
train_loss: 0.0022262265
test_loss: 0.002615632
train_loss: 0.0021309222
test_loss: 0.0025793733
train_loss: 0.0019513888
test_loss: 0.0025399541
train_loss: 0.002020162
test_loss: 0.0028373213
train_loss: 0.0020390162
test_loss: 0.0025095278
train_loss: 0.0020214047
test_loss: 0.0025126336
train_loss: 0.0019219024
test_loss: 0.0024603743
train_loss: 0.002101107
test_loss: 0.0025234409
train_loss: 0.002017024
test_loss: 0.002539514
train_loss: 0.0021050083
test_loss: 0.0025181829
train_loss: 0.0020092456
test_loss: 0.0027541192
train_loss: 0.0026615325
test_loss: 0.0026805517
train_loss: 0.0021803065
test_loss: 0.0026786954
train_loss: 0.0021344076
test_loss: 0.0025753737
train_loss: 0.0020349186
test_loss: 0.002495932
train_loss: 0.0020800068
test_loss: 0.00269618
train_loss: 0.002168566
test_loss: 0.002657058
train_loss: 0.002227849
test_loss: 0.0025362289
train_loss: 0.0020293836
test_loss: 0.0024441415
train_loss: 0.0019429345
test_loss: 0.0026031586
train_loss: 0.0018889515
test_loss: 0.0026639209
train_loss: 0.0021693287
test_loss: 0.0026790341
train_loss: 0.0019286668
test_loss: 0.0024498776
train_loss: 0.0019718236
test_loss: 0.002515909
train_loss: 0.0019139637
test_loss: 0.0025374566
train_loss: 0.0021707602
test_loss: 0.0026528344
train_loss: 0.0019025492
test_loss: 0.00243102
train_loss: 0.0021712715
test_loss: 0.0026307006
train_loss: 0.0018559415
test_loss: 0.0024660286
train_loss: 0.0019112173
test_loss: 0.0024431013
train_loss: 0.0019541283
test_loss: 0.002427712
train_loss: 0.0023174314
test_loss: 0.002821705
train_loss: 0.0022890118
test_loss: 0.0025169135
train_loss: 0.00210618
test_loss: 0.002516976
train_loss: 0.0022141528
test_loss: 0.0025602882
train_loss: 0.0022249627
test_loss: 0.0025525591
train_loss: 0.0021157041
test_loss: 0.0026067763
train_loss: 0.0019177146
test_loss: 0.0025556562
train_loss: 0.0021222322
test_loss: 0.0025823417
train_loss: 0.002006342
test_loss: 0.002478853
train_loss: 0.0019420332
test_loss: 0.0024646951
train_loss: 0.002344058
test_loss: 0.0024773907
train_loss: 0.001922773
test_loss: 0.002405251
train_loss: 0.002092848
test_loss: 0.0025191586
train_loss: 0.0021104787
test_loss: 0.0025886134
train_loss: 0.0020532822
test_loss: 0.002553499
train_loss: 0.002101013
test_loss: 0.002784419
train_loss: 0.0019459025
test_loss: 0.0025523016
train_loss: 0.0019114772
test_loss: 0.0025359942
train_loss: 0.0020088241
test_loss: 0.002581121
train_loss: 0.0018269478
test_loss: 0.0023412786
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 0.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabd62a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabcfe598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabc0b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabcfe378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabc77c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabd62c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabbb5c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabbb5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabb6a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabb07bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabacc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabae4e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabafb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aabafbb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aaba48840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aaba8cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aaba766a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aaba39bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aab9f47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aab9f4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2aab9eb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a6ecb52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a6ec778c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a6ec77598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a6ec8e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a6ec8e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a6ebf39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a6ec0b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a6ec0b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a6ebd2b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a482f59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a4831d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a4831db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a482d27b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a48284620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2a482386a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.65074106e-06
Iter: 2 loss: 4.94448886e-06
Iter: 3 loss: 1.45913009e-05
Iter: 4 loss: 4.94181768e-06
Iter: 5 loss: 4.57716533e-06
Iter: 6 loss: 5.08810672e-06
Iter: 7 loss: 4.39754331e-06
Iter: 8 loss: 4.20052402e-06
Iter: 9 loss: 4.98647705e-06
Iter: 10 loss: 4.15613522e-06
Iter: 11 loss: 3.9278948e-06
Iter: 12 loss: 4.28750172e-06
Iter: 13 loss: 3.82148801e-06
Iter: 14 loss: 3.67814619e-06
Iter: 15 loss: 3.79685457e-06
Iter: 16 loss: 3.59268756e-06
Iter: 17 loss: 3.53993119e-06
Iter: 18 loss: 3.51940184e-06
Iter: 19 loss: 3.45530771e-06
Iter: 20 loss: 3.27619546e-06
Iter: 21 loss: 4.23274832e-06
Iter: 22 loss: 3.22038227e-06
Iter: 23 loss: 3.00426518e-06
Iter: 24 loss: 4.06599247e-06
Iter: 25 loss: 2.96705048e-06
Iter: 26 loss: 2.77614117e-06
Iter: 27 loss: 3.36088397e-06
Iter: 28 loss: 2.71950375e-06
Iter: 29 loss: 2.60770366e-06
Iter: 30 loss: 2.60305251e-06
Iter: 31 loss: 2.51499932e-06
Iter: 32 loss: 2.51500251e-06
Iter: 33 loss: 2.44459306e-06
Iter: 34 loss: 2.38119583e-06
Iter: 35 loss: 2.45535693e-06
Iter: 36 loss: 2.34744493e-06
Iter: 37 loss: 2.27724354e-06
Iter: 38 loss: 2.54475458e-06
Iter: 39 loss: 2.26055113e-06
Iter: 40 loss: 2.22540393e-06
Iter: 41 loss: 2.22009817e-06
Iter: 42 loss: 2.19509411e-06
Iter: 43 loss: 2.12894497e-06
Iter: 44 loss: 2.61546302e-06
Iter: 45 loss: 2.11504357e-06
Iter: 46 loss: 2.07711719e-06
Iter: 47 loss: 2.06573e-06
Iter: 48 loss: 2.03661762e-06
Iter: 49 loss: 1.97314739e-06
Iter: 50 loss: 2.92328036e-06
Iter: 51 loss: 1.970423e-06
Iter: 52 loss: 1.90295782e-06
Iter: 53 loss: 2.40007421e-06
Iter: 54 loss: 1.89744765e-06
Iter: 55 loss: 1.83148973e-06
Iter: 56 loss: 2.27846817e-06
Iter: 57 loss: 1.82497286e-06
Iter: 58 loss: 1.79808853e-06
Iter: 59 loss: 1.7537842e-06
Iter: 60 loss: 1.75364084e-06
Iter: 61 loss: 1.71426791e-06
Iter: 62 loss: 1.9149029e-06
Iter: 63 loss: 1.70784438e-06
Iter: 64 loss: 1.69102009e-06
Iter: 65 loss: 1.68850397e-06
Iter: 66 loss: 1.67047506e-06
Iter: 67 loss: 1.6260168e-06
Iter: 68 loss: 2.06451705e-06
Iter: 69 loss: 1.62028118e-06
Iter: 70 loss: 1.57112549e-06
Iter: 71 loss: 1.81587006e-06
Iter: 72 loss: 1.5629264e-06
Iter: 73 loss: 1.52021016e-06
Iter: 74 loss: 1.56810859e-06
Iter: 75 loss: 1.49725838e-06
Iter: 76 loss: 1.45141087e-06
Iter: 77 loss: 1.45121794e-06
Iter: 78 loss: 1.42472072e-06
Iter: 79 loss: 1.41402404e-06
Iter: 80 loss: 1.39989299e-06
Iter: 81 loss: 1.37707639e-06
Iter: 82 loss: 1.67409689e-06
Iter: 83 loss: 1.37690438e-06
Iter: 84 loss: 1.35825803e-06
Iter: 85 loss: 1.37328539e-06
Iter: 86 loss: 1.34699383e-06
Iter: 87 loss: 1.32962623e-06
Iter: 88 loss: 1.32772493e-06
Iter: 89 loss: 1.31513627e-06
Iter: 90 loss: 1.30341778e-06
Iter: 91 loss: 1.30044316e-06
Iter: 92 loss: 1.29273371e-06
Iter: 93 loss: 1.27103522e-06
Iter: 94 loss: 1.38987571e-06
Iter: 95 loss: 1.26449061e-06
Iter: 96 loss: 1.22900758e-06
Iter: 97 loss: 1.28682404e-06
Iter: 98 loss: 1.21268135e-06
Iter: 99 loss: 1.19044921e-06
Iter: 100 loss: 1.18998969e-06
Iter: 101 loss: 1.16809451e-06
Iter: 102 loss: 1.2149153e-06
Iter: 103 loss: 1.15948808e-06
Iter: 104 loss: 1.14194484e-06
Iter: 105 loss: 1.11232009e-06
Iter: 106 loss: 1.11227905e-06
Iter: 107 loss: 1.09463122e-06
Iter: 108 loss: 1.09463303e-06
Iter: 109 loss: 1.07953167e-06
Iter: 110 loss: 1.22700783e-06
Iter: 111 loss: 1.07897836e-06
Iter: 112 loss: 1.0713236e-06
Iter: 113 loss: 1.06286757e-06
Iter: 114 loss: 1.06162042e-06
Iter: 115 loss: 1.05074798e-06
Iter: 116 loss: 1.19045671e-06
Iter: 117 loss: 1.05061679e-06
Iter: 118 loss: 1.0407482e-06
Iter: 119 loss: 1.02582794e-06
Iter: 120 loss: 1.02550121e-06
Iter: 121 loss: 1.01132161e-06
Iter: 122 loss: 1.14434874e-06
Iter: 123 loss: 1.01078797e-06
Iter: 124 loss: 1.0003522e-06
Iter: 125 loss: 1.0900992e-06
Iter: 126 loss: 9.99754093e-07
Iter: 127 loss: 9.9242834e-07
Iter: 128 loss: 9.76883257e-07
Iter: 129 loss: 1.23670941e-06
Iter: 130 loss: 9.76489787e-07
Iter: 131 loss: 9.61674687e-07
Iter: 132 loss: 1.03708862e-06
Iter: 133 loss: 9.59288627e-07
Iter: 134 loss: 9.51716e-07
Iter: 135 loss: 1.02703109e-06
Iter: 136 loss: 9.51411039e-07
Iter: 137 loss: 9.41717531e-07
Iter: 138 loss: 9.39821575e-07
Iter: 139 loss: 9.33330625e-07
Iter: 140 loss: 9.23526954e-07
Iter: 141 loss: 9.2463074e-07
Iter: 142 loss: 9.1601521e-07
Iter: 143 loss: 9.0561025e-07
Iter: 144 loss: 9.7338534e-07
Iter: 145 loss: 9.04543867e-07
Iter: 146 loss: 8.94114748e-07
Iter: 147 loss: 9.7848158e-07
Iter: 148 loss: 8.93461333e-07
Iter: 149 loss: 8.85361089e-07
Iter: 150 loss: 8.72240321e-07
Iter: 151 loss: 8.72149201e-07
Iter: 152 loss: 8.7102444e-07
Iter: 153 loss: 8.67333824e-07
Iter: 154 loss: 8.63673108e-07
Iter: 155 loss: 8.55047517e-07
Iter: 156 loss: 9.54805387e-07
Iter: 157 loss: 8.54284508e-07
Iter: 158 loss: 8.50356514e-07
Iter: 159 loss: 8.49600951e-07
Iter: 160 loss: 8.45808586e-07
Iter: 161 loss: 8.53309416e-07
Iter: 162 loss: 8.44248746e-07
Iter: 163 loss: 8.40730138e-07
Iter: 164 loss: 8.33383922e-07
Iter: 165 loss: 9.59807267e-07
Iter: 166 loss: 8.33259e-07
Iter: 167 loss: 8.24428639e-07
Iter: 168 loss: 8.70019676e-07
Iter: 169 loss: 8.22997436e-07
Iter: 170 loss: 8.16727322e-07
Iter: 171 loss: 8.61976503e-07
Iter: 172 loss: 8.16242e-07
Iter: 173 loss: 8.08438415e-07
Iter: 174 loss: 8.13347299e-07
Iter: 175 loss: 8.0345535e-07
Iter: 176 loss: 7.94336813e-07
Iter: 177 loss: 7.98925782e-07
Iter: 178 loss: 7.88314594e-07
Iter: 179 loss: 7.81486506e-07
Iter: 180 loss: 7.93729953e-07
Iter: 181 loss: 7.78468291e-07
Iter: 182 loss: 7.74113232e-07
Iter: 183 loss: 7.73258876e-07
Iter: 184 loss: 7.70604629e-07
Iter: 185 loss: 7.65812047e-07
Iter: 186 loss: 8.78193475e-07
Iter: 187 loss: 7.65797836e-07
Iter: 188 loss: 7.62432705e-07
Iter: 189 loss: 7.62419575e-07
Iter: 190 loss: 7.59103273e-07
Iter: 191 loss: 7.56258828e-07
Iter: 192 loss: 7.55328415e-07
Iter: 193 loss: 7.50203299e-07
Iter: 194 loss: 7.56718634e-07
Iter: 195 loss: 7.47584068e-07
Iter: 196 loss: 7.41541612e-07
Iter: 197 loss: 8.1865312e-07
Iter: 198 loss: 7.41529277e-07
Iter: 199 loss: 7.38860422e-07
Iter: 200 loss: 7.32342414e-07
Iter: 201 loss: 7.94394964e-07
Iter: 202 loss: 7.31467594e-07
Iter: 203 loss: 7.23560447e-07
Iter: 204 loss: 7.50786967e-07
Iter: 205 loss: 7.2146986e-07
Iter: 206 loss: 7.16765328e-07
Iter: 207 loss: 7.16548357e-07
Iter: 208 loss: 7.1236434e-07
Iter: 209 loss: 7.14927751e-07
Iter: 210 loss: 7.09687527e-07
Iter: 211 loss: 7.06062849e-07
Iter: 212 loss: 7.09607264e-07
Iter: 213 loss: 7.04046158e-07
Iter: 214 loss: 7.00123678e-07
Iter: 215 loss: 7.0998243e-07
Iter: 216 loss: 6.98754434e-07
Iter: 217 loss: 6.94555297e-07
Iter: 218 loss: 7.46819467e-07
Iter: 219 loss: 6.94481059e-07
Iter: 220 loss: 6.91706873e-07
Iter: 221 loss: 6.87389559e-07
Iter: 222 loss: 6.87342094e-07
Iter: 223 loss: 6.83377e-07
Iter: 224 loss: 7.19043783e-07
Iter: 225 loss: 6.83169048e-07
Iter: 226 loss: 6.78354127e-07
Iter: 227 loss: 6.76838e-07
Iter: 228 loss: 6.73995373e-07
Iter: 229 loss: 6.70411737e-07
Iter: 230 loss: 6.95391179e-07
Iter: 231 loss: 6.70089548e-07
Iter: 232 loss: 6.66127335e-07
Iter: 233 loss: 6.75491663e-07
Iter: 234 loss: 6.64704885e-07
Iter: 235 loss: 6.61504373e-07
Iter: 236 loss: 6.58747751e-07
Iter: 237 loss: 6.57859914e-07
Iter: 238 loss: 6.53929078e-07
Iter: 239 loss: 6.62382377e-07
Iter: 240 loss: 6.52450183e-07
Iter: 241 loss: 6.49706294e-07
Iter: 242 loss: 6.49618471e-07
Iter: 243 loss: 6.46612875e-07
Iter: 244 loss: 6.44321176e-07
Iter: 245 loss: 6.43333e-07
Iter: 246 loss: 6.4025636e-07
Iter: 247 loss: 6.39991072e-07
Iter: 248 loss: 6.37686867e-07
Iter: 249 loss: 6.3503677e-07
Iter: 250 loss: 6.34937464e-07
Iter: 251 loss: 6.3217e-07
Iter: 252 loss: 6.34271373e-07
Iter: 253 loss: 6.30519366e-07
Iter: 254 loss: 6.27651218e-07
Iter: 255 loss: 6.27779798e-07
Iter: 256 loss: 6.25422672e-07
Iter: 257 loss: 6.22385357e-07
Iter: 258 loss: 6.56406712e-07
Iter: 259 loss: 6.22313621e-07
Iter: 260 loss: 6.19209118e-07
Iter: 261 loss: 6.16783609e-07
Iter: 262 loss: 6.1580829e-07
Iter: 263 loss: 6.13319116e-07
Iter: 264 loss: 6.45694286e-07
Iter: 265 loss: 6.13327643e-07
Iter: 266 loss: 6.10904237e-07
Iter: 267 loss: 6.13561383e-07
Iter: 268 loss: 6.0958962e-07
Iter: 269 loss: 6.07064749e-07
Iter: 270 loss: 6.0356615e-07
Iter: 271 loss: 6.03392778e-07
Iter: 272 loss: 5.99811756e-07
Iter: 273 loss: 6.13003181e-07
Iter: 274 loss: 5.98952965e-07
Iter: 275 loss: 5.9752449e-07
Iter: 276 loss: 5.97143639e-07
Iter: 277 loss: 5.95566803e-07
Iter: 278 loss: 5.93267828e-07
Iter: 279 loss: 5.93196262e-07
Iter: 280 loss: 5.90827426e-07
Iter: 281 loss: 5.90204877e-07
Iter: 282 loss: 5.88701084e-07
Iter: 283 loss: 5.86134718e-07
Iter: 284 loss: 5.86031547e-07
Iter: 285 loss: 5.8362059e-07
Iter: 286 loss: 5.84391159e-07
Iter: 287 loss: 5.81886241e-07
Iter: 288 loss: 5.8021817e-07
Iter: 289 loss: 5.79995287e-07
Iter: 290 loss: 5.78842e-07
Iter: 291 loss: 5.76032733e-07
Iter: 292 loss: 5.99130772e-07
Iter: 293 loss: 5.7586908e-07
Iter: 294 loss: 5.73857051e-07
Iter: 295 loss: 5.73184082e-07
Iter: 296 loss: 5.7206978e-07
Iter: 297 loss: 5.69839699e-07
Iter: 298 loss: 5.92655e-07
Iter: 299 loss: 5.69796498e-07
Iter: 300 loss: 5.67570225e-07
Iter: 301 loss: 5.67016968e-07
Iter: 302 loss: 5.6557e-07
Iter: 303 loss: 5.63333629e-07
Iter: 304 loss: 5.65661e-07
Iter: 305 loss: 5.62052776e-07
Iter: 306 loss: 5.59538307e-07
Iter: 307 loss: 5.61530726e-07
Iter: 308 loss: 5.58060833e-07
Iter: 309 loss: 5.55583483e-07
Iter: 310 loss: 5.55569613e-07
Iter: 311 loss: 5.53442874e-07
Iter: 312 loss: 5.52871143e-07
Iter: 313 loss: 5.51612686e-07
Iter: 314 loss: 5.49757488e-07
Iter: 315 loss: 5.48759715e-07
Iter: 316 loss: 5.47886e-07
Iter: 317 loss: 5.46401452e-07
Iter: 318 loss: 5.46094725e-07
Iter: 319 loss: 5.44381862e-07
Iter: 320 loss: 5.41118e-07
Iter: 321 loss: 6.13119312e-07
Iter: 322 loss: 5.41130476e-07
Iter: 323 loss: 5.39242308e-07
Iter: 324 loss: 5.56932378e-07
Iter: 325 loss: 5.39168866e-07
Iter: 326 loss: 5.37436449e-07
Iter: 327 loss: 5.45776402e-07
Iter: 328 loss: 5.37118353e-07
Iter: 329 loss: 5.35422487e-07
Iter: 330 loss: 5.32512161e-07
Iter: 331 loss: 5.3250568e-07
Iter: 332 loss: 5.32144895e-07
Iter: 333 loss: 5.31109038e-07
Iter: 334 loss: 5.30114e-07
Iter: 335 loss: 5.27609473e-07
Iter: 336 loss: 5.51008497e-07
Iter: 337 loss: 5.2721208e-07
Iter: 338 loss: 5.24584152e-07
Iter: 339 loss: 5.40135147e-07
Iter: 340 loss: 5.2425105e-07
Iter: 341 loss: 5.22225321e-07
Iter: 342 loss: 5.25797816e-07
Iter: 343 loss: 5.21284903e-07
Iter: 344 loss: 5.2004151e-07
Iter: 345 loss: 5.19941864e-07
Iter: 346 loss: 5.19001333e-07
Iter: 347 loss: 5.17262947e-07
Iter: 348 loss: 5.54836049e-07
Iter: 349 loss: 5.17250555e-07
Iter: 350 loss: 5.15090505e-07
Iter: 351 loss: 5.14376779e-07
Iter: 352 loss: 5.13105647e-07
Iter: 353 loss: 5.11415e-07
Iter: 354 loss: 5.11235044e-07
Iter: 355 loss: 5.09596134e-07
Iter: 356 loss: 5.16368345e-07
Iter: 357 loss: 5.0926343e-07
Iter: 358 loss: 5.07962227e-07
Iter: 359 loss: 5.04528316e-07
Iter: 360 loss: 5.26830377e-07
Iter: 361 loss: 5.03683964e-07
Iter: 362 loss: 5.0501535e-07
Iter: 363 loss: 5.02198418e-07
Iter: 364 loss: 5.01150168e-07
Iter: 365 loss: 5.00198894e-07
Iter: 366 loss: 4.99945486e-07
Iter: 367 loss: 4.98855115e-07
Iter: 368 loss: 5.12002202e-07
Iter: 369 loss: 4.98873192e-07
Iter: 370 loss: 4.97748601e-07
Iter: 371 loss: 4.96027099e-07
Iter: 372 loss: 4.96003395e-07
Iter: 373 loss: 4.94297296e-07
Iter: 374 loss: 4.97170902e-07
Iter: 375 loss: 4.9351604e-07
Iter: 376 loss: 4.91758271e-07
Iter: 377 loss: 4.91840751e-07
Iter: 378 loss: 4.90386583e-07
Iter: 379 loss: 4.88769615e-07
Iter: 380 loss: 4.88609089e-07
Iter: 381 loss: 4.87230182e-07
Iter: 382 loss: 4.85733722e-07
Iter: 383 loss: 4.855425e-07
Iter: 384 loss: 4.83914562e-07
Iter: 385 loss: 4.90701098e-07
Iter: 386 loss: 4.83555596e-07
Iter: 387 loss: 4.82114842e-07
Iter: 388 loss: 4.94142228e-07
Iter: 389 loss: 4.82056521e-07
Iter: 390 loss: 4.80890094e-07
Iter: 391 loss: 4.80710241e-07
Iter: 392 loss: 4.79894311e-07
Iter: 393 loss: 4.78886932e-07
Iter: 394 loss: 4.80505378e-07
Iter: 395 loss: 4.78409618e-07
Iter: 396 loss: 4.77124274e-07
Iter: 397 loss: 4.88570947e-07
Iter: 398 loss: 4.7706277e-07
Iter: 399 loss: 4.76102457e-07
Iter: 400 loss: 4.75859139e-07
Iter: 401 loss: 4.75262283e-07
Iter: 402 loss: 4.74232479e-07
Iter: 403 loss: 4.86320232e-07
Iter: 404 loss: 4.74210196e-07
Iter: 405 loss: 4.73243546e-07
Iter: 406 loss: 4.71221369e-07
Iter: 407 loss: 5.08439371e-07
Iter: 408 loss: 4.71214605e-07
Iter: 409 loss: 4.69129702e-07
Iter: 410 loss: 4.75203194e-07
Iter: 411 loss: 4.68481971e-07
Iter: 412 loss: 4.66518401e-07
Iter: 413 loss: 4.70388159e-07
Iter: 414 loss: 4.65705966e-07
Iter: 415 loss: 4.64403e-07
Iter: 416 loss: 4.64303213e-07
Iter: 417 loss: 4.63246863e-07
Iter: 418 loss: 4.61917693e-07
Iter: 419 loss: 4.61789568e-07
Iter: 420 loss: 4.60361889e-07
Iter: 421 loss: 4.61740541e-07
Iter: 422 loss: 4.59560198e-07
Iter: 423 loss: 4.584968e-07
Iter: 424 loss: 4.58342669e-07
Iter: 425 loss: 4.57696473e-07
Iter: 426 loss: 4.56372106e-07
Iter: 427 loss: 4.78168374e-07
Iter: 428 loss: 4.56313245e-07
Iter: 429 loss: 4.55035888e-07
Iter: 430 loss: 4.65288878e-07
Iter: 431 loss: 4.54933286e-07
Iter: 432 loss: 4.538295e-07
Iter: 433 loss: 4.61463e-07
Iter: 434 loss: 4.53736902e-07
Iter: 435 loss: 4.53107191e-07
Iter: 436 loss: 4.52932625e-07
Iter: 437 loss: 4.52575676e-07
Iter: 438 loss: 4.51430765e-07
Iter: 439 loss: 4.5598216e-07
Iter: 440 loss: 4.5116e-07
Iter: 441 loss: 4.50293726e-07
Iter: 442 loss: 4.49540948e-07
Iter: 443 loss: 4.49327132e-07
Iter: 444 loss: 4.47891637e-07
Iter: 445 loss: 4.49893577e-07
Iter: 446 loss: 4.47162e-07
Iter: 447 loss: 4.45416532e-07
Iter: 448 loss: 4.48881877e-07
Iter: 449 loss: 4.44695e-07
Iter: 450 loss: 4.42905701e-07
Iter: 451 loss: 4.42894077e-07
Iter: 452 loss: 4.4212203e-07
Iter: 453 loss: 4.40950799e-07
Iter: 454 loss: 4.40936219e-07
Iter: 455 loss: 4.4005364e-07
Iter: 456 loss: 4.40053128e-07
Iter: 457 loss: 4.39192888e-07
Iter: 458 loss: 4.41155777e-07
Iter: 459 loss: 4.38881955e-07
Iter: 460 loss: 4.38351719e-07
Iter: 461 loss: 4.38006879e-07
Iter: 462 loss: 4.37769842e-07
Iter: 463 loss: 4.37210502e-07
Iter: 464 loss: 4.449887e-07
Iter: 465 loss: 4.37204363e-07
Iter: 466 loss: 4.36527e-07
Iter: 467 loss: 4.35280043e-07
Iter: 468 loss: 4.35292151e-07
Iter: 469 loss: 4.34347129e-07
Iter: 470 loss: 4.34334311e-07
Iter: 471 loss: 4.3345e-07
Iter: 472 loss: 4.32251539e-07
Iter: 473 loss: 4.32191257e-07
Iter: 474 loss: 4.30517616e-07
Iter: 475 loss: 4.32165052e-07
Iter: 476 loss: 4.29539256e-07
Iter: 477 loss: 4.28065505e-07
Iter: 478 loss: 4.34571916e-07
Iter: 479 loss: 4.27785778e-07
Iter: 480 loss: 4.26605e-07
Iter: 481 loss: 4.39788607e-07
Iter: 482 loss: 4.26567908e-07
Iter: 483 loss: 4.25601399e-07
Iter: 484 loss: 4.28990631e-07
Iter: 485 loss: 4.25352482e-07
Iter: 486 loss: 4.24738687e-07
Iter: 487 loss: 4.23953793e-07
Iter: 488 loss: 4.23901241e-07
Iter: 489 loss: 4.23248878e-07
Iter: 490 loss: 4.23193228e-07
Iter: 491 loss: 4.22456537e-07
Iter: 492 loss: 4.2098e-07
Iter: 493 loss: 4.47511951e-07
Iter: 494 loss: 4.20928984e-07
Iter: 495 loss: 4.19636137e-07
Iter: 496 loss: 4.24567361e-07
Iter: 497 loss: 4.19347259e-07
Iter: 498 loss: 4.18275931e-07
Iter: 499 loss: 4.18281047e-07
Iter: 500 loss: 4.17569538e-07
Iter: 501 loss: 4.16920955e-07
Iter: 502 loss: 4.16745536e-07
Iter: 503 loss: 4.16051307e-07
Iter: 504 loss: 4.16028684e-07
Iter: 505 loss: 4.15573396e-07
Iter: 506 loss: 4.14813769e-07
Iter: 507 loss: 4.14823717e-07
Iter: 508 loss: 4.13924113e-07
Iter: 509 loss: 4.14604301e-07
Iter: 510 loss: 4.13406468e-07
Iter: 511 loss: 4.12643232e-07
Iter: 512 loss: 4.23332324e-07
Iter: 513 loss: 4.12637633e-07
Iter: 514 loss: 4.11989419e-07
Iter: 515 loss: 4.14834176e-07
Iter: 516 loss: 4.11847026e-07
Iter: 517 loss: 4.11071653e-07
Iter: 518 loss: 4.09575875e-07
Iter: 519 loss: 4.41796317e-07
Iter: 520 loss: 4.09586278e-07
Iter: 521 loss: 4.08406976e-07
Iter: 522 loss: 4.19696647e-07
Iter: 523 loss: 4.08361728e-07
Iter: 524 loss: 4.07197888e-07
Iter: 525 loss: 4.1394091e-07
Iter: 526 loss: 4.07057513e-07
Iter: 527 loss: 4.0625298e-07
Iter: 528 loss: 4.05386714e-07
Iter: 529 loss: 4.05278257e-07
Iter: 530 loss: 4.04575701e-07
Iter: 531 loss: 4.14607683e-07
Iter: 532 loss: 4.04578458e-07
Iter: 533 loss: 4.03895854e-07
Iter: 534 loss: 4.05181311e-07
Iter: 535 loss: 4.03599984e-07
Iter: 536 loss: 4.03081e-07
Iter: 537 loss: 4.03663591e-07
Iter: 538 loss: 4.02775157e-07
Iter: 539 loss: 4.01973892e-07
Iter: 540 loss: 4.05337858e-07
Iter: 541 loss: 4.01815555e-07
Iter: 542 loss: 4.01372802e-07
Iter: 543 loss: 4.00431816e-07
Iter: 544 loss: 4.17176096e-07
Iter: 545 loss: 4.00428092e-07
Iter: 546 loss: 3.99318822e-07
Iter: 547 loss: 4.05906121e-07
Iter: 548 loss: 3.99180919e-07
Iter: 549 loss: 3.98281742e-07
Iter: 550 loss: 4.06017648e-07
Iter: 551 loss: 3.98235215e-07
Iter: 552 loss: 3.97271293e-07
Iter: 553 loss: 3.97357582e-07
Iter: 554 loss: 3.96514054e-07
Iter: 555 loss: 3.95595464e-07
Iter: 556 loss: 3.98311329e-07
Iter: 557 loss: 3.95328072e-07
Iter: 558 loss: 3.94777516e-07
Iter: 559 loss: 3.94771178e-07
Iter: 560 loss: 3.94196377e-07
Iter: 561 loss: 3.93292964e-07
Iter: 562 loss: 3.93284e-07
Iter: 563 loss: 3.92448868e-07
Iter: 564 loss: 3.93725969e-07
Iter: 565 loss: 3.9201268e-07
Iter: 566 loss: 3.91723404e-07
Iter: 567 loss: 3.9151422e-07
Iter: 568 loss: 3.91177878e-07
Iter: 569 loss: 3.90346429e-07
Iter: 570 loss: 4.00355873e-07
Iter: 571 loss: 3.90290836e-07
Iter: 572 loss: 3.89490083e-07
Iter: 573 loss: 3.89527344e-07
Iter: 574 loss: 3.88836781e-07
Iter: 575 loss: 3.88250669e-07
Iter: 576 loss: 3.880902e-07
Iter: 577 loss: 3.87174339e-07
Iter: 578 loss: 3.87229761e-07
Iter: 579 loss: 3.8649415e-07
Iter: 580 loss: 3.85526675e-07
Iter: 581 loss: 3.93352366e-07
Iter: 582 loss: 3.85434873e-07
Iter: 583 loss: 3.84776911e-07
Iter: 584 loss: 3.84781686e-07
Iter: 585 loss: 3.84404586e-07
Iter: 586 loss: 3.83626514e-07
Iter: 587 loss: 3.9693532e-07
Iter: 588 loss: 3.83624467e-07
Iter: 589 loss: 3.8275536e-07
Iter: 590 loss: 3.90331e-07
Iter: 591 loss: 3.82685812e-07
Iter: 592 loss: 3.82031828e-07
Iter: 593 loss: 3.87746979e-07
Iter: 594 loss: 3.82030805e-07
Iter: 595 loss: 3.81654672e-07
Iter: 596 loss: 3.80867959e-07
Iter: 597 loss: 3.90225e-07
Iter: 598 loss: 3.80782296e-07
Iter: 599 loss: 3.80003314e-07
Iter: 600 loss: 3.90263494e-07
Iter: 601 loss: 3.79977024e-07
Iter: 602 loss: 3.79260257e-07
Iter: 603 loss: 3.82283531e-07
Iter: 604 loss: 3.79094359e-07
Iter: 605 loss: 3.78593199e-07
Iter: 606 loss: 3.783517e-07
Iter: 607 loss: 3.78120689e-07
Iter: 608 loss: 3.77269373e-07
Iter: 609 loss: 3.8329415e-07
Iter: 610 loss: 3.7721361e-07
Iter: 611 loss: 3.76630481e-07
Iter: 612 loss: 3.75865568e-07
Iter: 613 loss: 3.75809e-07
Iter: 614 loss: 3.74973155e-07
Iter: 615 loss: 3.756316e-07
Iter: 616 loss: 3.74458835e-07
Iter: 617 loss: 3.74048113e-07
Iter: 618 loss: 3.73908932e-07
Iter: 619 loss: 3.73399416e-07
Iter: 620 loss: 3.730961e-07
Iter: 621 loss: 3.72909255e-07
Iter: 622 loss: 3.72262093e-07
Iter: 623 loss: 3.73272e-07
Iter: 624 loss: 3.71960198e-07
Iter: 625 loss: 3.71484902e-07
Iter: 626 loss: 3.71446788e-07
Iter: 627 loss: 3.71067756e-07
Iter: 628 loss: 3.70386033e-07
Iter: 629 loss: 3.70387909e-07
Iter: 630 loss: 3.69704708e-07
Iter: 631 loss: 3.71184683e-07
Iter: 632 loss: 3.69468e-07
Iter: 633 loss: 3.688437e-07
Iter: 634 loss: 3.77406138e-07
Iter: 635 loss: 3.68837846e-07
Iter: 636 loss: 3.68269809e-07
Iter: 637 loss: 3.67991163e-07
Iter: 638 loss: 3.67724056e-07
Iter: 639 loss: 3.67277437e-07
Iter: 640 loss: 3.72486198e-07
Iter: 641 loss: 3.67268626e-07
Iter: 642 loss: 3.66790175e-07
Iter: 643 loss: 3.65944487e-07
Iter: 644 loss: 3.65934227e-07
Iter: 645 loss: 3.65083167e-07
Iter: 646 loss: 3.66523096e-07
Iter: 647 loss: 3.64670257e-07
Iter: 648 loss: 3.63750644e-07
Iter: 649 loss: 3.6722281e-07
Iter: 650 loss: 3.63557319e-07
Iter: 651 loss: 3.63016284e-07
Iter: 652 loss: 3.62959099e-07
Iter: 653 loss: 3.62672722e-07
Iter: 654 loss: 3.62096841e-07
Iter: 655 loss: 3.75640042e-07
Iter: 656 loss: 3.62093317e-07
Iter: 657 loss: 3.6154114e-07
Iter: 658 loss: 3.6680035e-07
Iter: 659 loss: 3.61537019e-07
Iter: 660 loss: 3.60897445e-07
Iter: 661 loss: 3.61925288e-07
Iter: 662 loss: 3.60576053e-07
Iter: 663 loss: 3.60204638e-07
Iter: 664 loss: 3.60555191e-07
Iter: 665 loss: 3.59997273e-07
Iter: 666 loss: 3.59509244e-07
Iter: 667 loss: 3.60436445e-07
Iter: 668 loss: 3.59306682e-07
Iter: 669 loss: 3.58583861e-07
Iter: 670 loss: 3.61244361e-07
Iter: 671 loss: 3.58399546e-07
Iter: 672 loss: 3.57969753e-07
Iter: 673 loss: 3.58193915e-07
Iter: 674 loss: 3.57681159e-07
Iter: 675 loss: 3.57052386e-07
Iter: 676 loss: 3.60866352e-07
Iter: 677 loss: 3.56984913e-07
Iter: 678 loss: 3.56568023e-07
Iter: 679 loss: 3.55832185e-07
Iter: 680 loss: 3.55821953e-07
Iter: 681 loss: 3.54908622e-07
Iter: 682 loss: 3.57937552e-07
Iter: 683 loss: 3.54678548e-07
Iter: 684 loss: 3.5430736e-07
Iter: 685 loss: 3.54283799e-07
Iter: 686 loss: 3.53860571e-07
Iter: 687 loss: 3.5319988e-07
Iter: 688 loss: 3.531961e-07
Iter: 689 loss: 3.52568748e-07
Iter: 690 loss: 3.54930108e-07
Iter: 691 loss: 3.52432778e-07
Iter: 692 loss: 3.51810741e-07
Iter: 693 loss: 3.58516445e-07
Iter: 694 loss: 3.51790902e-07
Iter: 695 loss: 3.51391e-07
Iter: 696 loss: 3.50675464e-07
Iter: 697 loss: 3.50663612e-07
Iter: 698 loss: 3.50048026e-07
Iter: 699 loss: 3.55652219e-07
Iter: 700 loss: 3.50025e-07
Iter: 701 loss: 3.49618034e-07
Iter: 702 loss: 3.5441434e-07
Iter: 703 loss: 3.49615192e-07
Iter: 704 loss: 3.49309346e-07
Iter: 705 loss: 3.48831804e-07
Iter: 706 loss: 3.48820663e-07
Iter: 707 loss: 3.48424948e-07
Iter: 708 loss: 3.48436743e-07
Iter: 709 loss: 3.48113588e-07
Iter: 710 loss: 3.47826813e-07
Iter: 711 loss: 3.47753598e-07
Iter: 712 loss: 3.47205628e-07
Iter: 713 loss: 3.46555169e-07
Iter: 714 loss: 3.46488321e-07
Iter: 715 loss: 3.45794973e-07
Iter: 716 loss: 3.54294514e-07
Iter: 717 loss: 3.45779284e-07
Iter: 718 loss: 3.45141757e-07
Iter: 719 loss: 3.50443145e-07
Iter: 720 loss: 3.45091394e-07
Iter: 721 loss: 3.4476227e-07
Iter: 722 loss: 3.44118575e-07
Iter: 723 loss: 3.44112777e-07
Iter: 724 loss: 3.43826258e-07
Iter: 725 loss: 3.43712259e-07
Iter: 726 loss: 3.43382453e-07
Iter: 727 loss: 3.4301496e-07
Iter: 728 loss: 3.42946862e-07
Iter: 729 loss: 3.42454e-07
Iter: 730 loss: 3.42747029e-07
Iter: 731 loss: 3.42138577e-07
Iter: 732 loss: 3.41654811e-07
Iter: 733 loss: 3.41664816e-07
Iter: 734 loss: 3.41280213e-07
Iter: 735 loss: 3.40977e-07
Iter: 736 loss: 3.40852324e-07
Iter: 737 loss: 3.4028335e-07
Iter: 738 loss: 3.42486601e-07
Iter: 739 loss: 3.40175518e-07
Iter: 740 loss: 3.3961328e-07
Iter: 741 loss: 3.42077e-07
Iter: 742 loss: 3.39495e-07
Iter: 743 loss: 3.39116752e-07
Iter: 744 loss: 3.38610761e-07
Iter: 745 loss: 3.38565201e-07
Iter: 746 loss: 3.37961751e-07
Iter: 747 loss: 3.39417113e-07
Iter: 748 loss: 3.37738982e-07
Iter: 749 loss: 3.37476422e-07
Iter: 750 loss: 3.37377571e-07
Iter: 751 loss: 3.37146531e-07
Iter: 752 loss: 3.36919015e-07
Iter: 753 loss: 3.36844096e-07
Iter: 754 loss: 3.36478763e-07
Iter: 755 loss: 3.3688724e-07
Iter: 756 loss: 3.36275832e-07
Iter: 757 loss: 3.35714333e-07
Iter: 758 loss: 3.39038195e-07
Iter: 759 loss: 3.3565621e-07
Iter: 760 loss: 3.3531191e-07
Iter: 761 loss: 3.34757203e-07
Iter: 762 loss: 3.34737535e-07
Iter: 763 loss: 3.34207556e-07
Iter: 764 loss: 3.40878103e-07
Iter: 765 loss: 3.34183198e-07
Iter: 766 loss: 3.33590947e-07
Iter: 767 loss: 3.34206447e-07
Iter: 768 loss: 3.33279331e-07
Iter: 769 loss: 3.32760948e-07
Iter: 770 loss: 3.3431192e-07
Iter: 771 loss: 3.3260784e-07
Iter: 772 loss: 3.32193196e-07
Iter: 773 loss: 3.35003e-07
Iter: 774 loss: 3.32152297e-07
Iter: 775 loss: 3.31772753e-07
Iter: 776 loss: 3.31272958e-07
Iter: 777 loss: 3.31234e-07
Iter: 778 loss: 3.30715324e-07
Iter: 779 loss: 3.3204185e-07
Iter: 780 loss: 3.30535158e-07
Iter: 781 loss: 3.30034112e-07
Iter: 782 loss: 3.33831167e-07
Iter: 783 loss: 3.29995146e-07
Iter: 784 loss: 3.29516695e-07
Iter: 785 loss: 3.31819791e-07
Iter: 786 loss: 3.29415457e-07
Iter: 787 loss: 3.2910026e-07
Iter: 788 loss: 3.28608905e-07
Iter: 789 loss: 3.28601459e-07
Iter: 790 loss: 3.2842641e-07
Iter: 791 loss: 3.28303599e-07
Iter: 792 loss: 3.28061191e-07
Iter: 793 loss: 3.27531097e-07
Iter: 794 loss: 3.33958724e-07
Iter: 795 loss: 3.27476357e-07
Iter: 796 loss: 3.27092806e-07
Iter: 797 loss: 3.32114837e-07
Iter: 798 loss: 3.27091243e-07
Iter: 799 loss: 3.26745408e-07
Iter: 800 loss: 3.28611179e-07
Iter: 801 loss: 3.26700899e-07
Iter: 802 loss: 3.26405598e-07
Iter: 803 loss: 3.26000645e-07
Iter: 804 loss: 3.25968102e-07
Iter: 805 loss: 3.25415698e-07
Iter: 806 loss: 3.29368476e-07
Iter: 807 loss: 3.25374458e-07
Iter: 808 loss: 3.24861446e-07
Iter: 809 loss: 3.25409047e-07
Iter: 810 loss: 3.24607413e-07
Iter: 811 loss: 3.24163238e-07
Iter: 812 loss: 3.2402005e-07
Iter: 813 loss: 3.23780483e-07
Iter: 814 loss: 3.23193973e-07
Iter: 815 loss: 3.26085711e-07
Iter: 816 loss: 3.23074261e-07
Iter: 817 loss: 3.22837565e-07
Iter: 818 loss: 3.22819403e-07
Iter: 819 loss: 3.22586544e-07
Iter: 820 loss: 3.22017115e-07
Iter: 821 loss: 3.2647381e-07
Iter: 822 loss: 3.21894021e-07
Iter: 823 loss: 3.21724656e-07
Iter: 824 loss: 3.21599117e-07
Iter: 825 loss: 3.213041e-07
Iter: 826 loss: 3.21016216e-07
Iter: 827 loss: 3.20962897e-07
Iter: 828 loss: 3.20520428e-07
Iter: 829 loss: 3.20627692e-07
Iter: 830 loss: 3.20200797e-07
Iter: 831 loss: 3.19734966e-07
Iter: 832 loss: 3.19729025e-07
Iter: 833 loss: 3.1937148e-07
Iter: 834 loss: 3.19174376e-07
Iter: 835 loss: 3.19006176e-07
Iter: 836 loss: 3.1857212e-07
Iter: 837 loss: 3.21534287e-07
Iter: 838 loss: 3.18502941e-07
Iter: 839 loss: 3.18103275e-07
Iter: 840 loss: 3.18658294e-07
Iter: 841 loss: 3.1788494e-07
Iter: 842 loss: 3.17529896e-07
Iter: 843 loss: 3.17778415e-07
Iter: 844 loss: 3.17307126e-07
Iter: 845 loss: 3.169e-07
Iter: 846 loss: 3.17244627e-07
Iter: 847 loss: 3.16688215e-07
Iter: 848 loss: 3.1634454e-07
Iter: 849 loss: 3.16353351e-07
Iter: 850 loss: 3.1598762e-07
Iter: 851 loss: 3.15674527e-07
Iter: 852 loss: 3.15574255e-07
Iter: 853 loss: 3.15151311e-07
Iter: 854 loss: 3.1669714e-07
Iter: 855 loss: 3.15049618e-07
Iter: 856 loss: 3.14551073e-07
Iter: 857 loss: 3.17118094e-07
Iter: 858 loss: 3.14473084e-07
Iter: 859 loss: 3.1412111e-07
Iter: 860 loss: 3.13628107e-07
Iter: 861 loss: 3.13613896e-07
Iter: 862 loss: 3.13260045e-07
Iter: 863 loss: 3.13247511e-07
Iter: 864 loss: 3.12881923e-07
Iter: 865 loss: 3.12966108e-07
Iter: 866 loss: 3.12615498e-07
Iter: 867 loss: 3.12337761e-07
Iter: 868 loss: 3.14162179e-07
Iter: 869 loss: 3.12295185e-07
Iter: 870 loss: 3.12038509e-07
Iter: 871 loss: 3.12641106e-07
Iter: 872 loss: 3.11935253e-07
Iter: 873 loss: 3.11669481e-07
Iter: 874 loss: 3.11437049e-07
Iter: 875 loss: 3.11375089e-07
Iter: 876 loss: 3.10870035e-07
Iter: 877 loss: 3.11539111e-07
Iter: 878 loss: 3.10621402e-07
Iter: 879 loss: 3.10079457e-07
Iter: 880 loss: 3.12597024e-07
Iter: 881 loss: 3.09998143e-07
Iter: 882 loss: 3.0932668e-07
Iter: 883 loss: 3.11512565e-07
Iter: 884 loss: 3.09154245e-07
Iter: 885 loss: 3.08710469e-07
Iter: 886 loss: 3.0870109e-07
Iter: 887 loss: 3.08325639e-07
Iter: 888 loss: 3.08112675e-07
Iter: 889 loss: 3.08022948e-07
Iter: 890 loss: 3.07805919e-07
Iter: 891 loss: 3.07367429e-07
Iter: 892 loss: 3.13791759e-07
Iter: 893 loss: 3.07335426e-07
Iter: 894 loss: 3.07016194e-07
Iter: 895 loss: 3.0702256e-07
Iter: 896 loss: 3.0674272e-07
Iter: 897 loss: 3.07542422e-07
Iter: 898 loss: 3.06650406e-07
Iter: 899 loss: 3.06444434e-07
Iter: 900 loss: 3.06522026e-07
Iter: 901 loss: 3.06299228e-07
Iter: 902 loss: 3.05970559e-07
Iter: 903 loss: 3.07520907e-07
Iter: 904 loss: 3.05928523e-07
Iter: 905 loss: 3.0563055e-07
Iter: 906 loss: 3.05377426e-07
Iter: 907 loss: 3.05287926e-07
Iter: 908 loss: 3.04768946e-07
Iter: 909 loss: 3.05399283e-07
Iter: 910 loss: 3.04503402e-07
Iter: 911 loss: 3.03949264e-07
Iter: 912 loss: 3.07024095e-07
Iter: 913 loss: 3.0387298e-07
Iter: 914 loss: 3.03517936e-07
Iter: 915 loss: 3.03512877e-07
Iter: 916 loss: 3.03307758e-07
Iter: 917 loss: 3.03034739e-07
Iter: 918 loss: 3.03035193e-07
Iter: 919 loss: 3.02770303e-07
Iter: 920 loss: 3.02779881e-07
Iter: 921 loss: 3.02484807e-07
Iter: 922 loss: 3.02120071e-07
Iter: 923 loss: 3.02095515e-07
Iter: 924 loss: 3.01783928e-07
Iter: 925 loss: 3.04718753e-07
Iter: 926 loss: 3.01773e-07
Iter: 927 loss: 3.01528644e-07
Iter: 928 loss: 3.02400338e-07
Iter: 929 loss: 3.01451252e-07
Iter: 930 loss: 3.01201794e-07
Iter: 931 loss: 3.00893277e-07
Iter: 932 loss: 3.00858289e-07
Iter: 933 loss: 3.00483975e-07
Iter: 934 loss: 3.05747392e-07
Iter: 935 loss: 3.00493241e-07
Iter: 936 loss: 3.0018623e-07
Iter: 937 loss: 3.00090477e-07
Iter: 938 loss: 2.99926057e-07
Iter: 939 loss: 2.99532502e-07
Iter: 940 loss: 2.99677168e-07
Iter: 941 loss: 2.9923649e-07
Iter: 942 loss: 2.98873829e-07
Iter: 943 loss: 3.02654769e-07
Iter: 944 loss: 2.98830457e-07
Iter: 945 loss: 2.98602657e-07
Iter: 946 loss: 3.015098e-07
Iter: 947 loss: 2.98594784e-07
Iter: 948 loss: 2.98370708e-07
Iter: 949 loss: 2.9805841e-07
Iter: 950 loss: 2.98044483e-07
Iter: 951 loss: 2.97724398e-07
Iter: 952 loss: 3.00504382e-07
Iter: 953 loss: 2.97698705e-07
Iter: 954 loss: 2.97328427e-07
Iter: 955 loss: 2.9801646e-07
Iter: 956 loss: 2.97158891e-07
Iter: 957 loss: 2.96869473e-07
Iter: 958 loss: 2.96684732e-07
Iter: 959 loss: 2.96567e-07
Iter: 960 loss: 2.96137046e-07
Iter: 961 loss: 3.01960853e-07
Iter: 962 loss: 2.9613102e-07
Iter: 963 loss: 2.95838817e-07
Iter: 964 loss: 2.95589842e-07
Iter: 965 loss: 2.95523876e-07
Iter: 966 loss: 2.95207599e-07
Iter: 967 loss: 2.99330623e-07
Iter: 968 loss: 2.95197282e-07
Iter: 969 loss: 2.94936683e-07
Iter: 970 loss: 2.94878305e-07
Iter: 971 loss: 2.94704961e-07
Iter: 972 loss: 2.94317374e-07
Iter: 973 loss: 2.94062318e-07
Iter: 974 loss: 2.93897074e-07
Iter: 975 loss: 2.93415724e-07
Iter: 976 loss: 2.95328164e-07
Iter: 977 loss: 2.93321847e-07
Iter: 978 loss: 2.92919822e-07
Iter: 979 loss: 2.97659369e-07
Iter: 980 loss: 2.92915217e-07
Iter: 981 loss: 2.92540051e-07
Iter: 982 loss: 2.92604796e-07
Iter: 983 loss: 2.92251627e-07
Iter: 984 loss: 2.91875637e-07
Iter: 985 loss: 2.91850426e-07
Iter: 986 loss: 2.91555978e-07
Iter: 987 loss: 2.91209432e-07
Iter: 988 loss: 2.91169016e-07
Iter: 989 loss: 2.90983166e-07
Iter: 990 loss: 2.90650746e-07
Iter: 991 loss: 2.9831682e-07
Iter: 992 loss: 2.90630283e-07
Iter: 993 loss: 2.90407627e-07
Iter: 994 loss: 2.90402966e-07
Iter: 995 loss: 2.90191338e-07
Iter: 996 loss: 2.89785703e-07
Iter: 997 loss: 2.89791785e-07
Iter: 998 loss: 2.8941605e-07
Iter: 999 loss: 2.92854907e-07
Iter: 1000 loss: 2.8941119e-07
Iter: 1001 loss: 2.89109977e-07
Iter: 1002 loss: 2.8993523e-07
Iter: 1003 loss: 2.89039576e-07
Iter: 1004 loss: 2.88772412e-07
Iter: 1005 loss: 2.88598955e-07
Iter: 1006 loss: 2.88517469e-07
Iter: 1007 loss: 2.88138608e-07
Iter: 1008 loss: 2.88513064e-07
Iter: 1009 loss: 2.87914474e-07
Iter: 1010 loss: 2.87549256e-07
Iter: 1011 loss: 2.8995359e-07
Iter: 1012 loss: 2.8751424e-07
Iter: 1013 loss: 2.87172156e-07
Iter: 1014 loss: 2.89674603e-07
Iter: 1015 loss: 2.87146e-07
Iter: 1016 loss: 2.86917441e-07
Iter: 1017 loss: 2.86552023e-07
Iter: 1018 loss: 2.86552222e-07
Iter: 1019 loss: 2.86405054e-07
Iter: 1020 loss: 2.86338718e-07
Iter: 1021 loss: 2.86118706e-07
Iter: 1022 loss: 2.85722422e-07
Iter: 1023 loss: 2.94908e-07
Iter: 1024 loss: 2.85721114e-07
Iter: 1025 loss: 2.85373062e-07
Iter: 1026 loss: 2.87691705e-07
Iter: 1027 loss: 2.85348e-07
Iter: 1028 loss: 2.84977546e-07
Iter: 1029 loss: 2.86193853e-07
Iter: 1030 loss: 2.84863489e-07
Iter: 1031 loss: 2.8461136e-07
Iter: 1032 loss: 2.84665845e-07
Iter: 1033 loss: 2.84426108e-07
Iter: 1034 loss: 2.8411003e-07
Iter: 1035 loss: 2.87340924e-07
Iter: 1036 loss: 2.84102185e-07
Iter: 1037 loss: 2.83915313e-07
Iter: 1038 loss: 2.83927136e-07
Iter: 1039 loss: 2.83740462e-07
Iter: 1040 loss: 2.83505415e-07
Iter: 1041 loss: 2.83522894e-07
Iter: 1042 loss: 2.83317235e-07
Iter: 1043 loss: 2.82948264e-07
Iter: 1044 loss: 2.83930774e-07
Iter: 1045 loss: 2.82816956e-07
Iter: 1046 loss: 2.82591259e-07
Iter: 1047 loss: 2.82580856e-07
Iter: 1048 loss: 2.82371047e-07
Iter: 1049 loss: 2.81992271e-07
Iter: 1050 loss: 2.82000968e-07
Iter: 1051 loss: 2.8161449e-07
Iter: 1052 loss: 2.85827269e-07
Iter: 1053 loss: 2.81600791e-07
Iter: 1054 loss: 2.81254529e-07
Iter: 1055 loss: 2.81970955e-07
Iter: 1056 loss: 2.81112307e-07
Iter: 1057 loss: 2.80897979e-07
Iter: 1058 loss: 2.81097158e-07
Iter: 1059 loss: 2.80778096e-07
Iter: 1060 loss: 2.80520254e-07
Iter: 1061 loss: 2.82773982e-07
Iter: 1062 loss: 2.80499847e-07
Iter: 1063 loss: 2.80339293e-07
Iter: 1064 loss: 2.80067979e-07
Iter: 1065 loss: 2.80056071e-07
Iter: 1066 loss: 2.79852514e-07
Iter: 1067 loss: 2.79840094e-07
Iter: 1068 loss: 2.79654074e-07
Iter: 1069 loss: 2.79405526e-07
Iter: 1070 loss: 2.79398705e-07
Iter: 1071 loss: 2.79016945e-07
Iter: 1072 loss: 2.80377606e-07
Iter: 1073 loss: 2.78935943e-07
Iter: 1074 loss: 2.78593973e-07
Iter: 1075 loss: 2.80339151e-07
Iter: 1076 loss: 2.78557764e-07
Iter: 1077 loss: 2.78297364e-07
Iter: 1078 loss: 2.79262935e-07
Iter: 1079 loss: 2.78229493e-07
Iter: 1080 loss: 2.77924585e-07
Iter: 1081 loss: 2.77800552e-07
Iter: 1082 loss: 2.77631187e-07
Iter: 1083 loss: 2.77318435e-07
Iter: 1084 loss: 2.79502899e-07
Iter: 1085 loss: 2.77299108e-07
Iter: 1086 loss: 2.77001675e-07
Iter: 1087 loss: 2.79694063e-07
Iter: 1088 loss: 2.76980529e-07
Iter: 1089 loss: 2.76840126e-07
Iter: 1090 loss: 2.76596e-07
Iter: 1091 loss: 2.76602265e-07
Iter: 1092 loss: 2.76356303e-07
Iter: 1093 loss: 2.76363664e-07
Iter: 1094 loss: 2.76186455e-07
Iter: 1095 loss: 2.75863272e-07
Iter: 1096 loss: 2.83713177e-07
Iter: 1097 loss: 2.7585827e-07
Iter: 1098 loss: 2.75578344e-07
Iter: 1099 loss: 2.79560197e-07
Iter: 1100 loss: 2.75571978e-07
Iter: 1101 loss: 2.75307116e-07
Iter: 1102 loss: 2.75125046e-07
Iter: 1103 loss: 2.75038332e-07
Iter: 1104 loss: 2.74695026e-07
Iter: 1105 loss: 2.75865375e-07
Iter: 1106 loss: 2.74624711e-07
Iter: 1107 loss: 2.74352431e-07
Iter: 1108 loss: 2.75430722e-07
Iter: 1109 loss: 2.74301982e-07
Iter: 1110 loss: 2.74025865e-07
Iter: 1111 loss: 2.74600836e-07
Iter: 1112 loss: 2.73948302e-07
Iter: 1113 loss: 2.73629439e-07
Iter: 1114 loss: 2.74505879e-07
Iter: 1115 loss: 2.73524108e-07
Iter: 1116 loss: 2.73324588e-07
Iter: 1117 loss: 2.73601898e-07
Iter: 1118 loss: 2.73235742e-07
Iter: 1119 loss: 2.73015132e-07
Iter: 1120 loss: 2.75585592e-07
Iter: 1121 loss: 2.73017861e-07
Iter: 1122 loss: 2.72867197e-07
Iter: 1123 loss: 2.72534038e-07
Iter: 1124 loss: 2.77632864e-07
Iter: 1125 loss: 2.72517156e-07
Iter: 1126 loss: 2.72331363e-07
Iter: 1127 loss: 2.72308057e-07
Iter: 1128 loss: 2.72137328e-07
Iter: 1129 loss: 2.71868629e-07
Iter: 1130 loss: 2.71853708e-07
Iter: 1131 loss: 2.71588277e-07
Iter: 1132 loss: 2.74223623e-07
Iter: 1133 loss: 2.71566108e-07
Iter: 1134 loss: 2.71377417e-07
Iter: 1135 loss: 2.71365366e-07
Iter: 1136 loss: 2.71202708e-07
Iter: 1137 loss: 2.70872249e-07
Iter: 1138 loss: 2.71341918e-07
Iter: 1139 loss: 2.70731107e-07
Iter: 1140 loss: 2.70434214e-07
Iter: 1141 loss: 2.73250691e-07
Iter: 1142 loss: 2.70411761e-07
Iter: 1143 loss: 2.70141243e-07
Iter: 1144 loss: 2.70728862e-07
Iter: 1145 loss: 2.70048474e-07
Iter: 1146 loss: 2.69733789e-07
Iter: 1147 loss: 2.70879383e-07
Iter: 1148 loss: 2.69651622e-07
Iter: 1149 loss: 2.69434111e-07
Iter: 1150 loss: 2.69419928e-07
Iter: 1151 loss: 2.69230298e-07
Iter: 1152 loss: 2.69016226e-07
Iter: 1153 loss: 2.69004033e-07
Iter: 1154 loss: 2.68848225e-07
Iter: 1155 loss: 2.68553435e-07
Iter: 1156 loss: 2.68552014e-07
Iter: 1157 loss: 2.68380063e-07
Iter: 1158 loss: 2.70863382e-07
Iter: 1159 loss: 2.68384326e-07
Iter: 1160 loss: 2.68164058e-07
Iter: 1161 loss: 2.68268195e-07
Iter: 1162 loss: 2.68013e-07
Iter: 1163 loss: 2.6782061e-07
Iter: 1164 loss: 2.6841002e-07
Iter: 1165 loss: 2.67747026e-07
Iter: 1166 loss: 2.67498621e-07
Iter: 1167 loss: 2.68535672e-07
Iter: 1168 loss: 2.67448769e-07
Iter: 1169 loss: 2.67277727e-07
Iter: 1170 loss: 2.66995215e-07
Iter: 1171 loss: 2.66981715e-07
Iter: 1172 loss: 2.66634657e-07
Iter: 1173 loss: 2.69959799e-07
Iter: 1174 loss: 2.66630622e-07
Iter: 1175 loss: 2.66370023e-07
Iter: 1176 loss: 2.67861935e-07
Iter: 1177 loss: 2.66338844e-07
Iter: 1178 loss: 2.66135174e-07
Iter: 1179 loss: 2.6645651e-07
Iter: 1180 loss: 2.66037461e-07
Iter: 1181 loss: 2.65812e-07
Iter: 1182 loss: 2.65816197e-07
Iter: 1183 loss: 2.65630774e-07
Iter: 1184 loss: 2.65460073e-07
Iter: 1185 loss: 2.65442907e-07
Iter: 1186 loss: 2.65297786e-07
Iter: 1187 loss: 2.65176368e-07
Iter: 1188 loss: 2.65121344e-07
Iter: 1189 loss: 2.64918299e-07
Iter: 1190 loss: 2.65081979e-07
Iter: 1191 loss: 2.64810581e-07
Iter: 1192 loss: 2.64561521e-07
Iter: 1193 loss: 2.68201575e-07
Iter: 1194 loss: 2.64548618e-07
Iter: 1195 loss: 2.64450961e-07
Iter: 1196 loss: 2.64287166e-07
Iter: 1197 loss: 2.64277162e-07
Iter: 1198 loss: 2.64031854e-07
Iter: 1199 loss: 2.65662436e-07
Iter: 1200 loss: 2.6402131e-07
Iter: 1201 loss: 2.63840377e-07
Iter: 1202 loss: 2.63741299e-07
Iter: 1203 loss: 2.63645916e-07
Iter: 1204 loss: 2.63447106e-07
Iter: 1205 loss: 2.64136787e-07
Iter: 1206 loss: 2.63383271e-07
Iter: 1207 loss: 2.63207596e-07
Iter: 1208 loss: 2.65143086e-07
Iter: 1209 loss: 2.63187758e-07
Iter: 1210 loss: 2.63019388e-07
Iter: 1211 loss: 2.63241191e-07
Iter: 1212 loss: 2.6294407e-07
Iter: 1213 loss: 2.62749666e-07
Iter: 1214 loss: 2.62807049e-07
Iter: 1215 loss: 2.62628362e-07
Iter: 1216 loss: 2.62439215e-07
Iter: 1217 loss: 2.64172172e-07
Iter: 1218 loss: 2.62437823e-07
Iter: 1219 loss: 2.6223978e-07
Iter: 1220 loss: 2.62478068e-07
Iter: 1221 loss: 2.62130698e-07
Iter: 1222 loss: 2.61974606e-07
Iter: 1223 loss: 2.61806122e-07
Iter: 1224 loss: 2.61761215e-07
Iter: 1225 loss: 2.61628315e-07
Iter: 1226 loss: 2.61598558e-07
Iter: 1227 loss: 2.61471484e-07
Iter: 1228 loss: 2.61253348e-07
Iter: 1229 loss: 2.66262589e-07
Iter: 1230 loss: 2.61252296e-07
Iter: 1231 loss: 2.61058233e-07
Iter: 1232 loss: 2.61055106e-07
Iter: 1233 loss: 2.60914078e-07
Iter: 1234 loss: 2.60808463e-07
Iter: 1235 loss: 2.60762874e-07
Iter: 1236 loss: 2.60532062e-07
Iter: 1237 loss: 2.60413969e-07
Iter: 1238 loss: 2.60328079e-07
Iter: 1239 loss: 2.60109459e-07
Iter: 1240 loss: 2.60089223e-07
Iter: 1241 loss: 2.59924718e-07
Iter: 1242 loss: 2.6044566e-07
Iter: 1243 loss: 2.59871513e-07
Iter: 1244 loss: 2.59695611e-07
Iter: 1245 loss: 2.59639478e-07
Iter: 1246 loss: 2.59538382e-07
Iter: 1247 loss: 2.59360718e-07
Iter: 1248 loss: 2.61086484e-07
Iter: 1249 loss: 2.59333405e-07
Iter: 1250 loss: 2.59156053e-07
Iter: 1251 loss: 2.59753051e-07
Iter: 1252 loss: 2.5912459e-07
Iter: 1253 loss: 2.58969351e-07
Iter: 1254 loss: 2.58691557e-07
Iter: 1255 loss: 2.64723383e-07
Iter: 1256 loss: 2.58693973e-07
Iter: 1257 loss: 2.58485045e-07
Iter: 1258 loss: 2.58467054e-07
Iter: 1259 loss: 2.58265885e-07
Iter: 1260 loss: 2.58075545e-07
Iter: 1261 loss: 2.58013785e-07
Iter: 1262 loss: 2.57804288e-07
Iter: 1263 loss: 2.59480828e-07
Iter: 1264 loss: 2.57798433e-07
Iter: 1265 loss: 2.57570605e-07
Iter: 1266 loss: 2.57530843e-07
Iter: 1267 loss: 2.5737188e-07
Iter: 1268 loss: 2.57130239e-07
Iter: 1269 loss: 2.57178016e-07
Iter: 1270 loss: 2.5693862e-07
Iter: 1271 loss: 2.56715055e-07
Iter: 1272 loss: 2.57153289e-07
Iter: 1273 loss: 2.5659449e-07
Iter: 1274 loss: 2.56401194e-07
Iter: 1275 loss: 2.56385817e-07
Iter: 1276 loss: 2.56272585e-07
Iter: 1277 loss: 2.56245585e-07
Iter: 1278 loss: 2.56161371e-07
Iter: 1279 loss: 2.5598905e-07
Iter: 1280 loss: 2.56212672e-07
Iter: 1281 loss: 2.5591018e-07
Iter: 1282 loss: 2.55725524e-07
Iter: 1283 loss: 2.57910727e-07
Iter: 1284 loss: 2.55722682e-07
Iter: 1285 loss: 2.55570626e-07
Iter: 1286 loss: 2.55320288e-07
Iter: 1287 loss: 2.55307611e-07
Iter: 1288 loss: 2.55144869e-07
Iter: 1289 loss: 2.57581462e-07
Iter: 1290 loss: 2.55144698e-07
Iter: 1291 loss: 2.54971383e-07
Iter: 1292 loss: 2.55368633e-07
Iter: 1293 loss: 2.54938925e-07
Iter: 1294 loss: 2.54793804e-07
Iter: 1295 loss: 2.54814267e-07
Iter: 1296 loss: 2.54694243e-07
Iter: 1297 loss: 2.54502027e-07
Iter: 1298 loss: 2.55670784e-07
Iter: 1299 loss: 2.54462549e-07
Iter: 1300 loss: 2.54321264e-07
Iter: 1301 loss: 2.54157271e-07
Iter: 1302 loss: 2.54130185e-07
Iter: 1303 loss: 2.53844945e-07
Iter: 1304 loss: 2.5401161e-07
Iter: 1305 loss: 2.53659437e-07
Iter: 1306 loss: 2.53396166e-07
Iter: 1307 loss: 2.54934491e-07
Iter: 1308 loss: 2.53359076e-07
Iter: 1309 loss: 2.53100239e-07
Iter: 1310 loss: 2.56245755e-07
Iter: 1311 loss: 2.53102968e-07
Iter: 1312 loss: 2.52989651e-07
Iter: 1313 loss: 2.52926469e-07
Iter: 1314 loss: 2.52885258e-07
Iter: 1315 loss: 2.52691763e-07
Iter: 1316 loss: 2.53049677e-07
Iter: 1317 loss: 2.52634294e-07
Iter: 1318 loss: 2.52431363e-07
Iter: 1319 loss: 2.54373333e-07
Iter: 1320 loss: 2.52422296e-07
Iter: 1321 loss: 2.52287e-07
Iter: 1322 loss: 2.52049489e-07
Iter: 1323 loss: 2.52068844e-07
Iter: 1324 loss: 2.51901525e-07
Iter: 1325 loss: 2.54292758e-07
Iter: 1326 loss: 2.51896864e-07
Iter: 1327 loss: 2.51722952e-07
Iter: 1328 loss: 2.52004213e-07
Iter: 1329 loss: 2.51662044e-07
Iter: 1330 loss: 2.51536676e-07
Iter: 1331 loss: 2.51611425e-07
Iter: 1332 loss: 2.51451041e-07
Iter: 1333 loss: 2.51269881e-07
Iter: 1334 loss: 2.52221639e-07
Iter: 1335 loss: 2.51238276e-07
Iter: 1336 loss: 2.51097617e-07
Iter: 1337 loss: 2.50841765e-07
Iter: 1338 loss: 2.5682678e-07
Iter: 1339 loss: 2.50841822e-07
Iter: 1340 loss: 2.5053015e-07
Iter: 1341 loss: 2.51508283e-07
Iter: 1342 loss: 2.50422943e-07
Iter: 1343 loss: 2.50189032e-07
Iter: 1344 loss: 2.51416481e-07
Iter: 1345 loss: 2.50155239e-07
Iter: 1346 loss: 2.49980474e-07
Iter: 1347 loss: 2.49982804e-07
Iter: 1348 loss: 2.49853457e-07
Iter: 1349 loss: 2.49761683e-07
Iter: 1350 loss: 2.49735194e-07
Iter: 1351 loss: 2.49596923e-07
Iter: 1352 loss: 2.50295898e-07
Iter: 1353 loss: 2.49597036e-07
Iter: 1354 loss: 2.49403712e-07
Iter: 1355 loss: 2.50016285e-07
Iter: 1356 loss: 2.49387313e-07
Iter: 1357 loss: 2.49280163e-07
Iter: 1358 loss: 2.49155107e-07
Iter: 1359 loss: 2.49127083e-07
Iter: 1360 loss: 2.49019195e-07
Iter: 1361 loss: 2.49026129e-07
Iter: 1362 loss: 2.48880298e-07
Iter: 1363 loss: 2.48882571e-07
Iter: 1364 loss: 2.48789519e-07
Iter: 1365 loss: 2.48613105e-07
Iter: 1366 loss: 2.48513658e-07
Iter: 1367 loss: 2.4843564e-07
Iter: 1368 loss: 2.48281538e-07
Iter: 1369 loss: 2.48288615e-07
Iter: 1370 loss: 2.48155743e-07
Iter: 1371 loss: 2.47952698e-07
Iter: 1372 loss: 2.47953579e-07
Iter: 1373 loss: 2.47691673e-07
Iter: 1374 loss: 2.48322067e-07
Iter: 1375 loss: 2.47620449e-07
Iter: 1376 loss: 2.47439687e-07
Iter: 1377 loss: 2.47846884e-07
Iter: 1378 loss: 2.47354222e-07
Iter: 1379 loss: 2.47282173e-07
Iter: 1380 loss: 2.4726387e-07
Iter: 1381 loss: 2.47179742e-07
Iter: 1382 loss: 2.46992414e-07
Iter: 1383 loss: 2.50069576e-07
Iter: 1384 loss: 2.46988634e-07
Iter: 1385 loss: 2.46820946e-07
Iter: 1386 loss: 2.48726536e-07
Iter: 1387 loss: 2.46827938e-07
Iter: 1388 loss: 2.4665826e-07
Iter: 1389 loss: 2.46811425e-07
Iter: 1390 loss: 2.46594482e-07
Iter: 1391 loss: 2.46393199e-07
Iter: 1392 loss: 2.46116372e-07
Iter: 1393 loss: 2.46108e-07
Iter: 1394 loss: 2.45984268e-07
Iter: 1395 loss: 2.4595542e-07
Iter: 1396 loss: 2.45779717e-07
Iter: 1397 loss: 2.45554531e-07
Iter: 1398 loss: 2.45547739e-07
Iter: 1399 loss: 2.45399974e-07
Iter: 1400 loss: 2.4662e-07
Iter: 1401 loss: 2.45382694e-07
Iter: 1402 loss: 2.4522646e-07
Iter: 1403 loss: 2.4539591e-07
Iter: 1404 loss: 2.45155377e-07
Iter: 1405 loss: 2.4495597e-07
Iter: 1406 loss: 2.44891623e-07
Iter: 1407 loss: 2.44782711e-07
Iter: 1408 loss: 2.44563552e-07
Iter: 1409 loss: 2.45663784e-07
Iter: 1410 loss: 2.44526802e-07
Iter: 1411 loss: 2.4431111e-07
Iter: 1412 loss: 2.44283314e-07
Iter: 1413 loss: 2.4414598e-07
Iter: 1414 loss: 2.43821546e-07
Iter: 1415 loss: 2.45315135e-07
Iter: 1416 loss: 2.43744239e-07
Iter: 1417 loss: 2.43561573e-07
Iter: 1418 loss: 2.4654932e-07
Iter: 1419 loss: 2.43556883e-07
Iter: 1420 loss: 2.43362706e-07
Iter: 1421 loss: 2.43375325e-07
Iter: 1422 loss: 2.43189561e-07
Iter: 1423 loss: 2.43037022e-07
Iter: 1424 loss: 2.44315487e-07
Iter: 1425 loss: 2.43043303e-07
Iter: 1426 loss: 2.429029e-07
Iter: 1427 loss: 2.42997345e-07
Iter: 1428 loss: 2.42787934e-07
Iter: 1429 loss: 2.42647616e-07
Iter: 1430 loss: 2.42558627e-07
Iter: 1431 loss: 2.42498118e-07
Iter: 1432 loss: 2.4234896e-07
Iter: 1433 loss: 2.42351547e-07
Iter: 1434 loss: 2.42208898e-07
Iter: 1435 loss: 2.42048969e-07
Iter: 1436 loss: 2.42026175e-07
Iter: 1437 loss: 2.41828928e-07
Iter: 1438 loss: 2.42087168e-07
Iter: 1439 loss: 2.41740565e-07
Iter: 1440 loss: 2.41537919e-07
Iter: 1441 loss: 2.41547951e-07
Iter: 1442 loss: 2.41411556e-07
Iter: 1443 loss: 2.41267912e-07
Iter: 1444 loss: 2.4124779e-07
Iter: 1445 loss: 2.41026385e-07
Iter: 1446 loss: 2.41274307e-07
Iter: 1447 loss: 2.40911106e-07
Iter: 1448 loss: 2.40668385e-07
Iter: 1449 loss: 2.41679203e-07
Iter: 1450 loss: 2.40628e-07
Iter: 1451 loss: 2.40438112e-07
Iter: 1452 loss: 2.41921384e-07
Iter: 1453 loss: 2.4042086e-07
Iter: 1454 loss: 2.40240183e-07
Iter: 1455 loss: 2.4051451e-07
Iter: 1456 loss: 2.40129111e-07
Iter: 1457 loss: 2.39962446e-07
Iter: 1458 loss: 2.40498451e-07
Iter: 1459 loss: 2.3991629e-07
Iter: 1460 loss: 2.39698238e-07
Iter: 1461 loss: 2.40163388e-07
Iter: 1462 loss: 2.39626303e-07
Iter: 1463 loss: 2.39456256e-07
Iter: 1464 loss: 2.39124347e-07
Iter: 1465 loss: 2.46018317e-07
Iter: 1466 loss: 2.39129918e-07
Iter: 1467 loss: 2.39131822e-07
Iter: 1468 loss: 2.38985535e-07
Iter: 1469 loss: 2.38858377e-07
Iter: 1470 loss: 2.38715018e-07
Iter: 1471 loss: 2.38703677e-07
Iter: 1472 loss: 2.38494181e-07
Iter: 1473 loss: 2.38347042e-07
Iter: 1474 loss: 2.38281558e-07
Iter: 1475 loss: 2.38083118e-07
Iter: 1476 loss: 2.38080275e-07
Iter: 1477 loss: 2.37882631e-07
Iter: 1478 loss: 2.3825595e-07
Iter: 1479 loss: 2.37783397e-07
Iter: 1480 loss: 2.37598e-07
Iter: 1481 loss: 2.3732423e-07
Iter: 1482 loss: 2.37338611e-07
Iter: 1483 loss: 2.37097822e-07
Iter: 1484 loss: 2.39802489e-07
Iter: 1485 loss: 2.37103791e-07
Iter: 1486 loss: 2.36921267e-07
Iter: 1487 loss: 2.38216828e-07
Iter: 1488 loss: 2.36931555e-07
Iter: 1489 loss: 2.36775151e-07
Iter: 1490 loss: 2.37127153e-07
Iter: 1491 loss: 2.36735019e-07
Iter: 1492 loss: 2.36615776e-07
Iter: 1493 loss: 2.36928614e-07
Iter: 1494 loss: 2.36587709e-07
Iter: 1495 loss: 2.36432498e-07
Iter: 1496 loss: 2.36392538e-07
Iter: 1497 loss: 2.36283114e-07
Iter: 1498 loss: 2.36102892e-07
Iter: 1499 loss: 2.36278893e-07
Iter: 1500 loss: 2.36007054e-07
Iter: 1501 loss: 2.35814795e-07
Iter: 1502 loss: 2.35777094e-07
Iter: 1503 loss: 2.3566183e-07
Iter: 1504 loss: 2.3538459e-07
Iter: 1505 loss: 2.36928884e-07
Iter: 1506 loss: 2.35322489e-07
Iter: 1507 loss: 2.35159675e-07
Iter: 1508 loss: 2.35132774e-07
Iter: 1509 loss: 2.35054401e-07
Iter: 1510 loss: 2.34820163e-07
Iter: 1511 loss: 2.36315557e-07
Iter: 1512 loss: 2.34756413e-07
Iter: 1513 loss: 2.34478136e-07
Iter: 1514 loss: 2.36059378e-07
Iter: 1515 loss: 2.34435689e-07
Iter: 1516 loss: 2.34295769e-07
Iter: 1517 loss: 2.34276229e-07
Iter: 1518 loss: 2.34155493e-07
Iter: 1519 loss: 2.33929711e-07
Iter: 1520 loss: 2.38892028e-07
Iter: 1521 loss: 2.33937129e-07
Iter: 1522 loss: 2.33669e-07
Iter: 1523 loss: 2.33846094e-07
Iter: 1524 loss: 2.33509056e-07
Iter: 1525 loss: 2.33332287e-07
Iter: 1526 loss: 2.360118e-07
Iter: 1527 loss: 2.33328336e-07
Iter: 1528 loss: 2.33137698e-07
Iter: 1529 loss: 2.34010272e-07
Iter: 1530 loss: 2.33106562e-07
Iter: 1531 loss: 2.32946661e-07
Iter: 1532 loss: 2.3295911e-07
Iter: 1533 loss: 2.32819588e-07
Iter: 1534 loss: 2.326979e-07
Iter: 1535 loss: 2.32702462e-07
Iter: 1536 loss: 2.32588988e-07
Iter: 1537 loss: 2.32499531e-07
Iter: 1538 loss: 2.32474179e-07
Iter: 1539 loss: 2.323237e-07
Iter: 1540 loss: 2.32072651e-07
Iter: 1541 loss: 2.32074512e-07
Iter: 1542 loss: 2.31908047e-07
Iter: 1543 loss: 2.31894859e-07
Iter: 1544 loss: 2.31711425e-07
Iter: 1545 loss: 2.32163075e-07
Iter: 1546 loss: 2.31663677e-07
Iter: 1547 loss: 2.31491541e-07
Iter: 1548 loss: 2.31151347e-07
Iter: 1549 loss: 2.3801897e-07
Iter: 1550 loss: 2.3116057e-07
Iter: 1551 loss: 2.30877504e-07
Iter: 1552 loss: 2.32328091e-07
Iter: 1553 loss: 2.30847675e-07
Iter: 1554 loss: 2.30751652e-07
Iter: 1555 loss: 2.30728332e-07
Iter: 1556 loss: 2.30621126e-07
Iter: 1557 loss: 2.30519021e-07
Iter: 1558 loss: 2.3049725e-07
Iter: 1559 loss: 2.30343275e-07
Iter: 1560 loss: 2.30278232e-07
Iter: 1561 loss: 2.30207789e-07
Iter: 1562 loss: 2.3004e-07
Iter: 1563 loss: 2.31982909e-07
Iter: 1564 loss: 2.30039802e-07
Iter: 1565 loss: 2.29887831e-07
Iter: 1566 loss: 2.30461637e-07
Iter: 1567 loss: 2.29854521e-07
Iter: 1568 loss: 2.29729551e-07
Iter: 1569 loss: 2.29683565e-07
Iter: 1570 loss: 2.29618792e-07
Iter: 1571 loss: 2.29403696e-07
Iter: 1572 loss: 2.30507283e-07
Iter: 1573 loss: 2.29357227e-07
Iter: 1574 loss: 2.291942e-07
Iter: 1575 loss: 2.29155731e-07
Iter: 1576 loss: 2.29048439e-07
Iter: 1577 loss: 2.28887245e-07
Iter: 1578 loss: 2.29241209e-07
Iter: 1579 loss: 2.28841984e-07
Iter: 1580 loss: 2.28708274e-07
Iter: 1581 loss: 2.28707989e-07
Iter: 1582 loss: 2.28591659e-07
Iter: 1583 loss: 2.28544437e-07
Iter: 1584 loss: 2.28468252e-07
Iter: 1585 loss: 2.28331061e-07
Iter: 1586 loss: 2.28163017e-07
Iter: 1587 loss: 2.28151592e-07
Iter: 1588 loss: 2.27910178e-07
Iter: 1589 loss: 2.2930179e-07
Iter: 1590 loss: 2.27869663e-07
Iter: 1591 loss: 2.27732031e-07
Iter: 1592 loss: 2.27732045e-07
Iter: 1593 loss: 2.27662085e-07
Iter: 1594 loss: 2.27416081e-07
Iter: 1595 loss: 2.28580063e-07
Iter: 1596 loss: 2.27351023e-07
Iter: 1597 loss: 2.27219346e-07
Iter: 1598 loss: 2.27200928e-07
Iter: 1599 loss: 2.27068625e-07
Iter: 1600 loss: 2.27608453e-07
Iter: 1601 loss: 2.27037518e-07
Iter: 1602 loss: 2.26924556e-07
Iter: 1603 loss: 2.26716054e-07
Iter: 1604 loss: 2.31839763e-07
Iter: 1605 loss: 2.26718583e-07
Iter: 1606 loss: 2.26636885e-07
Iter: 1607 loss: 2.26573775e-07
Iter: 1608 loss: 2.26519234e-07
Iter: 1609 loss: 2.26401653e-07
Iter: 1610 loss: 2.29067382e-07
Iter: 1611 loss: 2.26398228e-07
Iter: 1612 loss: 2.26257271e-07
Iter: 1613 loss: 2.26247295e-07
Iter: 1614 loss: 2.26134233e-07
Iter: 1615 loss: 2.25948256e-07
Iter: 1616 loss: 2.26950107e-07
Iter: 1617 loss: 2.25921383e-07
Iter: 1618 loss: 2.2576576e-07
Iter: 1619 loss: 2.26014009e-07
Iter: 1620 loss: 2.25676928e-07
Iter: 1621 loss: 2.25547453e-07
Iter: 1622 loss: 2.25548149e-07
Iter: 1623 loss: 2.25420621e-07
Iter: 1624 loss: 2.25282037e-07
Iter: 1625 loss: 2.25284282e-07
Iter: 1626 loss: 2.2511486e-07
Iter: 1627 loss: 2.25328108e-07
Iter: 1628 loss: 2.25018539e-07
Iter: 1629 loss: 2.24874782e-07
Iter: 1630 loss: 2.24862134e-07
Iter: 1631 loss: 2.24783363e-07
Iter: 1632 loss: 2.24646357e-07
Iter: 1633 loss: 2.24648247e-07
Iter: 1634 loss: 2.24542987e-07
Iter: 1635 loss: 2.24541736e-07
Iter: 1636 loss: 2.24432796e-07
Iter: 1637 loss: 2.24273407e-07
Iter: 1638 loss: 2.24282502e-07
Iter: 1639 loss: 2.24155897e-07
Iter: 1640 loss: 2.24152942e-07
Iter: 1641 loss: 2.24051121e-07
Iter: 1642 loss: 2.2397802e-07
Iter: 1643 loss: 2.23957983e-07
Iter: 1644 loss: 2.23823164e-07
Iter: 1645 loss: 2.23652279e-07
Iter: 1646 loss: 2.23653814e-07
Iter: 1647 loss: 2.23491753e-07
Iter: 1648 loss: 2.23923308e-07
Iter: 1649 loss: 2.23438988e-07
Iter: 1650 loss: 2.23270774e-07
Iter: 1651 loss: 2.23059573e-07
Iter: 1652 loss: 2.23036665e-07
Iter: 1653 loss: 2.22668717e-07
Iter: 1654 loss: 2.23738482e-07
Iter: 1655 loss: 2.22556025e-07
Iter: 1656 loss: 2.22319102e-07
Iter: 1657 loss: 2.24028199e-07
Iter: 1658 loss: 2.2230202e-07
Iter: 1659 loss: 2.22127625e-07
Iter: 1660 loss: 2.23283251e-07
Iter: 1661 loss: 2.22111865e-07
Iter: 1662 loss: 2.21923116e-07
Iter: 1663 loss: 2.22196093e-07
Iter: 1664 loss: 2.21844658e-07
Iter: 1665 loss: 2.21745424e-07
Iter: 1666 loss: 2.21816435e-07
Iter: 1667 loss: 2.21673545e-07
Iter: 1668 loss: 2.21490495e-07
Iter: 1669 loss: 2.22060351e-07
Iter: 1670 loss: 2.21431094e-07
Iter: 1671 loss: 2.2127773e-07
Iter: 1672 loss: 2.2107929e-07
Iter: 1673 loss: 2.21042228e-07
Iter: 1674 loss: 2.20902351e-07
Iter: 1675 loss: 2.20894293e-07
Iter: 1676 loss: 2.20735629e-07
Iter: 1677 loss: 2.20688591e-07
Iter: 1678 loss: 2.20586372e-07
Iter: 1679 loss: 2.20387719e-07
Iter: 1680 loss: 2.20236601e-07
Iter: 1681 loss: 2.20182429e-07
Iter: 1682 loss: 2.20097135e-07
Iter: 1683 loss: 2.20036725e-07
Iter: 1684 loss: 2.19929944e-07
Iter: 1685 loss: 2.19696659e-07
Iter: 1686 loss: 2.23558757e-07
Iter: 1687 loss: 2.19686086e-07
Iter: 1688 loss: 2.19492549e-07
Iter: 1689 loss: 2.20798086e-07
Iter: 1690 loss: 2.19466386e-07
Iter: 1691 loss: 2.19308902e-07
Iter: 1692 loss: 2.20501818e-07
Iter: 1693 loss: 2.19302365e-07
Iter: 1694 loss: 2.19170687e-07
Iter: 1695 loss: 2.19502397e-07
Iter: 1696 loss: 2.19138045e-07
Iter: 1697 loss: 2.1901144e-07
Iter: 1698 loss: 2.18897412e-07
Iter: 1699 loss: 2.18865921e-07
Iter: 1700 loss: 2.18692563e-07
Iter: 1701 loss: 2.21040239e-07
Iter: 1702 loss: 2.18708138e-07
Iter: 1703 loss: 2.18564949e-07
Iter: 1704 loss: 2.18559578e-07
Iter: 1705 loss: 2.18459348e-07
Iter: 1706 loss: 2.18309225e-07
Iter: 1707 loss: 2.18308514e-07
Iter: 1708 loss: 2.18176154e-07
Iter: 1709 loss: 2.17985274e-07
Iter: 1710 loss: 2.19004448e-07
Iter: 1711 loss: 2.17948696e-07
Iter: 1712 loss: 2.17815511e-07
Iter: 1713 loss: 2.20021121e-07
Iter: 1714 loss: 2.17806843e-07
Iter: 1715 loss: 2.17686349e-07
Iter: 1716 loss: 2.17566196e-07
Iter: 1717 loss: 2.17540517e-07
Iter: 1718 loss: 2.17407333e-07
Iter: 1719 loss: 2.17306095e-07
Iter: 1720 loss: 2.17243e-07
Iter: 1721 loss: 2.17029196e-07
Iter: 1722 loss: 2.17020315e-07
Iter: 1723 loss: 2.16875407e-07
Iter: 1724 loss: 2.16625608e-07
Iter: 1725 loss: 2.16622936e-07
Iter: 1726 loss: 2.16478838e-07
Iter: 1727 loss: 2.16468692e-07
Iter: 1728 loss: 2.16315129e-07
Iter: 1729 loss: 2.16346919e-07
Iter: 1730 loss: 2.16188866e-07
Iter: 1731 loss: 2.16035517e-07
Iter: 1732 loss: 2.16222233e-07
Iter: 1733 loss: 2.15957172e-07
Iter: 1734 loss: 2.15759258e-07
Iter: 1735 loss: 2.17596465e-07
Iter: 1736 loss: 2.15770967e-07
Iter: 1737 loss: 2.15618812e-07
Iter: 1738 loss: 2.15379288e-07
Iter: 1739 loss: 2.21374449e-07
Iter: 1740 loss: 2.15369809e-07
Iter: 1741 loss: 2.15131266e-07
Iter: 1742 loss: 2.15507725e-07
Iter: 1743 loss: 2.1503152e-07
Iter: 1744 loss: 2.14777899e-07
Iter: 1745 loss: 2.16015309e-07
Iter: 1746 loss: 2.14726199e-07
Iter: 1747 loss: 2.14597122e-07
Iter: 1748 loss: 2.14574072e-07
Iter: 1749 loss: 2.14464251e-07
Iter: 1750 loss: 2.14397659e-07
Iter: 1751 loss: 2.14343146e-07
Iter: 1752 loss: 2.14187779e-07
Iter: 1753 loss: 2.14161602e-07
Iter: 1754 loss: 2.14072173e-07
Iter: 1755 loss: 2.13889365e-07
Iter: 1756 loss: 2.14942915e-07
Iter: 1757 loss: 2.13856993e-07
Iter: 1758 loss: 2.13721e-07
Iter: 1759 loss: 2.15595819e-07
Iter: 1760 loss: 2.13725556e-07
Iter: 1761 loss: 2.13622855e-07
Iter: 1762 loss: 2.13406025e-07
Iter: 1763 loss: 2.17211948e-07
Iter: 1764 loss: 2.13400199e-07
Iter: 1765 loss: 2.13190418e-07
Iter: 1766 loss: 2.15285937e-07
Iter: 1767 loss: 2.13179987e-07
Iter: 1768 loss: 2.12976019e-07
Iter: 1769 loss: 2.13559176e-07
Iter: 1770 loss: 2.12910294e-07
Iter: 1771 loss: 2.12782126e-07
Iter: 1772 loss: 2.12607176e-07
Iter: 1773 loss: 2.12604832e-07
Iter: 1774 loss: 2.12502698e-07
Iter: 1775 loss: 2.12481822e-07
Iter: 1776 loss: 2.1240109e-07
Iter: 1777 loss: 2.12393175e-07
Iter: 1778 loss: 2.1233339e-07
Iter: 1779 loss: 2.12227334e-07
Iter: 1780 loss: 2.12035076e-07
Iter: 1781 loss: 2.16157872e-07
Iter: 1782 loss: 2.12044341e-07
Iter: 1783 loss: 2.12048008e-07
Iter: 1784 loss: 2.11940318e-07
Iter: 1785 loss: 2.11866535e-07
Iter: 1786 loss: 2.11755335e-07
Iter: 1787 loss: 2.14480082e-07
Iter: 1788 loss: 2.11757254e-07
Iter: 1789 loss: 2.11617689e-07
Iter: 1790 loss: 2.11431157e-07
Iter: 1791 loss: 2.11411063e-07
Iter: 1792 loss: 2.11181501e-07
Iter: 1793 loss: 2.12357307e-07
Iter: 1794 loss: 2.11123165e-07
Iter: 1795 loss: 2.10972246e-07
Iter: 1796 loss: 2.10981156e-07
Iter: 1797 loss: 2.10886498e-07
Iter: 1798 loss: 2.10727094e-07
Iter: 1799 loss: 2.14355993e-07
Iter: 1800 loss: 2.10737113e-07
Iter: 1801 loss: 2.1053836e-07
Iter: 1802 loss: 2.10932527e-07
Iter: 1803 loss: 2.10471327e-07
Iter: 1804 loss: 2.10376257e-07
Iter: 1805 loss: 2.10345405e-07
Iter: 1806 loss: 2.10266649e-07
Iter: 1807 loss: 2.10146894e-07
Iter: 1808 loss: 2.10133535e-07
Iter: 1809 loss: 2.09996657e-07
Iter: 1810 loss: 2.11784169e-07
Iter: 1811 loss: 2.09990333e-07
Iter: 1812 loss: 2.09862975e-07
Iter: 1813 loss: 2.09746901e-07
Iter: 1814 loss: 2.09725542e-07
Iter: 1815 loss: 2.09547693e-07
Iter: 1816 loss: 2.09878721e-07
Iter: 1817 loss: 2.09492754e-07
Iter: 1818 loss: 2.09348542e-07
Iter: 1819 loss: 2.09341607e-07
Iter: 1820 loss: 2.09250771e-07
Iter: 1821 loss: 2.09088057e-07
Iter: 1822 loss: 2.09086565e-07
Iter: 1823 loss: 2.08903202e-07
Iter: 1824 loss: 2.09460836e-07
Iter: 1825 loss: 2.08868499e-07
Iter: 1826 loss: 2.08701124e-07
Iter: 1827 loss: 2.09349906e-07
Iter: 1828 loss: 2.08674223e-07
Iter: 1829 loss: 2.08525847e-07
Iter: 1830 loss: 2.0898635e-07
Iter: 1831 loss: 2.08486966e-07
Iter: 1832 loss: 2.08356397e-07
Iter: 1833 loss: 2.09537021e-07
Iter: 1834 loss: 2.08340907e-07
Iter: 1835 loss: 2.08237225e-07
Iter: 1836 loss: 2.07987441e-07
Iter: 1837 loss: 2.11733493e-07
Iter: 1838 loss: 2.07996948e-07
Iter: 1839 loss: 2.07807716e-07
Iter: 1840 loss: 2.07809492e-07
Iter: 1841 loss: 2.07666702e-07
Iter: 1842 loss: 2.07847037e-07
Iter: 1843 loss: 2.07592393e-07
Iter: 1844 loss: 2.07460346e-07
Iter: 1845 loss: 2.07649734e-07
Iter: 1846 loss: 2.07393143e-07
Iter: 1847 loss: 2.07208757e-07
Iter: 1848 loss: 2.07911171e-07
Iter: 1849 loss: 2.0715207e-07
Iter: 1850 loss: 2.07029927e-07
Iter: 1851 loss: 2.06993207e-07
Iter: 1852 loss: 2.06927055e-07
Iter: 1853 loss: 2.06774743e-07
Iter: 1854 loss: 2.06784179e-07
Iter: 1855 loss: 2.06682e-07
Iter: 1856 loss: 2.06463753e-07
Iter: 1857 loss: 2.0932157e-07
Iter: 1858 loss: 2.06456193e-07
Iter: 1859 loss: 2.06230965e-07
Iter: 1860 loss: 2.06657759e-07
Iter: 1861 loss: 2.06144847e-07
Iter: 1862 loss: 2.05932906e-07
Iter: 1863 loss: 2.0818527e-07
Iter: 1864 loss: 2.05916805e-07
Iter: 1865 loss: 2.05777042e-07
Iter: 1866 loss: 2.06832269e-07
Iter: 1867 loss: 2.05781674e-07
Iter: 1868 loss: 2.05666069e-07
Iter: 1869 loss: 2.05844572e-07
Iter: 1870 loss: 2.05605332e-07
Iter: 1871 loss: 2.05482394e-07
Iter: 1872 loss: 2.05551203e-07
Iter: 1873 loss: 2.0539683e-07
Iter: 1874 loss: 2.05249052e-07
Iter: 1875 loss: 2.06459532e-07
Iter: 1876 loss: 2.05247787e-07
Iter: 1877 loss: 2.05134228e-07
Iter: 1878 loss: 2.05047741e-07
Iter: 1879 loss: 2.05021138e-07
Iter: 1880 loss: 2.04892629e-07
Iter: 1881 loss: 2.0687969e-07
Iter: 1882 loss: 2.04893482e-07
Iter: 1883 loss: 2.04760426e-07
Iter: 1884 loss: 2.0456153e-07
Iter: 1885 loss: 2.04572302e-07
Iter: 1886 loss: 2.04422136e-07
Iter: 1887 loss: 2.04414292e-07
Iter: 1888 loss: 2.0425469e-07
Iter: 1889 loss: 2.04518102e-07
Iter: 1890 loss: 2.04175279e-07
Iter: 1891 loss: 2.04059205e-07
Iter: 1892 loss: 2.0390442e-07
Iter: 1893 loss: 2.03886955e-07
Iter: 1894 loss: 2.03677672e-07
Iter: 1895 loss: 2.04264069e-07
Iter: 1896 loss: 2.03613411e-07
Iter: 1897 loss: 2.03451748e-07
Iter: 1898 loss: 2.0532157e-07
Iter: 1899 loss: 2.03454945e-07
Iter: 1900 loss: 2.03337919e-07
Iter: 1901 loss: 2.03740584e-07
Iter: 1902 loss: 2.03279541e-07
Iter: 1903 loss: 2.03157214e-07
Iter: 1904 loss: 2.03551707e-07
Iter: 1905 loss: 2.03112165e-07
Iter: 1906 loss: 2.02996148e-07
Iter: 1907 loss: 2.03303046e-07
Iter: 1908 loss: 2.02954e-07
Iter: 1909 loss: 2.02804856e-07
Iter: 1910 loss: 2.02803676e-07
Iter: 1911 loss: 2.0267882e-07
Iter: 1912 loss: 2.02525769e-07
Iter: 1913 loss: 2.03662239e-07
Iter: 1914 loss: 2.02518933e-07
Iter: 1915 loss: 2.02391604e-07
Iter: 1916 loss: 2.02774842e-07
Iter: 1917 loss: 2.02347366e-07
Iter: 1918 loss: 2.02242646e-07
Iter: 1919 loss: 2.02088899e-07
Iter: 1920 loss: 2.02079946e-07
Iter: 1921 loss: 2.01982346e-07
Iter: 1922 loss: 2.01952574e-07
Iter: 1923 loss: 2.01877796e-07
Iter: 1924 loss: 2.01728099e-07
Iter: 1925 loss: 2.04889417e-07
Iter: 1926 loss: 2.01723338e-07
Iter: 1927 loss: 2.01538612e-07
Iter: 1928 loss: 2.0144023e-07
Iter: 1929 loss: 2.01368863e-07
Iter: 1930 loss: 2.01085044e-07
Iter: 1931 loss: 2.0197632e-07
Iter: 1932 loss: 2.0100083e-07
Iter: 1933 loss: 2.00857755e-07
Iter: 1934 loss: 2.00829e-07
Iter: 1935 loss: 2.00712151e-07
Iter: 1936 loss: 2.00683303e-07
Iter: 1937 loss: 2.00613357e-07
Iter: 1938 loss: 2.00476478e-07
Iter: 1939 loss: 2.01321598e-07
Iter: 1940 loss: 2.0044854e-07
Iter: 1941 loss: 2.00323171e-07
Iter: 1942 loss: 2.00514705e-07
Iter: 1943 loss: 2.00254746e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2
+ date
Thu Oct 22 01:34:20 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/500_500_500_500_1 --function f1 --psi -2 --phi 1.2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb5840ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb57e66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb57e6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb57e6d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb563b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb563bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb55c5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb563b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb563b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb5603620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0b51400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0b7ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0b6f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0b6f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb558fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0b63d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb558f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0aefc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0a706a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0a52f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0a2e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0a35598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb099e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb09a90d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb09a9488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0ac2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0b0da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0b0b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0b0b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb08fd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0939f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb088c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb088c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb08c7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb084d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1bb0980268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.017237414
test_loss: 0.01672508
train_loss: 0.0072323913
test_loss: 0.007169322
train_loss: 0.004576991
test_loss: 0.0046268776
train_loss: 0.0033490297
test_loss: 0.003778173
train_loss: 0.003093888
test_loss: 0.0036498043
train_loss: 0.0029921585
test_loss: 0.003599441
train_loss: 0.0029804409
test_loss: 0.003532305
train_loss: 0.002882545
test_loss: 0.003299303
train_loss: 0.0028120964
test_loss: 0.0032671522
train_loss: 0.002632108
test_loss: 0.003311696
train_loss: 0.0029300682
test_loss: 0.0033387383
train_loss: 0.002737118
test_loss: 0.003142649
train_loss: 0.0024177297
test_loss: 0.0030339726
train_loss: 0.0024310572
test_loss: 0.0032228115
train_loss: 0.0027863102
test_loss: 0.0033673279
train_loss: 0.0027630408
test_loss: 0.0031460254
train_loss: 0.0024669291
test_loss: 0.0031902364
train_loss: 0.0023345198
test_loss: 0.0029294938
train_loss: 0.002578741
test_loss: 0.0029041443
train_loss: 0.0028801013
test_loss: 0.0031248135
train_loss: 0.0023880345
test_loss: 0.0029742825
train_loss: 0.0023242305
test_loss: 0.0030753626
train_loss: 0.002481638
test_loss: 0.0028698929
train_loss: 0.0026196288
test_loss: 0.002993938
train_loss: 0.002341951
test_loss: 0.002975096
train_loss: 0.002455337
test_loss: 0.0031002057
train_loss: 0.002588403
test_loss: 0.003350533
train_loss: 0.0024888571
test_loss: 0.0030049032
train_loss: 0.0025767158
test_loss: 0.0032727038
train_loss: 0.0025502862
test_loss: 0.0030365735
train_loss: 0.0024946332
test_loss: 0.003086505
train_loss: 0.0025121078
test_loss: 0.0030014163
train_loss: 0.0027081522
test_loss: 0.003065686
train_loss: 0.0025076892
test_loss: 0.0030973612
train_loss: 0.002537669
test_loss: 0.0029420618
train_loss: 0.0023062436
test_loss: 0.0030424471
train_loss: 0.0022764634
test_loss: 0.0029162182
train_loss: 0.0022780073
test_loss: 0.003212112
train_loss: 0.0022438099
test_loss: 0.0028899384
train_loss: 0.002304779
test_loss: 0.0028414223
train_loss: 0.0022803235
test_loss: 0.0029184422
train_loss: 0.0023085275
test_loss: 0.0029960955
train_loss: 0.0024970956
test_loss: 0.0030070073
train_loss: 0.0022814132
test_loss: 0.003019072
train_loss: 0.0026182963
test_loss: 0.003103919
train_loss: 0.0023702488
test_loss: 0.0028181637
train_loss: 0.0025428552
test_loss: 0.0032045953
train_loss: 0.0024483826
test_loss: 0.003235232
train_loss: 0.0023513732
test_loss: 0.0030115114
train_loss: 0.0025102259
test_loss: 0.0031085322
train_loss: 0.0026359423
test_loss: 0.0030736732
train_loss: 0.002258043
test_loss: 0.0029167049
train_loss: 0.0023245253
test_loss: 0.0027492582
train_loss: 0.0022106064
test_loss: 0.0028375273
train_loss: 0.0026211753
test_loss: 0.0030382134
train_loss: 0.0025708762
test_loss: 0.0030671444
train_loss: 0.002570215
test_loss: 0.0029560002
train_loss: 0.0024569514
test_loss: 0.0030628259
train_loss: 0.002550758
test_loss: 0.0032375364
train_loss: 0.0024615272
test_loss: 0.002940253
train_loss: 0.0023618382
test_loss: 0.003094071
train_loss: 0.0023888499
test_loss: 0.0030001928
train_loss: 0.0022765356
test_loss: 0.0030560247
train_loss: 0.0023001237
test_loss: 0.0029947187
train_loss: 0.0027312795
test_loss: 0.00294323
train_loss: 0.0025363087
test_loss: 0.0029953409
train_loss: 0.0024497889
test_loss: 0.0028583703
train_loss: 0.0023517846
test_loss: 0.0030611998
train_loss: 0.0022631432
test_loss: 0.0028530315
train_loss: 0.0023335342
test_loss: 0.002810751
train_loss: 0.0023805588
test_loss: 0.0030771017
train_loss: 0.0024259205
test_loss: 0.002993913
train_loss: 0.0022480348
test_loss: 0.002894024
train_loss: 0.0024828468
test_loss: 0.0029648566
train_loss: 0.0022454236
test_loss: 0.0028454717
train_loss: 0.002175305
test_loss: 0.0027793145
train_loss: 0.002352773
test_loss: 0.0027989058
train_loss: 0.002385918
test_loss: 0.0027818463
train_loss: 0.0022859392
test_loss: 0.0029733023
train_loss: 0.002220096
test_loss: 0.0027661822
train_loss: 0.002310927
test_loss: 0.0027362255
train_loss: 0.0020962728
test_loss: 0.0028755483
train_loss: 0.0022079435
test_loss: 0.0027327756
train_loss: 0.0024182738
test_loss: 0.0031583386
train_loss: 0.0024467353
test_loss: 0.0031223039
train_loss: 0.0025079185
test_loss: 0.0028649727
train_loss: 0.0022777715
test_loss: 0.0029346999
train_loss: 0.0024675634
test_loss: 0.0029841696
train_loss: 0.0025266493
test_loss: 0.0029596137
train_loss: 0.002311564
test_loss: 0.0029144723
train_loss: 0.002422861
test_loss: 0.0027591006
train_loss: 0.0023021023
test_loss: 0.0030475643
train_loss: 0.0023642494
test_loss: 0.002900371
train_loss: 0.0025486303
test_loss: 0.0032234136
train_loss: 0.00234289
test_loss: 0.0030552682
train_loss: 0.0029106878
test_loss: 0.003334068
train_loss: 0.0022982275
test_loss: 0.0029747237
train_loss: 0.0025073749
test_loss: 0.0029460713
train_loss: 0.0022605646
test_loss: 0.0027832352
train_loss: 0.002221067
test_loss: 0.0028657038
train_loss: 0.0021078354
test_loss: 0.0028609168
train_loss: 0.0023035672
test_loss: 0.0029126275
train_loss: 0.0023352613
test_loss: 0.0030394858
train_loss: 0.002400497
test_loss: 0.002829724
train_loss: 0.002460041
test_loss: 0.0031645587
train_loss: 0.0022340592
test_loss: 0.0029565769
train_loss: 0.002280199
test_loss: 0.0028799078
train_loss: 0.0022417395
test_loss: 0.0028084442
train_loss: 0.0022693996
test_loss: 0.0030726434
train_loss: 0.0024272518
test_loss: 0.002886846
train_loss: 0.002558918
test_loss: 0.0028109145
train_loss: 0.00247779
test_loss: 0.002777412
train_loss: 0.0025724112
test_loss: 0.003078565
train_loss: 0.0022200064
test_loss: 0.0029034752
train_loss: 0.0025063858
test_loss: 0.0030800828
train_loss: 0.002291708
test_loss: 0.003146129
train_loss: 0.0024352015
test_loss: 0.0031217309
train_loss: 0.002331459
test_loss: 0.0029051981
train_loss: 0.002435212
test_loss: 0.0029448676
train_loss: 0.002331328
test_loss: 0.0028873223
train_loss: 0.0023375158
test_loss: 0.0028361336
train_loss: 0.0022328722
test_loss: 0.0028020178
train_loss: 0.0022988678
test_loss: 0.0030015898
train_loss: 0.0022188681
test_loss: 0.0029377092
train_loss: 0.002503877
test_loss: 0.002854162
train_loss: 0.0022206486
test_loss: 0.0028417106
train_loss: 0.0024441183
test_loss: 0.002880572
train_loss: 0.0023619782
test_loss: 0.0030165745
train_loss: 0.0024328907
test_loss: 0.0029700522
train_loss: 0.0024701678
test_loss: 0.0029860581
train_loss: 0.0022164336
test_loss: 0.002764651
train_loss: 0.0023846903
test_loss: 0.0028882183
train_loss: 0.0026343998
test_loss: 0.0028967678
train_loss: 0.0025046028
test_loss: 0.00286846
train_loss: 0.0022800919
test_loss: 0.0030612554
train_loss: 0.002219042
test_loss: 0.0029241287
train_loss: 0.0023359647
test_loss: 0.003053535
train_loss: 0.0022775931
test_loss: 0.0028971727
train_loss: 0.0025417658
test_loss: 0.003075965
train_loss: 0.0021934307
test_loss: 0.0027971258
train_loss: 0.0024056875
test_loss: 0.0028241358
train_loss: 0.0023131152
test_loss: 0.0028907636
train_loss: 0.0022047283
test_loss: 0.0029612815
train_loss: 0.0022592526
test_loss: 0.0029745724
train_loss: 0.0024194696
test_loss: 0.0029091646
train_loss: 0.0023415114
test_loss: 0.002801983
train_loss: 0.0022661139
test_loss: 0.0027729198
train_loss: 0.002416817
test_loss: 0.00290495
train_loss: 0.0024600206
test_loss: 0.0029687057
train_loss: 0.0022938834
test_loss: 0.0027639451
train_loss: 0.00217409
test_loss: 0.0027556994
train_loss: 0.0022211438
test_loss: 0.003005109
train_loss: 0.0022355223
test_loss: 0.002808404
train_loss: 0.0023601092
test_loss: 0.0029183072
train_loss: 0.0021671162
test_loss: 0.0028159963
train_loss: 0.002195327
test_loss: 0.0027473348
train_loss: 0.002211285
test_loss: 0.0027826445
train_loss: 0.0023960837
test_loss: 0.002873969
train_loss: 0.0021940216
test_loss: 0.002815577
train_loss: 0.002267104
test_loss: 0.0029217158
train_loss: 0.002165828
test_loss: 0.002878465
train_loss: 0.0023329458
test_loss: 0.0027932527
train_loss: 0.0021843098
test_loss: 0.0027917074
train_loss: 0.0022458865
test_loss: 0.0027738777
train_loss: 0.0022807932
test_loss: 0.0030741824
train_loss: 0.0023790214
test_loss: 0.0030241574
train_loss: 0.002328461
test_loss: 0.0028377674
train_loss: 0.002345778
test_loss: 0.0028228457
train_loss: 0.002310141
test_loss: 0.0031953652
train_loss: 0.0022593576
test_loss: 0.0028508424
train_loss: 0.0022871466
test_loss: 0.0029198856
train_loss: 0.002186205
test_loss: 0.0028631687
train_loss: 0.0021761563
test_loss: 0.0027488873
train_loss: 0.00218501
test_loss: 0.002842256
train_loss: 0.0023516172
test_loss: 0.0029373434
train_loss: 0.0022438108
test_loss: 0.0028750855
train_loss: 0.0021434617
test_loss: 0.0028767048
train_loss: 0.0021509405
test_loss: 0.0027613062
train_loss: 0.0020810184
test_loss: 0.0027614865
train_loss: 0.0020776072
test_loss: 0.0027534072
train_loss: 0.0023919716
test_loss: 0.002828904
train_loss: 0.002315415
test_loss: 0.0029952123
train_loss: 0.0025314619
test_loss: 0.002897978
train_loss: 0.0025193365
test_loss: 0.0028939876
train_loss: 0.0024121113
test_loss: 0.002846517
train_loss: 0.002139857
test_loss: 0.0028391394
train_loss: 0.0020839213
test_loss: 0.002781756
train_loss: 0.0020455779
test_loss: 0.002813978
train_loss: 0.002425142
test_loss: 0.002878929
train_loss: 0.0021199961
test_loss: 0.002755205
train_loss: 0.0022960333
test_loss: 0.0028746305
train_loss: 0.0021272027
test_loss: 0.002793574
train_loss: 0.0020911295
test_loss: 0.002893934
train_loss: 0.0023501178
test_loss: 0.0028233319
train_loss: 0.0022540546
test_loss: 0.0027998195
train_loss: 0.0024218243
test_loss: 0.002808456
train_loss: 0.002295454
test_loss: 0.002738593
train_loss: 0.0022389984
test_loss: 0.0027551402
train_loss: 0.0020772784
test_loss: 0.0029263212
train_loss: 0.0021501533
test_loss: 0.0027183322
train_loss: 0.0020615943
test_loss: 0.0028020232
train_loss: 0.002257249
test_loss: 0.002716041
train_loss: 0.002371588
test_loss: 0.0028291487
train_loss: 0.0023770116
test_loss: 0.0027974849
train_loss: 0.002190147
test_loss: 0.0027752295
train_loss: 0.0023303237
test_loss: 0.002836124
train_loss: 0.0023724672
test_loss: 0.0028528431
train_loss: 0.0023891677
test_loss: 0.00323052
train_loss: 0.0021570725
test_loss: 0.0029883285
train_loss: 0.002061264
test_loss: 0.0027051046
train_loss: 0.0021967175
test_loss: 0.0028776359
train_loss: 0.0023513164
test_loss: 0.0028483898
train_loss: 0.0023451312
test_loss: 0.0029643066
train_loss: 0.0022762592
test_loss: 0.002901809
train_loss: 0.0021468538
test_loss: 0.002821569
train_loss: 0.0021441667
test_loss: 0.002927028
train_loss: 0.0021967413
test_loss: 0.0028083832
train_loss: 0.0023230535
test_loss: 0.0028166575
train_loss: 0.0021990086
test_loss: 0.0026918692
train_loss: 0.0022478993
test_loss: 0.0027642231
train_loss: 0.0023715773
test_loss: 0.002735355
train_loss: 0.0023465331
test_loss: 0.0028364828
train_loss: 0.002322839
test_loss: 0.0027954988
train_loss: 0.0022030375
test_loss: 0.0029862367
train_loss: 0.0020623885
test_loss: 0.002759943
train_loss: 0.002221421
test_loss: 0.0026405018
train_loss: 0.002202879
test_loss: 0.002753634
train_loss: 0.00227856
test_loss: 0.0028958763
train_loss: 0.0022602913
test_loss: 0.002761678
train_loss: 0.00231762
test_loss: 0.0027964772
train_loss: 0.0025860474
test_loss: 0.0028328863
train_loss: 0.0021017755
test_loss: 0.0028358651
train_loss: 0.0023255611
test_loss: 0.0027278594
train_loss: 0.002252758
test_loss: 0.0027548457
train_loss: 0.002282889
test_loss: 0.002749616
train_loss: 0.0021624293
test_loss: 0.002816251
train_loss: 0.0022545422
test_loss: 0.0029763673
train_loss: 0.0024263056
test_loss: 0.0028255808
train_loss: 0.0020790899
test_loss: 0.0027363105
train_loss: 0.0021506702
test_loss: 0.002726868
train_loss: 0.002257771
test_loss: 0.0028683401
train_loss: 0.002485924
test_loss: 0.0030514249
train_loss: 0.002194498
test_loss: 0.0028400759
train_loss: 0.0020697762
test_loss: 0.0025808897
train_loss: 0.0023136185
test_loss: 0.0027728754
train_loss: 0.0021815272
test_loss: 0.0029532255
train_loss: 0.002113036
test_loss: 0.0026601786
train_loss: 0.0020374446
test_loss: 0.0026888913
train_loss: 0.0024511355
test_loss: 0.0029960356
train_loss: 0.0023404295
test_loss: 0.002805917
train_loss: 0.002224675
test_loss: 0.0027773418
train_loss: 0.0020636516
test_loss: 0.002882253
train_loss: 0.0020701152
test_loss: 0.0027193145
train_loss: 0.0021723327
test_loss: 0.0029143966
train_loss: 0.0022160802
test_loss: 0.0028031408
train_loss: 0.0020896215
test_loss: 0.0029338896
train_loss: 0.0020765862
test_loss: 0.0026385854
train_loss: 0.0021565405
test_loss: 0.0027482375
train_loss: 0.0020393776
test_loss: 0.0028552166
train_loss: 0.0022175505
test_loss: 0.0026663477
train_loss: 0.0023502733
test_loss: 0.0032470848
train_loss: 0.0022142327
test_loss: 0.0028568436
train_loss: 0.002120644
test_loss: 0.0027315815
train_loss: 0.0022046708
test_loss: 0.0027826373
train_loss: 0.0020571619
test_loss: 0.0028585067
train_loss: 0.001996051
test_loss: 0.002795977
train_loss: 0.0022476318
test_loss: 0.0027597195
train_loss: 0.0020857002
test_loss: 0.0028042702
train_loss: 0.002154918
test_loss: 0.0026887406
train_loss: 0.0021676403
test_loss: 0.0028016914
train_loss: 0.0020426728
test_loss: 0.0029184003
train_loss: 0.0023389757
test_loss: 0.0027828047
train_loss: 0.0021691378
test_loss: 0.0026732907
train_loss: 0.0023774398
test_loss: 0.0028692288
train_loss: 0.0023495713
test_loss: 0.002773122
train_loss: 0.0022132802
test_loss: 0.00287266
train_loss: 0.002168447
test_loss: 0.0028379718
train_loss: 0.002462912
test_loss: 0.002849002
train_loss: 0.0021207477
test_loss: 0.0028022095
train_loss: 0.0023851814
test_loss: 0.0027894948
train_loss: 0.0022938745
test_loss: 0.0028166354
train_loss: 0.0020442733
test_loss: 0.0026165817
train_loss: 0.0022550935
test_loss: 0.00268406
train_loss: 0.002167819
test_loss: 0.002776268
train_loss: 0.0023254333
test_loss: 0.002782236
train_loss: 0.0021911466
test_loss: 0.00287193
train_loss: 0.0025066044
test_loss: 0.0027953186
train_loss: 0.0021241882
test_loss: 0.002685306
train_loss: 0.002250752
test_loss: 0.0027969347
train_loss: 0.002259265
test_loss: 0.0027004017
train_loss: 0.0021914567
test_loss: 0.0028268658
train_loss: 0.0023550317
test_loss: 0.0028405697
train_loss: 0.002178794
test_loss: 0.0028275622
train_loss: 0.0020640627
test_loss: 0.0029467877
train_loss: 0.0025474618
test_loss: 0.002817844
train_loss: 0.0021295743
test_loss: 0.0030242205
train_loss: 0.0021942637
test_loss: 0.002724295
train_loss: 0.0021001888
test_loss: 0.0027816943
train_loss: 0.0022723356
test_loss: 0.0028929918
train_loss: 0.0022272444
test_loss: 0.002973738
train_loss: 0.0024109203
test_loss: 0.0028714729
train_loss: 0.0022786218
test_loss: 0.0027434698
train_loss: 0.0022911523
test_loss: 0.0026837203
train_loss: 0.0021089113
test_loss: 0.0027174668
train_loss: 0.0022867694
test_loss: 0.002859857
train_loss: 0.002082121
test_loss: 0.0028143304
train_loss: 0.0021037827
test_loss: 0.0027267344
train_loss: 0.0020536412
test_loss: 0.0026923376
train_loss: 0.0022849804
test_loss: 0.002668208
train_loss: 0.002271609
test_loss: 0.0026867043
train_loss: 0.002168906
test_loss: 0.002657393
train_loss: 0.0021076868
test_loss: 0.0026276421
train_loss: 0.002104322
test_loss: 0.0027837884
train_loss: 0.002303663
test_loss: 0.002940607
train_loss: 0.0022440786
test_loss: 0.0027899046
train_loss: 0.0022584547
test_loss: 0.002840236
train_loss: 0.002139974
test_loss: 0.0028190198
train_loss: 0.0021787304
test_loss: 0.0026870912
train_loss: 0.0022063865
test_loss: 0.002731141
train_loss: 0.0021718983
test_loss: 0.0027813981
train_loss: 0.0021572146
test_loss: 0.0027416241
train_loss: 0.0020649144
test_loss: 0.0029427416
train_loss: 0.0024987073
test_loss: 0.0027838682
train_loss: 0.0022328594
test_loss: 0.002642618
train_loss: 0.0023255858
test_loss: 0.0028749716
train_loss: 0.0020505018
test_loss: 0.0027751105
train_loss: 0.002195024
test_loss: 0.0028707166
train_loss: 0.0021575873
test_loss: 0.0028696712
train_loss: 0.0023252345
test_loss: 0.0028202904
train_loss: 0.0025547273
test_loss: 0.0027096705
train_loss: 0.0020868857
test_loss: 0.0028117718
train_loss: 0.0021826187
test_loss: 0.0028101944
train_loss: 0.0022546235
test_loss: 0.0027909633
train_loss: 0.0023405564
test_loss: 0.002738734
train_loss: 0.0023759273
test_loss: 0.0028055173
train_loss: 0.0022579054
test_loss: 0.002922068
train_loss: 0.0021646917
test_loss: 0.0027813169
train_loss: 0.0021134263
test_loss: 0.0028440238
train_loss: 0.0022528225/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0028773064
train_loss: 0.0023276804
test_loss: 0.0030666396
train_loss: 0.002211554
test_loss: 0.0027370981
train_loss: 0.0021984566
test_loss: 0.002834021
train_loss: 0.0020766323
test_loss: 0.0026098643
train_loss: 0.0021877217
test_loss: 0.002926474
train_loss: 0.0023288175
test_loss: 0.0028274176
train_loss: 0.0020944518
test_loss: 0.0026740285
train_loss: 0.002514725
test_loss: 0.0027290683
train_loss: 0.0022590463
test_loss: 0.0028665103
train_loss: 0.0023396413
test_loss: 0.0029117677
train_loss: 0.0021493416
test_loss: 0.0027595235
train_loss: 0.0020511125
test_loss: 0.0027247404
train_loss: 0.0020285272
test_loss: 0.0027473553
train_loss: 0.0021900292
test_loss: 0.00270426
train_loss: 0.0020954905
test_loss: 0.002821886
train_loss: 0.0021244641
test_loss: 0.0028291463
train_loss: 0.0021729502
test_loss: 0.0027691075
train_loss: 0.002192357
test_loss: 0.0027286145
train_loss: 0.0024608453
test_loss: 0.002790802
train_loss: 0.0020187825
test_loss: 0.0026866687
train_loss: 0.0021148939
test_loss: 0.0026962876
train_loss: 0.0020877735
test_loss: 0.0026231916
train_loss: 0.0020165602
test_loss: 0.0025431318
train_loss: 0.0020372476
test_loss: 0.002814478
train_loss: 0.0020876992
test_loss: 0.0027468367
train_loss: 0.0022247843
test_loss: 0.00300348
train_loss: 0.0023267388
test_loss: 0.002775689
train_loss: 0.00211334
test_loss: 0.0026771445
train_loss: 0.002025671
test_loss: 0.002713339
train_loss: 0.0020146912
test_loss: 0.002872329
train_loss: 0.0024422111
test_loss: 0.0028487956
train_loss: 0.002117585
test_loss: 0.0027524116
train_loss: 0.0020342048
test_loss: 0.002731781
train_loss: 0.00208375
test_loss: 0.0028867181
train_loss: 0.002209592
test_loss: 0.0027957668
train_loss: 0.0022532868
test_loss: 0.002719697
train_loss: 0.0022690585
test_loss: 0.0026881516
train_loss: 0.0020026648
test_loss: 0.0027140025
train_loss: 0.0021690251
test_loss: 0.0027018574
train_loss: 0.0021488369
test_loss: 0.0028441057
train_loss: 0.002070698
test_loss: 0.0027341938
train_loss: 0.0026932769
test_loss: 0.0029148923
train_loss: 0.0021195726
test_loss: 0.0027945612
train_loss: 0.0023337093
test_loss: 0.0028070016
train_loss: 0.0020535765
test_loss: 0.0028463092
train_loss: 0.0020987217
test_loss: 0.002828797
train_loss: 0.002011468
test_loss: 0.0026337467
train_loss: 0.0020527793
test_loss: 0.00255712
train_loss: 0.0023333472
test_loss: 0.002907989
train_loss: 0.0021939913
test_loss: 0.0027017163
train_loss: 0.0020622148
test_loss: 0.0026673318
train_loss: 0.0021469495
test_loss: 0.0026321383
train_loss: 0.0021567203
test_loss: 0.0028795183
train_loss: 0.0019656424
test_loss: 0.0027847087
train_loss: 0.0019506197
test_loss: 0.0027425592
train_loss: 0.002127658
test_loss: 0.0028787875
train_loss: 0.0021696514
test_loss: 0.0028414296
train_loss: 0.0022462073
test_loss: 0.0027489276
train_loss: 0.0020027016
test_loss: 0.0026660422
train_loss: 0.002003094
test_loss: 0.002959411
train_loss: 0.0024636604
test_loss: 0.0027745652
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc694079598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc68d663598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc68d663400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc68d663620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc68d6a3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc68d6a3c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc64b14f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc64b106598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc64b1061e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc64b0bdc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc64b0fa950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc64b074d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc64b08b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc64b08bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc6240d37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc64b01e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc62410b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc62410b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc624080950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc6240802f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc62407f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc624038ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc6240388c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc61c233598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc61c233378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc61c1e5ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc61c19e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc61c1b7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc61c1b72f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc61c185620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc61c117ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc61c0ca510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc61c0ca378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc61c0fa950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc61c0a29d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc61c058048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 8.24278686e-06
Iter: 2 loss: 6.45970158e-06
Iter: 3 loss: 6.40570579e-06
Iter: 4 loss: 5.77316769e-06
Iter: 5 loss: 7.0778151e-06
Iter: 6 loss: 5.52003303e-06
Iter: 7 loss: 5.21124775e-06
Iter: 8 loss: 6.01638567e-06
Iter: 9 loss: 5.10673772e-06
Iter: 10 loss: 4.74453236e-06
Iter: 11 loss: 6.14237251e-06
Iter: 12 loss: 4.65960875e-06
Iter: 13 loss: 4.51672e-06
Iter: 14 loss: 5.49351262e-06
Iter: 15 loss: 4.50293737e-06
Iter: 16 loss: 4.3700079e-06
Iter: 17 loss: 4.46024e-06
Iter: 18 loss: 4.28664134e-06
Iter: 19 loss: 4.18352192e-06
Iter: 20 loss: 4.02807973e-06
Iter: 21 loss: 4.02519e-06
Iter: 22 loss: 3.82453436e-06
Iter: 23 loss: 5.85405223e-06
Iter: 24 loss: 3.81841483e-06
Iter: 25 loss: 3.65807068e-06
Iter: 26 loss: 4.53723715e-06
Iter: 27 loss: 3.63461254e-06
Iter: 28 loss: 3.50191044e-06
Iter: 29 loss: 3.32242689e-06
Iter: 30 loss: 3.31317256e-06
Iter: 31 loss: 3.10876885e-06
Iter: 32 loss: 3.54390068e-06
Iter: 33 loss: 3.02887929e-06
Iter: 34 loss: 2.87785633e-06
Iter: 35 loss: 5.13132272e-06
Iter: 36 loss: 2.87771695e-06
Iter: 37 loss: 2.79454571e-06
Iter: 38 loss: 2.76479886e-06
Iter: 39 loss: 2.71807e-06
Iter: 40 loss: 2.62379194e-06
Iter: 41 loss: 4.10981329e-06
Iter: 42 loss: 2.62381491e-06
Iter: 43 loss: 2.55818236e-06
Iter: 44 loss: 3.28056967e-06
Iter: 45 loss: 2.55690156e-06
Iter: 46 loss: 2.51905021e-06
Iter: 47 loss: 2.46078753e-06
Iter: 48 loss: 2.45984575e-06
Iter: 49 loss: 2.40129248e-06
Iter: 50 loss: 2.40113627e-06
Iter: 51 loss: 2.3556745e-06
Iter: 52 loss: 2.29774719e-06
Iter: 53 loss: 2.29349e-06
Iter: 54 loss: 2.25116173e-06
Iter: 55 loss: 2.25008671e-06
Iter: 56 loss: 2.2135489e-06
Iter: 57 loss: 2.11880251e-06
Iter: 58 loss: 2.87227749e-06
Iter: 59 loss: 2.10105668e-06
Iter: 60 loss: 2.02457818e-06
Iter: 61 loss: 3.14620979e-06
Iter: 62 loss: 2.02436763e-06
Iter: 63 loss: 1.98812268e-06
Iter: 64 loss: 2.20725906e-06
Iter: 65 loss: 1.98353337e-06
Iter: 66 loss: 1.93865617e-06
Iter: 67 loss: 1.96962219e-06
Iter: 68 loss: 1.91055597e-06
Iter: 69 loss: 1.87557521e-06
Iter: 70 loss: 1.84729674e-06
Iter: 71 loss: 1.83694954e-06
Iter: 72 loss: 1.77851507e-06
Iter: 73 loss: 1.89960963e-06
Iter: 74 loss: 1.75526384e-06
Iter: 75 loss: 1.6878106e-06
Iter: 76 loss: 1.99931264e-06
Iter: 77 loss: 1.67518351e-06
Iter: 78 loss: 1.62049537e-06
Iter: 79 loss: 1.79257449e-06
Iter: 80 loss: 1.60466379e-06
Iter: 81 loss: 1.5963692e-06
Iter: 82 loss: 1.58195144e-06
Iter: 83 loss: 1.5652538e-06
Iter: 84 loss: 1.53155952e-06
Iter: 85 loss: 2.15743853e-06
Iter: 86 loss: 1.53099245e-06
Iter: 87 loss: 1.50465985e-06
Iter: 88 loss: 1.50466417e-06
Iter: 89 loss: 1.48685149e-06
Iter: 90 loss: 1.56336455e-06
Iter: 91 loss: 1.4833189e-06
Iter: 92 loss: 1.47058586e-06
Iter: 93 loss: 1.45053082e-06
Iter: 94 loss: 1.45033437e-06
Iter: 95 loss: 1.42931265e-06
Iter: 96 loss: 1.42918725e-06
Iter: 97 loss: 1.42057979e-06
Iter: 98 loss: 1.39537246e-06
Iter: 99 loss: 1.48802428e-06
Iter: 100 loss: 1.38435917e-06
Iter: 101 loss: 1.35087726e-06
Iter: 102 loss: 1.75651917e-06
Iter: 103 loss: 1.35052528e-06
Iter: 104 loss: 1.32803416e-06
Iter: 105 loss: 1.59106128e-06
Iter: 106 loss: 1.32765399e-06
Iter: 107 loss: 1.31272191e-06
Iter: 108 loss: 1.3048143e-06
Iter: 109 loss: 1.2979333e-06
Iter: 110 loss: 1.27344504e-06
Iter: 111 loss: 1.25124302e-06
Iter: 112 loss: 1.24511791e-06
Iter: 113 loss: 1.21902792e-06
Iter: 114 loss: 1.36801032e-06
Iter: 115 loss: 1.2154992e-06
Iter: 116 loss: 1.19660274e-06
Iter: 117 loss: 1.34823699e-06
Iter: 118 loss: 1.1953083e-06
Iter: 119 loss: 1.18133698e-06
Iter: 120 loss: 1.37668303e-06
Iter: 121 loss: 1.18130276e-06
Iter: 122 loss: 1.16872343e-06
Iter: 123 loss: 1.16247884e-06
Iter: 124 loss: 1.15646901e-06
Iter: 125 loss: 1.14639874e-06
Iter: 126 loss: 1.20830589e-06
Iter: 127 loss: 1.1451807e-06
Iter: 128 loss: 1.13078283e-06
Iter: 129 loss: 1.11332201e-06
Iter: 130 loss: 1.11165059e-06
Iter: 131 loss: 1.09571238e-06
Iter: 132 loss: 1.21122025e-06
Iter: 133 loss: 1.09438054e-06
Iter: 134 loss: 1.08242534e-06
Iter: 135 loss: 1.16472188e-06
Iter: 136 loss: 1.08130951e-06
Iter: 137 loss: 1.07379879e-06
Iter: 138 loss: 1.06831476e-06
Iter: 139 loss: 1.06572634e-06
Iter: 140 loss: 1.05323988e-06
Iter: 141 loss: 1.05464687e-06
Iter: 142 loss: 1.04360197e-06
Iter: 143 loss: 1.03667696e-06
Iter: 144 loss: 1.03493358e-06
Iter: 145 loss: 1.02813283e-06
Iter: 146 loss: 1.02308081e-06
Iter: 147 loss: 1.02079161e-06
Iter: 148 loss: 1.00794955e-06
Iter: 149 loss: 1.00491479e-06
Iter: 150 loss: 9.96642143e-07
Iter: 151 loss: 9.80324558e-07
Iter: 152 loss: 1.0086319e-06
Iter: 153 loss: 9.73008355e-07
Iter: 154 loss: 9.57541829e-07
Iter: 155 loss: 1.01463411e-06
Iter: 156 loss: 9.53774929e-07
Iter: 157 loss: 9.446033e-07
Iter: 158 loss: 9.43393673e-07
Iter: 159 loss: 9.363647e-07
Iter: 160 loss: 9.350303e-07
Iter: 161 loss: 9.30380793e-07
Iter: 162 loss: 9.24399728e-07
Iter: 163 loss: 9.55754672e-07
Iter: 164 loss: 9.23496e-07
Iter: 165 loss: 9.15633e-07
Iter: 166 loss: 9.18741591e-07
Iter: 167 loss: 9.10221843e-07
Iter: 168 loss: 9.05811874e-07
Iter: 169 loss: 9.12639621e-07
Iter: 170 loss: 9.0366666e-07
Iter: 171 loss: 8.97666041e-07
Iter: 172 loss: 9.30337819e-07
Iter: 173 loss: 8.9683607e-07
Iter: 174 loss: 8.91431e-07
Iter: 175 loss: 8.83582516e-07
Iter: 176 loss: 8.83332234e-07
Iter: 177 loss: 8.74288162e-07
Iter: 178 loss: 8.94763048e-07
Iter: 179 loss: 8.70905808e-07
Iter: 180 loss: 8.62759236e-07
Iter: 181 loss: 9.561345e-07
Iter: 182 loss: 8.62678689e-07
Iter: 183 loss: 8.53755921e-07
Iter: 184 loss: 8.52094502e-07
Iter: 185 loss: 8.46122e-07
Iter: 186 loss: 8.37995685e-07
Iter: 187 loss: 8.42380473e-07
Iter: 188 loss: 8.3262762e-07
Iter: 189 loss: 8.23942287e-07
Iter: 190 loss: 9.05528793e-07
Iter: 191 loss: 8.23580535e-07
Iter: 192 loss: 8.17370676e-07
Iter: 193 loss: 8.14021917e-07
Iter: 194 loss: 8.11206803e-07
Iter: 195 loss: 8.05793093e-07
Iter: 196 loss: 8.04540832e-07
Iter: 197 loss: 8.0178296e-07
Iter: 198 loss: 7.95683491e-07
Iter: 199 loss: 8.86215389e-07
Iter: 200 loss: 7.95450376e-07
Iter: 201 loss: 7.92933633e-07
Iter: 202 loss: 7.9180063e-07
Iter: 203 loss: 7.89387514e-07
Iter: 204 loss: 7.84666838e-07
Iter: 205 loss: 8.80938046e-07
Iter: 206 loss: 7.84617043e-07
Iter: 207 loss: 7.7913603e-07
Iter: 208 loss: 7.96303766e-07
Iter: 209 loss: 7.77552714e-07
Iter: 210 loss: 7.71483712e-07
Iter: 211 loss: 8.11952816e-07
Iter: 212 loss: 7.70905444e-07
Iter: 213 loss: 7.66575e-07
Iter: 214 loss: 7.61343529e-07
Iter: 215 loss: 7.60818068e-07
Iter: 216 loss: 7.54213829e-07
Iter: 217 loss: 7.59079967e-07
Iter: 218 loss: 7.50134745e-07
Iter: 219 loss: 7.41161e-07
Iter: 220 loss: 8.64457e-07
Iter: 221 loss: 7.41121426e-07
Iter: 222 loss: 7.37949563e-07
Iter: 223 loss: 7.42097427e-07
Iter: 224 loss: 7.36357379e-07
Iter: 225 loss: 7.3271292e-07
Iter: 226 loss: 7.25209873e-07
Iter: 227 loss: 8.52148e-07
Iter: 228 loss: 7.25019163e-07
Iter: 229 loss: 7.20776029e-07
Iter: 230 loss: 7.20522564e-07
Iter: 231 loss: 7.16911131e-07
Iter: 232 loss: 7.58456849e-07
Iter: 233 loss: 7.167896e-07
Iter: 234 loss: 7.14205271e-07
Iter: 235 loss: 7.10473671e-07
Iter: 236 loss: 7.10356971e-07
Iter: 237 loss: 7.06279707e-07
Iter: 238 loss: 7.44200406e-07
Iter: 239 loss: 7.06127764e-07
Iter: 240 loss: 7.02026341e-07
Iter: 241 loss: 7.07356492e-07
Iter: 242 loss: 6.99885504e-07
Iter: 243 loss: 6.9710029e-07
Iter: 244 loss: 6.95924882e-07
Iter: 245 loss: 6.94458663e-07
Iter: 246 loss: 6.90718593e-07
Iter: 247 loss: 7.35002402e-07
Iter: 248 loss: 6.90722686e-07
Iter: 249 loss: 6.87447596e-07
Iter: 250 loss: 6.88929504e-07
Iter: 251 loss: 6.85291184e-07
Iter: 252 loss: 6.81737163e-07
Iter: 253 loss: 6.82289453e-07
Iter: 254 loss: 6.79092125e-07
Iter: 255 loss: 6.75422598e-07
Iter: 256 loss: 6.9466256e-07
Iter: 257 loss: 6.74913451e-07
Iter: 258 loss: 6.70246664e-07
Iter: 259 loss: 6.82548375e-07
Iter: 260 loss: 6.68682333e-07
Iter: 261 loss: 6.64967445e-07
Iter: 262 loss: 6.68412781e-07
Iter: 263 loss: 6.62775e-07
Iter: 264 loss: 6.587386e-07
Iter: 265 loss: 6.56582188e-07
Iter: 266 loss: 6.54759901e-07
Iter: 267 loss: 6.51527102e-07
Iter: 268 loss: 6.51198206e-07
Iter: 269 loss: 6.47400441e-07
Iter: 270 loss: 6.52726499e-07
Iter: 271 loss: 6.45505907e-07
Iter: 272 loss: 6.42707164e-07
Iter: 273 loss: 6.42977454e-07
Iter: 274 loss: 6.40542112e-07
Iter: 275 loss: 6.37691073e-07
Iter: 276 loss: 6.37674589e-07
Iter: 277 loss: 6.35713604e-07
Iter: 278 loss: 6.33366085e-07
Iter: 279 loss: 6.33179184e-07
Iter: 280 loss: 6.3039306e-07
Iter: 281 loss: 6.29074407e-07
Iter: 282 loss: 6.27631437e-07
Iter: 283 loss: 6.24344e-07
Iter: 284 loss: 6.2421509e-07
Iter: 285 loss: 6.21968297e-07
Iter: 286 loss: 6.17884041e-07
Iter: 287 loss: 7.15238798e-07
Iter: 288 loss: 6.17883188e-07
Iter: 289 loss: 6.14076498e-07
Iter: 290 loss: 6.35917e-07
Iter: 291 loss: 6.1358503e-07
Iter: 292 loss: 6.09916071e-07
Iter: 293 loss: 6.18962758e-07
Iter: 294 loss: 6.08635617e-07
Iter: 295 loss: 6.04204274e-07
Iter: 296 loss: 6.2956974e-07
Iter: 297 loss: 6.03663636e-07
Iter: 298 loss: 6.01143768e-07
Iter: 299 loss: 5.97985718e-07
Iter: 300 loss: 5.97693941e-07
Iter: 301 loss: 5.94583867e-07
Iter: 302 loss: 5.94591256e-07
Iter: 303 loss: 5.92862079e-07
Iter: 304 loss: 6.17649675e-07
Iter: 305 loss: 5.92865e-07
Iter: 306 loss: 5.91741923e-07
Iter: 307 loss: 5.88569549e-07
Iter: 308 loss: 6.04837851e-07
Iter: 309 loss: 5.87542e-07
Iter: 310 loss: 5.87432964e-07
Iter: 311 loss: 5.85846465e-07
Iter: 312 loss: 5.84378427e-07
Iter: 313 loss: 5.81516815e-07
Iter: 314 loss: 6.35438937e-07
Iter: 315 loss: 5.81463837e-07
Iter: 316 loss: 5.78959316e-07
Iter: 317 loss: 5.78975573e-07
Iter: 318 loss: 5.7696866e-07
Iter: 319 loss: 5.74788146e-07
Iter: 320 loss: 5.74583737e-07
Iter: 321 loss: 5.72688293e-07
Iter: 322 loss: 5.72787e-07
Iter: 323 loss: 5.71232931e-07
Iter: 324 loss: 5.68817654e-07
Iter: 325 loss: 5.68560949e-07
Iter: 326 loss: 5.66833364e-07
Iter: 327 loss: 5.64268248e-07
Iter: 328 loss: 5.72801923e-07
Iter: 329 loss: 5.63586184e-07
Iter: 330 loss: 5.60716899e-07
Iter: 331 loss: 5.88553632e-07
Iter: 332 loss: 5.6063891e-07
Iter: 333 loss: 5.58884153e-07
Iter: 334 loss: 5.58597776e-07
Iter: 335 loss: 5.57358135e-07
Iter: 336 loss: 5.54868734e-07
Iter: 337 loss: 5.55633221e-07
Iter: 338 loss: 5.53072141e-07
Iter: 339 loss: 5.51314429e-07
Iter: 340 loss: 5.50901916e-07
Iter: 341 loss: 5.49902836e-07
Iter: 342 loss: 5.4723489e-07
Iter: 343 loss: 5.6175287e-07
Iter: 344 loss: 5.46396222e-07
Iter: 345 loss: 5.44517547e-07
Iter: 346 loss: 5.44185923e-07
Iter: 347 loss: 5.42545706e-07
Iter: 348 loss: 5.45639e-07
Iter: 349 loss: 5.41876204e-07
Iter: 350 loss: 5.40485757e-07
Iter: 351 loss: 5.37715437e-07
Iter: 352 loss: 5.93299205e-07
Iter: 353 loss: 5.37712197e-07
Iter: 354 loss: 5.34730134e-07
Iter: 355 loss: 5.56807777e-07
Iter: 356 loss: 5.34488208e-07
Iter: 357 loss: 5.32955653e-07
Iter: 358 loss: 5.32952185e-07
Iter: 359 loss: 5.3181725e-07
Iter: 360 loss: 5.31127966e-07
Iter: 361 loss: 5.30612056e-07
Iter: 362 loss: 5.28801252e-07
Iter: 363 loss: 5.26922e-07
Iter: 364 loss: 5.26556789e-07
Iter: 365 loss: 5.24971597e-07
Iter: 366 loss: 5.24963639e-07
Iter: 367 loss: 5.23165227e-07
Iter: 368 loss: 5.24697384e-07
Iter: 369 loss: 5.22092e-07
Iter: 370 loss: 5.19828234e-07
Iter: 371 loss: 5.2030191e-07
Iter: 372 loss: 5.18151296e-07
Iter: 373 loss: 5.15443844e-07
Iter: 374 loss: 5.21842423e-07
Iter: 375 loss: 5.14441467e-07
Iter: 376 loss: 5.14002693e-07
Iter: 377 loss: 5.1337679e-07
Iter: 378 loss: 5.12358724e-07
Iter: 379 loss: 5.09873189e-07
Iter: 380 loss: 5.37404844e-07
Iter: 381 loss: 5.09665824e-07
Iter: 382 loss: 5.07630887e-07
Iter: 383 loss: 5.30610578e-07
Iter: 384 loss: 5.07597633e-07
Iter: 385 loss: 5.06020569e-07
Iter: 386 loss: 5.17663182e-07
Iter: 387 loss: 5.05922571e-07
Iter: 388 loss: 5.04924685e-07
Iter: 389 loss: 5.03333e-07
Iter: 390 loss: 5.03307547e-07
Iter: 391 loss: 5.0154739e-07
Iter: 392 loss: 5.02969328e-07
Iter: 393 loss: 5.00500562e-07
Iter: 394 loss: 4.99212092e-07
Iter: 395 loss: 4.991183e-07
Iter: 396 loss: 4.98003544e-07
Iter: 397 loss: 4.9843436e-07
Iter: 398 loss: 4.97182725e-07
Iter: 399 loss: 4.95754762e-07
Iter: 400 loss: 4.95751863e-07
Iter: 401 loss: 4.94567e-07
Iter: 402 loss: 4.92564e-07
Iter: 403 loss: 4.91301762e-07
Iter: 404 loss: 4.90486855e-07
Iter: 405 loss: 4.89570141e-07
Iter: 406 loss: 4.88856074e-07
Iter: 407 loss: 4.87570048e-07
Iter: 408 loss: 4.85812961e-07
Iter: 409 loss: 4.85713372e-07
Iter: 410 loss: 4.83558381e-07
Iter: 411 loss: 4.91492415e-07
Iter: 412 loss: 4.82985627e-07
Iter: 413 loss: 4.81991037e-07
Iter: 414 loss: 4.81841482e-07
Iter: 415 loss: 4.80957397e-07
Iter: 416 loss: 4.78948323e-07
Iter: 417 loss: 5.07164145e-07
Iter: 418 loss: 4.78826905e-07
Iter: 419 loss: 4.7745209e-07
Iter: 420 loss: 4.94588221e-07
Iter: 421 loss: 4.77413948e-07
Iter: 422 loss: 4.76018158e-07
Iter: 423 loss: 4.80652716e-07
Iter: 424 loss: 4.75625825e-07
Iter: 425 loss: 4.747383e-07
Iter: 426 loss: 4.73620048e-07
Iter: 427 loss: 4.73498204e-07
Iter: 428 loss: 4.71946692e-07
Iter: 429 loss: 4.76393552e-07
Iter: 430 loss: 4.71470145e-07
Iter: 431 loss: 4.70194777e-07
Iter: 432 loss: 4.81750078e-07
Iter: 433 loss: 4.70112326e-07
Iter: 434 loss: 4.68588865e-07
Iter: 435 loss: 4.68843581e-07
Iter: 436 loss: 4.67452935e-07
Iter: 437 loss: 4.66148265e-07
Iter: 438 loss: 4.68980062e-07
Iter: 439 loss: 4.65638749e-07
Iter: 440 loss: 4.64011066e-07
Iter: 441 loss: 4.63024207e-07
Iter: 442 loss: 4.62412373e-07
Iter: 443 loss: 4.61257855e-07
Iter: 444 loss: 4.61174722e-07
Iter: 445 loss: 4.60104275e-07
Iter: 446 loss: 4.62450544e-07
Iter: 447 loss: 4.59703728e-07
Iter: 448 loss: 4.58381891e-07
Iter: 449 loss: 4.57780914e-07
Iter: 450 loss: 4.57154954e-07
Iter: 451 loss: 4.55440215e-07
Iter: 452 loss: 4.62759431e-07
Iter: 453 loss: 4.55097506e-07
Iter: 454 loss: 4.53158492e-07
Iter: 455 loss: 4.66598749e-07
Iter: 456 loss: 4.52988616e-07
Iter: 457 loss: 4.52127381e-07
Iter: 458 loss: 4.50836922e-07
Iter: 459 loss: 4.5083118e-07
Iter: 460 loss: 4.49771e-07
Iter: 461 loss: 4.49722e-07
Iter: 462 loss: 4.4883015e-07
Iter: 463 loss: 4.4841039e-07
Iter: 464 loss: 4.47970336e-07
Iter: 465 loss: 4.46944256e-07
Iter: 466 loss: 4.46709748e-07
Iter: 467 loss: 4.46063524e-07
Iter: 468 loss: 4.44700277e-07
Iter: 469 loss: 4.46747151e-07
Iter: 470 loss: 4.440829e-07
Iter: 471 loss: 4.42720562e-07
Iter: 472 loss: 4.42701463e-07
Iter: 473 loss: 4.42123593e-07
Iter: 474 loss: 4.41339125e-07
Iter: 475 loss: 4.41315706e-07
Iter: 476 loss: 4.39854915e-07
Iter: 477 loss: 4.38075148e-07
Iter: 478 loss: 4.37935938e-07
Iter: 479 loss: 4.3547621e-07
Iter: 480 loss: 4.50695552e-07
Iter: 481 loss: 4.35135291e-07
Iter: 482 loss: 4.33208356e-07
Iter: 483 loss: 4.63434446e-07
Iter: 484 loss: 4.33191133e-07
Iter: 485 loss: 4.32136119e-07
Iter: 486 loss: 4.31411451e-07
Iter: 487 loss: 4.31028468e-07
Iter: 488 loss: 4.29686224e-07
Iter: 489 loss: 4.44282307e-07
Iter: 490 loss: 4.2969026e-07
Iter: 491 loss: 4.2860313e-07
Iter: 492 loss: 4.34523457e-07
Iter: 493 loss: 4.28477279e-07
Iter: 494 loss: 4.27916092e-07
Iter: 495 loss: 4.26896861e-07
Iter: 496 loss: 4.5109968e-07
Iter: 497 loss: 4.26899305e-07
Iter: 498 loss: 4.26643425e-07
Iter: 499 loss: 4.26375465e-07
Iter: 500 loss: 4.2594155e-07
Iter: 501 loss: 4.25159783e-07
Iter: 502 loss: 4.41107431e-07
Iter: 503 loss: 4.25142872e-07
Iter: 504 loss: 4.24089279e-07
Iter: 505 loss: 4.24215813e-07
Iter: 506 loss: 4.23309388e-07
Iter: 507 loss: 4.21851382e-07
Iter: 508 loss: 4.28429473e-07
Iter: 509 loss: 4.21551391e-07
Iter: 510 loss: 4.2023737e-07
Iter: 511 loss: 4.3515746e-07
Iter: 512 loss: 4.20201189e-07
Iter: 513 loss: 4.19484024e-07
Iter: 514 loss: 4.18146129e-07
Iter: 515 loss: 4.47062064e-07
Iter: 516 loss: 4.18146897e-07
Iter: 517 loss: 4.16684941e-07
Iter: 518 loss: 4.24835093e-07
Iter: 519 loss: 4.16471181e-07
Iter: 520 loss: 4.15290657e-07
Iter: 521 loss: 4.16311593e-07
Iter: 522 loss: 4.14617716e-07
Iter: 523 loss: 4.13781123e-07
Iter: 524 loss: 4.1366502e-07
Iter: 525 loss: 4.13075043e-07
Iter: 526 loss: 4.12216281e-07
Iter: 527 loss: 4.12179304e-07
Iter: 528 loss: 4.11110364e-07
Iter: 529 loss: 4.14342765e-07
Iter: 530 loss: 4.1080574e-07
Iter: 531 loss: 4.09812401e-07
Iter: 532 loss: 4.24903078e-07
Iter: 533 loss: 4.09809331e-07
Iter: 534 loss: 4.09094525e-07
Iter: 535 loss: 4.07879838e-07
Iter: 536 loss: 4.07878332e-07
Iter: 537 loss: 4.06839831e-07
Iter: 538 loss: 4.17700534e-07
Iter: 539 loss: 4.06809079e-07
Iter: 540 loss: 4.05697222e-07
Iter: 541 loss: 4.06694255e-07
Iter: 542 loss: 4.05063531e-07
Iter: 543 loss: 4.04162847e-07
Iter: 544 loss: 4.05091072e-07
Iter: 545 loss: 4.03644549e-07
Iter: 546 loss: 4.02737527e-07
Iter: 547 loss: 4.03052866e-07
Iter: 548 loss: 4.02098e-07
Iter: 549 loss: 4.01427457e-07
Iter: 550 loss: 4.01423847e-07
Iter: 551 loss: 4.00657086e-07
Iter: 552 loss: 4.00145808e-07
Iter: 553 loss: 3.99859061e-07
Iter: 554 loss: 3.99092954e-07
Iter: 555 loss: 4.01114733e-07
Iter: 556 loss: 3.98826899e-07
Iter: 557 loss: 3.97813892e-07
Iter: 558 loss: 3.96069595e-07
Iter: 559 loss: 3.96052627e-07
Iter: 560 loss: 3.94549261e-07
Iter: 561 loss: 3.94540763e-07
Iter: 562 loss: 3.93817572e-07
Iter: 563 loss: 3.93768403e-07
Iter: 564 loss: 3.93179505e-07
Iter: 565 loss: 3.93391758e-07
Iter: 566 loss: 3.92766339e-07
Iter: 567 loss: 3.9214666e-07
Iter: 568 loss: 3.98388437e-07
Iter: 569 loss: 3.92121592e-07
Iter: 570 loss: 3.91616368e-07
Iter: 571 loss: 3.90925578e-07
Iter: 572 loss: 3.90876465e-07
Iter: 573 loss: 3.90105214e-07
Iter: 574 loss: 3.91374442e-07
Iter: 575 loss: 3.89733174e-07
Iter: 576 loss: 3.8898088e-07
Iter: 577 loss: 3.88966e-07
Iter: 578 loss: 3.88616655e-07
Iter: 579 loss: 3.87780631e-07
Iter: 580 loss: 3.99045291e-07
Iter: 581 loss: 3.87735668e-07
Iter: 582 loss: 3.86514841e-07
Iter: 583 loss: 3.88166654e-07
Iter: 584 loss: 3.85897266e-07
Iter: 585 loss: 3.84731322e-07
Iter: 586 loss: 3.87601375e-07
Iter: 587 loss: 3.8433032e-07
Iter: 588 loss: 3.83782549e-07
Iter: 589 loss: 3.83606647e-07
Iter: 590 loss: 3.83202064e-07
Iter: 591 loss: 3.82142105e-07
Iter: 592 loss: 3.89895433e-07
Iter: 593 loss: 3.8190683e-07
Iter: 594 loss: 3.80808899e-07
Iter: 595 loss: 3.93918697e-07
Iter: 596 loss: 3.80786361e-07
Iter: 597 loss: 3.80164352e-07
Iter: 598 loss: 3.80166512e-07
Iter: 599 loss: 3.79657223e-07
Iter: 600 loss: 3.79370363e-07
Iter: 601 loss: 3.79168625e-07
Iter: 602 loss: 3.78548947e-07
Iter: 603 loss: 3.85112486e-07
Iter: 604 loss: 3.78501227e-07
Iter: 605 loss: 3.77929524e-07
Iter: 606 loss: 3.76823721e-07
Iter: 607 loss: 3.98485838e-07
Iter: 608 loss: 3.76802291e-07
Iter: 609 loss: 3.76121221e-07
Iter: 610 loss: 3.76117185e-07
Iter: 611 loss: 3.75453823e-07
Iter: 612 loss: 3.77152e-07
Iter: 613 loss: 3.75230229e-07
Iter: 614 loss: 3.74638574e-07
Iter: 615 loss: 3.73792545e-07
Iter: 616 loss: 3.73777766e-07
Iter: 617 loss: 3.72617166e-07
Iter: 618 loss: 3.75041338e-07
Iter: 619 loss: 3.72159519e-07
Iter: 620 loss: 3.71432947e-07
Iter: 621 loss: 3.82758572e-07
Iter: 622 loss: 3.71439683e-07
Iter: 623 loss: 3.70892053e-07
Iter: 624 loss: 3.73576086e-07
Iter: 625 loss: 3.70806617e-07
Iter: 626 loss: 3.7017378e-07
Iter: 627 loss: 3.69622114e-07
Iter: 628 loss: 3.69454938e-07
Iter: 629 loss: 3.68684908e-07
Iter: 630 loss: 3.69105834e-07
Iter: 631 loss: 3.68190655e-07
Iter: 632 loss: 3.67700579e-07
Iter: 633 loss: 3.67590758e-07
Iter: 634 loss: 3.67073113e-07
Iter: 635 loss: 3.67118531e-07
Iter: 636 loss: 3.66716506e-07
Iter: 637 loss: 3.66118087e-07
Iter: 638 loss: 3.69818849e-07
Iter: 639 loss: 3.66029695e-07
Iter: 640 loss: 3.65497556e-07
Iter: 641 loss: 3.65167523e-07
Iter: 642 loss: 3.64941172e-07
Iter: 643 loss: 3.64257914e-07
Iter: 644 loss: 3.66692319e-07
Iter: 645 loss: 3.64068853e-07
Iter: 646 loss: 3.63306611e-07
Iter: 647 loss: 3.68821304e-07
Iter: 648 loss: 3.63216145e-07
Iter: 649 loss: 3.62791553e-07
Iter: 650 loss: 3.61694248e-07
Iter: 651 loss: 3.70304065e-07
Iter: 652 loss: 3.61476566e-07
Iter: 653 loss: 3.60592935e-07
Iter: 654 loss: 3.60595749e-07
Iter: 655 loss: 3.59874775e-07
Iter: 656 loss: 3.58972727e-07
Iter: 657 loss: 3.58865606e-07
Iter: 658 loss: 3.58584202e-07
Iter: 659 loss: 3.58235127e-07
Iter: 660 loss: 3.57742039e-07
Iter: 661 loss: 3.57648105e-07
Iter: 662 loss: 3.57334727e-07
Iter: 663 loss: 3.56683103e-07
Iter: 664 loss: 3.56795397e-07
Iter: 665 loss: 3.56208773e-07
Iter: 666 loss: 3.55449515e-07
Iter: 667 loss: 3.615379e-07
Iter: 668 loss: 3.55385879e-07
Iter: 669 loss: 3.54605959e-07
Iter: 670 loss: 3.57452564e-07
Iter: 671 loss: 3.54418603e-07
Iter: 672 loss: 3.53920768e-07
Iter: 673 loss: 3.54984536e-07
Iter: 674 loss: 3.53724545e-07
Iter: 675 loss: 3.52989645e-07
Iter: 676 loss: 3.53062035e-07
Iter: 677 loss: 3.52414531e-07
Iter: 678 loss: 3.51736304e-07
Iter: 679 loss: 3.54457569e-07
Iter: 680 loss: 3.51574442e-07
Iter: 681 loss: 3.50979462e-07
Iter: 682 loss: 3.57310341e-07
Iter: 683 loss: 3.50972869e-07
Iter: 684 loss: 3.50577e-07
Iter: 685 loss: 3.49909e-07
Iter: 686 loss: 3.65749912e-07
Iter: 687 loss: 3.49922345e-07
Iter: 688 loss: 3.49222518e-07
Iter: 689 loss: 3.49836e-07
Iter: 690 loss: 3.4881333e-07
Iter: 691 loss: 3.48140247e-07
Iter: 692 loss: 3.53638853e-07
Iter: 693 loss: 3.48093863e-07
Iter: 694 loss: 3.47428482e-07
Iter: 695 loss: 3.46665558e-07
Iter: 696 loss: 3.46562103e-07
Iter: 697 loss: 3.4664879e-07
Iter: 698 loss: 3.46110284e-07
Iter: 699 loss: 3.45750522e-07
Iter: 700 loss: 3.4508497e-07
Iter: 701 loss: 3.60773129e-07
Iter: 702 loss: 3.45074056e-07
Iter: 703 loss: 3.44561442e-07
Iter: 704 loss: 3.48272977e-07
Iter: 705 loss: 3.44520345e-07
Iter: 706 loss: 3.43850161e-07
Iter: 707 loss: 3.45508681e-07
Iter: 708 loss: 3.43612356e-07
Iter: 709 loss: 3.43043382e-07
Iter: 710 loss: 3.43929798e-07
Iter: 711 loss: 3.42767436e-07
Iter: 712 loss: 3.42222847e-07
Iter: 713 loss: 3.46088484e-07
Iter: 714 loss: 3.42181124e-07
Iter: 715 loss: 3.41801893e-07
Iter: 716 loss: 3.41267935e-07
Iter: 717 loss: 3.41272681e-07
Iter: 718 loss: 3.40741224e-07
Iter: 719 loss: 3.40735852e-07
Iter: 720 loss: 3.4020286e-07
Iter: 721 loss: 3.39536712e-07
Iter: 722 loss: 3.39479527e-07
Iter: 723 loss: 3.38650807e-07
Iter: 724 loss: 3.40256321e-07
Iter: 725 loss: 3.38323503e-07
Iter: 726 loss: 3.37571919e-07
Iter: 727 loss: 3.3948993e-07
Iter: 728 loss: 3.37301969e-07
Iter: 729 loss: 3.36699145e-07
Iter: 730 loss: 3.36451308e-07
Iter: 731 loss: 3.36097827e-07
Iter: 732 loss: 3.3549486e-07
Iter: 733 loss: 3.35426591e-07
Iter: 734 loss: 3.34997253e-07
Iter: 735 loss: 3.36793391e-07
Iter: 736 loss: 3.34936317e-07
Iter: 737 loss: 3.34558223e-07
Iter: 738 loss: 3.34005364e-07
Iter: 739 loss: 3.34015851e-07
Iter: 740 loss: 3.33810391e-07
Iter: 741 loss: 3.33632272e-07
Iter: 742 loss: 3.33374487e-07
Iter: 743 loss: 3.32696345e-07
Iter: 744 loss: 3.37659714e-07
Iter: 745 loss: 3.32555317e-07
Iter: 746 loss: 3.32087751e-07
Iter: 747 loss: 3.32011638e-07
Iter: 748 loss: 3.31632464e-07
Iter: 749 loss: 3.31214267e-07
Iter: 750 loss: 3.31127922e-07
Iter: 751 loss: 3.30740221e-07
Iter: 752 loss: 3.30726834e-07
Iter: 753 loss: 3.30400098e-07
Iter: 754 loss: 3.30128e-07
Iter: 755 loss: 3.30075295e-07
Iter: 756 loss: 3.2956865e-07
Iter: 757 loss: 3.29359096e-07
Iter: 758 loss: 3.29099805e-07
Iter: 759 loss: 3.28466456e-07
Iter: 760 loss: 3.29864775e-07
Iter: 761 loss: 3.28226065e-07
Iter: 762 loss: 3.27483747e-07
Iter: 763 loss: 3.31637523e-07
Iter: 764 loss: 3.27403626e-07
Iter: 765 loss: 3.26816746e-07
Iter: 766 loss: 3.28334238e-07
Iter: 767 loss: 3.26598553e-07
Iter: 768 loss: 3.2597012e-07
Iter: 769 loss: 3.31996659e-07
Iter: 770 loss: 3.25940164e-07
Iter: 771 loss: 3.25568578e-07
Iter: 772 loss: 3.25276517e-07
Iter: 773 loss: 3.25202478e-07
Iter: 774 loss: 3.24715131e-07
Iter: 775 loss: 3.31114251e-07
Iter: 776 loss: 3.2469913e-07
Iter: 777 loss: 3.24187965e-07
Iter: 778 loss: 3.2404256e-07
Iter: 779 loss: 3.23738305e-07
Iter: 780 loss: 3.23298252e-07
Iter: 781 loss: 3.25156975e-07
Iter: 782 loss: 3.23216454e-07
Iter: 783 loss: 3.2270745e-07
Iter: 784 loss: 3.23843039e-07
Iter: 785 loss: 3.22524784e-07
Iter: 786 loss: 3.22098259e-07
Iter: 787 loss: 3.22602773e-07
Iter: 788 loss: 3.21863297e-07
Iter: 789 loss: 3.21184132e-07
Iter: 790 loss: 3.2350448e-07
Iter: 791 loss: 3.20975175e-07
Iter: 792 loss: 3.20652219e-07
Iter: 793 loss: 3.20410265e-07
Iter: 794 loss: 3.2030124e-07
Iter: 795 loss: 3.19693868e-07
Iter: 796 loss: 3.20076538e-07
Iter: 797 loss: 3.19304974e-07
Iter: 798 loss: 3.18602019e-07
Iter: 799 loss: 3.21216021e-07
Iter: 800 loss: 3.18445473e-07
Iter: 801 loss: 3.17919046e-07
Iter: 802 loss: 3.2227598e-07
Iter: 803 loss: 3.17900827e-07
Iter: 804 loss: 3.17468732e-07
Iter: 805 loss: 3.20609075e-07
Iter: 806 loss: 3.17439941e-07
Iter: 807 loss: 3.17035813e-07
Iter: 808 loss: 3.16718626e-07
Iter: 809 loss: 3.16589706e-07
Iter: 810 loss: 3.1621849e-07
Iter: 811 loss: 3.21090965e-07
Iter: 812 loss: 3.16213061e-07
Iter: 813 loss: 3.1582897e-07
Iter: 814 loss: 3.16148771e-07
Iter: 815 loss: 3.15549585e-07
Iter: 816 loss: 3.15152789e-07
Iter: 817 loss: 3.14849757e-07
Iter: 818 loss: 3.14710917e-07
Iter: 819 loss: 3.14198985e-07
Iter: 820 loss: 3.1936716e-07
Iter: 821 loss: 3.14184547e-07
Iter: 822 loss: 3.13672047e-07
Iter: 823 loss: 3.1438725e-07
Iter: 824 loss: 3.13425e-07
Iter: 825 loss: 3.13005387e-07
Iter: 826 loss: 3.14814542e-07
Iter: 827 loss: 3.12950334e-07
Iter: 828 loss: 3.12449401e-07
Iter: 829 loss: 3.12983389e-07
Iter: 830 loss: 3.12218447e-07
Iter: 831 loss: 3.11916097e-07
Iter: 832 loss: 3.11481898e-07
Iter: 833 loss: 3.11460326e-07
Iter: 834 loss: 3.10734038e-07
Iter: 835 loss: 3.12630362e-07
Iter: 836 loss: 3.105134e-07
Iter: 837 loss: 3.09712476e-07
Iter: 838 loss: 3.10521216e-07
Iter: 839 loss: 3.09248207e-07
Iter: 840 loss: 3.08767227e-07
Iter: 841 loss: 3.08788231e-07
Iter: 842 loss: 3.08303726e-07
Iter: 843 loss: 3.09449661e-07
Iter: 844 loss: 3.08104546e-07
Iter: 845 loss: 3.07704767e-07
Iter: 846 loss: 3.08243443e-07
Iter: 847 loss: 3.07517382e-07
Iter: 848 loss: 3.07190788e-07
Iter: 849 loss: 3.09947723e-07
Iter: 850 loss: 3.07191e-07
Iter: 851 loss: 3.06857942e-07
Iter: 852 loss: 3.07570843e-07
Iter: 853 loss: 3.0670833e-07
Iter: 854 loss: 3.06473794e-07
Iter: 855 loss: 3.06160473e-07
Iter: 856 loss: 3.06143448e-07
Iter: 857 loss: 3.05688843e-07
Iter: 858 loss: 3.09293341e-07
Iter: 859 loss: 3.0565613e-07
Iter: 860 loss: 3.05265701e-07
Iter: 861 loss: 3.06383697e-07
Iter: 862 loss: 3.05127912e-07
Iter: 863 loss: 3.0473575e-07
Iter: 864 loss: 3.05158522e-07
Iter: 865 loss: 3.04529607e-07
Iter: 866 loss: 3.0407665e-07
Iter: 867 loss: 3.06893725e-07
Iter: 868 loss: 3.03982119e-07
Iter: 869 loss: 3.03692417e-07
Iter: 870 loss: 3.02989463e-07
Iter: 871 loss: 3.11046335e-07
Iter: 872 loss: 3.02944642e-07
Iter: 873 loss: 3.0236157e-07
Iter: 874 loss: 3.09708867e-07
Iter: 875 loss: 3.0236194e-07
Iter: 876 loss: 3.01883574e-07
Iter: 877 loss: 3.01424052e-07
Iter: 878 loss: 3.01325201e-07
Iter: 879 loss: 3.01113062e-07
Iter: 880 loss: 3.00899671e-07
Iter: 881 loss: 3.00620513e-07
Iter: 882 loss: 3.00212719e-07
Iter: 883 loss: 3.00184183e-07
Iter: 884 loss: 2.99724547e-07
Iter: 885 loss: 3.00197229e-07
Iter: 886 loss: 2.99473584e-07
Iter: 887 loss: 2.99087191e-07
Iter: 888 loss: 2.99066215e-07
Iter: 889 loss: 2.98736694e-07
Iter: 890 loss: 2.98535923e-07
Iter: 891 loss: 2.98380058e-07
Iter: 892 loss: 2.98031722e-07
Iter: 893 loss: 2.97796589e-07
Iter: 894 loss: 2.97660506e-07
Iter: 895 loss: 2.97434838e-07
Iter: 896 loss: 2.97335447e-07
Iter: 897 loss: 2.97133738e-07
Iter: 898 loss: 2.96937117e-07
Iter: 899 loss: 2.96895962e-07
Iter: 900 loss: 2.96597364e-07
Iter: 901 loss: 2.98724444e-07
Iter: 902 loss: 2.96570704e-07
Iter: 903 loss: 2.96336566e-07
Iter: 904 loss: 2.97099632e-07
Iter: 905 loss: 2.96258548e-07
Iter: 906 loss: 2.96021938e-07
Iter: 907 loss: 2.95486814e-07
Iter: 908 loss: 3.01273076e-07
Iter: 909 loss: 2.95401662e-07
Iter: 910 loss: 2.94748077e-07
Iter: 911 loss: 2.98622524e-07
Iter: 912 loss: 2.94645076e-07
Iter: 913 loss: 2.93998056e-07
Iter: 914 loss: 2.9628e-07
Iter: 915 loss: 2.93846483e-07
Iter: 916 loss: 2.93410778e-07
Iter: 917 loss: 2.93405463e-07
Iter: 918 loss: 2.93159133e-07
Iter: 919 loss: 2.9283558e-07
Iter: 920 loss: 2.92797495e-07
Iter: 921 loss: 2.9260795e-07
Iter: 922 loss: 2.92593711e-07
Iter: 923 loss: 2.92372476e-07
Iter: 924 loss: 2.91984918e-07
Iter: 925 loss: 3.00363553e-07
Iter: 926 loss: 2.91976278e-07
Iter: 927 loss: 2.91599832e-07
Iter: 928 loss: 2.92830975e-07
Iter: 929 loss: 2.91513686e-07
Iter: 930 loss: 2.91215542e-07
Iter: 931 loss: 2.92748183e-07
Iter: 932 loss: 2.91165122e-07
Iter: 933 loss: 2.9083489e-07
Iter: 934 loss: 2.91934782e-07
Iter: 935 loss: 2.9072703e-07
Iter: 936 loss: 2.90452192e-07
Iter: 937 loss: 2.8989831e-07
Iter: 938 loss: 3.01511193e-07
Iter: 939 loss: 2.89885236e-07
Iter: 940 loss: 2.89656896e-07
Iter: 941 loss: 2.89570039e-07
Iter: 942 loss: 2.89337038e-07
Iter: 943 loss: 2.89214682e-07
Iter: 944 loss: 2.89101308e-07
Iter: 945 loss: 2.88706445e-07
Iter: 946 loss: 2.88412565e-07
Iter: 947 loss: 2.88264687e-07
Iter: 948 loss: 2.87742154e-07
Iter: 949 loss: 2.90436105e-07
Iter: 950 loss: 2.87661322e-07
Iter: 951 loss: 2.87355817e-07
Iter: 952 loss: 2.87315601e-07
Iter: 953 loss: 2.87139585e-07
Iter: 954 loss: 2.86991963e-07
Iter: 955 loss: 2.86949103e-07
Iter: 956 loss: 2.86710218e-07
Iter: 957 loss: 2.89454135e-07
Iter: 958 loss: 2.86711838e-07
Iter: 959 loss: 2.86506634e-07
Iter: 960 loss: 2.86315071e-07
Iter: 961 loss: 2.86265816e-07
Iter: 962 loss: 2.85936494e-07
Iter: 963 loss: 2.8551392e-07
Iter: 964 loss: 2.85477256e-07
Iter: 965 loss: 2.84955775e-07
Iter: 966 loss: 2.88058601e-07
Iter: 967 loss: 2.84889779e-07
Iter: 968 loss: 2.84439864e-07
Iter: 969 loss: 2.90658818e-07
Iter: 970 loss: 2.84455922e-07
Iter: 971 loss: 2.84118528e-07
Iter: 972 loss: 2.83863386e-07
Iter: 973 loss: 2.83766497e-07
Iter: 974 loss: 2.83395877e-07
Iter: 975 loss: 2.8522723e-07
Iter: 976 loss: 2.83354268e-07
Iter: 977 loss: 2.82952954e-07
Iter: 978 loss: 2.84147063e-07
Iter: 979 loss: 2.82821048e-07
Iter: 980 loss: 2.82612405e-07
Iter: 981 loss: 2.82342455e-07
Iter: 982 loss: 2.82308832e-07
Iter: 983 loss: 2.81876311e-07
Iter: 984 loss: 2.84211808e-07
Iter: 985 loss: 2.81817137e-07
Iter: 986 loss: 2.81516122e-07
Iter: 987 loss: 2.85438659e-07
Iter: 988 loss: 2.81524734e-07
Iter: 989 loss: 2.81260412e-07
Iter: 990 loss: 2.81149369e-07
Iter: 991 loss: 2.81017321e-07
Iter: 992 loss: 2.8073697e-07
Iter: 993 loss: 2.84050628e-07
Iter: 994 loss: 2.80722617e-07
Iter: 995 loss: 2.80456732e-07
Iter: 996 loss: 2.80048368e-07
Iter: 997 loss: 2.80032168e-07
Iter: 998 loss: 2.79638641e-07
Iter: 999 loss: 2.80158616e-07
Iter: 1000 loss: 2.79420476e-07
Iter: 1001 loss: 2.78944469e-07
Iter: 1002 loss: 2.80262043e-07
Iter: 1003 loss: 2.78785109e-07
Iter: 1004 loss: 2.78758193e-07
Iter: 1005 loss: 2.78631887e-07
Iter: 1006 loss: 2.78517e-07
Iter: 1007 loss: 2.78183563e-07
Iter: 1008 loss: 2.80860746e-07
Iter: 1009 loss: 2.78122855e-07
Iter: 1010 loss: 2.77806805e-07
Iter: 1011 loss: 2.8159107e-07
Iter: 1012 loss: 2.7779663e-07
Iter: 1013 loss: 2.77524236e-07
Iter: 1014 loss: 2.78526898e-07
Iter: 1015 loss: 2.77470122e-07
Iter: 1016 loss: 2.77231322e-07
Iter: 1017 loss: 2.76887675e-07
Iter: 1018 loss: 2.76862181e-07
Iter: 1019 loss: 2.7645055e-07
Iter: 1020 loss: 2.77793845e-07
Iter: 1021 loss: 2.76343314e-07
Iter: 1022 loss: 2.76147176e-07
Iter: 1023 loss: 2.76087349e-07
Iter: 1024 loss: 2.75876289e-07
Iter: 1025 loss: 2.75388032e-07
Iter: 1026 loss: 2.81393341e-07
Iter: 1027 loss: 2.75339488e-07
Iter: 1028 loss: 2.75040406e-07
Iter: 1029 loss: 2.74974411e-07
Iter: 1030 loss: 2.74773356e-07
Iter: 1031 loss: 2.74496387e-07
Iter: 1032 loss: 2.74480982e-07
Iter: 1033 loss: 2.74131366e-07
Iter: 1034 loss: 2.7422746e-07
Iter: 1035 loss: 2.73876395e-07
Iter: 1036 loss: 2.73439355e-07
Iter: 1037 loss: 2.7395663e-07
Iter: 1038 loss: 2.73214823e-07
Iter: 1039 loss: 2.73010386e-07
Iter: 1040 loss: 2.72927025e-07
Iter: 1041 loss: 2.72696667e-07
Iter: 1042 loss: 2.72402019e-07
Iter: 1043 loss: 2.72369732e-07
Iter: 1044 loss: 2.72007782e-07
Iter: 1045 loss: 2.72851224e-07
Iter: 1046 loss: 2.71853622e-07
Iter: 1047 loss: 2.71506735e-07
Iter: 1048 loss: 2.74926776e-07
Iter: 1049 loss: 2.71516058e-07
Iter: 1050 loss: 2.71232e-07
Iter: 1051 loss: 2.71056621e-07
Iter: 1052 loss: 2.70943048e-07
Iter: 1053 loss: 2.70595962e-07
Iter: 1054 loss: 2.70850535e-07
Iter: 1055 loss: 2.70379189e-07
Iter: 1056 loss: 2.70033894e-07
Iter: 1057 loss: 2.74814568e-07
Iter: 1058 loss: 2.7002352e-07
Iter: 1059 loss: 2.69854837e-07
Iter: 1060 loss: 2.72633372e-07
Iter: 1061 loss: 2.69850943e-07
Iter: 1062 loss: 2.69712388e-07
Iter: 1063 loss: 2.69455626e-07
Iter: 1064 loss: 2.74730041e-07
Iter: 1065 loss: 2.69452585e-07
Iter: 1066 loss: 2.69101747e-07
Iter: 1067 loss: 2.71658024e-07
Iter: 1068 loss: 2.6905019e-07
Iter: 1069 loss: 2.68881365e-07
Iter: 1070 loss: 2.68435372e-07
Iter: 1071 loss: 2.74897559e-07
Iter: 1072 loss: 2.68418688e-07
Iter: 1073 loss: 2.67947115e-07
Iter: 1074 loss: 2.69915063e-07
Iter: 1075 loss: 2.6785176e-07
Iter: 1076 loss: 2.67323344e-07
Iter: 1077 loss: 2.68215587e-07
Iter: 1078 loss: 2.67092901e-07
Iter: 1079 loss: 2.66954089e-07
Iter: 1080 loss: 2.66826078e-07
Iter: 1081 loss: 2.66654808e-07
Iter: 1082 loss: 2.66335803e-07
Iter: 1083 loss: 2.66339072e-07
Iter: 1084 loss: 2.66046243e-07
Iter: 1085 loss: 2.66023164e-07
Iter: 1086 loss: 2.65818926e-07
Iter: 1087 loss: 2.65452172e-07
Iter: 1088 loss: 2.68247902e-07
Iter: 1089 loss: 2.6542881e-07
Iter: 1090 loss: 2.65185093e-07
Iter: 1091 loss: 2.65588568e-07
Iter: 1092 loss: 2.6507962e-07
Iter: 1093 loss: 2.64797251e-07
Iter: 1094 loss: 2.68057789e-07
Iter: 1095 loss: 2.6481041e-07
Iter: 1096 loss: 2.64573316e-07
Iter: 1097 loss: 2.64736798e-07
Iter: 1098 loss: 2.64410346e-07
Iter: 1099 loss: 2.64153044e-07
Iter: 1100 loss: 2.65716722e-07
Iter: 1101 loss: 2.64129142e-07
Iter: 1102 loss: 2.6387147e-07
Iter: 1103 loss: 2.63686587e-07
Iter: 1104 loss: 2.63601265e-07
Iter: 1105 loss: 2.63261825e-07
Iter: 1106 loss: 2.63143534e-07
Iter: 1107 loss: 2.62929404e-07
Iter: 1108 loss: 2.62418212e-07
Iter: 1109 loss: 2.64664436e-07
Iter: 1110 loss: 2.62317144e-07
Iter: 1111 loss: 2.61866489e-07
Iter: 1112 loss: 2.64045099e-07
Iter: 1113 loss: 2.61791683e-07
Iter: 1114 loss: 2.61535661e-07
Iter: 1115 loss: 2.61538787e-07
Iter: 1116 loss: 2.61364278e-07
Iter: 1117 loss: 2.61064628e-07
Iter: 1118 loss: 2.61059824e-07
Iter: 1119 loss: 2.60856808e-07
Iter: 1120 loss: 2.6083643e-07
Iter: 1121 loss: 2.60658567e-07
Iter: 1122 loss: 2.60433751e-07
Iter: 1123 loss: 2.6042315e-07
Iter: 1124 loss: 2.60169e-07
Iter: 1125 loss: 2.60562302e-07
Iter: 1126 loss: 2.60014815e-07
Iter: 1127 loss: 2.59774822e-07
Iter: 1128 loss: 2.63140862e-07
Iter: 1129 loss: 2.59763738e-07
Iter: 1130 loss: 2.59501945e-07
Iter: 1131 loss: 2.59414122e-07
Iter: 1132 loss: 2.59259679e-07
Iter: 1133 loss: 2.59048761e-07
Iter: 1134 loss: 2.59024773e-07
Iter: 1135 loss: 2.58905118e-07
Iter: 1136 loss: 2.58535067e-07
Iter: 1137 loss: 2.61266536e-07
Iter: 1138 loss: 2.58460432e-07
Iter: 1139 loss: 2.57999147e-07
Iter: 1140 loss: 2.59476508e-07
Iter: 1141 loss: 2.57849536e-07
Iter: 1142 loss: 2.57489887e-07
Iter: 1143 loss: 2.60122192e-07
Iter: 1144 loss: 2.57448761e-07
Iter: 1145 loss: 2.57196e-07
Iter: 1146 loss: 2.5839131e-07
Iter: 1147 loss: 2.57142034e-07
Iter: 1148 loss: 2.56849177e-07
Iter: 1149 loss: 2.58542968e-07
Iter: 1150 loss: 2.5681328e-07
Iter: 1151 loss: 2.56640192e-07
Iter: 1152 loss: 2.5683596e-07
Iter: 1153 loss: 2.56560696e-07
Iter: 1154 loss: 2.56326359e-07
Iter: 1155 loss: 2.57188134e-07
Iter: 1156 loss: 2.56273836e-07
Iter: 1157 loss: 2.56038192e-07
Iter: 1158 loss: 2.55892928e-07
Iter: 1159 loss: 2.55810562e-07
Iter: 1160 loss: 2.55494513e-07
Iter: 1161 loss: 2.5569841e-07
Iter: 1162 loss: 2.55315712e-07
Iter: 1163 loss: 2.55131908e-07
Iter: 1164 loss: 2.55086519e-07
Iter: 1165 loss: 2.54924686e-07
Iter: 1166 loss: 2.54705185e-07
Iter: 1167 loss: 2.54677417e-07
Iter: 1168 loss: 2.54413976e-07
Iter: 1169 loss: 2.56799808e-07
Iter: 1170 loss: 2.54377085e-07
Iter: 1171 loss: 2.54157783e-07
Iter: 1172 loss: 2.54014424e-07
Iter: 1173 loss: 2.53926629e-07
Iter: 1174 loss: 2.53665405e-07
Iter: 1175 loss: 2.53687858e-07
Iter: 1176 loss: 2.53457188e-07
Iter: 1177 loss: 2.5305377e-07
Iter: 1178 loss: 2.53523353e-07
Iter: 1179 loss: 2.52837594e-07
Iter: 1180 loss: 2.5254846e-07
Iter: 1181 loss: 2.52506595e-07
Iter: 1182 loss: 2.52274674e-07
Iter: 1183 loss: 2.52602604e-07
Iter: 1184 loss: 2.5215914e-07
Iter: 1185 loss: 2.51915935e-07
Iter: 1186 loss: 2.5249733e-07
Iter: 1187 loss: 2.51812054e-07
Iter: 1188 loss: 2.51456015e-07
Iter: 1189 loss: 2.51725481e-07
Iter: 1190 loss: 2.51271956e-07
Iter: 1191 loss: 2.50968981e-07
Iter: 1192 loss: 2.51157871e-07
Iter: 1193 loss: 2.50775656e-07
Iter: 1194 loss: 2.50461113e-07
Iter: 1195 loss: 2.52485876e-07
Iter: 1196 loss: 2.50410778e-07
Iter: 1197 loss: 2.5028794e-07
Iter: 1198 loss: 2.50269125e-07
Iter: 1199 loss: 2.50165357e-07
Iter: 1200 loss: 2.49963705e-07
Iter: 1201 loss: 2.54623671e-07
Iter: 1202 loss: 2.49957793e-07
Iter: 1203 loss: 2.49754095e-07
Iter: 1204 loss: 2.52278596e-07
Iter: 1205 loss: 2.49757818e-07
Iter: 1206 loss: 2.49604909e-07
Iter: 1207 loss: 2.49331379e-07
Iter: 1208 loss: 2.49327712e-07
Iter: 1209 loss: 2.49023941e-07
Iter: 1210 loss: 2.49512198e-07
Iter: 1211 loss: 2.48885954e-07
Iter: 1212 loss: 2.48511867e-07
Iter: 1213 loss: 2.49282351e-07
Iter: 1214 loss: 2.48346396e-07
Iter: 1215 loss: 2.48069028e-07
Iter: 1216 loss: 2.48052942e-07
Iter: 1217 loss: 2.47765911e-07
Iter: 1218 loss: 2.47813404e-07
Iter: 1219 loss: 2.47524298e-07
Iter: 1220 loss: 2.47328984e-07
Iter: 1221 loss: 2.5005366e-07
Iter: 1222 loss: 2.47318496e-07
Iter: 1223 loss: 2.47128128e-07
Iter: 1224 loss: 2.4687634e-07
Iter: 1225 loss: 2.46862783e-07
Iter: 1226 loss: 2.46569158e-07
Iter: 1227 loss: 2.47324465e-07
Iter: 1228 loss: 2.4646863e-07
Iter: 1229 loss: 2.46169691e-07
Iter: 1230 loss: 2.46202831e-07
Iter: 1231 loss: 2.45950929e-07
Iter: 1232 loss: 2.45672652e-07
Iter: 1233 loss: 2.49498157e-07
Iter: 1234 loss: 2.45665802e-07
Iter: 1235 loss: 2.45487882e-07
Iter: 1236 loss: 2.47712762e-07
Iter: 1237 loss: 2.45470602e-07
Iter: 1238 loss: 2.45310758e-07
Iter: 1239 loss: 2.4516163e-07
Iter: 1240 loss: 2.45114876e-07
Iter: 1241 loss: 2.44930902e-07
Iter: 1242 loss: 2.46657947e-07
Iter: 1243 loss: 2.44917402e-07
Iter: 1244 loss: 2.44703642e-07
Iter: 1245 loss: 2.44460779e-07
Iter: 1246 loss: 2.44450348e-07
Iter: 1247 loss: 2.44169257e-07
Iter: 1248 loss: 2.44340185e-07
Iter: 1249 loss: 2.43974114e-07
Iter: 1250 loss: 2.43635e-07
Iter: 1251 loss: 2.45162e-07
Iter: 1252 loss: 2.43567e-07
Iter: 1253 loss: 2.43283296e-07
Iter: 1254 loss: 2.43577261e-07
Iter: 1255 loss: 2.43110435e-07
Iter: 1256 loss: 2.42924358e-07
Iter: 1257 loss: 2.42897784e-07
Iter: 1258 loss: 2.42721086e-07
Iter: 1259 loss: 2.42417116e-07
Iter: 1260 loss: 2.42409612e-07
Iter: 1261 loss: 2.42171751e-07
Iter: 1262 loss: 2.42166323e-07
Iter: 1263 loss: 2.419699e-07
Iter: 1264 loss: 2.42330572e-07
Iter: 1265 loss: 2.41874432e-07
Iter: 1266 loss: 2.41700718e-07
Iter: 1267 loss: 2.4135602e-07
Iter: 1268 loss: 2.47466915e-07
Iter: 1269 loss: 2.41340388e-07
Iter: 1270 loss: 2.41357384e-07
Iter: 1271 loss: 2.41165367e-07
Iter: 1272 loss: 2.41009531e-07
Iter: 1273 loss: 2.40917927e-07
Iter: 1274 loss: 2.40863073e-07
Iter: 1275 loss: 2.40689218e-07
Iter: 1276 loss: 2.41693954e-07
Iter: 1277 loss: 2.40672534e-07
Iter: 1278 loss: 2.40492597e-07
Iter: 1279 loss: 2.40601764e-07
Iter: 1280 loss: 2.40386839e-07
Iter: 1281 loss: 2.40173677e-07
Iter: 1282 loss: 2.4004396e-07
Iter: 1283 loss: 2.39941471e-07
Iter: 1284 loss: 2.39600865e-07
Iter: 1285 loss: 2.39888095e-07
Iter: 1286 loss: 2.39426441e-07
Iter: 1287 loss: 2.39097403e-07
Iter: 1288 loss: 2.42737428e-07
Iter: 1289 loss: 2.3909115e-07
Iter: 1290 loss: 2.38897371e-07
Iter: 1291 loss: 2.3914285e-07
Iter: 1292 loss: 2.38778682e-07
Iter: 1293 loss: 2.38465304e-07
Iter: 1294 loss: 2.40732248e-07
Iter: 1295 loss: 2.38433103e-07
Iter: 1296 loss: 2.38283008e-07
Iter: 1297 loss: 2.38246713e-07
Iter: 1298 loss: 2.3812521e-07
Iter: 1299 loss: 2.37849036e-07
Iter: 1300 loss: 2.4004828e-07
Iter: 1301 loss: 2.37841121e-07
Iter: 1302 loss: 2.37689108e-07
Iter: 1303 loss: 2.37515451e-07
Iter: 1304 loss: 2.37509255e-07
Iter: 1305 loss: 2.37215545e-07
Iter: 1306 loss: 2.37238027e-07
Iter: 1307 loss: 2.36988569e-07
Iter: 1308 loss: 2.36880496e-07
Iter: 1309 loss: 2.36813023e-07
Iter: 1310 loss: 2.36631777e-07
Iter: 1311 loss: 2.36659915e-07
Iter: 1312 loss: 2.36507191e-07
Iter: 1313 loss: 2.36341947e-07
Iter: 1314 loss: 2.3634891e-07
Iter: 1315 loss: 2.36200719e-07
Iter: 1316 loss: 2.35958439e-07
Iter: 1317 loss: 2.37900792e-07
Iter: 1318 loss: 2.35936042e-07
Iter: 1319 loss: 2.35753987e-07
Iter: 1320 loss: 2.35924546e-07
Iter: 1321 loss: 2.35664601e-07
Iter: 1322 loss: 2.35468519e-07
Iter: 1323 loss: 2.35232292e-07
Iter: 1324 loss: 2.35195557e-07
Iter: 1325 loss: 2.34882364e-07
Iter: 1326 loss: 2.37428e-07
Iter: 1327 loss: 2.34867088e-07
Iter: 1328 loss: 2.34624764e-07
Iter: 1329 loss: 2.35947738e-07
Iter: 1330 loss: 2.34589251e-07
Iter: 1331 loss: 2.3427171e-07
Iter: 1332 loss: 2.35017779e-07
Iter: 1333 loss: 2.34174507e-07
Iter: 1334 loss: 2.33999799e-07
Iter: 1335 loss: 2.34598843e-07
Iter: 1336 loss: 2.3397169e-07
Iter: 1337 loss: 2.33746789e-07
Iter: 1338 loss: 2.33648208e-07
Iter: 1339 loss: 2.33539055e-07
Iter: 1340 loss: 2.3330773e-07
Iter: 1341 loss: 2.33632619e-07
Iter: 1342 loss: 2.33203423e-07
Iter: 1343 loss: 2.32980099e-07
Iter: 1344 loss: 2.33688496e-07
Iter: 1345 loss: 2.32905009e-07
Iter: 1346 loss: 2.32778689e-07
Iter: 1347 loss: 2.32771413e-07
Iter: 1348 loss: 2.32644211e-07
Iter: 1349 loss: 2.32346579e-07
Iter: 1350 loss: 2.36317305e-07
Iter: 1351 loss: 2.3232991e-07
Iter: 1352 loss: 2.32098728e-07
Iter: 1353 loss: 2.32375186e-07
Iter: 1354 loss: 2.31950722e-07
Iter: 1355 loss: 2.31725096e-07
Iter: 1356 loss: 2.31722879e-07
Iter: 1357 loss: 2.31514576e-07
Iter: 1358 loss: 2.31738269e-07
Iter: 1359 loss: 2.31423897e-07
Iter: 1360 loss: 2.31181332e-07
Iter: 1361 loss: 2.31157927e-07
Iter: 1362 loss: 2.30978529e-07
Iter: 1363 loss: 2.30755319e-07
Iter: 1364 loss: 2.32592427e-07
Iter: 1365 loss: 2.30755532e-07
Iter: 1366 loss: 2.30557703e-07
Iter: 1367 loss: 2.3144419e-07
Iter: 1368 loss: 2.30516775e-07
Iter: 1369 loss: 2.30326691e-07
Iter: 1370 loss: 2.30344924e-07
Iter: 1371 loss: 2.30177619e-07
Iter: 1372 loss: 2.30022863e-07
Iter: 1373 loss: 2.30027041e-07
Iter: 1374 loss: 2.29882858e-07
Iter: 1375 loss: 2.29540717e-07
Iter: 1376 loss: 2.32614042e-07
Iter: 1377 loss: 2.29487938e-07
Iter: 1378 loss: 2.29151411e-07
Iter: 1379 loss: 2.30992214e-07
Iter: 1380 loss: 2.29059765e-07
Iter: 1381 loss: 2.28828924e-07
Iter: 1382 loss: 2.30693814e-07
Iter: 1383 loss: 2.28801582e-07
Iter: 1384 loss: 2.28609849e-07
Iter: 1385 loss: 2.31466757e-07
Iter: 1386 loss: 2.28610588e-07
Iter: 1387 loss: 2.28485035e-07
Iter: 1388 loss: 2.28199156e-07
Iter: 1389 loss: 2.30848485e-07
Iter: 1390 loss: 2.28140593e-07
Iter: 1391 loss: 2.27930613e-07
Iter: 1392 loss: 2.29236235e-07
Iter: 1393 loss: 2.27891718e-07
Iter: 1394 loss: 2.27707972e-07
Iter: 1395 loss: 2.28010293e-07
Iter: 1396 loss: 2.27614066e-07
Iter: 1397 loss: 2.2740798e-07
Iter: 1398 loss: 2.29281738e-07
Iter: 1399 loss: 2.27396072e-07
Iter: 1400 loss: 2.27183975e-07
Iter: 1401 loss: 2.27190242e-07
Iter: 1402 loss: 2.27004023e-07
Iter: 1403 loss: 2.26858731e-07
Iter: 1404 loss: 2.27164193e-07
Iter: 1405 loss: 2.26786554e-07
Iter: 1406 loss: 2.26591482e-07
Iter: 1407 loss: 2.27605156e-07
Iter: 1408 loss: 2.26555699e-07
Iter: 1409 loss: 2.26391577e-07
Iter: 1410 loss: 2.26796885e-07
Iter: 1411 loss: 2.26340916e-07
Iter: 1412 loss: 2.26159187e-07
Iter: 1413 loss: 2.26073254e-07
Iter: 1414 loss: 2.25993574e-07
Iter: 1415 loss: 2.25754476e-07
Iter: 1416 loss: 2.26324474e-07
Iter: 1417 loss: 2.25650169e-07
Iter: 1418 loss: 2.25488805e-07
Iter: 1419 loss: 2.26705424e-07
Iter: 1420 loss: 2.25465726e-07
Iter: 1421 loss: 2.2527594e-07
Iter: 1422 loss: 2.2609791e-07
Iter: 1423 loss: 2.25244506e-07
Iter: 1424 loss: 2.25130336e-07
Iter: 1425 loss: 2.24937381e-07
Iter: 1426 loss: 2.24941147e-07
Iter: 1427 loss: 2.24695242e-07
Iter: 1428 loss: 2.25464177e-07
Iter: 1429 loss: 2.24636807e-07
Iter: 1430 loss: 2.24426216e-07
Iter: 1431 loss: 2.24631691e-07
Iter: 1432 loss: 2.24306561e-07
Iter: 1433 loss: 2.24084758e-07
Iter: 1434 loss: 2.27538393e-07
Iter: 1435 loss: 2.24084943e-07
Iter: 1436 loss: 2.2388744e-07
Iter: 1437 loss: 2.23816414e-07
Iter: 1438 loss: 2.23705726e-07
Iter: 1439 loss: 2.23471559e-07
Iter: 1440 loss: 2.24057629e-07
Iter: 1441 loss: 2.23382898e-07
Iter: 1442 loss: 2.2321484e-07
Iter: 1443 loss: 2.23469556e-07
Iter: 1444 loss: 2.2313256e-07
Iter: 1445 loss: 2.22935029e-07
Iter: 1446 loss: 2.25935807e-07
Iter: 1447 loss: 2.22934858e-07
Iter: 1448 loss: 2.22797325e-07
Iter: 1449 loss: 2.22831488e-07
Iter: 1450 loss: 2.22693046e-07
Iter: 1451 loss: 2.22550412e-07
Iter: 1452 loss: 2.22843127e-07
Iter: 1453 loss: 2.22503957e-07
Iter: 1454 loss: 2.2227e-07
Iter: 1455 loss: 2.22678025e-07
Iter: 1456 loss: 2.2217489e-07
Iter: 1457 loss: 2.22004104e-07
Iter: 1458 loss: 2.21743065e-07
Iter: 1459 loss: 2.21747442e-07
Iter: 1460 loss: 2.21496862e-07
Iter: 1461 loss: 2.24901783e-07
Iter: 1462 loss: 2.21486545e-07
Iter: 1463 loss: 2.21362825e-07
Iter: 1464 loss: 2.2136561e-07
Iter: 1465 loss: 2.21247632e-07
Iter: 1466 loss: 2.21085713e-07
Iter: 1467 loss: 2.21092222e-07
Iter: 1468 loss: 2.20929437e-07
Iter: 1469 loss: 2.20958839e-07
Iter: 1470 loss: 2.20815053e-07
Iter: 1471 loss: 2.20728893e-07
Iter: 1472 loss: 2.20709012e-07
Iter: 1473 loss: 2.20626546e-07
Iter: 1474 loss: 2.20445e-07
Iter: 1475 loss: 2.23246943e-07
Iter: 1476 loss: 2.2043821e-07
Iter: 1477 loss: 2.20232508e-07
Iter: 1478 loss: 2.20592696e-07
Iter: 1479 loss: 2.20138801e-07
Iter: 1480 loss: 2.19929092e-07
Iter: 1481 loss: 2.22677912e-07
Iter: 1482 loss: 2.19934208e-07
Iter: 1483 loss: 2.19743157e-07
Iter: 1484 loss: 2.20084829e-07
Iter: 1485 loss: 2.19662311e-07
Iter: 1486 loss: 2.1952134e-07
Iter: 1487 loss: 2.19385129e-07
Iter: 1488 loss: 2.19338219e-07
Iter: 1489 loss: 2.19065015e-07
Iter: 1490 loss: 2.22010939e-07
Iter: 1491 loss: 2.190705e-07
Iter: 1492 loss: 2.18951655e-07
Iter: 1493 loss: 2.18804814e-07
Iter: 1494 loss: 2.188049e-07
Iter: 1495 loss: 2.18629765e-07
Iter: 1496 loss: 2.2008075e-07
Iter: 1497 loss: 2.18617757e-07
Iter: 1498 loss: 2.18483819e-07
Iter: 1499 loss: 2.2007535e-07
Iter: 1500 loss: 2.18492588e-07
Iter: 1501 loss: 2.18369792e-07
Iter: 1502 loss: 2.1817786e-07
Iter: 1503 loss: 2.21669168e-07
Iter: 1504 loss: 2.18157652e-07
Iter: 1505 loss: 2.17958132e-07
Iter: 1506 loss: 2.18612229e-07
Iter: 1507 loss: 2.17906305e-07
Iter: 1508 loss: 2.1768048e-07
Iter: 1509 loss: 2.19389108e-07
Iter: 1510 loss: 2.17672905e-07
Iter: 1511 loss: 2.17481173e-07
Iter: 1512 loss: 2.17566566e-07
Iter: 1513 loss: 2.17357893e-07
Iter: 1514 loss: 2.17133902e-07
Iter: 1515 loss: 2.17191626e-07
Iter: 1516 loss: 2.16988312e-07
Iter: 1517 loss: 2.16813447e-07
Iter: 1518 loss: 2.19504798e-07
Iter: 1519 loss: 2.16814826e-07
Iter: 1520 loss: 2.16644651e-07
Iter: 1521 loss: 2.17055856e-07
Iter: 1522 loss: 2.16578599e-07
Iter: 1523 loss: 2.16450317e-07
Iter: 1524 loss: 2.16469104e-07
Iter: 1525 loss: 2.16341476e-07
Iter: 1526 loss: 2.1620852e-07
Iter: 1527 loss: 2.17572762e-07
Iter: 1528 loss: 2.16199794e-07
Iter: 1529 loss: 2.16050296e-07
Iter: 1530 loss: 2.1576966e-07
Iter: 1531 loss: 2.22370105e-07
Iter: 1532 loss: 2.15765638e-07
Iter: 1533 loss: 2.15577657e-07
Iter: 1534 loss: 2.17033318e-07
Iter: 1535 loss: 2.15569031e-07
Iter: 1536 loss: 2.15386947e-07
Iter: 1537 loss: 2.17458023e-07
Iter: 1538 loss: 2.15397435e-07
Iter: 1539 loss: 2.15289333e-07
Iter: 1540 loss: 2.1506537e-07
Iter: 1541 loss: 2.18215803e-07
Iter: 1542 loss: 2.15065455e-07
Iter: 1543 loss: 2.14794099e-07
Iter: 1544 loss: 2.15566686e-07
Iter: 1545 loss: 2.14700066e-07
Iter: 1546 loss: 2.14550283e-07
Iter: 1547 loss: 2.14551804e-07
Iter: 1548 loss: 2.14411529e-07
Iter: 1549 loss: 2.14249354e-07
Iter: 1550 loss: 2.14258392e-07
Iter: 1551 loss: 2.14062567e-07
Iter: 1552 loss: 2.145417e-07
Iter: 1553 loss: 2.13984436e-07
Iter: 1554 loss: 2.13856794e-07
Iter: 1555 loss: 2.15808313e-07
Iter: 1556 loss: 2.1384858e-07
Iter: 1557 loss: 2.13703942e-07
Iter: 1558 loss: 2.14194188e-07
Iter: 1559 loss: 2.13662034e-07
Iter: 1560 loss: 2.1357414e-07
Iter: 1561 loss: 2.13337216e-07
Iter: 1562 loss: 2.16231939e-07
Iter: 1563 loss: 2.13315246e-07
Iter: 1564 loss: 2.13120913e-07
Iter: 1565 loss: 2.1310359e-07
Iter: 1566 loss: 2.12949843e-07
Iter: 1567 loss: 2.13003943e-07
Iter: 1568 loss: 2.12852783e-07
Iter: 1569 loss: 2.12652083e-07
Iter: 1570 loss: 2.12795143e-07
Iter: 1571 loss: 2.12545501e-07
Iter: 1572 loss: 2.12420829e-07
Iter: 1573 loss: 2.12382616e-07
Iter: 1574 loss: 2.12280924e-07
Iter: 1575 loss: 2.12072919e-07
Iter: 1576 loss: 2.14889027e-07
Iter: 1577 loss: 2.12050566e-07
Iter: 1578 loss: 2.11832742e-07
Iter: 1579 loss: 2.1254904e-07
Iter: 1580 loss: 2.11799716e-07
Iter: 1581 loss: 2.11582432e-07
Iter: 1582 loss: 2.13659803e-07
Iter: 1583 loss: 2.11581778e-07
Iter: 1584 loss: 2.1142128e-07
Iter: 1585 loss: 2.11371031e-07
Iter: 1586 loss: 2.11278405e-07
Iter: 1587 loss: 2.11069803e-07
Iter: 1588 loss: 2.11051898e-07
Iter: 1589 loss: 2.10907061e-07
Iter: 1590 loss: 2.10804473e-07
Iter: 1591 loss: 2.10769e-07
Iter: 1592 loss: 2.1061976e-07
Iter: 1593 loss: 2.10446856e-07
Iter: 1594 loss: 2.10433512e-07
Iter: 1595 loss: 2.1027472e-07
Iter: 1596 loss: 2.10857706e-07
Iter: 1597 loss: 2.10235115e-07
Iter: 1598 loss: 2.1009555e-07
Iter: 1599 loss: 2.11026645e-07
Iter: 1600 loss: 2.10075399e-07
Iter: 1601 loss: 2.09927862e-07
Iter: 1602 loss: 2.0996896e-07
Iter: 1603 loss: 2.09842938e-07
Iter: 1604 loss: 2.09674312e-07
Iter: 1605 loss: 2.10018896e-07
Iter: 1606 loss: 2.09641897e-07
Iter: 1607 loss: 2.09414381e-07
Iter: 1608 loss: 2.10348603e-07
Iter: 1609 loss: 2.09367656e-07
Iter: 1610 loss: 2.09247418e-07
Iter: 1611 loss: 2.09168121e-07
Iter: 1612 loss: 2.09109373e-07
Iter: 1613 loss: 2.08911132e-07
Iter: 1614 loss: 2.08957943e-07
Iter: 1615 loss: 2.08754784e-07
Iter: 1616 loss: 2.08583828e-07
Iter: 1617 loss: 2.08593789e-07
Iter: 1618 loss: 2.08452406e-07
Iter: 1619 loss: 2.08324508e-07
Iter: 1620 loss: 2.08293784e-07
Iter: 1621 loss: 2.08101739e-07
Iter: 1622 loss: 2.08107849e-07
Iter: 1623 loss: 2.07960227e-07
Iter: 1624 loss: 2.07664186e-07
Iter: 1625 loss: 2.08597285e-07
Iter: 1626 loss: 2.07572981e-07
Iter: 1627 loss: 2.07522717e-07
Iter: 1628 loss: 2.07468588e-07
Iter: 1629 loss: 2.07372736e-07
Iter: 1630 loss: 2.07115789e-07
Iter: 1631 loss: 2.09394955e-07
Iter: 1632 loss: 2.07075956e-07
Iter: 1633 loss: 2.0681108e-07
Iter: 1634 loss: 2.07883701e-07
Iter: 1635 loss: 2.06746734e-07
Iter: 1636 loss: 2.06534935e-07
Iter: 1637 loss: 2.06547611e-07
Iter: 1638 loss: 2.06407719e-07
Iter: 1639 loss: 2.06595217e-07
Iter: 1640 loss: 2.06338257e-07
Iter: 1641 loss: 2.06235896e-07
Iter: 1642 loss: 2.07171951e-07
Iter: 1643 loss: 2.06227469e-07
Iter: 1644 loss: 2.06121058e-07
Iter: 1645 loss: 2.05852331e-07
Iter: 1646 loss: 2.09316227e-07
Iter: 1647 loss: 2.05831242e-07
Iter: 1648 loss: 2.05624346e-07
Iter: 1649 loss: 2.06516475e-07
Iter: 1650 loss: 2.05571567e-07
Iter: 1651 loss: 2.05386641e-07
Iter: 1652 loss: 2.06462133e-07
Iter: 1653 loss: 2.05363392e-07
Iter: 1654 loss: 2.05214519e-07
Iter: 1655 loss: 2.06192141e-07
Iter: 1656 loss: 2.05188982e-07
Iter: 1657 loss: 2.05056892e-07
Iter: 1658 loss: 2.04863454e-07
Iter: 1659 loss: 2.04851204e-07
Iter: 1660 loss: 2.04645886e-07
Iter: 1661 loss: 2.05240198e-07
Iter: 1662 loss: 2.04568181e-07
Iter: 1663 loss: 2.04372412e-07
Iter: 1664 loss: 2.06358678e-07
Iter: 1665 loss: 2.04375326e-07
Iter: 1666 loss: 2.04246817e-07
Iter: 1667 loss: 2.05446696e-07
Iter: 1668 loss: 2.04246078e-07
Iter: 1669 loss: 2.04152911e-07
Iter: 1670 loss: 2.03925282e-07
Iter: 1671 loss: 2.0582219e-07
Iter: 1672 loss: 2.03911654e-07
Iter: 1673 loss: 2.03693361e-07
Iter: 1674 loss: 2.04896736e-07
Iter: 1675 loss: 2.03644291e-07
Iter: 1676 loss: 2.03455727e-07
Iter: 1677 loss: 2.04313693e-07
Iter: 1678 loss: 2.03418381e-07
Iter: 1679 loss: 2.03303301e-07
Iter: 1680 loss: 2.03290227e-07
Iter: 1681 loss: 2.03197459e-07
Iter: 1682 loss: 2.03087396e-07
Iter: 1683 loss: 2.0307111e-07
Iter: 1684 loss: 2.02932611e-07
Iter: 1685 loss: 2.04581369e-07
Iter: 1686 loss: 2.02946595e-07
Iter: 1687 loss: 2.02832467e-07
Iter: 1688 loss: 2.02633828e-07
Iter: 1689 loss: 2.06441499e-07
Iter: 1690 loss: 2.02620029e-07
Iter: 1691 loss: 2.02511643e-07
Iter: 1692 loss: 2.02512169e-07
Iter: 1693 loss: 2.02399519e-07
Iter: 1694 loss: 2.02500701e-07
Iter: 1695 loss: 2.0233945e-07
Iter: 1696 loss: 2.02205115e-07
Iter: 1697 loss: 2.02223177e-07
Iter: 1698 loss: 2.02095237e-07
Iter: 1699 loss: 2.01920486e-07
Iter: 1700 loss: 2.01817898e-07
Iter: 1701 loss: 2.01745152e-07
Iter: 1702 loss: 2.01584641e-07
Iter: 1703 loss: 2.01588335e-07
Iter: 1704 loss: 2.01431078e-07
Iter: 1705 loss: 2.02218501e-07
Iter: 1706 loss: 2.01420036e-07
Iter: 1707 loss: 2.01314421e-07
Iter: 1708 loss: 2.0115391e-07
Iter: 1709 loss: 2.011638e-07
Iter: 1710 loss: 2.00964e-07
Iter: 1711 loss: 2.01185401e-07
Iter: 1712 loss: 2.00873103e-07
Iter: 1713 loss: 2.00664118e-07
Iter: 1714 loss: 2.01637931e-07
Iter: 1715 loss: 2.00640983e-07
Iter: 1716 loss: 2.00512432e-07
Iter: 1717 loss: 2.00505355e-07
Iter: 1718 loss: 2.00408763e-07
Iter: 1719 loss: 2.00389451e-07
Iter: 1720 loss: 2.00315696e-07
Iter: 1721 loss: 2.00205562e-07
Iter: 1722 loss: 2.00256807e-07
Iter: 1723 loss: 2.00126834e-07
Iter: 1724 loss: 1.99930867e-07
Iter: 1725 loss: 2.00743756e-07
Iter: 1726 loss: 1.99874066e-07
Iter: 1727 loss: 1.99754538e-07
Iter: 1728 loss: 1.99732597e-07
Iter: 1729 loss: 1.99645285e-07
Iter: 1730 loss: 1.99476631e-07
Iter: 1731 loss: 2.00002205e-07
Iter: 1732 loss: 1.99425415e-07
Iter: 1733 loss: 1.99271469e-07
Iter: 1734 loss: 2.0099813e-07
Iter: 1735 loss: 1.99266694e-07
Iter: 1736 loss: 1.99151714e-07
Iter: 1737 loss: 1.9889508e-07
Iter: 1738 loss: 2.02438898e-07
Iter: 1739 loss: 1.98875398e-07
Iter: 1740 loss: 1.98679373e-07
Iter: 1741 loss: 2.00434627e-07
Iter: 1742 loss: 1.98662434e-07
Iter: 1743 loss: 1.98526465e-07
Iter: 1744 loss: 1.98535588e-07
Iter: 1745 loss: 1.98425454e-07
Iter: 1746 loss: 1.98294785e-07
Iter: 1747 loss: 1.98289143e-07
Iter: 1748 loss: 1.98115529e-07
Iter: 1749 loss: 1.98097354e-07
Iter: 1750 loss: 1.97963644e-07
Iter: 1751 loss: 1.97883381e-07
Iter: 1752 loss: 1.97851776e-07
Iter: 1753 loss: 1.97736782e-07
Iter: 1754 loss: 1.9769017e-07
Iter: 1755 loss: 1.97622398e-07
Iter: 1756 loss: 1.97531e-07
Iter: 1757 loss: 1.97723921e-07
Iter: 1758 loss: 1.97468751e-07
Iter: 1759 loss: 1.97333847e-07
Iter: 1760 loss: 1.97950953e-07
Iter: 1761 loss: 1.97302256e-07
Iter: 1762 loss: 1.97163175e-07
Iter: 1763 loss: 1.97143351e-07
Iter: 1764 loss: 1.97036783e-07
Iter: 1765 loss: 1.96874907e-07
Iter: 1766 loss: 1.97056124e-07
Iter: 1767 loss: 1.96780661e-07
Iter: 1768 loss: 1.96660466e-07
Iter: 1769 loss: 1.96649154e-07
Iter: 1770 loss: 1.9656305e-07
Iter: 1771 loss: 1.96455659e-07
Iter: 1772 loss: 1.96446763e-07
Iter: 1773 loss: 1.96279103e-07
Iter: 1774 loss: 1.9645276e-07
Iter: 1775 loss: 1.96176956e-07
Iter: 1776 loss: 1.9610934e-07
Iter: 1777 loss: 1.96087285e-07
Iter: 1778 loss: 1.95999348e-07
Iter: 1779 loss: 1.958431e-07
Iter: 1780 loss: 1.99065582e-07
Iter: 1781 loss: 1.9583868e-07
Iter: 1782 loss: 1.9564942e-07
Iter: 1783 loss: 1.96090724e-07
Iter: 1784 loss: 1.95582e-07
Iter: 1785 loss: 1.95485569e-07
Iter: 1786 loss: 1.95489861e-07
Iter: 1787 loss: 1.95390399e-07
Iter: 1788 loss: 1.95326066e-07
Iter: 1789 loss: 1.95304409e-07
Iter: 1790 loss: 1.95159387e-07
Iter: 1791 loss: 1.94978568e-07
Iter: 1792 loss: 1.94980203e-07
Iter: 1793 loss: 1.94882119e-07
Iter: 1794 loss: 1.94835664e-07
Iter: 1795 loss: 1.94755515e-07
Iter: 1796 loss: 1.94580025e-07
Iter: 1797 loss: 1.97131143e-07
Iter: 1798 loss: 1.94562944e-07
Iter: 1799 loss: 1.94382494e-07
Iter: 1800 loss: 1.95946882e-07
Iter: 1801 loss: 1.94379879e-07
Iter: 1802 loss: 1.94259314e-07
Iter: 1803 loss: 1.94963405e-07
Iter: 1804 loss: 1.94234673e-07
Iter: 1805 loss: 1.94097922e-07
Iter: 1806 loss: 1.93904384e-07
Iter: 1807 loss: 1.93910239e-07
Iter: 1808 loss: 1.93753408e-07
Iter: 1809 loss: 1.93757586e-07
Iter: 1810 loss: 1.93675561e-07
Iter: 1811 loss: 1.93953113e-07
Iter: 1812 loss: 1.93632786e-07
Iter: 1813 loss: 1.9350037e-07
Iter: 1814 loss: 1.93312161e-07
Iter: 1815 loss: 1.93303563e-07
Iter: 1816 loss: 1.93135051e-07
Iter: 1817 loss: 1.94534024e-07
Iter: 1818 loss: 1.93113905e-07
Iter: 1819 loss: 1.92943915e-07
Iter: 1820 loss: 1.94388122e-07
Iter: 1821 loss: 1.92935389e-07
Iter: 1822 loss: 1.92826207e-07
Iter: 1823 loss: 1.92681512e-07
Iter: 1824 loss: 1.92684325e-07
Iter: 1825 loss: 1.92540881e-07
Iter: 1826 loss: 1.93667432e-07
Iter: 1827 loss: 1.92511862e-07
Iter: 1828 loss: 1.92379488e-07
Iter: 1829 loss: 1.92964436e-07
Iter: 1830 loss: 1.92342753e-07
Iter: 1831 loss: 1.92243704e-07
Iter: 1832 loss: 1.92117426e-07
Iter: 1833 loss: 1.92101112e-07
Iter: 1834 loss: 1.91983219e-07
Iter: 1835 loss: 1.91992015e-07
Iter: 1836 loss: 1.91875856e-07
Iter: 1837 loss: 1.91919597e-07
Iter: 1838 loss: 1.9179322e-07
Iter: 1839 loss: 1.91655531e-07
Iter: 1840 loss: 1.91868793e-07
Iter: 1841 loss: 1.91598431e-07
Iter: 1842 loss: 1.91470434e-07
Iter: 1843 loss: 1.92493033e-07
Iter: 1844 loss: 1.91456081e-07
Iter: 1845 loss: 1.91319742e-07
Iter: 1846 loss: 1.91349585e-07
Iter: 1847 loss: 1.91211043e-07
Iter: 1848 loss: 1.9107523e-07
Iter: 1849 loss: 1.91047747e-07
Iter: 1850 loss: 1.90958104e-07
Iter: 1851 loss: 1.90852887e-07
Iter: 1852 loss: 1.90845213e-07
Iter: 1853 loss: 1.90735747e-07
Iter: 1854 loss: 1.90645011e-07
Iter: 1855 loss: 1.90621165e-07
Iter: 1856 loss: 1.90490326e-07
Iter: 1857 loss: 1.90560698e-07
Iter: 1858 loss: 1.90416415e-07
Iter: 1859 loss: 1.90337488e-07
Iter: 1860 loss: 1.9034124e-07
Iter: 1861 loss: 1.90261659e-07
Iter: 1862 loss: 1.90102924e-07
Iter: 1863 loss: 1.926756e-07
Iter: 1864 loss: 1.90097211e-07
Iter: 1865 loss: 1.89926311e-07
Iter: 1866 loss: 1.90418234e-07
Iter: 1867 loss: 1.89879245e-07
Iter: 1868 loss: 1.89749699e-07
Iter: 1869 loss: 1.89739296e-07
Iter: 1870 loss: 1.89641455e-07
Iter: 1871 loss: 1.89724105e-07
Iter: 1872 loss: 1.89586828e-07
Iter: 1873 loss: 1.89462071e-07
Iter: 1874 loss: 1.89358374e-07
Iter: 1875 loss: 1.89330976e-07
Iter: 1876 loss: 1.89172667e-07
Iter: 1877 loss: 1.89155088e-07
Iter: 1878 loss: 1.89076871e-07
Iter: 1879 loss: 1.88992061e-07
Iter: 1880 loss: 1.8897498e-07
Iter: 1881 loss: 1.88848475e-07
Iter: 1882 loss: 1.89302767e-07
Iter: 1883 loss: 1.88828636e-07
Iter: 1884 loss: 1.88706125e-07
Iter: 1885 loss: 1.9000251e-07
Iter: 1886 loss: 1.88701875e-07
Iter: 1887 loss: 1.88637472e-07
Iter: 1888 loss: 1.88460689e-07
Iter: 1889 loss: 1.90772539e-07
Iter: 1890 loss: 1.8845742e-07
Iter: 1891 loss: 1.8832165e-07
Iter: 1892 loss: 1.89594e-07
Iter: 1893 loss: 1.8831129e-07
Iter: 1894 loss: 1.88191549e-07
Iter: 1895 loss: 1.88616482e-07
Iter: 1896 loss: 1.88150281e-07
Iter: 1897 loss: 1.88030214e-07
Iter: 1898 loss: 1.87972631e-07
Iter: 1899 loss: 1.8791772e-07
Iter: 1900 loss: 1.87755944e-07
Iter: 1901 loss: 1.88066537e-07
Iter: 1902 loss: 1.87679575e-07
Iter: 1903 loss: 1.8758584e-07
Iter: 1904 loss: 1.87564012e-07
Iter: 1905 loss: 1.87505435e-07
Iter: 1906 loss: 1.87379285e-07
Iter: 1907 loss: 1.87369977e-07
Iter: 1908 loss: 1.87266579e-07
Iter: 1909 loss: 1.8727323e-07
Iter: 1910 loss: 1.87180973e-07
Iter: 1911 loss: 1.87286631e-07
Iter: 1912 loss: 1.87132969e-07
Iter: 1913 loss: 1.87040456e-07
Iter: 1914 loss: 1.86962041e-07
Iter: 1915 loss: 1.86947659e-07
Iter: 1916 loss: 1.86869102e-07
Iter: 1917 loss: 1.86867766e-07
Iter: 1918 loss: 1.86767437e-07
Iter: 1919 loss: 1.86729409e-07
Iter: 1920 loss: 1.86689832e-07
Iter: 1921 loss: 1.86561493e-07
Iter: 1922 loss: 1.86517951e-07
Iter: 1923 loss: 1.86444282e-07
Iter: 1924 loss: 1.86255903e-07
Iter: 1925 loss: 1.87722378e-07
Iter: 1926 loss: 1.8624543e-07
Iter: 1927 loss: 1.86058315e-07
Iter: 1928 loss: 1.86515067e-07
Iter: 1929 loss: 1.8600764e-07
Iter: 1930 loss: 1.85893384e-07
Iter: 1931 loss: 1.85717084e-07
Iter: 1932 loss: 1.8571356e-07
Iter: 1933 loss: 1.85553517e-07
Iter: 1934 loss: 1.85556388e-07
Iter: 1935 loss: 1.85458134e-07
Iter: 1936 loss: 1.86172073e-07
Iter: 1937 loss: 1.8544236e-07
Iter: 1938 loss: 1.85361174e-07
Iter: 1939 loss: 1.85303918e-07
Iter: 1940 loss: 1.85290887e-07
Iter: 1941 loss: 1.85210425e-07
Iter: 1942 loss: 1.85215143e-07
Iter: 1943 loss: 1.85143662e-07
Iter: 1944 loss: 1.85028085e-07
Iter: 1945 loss: 1.87094841e-07
Iter: 1946 loss: 1.85025868e-07
Iter: 1947 loss: 1.84882907e-07
Iter: 1948 loss: 1.85017257e-07
Iter: 1949 loss: 1.84822795e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6
+ date
Thu Oct 22 03:45:12 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/500_500_500_500_1 --function f1 --psi -2 --phi 1.6 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda22d69950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda22c78620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda22c8f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda22bfda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda22c11268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda22c11598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda22b138c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda22c11950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda22c11e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda22ba39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda22b8f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9c458dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9c4577598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9c4577950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9c454c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9c454cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda22aea0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9c44c1ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9c454c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9c4508e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9c45cd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9c45bfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd980174620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9800c59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd980100598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd980195048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9800b58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9800ad950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9800ad2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd98003a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd98013c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd94056c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd94056c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd94059e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd94056cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd9404eda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.022536168
test_loss: 0.020188717
train_loss: 0.008607357
test_loss: 0.008920155
train_loss: 0.005556027
test_loss: 0.006048907
train_loss: 0.0046453676
test_loss: 0.0050726365
train_loss: 0.0038049144
test_loss: 0.0045068944
train_loss: 0.003663865
test_loss: 0.0043573123
train_loss: 0.003531813
test_loss: 0.004266521
train_loss: 0.0034494367
test_loss: 0.0042774663
train_loss: 0.0033967388
test_loss: 0.0040552323
train_loss: 0.0033014582
test_loss: 0.0040268293
train_loss: 0.0031385107
test_loss: 0.0038320904
train_loss: 0.003225379
test_loss: 0.004086705
train_loss: 0.0035516915
test_loss: 0.003960309
train_loss: 0.003337454
test_loss: 0.0039046218
train_loss: 0.0032915017
test_loss: 0.0039223577
train_loss: 0.0033055034
test_loss: 0.003971643
train_loss: 0.0030857765
test_loss: 0.0038206505
train_loss: 0.0032213423
test_loss: 0.003939543
train_loss: 0.003128531
test_loss: 0.0039271237
train_loss: 0.0031181898
test_loss: 0.0038169525
train_loss: 0.0031992123
test_loss: 0.0036801416
train_loss: 0.0030279038
test_loss: 0.003929214
train_loss: 0.003025571
test_loss: 0.0036728648
train_loss: 0.0029078424
test_loss: 0.0036869259
train_loss: 0.0031754593
test_loss: 0.0037922086
train_loss: 0.0030359963
test_loss: 0.0038299407
train_loss: 0.0028963345
test_loss: 0.0036593282
train_loss: 0.002939141
test_loss: 0.0034804926
train_loss: 0.0027996614
test_loss: 0.0035621868
train_loss: 0.003370638
test_loss: 0.003576221
train_loss: 0.00315492
test_loss: 0.0035089564
train_loss: 0.0030098595
test_loss: 0.003742603
train_loss: 0.0030151773
test_loss: 0.003766181
train_loss: 0.0033788153
test_loss: 0.0036365686
train_loss: 0.002643602
test_loss: 0.003543795
train_loss: 0.002986411
test_loss: 0.003565641
train_loss: 0.0029300211
test_loss: 0.0034417233
train_loss: 0.0025928523
test_loss: 0.0034443582
train_loss: 0.0026018831
test_loss: 0.0037193876
train_loss: 0.002673992
test_loss: 0.0035247335
train_loss: 0.0028841468
test_loss: 0.0035578273
train_loss: 0.002782717
test_loss: 0.003529536
train_loss: 0.0028325056
test_loss: 0.0034694371
train_loss: 0.0026673041
test_loss: 0.0036277506
train_loss: 0.0026905625
test_loss: 0.003771404
train_loss: 0.002960639
test_loss: 0.0036914337
train_loss: 0.0028272998
test_loss: 0.0035536357
train_loss: 0.0033733482
test_loss: 0.003705219
train_loss: 0.0031999177
test_loss: 0.0035521188
train_loss: 0.003104702
test_loss: 0.003844439
train_loss: 0.0029297094
test_loss: 0.003657142
train_loss: 0.0033623695
test_loss: 0.003608987
train_loss: 0.0029283143
test_loss: 0.003420782
train_loss: 0.0030822353
test_loss: 0.0035656346
train_loss: 0.0026889993
test_loss: 0.0035306402
train_loss: 0.002782542
test_loss: 0.0035679275
train_loss: 0.0027846596
test_loss: 0.0036765912
train_loss: 0.0026462413
test_loss: 0.0034714031
train_loss: 0.0027167082
test_loss: 0.0035345624
train_loss: 0.0025477498
test_loss: 0.0034158558
train_loss: 0.0028246592
test_loss: 0.0035017962
train_loss: 0.0028183444
test_loss: 0.003720845
train_loss: 0.0027801518
test_loss: 0.0038184542
train_loss: 0.002628502
test_loss: 0.0035056423
train_loss: 0.0026874882
test_loss: 0.0036909757
train_loss: 0.00271791
test_loss: 0.003508501
train_loss: 0.0025911578
test_loss: 0.0037284507
train_loss: 0.0024862685
test_loss: 0.0034115773
train_loss: 0.0026579078
test_loss: 0.0034515855
train_loss: 0.0031958497
test_loss: 0.0037206346
train_loss: 0.0030190074
test_loss: 0.0036035269
train_loss: 0.0029764557
test_loss: 0.0034756197
train_loss: 0.0029738545
test_loss: 0.003505627
train_loss: 0.0027207846
test_loss: 0.0036131553
train_loss: 0.0031271935
test_loss: 0.0034579367
train_loss: 0.0027081955
test_loss: 0.0034353654
train_loss: 0.0027486717
test_loss: 0.0035707785
train_loss: 0.0027368483
test_loss: 0.0035038174
train_loss: 0.0028102447
test_loss: 0.0036845913
train_loss: 0.0029134508
test_loss: 0.003482622
train_loss: 0.0028390214
test_loss: 0.0035501649
train_loss: 0.0026635528
test_loss: 0.0035509793
train_loss: 0.0027116588
test_loss: 0.0037939737
train_loss: 0.002819445
test_loss: 0.0034323293
train_loss: 0.002829331
test_loss: 0.0034115198
train_loss: 0.0024893146
test_loss: 0.0035513246
train_loss: 0.0028928863
test_loss: 0.003572018
train_loss: 0.0029807934
test_loss: 0.0034065244
train_loss: 0.002592606
test_loss: 0.0034751028
train_loss: 0.002703908
test_loss: 0.0033476246
train_loss: 0.0029064277
test_loss: 0.0037074182
train_loss: 0.0028190797
test_loss: 0.0036422173
train_loss: 0.0029318691
test_loss: 0.0034759068
train_loss: 0.0024537924
test_loss: 0.0035559682
train_loss: 0.0028300516
test_loss: 0.0038033873
train_loss: 0.0025667108
test_loss: 0.0034423876
train_loss: 0.0028653138
test_loss: 0.003414508
train_loss: 0.0028462545
test_loss: 0.0034833604
train_loss: 0.0027381976
test_loss: 0.00391327
train_loss: 0.0028161563
test_loss: 0.003492845
train_loss: 0.0026526838
test_loss: 0.0035315717
train_loss: 0.0024385187
test_loss: 0.0033291124
train_loss: 0.002718475
test_loss: 0.0035517823
train_loss: 0.0026183894
test_loss: 0.0035642495
train_loss: 0.0027015617
test_loss: 0.0034863334
train_loss: 0.002736189
test_loss: 0.0034902953
train_loss: 0.0028390442
test_loss: 0.0036364351
train_loss: 0.003112446
test_loss: 0.0035363776
train_loss: 0.00280605
test_loss: 0.0039428817
train_loss: 0.0031068805
test_loss: 0.0037488365
train_loss: 0.0025825188
test_loss: 0.003553104
train_loss: 0.0029366612
test_loss: 0.0035454165
train_loss: 0.0027456991
test_loss: 0.0034357586
train_loss: 0.0026409347
test_loss: 0.0035699655
train_loss: 0.002871934
test_loss: 0.0036455442
train_loss: 0.0026985297
test_loss: 0.0033902256
train_loss: 0.0025918556
test_loss: 0.0034104544
train_loss: 0.002534823
test_loss: 0.0033468348
train_loss: 0.002773523
test_loss: 0.0034698087
train_loss: 0.0029691015
test_loss: 0.0034981712
train_loss: 0.0026907907
test_loss: 0.0034395866
train_loss: 0.002665292
test_loss: 0.0035022562
train_loss: 0.0030594608
test_loss: 0.0035599647
train_loss: 0.0030643148
test_loss: 0.0038626485
train_loss: 0.0030122027
test_loss: 0.0035186505
train_loss: 0.0027881763
test_loss: 0.0033811873
train_loss: 0.003027311
test_loss: 0.0037202053
train_loss: 0.0026685372
test_loss: 0.0036683178
train_loss: 0.002818135
test_loss: 0.0035178354
train_loss: 0.0027261046
test_loss: 0.0035671045
train_loss: 0.0025162378
test_loss: 0.0035146626
train_loss: 0.0029440005
test_loss: 0.0035412698
train_loss: 0.0026059118
test_loss: 0.003453687
train_loss: 0.0026695365
test_loss: 0.0035565547
train_loss: 0.0025126894
test_loss: 0.0036714666
train_loss: 0.0030254773
test_loss: 0.0035789148
train_loss: 0.0028186333
test_loss: 0.0037906668
train_loss: 0.003224004
test_loss: 0.0036608293
train_loss: 0.0028731558
test_loss: 0.0036092717
train_loss: 0.0029658992
test_loss: 0.0037778863
train_loss: 0.0026866677
test_loss: 0.0034454516
train_loss: 0.0027871393
test_loss: 0.0034861711
train_loss: 0.0027545237
test_loss: 0.0037457587
train_loss: 0.0026052562
test_loss: 0.0035713378
train_loss: 0.0029234565
test_loss: 0.0036107886
train_loss: 0.002603445
test_loss: 0.0035005084
train_loss: 0.0027119264
test_loss: 0.0037887038
train_loss: 0.003519
test_loss: 0.003580474
train_loss: 0.0030985982
test_loss: 0.0035897603
train_loss: 0.0026614342
test_loss: 0.003601167
train_loss: 0.0029585985
test_loss: 0.0034464686
train_loss: 0.0028461833
test_loss: 0.003326914
train_loss: 0.0026909804
test_loss: 0.0034448695
train_loss: 0.0023804512
test_loss: 0.0036715963
train_loss: 0.002614486
test_loss: 0.003347661
train_loss: 0.0026497697
test_loss: 0.003405825
train_loss: 0.0024638795
test_loss: 0.003436718
train_loss: 0.0024710249
test_loss: 0.003476077
train_loss: 0.0026375158
test_loss: 0.003626683
train_loss: 0.0028314018
test_loss: 0.0034892156
train_loss: 0.0025835794
test_loss: 0.003481856
train_loss: 0.0026625516
test_loss: 0.0035686505
train_loss: 0.002609992
test_loss: 0.0035763115
train_loss: 0.0024718726
test_loss: 0.0034247676
train_loss: 0.0026975123
test_loss: 0.0036019345
train_loss: 0.0024467446
test_loss: 0.0033646182
train_loss: 0.0025989758
test_loss: 0.003534103
train_loss: 0.002646247
test_loss: 0.0034956548
train_loss: 0.002595504
test_loss: 0.003559917
train_loss: 0.0026664636
test_loss: 0.0035054707
train_loss: 0.0030708504
test_loss: 0.0036757204
train_loss: 0.002645119
test_loss: 0.003392006
train_loss: 0.0026203566
test_loss: 0.0034181888
train_loss: 0.003191879
test_loss: 0.003827252
train_loss: 0.0029434098
test_loss: 0.00342978
train_loss: 0.0029331003
test_loss: 0.0034749648
train_loss: 0.0027320147
test_loss: 0.0035054148
train_loss: 0.0026910119
test_loss: 0.0034483583
train_loss: 0.002781039
test_loss: 0.0033854332
train_loss: 0.0029904114
test_loss: 0.003603642
train_loss: 0.0024710242
test_loss: 0.0033839613
train_loss: 0.002826819
test_loss: 0.0035900145
train_loss: 0.0025364356
test_loss: 0.0033960287
train_loss: 0.0027211914
test_loss: 0.0038697335
train_loss: 0.0029897068
test_loss: 0.0036275035
train_loss: 0.00264464
test_loss: 0.0035010148
train_loss: 0.0023857434
test_loss: 0.003871087
train_loss: 0.0026639334
test_loss: 0.0033999213
train_loss: 0.0026156325
test_loss: 0.0034819394
train_loss: 0.0029619692
test_loss: 0.0037195492
train_loss: 0.002707467
test_loss: 0.0036154264
train_loss: 0.0029271538
test_loss: 0.0036090822
train_loss: 0.003050596
test_loss: 0.0037724718
train_loss: 0.002807309
test_loss: 0.003408521
train_loss: 0.0028094111
test_loss: 0.003529315
train_loss: 0.0025972943
test_loss: 0.0035520191
train_loss: 0.0024494044
test_loss: 0.0033347092
train_loss: 0.0028729483
test_loss: 0.003621608
train_loss: 0.0027271449
test_loss: 0.0033567392
train_loss: 0.0023318105
test_loss: 0.0033054543
train_loss: 0.0027601172
test_loss: 0.0035753835
train_loss: 0.0025491703
test_loss: 0.0034699459
train_loss: 0.002592003
test_loss: 0.0036546001
train_loss: 0.0024139306
test_loss: 0.0035303761
train_loss: 0.0027813483
test_loss: 0.0034963738
train_loss: 0.002621643
test_loss: 0.0033943986
train_loss: 0.0028469395
test_loss: 0.0036491703
train_loss: 0.0024894832
test_loss: 0.003518912
train_loss: 0.0022701337
test_loss: 0.0033522006
train_loss: 0.002677301
test_loss: 0.0033870088
train_loss: 0.0025212565
test_loss: 0.0036222253
train_loss: 0.0024894318
test_loss: 0.0032553645
train_loss: 0.002364812
test_loss: 0.0034481667
train_loss: 0.002517251
test_loss: 0.0033825478
train_loss: 0.002509727
test_loss: 0.003352481
train_loss: 0.0026842398
test_loss: 0.003570829
train_loss: 0.002731508
test_loss: 0.003446254
train_loss: 0.0026171212
test_loss: 0.0034304832
train_loss: 0.0026836682
test_loss: 0.0034135222
train_loss: 0.0025275091
test_loss: 0.003268987
train_loss: 0.0028526129
test_loss: 0.00355559
train_loss: 0.0028225407
test_loss: 0.0034470507
train_loss: 0.0029089898
test_loss: 0.0034767925
train_loss: 0.0029974333
test_loss: 0.003579598
train_loss: 0.002524162
test_loss: 0.0034128362
train_loss: 0.0026094564
test_loss: 0.0033998266
train_loss: 0.0027048711
test_loss: 0.0036012752
train_loss: 0.0027188342
test_loss: 0.0034611388
train_loss: 0.0026233832
test_loss: 0.0033117922
train_loss: 0.002642762
test_loss: 0.0033795112
train_loss: 0.0025967928
test_loss: 0.0033901
train_loss: 0.002416423
test_loss: 0.003284825
train_loss: 0.002779041
test_loss: 0.0035282113
train_loss: 0.002535702
test_loss: 0.003481943
train_loss: 0.0026813622
test_loss: 0.0034342278
train_loss: 0.0025568516
test_loss: 0.0036197212
train_loss: 0.0023611276
test_loss: 0.0033016047
train_loss: 0.0029750986
test_loss: 0.0035139367
train_loss: 0.0029643963
test_loss: 0.0035095722
train_loss: 0.0026897322
test_loss: 0.0036712997
train_loss: 0.0026327826
test_loss: 0.0034555723
train_loss: 0.002771787
test_loss: 0.0038446172
train_loss: 0.0028118892
test_loss: 0.0035238632
train_loss: 0.0025918186
test_loss: 0.0034578443
train_loss: 0.002474176
test_loss: 0.003455924
train_loss: 0.0024956083
test_loss: 0.0033234532
train_loss: 0.002504871
test_loss: 0.0033848367
train_loss: 0.0027778991
test_loss: 0.0034998094
train_loss: 0.0025721584
test_loss: 0.0034154437
train_loss: 0.0024944204
test_loss: 0.0034464626
train_loss: 0.002629243
test_loss: 0.0033472606
train_loss: 0.0025276847
test_loss: 0.003305476
train_loss: 0.0024992817
test_loss: 0.003348972
train_loss: 0.002602114
test_loss: 0.003394268
train_loss: 0.002327875
test_loss: 0.0037798688
train_loss: 0.0027511257
test_loss: 0.0033523603
train_loss: 0.0027307626
test_loss: 0.0032988011
train_loss: 0.002693891
test_loss: 0.0034500412
train_loss: 0.0025423374
test_loss: 0.0034764002
train_loss: 0.0024042288
test_loss: 0.0033191498
train_loss: 0.0025789523
test_loss: 0.0033897136
train_loss: 0.0027635
test_loss: 0.0036227305
train_loss: 0.0026767035
test_loss: 0.0033341923
train_loss: 0.002633948
test_loss: 0.0034081605
train_loss: 0.002397557
test_loss: 0.003309645
train_loss: 0.0025638673
test_loss: 0.003513704
train_loss: 0.0025374559
test_loss: 0.0032641897
train_loss: 0.0026276466
test_loss: 0.0033945378
train_loss: 0.002898164
test_loss: 0.0034520247
train_loss: 0.002632795
test_loss: 0.0035279884
train_loss: 0.0026065703
test_loss: 0.0033992147
train_loss: 0.0024019354
test_loss: 0.0035421127
train_loss: 0.0025530912
test_loss: 0.0035055636
train_loss: 0.0024054488
test_loss: 0.0033462627
train_loss: 0.002591114
test_loss: 0.003421887
train_loss: 0.0024075469
test_loss: 0.0034553672
train_loss: 0.0024810296
test_loss: 0.0033797752
train_loss: 0.0027102274
test_loss: 0.0033126024
train_loss: 0.0025462843
test_loss: 0.0036668517
train_loss: 0.0026088126
test_loss: 0.0033401558
train_loss: 0.0026245946
test_loss: 0.003350818
train_loss: 0.0027196393
test_loss: 0.0035184207
train_loss: 0.002638742
test_loss: 0.0036761472
train_loss: 0.0024390656
test_loss: 0.0037135887
train_loss: 0.0025641096
test_loss: 0.0034233637
train_loss: 0.002401942
test_loss: 0.0031930401
train_loss: 0.0023390404
test_loss: 0.0033887757
train_loss: 0.002569308
test_loss: 0.0036553876
train_loss: 0.0027865546
test_loss: 0.0034541932
train_loss: 0.002794213
test_loss: 0.0034374213
train_loss: 0.0024031377
test_loss: 0.003223508
train_loss: 0.0025190671
test_loss: 0.0033424043
train_loss: 0.0028914714
test_loss: 0.0034024718
train_loss: 0.0024382072
test_loss: 0.0033861077
train_loss: 0.0026077991
test_loss: 0.003427726
train_loss: 0.0026570763
test_loss: 0.0034638618
train_loss: 0.0026678464
test_loss: 0.0036723004
train_loss: 0.0025668773
test_loss: 0.0034502777
train_loss: 0.0026034685
test_loss: 0.0035096146
train_loss: 0.0026563494
test_loss: 0.0034837376
train_loss: 0.0025132287
test_loss: 0.0035987396
train_loss: 0.0027052397
test_loss: 0.0034792537
train_loss: 0.0026692583
test_loss: 0.003611886
train_loss: 0.0024187616
test_loss: 0.0033519098
train_loss: 0.0025183738
test_loss: 0.0034823264
train_loss: 0.002466646
test_loss: 0.0037241713
train_loss: 0.002438329
test_loss: 0.0034201827
train_loss: 0.0025078524
test_loss: 0.003286155
train_loss: 0.0023793643
test_loss: 0.0034776912
train_loss: 0.0024573181
test_loss: 0.0033852158
train_loss: 0.003083658
test_loss: 0.0036080175
train_loss: 0.0025806283
test_loss: 0.0034889814
train_loss: 0.0026715675
test_loss: 0.003520365
train_loss: 0.002700185
test_loss: 0.0037945649
train_loss: 0.0024992719
test_loss: 0.0035165723
train_loss: 0.002531214
test_loss: 0.003542212
train_loss: 0.0027375715
test_loss: 0.0035193232
train_loss: 0.002858833
test_loss: 0.0034848228
train_loss: 0.0025705644
test_loss: 0.0035522846
train_loss: 0.0026470362
test_loss: 0.003497216
train_loss: 0.0024303691
test_loss: 0.0032961215
train_loss: 0.002588131
test_loss: 0.0033651046
train_loss: 0.002373157
test_loss: 0.0034272752
train_loss: 0.002714248
test_loss: 0.0034594976
train_loss: 0.0028082752
test_loss: 0.0036893943
train_loss: 0.0029086228
test_loss: 0.0038123077
train_loss: 0.0025536404
test_loss: 0.003400835
train_loss: 0.002700836
test_loss: 0.0035191004
train_loss: 0.0024616122
test_loss: 0.0035301265
train_loss: 0.0025275033
test_loss: 0.003571287
train_loss: 0.0024330223
test_loss: 0.0032548257
train_loss: 0.0031332274
test_loss: 0.0035242252
train_loss: 0.002930615
test_loss: 0.0037407416
train_loss: 0.0026184365
test_loss: 0.0033537433
train_loss: 0.002592187
test_loss: 0.0033458401
train_loss: 0.002366666
test_loss: 0.00373348
train_loss: 0.0027992015
test_loss: 0.0033565313
train_loss: 0.0025321138
test_loss: 0.003428325
train_loss: 0.0028159157/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0036461574
train_loss: 0.002686096
test_loss: 0.0035959324
train_loss: 0.0026608359
test_loss: 0.0035881605
train_loss: 0.0024942947
test_loss: 0.0034304874
train_loss: 0.0026503527
test_loss: 0.0034785513
train_loss: 0.0024759532
test_loss: 0.0033151682
train_loss: 0.0023324024
test_loss: 0.003373337
train_loss: 0.002452551
test_loss: 0.004077508
train_loss: 0.0027125347
test_loss: 0.0035496063
train_loss: 0.0023739238
test_loss: 0.0033087162
train_loss: 0.0024759683
test_loss: 0.0033442301
train_loss: 0.0024453811
test_loss: 0.0034256098
train_loss: 0.0024264103
test_loss: 0.0033935856
train_loss: 0.002410117
test_loss: 0.0033981262
train_loss: 0.0024831716
test_loss: 0.0034080625
train_loss: 0.002433164
test_loss: 0.003394986
train_loss: 0.002638868
test_loss: 0.0034099298
train_loss: 0.0022289576
test_loss: 0.0033513152
train_loss: 0.002765156
test_loss: 0.00360308
train_loss: 0.0025893508
test_loss: 0.0035158885
train_loss: 0.0026234065
test_loss: 0.0037547678
train_loss: 0.0026858256
test_loss: 0.0034278187
train_loss: 0.0026768208
test_loss: 0.0033930198
train_loss: 0.0025415744
test_loss: 0.0038580417
train_loss: 0.0032272309
test_loss: 0.0035293647
train_loss: 0.0027921358
test_loss: 0.00338445
train_loss: 0.0025638237
test_loss: 0.0034971416
train_loss: 0.0025297874
test_loss: 0.003627785
train_loss: 0.0024267235
test_loss: 0.0034230042
train_loss: 0.0023997612
test_loss: 0.0032227114
train_loss: 0.0023437277
test_loss: 0.0032574139
train_loss: 0.0023011821
test_loss: 0.0033535587
train_loss: 0.0025195659
test_loss: 0.003404166
train_loss: 0.0025254951
test_loss: 0.0033917087
train_loss: 0.0026552305
test_loss: 0.003398977
train_loss: 0.0026021025
test_loss: 0.0035610064
train_loss: 0.0026887746
test_loss: 0.0034223446
train_loss: 0.002659237
test_loss: 0.003539803
train_loss: 0.0023544938
test_loss: 0.0033247373
train_loss: 0.0025704738
test_loss: 0.0034269367
train_loss: 0.0028321147
test_loss: 0.0036597308
train_loss: 0.002617117
test_loss: 0.003365711
train_loss: 0.0024011293
test_loss: 0.003394814
train_loss: 0.0024266236
test_loss: 0.0033848104
train_loss: 0.0024774473
test_loss: 0.0033654987
train_loss: 0.002603794
test_loss: 0.0035603966
train_loss: 0.0024845018
test_loss: 0.003294893
train_loss: 0.0026519636
test_loss: 0.0033765787
train_loss: 0.0025736336
test_loss: 0.003503394
train_loss: 0.0025292109
test_loss: 0.003268756
train_loss: 0.0023894813
test_loss: 0.0034450071
train_loss: 0.0024698465
test_loss: 0.0032269333
train_loss: 0.0024754584
test_loss: 0.0034099307
train_loss: 0.0027325589
test_loss: 0.0033179666
train_loss: 0.0027923235
test_loss: 0.0035453893
train_loss: 0.0024571756
test_loss: 0.0032747113
train_loss: 0.0023084641
test_loss: 0.0033984
train_loss: 0.0024575244
test_loss: 0.0033108676
train_loss: 0.0024776575
test_loss: 0.003329554
train_loss: 0.002500476
test_loss: 0.0033683327
train_loss: 0.002696567
test_loss: 0.0035447595
train_loss: 0.0025865997
test_loss: 0.0035037606
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.6 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf80b6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf81806a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf81b5d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf8080620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf80836a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf8083ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf80658c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf801b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf801b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf7fd6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf7fd6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf7fd6620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf7f9b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf7f9b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf7efb0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf7eaf6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf7eac378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf7edad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf7ea8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf7ea82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf7e32620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcaf7e3ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca99953840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca9997a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca9997a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca9992ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca9992f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca99888620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca99888158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca998a8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca99863950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca741041e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca74104378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca74139488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca740e19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fca74095510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.20937184e-05
Iter: 2 loss: 1.12481048e-05
Iter: 3 loss: 9.9989611e-06
Iter: 4 loss: 9.46807904e-06
Iter: 5 loss: 1.0120144e-05
Iter: 6 loss: 9.19111153e-06
Iter: 7 loss: 8.68771167e-06
Iter: 8 loss: 8.59127067e-06
Iter: 9 loss: 8.25433108e-06
Iter: 10 loss: 7.59876275e-06
Iter: 11 loss: 9.16474164e-06
Iter: 12 loss: 7.36057063e-06
Iter: 13 loss: 7.01192539e-06
Iter: 14 loss: 8.74811212e-06
Iter: 15 loss: 6.95326753e-06
Iter: 16 loss: 6.67691847e-06
Iter: 17 loss: 9.14496377e-06
Iter: 18 loss: 6.66349297e-06
Iter: 19 loss: 6.452452e-06
Iter: 20 loss: 6.20885476e-06
Iter: 21 loss: 6.17926344e-06
Iter: 22 loss: 5.69784424e-06
Iter: 23 loss: 8.38751112e-06
Iter: 24 loss: 5.62974219e-06
Iter: 25 loss: 5.36502785e-06
Iter: 26 loss: 5.3866479e-06
Iter: 27 loss: 5.15932834e-06
Iter: 28 loss: 4.82395626e-06
Iter: 29 loss: 5.44895056e-06
Iter: 30 loss: 4.68049348e-06
Iter: 31 loss: 4.42025384e-06
Iter: 32 loss: 4.73915406e-06
Iter: 33 loss: 4.28409567e-06
Iter: 34 loss: 4.07475727e-06
Iter: 35 loss: 7.08142943e-06
Iter: 36 loss: 4.07448852e-06
Iter: 37 loss: 3.95311281e-06
Iter: 38 loss: 4.44429088e-06
Iter: 39 loss: 3.92623224e-06
Iter: 40 loss: 3.85710064e-06
Iter: 41 loss: 3.84943178e-06
Iter: 42 loss: 3.80546089e-06
Iter: 43 loss: 3.68514247e-06
Iter: 44 loss: 4.40690292e-06
Iter: 45 loss: 3.65226356e-06
Iter: 46 loss: 3.53652877e-06
Iter: 47 loss: 3.53498672e-06
Iter: 48 loss: 3.47089713e-06
Iter: 49 loss: 3.4423731e-06
Iter: 50 loss: 3.4099503e-06
Iter: 51 loss: 3.29815202e-06
Iter: 52 loss: 3.35492632e-06
Iter: 53 loss: 3.2237956e-06
Iter: 54 loss: 3.12044585e-06
Iter: 55 loss: 3.11857184e-06
Iter: 56 loss: 3.06501056e-06
Iter: 57 loss: 3.0383294e-06
Iter: 58 loss: 3.01289128e-06
Iter: 59 loss: 2.91526339e-06
Iter: 60 loss: 3.34529432e-06
Iter: 61 loss: 2.89568879e-06
Iter: 62 loss: 2.84015937e-06
Iter: 63 loss: 2.88072374e-06
Iter: 64 loss: 2.80597897e-06
Iter: 65 loss: 2.7265844e-06
Iter: 66 loss: 2.67392033e-06
Iter: 67 loss: 2.64405821e-06
Iter: 68 loss: 2.55493342e-06
Iter: 69 loss: 2.81835651e-06
Iter: 70 loss: 2.52773248e-06
Iter: 71 loss: 2.44098237e-06
Iter: 72 loss: 2.9639516e-06
Iter: 73 loss: 2.43030172e-06
Iter: 74 loss: 2.35027574e-06
Iter: 75 loss: 2.56955127e-06
Iter: 76 loss: 2.32427124e-06
Iter: 77 loss: 2.28945055e-06
Iter: 78 loss: 2.83608e-06
Iter: 79 loss: 2.28946101e-06
Iter: 80 loss: 2.24090468e-06
Iter: 81 loss: 2.19325739e-06
Iter: 82 loss: 2.18285822e-06
Iter: 83 loss: 2.13075441e-06
Iter: 84 loss: 2.3065254e-06
Iter: 85 loss: 2.11674455e-06
Iter: 86 loss: 2.07942549e-06
Iter: 87 loss: 2.11364477e-06
Iter: 88 loss: 2.05784613e-06
Iter: 89 loss: 2.0120583e-06
Iter: 90 loss: 2.47055664e-06
Iter: 91 loss: 2.01054263e-06
Iter: 92 loss: 1.98152702e-06
Iter: 93 loss: 1.94122686e-06
Iter: 94 loss: 1.93941446e-06
Iter: 95 loss: 1.91216759e-06
Iter: 96 loss: 1.91054164e-06
Iter: 97 loss: 1.88559306e-06
Iter: 98 loss: 1.9101326e-06
Iter: 99 loss: 1.87155229e-06
Iter: 100 loss: 1.84187411e-06
Iter: 101 loss: 1.9170252e-06
Iter: 102 loss: 1.8317603e-06
Iter: 103 loss: 1.80632492e-06
Iter: 104 loss: 1.96405153e-06
Iter: 105 loss: 1.8034267e-06
Iter: 106 loss: 1.78429855e-06
Iter: 107 loss: 1.74204956e-06
Iter: 108 loss: 2.35248149e-06
Iter: 109 loss: 1.74009563e-06
Iter: 110 loss: 1.70232715e-06
Iter: 111 loss: 1.96291876e-06
Iter: 112 loss: 1.69879752e-06
Iter: 113 loss: 1.66505333e-06
Iter: 114 loss: 1.73728608e-06
Iter: 115 loss: 1.65181063e-06
Iter: 116 loss: 1.63917798e-06
Iter: 117 loss: 1.63339985e-06
Iter: 118 loss: 1.61576281e-06
Iter: 119 loss: 1.61010746e-06
Iter: 120 loss: 1.59983676e-06
Iter: 121 loss: 1.5863618e-06
Iter: 122 loss: 1.56982901e-06
Iter: 123 loss: 1.56836074e-06
Iter: 124 loss: 1.53849862e-06
Iter: 125 loss: 1.63809295e-06
Iter: 126 loss: 1.53030419e-06
Iter: 127 loss: 1.51477957e-06
Iter: 128 loss: 1.51484619e-06
Iter: 129 loss: 1.49901075e-06
Iter: 130 loss: 1.46664638e-06
Iter: 131 loss: 2.04432058e-06
Iter: 132 loss: 1.46605191e-06
Iter: 133 loss: 1.44664864e-06
Iter: 134 loss: 1.44629496e-06
Iter: 135 loss: 1.42875751e-06
Iter: 136 loss: 1.48174922e-06
Iter: 137 loss: 1.42361898e-06
Iter: 138 loss: 1.408549e-06
Iter: 139 loss: 1.42732779e-06
Iter: 140 loss: 1.40081647e-06
Iter: 141 loss: 1.38368443e-06
Iter: 142 loss: 1.43935381e-06
Iter: 143 loss: 1.37895677e-06
Iter: 144 loss: 1.36171582e-06
Iter: 145 loss: 1.38129406e-06
Iter: 146 loss: 1.35226765e-06
Iter: 147 loss: 1.33756771e-06
Iter: 148 loss: 1.31282388e-06
Iter: 149 loss: 1.31273714e-06
Iter: 150 loss: 1.30216426e-06
Iter: 151 loss: 1.29703608e-06
Iter: 152 loss: 1.2880372e-06
Iter: 153 loss: 1.43182206e-06
Iter: 154 loss: 1.28800821e-06
Iter: 155 loss: 1.28070303e-06
Iter: 156 loss: 1.26127077e-06
Iter: 157 loss: 1.40119641e-06
Iter: 158 loss: 1.25704878e-06
Iter: 159 loss: 1.24068526e-06
Iter: 160 loss: 1.27156272e-06
Iter: 161 loss: 1.23372411e-06
Iter: 162 loss: 1.21493827e-06
Iter: 163 loss: 1.36970334e-06
Iter: 164 loss: 1.21378184e-06
Iter: 165 loss: 1.20245909e-06
Iter: 166 loss: 1.30702585e-06
Iter: 167 loss: 1.20190589e-06
Iter: 168 loss: 1.19143442e-06
Iter: 169 loss: 1.18251933e-06
Iter: 170 loss: 1.17965919e-06
Iter: 171 loss: 1.17228092e-06
Iter: 172 loss: 1.17203217e-06
Iter: 173 loss: 1.16449166e-06
Iter: 174 loss: 1.15787986e-06
Iter: 175 loss: 1.15589467e-06
Iter: 176 loss: 1.14529155e-06
Iter: 177 loss: 1.15894727e-06
Iter: 178 loss: 1.13988415e-06
Iter: 179 loss: 1.12668749e-06
Iter: 180 loss: 1.21402559e-06
Iter: 181 loss: 1.12538203e-06
Iter: 182 loss: 1.11562576e-06
Iter: 183 loss: 1.12770022e-06
Iter: 184 loss: 1.11054317e-06
Iter: 185 loss: 1.10215069e-06
Iter: 186 loss: 1.0952283e-06
Iter: 187 loss: 1.09280927e-06
Iter: 188 loss: 1.08809547e-06
Iter: 189 loss: 1.08532595e-06
Iter: 190 loss: 1.07825076e-06
Iter: 191 loss: 1.08281847e-06
Iter: 192 loss: 1.07366566e-06
Iter: 193 loss: 1.06751133e-06
Iter: 194 loss: 1.06278935e-06
Iter: 195 loss: 1.06087589e-06
Iter: 196 loss: 1.05085383e-06
Iter: 197 loss: 1.0400189e-06
Iter: 198 loss: 1.03833509e-06
Iter: 199 loss: 1.02691979e-06
Iter: 200 loss: 1.02692434e-06
Iter: 201 loss: 1.01654553e-06
Iter: 202 loss: 1.05204651e-06
Iter: 203 loss: 1.01378282e-06
Iter: 204 loss: 1.00289674e-06
Iter: 205 loss: 1.04309891e-06
Iter: 206 loss: 1.00022567e-06
Iter: 207 loss: 9.94286665e-07
Iter: 208 loss: 1.02519425e-06
Iter: 209 loss: 9.93223352e-07
Iter: 210 loss: 9.85496058e-07
Iter: 211 loss: 9.77467835e-07
Iter: 212 loss: 9.76027195e-07
Iter: 213 loss: 9.69847179e-07
Iter: 214 loss: 1.00509703e-06
Iter: 215 loss: 9.69024541e-07
Iter: 216 loss: 9.62163199e-07
Iter: 217 loss: 9.7485e-07
Iter: 218 loss: 9.59313866e-07
Iter: 219 loss: 9.52060645e-07
Iter: 220 loss: 9.72737098e-07
Iter: 221 loss: 9.49824425e-07
Iter: 222 loss: 9.43784528e-07
Iter: 223 loss: 9.49022251e-07
Iter: 224 loss: 9.40306677e-07
Iter: 225 loss: 9.35170192e-07
Iter: 226 loss: 9.34714819e-07
Iter: 227 loss: 9.31805289e-07
Iter: 228 loss: 9.22612912e-07
Iter: 229 loss: 9.40389e-07
Iter: 230 loss: 9.16769864e-07
Iter: 231 loss: 9.08682e-07
Iter: 232 loss: 9.08391826e-07
Iter: 233 loss: 9.03171156e-07
Iter: 234 loss: 8.94620712e-07
Iter: 235 loss: 8.94587856e-07
Iter: 236 loss: 8.86242447e-07
Iter: 237 loss: 1.00385455e-06
Iter: 238 loss: 8.86231589e-07
Iter: 239 loss: 8.80377911e-07
Iter: 240 loss: 9.0640026e-07
Iter: 241 loss: 8.7924343e-07
Iter: 242 loss: 8.72754811e-07
Iter: 243 loss: 9.0127719e-07
Iter: 244 loss: 8.71398697e-07
Iter: 245 loss: 8.67491906e-07
Iter: 246 loss: 8.77534831e-07
Iter: 247 loss: 8.66138237e-07
Iter: 248 loss: 8.61434728e-07
Iter: 249 loss: 8.7451366e-07
Iter: 250 loss: 8.59928889e-07
Iter: 251 loss: 8.55995097e-07
Iter: 252 loss: 8.5015995e-07
Iter: 253 loss: 8.50012327e-07
Iter: 254 loss: 8.42363931e-07
Iter: 255 loss: 8.91316574e-07
Iter: 256 loss: 8.41500309e-07
Iter: 257 loss: 8.36502409e-07
Iter: 258 loss: 8.51735081e-07
Iter: 259 loss: 8.3493012e-07
Iter: 260 loss: 8.2836192e-07
Iter: 261 loss: 8.46729392e-07
Iter: 262 loss: 8.2612587e-07
Iter: 263 loss: 8.22989762e-07
Iter: 264 loss: 8.60702e-07
Iter: 265 loss: 8.23009827e-07
Iter: 266 loss: 8.19322281e-07
Iter: 267 loss: 8.16276724e-07
Iter: 268 loss: 8.15156511e-07
Iter: 269 loss: 8.11106361e-07
Iter: 270 loss: 8.10703341e-07
Iter: 271 loss: 8.07709398e-07
Iter: 272 loss: 8.01715089e-07
Iter: 273 loss: 8.13385384e-07
Iter: 274 loss: 7.99271675e-07
Iter: 275 loss: 7.93418508e-07
Iter: 276 loss: 7.9445e-07
Iter: 277 loss: 7.89023034e-07
Iter: 278 loss: 7.8729181e-07
Iter: 279 loss: 7.85701275e-07
Iter: 280 loss: 7.82807547e-07
Iter: 281 loss: 7.87351496e-07
Iter: 282 loss: 7.81444498e-07
Iter: 283 loss: 7.77935952e-07
Iter: 284 loss: 7.76004413e-07
Iter: 285 loss: 7.74435762e-07
Iter: 286 loss: 7.7027255e-07
Iter: 287 loss: 8.35416e-07
Iter: 288 loss: 7.70267434e-07
Iter: 289 loss: 7.67634106e-07
Iter: 290 loss: 7.62278091e-07
Iter: 291 loss: 8.55701387e-07
Iter: 292 loss: 7.62118532e-07
Iter: 293 loss: 7.56510758e-07
Iter: 294 loss: 7.97200528e-07
Iter: 295 loss: 7.56048848e-07
Iter: 296 loss: 7.52253868e-07
Iter: 297 loss: 7.62554748e-07
Iter: 298 loss: 7.51076755e-07
Iter: 299 loss: 7.48871798e-07
Iter: 300 loss: 7.48755e-07
Iter: 301 loss: 7.46570095e-07
Iter: 302 loss: 7.46875912e-07
Iter: 303 loss: 7.44945112e-07
Iter: 304 loss: 7.4135761e-07
Iter: 305 loss: 7.45833859e-07
Iter: 306 loss: 7.39458585e-07
Iter: 307 loss: 7.36779157e-07
Iter: 308 loss: 7.3677711e-07
Iter: 309 loss: 7.34535888e-07
Iter: 310 loss: 7.30082888e-07
Iter: 311 loss: 7.27881229e-07
Iter: 312 loss: 7.25727659e-07
Iter: 313 loss: 7.19650529e-07
Iter: 314 loss: 7.40857786e-07
Iter: 315 loss: 7.18001161e-07
Iter: 316 loss: 7.15399892e-07
Iter: 317 loss: 7.15235387e-07
Iter: 318 loss: 7.12078304e-07
Iter: 319 loss: 7.12643555e-07
Iter: 320 loss: 7.09685594e-07
Iter: 321 loss: 7.06996502e-07
Iter: 322 loss: 7.14063844e-07
Iter: 323 loss: 7.06107699e-07
Iter: 324 loss: 7.02437262e-07
Iter: 325 loss: 7.09736469e-07
Iter: 326 loss: 7.00913063e-07
Iter: 327 loss: 6.97962321e-07
Iter: 328 loss: 7.05478556e-07
Iter: 329 loss: 6.96901793e-07
Iter: 330 loss: 6.9426244e-07
Iter: 331 loss: 6.89973263e-07
Iter: 332 loss: 6.89972865e-07
Iter: 333 loss: 6.85092857e-07
Iter: 334 loss: 7.47406943e-07
Iter: 335 loss: 6.85062673e-07
Iter: 336 loss: 6.83193775e-07
Iter: 337 loss: 6.8296174e-07
Iter: 338 loss: 6.81248309e-07
Iter: 339 loss: 6.79113782e-07
Iter: 340 loss: 6.78885897e-07
Iter: 341 loss: 6.76428726e-07
Iter: 342 loss: 6.87819409e-07
Iter: 343 loss: 6.75974661e-07
Iter: 344 loss: 6.73239811e-07
Iter: 345 loss: 6.70296117e-07
Iter: 346 loss: 6.69845804e-07
Iter: 347 loss: 6.66981464e-07
Iter: 348 loss: 6.89480771e-07
Iter: 349 loss: 6.66753806e-07
Iter: 350 loss: 6.64427375e-07
Iter: 351 loss: 6.64516165e-07
Iter: 352 loss: 6.62611e-07
Iter: 353 loss: 6.61127103e-07
Iter: 354 loss: 6.60819239e-07
Iter: 355 loss: 6.5897342e-07
Iter: 356 loss: 6.54595738e-07
Iter: 357 loss: 7.05874072e-07
Iter: 358 loss: 6.54198516e-07
Iter: 359 loss: 6.50889888e-07
Iter: 360 loss: 6.89230774e-07
Iter: 361 loss: 6.50832874e-07
Iter: 362 loss: 6.48051696e-07
Iter: 363 loss: 6.59369675e-07
Iter: 364 loss: 6.47408967e-07
Iter: 365 loss: 6.44842601e-07
Iter: 366 loss: 6.4356e-07
Iter: 367 loss: 6.42352802e-07
Iter: 368 loss: 6.38292477e-07
Iter: 369 loss: 6.4850849e-07
Iter: 370 loss: 6.36884238e-07
Iter: 371 loss: 6.36419145e-07
Iter: 372 loss: 6.35731908e-07
Iter: 373 loss: 6.34343e-07
Iter: 374 loss: 6.30797786e-07
Iter: 375 loss: 6.58857e-07
Iter: 376 loss: 6.30097247e-07
Iter: 377 loss: 6.26958183e-07
Iter: 378 loss: 6.51160576e-07
Iter: 379 loss: 6.26709721e-07
Iter: 380 loss: 6.24781421e-07
Iter: 381 loss: 6.37900825e-07
Iter: 382 loss: 6.24587813e-07
Iter: 383 loss: 6.22773086e-07
Iter: 384 loss: 6.22819e-07
Iter: 385 loss: 6.21309766e-07
Iter: 386 loss: 6.18954118e-07
Iter: 387 loss: 6.17414685e-07
Iter: 388 loss: 6.16507577e-07
Iter: 389 loss: 6.13713e-07
Iter: 390 loss: 6.56780571e-07
Iter: 391 loss: 6.13700763e-07
Iter: 392 loss: 6.11635073e-07
Iter: 393 loss: 6.2809363e-07
Iter: 394 loss: 6.11512462e-07
Iter: 395 loss: 6.09675055e-07
Iter: 396 loss: 6.06256151e-07
Iter: 397 loss: 6.80571816e-07
Iter: 398 loss: 6.06296567e-07
Iter: 399 loss: 6.03233218e-07
Iter: 400 loss: 6.11685323e-07
Iter: 401 loss: 6.02272792e-07
Iter: 402 loss: 5.99053635e-07
Iter: 403 loss: 6.37332164e-07
Iter: 404 loss: 5.99023565e-07
Iter: 405 loss: 5.97460371e-07
Iter: 406 loss: 6.00337785e-07
Iter: 407 loss: 5.96810935e-07
Iter: 408 loss: 5.95304073e-07
Iter: 409 loss: 6.00172712e-07
Iter: 410 loss: 5.9492379e-07
Iter: 411 loss: 5.92545689e-07
Iter: 412 loss: 5.95129904e-07
Iter: 413 loss: 5.91322191e-07
Iter: 414 loss: 5.89753768e-07
Iter: 415 loss: 5.88107582e-07
Iter: 416 loss: 5.87845534e-07
Iter: 417 loss: 5.85802e-07
Iter: 418 loss: 6.06849767e-07
Iter: 419 loss: 5.85752332e-07
Iter: 420 loss: 5.83906683e-07
Iter: 421 loss: 5.85455837e-07
Iter: 422 loss: 5.82828648e-07
Iter: 423 loss: 5.80465553e-07
Iter: 424 loss: 5.8448154e-07
Iter: 425 loss: 5.79439302e-07
Iter: 426 loss: 5.77204673e-07
Iter: 427 loss: 5.77292042e-07
Iter: 428 loss: 5.75441788e-07
Iter: 429 loss: 5.73749901e-07
Iter: 430 loss: 5.73386e-07
Iter: 431 loss: 5.72238e-07
Iter: 432 loss: 5.698937e-07
Iter: 433 loss: 6.09498102e-07
Iter: 434 loss: 5.69822532e-07
Iter: 435 loss: 5.6777003e-07
Iter: 436 loss: 5.85436737e-07
Iter: 437 loss: 5.67664301e-07
Iter: 438 loss: 5.66399535e-07
Iter: 439 loss: 5.72612862e-07
Iter: 440 loss: 5.66154426e-07
Iter: 441 loss: 5.64729135e-07
Iter: 442 loss: 5.68348298e-07
Iter: 443 loss: 5.6421635e-07
Iter: 444 loss: 5.63101366e-07
Iter: 445 loss: 5.6803367e-07
Iter: 446 loss: 5.62892808e-07
Iter: 447 loss: 5.61208481e-07
Iter: 448 loss: 5.60251578e-07
Iter: 449 loss: 5.59559226e-07
Iter: 450 loss: 5.58002796e-07
Iter: 451 loss: 5.596184e-07
Iter: 452 loss: 5.57143039e-07
Iter: 453 loss: 5.5540329e-07
Iter: 454 loss: 5.55539771e-07
Iter: 455 loss: 5.54081566e-07
Iter: 456 loss: 5.52273377e-07
Iter: 457 loss: 5.52274344e-07
Iter: 458 loss: 5.51192329e-07
Iter: 459 loss: 5.49427455e-07
Iter: 460 loss: 5.49430638e-07
Iter: 461 loss: 5.4746954e-07
Iter: 462 loss: 5.63710728e-07
Iter: 463 loss: 5.47329876e-07
Iter: 464 loss: 5.45902537e-07
Iter: 465 loss: 5.61241336e-07
Iter: 466 loss: 5.45866726e-07
Iter: 467 loss: 5.44803243e-07
Iter: 468 loss: 5.43691272e-07
Iter: 469 loss: 5.43512556e-07
Iter: 470 loss: 5.41845282e-07
Iter: 471 loss: 5.39929147e-07
Iter: 472 loss: 5.39640951e-07
Iter: 473 loss: 5.38049107e-07
Iter: 474 loss: 5.37871301e-07
Iter: 475 loss: 5.3639161e-07
Iter: 476 loss: 5.39143628e-07
Iter: 477 loss: 5.35729839e-07
Iter: 478 loss: 5.34236506e-07
Iter: 479 loss: 5.45633952e-07
Iter: 480 loss: 5.34151127e-07
Iter: 481 loss: 5.32629542e-07
Iter: 482 loss: 5.32910121e-07
Iter: 483 loss: 5.31587318e-07
Iter: 484 loss: 5.3023e-07
Iter: 485 loss: 5.28667442e-07
Iter: 486 loss: 5.28479859e-07
Iter: 487 loss: 5.26488805e-07
Iter: 488 loss: 5.3190638e-07
Iter: 489 loss: 5.2577019e-07
Iter: 490 loss: 5.23589961e-07
Iter: 491 loss: 5.44837121e-07
Iter: 492 loss: 5.23534e-07
Iter: 493 loss: 5.22146138e-07
Iter: 494 loss: 5.27925749e-07
Iter: 495 loss: 5.21848165e-07
Iter: 496 loss: 5.2062245e-07
Iter: 497 loss: 5.18740308e-07
Iter: 498 loss: 5.18679599e-07
Iter: 499 loss: 5.1857e-07
Iter: 500 loss: 5.1771076e-07
Iter: 501 loss: 5.16980322e-07
Iter: 502 loss: 5.15053102e-07
Iter: 503 loss: 5.31216756e-07
Iter: 504 loss: 5.14754106e-07
Iter: 505 loss: 5.13044029e-07
Iter: 506 loss: 5.28790167e-07
Iter: 507 loss: 5.12952852e-07
Iter: 508 loss: 5.1167973e-07
Iter: 509 loss: 5.12853319e-07
Iter: 510 loss: 5.10961058e-07
Iter: 511 loss: 5.09788777e-07
Iter: 512 loss: 5.09812139e-07
Iter: 513 loss: 5.08711537e-07
Iter: 514 loss: 5.08966195e-07
Iter: 515 loss: 5.07876166e-07
Iter: 516 loss: 5.06473953e-07
Iter: 517 loss: 5.11577696e-07
Iter: 518 loss: 5.06114702e-07
Iter: 519 loss: 5.05195658e-07
Iter: 520 loss: 5.05272624e-07
Iter: 521 loss: 5.04555146e-07
Iter: 522 loss: 5.03161516e-07
Iter: 523 loss: 5.01339287e-07
Iter: 524 loss: 5.01256636e-07
Iter: 525 loss: 4.9991479e-07
Iter: 526 loss: 4.99873863e-07
Iter: 527 loss: 4.98732334e-07
Iter: 528 loss: 5.01334398e-07
Iter: 529 loss: 4.98236773e-07
Iter: 530 loss: 4.97010433e-07
Iter: 531 loss: 4.97265319e-07
Iter: 532 loss: 4.96091843e-07
Iter: 533 loss: 4.94648873e-07
Iter: 534 loss: 5.08562948e-07
Iter: 535 loss: 4.94596e-07
Iter: 536 loss: 4.93252628e-07
Iter: 537 loss: 4.95564905e-07
Iter: 538 loss: 4.92693289e-07
Iter: 539 loss: 4.91639e-07
Iter: 540 loss: 4.90027105e-07
Iter: 541 loss: 4.8999641e-07
Iter: 542 loss: 4.88445835e-07
Iter: 543 loss: 5.0478468e-07
Iter: 544 loss: 4.88369892e-07
Iter: 545 loss: 4.87289924e-07
Iter: 546 loss: 4.984322e-07
Iter: 547 loss: 4.87272e-07
Iter: 548 loss: 4.86192675e-07
Iter: 549 loss: 4.89024e-07
Iter: 550 loss: 4.8580938e-07
Iter: 551 loss: 4.85283806e-07
Iter: 552 loss: 4.86317276e-07
Iter: 553 loss: 4.85064163e-07
Iter: 554 loss: 4.84215548e-07
Iter: 555 loss: 4.82538667e-07
Iter: 556 loss: 5.12458939e-07
Iter: 557 loss: 4.82472615e-07
Iter: 558 loss: 4.81096947e-07
Iter: 559 loss: 4.90878335e-07
Iter: 560 loss: 4.80910046e-07
Iter: 561 loss: 4.79611e-07
Iter: 562 loss: 4.8169818e-07
Iter: 563 loss: 4.78948152e-07
Iter: 564 loss: 4.77336584e-07
Iter: 565 loss: 4.88174408e-07
Iter: 566 loss: 4.77136e-07
Iter: 567 loss: 4.75744741e-07
Iter: 568 loss: 4.75320945e-07
Iter: 569 loss: 4.74425434e-07
Iter: 570 loss: 4.73959318e-07
Iter: 571 loss: 4.73731404e-07
Iter: 572 loss: 4.73097259e-07
Iter: 573 loss: 4.71655852e-07
Iter: 574 loss: 4.88237902e-07
Iter: 575 loss: 4.71530825e-07
Iter: 576 loss: 4.69921133e-07
Iter: 577 loss: 4.79466053e-07
Iter: 578 loss: 4.69695692e-07
Iter: 579 loss: 4.68767638e-07
Iter: 580 loss: 4.69111455e-07
Iter: 581 loss: 4.68099927e-07
Iter: 582 loss: 4.67474848e-07
Iter: 583 loss: 4.67356358e-07
Iter: 584 loss: 4.66606167e-07
Iter: 585 loss: 4.6584e-07
Iter: 586 loss: 4.65689737e-07
Iter: 587 loss: 4.64614686e-07
Iter: 588 loss: 4.64919964e-07
Iter: 589 loss: 4.63820356e-07
Iter: 590 loss: 4.62615446e-07
Iter: 591 loss: 4.77567e-07
Iter: 592 loss: 4.62600525e-07
Iter: 593 loss: 4.61725506e-07
Iter: 594 loss: 4.60819081e-07
Iter: 595 loss: 4.60655571e-07
Iter: 596 loss: 4.59493378e-07
Iter: 597 loss: 4.61349174e-07
Iter: 598 loss: 4.58942e-07
Iter: 599 loss: 4.57522617e-07
Iter: 600 loss: 4.71915854e-07
Iter: 601 loss: 4.57466854e-07
Iter: 602 loss: 4.5663333e-07
Iter: 603 loss: 4.59430225e-07
Iter: 604 loss: 4.56363125e-07
Iter: 605 loss: 4.55720908e-07
Iter: 606 loss: 4.58468861e-07
Iter: 607 loss: 4.55600428e-07
Iter: 608 loss: 4.54703127e-07
Iter: 609 loss: 4.5424e-07
Iter: 610 loss: 4.53855904e-07
Iter: 611 loss: 4.52893261e-07
Iter: 612 loss: 4.52460711e-07
Iter: 613 loss: 4.51944e-07
Iter: 614 loss: 4.50963114e-07
Iter: 615 loss: 4.64608746e-07
Iter: 616 loss: 4.50952655e-07
Iter: 617 loss: 4.50182512e-07
Iter: 618 loss: 4.59172043e-07
Iter: 619 loss: 4.50156421e-07
Iter: 620 loss: 4.49483423e-07
Iter: 621 loss: 4.48122563e-07
Iter: 622 loss: 4.76103594e-07
Iter: 623 loss: 4.4811128e-07
Iter: 624 loss: 4.47077525e-07
Iter: 625 loss: 4.50435721e-07
Iter: 626 loss: 4.46791546e-07
Iter: 627 loss: 4.45441458e-07
Iter: 628 loss: 4.49686695e-07
Iter: 629 loss: 4.45053359e-07
Iter: 630 loss: 4.44182859e-07
Iter: 631 loss: 4.46662852e-07
Iter: 632 loss: 4.43927973e-07
Iter: 633 loss: 4.43157859e-07
Iter: 634 loss: 4.4277192e-07
Iter: 635 loss: 4.42374869e-07
Iter: 636 loss: 4.41589606e-07
Iter: 637 loss: 4.41580625e-07
Iter: 638 loss: 4.4097078e-07
Iter: 639 loss: 4.40555539e-07
Iter: 640 loss: 4.40308213e-07
Iter: 641 loss: 4.39537871e-07
Iter: 642 loss: 4.50888564e-07
Iter: 643 loss: 4.39543783e-07
Iter: 644 loss: 4.38930812e-07
Iter: 645 loss: 4.38238146e-07
Iter: 646 loss: 4.3814012e-07
Iter: 647 loss: 4.37185093e-07
Iter: 648 loss: 4.38714409e-07
Iter: 649 loss: 4.36768744e-07
Iter: 650 loss: 4.35679596e-07
Iter: 651 loss: 4.35981406e-07
Iter: 652 loss: 4.34936396e-07
Iter: 653 loss: 4.34520018e-07
Iter: 654 loss: 4.34098325e-07
Iter: 655 loss: 4.33646164e-07
Iter: 656 loss: 4.32734453e-07
Iter: 657 loss: 4.49656397e-07
Iter: 658 loss: 4.32683379e-07
Iter: 659 loss: 4.31694332e-07
Iter: 660 loss: 4.33344724e-07
Iter: 661 loss: 4.31209173e-07
Iter: 662 loss: 4.30432692e-07
Iter: 663 loss: 4.42197063e-07
Iter: 664 loss: 4.30422972e-07
Iter: 665 loss: 4.29837883e-07
Iter: 666 loss: 4.29747246e-07
Iter: 667 loss: 4.29298893e-07
Iter: 668 loss: 4.2856982e-07
Iter: 669 loss: 4.29175714e-07
Iter: 670 loss: 4.28133404e-07
Iter: 671 loss: 4.27118351e-07
Iter: 672 loss: 4.30297519e-07
Iter: 673 loss: 4.2681495e-07
Iter: 674 loss: 4.26083687e-07
Iter: 675 loss: 4.3052637e-07
Iter: 676 loss: 4.25987537e-07
Iter: 677 loss: 4.24988656e-07
Iter: 678 loss: 4.24785441e-07
Iter: 679 loss: 4.24153086e-07
Iter: 680 loss: 4.23169695e-07
Iter: 681 loss: 4.28797193e-07
Iter: 682 loss: 4.23070674e-07
Iter: 683 loss: 4.22306897e-07
Iter: 684 loss: 4.28027846e-07
Iter: 685 loss: 4.22229192e-07
Iter: 686 loss: 4.21446032e-07
Iter: 687 loss: 4.20773233e-07
Iter: 688 loss: 4.20593267e-07
Iter: 689 loss: 4.19590407e-07
Iter: 690 loss: 4.20965847e-07
Iter: 691 loss: 4.19135802e-07
Iter: 692 loss: 4.19011144e-07
Iter: 693 loss: 4.18675768e-07
Iter: 694 loss: 4.1829361e-07
Iter: 695 loss: 4.17590769e-07
Iter: 696 loss: 4.34141157e-07
Iter: 697 loss: 4.17582498e-07
Iter: 698 loss: 4.16788509e-07
Iter: 699 loss: 4.16861042e-07
Iter: 700 loss: 4.16164767e-07
Iter: 701 loss: 4.15411478e-07
Iter: 702 loss: 4.15430435e-07
Iter: 703 loss: 4.14748143e-07
Iter: 704 loss: 4.13785983e-07
Iter: 705 loss: 4.13758187e-07
Iter: 706 loss: 4.1266037e-07
Iter: 707 loss: 4.16314862e-07
Iter: 708 loss: 4.12397469e-07
Iter: 709 loss: 4.11228285e-07
Iter: 710 loss: 4.13751366e-07
Iter: 711 loss: 4.10749493e-07
Iter: 712 loss: 4.09663073e-07
Iter: 713 loss: 4.1990927e-07
Iter: 714 loss: 4.09609839e-07
Iter: 715 loss: 4.08732774e-07
Iter: 716 loss: 4.08721263e-07
Iter: 717 loss: 4.08023908e-07
Iter: 718 loss: 4.07325444e-07
Iter: 719 loss: 4.14940786e-07
Iter: 720 loss: 4.07284801e-07
Iter: 721 loss: 4.06551209e-07
Iter: 722 loss: 4.07031e-07
Iter: 723 loss: 4.06067699e-07
Iter: 724 loss: 4.0528667e-07
Iter: 725 loss: 4.07996197e-07
Iter: 726 loss: 4.05112672e-07
Iter: 727 loss: 4.04627713e-07
Iter: 728 loss: 4.04617595e-07
Iter: 729 loss: 4.04281877e-07
Iter: 730 loss: 4.03437383e-07
Iter: 731 loss: 4.11347912e-07
Iter: 732 loss: 4.03326624e-07
Iter: 733 loss: 4.02308416e-07
Iter: 734 loss: 4.07744437e-07
Iter: 735 loss: 4.02180831e-07
Iter: 736 loss: 4.01527387e-07
Iter: 737 loss: 4.07801565e-07
Iter: 738 loss: 4.01500102e-07
Iter: 739 loss: 4.00902223e-07
Iter: 740 loss: 4.00183239e-07
Iter: 741 loss: 4.00143279e-07
Iter: 742 loss: 3.99161593e-07
Iter: 743 loss: 4.01263037e-07
Iter: 744 loss: 3.98779804e-07
Iter: 745 loss: 3.97713507e-07
Iter: 746 loss: 4.04746459e-07
Iter: 747 loss: 3.9763222e-07
Iter: 748 loss: 3.96883337e-07
Iter: 749 loss: 4.01580508e-07
Iter: 750 loss: 3.96799322e-07
Iter: 751 loss: 3.96196413e-07
Iter: 752 loss: 3.96605969e-07
Iter: 753 loss: 3.95820621e-07
Iter: 754 loss: 3.95375224e-07
Iter: 755 loss: 4.00930332e-07
Iter: 756 loss: 3.9537872e-07
Iter: 757 loss: 3.9487955e-07
Iter: 758 loss: 3.94207291e-07
Iter: 759 loss: 3.9415113e-07
Iter: 760 loss: 3.93561095e-07
Iter: 761 loss: 3.93579967e-07
Iter: 762 loss: 3.93040409e-07
Iter: 763 loss: 3.9361106e-07
Iter: 764 loss: 3.92723706e-07
Iter: 765 loss: 3.92199354e-07
Iter: 766 loss: 3.91374471e-07
Iter: 767 loss: 3.91377881e-07
Iter: 768 loss: 3.90435957e-07
Iter: 769 loss: 3.94353833e-07
Iter: 770 loss: 3.90218588e-07
Iter: 771 loss: 3.89545562e-07
Iter: 772 loss: 3.89551985e-07
Iter: 773 loss: 3.8917392e-07
Iter: 774 loss: 3.88793978e-07
Iter: 775 loss: 3.88725709e-07
Iter: 776 loss: 3.8791103e-07
Iter: 777 loss: 3.87132218e-07
Iter: 778 loss: 3.86945885e-07
Iter: 779 loss: 3.86361336e-07
Iter: 780 loss: 3.86318419e-07
Iter: 781 loss: 3.85761467e-07
Iter: 782 loss: 3.86785729e-07
Iter: 783 loss: 3.85502887e-07
Iter: 784 loss: 3.84827615e-07
Iter: 785 loss: 3.8546122e-07
Iter: 786 loss: 3.84422407e-07
Iter: 787 loss: 3.83839904e-07
Iter: 788 loss: 3.89491561e-07
Iter: 789 loss: 3.83817735e-07
Iter: 790 loss: 3.83237477e-07
Iter: 791 loss: 3.8354375e-07
Iter: 792 loss: 3.82826556e-07
Iter: 793 loss: 3.82407848e-07
Iter: 794 loss: 3.85872113e-07
Iter: 795 loss: 3.8236027e-07
Iter: 796 loss: 3.8183876e-07
Iter: 797 loss: 3.81988087e-07
Iter: 798 loss: 3.81420875e-07
Iter: 799 loss: 3.80837037e-07
Iter: 800 loss: 3.80631633e-07
Iter: 801 loss: 3.80299809e-07
Iter: 802 loss: 3.79520714e-07
Iter: 803 loss: 3.79251162e-07
Iter: 804 loss: 3.78827053e-07
Iter: 805 loss: 3.77975482e-07
Iter: 806 loss: 3.83552e-07
Iter: 807 loss: 3.77834454e-07
Iter: 808 loss: 3.77059678e-07
Iter: 809 loss: 3.81136289e-07
Iter: 810 loss: 3.76927147e-07
Iter: 811 loss: 3.76308208e-07
Iter: 812 loss: 3.82718667e-07
Iter: 813 loss: 3.76287602e-07
Iter: 814 loss: 3.75889414e-07
Iter: 815 loss: 3.75742843e-07
Iter: 816 loss: 3.75523484e-07
Iter: 817 loss: 3.74923616e-07
Iter: 818 loss: 3.74191359e-07
Iter: 819 loss: 3.74127126e-07
Iter: 820 loss: 3.73486927e-07
Iter: 821 loss: 3.73369915e-07
Iter: 822 loss: 3.7299381e-07
Iter: 823 loss: 3.73374519e-07
Iter: 824 loss: 3.72788833e-07
Iter: 825 loss: 3.72279203e-07
Iter: 826 loss: 3.7215068e-07
Iter: 827 loss: 3.71818203e-07
Iter: 828 loss: 3.71052977e-07
Iter: 829 loss: 3.79571929e-07
Iter: 830 loss: 3.71010316e-07
Iter: 831 loss: 3.70589703e-07
Iter: 832 loss: 3.71545241e-07
Iter: 833 loss: 3.70408031e-07
Iter: 834 loss: 3.69927676e-07
Iter: 835 loss: 3.71535862e-07
Iter: 836 loss: 3.69773858e-07
Iter: 837 loss: 3.69436577e-07
Iter: 838 loss: 3.69144573e-07
Iter: 839 loss: 3.69025258e-07
Iter: 840 loss: 3.68516e-07
Iter: 841 loss: 3.68129747e-07
Iter: 842 loss: 3.67971495e-07
Iter: 843 loss: 3.67137403e-07
Iter: 844 loss: 3.68750591e-07
Iter: 845 loss: 3.66779716e-07
Iter: 846 loss: 3.6582378e-07
Iter: 847 loss: 3.75212323e-07
Iter: 848 loss: 3.65782057e-07
Iter: 849 loss: 3.65268278e-07
Iter: 850 loss: 3.70415819e-07
Iter: 851 loss: 3.65272911e-07
Iter: 852 loss: 3.64798524e-07
Iter: 853 loss: 3.64262348e-07
Iter: 854 loss: 3.64210848e-07
Iter: 855 loss: 3.63493399e-07
Iter: 856 loss: 3.64463688e-07
Iter: 857 loss: 3.63129914e-07
Iter: 858 loss: 3.62530471e-07
Iter: 859 loss: 3.68511223e-07
Iter: 860 loss: 3.62520439e-07
Iter: 861 loss: 3.62029766e-07
Iter: 862 loss: 3.65697758e-07
Iter: 863 loss: 3.61983666e-07
Iter: 864 loss: 3.61559501e-07
Iter: 865 loss: 3.62033973e-07
Iter: 866 loss: 3.61336163e-07
Iter: 867 loss: 3.60953663e-07
Iter: 868 loss: 3.64802929e-07
Iter: 869 loss: 3.60947354e-07
Iter: 870 loss: 3.60640286e-07
Iter: 871 loss: 3.60580771e-07
Iter: 872 loss: 3.60386878e-07
Iter: 873 loss: 3.59893505e-07
Iter: 874 loss: 3.60442442e-07
Iter: 875 loss: 3.59618753e-07
Iter: 876 loss: 3.59231905e-07
Iter: 877 loss: 3.59589848e-07
Iter: 878 loss: 3.59000637e-07
Iter: 879 loss: 3.58365924e-07
Iter: 880 loss: 3.57722854e-07
Iter: 881 loss: 3.57621957e-07
Iter: 882 loss: 3.56751485e-07
Iter: 883 loss: 3.63334095e-07
Iter: 884 loss: 3.56726332e-07
Iter: 885 loss: 3.55949851e-07
Iter: 886 loss: 3.57297893e-07
Iter: 887 loss: 3.55597763e-07
Iter: 888 loss: 3.550158e-07
Iter: 889 loss: 3.55023104e-07
Iter: 890 loss: 3.54539878e-07
Iter: 891 loss: 3.5519173e-07
Iter: 892 loss: 3.54322907e-07
Iter: 893 loss: 3.53931938e-07
Iter: 894 loss: 3.53556828e-07
Iter: 895 loss: 3.5346514e-07
Iter: 896 loss: 3.52882864e-07
Iter: 897 loss: 3.60921348e-07
Iter: 898 loss: 3.52893323e-07
Iter: 899 loss: 3.52469897e-07
Iter: 900 loss: 3.56982099e-07
Iter: 901 loss: 3.52440566e-07
Iter: 902 loss: 3.52267705e-07
Iter: 903 loss: 3.5214822e-07
Iter: 904 loss: 3.52029332e-07
Iter: 905 loss: 3.51549204e-07
Iter: 906 loss: 3.52159532e-07
Iter: 907 loss: 3.51268312e-07
Iter: 908 loss: 3.509e-07
Iter: 909 loss: 3.50635105e-07
Iter: 910 loss: 3.50492485e-07
Iter: 911 loss: 3.4985365e-07
Iter: 912 loss: 3.5570838e-07
Iter: 913 loss: 3.4982645e-07
Iter: 914 loss: 3.49385118e-07
Iter: 915 loss: 3.49062191e-07
Iter: 916 loss: 3.4892804e-07
Iter: 917 loss: 3.48307736e-07
Iter: 918 loss: 3.51602694e-07
Iter: 919 loss: 3.4822e-07
Iter: 920 loss: 3.47775512e-07
Iter: 921 loss: 3.47359872e-07
Iter: 922 loss: 3.472461e-07
Iter: 923 loss: 3.466767e-07
Iter: 924 loss: 3.54371423e-07
Iter: 925 loss: 3.46690683e-07
Iter: 926 loss: 3.46214875e-07
Iter: 927 loss: 3.45970022e-07
Iter: 928 loss: 3.45777096e-07
Iter: 929 loss: 3.4505706e-07
Iter: 930 loss: 3.55594636e-07
Iter: 931 loss: 3.45092474e-07
Iter: 932 loss: 3.44703665e-07
Iter: 933 loss: 3.44280181e-07
Iter: 934 loss: 3.44225271e-07
Iter: 935 loss: 3.43603631e-07
Iter: 936 loss: 3.47938567e-07
Iter: 937 loss: 3.43544343e-07
Iter: 938 loss: 3.43138254e-07
Iter: 939 loss: 3.47822464e-07
Iter: 940 loss: 3.43125976e-07
Iter: 941 loss: 3.42768033e-07
Iter: 942 loss: 3.42618023e-07
Iter: 943 loss: 3.42436e-07
Iter: 944 loss: 3.41981547e-07
Iter: 945 loss: 3.45911161e-07
Iter: 946 loss: 3.41983764e-07
Iter: 947 loss: 3.41721545e-07
Iter: 948 loss: 3.41307612e-07
Iter: 949 loss: 3.51946113e-07
Iter: 950 loss: 3.41303689e-07
Iter: 951 loss: 3.40703082e-07
Iter: 952 loss: 3.43751935e-07
Iter: 953 loss: 3.40552674e-07
Iter: 954 loss: 3.40098154e-07
Iter: 955 loss: 3.42246608e-07
Iter: 956 loss: 3.39995182e-07
Iter: 957 loss: 3.39589519e-07
Iter: 958 loss: 3.389531e-07
Iter: 959 loss: 3.38950827e-07
Iter: 960 loss: 3.3835704e-07
Iter: 961 loss: 3.42861853e-07
Iter: 962 loss: 3.38327368e-07
Iter: 963 loss: 3.37718717e-07
Iter: 964 loss: 3.38738943e-07
Iter: 965 loss: 3.37453514e-07
Iter: 966 loss: 3.36810956e-07
Iter: 967 loss: 3.3761853e-07
Iter: 968 loss: 3.36454832e-07
Iter: 969 loss: 3.35921328e-07
Iter: 970 loss: 3.40766462e-07
Iter: 971 loss: 3.35888217e-07
Iter: 972 loss: 3.35292896e-07
Iter: 973 loss: 3.37067661e-07
Iter: 974 loss: 3.35096956e-07
Iter: 975 loss: 3.34727474e-07
Iter: 976 loss: 3.35998294e-07
Iter: 977 loss: 3.34623792e-07
Iter: 978 loss: 3.34245556e-07
Iter: 979 loss: 3.35540904e-07
Iter: 980 loss: 3.34192805e-07
Iter: 981 loss: 3.33686444e-07
Iter: 982 loss: 3.34316951e-07
Iter: 983 loss: 3.33445769e-07
Iter: 984 loss: 3.33160074e-07
Iter: 985 loss: 3.3773847e-07
Iter: 986 loss: 3.33167691e-07
Iter: 987 loss: 3.32858406e-07
Iter: 988 loss: 3.32385639e-07
Iter: 989 loss: 3.32372537e-07
Iter: 990 loss: 3.31889169e-07
Iter: 991 loss: 3.34268123e-07
Iter: 992 loss: 3.31845058e-07
Iter: 993 loss: 3.3134711e-07
Iter: 994 loss: 3.31920432e-07
Iter: 995 loss: 3.31116354e-07
Iter: 996 loss: 3.30623493e-07
Iter: 997 loss: 3.31390879e-07
Iter: 998 loss: 3.30403623e-07
Iter: 999 loss: 3.29945e-07
Iter: 1000 loss: 3.29565921e-07
Iter: 1001 loss: 3.29432794e-07
Iter: 1002 loss: 3.2891387e-07
Iter: 1003 loss: 3.2891333e-07
Iter: 1004 loss: 3.28560361e-07
Iter: 1005 loss: 3.28016597e-07
Iter: 1006 loss: 3.28003352e-07
Iter: 1007 loss: 3.27515522e-07
Iter: 1008 loss: 3.27535162e-07
Iter: 1009 loss: 3.27065152e-07
Iter: 1010 loss: 3.28398386e-07
Iter: 1011 loss: 3.26908548e-07
Iter: 1012 loss: 3.26537901e-07
Iter: 1013 loss: 3.26937936e-07
Iter: 1014 loss: 3.26286965e-07
Iter: 1015 loss: 3.25861208e-07
Iter: 1016 loss: 3.2886183e-07
Iter: 1017 loss: 3.25828893e-07
Iter: 1018 loss: 3.25339442e-07
Iter: 1019 loss: 3.26302825e-07
Iter: 1020 loss: 3.25170106e-07
Iter: 1021 loss: 3.24901578e-07
Iter: 1022 loss: 3.27020331e-07
Iter: 1023 loss: 3.24872587e-07
Iter: 1024 loss: 3.24610738e-07
Iter: 1025 loss: 3.24080872e-07
Iter: 1026 loss: 3.33541379e-07
Iter: 1027 loss: 3.24088631e-07
Iter: 1028 loss: 3.23645082e-07
Iter: 1029 loss: 3.25268019e-07
Iter: 1030 loss: 3.23558623e-07
Iter: 1031 loss: 3.23078496e-07
Iter: 1032 loss: 3.23172287e-07
Iter: 1033 loss: 3.22720723e-07
Iter: 1034 loss: 3.22499687e-07
Iter: 1035 loss: 3.22437472e-07
Iter: 1036 loss: 3.2218469e-07
Iter: 1037 loss: 3.21701e-07
Iter: 1038 loss: 3.31047971e-07
Iter: 1039 loss: 3.21695467e-07
Iter: 1040 loss: 3.21207949e-07
Iter: 1041 loss: 3.22569292e-07
Iter: 1042 loss: 3.21045093e-07
Iter: 1043 loss: 3.20590971e-07
Iter: 1044 loss: 3.23287281e-07
Iter: 1045 loss: 3.20523156e-07
Iter: 1046 loss: 3.20105016e-07
Iter: 1047 loss: 3.20654721e-07
Iter: 1048 loss: 3.19885686e-07
Iter: 1049 loss: 3.19322e-07
Iter: 1050 loss: 3.23422398e-07
Iter: 1051 loss: 3.19270356e-07
Iter: 1052 loss: 3.18959621e-07
Iter: 1053 loss: 3.19667663e-07
Iter: 1054 loss: 3.18856053e-07
Iter: 1055 loss: 3.18459e-07
Iter: 1056 loss: 3.20467933e-07
Iter: 1057 loss: 3.18409178e-07
Iter: 1058 loss: 3.18124535e-07
Iter: 1059 loss: 3.18236971e-07
Iter: 1060 loss: 3.17934962e-07
Iter: 1061 loss: 3.17628121e-07
Iter: 1062 loss: 3.20012049e-07
Iter: 1063 loss: 3.17610528e-07
Iter: 1064 loss: 3.17372297e-07
Iter: 1065 loss: 3.16778966e-07
Iter: 1066 loss: 3.23809104e-07
Iter: 1067 loss: 3.16727778e-07
Iter: 1068 loss: 3.1615366e-07
Iter: 1069 loss: 3.19690287e-07
Iter: 1070 loss: 3.16063108e-07
Iter: 1071 loss: 3.15620241e-07
Iter: 1072 loss: 3.18214347e-07
Iter: 1073 loss: 3.1556138e-07
Iter: 1074 loss: 3.15099953e-07
Iter: 1075 loss: 3.16526268e-07
Iter: 1076 loss: 3.14992775e-07
Iter: 1077 loss: 3.14517933e-07
Iter: 1078 loss: 3.14130261e-07
Iter: 1079 loss: 3.14004296e-07
Iter: 1080 loss: 3.13587407e-07
Iter: 1081 loss: 3.15542451e-07
Iter: 1082 loss: 3.13507599e-07
Iter: 1083 loss: 3.12996292e-07
Iter: 1084 loss: 3.13527067e-07
Iter: 1085 loss: 3.12685927e-07
Iter: 1086 loss: 3.12336454e-07
Iter: 1087 loss: 3.15984238e-07
Iter: 1088 loss: 3.12352171e-07
Iter: 1089 loss: 3.11959582e-07
Iter: 1090 loss: 3.12672796e-07
Iter: 1091 loss: 3.11784788e-07
Iter: 1092 loss: 3.11507e-07
Iter: 1093 loss: 3.15027791e-07
Iter: 1094 loss: 3.1147863e-07
Iter: 1095 loss: 3.1125046e-07
Iter: 1096 loss: 3.11562701e-07
Iter: 1097 loss: 3.11117844e-07
Iter: 1098 loss: 3.1089192e-07
Iter: 1099 loss: 3.10948394e-07
Iter: 1100 loss: 3.10715876e-07
Iter: 1101 loss: 3.10446836e-07
Iter: 1102 loss: 3.12990437e-07
Iter: 1103 loss: 3.10428845e-07
Iter: 1104 loss: 3.10222333e-07
Iter: 1105 loss: 3.09685163e-07
Iter: 1106 loss: 3.13587094e-07
Iter: 1107 loss: 3.09564939e-07
Iter: 1108 loss: 3.09028565e-07
Iter: 1109 loss: 3.12678708e-07
Iter: 1110 loss: 3.08960182e-07
Iter: 1111 loss: 3.08467889e-07
Iter: 1112 loss: 3.11734709e-07
Iter: 1113 loss: 3.08421193e-07
Iter: 1114 loss: 3.08107332e-07
Iter: 1115 loss: 3.10437031e-07
Iter: 1116 loss: 3.08084907e-07
Iter: 1117 loss: 3.07834796e-07
Iter: 1118 loss: 3.07436324e-07
Iter: 1119 loss: 3.07430724e-07
Iter: 1120 loss: 3.06953808e-07
Iter: 1121 loss: 3.08825975e-07
Iter: 1122 loss: 3.06858567e-07
Iter: 1123 loss: 3.06415188e-07
Iter: 1124 loss: 3.08183928e-07
Iter: 1125 loss: 3.06336517e-07
Iter: 1126 loss: 3.0596172e-07
Iter: 1127 loss: 3.07385108e-07
Iter: 1128 loss: 3.05877109e-07
Iter: 1129 loss: 3.05508252e-07
Iter: 1130 loss: 3.09052922e-07
Iter: 1131 loss: 3.05485457e-07
Iter: 1132 loss: 3.05311801e-07
Iter: 1133 loss: 3.06888978e-07
Iter: 1134 loss: 3.05320157e-07
Iter: 1135 loss: 3.05175178e-07
Iter: 1136 loss: 3.04765365e-07
Iter: 1137 loss: 3.0570439e-07
Iter: 1138 loss: 3.04488907e-07
Iter: 1139 loss: 3.0411951e-07
Iter: 1140 loss: 3.04097625e-07
Iter: 1141 loss: 3.03746901e-07
Iter: 1142 loss: 3.04174563e-07
Iter: 1143 loss: 3.03568839e-07
Iter: 1144 loss: 3.03198306e-07
Iter: 1145 loss: 3.03234543e-07
Iter: 1146 loss: 3.02943647e-07
Iter: 1147 loss: 3.02543015e-07
Iter: 1148 loss: 3.02690125e-07
Iter: 1149 loss: 3.02264425e-07
Iter: 1150 loss: 3.01934165e-07
Iter: 1151 loss: 3.01913872e-07
Iter: 1152 loss: 3.01625164e-07
Iter: 1153 loss: 3.01041041e-07
Iter: 1154 loss: 3.11751478e-07
Iter: 1155 loss: 3.01031889e-07
Iter: 1156 loss: 3.00618893e-07
Iter: 1157 loss: 3.05062429e-07
Iter: 1158 loss: 3.00610623e-07
Iter: 1159 loss: 3.00256431e-07
Iter: 1160 loss: 2.99994326e-07
Iter: 1161 loss: 2.99876319e-07
Iter: 1162 loss: 2.99460055e-07
Iter: 1163 loss: 3.04414e-07
Iter: 1164 loss: 2.99468411e-07
Iter: 1165 loss: 2.99094467e-07
Iter: 1166 loss: 3.01912877e-07
Iter: 1167 loss: 2.99080199e-07
Iter: 1168 loss: 2.98828979e-07
Iter: 1169 loss: 2.99480803e-07
Iter: 1170 loss: 2.98771454e-07
Iter: 1171 loss: 2.98522195e-07
Iter: 1172 loss: 2.9874974e-07
Iter: 1173 loss: 2.98361897e-07
Iter: 1174 loss: 2.98087343e-07
Iter: 1175 loss: 2.98171926e-07
Iter: 1176 loss: 2.9787347e-07
Iter: 1177 loss: 2.97564867e-07
Iter: 1178 loss: 3.01664329e-07
Iter: 1179 loss: 2.97565748e-07
Iter: 1180 loss: 2.97314443e-07
Iter: 1181 loss: 2.96756383e-07
Iter: 1182 loss: 3.03665047e-07
Iter: 1183 loss: 2.96716621e-07
Iter: 1184 loss: 2.96302176e-07
Iter: 1185 loss: 3.00528257e-07
Iter: 1186 loss: 2.96279808e-07
Iter: 1187 loss: 2.95924252e-07
Iter: 1188 loss: 2.96137273e-07
Iter: 1189 loss: 2.95687272e-07
Iter: 1190 loss: 2.95372786e-07
Iter: 1191 loss: 2.95378896e-07
Iter: 1192 loss: 2.95150443e-07
Iter: 1193 loss: 2.9474171e-07
Iter: 1194 loss: 2.94735685e-07
Iter: 1195 loss: 2.94285371e-07
Iter: 1196 loss: 2.96258833e-07
Iter: 1197 loss: 2.94218182e-07
Iter: 1198 loss: 2.93840685e-07
Iter: 1199 loss: 2.96100865e-07
Iter: 1200 loss: 2.93805471e-07
Iter: 1201 loss: 2.93457816e-07
Iter: 1202 loss: 2.94936257e-07
Iter: 1203 loss: 2.93389348e-07
Iter: 1204 loss: 2.93173287e-07
Iter: 1205 loss: 2.93866663e-07
Iter: 1206 loss: 2.93081541e-07
Iter: 1207 loss: 2.92764469e-07
Iter: 1208 loss: 2.92514642e-07
Iter: 1209 loss: 2.92429263e-07
Iter: 1210 loss: 2.92059326e-07
Iter: 1211 loss: 2.94292192e-07
Iter: 1212 loss: 2.92038123e-07
Iter: 1213 loss: 2.91694761e-07
Iter: 1214 loss: 2.92980303e-07
Iter: 1215 loss: 2.91622143e-07
Iter: 1216 loss: 2.9133264e-07
Iter: 1217 loss: 2.91174956e-07
Iter: 1218 loss: 2.91090146e-07
Iter: 1219 loss: 2.90722227e-07
Iter: 1220 loss: 2.90821475e-07
Iter: 1221 loss: 2.90461429e-07
Iter: 1222 loss: 2.89975134e-07
Iter: 1223 loss: 2.92697109e-07
Iter: 1224 loss: 2.89889698e-07
Iter: 1225 loss: 2.89511036e-07
Iter: 1226 loss: 2.93532537e-07
Iter: 1227 loss: 2.89529254e-07
Iter: 1228 loss: 2.89242763e-07
Iter: 1229 loss: 2.89346474e-07
Iter: 1230 loss: 2.89041083e-07
Iter: 1231 loss: 2.8869556e-07
Iter: 1232 loss: 2.88437491e-07
Iter: 1233 loss: 2.88317608e-07
Iter: 1234 loss: 2.88219553e-07
Iter: 1235 loss: 2.88080969e-07
Iter: 1236 loss: 2.87887133e-07
Iter: 1237 loss: 2.88179706e-07
Iter: 1238 loss: 2.87775833e-07
Iter: 1239 loss: 2.87582509e-07
Iter: 1240 loss: 2.87896398e-07
Iter: 1241 loss: 2.87492526e-07
Iter: 1242 loss: 2.87241022e-07
Iter: 1243 loss: 2.8733723e-07
Iter: 1244 loss: 2.87058072e-07
Iter: 1245 loss: 2.86769733e-07
Iter: 1246 loss: 2.87389e-07
Iter: 1247 loss: 2.8663888e-07
Iter: 1248 loss: 2.86333034e-07
Iter: 1249 loss: 2.89184356e-07
Iter: 1250 loss: 2.86320898e-07
Iter: 1251 loss: 2.86146758e-07
Iter: 1252 loss: 2.85717533e-07
Iter: 1253 loss: 2.90598763e-07
Iter: 1254 loss: 2.85674105e-07
Iter: 1255 loss: 2.85261592e-07
Iter: 1256 loss: 2.89498018e-07
Iter: 1257 loss: 2.85247438e-07
Iter: 1258 loss: 2.84933321e-07
Iter: 1259 loss: 2.85410096e-07
Iter: 1260 loss: 2.84781834e-07
Iter: 1261 loss: 2.84481729e-07
Iter: 1262 loss: 2.88758372e-07
Iter: 1263 loss: 2.8448423e-07
Iter: 1264 loss: 2.84256657e-07
Iter: 1265 loss: 2.84038151e-07
Iter: 1266 loss: 2.8397622e-07
Iter: 1267 loss: 2.83683136e-07
Iter: 1268 loss: 2.85000965e-07
Iter: 1269 loss: 2.83657613e-07
Iter: 1270 loss: 2.83415034e-07
Iter: 1271 loss: 2.83409435e-07
Iter: 1272 loss: 2.83277103e-07
Iter: 1273 loss: 2.83076588e-07
Iter: 1274 loss: 2.83063571e-07
Iter: 1275 loss: 2.82841143e-07
Iter: 1276 loss: 2.85030239e-07
Iter: 1277 loss: 2.82838869e-07
Iter: 1278 loss: 2.82674222e-07
Iter: 1279 loss: 2.82505624e-07
Iter: 1280 loss: 2.82462707e-07
Iter: 1281 loss: 2.82252586e-07
Iter: 1282 loss: 2.85086088e-07
Iter: 1283 loss: 2.82253097e-07
Iter: 1284 loss: 2.82060199e-07
Iter: 1285 loss: 2.81942391e-07
Iter: 1286 loss: 2.81863748e-07
Iter: 1287 loss: 2.81580895e-07
Iter: 1288 loss: 2.81627138e-07
Iter: 1289 loss: 2.81364436e-07
Iter: 1290 loss: 2.80973097e-07
Iter: 1291 loss: 2.81495318e-07
Iter: 1292 loss: 2.80768802e-07
Iter: 1293 loss: 2.80489246e-07
Iter: 1294 loss: 2.84608888e-07
Iter: 1295 loss: 2.80480947e-07
Iter: 1296 loss: 2.80200368e-07
Iter: 1297 loss: 2.80768859e-07
Iter: 1298 loss: 2.80087761e-07
Iter: 1299 loss: 2.79691392e-07
Iter: 1300 loss: 2.79819062e-07
Iter: 1301 loss: 2.79422352e-07
Iter: 1302 loss: 2.79130887e-07
Iter: 1303 loss: 2.83648433e-07
Iter: 1304 loss: 2.79124691e-07
Iter: 1305 loss: 2.78834591e-07
Iter: 1306 loss: 2.79522339e-07
Iter: 1307 loss: 2.78733921e-07
Iter: 1308 loss: 2.78574817e-07
Iter: 1309 loss: 2.78655364e-07
Iter: 1310 loss: 2.78433845e-07
Iter: 1311 loss: 2.78171655e-07
Iter: 1312 loss: 2.79070775e-07
Iter: 1313 loss: 2.78096934e-07
Iter: 1314 loss: 2.7791765e-07
Iter: 1315 loss: 2.78325246e-07
Iter: 1316 loss: 2.77858504e-07
Iter: 1317 loss: 2.77626725e-07
Iter: 1318 loss: 2.78217442e-07
Iter: 1319 loss: 2.77562663e-07
Iter: 1320 loss: 2.77361437e-07
Iter: 1321 loss: 2.77193692e-07
Iter: 1322 loss: 2.77132017e-07
Iter: 1323 loss: 2.76857406e-07
Iter: 1324 loss: 2.77214e-07
Iter: 1325 loss: 2.7672246e-07
Iter: 1326 loss: 2.76371736e-07
Iter: 1327 loss: 2.77714577e-07
Iter: 1328 loss: 2.7628505e-07
Iter: 1329 loss: 2.76000549e-07
Iter: 1330 loss: 2.78325956e-07
Iter: 1331 loss: 2.7597946e-07
Iter: 1332 loss: 2.75710164e-07
Iter: 1333 loss: 2.76343798e-07
Iter: 1334 loss: 2.75594374e-07
Iter: 1335 loss: 2.75308366e-07
Iter: 1336 loss: 2.75203433e-07
Iter: 1337 loss: 2.75035063e-07
Iter: 1338 loss: 2.74713045e-07
Iter: 1339 loss: 2.74683828e-07
Iter: 1340 loss: 2.7450767e-07
Iter: 1341 loss: 2.74348821e-07
Iter: 1342 loss: 2.74286862e-07
Iter: 1343 loss: 2.74052809e-07
Iter: 1344 loss: 2.75760812e-07
Iter: 1345 loss: 2.74025297e-07
Iter: 1346 loss: 2.7380284e-07
Iter: 1347 loss: 2.73893278e-07
Iter: 1348 loss: 2.73669741e-07
Iter: 1349 loss: 2.73427844e-07
Iter: 1350 loss: 2.75348953e-07
Iter: 1351 loss: 2.73432477e-07
Iter: 1352 loss: 2.73229944e-07
Iter: 1353 loss: 2.73251374e-07
Iter: 1354 loss: 2.73099971e-07
Iter: 1355 loss: 2.72862508e-07
Iter: 1356 loss: 2.7260694e-07
Iter: 1357 loss: 2.72556349e-07
Iter: 1358 loss: 2.72170894e-07
Iter: 1359 loss: 2.73991247e-07
Iter: 1360 loss: 2.72077301e-07
Iter: 1361 loss: 2.71694148e-07
Iter: 1362 loss: 2.73522886e-07
Iter: 1363 loss: 2.71647423e-07
Iter: 1364 loss: 2.71357692e-07
Iter: 1365 loss: 2.7450028e-07
Iter: 1366 loss: 2.71362808e-07
Iter: 1367 loss: 2.71140948e-07
Iter: 1368 loss: 2.70983378e-07
Iter: 1369 loss: 2.70907e-07
Iter: 1370 loss: 2.70847551e-07
Iter: 1371 loss: 2.70783289e-07
Iter: 1372 loss: 2.70645756e-07
Iter: 1373 loss: 2.70451466e-07
Iter: 1374 loss: 2.75480602e-07
Iter: 1375 loss: 2.70443309e-07
Iter: 1376 loss: 2.70180522e-07
Iter: 1377 loss: 2.70968286e-07
Iter: 1378 loss: 2.70127487e-07
Iter: 1379 loss: 2.69854041e-07
Iter: 1380 loss: 2.71666778e-07
Iter: 1381 loss: 2.69834175e-07
Iter: 1382 loss: 2.69677031e-07
Iter: 1383 loss: 2.69685586e-07
Iter: 1384 loss: 2.69545922e-07
Iter: 1385 loss: 2.69257754e-07
Iter: 1386 loss: 2.70125867e-07
Iter: 1387 loss: 2.6919119e-07
Iter: 1388 loss: 2.68973594e-07
Iter: 1389 loss: 2.68885486e-07
Iter: 1390 loss: 2.68789677e-07
Iter: 1391 loss: 2.68465669e-07
Iter: 1392 loss: 2.69018528e-07
Iter: 1393 loss: 2.68332883e-07
Iter: 1394 loss: 2.67966072e-07
Iter: 1395 loss: 2.68622841e-07
Iter: 1396 loss: 2.67817597e-07
Iter: 1397 loss: 2.67530311e-07
Iter: 1398 loss: 2.71851405e-07
Iter: 1399 loss: 2.675269e-07
Iter: 1400 loss: 2.673236e-07
Iter: 1401 loss: 2.67772805e-07
Iter: 1402 loss: 2.67221083e-07
Iter: 1403 loss: 2.67037308e-07
Iter: 1404 loss: 2.67879386e-07
Iter: 1405 loss: 2.67002747e-07
Iter: 1406 loss: 2.66786117e-07
Iter: 1407 loss: 2.67447291e-07
Iter: 1408 loss: 2.66719155e-07
Iter: 1409 loss: 2.66539246e-07
Iter: 1410 loss: 2.66277368e-07
Iter: 1411 loss: 2.66267136e-07
Iter: 1412 loss: 2.66064774e-07
Iter: 1413 loss: 2.66053831e-07
Iter: 1414 loss: 2.65843795e-07
Iter: 1415 loss: 2.65548636e-07
Iter: 1416 loss: 2.6553414e-07
Iter: 1417 loss: 2.65281585e-07
Iter: 1418 loss: 2.65282722e-07
Iter: 1419 loss: 2.65137317e-07
Iter: 1420 loss: 2.65042161e-07
Iter: 1421 loss: 2.649669e-07
Iter: 1422 loss: 2.64740265e-07
Iter: 1423 loss: 2.64794181e-07
Iter: 1424 loss: 2.64547623e-07
Iter: 1425 loss: 2.64248911e-07
Iter: 1426 loss: 2.64378087e-07
Iter: 1427 loss: 2.64021565e-07
Iter: 1428 loss: 2.63678686e-07
Iter: 1429 loss: 2.65919368e-07
Iter: 1430 loss: 2.63639947e-07
Iter: 1431 loss: 2.63362438e-07
Iter: 1432 loss: 2.64452268e-07
Iter: 1433 loss: 2.63284051e-07
Iter: 1434 loss: 2.63039226e-07
Iter: 1435 loss: 2.64801201e-07
Iter: 1436 loss: 2.63010179e-07
Iter: 1437 loss: 2.62816542e-07
Iter: 1438 loss: 2.62984059e-07
Iter: 1439 loss: 2.62721386e-07
Iter: 1440 loss: 2.62481279e-07
Iter: 1441 loss: 2.65224173e-07
Iter: 1442 loss: 2.62485514e-07
Iter: 1443 loss: 2.62373135e-07
Iter: 1444 loss: 2.62059643e-07
Iter: 1445 loss: 2.64853924e-07
Iter: 1446 loss: 2.62016158e-07
Iter: 1447 loss: 2.6177247e-07
Iter: 1448 loss: 2.61769344e-07
Iter: 1449 loss: 2.61548337e-07
Iter: 1450 loss: 2.61698716e-07
Iter: 1451 loss: 2.61423679e-07
Iter: 1452 loss: 2.61200739e-07
Iter: 1453 loss: 2.61829655e-07
Iter: 1454 loss: 2.61129856e-07
Iter: 1455 loss: 2.60929482e-07
Iter: 1456 loss: 2.62480398e-07
Iter: 1457 loss: 2.60921666e-07
Iter: 1458 loss: 2.60774868e-07
Iter: 1459 loss: 2.60493039e-07
Iter: 1460 loss: 2.63991467e-07
Iter: 1461 loss: 2.60435428e-07
Iter: 1462 loss: 2.60114774e-07
Iter: 1463 loss: 2.62919826e-07
Iter: 1464 loss: 2.60092577e-07
Iter: 1465 loss: 2.59839823e-07
Iter: 1466 loss: 2.60120032e-07
Iter: 1467 loss: 2.59685834e-07
Iter: 1468 loss: 2.59467157e-07
Iter: 1469 loss: 2.62316519e-07
Iter: 1470 loss: 2.59444448e-07
Iter: 1471 loss: 2.5923174e-07
Iter: 1472 loss: 2.59476934e-07
Iter: 1473 loss: 2.59114785e-07
Iter: 1474 loss: 2.5891336e-07
Iter: 1475 loss: 2.61595119e-07
Iter: 1476 loss: 2.58912763e-07
Iter: 1477 loss: 2.58753403e-07
Iter: 1478 loss: 2.58651795e-07
Iter: 1479 loss: 2.58600068e-07
Iter: 1480 loss: 2.58385342e-07
Iter: 1481 loss: 2.58112834e-07
Iter: 1482 loss: 2.58085976e-07
Iter: 1483 loss: 2.57783825e-07
Iter: 1484 loss: 2.59605088e-07
Iter: 1485 loss: 2.57741334e-07
Iter: 1486 loss: 2.57523766e-07
Iter: 1487 loss: 2.57517172e-07
Iter: 1488 loss: 2.57392088e-07
Iter: 1489 loss: 2.5749668e-07
Iter: 1490 loss: 2.57321773e-07
Iter: 1491 loss: 2.57182421e-07
Iter: 1492 loss: 2.57569582e-07
Iter: 1493 loss: 2.57154625e-07
Iter: 1494 loss: 2.56972214e-07
Iter: 1495 loss: 2.56885841e-07
Iter: 1496 loss: 2.56808278e-07
Iter: 1497 loss: 2.56538698e-07
Iter: 1498 loss: 2.56661593e-07
Iter: 1499 loss: 2.56341906e-07
Iter: 1500 loss: 2.56042199e-07
Iter: 1501 loss: 2.56451074e-07
Iter: 1502 loss: 2.55855213e-07
Iter: 1503 loss: 2.55519353e-07
Iter: 1504 loss: 2.58334779e-07
Iter: 1505 loss: 2.55494513e-07
Iter: 1506 loss: 2.55338705e-07
Iter: 1507 loss: 2.55331315e-07
Iter: 1508 loss: 2.55190287e-07
Iter: 1509 loss: 2.55374374e-07
Iter: 1510 loss: 2.5510667e-07
Iter: 1511 loss: 2.54899334e-07
Iter: 1512 loss: 2.55571337e-07
Iter: 1513 loss: 2.54844338e-07
Iter: 1514 loss: 2.54736563e-07
Iter: 1515 loss: 2.54602952e-07
Iter: 1516 loss: 2.54596443e-07
Iter: 1517 loss: 2.54374527e-07
Iter: 1518 loss: 2.5486068e-07
Iter: 1519 loss: 2.54295969e-07
Iter: 1520 loss: 2.54137433e-07
Iter: 1521 loss: 2.54127201e-07
Iter: 1522 loss: 2.53996205e-07
Iter: 1523 loss: 2.53759509e-07
Iter: 1524 loss: 2.53767809e-07
Iter: 1525 loss: 2.53559392e-07
Iter: 1526 loss: 2.55056449e-07
Iter: 1527 loss: 2.53547682e-07
Iter: 1528 loss: 2.53326903e-07
Iter: 1529 loss: 2.53675353e-07
Iter: 1530 loss: 2.53228e-07
Iter: 1531 loss: 2.52978509e-07
Iter: 1532 loss: 2.53227711e-07
Iter: 1533 loss: 2.52861298e-07
Iter: 1534 loss: 2.52625625e-07
Iter: 1535 loss: 2.52361446e-07
Iter: 1536 loss: 2.52326544e-07
Iter: 1537 loss: 2.51951576e-07
Iter: 1538 loss: 2.55437612e-07
Iter: 1539 loss: 2.51928213e-07
Iter: 1540 loss: 2.51676141e-07
Iter: 1541 loss: 2.53134516e-07
Iter: 1542 loss: 2.51627569e-07
Iter: 1543 loss: 2.51448427e-07
Iter: 1544 loss: 2.54447116e-07
Iter: 1545 loss: 2.51450246e-07
Iter: 1546 loss: 2.51301486e-07
Iter: 1547 loss: 2.51349917e-07
Iter: 1548 loss: 2.51205392e-07
Iter: 1549 loss: 2.50989785e-07
Iter: 1550 loss: 2.51187203e-07
Iter: 1551 loss: 2.50881e-07
Iter: 1552 loss: 2.5069329e-07
Iter: 1553 loss: 2.50517644e-07
Iter: 1554 loss: 2.50469895e-07
Iter: 1555 loss: 2.50254914e-07
Iter: 1556 loss: 2.50246899e-07
Iter: 1557 loss: 2.500596e-07
Iter: 1558 loss: 2.50501955e-07
Iter: 1559 loss: 2.49993718e-07
Iter: 1560 loss: 2.49859028e-07
Iter: 1561 loss: 2.49592688e-07
Iter: 1562 loss: 2.49597804e-07
Iter: 1563 loss: 2.49353945e-07
Iter: 1564 loss: 2.52566224e-07
Iter: 1565 loss: 2.49349426e-07
Iter: 1566 loss: 2.49162269e-07
Iter: 1567 loss: 2.49588709e-07
Iter: 1568 loss: 2.49075583e-07
Iter: 1569 loss: 2.48844287e-07
Iter: 1570 loss: 2.49003335e-07
Iter: 1571 loss: 2.48733642e-07
Iter: 1572 loss: 2.48491858e-07
Iter: 1573 loss: 2.48542449e-07
Iter: 1574 loss: 2.48317377e-07
Iter: 1575 loss: 2.48036486e-07
Iter: 1576 loss: 2.49100623e-07
Iter: 1577 loss: 2.47975038e-07
Iter: 1578 loss: 2.47789899e-07
Iter: 1579 loss: 2.47763865e-07
Iter: 1580 loss: 2.47604532e-07
Iter: 1581 loss: 2.47538139e-07
Iter: 1582 loss: 2.47462651e-07
Iter: 1583 loss: 2.47247584e-07
Iter: 1584 loss: 2.48047741e-07
Iter: 1585 loss: 2.47219589e-07
Iter: 1586 loss: 2.47003072e-07
Iter: 1587 loss: 2.46906069e-07
Iter: 1588 loss: 2.46832769e-07
Iter: 1589 loss: 2.46558955e-07
Iter: 1590 loss: 2.46997672e-07
Iter: 1591 loss: 2.46412242e-07
Iter: 1592 loss: 2.46254416e-07
Iter: 1593 loss: 2.46253506e-07
Iter: 1594 loss: 2.46101308e-07
Iter: 1595 loss: 2.45912304e-07
Iter: 1596 loss: 2.45894569e-07
Iter: 1597 loss: 2.45670407e-07
Iter: 1598 loss: 2.46055e-07
Iter: 1599 loss: 2.45579031e-07
Iter: 1600 loss: 2.4538096e-07
Iter: 1601 loss: 2.47770373e-07
Iter: 1602 loss: 2.45378345e-07
Iter: 1603 loss: 2.45183685e-07
Iter: 1604 loss: 2.4504206e-07
Iter: 1605 loss: 2.44972142e-07
Iter: 1606 loss: 2.44756876e-07
Iter: 1607 loss: 2.45740296e-07
Iter: 1608 loss: 2.44743092e-07
Iter: 1609 loss: 2.44523847e-07
Iter: 1610 loss: 2.44427383e-07
Iter: 1611 loss: 2.44346779e-07
Iter: 1612 loss: 2.44209104e-07
Iter: 1613 loss: 2.44200038e-07
Iter: 1614 loss: 2.44032435e-07
Iter: 1615 loss: 2.44080212e-07
Iter: 1616 loss: 2.43906754e-07
Iter: 1617 loss: 2.4380077e-07
Iter: 1618 loss: 2.43978576e-07
Iter: 1619 loss: 2.4373557e-07
Iter: 1620 loss: 2.43570128e-07
Iter: 1621 loss: 2.43766e-07
Iter: 1622 loss: 2.43485914e-07
Iter: 1623 loss: 2.43282301e-07
Iter: 1624 loss: 2.4358377e-07
Iter: 1625 loss: 2.43219716e-07
Iter: 1626 loss: 2.43051886e-07
Iter: 1627 loss: 2.44013364e-07
Iter: 1628 loss: 2.4303651e-07
Iter: 1629 loss: 2.4284634e-07
Iter: 1630 loss: 2.43346108e-07
Iter: 1631 loss: 2.42803736e-07
Iter: 1632 loss: 2.42666488e-07
Iter: 1633 loss: 2.42476801e-07
Iter: 1634 loss: 2.42449858e-07
Iter: 1635 loss: 2.42235956e-07
Iter: 1636 loss: 2.43895556e-07
Iter: 1637 loss: 2.4221211e-07
Iter: 1638 loss: 2.41969644e-07
Iter: 1639 loss: 2.42507326e-07
Iter: 1640 loss: 2.41887619e-07
Iter: 1641 loss: 2.41668033e-07
Iter: 1642 loss: 2.41807129e-07
Iter: 1643 loss: 2.4153465e-07
Iter: 1644 loss: 2.41317252e-07
Iter: 1645 loss: 2.41322454e-07
Iter: 1646 loss: 2.41161729e-07
Iter: 1647 loss: 2.40987106e-07
Iter: 1648 loss: 2.4098108e-07
Iter: 1649 loss: 2.4084855e-07
Iter: 1650 loss: 2.42071621e-07
Iter: 1651 loss: 2.40840222e-07
Iter: 1652 loss: 2.40739098e-07
Iter: 1653 loss: 2.40531222e-07
Iter: 1654 loss: 2.43088806e-07
Iter: 1655 loss: 2.40503084e-07
Iter: 1656 loss: 2.40309646e-07
Iter: 1657 loss: 2.41600191e-07
Iter: 1658 loss: 2.40283356e-07
Iter: 1659 loss: 2.40047939e-07
Iter: 1660 loss: 2.40561917e-07
Iter: 1661 loss: 2.39991692e-07
Iter: 1662 loss: 2.39806894e-07
Iter: 1663 loss: 2.40204628e-07
Iter: 1664 loss: 2.3975349e-07
Iter: 1665 loss: 2.39572927e-07
Iter: 1666 loss: 2.41141322e-07
Iter: 1667 loss: 2.39559427e-07
Iter: 1668 loss: 2.39398105e-07
Iter: 1669 loss: 2.39403249e-07
Iter: 1670 loss: 2.39290159e-07
Iter: 1671 loss: 2.39095073e-07
Iter: 1672 loss: 2.38834843e-07
Iter: 1673 loss: 2.38818473e-07
Iter: 1674 loss: 2.3865158e-07
Iter: 1675 loss: 2.38590957e-07
Iter: 1676 loss: 2.38477583e-07
Iter: 1677 loss: 2.38308132e-07
Iter: 1678 loss: 2.383008e-07
Iter: 1679 loss: 2.381028e-07
Iter: 1680 loss: 2.38168553e-07
Iter: 1681 loss: 2.37970966e-07
Iter: 1682 loss: 2.37806617e-07
Iter: 1683 loss: 2.37790189e-07
Iter: 1684 loss: 2.37664651e-07
Iter: 1685 loss: 2.38296337e-07
Iter: 1686 loss: 2.37662462e-07
Iter: 1687 loss: 2.375957e-07
Iter: 1688 loss: 2.37411498e-07
Iter: 1689 loss: 2.39404585e-07
Iter: 1690 loss: 2.37412024e-07
Iter: 1691 loss: 2.37214948e-07
Iter: 1692 loss: 2.38613779e-07
Iter: 1693 loss: 2.37204603e-07
Iter: 1694 loss: 2.37022178e-07
Iter: 1695 loss: 2.37838449e-07
Iter: 1696 loss: 2.3699107e-07
Iter: 1697 loss: 2.36883565e-07
Iter: 1698 loss: 2.36829521e-07
Iter: 1699 loss: 2.36753124e-07
Iter: 1700 loss: 2.36620494e-07
Iter: 1701 loss: 2.3662065e-07
Iter: 1702 loss: 2.36498e-07
Iter: 1703 loss: 2.36339901e-07
Iter: 1704 loss: 2.36341037e-07
Iter: 1705 loss: 2.3613174e-07
Iter: 1706 loss: 2.36772593e-07
Iter: 1707 loss: 2.36083764e-07
Iter: 1708 loss: 2.3590016e-07
Iter: 1709 loss: 2.36854476e-07
Iter: 1710 loss: 2.35885977e-07
Iter: 1711 loss: 2.35698081e-07
Iter: 1712 loss: 2.36026e-07
Iter: 1713 loss: 2.3560888e-07
Iter: 1714 loss: 2.3545536e-07
Iter: 1715 loss: 2.35540654e-07
Iter: 1716 loss: 2.35397081e-07
Iter: 1717 loss: 2.3524953e-07
Iter: 1718 loss: 2.35231141e-07
Iter: 1719 loss: 2.35154957e-07
Iter: 1720 loss: 2.35077522e-07
Iter: 1721 loss: 2.35061322e-07
Iter: 1722 loss: 2.34963437e-07
Iter: 1723 loss: 2.34761941e-07
Iter: 1724 loss: 2.34766702e-07
Iter: 1725 loss: 2.34603078e-07
Iter: 1726 loss: 2.34603846e-07
Iter: 1727 loss: 2.34489704e-07
Iter: 1728 loss: 2.34681579e-07
Iter: 1729 loss: 2.34440151e-07
Iter: 1730 loss: 2.34282467e-07
Iter: 1731 loss: 2.34423482e-07
Iter: 1732 loss: 2.34208187e-07
Iter: 1733 loss: 2.34096618e-07
Iter: 1734 loss: 2.34089796e-07
Iter: 1735 loss: 2.3401023e-07
Iter: 1736 loss: 2.3382475e-07
Iter: 1737 loss: 2.36428477e-07
Iter: 1738 loss: 2.33808478e-07
Iter: 1739 loss: 2.33593042e-07
Iter: 1740 loss: 2.34274268e-07
Iter: 1741 loss: 2.33528198e-07
Iter: 1742 loss: 2.33355991e-07
Iter: 1743 loss: 2.35768397e-07
Iter: 1744 loss: 2.33351727e-07
Iter: 1745 loss: 2.33242957e-07
Iter: 1746 loss: 2.33285505e-07
Iter: 1747 loss: 2.3316062e-07
Iter: 1748 loss: 2.3300008e-07
Iter: 1749 loss: 2.33307588e-07
Iter: 1750 loss: 2.32952573e-07
Iter: 1751 loss: 2.3275264e-07
Iter: 1752 loss: 2.34590146e-07
Iter: 1753 loss: 2.32754132e-07
Iter: 1754 loss: 2.32692386e-07
Iter: 1755 loss: 2.32561774e-07
Iter: 1756 loss: 2.35243832e-07
Iter: 1757 loss: 2.32552054e-07
Iter: 1758 loss: 2.3239096e-07
Iter: 1759 loss: 2.32788409e-07
Iter: 1760 loss: 2.32326386e-07
Iter: 1761 loss: 2.32150768e-07
Iter: 1762 loss: 2.32850624e-07
Iter: 1763 loss: 2.32124009e-07
Iter: 1764 loss: 2.31950565e-07
Iter: 1765 loss: 2.32979062e-07
Iter: 1766 loss: 2.31955809e-07
Iter: 1767 loss: 2.31840318e-07
Iter: 1768 loss: 2.31758577e-07
Iter: 1769 loss: 2.31699374e-07
Iter: 1770 loss: 2.31540824e-07
Iter: 1771 loss: 2.3154459e-07
Iter: 1772 loss: 2.31443281e-07
Iter: 1773 loss: 2.31444631e-07
Iter: 1774 loss: 2.31365874e-07
Iter: 1775 loss: 2.31206499e-07
Iter: 1776 loss: 2.31062486e-07
Iter: 1777 loss: 2.31025098e-07
Iter: 1778 loss: 2.30926958e-07
Iter: 1779 loss: 2.30881341e-07
Iter: 1780 loss: 2.30804886e-07
Iter: 1781 loss: 2.30762936e-07
Iter: 1782 loss: 2.30717148e-07
Iter: 1783 loss: 2.30603519e-07
Iter: 1784 loss: 2.32178166e-07
Iter: 1785 loss: 2.30599056e-07
Iter: 1786 loss: 2.30487217e-07
Iter: 1787 loss: 2.3043151e-07
Iter: 1788 loss: 2.30392772e-07
Iter: 1789 loss: 2.30292088e-07
Iter: 1790 loss: 2.30175232e-07
Iter: 1791 loss: 2.30178529e-07
Iter: 1792 loss: 2.29953528e-07
Iter: 1793 loss: 2.304524e-07
Iter: 1794 loss: 2.29904259e-07
Iter: 1795 loss: 2.29742014e-07
Iter: 1796 loss: 2.32232651e-07
Iter: 1797 loss: 2.29735235e-07
Iter: 1798 loss: 2.29609725e-07
Iter: 1799 loss: 2.29696383e-07
Iter: 1800 loss: 2.29519429e-07
Iter: 1801 loss: 2.293731e-07
Iter: 1802 loss: 2.29533526e-07
Iter: 1803 loss: 2.29308654e-07
Iter: 1804 loss: 2.29090816e-07
Iter: 1805 loss: 2.3052641e-07
Iter: 1806 loss: 2.29078779e-07
Iter: 1807 loss: 2.28960801e-07
Iter: 1808 loss: 2.28909315e-07
Iter: 1809 loss: 2.28842239e-07
Iter: 1810 loss: 2.2869159e-07
Iter: 1811 loss: 2.2911459e-07
Iter: 1812 loss: 2.28621829e-07
Iter: 1813 loss: 2.28443966e-07
Iter: 1814 loss: 2.2973623e-07
Iter: 1815 loss: 2.28436448e-07
Iter: 1816 loss: 2.28303463e-07
Iter: 1817 loss: 2.28358687e-07
Iter: 1818 loss: 2.28196214e-07
Iter: 1819 loss: 2.28079372e-07
Iter: 1820 loss: 2.28079728e-07
Iter: 1821 loss: 2.27973658e-07
Iter: 1822 loss: 2.278219e-07
Iter: 1823 loss: 2.30723714e-07
Iter: 1824 loss: 2.27825453e-07
Iter: 1825 loss: 2.27638111e-07
Iter: 1826 loss: 2.2810417e-07
Iter: 1827 loss: 2.27575811e-07
Iter: 1828 loss: 2.27399283e-07
Iter: 1829 loss: 2.27269652e-07
Iter: 1830 loss: 2.27209043e-07
Iter: 1831 loss: 2.27067289e-07
Iter: 1832 loss: 2.27021786e-07
Iter: 1833 loss: 2.26912192e-07
Iter: 1834 loss: 2.27064845e-07
Iter: 1835 loss: 2.26855292e-07
Iter: 1836 loss: 2.26731942e-07
Iter: 1837 loss: 2.26822e-07
Iter: 1838 loss: 2.26649789e-07
Iter: 1839 loss: 2.2643809e-07
Iter: 1840 loss: 2.27538152e-07
Iter: 1841 loss: 2.26384955e-07
Iter: 1842 loss: 2.26279255e-07
Iter: 1843 loss: 2.26187865e-07
Iter: 1844 loss: 2.26151315e-07
Iter: 1845 loss: 2.25968876e-07
Iter: 1846 loss: 2.26486819e-07
Iter: 1847 loss: 2.25920644e-07
Iter: 1848 loss: 2.25741289e-07
Iter: 1849 loss: 2.27250624e-07
Iter: 1850 loss: 2.25719376e-07
Iter: 1851 loss: 2.2558288e-07
Iter: 1852 loss: 2.25811604e-07
Iter: 1853 loss: 2.25522371e-07
Iter: 1854 loss: 2.25400015e-07
Iter: 1855 loss: 2.25401095e-07
Iter: 1856 loss: 2.25334261e-07
Iter: 1857 loss: 2.25151865e-07
Iter: 1858 loss: 2.27542742e-07
Iter: 1859 loss: 2.25137427e-07
Iter: 1860 loss: 2.24942696e-07
Iter: 1861 loss: 2.2536048e-07
Iter: 1862 loss: 2.24898173e-07
Iter: 1863 loss: 2.24694304e-07
Iter: 1864 loss: 2.24954235e-07
Iter: 1865 loss: 2.24599177e-07
Iter: 1866 loss: 2.24389197e-07
Iter: 1867 loss: 2.26194302e-07
Iter: 1868 loss: 2.24392977e-07
Iter: 1869 loss: 2.24204598e-07
Iter: 1870 loss: 2.2507561e-07
Iter: 1871 loss: 2.24188625e-07
Iter: 1872 loss: 2.24073048e-07
Iter: 1873 loss: 2.24090257e-07
Iter: 1874 loss: 2.23983093e-07
Iter: 1875 loss: 2.23793649e-07
Iter: 1876 loss: 2.24831865e-07
Iter: 1877 loss: 2.23772972e-07
Iter: 1878 loss: 2.23647078e-07
Iter: 1879 loss: 2.23623914e-07
Iter: 1880 loss: 2.23561045e-07
Iter: 1881 loss: 2.23397e-07
Iter: 1882 loss: 2.23222045e-07
Iter: 1883 loss: 2.23209213e-07
Iter: 1884 loss: 2.22922182e-07
Iter: 1885 loss: 2.250964e-07
Iter: 1886 loss: 2.2292032e-07
Iter: 1887 loss: 2.22809419e-07
Iter: 1888 loss: 2.22788941e-07
Iter: 1889 loss: 2.22680171e-07
Iter: 1890 loss: 2.22575949e-07
Iter: 1891 loss: 2.22524733e-07
Iter: 1892 loss: 2.22389104e-07
Iter: 1893 loss: 2.22227612e-07
Iter: 1894 loss: 2.22213714e-07
Iter: 1895 loss: 2.21991598e-07
Iter: 1896 loss: 2.23348707e-07
Iter: 1897 loss: 2.21975185e-07
Iter: 1898 loss: 2.21804811e-07
Iter: 1899 loss: 2.22743097e-07
Iter: 1900 loss: 2.21801955e-07
Iter: 1901 loss: 2.21630444e-07
Iter: 1902 loss: 2.22071662e-07
Iter: 1903 loss: 2.21573814e-07
Iter: 1904 loss: 2.2145116e-07
Iter: 1905 loss: 2.21376354e-07
Iter: 1906 loss: 2.21317e-07
Iter: 1907 loss: 2.21224184e-07
Iter: 1908 loss: 2.2122417e-07
Iter: 1909 loss: 2.21114888e-07
Iter: 1910 loss: 2.20981349e-07
Iter: 1911 loss: 2.20988056e-07
Iter: 1912 loss: 2.2083907e-07
Iter: 1913 loss: 2.21124651e-07
Iter: 1914 loss: 2.20770417e-07
Iter: 1915 loss: 2.20586173e-07
Iter: 1916 loss: 2.20500283e-07
Iter: 1917 loss: 2.20424454e-07
Iter: 1918 loss: 2.20339061e-07
Iter: 1919 loss: 2.20300393e-07
Iter: 1920 loss: 2.20180254e-07
Iter: 1921 loss: 2.20528847e-07
Iter: 1922 loss: 2.20150156e-07
Iter: 1923 loss: 2.20024901e-07
Iter: 1924 loss: 2.2021861e-07
Iter: 1925 loss: 2.19948632e-07
Iter: 1926 loss: 2.19822113e-07
Iter: 1927 loss: 2.19835769e-07
Iter: 1928 loss: 2.19743498e-07
Iter: 1929 loss: 2.19629584e-07
Iter: 1930 loss: 2.1969251e-07
Iter: 1931 loss: 2.19566374e-07
Iter: 1932 loss: 2.19416989e-07
Iter: 1933 loss: 2.20903132e-07
Iter: 1934 loss: 2.19406914e-07
Iter: 1935 loss: 2.1928517e-07
Iter: 1936 loss: 2.19144709e-07
Iter: 1937 loss: 2.19139352e-07
Iter: 1938 loss: 2.1901181e-07
Iter: 1939 loss: 2.20791364e-07
Iter: 1940 loss: 2.19012179e-07
Iter: 1941 loss: 2.18881539e-07
Iter: 1942 loss: 2.19094886e-07
Iter: 1943 loss: 2.18812573e-07
Iter: 1944 loss: 2.18655643e-07
Iter: 1945 loss: 2.18697622e-07
Iter: 1946 loss: 2.18554732e-07
Iter: 1947 loss: 2.18405972e-07
Iter: 1948 loss: 2.18307662e-07
Iter: 1949 loss: 2.18233367e-07
Iter: 1950 loss: 2.18054993e-07
Iter: 1951 loss: 2.18055661e-07
Iter: 1952 loss: 2.17914717e-07
Iter: 1953 loss: 2.18775739e-07
Iter: 1954 loss: 2.17909417e-07
Iter: 1955 loss: 2.17807113e-07
Iter: 1956 loss: 2.18523681e-07
Iter: 1957 loss: 2.17794025e-07
Iter: 1958 loss: 2.17712511e-07
Iter: 1959 loss: 2.17594319e-07
Iter: 1960 loss: 2.17588664e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2
+ date
Thu Oct 22 05:56:35 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/500_500_500_500_1 --function f1 --psi -2 --phi 2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121e84840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121f55620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121e84c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121ed9ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121eb3598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121eb3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121d6eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121e20ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121e26048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121de5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121e1a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121cd4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121cd4bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121c89400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121c62840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121c79b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121d352f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121d01d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa121d01ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa100747ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa10078b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa100788598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1007028c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa100792840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa100792510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0d84b0ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0d84b0d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1007b1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0d847d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1006a8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0d83cb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0d83b27b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0d83b2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0d8397620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0d840c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0d8315620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.047459226
test_loss: 0.07498261
train_loss: 0.019901343
test_loss: 0.020374492
train_loss: 0.014749246
test_loss: 0.013421169
train_loss: 0.014731068
test_loss: 0.01592682
train_loss: 0.008885143
test_loss: 0.010798569
train_loss: 0.007332199
test_loss: 0.009155247
train_loss: 0.011767076
test_loss: 0.009176186
train_loss: 0.009910053
test_loss: 0.011937609
train_loss: 0.008088966
test_loss: 0.00987278
train_loss: 0.006010667
test_loss: 0.008773493
train_loss: 0.006725258
test_loss: 0.008978837
train_loss: 0.0074821394
test_loss: 0.009014017
train_loss: 0.0067687873
test_loss: 0.008053909
train_loss: 0.008646077
test_loss: 0.0076923934
train_loss: 0.0060329507
test_loss: 0.0076730503
train_loss: 0.0066576605
test_loss: 0.008248794
train_loss: 0.008407664
test_loss: 0.007631971
train_loss: 0.0067148553
test_loss: 0.0072004246
train_loss: 0.0061042737
test_loss: 0.008506036
train_loss: 0.0052290917
test_loss: 0.0075130207
train_loss: 0.005473051
test_loss: 0.007257228
train_loss: 0.0057593696
test_loss: 0.0074842656
train_loss: 0.00649796
test_loss: 0.007557237
train_loss: 0.005220008
test_loss: 0.0069280276
train_loss: 0.0062800846
test_loss: 0.00857374
train_loss: 0.0058237338
test_loss: 0.0075323526
train_loss: 0.009469199
test_loss: 0.011588732
train_loss: 0.008449958
test_loss: 0.009264842
train_loss: 0.0054791737
test_loss: 0.007988791
train_loss: 0.0073592924
test_loss: 0.008206336
train_loss: 0.004915649
test_loss: 0.007227633
train_loss: 0.0054222136
test_loss: 0.0074348836
train_loss: 0.0047886954
test_loss: 0.006975332
train_loss: 0.0056048823
test_loss: 0.0074406806
train_loss: 0.005486211
test_loss: 0.007101094
train_loss: 0.004804915
test_loss: 0.007505911
train_loss: 0.009162558
test_loss: 0.009773271
train_loss: 0.0056490395
test_loss: 0.0076411264
train_loss: 0.0058574243
test_loss: 0.008075104
train_loss: 0.0048604887
test_loss: 0.0070633884
train_loss: 0.0045155287
test_loss: 0.006973753
train_loss: 0.0049611293
test_loss: 0.007199644
train_loss: 0.0053650495
test_loss: 0.0073926398
train_loss: 0.004773589
test_loss: 0.006882067
train_loss: 0.005997895
test_loss: 0.0074549764
train_loss: 0.004406448
test_loss: 0.007300377
train_loss: 0.0050123218
test_loss: 0.007161011
train_loss: 0.0045446483
test_loss: 0.007513151
train_loss: 0.0045506526
test_loss: 0.007029073
train_loss: 0.0073039797
test_loss: 0.008203929
train_loss: 0.0066350047
test_loss: 0.007808255
train_loss: 0.004436642
test_loss: 0.007339994
train_loss: 0.0059997602
test_loss: 0.006716798
train_loss: 0.0065245572
test_loss: 0.007564781
train_loss: 0.004762973
test_loss: 0.007361824
train_loss: 0.009170461
test_loss: 0.0069200094
train_loss: 0.0053250827
test_loss: 0.00697967
train_loss: 0.005694173
test_loss: 0.0072011445
train_loss: 0.0043890104
test_loss: 0.0071073323
train_loss: 0.005366995
test_loss: 0.007569885
train_loss: 0.005663952
test_loss: 0.007591526
train_loss: 0.006826476
test_loss: 0.0075954683
train_loss: 0.005307916
test_loss: 0.0069631124
train_loss: 0.0045727557
test_loss: 0.0070618647
train_loss: 0.0044981977
test_loss: 0.0072876136
train_loss: 0.0059483303
test_loss: 0.0075657195
train_loss: 0.005137814
test_loss: 0.007224777
train_loss: 0.0046669026
test_loss: 0.0070740753
train_loss: 0.00696789
test_loss: 0.0082686925
train_loss: 0.005422023
test_loss: 0.006875955
train_loss: 0.006447783
test_loss: 0.0074508614
train_loss: 0.005131937
test_loss: 0.0070730364
train_loss: 0.0041608782
test_loss: 0.006370213
train_loss: 0.004408721
test_loss: 0.006983787
train_loss: 0.004802782
test_loss: 0.006688767
train_loss: 0.004781372
test_loss: 0.006993084
train_loss: 0.005708108
test_loss: 0.0075963615
train_loss: 0.0067438083
test_loss: 0.0070267413
train_loss: 0.004947271
test_loss: 0.006828992
train_loss: 0.005254709
test_loss: 0.007922598
train_loss: 0.004873296
test_loss: 0.0068897572
train_loss: 0.005409097
test_loss: 0.0072797984
train_loss: 0.004978317
test_loss: 0.0072803157
train_loss: 0.005134442
test_loss: 0.007223545
train_loss: 0.004551649
test_loss: 0.007012341
train_loss: 0.0048239613
test_loss: 0.0066987574
train_loss: 0.005135241
test_loss: 0.007280457
train_loss: 0.0039464952
test_loss: 0.006689075
train_loss: 0.0040533347
test_loss: 0.006448055
train_loss: 0.0043328777
test_loss: 0.007137474
train_loss: 0.0052600745
test_loss: 0.006820039
train_loss: 0.0044002086
test_loss: 0.00715558
train_loss: 0.0041977833
test_loss: 0.006663484
train_loss: 0.004454145
test_loss: 0.0070488695
train_loss: 0.0054227496
test_loss: 0.0073571964
train_loss: 0.0055918354
test_loss: 0.0071169296
train_loss: 0.0048795072
test_loss: 0.0068525807
train_loss: 0.0039686738
test_loss: 0.0068126824
train_loss: 0.004449658
test_loss: 0.0065762145
train_loss: 0.004208096
test_loss: 0.0066748555
train_loss: 0.004307173
test_loss: 0.0067478213
train_loss: 0.00507743
test_loss: 0.0068384786
train_loss: 0.0050299494
test_loss: 0.0073538646
train_loss: 0.0046570883
test_loss: 0.006882008
train_loss: 0.004417263
test_loss: 0.0069129313
train_loss: 0.0045834845
test_loss: 0.0065801665
train_loss: 0.004778355
test_loss: 0.0067721372
train_loss: 0.0045109447
test_loss: 0.007177063
train_loss: 0.004570854
test_loss: 0.0068354043
train_loss: 0.007826688
test_loss: 0.0072563547
train_loss: 0.0041444604
test_loss: 0.0066821757
train_loss: 0.0045394097
test_loss: 0.0068874448
train_loss: 0.0046822075
test_loss: 0.0074411035
train_loss: 0.0057362476
test_loss: 0.0069345077
train_loss: 0.0050842855
test_loss: 0.0073131425
train_loss: 0.0045103882
test_loss: 0.006520089
train_loss: 0.0050209365
test_loss: 0.0069079166
train_loss: 0.0048670047
test_loss: 0.0070711044
train_loss: 0.004944325
test_loss: 0.006946989
train_loss: 0.0044266386
test_loss: 0.0069317254
train_loss: 0.00446723
test_loss: 0.007017036
train_loss: 0.0049026646
test_loss: 0.0068944776
train_loss: 0.0043228394
test_loss: 0.0070306608
train_loss: 0.004966031
test_loss: 0.007438761
train_loss: 0.004475729
test_loss: 0.0071540982
train_loss: 0.0042124884
test_loss: 0.0063268146
train_loss: 0.005376838
test_loss: 0.0067832144
train_loss: 0.0042754887
test_loss: 0.006796958
train_loss: 0.004622932
test_loss: 0.0074621863
train_loss: 0.005062681
test_loss: 0.006994069
train_loss: 0.004041097
test_loss: 0.007250849
train_loss: 0.0049046376
test_loss: 0.0070914347
train_loss: 0.006381604
test_loss: 0.0070999353
train_loss: 0.0047543272
test_loss: 0.007088714
train_loss: 0.0053029126
test_loss: 0.0071030413
train_loss: 0.0049104914
test_loss: 0.007158255
train_loss: 0.004840047
test_loss: 0.0069083353
train_loss: 0.0041313046
test_loss: 0.0066935658
train_loss: 0.004852601
test_loss: 0.007354316
train_loss: 0.004498144
test_loss: 0.006664178
train_loss: 0.0044048005
test_loss: 0.006859377
train_loss: 0.0039814813
test_loss: 0.0068475874
train_loss: 0.005260129
test_loss: 0.006956658
train_loss: 0.004403862
test_loss: 0.0068928436
train_loss: 0.004623225
test_loss: 0.00663695
train_loss: 0.004244698
test_loss: 0.006902341
train_loss: 0.0044269436
test_loss: 0.00716388
train_loss: 0.004377556
test_loss: 0.0067488314
train_loss: 0.004644106
test_loss: 0.007305548
train_loss: 0.004571777
test_loss: 0.0071766265
train_loss: 0.004739026
test_loss: 0.006795971
train_loss: 0.0047811307
test_loss: 0.0065524112
train_loss: 0.0044588684
test_loss: 0.006416591
train_loss: 0.0047865855
test_loss: 0.007165412
train_loss: 0.005713154
test_loss: 0.007983659
train_loss: 0.00449084
test_loss: 0.006782844
train_loss: 0.0045696697
test_loss: 0.0073155095
train_loss: 0.0043531265
test_loss: 0.00727532
train_loss: 0.004293698
test_loss: 0.006638535
train_loss: 0.0047041783
test_loss: 0.007219642
train_loss: 0.0053973524
test_loss: 0.007540104
train_loss: 0.0038804528
test_loss: 0.0067933826
train_loss: 0.004198615
test_loss: 0.0070621823
train_loss: 0.0046875137
test_loss: 0.0072502345
train_loss: 0.0043144473
test_loss: 0.0069988878
train_loss: 0.0039210604
test_loss: 0.006766015
train_loss: 0.004461902
test_loss: 0.0070747896
train_loss: 0.005423391
test_loss: 0.0067303996
train_loss: 0.0053754807
test_loss: 0.0067198896
train_loss: 0.00448303
test_loss: 0.0071842354
train_loss: 0.0050983247
test_loss: 0.0069373203
train_loss: 0.004481165
test_loss: 0.0068230922
train_loss: 0.0052910154
test_loss: 0.006885603
train_loss: 0.004915149
test_loss: 0.007305259
train_loss: 0.0042797523
test_loss: 0.007400291
train_loss: 0.0043364987
test_loss: 0.0069911503
train_loss: 0.00417594
test_loss: 0.0073175775
train_loss: 0.00510659
test_loss: 0.0075260405
train_loss: 0.0044037756
test_loss: 0.006950138
train_loss: 0.004555614
test_loss: 0.007138984
train_loss: 0.0041524735
test_loss: 0.006965802
train_loss: 0.004419124
test_loss: 0.00725652
train_loss: 0.0041700234
test_loss: 0.0069662915
train_loss: 0.004511474
test_loss: 0.007054055
train_loss: 0.0045933817
test_loss: 0.006990029
train_loss: 0.0046569025
test_loss: 0.0073354472
train_loss: 0.0045385007
test_loss: 0.0067647444
train_loss: 0.004366952
test_loss: 0.0073273825
train_loss: 0.004124235
test_loss: 0.0070658326
train_loss: 0.004616186
test_loss: 0.0070415577
train_loss: 0.0044994997
test_loss: 0.0066829356
train_loss: 0.005959721
test_loss: 0.007200857
train_loss: 0.0052468344
test_loss: 0.0070901643
train_loss: 0.0044817845
test_loss: 0.0069246264
train_loss: 0.0044238763
test_loss: 0.0073176483
train_loss: 0.0051476522
test_loss: 0.0067911306
train_loss: 0.0045922124
test_loss: 0.007245211
train_loss: 0.004811979
test_loss: 0.007182583
train_loss: 0.004267414
test_loss: 0.0068039857
train_loss: 0.003864801
test_loss: 0.0068676216
train_loss: 0.0042800177
test_loss: 0.0072692377
train_loss: 0.004381275
test_loss: 0.0075881225
train_loss: 0.0040864414
test_loss: 0.006794746
train_loss: 0.004052385
test_loss: 0.006635266
train_loss: 0.00435174
test_loss: 0.0071283896
train_loss: 0.004569515
test_loss: 0.0072499393
train_loss: 0.003740293
test_loss: 0.0068591526
train_loss: 0.0044267313
test_loss: 0.0072366367
train_loss: 0.004196075
test_loss: 0.007214839
train_loss: 0.0043132394
test_loss: 0.007122956
train_loss: 0.0043351795
test_loss: 0.006992171
train_loss: 0.004470968
test_loss: 0.0073031113
train_loss: 0.0038170882
test_loss: 0.0068007987
train_loss: 0.004079196
test_loss: 0.007178517
train_loss: 0.0041296096
test_loss: 0.006865901
train_loss: 0.0037338657
test_loss: 0.006693044
train_loss: 0.0048627127
test_loss: 0.007552638
train_loss: 0.004901892
test_loss: 0.007547493
train_loss: 0.004090146
test_loss: 0.0067942333
train_loss: 0.0042032762
test_loss: 0.0071026217
train_loss: 0.0040704925
test_loss: 0.006849945
train_loss: 0.0039415257
test_loss: 0.0064722872
train_loss: 0.004599441
test_loss: 0.007468884
train_loss: 0.00477109
test_loss: 0.0070373085
train_loss: 0.0036921948
test_loss: 0.006809616
train_loss: 0.004852244
test_loss: 0.006905525
train_loss: 0.0044557736
test_loss: 0.006915952
train_loss: 0.004032565
test_loss: 0.007479548
train_loss: 0.00471956
test_loss: 0.007101896
train_loss: 0.0041481135
test_loss: 0.0065781637
train_loss: 0.0050718854
test_loss: 0.006998614
train_loss: 0.0039081112
test_loss: 0.0070338463
train_loss: 0.0040758937
test_loss: 0.0070575117
train_loss: 0.004336266
test_loss: 0.0073656403
train_loss: 0.0046357578
test_loss: 0.007245485
train_loss: 0.004517616
test_loss: 0.0068465006
train_loss: 0.0037265432
test_loss: 0.0067761554
train_loss: 0.0044686766
test_loss: 0.007035938
train_loss: 0.00399327
test_loss: 0.006813845
train_loss: 0.0049289493
test_loss: 0.0075268247
train_loss: 0.0040298454
test_loss: 0.0071100043
train_loss: 0.0039634723
test_loss: 0.00712408
train_loss: 0.003966598
test_loss: 0.006667271
train_loss: 0.0054055746
test_loss: 0.006919932
train_loss: 0.0043340847
test_loss: 0.007363142
train_loss: 0.0039925426
test_loss: 0.006778737
train_loss: 0.0041062357
test_loss: 0.007088351
train_loss: 0.0037414317
test_loss: 0.0070458963
train_loss: 0.004541257
test_loss: 0.0072600497
train_loss: 0.004582476
test_loss: 0.007772315
train_loss: 0.0036521936
test_loss: 0.0068218773
train_loss: 0.0034446842
test_loss: 0.006754163
train_loss: 0.004443805
test_loss: 0.0070996047
train_loss: 0.003916485
test_loss: 0.0070316377
train_loss: 0.004060298
test_loss: 0.0066092676
train_loss: 0.0036601515
test_loss: 0.006920807
train_loss: 0.00405593
test_loss: 0.0069055934
train_loss: 0.005472741
test_loss: 0.0076086842
train_loss: 0.004383085
test_loss: 0.0067468644
train_loss: 0.0041659027
test_loss: 0.006919752
train_loss: 0.00415453
test_loss: 0.007069273
train_loss: 0.0058494518
test_loss: 0.007198525
train_loss: 0.0045915493
test_loss: 0.0073028905
train_loss: 0.004461417
test_loss: 0.0070485864
train_loss: 0.0042864745
test_loss: 0.006947502
train_loss: 0.003600045
test_loss: 0.006861653
train_loss: 0.004539444
test_loss: 0.0067197084
train_loss: 0.0038986672
test_loss: 0.0066895424
train_loss: 0.004481324
test_loss: 0.0067556
train_loss: 0.004322166
test_loss: 0.007217512
train_loss: 0.004426919
test_loss: 0.006699747
train_loss: 0.00396799
test_loss: 0.0071189045
train_loss: 0.004141463
test_loss: 0.007206226
train_loss: 0.0038132444
test_loss: 0.006747745
train_loss: 0.004979142
test_loss: 0.007396306
train_loss: 0.0051847138
test_loss: 0.007157702
train_loss: 0.0040506404
test_loss: 0.0071209115
train_loss: 0.003813301
test_loss: 0.007105048
train_loss: 0.0036799675
test_loss: 0.006717573
train_loss: 0.004238409
test_loss: 0.007137513
train_loss: 0.0041094823
test_loss: 0.006847211
train_loss: 0.00444594
test_loss: 0.00673312
train_loss: 0.004088276
test_loss: 0.0068857865
train_loss: 0.005083297
test_loss: 0.006907176
train_loss: 0.004074589
test_loss: 0.006993163
train_loss: 0.0042234296
test_loss: 0.0068425876
train_loss: 0.004196031
test_loss: 0.0067973477
train_loss: 0.0042901044
test_loss: 0.0064983703
train_loss: 0.0038137573
test_loss: 0.0069352393
train_loss: 0.0039695264
test_loss: 0.007036107
train_loss: 0.0036903683
test_loss: 0.0069222436
train_loss: 0.004086647
test_loss: 0.0067183427
train_loss: 0.004844309
test_loss: 0.0067038853
train_loss: 0.0043266295
test_loss: 0.0069934353
train_loss: 0.0041588205
test_loss: 0.006905506
train_loss: 0.003897829
test_loss: 0.0072914716
train_loss: 0.004146398
test_loss: 0.0068362276
train_loss: 0.0037052382
test_loss: 0.0069847456
train_loss: 0.0039120875
test_loss: 0.00705019
train_loss: 0.003986001
test_loss: 0.007485031
train_loss: 0.004097474
test_loss: 0.006676879
train_loss: 0.003988022
test_loss: 0.0069945017
train_loss: 0.0042438004
test_loss: 0.0071044797
train_loss: 0.0037851897
test_loss: 0.007116177
train_loss: 0.004147516
test_loss: 0.0069056535
train_loss: 0.0037735887
test_loss: 0.0070483424
train_loss: 0.0039297435
test_loss: 0.006814263
train_loss: 0.0037999724
test_loss: 0.00689644
train_loss: 0.00388119
test_loss: 0.006674769
train_loss: 0.003847479
test_loss: 0.007153618
train_loss: 0.0043245945
test_loss: 0.0069107907
train_loss: 0.0040583676
test_loss: 0.0070577175
train_loss: 0.004130217
test_loss: 0.0067787534
train_loss: 0.004057638
test_loss: 0.007161127
train_loss: 0.004049574
test_loss: 0.007154626
train_loss: 0.0048526237
test_loss: 0.0069921864
train_loss: 0.004243069
test_loss: 0.0071046306
train_loss: 0.00415227
test_loss: 0.0074948748
train_loss: 0.0038274245
test_loss: 0.007244036
train_loss: 0.004102571
test_loss: 0.0071826493
train_loss: 0.0046594343
test_loss: 0.006795128
train_loss: 0.003968697
test_loss: 0.0069831046
train_loss: 0.0044478476
test_loss: 0.007201974
train_loss: 0.0038388027
test_loss: 0.007263437
train_loss: 0.004405338
test_loss: 0.0071458374
train_loss: 0.003807555
test_loss: 0.0068849637
train_loss: 0.004595241
test_loss: 0.007122942
train_loss: 0.0044213245
test_loss: 0.0069543603
train_loss: 0.0038366872
test_loss: 0.0070371283
train_loss: 0.004352275
test_loss: 0.007444026
train_loss: 0.0039298125
test_loss: 0.0069145164
train_loss: 0.004744723
test_loss: 0.0069689914
train_loss: 0.003746556
test_loss: 0.0069254874
train_loss: 0.005346016
test_loss: 0.0071427524
train_loss: 0.0039615827
test_loss: 0.006863375
train_loss: 0.0051032947
test_loss: 0.0072216773
train_loss: 0.004555136
test_loss: 0.007846515
train_loss: 0.0038466537
test_loss: 0.0067700986
train_loss: 0.004304371
test_loss: 0.006949918
train_loss: 0.0035346062
test_loss: 0.0066851834
train_loss: 0.0036441125
test_loss: 0.0071197045
train_loss: 0.003876122
test_loss: 0.006845277
train_loss: /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
0.0046769334
test_loss: 0.007088762
train_loss: 0.0041339663
test_loss: 0.0072064362
train_loss: 0.0040979893
test_loss: 0.007623258
train_loss: 0.0040253787
test_loss: 0.0068568266
train_loss: 0.004107259
test_loss: 0.0068681394
train_loss: 0.0040803673
test_loss: 0.0070189843
train_loss: 0.0038381584
test_loss: 0.0068826494
train_loss: 0.00417758
test_loss: 0.0072090854
train_loss: 0.0040974617
test_loss: 0.0069469013
train_loss: 0.0038869528
test_loss: 0.007469507
train_loss: 0.003974414
test_loss: 0.007050084
train_loss: 0.0037045532
test_loss: 0.006982563
train_loss: 0.0039008125
test_loss: 0.0070735384
train_loss: 0.003413809
test_loss: 0.006780071
train_loss: 0.0037176316
test_loss: 0.006768928
train_loss: 0.0037970713
test_loss: 0.0067073237
train_loss: 0.0034815264
test_loss: 0.0068046767
train_loss: 0.0036528849
test_loss: 0.006895745
train_loss: 0.004383063
test_loss: 0.0072358055
train_loss: 0.0037567995
test_loss: 0.0069901636
train_loss: 0.0041857413
test_loss: 0.0073146266
train_loss: 0.004125255
test_loss: 0.0069680437
train_loss: 0.0040427437
test_loss: 0.0068105534
train_loss: 0.0039487844
test_loss: 0.0070864926
train_loss: 0.0036550327
test_loss: 0.0070040813
train_loss: 0.0038069503
test_loss: 0.0066816513
train_loss: 0.004657169
test_loss: 0.0069286223
train_loss: 0.0041361814
test_loss: 0.0068270694
train_loss: 0.003656247
test_loss: 0.0070166173
train_loss: 0.0044358023
test_loss: 0.006845868
train_loss: 0.004170593
test_loss: 0.006628743
train_loss: 0.0038170936
test_loss: 0.0069302744
train_loss: 0.0042447345
test_loss: 0.006917841
train_loss: 0.0039397245
test_loss: 0.0070886193
train_loss: 0.0037169307
test_loss: 0.0068021645
train_loss: 0.0040409938
test_loss: 0.007394932
train_loss: 0.0046707783
test_loss: 0.00698799
train_loss: 0.0041078874
test_loss: 0.007536272
train_loss: 0.003842014
test_loss: 0.0069296714
train_loss: 0.0033374499
test_loss: 0.0070058876
train_loss: 0.003556658
test_loss: 0.007115337
train_loss: 0.0035041452
test_loss: 0.006772943
train_loss: 0.004236898
test_loss: 0.007524188
train_loss: 0.0038010722
test_loss: 0.0072509055
train_loss: 0.0040689297
test_loss: 0.006994568
train_loss: 0.0040597944
test_loss: 0.0069043767
train_loss: 0.004504795
test_loss: 0.0073490166
train_loss: 0.003661523
test_loss: 0.0071640876
train_loss: 0.0040372787
test_loss: 0.0068061696
train_loss: 0.0041308226
test_loss: 0.0071688243
train_loss: 0.0041030077
test_loss: 0.007082041
train_loss: 0.004237799
test_loss: 0.006970045
train_loss: 0.0038603141
test_loss: 0.0068485583
train_loss: 0.0034508351
test_loss: 0.0069784853
train_loss: 0.0038677598
test_loss: 0.006692774
train_loss: 0.0043380214
test_loss: 0.0072790207
train_loss: 0.0043709422
test_loss: 0.007674269
train_loss: 0.0037634848
test_loss: 0.0071943603
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78fa4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78ef0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78ef0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78e1be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78e8d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78e8d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78da9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78d73950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78d731e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78d73a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78d412f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78ce6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78ce0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78ce0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78c50d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78c83048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78c831e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78c507b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78bfb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78bedf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f78b91840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f641a3598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f641df8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f64185620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f641786a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f64178730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f640f0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f6411a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f6411a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f640d26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f640737b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f6408f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f6408fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f64049b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f63ff77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f63ff7d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 8.71582233e-05
Iter: 2 loss: 6.14231394e-05
Iter: 3 loss: 6.13740849e-05
Iter: 4 loss: 5.08732337e-05
Iter: 5 loss: 5.08729936e-05
Iter: 6 loss: 4.59625371e-05
Iter: 7 loss: 3.90571222e-05
Iter: 8 loss: 3.87882174e-05
Iter: 9 loss: 3.34583965e-05
Iter: 10 loss: 5.56563391e-05
Iter: 11 loss: 3.2316846e-05
Iter: 12 loss: 2.81811463e-05
Iter: 13 loss: 3.99023047e-05
Iter: 14 loss: 2.68752428e-05
Iter: 15 loss: 2.45168158e-05
Iter: 16 loss: 6.06555259e-05
Iter: 17 loss: 2.45160991e-05
Iter: 18 loss: 2.29317884e-05
Iter: 19 loss: 2.36622836e-05
Iter: 20 loss: 2.1859174e-05
Iter: 21 loss: 1.9735875e-05
Iter: 22 loss: 2.77313375e-05
Iter: 23 loss: 1.92234875e-05
Iter: 24 loss: 1.81151445e-05
Iter: 25 loss: 1.94131171e-05
Iter: 26 loss: 1.75255518e-05
Iter: 27 loss: 1.63277873e-05
Iter: 28 loss: 2.21367809e-05
Iter: 29 loss: 1.61166608e-05
Iter: 30 loss: 1.51210334e-05
Iter: 31 loss: 1.63675249e-05
Iter: 32 loss: 1.46050206e-05
Iter: 33 loss: 1.34015863e-05
Iter: 34 loss: 2.00620671e-05
Iter: 35 loss: 1.32297082e-05
Iter: 36 loss: 1.2815739e-05
Iter: 37 loss: 1.2804051e-05
Iter: 38 loss: 1.2299337e-05
Iter: 39 loss: 1.22454912e-05
Iter: 40 loss: 1.1879486e-05
Iter: 41 loss: 1.13250035e-05
Iter: 42 loss: 1.15749372e-05
Iter: 43 loss: 1.09478333e-05
Iter: 44 loss: 1.04736428e-05
Iter: 45 loss: 1.24734415e-05
Iter: 46 loss: 1.03736875e-05
Iter: 47 loss: 9.90639455e-06
Iter: 48 loss: 1.08912536e-05
Iter: 49 loss: 9.72163434e-06
Iter: 50 loss: 9.38423636e-06
Iter: 51 loss: 1.33517133e-05
Iter: 52 loss: 9.37936e-06
Iter: 53 loss: 9.08927359e-06
Iter: 54 loss: 8.83861321e-06
Iter: 55 loss: 8.76040758e-06
Iter: 56 loss: 8.31905709e-06
Iter: 57 loss: 1.14197501e-05
Iter: 58 loss: 8.27868826e-06
Iter: 59 loss: 7.99433474e-06
Iter: 60 loss: 7.9093e-06
Iter: 61 loss: 7.73850661e-06
Iter: 62 loss: 7.4124132e-06
Iter: 63 loss: 9.76956107e-06
Iter: 64 loss: 7.38461858e-06
Iter: 65 loss: 7.11082293e-06
Iter: 66 loss: 7.64358902e-06
Iter: 67 loss: 6.9965854e-06
Iter: 68 loss: 6.81222627e-06
Iter: 69 loss: 9.59487352e-06
Iter: 70 loss: 6.81211122e-06
Iter: 71 loss: 6.70388636e-06
Iter: 72 loss: 8.21263257e-06
Iter: 73 loss: 6.70336522e-06
Iter: 74 loss: 6.60439309e-06
Iter: 75 loss: 6.36410459e-06
Iter: 76 loss: 8.89294824e-06
Iter: 77 loss: 6.33689206e-06
Iter: 78 loss: 6.12963777e-06
Iter: 79 loss: 7.27264251e-06
Iter: 80 loss: 6.09969e-06
Iter: 81 loss: 5.90735317e-06
Iter: 82 loss: 6.17219621e-06
Iter: 83 loss: 5.81173026e-06
Iter: 84 loss: 5.66083963e-06
Iter: 85 loss: 7.50295158e-06
Iter: 86 loss: 5.65923256e-06
Iter: 87 loss: 5.56349642e-06
Iter: 88 loss: 5.74300566e-06
Iter: 89 loss: 5.52289202e-06
Iter: 90 loss: 5.37891628e-06
Iter: 91 loss: 5.29377621e-06
Iter: 92 loss: 5.23316976e-06
Iter: 93 loss: 5.095299e-06
Iter: 94 loss: 6.63059564e-06
Iter: 95 loss: 5.09251186e-06
Iter: 96 loss: 4.99288399e-06
Iter: 97 loss: 4.94995948e-06
Iter: 98 loss: 4.89899776e-06
Iter: 99 loss: 4.77244794e-06
Iter: 100 loss: 5.56051054e-06
Iter: 101 loss: 4.7577405e-06
Iter: 102 loss: 4.64923824e-06
Iter: 103 loss: 4.78101e-06
Iter: 104 loss: 4.59272087e-06
Iter: 105 loss: 4.52815357e-06
Iter: 106 loss: 4.51948563e-06
Iter: 107 loss: 4.4549115e-06
Iter: 108 loss: 4.60107913e-06
Iter: 109 loss: 4.43033105e-06
Iter: 110 loss: 4.371263e-06
Iter: 111 loss: 4.24745758e-06
Iter: 112 loss: 6.34301205e-06
Iter: 113 loss: 4.24470636e-06
Iter: 114 loss: 4.11903784e-06
Iter: 115 loss: 4.53279e-06
Iter: 116 loss: 4.08407232e-06
Iter: 117 loss: 3.97332724e-06
Iter: 118 loss: 4.87815487e-06
Iter: 119 loss: 3.96658288e-06
Iter: 120 loss: 3.90153309e-06
Iter: 121 loss: 4.1962262e-06
Iter: 122 loss: 3.88887e-06
Iter: 123 loss: 3.81848895e-06
Iter: 124 loss: 3.87034834e-06
Iter: 125 loss: 3.77511969e-06
Iter: 126 loss: 3.68709789e-06
Iter: 127 loss: 4.11082e-06
Iter: 128 loss: 3.67147436e-06
Iter: 129 loss: 3.61945422e-06
Iter: 130 loss: 3.6061881e-06
Iter: 131 loss: 3.57352747e-06
Iter: 132 loss: 3.48634512e-06
Iter: 133 loss: 3.88707122e-06
Iter: 134 loss: 3.4696568e-06
Iter: 135 loss: 3.41787e-06
Iter: 136 loss: 3.47119044e-06
Iter: 137 loss: 3.38902373e-06
Iter: 138 loss: 3.32402828e-06
Iter: 139 loss: 3.72763679e-06
Iter: 140 loss: 3.3162321e-06
Iter: 141 loss: 3.30969488e-06
Iter: 142 loss: 3.29508566e-06
Iter: 143 loss: 3.27981093e-06
Iter: 144 loss: 3.24122334e-06
Iter: 145 loss: 3.57974977e-06
Iter: 146 loss: 3.23482755e-06
Iter: 147 loss: 3.18021876e-06
Iter: 148 loss: 3.27494331e-06
Iter: 149 loss: 3.1558161e-06
Iter: 150 loss: 3.10920768e-06
Iter: 151 loss: 3.129672e-06
Iter: 152 loss: 3.0771314e-06
Iter: 153 loss: 3.01571572e-06
Iter: 154 loss: 3.52609686e-06
Iter: 155 loss: 3.01188584e-06
Iter: 156 loss: 2.97144379e-06
Iter: 157 loss: 3.18139246e-06
Iter: 158 loss: 2.96486451e-06
Iter: 159 loss: 2.92595723e-06
Iter: 160 loss: 2.95614836e-06
Iter: 161 loss: 2.90225626e-06
Iter: 162 loss: 2.85373244e-06
Iter: 163 loss: 3.03535444e-06
Iter: 164 loss: 2.84197336e-06
Iter: 165 loss: 2.80421068e-06
Iter: 166 loss: 2.799409e-06
Iter: 167 loss: 2.77249501e-06
Iter: 168 loss: 2.73508931e-06
Iter: 169 loss: 3.17063586e-06
Iter: 170 loss: 2.73447017e-06
Iter: 171 loss: 2.70296096e-06
Iter: 172 loss: 2.67848964e-06
Iter: 173 loss: 2.66869029e-06
Iter: 174 loss: 2.65659151e-06
Iter: 175 loss: 2.65040717e-06
Iter: 176 loss: 2.6302514e-06
Iter: 177 loss: 2.65402946e-06
Iter: 178 loss: 2.61951232e-06
Iter: 179 loss: 2.59808621e-06
Iter: 180 loss: 2.57897545e-06
Iter: 181 loss: 2.57339389e-06
Iter: 182 loss: 2.54574343e-06
Iter: 183 loss: 2.6063251e-06
Iter: 184 loss: 2.53495e-06
Iter: 185 loss: 2.49908089e-06
Iter: 186 loss: 2.51048e-06
Iter: 187 loss: 2.47334197e-06
Iter: 188 loss: 2.4416438e-06
Iter: 189 loss: 2.83373947e-06
Iter: 190 loss: 2.44136891e-06
Iter: 191 loss: 2.41587941e-06
Iter: 192 loss: 2.48890342e-06
Iter: 193 loss: 2.40779536e-06
Iter: 194 loss: 2.38246139e-06
Iter: 195 loss: 2.44509147e-06
Iter: 196 loss: 2.3736377e-06
Iter: 197 loss: 2.34898334e-06
Iter: 198 loss: 2.39357951e-06
Iter: 199 loss: 2.33827723e-06
Iter: 200 loss: 2.31226295e-06
Iter: 201 loss: 2.36020765e-06
Iter: 202 loss: 2.30091496e-06
Iter: 203 loss: 2.27760779e-06
Iter: 204 loss: 2.30140245e-06
Iter: 205 loss: 2.26468546e-06
Iter: 206 loss: 2.23204688e-06
Iter: 207 loss: 2.40241911e-06
Iter: 208 loss: 2.22702238e-06
Iter: 209 loss: 2.21501978e-06
Iter: 210 loss: 2.21415849e-06
Iter: 211 loss: 2.19944081e-06
Iter: 212 loss: 2.17815e-06
Iter: 213 loss: 2.17749903e-06
Iter: 214 loss: 2.16212629e-06
Iter: 215 loss: 2.21252753e-06
Iter: 216 loss: 2.15762179e-06
Iter: 217 loss: 2.14158899e-06
Iter: 218 loss: 2.12966324e-06
Iter: 219 loss: 2.12431405e-06
Iter: 220 loss: 2.10291068e-06
Iter: 221 loss: 2.34860113e-06
Iter: 222 loss: 2.10248072e-06
Iter: 223 loss: 2.08895881e-06
Iter: 224 loss: 2.08575216e-06
Iter: 225 loss: 2.07715357e-06
Iter: 226 loss: 2.05325523e-06
Iter: 227 loss: 2.2200411e-06
Iter: 228 loss: 2.05117362e-06
Iter: 229 loss: 2.03794934e-06
Iter: 230 loss: 2.07845801e-06
Iter: 231 loss: 2.03413083e-06
Iter: 232 loss: 2.02054298e-06
Iter: 233 loss: 2.0159755e-06
Iter: 234 loss: 2.00823524e-06
Iter: 235 loss: 1.98612133e-06
Iter: 236 loss: 2.06685149e-06
Iter: 237 loss: 1.9803997e-06
Iter: 238 loss: 1.96284554e-06
Iter: 239 loss: 1.96124279e-06
Iter: 240 loss: 1.94836275e-06
Iter: 241 loss: 1.93469054e-06
Iter: 242 loss: 1.93384631e-06
Iter: 243 loss: 1.92506104e-06
Iter: 244 loss: 2.04752087e-06
Iter: 245 loss: 1.92496827e-06
Iter: 246 loss: 1.91747e-06
Iter: 247 loss: 1.8996775e-06
Iter: 248 loss: 2.11403813e-06
Iter: 249 loss: 1.89815523e-06
Iter: 250 loss: 1.88110289e-06
Iter: 251 loss: 1.88225408e-06
Iter: 252 loss: 1.8677473e-06
Iter: 253 loss: 1.85012686e-06
Iter: 254 loss: 1.85013289e-06
Iter: 255 loss: 1.84014948e-06
Iter: 256 loss: 1.83300494e-06
Iter: 257 loss: 1.82948952e-06
Iter: 258 loss: 1.81424423e-06
Iter: 259 loss: 1.95767734e-06
Iter: 260 loss: 1.81367113e-06
Iter: 261 loss: 1.80419329e-06
Iter: 262 loss: 1.86763054e-06
Iter: 263 loss: 1.80329562e-06
Iter: 264 loss: 1.79361268e-06
Iter: 265 loss: 1.77578534e-06
Iter: 266 loss: 2.17733214e-06
Iter: 267 loss: 1.77574691e-06
Iter: 268 loss: 1.75960429e-06
Iter: 269 loss: 1.99883084e-06
Iter: 270 loss: 1.75960633e-06
Iter: 271 loss: 1.748165e-06
Iter: 272 loss: 1.73500462e-06
Iter: 273 loss: 1.73327726e-06
Iter: 274 loss: 1.71525494e-06
Iter: 275 loss: 1.87497403e-06
Iter: 276 loss: 1.71433646e-06
Iter: 277 loss: 1.70504734e-06
Iter: 278 loss: 1.77986328e-06
Iter: 279 loss: 1.70451563e-06
Iter: 280 loss: 1.69248892e-06
Iter: 281 loss: 1.7077773e-06
Iter: 282 loss: 1.68631425e-06
Iter: 283 loss: 1.67763972e-06
Iter: 284 loss: 1.68295469e-06
Iter: 285 loss: 1.6718601e-06
Iter: 286 loss: 1.66219252e-06
Iter: 287 loss: 1.65201675e-06
Iter: 288 loss: 1.65025415e-06
Iter: 289 loss: 1.63537891e-06
Iter: 290 loss: 1.77685502e-06
Iter: 291 loss: 1.63484742e-06
Iter: 292 loss: 1.62434401e-06
Iter: 293 loss: 1.64585867e-06
Iter: 294 loss: 1.62024639e-06
Iter: 295 loss: 1.60855188e-06
Iter: 296 loss: 1.65569054e-06
Iter: 297 loss: 1.60599393e-06
Iter: 298 loss: 1.59731815e-06
Iter: 299 loss: 1.6601241e-06
Iter: 300 loss: 1.59653575e-06
Iter: 301 loss: 1.58855596e-06
Iter: 302 loss: 1.5881742e-06
Iter: 303 loss: 1.58204853e-06
Iter: 304 loss: 1.57295472e-06
Iter: 305 loss: 1.59133333e-06
Iter: 306 loss: 1.56928331e-06
Iter: 307 loss: 1.55721477e-06
Iter: 308 loss: 1.58302475e-06
Iter: 309 loss: 1.55262478e-06
Iter: 310 loss: 1.54315535e-06
Iter: 311 loss: 1.58894659e-06
Iter: 312 loss: 1.54154395e-06
Iter: 313 loss: 1.53454528e-06
Iter: 314 loss: 1.61226922e-06
Iter: 315 loss: 1.53443875e-06
Iter: 316 loss: 1.52631844e-06
Iter: 317 loss: 1.52669531e-06
Iter: 318 loss: 1.51980407e-06
Iter: 319 loss: 1.51457948e-06
Iter: 320 loss: 1.50695962e-06
Iter: 321 loss: 1.5066671e-06
Iter: 322 loss: 1.49425045e-06
Iter: 323 loss: 1.54486361e-06
Iter: 324 loss: 1.49161724e-06
Iter: 325 loss: 1.48356855e-06
Iter: 326 loss: 1.49714822e-06
Iter: 327 loss: 1.47997048e-06
Iter: 328 loss: 1.46992829e-06
Iter: 329 loss: 1.50329879e-06
Iter: 330 loss: 1.46706918e-06
Iter: 331 loss: 1.45870217e-06
Iter: 332 loss: 1.51035954e-06
Iter: 333 loss: 1.45775493e-06
Iter: 334 loss: 1.45009017e-06
Iter: 335 loss: 1.46978573e-06
Iter: 336 loss: 1.44743979e-06
Iter: 337 loss: 1.4406188e-06
Iter: 338 loss: 1.47231742e-06
Iter: 339 loss: 1.43933994e-06
Iter: 340 loss: 1.43412137e-06
Iter: 341 loss: 1.42732358e-06
Iter: 342 loss: 1.42691101e-06
Iter: 343 loss: 1.41826422e-06
Iter: 344 loss: 1.50729079e-06
Iter: 345 loss: 1.41807061e-06
Iter: 346 loss: 1.4111024e-06
Iter: 347 loss: 1.41583416e-06
Iter: 348 loss: 1.40673274e-06
Iter: 349 loss: 1.40462e-06
Iter: 350 loss: 1.40257407e-06
Iter: 351 loss: 1.39945075e-06
Iter: 352 loss: 1.39432859e-06
Iter: 353 loss: 1.3944159e-06
Iter: 354 loss: 1.38840164e-06
Iter: 355 loss: 1.37802317e-06
Iter: 356 loss: 1.37800691e-06
Iter: 357 loss: 1.37262646e-06
Iter: 358 loss: 1.372209e-06
Iter: 359 loss: 1.36651295e-06
Iter: 360 loss: 1.35740947e-06
Iter: 361 loss: 1.35718608e-06
Iter: 362 loss: 1.34882293e-06
Iter: 363 loss: 1.44791409e-06
Iter: 364 loss: 1.34872835e-06
Iter: 365 loss: 1.34152344e-06
Iter: 366 loss: 1.35147047e-06
Iter: 367 loss: 1.33786398e-06
Iter: 368 loss: 1.33256322e-06
Iter: 369 loss: 1.41771966e-06
Iter: 370 loss: 1.3325307e-06
Iter: 371 loss: 1.32821231e-06
Iter: 372 loss: 1.32490959e-06
Iter: 373 loss: 1.32358537e-06
Iter: 374 loss: 1.31558613e-06
Iter: 375 loss: 1.34087713e-06
Iter: 376 loss: 1.31341517e-06
Iter: 377 loss: 1.30739431e-06
Iter: 378 loss: 1.31097659e-06
Iter: 379 loss: 1.30358728e-06
Iter: 380 loss: 1.29727414e-06
Iter: 381 loss: 1.3628885e-06
Iter: 382 loss: 1.29714363e-06
Iter: 383 loss: 1.29155501e-06
Iter: 384 loss: 1.33240542e-06
Iter: 385 loss: 1.29105229e-06
Iter: 386 loss: 1.2871044e-06
Iter: 387 loss: 1.28327974e-06
Iter: 388 loss: 1.28237139e-06
Iter: 389 loss: 1.277661e-06
Iter: 390 loss: 1.29050318e-06
Iter: 391 loss: 1.27606745e-06
Iter: 392 loss: 1.27191549e-06
Iter: 393 loss: 1.26517e-06
Iter: 394 loss: 1.26515374e-06
Iter: 395 loss: 1.25804968e-06
Iter: 396 loss: 1.2579934e-06
Iter: 397 loss: 1.25353085e-06
Iter: 398 loss: 1.25733504e-06
Iter: 399 loss: 1.25084011e-06
Iter: 400 loss: 1.24484336e-06
Iter: 401 loss: 1.26917791e-06
Iter: 402 loss: 1.24350436e-06
Iter: 403 loss: 1.23927725e-06
Iter: 404 loss: 1.26896009e-06
Iter: 405 loss: 1.23889322e-06
Iter: 406 loss: 1.23429197e-06
Iter: 407 loss: 1.23121276e-06
Iter: 408 loss: 1.22947949e-06
Iter: 409 loss: 1.22479992e-06
Iter: 410 loss: 1.27624128e-06
Iter: 411 loss: 1.2246611e-06
Iter: 412 loss: 1.22115807e-06
Iter: 413 loss: 1.21657922e-06
Iter: 414 loss: 1.21631683e-06
Iter: 415 loss: 1.21265703e-06
Iter: 416 loss: 1.21222297e-06
Iter: 417 loss: 1.20889717e-06
Iter: 418 loss: 1.22524489e-06
Iter: 419 loss: 1.20837558e-06
Iter: 420 loss: 1.20624532e-06
Iter: 421 loss: 1.20040659e-06
Iter: 422 loss: 1.23391396e-06
Iter: 423 loss: 1.19869924e-06
Iter: 424 loss: 1.19208357e-06
Iter: 425 loss: 1.27918292e-06
Iter: 426 loss: 1.19196693e-06
Iter: 427 loss: 1.18775938e-06
Iter: 428 loss: 1.18030573e-06
Iter: 429 loss: 1.36674123e-06
Iter: 430 loss: 1.18037235e-06
Iter: 431 loss: 1.17361e-06
Iter: 432 loss: 1.25330484e-06
Iter: 433 loss: 1.17353898e-06
Iter: 434 loss: 1.16856313e-06
Iter: 435 loss: 1.19294964e-06
Iter: 436 loss: 1.16774186e-06
Iter: 437 loss: 1.16355727e-06
Iter: 438 loss: 1.17799345e-06
Iter: 439 loss: 1.16250612e-06
Iter: 440 loss: 1.15845705e-06
Iter: 441 loss: 1.17332274e-06
Iter: 442 loss: 1.15742444e-06
Iter: 443 loss: 1.15325224e-06
Iter: 444 loss: 1.16375236e-06
Iter: 445 loss: 1.15177772e-06
Iter: 446 loss: 1.14795091e-06
Iter: 447 loss: 1.14570446e-06
Iter: 448 loss: 1.14404907e-06
Iter: 449 loss: 1.13836938e-06
Iter: 450 loss: 1.19662059e-06
Iter: 451 loss: 1.13834358e-06
Iter: 452 loss: 1.13653425e-06
Iter: 453 loss: 1.136425e-06
Iter: 454 loss: 1.13460567e-06
Iter: 455 loss: 1.13049327e-06
Iter: 456 loss: 1.1901102e-06
Iter: 457 loss: 1.13026181e-06
Iter: 458 loss: 1.12604312e-06
Iter: 459 loss: 1.13566603e-06
Iter: 460 loss: 1.12441876e-06
Iter: 461 loss: 1.12036184e-06
Iter: 462 loss: 1.12707221e-06
Iter: 463 loss: 1.11862357e-06
Iter: 464 loss: 1.1137713e-06
Iter: 465 loss: 1.12667908e-06
Iter: 466 loss: 1.11210147e-06
Iter: 467 loss: 1.10832627e-06
Iter: 468 loss: 1.10829433e-06
Iter: 469 loss: 1.10530095e-06
Iter: 470 loss: 1.10050109e-06
Iter: 471 loss: 1.1489999e-06
Iter: 472 loss: 1.10032943e-06
Iter: 473 loss: 1.09684493e-06
Iter: 474 loss: 1.10968381e-06
Iter: 475 loss: 1.09598989e-06
Iter: 476 loss: 1.09233167e-06
Iter: 477 loss: 1.10593555e-06
Iter: 478 loss: 1.09146765e-06
Iter: 479 loss: 1.08831648e-06
Iter: 480 loss: 1.0947881e-06
Iter: 481 loss: 1.08709798e-06
Iter: 482 loss: 1.08333302e-06
Iter: 483 loss: 1.08324696e-06
Iter: 484 loss: 1.08040581e-06
Iter: 485 loss: 1.07697133e-06
Iter: 486 loss: 1.11828354e-06
Iter: 487 loss: 1.076937e-06
Iter: 488 loss: 1.07364292e-06
Iter: 489 loss: 1.09311691e-06
Iter: 490 loss: 1.07320329e-06
Iter: 491 loss: 1.07055507e-06
Iter: 492 loss: 1.06722064e-06
Iter: 493 loss: 1.06684433e-06
Iter: 494 loss: 1.06404536e-06
Iter: 495 loss: 1.07381788e-06
Iter: 496 loss: 1.06332845e-06
Iter: 497 loss: 1.06016887e-06
Iter: 498 loss: 1.05786069e-06
Iter: 499 loss: 1.05677771e-06
Iter: 500 loss: 1.0534352e-06
Iter: 501 loss: 1.10706378e-06
Iter: 502 loss: 1.05336653e-06
Iter: 503 loss: 1.05071695e-06
Iter: 504 loss: 1.04676121e-06
Iter: 505 loss: 1.04671949e-06
Iter: 506 loss: 1.04274193e-06
Iter: 507 loss: 1.08345762e-06
Iter: 508 loss: 1.04256389e-06
Iter: 509 loss: 1.03888317e-06
Iter: 510 loss: 1.04350033e-06
Iter: 511 loss: 1.03694401e-06
Iter: 512 loss: 1.03329478e-06
Iter: 513 loss: 1.08222491e-06
Iter: 514 loss: 1.03322179e-06
Iter: 515 loss: 1.03142372e-06
Iter: 516 loss: 1.03061177e-06
Iter: 517 loss: 1.02962508e-06
Iter: 518 loss: 1.02584204e-06
Iter: 519 loss: 1.02743513e-06
Iter: 520 loss: 1.02325043e-06
Iter: 521 loss: 1.02107356e-06
Iter: 522 loss: 1.02089211e-06
Iter: 523 loss: 1.01840101e-06
Iter: 524 loss: 1.01869955e-06
Iter: 525 loss: 1.01652654e-06
Iter: 526 loss: 1.01387559e-06
Iter: 527 loss: 1.01467992e-06
Iter: 528 loss: 1.01200294e-06
Iter: 529 loss: 1.00916759e-06
Iter: 530 loss: 1.01273326e-06
Iter: 531 loss: 1.00778027e-06
Iter: 532 loss: 1.00448324e-06
Iter: 533 loss: 1.01762271e-06
Iter: 534 loss: 1.00374245e-06
Iter: 535 loss: 1.00105251e-06
Iter: 536 loss: 1.00121042e-06
Iter: 537 loss: 9.98961468e-07
Iter: 538 loss: 9.95419214e-07
Iter: 539 loss: 1.02477406e-06
Iter: 540 loss: 9.95257096e-07
Iter: 541 loss: 9.92446758e-07
Iter: 542 loss: 9.91843081e-07
Iter: 543 loss: 9.90024773e-07
Iter: 544 loss: 9.86461e-07
Iter: 545 loss: 1.01844716e-06
Iter: 546 loss: 9.86205237e-07
Iter: 547 loss: 9.84310645e-07
Iter: 548 loss: 1.00399188e-06
Iter: 549 loss: 9.84255e-07
Iter: 550 loss: 9.8223245e-07
Iter: 551 loss: 9.7744487e-07
Iter: 552 loss: 1.03088792e-06
Iter: 553 loss: 9.77068339e-07
Iter: 554 loss: 9.759151e-07
Iter: 555 loss: 9.74757313e-07
Iter: 556 loss: 9.73472e-07
Iter: 557 loss: 9.86253326e-07
Iter: 558 loss: 9.73373744e-07
Iter: 559 loss: 9.72082375e-07
Iter: 560 loss: 9.68947916e-07
Iter: 561 loss: 1.00432749e-06
Iter: 562 loss: 9.68645622e-07
Iter: 563 loss: 9.6606243e-07
Iter: 564 loss: 9.7896077e-07
Iter: 565 loss: 9.65627805e-07
Iter: 566 loss: 9.62856348e-07
Iter: 567 loss: 9.63192292e-07
Iter: 568 loss: 9.60724492e-07
Iter: 569 loss: 9.57714633e-07
Iter: 570 loss: 9.83146151e-07
Iter: 571 loss: 9.57575821e-07
Iter: 572 loss: 9.55424866e-07
Iter: 573 loss: 9.5240074e-07
Iter: 574 loss: 9.52337928e-07
Iter: 575 loss: 9.48697334e-07
Iter: 576 loss: 9.89547289e-07
Iter: 577 loss: 9.48630884e-07
Iter: 578 loss: 9.45741363e-07
Iter: 579 loss: 9.50191861e-07
Iter: 580 loss: 9.44434873e-07
Iter: 581 loss: 9.41984865e-07
Iter: 582 loss: 9.58562396e-07
Iter: 583 loss: 9.41678877e-07
Iter: 584 loss: 9.39429356e-07
Iter: 585 loss: 9.47123908e-07
Iter: 586 loss: 9.38874109e-07
Iter: 587 loss: 9.3631138e-07
Iter: 588 loss: 9.38161861e-07
Iter: 589 loss: 9.34739774e-07
Iter: 590 loss: 9.32286355e-07
Iter: 591 loss: 9.39661618e-07
Iter: 592 loss: 9.31540853e-07
Iter: 593 loss: 9.29168039e-07
Iter: 594 loss: 9.67063897e-07
Iter: 595 loss: 9.29189866e-07
Iter: 596 loss: 9.27953693e-07
Iter: 597 loss: 9.27220185e-07
Iter: 598 loss: 9.26777091e-07
Iter: 599 loss: 9.2491922e-07
Iter: 600 loss: 9.21876961e-07
Iter: 601 loss: 9.21869628e-07
Iter: 602 loss: 9.19051274e-07
Iter: 603 loss: 9.62022568e-07
Iter: 604 loss: 9.19058095e-07
Iter: 605 loss: 9.17229727e-07
Iter: 606 loss: 9.17220746e-07
Iter: 607 loss: 9.15803412e-07
Iter: 608 loss: 9.13033034e-07
Iter: 609 loss: 9.27940789e-07
Iter: 610 loss: 9.12657129e-07
Iter: 611 loss: 9.10926246e-07
Iter: 612 loss: 9.08603965e-07
Iter: 613 loss: 9.08525408e-07
Iter: 614 loss: 9.04897945e-07
Iter: 615 loss: 9.36347419e-07
Iter: 616 loss: 9.04662556e-07
Iter: 617 loss: 9.02665306e-07
Iter: 618 loss: 9.07319077e-07
Iter: 619 loss: 9.01899853e-07
Iter: 620 loss: 8.99864176e-07
Iter: 621 loss: 9.15071666e-07
Iter: 622 loss: 8.99724398e-07
Iter: 623 loss: 8.98394546e-07
Iter: 624 loss: 9.02728914e-07
Iter: 625 loss: 8.9794e-07
Iter: 626 loss: 8.96810718e-07
Iter: 627 loss: 9.02017348e-07
Iter: 628 loss: 8.9656794e-07
Iter: 629 loss: 8.95002529e-07
Iter: 630 loss: 8.93159836e-07
Iter: 631 loss: 8.92978278e-07
Iter: 632 loss: 8.91176512e-07
Iter: 633 loss: 8.98097255e-07
Iter: 634 loss: 8.9077264e-07
Iter: 635 loss: 8.89074954e-07
Iter: 636 loss: 8.87733563e-07
Iter: 637 loss: 8.87261706e-07
Iter: 638 loss: 8.84865e-07
Iter: 639 loss: 9.02260638e-07
Iter: 640 loss: 8.84604958e-07
Iter: 641 loss: 8.82906079e-07
Iter: 642 loss: 8.84539929e-07
Iter: 643 loss: 8.8197e-07
Iter: 644 loss: 8.79876779e-07
Iter: 645 loss: 8.92178946e-07
Iter: 646 loss: 8.7969272e-07
Iter: 647 loss: 8.77841444e-07
Iter: 648 loss: 8.76897843e-07
Iter: 649 loss: 8.76013246e-07
Iter: 650 loss: 8.73735871e-07
Iter: 651 loss: 8.90436809e-07
Iter: 652 loss: 8.73474164e-07
Iter: 653 loss: 8.71410805e-07
Iter: 654 loss: 8.75224316e-07
Iter: 655 loss: 8.70527458e-07
Iter: 656 loss: 8.68227517e-07
Iter: 657 loss: 8.87254146e-07
Iter: 658 loss: 8.68041809e-07
Iter: 659 loss: 8.6673333e-07
Iter: 660 loss: 8.70097779e-07
Iter: 661 loss: 8.66220489e-07
Iter: 662 loss: 8.64877052e-07
Iter: 663 loss: 8.76473e-07
Iter: 664 loss: 8.64818503e-07
Iter: 665 loss: 8.63310845e-07
Iter: 666 loss: 8.62125489e-07
Iter: 667 loss: 8.61683873e-07
Iter: 668 loss: 8.6015325e-07
Iter: 669 loss: 8.58697376e-07
Iter: 670 loss: 8.58301064e-07
Iter: 671 loss: 8.55324515e-07
Iter: 672 loss: 8.70847884e-07
Iter: 673 loss: 8.54843506e-07
Iter: 674 loss: 8.53129734e-07
Iter: 675 loss: 8.59338229e-07
Iter: 676 loss: 8.52727226e-07
Iter: 677 loss: 8.50986794e-07
Iter: 678 loss: 8.50411e-07
Iter: 679 loss: 8.49321168e-07
Iter: 680 loss: 8.47427714e-07
Iter: 681 loss: 8.72085934e-07
Iter: 682 loss: 8.47470687e-07
Iter: 683 loss: 8.45944726e-07
Iter: 684 loss: 8.45994066e-07
Iter: 685 loss: 8.44798e-07
Iter: 686 loss: 8.42623422e-07
Iter: 687 loss: 8.45219517e-07
Iter: 688 loss: 8.41543851e-07
Iter: 689 loss: 8.38956453e-07
Iter: 690 loss: 8.52935386e-07
Iter: 691 loss: 8.38565256e-07
Iter: 692 loss: 8.37104437e-07
Iter: 693 loss: 8.58748365e-07
Iter: 694 loss: 8.37107905e-07
Iter: 695 loss: 8.35848084e-07
Iter: 696 loss: 8.3442751e-07
Iter: 697 loss: 8.34300124e-07
Iter: 698 loss: 8.32907631e-07
Iter: 699 loss: 8.32722776e-07
Iter: 700 loss: 8.31756495e-07
Iter: 701 loss: 8.31991201e-07
Iter: 702 loss: 8.31016962e-07
Iter: 703 loss: 8.29977523e-07
Iter: 704 loss: 8.27525923e-07
Iter: 705 loss: 8.58807766e-07
Iter: 706 loss: 8.27411668e-07
Iter: 707 loss: 8.25476718e-07
Iter: 708 loss: 8.25480527e-07
Iter: 709 loss: 8.23804271e-07
Iter: 710 loss: 8.23013e-07
Iter: 711 loss: 8.22242043e-07
Iter: 712 loss: 8.20176069e-07
Iter: 713 loss: 8.38612038e-07
Iter: 714 loss: 8.20089326e-07
Iter: 715 loss: 8.18867875e-07
Iter: 716 loss: 8.17913474e-07
Iter: 717 loss: 8.17535067e-07
Iter: 718 loss: 8.15327553e-07
Iter: 719 loss: 8.28661086e-07
Iter: 720 loss: 8.15077044e-07
Iter: 721 loss: 8.13490544e-07
Iter: 722 loss: 8.14181249e-07
Iter: 723 loss: 8.12456051e-07
Iter: 724 loss: 8.10599943e-07
Iter: 725 loss: 8.21474487e-07
Iter: 726 loss: 8.10346e-07
Iter: 727 loss: 8.09179369e-07
Iter: 728 loss: 8.11998461e-07
Iter: 729 loss: 8.08759637e-07
Iter: 730 loss: 8.07513e-07
Iter: 731 loss: 8.06389721e-07
Iter: 732 loss: 8.06051844e-07
Iter: 733 loss: 8.04452725e-07
Iter: 734 loss: 8.16491934e-07
Iter: 735 loss: 8.04257e-07
Iter: 736 loss: 8.02946488e-07
Iter: 737 loss: 8.09006224e-07
Iter: 738 loss: 8.0266625e-07
Iter: 739 loss: 8.01764713e-07
Iter: 740 loss: 8.01703607e-07
Iter: 741 loss: 8.01067642e-07
Iter: 742 loss: 7.99470058e-07
Iter: 743 loss: 8.20001617e-07
Iter: 744 loss: 7.99447207e-07
Iter: 745 loss: 7.97762596e-07
Iter: 746 loss: 8.02527495e-07
Iter: 747 loss: 7.97255382e-07
Iter: 748 loss: 7.95814401e-07
Iter: 749 loss: 8.01248461e-07
Iter: 750 loss: 7.95466519e-07
Iter: 751 loss: 7.93982565e-07
Iter: 752 loss: 7.95520918e-07
Iter: 753 loss: 7.93181357e-07
Iter: 754 loss: 7.91810407e-07
Iter: 755 loss: 7.942607e-07
Iter: 756 loss: 7.91337641e-07
Iter: 757 loss: 7.89738465e-07
Iter: 758 loss: 7.99676968e-07
Iter: 759 loss: 7.89518481e-07
Iter: 760 loss: 7.88567718e-07
Iter: 761 loss: 7.92308128e-07
Iter: 762 loss: 7.88303282e-07
Iter: 763 loss: 7.87303122e-07
Iter: 764 loss: 7.89620117e-07
Iter: 765 loss: 7.86907719e-07
Iter: 766 loss: 7.85844918e-07
Iter: 767 loss: 7.89671333e-07
Iter: 768 loss: 7.85532507e-07
Iter: 769 loss: 7.84767167e-07
Iter: 770 loss: 7.84666668e-07
Iter: 771 loss: 7.84134727e-07
Iter: 772 loss: 7.83440896e-07
Iter: 773 loss: 7.83357905e-07
Iter: 774 loss: 7.82816414e-07
Iter: 775 loss: 7.82181e-07
Iter: 776 loss: 7.82036864e-07
Iter: 777 loss: 7.81282779e-07
Iter: 778 loss: 7.80907953e-07
Iter: 779 loss: 7.80470828e-07
Iter: 780 loss: 7.79170875e-07
Iter: 781 loss: 7.83884843e-07
Iter: 782 loss: 7.78855338e-07
Iter: 783 loss: 7.77571586e-07
Iter: 784 loss: 7.77679247e-07
Iter: 785 loss: 7.76597687e-07
Iter: 786 loss: 7.75058083e-07
Iter: 787 loss: 7.89435944e-07
Iter: 788 loss: 7.74962e-07
Iter: 789 loss: 7.7383794e-07
Iter: 790 loss: 7.75273179e-07
Iter: 791 loss: 7.73323e-07
Iter: 792 loss: 7.71912937e-07
Iter: 793 loss: 7.75697799e-07
Iter: 794 loss: 7.71483599e-07
Iter: 795 loss: 7.7043444e-07
Iter: 796 loss: 7.7829884e-07
Iter: 797 loss: 7.70374754e-07
Iter: 798 loss: 7.6942456e-07
Iter: 799 loss: 7.7026823e-07
Iter: 800 loss: 7.68888412e-07
Iter: 801 loss: 7.67994493e-07
Iter: 802 loss: 7.7255595e-07
Iter: 803 loss: 7.67796394e-07
Iter: 804 loss: 7.67108872e-07
Iter: 805 loss: 7.69905739e-07
Iter: 806 loss: 7.66951587e-07
Iter: 807 loss: 7.65943128e-07
Iter: 808 loss: 7.68110453e-07
Iter: 809 loss: 7.65546361e-07
Iter: 810 loss: 7.64886181e-07
Iter: 811 loss: 7.6438937e-07
Iter: 812 loss: 7.64157107e-07
Iter: 813 loss: 7.63152684e-07
Iter: 814 loss: 7.64698711e-07
Iter: 815 loss: 7.62668208e-07
Iter: 816 loss: 7.61523381e-07
Iter: 817 loss: 7.67277584e-07
Iter: 818 loss: 7.61300157e-07
Iter: 819 loss: 7.60546754e-07
Iter: 820 loss: 7.60662601e-07
Iter: 821 loss: 7.59956833e-07
Iter: 822 loss: 7.58822864e-07
Iter: 823 loss: 7.65159712e-07
Iter: 824 loss: 7.58666488e-07
Iter: 825 loss: 7.57687076e-07
Iter: 826 loss: 7.59040347e-07
Iter: 827 loss: 7.57190264e-07
Iter: 828 loss: 7.55925839e-07
Iter: 829 loss: 7.61073693e-07
Iter: 830 loss: 7.55727285e-07
Iter: 831 loss: 7.54833081e-07
Iter: 832 loss: 7.60280386e-07
Iter: 833 loss: 7.5466005e-07
Iter: 834 loss: 7.53792392e-07
Iter: 835 loss: 7.52815e-07
Iter: 836 loss: 7.52687e-07
Iter: 837 loss: 7.51327718e-07
Iter: 838 loss: 7.63244543e-07
Iter: 839 loss: 7.51274513e-07
Iter: 840 loss: 7.50407082e-07
Iter: 841 loss: 7.50446304e-07
Iter: 842 loss: 7.49831543e-07
Iter: 843 loss: 7.48408809e-07
Iter: 844 loss: 7.65667892e-07
Iter: 845 loss: 7.48352363e-07
Iter: 846 loss: 7.47150921e-07
Iter: 847 loss: 7.51209e-07
Iter: 848 loss: 7.46850674e-07
Iter: 849 loss: 7.45548448e-07
Iter: 850 loss: 7.47339186e-07
Iter: 851 loss: 7.44846261e-07
Iter: 852 loss: 7.43569615e-07
Iter: 853 loss: 7.53778579e-07
Iter: 854 loss: 7.43495661e-07
Iter: 855 loss: 7.42649604e-07
Iter: 856 loss: 7.42154725e-07
Iter: 857 loss: 7.41806616e-07
Iter: 858 loss: 7.4048728e-07
Iter: 859 loss: 7.51788491e-07
Iter: 860 loss: 7.40456073e-07
Iter: 861 loss: 7.39614677e-07
Iter: 862 loss: 7.42311613e-07
Iter: 863 loss: 7.39387815e-07
Iter: 864 loss: 7.38549147e-07
Iter: 865 loss: 7.40539122e-07
Iter: 866 loss: 7.38238327e-07
Iter: 867 loss: 7.37373057e-07
Iter: 868 loss: 7.40379392e-07
Iter: 869 loss: 7.37190476e-07
Iter: 870 loss: 7.36280754e-07
Iter: 871 loss: 7.35477784e-07
Iter: 872 loss: 7.35218805e-07
Iter: 873 loss: 7.34648893e-07
Iter: 874 loss: 7.3434353e-07
Iter: 875 loss: 7.33762e-07
Iter: 876 loss: 7.33365255e-07
Iter: 877 loss: 7.33106958e-07
Iter: 878 loss: 7.32440469e-07
Iter: 879 loss: 7.31500791e-07
Iter: 880 loss: 7.31468219e-07
Iter: 881 loss: 7.30289798e-07
Iter: 882 loss: 7.40272753e-07
Iter: 883 loss: 7.30247336e-07
Iter: 884 loss: 7.29503313e-07
Iter: 885 loss: 7.2984858e-07
Iter: 886 loss: 7.28843872e-07
Iter: 887 loss: 7.27644476e-07
Iter: 888 loss: 7.33572165e-07
Iter: 889 loss: 7.2746252e-07
Iter: 890 loss: 7.26625672e-07
Iter: 891 loss: 7.26668247e-07
Iter: 892 loss: 7.25914219e-07
Iter: 893 loss: 7.24700499e-07
Iter: 894 loss: 7.32166882e-07
Iter: 895 loss: 7.24537301e-07
Iter: 896 loss: 7.23517246e-07
Iter: 897 loss: 7.27153179e-07
Iter: 898 loss: 7.23245364e-07
Iter: 899 loss: 7.22344623e-07
Iter: 900 loss: 7.25904897e-07
Iter: 901 loss: 7.22139873e-07
Iter: 902 loss: 7.21332412e-07
Iter: 903 loss: 7.22929258e-07
Iter: 904 loss: 7.2095952e-07
Iter: 905 loss: 7.20266826e-07
Iter: 906 loss: 7.25853113e-07
Iter: 907 loss: 7.2018986e-07
Iter: 908 loss: 7.19331638e-07
Iter: 909 loss: 7.18959768e-07
Iter: 910 loss: 7.18472904e-07
Iter: 911 loss: 7.17562443e-07
Iter: 912 loss: 7.18878482e-07
Iter: 913 loss: 7.1714328e-07
Iter: 914 loss: 7.16378281e-07
Iter: 915 loss: 7.16481281e-07
Iter: 916 loss: 7.15713043e-07
Iter: 917 loss: 7.14647626e-07
Iter: 918 loss: 7.20871299e-07
Iter: 919 loss: 7.1448693e-07
Iter: 920 loss: 7.13556346e-07
Iter: 921 loss: 7.15060594e-07
Iter: 922 loss: 7.13122859e-07
Iter: 923 loss: 7.12076883e-07
Iter: 924 loss: 7.15572355e-07
Iter: 925 loss: 7.11720418e-07
Iter: 926 loss: 7.10939844e-07
Iter: 927 loss: 7.11915845e-07
Iter: 928 loss: 7.10417e-07
Iter: 929 loss: 7.09258188e-07
Iter: 930 loss: 7.16541138e-07
Iter: 931 loss: 7.09117671e-07
Iter: 932 loss: 7.08315667e-07
Iter: 933 loss: 7.11166649e-07
Iter: 934 loss: 7.08157359e-07
Iter: 935 loss: 7.07227571e-07
Iter: 936 loss: 7.0792521e-07
Iter: 937 loss: 7.0679539e-07
Iter: 938 loss: 7.05906473e-07
Iter: 939 loss: 7.1440769e-07
Iter: 940 loss: 7.05861908e-07
Iter: 941 loss: 7.05221453e-07
Iter: 942 loss: 7.10488507e-07
Iter: 943 loss: 7.05190189e-07
Iter: 944 loss: 7.04696447e-07
Iter: 945 loss: 7.03720445e-07
Iter: 946 loss: 7.11460245e-07
Iter: 947 loss: 7.03508874e-07
Iter: 948 loss: 7.02403895e-07
Iter: 949 loss: 7.11660846e-07
Iter: 950 loss: 7.02309194e-07
Iter: 951 loss: 7.01519866e-07
Iter: 952 loss: 7.0146541e-07
Iter: 953 loss: 7.00851274e-07
Iter: 954 loss: 6.9973288e-07
Iter: 955 loss: 7.07474442e-07
Iter: 956 loss: 6.99585598e-07
Iter: 957 loss: 6.98708618e-07
Iter: 958 loss: 7.00964051e-07
Iter: 959 loss: 6.98410759e-07
Iter: 960 loss: 6.97395876e-07
Iter: 961 loss: 6.9829207e-07
Iter: 962 loss: 6.96872689e-07
Iter: 963 loss: 6.95969334e-07
Iter: 964 loss: 7.0451091e-07
Iter: 965 loss: 6.95934318e-07
Iter: 966 loss: 6.95235542e-07
Iter: 967 loss: 6.96862799e-07
Iter: 968 loss: 6.94922846e-07
Iter: 969 loss: 6.9411351e-07
Iter: 970 loss: 6.96174084e-07
Iter: 971 loss: 6.93896141e-07
Iter: 972 loss: 6.93137963e-07
Iter: 973 loss: 6.95128051e-07
Iter: 974 loss: 6.92867729e-07
Iter: 975 loss: 6.92121944e-07
Iter: 976 loss: 7.01532144e-07
Iter: 977 loss: 6.92129845e-07
Iter: 978 loss: 6.91667935e-07
Iter: 979 loss: 6.91104617e-07
Iter: 980 loss: 6.90962793e-07
Iter: 981 loss: 6.9034644e-07
Iter: 982 loss: 6.89460933e-07
Iter: 983 loss: 6.89384933e-07
Iter: 984 loss: 6.88043656e-07
Iter: 985 loss: 6.98651945e-07
Iter: 986 loss: 6.87987892e-07
Iter: 987 loss: 6.87128818e-07
Iter: 988 loss: 6.89247258e-07
Iter: 989 loss: 6.86837666e-07
Iter: 990 loss: 6.85934651e-07
Iter: 991 loss: 6.88217e-07
Iter: 992 loss: 6.85651344e-07
Iter: 993 loss: 6.84656527e-07
Iter: 994 loss: 6.87644274e-07
Iter: 995 loss: 6.84383622e-07
Iter: 996 loss: 6.8350846e-07
Iter: 997 loss: 6.85768782e-07
Iter: 998 loss: 6.83166377e-07
Iter: 999 loss: 6.82504435e-07
Iter: 1000 loss: 6.89647095e-07
Iter: 1001 loss: 6.82518476e-07
Iter: 1002 loss: 6.81974427e-07
Iter: 1003 loss: 6.81567258e-07
Iter: 1004 loss: 6.81434358e-07
Iter: 1005 loss: 6.80475296e-07
Iter: 1006 loss: 6.84973e-07
Iter: 1007 loss: 6.8023553e-07
Iter: 1008 loss: 6.79818925e-07
Iter: 1009 loss: 6.79822392e-07
Iter: 1010 loss: 6.79423692e-07
Iter: 1011 loss: 6.78897095e-07
Iter: 1012 loss: 6.78865547e-07
Iter: 1013 loss: 6.78143806e-07
Iter: 1014 loss: 6.77567471e-07
Iter: 1015 loss: 6.77344644e-07
Iter: 1016 loss: 6.76344825e-07
Iter: 1017 loss: 6.7890204e-07
Iter: 1018 loss: 6.76034574e-07
Iter: 1019 loss: 6.74715807e-07
Iter: 1020 loss: 6.7992471e-07
Iter: 1021 loss: 6.74425678e-07
Iter: 1022 loss: 6.73538352e-07
Iter: 1023 loss: 6.76665081e-07
Iter: 1024 loss: 6.73253567e-07
Iter: 1025 loss: 6.72215037e-07
Iter: 1026 loss: 6.73150566e-07
Iter: 1027 loss: 6.71630346e-07
Iter: 1028 loss: 6.70526674e-07
Iter: 1029 loss: 6.77733851e-07
Iter: 1030 loss: 6.70455847e-07
Iter: 1031 loss: 6.69589042e-07
Iter: 1032 loss: 6.71400358e-07
Iter: 1033 loss: 6.69264693e-07
Iter: 1034 loss: 6.68391408e-07
Iter: 1035 loss: 6.7395348e-07
Iter: 1036 loss: 6.68283406e-07
Iter: 1037 loss: 6.67672e-07
Iter: 1038 loss: 6.68496455e-07
Iter: 1039 loss: 6.67365555e-07
Iter: 1040 loss: 6.66656149e-07
Iter: 1041 loss: 6.74406749e-07
Iter: 1042 loss: 6.66674282e-07
Iter: 1043 loss: 6.66059464e-07
Iter: 1044 loss: 6.66495168e-07
Iter: 1045 loss: 6.65639163e-07
Iter: 1046 loss: 6.65079824e-07
Iter: 1047 loss: 6.64168851e-07
Iter: 1048 loss: 6.64207789e-07
Iter: 1049 loss: 6.62974969e-07
Iter: 1050 loss: 6.65685889e-07
Iter: 1051 loss: 6.62551031e-07
Iter: 1052 loss: 6.61595948e-07
Iter: 1053 loss: 6.66519441e-07
Iter: 1054 loss: 6.61473337e-07
Iter: 1055 loss: 6.60474029e-07
Iter: 1056 loss: 6.62187517e-07
Iter: 1057 loss: 6.60046e-07
Iter: 1058 loss: 6.59233137e-07
Iter: 1059 loss: 6.63778906e-07
Iter: 1060 loss: 6.59103478e-07
Iter: 1061 loss: 6.5832171e-07
Iter: 1062 loss: 6.59021e-07
Iter: 1063 loss: 6.57869919e-07
Iter: 1064 loss: 6.56998736e-07
Iter: 1065 loss: 6.63001742e-07
Iter: 1066 loss: 6.56930638e-07
Iter: 1067 loss: 6.56245618e-07
Iter: 1068 loss: 6.57794317e-07
Iter: 1069 loss: 6.55949407e-07
Iter: 1070 loss: 6.55105453e-07
Iter: 1071 loss: 6.56999191e-07
Iter: 1072 loss: 6.54792643e-07
Iter: 1073 loss: 6.54285941e-07
Iter: 1074 loss: 6.54275766e-07
Iter: 1075 loss: 6.53799361e-07
Iter: 1076 loss: 6.54387691e-07
Iter: 1077 loss: 6.53574034e-07
Iter: 1078 loss: 6.53034704e-07
Iter: 1079 loss: 6.52072231e-07
Iter: 1080 loss: 6.52068707e-07
Iter: 1081 loss: 6.51208097e-07
Iter: 1082 loss: 6.55692816e-07
Iter: 1083 loss: 6.5100545e-07
Iter: 1084 loss: 6.50226639e-07
Iter: 1085 loss: 6.50791435e-07
Iter: 1086 loss: 6.49698791e-07
Iter: 1087 loss: 6.48941068e-07
Iter: 1088 loss: 6.56707243e-07
Iter: 1089 loss: 6.48884679e-07
Iter: 1090 loss: 6.48185619e-07
Iter: 1091 loss: 6.47862464e-07
Iter: 1092 loss: 6.4757694e-07
Iter: 1093 loss: 6.4655228e-07
Iter: 1094 loss: 6.52678e-07
Iter: 1095 loss: 6.46468948e-07
Iter: 1096 loss: 6.45605553e-07
Iter: 1097 loss: 6.47373497e-07
Iter: 1098 loss: 6.45263754e-07
Iter: 1099 loss: 6.44505121e-07
Iter: 1100 loss: 6.49746198e-07
Iter: 1101 loss: 6.44370346e-07
Iter: 1102 loss: 6.4370937e-07
Iter: 1103 loss: 6.4476103e-07
Iter: 1104 loss: 6.43390592e-07
Iter: 1105 loss: 6.42639748e-07
Iter: 1106 loss: 6.46017384e-07
Iter: 1107 loss: 6.42452278e-07
Iter: 1108 loss: 6.41797783e-07
Iter: 1109 loss: 6.48188575e-07
Iter: 1110 loss: 6.41763279e-07
Iter: 1111 loss: 6.41369752e-07
Iter: 1112 loss: 6.4088465e-07
Iter: 1113 loss: 6.40824055e-07
Iter: 1114 loss: 6.40155577e-07
Iter: 1115 loss: 6.39825544e-07
Iter: 1116 loss: 6.39480959e-07
Iter: 1117 loss: 6.38572146e-07
Iter: 1118 loss: 6.42901114e-07
Iter: 1119 loss: 6.3840082e-07
Iter: 1120 loss: 6.37505195e-07
Iter: 1121 loss: 6.40326562e-07
Iter: 1122 loss: 6.37218e-07
Iter: 1123 loss: 6.36549657e-07
Iter: 1124 loss: 6.40114e-07
Iter: 1125 loss: 6.36434265e-07
Iter: 1126 loss: 6.35702e-07
Iter: 1127 loss: 6.34841285e-07
Iter: 1128 loss: 6.34689627e-07
Iter: 1129 loss: 6.33898935e-07
Iter: 1130 loss: 6.33855677e-07
Iter: 1131 loss: 6.33283776e-07
Iter: 1132 loss: 6.34543142e-07
Iter: 1133 loss: 6.33074535e-07
Iter: 1134 loss: 6.32362344e-07
Iter: 1135 loss: 6.33552816e-07
Iter: 1136 loss: 6.32052263e-07
Iter: 1137 loss: 6.31343823e-07
Iter: 1138 loss: 6.35296715e-07
Iter: 1139 loss: 6.3129e-07
Iter: 1140 loss: 6.30855936e-07
Iter: 1141 loss: 6.30875263e-07
Iter: 1142 loss: 6.30488387e-07
Iter: 1143 loss: 6.29852e-07
Iter: 1144 loss: 6.40682572e-07
Iter: 1145 loss: 6.29798e-07
Iter: 1146 loss: 6.29066392e-07
Iter: 1147 loss: 6.30914769e-07
Iter: 1148 loss: 6.28823273e-07
Iter: 1149 loss: 6.28058046e-07
Iter: 1150 loss: 6.27648888e-07
Iter: 1151 loss: 6.27251211e-07
Iter: 1152 loss: 6.26502e-07
Iter: 1153 loss: 6.38275424e-07
Iter: 1154 loss: 6.26457052e-07
Iter: 1155 loss: 6.25807729e-07
Iter: 1156 loss: 6.25800908e-07
Iter: 1157 loss: 6.25303642e-07
Iter: 1158 loss: 6.24449513e-07
Iter: 1159 loss: 6.32029582e-07
Iter: 1160 loss: 6.24442578e-07
Iter: 1161 loss: 6.23826054e-07
Iter: 1162 loss: 6.23793312e-07
Iter: 1163 loss: 6.23353e-07
Iter: 1164 loss: 6.22449932e-07
Iter: 1165 loss: 6.28013652e-07
Iter: 1166 loss: 6.22338234e-07
Iter: 1167 loss: 6.21643096e-07
Iter: 1168 loss: 6.25559778e-07
Iter: 1169 loss: 6.21538504e-07
Iter: 1170 loss: 6.2088543e-07
Iter: 1171 loss: 6.21749336e-07
Iter: 1172 loss: 6.20604453e-07
Iter: 1173 loss: 6.19989748e-07
Iter: 1174 loss: 6.27417592e-07
Iter: 1175 loss: 6.19977e-07
Iter: 1176 loss: 6.19392267e-07
Iter: 1177 loss: 6.19278353e-07
Iter: 1178 loss: 6.18980607e-07
Iter: 1179 loss: 6.18375623e-07
Iter: 1180 loss: 6.17933154e-07
Iter: 1181 loss: 6.17769729e-07
Iter: 1182 loss: 6.16903151e-07
Iter: 1183 loss: 6.20749063e-07
Iter: 1184 loss: 6.16758655e-07
Iter: 1185 loss: 6.15928343e-07
Iter: 1186 loss: 6.16066814e-07
Iter: 1187 loss: 6.15384579e-07
Iter: 1188 loss: 6.14546252e-07
Iter: 1189 loss: 6.22746825e-07
Iter: 1190 loss: 6.14488329e-07
Iter: 1191 loss: 6.13780742e-07
Iter: 1192 loss: 6.1427022e-07
Iter: 1193 loss: 6.1338784e-07
Iter: 1194 loss: 6.12496365e-07
Iter: 1195 loss: 6.17769842e-07
Iter: 1196 loss: 6.12378415e-07
Iter: 1197 loss: 6.11774794e-07
Iter: 1198 loss: 6.11693565e-07
Iter: 1199 loss: 6.11237169e-07
Iter: 1200 loss: 6.10155666e-07
Iter: 1201 loss: 6.17187425e-07
Iter: 1202 loss: 6.10046754e-07
Iter: 1203 loss: 6.09324047e-07
Iter: 1204 loss: 6.14162332e-07
Iter: 1205 loss: 6.09282267e-07
Iter: 1206 loss: 6.08843152e-07
Iter: 1207 loss: 6.12140354e-07
Iter: 1208 loss: 6.08760331e-07
Iter: 1209 loss: 6.08313144e-07
Iter: 1210 loss: 6.08236405e-07
Iter: 1211 loss: 6.07928939e-07
Iter: 1212 loss: 6.07344475e-07
Iter: 1213 loss: 6.07445202e-07
Iter: 1214 loss: 6.06904223e-07
Iter: 1215 loss: 6.06277695e-07
Iter: 1216 loss: 6.06729373e-07
Iter: 1217 loss: 6.05909236e-07
Iter: 1218 loss: 6.05008893e-07
Iter: 1219 loss: 6.07618858e-07
Iter: 1220 loss: 6.04729053e-07
Iter: 1221 loss: 6.04030788e-07
Iter: 1222 loss: 6.04918682e-07
Iter: 1223 loss: 6.03637432e-07
Iter: 1224 loss: 6.02619593e-07
Iter: 1225 loss: 6.05544301e-07
Iter: 1226 loss: 6.02281e-07
Iter: 1227 loss: 6.01437591e-07
Iter: 1228 loss: 6.07249831e-07
Iter: 1229 loss: 6.01370402e-07
Iter: 1230 loss: 6.00635644e-07
Iter: 1231 loss: 6.0071e-07
Iter: 1232 loss: 6.00034923e-07
Iter: 1233 loss: 5.99426471e-07
Iter: 1234 loss: 5.99444093e-07
Iter: 1235 loss: 5.98945462e-07
Iter: 1236 loss: 5.99465352e-07
Iter: 1237 loss: 5.98603492e-07
Iter: 1238 loss: 5.98040799e-07
Iter: 1239 loss: 6.01879037e-07
Iter: 1240 loss: 5.9802727e-07
Iter: 1241 loss: 5.97472649e-07
Iter: 1242 loss: 6.01411e-07
Iter: 1243 loss: 5.97431381e-07
Iter: 1244 loss: 5.9717695e-07
Iter: 1245 loss: 5.96626137e-07
Iter: 1246 loss: 6.07696e-07
Iter: 1247 loss: 5.96629548e-07
Iter: 1248 loss: 5.95775532e-07
Iter: 1249 loss: 5.95396841e-07
Iter: 1250 loss: 5.95028e-07
Iter: 1251 loss: 5.94095866e-07
Iter: 1252 loss: 6.0172755e-07
Iter: 1253 loss: 5.94022936e-07
Iter: 1254 loss: 5.9329443e-07
Iter: 1255 loss: 5.94017251e-07
Iter: 1256 loss: 5.92810579e-07
Iter: 1257 loss: 5.92022673e-07
Iter: 1258 loss: 5.94978133e-07
Iter: 1259 loss: 5.91809567e-07
Iter: 1260 loss: 5.90956517e-07
Iter: 1261 loss: 5.94301e-07
Iter: 1262 loss: 5.90793206e-07
Iter: 1263 loss: 5.90094942e-07
Iter: 1264 loss: 5.92077186e-07
Iter: 1265 loss: 5.89846366e-07
Iter: 1266 loss: 5.89113824e-07
Iter: 1267 loss: 5.90038781e-07
Iter: 1268 loss: 5.88755256e-07
Iter: 1269 loss: 5.88087573e-07
Iter: 1270 loss: 5.88095304e-07
Iter: 1271 loss: 5.87658747e-07
Iter: 1272 loss: 5.88073192e-07
Iter: 1273 loss: 5.87431771e-07
Iter: 1274 loss: 5.86872602e-07
Iter: 1275 loss: 5.91567414e-07
Iter: 1276 loss: 5.8684077e-07
Iter: 1277 loss: 5.86414728e-07
Iter: 1278 loss: 5.86112265e-07
Iter: 1279 loss: 5.85980672e-07
Iter: 1280 loss: 5.85479484e-07
Iter: 1281 loss: 5.85606131e-07
Iter: 1282 loss: 5.85092039e-07
Iter: 1283 loss: 5.84270595e-07
Iter: 1284 loss: 5.84450845e-07
Iter: 1285 loss: 5.8358205e-07
Iter: 1286 loss: 5.82859684e-07
Iter: 1287 loss: 5.89341084e-07
Iter: 1288 loss: 5.82846951e-07
Iter: 1289 loss: 5.82107646e-07
Iter: 1290 loss: 5.81738732e-07
Iter: 1291 loss: 5.81403469e-07
Iter: 1292 loss: 5.80721348e-07
Iter: 1293 loss: 5.91806952e-07
Iter: 1294 loss: 5.80747042e-07
Iter: 1295 loss: 5.80146434e-07
Iter: 1296 loss: 5.80014557e-07
Iter: 1297 loss: 5.79681e-07
Iter: 1298 loss: 5.78843867e-07
Iter: 1299 loss: 5.83055112e-07
Iter: 1300 loss: 5.78700906e-07
Iter: 1301 loss: 5.78143954e-07
Iter: 1302 loss: 5.81253062e-07
Iter: 1303 loss: 5.78038453e-07
Iter: 1304 loss: 5.7746206e-07
Iter: 1305 loss: 5.79149855e-07
Iter: 1306 loss: 5.77267315e-07
Iter: 1307 loss: 5.76848265e-07
Iter: 1308 loss: 5.82438929e-07
Iter: 1309 loss: 5.76820355e-07
Iter: 1310 loss: 5.76437799e-07
Iter: 1311 loss: 5.76143179e-07
Iter: 1312 loss: 5.75987087e-07
Iter: 1313 loss: 5.75445597e-07
Iter: 1314 loss: 5.75529498e-07
Iter: 1315 loss: 5.74998239e-07
Iter: 1316 loss: 5.74308558e-07
Iter: 1317 loss: 5.76399e-07
Iter: 1318 loss: 5.74121486e-07
Iter: 1319 loss: 5.73374791e-07
Iter: 1320 loss: 5.73729039e-07
Iter: 1321 loss: 5.7287707e-07
Iter: 1322 loss: 5.7207e-07
Iter: 1323 loss: 5.770803e-07
Iter: 1324 loss: 5.72004637e-07
Iter: 1325 loss: 5.71310409e-07
Iter: 1326 loss: 5.72367298e-07
Iter: 1327 loss: 5.70984866e-07
Iter: 1328 loss: 5.70334294e-07
Iter: 1329 loss: 5.73910427e-07
Iter: 1330 loss: 5.70207419e-07
Iter: 1331 loss: 5.69566339e-07
Iter: 1332 loss: 5.70356633e-07
Iter: 1333 loss: 5.69193844e-07
Iter: 1334 loss: 5.68609607e-07
Iter: 1335 loss: 5.74654791e-07
Iter: 1336 loss: 5.68555322e-07
Iter: 1337 loss: 5.68181576e-07
Iter: 1338 loss: 5.69855047e-07
Iter: 1339 loss: 5.68086932e-07
Iter: 1340 loss: 5.67694e-07
Iter: 1341 loss: 5.68622568e-07
Iter: 1342 loss: 5.67517588e-07
Iter: 1343 loss: 5.6694364e-07
Iter: 1344 loss: 5.68472217e-07
Iter: 1345 loss: 5.66718143e-07
Iter: 1346 loss: 5.66362189e-07
Iter: 1347 loss: 5.66133167e-07
Iter: 1348 loss: 5.66007486e-07
Iter: 1349 loss: 5.65341907e-07
Iter: 1350 loss: 5.65406765e-07
Iter: 1351 loss: 5.6478865e-07
Iter: 1352 loss: 5.64104425e-07
Iter: 1353 loss: 5.6873904e-07
Iter: 1354 loss: 5.6400188e-07
Iter: 1355 loss: 5.63389449e-07
Iter: 1356 loss: 5.62846e-07
Iter: 1357 loss: 5.62655487e-07
Iter: 1358 loss: 5.61960462e-07
Iter: 1359 loss: 5.61993e-07
Iter: 1360 loss: 5.61537888e-07
Iter: 1361 loss: 5.61385264e-07
Iter: 1362 loss: 5.61125432e-07
Iter: 1363 loss: 5.60388401e-07
Iter: 1364 loss: 5.64269726e-07
Iter: 1365 loss: 5.60269541e-07
Iter: 1366 loss: 5.59657224e-07
Iter: 1367 loss: 5.61292154e-07
Iter: 1368 loss: 5.59511477e-07
Iter: 1369 loss: 5.58903594e-07
Iter: 1370 loss: 5.61611614e-07
Iter: 1371 loss: 5.58761201e-07
Iter: 1372 loss: 5.58251145e-07
Iter: 1373 loss: 5.63630067e-07
Iter: 1374 loss: 5.5826365e-07
Iter: 1375 loss: 5.57939302e-07
Iter: 1376 loss: 5.59031719e-07
Iter: 1377 loss: 5.57814133e-07
Iter: 1378 loss: 5.57486942e-07
Iter: 1379 loss: 5.56601435e-07
Iter: 1380 loss: 5.66029257e-07
Iter: 1381 loss: 5.56541863e-07
Iter: 1382 loss: 5.55705242e-07
Iter: 1383 loss: 5.63466415e-07
Iter: 1384 loss: 5.55707118e-07
Iter: 1385 loss: 5.55171709e-07
Iter: 1386 loss: 5.55397094e-07
Iter: 1387 loss: 5.54791882e-07
Iter: 1388 loss: 5.54182236e-07
Iter: 1389 loss: 5.55957399e-07
Iter: 1390 loss: 5.53956681e-07
Iter: 1391 loss: 5.53221355e-07
Iter: 1392 loss: 5.53898929e-07
Iter: 1393 loss: 5.52829306e-07
Iter: 1394 loss: 5.52257e-07
Iter: 1395 loss: 5.60602246e-07
Iter: 1396 loss: 5.52277754e-07
Iter: 1397 loss: 5.51899632e-07
Iter: 1398 loss: 5.51512812e-07
Iter: 1399 loss: 5.51403332e-07
Iter: 1400 loss: 5.50741e-07
Iter: 1401 loss: 5.56450232e-07
Iter: 1402 loss: 5.50713821e-07
Iter: 1403 loss: 5.50226275e-07
Iter: 1404 loss: 5.51810558e-07
Iter: 1405 loss: 5.50131404e-07
Iter: 1406 loss: 5.49657784e-07
Iter: 1407 loss: 5.52270649e-07
Iter: 1408 loss: 5.49546883e-07
Iter: 1409 loss: 5.49166487e-07
Iter: 1410 loss: 5.52992617e-07
Iter: 1411 loss: 5.49139259e-07
Iter: 1412 loss: 5.48864762e-07
Iter: 1413 loss: 5.48459411e-07
Iter: 1414 loss: 5.48461458e-07
Iter: 1415 loss: 5.47818559e-07
Iter: 1416 loss: 5.47412128e-07
Iter: 1417 loss: 5.47184527e-07
Iter: 1418 loss: 5.46604952e-07
Iter: 1419 loss: 5.46603587e-07
Iter: 1420 loss: 5.46241836e-07
Iter: 1421 loss: 5.45817102e-07
Iter: 1422 loss: 5.4576725e-07
Iter: 1423 loss: 5.45125e-07
Iter: 1424 loss: 5.49124593e-07
Iter: 1425 loss: 5.45083367e-07
Iter: 1426 loss: 5.44546765e-07
Iter: 1427 loss: 5.4472315e-07
Iter: 1428 loss: 5.44245609e-07
Iter: 1429 loss: 5.43641647e-07
Iter: 1430 loss: 5.4839262e-07
Iter: 1431 loss: 5.43601232e-07
Iter: 1432 loss: 5.43080091e-07
Iter: 1433 loss: 5.42869657e-07
Iter: 1434 loss: 5.4264342e-07
Iter: 1435 loss: 5.42110797e-07
Iter: 1436 loss: 5.48012736e-07
Iter: 1437 loss: 5.42089083e-07
Iter: 1438 loss: 5.4167981e-07
Iter: 1439 loss: 5.41843178e-07
Iter: 1440 loss: 5.41428278e-07
Iter: 1441 loss: 5.40989163e-07
Iter: 1442 loss: 5.44783404e-07
Iter: 1443 loss: 5.4097319e-07
Iter: 1444 loss: 5.40515089e-07
Iter: 1445 loss: 5.41991255e-07
Iter: 1446 loss: 5.40417e-07
Iter: 1447 loss: 5.40118322e-07
Iter: 1448 loss: 5.39591554e-07
Iter: 1449 loss: 5.39596385e-07
Iter: 1450 loss: 5.38956272e-07
Iter: 1451 loss: 5.41921906e-07
Iter: 1452 loss: 5.38826498e-07
Iter: 1453 loss: 5.38350889e-07
Iter: 1454 loss: 5.38931e-07
Iter: 1455 loss: 5.3819042e-07
Iter: 1456 loss: 5.37620622e-07
Iter: 1457 loss: 5.40229564e-07
Iter: 1458 loss: 5.37540473e-07
Iter: 1459 loss: 5.37112385e-07
Iter: 1460 loss: 5.37189464e-07
Iter: 1461 loss: 5.36786501e-07
Iter: 1462 loss: 5.36205278e-07
Iter: 1463 loss: 5.3918086e-07
Iter: 1464 loss: 5.36056689e-07
Iter: 1465 loss: 5.35620245e-07
Iter: 1466 loss: 5.35862455e-07
Iter: 1467 loss: 5.35330059e-07
Iter: 1468 loss: 5.34882872e-07
Iter: 1469 loss: 5.34896458e-07
Iter: 1470 loss: 5.34620881e-07
Iter: 1471 loss: 5.34709557e-07
Iter: 1472 loss: 5.3440823e-07
Iter: 1473 loss: 5.33948764e-07
Iter: 1474 loss: 5.36274456e-07
Iter: 1475 loss: 5.33889931e-07
Iter: 1476 loss: 5.33578032e-07
Iter: 1477 loss: 5.35708125e-07
Iter: 1478 loss: 5.33563934e-07
Iter: 1479 loss: 5.33228331e-07
Iter: 1480 loss: 5.33589798e-07
Iter: 1481 loss: 5.33109528e-07
Iter: 1482 loss: 5.32833042e-07
Iter: 1483 loss: 5.32346e-07
Iter: 1484 loss: 5.32355443e-07
Iter: 1485 loss: 5.31921614e-07
Iter: 1486 loss: 5.35559707e-07
Iter: 1487 loss: 5.31902799e-07
Iter: 1488 loss: 5.3145186e-07
Iter: 1489 loss: 5.31725561e-07
Iter: 1490 loss: 5.31172645e-07
Iter: 1491 loss: 5.30780881e-07
Iter: 1492 loss: 5.33426771e-07
Iter: 1493 loss: 5.30723412e-07
Iter: 1494 loss: 5.30329544e-07
Iter: 1495 loss: 5.30005764e-07
Iter: 1496 loss: 5.29891395e-07
Iter: 1497 loss: 5.29275894e-07
Iter: 1498 loss: 5.32828835e-07
Iter: 1499 loss: 5.29168233e-07
Iter: 1500 loss: 5.28631631e-07
Iter: 1501 loss: 5.29407032e-07
Iter: 1502 loss: 5.28390387e-07
Iter: 1503 loss: 5.27882548e-07
Iter: 1504 loss: 5.33770731e-07
Iter: 1505 loss: 5.27907162e-07
Iter: 1506 loss: 5.27526936e-07
Iter: 1507 loss: 5.29274189e-07
Iter: 1508 loss: 5.27462e-07
Iter: 1509 loss: 5.27153702e-07
Iter: 1510 loss: 5.28832118e-07
Iter: 1511 loss: 5.27070256e-07
Iter: 1512 loss: 5.26806616e-07
Iter: 1513 loss: 5.27840768e-07
Iter: 1514 loss: 5.26732435e-07
Iter: 1515 loss: 5.26539623e-07
Iter: 1516 loss: 5.26529107e-07
Iter: 1517 loss: 5.26376198e-07
Iter: 1518 loss: 5.26063502e-07
Iter: 1519 loss: 5.256299e-07
Iter: 1520 loss: 5.25642861e-07
Iter: 1521 loss: 5.25152814e-07
Iter: 1522 loss: 5.30392185e-07
Iter: 1523 loss: 5.25152132e-07
Iter: 1524 loss: 5.2468647e-07
Iter: 1525 loss: 5.24875304e-07
Iter: 1526 loss: 5.24441873e-07
Iter: 1527 loss: 5.23986273e-07
Iter: 1528 loss: 5.26429403e-07
Iter: 1529 loss: 5.23937274e-07
Iter: 1530 loss: 5.23519418e-07
Iter: 1531 loss: 5.23628159e-07
Iter: 1532 loss: 5.23147264e-07
Iter: 1533 loss: 5.22708888e-07
Iter: 1534 loss: 5.25210226e-07
Iter: 1535 loss: 5.22631808e-07
Iter: 1536 loss: 5.22214521e-07
Iter: 1537 loss: 5.23157155e-07
Iter: 1538 loss: 5.22029836e-07
Iter: 1539 loss: 5.217031e-07
Iter: 1540 loss: 5.26332656e-07
Iter: 1541 loss: 5.21714e-07
Iter: 1542 loss: 5.2146288e-07
Iter: 1543 loss: 5.21908191e-07
Iter: 1544 loss: 5.2138688e-07
Iter: 1545 loss: 5.21067363e-07
Iter: 1546 loss: 5.22352536e-07
Iter: 1547 loss: 5.20999833e-07
Iter: 1548 loss: 5.2072221e-07
Iter: 1549 loss: 5.21358345e-07
Iter: 1550 loss: 5.20634558e-07
Iter: 1551 loss: 5.20387e-07
Iter: 1552 loss: 5.20156789e-07
Iter: 1553 loss: 5.2012507e-07
Iter: 1554 loss: 5.19680498e-07
Iter: 1555 loss: 5.2031487e-07
Iter: 1556 loss: 5.19521848e-07
Iter: 1557 loss: 5.19029925e-07
Iter: 1558 loss: 5.21314405e-07
Iter: 1559 loss: 5.1894375e-07
Iter: 1560 loss: 5.18489912e-07
Iter: 1561 loss: 5.19398554e-07
Iter: 1562 loss: 5.183702e-07
Iter: 1563 loss: 5.18041873e-07
Iter: 1564 loss: 5.1865004e-07
Iter: 1565 loss: 5.17915623e-07
Iter: 1566 loss: 5.17392209e-07
Iter: 1567 loss: 5.18511911e-07
Iter: 1568 loss: 5.17235378e-07
Iter: 1569 loss: 5.1691012e-07
Iter: 1570 loss: 5.1794791e-07
Iter: 1571 loss: 5.16814055e-07
Iter: 1572 loss: 5.16434056e-07
Iter: 1573 loss: 5.1791767e-07
Iter: 1574 loss: 5.16350497e-07
Iter: 1575 loss: 5.16098567e-07
Iter: 1576 loss: 5.16107661e-07
Iter: 1577 loss: 5.1591644e-07
Iter: 1578 loss: 5.1613e-07
Iter: 1579 loss: 5.15779391e-07
Iter: 1580 loss: 5.15553097e-07
Iter: 1581 loss: 5.15697081e-07
Iter: 1582 loss: 5.15407066e-07
Iter: 1583 loss: 5.15055774e-07
Iter: 1584 loss: 5.15618467e-07
Iter: 1585 loss: 5.14924295e-07
Iter: 1586 loss: 5.14668841e-07
Iter: 1587 loss: 5.14403382e-07
Iter: 1588 loss: 5.14337e-07
Iter: 1589 loss: 5.13825341e-07
Iter: 1590 loss: 5.16564683e-07
Iter: 1591 loss: 5.13756277e-07
Iter: 1592 loss: 5.13418797e-07
Iter: 1593 loss: 5.14721e-07
Iter: 1594 loss: 5.13336e-07
Iter: 1595 loss: 5.12954784e-07
Iter: 1596 loss: 5.13571649e-07
Iter: 1597 loss: 5.12794827e-07
Iter: 1598 loss: 5.1244217e-07
Iter: 1599 loss: 5.12439158e-07
Iter: 1600 loss: 5.12158408e-07
Iter: 1601 loss: 5.11663245e-07
Iter: 1602 loss: 5.16066223e-07
Iter: 1603 loss: 5.1164136e-07
Iter: 1604 loss: 5.1129507e-07
Iter: 1605 loss: 5.11390397e-07
Iter: 1606 loss: 5.11095493e-07
Iter: 1607 loss: 5.10719531e-07
Iter: 1608 loss: 5.10740961e-07
Iter: 1609 loss: 5.10433892e-07
Iter: 1610 loss: 5.11819565e-07
Iter: 1611 loss: 5.10395353e-07
Iter: 1612 loss: 5.1019731e-07
Iter: 1613 loss: 5.10427924e-07
Iter: 1614 loss: 5.10072482e-07
Iter: 1615 loss: 5.09761207e-07
Iter: 1616 loss: 5.09783604e-07
Iter: 1617 loss: 5.09492e-07
Iter: 1618 loss: 5.09130473e-07
Iter: 1619 loss: 5.1030861e-07
Iter: 1620 loss: 5.09021845e-07
Iter: 1621 loss: 5.08724781e-07
Iter: 1622 loss: 5.08611379e-07
Iter: 1623 loss: 5.08409812e-07
Iter: 1624 loss: 5.07995139e-07
Iter: 1625 loss: 5.09892629e-07
Iter: 1626 loss: 5.07890149e-07
Iter: 1627 loss: 5.07491791e-07
Iter: 1628 loss: 5.08091603e-07
Iter: 1629 loss: 5.07269476e-07
Iter: 1630 loss: 5.0690295e-07
Iter: 1631 loss: 5.09861763e-07
Iter: 1632 loss: 5.06836443e-07
Iter: 1633 loss: 5.06534434e-07
Iter: 1634 loss: 5.06116407e-07
Iter: 1635 loss: 5.06104755e-07
Iter: 1636 loss: 5.05553032e-07
Iter: 1637 loss: 5.09722554e-07
Iter: 1638 loss: 5.05489197e-07
Iter: 1639 loss: 5.05103458e-07
Iter: 1640 loss: 5.07093489e-07
Iter: 1641 loss: 5.04997956e-07
Iter: 1642 loss: 5.04764103e-07
Iter: 1643 loss: 5.04731702e-07
Iter: 1644 loss: 5.04552872e-07
Iter: 1645 loss: 5.04627792e-07
Iter: 1646 loss: 5.04394507e-07
Iter: 1647 loss: 5.04122681e-07
Iter: 1648 loss: 5.044148e-07
Iter: 1649 loss: 5.03976594e-07
Iter: 1650 loss: 5.03567207e-07
Iter: 1651 loss: 5.03888941e-07
Iter: 1652 loss: 5.03368767e-07
Iter: 1653 loss: 5.02997864e-07
Iter: 1654 loss: 5.03921e-07
Iter: 1655 loss: 5.02898388e-07
Iter: 1656 loss: 5.02484852e-07
Iter: 1657 loss: 5.02600756e-07
Iter: 1658 loss: 5.02135777e-07
Iter: 1659 loss: 5.01710474e-07
Iter: 1660 loss: 5.03375588e-07
Iter: 1661 loss: 5.01568138e-07
Iter: 1662 loss: 5.01131808e-07
Iter: 1663 loss: 5.0247877e-07
Iter: 1664 loss: 5.00960937e-07
Iter: 1665 loss: 5.00643637e-07
Iter: 1666 loss: 5.03010199e-07
Iter: 1667 loss: 5.00643296e-07
Iter: 1668 loss: 5.00309397e-07
Iter: 1669 loss: 5.00044564e-07
Iter: 1670 loss: 4.9997584e-07
Iter: 1671 loss: 4.99488692e-07
Iter: 1672 loss: 5.00961505e-07
Iter: 1673 loss: 4.99421787e-07
Iter: 1674 loss: 4.99095961e-07
Iter: 1675 loss: 5.03969e-07
Iter: 1676 loss: 4.99080102e-07
Iter: 1677 loss: 4.98817485e-07
Iter: 1678 loss: 5.00668932e-07
Iter: 1679 loss: 4.98810834e-07
Iter: 1680 loss: 4.9860455e-07
Iter: 1681 loss: 4.98674694e-07
Iter: 1682 loss: 4.98452437e-07
Iter: 1683 loss: 4.98165264e-07
Iter: 1684 loss: 4.98000134e-07
Iter: 1685 loss: 4.97871042e-07
Iter: 1686 loss: 4.97543169e-07
Iter: 1687 loss: 4.99763189e-07
Iter: 1688 loss: 4.97479391e-07
Iter: 1689 loss: 4.97234169e-07
Iter: 1690 loss: 4.97134408e-07
Iter: 1691 loss: 4.96942675e-07
Iter: 1692 loss: 4.96513508e-07
Iter: 1693 loss: 4.97536405e-07
Iter: 1694 loss: 4.96336611e-07
Iter: 1695 loss: 4.95913866e-07
Iter: 1696 loss: 4.96458597e-07
Iter: 1697 loss: 4.95686663e-07
Iter: 1698 loss: 4.9509606e-07
Iter: 1699 loss: 4.98857389e-07
Iter: 1700 loss: 4.95020174e-07
Iter: 1701 loss: 4.94684173e-07
Iter: 1702 loss: 4.95137954e-07
Iter: 1703 loss: 4.94517167e-07
Iter: 1704 loss: 4.94066967e-07
Iter: 1705 loss: 4.95377208e-07
Iter: 1706 loss: 4.93928383e-07
Iter: 1707 loss: 4.93563789e-07
Iter: 1708 loss: 4.9429616e-07
Iter: 1709 loss: 4.93399284e-07
Iter: 1710 loss: 4.93343805e-07
Iter: 1711 loss: 4.93228185e-07
Iter: 1712 loss: 4.93060782e-07
Iter: 1713 loss: 4.92972163e-07
Iter: 1714 loss: 4.92901108e-07
Iter: 1715 loss: 4.92672712e-07
Iter: 1716 loss: 4.92830225e-07
Iter: 1717 loss: 4.92519177e-07
Iter: 1718 loss: 4.92077504e-07
Iter: 1719 loss: 4.92256163e-07
Iter: 1720 loss: 4.91810624e-07
Iter: 1721 loss: 4.91442336e-07
Iter: 1722 loss: 4.92980348e-07
Iter: 1723 loss: 4.91421474e-07
Iter: 1724 loss: 4.9107723e-07
Iter: 1725 loss: 4.91353887e-07
Iter: 1726 loss: 4.90846674e-07
Iter: 1727 loss: 4.90433592e-07
Iter: 1728 loss: 4.91611956e-07
Iter: 1729 loss: 4.90271873e-07
Iter: 1730 loss: 4.89922854e-07
Iter: 1731 loss: 4.90438879e-07
Iter: 1732 loss: 4.89734e-07
Iter: 1733 loss: 4.89372e-07
Iter: 1734 loss: 4.91961714e-07
Iter: 1735 loss: 4.89323213e-07
Iter: 1736 loss: 4.88993635e-07
Iter: 1737 loss: 4.88725846e-07
Iter: 1738 loss: 4.88614944e-07
Iter: 1739 loss: 4.88156843e-07
Iter: 1740 loss: 4.92969832e-07
Iter: 1741 loss: 4.88157127e-07
Iter: 1742 loss: 4.87816123e-07
Iter: 1743 loss: 4.88203625e-07
Iter: 1744 loss: 4.87675777e-07
Iter: 1745 loss: 4.87357397e-07
Iter: 1746 loss: 4.87363593e-07
Iter: 1747 loss: 4.87233535e-07
Iter: 1748 loss: 4.8700258e-07
Iter: 1749 loss: 4.86990416e-07
Iter: 1750 loss: 4.86675162e-07
Iter: 1751 loss: 4.87496891e-07
Iter: 1752 loss: 4.86564318e-07
Iter: 1753 loss: 4.86201e-07
Iter: 1754 loss: 4.86525e-07
Iter: 1755 loss: 4.86003842e-07
Iter: 1756 loss: 4.85673127e-07
Iter: 1757 loss: 4.85666646e-07
Iter: 1758 loss: 4.85434e-07
Iter: 1759 loss: 4.84903865e-07
Iter: 1760 loss: 4.87461648e-07
Iter: 1761 loss: 4.84773864e-07
Iter: 1762 loss: 4.84397276e-07
Iter: 1763 loss: 4.84432746e-07
Iter: 1764 loss: 4.84115617e-07
Iter: 1765 loss: 4.83705662e-07
Iter: 1766 loss: 4.87064312e-07
Iter: 1767 loss: 4.83685653e-07
Iter: 1768 loss: 4.83362328e-07
Iter: 1769 loss: 4.82907069e-07
Iter: 1770 loss: 4.82870746e-07
Iter: 1771 loss: 4.82448286e-07
Iter: 1772 loss: 4.8243794e-07
Iter: 1773 loss: 4.82121493e-07
Iter: 1774 loss: 4.82456585e-07
Iter: 1775 loss: 4.81887753e-07
Iter: 1776 loss: 4.81556526e-07
Iter: 1777 loss: 4.8338859e-07
Iter: 1778 loss: 4.81508209e-07
Iter: 1779 loss: 4.81215125e-07
Iter: 1780 loss: 4.81199e-07
Iter: 1781 loss: 4.81044367e-07
Iter: 1782 loss: 4.80709389e-07
Iter: 1783 loss: 4.80697054e-07
Iter: 1784 loss: 4.80473147e-07
Iter: 1785 loss: 4.82537644e-07
Iter: 1786 loss: 4.80433869e-07
Iter: 1787 loss: 4.80203653e-07
Iter: 1788 loss: 4.80483322e-07
Iter: 1789 loss: 4.80104688e-07
Iter: 1790 loss: 4.79830248e-07
Iter: 1791 loss: 4.79814844e-07
Iter: 1792 loss: 4.7960043e-07
Iter: 1793 loss: 4.79232199e-07
Iter: 1794 loss: 4.79246069e-07
Iter: 1795 loss: 4.78880111e-07
Iter: 1796 loss: 4.78552465e-07
Iter: 1797 loss: 4.78537743e-07
Iter: 1798 loss: 4.78296784e-07
Iter: 1799 loss: 4.78210779e-07
Iter: 1800 loss: 4.78090897e-07
Iter: 1801 loss: 4.77704248e-07
Iter: 1802 loss: 4.79008179e-07
Iter: 1803 loss: 4.77623473e-07
Iter: 1804 loss: 4.7732e-07
Iter: 1805 loss: 4.77425772e-07
Iter: 1806 loss: 4.77074309e-07
Iter: 1807 loss: 4.76689308e-07
Iter: 1808 loss: 4.80512313e-07
Iter: 1809 loss: 4.7671881e-07
Iter: 1810 loss: 4.76479386e-07
Iter: 1811 loss: 4.76544358e-07
Iter: 1812 loss: 4.76223e-07
Iter: 1813 loss: 4.76080032e-07
Iter: 1814 loss: 4.76015657e-07
Iter: 1815 loss: 4.75901032e-07
Iter: 1816 loss: 4.75727461e-07
Iter: 1817 loss: 4.75707679e-07
Iter: 1818 loss: 4.75482e-07
Iter: 1819 loss: 4.75206e-07
Iter: 1820 loss: 4.75176194e-07
Iter: 1821 loss: 4.74897831e-07
Iter: 1822 loss: 4.74887372e-07
Iter: 1823 loss: 4.7464826e-07
Iter: 1824 loss: 4.7451573e-07
Iter: 1825 loss: 4.74422393e-07
Iter: 1826 loss: 4.73985324e-07
Iter: 1827 loss: 4.74323571e-07
Iter: 1828 loss: 4.7375471e-07
Iter: 1829 loss: 4.73403247e-07
Iter: 1830 loss: 4.75648e-07
Iter: 1831 loss: 4.73306e-07
Iter: 1832 loss: 4.72969646e-07
Iter: 1833 loss: 4.73471971e-07
Iter: 1834 loss: 4.7282245e-07
Iter: 1835 loss: 4.7243023e-07
Iter: 1836 loss: 4.7423751e-07
Iter: 1837 loss: 4.72376684e-07
Iter: 1838 loss: 4.72001489e-07
Iter: 1839 loss: 4.7241889e-07
Iter: 1840 loss: 4.71860062e-07
Iter: 1841 loss: 4.71493e-07
Iter: 1842 loss: 4.71716788e-07
Iter: 1843 loss: 4.71277303e-07
Iter: 1844 loss: 4.70952e-07
Iter: 1845 loss: 4.7095682e-07
Iter: 1846 loss: 4.70794873e-07
Iter: 1847 loss: 4.70804537e-07
Iter: 1848 loss: 4.70653902e-07
Iter: 1849 loss: 4.70293173e-07
Iter: 1850 loss: 4.7452329e-07
Iter: 1851 loss: 4.70325119e-07
Iter: 1852 loss: 4.69984144e-07
Iter: 1853 loss: 4.71249e-07
Iter: 1854 loss: 4.69928e-07
Iter: 1855 loss: 4.69689269e-07
Iter: 1856 loss: 4.70156209e-07
Iter: 1857 loss: 4.69596301e-07
Iter: 1858 loss: 4.69292956e-07
Iter: 1859 loss: 4.70146801e-07
Iter: 1860 loss: 4.69137404e-07
Iter: 1861 loss: 4.68882689e-07
Iter: 1862 loss: 4.69420655e-07
Iter: 1863 loss: 4.68785231e-07
Iter: 1864 loss: 4.68495983e-07
Iter: 1865 loss: 4.68546801e-07
Iter: 1866 loss: 4.68275772e-07
Iter: 1867 loss: 4.67948098e-07
Iter: 1868 loss: 4.69788915e-07
Iter: 1869 loss: 4.67880227e-07
Iter: 1870 loss: 4.67560568e-07
Iter: 1871 loss: 4.67787402e-07
Iter: 1872 loss: 4.67322621e-07
Iter: 1873 loss: 4.66942083e-07
Iter: 1874 loss: 4.69182282e-07
Iter: 1875 loss: 4.66889617e-07
Iter: 1876 loss: 4.6663888e-07
Iter: 1877 loss: 4.66557054e-07
Iter: 1878 loss: 4.66391214e-07
Iter: 1879 loss: 4.66148919e-07
Iter: 1880 loss: 4.66142751e-07
Iter: 1881 loss: 4.65892754e-07
Iter: 1882 loss: 4.67122703e-07
Iter: 1883 loss: 4.65871238e-07
Iter: 1884 loss: 4.65686e-07
Iter: 1885 loss: 4.65332761e-07
Iter: 1886 loss: 4.70454125e-07
Iter: 1887 loss: 4.65339468e-07
Iter: 1888 loss: 4.65055393e-07
Iter: 1889 loss: 4.68722789e-07
Iter: 1890 loss: 4.65038511e-07
Iter: 1891 loss: 4.64809148e-07
Iter: 1892 loss: 4.6459536e-07
Iter: 1893 loss: 4.64537777e-07
Iter: 1894 loss: 4.64087663e-07
Iter: 1895 loss: 4.67612608e-07
Iter: 1896 loss: 4.6406285e-07
Iter: 1897 loss: 4.63849119e-07
Iter: 1898 loss: 4.63577265e-07
Iter: 1899 loss: 4.6359537e-07
Iter: 1900 loss: 4.63130704e-07
Iter: 1901 loss: 4.64735876e-07
Iter: 1902 loss: 4.62983508e-07
Iter: 1903 loss: 4.62645261e-07
Iter: 1904 loss: 4.63238649e-07
Iter: 1905 loss: 4.62504204e-07
Iter: 1906 loss: 4.62116873e-07
Iter: 1907 loss: 4.63474066e-07
Iter: 1908 loss: 4.62011599e-07
Iter: 1909 loss: 4.61751313e-07
Iter: 1910 loss: 4.6222533e-07
Iter: 1911 loss: 4.61611876e-07
Iter: 1912 loss: 4.61261749e-07
Iter: 1913 loss: 4.61650757e-07
Iter: 1914 loss: 4.61027e-07
Iter: 1915 loss: 4.60734611e-07
Iter: 1916 loss: 4.6072347e-07
Iter: 1917 loss: 4.60483278e-07
Iter: 1918 loss: 4.61296935e-07
Iter: 1919 loss: 4.60361463e-07
Iter: 1920 loss: 4.60163903e-07
Iter: 1921 loss: 4.59686049e-07
Iter: 1922 loss: 4.68770736e-07
Iter: 1923 loss: 4.59706e-07
Iter: 1924 loss: 4.59319381e-07
Iter: 1925 loss: 4.59655269e-07
Iter: 1926 loss: 4.59105848e-07
Iter: 1927 loss: 4.58614949e-07
Iter: 1928 loss: 4.64330839e-07
Iter: 1929 loss: 4.58658178e-07
Iter: 1930 loss: 4.58331982e-07
Iter: 1931 loss: 4.59943749e-07
Iter: 1932 loss: 4.58308733e-07
Iter: 1933 loss: 4.57978899e-07
Iter: 1934 loss: 4.57843612e-07
Iter: 1935 loss: 4.57695933e-07
Iter: 1936 loss: 4.5732142e-07
Iter: 1937 loss: 4.593673e-07
Iter: 1938 loss: 4.57266e-07
Iter: 1939 loss: 4.56949778e-07
Iter: 1940 loss: 4.56748666e-07
Iter: 1941 loss: 4.56634183e-07
Iter: 1942 loss: 4.56202201e-07
Iter: 1943 loss: 4.59255233e-07
Iter: 1944 loss: 4.56223e-07
Iter: 1945 loss: 4.55841189e-07
Iter: 1946 loss: 4.55728667e-07
Iter: 1947 loss: 4.55531136e-07
Iter: 1948 loss: 4.55181436e-07
Iter: 1949 loss: 4.59540161e-07
Iter: 1950 loss: 4.55187205e-07
Iter: 1951 loss: 4.54943461e-07
Iter: 1952 loss: 4.57697638e-07
Iter: 1953 loss: 4.54924077e-07
Iter: 1954 loss: 4.54688433e-07
Iter: 1955 loss: 4.54809e-07
Iter: 1956 loss: 4.54545471e-07
Iter: 1957 loss: 4.5431986e-07
Iter: 1958 loss: 4.54095129e-07
Iter: 1959 loss: 4.54076769e-07
Iter: 1960 loss: 4.5365033e-07
Iter: 1961 loss: 4.54958922e-07
Iter: 1962 loss: 4.53564496e-07
Iter: 1963 loss: 4.53289374e-07
Iter: 1964 loss: 4.53814835e-07
Iter: 1965 loss: 4.53142e-07
Iter: 1966 loss: 4.52779744e-07
Iter: 1967 loss: 4.532601e-07
Iter: 1968 loss: 4.52572664e-07
Iter: 1969 loss: 4.52223162e-07
Iter: 1970 loss: 4.56914e-07
Iter: 1971 loss: 4.52207644e-07
Iter: 1972 loss: 4.51948495e-07
Iter: 1973 loss: 4.5163776e-07
Iter: 1974 loss: 4.51613602e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.4
+ date
Thu Oct 22 08:07:23 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/500_500_500_500_1 --function f1 --psi -2 --phi 2.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e58e4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e57f37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e57f3b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e57f3d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e578e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e578e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e5630d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e5625598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e56e4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e55a2e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e55aa510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e5575d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e5567598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e5567268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e5660a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e56609d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e567e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15deb30c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15dead4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15dead6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e5558840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15dea5d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15dea6e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15dea6e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e54e1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15e54e1d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15de97d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15de997950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15de997840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15dea90378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15de8f4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15de9ed6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15de9ed7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15de9df400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15de8c18c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f15de89b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.17543577
test_loss: 0.18880711
train_loss: 0.15810627
test_loss: 0.21586935
train_loss: 0.18821536
test_loss: 0.21936728
train_loss: 0.19538586
test_loss: 0.22008699
train_loss: 0.1994115
test_loss: 0.21943717
train_loss: 0.19357343
test_loss: 0.21951528
train_loss: 0.19566591
test_loss: 0.21964708
train_loss: 0.17524654
test_loss: 0.21783723
train_loss: 0.18558574
test_loss: 0.20934278
train_loss: 0.17531535
test_loss: 0.18617569
train_loss: 0.15373376
test_loss: 0.16913979
train_loss: 0.11301959
test_loss: 0.1551435
train_loss: 0.1074073
test_loss: 0.14545223
train_loss: 0.10184331
test_loss: 0.13548219
train_loss: 0.1021582
test_loss: 0.12769179
train_loss: 0.114939384
test_loss: 0.12410788
train_loss: 0.08505653
test_loss: 0.11826547
train_loss: 0.10184612
test_loss: 0.1138641
train_loss: 0.112132594
test_loss: 0.118645884
train_loss: 0.09136198
test_loss: 0.115120314
train_loss: 0.09028788
test_loss: 0.11264397
train_loss: 0.12558052
test_loss: 0.10771983
train_loss: 0.0777326
test_loss: 0.10518065
train_loss: 0.07432586
test_loss: 0.10194344
train_loss: 0.073524535
test_loss: 0.09993369
train_loss: 0.07273325
test_loss: 0.09605908
train_loss: 0.07401864
test_loss: 0.09496893
train_loss: 0.081115864
test_loss: 0.090026304
train_loss: 0.07205944
test_loss: 0.089036666
train_loss: 0.08461321
test_loss: 0.085087575
train_loss: 0.07909055
test_loss: 0.085574575
train_loss: 0.07537142
test_loss: 0.080506235
train_loss: 0.07431945
test_loss: 0.078387864
train_loss: 0.06547596
test_loss: 0.07682352
train_loss: 0.07084334
test_loss: 0.07279359
train_loss: 0.06735786
test_loss: 0.07299886
train_loss: 0.055623405
test_loss: 0.07322563
train_loss: 0.05247878
test_loss: 0.070202015
train_loss: 0.049248457
test_loss: 0.06856804
train_loss: 0.044271294
test_loss: 0.06759083
train_loss: 0.05517799
test_loss: 0.06848946
train_loss: 0.04257074
test_loss: 0.06394926
train_loss: 0.05757897
test_loss: 0.06394791
train_loss: 0.04282642
test_loss: 0.064024635
train_loss: 0.039960742
test_loss: 0.06427068
train_loss: 0.05482326
test_loss: 0.061648704
train_loss: 0.062911384
test_loss: 0.06263294
train_loss: 0.049826898
test_loss: 0.060099103
train_loss: 0.03863293
test_loss: 0.059397567
train_loss: 0.04123574
test_loss: 0.060136575
train_loss: 0.03624263
test_loss: 0.05765587
train_loss: 0.05274506
test_loss: 0.06627511
train_loss: 0.03953752
test_loss: 0.06116053
train_loss: 0.07073889
test_loss: 0.059296247
train_loss: 0.041506913
test_loss: 0.056806386
train_loss: 0.036516268
test_loss: 0.055319823
train_loss: 0.034881324
test_loss: 0.054450557
train_loss: 0.035809968
test_loss: 0.054280765
train_loss: 0.05375541
test_loss: 0.05282556
train_loss: 0.058282636
test_loss: 0.052352753
train_loss: 0.04264045
test_loss: 0.050579857
train_loss: 0.03776569
test_loss: 0.053209852
train_loss: 0.031733412
test_loss: 0.05006326
train_loss: 0.035096735
test_loss: 0.049535345
train_loss: 0.033314236
test_loss: 0.049679894
train_loss: 0.034069102
test_loss: 0.048646905
train_loss: 0.029377392
test_loss: 0.04919953
train_loss: 0.0439755
test_loss: 0.047899365
train_loss: 0.031005584
test_loss: 0.04764216
train_loss: 0.029972926
test_loss: 0.048556525
train_loss: 0.030837256
test_loss: 0.046943415
train_loss: 0.031801492
test_loss: 0.04801437
train_loss: 0.028973985
test_loss: 0.046712164
train_loss: 0.02755592
test_loss: 0.046802472
train_loss: 0.029702805
test_loss: 0.046009816
train_loss: 0.027906075
test_loss: 0.046029393
train_loss: 0.0280632
test_loss: 0.04564324
train_loss: 0.0270624
test_loss: 0.04530047
train_loss: 0.027296614
test_loss: 0.04528835
train_loss: 0.028233306
test_loss: 0.045719784
train_loss: 0.028046364
test_loss: 0.0452572
train_loss: 0.026770592
test_loss: 0.044116266
train_loss: 0.039874285
test_loss: 0.04471422
train_loss: 0.026583608
test_loss: 0.04698724
train_loss: 0.025833996
test_loss: 0.044229686
train_loss: 0.025619287
test_loss: 0.044236604
train_loss: 0.028142337
test_loss: 0.043718297
train_loss: 0.025554683
test_loss: 0.044675026
train_loss: 0.02565382
test_loss: 0.04215885
train_loss: 0.034179334
test_loss: 0.04418067
train_loss: 0.028914679
test_loss: 0.043818686
train_loss: 0.02596264
test_loss: 0.04229666
train_loss: 0.027968213
test_loss: 0.042120274
train_loss: 0.02543803
test_loss: 0.044095132
train_loss: 0.025457412
test_loss: 0.04185234
train_loss: 0.024092395
test_loss: 0.043160368
train_loss: 0.02389431
test_loss: 0.04213351
train_loss: 0.027690316
test_loss: 0.042402375
train_loss: 0.023648117
test_loss: 0.043642007
train_loss: 0.02233788
test_loss: 0.039400432
train_loss: 0.023082804
test_loss: 0.042645507
train_loss: 0.024251282
test_loss: 0.04295301
train_loss: 0.022353947
test_loss: 0.040603537
train_loss: 0.022811702
test_loss: 0.040069383
train_loss: 0.022909481
test_loss: 0.04163952
train_loss: 0.021495149
test_loss: 0.039681647
train_loss: 0.023039835
test_loss: 0.042002544
train_loss: 0.023725513
test_loss: 0.041611277
train_loss: 0.022517933
test_loss: 0.03972958
train_loss: 0.024697172
test_loss: 0.040003747
train_loss: 0.021402383
test_loss: 0.040987864
train_loss: 0.024018215
test_loss: 0.038748827
train_loss: 0.020204129
test_loss: 0.04018
train_loss: 0.020049172
test_loss: 0.040695135
train_loss: 0.020498531
test_loss: 0.04022851
train_loss: 0.01985527
test_loss: 0.03940103
train_loss: 0.020622557
test_loss: 0.039243188
train_loss: 0.021827431
test_loss: 0.0417684
train_loss: 0.0210678
test_loss: 0.03968405
train_loss: 0.020216
test_loss: 0.039550748
train_loss: 0.018969998
test_loss: 0.03814066
train_loss: 0.017770236
test_loss: 0.039777048
train_loss: 0.0208912
test_loss: 0.03938711
train_loss: 0.019111603
test_loss: 0.036675777
train_loss: 0.020608012
test_loss: 0.04095111
train_loss: 0.019452883
test_loss: 0.038465142
train_loss: 0.0188216
test_loss: 0.03808577
train_loss: 0.019776603
test_loss: 0.0362747
train_loss: 0.020197121
test_loss: 0.038501047
train_loss: 0.017356183
test_loss: 0.038016047
train_loss: 0.01825916
test_loss: 0.037536543
train_loss: 0.018163491
test_loss: 0.037393976
train_loss: 0.01872413
test_loss: 0.03711296
train_loss: 0.017700506
test_loss: 0.03893848
train_loss: 0.01756082
test_loss: 0.038973738
train_loss: 0.017359901
test_loss: 0.037022874
train_loss: 0.01796093
test_loss: 0.038013205
train_loss: 0.016727572
test_loss: 0.036801707
train_loss: 0.016237123
test_loss: 0.03733142
train_loss: 0.016541658
test_loss: 0.036658842
train_loss: 0.017067986
test_loss: 0.037855845
train_loss: 0.01597589
test_loss: 0.037047848
train_loss: 0.016608797
test_loss: 0.03482935
train_loss: 0.015586067
test_loss: 0.037129965
train_loss: 0.016325353
test_loss: 0.036036603
train_loss: 0.015715703
test_loss: 0.036698654
train_loss: 0.016767278
test_loss: 0.0364719
train_loss: 0.016856283
test_loss: 0.03677532
train_loss: 0.015299225
test_loss: 0.03611
train_loss: 0.015668565
test_loss: 0.035496317
train_loss: 0.016219536
test_loss: 0.037816048
train_loss: 0.016301379
test_loss: 0.034703135
train_loss: 0.015332887
test_loss: 0.036850166
train_loss: 0.015436301
test_loss: 0.03635074
train_loss: 0.0153081
test_loss: 0.034398813
train_loss: 0.017593624
test_loss: 0.0374798
train_loss: 0.016395664
test_loss: 0.035436157
train_loss: 0.014216989
test_loss: 0.03425035
train_loss: 0.014182204
test_loss: 0.03563323
train_loss: 0.014276693
test_loss: 0.03558642
train_loss: 0.014020181
test_loss: 0.034262165
train_loss: 0.016343964
test_loss: 0.034145042
train_loss: 0.014270778
test_loss: 0.035127208
train_loss: 0.01437626
test_loss: 0.034965303
train_loss: 0.013397818
test_loss: 0.034810673
train_loss: 0.013370937
test_loss: 0.03458882
train_loss: 0.014846602
test_loss: 0.033169113
train_loss: 0.018783465
test_loss: 0.035552327
train_loss: 0.013127994
test_loss: 0.033856753
train_loss: 0.013571024
test_loss: 0.03381049
train_loss: 0.015603861
test_loss: 0.03400589
train_loss: 0.01256909
test_loss: 0.03270456
train_loss: 0.015238544
test_loss: 0.033050366
train_loss: 0.012399249
test_loss: 0.0320942
train_loss: 0.012111415
test_loss: 0.034887344
train_loss: 0.012219604
test_loss: 0.03297073
train_loss: 0.013065768
test_loss: 0.03269786
train_loss: 0.012985915
test_loss: 0.032507706
train_loss: 0.012049487
test_loss: 0.031834833
train_loss: 0.015637914
test_loss: 0.03246809
train_loss: 0.013000699
test_loss: 0.034506842
train_loss: 0.012983911
test_loss: 0.0326205
train_loss: 0.011864558
test_loss: 0.031944443
train_loss: 0.011914165
test_loss: 0.033905957
train_loss: 0.011699262
test_loss: 0.032292902
train_loss: 0.01129331
test_loss: 0.031942893
train_loss: 0.011617122
test_loss: 0.033458848
train_loss: 0.011485983
test_loss: 0.031755302
train_loss: 0.011500254
test_loss: 0.031329364
train_loss: 0.011345746
test_loss: 0.032550465
train_loss: 0.011890214
test_loss: 0.030801035
train_loss: 0.0108403545
test_loss: 0.031294335
train_loss: 0.01148693
test_loss: 0.031014644
train_loss: 0.011770615
test_loss: 0.031881634
train_loss: 0.012974137
test_loss: 0.030148638
train_loss: 0.011055683
test_loss: 0.03140746
train_loss: 0.011477966
test_loss: 0.030884229
train_loss: 0.010874707
test_loss: 0.031719547
train_loss: 0.01088304
test_loss: 0.030709706
train_loss: 0.011092532
test_loss: 0.030464835
train_loss: 0.009933993
test_loss: 0.03018742
train_loss: 0.009985147
test_loss: 0.030840082
train_loss: 0.009766245
test_loss: 0.030048134
train_loss: 0.009848368
test_loss: 0.029762171
train_loss: 0.00971913
test_loss: 0.030107277
train_loss: 0.010619163
test_loss: 0.030296214
train_loss: 0.009520357
test_loss: 0.030108074
train_loss: 0.010037099
test_loss: 0.030140547
train_loss: 0.010048387
test_loss: 0.02976941
train_loss: 0.009546889
test_loss: 0.029562088
train_loss: 0.009515865
test_loss: 0.02923832
train_loss: 0.010013454
test_loss: 0.02905768
train_loss: 0.009709936
test_loss: 0.029528752
train_loss: 0.010172383
test_loss: 0.02945833
train_loss: 0.009509398
test_loss: 0.029744329
train_loss: 0.010560924
test_loss: 0.028177967
train_loss: 0.009886522
test_loss: 0.030050496
train_loss: 0.00918196
test_loss: 0.029146377
train_loss: 0.010758806
test_loss: 0.028360197
train_loss: 0.008852864
test_loss: 0.02927779
train_loss: 0.00890547
test_loss: 0.029515717
train_loss: 0.009264222
test_loss: 0.029618239
train_loss: 0.008905922
test_loss: 0.02844185
train_loss: 0.008704221
test_loss: 0.027657231
train_loss: 0.009500498
test_loss: 0.028476274
train_loss: 0.008531736
test_loss: 0.02831903
train_loss: 0.008655668
test_loss: 0.027870644
train_loss: 0.008496721
test_loss: 0.028521694
train_loss: 0.008878472
test_loss: 0.027158873
train_loss: 0.009145822
test_loss: 0.029231615
train_loss: 0.009134938
test_loss: 0.028292686
train_loss: 0.009267286
test_loss: 0.027546186
train_loss: 0.009893147
test_loss: 0.027268205
train_loss: 0.008466089
test_loss: 0.028288106
train_loss: 0.0078082327
test_loss: 0.02643829
train_loss: 0.008036099
test_loss: 0.026765611
train_loss: 0.008744378
test_loss: 0.026824733
train_loss: 0.00809994
test_loss: 0.026461333
train_loss: 0.008109734
test_loss: 0.027880855
train_loss: 0.009816573
test_loss: 0.026731962
train_loss: 0.00968893
test_loss: 0.027819647
train_loss: 0.007734753
test_loss: 0.027318532
train_loss: 0.00916473
test_loss: 0.027074637
train_loss: 0.00941382
test_loss: 0.026701462
train_loss: 0.009125469
test_loss: 0.027401308
train_loss: 0.0075011346
test_loss: 0.02749457
train_loss: 0.008070911
test_loss: 0.026817547
train_loss: 0.007804495
test_loss: 0.0263008
train_loss: 0.0076806974
test_loss: 0.02750211
train_loss: 0.0080954805
test_loss: 0.025623292
train_loss: 0.006959819
test_loss: 0.026810687
train_loss: 0.009087146
test_loss: 0.026793245
train_loss: 0.007024714
test_loss: 0.026134148
train_loss: 0.0077827107
test_loss: 0.02779945
train_loss: 0.007858157
test_loss: 0.025984228
train_loss: 0.007327938
test_loss: 0.025903275
train_loss: 0.0077022137
test_loss: 0.026679194
train_loss: 0.0071286224
test_loss: 0.025239563
train_loss: 0.007801935
test_loss: 0.026407864
train_loss: 0.0070981733
test_loss: 0.025217393
train_loss: 0.0072168005
test_loss: 0.024906851
train_loss: 0.008181997
test_loss: 0.025163623
train_loss: 0.0070292368
test_loss: 0.025825491
train_loss: 0.006946982
test_loss: 0.02646917
train_loss: 0.006833138
test_loss: 0.025721787
train_loss: 0.0076557016
test_loss: 0.024783473
train_loss: 0.007899767
test_loss: 0.025032261
train_loss: 0.0072977664
test_loss: 0.025731863
train_loss: 0.009424487
test_loss: 0.025035283
train_loss: 0.0065521635
test_loss: 0.025064325
train_loss: 0.006640561
test_loss: 0.025369257
train_loss: 0.007407275
test_loss: 0.023834752
train_loss: 0.0071199196
test_loss: 0.024051866
train_loss: 0.0064171725
test_loss: 0.024449276
train_loss: 0.0068620336
test_loss: 0.024101686
train_loss: 0.006588716
test_loss: 0.024115527
train_loss: 0.0068749543
test_loss: 0.024987906
train_loss: 0.007138999
test_loss: 0.024289997
train_loss: 0.007783058
test_loss: 0.02561117
train_loss: 0.008443884
test_loss: 0.023459293
train_loss: 0.006247591
test_loss: 0.02396352
train_loss: 0.006202923
test_loss: 0.024548117
train_loss: 0.0068765115
test_loss: 0.02499148
train_loss: 0.006327288
test_loss: 0.024466237
train_loss: 0.006088554
test_loss: 0.023401208
train_loss: 0.0061907116
test_loss: 0.023546655
train_loss: 0.0068359314
test_loss: 0.024609616
train_loss: 0.0061856434
test_loss: 0.024558611
train_loss: 0.0067833927
test_loss: 0.02285262
train_loss: 0.0058383117
test_loss: 0.023380432
train_loss: 0.0066690356
test_loss: 0.024350304
train_loss: 0.0061034453
test_loss: 0.022480011
train_loss: 0.006137508
test_loss: 0.024281096
train_loss: 0.006279064
test_loss: 0.023723448
train_loss: 0.0058947094
test_loss: 0.02354578
train_loss: 0.006394357
test_loss: 0.024269618
train_loss: 0.006047975
test_loss: 0.022526028
train_loss: 0.0060130646
test_loss: 0.023407344
train_loss: 0.0067340247
test_loss: 0.023131354
train_loss: 0.006239334
test_loss: 0.02313715
train_loss: 0.0065539232
test_loss: 0.022414502
train_loss: 0.0056075575
test_loss: 0.02238017
train_loss: 0.0053857756
test_loss: 0.02120019
train_loss: 0.0062203826
test_loss: 0.022640666
train_loss: 0.006548904
test_loss: 0.02282105
train_loss: 0.0060550314
test_loss: 0.023006417
train_loss: 0.006115118
test_loss: 0.022232685
train_loss: 0.005464609
test_loss: 0.022109114
train_loss: 0.005821168
test_loss: 0.022650337
train_loss: 0.006958418
test_loss: 0.022078248
train_loss: 0.006778879
test_loss: 0.021977603
train_loss: 0.0060825325
test_loss: 0.022288535
train_loss: 0.0061581116
test_loss: 0.02296034
train_loss: 0.0053205816
test_loss: 0.021601345
train_loss: 0.0062389383
test_loss: 0.02202348
train_loss: 0.005523765
test_loss: 0.022025932
train_loss: 0.005559246
test_loss: 0.022454195
train_loss: 0.0051102857
test_loss: 0.02192701
train_loss: 0.006442155
test_loss: 0.022726309
train_loss: 0.0052450197
test_loss: 0.021837927
train_loss: 0.0056238445
test_loss: 0.021876471
train_loss: 0.005161687
test_loss: 0.021451322
train_loss: 0.005584052
test_loss: 0.022047328
train_loss: 0.0051583657
test_loss: 0.022418182
train_loss: 0.0051366175
test_loss: 0.02223125
train_loss: 0.0059728334
test_loss: 0.02240225
train_loss: 0.0049799094
test_loss: 0.021717846
train_loss: 0.0055579403
test_loss: 0.0215314
train_loss: 0.0057369424
test_loss: 0.02282192
train_loss: 0.0054900423
test_loss: 0.021879492
train_loss: 0.0056508067
test_loss: 0.021426793
train_loss: 0.0055422783
test_loss: 0.021432769
train_loss: 0.0064030075
test_loss: 0.021805698
train_loss: 0.005217332
test_loss: 0.021656726
train_loss: 0.0055230665
test_loss: 0.01996336
train_loss: 0.005499648
test_loss: 0.022385716
train_loss: 0.006837954
test_loss: 0.020939652
train_loss: 0.0060781226
test_loss: 0.020540902
train_loss: 0.0059358426
test_loss: 0.02210016
train_loss: 0.0054241023
test_loss: 0.020580446
train_loss: 0.0064769946
test_loss: 0.020621885
train_loss: 0.0073360284
test_loss: 0.01957484
train_loss: 0.0060433457
test_loss: 0.022268243
train_loss: 0.0047982335
test_loss: 0.019423263
train_loss: 0.0047831973
test_loss: 0.020591717
train_loss: 0.0055364473
test_loss: 0.020514887
train_loss: 0.0051302416
test_loss: 0.020391893
train_loss: 0.004846041
test_loss: 0.020724941
train_loss: 0.004989133
test_loss: 0.01998341
train_loss: 0.0052953027
test_loss: 0.020560076
train_loss: 0.005328545
test_loss: 0.020839548
train_loss: 0.00505409
test_loss: 0.019569995
train_loss: 0.0052645383/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.01967211
train_loss: 0.0046968083
test_loss: 0.020642905
train_loss: 0.005382396
test_loss: 0.02041253
train_loss: 0.004909547
test_loss: 0.020448491
train_loss: 0.0054063043
test_loss: 0.020339325
train_loss: 0.0054265377
test_loss: 0.020253804
train_loss: 0.0057745804
test_loss: 0.019319532
train_loss: 0.0046933484
test_loss: 0.020069132
train_loss: 0.004423266
test_loss: 0.019581815
train_loss: 0.004588899
test_loss: 0.019369636
train_loss: 0.005027633
test_loss: 0.020904286
train_loss: 0.0055412734
test_loss: 0.020925092
train_loss: 0.005033844
test_loss: 0.020006066
train_loss: 0.004654572
test_loss: 0.018185578
train_loss: 0.0046369717
test_loss: 0.019421188
train_loss: 0.005822905
test_loss: 0.018543037
train_loss: 0.0052524805
test_loss: 0.018796355
train_loss: 0.005929133
test_loss: 0.018307967
train_loss: 0.0051977555
test_loss: 0.019571155
train_loss: 0.0048236055
test_loss: 0.018779451
train_loss: 0.0057432735
test_loss: 0.01908124
train_loss: 0.00569534
test_loss: 0.019254096
train_loss: 0.004526677
test_loss: 0.01971016
train_loss: 0.0046207206
test_loss: 0.018952446
train_loss: 0.0056841266
test_loss: 0.02069468
train_loss: 0.0047082934
test_loss: 0.018823795
train_loss: 0.0054892227
test_loss: 0.018629165
train_loss: 0.0045083086
test_loss: 0.019461319
train_loss: 0.00468883
test_loss: 0.019275466
train_loss: 0.0045051533
test_loss: 0.019760774
train_loss: 0.0045696595
test_loss: 0.018097963
train_loss: 0.004336646
test_loss: 0.01786971
train_loss: 0.004887246
test_loss: 0.01931749
train_loss: 0.0053108763
test_loss: 0.019036783
train_loss: 0.0044799997
test_loss: 0.01805824
train_loss: 0.004492654
test_loss: 0.020209137
train_loss: 0.004528721
test_loss: 0.019046282
train_loss: 0.0044639697
test_loss: 0.019017538
train_loss: 0.004561394
test_loss: 0.018473601
train_loss: 0.0049197483
test_loss: 0.019979935
train_loss: 0.0043644183
test_loss: 0.018634656
train_loss: 0.0045930585
test_loss: 0.01996411
train_loss: 0.004778968
test_loss: 0.018899119
train_loss: 0.0050610737
test_loss: 0.01859518
train_loss: 0.0044858195
test_loss: 0.017899923
train_loss: 0.005015689
test_loss: 0.018599495
train_loss: 0.005065684
test_loss: 0.019100897
train_loss: 0.005245347
test_loss: 0.01835397
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.4/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.4/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb062abbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb062a2a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb062a2a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb062a6a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0629c2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0629c2d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0629ae620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb062960598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb06295a048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb062920730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0628cb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0628e1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0628d7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0628d72f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb062846730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0628468c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb062802378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0628347b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0627e7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0627e8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb04c3297b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb04c329d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb04c302ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb04c2bc840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb04c2aa488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb04c26d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb04c228950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb04c24e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb04c24e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb024193378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb0241438c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb02410e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb02410e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb02410f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb024193950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb024086a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.27392222e-05
Iter: 2 loss: 0.0136024505
Iter: 3 loss: 4.60863921e-05
Iter: 4 loss: 4.23334932e-05
Iter: 5 loss: 7.78336835e-05
Iter: 6 loss: 4.21603618e-05
Iter: 7 loss: 4.05574028e-05
Iter: 8 loss: 3.58218458e-05
Iter: 9 loss: 5.214267e-05
Iter: 10 loss: 3.36567427e-05
Iter: 11 loss: 3.10800278e-05
Iter: 12 loss: 3.19966202e-05
Iter: 13 loss: 2.92751956e-05
Iter: 14 loss: 2.71075769e-05
Iter: 15 loss: 2.55594841e-05
Iter: 16 loss: 2.47978787e-05
Iter: 17 loss: 2.34299187e-05
Iter: 18 loss: 2.75305083e-05
Iter: 19 loss: 2.30113073e-05
Iter: 20 loss: 2.23798106e-05
Iter: 21 loss: 2.22269118e-05
Iter: 22 loss: 2.18270816e-05
Iter: 23 loss: 2.10627477e-05
Iter: 24 loss: 2.45468254e-05
Iter: 25 loss: 2.09152095e-05
Iter: 26 loss: 2.04986e-05
Iter: 27 loss: 2.1221078e-05
Iter: 28 loss: 2.03106374e-05
Iter: 29 loss: 1.98591952e-05
Iter: 30 loss: 2.0297477e-05
Iter: 31 loss: 1.9604362e-05
Iter: 32 loss: 1.91197178e-05
Iter: 33 loss: 2.15464188e-05
Iter: 34 loss: 1.90390383e-05
Iter: 35 loss: 1.86521484e-05
Iter: 36 loss: 1.91838935e-05
Iter: 37 loss: 1.84616802e-05
Iter: 38 loss: 1.8115863e-05
Iter: 39 loss: 1.99382412e-05
Iter: 40 loss: 1.80627139e-05
Iter: 41 loss: 1.8615001e-05
Iter: 42 loss: 1.79612143e-05
Iter: 43 loss: 1.79306444e-05
Iter: 44 loss: 1.78423215e-05
Iter: 45 loss: 1.82509721e-05
Iter: 46 loss: 1.78114115e-05
Iter: 47 loss: 1.76435788e-05
Iter: 48 loss: 1.83399607e-05
Iter: 49 loss: 1.76070498e-05
Iter: 50 loss: 1.74677934e-05
Iter: 51 loss: 1.7457327e-05
Iter: 52 loss: 1.73526805e-05
Iter: 53 loss: 1.71537613e-05
Iter: 54 loss: 1.72618074e-05
Iter: 55 loss: 1.70240128e-05
Iter: 56 loss: 1.68646766e-05
Iter: 57 loss: 1.75353252e-05
Iter: 58 loss: 1.6830978e-05
Iter: 59 loss: 1.66989903e-05
Iter: 60 loss: 1.78013779e-05
Iter: 61 loss: 1.66911232e-05
Iter: 62 loss: 1.6575028e-05
Iter: 63 loss: 1.67129438e-05
Iter: 64 loss: 1.65137972e-05
Iter: 65 loss: 1.63977184e-05
Iter: 66 loss: 1.63735676e-05
Iter: 67 loss: 1.62970809e-05
Iter: 68 loss: 1.61555654e-05
Iter: 69 loss: 1.65402453e-05
Iter: 70 loss: 1.61088574e-05
Iter: 71 loss: 1.60048767e-05
Iter: 72 loss: 1.63382756e-05
Iter: 73 loss: 1.59753763e-05
Iter: 74 loss: 1.62688921e-05
Iter: 75 loss: 1.59508345e-05
Iter: 76 loss: 1.59246738e-05
Iter: 77 loss: 1.58509501e-05
Iter: 78 loss: 1.6246333e-05
Iter: 79 loss: 1.58265102e-05
Iter: 80 loss: 1.57773702e-05
Iter: 81 loss: 1.57992072e-05
Iter: 82 loss: 1.57438426e-05
Iter: 83 loss: 1.5683032e-05
Iter: 84 loss: 1.57463983e-05
Iter: 85 loss: 1.56496153e-05
Iter: 86 loss: 1.55817543e-05
Iter: 87 loss: 1.56187034e-05
Iter: 88 loss: 1.55371345e-05
Iter: 89 loss: 1.5450647e-05
Iter: 90 loss: 1.56631613e-05
Iter: 91 loss: 1.54199824e-05
Iter: 92 loss: 1.53255041e-05
Iter: 93 loss: 1.54406353e-05
Iter: 94 loss: 1.52759967e-05
Iter: 95 loss: 1.52124685e-05
Iter: 96 loss: 1.52107586e-05
Iter: 97 loss: 1.51596123e-05
Iter: 98 loss: 1.50884298e-05
Iter: 99 loss: 1.5085001e-05
Iter: 100 loss: 1.49570915e-05
Iter: 101 loss: 1.59768242e-05
Iter: 102 loss: 1.49485568e-05
Iter: 103 loss: 1.48847312e-05
Iter: 104 loss: 1.49196876e-05
Iter: 105 loss: 1.48423751e-05
Iter: 106 loss: 1.49183679e-05
Iter: 107 loss: 1.48283116e-05
Iter: 108 loss: 1.48174404e-05
Iter: 109 loss: 1.47842529e-05
Iter: 110 loss: 1.48749577e-05
Iter: 111 loss: 1.47664086e-05
Iter: 112 loss: 1.47168294e-05
Iter: 113 loss: 1.47364099e-05
Iter: 114 loss: 1.46825241e-05
Iter: 115 loss: 1.46230486e-05
Iter: 116 loss: 1.48260879e-05
Iter: 117 loss: 1.46070415e-05
Iter: 118 loss: 1.45544318e-05
Iter: 119 loss: 1.46129178e-05
Iter: 120 loss: 1.45252216e-05
Iter: 121 loss: 1.44699707e-05
Iter: 122 loss: 1.45476442e-05
Iter: 123 loss: 1.44430369e-05
Iter: 124 loss: 1.43833931e-05
Iter: 125 loss: 1.44925179e-05
Iter: 126 loss: 1.43585139e-05
Iter: 127 loss: 1.430878e-05
Iter: 128 loss: 1.48345116e-05
Iter: 129 loss: 1.4307926e-05
Iter: 130 loss: 1.42641e-05
Iter: 131 loss: 1.42975732e-05
Iter: 132 loss: 1.42372774e-05
Iter: 133 loss: 1.4175972e-05
Iter: 134 loss: 1.41820728e-05
Iter: 135 loss: 1.41294031e-05
Iter: 136 loss: 1.40878201e-05
Iter: 137 loss: 1.40877382e-05
Iter: 138 loss: 1.41243809e-05
Iter: 139 loss: 1.40818265e-05
Iter: 140 loss: 1.40754946e-05
Iter: 141 loss: 1.40574557e-05
Iter: 142 loss: 1.41316268e-05
Iter: 143 loss: 1.40498723e-05
Iter: 144 loss: 1.40251523e-05
Iter: 145 loss: 1.40017564e-05
Iter: 146 loss: 1.39964022e-05
Iter: 147 loss: 1.39516305e-05
Iter: 148 loss: 1.40034008e-05
Iter: 149 loss: 1.39269114e-05
Iter: 150 loss: 1.3882629e-05
Iter: 151 loss: 1.40556294e-05
Iter: 152 loss: 1.38719161e-05
Iter: 153 loss: 1.38337437e-05
Iter: 154 loss: 1.38282849e-05
Iter: 155 loss: 1.38010182e-05
Iter: 156 loss: 1.3747509e-05
Iter: 157 loss: 1.39133854e-05
Iter: 158 loss: 1.3731149e-05
Iter: 159 loss: 1.36821927e-05
Iter: 160 loss: 1.38325995e-05
Iter: 161 loss: 1.36677854e-05
Iter: 162 loss: 1.36137878e-05
Iter: 163 loss: 1.38492678e-05
Iter: 164 loss: 1.36029676e-05
Iter: 165 loss: 1.35517348e-05
Iter: 166 loss: 1.36446288e-05
Iter: 167 loss: 1.35298824e-05
Iter: 168 loss: 1.34929569e-05
Iter: 169 loss: 1.36189656e-05
Iter: 170 loss: 1.34834299e-05
Iter: 171 loss: 1.35542678e-05
Iter: 172 loss: 1.34753864e-05
Iter: 173 loss: 1.34671191e-05
Iter: 174 loss: 1.34471966e-05
Iter: 175 loss: 1.36328872e-05
Iter: 176 loss: 1.34442707e-05
Iter: 177 loss: 1.34230922e-05
Iter: 178 loss: 1.34117554e-05
Iter: 179 loss: 1.34020138e-05
Iter: 180 loss: 1.33670874e-05
Iter: 181 loss: 1.34396369e-05
Iter: 182 loss: 1.33532885e-05
Iter: 183 loss: 1.33216436e-05
Iter: 184 loss: 1.33702415e-05
Iter: 185 loss: 1.33069098e-05
Iter: 186 loss: 1.32736186e-05
Iter: 187 loss: 1.33105914e-05
Iter: 188 loss: 1.32559326e-05
Iter: 189 loss: 1.32191417e-05
Iter: 190 loss: 1.32805235e-05
Iter: 191 loss: 1.32015721e-05
Iter: 192 loss: 1.3162894e-05
Iter: 193 loss: 1.33136036e-05
Iter: 194 loss: 1.31543457e-05
Iter: 195 loss: 1.31151974e-05
Iter: 196 loss: 1.32748482e-05
Iter: 197 loss: 1.3106217e-05
Iter: 198 loss: 1.30675126e-05
Iter: 199 loss: 1.30924454e-05
Iter: 200 loss: 1.30421995e-05
Iter: 201 loss: 1.30065673e-05
Iter: 202 loss: 1.31350553e-05
Iter: 203 loss: 1.29973669e-05
Iter: 204 loss: 1.30268991e-05
Iter: 205 loss: 1.29879954e-05
Iter: 206 loss: 1.29758919e-05
Iter: 207 loss: 1.29526725e-05
Iter: 208 loss: 1.34529528e-05
Iter: 209 loss: 1.29527216e-05
Iter: 210 loss: 1.29364571e-05
Iter: 211 loss: 1.29139262e-05
Iter: 212 loss: 1.29129712e-05
Iter: 213 loss: 1.28837501e-05
Iter: 214 loss: 1.29634136e-05
Iter: 215 loss: 1.28747342e-05
Iter: 216 loss: 1.2848097e-05
Iter: 217 loss: 1.284259e-05
Iter: 218 loss: 1.28244592e-05
Iter: 219 loss: 1.2785782e-05
Iter: 220 loss: 1.29079745e-05
Iter: 221 loss: 1.2774869e-05
Iter: 222 loss: 1.27399671e-05
Iter: 223 loss: 1.28165384e-05
Iter: 224 loss: 1.27264566e-05
Iter: 225 loss: 1.26907953e-05
Iter: 226 loss: 1.27226149e-05
Iter: 227 loss: 1.26695595e-05
Iter: 228 loss: 1.26299092e-05
Iter: 229 loss: 1.28930324e-05
Iter: 230 loss: 1.26257964e-05
Iter: 231 loss: 1.25868755e-05
Iter: 232 loss: 1.26391224e-05
Iter: 233 loss: 1.25673541e-05
Iter: 234 loss: 1.25324104e-05
Iter: 235 loss: 1.2644653e-05
Iter: 236 loss: 1.25219103e-05
Iter: 237 loss: 1.25535516e-05
Iter: 238 loss: 1.2513502e-05
Iter: 239 loss: 1.25028691e-05
Iter: 240 loss: 1.24896096e-05
Iter: 241 loss: 1.24881462e-05
Iter: 242 loss: 1.24776716e-05
Iter: 243 loss: 1.24642638e-05
Iter: 244 loss: 1.24632224e-05
Iter: 245 loss: 1.2446184e-05
Iter: 246 loss: 1.25416764e-05
Iter: 247 loss: 1.24432427e-05
Iter: 248 loss: 1.24281196e-05
Iter: 249 loss: 1.23992759e-05
Iter: 250 loss: 1.30060535e-05
Iter: 251 loss: 1.2399003e-05
Iter: 252 loss: 1.23595692e-05
Iter: 253 loss: 1.25021252e-05
Iter: 254 loss: 1.23494392e-05
Iter: 255 loss: 1.23185582e-05
Iter: 256 loss: 1.2448807e-05
Iter: 257 loss: 1.23119689e-05
Iter: 258 loss: 1.22852925e-05
Iter: 259 loss: 1.23135815e-05
Iter: 260 loss: 1.22699803e-05
Iter: 261 loss: 1.22352321e-05
Iter: 262 loss: 1.2445e-05
Iter: 263 loss: 1.22306747e-05
Iter: 264 loss: 1.22008423e-05
Iter: 265 loss: 1.22758147e-05
Iter: 266 loss: 1.21907424e-05
Iter: 267 loss: 1.21663725e-05
Iter: 268 loss: 1.21821067e-05
Iter: 269 loss: 1.21508456e-05
Iter: 270 loss: 1.21610819e-05
Iter: 271 loss: 1.21430348e-05
Iter: 272 loss: 1.2131427e-05
Iter: 273 loss: 1.21313888e-05
Iter: 274 loss: 1.21220119e-05
Iter: 275 loss: 1.21153262e-05
Iter: 276 loss: 1.20982368e-05
Iter: 277 loss: 1.22663832e-05
Iter: 278 loss: 1.20956665e-05
Iter: 279 loss: 1.20780223e-05
Iter: 280 loss: 1.22195506e-05
Iter: 281 loss: 1.20768527e-05
Iter: 282 loss: 1.20631303e-05
Iter: 283 loss: 1.20443747e-05
Iter: 284 loss: 1.20435343e-05
Iter: 285 loss: 1.20197165e-05
Iter: 286 loss: 1.21307685e-05
Iter: 287 loss: 1.20154345e-05
Iter: 288 loss: 1.19953129e-05
Iter: 289 loss: 1.20595196e-05
Iter: 290 loss: 1.19895321e-05
Iter: 291 loss: 1.19657689e-05
Iter: 292 loss: 1.19509423e-05
Iter: 293 loss: 1.19416254e-05
Iter: 294 loss: 1.1911703e-05
Iter: 295 loss: 1.21721459e-05
Iter: 296 loss: 1.19098449e-05
Iter: 297 loss: 1.18850912e-05
Iter: 298 loss: 1.19928591e-05
Iter: 299 loss: 1.1879526e-05
Iter: 300 loss: 1.18592034e-05
Iter: 301 loss: 1.18544231e-05
Iter: 302 loss: 1.18410262e-05
Iter: 303 loss: 1.18307016e-05
Iter: 304 loss: 1.18266489e-05
Iter: 305 loss: 1.18098851e-05
Iter: 306 loss: 1.1905925e-05
Iter: 307 loss: 1.18079552e-05
Iter: 308 loss: 1.18039825e-05
Iter: 309 loss: 1.17917525e-05
Iter: 310 loss: 1.18261378e-05
Iter: 311 loss: 1.17854051e-05
Iter: 312 loss: 1.17689342e-05
Iter: 313 loss: 1.18607368e-05
Iter: 314 loss: 1.17661748e-05
Iter: 315 loss: 1.17505624e-05
Iter: 316 loss: 1.17437448e-05
Iter: 317 loss: 1.17351919e-05
Iter: 318 loss: 1.1713294e-05
Iter: 319 loss: 1.17210857e-05
Iter: 320 loss: 1.16978426e-05
Iter: 321 loss: 1.16799256e-05
Iter: 322 loss: 1.19111219e-05
Iter: 323 loss: 1.16797428e-05
Iter: 324 loss: 1.16634856e-05
Iter: 325 loss: 1.16531337e-05
Iter: 326 loss: 1.16463625e-05
Iter: 327 loss: 1.16299434e-05
Iter: 328 loss: 1.18384123e-05
Iter: 329 loss: 1.163e-05
Iter: 330 loss: 1.16175615e-05
Iter: 331 loss: 1.16399897e-05
Iter: 332 loss: 1.16125975e-05
Iter: 333 loss: 1.1596585e-05
Iter: 334 loss: 1.16056835e-05
Iter: 335 loss: 1.15865987e-05
Iter: 336 loss: 1.15682669e-05
Iter: 337 loss: 1.17914806e-05
Iter: 338 loss: 1.15683015e-05
Iter: 339 loss: 1.1555413e-05
Iter: 340 loss: 1.15546009e-05
Iter: 341 loss: 1.15523544e-05
Iter: 342 loss: 1.15437533e-05
Iter: 343 loss: 1.15483817e-05
Iter: 344 loss: 1.15358544e-05
Iter: 345 loss: 1.15209532e-05
Iter: 346 loss: 1.15938437e-05
Iter: 347 loss: 1.15186085e-05
Iter: 348 loss: 1.15018574e-05
Iter: 349 loss: 1.14923241e-05
Iter: 350 loss: 1.14845207e-05
Iter: 351 loss: 1.14598242e-05
Iter: 352 loss: 1.14844588e-05
Iter: 353 loss: 1.1446089e-05
Iter: 354 loss: 1.14197792e-05
Iter: 355 loss: 1.14776976e-05
Iter: 356 loss: 1.14097838e-05
Iter: 357 loss: 1.1375083e-05
Iter: 358 loss: 1.14535169e-05
Iter: 359 loss: 1.13624264e-05
Iter: 360 loss: 1.1328274e-05
Iter: 361 loss: 1.14585182e-05
Iter: 362 loss: 1.13200913e-05
Iter: 363 loss: 1.12924909e-05
Iter: 364 loss: 1.14110144e-05
Iter: 365 loss: 1.12869893e-05
Iter: 366 loss: 1.12553553e-05
Iter: 367 loss: 1.12815187e-05
Iter: 368 loss: 1.12363177e-05
Iter: 369 loss: 1.12159787e-05
Iter: 370 loss: 1.12161233e-05
Iter: 371 loss: 1.12144126e-05
Iter: 372 loss: 1.12075322e-05
Iter: 373 loss: 1.12051557e-05
Iter: 374 loss: 1.1197445e-05
Iter: 375 loss: 1.11838017e-05
Iter: 376 loss: 1.11831569e-05
Iter: 377 loss: 1.11650324e-05
Iter: 378 loss: 1.12727193e-05
Iter: 379 loss: 1.11623303e-05
Iter: 380 loss: 1.1142788e-05
Iter: 381 loss: 1.11761092e-05
Iter: 382 loss: 1.11334939e-05
Iter: 383 loss: 1.11185309e-05
Iter: 384 loss: 1.11449881e-05
Iter: 385 loss: 1.11116224e-05
Iter: 386 loss: 1.10989567e-05
Iter: 387 loss: 1.11025947e-05
Iter: 388 loss: 1.1089398e-05
Iter: 389 loss: 1.10709507e-05
Iter: 390 loss: 1.11217332e-05
Iter: 391 loss: 1.10648507e-05
Iter: 392 loss: 1.10427354e-05
Iter: 393 loss: 1.11174431e-05
Iter: 394 loss: 1.103664e-05
Iter: 395 loss: 1.10195078e-05
Iter: 396 loss: 1.10521805e-05
Iter: 397 loss: 1.1012e-05
Iter: 398 loss: 1.0991721e-05
Iter: 399 loss: 1.11021445e-05
Iter: 400 loss: 1.09881939e-05
Iter: 401 loss: 1.09755611e-05
Iter: 402 loss: 1.11622903e-05
Iter: 403 loss: 1.09752782e-05
Iter: 404 loss: 1.09773346e-05
Iter: 405 loss: 1.09712819e-05
Iter: 406 loss: 1.0968779e-05
Iter: 407 loss: 1.09611792e-05
Iter: 408 loss: 1.09544644e-05
Iter: 409 loss: 1.09505372e-05
Iter: 410 loss: 1.09375105e-05
Iter: 411 loss: 1.09606081e-05
Iter: 412 loss: 1.09318926e-05
Iter: 413 loss: 1.09135108e-05
Iter: 414 loss: 1.09896955e-05
Iter: 415 loss: 1.09100683e-05
Iter: 416 loss: 1.08956974e-05
Iter: 417 loss: 1.08914992e-05
Iter: 418 loss: 1.08830873e-05
Iter: 419 loss: 1.0863384e-05
Iter: 420 loss: 1.08944569e-05
Iter: 421 loss: 1.08542199e-05
Iter: 422 loss: 1.08339391e-05
Iter: 423 loss: 1.08776894e-05
Iter: 424 loss: 1.08261738e-05
Iter: 425 loss: 1.08034474e-05
Iter: 426 loss: 1.09142165e-05
Iter: 427 loss: 1.0799522e-05
Iter: 428 loss: 1.07799879e-05
Iter: 429 loss: 1.07905289e-05
Iter: 430 loss: 1.07664937e-05
Iter: 431 loss: 1.07412852e-05
Iter: 432 loss: 1.08544755e-05
Iter: 433 loss: 1.07361157e-05
Iter: 434 loss: 1.07172618e-05
Iter: 435 loss: 1.07170581e-05
Iter: 436 loss: 1.07245005e-05
Iter: 437 loss: 1.07127908e-05
Iter: 438 loss: 1.0709733e-05
Iter: 439 loss: 1.07001761e-05
Iter: 440 loss: 1.07043306e-05
Iter: 441 loss: 1.06915259e-05
Iter: 442 loss: 1.06794e-05
Iter: 443 loss: 1.06882735e-05
Iter: 444 loss: 1.06717434e-05
Iter: 445 loss: 1.06566458e-05
Iter: 446 loss: 1.07974083e-05
Iter: 447 loss: 1.06564312e-05
Iter: 448 loss: 1.06456182e-05
Iter: 449 loss: 1.06327516e-05
Iter: 450 loss: 1.06312455e-05
Iter: 451 loss: 1.06163325e-05
Iter: 452 loss: 1.06886091e-05
Iter: 453 loss: 1.06135958e-05
Iter: 454 loss: 1.05990875e-05
Iter: 455 loss: 1.05985191e-05
Iter: 456 loss: 1.05872577e-05
Iter: 457 loss: 1.05669624e-05
Iter: 458 loss: 1.06861171e-05
Iter: 459 loss: 1.05638546e-05
Iter: 460 loss: 1.05488925e-05
Iter: 461 loss: 1.05977451e-05
Iter: 462 loss: 1.05441695e-05
Iter: 463 loss: 1.0529875e-05
Iter: 464 loss: 1.05227218e-05
Iter: 465 loss: 1.05159233e-05
Iter: 466 loss: 1.04966e-05
Iter: 467 loss: 1.06992829e-05
Iter: 468 loss: 1.04960518e-05
Iter: 469 loss: 1.05129757e-05
Iter: 470 loss: 1.04915362e-05
Iter: 471 loss: 1.04877636e-05
Iter: 472 loss: 1.04761039e-05
Iter: 473 loss: 1.05073395e-05
Iter: 474 loss: 1.04704377e-05
Iter: 475 loss: 1.04618593e-05
Iter: 476 loss: 1.0468897e-05
Iter: 477 loss: 1.04562123e-05
Iter: 478 loss: 1.04463861e-05
Iter: 479 loss: 1.05022082e-05
Iter: 480 loss: 1.04452838e-05
Iter: 481 loss: 1.04348346e-05
Iter: 482 loss: 1.04284327e-05
Iter: 483 loss: 1.042441e-05
Iter: 484 loss: 1.0409447e-05
Iter: 485 loss: 1.04214541e-05
Iter: 486 loss: 1.03999319e-05
Iter: 487 loss: 1.038153e-05
Iter: 488 loss: 1.04318042e-05
Iter: 489 loss: 1.0375491e-05
Iter: 490 loss: 1.03570828e-05
Iter: 491 loss: 1.03935345e-05
Iter: 492 loss: 1.03487564e-05
Iter: 493 loss: 1.03273342e-05
Iter: 494 loss: 1.04389546e-05
Iter: 495 loss: 1.032412e-05
Iter: 496 loss: 1.03072016e-05
Iter: 497 loss: 1.03341063e-05
Iter: 498 loss: 1.02991762e-05
Iter: 499 loss: 1.02827153e-05
Iter: 500 loss: 1.03813782e-05
Iter: 501 loss: 1.02804497e-05
Iter: 502 loss: 1.0302183e-05
Iter: 503 loss: 1.0278045e-05
Iter: 504 loss: 1.02749664e-05
Iter: 505 loss: 1.02676795e-05
Iter: 506 loss: 1.03319935e-05
Iter: 507 loss: 1.02665153e-05
Iter: 508 loss: 1.02599588e-05
Iter: 509 loss: 1.02480426e-05
Iter: 510 loss: 1.05016643e-05
Iter: 511 loss: 1.02477588e-05
Iter: 512 loss: 1.02377244e-05
Iter: 513 loss: 1.03408665e-05
Iter: 514 loss: 1.02376107e-05
Iter: 515 loss: 1.0227257e-05
Iter: 516 loss: 1.02450222e-05
Iter: 517 loss: 1.02225422e-05
Iter: 518 loss: 1.02136528e-05
Iter: 519 loss: 1.02159065e-05
Iter: 520 loss: 1.02068425e-05
Iter: 521 loss: 1.01959604e-05
Iter: 522 loss: 1.02442818e-05
Iter: 523 loss: 1.01938313e-05
Iter: 524 loss: 1.01826608e-05
Iter: 525 loss: 1.0170048e-05
Iter: 526 loss: 1.01686437e-05
Iter: 527 loss: 1.01505993e-05
Iter: 528 loss: 1.03077728e-05
Iter: 529 loss: 1.0149668e-05
Iter: 530 loss: 1.01341975e-05
Iter: 531 loss: 1.01693295e-05
Iter: 532 loss: 1.01283185e-05
Iter: 533 loss: 1.01135156e-05
Iter: 534 loss: 1.0151789e-05
Iter: 535 loss: 1.01083951e-05
Iter: 536 loss: 1.01258684e-05
Iter: 537 loss: 1.01046408e-05
Iter: 538 loss: 1.00996822e-05
Iter: 539 loss: 1.00909474e-05
Iter: 540 loss: 1.00909256e-05
Iter: 541 loss: 1.00843881e-05
Iter: 542 loss: 1.00726666e-05
Iter: 543 loss: 1.03414532e-05
Iter: 544 loss: 1.00724274e-05
Iter: 545 loss: 1.00621264e-05
Iter: 546 loss: 1.00898842e-05
Iter: 547 loss: 1.00586813e-05
Iter: 548 loss: 1.0046454e-05
Iter: 549 loss: 1.01515107e-05
Iter: 550 loss: 1.00459401e-05
Iter: 551 loss: 1.0037661e-05
Iter: 552 loss: 1.00289326e-05
Iter: 553 loss: 1.00275911e-05
Iter: 554 loss: 1.0017231e-05
Iter: 555 loss: 1.00350626e-05
Iter: 556 loss: 1.00127509e-05
Iter: 557 loss: 9.99929216e-06
Iter: 558 loss: 1.00057732e-05
Iter: 559 loss: 9.99022632e-06
Iter: 560 loss: 9.97518327e-06
Iter: 561 loss: 1.00711995e-05
Iter: 562 loss: 9.97344159e-06
Iter: 563 loss: 9.96051222e-06
Iter: 564 loss: 9.98595169e-06
Iter: 565 loss: 9.95502432e-06
Iter: 566 loss: 9.94450511e-06
Iter: 567 loss: 9.96033305e-06
Iter: 568 loss: 9.93940375e-06
Iter: 569 loss: 9.94111178e-06
Iter: 570 loss: 9.93611775e-06
Iter: 571 loss: 9.93222238e-06
Iter: 572 loss: 9.933814e-06
Iter: 573 loss: 9.92966579e-06
Iter: 574 loss: 9.92716377e-06
Iter: 575 loss: 9.92183e-06
Iter: 576 loss: 9.98748601e-06
Iter: 577 loss: 9.92094192e-06
Iter: 578 loss: 9.91473098e-06
Iter: 579 loss: 9.90658737e-06
Iter: 580 loss: 9.9060544e-06
Iter: 581 loss: 9.89545697e-06
Iter: 582 loss: 1.00268826e-05
Iter: 583 loss: 9.89533e-06
Iter: 584 loss: 9.88448392e-06
Iter: 585 loss: 9.892653e-06
Iter: 586 loss: 9.8783421e-06
Iter: 587 loss: 9.86776558e-06
Iter: 588 loss: 9.87507065e-06
Iter: 589 loss: 9.86132909e-06
Iter: 590 loss: 9.84915096e-06
Iter: 591 loss: 9.89225373e-06
Iter: 592 loss: 9.8461951e-06
Iter: 593 loss: 9.83211066e-06
Iter: 594 loss: 9.85776751e-06
Iter: 595 loss: 9.82635174e-06
Iter: 596 loss: 9.81352059e-06
Iter: 597 loss: 9.87754902e-06
Iter: 598 loss: 9.81118137e-06
Iter: 599 loss: 9.8019882e-06
Iter: 600 loss: 9.86732357e-06
Iter: 601 loss: 9.80147524e-06
Iter: 602 loss: 9.80568529e-06
Iter: 603 loss: 9.79936522e-06
Iter: 604 loss: 9.79741253e-06
Iter: 605 loss: 9.79689867e-06
Iter: 606 loss: 9.79559445e-06
Iter: 607 loss: 9.79328797e-06
Iter: 608 loss: 9.78706339e-06
Iter: 609 loss: 9.83473456e-06
Iter: 610 loss: 9.78616572e-06
Iter: 611 loss: 9.77890431e-06
Iter: 612 loss: 9.78400931e-06
Iter: 613 loss: 9.77435229e-06
Iter: 614 loss: 9.76648062e-06
Iter: 615 loss: 9.81267476e-06
Iter: 616 loss: 9.7652628e-06
Iter: 617 loss: 9.75681e-06
Iter: 618 loss: 9.7829743e-06
Iter: 619 loss: 9.75409876e-06
Iter: 620 loss: 9.74768773e-06
Iter: 621 loss: 9.74211252e-06
Iter: 622 loss: 9.7402044e-06
Iter: 623 loss: 9.7287093e-06
Iter: 624 loss: 9.73448e-06
Iter: 625 loss: 9.72086764e-06
Iter: 626 loss: 9.70269866e-06
Iter: 627 loss: 9.75384683e-06
Iter: 628 loss: 9.6971e-06
Iter: 629 loss: 9.68426684e-06
Iter: 630 loss: 9.80672485e-06
Iter: 631 loss: 9.68366567e-06
Iter: 632 loss: 9.67436e-06
Iter: 633 loss: 9.68974473e-06
Iter: 634 loss: 9.66985681e-06
Iter: 635 loss: 9.67389133e-06
Iter: 636 loss: 9.66646257e-06
Iter: 637 loss: 9.6631411e-06
Iter: 638 loss: 9.66801599e-06
Iter: 639 loss: 9.66154948e-06
Iter: 640 loss: 9.65878644e-06
Iter: 641 loss: 9.65221079e-06
Iter: 642 loss: 9.70030578e-06
Iter: 643 loss: 9.65067738e-06
Iter: 644 loss: 9.64429182e-06
Iter: 645 loss: 9.661082e-06
Iter: 646 loss: 9.64254286e-06
Iter: 647 loss: 9.63736784e-06
Iter: 648 loss: 9.6417134e-06
Iter: 649 loss: 9.63425555e-06
Iter: 650 loss: 9.62781269e-06
Iter: 651 loss: 9.68687709e-06
Iter: 652 loss: 9.62716149e-06
Iter: 653 loss: 9.6215972e-06
Iter: 654 loss: 9.61238311e-06
Iter: 655 loss: 9.61237583e-06
Iter: 656 loss: 9.60145e-06
Iter: 657 loss: 9.61652131e-06
Iter: 658 loss: 9.5958394e-06
Iter: 659 loss: 9.58324927e-06
Iter: 660 loss: 9.64307492e-06
Iter: 661 loss: 9.58098826e-06
Iter: 662 loss: 9.57165685e-06
Iter: 663 loss: 9.5992973e-06
Iter: 664 loss: 9.56885378e-06
Iter: 665 loss: 9.55837641e-06
Iter: 666 loss: 9.59550744e-06
Iter: 667 loss: 9.55537507e-06
Iter: 668 loss: 9.5541e-06
Iter: 669 loss: 9.55168434e-06
Iter: 670 loss: 9.54722418e-06
Iter: 671 loss: 9.56397889e-06
Iter: 672 loss: 9.54670213e-06
Iter: 673 loss: 9.54473217e-06
Iter: 674 loss: 9.53983e-06
Iter: 675 loss: 9.57569318e-06
Iter: 676 loss: 9.53877679e-06
Iter: 677 loss: 9.53384824e-06
Iter: 678 loss: 9.54186635e-06
Iter: 679 loss: 9.53175731e-06
Iter: 680 loss: 9.52617938e-06
Iter: 681 loss: 9.52236496e-06
Iter: 682 loss: 9.52029586e-06
Iter: 683 loss: 9.51194306e-06
Iter: 684 loss: 9.59681e-06
Iter: 685 loss: 9.5116593e-06
Iter: 686 loss: 9.50497451e-06
Iter: 687 loss: 9.5087762e-06
Iter: 688 loss: 9.50044796e-06
Iter: 689 loss: 9.49392233e-06
Iter: 690 loss: 9.4934594e-06
Iter: 691 loss: 9.48843626e-06
Iter: 692 loss: 9.47697299e-06
Iter: 693 loss: 9.49971673e-06
Iter: 694 loss: 9.47243097e-06
Iter: 695 loss: 9.46146793e-06
Iter: 696 loss: 9.49153491e-06
Iter: 697 loss: 9.45839747e-06
Iter: 698 loss: 9.44833118e-06
Iter: 699 loss: 9.52816845e-06
Iter: 700 loss: 9.44767635e-06
Iter: 701 loss: 9.4449988e-06
Iter: 702 loss: 9.44426574e-06
Iter: 703 loss: 9.44068142e-06
Iter: 704 loss: 9.47049466e-06
Iter: 705 loss: 9.44043404e-06
Iter: 706 loss: 9.43932719e-06
Iter: 707 loss: 9.4357e-06
Iter: 708 loss: 9.44819476e-06
Iter: 709 loss: 9.43417e-06
Iter: 710 loss: 9.4284751e-06
Iter: 711 loss: 9.42948736e-06
Iter: 712 loss: 9.42436145e-06
Iter: 713 loss: 9.41649887e-06
Iter: 714 loss: 9.4246443e-06
Iter: 715 loss: 9.4118659e-06
Iter: 716 loss: 9.40506834e-06
Iter: 717 loss: 9.50298818e-06
Iter: 718 loss: 9.40519e-06
Iter: 719 loss: 9.40010614e-06
Iter: 720 loss: 9.40215614e-06
Iter: 721 loss: 9.39666279e-06
Iter: 722 loss: 9.3895851e-06
Iter: 723 loss: 9.37924415e-06
Iter: 724 loss: 9.37912864e-06
Iter: 725 loss: 9.36464767e-06
Iter: 726 loss: 9.44286785e-06
Iter: 727 loss: 9.36258584e-06
Iter: 728 loss: 9.35392836e-06
Iter: 729 loss: 9.37121877e-06
Iter: 730 loss: 9.35002481e-06
Iter: 731 loss: 9.34036e-06
Iter: 732 loss: 9.3825538e-06
Iter: 733 loss: 9.33803858e-06
Iter: 734 loss: 9.33368e-06
Iter: 735 loss: 9.33272713e-06
Iter: 736 loss: 9.3287681e-06
Iter: 737 loss: 9.37954e-06
Iter: 738 loss: 9.32857347e-06
Iter: 739 loss: 9.32726289e-06
Iter: 740 loss: 9.32226794e-06
Iter: 741 loss: 9.33474439e-06
Iter: 742 loss: 9.31939e-06
Iter: 743 loss: 9.31281465e-06
Iter: 744 loss: 9.31248e-06
Iter: 745 loss: 9.3069084e-06
Iter: 746 loss: 9.2972e-06
Iter: 747 loss: 9.31814247e-06
Iter: 748 loss: 9.29361158e-06
Iter: 749 loss: 9.28570353e-06
Iter: 750 loss: 9.36742163e-06
Iter: 751 loss: 9.28568443e-06
Iter: 752 loss: 9.27897418e-06
Iter: 753 loss: 9.28886402e-06
Iter: 754 loss: 9.27540077e-06
Iter: 755 loss: 9.26775e-06
Iter: 756 loss: 9.25733912e-06
Iter: 757 loss: 9.2569062e-06
Iter: 758 loss: 9.24441883e-06
Iter: 759 loss: 9.32192233e-06
Iter: 760 loss: 9.24286815e-06
Iter: 761 loss: 9.23407151e-06
Iter: 762 loss: 9.24046071e-06
Iter: 763 loss: 9.22838444e-06
Iter: 764 loss: 9.21648279e-06
Iter: 765 loss: 9.28585087e-06
Iter: 766 loss: 9.21563696e-06
Iter: 767 loss: 9.21187711e-06
Iter: 768 loss: 9.21064475e-06
Iter: 769 loss: 9.20738603e-06
Iter: 770 loss: 9.2071823e-06
Iter: 771 loss: 9.2061382e-06
Iter: 772 loss: 9.20264e-06
Iter: 773 loss: 9.20294497e-06
Iter: 774 loss: 9.19939612e-06
Iter: 775 loss: 9.19168724e-06
Iter: 776 loss: 9.20247476e-06
Iter: 777 loss: 9.18831938e-06
Iter: 778 loss: 9.1803613e-06
Iter: 779 loss: 9.2022492e-06
Iter: 780 loss: 9.1775546e-06
Iter: 781 loss: 9.17139732e-06
Iter: 782 loss: 9.19685772e-06
Iter: 783 loss: 9.16992485e-06
Iter: 784 loss: 9.16267163e-06
Iter: 785 loss: 9.17009766e-06
Iter: 786 loss: 9.15858163e-06
Iter: 787 loss: 9.14844168e-06
Iter: 788 loss: 9.14368593e-06
Iter: 789 loss: 9.1385773e-06
Iter: 790 loss: 9.12666928e-06
Iter: 791 loss: 9.18326077e-06
Iter: 792 loss: 9.1247166e-06
Iter: 793 loss: 9.11476673e-06
Iter: 794 loss: 9.12138785e-06
Iter: 795 loss: 9.10872768e-06
Iter: 796 loss: 9.09365917e-06
Iter: 797 loss: 9.12852283e-06
Iter: 798 loss: 9.08859693e-06
Iter: 799 loss: 9.08048241e-06
Iter: 800 loss: 9.07954563e-06
Iter: 801 loss: 9.07653e-06
Iter: 802 loss: 9.07524918e-06
Iter: 803 loss: 9.07355752e-06
Iter: 804 loss: 9.06831519e-06
Iter: 805 loss: 9.06622518e-06
Iter: 806 loss: 9.06216246e-06
Iter: 807 loss: 9.0523954e-06
Iter: 808 loss: 9.08623e-06
Iter: 809 loss: 9.04998706e-06
Iter: 810 loss: 9.03917498e-06
Iter: 811 loss: 9.03976252e-06
Iter: 812 loss: 9.03083401e-06
Iter: 813 loss: 9.01888598e-06
Iter: 814 loss: 9.1073e-06
Iter: 815 loss: 9.01783642e-06
Iter: 816 loss: 9.00870782e-06
Iter: 817 loss: 9.05274e-06
Iter: 818 loss: 9.00723171e-06
Iter: 819 loss: 8.99870247e-06
Iter: 820 loss: 8.99802581e-06
Iter: 821 loss: 8.99192491e-06
Iter: 822 loss: 8.9807354e-06
Iter: 823 loss: 8.99349e-06
Iter: 824 loss: 8.97447353e-06
Iter: 825 loss: 8.96025267e-06
Iter: 826 loss: 8.98145208e-06
Iter: 827 loss: 8.95367702e-06
Iter: 828 loss: 8.93630568e-06
Iter: 829 loss: 8.98353392e-06
Iter: 830 loss: 8.93085235e-06
Iter: 831 loss: 8.91912896e-06
Iter: 832 loss: 9.05803336e-06
Iter: 833 loss: 8.91887e-06
Iter: 834 loss: 8.92380376e-06
Iter: 835 loss: 8.91470881e-06
Iter: 836 loss: 8.91295531e-06
Iter: 837 loss: 8.90793e-06
Iter: 838 loss: 8.89717921e-06
Iter: 839 loss: 8.89717194e-06
Iter: 840 loss: 8.88400427e-06
Iter: 841 loss: 8.93037395e-06
Iter: 842 loss: 8.88045724e-06
Iter: 843 loss: 8.86778435e-06
Iter: 844 loss: 8.91889795e-06
Iter: 845 loss: 8.86520229e-06
Iter: 846 loss: 8.85763257e-06
Iter: 847 loss: 8.90425144e-06
Iter: 848 loss: 8.85658665e-06
Iter: 849 loss: 8.8489e-06
Iter: 850 loss: 8.85791724e-06
Iter: 851 loss: 8.84451765e-06
Iter: 852 loss: 8.8335546e-06
Iter: 853 loss: 8.82288168e-06
Iter: 854 loss: 8.82049608e-06
Iter: 855 loss: 8.80736116e-06
Iter: 856 loss: 8.84733345e-06
Iter: 857 loss: 8.80374137e-06
Iter: 858 loss: 8.79347317e-06
Iter: 859 loss: 8.82592758e-06
Iter: 860 loss: 8.79021172e-06
Iter: 861 loss: 8.78039646e-06
Iter: 862 loss: 8.79380696e-06
Iter: 863 loss: 8.77510865e-06
Iter: 864 loss: 8.76458489e-06
Iter: 865 loss: 8.7843855e-06
Iter: 866 loss: 8.76022568e-06
Iter: 867 loss: 8.79505387e-06
Iter: 868 loss: 8.75768637e-06
Iter: 869 loss: 8.7564531e-06
Iter: 870 loss: 8.75207843e-06
Iter: 871 loss: 8.7452463e-06
Iter: 872 loss: 8.74446869e-06
Iter: 873 loss: 8.73396129e-06
Iter: 874 loss: 8.74949365e-06
Iter: 875 loss: 8.72921737e-06
Iter: 876 loss: 8.7197659e-06
Iter: 877 loss: 8.78297e-06
Iter: 878 loss: 8.71876819e-06
Iter: 879 loss: 8.71272823e-06
Iter: 880 loss: 8.70577151e-06
Iter: 881 loss: 8.70486656e-06
Iter: 882 loss: 8.69394e-06
Iter: 883 loss: 8.83490611e-06
Iter: 884 loss: 8.69381256e-06
Iter: 885 loss: 8.68645202e-06
Iter: 886 loss: 8.72274722e-06
Iter: 887 loss: 8.68529332e-06
Iter: 888 loss: 8.68032657e-06
Iter: 889 loss: 8.67234576e-06
Iter: 890 loss: 8.67243307e-06
Iter: 891 loss: 8.66169921e-06
Iter: 892 loss: 8.6648879e-06
Iter: 893 loss: 8.65412767e-06
Iter: 894 loss: 8.64154e-06
Iter: 895 loss: 8.71706743e-06
Iter: 896 loss: 8.64006415e-06
Iter: 897 loss: 8.63090645e-06
Iter: 898 loss: 8.62293e-06
Iter: 899 loss: 8.62040724e-06
Iter: 900 loss: 8.68594543e-06
Iter: 901 loss: 8.6194359e-06
Iter: 902 loss: 8.6176e-06
Iter: 903 loss: 8.61264e-06
Iter: 904 loss: 8.61773515e-06
Iter: 905 loss: 8.60876298e-06
Iter: 906 loss: 8.59825104e-06
Iter: 907 loss: 8.58636849e-06
Iter: 908 loss: 8.58442581e-06
Iter: 909 loss: 8.57047326e-06
Iter: 910 loss: 8.57040141e-06
Iter: 911 loss: 8.55709459e-06
Iter: 912 loss: 8.56147381e-06
Iter: 913 loss: 8.5479378e-06
Iter: 914 loss: 8.53603342e-06
Iter: 915 loss: 8.58155545e-06
Iter: 916 loss: 8.53335769e-06
Iter: 917 loss: 8.52555604e-06
Iter: 918 loss: 8.5354277e-06
Iter: 919 loss: 8.52118683e-06
Iter: 920 loss: 8.50808101e-06
Iter: 921 loss: 8.52224093e-06
Iter: 922 loss: 8.50032666e-06
Iter: 923 loss: 8.48454056e-06
Iter: 924 loss: 8.53407437e-06
Iter: 925 loss: 8.47967749e-06
Iter: 926 loss: 8.46895091e-06
Iter: 927 loss: 8.48572927e-06
Iter: 928 loss: 8.46389594e-06
Iter: 929 loss: 8.45048817e-06
Iter: 930 loss: 8.46265175e-06
Iter: 931 loss: 8.44313672e-06
Iter: 932 loss: 8.44260103e-06
Iter: 933 loss: 8.43796079e-06
Iter: 934 loss: 8.43143425e-06
Iter: 935 loss: 8.44275928e-06
Iter: 936 loss: 8.42855661e-06
Iter: 937 loss: 8.42690497e-06
Iter: 938 loss: 8.42196823e-06
Iter: 939 loss: 8.4201165e-06
Iter: 940 loss: 8.41589281e-06
Iter: 941 loss: 8.40222128e-06
Iter: 942 loss: 8.4050007e-06
Iter: 943 loss: 8.39251e-06
Iter: 944 loss: 8.38080268e-06
Iter: 945 loss: 8.49748903e-06
Iter: 946 loss: 8.38032611e-06
Iter: 947 loss: 8.3695013e-06
Iter: 948 loss: 8.40471421e-06
Iter: 949 loss: 8.36624258e-06
Iter: 950 loss: 8.35681567e-06
Iter: 951 loss: 8.43103953e-06
Iter: 952 loss: 8.35620267e-06
Iter: 953 loss: 8.3465693e-06
Iter: 954 loss: 8.37712287e-06
Iter: 955 loss: 8.34421371e-06
Iter: 956 loss: 8.33801823e-06
Iter: 957 loss: 8.3447012e-06
Iter: 958 loss: 8.33505146e-06
Iter: 959 loss: 8.32725073e-06
Iter: 960 loss: 8.3255627e-06
Iter: 961 loss: 8.32050318e-06
Iter: 962 loss: 8.30709723e-06
Iter: 963 loss: 8.31028228e-06
Iter: 964 loss: 8.29718829e-06
Iter: 965 loss: 8.29482724e-06
Iter: 966 loss: 8.29124929e-06
Iter: 967 loss: 8.2876395e-06
Iter: 968 loss: 8.28721204e-06
Iter: 969 loss: 8.28615339e-06
Iter: 970 loss: 8.28178509e-06
Iter: 971 loss: 8.26911e-06
Iter: 972 loss: 8.41673318e-06
Iter: 973 loss: 8.26777432e-06
Iter: 974 loss: 8.25427e-06
Iter: 975 loss: 8.38898541e-06
Iter: 976 loss: 8.25338157e-06
Iter: 977 loss: 8.24623385e-06
Iter: 978 loss: 8.24025665e-06
Iter: 979 loss: 8.23785194e-06
Iter: 980 loss: 8.22364927e-06
Iter: 981 loss: 8.2857141e-06
Iter: 982 loss: 8.2207e-06
Iter: 983 loss: 8.20889727e-06
Iter: 984 loss: 8.3291e-06
Iter: 985 loss: 8.20884816e-06
Iter: 986 loss: 8.19927845e-06
Iter: 987 loss: 8.23147275e-06
Iter: 988 loss: 8.19721117e-06
Iter: 989 loss: 8.18905664e-06
Iter: 990 loss: 8.19443085e-06
Iter: 991 loss: 8.18403078e-06
Iter: 992 loss: 8.16987176e-06
Iter: 993 loss: 8.17148521e-06
Iter: 994 loss: 8.15883777e-06
Iter: 995 loss: 8.14408395e-06
Iter: 996 loss: 8.21578e-06
Iter: 997 loss: 8.14142641e-06
Iter: 998 loss: 8.13255429e-06
Iter: 999 loss: 8.1549e-06
Iter: 1000 loss: 8.12970211e-06
Iter: 1001 loss: 8.15295061e-06
Iter: 1002 loss: 8.12756116e-06
Iter: 1003 loss: 8.12622056e-06
Iter: 1004 loss: 8.12222152e-06
Iter: 1005 loss: 8.11449445e-06
Iter: 1006 loss: 8.11446534e-06
Iter: 1007 loss: 8.10684469e-06
Iter: 1008 loss: 8.10617439e-06
Iter: 1009 loss: 8.10072561e-06
Iter: 1010 loss: 8.0890768e-06
Iter: 1011 loss: 8.15119802e-06
Iter: 1012 loss: 8.08713412e-06
Iter: 1013 loss: 8.07680226e-06
Iter: 1014 loss: 8.12798135e-06
Iter: 1015 loss: 8.07514516e-06
Iter: 1016 loss: 8.06838e-06
Iter: 1017 loss: 8.06560729e-06
Iter: 1018 loss: 8.06165917e-06
Iter: 1019 loss: 8.05666332e-06
Iter: 1020 loss: 8.05510444e-06
Iter: 1021 loss: 8.04990668e-06
Iter: 1022 loss: 8.04419597e-06
Iter: 1023 loss: 8.04331103e-06
Iter: 1024 loss: 8.03615694e-06
Iter: 1025 loss: 8.03469175e-06
Iter: 1026 loss: 8.03005423e-06
Iter: 1027 loss: 8.02047907e-06
Iter: 1028 loss: 8.0813179e-06
Iter: 1029 loss: 8.01950046e-06
Iter: 1030 loss: 8.01094393e-06
Iter: 1031 loss: 7.99964073e-06
Iter: 1032 loss: 7.99874397e-06
Iter: 1033 loss: 8.02367049e-06
Iter: 1034 loss: 7.99575173e-06
Iter: 1035 loss: 7.99097415e-06
Iter: 1036 loss: 7.98719429e-06
Iter: 1037 loss: 7.98617111e-06
Iter: 1038 loss: 7.9839092e-06
Iter: 1039 loss: 7.97849498e-06
Iter: 1040 loss: 8.05546188e-06
Iter: 1041 loss: 7.97812936e-06
Iter: 1042 loss: 7.96698e-06
Iter: 1043 loss: 7.97418e-06
Iter: 1044 loss: 7.95972664e-06
Iter: 1045 loss: 7.95376582e-06
Iter: 1046 loss: 7.95145752e-06
Iter: 1047 loss: 7.94515472e-06
Iter: 1048 loss: 7.93577419e-06
Iter: 1049 loss: 7.93536219e-06
Iter: 1050 loss: 7.9253814e-06
Iter: 1051 loss: 7.92549054e-06
Iter: 1052 loss: 7.91684579e-06
Iter: 1053 loss: 7.96745735e-06
Iter: 1054 loss: 7.9156971e-06
Iter: 1055 loss: 7.91157527e-06
Iter: 1056 loss: 7.90332069e-06
Iter: 1057 loss: 8.08978257e-06
Iter: 1058 loss: 7.90358081e-06
Iter: 1059 loss: 7.89319256e-06
Iter: 1060 loss: 7.94358129e-06
Iter: 1061 loss: 7.89157821e-06
Iter: 1062 loss: 7.88257512e-06
Iter: 1063 loss: 7.90931699e-06
Iter: 1064 loss: 7.88018315e-06
Iter: 1065 loss: 7.87297267e-06
Iter: 1066 loss: 7.87410409e-06
Iter: 1067 loss: 7.86751843e-06
Iter: 1068 loss: 7.88633588e-06
Iter: 1069 loss: 7.86582495e-06
Iter: 1070 loss: 7.86340752e-06
Iter: 1071 loss: 7.85668726e-06
Iter: 1072 loss: 7.88564648e-06
Iter: 1073 loss: 7.85407519e-06
Iter: 1074 loss: 7.84859822e-06
Iter: 1075 loss: 7.83814357e-06
Iter: 1076 loss: 8.0693062e-06
Iter: 1077 loss: 7.83795804e-06
Iter: 1078 loss: 7.8273024e-06
Iter: 1079 loss: 7.8741341e-06
Iter: 1080 loss: 7.82541701e-06
Iter: 1081 loss: 7.81354e-06
Iter: 1082 loss: 7.82906591e-06
Iter: 1083 loss: 7.80804112e-06
Iter: 1084 loss: 7.79768561e-06
Iter: 1085 loss: 7.86096462e-06
Iter: 1086 loss: 7.79615402e-06
Iter: 1087 loss: 7.79127822e-06
Iter: 1088 loss: 7.8708008e-06
Iter: 1089 loss: 7.7913719e-06
Iter: 1090 loss: 7.78654794e-06
Iter: 1091 loss: 7.80169103e-06
Iter: 1092 loss: 7.78528101e-06
Iter: 1093 loss: 7.78056165e-06
Iter: 1094 loss: 7.77425248e-06
Iter: 1095 loss: 7.77381501e-06
Iter: 1096 loss: 7.76720481e-06
Iter: 1097 loss: 7.75754779e-06
Iter: 1098 loss: 7.75697117e-06
Iter: 1099 loss: 7.74529e-06
Iter: 1100 loss: 7.76139132e-06
Iter: 1101 loss: 7.73924e-06
Iter: 1102 loss: 7.73032843e-06
Iter: 1103 loss: 7.81883227e-06
Iter: 1104 loss: 7.72997373e-06
Iter: 1105 loss: 7.73551346e-06
Iter: 1106 loss: 7.72907879e-06
Iter: 1107 loss: 7.72779276e-06
Iter: 1108 loss: 7.72286512e-06
Iter: 1109 loss: 7.70759834e-06
Iter: 1110 loss: 7.85235443e-06
Iter: 1111 loss: 7.70525548e-06
Iter: 1112 loss: 7.6928e-06
Iter: 1113 loss: 7.75310673e-06
Iter: 1114 loss: 7.6907e-06
Iter: 1115 loss: 7.68031714e-06
Iter: 1116 loss: 7.69762482e-06
Iter: 1117 loss: 7.67586698e-06
Iter: 1118 loss: 7.66517678e-06
Iter: 1119 loss: 7.74527416e-06
Iter: 1120 loss: 7.66463199e-06
Iter: 1121 loss: 7.65656478e-06
Iter: 1122 loss: 7.70811221e-06
Iter: 1123 loss: 7.65575896e-06
Iter: 1124 loss: 7.64901415e-06
Iter: 1125 loss: 7.70194947e-06
Iter: 1126 loss: 7.64862853e-06
Iter: 1127 loss: 7.6436263e-06
Iter: 1128 loss: 7.66764424e-06
Iter: 1129 loss: 7.64292417e-06
Iter: 1130 loss: 7.638434e-06
Iter: 1131 loss: 7.6301676e-06
Iter: 1132 loss: 7.80020855e-06
Iter: 1133 loss: 7.63022217e-06
Iter: 1134 loss: 7.62047193e-06
Iter: 1135 loss: 7.64740525e-06
Iter: 1136 loss: 7.6173028e-06
Iter: 1137 loss: 7.61013962e-06
Iter: 1138 loss: 7.65740697e-06
Iter: 1139 loss: 7.60932653e-06
Iter: 1140 loss: 7.60627472e-06
Iter: 1141 loss: 7.60618332e-06
Iter: 1142 loss: 7.60104e-06
Iter: 1143 loss: 7.59850354e-06
Iter: 1144 loss: 7.59625573e-06
Iter: 1145 loss: 7.59408977e-06
Iter: 1146 loss: 7.5880057e-06
Iter: 1147 loss: 7.62144873e-06
Iter: 1148 loss: 7.58613805e-06
Iter: 1149 loss: 7.57797352e-06
Iter: 1150 loss: 7.58859824e-06
Iter: 1151 loss: 7.57426324e-06
Iter: 1152 loss: 7.56188592e-06
Iter: 1153 loss: 7.59788054e-06
Iter: 1154 loss: 7.55835481e-06
Iter: 1155 loss: 7.54797657e-06
Iter: 1156 loss: 7.55562678e-06
Iter: 1157 loss: 7.54190523e-06
Iter: 1158 loss: 7.53188397e-06
Iter: 1159 loss: 7.6267379e-06
Iter: 1160 loss: 7.5312937e-06
Iter: 1161 loss: 7.52957931e-06
Iter: 1162 loss: 7.52780761e-06
Iter: 1163 loss: 7.52525784e-06
Iter: 1164 loss: 7.52671531e-06
Iter: 1165 loss: 7.5237167e-06
Iter: 1166 loss: 7.52059e-06
Iter: 1167 loss: 7.5167186e-06
Iter: 1168 loss: 7.51632251e-06
Iter: 1169 loss: 7.51096513e-06
Iter: 1170 loss: 7.50911204e-06
Iter: 1171 loss: 7.50568552e-06
Iter: 1172 loss: 7.49917581e-06
Iter: 1173 loss: 7.52909818e-06
Iter: 1174 loss: 7.49804e-06
Iter: 1175 loss: 7.49944365e-06
Iter: 1176 loss: 7.49525316e-06
Iter: 1177 loss: 7.49432456e-06
Iter: 1178 loss: 7.49103856e-06
Iter: 1179 loss: 7.48729144e-06
Iter: 1180 loss: 7.48637194e-06
Iter: 1181 loss: 7.47911145e-06
Iter: 1182 loss: 7.47089416e-06
Iter: 1183 loss: 7.46997694e-06
Iter: 1184 loss: 7.45769876e-06
Iter: 1185 loss: 7.54723715e-06
Iter: 1186 loss: 7.45676516e-06
Iter: 1187 loss: 7.44969e-06
Iter: 1188 loss: 7.46074966e-06
Iter: 1189 loss: 7.44632871e-06
Iter: 1190 loss: 7.43831151e-06
Iter: 1191 loss: 7.45905163e-06
Iter: 1192 loss: 7.43533747e-06
Iter: 1193 loss: 7.4308482e-06
Iter: 1194 loss: 7.43071632e-06
Iter: 1195 loss: 7.42639804e-06
Iter: 1196 loss: 7.44985755e-06
Iter: 1197 loss: 7.42569409e-06
Iter: 1198 loss: 7.42327484e-06
Iter: 1199 loss: 7.4211257e-06
Iter: 1200 loss: 7.42024395e-06
Iter: 1201 loss: 7.41558688e-06
Iter: 1202 loss: 7.40792802e-06
Iter: 1203 loss: 7.40755695e-06
Iter: 1204 loss: 7.40369205e-06
Iter: 1205 loss: 7.40374298e-06
Iter: 1206 loss: 7.40652877e-06
Iter: 1207 loss: 7.40271662e-06
Iter: 1208 loss: 7.40216819e-06
Iter: 1209 loss: 7.40047062e-06
Iter: 1210 loss: 7.3974129e-06
Iter: 1211 loss: 7.39739835e-06
Iter: 1212 loss: 7.39231109e-06
Iter: 1213 loss: 7.38632e-06
Iter: 1214 loss: 7.38576364e-06
Iter: 1215 loss: 7.38096469e-06
Iter: 1216 loss: 7.41901249e-06
Iter: 1217 loss: 7.38043491e-06
Iter: 1218 loss: 7.37599385e-06
Iter: 1219 loss: 7.3703236e-06
Iter: 1220 loss: 7.36971197e-06
Iter: 1221 loss: 7.36331504e-06
Iter: 1222 loss: 7.3999413e-06
Iter: 1223 loss: 7.36212496e-06
Iter: 1224 loss: 7.35924459e-06
Iter: 1225 loss: 7.38637073e-06
Iter: 1226 loss: 7.35913773e-06
Iter: 1227 loss: 7.35617141e-06
Iter: 1228 loss: 7.39228835e-06
Iter: 1229 loss: 7.35601043e-06
Iter: 1230 loss: 7.35365575e-06
Iter: 1231 loss: 7.35565e-06
Iter: 1232 loss: 7.35231333e-06
Iter: 1233 loss: 7.34979403e-06
Iter: 1234 loss: 7.34632431e-06
Iter: 1235 loss: 7.34606874e-06
Iter: 1236 loss: 7.34268042e-06
Iter: 1237 loss: 7.361e-06
Iter: 1238 loss: 7.34193873e-06
Iter: 1239 loss: 7.34623427e-06
Iter: 1240 loss: 7.34106743e-06
Iter: 1241 loss: 7.34010382e-06
Iter: 1242 loss: 7.33721299e-06
Iter: 1243 loss: 7.33711e-06
Iter: 1244 loss: 7.3341057e-06
Iter: 1245 loss: 7.329244e-06
Iter: 1246 loss: 7.33231e-06
Iter: 1247 loss: 7.32607805e-06
Iter: 1248 loss: 7.32160697e-06
Iter: 1249 loss: 7.32090757e-06
Iter: 1250 loss: 7.31786e-06
Iter: 1251 loss: 7.31e-06
Iter: 1252 loss: 7.33135766e-06
Iter: 1253 loss: 7.30728516e-06
Iter: 1254 loss: 7.30229158e-06
Iter: 1255 loss: 7.34163632e-06
Iter: 1256 loss: 7.3015558e-06
Iter: 1257 loss: 7.29650219e-06
Iter: 1258 loss: 7.30251713e-06
Iter: 1259 loss: 7.29397561e-06
Iter: 1260 loss: 7.28978239e-06
Iter: 1261 loss: 7.28982e-06
Iter: 1262 loss: 7.28672239e-06
Iter: 1263 loss: 7.31358068e-06
Iter: 1264 loss: 7.28633859e-06
Iter: 1265 loss: 7.2841558e-06
Iter: 1266 loss: 7.27876659e-06
Iter: 1267 loss: 7.36666607e-06
Iter: 1268 loss: 7.27874e-06
Iter: 1269 loss: 7.27416182e-06
Iter: 1270 loss: 7.28804935e-06
Iter: 1271 loss: 7.27225e-06
Iter: 1272 loss: 7.2787866e-06
Iter: 1273 loss: 7.27130328e-06
Iter: 1274 loss: 7.26982671e-06
Iter: 1275 loss: 7.26658664e-06
Iter: 1276 loss: 7.28395298e-06
Iter: 1277 loss: 7.26565122e-06
Iter: 1278 loss: 7.26201415e-06
Iter: 1279 loss: 7.25909e-06
Iter: 1280 loss: 7.25828613e-06
Iter: 1281 loss: 7.25443761e-06
Iter: 1282 loss: 7.26013104e-06
Iter: 1283 loss: 7.25273185e-06
Iter: 1284 loss: 7.24835e-06
Iter: 1285 loss: 7.24747952e-06
Iter: 1286 loss: 7.24486608e-06
Iter: 1287 loss: 7.23774428e-06
Iter: 1288 loss: 7.24882966e-06
Iter: 1289 loss: 7.23406265e-06
Iter: 1290 loss: 7.22858931e-06
Iter: 1291 loss: 7.30298461e-06
Iter: 1292 loss: 7.22878212e-06
Iter: 1293 loss: 7.22451296e-06
Iter: 1294 loss: 7.24208712e-06
Iter: 1295 loss: 7.22352934e-06
Iter: 1296 loss: 7.21874585e-06
Iter: 1297 loss: 7.24452275e-06
Iter: 1298 loss: 7.21783499e-06
Iter: 1299 loss: 7.21321476e-06
Iter: 1300 loss: 7.21580545e-06
Iter: 1301 loss: 7.20986645e-06
Iter: 1302 loss: 7.20652724e-06
Iter: 1303 loss: 7.20559729e-06
Iter: 1304 loss: 7.20395883e-06
Iter: 1305 loss: 7.19933269e-06
Iter: 1306 loss: 7.22605591e-06
Iter: 1307 loss: 7.19895297e-06
Iter: 1308 loss: 7.19271475e-06
Iter: 1309 loss: 7.22236518e-06
Iter: 1310 loss: 7.19138461e-06
Iter: 1311 loss: 7.18964e-06
Iter: 1312 loss: 7.18427873e-06
Iter: 1313 loss: 7.21062406e-06
Iter: 1314 loss: 7.18264937e-06
Iter: 1315 loss: 7.17655e-06
Iter: 1316 loss: 7.19716127e-06
Iter: 1317 loss: 7.17497551e-06
Iter: 1318 loss: 7.17050489e-06
Iter: 1319 loss: 7.17478179e-06
Iter: 1320 loss: 7.16790328e-06
Iter: 1321 loss: 7.16247e-06
Iter: 1322 loss: 7.15804526e-06
Iter: 1323 loss: 7.15642727e-06
Iter: 1324 loss: 7.15221e-06
Iter: 1325 loss: 7.15154783e-06
Iter: 1326 loss: 7.14850103e-06
Iter: 1327 loss: 7.14875841e-06
Iter: 1328 loss: 7.14613725e-06
Iter: 1329 loss: 7.14106318e-06
Iter: 1330 loss: 7.20046046e-06
Iter: 1331 loss: 7.14088e-06
Iter: 1332 loss: 7.13603231e-06
Iter: 1333 loss: 7.13881491e-06
Iter: 1334 loss: 7.13314421e-06
Iter: 1335 loss: 7.12969813e-06
Iter: 1336 loss: 7.12814426e-06
Iter: 1337 loss: 7.12643578e-06
Iter: 1338 loss: 7.12253404e-06
Iter: 1339 loss: 7.11563462e-06
Iter: 1340 loss: 7.28629084e-06
Iter: 1341 loss: 7.11550774e-06
Iter: 1342 loss: 7.18561887e-06
Iter: 1343 loss: 7.11439e-06
Iter: 1344 loss: 7.11328175e-06
Iter: 1345 loss: 7.10991935e-06
Iter: 1346 loss: 7.11787288e-06
Iter: 1347 loss: 7.10799577e-06
Iter: 1348 loss: 7.10432914e-06
Iter: 1349 loss: 7.1111449e-06
Iter: 1350 loss: 7.10299537e-06
Iter: 1351 loss: 7.09795358e-06
Iter: 1352 loss: 7.09705091e-06
Iter: 1353 loss: 7.09346568e-06
Iter: 1354 loss: 7.08652306e-06
Iter: 1355 loss: 7.09812639e-06
Iter: 1356 loss: 7.08328207e-06
Iter: 1357 loss: 7.07869503e-06
Iter: 1358 loss: 7.07943309e-06
Iter: 1359 loss: 7.07506888e-06
Iter: 1360 loss: 7.06815899e-06
Iter: 1361 loss: 7.1025047e-06
Iter: 1362 loss: 7.06718811e-06
Iter: 1363 loss: 7.06293667e-06
Iter: 1364 loss: 7.11879966e-06
Iter: 1365 loss: 7.06282572e-06
Iter: 1366 loss: 7.05929324e-06
Iter: 1367 loss: 7.07949766e-06
Iter: 1368 loss: 7.05907905e-06
Iter: 1369 loss: 7.0563683e-06
Iter: 1370 loss: 7.05487855e-06
Iter: 1371 loss: 7.05370076e-06
Iter: 1372 loss: 7.05018283e-06
Iter: 1373 loss: 7.0457354e-06
Iter: 1374 loss: 7.04532113e-06
Iter: 1375 loss: 7.04038212e-06
Iter: 1376 loss: 7.05267121e-06
Iter: 1377 loss: 7.0385322e-06
Iter: 1378 loss: 7.03674141e-06
Iter: 1379 loss: 7.03457681e-06
Iter: 1380 loss: 7.03382057e-06
Iter: 1381 loss: 7.03063051e-06
Iter: 1382 loss: 7.02791795e-06
Iter: 1383 loss: 7.02613943e-06
Iter: 1384 loss: 7.0205333e-06
Iter: 1385 loss: 7.01852696e-06
Iter: 1386 loss: 7.01510726e-06
Iter: 1387 loss: 7.00988085e-06
Iter: 1388 loss: 7.03662135e-06
Iter: 1389 loss: 7.00921555e-06
Iter: 1390 loss: 7.00343935e-06
Iter: 1391 loss: 7.0447586e-06
Iter: 1392 loss: 7.00299734e-06
Iter: 1393 loss: 6.99817883e-06
Iter: 1394 loss: 6.99945213e-06
Iter: 1395 loss: 6.9947846e-06
Iter: 1396 loss: 6.99162683e-06
Iter: 1397 loss: 7.0331007e-06
Iter: 1398 loss: 6.99171596e-06
Iter: 1399 loss: 6.98948952e-06
Iter: 1400 loss: 7.00347846e-06
Iter: 1401 loss: 6.98928898e-06
Iter: 1402 loss: 6.98684335e-06
Iter: 1403 loss: 6.98288204e-06
Iter: 1404 loss: 6.9829739e-06
Iter: 1405 loss: 6.97665155e-06
Iter: 1406 loss: 6.97427822e-06
Iter: 1407 loss: 6.97067117e-06
Iter: 1408 loss: 6.9635912e-06
Iter: 1409 loss: 7.01321233e-06
Iter: 1410 loss: 6.96278039e-06
Iter: 1411 loss: 6.95836434e-06
Iter: 1412 loss: 6.96525558e-06
Iter: 1413 loss: 6.95627887e-06
Iter: 1414 loss: 6.94852861e-06
Iter: 1415 loss: 6.99774455e-06
Iter: 1416 loss: 6.94757819e-06
Iter: 1417 loss: 6.9461621e-06
Iter: 1418 loss: 6.94224673e-06
Iter: 1419 loss: 6.97044925e-06
Iter: 1420 loss: 6.94161326e-06
Iter: 1421 loss: 6.9381008e-06
Iter: 1422 loss: 6.9378716e-06
Iter: 1423 loss: 6.93523862e-06
Iter: 1424 loss: 6.92910544e-06
Iter: 1425 loss: 6.92273534e-06
Iter: 1426 loss: 6.92134881e-06
Iter: 1427 loss: 6.91780497e-06
Iter: 1428 loss: 6.91636069e-06
Iter: 1429 loss: 6.91381047e-06
Iter: 1430 loss: 6.91093464e-06
Iter: 1431 loss: 6.9106809e-06
Iter: 1432 loss: 6.90449542e-06
Iter: 1433 loss: 6.92956201e-06
Iter: 1434 loss: 6.9028174e-06
Iter: 1435 loss: 6.89602848e-06
Iter: 1436 loss: 6.88631962e-06
Iter: 1437 loss: 6.88561295e-06
Iter: 1438 loss: 6.90136721e-06
Iter: 1439 loss: 6.88153068e-06
Iter: 1440 loss: 6.87918282e-06
Iter: 1441 loss: 6.87455849e-06
Iter: 1442 loss: 6.96666848e-06
Iter: 1443 loss: 6.87440524e-06
Iter: 1444 loss: 6.87121292e-06
Iter: 1445 loss: 6.87171178e-06
Iter: 1446 loss: 6.86847716e-06
Iter: 1447 loss: 6.86308886e-06
Iter: 1448 loss: 6.86502835e-06
Iter: 1449 loss: 6.85910118e-06
Iter: 1450 loss: 6.87026477e-06
Iter: 1451 loss: 6.8569525e-06
Iter: 1452 loss: 6.85602936e-06
Iter: 1453 loss: 6.85315899e-06
Iter: 1454 loss: 6.85364421e-06
Iter: 1455 loss: 6.85033592e-06
Iter: 1456 loss: 6.84370661e-06
Iter: 1457 loss: 6.84465294e-06
Iter: 1458 loss: 6.83894268e-06
Iter: 1459 loss: 6.83135477e-06
Iter: 1460 loss: 6.87826059e-06
Iter: 1461 loss: 6.83041708e-06
Iter: 1462 loss: 6.82896689e-06
Iter: 1463 loss: 6.82736254e-06
Iter: 1464 loss: 6.82543896e-06
Iter: 1465 loss: 6.82141535e-06
Iter: 1466 loss: 6.88683576e-06
Iter: 1467 loss: 6.82141354e-06
Iter: 1468 loss: 6.81655911e-06
Iter: 1469 loss: 6.80865924e-06
Iter: 1470 loss: 6.80848279e-06
Iter: 1471 loss: 6.80395942e-06
Iter: 1472 loss: 6.86852127e-06
Iter: 1473 loss: 6.80381117e-06
Iter: 1474 loss: 6.80082394e-06
Iter: 1475 loss: 6.82759764e-06
Iter: 1476 loss: 6.80062612e-06
Iter: 1477 loss: 6.79804134e-06
Iter: 1478 loss: 6.79857931e-06
Iter: 1479 loss: 6.79645e-06
Iter: 1480 loss: 6.79271807e-06
Iter: 1481 loss: 6.79172899e-06
Iter: 1482 loss: 6.78940523e-06
Iter: 1483 loss: 6.79242e-06
Iter: 1484 loss: 6.78755896e-06
Iter: 1485 loss: 6.78633705e-06
Iter: 1486 loss: 6.78219749e-06
Iter: 1487 loss: 6.79329287e-06
Iter: 1488 loss: 6.77995331e-06
Iter: 1489 loss: 6.77647222e-06
Iter: 1490 loss: 6.77369962e-06
Iter: 1491 loss: 6.77254593e-06
Iter: 1492 loss: 6.76683703e-06
Iter: 1493 loss: 6.78496326e-06
Iter: 1494 loss: 6.76504851e-06
Iter: 1495 loss: 6.75998035e-06
Iter: 1496 loss: 6.81804568e-06
Iter: 1497 loss: 6.76012769e-06
Iter: 1498 loss: 6.75652791e-06
Iter: 1499 loss: 6.80744915e-06
Iter: 1500 loss: 6.75648289e-06
Iter: 1501 loss: 6.75395222e-06
Iter: 1502 loss: 6.74802368e-06
Iter: 1503 loss: 6.80893891e-06
Iter: 1504 loss: 6.74756029e-06
Iter: 1505 loss: 6.74130843e-06
Iter: 1506 loss: 6.78878268e-06
Iter: 1507 loss: 6.74103921e-06
Iter: 1508 loss: 6.73804561e-06
Iter: 1509 loss: 6.73689419e-06
Iter: 1510 loss: 6.7354822e-06
Iter: 1511 loss: 6.73214527e-06
Iter: 1512 loss: 6.72722035e-06
Iter: 1513 loss: 6.7269375e-06
Iter: 1514 loss: 6.72444185e-06
Iter: 1515 loss: 6.72694841e-06
Iter: 1516 loss: 6.72292936e-06
Iter: 1517 loss: 6.72055285e-06
Iter: 1518 loss: 6.71632051e-06
Iter: 1519 loss: 6.81966094e-06
Iter: 1520 loss: 6.71626594e-06
Iter: 1521 loss: 6.71360522e-06
Iter: 1522 loss: 6.71767793e-06
Iter: 1523 loss: 6.71226189e-06
Iter: 1524 loss: 6.70962163e-06
Iter: 1525 loss: 6.71467387e-06
Iter: 1526 loss: 6.70850568e-06
Iter: 1527 loss: 6.70581539e-06
Iter: 1528 loss: 6.69937845e-06
Iter: 1529 loss: 6.79495861e-06
Iter: 1530 loss: 6.69944e-06
Iter: 1531 loss: 6.69351903e-06
Iter: 1532 loss: 6.77175512e-06
Iter: 1533 loss: 6.69355768e-06
Iter: 1534 loss: 6.68973462e-06
Iter: 1535 loss: 6.68438724e-06
Iter: 1536 loss: 6.68432676e-06
Iter: 1537 loss: 6.67716904e-06
Iter: 1538 loss: 6.72838723e-06
Iter: 1539 loss: 6.67691893e-06
Iter: 1540 loss: 6.67572567e-06
Iter: 1541 loss: 6.67435961e-06
Iter: 1542 loss: 6.67149379e-06
Iter: 1543 loss: 6.66848064e-06
Iter: 1544 loss: 6.66786855e-06
Iter: 1545 loss: 6.66407232e-06
Iter: 1546 loss: 6.68179928e-06
Iter: 1547 loss: 6.66323831e-06
Iter: 1548 loss: 6.66428286e-06
Iter: 1549 loss: 6.6618536e-06
Iter: 1550 loss: 6.66116921e-06
Iter: 1551 loss: 6.65899097e-06
Iter: 1552 loss: 6.69439214e-06
Iter: 1553 loss: 6.65895413e-06
Iter: 1554 loss: 6.65648713e-06
Iter: 1555 loss: 6.65430343e-06
Iter: 1556 loss: 6.65376501e-06
Iter: 1557 loss: 6.6484622e-06
Iter: 1558 loss: 6.65759399e-06
Iter: 1559 loss: 6.6460334e-06
Iter: 1560 loss: 6.63947321e-06
Iter: 1561 loss: 6.63228457e-06
Iter: 1562 loss: 6.63132414e-06
Iter: 1563 loss: 6.62268212e-06
Iter: 1564 loss: 6.67589575e-06
Iter: 1565 loss: 6.62171169e-06
Iter: 1566 loss: 6.61558852e-06
Iter: 1567 loss: 6.61900503e-06
Iter: 1568 loss: 6.61163813e-06
Iter: 1569 loss: 6.60478054e-06
Iter: 1570 loss: 6.69096653e-06
Iter: 1571 loss: 6.60492697e-06
Iter: 1572 loss: 6.6004377e-06
Iter: 1573 loss: 6.59761963e-06
Iter: 1574 loss: 6.59570105e-06
Iter: 1575 loss: 6.5975928e-06
Iter: 1576 loss: 6.59256875e-06
Iter: 1577 loss: 6.58942645e-06
Iter: 1578 loss: 6.58576892e-06
Iter: 1579 loss: 6.58577346e-06
Iter: 1580 loss: 6.58093541e-06
Iter: 1581 loss: 6.58804402e-06
Iter: 1582 loss: 6.57857527e-06
Iter: 1583 loss: 6.58449972e-06
Iter: 1584 loss: 6.57739201e-06
Iter: 1585 loss: 6.57663304e-06
Iter: 1586 loss: 6.5748095e-06
Iter: 1587 loss: 6.58692261e-06
Iter: 1588 loss: 6.57442433e-06
Iter: 1589 loss: 6.56922748e-06
Iter: 1590 loss: 6.57397823e-06
Iter: 1591 loss: 6.56612247e-06
Iter: 1592 loss: 6.56260318e-06
Iter: 1593 loss: 6.5618151e-06
Iter: 1594 loss: 6.55712029e-06
Iter: 1595 loss: 6.55215536e-06
Iter: 1596 loss: 6.55135045e-06
Iter: 1597 loss: 6.54276437e-06
Iter: 1598 loss: 6.54808173e-06
Iter: 1599 loss: 6.53741e-06
Iter: 1600 loss: 6.5314307e-06
Iter: 1601 loss: 6.56313659e-06
Iter: 1602 loss: 6.53046845e-06
Iter: 1603 loss: 6.52489553e-06
Iter: 1604 loss: 6.5222257e-06
Iter: 1605 loss: 6.51937125e-06
Iter: 1606 loss: 6.51009395e-06
Iter: 1607 loss: 6.56515385e-06
Iter: 1608 loss: 6.50839547e-06
Iter: 1609 loss: 6.50211314e-06
Iter: 1610 loss: 6.58898e-06
Iter: 1611 loss: 6.50214679e-06
Iter: 1612 loss: 6.49714e-06
Iter: 1613 loss: 6.49711274e-06
Iter: 1614 loss: 6.49609456e-06
Iter: 1615 loss: 6.4969945e-06
Iter: 1616 loss: 6.49534104e-06
Iter: 1617 loss: 6.49277354e-06
Iter: 1618 loss: 6.48739842e-06
Iter: 1619 loss: 6.59539683e-06
Iter: 1620 loss: 6.48750392e-06
Iter: 1621 loss: 6.48426067e-06
Iter: 1622 loss: 6.47769593e-06
Iter: 1623 loss: 6.57537657e-06
Iter: 1624 loss: 6.47734805e-06
Iter: 1625 loss: 6.47241177e-06
Iter: 1626 loss: 6.47258412e-06
Iter: 1627 loss: 6.46856734e-06
Iter: 1628 loss: 6.47334218e-06
Iter: 1629 loss: 6.466782e-06
Iter: 1630 loss: 6.46230819e-06
Iter: 1631 loss: 6.48843616e-06
Iter: 1632 loss: 6.46179706e-06
Iter: 1633 loss: 6.45843875e-06
Iter: 1634 loss: 6.454502e-06
Iter: 1635 loss: 6.45383398e-06
Iter: 1636 loss: 6.44933152e-06
Iter: 1637 loss: 6.47391789e-06
Iter: 1638 loss: 6.44869169e-06
Iter: 1639 loss: 6.44537886e-06
Iter: 1640 loss: 6.45120144e-06
Iter: 1641 loss: 6.44360443e-06
Iter: 1642 loss: 6.43965e-06
Iter: 1643 loss: 6.44640204e-06
Iter: 1644 loss: 6.43791282e-06
Iter: 1645 loss: 6.44480861e-06
Iter: 1646 loss: 6.43676049e-06
Iter: 1647 loss: 6.43592648e-06
Iter: 1648 loss: 6.43977546e-06
Iter: 1649 loss: 6.4355936e-06
Iter: 1650 loss: 6.43467047e-06
Iter: 1651 loss: 6.43310341e-06
Iter: 1652 loss: 6.43303929e-06
Iter: 1653 loss: 6.4313e-06
Iter: 1654 loss: 6.42747091e-06
Iter: 1655 loss: 6.48097284e-06
Iter: 1656 loss: 6.42736131e-06
Iter: 1657 loss: 6.42422128e-06
Iter: 1658 loss: 6.43967178e-06
Iter: 1659 loss: 6.4238784e-06
Iter: 1660 loss: 6.42112946e-06
Iter: 1661 loss: 6.42425857e-06
Iter: 1662 loss: 6.41963e-06
Iter: 1663 loss: 6.41535553e-06
Iter: 1664 loss: 6.4356409e-06
Iter: 1665 loss: 6.41455199e-06
Iter: 1666 loss: 6.41141833e-06
Iter: 1667 loss: 6.4212345e-06
Iter: 1668 loss: 6.41044699e-06
Iter: 1669 loss: 6.4066794e-06
Iter: 1670 loss: 6.40544386e-06
Iter: 1671 loss: 6.40338567e-06
Iter: 1672 loss: 6.39659174e-06
Iter: 1673 loss: 6.39458085e-06
Iter: 1674 loss: 6.39073232e-06
Iter: 1675 loss: 6.38198708e-06
Iter: 1676 loss: 6.41851966e-06
Iter: 1677 loss: 6.38006259e-06
Iter: 1678 loss: 6.37345693e-06
Iter: 1679 loss: 6.39421887e-06
Iter: 1680 loss: 6.37169705e-06
Iter: 1681 loss: 6.36431287e-06
Iter: 1682 loss: 6.37001131e-06
Iter: 1683 loss: 6.36012192e-06
Iter: 1684 loss: 6.42237319e-06
Iter: 1685 loss: 6.35911147e-06
Iter: 1686 loss: 6.3586358e-06
Iter: 1687 loss: 6.35688957e-06
Iter: 1688 loss: 6.35731249e-06
Iter: 1689 loss: 6.35500146e-06
Iter: 1690 loss: 6.35259357e-06
Iter: 1691 loss: 6.35212382e-06
Iter: 1692 loss: 6.35072092e-06
Iter: 1693 loss: 6.34577555e-06
Iter: 1694 loss: 6.33784475e-06
Iter: 1695 loss: 6.33781292e-06
Iter: 1696 loss: 6.33379568e-06
Iter: 1697 loss: 6.33297805e-06
Iter: 1698 loss: 6.32857154e-06
Iter: 1699 loss: 6.33971467e-06
Iter: 1700 loss: 6.32679075e-06
Iter: 1701 loss: 6.32313504e-06
Iter: 1702 loss: 6.32163665e-06
Iter: 1703 loss: 6.31982493e-06
Iter: 1704 loss: 6.3148459e-06
Iter: 1705 loss: 6.32165211e-06
Iter: 1706 loss: 6.31219336e-06
Iter: 1707 loss: 6.30829572e-06
Iter: 1708 loss: 6.32082538e-06
Iter: 1709 loss: 6.30679278e-06
Iter: 1710 loss: 6.30218801e-06
Iter: 1711 loss: 6.29406e-06
Iter: 1712 loss: 6.29431088e-06
Iter: 1713 loss: 6.28367e-06
Iter: 1714 loss: 6.35311244e-06
Iter: 1715 loss: 6.28258294e-06
Iter: 1716 loss: 6.27692816e-06
Iter: 1717 loss: 6.32596357e-06
Iter: 1718 loss: 6.27674581e-06
Iter: 1719 loss: 6.28846101e-06
Iter: 1720 loss: 6.27597728e-06
Iter: 1721 loss: 6.2754325e-06
Iter: 1722 loss: 6.27363261e-06
Iter: 1723 loss: 6.26763449e-06
Iter: 1724 loss: 6.31845796e-06
Iter: 1725 loss: 6.26667224e-06
Iter: 1726 loss: 6.25973144e-06
Iter: 1727 loss: 6.28065573e-06
Iter: 1728 loss: 6.2578506e-06
Iter: 1729 loss: 6.25283974e-06
Iter: 1730 loss: 6.28079533e-06
Iter: 1731 loss: 6.25186431e-06
Iter: 1732 loss: 6.24857967e-06
Iter: 1733 loss: 6.271247e-06
Iter: 1734 loss: 6.24803215e-06
Iter: 1735 loss: 6.24350923e-06
Iter: 1736 loss: 6.24022323e-06
Iter: 1737 loss: 6.23853703e-06
Iter: 1738 loss: 6.2326917e-06
Iter: 1739 loss: 6.27315421e-06
Iter: 1740 loss: 6.23203732e-06
Iter: 1741 loss: 6.22877633e-06
Iter: 1742 loss: 6.22073458e-06
Iter: 1743 loss: 6.32011916e-06
Iter: 1744 loss: 6.22007337e-06
Iter: 1745 loss: 6.21296658e-06
Iter: 1746 loss: 6.22970038e-06
Iter: 1747 loss: 6.21043682e-06
Iter: 1748 loss: 6.20624e-06
Iter: 1749 loss: 6.20411174e-06
Iter: 1750 loss: 6.20209539e-06
Iter: 1751 loss: 6.19684806e-06
Iter: 1752 loss: 6.19829052e-06
Iter: 1753 loss: 6.1929295e-06
Iter: 1754 loss: 6.2022882e-06
Iter: 1755 loss: 6.19201865e-06
Iter: 1756 loss: 6.1915307e-06
Iter: 1757 loss: 6.18965896e-06
Iter: 1758 loss: 6.1863484e-06
Iter: 1759 loss: 6.18621289e-06
Iter: 1760 loss: 6.18249169e-06
Iter: 1761 loss: 6.18861e-06
Iter: 1762 loss: 6.1808455e-06
Iter: 1763 loss: 6.17628666e-06
Iter: 1764 loss: 6.18902777e-06
Iter: 1765 loss: 6.17518526e-06
Iter: 1766 loss: 6.17054275e-06
Iter: 1767 loss: 6.20560195e-06
Iter: 1768 loss: 6.17020078e-06
Iter: 1769 loss: 6.16515536e-06
Iter: 1770 loss: 6.19732054e-06
Iter: 1771 loss: 6.16481157e-06
Iter: 1772 loss: 6.16183388e-06
Iter: 1773 loss: 6.16525267e-06
Iter: 1774 loss: 6.15999852e-06
Iter: 1775 loss: 6.15645286e-06
Iter: 1776 loss: 6.15225144e-06
Iter: 1777 loss: 6.15160934e-06
Iter: 1778 loss: 6.14577857e-06
Iter: 1779 loss: 6.18568038e-06
Iter: 1780 loss: 6.14554847e-06
Iter: 1781 loss: 6.14180408e-06
Iter: 1782 loss: 6.14410874e-06
Iter: 1783 loss: 6.13933798e-06
Iter: 1784 loss: 6.13384145e-06
Iter: 1785 loss: 6.15579e-06
Iter: 1786 loss: 6.13263092e-06
Iter: 1787 loss: 6.142247e-06
Iter: 1788 loss: 6.1321939e-06
Iter: 1789 loss: 6.13123666e-06
Iter: 1790 loss: 6.12948497e-06
Iter: 1791 loss: 6.14991677e-06
Iter: 1792 loss: 6.12915119e-06
Iter: 1793 loss: 6.12810709e-06
Iter: 1794 loss: 6.1251003e-06
Iter: 1795 loss: 6.14258624e-06
Iter: 1796 loss: 6.12434e-06
Iter: 1797 loss: 6.11910082e-06
Iter: 1798 loss: 6.12836902e-06
Iter: 1799 loss: 6.11667656e-06
Iter: 1800 loss: 6.11226915e-06
Iter: 1801 loss: 6.17357364e-06
Iter: 1802 loss: 6.11234964e-06
Iter: 1803 loss: 6.10921325e-06
Iter: 1804 loss: 6.12176518e-06
Iter: 1805 loss: 6.1087344e-06
Iter: 1806 loss: 6.10552024e-06
Iter: 1807 loss: 6.11346923e-06
Iter: 1808 loss: 6.10401275e-06
Iter: 1809 loss: 6.10017378e-06
Iter: 1810 loss: 6.10061579e-06
Iter: 1811 loss: 6.09707331e-06
Iter: 1812 loss: 6.09166682e-06
Iter: 1813 loss: 6.0924267e-06
Iter: 1814 loss: 6.08759956e-06
Iter: 1815 loss: 6.08184382e-06
Iter: 1816 loss: 6.09336757e-06
Iter: 1817 loss: 6.07969287e-06
Iter: 1818 loss: 6.07314178e-06
Iter: 1819 loss: 6.07876336e-06
Iter: 1820 loss: 6.06891081e-06
Iter: 1821 loss: 6.0598968e-06
Iter: 1822 loss: 6.10776806e-06
Iter: 1823 loss: 6.0584448e-06
Iter: 1824 loss: 6.05774312e-06
Iter: 1825 loss: 6.05342893e-06
Iter: 1826 loss: 6.05255946e-06
Iter: 1827 loss: 6.04960769e-06
Iter: 1828 loss: 6.050519e-06
Iter: 1829 loss: 6.04647175e-06
Iter: 1830 loss: 6.04220622e-06
Iter: 1831 loss: 6.04338538e-06
Iter: 1832 loss: 6.03912031e-06
Iter: 1833 loss: 6.03214266e-06
Iter: 1834 loss: 6.05026617e-06
Iter: 1835 loss: 6.02955333e-06
Iter: 1836 loss: 6.02426e-06
Iter: 1837 loss: 6.024291e-06
Iter: 1838 loss: 6.02136424e-06
Iter: 1839 loss: 6.02689215e-06
Iter: 1840 loss: 6.02049568e-06
Iter: 1841 loss: 6.01613647e-06
Iter: 1842 loss: 6.01571219e-06
Iter: 1843 loss: 6.01283318e-06
Iter: 1844 loss: 6.00643443e-06
Iter: 1845 loss: 6.02017826e-06
Iter: 1846 loss: 6.00412113e-06
Iter: 1847 loss: 5.9993763e-06
Iter: 1848 loss: 5.99615214e-06
Iter: 1849 loss: 5.9944723e-06
Iter: 1850 loss: 5.98779025e-06
Iter: 1851 loss: 5.99672876e-06
Iter: 1852 loss: 5.98460974e-06
Iter: 1853 loss: 5.98061e-06
Iter: 1854 loss: 5.98030056e-06
Iter: 1855 loss: 5.98093266e-06
Iter: 1856 loss: 5.97909275e-06
Iter: 1857 loss: 5.97841063e-06
Iter: 1858 loss: 5.97593089e-06
Iter: 1859 loss: 5.97087183e-06
Iter: 1860 loss: 5.97102371e-06
Iter: 1861 loss: 5.96385e-06
Iter: 1862 loss: 5.9754525e-06
Iter: 1863 loss: 5.96090285e-06
Iter: 1864 loss: 5.95320944e-06
Iter: 1865 loss: 5.96672362e-06
Iter: 1866 loss: 5.94999847e-06
Iter: 1867 loss: 5.9443264e-06
Iter: 1868 loss: 6.0075e-06
Iter: 1869 loss: 5.94426729e-06
Iter: 1870 loss: 5.94077392e-06
Iter: 1871 loss: 5.96053542e-06
Iter: 1872 loss: 5.9404224e-06
Iter: 1873 loss: 5.93697723e-06
Iter: 1874 loss: 5.94932135e-06
Iter: 1875 loss: 5.93615596e-06
Iter: 1876 loss: 5.93295135e-06
Iter: 1877 loss: 5.93554751e-06
Iter: 1878 loss: 5.93078812e-06
Iter: 1879 loss: 5.92787228e-06
Iter: 1880 loss: 5.93585628e-06
Iter: 1881 loss: 5.92683818e-06
Iter: 1882 loss: 5.92403194e-06
Iter: 1883 loss: 5.92829338e-06
Iter: 1884 loss: 5.92285232e-06
Iter: 1885 loss: 5.92026799e-06
Iter: 1886 loss: 5.91766911e-06
Iter: 1887 loss: 5.91695152e-06
Iter: 1888 loss: 5.91692469e-06
Iter: 1889 loss: 5.91531898e-06
Iter: 1890 loss: 5.91287244e-06
Iter: 1891 loss: 5.90951595e-06
Iter: 1892 loss: 5.90935042e-06
Iter: 1893 loss: 5.90698301e-06
Iter: 1894 loss: 5.90051877e-06
Iter: 1895 loss: 5.93964251e-06
Iter: 1896 loss: 5.89862111e-06
Iter: 1897 loss: 5.89284764e-06
Iter: 1898 loss: 5.93853e-06
Iter: 1899 loss: 5.89233287e-06
Iter: 1900 loss: 5.8877913e-06
Iter: 1901 loss: 5.89118508e-06
Iter: 1902 loss: 5.88487956e-06
Iter: 1903 loss: 5.88112243e-06
Iter: 1904 loss: 5.89268711e-06
Iter: 1905 loss: 5.87991826e-06
Iter: 1906 loss: 5.87671047e-06
Iter: 1907 loss: 5.89544561e-06
Iter: 1908 loss: 5.87626255e-06
Iter: 1909 loss: 5.87388786e-06
Iter: 1910 loss: 5.87974318e-06
Iter: 1911 loss: 5.87312024e-06
Iter: 1912 loss: 5.86995247e-06
Iter: 1913 loss: 5.87648037e-06
Iter: 1914 loss: 5.86883107e-06
Iter: 1915 loss: 5.86481701e-06
Iter: 1916 loss: 5.87539535e-06
Iter: 1917 loss: 5.86353053e-06
Iter: 1918 loss: 5.85937732e-06
Iter: 1919 loss: 5.8553178e-06
Iter: 1920 loss: 5.85439375e-06
Iter: 1921 loss: 5.85155203e-06
Iter: 1922 loss: 5.85129692e-06
Iter: 1923 loss: 5.84752388e-06
Iter: 1924 loss: 5.86741908e-06
Iter: 1925 loss: 5.84698228e-06
Iter: 1926 loss: 5.84605323e-06
Iter: 1927 loss: 5.8432679e-06
Iter: 1928 loss: 5.85118551e-06
Iter: 1929 loss: 5.84169538e-06
Iter: 1930 loss: 5.8380665e-06
Iter: 1931 loss: 5.83779774e-06
Iter: 1932 loss: 5.83500196e-06
Iter: 1933 loss: 5.8302162e-06
Iter: 1934 loss: 5.84843383e-06
Iter: 1935 loss: 5.82901066e-06
Iter: 1936 loss: 5.82443045e-06
Iter: 1937 loss: 5.84037844e-06
Iter: 1938 loss: 5.82326265e-06
Iter: 1939 loss: 5.81914765e-06
Iter: 1940 loss: 5.87973318e-06
Iter: 1941 loss: 5.81919358e-06
Iter: 1942 loss: 5.81678523e-06
Iter: 1943 loss: 5.81610493e-06
Iter: 1944 loss: 5.8148762e-06
Iter: 1945 loss: 5.81216864e-06
Iter: 1946 loss: 5.81766517e-06
Iter: 1947 loss: 5.81105e-06
Iter: 1948 loss: 5.80756296e-06
Iter: 1949 loss: 5.82367375e-06
Iter: 1950 loss: 5.80657206e-06
Iter: 1951 loss: 5.80347933e-06
Iter: 1952 loss: 5.81232189e-06
Iter: 1953 loss: 5.80252026e-06
Iter: 1954 loss: 5.80052e-06
Iter: 1955 loss: 5.79959669e-06
Iter: 1956 loss: 5.79841526e-06
Iter: 1957 loss: 5.79904463e-06
Iter: 1958 loss: 5.79731659e-06
Iter: 1959 loss: 5.79577e-06
Iter: 1960 loss: 5.79103971e-06
Iter: 1961 loss: 5.83861038e-06
Iter: 1962 loss: 5.79068683e-06
Iter: 1963 loss: 5.78877098e-06
Iter: 1964 loss: 5.78756408e-06
Iter: 1965 loss: 5.787e-06
Iter: 1966 loss: 5.78380377e-06
Iter: 1967 loss: 5.77940591e-06
Iter: 1968 loss: 5.77904166e-06
Iter: 1969 loss: 5.77105311e-06
Iter: 1970 loss: 5.79465723e-06
Iter: 1971 loss: 5.76826096e-06
Iter: 1972 loss: 5.76225102e-06
Iter: 1973 loss: 5.76987259e-06
Iter: 1974 loss: 5.75896638e-06
Iter: 1975 loss: 5.75248214e-06
Iter: 1976 loss: 5.77565879e-06
Iter: 1977 loss: 5.75070135e-06
Iter: 1978 loss: 5.74764135e-06
Iter: 1979 loss: 5.74681508e-06
Iter: 1980 loss: 5.74357e-06
Iter: 1981 loss: 5.73950547e-06
Iter: 1982 loss: 5.7393836e-06
Iter: 1983 loss: 5.73409216e-06
Iter: 1984 loss: 5.76122238e-06
Iter: 1985 loss: 5.73328452e-06
Iter: 1986 loss: 5.72947556e-06
Iter: 1987 loss: 5.73105035e-06
Iter: 1988 loss: 5.7269408e-06
Iter: 1989 loss: 5.72312229e-06
Iter: 1990 loss: 5.73684156e-06
Iter: 1991 loss: 5.72215185e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.8
+ date
Thu Oct 22 10:19:07 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/500_500_500_500_1 --function f1 --psi -2 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2de26e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2de0996a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2de0dd400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2de108a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2de105510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2de105730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2de043400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2ddfe9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2ddfe9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2ddf98620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2ddf23400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2ddf42d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2ddf2e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2ddf2ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2ddee1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2ddf228c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2ddef82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2ddea2e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2ddef8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2dde350d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2b8963510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2b896fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2dde64f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2b88b0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2b88b0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2b88f7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2b8931730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2b88637b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2b8863950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2b885b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2b885b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2940d5158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2940d5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd2940c8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd29410b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd280675488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 1.9922082
test_loss: 2.095222
train_loss: 1.9996083
test_loss: 2.1249278
train_loss: 1.998549
test_loss: 2.0000784
train_loss: 1.9992433
test_loss: 1.9992014
train_loss: 1.9987102
test_loss: 1.9979074
train_loss: 1.9996167
test_loss: 1.9978585
train_loss: 1.9962888
test_loss: 1.9978975
train_loss: 1.9987755
test_loss: 1.9982961
train_loss: 1.998129
test_loss: 1.9982358
train_loss: 1.9975471
test_loss: 1.9978542
train_loss: 1.9968389
test_loss: 1.9984118
train_loss: 1.998349
test_loss: 1.9983133
train_loss: 1.9974356
test_loss: 1.9980934
train_loss: 1.9951781
test_loss: 1.9983392
train_loss: 1.997444
test_loss: 1.9979105
train_loss: 1.9990507
test_loss: 1.9982399
train_loss: 1.9961945
test_loss: 1.9980495
train_loss: 1.9979767
test_loss: 1.9984084
train_loss: 1.9971819
test_loss: 1.9982231
train_loss: 1.9980572
test_loss: 1.9980924
train_loss: 1.997104
test_loss: 1.9980808
train_loss: 1.9974976
test_loss: 1.9982888
train_loss: 1.9960796
test_loss: 1.9981782
train_loss: 1.9973986
test_loss: 1.9982955
train_loss: 1.9967988
test_loss: 1.9982044
train_loss: 1.9986312
test_loss: 1.998074
train_loss: 1.997428
test_loss: 1.9983484
train_loss: 1.99629
test_loss: 1.9984076
train_loss: 1.9966221
test_loss: 1.9983363
train_loss: 1.9972644
test_loss: 1.9983664
train_loss: 1.9968505
test_loss: 1.9980787
train_loss: 1.9969008
test_loss: 1.9984719
train_loss: 1.9956774
test_loss: 1.9982642
train_loss: 1.9959005
test_loss: 1.998337
train_loss: 1.9962335
test_loss: 1.998437
train_loss: 1.9962221
test_loss: 1.9979463
train_loss: 1.9967833
test_loss: 1.9982882
train_loss: 1.9966216
test_loss: 1.9981544
train_loss: 1.9953322
test_loss: 1.9983218
train_loss: 1.9968315
test_loss: 1.9983122
train_loss: 1.9961553
test_loss: 1.9984533
train_loss: 1.9957612
test_loss: 1.998394
train_loss: 1.9963417
test_loss: 1.9982399
train_loss: 1.9962786
test_loss: 1.9982514
train_loss: 1.9961072
test_loss: 1.9981836
train_loss: 1.9963415
test_loss: 1.9984267
train_loss: 1.9966977
test_loss: 1.9984637
train_loss: 1.995748
test_loss: 1.9984242
train_loss: 1.9955531
test_loss: 1.998605
train_loss: 1.9960536
test_loss: 1.9983716
train_loss: 1.9973421
test_loss: 1.9984686
train_loss: 1.9960694
test_loss: 1.9981885
train_loss: 1.9960983
test_loss: 1.998054
train_loss: 1.9942478
test_loss: 1.9981904
train_loss: 1.9957695
test_loss: 1.9983678
train_loss: 1.9968597
test_loss: 1.9984055
train_loss: 1.996994
test_loss: 1.9984818
train_loss: 1.9964297
test_loss: 1.9986548
train_loss: 1.996809
test_loss: 1.9986552
train_loss: 1.996185
test_loss: 1.9981169
train_loss: 1.9961629
test_loss: 1.9982288
train_loss: 1.9958348
test_loss: 1.9984913
train_loss: 1.9966788
test_loss: 1.9986742
train_loss: 1.9952742
test_loss: 1.9979721
train_loss: 1.9932276
test_loss: 1.9980464
train_loss: 1.995472
test_loss: 1.9984835
train_loss: 1.996605
test_loss: 1.9980843
train_loss: 1.9951853
test_loss: 1.9981868
train_loss: 1.9958919
test_loss: 1.9983392
train_loss: 1.9949956
test_loss: 1.9979882
train_loss: 1.9959598
test_loss: 1.9983158
train_loss: 1.9954777
test_loss: 1.998418
train_loss: 1.9944768
test_loss: 1.9983433
train_loss: 1.9956088
test_loss: 1.9985272
train_loss: 1.9956763
test_loss: 1.9983149
train_loss: 1.9964157
test_loss: 1.9984988
train_loss: 1.99494
test_loss: 1.9984361
train_loss: 1.9969318
test_loss: 1.9983644
train_loss: 1.9955976
test_loss: 1.9984384
train_loss: 1.9960606
test_loss: 1.9983046
train_loss: 1.9959263
test_loss: 1.9982113
train_loss: 1.9944412
test_loss: 1.9980484
train_loss: 1.9943378
test_loss: 1.9986516
train_loss: 1.9991839
test_loss: 1.9983015
train_loss: 1.9958904
test_loss: 1.9985616
train_loss: 1.9970758
test_loss: 1.9984761
train_loss: 1.9950259
test_loss: 1.998351
train_loss: 1.9943637
test_loss: 1.9980274
train_loss: 1.9961119
test_loss: 1.9983481
train_loss: 1.9961812
test_loss: 1.9983504
train_loss: 1.9958258
test_loss: 1.9985619
train_loss: 1.9963636
test_loss: 1.9984312
train_loss: 1.9943935
test_loss: 1.9984546
train_loss: 1.9963696
test_loss: 1.9983743
train_loss: 1.9943275
test_loss: 1.9988134
train_loss: 1.9962217
test_loss: 1.998418
train_loss: 1.9948955
test_loss: 1.9985714
train_loss: 1.997385
test_loss: 1.9989992
train_loss: 1.9945719
test_loss: 1.9988881
train_loss: 1.9955302
test_loss: 1.998503
train_loss: 1.9958296
test_loss: 1.9983122
train_loss: 1.9937422
test_loss: 1.9985434
train_loss: 1.9957085
test_loss: 1.9984806
train_loss: 1.9951974
test_loss: 1.9983865
train_loss: 1.9950814
test_loss: 1.9983624
train_loss: 1.9954052
test_loss: 1.997878
train_loss: 1.9948323
test_loss: 1.9987012
train_loss: 1.9949405
test_loss: 1.9985396
train_loss: 1.995061
test_loss: 1.9985867
train_loss: 1.9939321
test_loss: 1.9986302
train_loss: 1.9955989
test_loss: 1.9983176
train_loss: 1.9958787
test_loss: 1.9983003
train_loss: 1.995084
test_loss: 1.9981312
train_loss: 1.9951472
test_loss: 1.9985172
train_loss: 1.9958997
test_loss: 1.9987842
train_loss: 1.9949924
test_loss: 1.9990003
train_loss: 1.9959025
test_loss: 1.9987079
train_loss: 1.9933392
test_loss: 1.9986883
train_loss: 1.993238
test_loss: 1.9985379
train_loss: 1.9944136
test_loss: 1.998466
train_loss: 1.9955037
test_loss: 1.9986326
train_loss: 1.9954967
test_loss: 1.9986326
train_loss: 1.9944267
test_loss: 1.9986112
train_loss: 1.9951692
test_loss: 1.9986765
train_loss: 1.9945974
test_loss: 1.9989277
train_loss: 1.995552
test_loss: 1.9988518
train_loss: 1.9953465
test_loss: 1.9984756
train_loss: 1.9948343
test_loss: 1.9981372
train_loss: 1.9955909
test_loss: 1.9986372
train_loss: 1.9943686
test_loss: 1.9985174
train_loss: 1.9952025
test_loss: 1.9985137
train_loss: 1.9954287
test_loss: 1.9985442
train_loss: 1.994252
test_loss: 1.9986485
train_loss: 1.9957196
test_loss: 1.9984968
train_loss: 1.9958506
test_loss: 1.9984671
train_loss: 1.9985191
test_loss: 1.9988085
train_loss: 1.9958813
test_loss: 1.9988384
train_loss: 1.9960047
test_loss: 1.9986776
train_loss: 1.9956925
test_loss: 1.9987724
train_loss: 1.9953871
test_loss: 1.9986827
train_loss: 1.9940672
test_loss: 1.9983981
train_loss: 1.9953969
test_loss: 1.9985378
train_loss: 1.9948065
test_loss: 1.9985723
train_loss: 1.9954531
test_loss: 1.9985461
train_loss: 1.9988141
test_loss: 1.9987913
train_loss: 1.9961957
test_loss: 1.9986024
train_loss: 1.9950945
test_loss: 1.9989239
train_loss: 1.9946275
test_loss: 1.9984914
train_loss: 1.9958785
test_loss: 1.9984637
train_loss: 1.9969238
test_loss: 1.9985437
train_loss: 1.9935122
test_loss: 1.998544
train_loss: 1.9955332
test_loss: 1.9985337
train_loss: 1.9937311
test_loss: 1.9984999
train_loss: 1.9945858
test_loss: 1.9982233
train_loss: 1.9940197
test_loss: 1.998739
train_loss: 1.993607
test_loss: 1.9986454
train_loss: 1.9934525
test_loss: 1.9985191
train_loss: 1.995439
test_loss: 1.9986796
train_loss: 1.9951439
test_loss: 1.9980589
train_loss: 1.9950521
test_loss: 1.9985042
train_loss: 1.9941137
test_loss: 1.9980743
train_loss: 1.9946706
test_loss: 1.9983135
train_loss: 1.9949784
test_loss: 1.998726
train_loss: 1.9947908
test_loss: 1.9983468
train_loss: 1.9935312
test_loss: 1.9989687
train_loss: 1.9937134
test_loss: 1.9985163
train_loss: 1.9947195
test_loss: 1.9985262
train_loss: 1.9944276
test_loss: 1.9984257
train_loss: 1.9946011
test_loss: 1.9986093
train_loss: 1.9951825
test_loss: 1.997913
train_loss: 1.9951183
test_loss: 1.9987009
train_loss: 1.9939909
test_loss: 1.9987973
train_loss: 1.9953253
test_loss: 1.9983022
train_loss: 1.9949492
test_loss: 1.9978547
train_loss: 1.9987069
test_loss: 1.9984436
train_loss: 1.9964495
test_loss: 1.9981027
train_loss: 1.999598
test_loss: 1.9985375
train_loss: 1.9958403
test_loss: 1.9980955
train_loss: 1.9972004
test_loss: 1.9984835
train_loss: 1.9988027
test_loss: 1.9983957
train_loss: 1.9998168
test_loss: 1.9982417
train_loss: 1.9975097
test_loss: 1.998163
train_loss: 1.9978098
test_loss: 1.9985795
train_loss: 1.9997075
test_loss: 1.9990774
train_loss: 1.9996965
test_loss: 1.998627
train_loss: 1.998671
test_loss: 1.9988614
train_loss: 1.9991881
test_loss: 1.998747
train_loss: 1.999337
test_loss: 1.9986026
train_loss: 1.9969075
test_loss: 1.9981748
train_loss: 1.996659
test_loss: 1.9981154
train_loss: 1.9990975
test_loss: 1.9986361
train_loss: 1.99955
test_loss: 1.9989668
train_loss: 1.998733
test_loss: 1.9981945
train_loss: 1.9964347
test_loss: 1.998856
train_loss: 1.999668
test_loss: 1.9983168
train_loss: 1.9987619
test_loss: 1.9987358
train_loss: 1.9994037
test_loss: 1.9984533
train_loss: 1.9969975
test_loss: 1.9982579
train_loss: 1.9986186
test_loss: 1.9986811
train_loss: 1.9993875
test_loss: 1.9987392
train_loss: 1.9968832
test_loss: 1.9986972
train_loss: 1.9966751
test_loss: 1.998578
train_loss: 1.998719
test_loss: 1.9992163
train_loss: 1.9991672
test_loss: 1.9986466
train_loss: 1.999825
test_loss: 1.9985462
train_loss: 1.9968187
test_loss: 1.9985805
train_loss: 1.9975154
test_loss: 1.9990935
train_loss: 1.9992805
test_loss: 1.999016
train_loss: 1.9994224
test_loss: 1.9982884
train_loss: 1.9983529
test_loss: 1.9985071
train_loss: 1.9982425
test_loss: 1.9989527
train_loss: 1.9975582
test_loss: 1.9985882
train_loss: 1.9970586
test_loss: 1.9985398
train_loss: 1.9998598
test_loss: 1.9988226
train_loss: 1.9970464
test_loss: 1.9984967
train_loss: 1.9973836
test_loss: 1.9988894
train_loss: 1.9988526
test_loss: 1.9983987
train_loss: 1.9997572
test_loss: 1.9990441
train_loss: 1.9989765
test_loss: 1.998878
train_loss: 1.9973621
test_loss: 1.9985623
train_loss: 1.9993668
test_loss: 1.998705
train_loss: 1.9956638
test_loss: 1.9989309
train_loss: 1.9988908
test_loss: 1.9983321
train_loss: 1.9994907
test_loss: 1.9984235
train_loss: 1.9998806
test_loss: 1.9984587
train_loss: 1.9971426
test_loss: 1.9990894
train_loss: 1.9983541
test_loss: 1.9989195
train_loss: 1.9962422
test_loss: 1.9987022
train_loss: 1.9978088
test_loss: 1.9982812
train_loss: 1.999481
test_loss: 1.9985644
train_loss: 1.9990795
test_loss: 1.9986966
train_loss: 1.9990073
test_loss: 1.9983333
train_loss: 1.997863
test_loss: 1.9985994
train_loss: 1.9978802
test_loss: 1.9981884
train_loss: 1.9998071
test_loss: 1.9984245
train_loss: 1.9991428
test_loss: 1.9985617
train_loss: 1.999007
test_loss: 1.998907
train_loss: 1.9966569
test_loss: 1.9985098
train_loss: 1.9995039
test_loss: 1.9985853
train_loss: 1.997802
test_loss: 1.9985161
train_loss: 1.9976773
test_loss: 1.9988532
train_loss: 1.9991739
test_loss: 1.9986968
train_loss: 1.9962897
test_loss: 1.9985564
train_loss: 1.999208
test_loss: 1.9987292
train_loss: 1.9996725
test_loss: 1.9981949
train_loss: 1.9972228
test_loss: 1.9986349
train_loss: 1.9971945
test_loss: 1.9986764
train_loss: 1.9997369
test_loss: 1.9987335
train_loss: 1.9971678
test_loss: 1.9986942
train_loss: 1.9979818
test_loss: 1.9986509
train_loss: 1.9997597
test_loss: 1.9989785
train_loss: 1.9990988
test_loss: 1.9987633
train_loss: 1.9993167
test_loss: 1.9984267
train_loss: 1.9991608
test_loss: 1.9986748
train_loss: 1.9989829
test_loss: 1.9988052
train_loss: 1.9994378
test_loss: 1.9991616
train_loss: 1.9973719
test_loss: 1.998663
train_loss: 1.9988706
test_loss: 1.9988147
train_loss: 1.9985448
test_loss: 1.9984022
train_loss: 1.9984906
test_loss: 1.998036
train_loss: 1.99861
test_loss: 1.998208
train_loss: 1.9996918
test_loss: 1.998926
train_loss: 1.9973379
test_loss: 1.9984325
train_loss: 1.9983658
test_loss: 1.9990436
train_loss: 1.9986216
test_loss: 1.9984642
train_loss: 1.9952072
test_loss: 1.998476
train_loss: 1.9977344
test_loss: 1.998277
train_loss: 1.9977698
test_loss: 1.9981163
train_loss: 1.9979699
test_loss: 1.9992248
train_loss: 1.9973968
test_loss: 1.9980457
train_loss: 1.9992766
test_loss: 1.9986526
train_loss: 1.9980531
test_loss: 1.9985917
train_loss: 1.9992415
test_loss: 1.998816
train_loss: 1.9998682
test_loss: 1.9987755
train_loss: 1.999727
test_loss: 1.9991723
train_loss: 1.9993007
test_loss: 1.9986603
train_loss: 1.9961958
test_loss: 1.9987894
train_loss: 1.9989841
test_loss: 1.998846
train_loss: 1.9985888
test_loss: 1.9988427
train_loss: 1.9980259
test_loss: 1.9990085
train_loss: 1.9970458
test_loss: 1.9988031
train_loss: 1.9972695
test_loss: 1.9991922
train_loss: 1.9987718
test_loss: 2.000555
train_loss: 1.9992661
test_loss: 1.9987875
train_loss: 1.99737
test_loss: 1.9985478
train_loss: 1.9986266
test_loss: 1.9992918
train_loss: 1.9986713
test_loss: 1.9998233
train_loss: 1.9994824
test_loss: 1.9990445
train_loss: 1.9998373
test_loss: 1.9986031
train_loss: 1.99884
test_loss: 1.9987563
train_loss: 1.9978776
test_loss: 1.9985174
train_loss: 1.9981289
test_loss: 1.9991509
train_loss: 1.9990453
test_loss: 1.9985899
train_loss: 1.9998028
test_loss: 1.9989185
train_loss: 1.9975867
test_loss: 1.9995177
train_loss: 1.9995608
test_loss: 1.9990411
train_loss: 1.998555
test_loss: 1.9991082
train_loss: 1.9992623
test_loss: 1.9994678
train_loss: 1.9995733
test_loss: 1.9989505
train_loss: 1.9974635
test_loss: 1.999795
train_loss: 1.9994833
test_loss: 1.9987108
train_loss: 1.9982187
test_loss: 1.9990549
train_loss: 1.9988322
test_loss: 2.0009172
train_loss: 1.9996557
test_loss: 2.009856
train_loss: 2.0002995
test_loss: 2.001401
train_loss: 1.999247
test_loss: 2.003395
train_loss: 1.9987259
test_loss: 2.0092952
train_loss: 1.9983983
test_loss: 1.9998258
train_loss: 1.9999075
test_loss: 1.9991322
train_loss: 1.9999037
test_loss: 1.9990216
train_loss: 1.9987392
test_loss: 2.0068088
train_loss: 1.99915
test_loss: 2.0053697
train_loss: 1.9990687
test_loss: 2.0052576
train_loss: 2.0397696
test_loss: 2.0001855
train_loss: 1.9995773
test_loss: 1.998863
train_loss: 1.9990813
test_loss: 1.9990385
train_loss: 1.9974935
test_loss: 1.9989502
train_loss: 1.9983275
test_loss: 1.9989526
train_loss: 2.0407093
test_loss: 1.9986112
train_loss: 1.9962628
test_loss: 1.9986129
train_loss: 1.9996469
test_loss: 1.9988908
train_loss: 1.9995072
test_loss: 1.9983588
train_loss: 1.9976158
test_loss: 1.9986339
train_loss: 1.9993179
test_loss: 1.9988887
train_loss: 1.999324
test_loss: 1.9987981
train_loss: 1.9977818
test_loss: 1.9987217
train_loss: 1.9977255
test_loss: 1.9986746
train_loss: 1.998915
test_loss: 1.9985754
train_loss: 1.996738
test_loss: 1.9984345
train_loss: 1.999038
test_loss: 1.9984875
train_loss: 1.9963131
test_loss: 1.9984114
train_loss: 1.9970982
test_loss: 1.9979885
train_loss: 1.9943551
test_loss: 1.9981489
train_loss: 1.9973036
test_loss: 1.9987801
train_loss: 1.9994035
test_loss: 1.998827
train_loss: 1.9969895
test_loss: 1.9984055
train_loss: 1.9983244
test_loss: 1.9988177
train_loss: 1.9990368
test_loss: 1.9984459
train_loss: 1.9971423
test_loss: 1.9983218
train_loss: 1.9996986
test_loss: 1.9984764
train_loss: 1.9968407
test_loss: 1.99901
train_loss: 1.9997609
test_loss: 1.9986055
train_loss: 1.9989958
test_loss: 1.9986602
train_loss: 1.9982964
test_loss: 1.9987355
train_loss: 1.9970261
test_loss: 1.9987239
train_loss: 1.9974
test_loss: 1.9988331
train_loss: 1.9996833
test_loss: 1.9990947
train_loss: 1.9986215
test_loss: 1.998066
train_loss: 1.999894
test_loss: 1.9983084
train_loss: 1.9963405
test_loss: 1.9984633
train_loss: 1.9972974
test_loss: 1.9983352
train_loss: 1.9973556
test_loss: 1.9987774
train_loss: 1.9989988
test_loss: 1.9986836
train_loss: 1.9967372
test_loss: 1.9987231
train_loss: 1.9996471
test_loss: 1.9989194
train_loss: 1.9991295
test_loss: 1.9988275
train_loss: 1.9997528
test_loss: 1.9988157
train_loss: 1.9990764
test_loss: 1.9987377
train_loss: 1.9981086
test_loss: 1.9989088
train_loss: 1.998087
test_loss: 1.9989604
train_loss: 1.999021
test_loss: 1.9993266
train_loss: 1.9990294
test_loss: 1.9986178
train_loss: 1.9988465
test_loss: 1.9987319
train_loss: 1.9987252
test_loss: 1.9985142
train_loss: 1.9991313
test_loss: 1.9989403
train_loss: 1.9989052
test_loss: 1.9996814
train_loss: 1.9990194
test_loss: 1.9989247
train_loss: 1.9991331
test_loss: 1.9997567
train_loss: 1.9983367
test_loss: 1.9988683
train_loss: 1.9995784
test_loss: 2.000604
train_loss: 1.9984258
test_loss: 1.9997034
train_loss: 1.9975917
test_loss: 1.998887
train_loss: 1.9990308
test_loss: 1.9989437
train_loss: 1.998541
test_loss: 1.998845
train_loss: 1.9974272
test_loss: 1.9994347
train_loss: 1.9998492
test_loss: 1.9992014
train_loss: 1.9987345
test_loss: 1.9988108
train_loss: 1.9992107
test_loss: 2.0020022
train_loss: 1.9997894
test_loss: 1.9992995
train_loss: 1.996876
test_loss: 1.9998008
train_loss: 1.9991889
test_loss: 1.9988714
train_loss: 1.9992645
test_loss: 1.998561
train_loss: 1.9986145
test_loss: 2.0001473
train_loss: 1.9976475
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 1.9989522
train_loss: 1.9995623
test_loss: 1.9985899
train_loss: 1.9987438
test_loss: 1.9984851
train_loss: 1.9983879
test_loss: 1.9992237
train_loss: 1.9970849
test_loss: 1.9987159
train_loss: 1.9996538
test_loss: 1.9981822
train_loss: 1.9994872
test_loss: 1.9985806
train_loss: 1.9990754
test_loss: 1.9995962
train_loss: 1.998605
test_loss: 1.9989282
train_loss: 1.9978583
test_loss: 1.9994539
train_loss: 1.9995421
test_loss: 2.0001988
train_loss: 1.9983324
test_loss: 1.9990638
train_loss: 1.998946
test_loss: 1.9996778
train_loss: 1.9988496
test_loss: 1.9989015
train_loss: 1.9992194
test_loss: 1.9988942
train_loss: 1.9998832
test_loss: 2.0001996
train_loss: 1.99795
test_loss: 1.9985348
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.8/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5dfcd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5cb8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5cb8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5d5c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5c3f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5c3fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5c33620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5bea598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5be7048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5ba1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5b52c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5b74048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5b5ee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5b5e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5acabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5acaf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5afb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5aa76a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5a726a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f60a5a78f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6099051598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6099084ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098fd56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098ff7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098ff7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098fb0bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098fb0a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098f89730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098f892f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098f30598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098ee0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098e98158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098e980d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098ebc400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098e5d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6098e5dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 26192.7891
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Traceback (most recent call last):
  File "biholoNN_train.py", line 169, in <module>
    results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=train_func, initial_position=init_params, max_iterations=max_epochs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 287, in minimize
    parallel_iterations=parallel_iterations)[0]
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 574, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2499, in while_loop_v2
    return_same_structure=True)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2735, in while_loop
    loop_vars = body(*loop_vars)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 260, in _body
    max_line_search_iterations)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 156, in line_search_step
    max_iterations=max_iterations)  # No search needed for these.
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 259, in hager_zhang
    threshold_use_approximate_wolfe_condition)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 609, in _prepare_args
    val_initial = value_and_gradients_function(initial_step_size)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 251, in _restricted_func
    objective_value, gradient = value_and_gradients_function(pt)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 814, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
	 [[Sum_1/_34]]
  (1) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_f_16876]

Function call stack:
f -> f

++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi3
+ date
Thu Oct 22 12:17:33 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/500_500_500_500_1 --function f1 --psi -2 --phi 3 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b0ad0400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b0b46730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b0a60a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b0a7eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b0aa78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b0aa7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b094fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b09e69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b09e6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b09b1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b09aca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b0893950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b0893f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b08d96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b0893bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b0910488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b0910a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b0814ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a25d77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a2605f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b07b38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2b07b3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a253c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a2583840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a2561598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a2561ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a24e79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a25088c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a2508950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a2508840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a25ae400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a2424400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a2424d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a2446ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a2484a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a23d7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 1.9969447
test_loss: 1.9980792
train_loss: 1.9982718
test_loss: 1.9982721
train_loss: 1.9998742
test_loss: 1.9986132
train_loss: 2.2234871
test_loss: 2.0349953
train_loss: 2.0366707
test_loss: 2.0092938
train_loss: 1.9993509
test_loss: 2.0093853
train_loss: 1.9988513
test_loss: 2.0002656
train_loss: 1.9990535
test_loss: 1.9999093
train_loss: 1.9971597
test_loss: 1.9997746
train_loss: 1.9984366
test_loss: 1.9996681
train_loss: 2.0045905
test_loss: 2.0005565
train_loss: 1.9999129
test_loss: 2.0013752
train_loss: 1.9977587
test_loss: 2.0079277
train_loss: 1.9981397
test_loss: 2.002102
train_loss: 1.9998673
test_loss: 1.9990836
train_loss: 1.9998207
test_loss: 1.9989476
train_loss: 1.9978614
test_loss: 1.999409
train_loss: 1.998954
test_loss: 1.9991326
train_loss: 1.9976636
test_loss: 1.9987731
train_loss: 1.9971083
test_loss: 1.9998342
train_loss: 1.9990878
test_loss: 1.999029
train_loss: 1.9997722
test_loss: 2.0003574
train_loss: 2.0003695
test_loss: 1.9994916
train_loss: 1.9996834
test_loss: 2.0027878
train_loss: 1.999022
test_loss: 1.9991517
train_loss: 2.0153599
test_loss: 1.9995009
train_loss: 1.9972748
test_loss: 2.0170374
train_loss: 1.9978975
test_loss: 2.00225
train_loss: 1.9993479
test_loss: 2.0104206
train_loss: 1.9979361
test_loss: 1.9988465
train_loss: 1.9991077
test_loss: 1.9982206
train_loss: 1.9984297
test_loss: 1.9981121
train_loss: 1.9993788
test_loss: 2.0035753
train_loss: 1.9986355
test_loss: 1.9996417
train_loss: 1.998152
test_loss: 1.9993025
train_loss: 1.9975569
test_loss: 1.9984512
train_loss: 1.9971783
test_loss: 1.9987766
train_loss: 1.9988441
test_loss: 1.9982449
train_loss: 1.9983757
test_loss: 1.998627
train_loss: 1.998547
test_loss: 1.998836
train_loss: 1.998744
test_loss: 1.9982966
train_loss: 1.9989144
test_loss: 2.0824425
train_loss: 1.999678
test_loss: 1.9984161
train_loss: 2.0388787
test_loss: 23.601109
train_loss: 0.444179
test_loss: 0.44570833
train_loss: 0.44593686
test_loss: 0.43942404
train_loss: 0.4387234
test_loss: 0.43945014
train_loss: 0.44197917
test_loss: 0.43923077
train_loss: 0.44601735
test_loss: 0.4392591
train_loss: 0.43691325
test_loss: 0.43925163
train_loss: 0.43693876
test_loss: 0.43911624
train_loss: 0.43662846
test_loss: 0.439152
train_loss: 0.43107453
test_loss: 0.438979
train_loss: 0.4447922
test_loss: 0.43891275
train_loss: 0.42570868
test_loss: 0.43885788
train_loss: 0.43413064
test_loss: 0.43882
train_loss: 0.4369818
test_loss: 0.43864164
train_loss: 0.44520196
test_loss: 0.43854967
train_loss: 0.43453124
test_loss: 0.43853778
train_loss: 0.4373271
test_loss: 0.43840653
train_loss: 0.4245114
test_loss: 0.43837455
train_loss: 0.44754666
test_loss: 0.43825167
train_loss: 0.43703425
test_loss: 0.43813565
train_loss: 0.43577415
test_loss: 0.43807408
train_loss: 0.4362569
test_loss: 0.43802792
train_loss: 0.44634566
test_loss: 0.43794456
train_loss: 0.42897028
test_loss: 0.43780676
train_loss: 0.44588137
test_loss: 0.43775666
train_loss: 0.4303848
test_loss: 0.43766132
train_loss: 0.44184023
test_loss: 0.43752187
train_loss: 0.4425838
test_loss: 0.43749347
train_loss: 0.44935846
test_loss: 0.43728536
train_loss: 0.42575437
test_loss: 0.43722722
train_loss: 0.44207472
test_loss: 0.4371654
train_loss: 0.43583292
test_loss: 0.43705878
train_loss: 0.43668574
test_loss: 0.436973
train_loss: 0.44577911
test_loss: 0.436853
train_loss: 0.43294778
test_loss: 0.43679786
train_loss: 0.4367926
test_loss: 0.43663794
train_loss: 0.4446614
test_loss: 0.43652552
train_loss: 0.42976114
test_loss: 0.43641725
train_loss: 0.43041375
test_loss: 0.43630284
train_loss: 0.42402387
test_loss: 0.43623734
train_loss: 0.4336862
test_loss: 0.43609232
train_loss: 0.4425707
test_loss: 0.43594518
train_loss: 0.43101346
test_loss: 0.43581253
train_loss: 0.43053395
test_loss: 0.43568066
train_loss: 0.439111
test_loss: 0.43561321
train_loss: 0.43682438
test_loss: 0.4354793
train_loss: 0.43181613
test_loss: 0.43539172
train_loss: 0.43112737
test_loss: 0.4352919
train_loss: 0.41802728
test_loss: 0.43497464
train_loss: 0.43109334
test_loss: 0.43496722
train_loss: 0.43082327
test_loss: 0.43475845
train_loss: 0.44706887
test_loss: 0.4346864
train_loss: 0.43495372
test_loss: 0.4346093
train_loss: 0.43230128
test_loss: 0.43444723
train_loss: 0.42515245
test_loss: 0.43433824
train_loss: 0.43619683
test_loss: 0.4341786
train_loss: 0.43397683
test_loss: 0.4340148
train_loss: 0.42293024
test_loss: 0.433975
train_loss: 0.4383253
test_loss: 0.43376172
train_loss: 0.43539667
test_loss: 0.43362927
train_loss: 0.43469554
test_loss: 0.4335507
train_loss: 0.42524582
test_loss: 0.43339074
train_loss: 0.42996466
test_loss: 0.43326354
train_loss: 0.43026555
test_loss: 0.43313366
train_loss: 0.43439445
test_loss: 0.4329584
train_loss: 0.43262765
test_loss: 0.43283215
train_loss: 0.43583792
test_loss: 0.43272117
train_loss: 0.4266069
test_loss: 0.4325379
train_loss: 0.42788815
test_loss: 0.4324042
train_loss: 0.42792884
test_loss: 0.43215832
train_loss: 0.4141531
test_loss: 0.43201914
train_loss: 0.43038642
test_loss: 0.43194127
train_loss: 0.4376878
test_loss: 0.43173665
train_loss: 0.4372934
test_loss: 0.43152466
train_loss: 0.41968352
test_loss: 0.43140954
train_loss: 0.4412132
test_loss: 0.43123052
train_loss: 0.43246472
test_loss: 0.4311385
train_loss: 0.43290895
test_loss: 0.43094406
train_loss: 0.434473
test_loss: 0.43075958
train_loss: 0.43286037
test_loss: 0.4305696
train_loss: 0.4387607
test_loss: 0.43038762
train_loss: 0.4351003
test_loss: 0.43026868
train_loss: 0.42530328
test_loss: 0.4300572
train_loss: 0.41891927
test_loss: 0.42986432
train_loss: 0.4327841
test_loss: 0.42964572
train_loss: 0.43555206
test_loss: 0.42948648
train_loss: 0.42324352
test_loss: 0.42935085
train_loss: 0.41901302
test_loss: 0.42911306
train_loss: 0.42048806
test_loss: 0.42900565
train_loss: 0.42633837
test_loss: 0.42871198
train_loss: 0.4262576
test_loss: 0.42865342
train_loss: 0.42749465
test_loss: 0.42836854
train_loss: 0.43381727
test_loss: 0.42824307
train_loss: 0.42880479
test_loss: 0.428047
train_loss: 0.41730368
test_loss: 0.42784524
train_loss: 0.43113655
test_loss: 0.4276785
train_loss: 0.42315924
test_loss: 0.42742366
train_loss: 0.4216261
test_loss: 0.4272863
train_loss: 0.41320327
test_loss: 0.4270628
train_loss: 0.42896083
test_loss: 0.426814
train_loss: 0.43246508
test_loss: 0.42665735
train_loss: 0.4161083
test_loss: 0.4264328
train_loss: 0.43099952
test_loss: 0.42624822
train_loss: 0.4205798
test_loss: 0.42602286
train_loss: 0.43271267
test_loss: 0.42580396
train_loss: 0.4235899
test_loss: 0.4256527
train_loss: 0.42267567
test_loss: 0.42537946
train_loss: 0.420815
test_loss: 0.42520893
train_loss: 0.42012262
test_loss: 0.42495647
train_loss: 0.42714188
test_loss: 0.4248161
train_loss: 0.4224322
test_loss: 0.4244539
train_loss: 0.4286406
test_loss: 0.42429048
train_loss: 0.42776334
test_loss: 0.4240729
train_loss: 0.41389287
test_loss: 0.4238476
train_loss: 0.4274209
test_loss: 0.42357126
train_loss: 0.41449952
test_loss: 0.42340896
train_loss: 0.41956672
test_loss: 0.42308247
train_loss: 0.42125356
test_loss: 0.42286995
train_loss: 0.4184466
test_loss: 0.42263076
train_loss: 0.4155514
test_loss: 0.42243627
train_loss: 0.4307822
test_loss: 0.42206043
train_loss: 0.4151474
test_loss: 0.42195648
train_loss: 0.4272352
test_loss: 0.4216764
train_loss: 0.42671922
test_loss: 0.4214082
train_loss: 0.41950384
test_loss: 0.42118865
train_loss: 0.423052
test_loss: 0.4209255
train_loss: 0.4150452
test_loss: 0.4206417
train_loss: 0.421044
test_loss: 0.42043847
train_loss: 0.42009124
test_loss: 0.42016712
train_loss: 0.41004923
test_loss: 0.41988164
train_loss: 0.42342755
test_loss: 0.41964218
train_loss: 0.4128213
test_loss: 0.4194348
train_loss: 0.42293108
test_loss: 0.41909325
train_loss: 0.41630137
test_loss: 0.4188481
train_loss: 0.4186551
test_loss: 0.41860333
train_loss: 0.42261314
test_loss: 0.4183338
train_loss: 0.42911077
test_loss: 0.4180524
train_loss: 0.4028082
test_loss: 0.41776335
train_loss: 0.4303342
test_loss: 0.41750506
train_loss: 0.42485982
test_loss: 0.41716525
train_loss: 0.4145677
test_loss: 0.41690993
train_loss: 0.41197294
test_loss: 0.4165963
train_loss: 0.40787673
test_loss: 0.41628116
train_loss: 0.40753677
test_loss: 0.41602215
train_loss: 0.40926126
test_loss: 0.41574776
train_loss: 0.40403962
test_loss: 0.41542003
train_loss: 0.41225237
test_loss: 0.41514197
train_loss: 0.4079027
test_loss: 0.4148328
train_loss: 0.41665354
test_loss: 0.4145414
train_loss: 0.39892983
test_loss: 0.41424584
train_loss: 0.41912585
test_loss: 0.41396528
train_loss: 0.40935946
test_loss: 0.4137045
train_loss: 0.4155624
test_loss: 0.41329026
train_loss: 0.40645814
test_loss: 0.4130491
train_loss: 0.41698104
test_loss: 0.41269135
train_loss: 0.42023638
test_loss: 0.41232726
train_loss: 0.40590906
test_loss: 0.41206294
train_loss: 0.41513178
test_loss: 0.41172552
train_loss: 0.41366073
test_loss: 0.41133812
train_loss: 0.40386882
test_loss: 0.41102156
train_loss: 0.42957383
test_loss: 0.41073224
train_loss: 0.40960178
test_loss: 0.4103709
train_loss: 0.4047916
test_loss: 0.41000348
train_loss: 0.39949036
test_loss: 0.40974358
train_loss: 0.41103402
test_loss: 0.4094132
train_loss: 0.40628353
test_loss: 0.40899128
train_loss: 0.41469306
test_loss: 0.40866828
train_loss: 0.4115504
test_loss: 0.40837514
train_loss: 0.3980587
test_loss: 0.40789825
train_loss: 0.4047834
test_loss: 0.4076389
train_loss: 0.4067093
test_loss: 0.40727296
train_loss: 0.41161275
test_loss: 0.40689975
train_loss: 0.39921215
test_loss: 0.40650165
train_loss: 0.3973959
test_loss: 0.40611783
train_loss: 0.40414965
test_loss: 0.40584436
train_loss: 0.40344253
test_loss: 0.40542012
train_loss: 0.4010149
test_loss: 0.40503895
train_loss: 0.39752895
test_loss: 0.40462193
train_loss: 0.40193844
test_loss: 0.40427423
train_loss: 0.4069895
test_loss: 0.40385053
train_loss: 0.40109062
test_loss: 0.40352312
train_loss: 0.38826644
test_loss: 0.40309095
train_loss: 0.3930555
test_loss: 0.4027308
train_loss: 0.39978948
test_loss: 0.40232587
train_loss: 0.41049072
test_loss: 0.40193972
train_loss: 0.39749902
test_loss: 0.40148216
train_loss: 0.38639838
test_loss: 0.40107143
train_loss: 0.4056977
test_loss: 0.4006495
train_loss: 0.38585028
test_loss: 0.40028054
train_loss: 0.4094736
test_loss: 0.3998887
train_loss: 0.40015537
test_loss: 0.39946282
train_loss: 0.38853973
test_loss: 0.39904
train_loss: 0.3881887
test_loss: 0.39862412
train_loss: 0.39622998
test_loss: 0.39812493
train_loss: 0.39872605
test_loss: 0.39776322
train_loss: 0.39185065
test_loss: 0.39727816
train_loss: 0.40373135
test_loss: 0.3968606
train_loss: 0.40517282
test_loss: 0.39644605
train_loss: 0.39538825
test_loss: 0.39594728
train_loss: 0.3942361
test_loss: 0.3955397
train_loss: 0.3959995
test_loss: 0.39508662
train_loss: 0.38557297
test_loss: 0.39459047
train_loss: 0.38916886
test_loss: 0.39417702
train_loss: 0.38563496
test_loss: 0.39365172
train_loss: 0.39154518
test_loss: 0.39326057
train_loss: 0.39933383
test_loss: 0.3926756
train_loss: 0.3922041
test_loss: 0.39226946
train_loss: 0.39036265
test_loss: 0.39183062
train_loss: 0.3968977
test_loss: 0.3912629
train_loss: 0.3867914
test_loss: 0.39084622
train_loss: 0.38095602
test_loss: 0.39027748
train_loss: 0.38870272
test_loss: 0.38981396
train_loss: 0.38758
test_loss: 0.3892864
train_loss: 0.38414848
test_loss: 0.38879794
train_loss: 0.3920284
test_loss: 0.38835287
train_loss: 0.38775232
test_loss: 0.38776064
train_loss: 0.3902585
test_loss: 0.38720375
train_loss: 0.38034603
test_loss: 0.3867865
train_loss: 0.39452988
test_loss: 0.38620418
train_loss: 0.37473008
test_loss: 0.3856161
train_loss: 0.3829775
test_loss: 0.3851084
train_loss: 0.39110294
test_loss: 0.38455465
train_loss: 0.38185826
test_loss: 0.38404772
train_loss: 0.3912179
test_loss: 0.38352823
train_loss: 0.3806767
test_loss: 0.38295403
train_loss: 0.38282204
test_loss: 0.38238984
train_loss: 0.36880437
test_loss: 0.3817789
train_loss: 0.371409
test_loss: 0.38117236
train_loss: 0.38253006
test_loss: 0.38061884
train_loss: 0.37839752
test_loss: 0.38001075
train_loss: 0.3875897
test_loss: 0.37947604
train_loss: 0.383299
test_loss: 0.37880793
train_loss: 0.39451486
test_loss: 0.3781861
train_loss: 0.37201962
test_loss: 0.37748164
train_loss: 0.38289592
test_loss: 0.37692237
train_loss: 0.37729758
test_loss: 0.37628642
train_loss: 0.37792522
test_loss: 0.3755709
train_loss: 0.36939752
test_loss: 0.37489408
train_loss: 0.37735265
test_loss: 0.37428552
train_loss: 0.3733921
test_loss: 0.37359834
train_loss: 0.36564606
test_loss: 0.37292314
train_loss: 0.3678056
test_loss: 0.37221408
train_loss: 0.36611736
test_loss: 0.37148553
train_loss: 0.35991645
test_loss: 0.3707164
train_loss: 0.37099397
test_loss: 0.36996385
train_loss: 0.36623758
test_loss: 0.3691704
train_loss: 0.36932766
test_loss: 0.3683423
train_loss: 0.37551504
test_loss: 0.36757672
train_loss: 0.37155956
test_loss: 0.36677662
train_loss: 0.36318097
test_loss: 0.365854
train_loss: 0.35359883
test_loss: 0.36497954
train_loss: 0.36598778
test_loss: 0.3639711
train_loss: 0.36344737
test_loss: 0.36305833
train_loss: 0.36034438
test_loss: 0.36207423
train_loss: 0.35741526
test_loss: 0.36099043
train_loss: 0.36030516
test_loss: 0.35988432
train_loss: 0.35674608
test_loss: 0.35874197
train_loss: 0.36174706
test_loss: 0.35754922
train_loss: 0.3565719
test_loss: 0.35622692
train_loss: 0.35708374
test_loss: 0.35482004
train_loss: 0.3681425
test_loss: 0.35348374
train_loss: 0.34512284
test_loss: 0.35193354
train_loss: 0.3509228
test_loss: 0.35030544
train_loss: 0.3475807
test_loss: 0.34856084
train_loss: 0.3473438
test_loss: 0.34675652
train_loss: 0.3481105
test_loss: 0.34478036
train_loss: 0.34882912
test_loss: 0.3426715
train_loss: 0.3406499
test_loss: 0.34046862
train_loss: 0.33797762
test_loss: 0.33808976
train_loss: 0.33583564
test_loss: 0.33558726
train_loss: 0.34030125
test_loss: 0.33294266
train_loss: 0.33852905
test_loss: 0.3301371
train_loss: 0.33309472
test_loss: 0.32727942
train_loss: 0.32439283
test_loss: 0.3242991
train_loss: 0.315742
test_loss: 0.32113197
train_loss: 0.32502222
test_loss: 0.31798378
train_loss: 0.31546077
test_loss: 0.3148768
train_loss: 0.3085869
test_loss: 0.31166768
train_loss: 0.30344754
test_loss: 0.3083741
train_loss: 0.30981982
test_loss: 0.30523804
train_loss: 0.2936277
test_loss: 0.30204126
train_loss: 0.29444954
test_loss: 0.2989825
train_loss: 0.2969534
test_loss: 0.29586887
train_loss: 0.30259115
test_loss: 0.292795
train_loss: 0.29677308
test_loss: 0.28982076
train_loss: 0.28446034
test_loss: 0.28681207
train_loss: 0.2756874
test_loss: 0.2838299
train_loss: 0.28684542
test_loss: 0.28089398
train_loss: 0.28121188
test_loss: 0.27798158
train_loss: 0.2765294
test_loss: 0.2750198
train_loss: 0.27746496
test_loss: 0.27209318
train_loss: 0.26684555
test_loss: 0.26910204
train_loss: 0.26393706
test_loss: 0.26611534
train_loss: 0.2550115
test_loss: 0.26307327
train_loss: 0.2691564
test_loss: 0.25995514
train_loss: 0.25369963
test_loss: 0.25675634
train_loss: 0.25801367
test_loss: 0.25349057
train_loss: 0.24378856
test_loss: 0.25014478
train_loss: 0.24961701
test_loss: 0.246752
train_loss: 0.2482974
test_loss: 0.243266
train_loss: 0.23428127
test_loss: 0.23970984
train_loss: 0.23477137
test_loss: 0.23618865
train_loss: 0.22966376
test_loss: 0.23268892
train_loss: 0.22616902
test_loss: 0.2293314
train_loss: 0.21957825
test_loss: 0.22600876
train_loss: 0.21887927
test_loss: 0.22285312
train_loss: 0.22051206
test_loss: 0.21981789
train_loss: 0.22149186
test_loss: 0.21686506
train_loss: 0.21088904
test_loss: 0.2140012
train_loss: 0.21219319
test_loss: 0.2112071
train_loss: 0.2043938
test_loss: 0.20847075
train_loss: 0.21089126
test_loss: 0.2059166
train_loss: 0.20955355
test_loss: 0.20335293
train_loss: 0.20548287
test_loss: 0.20087986
train_loss: 0.20058778
test_loss: 0.1983617
train_loss: 0.19764423
test_loss: 0.19595204
train_loss: 0.19219853
test_loss: 0.19353648
train_loss: 0.19465305
test_loss: 0.1911315
train_loss: 0.19441852
test_loss: 0.18884213
train_loss: 0.18340343
test_loss: 0.186491
train_loss: 0.18490162
test_loss: 0.18423682
train_loss: 0.17908245
test_loss: 0.18194552
train_loss: 0.17704573
test_loss: 0.17963946
train_loss: 0.17889053
test_loss: 0.17752887
train_loss: 0.17081887
test_loss: 0.17527832
train_loss: 0.16671443
test_loss: 0.17313485
train_loss: 0.16817309
test_loss: 0.1710271
train_loss: 0.16991106
test_loss: 0.16890094
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.16583255
test_loss: 0.16684839
train_loss: 0.16027835
test_loss: 0.16486499
train_loss: 0.16783026
test_loss: 0.16286199
train_loss: 0.16423526
test_loss: 0.16085157
train_loss: 0.1621221
test_loss: 0.15889767
train_loss: 0.15277806
test_loss: 0.15699044
train_loss: 0.16219853
test_loss: 0.15511018
train_loss: 0.15793276
test_loss: 0.15325364
train_loss: 0.15405174
test_loss: 0.15139692
train_loss: 0.1498606
test_loss: 0.14959171
train_loss: 0.15144764
test_loss: 0.14780465
train_loss: 0.14439985
test_loss: 0.14605495
train_loss: 0.14692931
test_loss: 0.14428478
train_loss: 0.1374405
test_loss: 0.14258638
train_loss: 0.1401984
test_loss: 0.14092687
train_loss: 0.13600938
test_loss: 0.13919947
train_loss: 0.1439937
test_loss: 0.13758251
train_loss: 0.14278159
test_loss: 0.1359875
train_loss: 0.13562323
test_loss: 0.13436675
train_loss: 0.1256415
test_loss: 0.13281804
train_loss: 0.13199413
test_loss: 0.13123833
train_loss: 0.12836461
test_loss: 0.12970003
train_loss: 0.1294111
test_loss: 0.12824677
train_loss: 0.123017944
test_loss: 0.12672576
train_loss: 0.121091165
test_loss: 0.12519942
train_loss: 0.1223031
test_loss: 0.1237865
train_loss: 0.119886495
test_loss: 0.12229816
train_loss: 0.11903629
test_loss: 0.1209246
train_loss: 0.11972912
test_loss: 0.11953942
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi3/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/500_500_500_500_1 --optimizer lbfgs --function f1 --psi -2 --phi 3 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi3/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5046718e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5046694598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50467187b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50466942f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5046618730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5046618950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5046532f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50464e8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50464e8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50464a1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50464a1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f504647bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5046491d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5046443a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50464437b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5046443840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50463a4268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50463ae8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50463939d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f504638af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5046336840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f504634abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f504631e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50462cdd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50462cda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50278c78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50278849d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50278a9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50278a9730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f502785c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5027806378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50277ab1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50277ab158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50277d8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50277709d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5027733598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.0244770702
Iter: 2 loss: 0.0223921351
Iter: 3 loss: 0.020537883
Iter: 4 loss: 0.0199420601
Iter: 5 loss: 0.0181874912
Iter: 6 loss: 0.0179041382
Iter: 7 loss: 0.0162564404
Iter: 8 loss: 0.0192667358
Iter: 9 loss: 0.0155907739
Iter: 10 loss: 0.0144485254
Iter: 11 loss: 0.0173360929
Iter: 12 loss: 0.0140867289
Iter: 13 loss: 0.0131886639
Iter: 14 loss: 0.0144057255
Iter: 15 loss: 0.012758825
Iter: 16 loss: 0.0120250937
Iter: 17 loss: 0.0138038844
Iter: 18 loss: 0.0117642712
Iter: 19 loss: 0.0113252681
Iter: 20 loss: 0.011531122
Iter: 21 loss: 0.0110318027
Iter: 22 loss: 0.0105060283
Iter: 23 loss: 0.0121644838
Iter: 24 loss: 0.0103525538
Iter: 25 loss: 0.00998542
Iter: 26 loss: 0.0102175437
Iter: 27 loss: 0.00974906
Iter: 28 loss: 0.0093177883
Iter: 29 loss: 0.0104715358
Iter: 30 loss: 0.00917333
Iter: 31 loss: 0.00874074921
Iter: 32 loss: 0.0103543159
Iter: 33 loss: 0.00863537099
Iter: 34 loss: 0.00824792311
Iter: 35 loss: 0.00842558406
Iter: 36 loss: 0.00799240638
Iter: 37 loss: 0.00768008828
Iter: 38 loss: 0.0098076649
Iter: 39 loss: 0.0076401
Iter: 40 loss: 0.00742299249
Iter: 41 loss: 0.00987287425
Iter: 42 loss: 0.00741859339
Iter: 43 loss: 0.00727527402
Iter: 44 loss: 0.00706584565
Iter: 45 loss: 0.00705997553
Iter: 46 loss: 0.0068108975
Iter: 47 loss: 0.00826285128
Iter: 48 loss: 0.00677658897
Iter: 49 loss: 0.00654177461
Iter: 50 loss: 0.00795568246
Iter: 51 loss: 0.00651277881
Iter: 52 loss: 0.00633350387
Iter: 53 loss: 0.00652705785
Iter: 54 loss: 0.00622360129
Iter: 55 loss: 0.00597898569
Iter: 56 loss: 0.00678209402
Iter: 57 loss: 0.00591260521
Iter: 58 loss: 0.00573439104
Iter: 59 loss: 0.00653371774
Iter: 60 loss: 0.00569076929
Iter: 61 loss: 0.00552920159
Iter: 62 loss: 0.00577893388
Iter: 63 loss: 0.00545316376
Iter: 64 loss: 0.00522117689
Iter: 65 loss: 0.00626448542
Iter: 66 loss: 0.00516762072
Iter: 67 loss: 0.0050415555
Iter: 68 loss: 0.00519947754
Iter: 69 loss: 0.00497592147
Iter: 70 loss: 0.00484862598
Iter: 71 loss: 0.00493064336
Iter: 72 loss: 0.00476595946
Iter: 73 loss: 0.00464759441
Iter: 74 loss: 0.00469569908
Iter: 75 loss: 0.00456594722
Iter: 76 loss: 0.00443449616
Iter: 77 loss: 0.00442626607
Iter: 78 loss: 0.00428504637
Iter: 79 loss: 0.00614818791
Iter: 80 loss: 0.00428408943
Iter: 81 loss: 0.00418465445
Iter: 82 loss: 0.00447234232
Iter: 83 loss: 0.00415272685
Iter: 84 loss: 0.00405638851
Iter: 85 loss: 0.00433821976
Iter: 86 loss: 0.00402607396
Iter: 87 loss: 0.00393993128
Iter: 88 loss: 0.00406107958
Iter: 89 loss: 0.00389743247
Iter: 90 loss: 0.0038132458
Iter: 91 loss: 0.00381301437
Iter: 92 loss: 0.00375286187
Iter: 93 loss: 0.00401599426
Iter: 94 loss: 0.00373901264
Iter: 95 loss: 0.0036813966
Iter: 96 loss: 0.00367463729
Iter: 97 loss: 0.00363332825
Iter: 98 loss: 0.00355097791
Iter: 99 loss: 0.0039140489
Iter: 100 loss: 0.00353409303
Iter: 101 loss: 0.00346530881
Iter: 102 loss: 0.00354850804
Iter: 103 loss: 0.00342948246
Iter: 104 loss: 0.00335002085
Iter: 105 loss: 0.00360185187
Iter: 106 loss: 0.00332798646
Iter: 107 loss: 0.00324028544
Iter: 108 loss: 0.00411623204
Iter: 109 loss: 0.00323767937
Iter: 110 loss: 0.00317614176
Iter: 111 loss: 0.00335198292
Iter: 112 loss: 0.0031567628
Iter: 113 loss: 0.00309129804
Iter: 114 loss: 0.00336077157
Iter: 115 loss: 0.00307759759
Iter: 116 loss: 0.0030170572
Iter: 117 loss: 0.00316407671
Iter: 118 loss: 0.00299556414
Iter: 119 loss: 0.00294733932
Iter: 120 loss: 0.00309324521
Iter: 121 loss: 0.00293243816
Iter: 122 loss: 0.00288453978
Iter: 123 loss: 0.00312948506
Iter: 124 loss: 0.00287668128
Iter: 125 loss: 0.00283045275
Iter: 126 loss: 0.00293315877
Iter: 127 loss: 0.00281261862
Iter: 128 loss: 0.00277335686
Iter: 129 loss: 0.00276426342
Iter: 130 loss: 0.00273921806
Iter: 131 loss: 0.00267700385
Iter: 132 loss: 0.0027590855
Iter: 133 loss: 0.00264527043
Iter: 134 loss: 0.00258656801
Iter: 135 loss: 0.00300720707
Iter: 136 loss: 0.00258143828
Iter: 137 loss: 0.00253848964
Iter: 138 loss: 0.00260338443
Iter: 139 loss: 0.00251791487
Iter: 140 loss: 0.00247573527
Iter: 141 loss: 0.0026800232
Iter: 142 loss: 0.00246810471
Iter: 143 loss: 0.00243209023
Iter: 144 loss: 0.00243472448
Iter: 145 loss: 0.00240419968
Iter: 146 loss: 0.00236045755
Iter: 147 loss: 0.00264292583
Iter: 148 loss: 0.00235579675
Iter: 149 loss: 0.00233249436
Iter: 150 loss: 0.00233070413
Iter: 151 loss: 0.00231150165
Iter: 152 loss: 0.00228075404
Iter: 153 loss: 0.00228051329
Iter: 154 loss: 0.00224574888
Iter: 155 loss: 0.00230128
Iter: 156 loss: 0.00222935015
Iter: 157 loss: 0.00220278581
Iter: 158 loss: 0.00220191688
Iter: 159 loss: 0.00217365753
Iter: 160 loss: 0.00221463665
Iter: 161 loss: 0.00215987512
Iter: 162 loss: 0.00213707518
Iter: 163 loss: 0.00213789754
Iter: 164 loss: 0.0021191265
Iter: 165 loss: 0.0020871358
Iter: 166 loss: 0.00215115189
Iter: 167 loss: 0.00207418692
Iter: 168 loss: 0.00204602187
Iter: 169 loss: 0.00212399894
Iter: 170 loss: 0.00203693798
Iter: 171 loss: 0.0020044
Iter: 172 loss: 0.00205695373
Iter: 173 loss: 0.00198935764
Iter: 174 loss: 0.00195589615
Iter: 175 loss: 0.00209089718
Iter: 176 loss: 0.00194849365
Iter: 177 loss: 0.00191773777
Iter: 178 loss: 0.00194352341
Iter: 179 loss: 0.00189937744
Iter: 180 loss: 0.00187154149
Iter: 181 loss: 0.00210366771
Iter: 182 loss: 0.00186998921
Iter: 183 loss: 0.00184824935
Iter: 184 loss: 0.00199399097
Iter: 185 loss: 0.00184604572
Iter: 186 loss: 0.00182683021
Iter: 187 loss: 0.00183206273
Iter: 188 loss: 0.00181283802
Iter: 189 loss: 0.00179218221
Iter: 190 loss: 0.00178512861
Iter: 191 loss: 0.00177339045
Iter: 192 loss: 0.00174396485
Iter: 193 loss: 0.00187517097
Iter: 194 loss: 0.0017381683
Iter: 195 loss: 0.00172282278
Iter: 196 loss: 0.00172268576
Iter: 197 loss: 0.00170529424
Iter: 198 loss: 0.00175294979
Iter: 199 loss: 0.00169965532
Iter: 200 loss: 0.00168054493
Iter: 201 loss: 0.00168779725
Iter: 202 loss: 0.00166723283
Iter: 203 loss: 0.00163159426
Iter: 204 loss: 0.0018900322
Iter: 205 loss: 0.00162855687
Iter: 206 loss: 0.00160495436
Iter: 207 loss: 0.00160494889
Iter: 208 loss: 0.00158756413
Iter: 209 loss: 0.00159113319
Iter: 210 loss: 0.00157466857
Iter: 211 loss: 0.00155250705
Iter: 212 loss: 0.00160955288
Iter: 213 loss: 0.00154493423
Iter: 214 loss: 0.00152217352
Iter: 215 loss: 0.00162942708
Iter: 216 loss: 0.00151797663
Iter: 217 loss: 0.00150065485
Iter: 218 loss: 0.00159888237
Iter: 219 loss: 0.00149829767
Iter: 220 loss: 0.00148417871
Iter: 221 loss: 0.00158084976
Iter: 222 loss: 0.00148282445
Iter: 223 loss: 0.00147114275
Iter: 224 loss: 0.00146597589
Iter: 225 loss: 0.00146004767
Iter: 226 loss: 0.00144460436
Iter: 227 loss: 0.00150097185
Iter: 228 loss: 0.00144073844
Iter: 229 loss: 0.00143216341
Iter: 230 loss: 0.00143182185
Iter: 231 loss: 0.00142212212
Iter: 232 loss: 0.00142066902
Iter: 233 loss: 0.00141388481
Iter: 234 loss: 0.0014016066
Iter: 235 loss: 0.00140579371
Iter: 236 loss: 0.00139285973
Iter: 237 loss: 0.00137448951
Iter: 238 loss: 0.00144523219
Iter: 239 loss: 0.00137017434
Iter: 240 loss: 0.0013553506
Iter: 241 loss: 0.00140428892
Iter: 242 loss: 0.00135131087
Iter: 243 loss: 0.00133837876
Iter: 244 loss: 0.00143562909
Iter: 245 loss: 0.00133734685
Iter: 246 loss: 0.00132700033
Iter: 247 loss: 0.00133182644
Iter: 248 loss: 0.00132004486
Iter: 249 loss: 0.00130645325
Iter: 250 loss: 0.00134331873
Iter: 251 loss: 0.0013019694
Iter: 252 loss: 0.00129143405
Iter: 253 loss: 0.00144144008
Iter: 254 loss: 0.00129141239
Iter: 255 loss: 0.00128239603
Iter: 256 loss: 0.00129187712
Iter: 257 loss: 0.00127741764
Iter: 258 loss: 0.00126833189
Iter: 259 loss: 0.00127141108
Iter: 260 loss: 0.0012619
Iter: 261 loss: 0.00124897435
Iter: 262 loss: 0.00127364625
Iter: 263 loss: 0.00124353939
Iter: 264 loss: 0.00123079214
Iter: 265 loss: 0.00128138263
Iter: 266 loss: 0.00122797489
Iter: 267 loss: 0.00122251315
Iter: 268 loss: 0.00122139417
Iter: 269 loss: 0.00121609052
Iter: 270 loss: 0.00120694283
Iter: 271 loss: 0.00120694
Iter: 272 loss: 0.00118981511
Iter: 273 loss: 0.00123909814
Iter: 274 loss: 0.00118436897
Iter: 275 loss: 0.00117102184
Iter: 276 loss: 0.00130209257
Iter: 277 loss: 0.00117050845
Iter: 278 loss: 0.00115846854
Iter: 279 loss: 0.00122181477
Iter: 280 loss: 0.00115662115
Iter: 281 loss: 0.00114733202
Iter: 282 loss: 0.0011523373
Iter: 283 loss: 0.00114121521
Iter: 284 loss: 0.00113058905
Iter: 285 loss: 0.00118953339
Iter: 286 loss: 0.00112906843
Iter: 287 loss: 0.0011205629
Iter: 288 loss: 0.00125036109
Iter: 289 loss: 0.00112056173
Iter: 290 loss: 0.00111438241
Iter: 291 loss: 0.0011070132
Iter: 292 loss: 0.00110624358
Iter: 293 loss: 0.00109525304
Iter: 294 loss: 0.00112970325
Iter: 295 loss: 0.00109174079
Iter: 296 loss: 0.0010834526
Iter: 297 loss: 0.00108715228
Iter: 298 loss: 0.00107782404
Iter: 299 loss: 0.00106804073
Iter: 300 loss: 0.00121686456
Iter: 301 loss: 0.001068037
Iter: 302 loss: 0.00106048444
Iter: 303 loss: 0.00107212667
Iter: 304 loss: 0.00105693983
Iter: 305 loss: 0.00104800635
Iter: 306 loss: 0.0010432211
Iter: 307 loss: 0.00103917392
Iter: 308 loss: 0.00102660386
Iter: 309 loss: 0.00111793296
Iter: 310 loss: 0.00102551864
Iter: 311 loss: 0.00101476011
Iter: 312 loss: 0.00105506845
Iter: 313 loss: 0.00101214647
Iter: 314 loss: 0.00100236677
Iter: 315 loss: 0.00103946368
Iter: 316 loss: 0.00100004289
Iter: 317 loss: 0.000991141
Iter: 318 loss: 0.00103531405
Iter: 319 loss: 0.000989581225
Iter: 320 loss: 0.000982400612
Iter: 321 loss: 0.00102212199
Iter: 322 loss: 0.000981383375
Iter: 323 loss: 0.000975429663
Iter: 324 loss: 0.00100861071
Iter: 325 loss: 0.000974591705
Iter: 326 loss: 0.000968704699
Iter: 327 loss: 0.000971249654
Iter: 328 loss: 0.000964679813
Iter: 329 loss: 0.000958587334
Iter: 330 loss: 0.00099128054
Iter: 331 loss: 0.00095767976
Iter: 332 loss: 0.00095264829
Iter: 333 loss: 0.000991638284
Iter: 334 loss: 0.000952272443
Iter: 335 loss: 0.000947456923
Iter: 336 loss: 0.00095074554
Iter: 337 loss: 0.000944432395
Iter: 338 loss: 0.000938159879
Iter: 339 loss: 0.00093682704
Iter: 340 loss: 0.000932729337
Iter: 341 loss: 0.000923224
Iter: 342 loss: 0.000941307633
Iter: 343 loss: 0.000919224
Iter: 344 loss: 0.000909064489
Iter: 345 loss: 0.000937857956
Iter: 346 loss: 0.000905849447
Iter: 347 loss: 0.000897204853
Iter: 348 loss: 0.000911096868
Iter: 349 loss: 0.00089318346
Iter: 350 loss: 0.000885710935
Iter: 351 loss: 0.000987721607
Iter: 352 loss: 0.000885680667
Iter: 353 loss: 0.000880287495
Iter: 354 loss: 0.000892470591
Iter: 355 loss: 0.000878234452
Iter: 356 loss: 0.00087229273
Iter: 357 loss: 0.00090496731
Iter: 358 loss: 0.000871442957
Iter: 359 loss: 0.000866580871
Iter: 360 loss: 0.000878048711
Iter: 361 loss: 0.000864814327
Iter: 362 loss: 0.000859421
Iter: 363 loss: 0.000859784486
Iter: 364 loss: 0.000855206628
Iter: 365 loss: 0.000849480275
Iter: 366 loss: 0.000902125088
Iter: 367 loss: 0.000849224511
Iter: 368 loss: 0.000843581744
Iter: 369 loss: 0.000880358391
Iter: 370 loss: 0.000842982
Iter: 371 loss: 0.000838221924
Iter: 372 loss: 0.000836627
Iter: 373 loss: 0.000833884522
Iter: 374 loss: 0.000826842268
Iter: 375 loss: 0.00083907618
Iter: 376 loss: 0.00082370796
Iter: 377 loss: 0.000815901614
Iter: 378 loss: 0.000842843088
Iter: 379 loss: 0.000813829829
Iter: 380 loss: 0.00080684945
Iter: 381 loss: 0.000825940398
Iter: 382 loss: 0.000804564625
Iter: 383 loss: 0.000798191584
Iter: 384 loss: 0.000825372408
Iter: 385 loss: 0.000796876731
Iter: 386 loss: 0.000790625229
Iter: 387 loss: 0.000803458854
Iter: 388 loss: 0.000788112346
Iter: 389 loss: 0.000782443036
Iter: 390 loss: 0.000828374294
Iter: 391 loss: 0.000782064279
Iter: 392 loss: 0.000777482
Iter: 393 loss: 0.000791956438
Iter: 394 loss: 0.000776163652
Iter: 395 loss: 0.000771883177
Iter: 396 loss: 0.000778944814
Iter: 397 loss: 0.000769935723
Iter: 398 loss: 0.000765310426
Iter: 399 loss: 0.00077796285
Iter: 400 loss: 0.000763788819
Iter: 401 loss: 0.000760277442
Iter: 402 loss: 0.000795468397
Iter: 403 loss: 0.000760168419
Iter: 404 loss: 0.000756316702
Iter: 405 loss: 0.000757646048
Iter: 406 loss: 0.00075359
Iter: 407 loss: 0.000749563915
Iter: 408 loss: 0.000752888853
Iter: 409 loss: 0.000747147191
Iter: 410 loss: 0.000741022406
Iter: 411 loss: 0.000754976063
Iter: 412 loss: 0.00073873566
Iter: 413 loss: 0.000732765067
Iter: 414 loss: 0.000755641377
Iter: 415 loss: 0.000731354405
Iter: 416 loss: 0.00072556996
Iter: 417 loss: 0.000738560455
Iter: 418 loss: 0.000723373494
Iter: 419 loss: 0.000717804767
Iter: 420 loss: 0.000736071845
Iter: 421 loss: 0.000716256
Iter: 422 loss: 0.000710276654
Iter: 423 loss: 0.000727347797
Iter: 424 loss: 0.000708397478
Iter: 425 loss: 0.000704031729
Iter: 426 loss: 0.000760952593
Iter: 427 loss: 0.000704001752
Iter: 428 loss: 0.000700385834
Iter: 429 loss: 0.00069950847
Iter: 430 loss: 0.000697214273
Iter: 431 loss: 0.00069228292
Iter: 432 loss: 0.000710273045
Iter: 433 loss: 0.000691063
Iter: 434 loss: 0.00068615796
Iter: 435 loss: 0.000692193047
Iter: 436 loss: 0.000683580292
Iter: 437 loss: 0.000678794
Iter: 438 loss: 0.00071132672
Iter: 439 loss: 0.000678309589
Iter: 440 loss: 0.00067472714
Iter: 441 loss: 0.000722159923
Iter: 442 loss: 0.000674709503
Iter: 443 loss: 0.000672502851
Iter: 444 loss: 0.000668481516
Iter: 445 loss: 0.000764793716
Iter: 446 loss: 0.000668478082
Iter: 447 loss: 0.000663248356
Iter: 448 loss: 0.000679707387
Iter: 449 loss: 0.000661737169
Iter: 450 loss: 0.000656400225
Iter: 451 loss: 0.000674831506
Iter: 452 loss: 0.000654978212
Iter: 453 loss: 0.000650005531
Iter: 454 loss: 0.000669781119
Iter: 455 loss: 0.000648875139
Iter: 456 loss: 0.000644757762
Iter: 457 loss: 0.000653865398
Iter: 458 loss: 0.000643187377
Iter: 459 loss: 0.00063921354
Iter: 460 loss: 0.000663394458
Iter: 461 loss: 0.000638733793
Iter: 462 loss: 0.000635112752
Iter: 463 loss: 0.000655916752
Iter: 464 loss: 0.000634608965
Iter: 465 loss: 0.000631553063
Iter: 466 loss: 0.000634337892
Iter: 467 loss: 0.000629783317
Iter: 468 loss: 0.00062611571
Iter: 469 loss: 0.000631016213
Iter: 470 loss: 0.000624265
Iter: 471 loss: 0.000620054489
Iter: 472 loss: 0.000641775783
Iter: 473 loss: 0.000619368395
Iter: 474 loss: 0.00061673508
Iter: 475 loss: 0.000637051475
Iter: 476 loss: 0.000616538862
Iter: 477 loss: 0.000613468466
Iter: 478 loss: 0.000612744829
Iter: 479 loss: 0.000610772287
Iter: 480 loss: 0.000606900838
Iter: 481 loss: 0.000615369121
Iter: 482 loss: 0.000605363748
Iter: 483 loss: 0.000601329142
Iter: 484 loss: 0.00060392241
Iter: 485 loss: 0.000598770508
Iter: 486 loss: 0.000594457262
Iter: 487 loss: 0.000638134428
Iter: 488 loss: 0.000594322628
Iter: 489 loss: 0.000590884
Iter: 490 loss: 0.000590578944
Iter: 491 loss: 0.000588032941
Iter: 492 loss: 0.000583900139
Iter: 493 loss: 0.000617357087
Iter: 494 loss: 0.000583631336
Iter: 495 loss: 0.000579904183
Iter: 496 loss: 0.000581765198
Iter: 497 loss: 0.000577423139
Iter: 498 loss: 0.000573629106
Iter: 499 loss: 0.000573625322
Iter: 500 loss: 0.000571185315
Iter: 501 loss: 0.000569830299
Iter: 502 loss: 0.000568760675
Iter: 503 loss: 0.000564822
Iter: 504 loss: 0.000573845115
Iter: 505 loss: 0.000563357258
Iter: 506 loss: 0.000559658569
Iter: 507 loss: 0.000578550447
Iter: 508 loss: 0.000559042208
Iter: 509 loss: 0.000556467799
Iter: 510 loss: 0.000572478806
Iter: 511 loss: 0.000556181069
Iter: 512 loss: 0.000553035759
Iter: 513 loss: 0.000557504129
Iter: 514 loss: 0.000551492441
Iter: 515 loss: 0.000548523269
Iter: 516 loss: 0.000545398216
Iter: 517 loss: 0.000544850132
Iter: 518 loss: 0.000540368725
Iter: 519 loss: 0.000540329958
Iter: 520 loss: 0.000536733482
Iter: 521 loss: 0.000548742
Iter: 522 loss: 0.000535751518
Iter: 523 loss: 0.000532039674
Iter: 524 loss: 0.000541046
Iter: 525 loss: 0.000530710618
Iter: 526 loss: 0.000526489574
Iter: 527 loss: 0.000538222
Iter: 528 loss: 0.0005251
Iter: 529 loss: 0.00052131305
Iter: 530 loss: 0.000553308
Iter: 531 loss: 0.000521105598
Iter: 532 loss: 0.000518087705
Iter: 533 loss: 0.000529252575
Iter: 534 loss: 0.000517345616
Iter: 535 loss: 0.000514288142
Iter: 536 loss: 0.000517817214
Iter: 537 loss: 0.000512648374
Iter: 538 loss: 0.000510254875
Iter: 539 loss: 0.000522661896
Iter: 540 loss: 0.000509876409
Iter: 541 loss: 0.000507325109
Iter: 542 loss: 0.000511296268
Iter: 543 loss: 0.000506116776
Iter: 544 loss: 0.000503262156
Iter: 545 loss: 0.000526920776
Iter: 546 loss: 0.000503068324
Iter: 547 loss: 0.000501001778
Iter: 548 loss: 0.000500949274
Iter: 549 loss: 0.000499328366
Iter: 550 loss: 0.000496466062
Iter: 551 loss: 0.0005009952
Iter: 552 loss: 0.000495117391
Iter: 553 loss: 0.000492015562
Iter: 554 loss: 0.000497845933
Iter: 555 loss: 0.000490699196
Iter: 556 loss: 0.000487524143
Iter: 557 loss: 0.000495401735
Iter: 558 loss: 0.000486399338
Iter: 559 loss: 0.000482575531
Iter: 560 loss: 0.000488601043
Iter: 561 loss: 0.000480795512
Iter: 562 loss: 0.000477256253
Iter: 563 loss: 0.00050524791
Iter: 564 loss: 0.000477009628
Iter: 565 loss: 0.000473784748
Iter: 566 loss: 0.000478016736
Iter: 567 loss: 0.000472146785
Iter: 568 loss: 0.000468484068
Iter: 569 loss: 0.000495317
Iter: 570 loss: 0.00046817449
Iter: 571 loss: 0.000465440855
Iter: 572 loss: 0.000466625614
Iter: 573 loss: 0.000463571312
Iter: 574 loss: 0.000460374897
Iter: 575 loss: 0.000467841
Iter: 576 loss: 0.000459212635
Iter: 577 loss: 0.000457259273
Iter: 578 loss: 0.000457152666
Iter: 579 loss: 0.00045534689
Iter: 580 loss: 0.000456788868
Iter: 581 loss: 0.000454252498
Iter: 582 loss: 0.00045186124
Iter: 583 loss: 0.00045360779
Iter: 584 loss: 0.000450390216
Iter: 585 loss: 0.000447899394
Iter: 586 loss: 0.000448995095
Iter: 587 loss: 0.000446196
Iter: 588 loss: 0.000443036377
Iter: 589 loss: 0.000464596902
Iter: 590 loss: 0.00044272907
Iter: 591 loss: 0.000439975032
Iter: 592 loss: 0.000442443183
Iter: 593 loss: 0.000438362418
Iter: 594 loss: 0.000435395719
Iter: 595 loss: 0.000444823439
Iter: 596 loss: 0.000434543705
Iter: 597 loss: 0.00043160413
Iter: 598 loss: 0.000435500115
Iter: 599 loss: 0.000430104235
Iter: 600 loss: 0.000427604886
Iter: 601 loss: 0.000458109309
Iter: 602 loss: 0.000427577
Iter: 603 loss: 0.000425397448
Iter: 604 loss: 0.000429231557
Iter: 605 loss: 0.00042443
Iter: 606 loss: 0.00042159413
Iter: 607 loss: 0.00043199939
Iter: 608 loss: 0.000420885044
Iter: 609 loss: 0.000418737676
Iter: 610 loss: 0.000420759548
Iter: 611 loss: 0.00041750283
Iter: 612 loss: 0.000414738082
Iter: 613 loss: 0.000427856081
Iter: 614 loss: 0.000414235139
Iter: 615 loss: 0.000412332651
Iter: 616 loss: 0.000412332098
Iter: 617 loss: 0.000411176152
Iter: 618 loss: 0.000409099448
Iter: 619 loss: 0.000460458337
Iter: 620 loss: 0.000409101165
Iter: 621 loss: 0.000406189531
Iter: 622 loss: 0.000414290407
Iter: 623 loss: 0.000405241444
Iter: 624 loss: 0.000402575795
Iter: 625 loss: 0.000411912188
Iter: 626 loss: 0.00040187
Iter: 627 loss: 0.000399160839
Iter: 628 loss: 0.000410788518
Iter: 629 loss: 0.000398594362
Iter: 630 loss: 0.000396441435
Iter: 631 loss: 0.000398590812
Iter: 632 loss: 0.000395224779
Iter: 633 loss: 0.000392558228
Iter: 634 loss: 0.000400642282
Iter: 635 loss: 0.000391750829
Iter: 636 loss: 0.000389225228
Iter: 637 loss: 0.000400055083
Iter: 638 loss: 0.000388702436
Iter: 639 loss: 0.00038664657
Iter: 640 loss: 0.0003900702
Iter: 641 loss: 0.000385712716
Iter: 642 loss: 0.000383346
Iter: 643 loss: 0.000390700647
Iter: 644 loss: 0.000382641185
Iter: 645 loss: 0.000380837533
Iter: 646 loss: 0.000403080252
Iter: 647 loss: 0.000380819
Iter: 648 loss: 0.000379335921
Iter: 649 loss: 0.000377586373
Iter: 650 loss: 0.000377395598
Iter: 651 loss: 0.000375462696
Iter: 652 loss: 0.000387557026
Iter: 653 loss: 0.000375236617
Iter: 654 loss: 0.000373698771
Iter: 655 loss: 0.000382604892
Iter: 656 loss: 0.000373492687
Iter: 657 loss: 0.00037166453
Iter: 658 loss: 0.000377056451
Iter: 659 loss: 0.000371099915
Iter: 660 loss: 0.000369724876
Iter: 661 loss: 0.000370738475
Iter: 662 loss: 0.0003688703
Iter: 663 loss: 0.000366895692
Iter: 664 loss: 0.000366802618
Iter: 665 loss: 0.000365285261
Iter: 666 loss: 0.000363236904
Iter: 667 loss: 0.000372145267
Iter: 668 loss: 0.000362813502
Iter: 669 loss: 0.000360354374
Iter: 670 loss: 0.000361740356
Iter: 671 loss: 0.000358748774
Iter: 672 loss: 0.000356508332
Iter: 673 loss: 0.000381021935
Iter: 674 loss: 0.000356459583
Iter: 675 loss: 0.000354629417
Iter: 676 loss: 0.000355163123
Iter: 677 loss: 0.000353316194
Iter: 678 loss: 0.000351245049
Iter: 679 loss: 0.000364052627
Iter: 680 loss: 0.000351004332
Iter: 681 loss: 0.000349311915
Iter: 682 loss: 0.000351863244
Iter: 683 loss: 0.000348495087
Iter: 684 loss: 0.000347078196
Iter: 685 loss: 0.000368629058
Iter: 686 loss: 0.000347074878
Iter: 687 loss: 0.000345912878
Iter: 688 loss: 0.000344251981
Iter: 689 loss: 0.000344199128
Iter: 690 loss: 0.000342267682
Iter: 691 loss: 0.000347863563
Iter: 692 loss: 0.000341666862
Iter: 693 loss: 0.00034021461
Iter: 694 loss: 0.000340200029
Iter: 695 loss: 0.00033884318
Iter: 696 loss: 0.000339454564
Iter: 697 loss: 0.000337916717
Iter: 698 loss: 0.00033661566
Iter: 699 loss: 0.000336480734
Iter: 700 loss: 0.000335528457
Iter: 701 loss: 0.000333696895
Iter: 702 loss: 0.00034180138
Iter: 703 loss: 0.000333329546
Iter: 704 loss: 0.00033147505
Iter: 705 loss: 0.000334134558
Iter: 706 loss: 0.000330559909
Iter: 707 loss: 0.000328804716
Iter: 708 loss: 0.000338078127
Iter: 709 loss: 0.000328528928
Iter: 710 loss: 0.000326839741
Iter: 711 loss: 0.000328286435
Iter: 712 loss: 0.000325836765
Iter: 713 loss: 0.000324069231
Iter: 714 loss: 0.000330522598
Iter: 715 loss: 0.000323626911
Iter: 716 loss: 0.000321564061
Iter: 717 loss: 0.000325662142
Iter: 718 loss: 0.000320715713
Iter: 719 loss: 0.000319050683
Iter: 720 loss: 0.000327329064
Iter: 721 loss: 0.00031877021
Iter: 722 loss: 0.000317199941
Iter: 723 loss: 0.000321137486
Iter: 724 loss: 0.00031665439
Iter: 725 loss: 0.000314922567
Iter: 726 loss: 0.000324701134
Iter: 727 loss: 0.000314678735
Iter: 728 loss: 0.000313657685
Iter: 729 loss: 0.000320223742
Iter: 730 loss: 0.000313545839
Iter: 731 loss: 0.000312472635
Iter: 732 loss: 0.000312942895
Iter: 733 loss: 0.000311745971
Iter: 734 loss: 0.000310450094
Iter: 735 loss: 0.000311326032
Iter: 736 loss: 0.0003096367
Iter: 737 loss: 0.000308248942
Iter: 738 loss: 0.000309785391
Iter: 739 loss: 0.000307493086
Iter: 740 loss: 0.000305749447
Iter: 741 loss: 0.000310118921
Iter: 742 loss: 0.000305135967
Iter: 743 loss: 0.000303403765
Iter: 744 loss: 0.000308256829
Iter: 745 loss: 0.000302852306
Iter: 746 loss: 0.000301027088
Iter: 747 loss: 0.000313855882
Iter: 748 loss: 0.000300855027
Iter: 749 loss: 0.000299559091
Iter: 750 loss: 0.000298675266
Iter: 751 loss: 0.000298198924
Iter: 752 loss: 0.000296414
Iter: 753 loss: 0.000310664473
Iter: 754 loss: 0.000296295737
Iter: 755 loss: 0.000294822559
Iter: 756 loss: 0.000295858801
Iter: 757 loss: 0.000293909456
Iter: 758 loss: 0.000292335666
Iter: 759 loss: 0.000312978693
Iter: 760 loss: 0.000292324286
Iter: 761 loss: 0.000291299773
Iter: 762 loss: 0.000294570636
Iter: 763 loss: 0.000291005825
Iter: 764 loss: 0.000289743766
Iter: 765 loss: 0.000293273537
Iter: 766 loss: 0.000289334392
Iter: 767 loss: 0.000288205629
Iter: 768 loss: 0.000294012192
Iter: 769 loss: 0.000288018142
Iter: 770 loss: 0.000287191
Iter: 771 loss: 0.000285571
Iter: 772 loss: 0.000318343227
Iter: 773 loss: 0.000285555201
Iter: 774 loss: 0.000283757574
Iter: 775 loss: 0.000296773447
Iter: 776 loss: 0.000283604837
Iter: 777 loss: 0.000282251247
Iter: 778 loss: 0.000285595743
Iter: 779 loss: 0.000281778164
Iter: 780 loss: 0.000280358887
Iter: 781 loss: 0.000283244124
Iter: 782 loss: 0.000279790664
Iter: 783 loss: 0.00027833943
Iter: 784 loss: 0.000283670524
Iter: 785 loss: 0.000277971529
Iter: 786 loss: 0.000276663748
Iter: 787 loss: 0.000282723
Iter: 788 loss: 0.000276416918
Iter: 789 loss: 0.000275220315
Iter: 790 loss: 0.00027710764
Iter: 791 loss: 0.000274659
Iter: 792 loss: 0.000273438229
Iter: 793 loss: 0.000275520259
Iter: 794 loss: 0.000272894307
Iter: 795 loss: 0.000271499914
Iter: 796 loss: 0.00028016
Iter: 797 loss: 0.000271336758
Iter: 798 loss: 0.000270452467
Iter: 799 loss: 0.000283518748
Iter: 800 loss: 0.000270451157
Iter: 801 loss: 0.000269692158
Iter: 802 loss: 0.000269761716
Iter: 803 loss: 0.000269104436
Iter: 804 loss: 0.000267917523
Iter: 805 loss: 0.000270533492
Iter: 806 loss: 0.000267463096
Iter: 807 loss: 0.000266405317
Iter: 808 loss: 0.000268135
Iter: 809 loss: 0.000265920069
Iter: 810 loss: 0.000264764793
Iter: 811 loss: 0.000265860843
Iter: 812 loss: 0.000264105038
Iter: 813 loss: 0.000262918125
Iter: 814 loss: 0.000273226557
Iter: 815 loss: 0.000262855086
Iter: 816 loss: 0.00026179594
Iter: 817 loss: 0.000260528206
Iter: 818 loss: 0.000260402274
Iter: 819 loss: 0.000258854649
Iter: 820 loss: 0.000271097175
Iter: 821 loss: 0.000258745364
Iter: 822 loss: 0.000257472886
Iter: 823 loss: 0.000261519395
Iter: 824 loss: 0.000257111213
Iter: 825 loss: 0.000255876512
Iter: 826 loss: 0.000260897476
Iter: 827 loss: 0.000255601539
Iter: 828 loss: 0.000254505518
Iter: 829 loss: 0.000257000385
Iter: 830 loss: 0.000254098733
Iter: 831 loss: 0.000253263512
Iter: 832 loss: 0.000261831156
Iter: 833 loss: 0.00025323953
Iter: 834 loss: 0.000252528756
Iter: 835 loss: 0.000255821913
Iter: 836 loss: 0.000252394704
Iter: 837 loss: 0.000251758291
Iter: 838 loss: 0.000251563382
Iter: 839 loss: 0.000251182733
Iter: 840 loss: 0.000250199781
Iter: 841 loss: 0.000252937578
Iter: 842 loss: 0.000249884848
Iter: 843 loss: 0.000248920114
Iter: 844 loss: 0.00025002769
Iter: 845 loss: 0.000248398719
Iter: 846 loss: 0.000247328775
Iter: 847 loss: 0.000248866389
Iter: 848 loss: 0.00024680284
Iter: 849 loss: 0.000245605072
Iter: 850 loss: 0.000250712648
Iter: 851 loss: 0.000245349103
Iter: 852 loss: 0.000244203547
Iter: 853 loss: 0.000246753159
Iter: 854 loss: 0.00024376325
Iter: 855 loss: 0.000242658498
Iter: 856 loss: 0.000244184979
Iter: 857 loss: 0.000242110851
Iter: 858 loss: 0.000241031521
Iter: 859 loss: 0.000246060488
Iter: 860 loss: 0.00024083395
Iter: 861 loss: 0.000239729416
Iter: 862 loss: 0.00024314248
Iter: 863 loss: 0.000239402842
Iter: 864 loss: 0.00023845509
Iter: 865 loss: 0.000242334951
Iter: 866 loss: 0.000238247798
Iter: 867 loss: 0.000237443834
Iter: 868 loss: 0.000243718459
Iter: 869 loss: 0.000237385801
Iter: 870 loss: 0.000236452019
Iter: 871 loss: 0.000236776323
Iter: 872 loss: 0.000235800384
Iter: 873 loss: 0.000235057494
Iter: 874 loss: 0.00023850755
Iter: 875 loss: 0.000234909865
Iter: 876 loss: 0.000234125648
Iter: 877 loss: 0.00023337832
Iter: 878 loss: 0.000233200772
Iter: 879 loss: 0.000232012069
Iter: 880 loss: 0.000240744092
Iter: 881 loss: 0.000231909973
Iter: 882 loss: 0.000231031037
Iter: 883 loss: 0.00023061884
Iter: 884 loss: 0.000230184931
Iter: 885 loss: 0.000229052792
Iter: 886 loss: 0.000238079898
Iter: 887 loss: 0.0002289767
Iter: 888 loss: 0.000227958153
Iter: 889 loss: 0.000229283192
Iter: 890 loss: 0.000227438461
Iter: 891 loss: 0.000226423261
Iter: 892 loss: 0.000229097641
Iter: 893 loss: 0.000226081174
Iter: 894 loss: 0.00022501513
Iter: 895 loss: 0.000227396085
Iter: 896 loss: 0.000224612566
Iter: 897 loss: 0.000223542389
Iter: 898 loss: 0.000229322119
Iter: 899 loss: 0.000223377589
Iter: 900 loss: 0.000222505827
Iter: 901 loss: 0.000226846736
Iter: 902 loss: 0.000222359391
Iter: 903 loss: 0.000221703405
Iter: 904 loss: 0.000229812882
Iter: 905 loss: 0.000221692666
Iter: 906 loss: 0.000221182461
Iter: 907 loss: 0.000220517453
Iter: 908 loss: 0.000220471964
Iter: 909 loss: 0.000219529597
Iter: 910 loss: 0.000223917363
Iter: 911 loss: 0.000219356909
Iter: 912 loss: 0.000218615227
Iter: 913 loss: 0.000220789836
Iter: 914 loss: 0.000218387955
Iter: 915 loss: 0.000217539113
Iter: 916 loss: 0.000218371977
Iter: 917 loss: 0.000217057663
Iter: 918 loss: 0.000216205372
Iter: 919 loss: 0.000217315755
Iter: 920 loss: 0.000215771826
Iter: 921 loss: 0.000214669592
Iter: 922 loss: 0.000217527195
Iter: 923 loss: 0.00021429811
Iter: 924 loss: 0.000213273801
Iter: 925 loss: 0.000218231056
Iter: 926 loss: 0.000213093634
Iter: 927 loss: 0.00021209805
Iter: 928 loss: 0.000212410407
Iter: 929 loss: 0.000211382649
Iter: 930 loss: 0.000210419385
Iter: 931 loss: 0.000217340435
Iter: 932 loss: 0.000210336904
Iter: 933 loss: 0.000209485821
Iter: 934 loss: 0.000211707898
Iter: 935 loss: 0.000209198799
Iter: 936 loss: 0.00020837487
Iter: 937 loss: 0.000216145156
Iter: 938 loss: 0.000208343336
Iter: 939 loss: 0.000207678808
Iter: 940 loss: 0.000210379425
Iter: 941 loss: 0.000207533682
Iter: 942 loss: 0.000207011501
Iter: 943 loss: 0.000206354976
Iter: 944 loss: 0.000206304627
Iter: 945 loss: 0.000205511722
Iter: 946 loss: 0.000210989718
Iter: 947 loss: 0.000205433695
Iter: 948 loss: 0.000204719399
Iter: 949 loss: 0.000206113764
Iter: 950 loss: 0.000204421667
Iter: 951 loss: 0.000203655945
Iter: 952 loss: 0.000204697368
Iter: 953 loss: 0.000203270291
Iter: 954 loss: 0.000202386029
Iter: 955 loss: 0.000204168362
Iter: 956 loss: 0.000202024909
Iter: 957 loss: 0.000201173782
Iter: 958 loss: 0.000203937467
Iter: 959 loss: 0.000200932467
Iter: 960 loss: 0.00020021564
Iter: 961 loss: 0.000203365169
Iter: 962 loss: 0.000200071576
Iter: 963 loss: 0.000199305927
Iter: 964 loss: 0.00019981325
Iter: 965 loss: 0.000198827562
Iter: 966 loss: 0.000197960966
Iter: 967 loss: 0.000200637442
Iter: 968 loss: 0.000197703586
Iter: 969 loss: 0.000197071422
Iter: 970 loss: 0.000205487275
Iter: 971 loss: 0.000197070811
Iter: 972 loss: 0.000196521927
Iter: 973 loss: 0.000198697089
Iter: 974 loss: 0.000196397799
Iter: 975 loss: 0.000195826011
Iter: 976 loss: 0.000196283858
Iter: 977 loss: 0.000195481494
Iter: 978 loss: 0.000194961627
Iter: 979 loss: 0.000195378307
Iter: 980 loss: 0.000194648455
Iter: 981 loss: 0.000193975226
Iter: 982 loss: 0.000194754568
Iter: 983 loss: 0.000193615648
Iter: 984 loss: 0.000192801075
Iter: 985 loss: 0.000198672526
Iter: 986 loss: 0.000192734558
Iter: 987 loss: 0.00019217658
Iter: 988 loss: 0.000191778032
Iter: 989 loss: 0.000191585976
Iter: 990 loss: 0.000190714753
Iter: 991 loss: 0.000195760818
Iter: 992 loss: 0.00019059956
Iter: 993 loss: 0.000189835584
Iter: 994 loss: 0.00018974187
Iter: 995 loss: 0.000189194558
Iter: 996 loss: 0.00018832965
Iter: 997 loss: 0.000196702182
Iter: 998 loss: 0.00018829653
Iter: 999 loss: 0.000187603946
Iter: 1000 loss: 0.000187764352
Iter: 1001 loss: 0.000187097699
Iter: 1002 loss: 0.00018625542
Iter: 1003 loss: 0.000191229105
Iter: 1004 loss: 0.000186149759
Iter: 1005 loss: 0.00018550262
Iter: 1006 loss: 0.000187724145
Iter: 1007 loss: 0.000185328012
Iter: 1008 loss: 0.000184595454
Iter: 1009 loss: 0.000191545521
Iter: 1010 loss: 0.000184565899
Iter: 1011 loss: 0.000184178818
Iter: 1012 loss: 0.000184080505
Iter: 1013 loss: 0.000183836382
Iter: 1014 loss: 0.000183227923
Iter: 1015 loss: 0.000183752811
Iter: 1016 loss: 0.000182867312
Iter: 1017 loss: 0.000182165895
Iter: 1018 loss: 0.000184356104
Iter: 1019 loss: 0.000181961048
Iter: 1020 loss: 0.000181382115
Iter: 1021 loss: 0.000184202188
Iter: 1022 loss: 0.000181282754
Iter: 1023 loss: 0.000180615054
Iter: 1024 loss: 0.000180815841
Iter: 1025 loss: 0.000180135801
Iter: 1026 loss: 0.000179371855
Iter: 1027 loss: 0.000180872172
Iter: 1028 loss: 0.000179057795
Iter: 1029 loss: 0.000178185233
Iter: 1030 loss: 0.000180813455
Iter: 1031 loss: 0.000177922309
Iter: 1032 loss: 0.000177235852
Iter: 1033 loss: 0.000180550662
Iter: 1034 loss: 0.000177116162
Iter: 1035 loss: 0.0001764513
Iter: 1036 loss: 0.000176676433
Iter: 1037 loss: 0.000175980851
Iter: 1038 loss: 0.00017537878
Iter: 1039 loss: 0.000180775009
Iter: 1040 loss: 0.000175349
Iter: 1041 loss: 0.000174848814
Iter: 1042 loss: 0.000177478316
Iter: 1043 loss: 0.000174769317
Iter: 1044 loss: 0.000174176734
Iter: 1045 loss: 0.000176623987
Iter: 1046 loss: 0.0001740468
Iter: 1047 loss: 0.000173685228
Iter: 1048 loss: 0.000173454071
Iter: 1049 loss: 0.000173316192
Iter: 1050 loss: 0.000172688044
Iter: 1051 loss: 0.000174139728
Iter: 1052 loss: 0.000172451939
Iter: 1053 loss: 0.000171812135
Iter: 1054 loss: 0.000174588276
Iter: 1055 loss: 0.000171682012
Iter: 1056 loss: 0.000171133026
Iter: 1057 loss: 0.00017143166
Iter: 1058 loss: 0.000170775573
Iter: 1059 loss: 0.000170143467
Iter: 1060 loss: 0.000177359019
Iter: 1061 loss: 0.000170133077
Iter: 1062 loss: 0.000169748149
Iter: 1063 loss: 0.000169303617
Iter: 1064 loss: 0.000169254956
Iter: 1065 loss: 0.000168613507
Iter: 1066 loss: 0.000172046683
Iter: 1067 loss: 0.000168514787
Iter: 1068 loss: 0.000167954655
Iter: 1069 loss: 0.000169305582
Iter: 1070 loss: 0.000167754537
Iter: 1071 loss: 0.000167155173
Iter: 1072 loss: 0.000168027706
Iter: 1073 loss: 0.000166867263
Iter: 1074 loss: 0.000166224752
Iter: 1075 loss: 0.000167988677
Iter: 1076 loss: 0.000166015088
Iter: 1077 loss: 0.000165773614
Iter: 1078 loss: 0.000165671212
Iter: 1079 loss: 0.000165388105
Iter: 1080 loss: 0.000165048274
Iter: 1081 loss: 0.000165017467
Iter: 1082 loss: 0.000164458499
Iter: 1083 loss: 0.000164724086
Iter: 1084 loss: 0.00016408597
Iter: 1085 loss: 0.000163407909
Iter: 1086 loss: 0.00016708365
Iter: 1087 loss: 0.000163311051
Iter: 1088 loss: 0.000162682642
Iter: 1089 loss: 0.00016463612
Iter: 1090 loss: 0.000162501179
Iter: 1091 loss: 0.000161911797
Iter: 1092 loss: 0.000162428711
Iter: 1093 loss: 0.000161566248
Iter: 1094 loss: 0.000160820637
Iter: 1095 loss: 0.000165414895
Iter: 1096 loss: 0.000160729891
Iter: 1097 loss: 0.000160237847
Iter: 1098 loss: 0.00016102157
Iter: 1099 loss: 0.000160008727
Iter: 1100 loss: 0.000159380521
Iter: 1101 loss: 0.000159330957
Iter: 1102 loss: 0.000158864597
Iter: 1103 loss: 0.000158132927
Iter: 1104 loss: 0.000162442477
Iter: 1105 loss: 0.000158041134
Iter: 1106 loss: 0.000157386865
Iter: 1107 loss: 0.000159085845
Iter: 1108 loss: 0.000157164686
Iter: 1109 loss: 0.000156540365
Iter: 1110 loss: 0.000158630151
Iter: 1111 loss: 0.000156367052
Iter: 1112 loss: 0.000155890739
Iter: 1113 loss: 0.00015929548
Iter: 1114 loss: 0.000155851711
Iter: 1115 loss: 0.000155269634
Iter: 1116 loss: 0.000157408736
Iter: 1117 loss: 0.000155122587
Iter: 1118 loss: 0.000154771056
Iter: 1119 loss: 0.000154327514
Iter: 1120 loss: 0.00015429355
Iter: 1121 loss: 0.000153512839
Iter: 1122 loss: 0.00015720693
Iter: 1123 loss: 0.000153370929
Iter: 1124 loss: 0.00015279428
Iter: 1125 loss: 0.000156207185
Iter: 1126 loss: 0.000152724155
Iter: 1127 loss: 0.00015220145
Iter: 1128 loss: 0.0001529211
Iter: 1129 loss: 0.000151935732
Iter: 1130 loss: 0.000151368615
Iter: 1131 loss: 0.000154936148
Iter: 1132 loss: 0.000151300977
Iter: 1133 loss: 0.000150845503
Iter: 1134 loss: 0.000150975728
Iter: 1135 loss: 0.000150518783
Iter: 1136 loss: 0.000149876112
Iter: 1137 loss: 0.000152235065
Iter: 1138 loss: 0.00014971396
Iter: 1139 loss: 0.000149199317
Iter: 1140 loss: 0.000149490457
Iter: 1141 loss: 0.000148863764
Iter: 1142 loss: 0.000148176856
Iter: 1143 loss: 0.000149121406
Iter: 1144 loss: 0.000147832689
Iter: 1145 loss: 0.000147251048
Iter: 1146 loss: 0.000151586602
Iter: 1147 loss: 0.000147202925
Iter: 1148 loss: 0.000146697101
Iter: 1149 loss: 0.000148523948
Iter: 1150 loss: 0.000146570586
Iter: 1151 loss: 0.00014628534
Iter: 1152 loss: 0.000146260922
Iter: 1153 loss: 0.000146026228
Iter: 1154 loss: 0.000145421916
Iter: 1155 loss: 0.000150771462
Iter: 1156 loss: 0.000145324855
Iter: 1157 loss: 0.00014455698
Iter: 1158 loss: 0.000148553154
Iter: 1159 loss: 0.000144437159
Iter: 1160 loss: 0.000143827318
Iter: 1161 loss: 0.000144939651
Iter: 1162 loss: 0.000143562662
Iter: 1163 loss: 0.000142854726
Iter: 1164 loss: 0.000147782863
Iter: 1165 loss: 0.000142789169
Iter: 1166 loss: 0.000142331657
Iter: 1167 loss: 0.000143618585
Iter: 1168 loss: 0.000142184377
Iter: 1169 loss: 0.000141689277
Iter: 1170 loss: 0.000142911289
Iter: 1171 loss: 0.000141508863
Iter: 1172 loss: 0.000141052238
Iter: 1173 loss: 0.000142987381
Iter: 1174 loss: 0.000140958888
Iter: 1175 loss: 0.000140506832
Iter: 1176 loss: 0.000140603661
Iter: 1177 loss: 0.000140173695
Iter: 1178 loss: 0.000139558673
Iter: 1179 loss: 0.000140954216
Iter: 1180 loss: 0.00013932852
Iter: 1181 loss: 0.000138759817
Iter: 1182 loss: 0.0001402091
Iter: 1183 loss: 0.000138568823
Iter: 1184 loss: 0.000138047835
Iter: 1185 loss: 0.000141009848
Iter: 1186 loss: 0.000137976167
Iter: 1187 loss: 0.000137520037
Iter: 1188 loss: 0.000144137666
Iter: 1189 loss: 0.000137517403
Iter: 1190 loss: 0.000137253286
Iter: 1191 loss: 0.000136834366
Iter: 1192 loss: 0.000136831572
Iter: 1193 loss: 0.000136326387
Iter: 1194 loss: 0.000137135765
Iter: 1195 loss: 0.000136089337
Iter: 1196 loss: 0.000135562688
Iter: 1197 loss: 0.000137887138
Iter: 1198 loss: 0.000135453782
Iter: 1199 loss: 0.000134960603
Iter: 1200 loss: 0.000135181253
Iter: 1201 loss: 0.00013462521
Iter: 1202 loss: 0.000134059286
Iter: 1203 loss: 0.000138845426
Iter: 1204 loss: 0.00013402733
Iter: 1205 loss: 0.000133548951
Iter: 1206 loss: 0.000133768233
Iter: 1207 loss: 0.000133228343
Iter: 1208 loss: 0.000132789719
Iter: 1209 loss: 0.000139446347
Iter: 1210 loss: 0.000132791261
Iter: 1211 loss: 0.00013249219
Iter: 1212 loss: 0.000132383182
Iter: 1213 loss: 0.000132218876
Iter: 1214 loss: 0.00013172008
Iter: 1215 loss: 0.000132525485
Iter: 1216 loss: 0.000131488952
Iter: 1217 loss: 0.000131007691
Iter: 1218 loss: 0.000134133792
Iter: 1219 loss: 0.0001309574
Iter: 1220 loss: 0.000130537228
Iter: 1221 loss: 0.000130202068
Iter: 1222 loss: 0.00013007308
Iter: 1223 loss: 0.00012959569
Iter: 1224 loss: 0.00013401336
Iter: 1225 loss: 0.000129575812
Iter: 1226 loss: 0.000129218097
Iter: 1227 loss: 0.000134497852
Iter: 1228 loss: 0.000129218883
Iter: 1229 loss: 0.000128924032
Iter: 1230 loss: 0.000128590895
Iter: 1231 loss: 0.000128548199
Iter: 1232 loss: 0.000128160231
Iter: 1233 loss: 0.000128058586
Iter: 1234 loss: 0.000127820153
Iter: 1235 loss: 0.000127166801
Iter: 1236 loss: 0.000129831882
Iter: 1237 loss: 0.000127021864
Iter: 1238 loss: 0.000126486324
Iter: 1239 loss: 0.000128505417
Iter: 1240 loss: 0.000126358893
Iter: 1241 loss: 0.00012583677
Iter: 1242 loss: 0.000127591818
Iter: 1243 loss: 0.000125694423
Iter: 1244 loss: 0.000125207
Iter: 1245 loss: 0.000125921797
Iter: 1246 loss: 0.000124972168
Iter: 1247 loss: 0.000124406215
Iter: 1248 loss: 0.000126882805
Iter: 1249 loss: 0.000124291095
Iter: 1250 loss: 0.000123902573
Iter: 1251 loss: 0.000126943123
Iter: 1252 loss: 0.000123878985
Iter: 1253 loss: 0.000123537116
Iter: 1254 loss: 0.000123725084
Iter: 1255 loss: 0.000123316451
Iter: 1256 loss: 0.000122919038
Iter: 1257 loss: 0.000124011509
Iter: 1258 loss: 0.000122789657
Iter: 1259 loss: 0.000122443438
Iter: 1260 loss: 0.000124278144
Iter: 1261 loss: 0.00012239044
Iter: 1262 loss: 0.000122083467
Iter: 1263 loss: 0.000124782076
Iter: 1264 loss: 0.000122066747
Iter: 1265 loss: 0.000121813559
Iter: 1266 loss: 0.000121782476
Iter: 1267 loss: 0.000121603167
Iter: 1268 loss: 0.000121264
Iter: 1269 loss: 0.000121289529
Iter: 1270 loss: 0.00012099649
Iter: 1271 loss: 0.000120542667
Iter: 1272 loss: 0.000122535072
Iter: 1273 loss: 0.000120451892
Iter: 1274 loss: 0.000120045341
Iter: 1275 loss: 0.000120509372
Iter: 1276 loss: 0.00011982835
Iter: 1277 loss: 0.000119340897
Iter: 1278 loss: 0.000121400866
Iter: 1279 loss: 0.000119241842
Iter: 1280 loss: 0.000118841337
Iter: 1281 loss: 0.00012068222
Iter: 1282 loss: 0.000118764226
Iter: 1283 loss: 0.000118372
Iter: 1284 loss: 0.000118973432
Iter: 1285 loss: 0.000118185548
Iter: 1286 loss: 0.000117781725
Iter: 1287 loss: 0.000118824049
Iter: 1288 loss: 0.000117642834
Iter: 1289 loss: 0.000117237483
Iter: 1290 loss: 0.000119323406
Iter: 1291 loss: 0.000117171236
Iter: 1292 loss: 0.000116812924
Iter: 1293 loss: 0.000117563155
Iter: 1294 loss: 0.00011667428
Iter: 1295 loss: 0.000116341609
Iter: 1296 loss: 0.000118631004
Iter: 1297 loss: 0.000116309624
Iter: 1298 loss: 0.000116032614
Iter: 1299 loss: 0.000118628901
Iter: 1300 loss: 0.000116021969
Iter: 1301 loss: 0.00011579592
Iter: 1302 loss: 0.000115605224
Iter: 1303 loss: 0.000115541145
Iter: 1304 loss: 0.000115205337
Iter: 1305 loss: 0.000115697767
Iter: 1306 loss: 0.000115044306
Iter: 1307 loss: 0.000114671137
Iter: 1308 loss: 0.000115660427
Iter: 1309 loss: 0.000114544695
Iter: 1310 loss: 0.000114183029
Iter: 1311 loss: 0.000115202012
Iter: 1312 loss: 0.000114066206
Iter: 1313 loss: 0.000113706941
Iter: 1314 loss: 0.000113529684
Iter: 1315 loss: 0.000113357397
Iter: 1316 loss: 0.00011290126
Iter: 1317 loss: 0.000116420255
Iter: 1318 loss: 0.000112866488
Iter: 1319 loss: 0.000112472509
Iter: 1320 loss: 0.000113090806
Iter: 1321 loss: 0.000112287169
Iter: 1322 loss: 0.000111932488
Iter: 1323 loss: 0.000114725124
Iter: 1324 loss: 0.000111909758
Iter: 1325 loss: 0.000111573834
Iter: 1326 loss: 0.000111334484
Iter: 1327 loss: 0.000111215588
Iter: 1328 loss: 0.000110823567
Iter: 1329 loss: 0.000116845353
Iter: 1330 loss: 0.000110822752
Iter: 1331 loss: 0.000110538676
Iter: 1332 loss: 0.000110468318
Iter: 1333 loss: 0.000110289344
Iter: 1334 loss: 0.000110080546
Iter: 1335 loss: 0.000110034467
Iter: 1336 loss: 0.000109839035
Iter: 1337 loss: 0.000109927721
Iter: 1338 loss: 0.000109707646
Iter: 1339 loss: 0.000109453453
Iter: 1340 loss: 0.000109130415
Iter: 1341 loss: 0.000109107299
Iter: 1342 loss: 0.000108714936
Iter: 1343 loss: 0.000111884932
Iter: 1344 loss: 0.000108689892
Iter: 1345 loss: 0.000108390843
Iter: 1346 loss: 0.000109171837
Iter: 1347 loss: 0.000108287197
Iter: 1348 loss: 0.000107984
Iter: 1349 loss: 0.000108541848
Iter: 1350 loss: 0.000107853491
Iter: 1351 loss: 0.000107514614
Iter: 1352 loss: 0.000107831547
Iter: 1353 loss: 0.000107320986
Iter: 1354 loss: 0.00010693779
Iter: 1355 loss: 0.000107626911
Iter: 1356 loss: 0.000106767722
Iter: 1357 loss: 0.000106399792
Iter: 1358 loss: 0.000109892942
Iter: 1359 loss: 0.000106382839
Iter: 1360 loss: 0.000106097636
Iter: 1361 loss: 0.000106339648
Iter: 1362 loss: 0.000105929503
Iter: 1363 loss: 0.000105564
Iter: 1364 loss: 0.000107000771
Iter: 1365 loss: 0.000105484738
Iter: 1366 loss: 0.000105196341
Iter: 1367 loss: 0.000107188956
Iter: 1368 loss: 0.000105168961
Iter: 1369 loss: 0.000104930979
Iter: 1370 loss: 0.000107425483
Iter: 1371 loss: 0.000104924889
Iter: 1372 loss: 0.000104736973
Iter: 1373 loss: 0.000104497813
Iter: 1374 loss: 0.000104481594
Iter: 1375 loss: 0.000104188657
Iter: 1376 loss: 0.000104619328
Iter: 1377 loss: 0.000104047722
Iter: 1378 loss: 0.000103709011
Iter: 1379 loss: 0.000104873594
Iter: 1380 loss: 0.000103622951
Iter: 1381 loss: 0.000103351893
Iter: 1382 loss: 0.000104030376
Iter: 1383 loss: 0.000103257771
Iter: 1384 loss: 0.00010291682
Iter: 1385 loss: 0.000103497223
Iter: 1386 loss: 0.000102763101
Iter: 1387 loss: 0.000102481681
Iter: 1388 loss: 0.000103307335
Iter: 1389 loss: 0.000102391554
Iter: 1390 loss: 0.000102067897
Iter: 1391 loss: 0.000102200138
Iter: 1392 loss: 0.000101840778
Iter: 1393 loss: 0.000101520855
Iter: 1394 loss: 0.000102951497
Iter: 1395 loss: 0.000101456651
Iter: 1396 loss: 0.000101102967
Iter: 1397 loss: 0.000102104772
Iter: 1398 loss: 0.000100988778
Iter: 1399 loss: 0.000100688267
Iter: 1400 loss: 0.000102059705
Iter: 1401 loss: 0.000100628473
Iter: 1402 loss: 0.000100356265
Iter: 1403 loss: 0.000101896047
Iter: 1404 loss: 0.000100318081
Iter: 1405 loss: 0.000100027566
Iter: 1406 loss: 0.000101860343
Iter: 1407 loss: 9.99933545e-05
Iter: 1408 loss: 9.98505275e-05
Iter: 1409 loss: 9.97403404e-05
Iter: 1410 loss: 9.9696219e-05
Iter: 1411 loss: 9.94279399e-05
Iter: 1412 loss: 9.95008158e-05
Iter: 1413 loss: 9.92363348e-05
Iter: 1414 loss: 9.89349574e-05
Iter: 1415 loss: 0.00010097104
Iter: 1416 loss: 9.8904653e-05
Iter: 1417 loss: 9.86358209e-05
Iter: 1418 loss: 9.88672691e-05
Iter: 1419 loss: 9.84747167e-05
Iter: 1420 loss: 9.81457e-05
Iter: 1421 loss: 9.94433794e-05
Iter: 1422 loss: 9.80683399e-05
Iter: 1423 loss: 9.78015742e-05
Iter: 1424 loss: 9.83384e-05
Iter: 1425 loss: 9.76916344e-05
Iter: 1426 loss: 9.73750721e-05
Iter: 1427 loss: 9.76398369e-05
Iter: 1428 loss: 9.71837871e-05
Iter: 1429 loss: 9.6857e-05
Iter: 1430 loss: 9.81029661e-05
Iter: 1431 loss: 9.67800806e-05
Iter: 1432 loss: 9.64723149e-05
Iter: 1433 loss: 9.69643734e-05
Iter: 1434 loss: 9.63273924e-05
Iter: 1435 loss: 9.60257603e-05
Iter: 1436 loss: 9.87095846e-05
Iter: 1437 loss: 9.60118632e-05
Iter: 1438 loss: 9.58352612e-05
Iter: 1439 loss: 9.58354794e-05
Iter: 1440 loss: 9.5665644e-05
Iter: 1441 loss: 9.57757438e-05
Iter: 1442 loss: 9.5557858e-05
Iter: 1443 loss: 9.53897106e-05
Iter: 1444 loss: 9.52319315e-05
Iter: 1445 loss: 9.51939728e-05
Iter: 1446 loss: 9.49095556e-05
Iter: 1447 loss: 9.59417e-05
Iter: 1448 loss: 9.48420493e-05
Iter: 1449 loss: 9.45679785e-05
Iter: 1450 loss: 9.53722629e-05
Iter: 1451 loss: 9.44822605e-05
Iter: 1452 loss: 9.42247e-05
Iter: 1453 loss: 9.51440306e-05
Iter: 1454 loss: 9.41585895e-05
Iter: 1455 loss: 9.39090241e-05
Iter: 1456 loss: 9.4471412e-05
Iter: 1457 loss: 9.38143785e-05
Iter: 1458 loss: 9.35657445e-05
Iter: 1459 loss: 9.39722522e-05
Iter: 1460 loss: 9.3452385e-05
Iter: 1461 loss: 9.31697432e-05
Iter: 1462 loss: 9.38857e-05
Iter: 1463 loss: 9.30715178e-05
Iter: 1464 loss: 9.27796063e-05
Iter: 1465 loss: 9.32001567e-05
Iter: 1466 loss: 9.26326466e-05
Iter: 1467 loss: 9.234545e-05
Iter: 1468 loss: 9.28857771e-05
Iter: 1469 loss: 9.22243489e-05
Iter: 1470 loss: 9.19455488e-05
Iter: 1471 loss: 9.47319058e-05
Iter: 1472 loss: 9.1937829e-05
Iter: 1473 loss: 9.18275691e-05
Iter: 1474 loss: 9.18127043e-05
Iter: 1475 loss: 9.17072903e-05
Iter: 1476 loss: 9.14664415e-05
Iter: 1477 loss: 9.48574234e-05
Iter: 1478 loss: 9.14534612e-05
Iter: 1479 loss: 9.11946263e-05
Iter: 1480 loss: 9.25631757e-05
Iter: 1481 loss: 9.11541283e-05
Iter: 1482 loss: 9.09504161e-05
Iter: 1483 loss: 9.08724251e-05
Iter: 1484 loss: 9.07613139e-05
Iter: 1485 loss: 9.0510759e-05
Iter: 1486 loss: 9.38866579e-05
Iter: 1487 loss: 9.05092675e-05
Iter: 1488 loss: 9.03374e-05
Iter: 1489 loss: 9.01789463e-05
Iter: 1490 loss: 9.01374369e-05
Iter: 1491 loss: 8.9871166e-05
Iter: 1492 loss: 9.29890157e-05
Iter: 1493 loss: 8.98657745e-05
Iter: 1494 loss: 8.9684574e-05
Iter: 1495 loss: 8.96317724e-05
Iter: 1496 loss: 8.95206686e-05
Iter: 1497 loss: 8.92550452e-05
Iter: 1498 loss: 9.02814281e-05
Iter: 1499 loss: 8.91927266e-05
Iter: 1500 loss: 8.89378134e-05
Iter: 1501 loss: 8.91043746e-05
Iter: 1502 loss: 8.87792557e-05
Iter: 1503 loss: 8.84433466e-05
Iter: 1504 loss: 9.05046109e-05
Iter: 1505 loss: 8.84040928e-05
Iter: 1506 loss: 8.82067179e-05
Iter: 1507 loss: 8.92594253e-05
Iter: 1508 loss: 8.81788292e-05
Iter: 1509 loss: 8.80132211e-05
Iter: 1510 loss: 9.06234054e-05
Iter: 1511 loss: 8.80119915e-05
Iter: 1512 loss: 8.79092258e-05
Iter: 1513 loss: 8.77540151e-05
Iter: 1514 loss: 8.77532293e-05
Iter: 1515 loss: 8.75937112e-05
Iter: 1516 loss: 8.77384809e-05
Iter: 1517 loss: 8.75037949e-05
Iter: 1518 loss: 8.72374585e-05
Iter: 1519 loss: 8.74393445e-05
Iter: 1520 loss: 8.70770673e-05
Iter: 1521 loss: 8.68103743e-05
Iter: 1522 loss: 8.74503312e-05
Iter: 1523 loss: 8.67154158e-05
Iter: 1524 loss: 8.64682661e-05
Iter: 1525 loss: 8.87657952e-05
Iter: 1526 loss: 8.64582253e-05
Iter: 1527 loss: 8.62673041e-05
Iter: 1528 loss: 8.6428583e-05
Iter: 1529 loss: 8.6155982e-05
Iter: 1530 loss: 8.59480206e-05
Iter: 1531 loss: 8.66415e-05
Iter: 1532 loss: 8.58926724e-05
Iter: 1533 loss: 8.56435945e-05
Iter: 1534 loss: 8.56775223e-05
Iter: 1535 loss: 8.54558239e-05
Iter: 1536 loss: 8.52323064e-05
Iter: 1537 loss: 8.6988628e-05
Iter: 1538 loss: 8.5216263e-05
Iter: 1539 loss: 8.50245196e-05
Iter: 1540 loss: 8.5023974e-05
Iter: 1541 loss: 8.48715572e-05
Iter: 1542 loss: 8.46778421e-05
Iter: 1543 loss: 8.46774346e-05
Iter: 1544 loss: 8.45274189e-05
Iter: 1545 loss: 8.6069027e-05
Iter: 1546 loss: 8.45240138e-05
Iter: 1547 loss: 8.44465249e-05
Iter: 1548 loss: 8.42411391e-05
Iter: 1549 loss: 8.5822925e-05
Iter: 1550 loss: 8.42032314e-05
Iter: 1551 loss: 8.39547138e-05
Iter: 1552 loss: 8.54721584e-05
Iter: 1553 loss: 8.39244385e-05
Iter: 1554 loss: 8.37195112e-05
Iter: 1555 loss: 8.43527087e-05
Iter: 1556 loss: 8.36599793e-05
Iter: 1557 loss: 8.34401144e-05
Iter: 1558 loss: 8.36145264e-05
Iter: 1559 loss: 8.33063896e-05
Iter: 1560 loss: 8.30589561e-05
Iter: 1561 loss: 8.40609719e-05
Iter: 1562 loss: 8.30045756e-05
Iter: 1563 loss: 8.27633849e-05
Iter: 1564 loss: 8.3890649e-05
Iter: 1565 loss: 8.27161784e-05
Iter: 1566 loss: 8.25273455e-05
Iter: 1567 loss: 8.27753247e-05
Iter: 1568 loss: 8.24320159e-05
Iter: 1569 loss: 8.22217553e-05
Iter: 1570 loss: 8.26894247e-05
Iter: 1571 loss: 8.21423455e-05
Iter: 1572 loss: 8.19465931e-05
Iter: 1573 loss: 8.29993733e-05
Iter: 1574 loss: 8.19184352e-05
Iter: 1575 loss: 8.17507098e-05
Iter: 1576 loss: 8.17022374e-05
Iter: 1577 loss: 8.16019e-05
Iter: 1578 loss: 8.15546955e-05
Iter: 1579 loss: 8.14822e-05
Iter: 1580 loss: 8.1371436e-05
Iter: 1581 loss: 8.12583894e-05
Iter: 1582 loss: 8.12399812e-05
Iter: 1583 loss: 8.10791898e-05
Iter: 1584 loss: 8.08721525e-05
Iter: 1585 loss: 8.08582845e-05
Iter: 1586 loss: 8.06086e-05
Iter: 1587 loss: 8.24083836e-05
Iter: 1588 loss: 8.05872696e-05
Iter: 1589 loss: 8.03790463e-05
Iter: 1590 loss: 8.10047422e-05
Iter: 1591 loss: 8.03145376e-05
Iter: 1592 loss: 8.01203205e-05
Iter: 1593 loss: 8.09653429e-05
Iter: 1594 loss: 8.0079124e-05
Iter: 1595 loss: 7.9938196e-05
Iter: 1596 loss: 7.99715635e-05
Iter: 1597 loss: 7.98281399e-05
Iter: 1598 loss: 7.958887e-05
Iter: 1599 loss: 8.07583128e-05
Iter: 1600 loss: 7.95453e-05
Iter: 1601 loss: 7.93733e-05
Iter: 1602 loss: 7.9614525e-05
Iter: 1603 loss: 7.92885257e-05
Iter: 1604 loss: 7.90736813e-05
Iter: 1605 loss: 7.94053922e-05
Iter: 1606 loss: 7.89762635e-05
Iter: 1607 loss: 7.87863828e-05
Iter: 1608 loss: 7.9726844e-05
Iter: 1609 loss: 7.87535901e-05
Iter: 1610 loss: 7.85718585e-05
Iter: 1611 loss: 7.91405473e-05
Iter: 1612 loss: 7.85203156e-05
Iter: 1613 loss: 7.83392e-05
Iter: 1614 loss: 8.08365e-05
Iter: 1615 loss: 7.83385549e-05
Iter: 1616 loss: 7.82501811e-05
Iter: 1617 loss: 7.80697519e-05
Iter: 1618 loss: 8.12321814e-05
Iter: 1619 loss: 7.80667397e-05
Iter: 1620 loss: 7.78656831e-05
Iter: 1621 loss: 7.81095441e-05
Iter: 1622 loss: 7.7761506e-05
Iter: 1623 loss: 7.75451e-05
Iter: 1624 loss: 7.86456221e-05
Iter: 1625 loss: 7.75117514e-05
Iter: 1626 loss: 7.73327265e-05
Iter: 1627 loss: 7.79939801e-05
Iter: 1628 loss: 7.72853382e-05
Iter: 1629 loss: 7.71035557e-05
Iter: 1630 loss: 7.74191431e-05
Iter: 1631 loss: 7.70231272e-05
Iter: 1632 loss: 7.6839111e-05
Iter: 1633 loss: 7.74483487e-05
Iter: 1634 loss: 7.67870879e-05
Iter: 1635 loss: 7.65956938e-05
Iter: 1636 loss: 7.73407883e-05
Iter: 1637 loss: 7.65506556e-05
Iter: 1638 loss: 7.63805147e-05
Iter: 1639 loss: 7.64814176e-05
Iter: 1640 loss: 7.62691925e-05
Iter: 1641 loss: 7.60950861e-05
Iter: 1642 loss: 7.67521778e-05
Iter: 1643 loss: 7.60537077e-05
Iter: 1644 loss: 7.58515307e-05
Iter: 1645 loss: 7.61313786e-05
Iter: 1646 loss: 7.57512753e-05
Iter: 1647 loss: 7.57564776e-05
Iter: 1648 loss: 7.56620575e-05
Iter: 1649 loss: 7.55939836e-05
Iter: 1650 loss: 7.54533e-05
Iter: 1651 loss: 7.78114045e-05
Iter: 1652 loss: 7.54476932e-05
Iter: 1653 loss: 7.52752312e-05
Iter: 1654 loss: 7.5270138e-05
Iter: 1655 loss: 7.51335901e-05
Iter: 1656 loss: 7.49455285e-05
Iter: 1657 loss: 7.57563175e-05
Iter: 1658 loss: 7.49085e-05
Iter: 1659 loss: 7.4721509e-05
Iter: 1660 loss: 7.51690168e-05
Iter: 1661 loss: 7.46526857e-05
Iter: 1662 loss: 7.44655481e-05
Iter: 1663 loss: 7.54362e-05
Iter: 1664 loss: 7.44339341e-05
Iter: 1665 loss: 7.42743796e-05
Iter: 1666 loss: 7.44404242e-05
Iter: 1667 loss: 7.41859039e-05
Iter: 1668 loss: 7.39994066e-05
Iter: 1669 loss: 7.50525069e-05
Iter: 1670 loss: 7.39742245e-05
Iter: 1671 loss: 7.3828458e-05
Iter: 1672 loss: 7.42503908e-05
Iter: 1673 loss: 7.3783609e-05
Iter: 1674 loss: 7.36312286e-05
Iter: 1675 loss: 7.35610083e-05
Iter: 1676 loss: 7.34832138e-05
Iter: 1677 loss: 7.32926055e-05
Iter: 1678 loss: 7.4580108e-05
Iter: 1679 loss: 7.32724147e-05
Iter: 1680 loss: 7.31272e-05
Iter: 1681 loss: 7.4275682e-05
Iter: 1682 loss: 7.31156833e-05
Iter: 1683 loss: 7.29536769e-05
Iter: 1684 loss: 7.36715738e-05
Iter: 1685 loss: 7.29230815e-05
Iter: 1686 loss: 7.28271843e-05
Iter: 1687 loss: 7.2677125e-05
Iter: 1688 loss: 7.26770813e-05
Iter: 1689 loss: 7.24915444e-05
Iter: 1690 loss: 7.2743489e-05
Iter: 1691 loss: 7.24051206e-05
Iter: 1692 loss: 7.22156474e-05
Iter: 1693 loss: 7.31276159e-05
Iter: 1694 loss: 7.21842807e-05
Iter: 1695 loss: 7.20138341e-05
Iter: 1696 loss: 7.24738275e-05
Iter: 1697 loss: 7.19574746e-05
Iter: 1698 loss: 7.17859657e-05
Iter: 1699 loss: 7.25279679e-05
Iter: 1700 loss: 7.17519142e-05
Iter: 1701 loss: 7.16014e-05
Iter: 1702 loss: 7.18108931e-05
Iter: 1703 loss: 7.15249844e-05
Iter: 1704 loss: 7.13669651e-05
Iter: 1705 loss: 7.24225392e-05
Iter: 1706 loss: 7.13514237e-05
Iter: 1707 loss: 7.1225033e-05
Iter: 1708 loss: 7.12948095e-05
Iter: 1709 loss: 7.11454049e-05
Iter: 1710 loss: 7.09589222e-05
Iter: 1711 loss: 7.10312161e-05
Iter: 1712 loss: 7.08310836e-05
Iter: 1713 loss: 7.06782303e-05
Iter: 1714 loss: 7.24104466e-05
Iter: 1715 loss: 7.06744104e-05
Iter: 1716 loss: 7.05713173e-05
Iter: 1717 loss: 7.05718267e-05
Iter: 1718 loss: 7.04836275e-05
Iter: 1719 loss: 7.03278565e-05
Iter: 1720 loss: 7.03280239e-05
Iter: 1721 loss: 7.01930549e-05
Iter: 1722 loss: 7.02845718e-05
Iter: 1723 loss: 7.01091267e-05
Iter: 1724 loss: 6.9924281e-05
Iter: 1725 loss: 7.0375354e-05
Iter: 1726 loss: 6.98616786e-05
Iter: 1727 loss: 6.96827046e-05
Iter: 1728 loss: 7.02827674e-05
Iter: 1729 loss: 6.96323841e-05
Iter: 1730 loss: 6.94781629e-05
Iter: 1731 loss: 7.03473343e-05
Iter: 1732 loss: 6.94548e-05
Iter: 1733 loss: 6.93060429e-05
Iter: 1734 loss: 6.95717172e-05
Iter: 1735 loss: 6.92424655e-05
Iter: 1736 loss: 6.91048772e-05
Iter: 1737 loss: 6.99283191e-05
Iter: 1738 loss: 6.90897723e-05
Iter: 1739 loss: 6.89711669e-05
Iter: 1740 loss: 6.90407178e-05
Iter: 1741 loss: 6.88923465e-05
Iter: 1742 loss: 6.87394277e-05
Iter: 1743 loss: 6.91954265e-05
Iter: 1744 loss: 6.86889107e-05
Iter: 1745 loss: 6.85556588e-05
Iter: 1746 loss: 6.86421699e-05
Iter: 1747 loss: 6.84709958e-05
Iter: 1748 loss: 6.83581e-05
Iter: 1749 loss: 6.83555045e-05
Iter: 1750 loss: 6.82314712e-05
Iter: 1751 loss: 6.82955506e-05
Iter: 1752 loss: 6.81481324e-05
Iter: 1753 loss: 6.80541343e-05
Iter: 1754 loss: 6.80292869e-05
Iter: 1755 loss: 6.79711302e-05
Iter: 1756 loss: 6.78225042e-05
Iter: 1757 loss: 6.78917e-05
Iter: 1758 loss: 6.77237404e-05
Iter: 1759 loss: 6.75725896e-05
Iter: 1760 loss: 6.81638776e-05
Iter: 1761 loss: 6.75361371e-05
Iter: 1762 loss: 6.73724426e-05
Iter: 1763 loss: 6.79335644e-05
Iter: 1764 loss: 6.73265458e-05
Iter: 1765 loss: 6.71956295e-05
Iter: 1766 loss: 6.78707947e-05
Iter: 1767 loss: 6.71737216e-05
Iter: 1768 loss: 6.70393e-05
Iter: 1769 loss: 6.71781891e-05
Iter: 1770 loss: 6.69660658e-05
Iter: 1771 loss: 6.68257926e-05
Iter: 1772 loss: 6.76992786e-05
Iter: 1773 loss: 6.68103385e-05
Iter: 1774 loss: 6.67051863e-05
Iter: 1775 loss: 6.69070578e-05
Iter: 1776 loss: 6.66628112e-05
Iter: 1777 loss: 6.65391271e-05
Iter: 1778 loss: 6.64860054e-05
Iter: 1779 loss: 6.64222098e-05
Iter: 1780 loss: 6.63079554e-05
Iter: 1781 loss: 6.63091632e-05
Iter: 1782 loss: 6.62117454e-05
Iter: 1783 loss: 6.71367306e-05
Iter: 1784 loss: 6.62078382e-05
Iter: 1785 loss: 6.61421509e-05
Iter: 1786 loss: 6.60095538e-05
Iter: 1787 loss: 6.8347108e-05
Iter: 1788 loss: 6.60042861e-05
Iter: 1789 loss: 6.58689169e-05
Iter: 1790 loss: 6.63226092e-05
Iter: 1791 loss: 6.5832086e-05
Iter: 1792 loss: 6.57000055e-05
Iter: 1793 loss: 6.58551726e-05
Iter: 1794 loss: 6.56322372e-05
Iter: 1795 loss: 6.54948162e-05
Iter: 1796 loss: 6.61050362e-05
Iter: 1797 loss: 6.54705291e-05
Iter: 1798 loss: 6.53357856e-05
Iter: 1799 loss: 6.56080956e-05
Iter: 1800 loss: 6.52838e-05
Iter: 1801 loss: 6.51497321e-05
Iter: 1802 loss: 6.60938385e-05
Iter: 1803 loss: 6.51373266e-05
Iter: 1804 loss: 6.50378715e-05
Iter: 1805 loss: 6.51335577e-05
Iter: 1806 loss: 6.49818467e-05
Iter: 1807 loss: 6.48522036e-05
Iter: 1808 loss: 6.53875e-05
Iter: 1809 loss: 6.48231216e-05
Iter: 1810 loss: 6.47149864e-05
Iter: 1811 loss: 6.48722635e-05
Iter: 1812 loss: 6.46613407e-05
Iter: 1813 loss: 6.45288819e-05
Iter: 1814 loss: 6.45931286e-05
Iter: 1815 loss: 6.4437816e-05
Iter: 1816 loss: 6.44000247e-05
Iter: 1817 loss: 6.43700041e-05
Iter: 1818 loss: 6.42903178e-05
Iter: 1819 loss: 6.41828228e-05
Iter: 1820 loss: 6.41753722e-05
Iter: 1821 loss: 6.40677827e-05
Iter: 1822 loss: 6.41464649e-05
Iter: 1823 loss: 6.40039507e-05
Iter: 1824 loss: 6.38833e-05
Iter: 1825 loss: 6.40789513e-05
Iter: 1826 loss: 6.38318e-05
Iter: 1827 loss: 6.3672167e-05
Iter: 1828 loss: 6.40749786e-05
Iter: 1829 loss: 6.361743e-05
Iter: 1830 loss: 6.34815369e-05
Iter: 1831 loss: 6.35808537e-05
Iter: 1832 loss: 6.34004318e-05
Iter: 1833 loss: 6.32582596e-05
Iter: 1834 loss: 6.49318827e-05
Iter: 1835 loss: 6.32578e-05
Iter: 1836 loss: 6.31604489e-05
Iter: 1837 loss: 6.35163087e-05
Iter: 1838 loss: 6.31365692e-05
Iter: 1839 loss: 6.30447175e-05
Iter: 1840 loss: 6.31664443e-05
Iter: 1841 loss: 6.29980932e-05
Iter: 1842 loss: 6.28830894e-05
Iter: 1843 loss: 6.32045485e-05
Iter: 1844 loss: 6.28474663e-05
Iter: 1845 loss: 6.27397385e-05
Iter: 1846 loss: 6.29356582e-05
Iter: 1847 loss: 6.2691739e-05
Iter: 1848 loss: 6.25885514e-05
Iter: 1849 loss: 6.28905254e-05
Iter: 1850 loss: 6.25551911e-05
Iter: 1851 loss: 6.25284301e-05
Iter: 1852 loss: 6.25026369e-05
Iter: 1853 loss: 6.24653039e-05
Iter: 1854 loss: 6.23626e-05
Iter: 1855 loss: 6.30379e-05
Iter: 1856 loss: 6.23358501e-05
Iter: 1857 loss: 6.22147199e-05
Iter: 1858 loss: 6.22329098e-05
Iter: 1859 loss: 6.21178551e-05
Iter: 1860 loss: 6.19946659e-05
Iter: 1861 loss: 6.36615296e-05
Iter: 1862 loss: 6.19945931e-05
Iter: 1863 loss: 6.18920749e-05
Iter: 1864 loss: 6.186e-05
Iter: 1865 loss: 6.18000195e-05
Iter: 1866 loss: 6.16723701e-05
Iter: 1867 loss: 6.27925183e-05
Iter: 1868 loss: 6.16661e-05
Iter: 1869 loss: 6.1561208e-05
Iter: 1870 loss: 6.14230812e-05
Iter: 1871 loss: 6.1414612e-05
Iter: 1872 loss: 6.13369193e-05
Iter: 1873 loss: 6.13168377e-05
Iter: 1874 loss: 6.12326694e-05
Iter: 1875 loss: 6.12536605e-05
Iter: 1876 loss: 6.11718569e-05
Iter: 1877 loss: 6.10548e-05
Iter: 1878 loss: 6.12292279e-05
Iter: 1879 loss: 6.09947747e-05
Iter: 1880 loss: 6.08748887e-05
Iter: 1881 loss: 6.16348407e-05
Iter: 1882 loss: 6.08605951e-05
Iter: 1883 loss: 6.07725e-05
Iter: 1884 loss: 6.09627205e-05
Iter: 1885 loss: 6.07361035e-05
Iter: 1886 loss: 6.06241083e-05
Iter: 1887 loss: 6.17043188e-05
Iter: 1888 loss: 6.06216599e-05
Iter: 1889 loss: 6.05735622e-05
Iter: 1890 loss: 6.04682718e-05
Iter: 1891 loss: 6.19774219e-05
Iter: 1892 loss: 6.04602828e-05
Iter: 1893 loss: 6.03327208e-05
Iter: 1894 loss: 6.06127942e-05
Iter: 1895 loss: 6.02839791e-05
Iter: 1896 loss: 6.01724969e-05
Iter: 1897 loss: 6.02287255e-05
Iter: 1898 loss: 6.00992716e-05
Iter: 1899 loss: 5.99673331e-05
Iter: 1900 loss: 6.13327211e-05
Iter: 1901 loss: 5.99661289e-05
Iter: 1902 loss: 5.9865386e-05
Iter: 1903 loss: 5.99844279e-05
Iter: 1904 loss: 5.98140323e-05
Iter: 1905 loss: 5.97051549e-05
Iter: 1906 loss: 6.0024322e-05
Iter: 1907 loss: 5.96713435e-05
Iter: 1908 loss: 5.95641e-05
Iter: 1909 loss: 5.99056366e-05
Iter: 1910 loss: 5.95320635e-05
Iter: 1911 loss: 5.94160665e-05
Iter: 1912 loss: 6.0025639e-05
Iter: 1913 loss: 5.93978039e-05
Iter: 1914 loss: 5.93128862e-05
Iter: 1915 loss: 5.95082711e-05
Iter: 1916 loss: 5.92805809e-05
Iter: 1917 loss: 5.91971257e-05
Iter: 1918 loss: 5.93441691e-05
Iter: 1919 loss: 5.91608768e-05
Iter: 1920 loss: 5.90670825e-05
Iter: 1921 loss: 5.93887053e-05
Iter: 1922 loss: 5.9040598e-05
Iter: 1923 loss: 5.90112722e-05
Iter: 1924 loss: 5.89962401e-05
Iter: 1925 loss: 5.89577503e-05
Iter: 1926 loss: 5.88638068e-05
Iter: 1927 loss: 5.98559927e-05
Iter: 1928 loss: 5.88525545e-05
Iter: 1929 loss: 5.87458708e-05
Iter: 1930 loss: 5.88963667e-05
Iter: 1931 loss: 5.86932183e-05
Iter: 1932 loss: 5.85878151e-05
Iter: 1933 loss: 5.87218237e-05
Iter: 1934 loss: 5.85313137e-05
Iter: 1935 loss: 5.83920446e-05
Iter: 1936 loss: 5.87242357e-05
Iter: 1937 loss: 5.83397559e-05
Iter: 1938 loss: 5.82072171e-05
Iter: 1939 loss: 5.88329858e-05
Iter: 1940 loss: 5.81819622e-05
Iter: 1941 loss: 5.8074038e-05
Iter: 1942 loss: 5.85771559e-05
Iter: 1943 loss: 5.80545311e-05
Iter: 1944 loss: 5.79648331e-05
Iter: 1945 loss: 5.80604246e-05
Iter: 1946 loss: 5.7917794e-05
Iter: 1947 loss: 5.78083745e-05
Iter: 1948 loss: 5.83680448e-05
Iter: 1949 loss: 5.77919564e-05
Iter: 1950 loss: 5.76889215e-05
Iter: 1951 loss: 5.81620261e-05
Iter: 1952 loss: 5.76719358e-05
Iter: 1953 loss: 5.75944505e-05
Iter: 1954 loss: 5.76056736e-05
Iter: 1955 loss: 5.75360609e-05
Iter: 1956 loss: 5.74237311e-05
Iter: 1957 loss: 5.76483326e-05
Iter: 1958 loss: 5.73757243e-05
Iter: 1959 loss: 5.72729623e-05
Iter: 1960 loss: 5.79345215e-05
Iter: 1961 loss: 5.72628342e-05
Iter: 1962 loss: 5.71947894e-05
Iter: 1963 loss: 5.76897364e-05
Iter: 1964 loss: 5.71913879e-05
Iter: 1965 loss: 5.71142846e-05
Iter: 1966 loss: 5.73982361e-05
Iter: 1967 loss: 5.70937118e-05
Iter: 1968 loss: 5.70412303e-05
Iter: 1969 loss: 5.69482618e-05
Iter: 1970 loss: 5.69476906e-05
Iter: 1971 loss: 5.68539144e-05
Iter: 1972 loss: 5.67961324e-05
Iter: 1973 loss: 5.6759829e-05
Iter: 1974 loss: 5.66427407e-05
Iter: 1975 loss: 5.85085581e-05
Iter: 1976 loss: 5.66433919e-05
Iter: 1977 loss: 5.65573609e-05
Iter: 1978 loss: 5.65079426e-05
Iter: 1979 loss: 5.64743277e-05
Iter: 1980 loss: 5.63653957e-05
Iter: 1981 loss: 5.72567369e-05
Iter: 1982 loss: 5.6357203e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/500_500_500_500_1
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-1_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-1_phi0
+ date
Thu Oct 22 14:31:54 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi -1 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-1_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd848c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd867840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd7bd510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd82e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd8c82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd7492f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd8d3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd8c8bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd670158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd6acae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd6709d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd6e80d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd6fa268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd6faae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd6fc8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd5ff268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bcc269d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd605f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd6059d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd638ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7cd638d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bcb279d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bcaea598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bcafed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bcae3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bcbcfe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bca74598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bca74d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bcac12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bca74f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bc9f09d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bc98bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bc98bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bc96b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bc922950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc7bc9227b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.0118984785
test_loss: 0.011258803
train_loss: 0.005515171
test_loss: 0.0058632735
train_loss: 0.0043428713
test_loss: 0.004157645
train_loss: 0.0033504828
test_loss: 0.0035457858
train_loss: 0.0030652015
test_loss: 0.0030400255
train_loss: 0.0028100219
test_loss: 0.0031427925
train_loss: 0.0028086058
test_loss: 0.0029632202
train_loss: 0.0028430836
test_loss: 0.0030702064
train_loss: 0.002768589
test_loss: 0.0034100458
train_loss: 0.0028058018
test_loss: 0.002659652
train_loss: 0.0027839527
test_loss: 0.0030944613
train_loss: 0.00253018
test_loss: 0.0027444165
train_loss: 0.0027332248
test_loss: 0.0028611207
train_loss: 0.0025311979
test_loss: 0.002879186
train_loss: 0.002484905
test_loss: 0.0023261998
train_loss: 0.002337376
test_loss: 0.0022392306
train_loss: 0.0023949924
test_loss: 0.002834759
train_loss: 0.0025087092
test_loss: 0.0025359811
train_loss: 0.0026917714
test_loss: 0.002562061
train_loss: 0.0022844402
test_loss: 0.0022847534
train_loss: 0.0024399222
test_loss: 0.002293741
train_loss: 0.0024347138
test_loss: 0.002400042
train_loss: 0.0030282817
test_loss: 0.0023889765
train_loss: 0.00210673
test_loss: 0.0023007134
train_loss: 0.0022043544
test_loss: 0.0022186185
train_loss: 0.0024393788
test_loss: 0.0024092703
train_loss: 0.0023736013
test_loss: 0.0021204923
train_loss: 0.0021880642
test_loss: 0.0023548128
train_loss: 0.0021819905
test_loss: 0.0023669424
train_loss: 0.0025355234
test_loss: 0.0021823316
train_loss: 0.0021705609
test_loss: 0.0022969164
train_loss: 0.0020320371
test_loss: 0.0020497814
train_loss: 0.002352396
test_loss: 0.0022963411
train_loss: 0.0021961457
test_loss: 0.002055827
train_loss: 0.0023862848
test_loss: 0.0022615665
train_loss: 0.002214431
test_loss: 0.0022913634
train_loss: 0.0022875813
test_loss: 0.0021863594
train_loss: 0.002166033
test_loss: 0.0022653402
train_loss: 0.0022942
test_loss: 0.0021923115
train_loss: 0.0022954429
test_loss: 0.0021615138
train_loss: 0.0020051952
test_loss: 0.0022849815
train_loss: 0.0024214624
test_loss: 0.0021121344
train_loss: 0.002294039
test_loss: 0.0025380065
train_loss: 0.0022922885
test_loss: 0.0023457322
train_loss: 0.002031626
test_loss: 0.0021690137
train_loss: 0.0021400335
test_loss: 0.0024514846
train_loss: 0.002187919
test_loss: 0.002915766
train_loss: 0.0020739792
test_loss: 0.0023057798
train_loss: 0.0019184111
test_loss: 0.0021080505
train_loss: 0.0018761579
test_loss: 0.0020431618
train_loss: 0.002186248
test_loss: 0.0021240893
train_loss: 0.0021099243
test_loss: 0.0020577435
train_loss: 0.002529867
test_loss: 0.0023542119
train_loss: 0.0019366429
test_loss: 0.0021610756
train_loss: 0.0021239542
test_loss: 0.0021672903
train_loss: 0.0020012262
test_loss: 0.0022753929
train_loss: 0.0021089504
test_loss: 0.0021000002
train_loss: 0.0021960007
test_loss: 0.0023108672
train_loss: 0.0020697562
test_loss: 0.0021511326
train_loss: 0.0019855774
test_loss: 0.0021407688
train_loss: 0.0019863648
test_loss: 0.0020102346
train_loss: 0.0020172119
test_loss: 0.0019528792
train_loss: 0.002198039
test_loss: 0.0021902227
train_loss: 0.0021800639
test_loss: 0.0022573909
train_loss: 0.0020181057
test_loss: 0.0020293836
train_loss: 0.0020897528
test_loss: 0.0021700817
train_loss: 0.0020736959
test_loss: 0.0020527297
train_loss: 0.0019381332
test_loss: 0.0020850594
train_loss: 0.0020676448
test_loss: 0.002062754
train_loss: 0.0023692388
test_loss: 0.0025676338
train_loss: 0.0026571886
test_loss: 0.002290407
train_loss: 0.0018832894
test_loss: 0.0019635085
train_loss: 0.002008948
test_loss: 0.0020629333
train_loss: 0.0020511434
test_loss: 0.0019689212
train_loss: 0.0022176686
test_loss: 0.0020984015
train_loss: 0.0020822936
test_loss: 0.002065376
train_loss: 0.0020041873
test_loss: 0.0018465122
train_loss: 0.0020349321
test_loss: 0.0026014326
train_loss: 0.0021886774
test_loss: 0.0021501898
train_loss: 0.0018726047
test_loss: 0.0020779204
train_loss: 0.0020653252
test_loss: 0.0023865832
train_loss: 0.0019902305
test_loss: 0.002023778
train_loss: 0.0019764306
test_loss: 0.0021739937
train_loss: 0.001941759
test_loss: 0.0022260852
train_loss: 0.0018968782
test_loss: 0.0023146647
train_loss: 0.0020578215
test_loss: 0.0022639073
train_loss: 0.0019584054
test_loss: 0.0021115432
train_loss: 0.0019540705
test_loss: 0.0018788377
train_loss: 0.0019340212
test_loss: 0.00192161
train_loss: 0.0019404695
test_loss: 0.0019989617
train_loss: 0.0018880167
test_loss: 0.002012552
train_loss: 0.0019542454
test_loss: 0.002006157
train_loss: 0.0019464646
test_loss: 0.0019627332
train_loss: 0.0021833864
test_loss: 0.002024387
train_loss: 0.0018272845
test_loss: 0.0018877238
train_loss: 0.0019121089
test_loss: 0.0018995341
train_loss: 0.00225119
test_loss: 0.0022529163
train_loss: 0.0018701965
test_loss: 0.0019630643
train_loss: 0.0020718684
test_loss: 0.0019599649
train_loss: 0.0019543695
test_loss: 0.0022744786
train_loss: 0.0018438896
test_loss: 0.00212214
train_loss: 0.001921007
test_loss: 0.0019022232
train_loss: 0.0019253256
test_loss: 0.0022516858
train_loss: 0.0020649836
test_loss: 0.0022631125
train_loss: 0.0019467552
test_loss: 0.0018516193
train_loss: 0.002128259
test_loss: 0.0019116844
train_loss: 0.001880115
test_loss: 0.0022237436
train_loss: 0.0019614699
test_loss: 0.001985963
train_loss: 0.0018814105
test_loss: 0.0019117426
train_loss: 0.0016671561
test_loss: 0.0018697289
train_loss: 0.0018727265
test_loss: 0.0017828476
train_loss: 0.0020770114
test_loss: 0.0020202082
train_loss: 0.0017789528
test_loss: 0.0017942785
train_loss: 0.00182504
test_loss: 0.0018912047
train_loss: 0.0018106855
test_loss: 0.0020381822
train_loss: 0.0018793279
test_loss: 0.0018981211
train_loss: 0.0017153337
test_loss: 0.0018847506
train_loss: 0.0022796406
test_loss: 0.0019917684
train_loss: 0.0019425793
test_loss: 0.002044634
train_loss: 0.0018632512
test_loss: 0.001902374
train_loss: 0.00173403
test_loss: 0.002114283
train_loss: 0.002021411
test_loss: 0.0021287133
train_loss: 0.0019069915
test_loss: 0.002043988
train_loss: 0.0019682837
test_loss: 0.0027431445
train_loss: 0.0022672855
test_loss: 0.0021780266
train_loss: 0.0020109229
test_loss: 0.002246609
train_loss: 0.0021928437
test_loss: 0.0019990497
train_loss: 0.002098373
test_loss: 0.0020716833
train_loss: 0.0019435696
test_loss: 0.0023384553
train_loss: 0.0018540848
test_loss: 0.0019817043
train_loss: 0.0019009896
test_loss: 0.0020012176
train_loss: 0.0017999765
test_loss: 0.0018362347
train_loss: 0.0018246965
test_loss: 0.0018812026
train_loss: 0.0020315158
test_loss: 0.00205851
train_loss: 0.0016724761
test_loss: 0.001741906
train_loss: 0.001698609
test_loss: 0.0021326714
train_loss: 0.00181545
test_loss: 0.0018555976
train_loss: 0.0016677943
test_loss: 0.0018118373
train_loss: 0.0016864922
test_loss: 0.0017596927
train_loss: 0.0017865727
test_loss: 0.0018156164
train_loss: 0.0018392638
test_loss: 0.0020798333
train_loss: 0.0019692949
test_loss: 0.0019666154
train_loss: 0.0017728558
test_loss: 0.0020164985
train_loss: 0.0018368133
test_loss: 0.0020236417
train_loss: 0.0018142974
test_loss: 0.0019025783
train_loss: 0.002075485
test_loss: 0.001992385
train_loss: 0.0021414345
test_loss: 0.0020693822
train_loss: 0.0018641723
test_loss: 0.0018089287
train_loss: 0.0021672728
test_loss: 0.0019604487
train_loss: 0.001999313
test_loss: 0.0021532837
train_loss: 0.0020119431
test_loss: 0.0020020928
train_loss: 0.0017928327
test_loss: 0.0020136514
train_loss: 0.0016792356
test_loss: 0.0018280328
train_loss: 0.0017489252
test_loss: 0.0018519209
train_loss: 0.0017719395
test_loss: 0.0018448508
train_loss: 0.0017791335
test_loss: 0.001776461
train_loss: 0.0017398889
test_loss: 0.0017943236
train_loss: 0.0021904784
test_loss: 0.0019213143
train_loss: 0.0016632201
test_loss: 0.0019308578
train_loss: 0.0020029321
test_loss: 0.0019378618
train_loss: 0.0017142843
test_loss: 0.001904402
train_loss: 0.0017426119
test_loss: 0.0019232448
train_loss: 0.00203175
test_loss: 0.002121688
train_loss: 0.001706426
test_loss: 0.002026994
train_loss: 0.0016506424
test_loss: 0.0018240619
train_loss: 0.0016818612
test_loss: 0.0019500062
train_loss: 0.0018542727
test_loss: 0.0018745278
train_loss: 0.0017646644
test_loss: 0.0020836936
train_loss: 0.0017521093
test_loss: 0.0019256559