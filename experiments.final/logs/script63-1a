+ RUN=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='-2 -1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0
+ date
Wed Oct 21 09:18:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4
+ date
Wed Oct 21 09:18:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8
+ date
Wed Oct 21 09:18:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2
+ date
Wed Oct 21 09:18:47 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi0.8/300_300_300_1 --function f1 --psi -2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9b22b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9b726a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9ab7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9ac47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9b22ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9af9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e99b5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e99c2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e99727b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a44d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a19510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a00d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a0c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9a0cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e98d77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e98ed6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e98ed840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9844a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e9844c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e985a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c835a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c835dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c82e6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c82ae620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c82ae378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50c8309840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e99417b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e991d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50e991d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50804496a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5080449488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f508039b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f508039b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5080399488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50803990d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50803ffea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.021841295
test_loss: 0.021190982
train_loss: 0.010608679
test_loss: 0.010815483
train_loss: 0.0082497625
test_loss: 0.008591315
train_loss: 0.00708322
test_loss: 0.0075814873
train_loss: 0.0068265256
test_loss: 0.0071001314
train_loss: 0.006518971
test_loss: 0.0070450567
train_loss: 0.0061302795
test_loss: 0.006783094
train_loss: 0.0061742733
test_loss: 0.006875619
train_loss: 0.0062974775
test_loss: 0.006556472
train_loss: 0.0060953405
test_loss: 0.0063496255
train_loss: 0.0056433785
test_loss: 0.006631164
train_loss: 0.005513102
test_loss: 0.00639936
train_loss: 0.00548035
test_loss: 0.0063825627
train_loss: 0.0059448555
test_loss: 0.006330065
train_loss: 0.005315227
test_loss: 0.006339741
train_loss: 0.0055789137
test_loss: 0.006004008
train_loss: 0.0050865356
test_loss: 0.0058268653
train_loss: 0.0051076366
test_loss: 0.005969283
train_loss: 0.004924215
test_loss: 0.0058081066
train_loss: 0.0050095497
test_loss: 0.005802244
train_loss: 0.005174317
test_loss: 0.0060232976
train_loss: 0.0051678903
test_loss: 0.0057226224
train_loss: 0.0050662807
test_loss: 0.005852796
train_loss: 0.0047696074
test_loss: 0.005673038
train_loss: 0.004694115
test_loss: 0.0056269285
train_loss: 0.0045562396
test_loss: 0.005580263
train_loss: 0.0050458717
test_loss: 0.0056806253
train_loss: 0.0049402784
test_loss: 0.005546032
train_loss: 0.004821866
test_loss: 0.005376556
train_loss: 0.0049817916
test_loss: 0.0055849296
train_loss: 0.0046052565
test_loss: 0.0054069404
train_loss: 0.0043715825
test_loss: 0.005639102
train_loss: 0.004626459
test_loss: 0.005438026
train_loss: 0.004508894
test_loss: 0.005966049
train_loss: 0.00471092
test_loss: 0.0054718344
train_loss: 0.0044156965
test_loss: 0.005196775
train_loss: 0.0044202507
test_loss: 0.005259384
train_loss: 0.0043004313
test_loss: 0.0052551245
train_loss: 0.004591899
test_loss: 0.0051469738
train_loss: 0.0042014853
test_loss: 0.0050134053
train_loss: 0.0045157587
test_loss: 0.005051113
train_loss: 0.004162776
test_loss: 0.0051346477
train_loss: 0.004140989
test_loss: 0.005033076
train_loss: 0.0042116703
test_loss: 0.004955166
train_loss: 0.0042584306
test_loss: 0.0051701204
train_loss: 0.004121415
test_loss: 0.0049946597
train_loss: 0.0039855214
test_loss: 0.0050211763
train_loss: 0.0042400146
test_loss: 0.004937819
train_loss: 0.0041921153
test_loss: 0.0052647037
train_loss: 0.00414148
test_loss: 0.005065789
train_loss: 0.004051873
test_loss: 0.0049560163
train_loss: 0.004102352
test_loss: 0.004780862
train_loss: 0.004172706
test_loss: 0.0048953523
train_loss: 0.004236169
test_loss: 0.0047613024
train_loss: 0.004184692
test_loss: 0.004940241
train_loss: 0.0042022434
test_loss: 0.004836364
train_loss: 0.0044966806
test_loss: 0.005120908
train_loss: 0.0041001155
test_loss: 0.0047676796
train_loss: 0.0038242566
test_loss: 0.004846425
train_loss: 0.0038725014
test_loss: 0.004655126
train_loss: 0.003856185
test_loss: 0.004693798
train_loss: 0.003966935
test_loss: 0.0047858646
train_loss: 0.00372295
test_loss: 0.0046691033
train_loss: 0.0036993688
test_loss: 0.004639943
train_loss: 0.003870769
test_loss: 0.004800081
train_loss: 0.0037584687
test_loss: 0.004569838
train_loss: 0.0040200073
test_loss: 0.004919059
train_loss: 0.003925394
test_loss: 0.0046241865
train_loss: 0.0041175904
test_loss: 0.004848432
train_loss: 0.004006299
test_loss: 0.004644277
train_loss: 0.0037973644
test_loss: 0.0046702204
train_loss: 0.004097294
test_loss: 0.004639361
train_loss: 0.0036880947
test_loss: 0.0044482863
train_loss: 0.0037412606
test_loss: 0.0046261065
train_loss: 0.003558064
test_loss: 0.0045383824
train_loss: 0.003692758
test_loss: 0.004538348
train_loss: 0.0038698863
test_loss: 0.0046091992
train_loss: 0.0036645648
test_loss: 0.004601266
train_loss: 0.0036385392
test_loss: 0.0044518462
train_loss: 0.0038880892
test_loss: 0.004554447
train_loss: 0.0037284382
test_loss: 0.004600406
train_loss: 0.004082782
test_loss: 0.004589605
train_loss: 0.0038754195
test_loss: 0.0048241587
train_loss: 0.003733444
test_loss: 0.0046430263
train_loss: 0.0036973548
test_loss: 0.0045421873
train_loss: 0.003638552
test_loss: 0.0045489115
train_loss: 0.0036792196
test_loss: 0.0043367227
train_loss: 0.004068888
test_loss: 0.0047253836
train_loss: 0.0036568886
test_loss: 0.0043918155
train_loss: 0.003390548
test_loss: 0.00424622
train_loss: 0.0035902546
test_loss: 0.004474839
train_loss: 0.0034680502
test_loss: 0.004420526
train_loss: 0.0034753156
test_loss: 0.004374778
train_loss: 0.0035549896
test_loss: 0.004380691
train_loss: 0.0034945216
test_loss: 0.0043683974
train_loss: 0.0038243001
test_loss: 0.004662072
train_loss: 0.004022667
test_loss: 0.004794557
train_loss: 0.0034577705
test_loss: 0.004392472
train_loss: 0.0036199007
test_loss: 0.0045067198
train_loss: 0.00353831
test_loss: 0.0044276766
train_loss: 0.0036012155
test_loss: 0.0044241757
train_loss: 0.0035990742
test_loss: 0.004489387
train_loss: 0.0037386124
test_loss: 0.0043465146
train_loss: 0.003406996
test_loss: 0.0044392766
train_loss: 0.0034681724
test_loss: 0.0045839013
train_loss: 0.0033967523
test_loss: 0.0042565363
train_loss: 0.0034690737
test_loss: 0.004341821
train_loss: 0.0035353894
test_loss: 0.0043229796
train_loss: 0.003724771
test_loss: 0.0041764616
train_loss: 0.0035361974
test_loss: 0.0044078594
train_loss: 0.0037153205
test_loss: 0.004267634
train_loss: 0.0033385325
test_loss: 0.004349864
train_loss: 0.0033595082
test_loss: 0.0043363185
train_loss: 0.0033958983
test_loss: 0.0041910657
train_loss: 0.003577192
test_loss: 0.0042507634
train_loss: 0.0035773632
test_loss: 0.0042026043
train_loss: 0.0034938883
test_loss: 0.0043790475
train_loss: 0.003397062
test_loss: 0.004433453
train_loss: 0.003276146
test_loss: 0.0041495953
train_loss: 0.0035223844
test_loss: 0.0043665865
train_loss: 0.003427176
test_loss: 0.004234013
train_loss: 0.0032945615
test_loss: 0.004025006
train_loss: 0.0034588529
test_loss: 0.0042100404
train_loss: 0.0035324898
test_loss: 0.0043724286
train_loss: 0.0036485717
test_loss: 0.0043292004
train_loss: 0.0033324508
test_loss: 0.004356608
train_loss: 0.0031989715
test_loss: 0.0041698646
train_loss: 0.0034474775
test_loss: 0.0041663074
train_loss: 0.0033746161
test_loss: 0.0041520563
train_loss: 0.003161972
test_loss: 0.004123709
train_loss: 0.0035073082
test_loss: 0.004151492
train_loss: 0.0033225352
test_loss: 0.004208934
train_loss: 0.0033651793
test_loss: 0.004177568
train_loss: 0.003442727
test_loss: 0.004336745
train_loss: 0.0035048046
test_loss: 0.0042063035
train_loss: 0.0033639758
test_loss: 0.004158043
train_loss: 0.0035304795
test_loss: 0.0042504296
train_loss: 0.0031742651
test_loss: 0.004219253
train_loss: 0.0033586337
test_loss: 0.00409006
train_loss: 0.0032927855
test_loss: 0.004120284
train_loss: 0.0032985099
test_loss: 0.0041039283
train_loss: 0.0033064133
test_loss: 0.004110253
train_loss: 0.0033553708
test_loss: 0.004088762
train_loss: 0.0033088347
test_loss: 0.004149748
train_loss: 0.0032853917
test_loss: 0.004151167
train_loss: 0.0034417538
test_loss: 0.004026686
train_loss: 0.0034844421
test_loss: 0.0040770923
train_loss: 0.0033039316
test_loss: 0.004080111
train_loss: 0.0033956615
test_loss: 0.004034297
train_loss: 0.003151541
test_loss: 0.003997235
train_loss: 0.0031756815
test_loss: 0.00402418
train_loss: 0.0033338487
test_loss: 0.0043195137
train_loss: 0.0034177809
test_loss: 0.004218853
train_loss: 0.0033868751
test_loss: 0.004200719
train_loss: 0.0031526329
test_loss: 0.003979364
train_loss: 0.0031833244
test_loss: 0.0040464946
train_loss: 0.0032902663
test_loss: 0.0040161796
train_loss: 0.0033820316
test_loss: 0.0041696746
train_loss: 0.0032661783
test_loss: 0.0039988738
train_loss: 0.0031531448
test_loss: 0.003859729
train_loss: 0.0035377615
test_loss: 0.0043278807
train_loss: 0.0034150805
test_loss: 0.0041430765
train_loss: 0.0031819183
test_loss: 0.00401621
train_loss: 0.0035598695
test_loss: 0.0042127264
train_loss: 0.0034008445
test_loss: 0.004156681
train_loss: 0.003470745
test_loss: 0.004208918
train_loss: 0.003145379
test_loss: 0.004034552
train_loss: 0.0031843118
test_loss: 0.0042366995
train_loss: 0.003354589
test_loss: 0.004049832
train_loss: 0.0033137514
test_loss: 0.004058835
train_loss: 0.00313726
test_loss: 0.0040056384
train_loss: 0.0034294957
test_loss: 0.0039841365
train_loss: 0.0033327998
test_loss: 0.0041349693
train_loss: 0.0031591468
test_loss: 0.00395399
train_loss: 0.0032596975
test_loss: 0.0041411766
train_loss: 0.0031511695
test_loss: 0.004096768
train_loss: 0.003407426
test_loss: 0.004408958
train_loss: 0.003154217
test_loss: 0.004029545
train_loss: 0.003078462
test_loss: 0.0039146133
train_loss: 0.0030417177
test_loss: 0.003919839
train_loss: 0.003363831
test_loss: 0.003975986
train_loss: 0.0030494335
test_loss: 0.003932963
train_loss: 0.0029614186
test_loss: 0.003951232
train_loss: 0.0029175165
test_loss: 0.0038829348
train_loss: 0.0031781567
test_loss: 0.0042306352
train_loss: 0.0030915693
test_loss: 0.003855718
train_loss: 0.00335219
test_loss: 0.0040464727
train_loss: 0.0030960785
test_loss: 0.0038979773
train_loss: 0.0032562981
test_loss: 0.0038694707
train_loss: 0.0031729683
test_loss: 0.0039271857
train_loss: 0.002934087
test_loss: 0.0039678146
train_loss: 0.0030972427
test_loss: 0.003834162
train_loss: 0.0032029748
test_loss: 0.0042075827
train_loss: 0.0031119022
test_loss: 0.003964558
train_loss: 0.0029382012
test_loss: 0.0039826967
train_loss: 0.0030713405
test_loss: 0.0039547244
train_loss: 0.0029903757
test_loss: 0.0038941575
train_loss: 0.0030570296
test_loss: 0.0039137914
train_loss: 0.0032718522
test_loss: 0.0039771823
train_loss: 0.003146044
test_loss: 0.0038048541
train_loss: 0.0030899292
test_loss: 0.0038960555
train_loss: 0.0034838666
test_loss: 0.0044543142
train_loss: 0.0035003624
test_loss: 0.00391882
train_loss: 0.0033080685
test_loss: 0.0040218285
train_loss: 0.0031085974
test_loss: 0.0040022815
train_loss: 0.003146377
test_loss: 0.0039450275
train_loss: 0.0030544037
test_loss: 0.0038104581
train_loss: 0.0028704728
test_loss: 0.0038368104
train_loss: 0.0033312533
test_loss: 0.00401676
train_loss: 0.003214291
test_loss: 0.003959109
train_loss: 0.0032419872
test_loss: 0.003985854
train_loss: 0.003291354
test_loss: 0.0037861145
train_loss: 0.0029321664
test_loss: 0.0038425939
train_loss: 0.002964269
test_loss: 0.0038829183
train_loss: 0.0029569168
test_loss: 0.0039116303
train_loss: 0.003175784
test_loss: 0.0036848253
train_loss: 0.003012405
test_loss: 0.0037501024
train_loss: 0.0030583232
test_loss: 0.0038621225
train_loss: 0.0031565384
test_loss: 0.0039946395
train_loss: 0.0031691547
test_loss: 0.004046898
train_loss: 0.0032384119
test_loss: 0.0038915833
train_loss: 0.0029655942
test_loss: 0.0036773316
train_loss: 0.0030389237
test_loss: 0.003960643
train_loss: 0.0029732012
test_loss: 0.0038708092
train_loss: 0.003029606
test_loss: 0.0037517492
train_loss: 0.0031063221
test_loss: 0.003871775
train_loss: 0.0033233548
test_loss: 0.004067644
train_loss: 0.0031052907
test_loss: 0.004012438
train_loss: 0.0028731704
test_loss: 0.0037699686
train_loss: 0.0032047632
test_loss: 0.0040116245
train_loss: 0.0028656465
test_loss: 0.0039103664
train_loss: 0.0030436446
test_loss: 0.0037083037
train_loss: 0.0031686444
test_loss: 0.0038148144
train_loss: 0.0033478043
test_loss: 0.0039903163
train_loss: 0.0029617837
test_loss: 0.003818497
train_loss: 0.003127126
test_loss: 0.003902102
train_loss: 0.00326161
test_loss: 0.003868237
train_loss: 0.0030529033
test_loss: 0.0038171066
train_loss: 0.0028423702
test_loss: 0.003807607
train_loss: 0.0032052146
test_loss: 0.0038790165
train_loss: 0.003102321
test_loss: 0.0038269043
train_loss: 0.003397848
test_loss: 0.003890391
train_loss: 0.0029168134
test_loss: 0.003867908
train_loss: 0.0027219574
test_loss: 0.0036292213
train_loss: 0.003290067
test_loss: 0.0038310539
train_loss: 0.0029884102
test_loss: 0.0037565012
train_loss: 0.0029240295
test_loss: 0.0037978822
train_loss: 0.0030036552
test_loss: 0.0037614403
train_loss: 0.0029924854
test_loss: 0.0036819964
train_loss: 0.0030523571
test_loss: 0.0037935511
train_loss: 0.0029963742
test_loss: 0.003992253
train_loss: 0.0030861704
test_loss: 0.00376423
train_loss: 0.0028806813
test_loss: 0.0036919995
train_loss: 0.003037244
test_loss: 0.0038240496
train_loss: 0.0029208022
test_loss: 0.003747258
train_loss: 0.0028616218
test_loss: 0.0039069518
train_loss: 0.002960842
test_loss: 0.0037635947
train_loss: 0.0030126213
test_loss: 0.0038140758
train_loss: 0.0028117998
test_loss: 0.003714278
train_loss: 0.0028595333
test_loss: 0.0037033367
train_loss: 0.002803964
test_loss: 0.0039047468
train_loss: 0.0028563635
test_loss: 0.003801638
train_loss: 0.0032267876
test_loss: 0.0037097272
train_loss: 0.0031301412
test_loss: 0.003797508
train_loss: 0.0029942042
test_loss: 0.0036605538
train_loss: 0.0034471997
test_loss: 0.0038871232
train_loss: 0.0030007514
test_loss: 0.0038887362
train_loss: 0.0028834974
test_loss: 0.0036277068
train_loss: 0.002962173
test_loss: 0.003691451
train_loss: 0.002987802
test_loss: 0.0037808141
train_loss: 0.0031115145
test_loss: 0.0037630922
train_loss: 0.0027827364
test_loss: 0.0037555827
train_loss: 0.0029499228
test_loss: 0.0037936715
train_loss: 0.0029349022
test_loss: 0.0037182476
train_loss: 0.003287441
test_loss: 0.0037883804
train_loss: 0.003119721
test_loss: 0.0037924806
train_loss: 0.0029170474
test_loss: 0.003772275
train_loss: 0.002745871
test_loss: 0.0038009868
train_loss: 0.0029141828
test_loss: 0.0039026823
train_loss: 0.0031784046
test_loss: 0.0037163768
train_loss: 0.0028864045
test_loss: 0.0036838902
train_loss: 0.0028225058
test_loss: 0.0036645923
train_loss: 0.0029255908
test_loss: 0.0035713208
train_loss: 0.0029321932
test_loss: 0.003592519
train_loss: 0.003040615
test_loss: 0.0038118034
train_loss: 0.0031078288
test_loss: 0.00378615
train_loss: 0.0028037312
test_loss: 0.0036009513
train_loss: 0.0028741616
test_loss: 0.003734072
train_loss: 0.002691111
test_loss: 0.00354364
train_loss: 0.0029639248
test_loss: 0.00374795
train_loss: 0.0030055786
test_loss: 0.003801165
train_loss: 0.0031194272
test_loss: 0.003803547
train_loss: 0.0031606061
test_loss: 0.003915665
train_loss: 0.0029516444
test_loss: 0.0036123034
train_loss: 0.0030157373
test_loss: 0.0036762692
train_loss: 0.0027217246
test_loss: 0.0036942854
train_loss: 0.0029880437
test_loss: 0.0036955394
train_loss: 0.002965066
test_loss: 0.0036608633
train_loss: 0.003007018
test_loss: 0.0036643627
train_loss: 0.0029387425
test_loss: 0.0037095197
train_loss: 0.0029163244
test_loss: 0.003753064
train_loss: 0.0029524472
test_loss: 0.0037465799
train_loss: 0.0028498522
test_loss: 0.0038284783
train_loss: 0.0028253146
test_loss: 0.0037675072
train_loss: 0.0029026205
test_loss: 0.003642134
train_loss: 0.0028066335
test_loss: 0.0037291776
train_loss: 0.0026640643
test_loss: 0.0037435764
train_loss: 0.0030139324
test_loss: 0.0036504057
train_loss: 0.0027695762
test_loss: 0.003584185
train_loss: 0.0028310819
test_loss: 0.0036965448
train_loss: 0.0030135247
test_loss: 0.003697132
train_loss: 0.0027707643
test_loss: 0.0036001734
train_loss: 0.002774871
test_loss: 0.0036425157
train_loss: 0.002881335
test_loss: 0.0037480548
train_loss: 0.0028582627
test_loss: 0.0037377705
train_loss: 0.0028382163
test_loss: 0.0036181994
train_loss: 0.00271264
test_loss: 0.0035210066
train_loss: 0.0027191637
test_loss: 0.0035751327
train_loss: 0.0028662905
test_loss: 0.003880308
train_loss: 0.0028012583
test_loss: 0.0037539373
train_loss: 0.0028950514
test_loss: 0.0036111097
train_loss: 0.0028856522
test_loss: 0.0035697399
train_loss: 0.0028543584
test_loss: 0.0036028358
train_loss: 0.0029244558
test_loss: 0.0036291913
train_loss: 0.0030154367
test_loss: 0.0036841321
train_loss: 0.0027377056
test_loss: 0.0036359068
train_loss: 0.0029205116
test_loss: 0.0036465314
train_loss: 0.003078429
test_loss: 0.003734277
train_loss: 0.0032282257
test_loss: 0.0037917313
train_loss: 0.0029114562
test_loss: 0.0037341996
train_loss: 0.0028034337
test_loss: 0.0040023457
train_loss: 0.003085792
test_loss: 0.0036210837
train_loss: 0.0028262194
test_loss: 0.003643603
train_loss: 0.0027047691
test_loss: 0.0035887896
train_loss: 0.0028485355
test_loss: 0.003565661
train_loss: 0.0029159319
test_loss: 0.0036095148
train_loss: 0.002874794
test_loss: 0.0036862462
train_loss: 0.002982467
test_loss: 0.0035485718
train_loss: 0.0029358163
test_loss: 0.003647636
train_loss: 0.0028367913
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0035291896
train_loss: 0.0028066682
test_loss: 0.0035367357
train_loss: 0.0029031252
test_loss: 0.0035374996
train_loss: 0.0028760089
test_loss: 0.0036449549
train_loss: 0.0029838847
test_loss: 0.0036058207
train_loss: 0.0029187964
test_loss: 0.0037986175
train_loss: 0.0028903268
test_loss: 0.0037031362
train_loss: 0.002752013
test_loss: 0.003571209
train_loss: 0.003081764
test_loss: 0.003681048
train_loss: 0.0029990412
test_loss: 0.0037395768
train_loss: 0.00299531
test_loss: 0.0037525147
train_loss: 0.0028696833
test_loss: 0.0037340724
train_loss: 0.0028437593
test_loss: 0.0036058698
train_loss: 0.0028861226
test_loss: 0.0036706203
train_loss: 0.0026039255
test_loss: 0.0036999793
train_loss: 0.0028622397
test_loss: 0.0035458535
train_loss: 0.003085244
test_loss: 0.0036206387
train_loss: 0.0028323743
test_loss: 0.003486491
train_loss: 0.0026026191
test_loss: 0.0035378668
train_loss: 0.0027583083
test_loss: 0.0037109533
train_loss: 0.0028571498
test_loss: 0.0035815265
train_loss: 0.003019749
test_loss: 0.0036981814
train_loss: 0.002657531
test_loss: 0.003467456
train_loss: 0.002788214
test_loss: 0.0036381492
train_loss: 0.0027566939
test_loss: 0.0035154393
train_loss: 0.0027273665
test_loss: 0.0036199696
train_loss: 0.0027977042
test_loss: 0.0036227063
train_loss: 0.002885798
test_loss: 0.003599205
train_loss: 0.0029284698
test_loss: 0.003856682
train_loss: 0.002910701
test_loss: 0.0036426187
train_loss: 0.002676687
test_loss: 0.003681007
train_loss: 0.0029046247
test_loss: 0.0035938805
train_loss: 0.002885936
test_loss: 0.0036207472
train_loss: 0.002709941
test_loss: 0.0035106086
train_loss: 0.0031035105
test_loss: 0.003605149
train_loss: 0.0028213423
test_loss: 0.0036343895
train_loss: 0.0028842299
test_loss: 0.00358041
train_loss: 0.0027680171
test_loss: 0.0036884292
train_loss: 0.0027636783
test_loss: 0.0036197784
train_loss: 0.0032647036
test_loss: 0.0037888617
train_loss: 0.0028447271
test_loss: 0.0036200534
train_loss: 0.0025678966
test_loss: 0.0035349347
train_loss: 0.0027102737
test_loss: 0.003700044
train_loss: 0.0025451037
test_loss: 0.003565952
train_loss: 0.0025979725
test_loss: 0.0035203903
train_loss: 0.0027935964
test_loss: 0.0035999184
train_loss: 0.0027170917
test_loss: 0.0036065595
train_loss: 0.0026319618
test_loss: 0.0035291351
train_loss: 0.002830022
test_loss: 0.0036171635
train_loss: 0.002752849
test_loss: 0.003581333
train_loss: 0.002824122
test_loss: 0.0035926965
train_loss: 0.0028149828
test_loss: 0.0035012153
train_loss: 0.0028305957
test_loss: 0.0036234923
train_loss: 0.0026705984
test_loss: 0.0035380616
train_loss: 0.002645508
test_loss: 0.0035274222
train_loss: 0.0029350359
test_loss: 0.0037134807
train_loss: 0.0029833647
test_loss: 0.0034768076
train_loss: 0.0026921427
test_loss: 0.0034785944
train_loss: 0.002711893
test_loss: 0.003469985
train_loss: 0.0027580962
test_loss: 0.0035596061
train_loss: 0.0026624938
test_loss: 0.003534568
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accb5c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accc11598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accc11400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accc11620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accaff488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accaffc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accafa950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accaa2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6accaa21e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca66c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca66b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca668c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca21598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acca21ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acc98e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6acc9c26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90cd58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90cd5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90cd0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90cd02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90c549d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90c76ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90c768c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90bf8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a90bf8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c4b5ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c4679d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c40f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c40f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c42f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c3dd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c39c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c39c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c3c08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c371bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6a6c371ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.39151507e-05
Iter: 2 loss: 1.15776511e-05
Iter: 3 loss: 1.15670409e-05
Iter: 4 loss: 1.04372666e-05
Iter: 5 loss: 1.41253367e-05
Iter: 6 loss: 1.01219703e-05
Iter: 7 loss: 9.39565416e-06
Iter: 8 loss: 1.12101934e-05
Iter: 9 loss: 9.14048e-06
Iter: 10 loss: 8.44455735e-06
Iter: 11 loss: 9.89043474e-06
Iter: 12 loss: 8.16658394e-06
Iter: 13 loss: 7.55381961e-06
Iter: 14 loss: 9.64080573e-06
Iter: 15 loss: 7.39142251e-06
Iter: 16 loss: 6.92532558e-06
Iter: 17 loss: 8.34802813e-06
Iter: 18 loss: 6.78676406e-06
Iter: 19 loss: 6.31352577e-06
Iter: 20 loss: 9.29022826e-06
Iter: 21 loss: 6.25934445e-06
Iter: 22 loss: 5.98558381e-06
Iter: 23 loss: 5.57204203e-06
Iter: 24 loss: 5.56421719e-06
Iter: 25 loss: 5.33348339e-06
Iter: 26 loss: 5.33270304e-06
Iter: 27 loss: 5.1222687e-06
Iter: 28 loss: 6.33172976e-06
Iter: 29 loss: 5.09403071e-06
Iter: 30 loss: 4.97948031e-06
Iter: 31 loss: 4.84331485e-06
Iter: 32 loss: 4.82912219e-06
Iter: 33 loss: 4.62396156e-06
Iter: 34 loss: 4.52918448e-06
Iter: 35 loss: 4.42759847e-06
Iter: 36 loss: 4.42623423e-06
Iter: 37 loss: 4.30261252e-06
Iter: 38 loss: 4.18212858e-06
Iter: 39 loss: 4.1516796e-06
Iter: 40 loss: 4.07590414e-06
Iter: 41 loss: 3.9467086e-06
Iter: 42 loss: 4.76706737e-06
Iter: 43 loss: 3.93212213e-06
Iter: 44 loss: 3.82164899e-06
Iter: 45 loss: 3.97085114e-06
Iter: 46 loss: 3.76627167e-06
Iter: 47 loss: 3.67665234e-06
Iter: 48 loss: 4.15410659e-06
Iter: 49 loss: 3.66291306e-06
Iter: 50 loss: 3.59429032e-06
Iter: 51 loss: 3.77607694e-06
Iter: 52 loss: 3.5712028e-06
Iter: 53 loss: 3.48747039e-06
Iter: 54 loss: 3.69876057e-06
Iter: 55 loss: 3.45841931e-06
Iter: 56 loss: 3.3917654e-06
Iter: 57 loss: 3.29580257e-06
Iter: 58 loss: 3.29278555e-06
Iter: 59 loss: 3.19292985e-06
Iter: 60 loss: 4.2427132e-06
Iter: 61 loss: 3.19041465e-06
Iter: 62 loss: 3.08820745e-06
Iter: 63 loss: 3.54603139e-06
Iter: 64 loss: 3.06819038e-06
Iter: 65 loss: 3.01279169e-06
Iter: 66 loss: 2.93867561e-06
Iter: 67 loss: 2.93452786e-06
Iter: 68 loss: 2.85861915e-06
Iter: 69 loss: 3.22380174e-06
Iter: 70 loss: 2.84511975e-06
Iter: 71 loss: 2.79763253e-06
Iter: 72 loss: 3.27414e-06
Iter: 73 loss: 2.79613027e-06
Iter: 74 loss: 2.74551439e-06
Iter: 75 loss: 2.96842609e-06
Iter: 76 loss: 2.73538035e-06
Iter: 77 loss: 2.70731721e-06
Iter: 78 loss: 2.70018836e-06
Iter: 79 loss: 2.68258759e-06
Iter: 80 loss: 2.6271282e-06
Iter: 81 loss: 2.75146158e-06
Iter: 82 loss: 2.60604202e-06
Iter: 83 loss: 2.5683064e-06
Iter: 84 loss: 2.68267718e-06
Iter: 85 loss: 2.55703299e-06
Iter: 86 loss: 2.52139e-06
Iter: 87 loss: 2.72045236e-06
Iter: 88 loss: 2.51628262e-06
Iter: 89 loss: 2.48626748e-06
Iter: 90 loss: 2.57236434e-06
Iter: 91 loss: 2.47684488e-06
Iter: 92 loss: 2.44950661e-06
Iter: 93 loss: 2.4459714e-06
Iter: 94 loss: 2.42653232e-06
Iter: 95 loss: 2.39642986e-06
Iter: 96 loss: 2.49138498e-06
Iter: 97 loss: 2.38762027e-06
Iter: 98 loss: 2.36663755e-06
Iter: 99 loss: 2.36616825e-06
Iter: 100 loss: 2.3512107e-06
Iter: 101 loss: 2.31255399e-06
Iter: 102 loss: 2.62732783e-06
Iter: 103 loss: 2.30561091e-06
Iter: 104 loss: 2.26571319e-06
Iter: 105 loss: 2.30969977e-06
Iter: 106 loss: 2.2439383e-06
Iter: 107 loss: 2.2311649e-06
Iter: 108 loss: 2.22154904e-06
Iter: 109 loss: 2.2003353e-06
Iter: 110 loss: 2.2527372e-06
Iter: 111 loss: 2.19285175e-06
Iter: 112 loss: 2.1781575e-06
Iter: 113 loss: 2.18298942e-06
Iter: 114 loss: 2.1678145e-06
Iter: 115 loss: 2.14918555e-06
Iter: 116 loss: 2.29640659e-06
Iter: 117 loss: 2.14792794e-06
Iter: 118 loss: 2.13522753e-06
Iter: 119 loss: 2.11624274e-06
Iter: 120 loss: 2.11585984e-06
Iter: 121 loss: 2.09899304e-06
Iter: 122 loss: 2.09835298e-06
Iter: 123 loss: 2.08592564e-06
Iter: 124 loss: 2.07464382e-06
Iter: 125 loss: 2.07152766e-06
Iter: 126 loss: 2.0455102e-06
Iter: 127 loss: 2.07075664e-06
Iter: 128 loss: 2.03064405e-06
Iter: 129 loss: 2.01388616e-06
Iter: 130 loss: 2.10423877e-06
Iter: 131 loss: 2.0113614e-06
Iter: 132 loss: 1.99534156e-06
Iter: 133 loss: 2.13406292e-06
Iter: 134 loss: 1.99448959e-06
Iter: 135 loss: 1.98368139e-06
Iter: 136 loss: 1.96676274e-06
Iter: 137 loss: 1.96655037e-06
Iter: 138 loss: 1.94907284e-06
Iter: 139 loss: 1.95629627e-06
Iter: 140 loss: 1.9369586e-06
Iter: 141 loss: 1.92714469e-06
Iter: 142 loss: 1.92554126e-06
Iter: 143 loss: 1.91264758e-06
Iter: 144 loss: 1.92515586e-06
Iter: 145 loss: 1.90524941e-06
Iter: 146 loss: 1.89456887e-06
Iter: 147 loss: 1.89647051e-06
Iter: 148 loss: 1.88653132e-06
Iter: 149 loss: 1.87275782e-06
Iter: 150 loss: 2.01811827e-06
Iter: 151 loss: 1.87241085e-06
Iter: 152 loss: 1.86464922e-06
Iter: 153 loss: 1.85863212e-06
Iter: 154 loss: 1.8561592e-06
Iter: 155 loss: 1.84946714e-06
Iter: 156 loss: 1.84897169e-06
Iter: 157 loss: 1.84358601e-06
Iter: 158 loss: 1.83424686e-06
Iter: 159 loss: 1.83422765e-06
Iter: 160 loss: 1.82319661e-06
Iter: 161 loss: 1.89276557e-06
Iter: 162 loss: 1.82190831e-06
Iter: 163 loss: 1.8143835e-06
Iter: 164 loss: 1.81736527e-06
Iter: 165 loss: 1.80920051e-06
Iter: 166 loss: 1.79732672e-06
Iter: 167 loss: 1.88569391e-06
Iter: 168 loss: 1.79635379e-06
Iter: 169 loss: 1.78729317e-06
Iter: 170 loss: 1.7776041e-06
Iter: 171 loss: 1.77603988e-06
Iter: 172 loss: 1.76464539e-06
Iter: 173 loss: 1.76504511e-06
Iter: 174 loss: 1.75570301e-06
Iter: 175 loss: 1.74737079e-06
Iter: 176 loss: 1.74681236e-06
Iter: 177 loss: 1.73861645e-06
Iter: 178 loss: 1.78259131e-06
Iter: 179 loss: 1.73739159e-06
Iter: 180 loss: 1.73338151e-06
Iter: 181 loss: 1.72410103e-06
Iter: 182 loss: 1.83919894e-06
Iter: 183 loss: 1.72339696e-06
Iter: 184 loss: 1.71291958e-06
Iter: 185 loss: 1.71291651e-06
Iter: 186 loss: 1.70709097e-06
Iter: 187 loss: 1.70094756e-06
Iter: 188 loss: 1.69993575e-06
Iter: 189 loss: 1.6925411e-06
Iter: 190 loss: 1.69252326e-06
Iter: 191 loss: 1.6867225e-06
Iter: 192 loss: 1.68159465e-06
Iter: 193 loss: 1.68001429e-06
Iter: 194 loss: 1.67304597e-06
Iter: 195 loss: 1.70410374e-06
Iter: 196 loss: 1.67159635e-06
Iter: 197 loss: 1.6653853e-06
Iter: 198 loss: 1.69371117e-06
Iter: 199 loss: 1.66421046e-06
Iter: 200 loss: 1.65988683e-06
Iter: 201 loss: 1.7063486e-06
Iter: 202 loss: 1.65976076e-06
Iter: 203 loss: 1.65635038e-06
Iter: 204 loss: 1.64852634e-06
Iter: 205 loss: 1.75052935e-06
Iter: 206 loss: 1.64804192e-06
Iter: 207 loss: 1.640037e-06
Iter: 208 loss: 1.64015216e-06
Iter: 209 loss: 1.6335689e-06
Iter: 210 loss: 1.62289336e-06
Iter: 211 loss: 1.72598232e-06
Iter: 212 loss: 1.62250399e-06
Iter: 213 loss: 1.61919615e-06
Iter: 214 loss: 1.61838727e-06
Iter: 215 loss: 1.61431899e-06
Iter: 216 loss: 1.60702166e-06
Iter: 217 loss: 1.78155517e-06
Iter: 218 loss: 1.60703667e-06
Iter: 219 loss: 1.60276136e-06
Iter: 220 loss: 1.63276059e-06
Iter: 221 loss: 1.60237244e-06
Iter: 222 loss: 1.5970428e-06
Iter: 223 loss: 1.59932699e-06
Iter: 224 loss: 1.59338879e-06
Iter: 225 loss: 1.58822661e-06
Iter: 226 loss: 1.60133698e-06
Iter: 227 loss: 1.58642331e-06
Iter: 228 loss: 1.57982254e-06
Iter: 229 loss: 1.61124376e-06
Iter: 230 loss: 1.57868465e-06
Iter: 231 loss: 1.57440388e-06
Iter: 232 loss: 1.56988904e-06
Iter: 233 loss: 1.56911574e-06
Iter: 234 loss: 1.56238525e-06
Iter: 235 loss: 1.6111477e-06
Iter: 236 loss: 1.56183876e-06
Iter: 237 loss: 1.5563744e-06
Iter: 238 loss: 1.59132196e-06
Iter: 239 loss: 1.55574378e-06
Iter: 240 loss: 1.55143618e-06
Iter: 241 loss: 1.56441945e-06
Iter: 242 loss: 1.55012992e-06
Iter: 243 loss: 1.54732675e-06
Iter: 244 loss: 1.5420643e-06
Iter: 245 loss: 1.66355812e-06
Iter: 246 loss: 1.54207191e-06
Iter: 247 loss: 1.53615963e-06
Iter: 248 loss: 1.55617226e-06
Iter: 249 loss: 1.53454243e-06
Iter: 250 loss: 1.52799703e-06
Iter: 251 loss: 1.53664416e-06
Iter: 252 loss: 1.52470693e-06
Iter: 253 loss: 1.52372593e-06
Iter: 254 loss: 1.52169503e-06
Iter: 255 loss: 1.51848383e-06
Iter: 256 loss: 1.5138246e-06
Iter: 257 loss: 1.51367e-06
Iter: 258 loss: 1.50962421e-06
Iter: 259 loss: 1.50897495e-06
Iter: 260 loss: 1.50623612e-06
Iter: 261 loss: 1.50288065e-06
Iter: 262 loss: 1.50283381e-06
Iter: 263 loss: 1.49973e-06
Iter: 264 loss: 1.50632468e-06
Iter: 265 loss: 1.49844664e-06
Iter: 266 loss: 1.49524635e-06
Iter: 267 loss: 1.4911127e-06
Iter: 268 loss: 1.49082973e-06
Iter: 269 loss: 1.48912613e-06
Iter: 270 loss: 1.48801098e-06
Iter: 271 loss: 1.48570803e-06
Iter: 272 loss: 1.48104345e-06
Iter: 273 loss: 1.56970623e-06
Iter: 274 loss: 1.48102185e-06
Iter: 275 loss: 1.47626065e-06
Iter: 276 loss: 1.51571612e-06
Iter: 277 loss: 1.47592255e-06
Iter: 278 loss: 1.47267167e-06
Iter: 279 loss: 1.47634091e-06
Iter: 280 loss: 1.47090486e-06
Iter: 281 loss: 1.46696311e-06
Iter: 282 loss: 1.50871369e-06
Iter: 283 loss: 1.4668401e-06
Iter: 284 loss: 1.46507716e-06
Iter: 285 loss: 1.46107914e-06
Iter: 286 loss: 1.51487313e-06
Iter: 287 loss: 1.46079401e-06
Iter: 288 loss: 1.45668491e-06
Iter: 289 loss: 1.49061498e-06
Iter: 290 loss: 1.45638592e-06
Iter: 291 loss: 1.45337208e-06
Iter: 292 loss: 1.50114533e-06
Iter: 293 loss: 1.45335559e-06
Iter: 294 loss: 1.45151648e-06
Iter: 295 loss: 1.44632838e-06
Iter: 296 loss: 1.47381286e-06
Iter: 297 loss: 1.4446988e-06
Iter: 298 loss: 1.43863883e-06
Iter: 299 loss: 1.470445e-06
Iter: 300 loss: 1.43770126e-06
Iter: 301 loss: 1.43466423e-06
Iter: 302 loss: 1.471828e-06
Iter: 303 loss: 1.43463603e-06
Iter: 304 loss: 1.43154216e-06
Iter: 305 loss: 1.44033459e-06
Iter: 306 loss: 1.43052512e-06
Iter: 307 loss: 1.42835393e-06
Iter: 308 loss: 1.42765862e-06
Iter: 309 loss: 1.42637634e-06
Iter: 310 loss: 1.4239082e-06
Iter: 311 loss: 1.42390741e-06
Iter: 312 loss: 1.42158217e-06
Iter: 313 loss: 1.41880923e-06
Iter: 314 loss: 1.41855469e-06
Iter: 315 loss: 1.41505893e-06
Iter: 316 loss: 1.42293879e-06
Iter: 317 loss: 1.41378302e-06
Iter: 318 loss: 1.40956047e-06
Iter: 319 loss: 1.42864508e-06
Iter: 320 loss: 1.40875238e-06
Iter: 321 loss: 1.40599e-06
Iter: 322 loss: 1.43946113e-06
Iter: 323 loss: 1.40597797e-06
Iter: 324 loss: 1.40416455e-06
Iter: 325 loss: 1.40049656e-06
Iter: 326 loss: 1.46260027e-06
Iter: 327 loss: 1.40038901e-06
Iter: 328 loss: 1.39685312e-06
Iter: 329 loss: 1.40448026e-06
Iter: 330 loss: 1.39552617e-06
Iter: 331 loss: 1.39173733e-06
Iter: 332 loss: 1.40567977e-06
Iter: 333 loss: 1.39080657e-06
Iter: 334 loss: 1.38959217e-06
Iter: 335 loss: 1.38906273e-06
Iter: 336 loss: 1.3871869e-06
Iter: 337 loss: 1.3827422e-06
Iter: 338 loss: 1.43449211e-06
Iter: 339 loss: 1.38237863e-06
Iter: 340 loss: 1.37976804e-06
Iter: 341 loss: 1.40370685e-06
Iter: 342 loss: 1.37968709e-06
Iter: 343 loss: 1.37772849e-06
Iter: 344 loss: 1.39317262e-06
Iter: 345 loss: 1.37758377e-06
Iter: 346 loss: 1.37576558e-06
Iter: 347 loss: 1.3731526e-06
Iter: 348 loss: 1.37308041e-06
Iter: 349 loss: 1.37076086e-06
Iter: 350 loss: 1.38861992e-06
Iter: 351 loss: 1.37058862e-06
Iter: 352 loss: 1.36804988e-06
Iter: 353 loss: 1.37861639e-06
Iter: 354 loss: 1.36749486e-06
Iter: 355 loss: 1.36550398e-06
Iter: 356 loss: 1.36376184e-06
Iter: 357 loss: 1.36325616e-06
Iter: 358 loss: 1.35947323e-06
Iter: 359 loss: 1.37232303e-06
Iter: 360 loss: 1.35846494e-06
Iter: 361 loss: 1.35628829e-06
Iter: 362 loss: 1.38587234e-06
Iter: 363 loss: 1.35627886e-06
Iter: 364 loss: 1.35436426e-06
Iter: 365 loss: 1.35503319e-06
Iter: 366 loss: 1.35301912e-06
Iter: 367 loss: 1.35088726e-06
Iter: 368 loss: 1.34922971e-06
Iter: 369 loss: 1.34856964e-06
Iter: 370 loss: 1.34579159e-06
Iter: 371 loss: 1.3475526e-06
Iter: 372 loss: 1.34404763e-06
Iter: 373 loss: 1.34522247e-06
Iter: 374 loss: 1.34265599e-06
Iter: 375 loss: 1.34160155e-06
Iter: 376 loss: 1.33885487e-06
Iter: 377 loss: 1.35668643e-06
Iter: 378 loss: 1.33816332e-06
Iter: 379 loss: 1.33464619e-06
Iter: 380 loss: 1.35067478e-06
Iter: 381 loss: 1.33398703e-06
Iter: 382 loss: 1.33176218e-06
Iter: 383 loss: 1.33173921e-06
Iter: 384 loss: 1.33055073e-06
Iter: 385 loss: 1.32791115e-06
Iter: 386 loss: 1.36498215e-06
Iter: 387 loss: 1.32781099e-06
Iter: 388 loss: 1.32630612e-06
Iter: 389 loss: 1.3262046e-06
Iter: 390 loss: 1.32468745e-06
Iter: 391 loss: 1.32728837e-06
Iter: 392 loss: 1.32393848e-06
Iter: 393 loss: 1.32258208e-06
Iter: 394 loss: 1.32180912e-06
Iter: 395 loss: 1.32124796e-06
Iter: 396 loss: 1.31858303e-06
Iter: 397 loss: 1.3243872e-06
Iter: 398 loss: 1.31749698e-06
Iter: 399 loss: 1.31561171e-06
Iter: 400 loss: 1.31562558e-06
Iter: 401 loss: 1.31404386e-06
Iter: 402 loss: 1.31278489e-06
Iter: 403 loss: 1.31232912e-06
Iter: 404 loss: 1.3100248e-06
Iter: 405 loss: 1.30956755e-06
Iter: 406 loss: 1.30807757e-06
Iter: 407 loss: 1.3074042e-06
Iter: 408 loss: 1.30695594e-06
Iter: 409 loss: 1.30571948e-06
Iter: 410 loss: 1.30421938e-06
Iter: 411 loss: 1.30404874e-06
Iter: 412 loss: 1.30224316e-06
Iter: 413 loss: 1.30073045e-06
Iter: 414 loss: 1.30022227e-06
Iter: 415 loss: 1.29838531e-06
Iter: 416 loss: 1.29826219e-06
Iter: 417 loss: 1.29667433e-06
Iter: 418 loss: 1.29395482e-06
Iter: 419 loss: 1.36273627e-06
Iter: 420 loss: 1.29395949e-06
Iter: 421 loss: 1.29087653e-06
Iter: 422 loss: 1.30093713e-06
Iter: 423 loss: 1.29001387e-06
Iter: 424 loss: 1.28838167e-06
Iter: 425 loss: 1.28816316e-06
Iter: 426 loss: 1.28709644e-06
Iter: 427 loss: 1.28504962e-06
Iter: 428 loss: 1.32731782e-06
Iter: 429 loss: 1.28504814e-06
Iter: 430 loss: 1.28325769e-06
Iter: 431 loss: 1.29735804e-06
Iter: 432 loss: 1.2831033e-06
Iter: 433 loss: 1.28153692e-06
Iter: 434 loss: 1.28759257e-06
Iter: 435 loss: 1.28109446e-06
Iter: 436 loss: 1.27935778e-06
Iter: 437 loss: 1.2834455e-06
Iter: 438 loss: 1.27869077e-06
Iter: 439 loss: 1.2770488e-06
Iter: 440 loss: 1.27544399e-06
Iter: 441 loss: 1.27509327e-06
Iter: 442 loss: 1.27260751e-06
Iter: 443 loss: 1.2760861e-06
Iter: 444 loss: 1.27137218e-06
Iter: 445 loss: 1.27041778e-06
Iter: 446 loss: 1.2699129e-06
Iter: 447 loss: 1.26851376e-06
Iter: 448 loss: 1.26676366e-06
Iter: 449 loss: 1.26655891e-06
Iter: 450 loss: 1.26525049e-06
Iter: 451 loss: 1.26520263e-06
Iter: 452 loss: 1.26414102e-06
Iter: 453 loss: 1.26241332e-06
Iter: 454 loss: 1.2886519e-06
Iter: 455 loss: 1.26241162e-06
Iter: 456 loss: 1.26099894e-06
Iter: 457 loss: 1.25988845e-06
Iter: 458 loss: 1.25943825e-06
Iter: 459 loss: 1.25739393e-06
Iter: 460 loss: 1.25765177e-06
Iter: 461 loss: 1.2558678e-06
Iter: 462 loss: 1.25538111e-06
Iter: 463 loss: 1.25454153e-06
Iter: 464 loss: 1.25362976e-06
Iter: 465 loss: 1.25137331e-06
Iter: 466 loss: 1.2692542e-06
Iter: 467 loss: 1.25096221e-06
Iter: 468 loss: 1.24930045e-06
Iter: 469 loss: 1.24930466e-06
Iter: 470 loss: 1.24799965e-06
Iter: 471 loss: 1.25280155e-06
Iter: 472 loss: 1.24772396e-06
Iter: 473 loss: 1.24640223e-06
Iter: 474 loss: 1.24946757e-06
Iter: 475 loss: 1.24588121e-06
Iter: 476 loss: 1.24458802e-06
Iter: 477 loss: 1.24347378e-06
Iter: 478 loss: 1.24313988e-06
Iter: 479 loss: 1.24090559e-06
Iter: 480 loss: 1.24412702e-06
Iter: 481 loss: 1.23975269e-06
Iter: 482 loss: 1.23912264e-06
Iter: 483 loss: 1.23838095e-06
Iter: 484 loss: 1.23774362e-06
Iter: 485 loss: 1.23569066e-06
Iter: 486 loss: 1.23947257e-06
Iter: 487 loss: 1.23435962e-06
Iter: 488 loss: 1.23196139e-06
Iter: 489 loss: 1.2557839e-06
Iter: 490 loss: 1.23192785e-06
Iter: 491 loss: 1.23041218e-06
Iter: 492 loss: 1.24241e-06
Iter: 493 loss: 1.23028019e-06
Iter: 494 loss: 1.22890174e-06
Iter: 495 loss: 1.23555287e-06
Iter: 496 loss: 1.2286e-06
Iter: 497 loss: 1.22751385e-06
Iter: 498 loss: 1.22585243e-06
Iter: 499 loss: 1.2258547e-06
Iter: 500 loss: 1.22408937e-06
Iter: 501 loss: 1.22404413e-06
Iter: 502 loss: 1.22323468e-06
Iter: 503 loss: 1.2211093e-06
Iter: 504 loss: 1.23953191e-06
Iter: 505 loss: 1.22078018e-06
Iter: 506 loss: 1.21866196e-06
Iter: 507 loss: 1.2411391e-06
Iter: 508 loss: 1.21860796e-06
Iter: 509 loss: 1.21726566e-06
Iter: 510 loss: 1.2252533e-06
Iter: 511 loss: 1.21712662e-06
Iter: 512 loss: 1.21598373e-06
Iter: 513 loss: 1.22117399e-06
Iter: 514 loss: 1.21575772e-06
Iter: 515 loss: 1.21451285e-06
Iter: 516 loss: 1.21368339e-06
Iter: 517 loss: 1.21320886e-06
Iter: 518 loss: 1.21167454e-06
Iter: 519 loss: 1.21827748e-06
Iter: 520 loss: 1.21136293e-06
Iter: 521 loss: 1.20933862e-06
Iter: 522 loss: 1.21554581e-06
Iter: 523 loss: 1.20872994e-06
Iter: 524 loss: 1.20761774e-06
Iter: 525 loss: 1.20562515e-06
Iter: 526 loss: 1.205643e-06
Iter: 527 loss: 1.20319623e-06
Iter: 528 loss: 1.21050482e-06
Iter: 529 loss: 1.20242544e-06
Iter: 530 loss: 1.20082746e-06
Iter: 531 loss: 1.21503217e-06
Iter: 532 loss: 1.2007705e-06
Iter: 533 loss: 1.19933884e-06
Iter: 534 loss: 1.21167875e-06
Iter: 535 loss: 1.19924061e-06
Iter: 536 loss: 1.19853053e-06
Iter: 537 loss: 1.19740457e-06
Iter: 538 loss: 1.19739354e-06
Iter: 539 loss: 1.19608717e-06
Iter: 540 loss: 1.19608603e-06
Iter: 541 loss: 1.19522326e-06
Iter: 542 loss: 1.19392121e-06
Iter: 543 loss: 1.19388415e-06
Iter: 544 loss: 1.19227809e-06
Iter: 545 loss: 1.19073457e-06
Iter: 546 loss: 1.19042761e-06
Iter: 547 loss: 1.18858952e-06
Iter: 548 loss: 1.18854587e-06
Iter: 549 loss: 1.18729577e-06
Iter: 550 loss: 1.19057643e-06
Iter: 551 loss: 1.18689911e-06
Iter: 552 loss: 1.18606249e-06
Iter: 553 loss: 1.18604703e-06
Iter: 554 loss: 1.18527191e-06
Iter: 555 loss: 1.18365915e-06
Iter: 556 loss: 1.21286439e-06
Iter: 557 loss: 1.1836446e-06
Iter: 558 loss: 1.18290916e-06
Iter: 559 loss: 1.18275864e-06
Iter: 560 loss: 1.18197761e-06
Iter: 561 loss: 1.17991897e-06
Iter: 562 loss: 1.19774677e-06
Iter: 563 loss: 1.17958734e-06
Iter: 564 loss: 1.17748891e-06
Iter: 565 loss: 1.18438948e-06
Iter: 566 loss: 1.17692139e-06
Iter: 567 loss: 1.17507716e-06
Iter: 568 loss: 1.17924037e-06
Iter: 569 loss: 1.17435377e-06
Iter: 570 loss: 1.17379182e-06
Iter: 571 loss: 1.1734478e-06
Iter: 572 loss: 1.17276625e-06
Iter: 573 loss: 1.17147704e-06
Iter: 574 loss: 1.19760909e-06
Iter: 575 loss: 1.17148147e-06
Iter: 576 loss: 1.17052673e-06
Iter: 577 loss: 1.17052014e-06
Iter: 578 loss: 1.16949218e-06
Iter: 579 loss: 1.16858109e-06
Iter: 580 loss: 1.16826811e-06
Iter: 581 loss: 1.16690694e-06
Iter: 582 loss: 1.16869603e-06
Iter: 583 loss: 1.16620913e-06
Iter: 584 loss: 1.16477713e-06
Iter: 585 loss: 1.17562604e-06
Iter: 586 loss: 1.16467345e-06
Iter: 587 loss: 1.16337446e-06
Iter: 588 loss: 1.17079924e-06
Iter: 589 loss: 1.16319609e-06
Iter: 590 loss: 1.16201625e-06
Iter: 591 loss: 1.16219576e-06
Iter: 592 loss: 1.16113642e-06
Iter: 593 loss: 1.16041952e-06
Iter: 594 loss: 1.16042e-06
Iter: 595 loss: 1.15973012e-06
Iter: 596 loss: 1.15823354e-06
Iter: 597 loss: 1.17854677e-06
Iter: 598 loss: 1.15815237e-06
Iter: 599 loss: 1.15688442e-06
Iter: 600 loss: 1.16047113e-06
Iter: 601 loss: 1.1564714e-06
Iter: 602 loss: 1.15503099e-06
Iter: 603 loss: 1.15449916e-06
Iter: 604 loss: 1.1536772e-06
Iter: 605 loss: 1.15279101e-06
Iter: 606 loss: 1.1524603e-06
Iter: 607 loss: 1.15133196e-06
Iter: 608 loss: 1.14977047e-06
Iter: 609 loss: 1.1497084e-06
Iter: 610 loss: 1.14875206e-06
Iter: 611 loss: 1.14874081e-06
Iter: 612 loss: 1.14781665e-06
Iter: 613 loss: 1.14748605e-06
Iter: 614 loss: 1.14693751e-06
Iter: 615 loss: 1.14594445e-06
Iter: 616 loss: 1.14577256e-06
Iter: 617 loss: 1.14503177e-06
Iter: 618 loss: 1.14367015e-06
Iter: 619 loss: 1.15102e-06
Iter: 620 loss: 1.1434704e-06
Iter: 621 loss: 1.14244881e-06
Iter: 622 loss: 1.15455953e-06
Iter: 623 loss: 1.14246086e-06
Iter: 624 loss: 1.14155546e-06
Iter: 625 loss: 1.14274121e-06
Iter: 626 loss: 1.14105626e-06
Iter: 627 loss: 1.14009651e-06
Iter: 628 loss: 1.1402318e-06
Iter: 629 loss: 1.13940007e-06
Iter: 630 loss: 1.13782983e-06
Iter: 631 loss: 1.14753163e-06
Iter: 632 loss: 1.13766282e-06
Iter: 633 loss: 1.13668261e-06
Iter: 634 loss: 1.13549288e-06
Iter: 635 loss: 1.13540204e-06
Iter: 636 loss: 1.13414058e-06
Iter: 637 loss: 1.13810415e-06
Iter: 638 loss: 1.13377666e-06
Iter: 639 loss: 1.13280839e-06
Iter: 640 loss: 1.14442923e-06
Iter: 641 loss: 1.13278e-06
Iter: 642 loss: 1.13178044e-06
Iter: 643 loss: 1.13300973e-06
Iter: 644 loss: 1.13128942e-06
Iter: 645 loss: 1.13031479e-06
Iter: 646 loss: 1.13020678e-06
Iter: 647 loss: 1.12951955e-06
Iter: 648 loss: 1.12779e-06
Iter: 649 loss: 1.13902945e-06
Iter: 650 loss: 1.12758471e-06
Iter: 651 loss: 1.12668181e-06
Iter: 652 loss: 1.12482473e-06
Iter: 653 loss: 1.15309467e-06
Iter: 654 loss: 1.12474049e-06
Iter: 655 loss: 1.12286637e-06
Iter: 656 loss: 1.13142028e-06
Iter: 657 loss: 1.12251792e-06
Iter: 658 loss: 1.12177372e-06
Iter: 659 loss: 1.12162e-06
Iter: 660 loss: 1.12086855e-06
Iter: 661 loss: 1.12235273e-06
Iter: 662 loss: 1.12051907e-06
Iter: 663 loss: 1.11984707e-06
Iter: 664 loss: 1.11931058e-06
Iter: 665 loss: 1.11908389e-06
Iter: 666 loss: 1.11767645e-06
Iter: 667 loss: 1.12542853e-06
Iter: 668 loss: 1.11747499e-06
Iter: 669 loss: 1.11642339e-06
Iter: 670 loss: 1.11817349e-06
Iter: 671 loss: 1.1159608e-06
Iter: 672 loss: 1.11490215e-06
Iter: 673 loss: 1.1140479e-06
Iter: 674 loss: 1.1137322e-06
Iter: 675 loss: 1.11229929e-06
Iter: 676 loss: 1.11720919e-06
Iter: 677 loss: 1.11192639e-06
Iter: 678 loss: 1.11083955e-06
Iter: 679 loss: 1.11825443e-06
Iter: 680 loss: 1.11070631e-06
Iter: 681 loss: 1.10978419e-06
Iter: 682 loss: 1.11952011e-06
Iter: 683 loss: 1.10976475e-06
Iter: 684 loss: 1.10917085e-06
Iter: 685 loss: 1.10802262e-06
Iter: 686 loss: 1.13290423e-06
Iter: 687 loss: 1.10801739e-06
Iter: 688 loss: 1.1066154e-06
Iter: 689 loss: 1.12401312e-06
Iter: 690 loss: 1.10664632e-06
Iter: 691 loss: 1.10574115e-06
Iter: 692 loss: 1.10424276e-06
Iter: 693 loss: 1.13842202e-06
Iter: 694 loss: 1.10424116e-06
Iter: 695 loss: 1.10262317e-06
Iter: 696 loss: 1.1074701e-06
Iter: 697 loss: 1.10214933e-06
Iter: 698 loss: 1.10085728e-06
Iter: 699 loss: 1.10936639e-06
Iter: 700 loss: 1.10067845e-06
Iter: 701 loss: 1.09947439e-06
Iter: 702 loss: 1.11454267e-06
Iter: 703 loss: 1.09945768e-06
Iter: 704 loss: 1.09897519e-06
Iter: 705 loss: 1.0981604e-06
Iter: 706 loss: 1.09813527e-06
Iter: 707 loss: 1.09709117e-06
Iter: 708 loss: 1.1052407e-06
Iter: 709 loss: 1.09701807e-06
Iter: 710 loss: 1.09612233e-06
Iter: 711 loss: 1.0986962e-06
Iter: 712 loss: 1.0958538e-06
Iter: 713 loss: 1.09482562e-06
Iter: 714 loss: 1.09406642e-06
Iter: 715 loss: 1.0937672e-06
Iter: 716 loss: 1.09262328e-06
Iter: 717 loss: 1.09594919e-06
Iter: 718 loss: 1.09227938e-06
Iter: 719 loss: 1.09110056e-06
Iter: 720 loss: 1.09075825e-06
Iter: 721 loss: 1.09009898e-06
Iter: 722 loss: 1.0905037e-06
Iter: 723 loss: 1.08950758e-06
Iter: 724 loss: 1.08902066e-06
Iter: 725 loss: 1.08784354e-06
Iter: 726 loss: 1.09848293e-06
Iter: 727 loss: 1.08762481e-06
Iter: 728 loss: 1.08627466e-06
Iter: 729 loss: 1.0899023e-06
Iter: 730 loss: 1.08580412e-06
Iter: 731 loss: 1.08450877e-06
Iter: 732 loss: 1.09008386e-06
Iter: 733 loss: 1.08424638e-06
Iter: 734 loss: 1.08286213e-06
Iter: 735 loss: 1.09291943e-06
Iter: 736 loss: 1.08273764e-06
Iter: 737 loss: 1.08203767e-06
Iter: 738 loss: 1.0817472e-06
Iter: 739 loss: 1.08133474e-06
Iter: 740 loss: 1.08028837e-06
Iter: 741 loss: 1.09615439e-06
Iter: 742 loss: 1.08028564e-06
Iter: 743 loss: 1.07984863e-06
Iter: 744 loss: 1.07903975e-06
Iter: 745 loss: 1.07903702e-06
Iter: 746 loss: 1.07822939e-06
Iter: 747 loss: 1.09124608e-06
Iter: 748 loss: 1.07823462e-06
Iter: 749 loss: 1.0777004e-06
Iter: 750 loss: 1.07694416e-06
Iter: 751 loss: 1.07693177e-06
Iter: 752 loss: 1.07574579e-06
Iter: 753 loss: 1.07767107e-06
Iter: 754 loss: 1.07516348e-06
Iter: 755 loss: 1.07396136e-06
Iter: 756 loss: 1.07781932e-06
Iter: 757 loss: 1.07360029e-06
Iter: 758 loss: 1.07275207e-06
Iter: 759 loss: 1.07274082e-06
Iter: 760 loss: 1.07212361e-06
Iter: 761 loss: 1.07112783e-06
Iter: 762 loss: 1.07113146e-06
Iter: 763 loss: 1.07000255e-06
Iter: 764 loss: 1.0709764e-06
Iter: 765 loss: 1.0693542e-06
Iter: 766 loss: 1.06851144e-06
Iter: 767 loss: 1.07919402e-06
Iter: 768 loss: 1.0684937e-06
Iter: 769 loss: 1.06744551e-06
Iter: 770 loss: 1.06781192e-06
Iter: 771 loss: 1.06674543e-06
Iter: 772 loss: 1.06588732e-06
Iter: 773 loss: 1.07143387e-06
Iter: 774 loss: 1.06576795e-06
Iter: 775 loss: 1.06478728e-06
Iter: 776 loss: 1.06622497e-06
Iter: 777 loss: 1.06432367e-06
Iter: 778 loss: 1.06334267e-06
Iter: 779 loss: 1.06220068e-06
Iter: 780 loss: 1.06207176e-06
Iter: 781 loss: 1.06176708e-06
Iter: 782 loss: 1.06134826e-06
Iter: 783 loss: 1.06091466e-06
Iter: 784 loss: 1.0598709e-06
Iter: 785 loss: 1.07474671e-06
Iter: 786 loss: 1.05979893e-06
Iter: 787 loss: 1.05886193e-06
Iter: 788 loss: 1.06568268e-06
Iter: 789 loss: 1.05875711e-06
Iter: 790 loss: 1.05781055e-06
Iter: 791 loss: 1.0580751e-06
Iter: 792 loss: 1.05713389e-06
Iter: 793 loss: 1.05608217e-06
Iter: 794 loss: 1.0574613e-06
Iter: 795 loss: 1.05551339e-06
Iter: 796 loss: 1.05457684e-06
Iter: 797 loss: 1.05454365e-06
Iter: 798 loss: 1.05382105e-06
Iter: 799 loss: 1.05288325e-06
Iter: 800 loss: 1.05279025e-06
Iter: 801 loss: 1.05176446e-06
Iter: 802 loss: 1.05190816e-06
Iter: 803 loss: 1.05093102e-06
Iter: 804 loss: 1.04984247e-06
Iter: 805 loss: 1.05478989e-06
Iter: 806 loss: 1.04967012e-06
Iter: 807 loss: 1.04883759e-06
Iter: 808 loss: 1.04879791e-06
Iter: 809 loss: 1.04827427e-06
Iter: 810 loss: 1.04782418e-06
Iter: 811 loss: 1.04764672e-06
Iter: 812 loss: 1.04657056e-06
Iter: 813 loss: 1.05184131e-06
Iter: 814 loss: 1.04639059e-06
Iter: 815 loss: 1.04577691e-06
Iter: 816 loss: 1.04461583e-06
Iter: 817 loss: 1.06638765e-06
Iter: 818 loss: 1.04459605e-06
Iter: 819 loss: 1.04370258e-06
Iter: 820 loss: 1.04367859e-06
Iter: 821 loss: 1.04276103e-06
Iter: 822 loss: 1.04278388e-06
Iter: 823 loss: 1.04206435e-06
Iter: 824 loss: 1.04124069e-06
Iter: 825 loss: 1.04164735e-06
Iter: 826 loss: 1.0406593e-06
Iter: 827 loss: 1.03984814e-06
Iter: 828 loss: 1.04564e-06
Iter: 829 loss: 1.03978834e-06
Iter: 830 loss: 1.03893194e-06
Iter: 831 loss: 1.03855677e-06
Iter: 832 loss: 1.03811146e-06
Iter: 833 loss: 1.03711727e-06
Iter: 834 loss: 1.04109597e-06
Iter: 835 loss: 1.0369024e-06
Iter: 836 loss: 1.03573188e-06
Iter: 837 loss: 1.04645505e-06
Iter: 838 loss: 1.03570687e-06
Iter: 839 loss: 1.03500395e-06
Iter: 840 loss: 1.03392517e-06
Iter: 841 loss: 1.03390596e-06
Iter: 842 loss: 1.03274101e-06
Iter: 843 loss: 1.03583659e-06
Iter: 844 loss: 1.03241416e-06
Iter: 845 loss: 1.0322118e-06
Iter: 846 loss: 1.03189404e-06
Iter: 847 loss: 1.03150546e-06
Iter: 848 loss: 1.0305946e-06
Iter: 849 loss: 1.04199762e-06
Iter: 850 loss: 1.03053446e-06
Iter: 851 loss: 1.02999195e-06
Iter: 852 loss: 1.02999013e-06
Iter: 853 loss: 1.02939293e-06
Iter: 854 loss: 1.02829824e-06
Iter: 855 loss: 1.05267145e-06
Iter: 856 loss: 1.02828938e-06
Iter: 857 loss: 1.02757099e-06
Iter: 858 loss: 1.02756508e-06
Iter: 859 loss: 1.02683271e-06
Iter: 860 loss: 1.02734384e-06
Iter: 861 loss: 1.02638808e-06
Iter: 862 loss: 1.02555896e-06
Iter: 863 loss: 1.02444233e-06
Iter: 864 loss: 1.02438082e-06
Iter: 865 loss: 1.02331228e-06
Iter: 866 loss: 1.0261856e-06
Iter: 867 loss: 1.02291381e-06
Iter: 868 loss: 1.02172169e-06
Iter: 869 loss: 1.0289026e-06
Iter: 870 loss: 1.02157901e-06
Iter: 871 loss: 1.02059494e-06
Iter: 872 loss: 1.02810895e-06
Iter: 873 loss: 1.02050035e-06
Iter: 874 loss: 1.01989713e-06
Iter: 875 loss: 1.02617366e-06
Iter: 876 loss: 1.01990361e-06
Iter: 877 loss: 1.01941828e-06
Iter: 878 loss: 1.01833189e-06
Iter: 879 loss: 1.03465709e-06
Iter: 880 loss: 1.01827447e-06
Iter: 881 loss: 1.01718456e-06
Iter: 882 loss: 1.02118679e-06
Iter: 883 loss: 1.01688897e-06
Iter: 884 loss: 1.01642377e-06
Iter: 885 loss: 1.01634919e-06
Iter: 886 loss: 1.01577575e-06
Iter: 887 loss: 1.01490298e-06
Iter: 888 loss: 1.01489104e-06
Iter: 889 loss: 1.01399246e-06
Iter: 890 loss: 1.01361627e-06
Iter: 891 loss: 1.01313879e-06
Iter: 892 loss: 1.01263663e-06
Iter: 893 loss: 1.01252203e-06
Iter: 894 loss: 1.01181013e-06
Iter: 895 loss: 1.0106055e-06
Iter: 896 loss: 1.01062358e-06
Iter: 897 loss: 1.00966656e-06
Iter: 898 loss: 1.01757644e-06
Iter: 899 loss: 1.00957391e-06
Iter: 900 loss: 1.00877151e-06
Iter: 901 loss: 1.0124013e-06
Iter: 902 loss: 1.00860473e-06
Iter: 903 loss: 1.00794864e-06
Iter: 904 loss: 1.00723628e-06
Iter: 905 loss: 1.00712759e-06
Iter: 906 loss: 1.00624322e-06
Iter: 907 loss: 1.00941554e-06
Iter: 908 loss: 1.00599095e-06
Iter: 909 loss: 1.00498335e-06
Iter: 910 loss: 1.01389583e-06
Iter: 911 loss: 1.00489706e-06
Iter: 912 loss: 1.00442639e-06
Iter: 913 loss: 1.00381442e-06
Iter: 914 loss: 1.00378179e-06
Iter: 915 loss: 1.00290799e-06
Iter: 916 loss: 1.00637783e-06
Iter: 917 loss: 1.00270472e-06
Iter: 918 loss: 1.00195734e-06
Iter: 919 loss: 1.01043156e-06
Iter: 920 loss: 1.00192824e-06
Iter: 921 loss: 1.00133775e-06
Iter: 922 loss: 1.00180546e-06
Iter: 923 loss: 1.00092302e-06
Iter: 924 loss: 1.00027148e-06
Iter: 925 loss: 9.9966428e-07
Iter: 926 loss: 9.99490453e-07
Iter: 927 loss: 9.98693395e-07
Iter: 928 loss: 1.00529428e-06
Iter: 929 loss: 9.98638257e-07
Iter: 930 loss: 9.97749794e-07
Iter: 931 loss: 1.00176703e-06
Iter: 932 loss: 9.97672601e-07
Iter: 933 loss: 9.97110874e-07
Iter: 934 loss: 9.96228e-07
Iter: 935 loss: 9.96235144e-07
Iter: 936 loss: 9.95524715e-07
Iter: 937 loss: 9.9547583e-07
Iter: 938 loss: 9.94872835e-07
Iter: 939 loss: 9.93561571e-07
Iter: 940 loss: 1.01409e-06
Iter: 941 loss: 9.93511321e-07
Iter: 942 loss: 9.92299e-07
Iter: 943 loss: 1.00003274e-06
Iter: 944 loss: 9.92211199e-07
Iter: 945 loss: 9.91516231e-07
Iter: 946 loss: 1.00202419e-06
Iter: 947 loss: 9.91522e-07
Iter: 948 loss: 9.90819444e-07
Iter: 949 loss: 9.90375838e-07
Iter: 950 loss: 9.90099579e-07
Iter: 951 loss: 9.89381761e-07
Iter: 952 loss: 9.89261707e-07
Iter: 953 loss: 9.88760803e-07
Iter: 954 loss: 9.87824933e-07
Iter: 955 loss: 9.9496333e-07
Iter: 956 loss: 9.8774467e-07
Iter: 957 loss: 9.87049589e-07
Iter: 958 loss: 9.94633524e-07
Iter: 959 loss: 9.8704e-07
Iter: 960 loss: 9.86408168e-07
Iter: 961 loss: 9.86565851e-07
Iter: 962 loss: 9.85959e-07
Iter: 963 loss: 9.85132829e-07
Iter: 964 loss: 9.84683311e-07
Iter: 965 loss: 9.84349299e-07
Iter: 966 loss: 9.8327132e-07
Iter: 967 loss: 9.86018222e-07
Iter: 968 loss: 9.82901e-07
Iter: 969 loss: 9.81983248e-07
Iter: 970 loss: 9.81970516e-07
Iter: 971 loss: 9.81537141e-07
Iter: 972 loss: 9.80559776e-07
Iter: 973 loss: 9.95754e-07
Iter: 974 loss: 9.8054079e-07
Iter: 975 loss: 9.79785341e-07
Iter: 976 loss: 9.79765559e-07
Iter: 977 loss: 9.79187917e-07
Iter: 978 loss: 9.78933713e-07
Iter: 979 loss: 9.78638809e-07
Iter: 980 loss: 9.77905e-07
Iter: 981 loss: 9.76992624e-07
Iter: 982 loss: 9.76911906e-07
Iter: 983 loss: 9.76279e-07
Iter: 984 loss: 9.7607824e-07
Iter: 985 loss: 9.75345074e-07
Iter: 986 loss: 9.75294483e-07
Iter: 987 loss: 9.74751742e-07
Iter: 988 loss: 9.73936e-07
Iter: 989 loss: 9.73260512e-07
Iter: 990 loss: 9.73026886e-07
Iter: 991 loss: 9.72933208e-07
Iter: 992 loss: 9.72562361e-07
Iter: 993 loss: 9.72159569e-07
Iter: 994 loss: 9.71617e-07
Iter: 995 loss: 9.71564759e-07
Iter: 996 loss: 9.70936185e-07
Iter: 997 loss: 9.70203132e-07
Iter: 998 loss: 9.70144129e-07
Iter: 999 loss: 9.69324788e-07
Iter: 1000 loss: 9.69311486e-07
Iter: 1001 loss: 9.68533641e-07
Iter: 1002 loss: 9.69266694e-07
Iter: 1003 loss: 9.6805627e-07
Iter: 1004 loss: 9.67306391e-07
Iter: 1005 loss: 9.69231451e-07
Iter: 1006 loss: 9.67063443e-07
Iter: 1007 loss: 9.6632084e-07
Iter: 1008 loss: 9.67715e-07
Iter: 1009 loss: 9.65999106e-07
Iter: 1010 loss: 9.65200343e-07
Iter: 1011 loss: 9.70511564e-07
Iter: 1012 loss: 9.65123377e-07
Iter: 1013 loss: 9.64608375e-07
Iter: 1014 loss: 9.64453079e-07
Iter: 1015 loss: 9.64076207e-07
Iter: 1016 loss: 9.63287903e-07
Iter: 1017 loss: 9.63106572e-07
Iter: 1018 loss: 9.62644435e-07
Iter: 1019 loss: 9.61998353e-07
Iter: 1020 loss: 9.61972091e-07
Iter: 1021 loss: 9.6128e-07
Iter: 1022 loss: 9.61423552e-07
Iter: 1023 loss: 9.60726879e-07
Iter: 1024 loss: 9.59934368e-07
Iter: 1025 loss: 9.59906288e-07
Iter: 1026 loss: 9.5931739e-07
Iter: 1027 loss: 9.58789542e-07
Iter: 1028 loss: 9.58724513e-07
Iter: 1029 loss: 9.58140276e-07
Iter: 1030 loss: 9.57115617e-07
Iter: 1031 loss: 9.57117663e-07
Iter: 1032 loss: 9.56335271e-07
Iter: 1033 loss: 9.59456202e-07
Iter: 1034 loss: 9.56157692e-07
Iter: 1035 loss: 9.5560722e-07
Iter: 1036 loss: 9.55615178e-07
Iter: 1037 loss: 9.55129735e-07
Iter: 1038 loss: 9.54388497e-07
Iter: 1039 loss: 9.54387133e-07
Iter: 1040 loss: 9.53460926e-07
Iter: 1041 loss: 9.59764066e-07
Iter: 1042 loss: 9.53381971e-07
Iter: 1043 loss: 9.5272361e-07
Iter: 1044 loss: 9.54894631e-07
Iter: 1045 loss: 9.52543e-07
Iter: 1046 loss: 9.51810478e-07
Iter: 1047 loss: 9.52600033e-07
Iter: 1048 loss: 9.5147675e-07
Iter: 1049 loss: 9.50587605e-07
Iter: 1050 loss: 9.49843468e-07
Iter: 1051 loss: 9.49539412e-07
Iter: 1052 loss: 9.48510149e-07
Iter: 1053 loss: 9.58549e-07
Iter: 1054 loss: 9.48486843e-07
Iter: 1055 loss: 9.47596845e-07
Iter: 1056 loss: 9.57301836e-07
Iter: 1057 loss: 9.4759821e-07
Iter: 1058 loss: 9.47162221e-07
Iter: 1059 loss: 9.46227374e-07
Iter: 1060 loss: 9.61590331e-07
Iter: 1061 loss: 9.4621e-07
Iter: 1062 loss: 9.45534964e-07
Iter: 1063 loss: 9.4553e-07
Iter: 1064 loss: 9.44826752e-07
Iter: 1065 loss: 9.47128456e-07
Iter: 1066 loss: 9.44656335e-07
Iter: 1067 loss: 9.44113651e-07
Iter: 1068 loss: 9.43085411e-07
Iter: 1069 loss: 9.63727189e-07
Iter: 1070 loss: 9.43084444e-07
Iter: 1071 loss: 9.4260372e-07
Iter: 1072 loss: 9.4246974e-07
Iter: 1073 loss: 9.41873395e-07
Iter: 1074 loss: 9.41151143e-07
Iter: 1075 loss: 9.41073438e-07
Iter: 1076 loss: 9.40349423e-07
Iter: 1077 loss: 9.45626823e-07
Iter: 1078 loss: 9.40283144e-07
Iter: 1079 loss: 9.39653319e-07
Iter: 1080 loss: 9.41789665e-07
Iter: 1081 loss: 9.39488416e-07
Iter: 1082 loss: 9.38891617e-07
Iter: 1083 loss: 9.39613642e-07
Iter: 1084 loss: 9.38592336e-07
Iter: 1085 loss: 9.37847858e-07
Iter: 1086 loss: 9.38320795e-07
Iter: 1087 loss: 9.37332175e-07
Iter: 1088 loss: 9.36617482e-07
Iter: 1089 loss: 9.39559925e-07
Iter: 1090 loss: 9.36447236e-07
Iter: 1091 loss: 9.35791149e-07
Iter: 1092 loss: 9.4444863e-07
Iter: 1093 loss: 9.35782282e-07
Iter: 1094 loss: 9.35315029e-07
Iter: 1095 loss: 9.34219202e-07
Iter: 1096 loss: 9.45129273e-07
Iter: 1097 loss: 9.3405896e-07
Iter: 1098 loss: 9.33259e-07
Iter: 1099 loss: 9.4569e-07
Iter: 1100 loss: 9.33260537e-07
Iter: 1101 loss: 9.32475302e-07
Iter: 1102 loss: 9.37864968e-07
Iter: 1103 loss: 9.32447165e-07
Iter: 1104 loss: 9.3197e-07
Iter: 1105 loss: 9.30968781e-07
Iter: 1106 loss: 9.49628941e-07
Iter: 1107 loss: 9.30975261e-07
Iter: 1108 loss: 9.30518127e-07
Iter: 1109 loss: 9.30433316e-07
Iter: 1110 loss: 9.29911835e-07
Iter: 1111 loss: 9.29319754e-07
Iter: 1112 loss: 9.29244379e-07
Iter: 1113 loss: 9.2839332e-07
Iter: 1114 loss: 9.29436965e-07
Iter: 1115 loss: 9.27959491e-07
Iter: 1116 loss: 9.27091378e-07
Iter: 1117 loss: 9.38153846e-07
Iter: 1118 loss: 9.27083533e-07
Iter: 1119 loss: 9.26594055e-07
Iter: 1120 loss: 9.26777489e-07
Iter: 1121 loss: 9.26232133e-07
Iter: 1122 loss: 9.25569964e-07
Iter: 1123 loss: 9.26881512e-07
Iter: 1124 loss: 9.25280233e-07
Iter: 1125 loss: 9.24639608e-07
Iter: 1126 loss: 9.26854511e-07
Iter: 1127 loss: 9.24463791e-07
Iter: 1128 loss: 9.23945549e-07
Iter: 1129 loss: 9.31251748e-07
Iter: 1130 loss: 9.23951802e-07
Iter: 1131 loss: 9.23614948e-07
Iter: 1132 loss: 9.22829031e-07
Iter: 1133 loss: 9.32174714e-07
Iter: 1134 loss: 9.22767185e-07
Iter: 1135 loss: 9.21986441e-07
Iter: 1136 loss: 9.27547376e-07
Iter: 1137 loss: 9.21915102e-07
Iter: 1138 loss: 9.21094511e-07
Iter: 1139 loss: 9.27300277e-07
Iter: 1140 loss: 9.21024366e-07
Iter: 1141 loss: 9.20607533e-07
Iter: 1142 loss: 9.19734532e-07
Iter: 1143 loss: 9.35846515e-07
Iter: 1144 loss: 9.19721231e-07
Iter: 1145 loss: 9.19177467e-07
Iter: 1146 loss: 9.19105503e-07
Iter: 1147 loss: 9.1863393e-07
Iter: 1148 loss: 9.18312708e-07
Iter: 1149 loss: 9.18168e-07
Iter: 1150 loss: 9.17576e-07
Iter: 1151 loss: 9.17991315e-07
Iter: 1152 loss: 9.17228e-07
Iter: 1153 loss: 9.16519753e-07
Iter: 1154 loss: 9.23923892e-07
Iter: 1155 loss: 9.16506906e-07
Iter: 1156 loss: 9.16073759e-07
Iter: 1157 loss: 9.15667499e-07
Iter: 1158 loss: 9.15537385e-07
Iter: 1159 loss: 9.14697182e-07
Iter: 1160 loss: 9.16408851e-07
Iter: 1161 loss: 9.14329348e-07
Iter: 1162 loss: 9.13570261e-07
Iter: 1163 loss: 9.21093545e-07
Iter: 1164 loss: 9.13527799e-07
Iter: 1165 loss: 9.12986593e-07
Iter: 1166 loss: 9.16130659e-07
Iter: 1167 loss: 9.12904056e-07
Iter: 1168 loss: 9.12506096e-07
Iter: 1169 loss: 9.11631673e-07
Iter: 1170 loss: 9.25030463e-07
Iter: 1171 loss: 9.11559255e-07
Iter: 1172 loss: 9.11385655e-07
Iter: 1173 loss: 9.11177665e-07
Iter: 1174 loss: 9.1077095e-07
Iter: 1175 loss: 9.10464507e-07
Iter: 1176 loss: 9.10348035e-07
Iter: 1177 loss: 9.09827861e-07
Iter: 1178 loss: 9.09901928e-07
Iter: 1179 loss: 9.09447408e-07
Iter: 1180 loss: 9.08756533e-07
Iter: 1181 loss: 9.16517308e-07
Iter: 1182 loss: 9.08732773e-07
Iter: 1183 loss: 9.08217373e-07
Iter: 1184 loss: 9.07495235e-07
Iter: 1185 loss: 9.07453455e-07
Iter: 1186 loss: 9.0669937e-07
Iter: 1187 loss: 9.12774567e-07
Iter: 1188 loss: 9.06632351e-07
Iter: 1189 loss: 9.05867296e-07
Iter: 1190 loss: 9.07947651e-07
Iter: 1191 loss: 9.05573927e-07
Iter: 1192 loss: 9.05003276e-07
Iter: 1193 loss: 9.05370143e-07
Iter: 1194 loss: 9.04625267e-07
Iter: 1195 loss: 9.03958778e-07
Iter: 1196 loss: 9.0768674e-07
Iter: 1197 loss: 9.038489e-07
Iter: 1198 loss: 9.03333614e-07
Iter: 1199 loss: 9.05144134e-07
Iter: 1200 loss: 9.03206171e-07
Iter: 1201 loss: 9.0259357e-07
Iter: 1202 loss: 9.053133e-07
Iter: 1203 loss: 9.02460386e-07
Iter: 1204 loss: 9.02041847e-07
Iter: 1205 loss: 9.01320618e-07
Iter: 1206 loss: 9.01309761e-07
Iter: 1207 loss: 9.00718589e-07
Iter: 1208 loss: 9.00715861e-07
Iter: 1209 loss: 9.00126906e-07
Iter: 1210 loss: 8.99467182e-07
Iter: 1211 loss: 8.99389306e-07
Iter: 1212 loss: 8.98832127e-07
Iter: 1213 loss: 9.00294083e-07
Iter: 1214 loss: 8.98620101e-07
Iter: 1215 loss: 8.97813663e-07
Iter: 1216 loss: 9.01456e-07
Iter: 1217 loss: 8.97657401e-07
Iter: 1218 loss: 8.97182531e-07
Iter: 1219 loss: 8.97043151e-07
Iter: 1220 loss: 8.96753136e-07
Iter: 1221 loss: 8.96224378e-07
Iter: 1222 loss: 8.99999463e-07
Iter: 1223 loss: 8.96197548e-07
Iter: 1224 loss: 8.95637868e-07
Iter: 1225 loss: 8.96293159e-07
Iter: 1226 loss: 8.95326e-07
Iter: 1227 loss: 8.94771915e-07
Iter: 1228 loss: 8.95171183e-07
Iter: 1229 loss: 8.94441257e-07
Iter: 1230 loss: 8.93719857e-07
Iter: 1231 loss: 8.9537e-07
Iter: 1232 loss: 8.93420633e-07
Iter: 1233 loss: 8.92680589e-07
Iter: 1234 loss: 8.96801453e-07
Iter: 1235 loss: 8.92576736e-07
Iter: 1236 loss: 8.91844422e-07
Iter: 1237 loss: 8.95803566e-07
Iter: 1238 loss: 8.91706236e-07
Iter: 1239 loss: 8.91236709e-07
Iter: 1240 loss: 8.90902697e-07
Iter: 1241 loss: 8.90723868e-07
Iter: 1242 loss: 8.90335457e-07
Iter: 1243 loss: 8.90274464e-07
Iter: 1244 loss: 8.89948922e-07
Iter: 1245 loss: 8.8916704e-07
Iter: 1246 loss: 9.0083438e-07
Iter: 1247 loss: 8.89152034e-07
Iter: 1248 loss: 8.88537e-07
Iter: 1249 loss: 8.9355126e-07
Iter: 1250 loss: 8.88490149e-07
Iter: 1251 loss: 8.87802e-07
Iter: 1252 loss: 8.90037427e-07
Iter: 1253 loss: 8.87569797e-07
Iter: 1254 loss: 8.87081e-07
Iter: 1255 loss: 8.86115458e-07
Iter: 1256 loss: 9.09860432e-07
Iter: 1257 loss: 8.86131602e-07
Iter: 1258 loss: 8.85522923e-07
Iter: 1259 loss: 8.85470513e-07
Iter: 1260 loss: 8.8488008e-07
Iter: 1261 loss: 8.84621e-07
Iter: 1262 loss: 8.84323754e-07
Iter: 1263 loss: 8.83585699e-07
Iter: 1264 loss: 8.85219e-07
Iter: 1265 loss: 8.83282894e-07
Iter: 1266 loss: 8.82789e-07
Iter: 1267 loss: 8.87085605e-07
Iter: 1268 loss: 8.82740437e-07
Iter: 1269 loss: 8.82259428e-07
Iter: 1270 loss: 8.8303e-07
Iter: 1271 loss: 8.82022675e-07
Iter: 1272 loss: 8.8127382e-07
Iter: 1273 loss: 8.83438702e-07
Iter: 1274 loss: 8.81086237e-07
Iter: 1275 loss: 8.80585958e-07
Iter: 1276 loss: 8.80813559e-07
Iter: 1277 loss: 8.8026718e-07
Iter: 1278 loss: 8.79678112e-07
Iter: 1279 loss: 8.87514091e-07
Iter: 1280 loss: 8.79655374e-07
Iter: 1281 loss: 8.79224899e-07
Iter: 1282 loss: 8.78410049e-07
Iter: 1283 loss: 8.95328e-07
Iter: 1284 loss: 8.78403739e-07
Iter: 1285 loss: 8.77835305e-07
Iter: 1286 loss: 8.77852813e-07
Iter: 1287 loss: 8.77294724e-07
Iter: 1288 loss: 8.77699108e-07
Iter: 1289 loss: 8.76952186e-07
Iter: 1290 loss: 8.76478e-07
Iter: 1291 loss: 8.76365675e-07
Iter: 1292 loss: 8.76099193e-07
Iter: 1293 loss: 8.75499325e-07
Iter: 1294 loss: 8.80942252e-07
Iter: 1295 loss: 8.75431965e-07
Iter: 1296 loss: 8.7477963e-07
Iter: 1297 loss: 8.74924069e-07
Iter: 1298 loss: 8.74326304e-07
Iter: 1299 loss: 8.73651743e-07
Iter: 1300 loss: 8.73483941e-07
Iter: 1301 loss: 8.7309536e-07
Iter: 1302 loss: 8.72236853e-07
Iter: 1303 loss: 8.80950324e-07
Iter: 1304 loss: 8.7223043e-07
Iter: 1305 loss: 8.71666714e-07
Iter: 1306 loss: 8.77476737e-07
Iter: 1307 loss: 8.71657221e-07
Iter: 1308 loss: 8.71216457e-07
Iter: 1309 loss: 8.71401085e-07
Iter: 1310 loss: 8.70936674e-07
Iter: 1311 loss: 8.70440033e-07
Iter: 1312 loss: 8.7111107e-07
Iter: 1313 loss: 8.70179178e-07
Iter: 1314 loss: 8.69610801e-07
Iter: 1315 loss: 8.76360502e-07
Iter: 1316 loss: 8.69602673e-07
Iter: 1317 loss: 8.69325277e-07
Iter: 1318 loss: 8.68734105e-07
Iter: 1319 loss: 8.80451e-07
Iter: 1320 loss: 8.68748543e-07
Iter: 1321 loss: 8.68173288e-07
Iter: 1322 loss: 8.73946078e-07
Iter: 1323 loss: 8.68172322e-07
Iter: 1324 loss: 8.67583253e-07
Iter: 1325 loss: 8.67991218e-07
Iter: 1326 loss: 8.67194785e-07
Iter: 1327 loss: 8.66613277e-07
Iter: 1328 loss: 8.65758352e-07
Iter: 1329 loss: 8.65742891e-07
Iter: 1330 loss: 8.65263587e-07
Iter: 1331 loss: 8.6513154e-07
Iter: 1332 loss: 8.64667584e-07
Iter: 1333 loss: 8.64961294e-07
Iter: 1334 loss: 8.64356821e-07
Iter: 1335 loss: 8.63942773e-07
Iter: 1336 loss: 8.63925379e-07
Iter: 1337 loss: 8.63575e-07
Iter: 1338 loss: 8.62926584e-07
Iter: 1339 loss: 8.64298272e-07
Iter: 1340 loss: 8.62704269e-07
Iter: 1341 loss: 8.62183924e-07
Iter: 1342 loss: 8.62179434e-07
Iter: 1343 loss: 8.6175703e-07
Iter: 1344 loss: 8.62711261e-07
Iter: 1345 loss: 8.61582e-07
Iter: 1346 loss: 8.61098329e-07
Iter: 1347 loss: 8.60942066e-07
Iter: 1348 loss: 8.60671548e-07
Iter: 1349 loss: 8.60208331e-07
Iter: 1350 loss: 8.60202931e-07
Iter: 1351 loss: 8.59809461e-07
Iter: 1352 loss: 8.5947454e-07
Iter: 1353 loss: 8.59342379e-07
Iter: 1354 loss: 8.58846306e-07
Iter: 1355 loss: 8.58806288e-07
Iter: 1356 loss: 8.58469775e-07
Iter: 1357 loss: 8.58080625e-07
Iter: 1358 loss: 8.58009e-07
Iter: 1359 loss: 8.57700172e-07
Iter: 1360 loss: 8.57006512e-07
Iter: 1361 loss: 8.69067151e-07
Iter: 1362 loss: 8.56999634e-07
Iter: 1363 loss: 8.5624265e-07
Iter: 1364 loss: 8.58034184e-07
Iter: 1365 loss: 8.55983387e-07
Iter: 1366 loss: 8.55186045e-07
Iter: 1367 loss: 8.64149115e-07
Iter: 1368 loss: 8.5514597e-07
Iter: 1369 loss: 8.54708446e-07
Iter: 1370 loss: 8.54099881e-07
Iter: 1371 loss: 8.54065775e-07
Iter: 1372 loss: 8.53262236e-07
Iter: 1373 loss: 8.55711505e-07
Iter: 1374 loss: 8.53005417e-07
Iter: 1375 loss: 8.52430503e-07
Iter: 1376 loss: 8.58726821e-07
Iter: 1377 loss: 8.52435107e-07
Iter: 1378 loss: 8.51949835e-07
Iter: 1379 loss: 8.54658481e-07
Iter: 1380 loss: 8.51883499e-07
Iter: 1381 loss: 8.51474624e-07
Iter: 1382 loss: 8.51327172e-07
Iter: 1383 loss: 8.51059326e-07
Iter: 1384 loss: 8.50534434e-07
Iter: 1385 loss: 8.54138534e-07
Iter: 1386 loss: 8.50464914e-07
Iter: 1387 loss: 8.49924504e-07
Iter: 1388 loss: 8.51635605e-07
Iter: 1389 loss: 8.49728679e-07
Iter: 1390 loss: 8.4936562e-07
Iter: 1391 loss: 8.48699528e-07
Iter: 1392 loss: 8.65722711e-07
Iter: 1393 loss: 8.48695549e-07
Iter: 1394 loss: 8.47936462e-07
Iter: 1395 loss: 8.54191455e-07
Iter: 1396 loss: 8.47864385e-07
Iter: 1397 loss: 8.47235469e-07
Iter: 1398 loss: 8.53011784e-07
Iter: 1399 loss: 8.47231547e-07
Iter: 1400 loss: 8.46876503e-07
Iter: 1401 loss: 8.46138505e-07
Iter: 1402 loss: 8.58499391e-07
Iter: 1403 loss: 8.46136345e-07
Iter: 1404 loss: 8.45556883e-07
Iter: 1405 loss: 8.54753239e-07
Iter: 1406 loss: 8.45543582e-07
Iter: 1407 loss: 8.44961676e-07
Iter: 1408 loss: 8.45554382e-07
Iter: 1409 loss: 8.44645342e-07
Iter: 1410 loss: 8.44047577e-07
Iter: 1411 loss: 8.4381378e-07
Iter: 1412 loss: 8.43491421e-07
Iter: 1413 loss: 8.4257988e-07
Iter: 1414 loss: 8.45158297e-07
Iter: 1415 loss: 8.4230561e-07
Iter: 1416 loss: 8.41607061e-07
Iter: 1417 loss: 8.47360411e-07
Iter: 1418 loss: 8.41557608e-07
Iter: 1419 loss: 8.4101805e-07
Iter: 1420 loss: 8.44628062e-07
Iter: 1421 loss: 8.40978373e-07
Iter: 1422 loss: 8.40451094e-07
Iter: 1423 loss: 8.41507926e-07
Iter: 1424 loss: 8.40218831e-07
Iter: 1425 loss: 8.39677796e-07
Iter: 1426 loss: 8.39827749e-07
Iter: 1427 loss: 8.39329743e-07
Iter: 1428 loss: 8.38963558e-07
Iter: 1429 loss: 8.38940366e-07
Iter: 1430 loss: 8.38595497e-07
Iter: 1431 loss: 8.37870118e-07
Iter: 1432 loss: 8.49858e-07
Iter: 1433 loss: 8.37841469e-07
Iter: 1434 loss: 8.37115124e-07
Iter: 1435 loss: 8.38341805e-07
Iter: 1436 loss: 8.36792481e-07
Iter: 1437 loss: 8.36326933e-07
Iter: 1438 loss: 8.36301126e-07
Iter: 1439 loss: 8.35830804e-07
Iter: 1440 loss: 8.35377193e-07
Iter: 1441 loss: 8.3531063e-07
Iter: 1442 loss: 8.34759248e-07
Iter: 1443 loss: 8.37099606e-07
Iter: 1444 loss: 8.34701837e-07
Iter: 1445 loss: 8.3406826e-07
Iter: 1446 loss: 8.35692731e-07
Iter: 1447 loss: 8.33847253e-07
Iter: 1448 loss: 8.334e-07
Iter: 1449 loss: 8.333908e-07
Iter: 1450 loss: 8.33012848e-07
Iter: 1451 loss: 8.32453338e-07
Iter: 1452 loss: 8.33647846e-07
Iter: 1453 loss: 8.3221147e-07
Iter: 1454 loss: 8.31520936e-07
Iter: 1455 loss: 8.33690535e-07
Iter: 1456 loss: 8.3132835e-07
Iter: 1457 loss: 8.30584838e-07
Iter: 1458 loss: 8.32059754e-07
Iter: 1459 loss: 8.30260774e-07
Iter: 1460 loss: 8.29841213e-07
Iter: 1461 loss: 8.29817338e-07
Iter: 1462 loss: 8.29436317e-07
Iter: 1463 loss: 8.29650901e-07
Iter: 1464 loss: 8.29196665e-07
Iter: 1465 loss: 8.28857935e-07
Iter: 1466 loss: 8.2857639e-07
Iter: 1467 loss: 8.28493626e-07
Iter: 1468 loss: 8.28011878e-07
Iter: 1469 loss: 8.28027908e-07
Iter: 1470 loss: 8.2773829e-07
Iter: 1471 loss: 8.27323788e-07
Iter: 1472 loss: 8.27299573e-07
Iter: 1473 loss: 8.26760925e-07
Iter: 1474 loss: 8.276009e-07
Iter: 1475 loss: 8.26542873e-07
Iter: 1476 loss: 8.2596739e-07
Iter: 1477 loss: 8.33727427e-07
Iter: 1478 loss: 8.25964e-07
Iter: 1479 loss: 8.25560903e-07
Iter: 1480 loss: 8.2505295e-07
Iter: 1481 loss: 8.25001e-07
Iter: 1482 loss: 8.24518338e-07
Iter: 1483 loss: 8.24523e-07
Iter: 1484 loss: 8.24167046e-07
Iter: 1485 loss: 8.23714e-07
Iter: 1486 loss: 8.23663186e-07
Iter: 1487 loss: 8.23144262e-07
Iter: 1488 loss: 8.25143502e-07
Iter: 1489 loss: 8.23061e-07
Iter: 1490 loss: 8.2260533e-07
Iter: 1491 loss: 8.23974517e-07
Iter: 1492 loss: 8.22473567e-07
Iter: 1493 loss: 8.21972833e-07
Iter: 1494 loss: 8.21611138e-07
Iter: 1495 loss: 8.21479659e-07
Iter: 1496 loss: 8.21098922e-07
Iter: 1497 loss: 8.21003312e-07
Iter: 1498 loss: 8.20585285e-07
Iter: 1499 loss: 8.20625189e-07
Iter: 1500 loss: 8.20263722e-07
Iter: 1501 loss: 8.19815796e-07
Iter: 1502 loss: 8.19696766e-07
Iter: 1503 loss: 8.1944853e-07
Iter: 1504 loss: 8.18780052e-07
Iter: 1505 loss: 8.27045255e-07
Iter: 1506 loss: 8.18786418e-07
Iter: 1507 loss: 8.18479123e-07
Iter: 1508 loss: 8.18031367e-07
Iter: 1509 loss: 8.18018293e-07
Iter: 1510 loss: 8.17549505e-07
Iter: 1511 loss: 8.22550817e-07
Iter: 1512 loss: 8.17532964e-07
Iter: 1513 loss: 8.17116302e-07
Iter: 1514 loss: 8.17781824e-07
Iter: 1515 loss: 8.16902684e-07
Iter: 1516 loss: 8.16600505e-07
Iter: 1517 loss: 8.17475325e-07
Iter: 1518 loss: 8.1649091e-07
Iter: 1519 loss: 8.16001375e-07
Iter: 1520 loss: 8.16546333e-07
Iter: 1521 loss: 8.15734893e-07
Iter: 1522 loss: 8.15293504e-07
Iter: 1523 loss: 8.15409578e-07
Iter: 1524 loss: 8.14975067e-07
Iter: 1525 loss: 8.14446707e-07
Iter: 1526 loss: 8.16043269e-07
Iter: 1527 loss: 8.14276518e-07
Iter: 1528 loss: 8.13649137e-07
Iter: 1529 loss: 8.14796635e-07
Iter: 1530 loss: 8.13354177e-07
Iter: 1531 loss: 8.12905341e-07
Iter: 1532 loss: 8.15919918e-07
Iter: 1533 loss: 8.1287078e-07
Iter: 1534 loss: 8.12392557e-07
Iter: 1535 loss: 8.15182602e-07
Iter: 1536 loss: 8.12348731e-07
Iter: 1537 loss: 8.12027565e-07
Iter: 1538 loss: 8.11537234e-07
Iter: 1539 loss: 8.11546272e-07
Iter: 1540 loss: 8.11194468e-07
Iter: 1541 loss: 8.11138136e-07
Iter: 1542 loss: 8.10791903e-07
Iter: 1543 loss: 8.10122344e-07
Iter: 1544 loss: 8.23152789e-07
Iter: 1545 loss: 8.10125641e-07
Iter: 1546 loss: 8.09563062e-07
Iter: 1547 loss: 8.13820691e-07
Iter: 1548 loss: 8.09506389e-07
Iter: 1549 loss: 8.08925677e-07
Iter: 1550 loss: 8.11208e-07
Iter: 1551 loss: 8.08770437e-07
Iter: 1552 loss: 8.08332459e-07
Iter: 1553 loss: 8.0825231e-07
Iter: 1554 loss: 8.07953029e-07
Iter: 1555 loss: 8.07434844e-07
Iter: 1556 loss: 8.07427909e-07
Iter: 1557 loss: 8.07114134e-07
Iter: 1558 loss: 8.0665427e-07
Iter: 1559 loss: 8.06651144e-07
Iter: 1560 loss: 8.06058779e-07
Iter: 1561 loss: 8.07400568e-07
Iter: 1562 loss: 8.05824129e-07
Iter: 1563 loss: 8.05158038e-07
Iter: 1564 loss: 8.08034429e-07
Iter: 1565 loss: 8.05010927e-07
Iter: 1566 loss: 8.0444272e-07
Iter: 1567 loss: 8.04887577e-07
Iter: 1568 loss: 8.0409518e-07
Iter: 1569 loss: 8.03754801e-07
Iter: 1570 loss: 8.03671696e-07
Iter: 1571 loss: 8.03376395e-07
Iter: 1572 loss: 8.02793238e-07
Iter: 1573 loss: 8.16756312e-07
Iter: 1574 loss: 8.02785678e-07
Iter: 1575 loss: 8.02372142e-07
Iter: 1576 loss: 8.08660729e-07
Iter: 1577 loss: 8.02381521e-07
Iter: 1578 loss: 8.01973556e-07
Iter: 1579 loss: 8.01928252e-07
Iter: 1580 loss: 8.01611918e-07
Iter: 1581 loss: 8.01262729e-07
Iter: 1582 loss: 8.01011652e-07
Iter: 1583 loss: 8.00863688e-07
Iter: 1584 loss: 8.00352893e-07
Iter: 1585 loss: 8.05562877e-07
Iter: 1586 loss: 8.00317821e-07
Iter: 1587 loss: 7.99724205e-07
Iter: 1588 loss: 8.00240514e-07
Iter: 1589 loss: 7.99373879e-07
Iter: 1590 loss: 7.98935332e-07
Iter: 1591 loss: 7.99367058e-07
Iter: 1592 loss: 7.98661517e-07
Iter: 1593 loss: 7.98050905e-07
Iter: 1594 loss: 8.03749117e-07
Iter: 1595 loss: 7.98038e-07
Iter: 1596 loss: 7.97644759e-07
Iter: 1597 loss: 7.97187852e-07
Iter: 1598 loss: 7.97138455e-07
Iter: 1599 loss: 7.96570816e-07
Iter: 1600 loss: 7.9960364e-07
Iter: 1601 loss: 7.96507038e-07
Iter: 1602 loss: 7.95979304e-07
Iter: 1603 loss: 7.97609118e-07
Iter: 1604 loss: 7.95837479e-07
Iter: 1605 loss: 7.95502217e-07
Iter: 1606 loss: 8.00494036e-07
Iter: 1607 loss: 7.95490791e-07
Iter: 1608 loss: 7.95138931e-07
Iter: 1609 loss: 7.94664629e-07
Iter: 1610 loss: 7.94649111e-07
Iter: 1611 loss: 7.942715e-07
Iter: 1612 loss: 7.98230872e-07
Iter: 1613 loss: 7.94263e-07
Iter: 1614 loss: 7.93847903e-07
Iter: 1615 loss: 7.93673905e-07
Iter: 1616 loss: 7.93458e-07
Iter: 1617 loss: 7.92934657e-07
Iter: 1618 loss: 7.92747869e-07
Iter: 1619 loss: 7.92466949e-07
Iter: 1620 loss: 7.91943876e-07
Iter: 1621 loss: 7.96556435e-07
Iter: 1622 loss: 7.91900675e-07
Iter: 1623 loss: 7.91380103e-07
Iter: 1624 loss: 7.94299524e-07
Iter: 1625 loss: 7.91309049e-07
Iter: 1626 loss: 7.91020057e-07
Iter: 1627 loss: 7.90723561e-07
Iter: 1628 loss: 7.90674108e-07
Iter: 1629 loss: 7.90236072e-07
Iter: 1630 loss: 7.9671662e-07
Iter: 1631 loss: 7.90240961e-07
Iter: 1632 loss: 7.89881256e-07
Iter: 1633 loss: 7.89227101e-07
Iter: 1634 loss: 7.89220394e-07
Iter: 1635 loss: 7.88630132e-07
Iter: 1636 loss: 7.91029947e-07
Iter: 1637 loss: 7.88515877e-07
Iter: 1638 loss: 7.87953923e-07
Iter: 1639 loss: 7.91200705e-07
Iter: 1640 loss: 7.87836484e-07
Iter: 1641 loss: 7.87487124e-07
Iter: 1642 loss: 7.92109176e-07
Iter: 1643 loss: 7.87498834e-07
Iter: 1644 loss: 7.87171473e-07
Iter: 1645 loss: 7.87068814e-07
Iter: 1646 loss: 7.86932901e-07
Iter: 1647 loss: 7.86596274e-07
Iter: 1648 loss: 7.87670274e-07
Iter: 1649 loss: 7.86499186e-07
Iter: 1650 loss: 7.86074111e-07
Iter: 1651 loss: 7.87024476e-07
Iter: 1652 loss: 7.85914438e-07
Iter: 1653 loss: 7.85611064e-07
Iter: 1654 loss: 7.85075486e-07
Iter: 1655 loss: 7.85089128e-07
Iter: 1656 loss: 7.84429517e-07
Iter: 1657 loss: 7.85979807e-07
Iter: 1658 loss: 7.84195663e-07
Iter: 1659 loss: 7.83616258e-07
Iter: 1660 loss: 7.83605174e-07
Iter: 1661 loss: 7.83067492e-07
Iter: 1662 loss: 7.82813174e-07
Iter: 1663 loss: 7.825588e-07
Iter: 1664 loss: 7.82162488e-07
Iter: 1665 loss: 7.85436782e-07
Iter: 1666 loss: 7.82128552e-07
Iter: 1667 loss: 7.81618496e-07
Iter: 1668 loss: 7.8196382e-07
Iter: 1669 loss: 7.81325298e-07
Iter: 1670 loss: 7.8092512e-07
Iter: 1671 loss: 7.80942457e-07
Iter: 1672 loss: 7.80613846e-07
Iter: 1673 loss: 7.80086395e-07
Iter: 1674 loss: 7.82443919e-07
Iter: 1675 loss: 7.80010282e-07
Iter: 1676 loss: 7.79479194e-07
Iter: 1677 loss: 7.82938514e-07
Iter: 1678 loss: 7.79412915e-07
Iter: 1679 loss: 7.78929575e-07
Iter: 1680 loss: 7.7953581e-07
Iter: 1681 loss: 7.78697654e-07
Iter: 1682 loss: 7.78189928e-07
Iter: 1683 loss: 7.77961532e-07
Iter: 1684 loss: 7.77701871e-07
Iter: 1685 loss: 7.77260652e-07
Iter: 1686 loss: 7.77230525e-07
Iter: 1687 loss: 7.76961826e-07
Iter: 1688 loss: 7.76330921e-07
Iter: 1689 loss: 7.85008297e-07
Iter: 1690 loss: 7.76302954e-07
Iter: 1691 loss: 7.75555691e-07
Iter: 1692 loss: 7.77033108e-07
Iter: 1693 loss: 7.752584e-07
Iter: 1694 loss: 7.74836963e-07
Iter: 1695 loss: 7.74811269e-07
Iter: 1696 loss: 7.74360956e-07
Iter: 1697 loss: 7.74927685e-07
Iter: 1698 loss: 7.74131422e-07
Iter: 1699 loss: 7.73734143e-07
Iter: 1700 loss: 7.74196e-07
Iter: 1701 loss: 7.73517058e-07
Iter: 1702 loss: 7.72818e-07
Iter: 1703 loss: 7.74187129e-07
Iter: 1704 loss: 7.72532871e-07
Iter: 1705 loss: 7.72051067e-07
Iter: 1706 loss: 7.71700115e-07
Iter: 1707 loss: 7.71522366e-07
Iter: 1708 loss: 7.70925112e-07
Iter: 1709 loss: 7.74414104e-07
Iter: 1710 loss: 7.70830184e-07
Iter: 1711 loss: 7.70531472e-07
Iter: 1712 loss: 7.70500264e-07
Iter: 1713 loss: 7.70275392e-07
Iter: 1714 loss: 7.69896474e-07
Iter: 1715 loss: 7.77750188e-07
Iter: 1716 loss: 7.69875385e-07
Iter: 1717 loss: 7.69528924e-07
Iter: 1718 loss: 7.73769727e-07
Iter: 1719 loss: 7.69524036e-07
Iter: 1720 loss: 7.6916217e-07
Iter: 1721 loss: 7.68985387e-07
Iter: 1722 loss: 7.68786208e-07
Iter: 1723 loss: 7.68253358e-07
Iter: 1724 loss: 7.69467874e-07
Iter: 1725 loss: 7.68077712e-07
Iter: 1726 loss: 7.67497966e-07
Iter: 1727 loss: 7.67763481e-07
Iter: 1728 loss: 7.67145e-07
Iter: 1729 loss: 7.66445226e-07
Iter: 1730 loss: 7.66889286e-07
Iter: 1731 loss: 7.6600503e-07
Iter: 1732 loss: 7.65561254e-07
Iter: 1733 loss: 7.65496623e-07
Iter: 1734 loss: 7.65096843e-07
Iter: 1735 loss: 7.65587174e-07
Iter: 1736 loss: 7.64893286e-07
Iter: 1737 loss: 7.64600088e-07
Iter: 1738 loss: 7.64965534e-07
Iter: 1739 loss: 7.64405797e-07
Iter: 1740 loss: 7.63894491e-07
Iter: 1741 loss: 7.64382321e-07
Iter: 1742 loss: 7.63568892e-07
Iter: 1743 loss: 7.63063554e-07
Iter: 1744 loss: 7.62624836e-07
Iter: 1745 loss: 7.62490117e-07
Iter: 1746 loss: 7.62388936e-07
Iter: 1747 loss: 7.62137915e-07
Iter: 1748 loss: 7.61796059e-07
Iter: 1749 loss: 7.61074602e-07
Iter: 1750 loss: 7.72692033e-07
Iter: 1751 loss: 7.61053229e-07
Iter: 1752 loss: 7.60495084e-07
Iter: 1753 loss: 7.65568529e-07
Iter: 1754 loss: 7.60467742e-07
Iter: 1755 loss: 7.59966269e-07
Iter: 1756 loss: 7.63062303e-07
Iter: 1757 loss: 7.59894533e-07
Iter: 1758 loss: 7.59629302e-07
Iter: 1759 loss: 7.58931719e-07
Iter: 1760 loss: 7.67404515e-07
Iter: 1761 loss: 7.5891711e-07
Iter: 1762 loss: 7.58225383e-07
Iter: 1763 loss: 7.61487968e-07
Iter: 1764 loss: 7.58111e-07
Iter: 1765 loss: 7.57477437e-07
Iter: 1766 loss: 7.59577688e-07
Iter: 1767 loss: 7.57293662e-07
Iter: 1768 loss: 7.56776217e-07
Iter: 1769 loss: 7.65112304e-07
Iter: 1770 loss: 7.56774057e-07
Iter: 1771 loss: 7.56379393e-07
Iter: 1772 loss: 7.56265763e-07
Iter: 1773 loss: 7.56026566e-07
Iter: 1774 loss: 7.55472627e-07
Iter: 1775 loss: 7.57674627e-07
Iter: 1776 loss: 7.55352971e-07
Iter: 1777 loss: 7.54992755e-07
Iter: 1778 loss: 7.57321459e-07
Iter: 1779 loss: 7.54939606e-07
Iter: 1780 loss: 7.54595931e-07
Iter: 1781 loss: 7.54863606e-07
Iter: 1782 loss: 7.54373048e-07
Iter: 1783 loss: 7.54013513e-07
Iter: 1784 loss: 7.55676922e-07
Iter: 1785 loss: 7.53965082e-07
Iter: 1786 loss: 7.53487768e-07
Iter: 1787 loss: 7.53806603e-07
Iter: 1788 loss: 7.53208155e-07
Iter: 1789 loss: 7.52813492e-07
Iter: 1790 loss: 7.5230696e-07
Iter: 1791 loss: 7.52284336e-07
Iter: 1792 loss: 7.51967718e-07
Iter: 1793 loss: 7.51879611e-07
Iter: 1794 loss: 7.51560492e-07
Iter: 1795 loss: 7.50904519e-07
Iter: 1796 loss: 7.61976708e-07
Iter: 1797 loss: 7.50903268e-07
Iter: 1798 loss: 7.50351887e-07
Iter: 1799 loss: 7.51945095e-07
Iter: 1800 loss: 7.50144522e-07
Iter: 1801 loss: 7.49613719e-07
Iter: 1802 loss: 7.49981155e-07
Iter: 1803 loss: 7.49268906e-07
Iter: 1804 loss: 7.49125e-07
Iter: 1805 loss: 7.4894848e-07
Iter: 1806 loss: 7.48695641e-07
Iter: 1807 loss: 7.48501407e-07
Iter: 1808 loss: 7.48413527e-07
Iter: 1809 loss: 7.47987826e-07
Iter: 1810 loss: 7.4885628e-07
Iter: 1811 loss: 7.47856518e-07
Iter: 1812 loss: 7.47416095e-07
Iter: 1813 loss: 7.49516175e-07
Iter: 1814 loss: 7.47308206e-07
Iter: 1815 loss: 7.4696294e-07
Iter: 1816 loss: 7.46729597e-07
Iter: 1817 loss: 7.46605394e-07
Iter: 1818 loss: 7.46235e-07
Iter: 1819 loss: 7.4620641e-07
Iter: 1820 loss: 7.46018429e-07
Iter: 1821 loss: 7.45590512e-07
Iter: 1822 loss: 7.51563107e-07
Iter: 1823 loss: 7.45540262e-07
Iter: 1824 loss: 7.45107741e-07
Iter: 1825 loss: 7.49677895e-07
Iter: 1826 loss: 7.45114733e-07
Iter: 1827 loss: 7.44760882e-07
Iter: 1828 loss: 7.45502859e-07
Iter: 1829 loss: 7.44586373e-07
Iter: 1830 loss: 7.44177896e-07
Iter: 1831 loss: 7.44492695e-07
Iter: 1832 loss: 7.43917951e-07
Iter: 1833 loss: 7.43441831e-07
Iter: 1834 loss: 7.43054784e-07
Iter: 1835 loss: 7.42927568e-07
Iter: 1836 loss: 7.42213615e-07
Iter: 1837 loss: 7.4561666e-07
Iter: 1838 loss: 7.42083e-07
Iter: 1839 loss: 7.41717713e-07
Iter: 1840 loss: 7.41699694e-07
Iter: 1841 loss: 7.41350107e-07
Iter: 1842 loss: 7.41760061e-07
Iter: 1843 loss: 7.41186909e-07
Iter: 1844 loss: 7.40954931e-07
Iter: 1845 loss: 7.41416557e-07
Iter: 1846 loss: 7.40862902e-07
Iter: 1847 loss: 7.40547137e-07
Iter: 1848 loss: 7.41349197e-07
Iter: 1849 loss: 7.40441e-07
Iter: 1850 loss: 7.40132634e-07
Iter: 1851 loss: 7.40077212e-07
Iter: 1852 loss: 7.39854556e-07
Iter: 1853 loss: 7.39496727e-07
Iter: 1854 loss: 7.39482516e-07
Iter: 1855 loss: 7.39258439e-07
Iter: 1856 loss: 7.38710696e-07
Iter: 1857 loss: 7.46128194e-07
Iter: 1858 loss: 7.3867875e-07
Iter: 1859 loss: 7.38297842e-07
Iter: 1860 loss: 7.38285735e-07
Iter: 1861 loss: 7.37910341e-07
Iter: 1862 loss: 7.38411416e-07
Iter: 1863 loss: 7.37730716e-07
Iter: 1864 loss: 7.37461278e-07
Iter: 1865 loss: 7.38580411e-07
Iter: 1866 loss: 7.37379e-07
Iter: 1867 loss: 7.37102312e-07
Iter: 1868 loss: 7.36973391e-07
Iter: 1869 loss: 7.36831e-07
Iter: 1870 loss: 7.36413199e-07
Iter: 1871 loss: 7.36725042e-07
Iter: 1872 loss: 7.3613171e-07
Iter: 1873 loss: 7.35678e-07
Iter: 1874 loss: 7.37622713e-07
Iter: 1875 loss: 7.35566516e-07
Iter: 1876 loss: 7.35010417e-07
Iter: 1877 loss: 7.39995e-07
Iter: 1878 loss: 7.35002e-07
Iter: 1879 loss: 7.34714604e-07
Iter: 1880 loss: 7.34188575e-07
Iter: 1881 loss: 7.44220699e-07
Iter: 1882 loss: 7.34183743e-07
Iter: 1883 loss: 7.33895263e-07
Iter: 1884 loss: 7.33829893e-07
Iter: 1885 loss: 7.33605e-07
Iter: 1886 loss: 7.33581601e-07
Iter: 1887 loss: 7.33391e-07
Iter: 1888 loss: 7.33145839e-07
Iter: 1889 loss: 7.33139359e-07
Iter: 1890 loss: 7.32952515e-07
Iter: 1891 loss: 7.32533067e-07
Iter: 1892 loss: 7.37424557e-07
Iter: 1893 loss: 7.32476963e-07
Iter: 1894 loss: 7.32010392e-07
Iter: 1895 loss: 7.34031971e-07
Iter: 1896 loss: 7.31929504e-07
Iter: 1897 loss: 7.31449632e-07
Iter: 1898 loss: 7.35960953e-07
Iter: 1899 loss: 7.31401656e-07
Iter: 1900 loss: 7.3113614e-07
Iter: 1901 loss: 7.30784109e-07
Iter: 1902 loss: 7.30753754e-07
Iter: 1903 loss: 7.3021215e-07
Iter: 1904 loss: 7.32923695e-07
Iter: 1905 loss: 7.30134047e-07
Iter: 1906 loss: 7.29724718e-07
Iter: 1907 loss: 7.30380521e-07
Iter: 1908 loss: 7.29537874e-07
Iter: 1909 loss: 7.29118085e-07
Iter: 1910 loss: 7.29428052e-07
Iter: 1911 loss: 7.28870873e-07
Iter: 1912 loss: 7.28622695e-07
Iter: 1913 loss: 7.28589725e-07
Iter: 1914 loss: 7.28338762e-07
Iter: 1915 loss: 7.28303576e-07
Iter: 1916 loss: 7.28104624e-07
Iter: 1917 loss: 7.27796078e-07
Iter: 1918 loss: 7.27175461e-07
Iter: 1919 loss: 7.40926112e-07
Iter: 1920 loss: 7.27174324e-07
Iter: 1921 loss: 7.26833264e-07
Iter: 1922 loss: 7.26784151e-07
Iter: 1923 loss: 7.26398525e-07
Iter: 1924 loss: 7.27555346e-07
Iter: 1925 loss: 7.26239819e-07
Iter: 1926 loss: 7.2596174e-07
Iter: 1927 loss: 7.25774953e-07
Iter: 1928 loss: 7.25642622e-07
Iter: 1929 loss: 7.25385917e-07
Iter: 1930 loss: 7.25321172e-07
Iter: 1931 loss: 7.25169571e-07
Iter: 1932 loss: 7.24824645e-07
Iter: 1933 loss: 7.32046715e-07
Iter: 1934 loss: 7.24803385e-07
Iter: 1935 loss: 7.24586926e-07
Iter: 1936 loss: 7.2457965e-07
Iter: 1937 loss: 7.2433e-07
Iter: 1938 loss: 7.23896505e-07
Iter: 1939 loss: 7.23891958e-07
Iter: 1940 loss: 7.23429594e-07
Iter: 1941 loss: 7.25010182e-07
Iter: 1942 loss: 7.23302492e-07
Iter: 1943 loss: 7.22830805e-07
Iter: 1944 loss: 7.23430048e-07
Iter: 1945 loss: 7.22570121e-07
Iter: 1946 loss: 7.2210355e-07
Iter: 1947 loss: 7.232548e-07
Iter: 1948 loss: 7.21936658e-07
Iter: 1949 loss: 7.21740491e-07
Iter: 1950 loss: 7.21705248e-07
Iter: 1951 loss: 7.21506353e-07
Iter: 1952 loss: 7.21093102e-07
Iter: 1953 loss: 7.29219096e-07
Iter: 1954 loss: 7.21107824e-07
Iter: 1955 loss: 7.207301e-07
Iter: 1956 loss: 7.21479864e-07
Iter: 1957 loss: 7.20606067e-07
Iter: 1958 loss: 7.20260459e-07
Iter: 1959 loss: 7.24562653e-07
Iter: 1960 loss: 7.20255912e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6
+ date
Wed Oct 21 10:54:04 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.2/300_300_300_1 --function f1 --psi -2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4a2d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4ab8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4ab8b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc497e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc497e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4a360d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc49e7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48fb598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48fb2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc49468c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc494f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48de730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48dee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4868510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc4858840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48ab6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc48ab510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc15f5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc15a8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc15c2f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc47ff840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc47fff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc47b7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14d0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14d0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14d02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc1497950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14ac598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14ac488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc14486a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc1488f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc13a01e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc1398620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc13989d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc1517268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7dc13cf8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.024691213
test_loss: 0.0237323
train_loss: 0.010842392
test_loss: 0.011419016
train_loss: 0.007116771
test_loss: 0.007922669
train_loss: 0.0064131664
test_loss: 0.006636905
train_loss: 0.005281979
test_loss: 0.006147934
train_loss: 0.0047164983
test_loss: 0.0057614245
train_loss: 0.004674913
test_loss: 0.005322989
train_loss: 0.0045824
test_loss: 0.005414203
train_loss: 0.0041051935
test_loss: 0.0052017383
train_loss: 0.004320536
test_loss: 0.005181114
train_loss: 0.004142129
test_loss: 0.0050392495
train_loss: 0.004244495
test_loss: 0.0049497406
train_loss: 0.0042857896
test_loss: 0.0050960793
train_loss: 0.0042080595
test_loss: 0.0051698475
train_loss: 0.0043510217
test_loss: 0.0050986395
train_loss: 0.004027813
test_loss: 0.0049993377
train_loss: 0.0038732968
test_loss: 0.004940592
train_loss: 0.0040004146
test_loss: 0.0049524987
train_loss: 0.0039799362
test_loss: 0.005197263
train_loss: 0.004079961
test_loss: 0.0051902323
train_loss: 0.0038258857
test_loss: 0.0048476877
train_loss: 0.0037381086
test_loss: 0.0051992675
train_loss: 0.0039823186
test_loss: 0.0051554856
train_loss: 0.0038241805
test_loss: 0.004694985
train_loss: 0.004007274
test_loss: 0.004820642
train_loss: 0.0039051543
test_loss: 0.0050420915
train_loss: 0.00395396
test_loss: 0.004856654
train_loss: 0.0035287442
test_loss: 0.004927245
train_loss: 0.00350927
test_loss: 0.0046581747
train_loss: 0.00418158
test_loss: 0.005022465
train_loss: 0.0038871085
test_loss: 0.004700234
train_loss: 0.0038000366
test_loss: 0.004685028
train_loss: 0.003844186
test_loss: 0.0046972265
train_loss: 0.0038138689
test_loss: 0.004952
train_loss: 0.0035941121
test_loss: 0.0049776454
train_loss: 0.003615374
test_loss: 0.004712863
train_loss: 0.0039812727
test_loss: 0.0047391746
train_loss: 0.0037626356
test_loss: 0.0047305925
train_loss: 0.003762607
test_loss: 0.004675068
train_loss: 0.0037230016
test_loss: 0.004727916
train_loss: 0.003724188
test_loss: 0.0046702963
train_loss: 0.0039138356
test_loss: 0.0050081788
train_loss: 0.0036303664
test_loss: 0.004895965
train_loss: 0.003679594
test_loss: 0.0047841244
train_loss: 0.0034801103
test_loss: 0.004692602
train_loss: 0.0034944648
test_loss: 0.004569122
train_loss: 0.0039518345
test_loss: 0.004640182
train_loss: 0.003487298
test_loss: 0.004734398
train_loss: 0.004106274
test_loss: 0.005033981
train_loss: 0.003713897
test_loss: 0.004684965
train_loss: 0.0037537552
test_loss: 0.004660607
train_loss: 0.0034680252
test_loss: 0.004712301
train_loss: 0.0035195854
test_loss: 0.0046598194
train_loss: 0.00362779
test_loss: 0.0048129107
train_loss: 0.003804081
test_loss: 0.004764039
train_loss: 0.0036958647
test_loss: 0.0045626042
train_loss: 0.003851566
test_loss: 0.0048199533
train_loss: 0.0036141388
test_loss: 0.0045647663
train_loss: 0.0033203545
test_loss: 0.0046149786
train_loss: 0.0035452119
test_loss: 0.0047885654
train_loss: 0.0036022621
test_loss: 0.0045928126
train_loss: 0.0037102306
test_loss: 0.0045405896
train_loss: 0.0037369826
test_loss: 0.004641317
train_loss: 0.0034051647
test_loss: 0.004583089
train_loss: 0.0035641866
test_loss: 0.0045976713
train_loss: 0.0034342837
test_loss: 0.0046813004
train_loss: 0.0035445197
test_loss: 0.004801152
train_loss: 0.0035137886
test_loss: 0.0047444855
train_loss: 0.003620115
test_loss: 0.0046300823
train_loss: 0.0036195056
test_loss: 0.0047449097
train_loss: 0.0037288738
test_loss: 0.004566075
train_loss: 0.0035948516
test_loss: 0.004673381
train_loss: 0.0037011078
test_loss: 0.0047536506
train_loss: 0.003648867
test_loss: 0.0045618773
train_loss: 0.0034338206
test_loss: 0.004595958
train_loss: 0.0033270912
test_loss: 0.0046496405
train_loss: 0.0036668035
test_loss: 0.0044393134
train_loss: 0.003586249
test_loss: 0.004757923
train_loss: 0.0035798892
test_loss: 0.0044870046
train_loss: 0.0037309397
test_loss: 0.004747658
train_loss: 0.003694287
test_loss: 0.004675271
train_loss: 0.00357808
test_loss: 0.004722557
train_loss: 0.004040352
test_loss: 0.0046410384
train_loss: 0.0035997597
test_loss: 0.0045483923
train_loss: 0.003485582
test_loss: 0.004665477
train_loss: 0.003309299
test_loss: 0.0045024767
train_loss: 0.0036919415
test_loss: 0.0046965927
train_loss: 0.0034668879
test_loss: 0.004543792
train_loss: 0.003545037
test_loss: 0.004533045
train_loss: 0.0037685272
test_loss: 0.004732998
train_loss: 0.0037402278
test_loss: 0.004585319
train_loss: 0.0041473396
test_loss: 0.004862735
train_loss: 0.0037072808
test_loss: 0.004615098
train_loss: 0.0036835764
test_loss: 0.0046203835
train_loss: 0.0033572377
test_loss: 0.004481441
train_loss: 0.0035815062
test_loss: 0.004567552
train_loss: 0.0037262528
test_loss: 0.004621657
train_loss: 0.003776738
test_loss: 0.004882351
train_loss: 0.0035449623
test_loss: 0.004597182
train_loss: 0.0034089475
test_loss: 0.004543895
train_loss: 0.0032098396
test_loss: 0.0042779325
train_loss: 0.0033442576
test_loss: 0.0045264564
train_loss: 0.003332823
test_loss: 0.0046022004
train_loss: 0.0030668227
test_loss: 0.0043513263
train_loss: 0.0038594585
test_loss: 0.0044974703
train_loss: 0.0044911033
test_loss: 0.005063654
train_loss: 0.0034529855
test_loss: 0.0047582258
train_loss: 0.0034862724
test_loss: 0.004475605
train_loss: 0.003520229
test_loss: 0.0045257397
train_loss: 0.0034551537
test_loss: 0.0044991365
train_loss: 0.0034477445
test_loss: 0.0044286423
train_loss: 0.0037994375
test_loss: 0.004737329
train_loss: 0.0036487337
test_loss: 0.0046095243
train_loss: 0.003500712
test_loss: 0.004721702
train_loss: 0.0033969027
test_loss: 0.004482158
train_loss: 0.003571251
test_loss: 0.0045729843
train_loss: 0.003546694
test_loss: 0.0044388846
train_loss: 0.0032294015
test_loss: 0.0045479117
train_loss: 0.003600503
test_loss: 0.0046392516
train_loss: 0.0037136697
test_loss: 0.0046531297
train_loss: 0.0031953603
test_loss: 0.004419859
train_loss: 0.0035659834
test_loss: 0.004576433
train_loss: 0.0034946767
test_loss: 0.0045767007
train_loss: 0.0034131955
test_loss: 0.0044750194
train_loss: 0.0034713657
test_loss: 0.004516781
train_loss: 0.003486358
test_loss: 0.0044710836
train_loss: 0.003476811
test_loss: 0.0045201695
train_loss: 0.0033427551
test_loss: 0.004725129
train_loss: 0.0034218347
test_loss: 0.0045927796
train_loss: 0.0037656561
test_loss: 0.0046469034
train_loss: 0.0034300836
test_loss: 0.0047430396
train_loss: 0.00374057
test_loss: 0.004963045
train_loss: 0.0035538687
test_loss: 0.004936328
train_loss: 0.0033708422
test_loss: 0.0044534416
train_loss: 0.0033620833
test_loss: 0.004463273
train_loss: 0.0031597614
test_loss: 0.0044799014
train_loss: 0.0034665735
test_loss: 0.004650817
train_loss: 0.0033823787
test_loss: 0.004686303
train_loss: 0.0030904058
test_loss: 0.0044926507
train_loss: 0.003257215
test_loss: 0.004431937
train_loss: 0.003704973
test_loss: 0.004363583
train_loss: 0.0033866125
test_loss: 0.004856785
train_loss: 0.0037239569
test_loss: 0.0047267633
train_loss: 0.0035857032
test_loss: 0.004709822
train_loss: 0.0034022275
test_loss: 0.0045679267
train_loss: 0.0036118417
test_loss: 0.004500926
train_loss: 0.0034407575
test_loss: 0.004474888
train_loss: 0.0033970731
test_loss: 0.0045028604
train_loss: 0.0035017228
test_loss: 0.0044638696
train_loss: 0.0036691641
test_loss: 0.004441474
train_loss: 0.0037063167
test_loss: 0.0047729043
train_loss: 0.00347918
test_loss: 0.0043853167
train_loss: 0.0037614824
test_loss: 0.0045680236
train_loss: 0.0031794915
test_loss: 0.0043530636
train_loss: 0.0035371587
test_loss: 0.0043390486
train_loss: 0.003328125
test_loss: 0.004464878
train_loss: 0.0036568306
test_loss: 0.004563007
train_loss: 0.0035900294
test_loss: 0.0044642
train_loss: 0.0032377755
test_loss: 0.0043934872
train_loss: 0.0034197825
test_loss: 0.004508509
train_loss: 0.003368649
test_loss: 0.0045834044
train_loss: 0.003434019
test_loss: 0.004531462
train_loss: 0.0033439202
test_loss: 0.0043636747
train_loss: 0.003439004
test_loss: 0.004546827
train_loss: 0.0035141464
test_loss: 0.004440849
train_loss: 0.0035718041
test_loss: 0.0046576257
train_loss: 0.0033899602
test_loss: 0.0044634305
train_loss: 0.0033793459
test_loss: 0.0045467657
train_loss: 0.0033908514
test_loss: 0.0044389884
train_loss: 0.0031990306
test_loss: 0.0043796278
train_loss: 0.0037099319
test_loss: 0.0045250338
train_loss: 0.0032523228
test_loss: 0.0045351842
train_loss: 0.0032755989
test_loss: 0.0045580715
train_loss: 0.0036443886
test_loss: 0.004693937
train_loss: 0.0034042904
test_loss: 0.0045825047
train_loss: 0.0033670552
test_loss: 0.0044667223
train_loss: 0.0033369423
test_loss: 0.004511112
train_loss: 0.003602397
test_loss: 0.0044807475
train_loss: 0.0032811828
test_loss: 0.004514674
train_loss: 0.00334813
test_loss: 0.0043689488
train_loss: 0.003381327
test_loss: 0.004515013
train_loss: 0.003414614
test_loss: 0.004354102
train_loss: 0.003151475
test_loss: 0.004437218
train_loss: 0.0034188707
test_loss: 0.0045097107
train_loss: 0.003377698
test_loss: 0.0045504225
train_loss: 0.003517579
test_loss: 0.0045546093
train_loss: 0.0032312565
test_loss: 0.0043605026
train_loss: 0.0032417122
test_loss: 0.0042808326
train_loss: 0.0032011836
test_loss: 0.004271913
train_loss: 0.0032397276
test_loss: 0.004363818
train_loss: 0.0033318854
test_loss: 0.004442145
train_loss: 0.0032485146
test_loss: 0.004560732
train_loss: 0.0032873575
test_loss: 0.0046256706
train_loss: 0.003168401
test_loss: 0.004414708
train_loss: 0.0035388963
test_loss: 0.0043489733
train_loss: 0.0032866579
test_loss: 0.0045303195
train_loss: 0.0033779806
test_loss: 0.00448279
train_loss: 0.0033298742
test_loss: 0.0044584307
train_loss: 0.0035403608
test_loss: 0.004532972
train_loss: 0.0031599747
test_loss: 0.0043939794
train_loss: 0.0032386337
test_loss: 0.004452039
train_loss: 0.0029335984
test_loss: 0.0043169702
train_loss: 0.0032464436
test_loss: 0.004589615
train_loss: 0.0033866586
test_loss: 0.0046517793
train_loss: 0.0033338917
test_loss: 0.004441811
train_loss: 0.003523273
test_loss: 0.0044233175
train_loss: 0.0036729886
test_loss: 0.004412025
train_loss: 0.0034174793
test_loss: 0.004486136
train_loss: 0.003684173
test_loss: 0.004758312
train_loss: 0.0034236317
test_loss: 0.004430308
train_loss: 0.0030607684
test_loss: 0.0043634526
train_loss: 0.0033212383
test_loss: 0.0043636104
train_loss: 0.0029505142
test_loss: 0.0042684353
train_loss: 0.0035295049
test_loss: 0.004458062
train_loss: 0.0033404818
test_loss: 0.0044617453
train_loss: 0.0032464005
test_loss: 0.0043629208
train_loss: 0.0038662653
test_loss: 0.004457114
train_loss: 0.0034303372
test_loss: 0.0044924035
train_loss: 0.0031030457
test_loss: 0.004342556
train_loss: 0.0030249911
test_loss: 0.004328059
train_loss: 0.0031451632
test_loss: 0.00445109
train_loss: 0.003182645
test_loss: 0.004452281
train_loss: 0.0034402562
test_loss: 0.00449754
train_loss: 0.003314009
test_loss: 0.0044927457
train_loss: 0.0030280808
test_loss: 0.0043660123
train_loss: 0.0032170564
test_loss: 0.0043610893
train_loss: 0.0034488146
test_loss: 0.0044721887
train_loss: 0.0031806445
test_loss: 0.004431776
train_loss: 0.0036812655
test_loss: 0.004450867
train_loss: 0.0033457717
test_loss: 0.0044553126
train_loss: 0.003580818
test_loss: 0.004359486
train_loss: 0.003229715
test_loss: 0.004351953
train_loss: 0.0031011696
test_loss: 0.004388447
train_loss: 0.003259922
test_loss: 0.004274031
train_loss: 0.0031639389
test_loss: 0.004386456
train_loss: 0.0033826027
test_loss: 0.0046628215
train_loss: 0.0034596696
test_loss: 0.0043759057
train_loss: 0.0035228105
test_loss: 0.0047271117
train_loss: 0.0030963148
test_loss: 0.004585856
train_loss: 0.003180412
test_loss: 0.004460032
train_loss: 0.0031594275
test_loss: 0.004477412
train_loss: 0.003115518
test_loss: 0.0044995523
train_loss: 0.0033016976
test_loss: 0.004388013
train_loss: 0.003888272
test_loss: 0.0046409434
train_loss: 0.0035182806
test_loss: 0.004357657
train_loss: 0.003279976
test_loss: 0.0043359064
train_loss: 0.003398064
test_loss: 0.004757242
train_loss: 0.0031279863
test_loss: 0.004400679
train_loss: 0.0031288732
test_loss: 0.0043286704
train_loss: 0.0033937655
test_loss: 0.0044362475
train_loss: 0.003189955
test_loss: 0.0044850465
train_loss: 0.0030794996
test_loss: 0.0042056083
train_loss: 0.0036669336
test_loss: 0.0044125626
train_loss: 0.0032845526
test_loss: 0.0043911454
train_loss: 0.0034313106
test_loss: 0.004533366
train_loss: 0.0032321485
test_loss: 0.0044143936
train_loss: 0.0031686223
test_loss: 0.004323725
train_loss: 0.003029955
test_loss: 0.004335937
train_loss: 0.003309945
test_loss: 0.0043357145
train_loss: 0.003185107
test_loss: 0.004308714
train_loss: 0.0032712673
test_loss: 0.0044168537
train_loss: 0.0031734349
test_loss: 0.0043065916
train_loss: 0.0030139084
test_loss: 0.0042188615
train_loss: 0.0032204106
test_loss: 0.004456125
train_loss: 0.0032443882
test_loss: 0.004499923
train_loss: 0.0033230204
test_loss: 0.0044432417
train_loss: 0.003597279
test_loss: 0.004658228
train_loss: 0.0035615508
test_loss: 0.004280226
train_loss: 0.0032108729
test_loss: 0.004457251
train_loss: 0.003293795
test_loss: 0.0044929185
train_loss: 0.0034180488
test_loss: 0.0044009425
train_loss: 0.0031194056
test_loss: 0.0043622297
train_loss: 0.003173777
test_loss: 0.0043848283
train_loss: 0.0031567405
test_loss: 0.004299457
train_loss: 0.0030676292
test_loss: 0.0044937753
train_loss: 0.003108688
test_loss: 0.0045253304
train_loss: 0.0034779743
test_loss: 0.0045218305
train_loss: 0.003334722
test_loss: 0.0045210677
train_loss: 0.0031929535
test_loss: 0.0045980155
train_loss: 0.0032969334
test_loss: 0.00459212
train_loss: 0.0032489998
test_loss: 0.0047427476
train_loss: 0.0034404623
test_loss: 0.00444502
train_loss: 0.003380399
test_loss: 0.004445912
train_loss: 0.0030915826
test_loss: 0.0044275667
train_loss: 0.003262601
test_loss: 0.004415689
train_loss: 0.003049138
test_loss: 0.004456826
train_loss: 0.0030754884
test_loss: 0.004475861
train_loss: 0.0032177006
test_loss: 0.004405274
train_loss: 0.003374644
test_loss: 0.004519828
train_loss: 0.0029985963
test_loss: 0.004216505
train_loss: 0.0033433079
test_loss: 0.004373676
train_loss: 0.003111925
test_loss: 0.004420058
train_loss: 0.0031948544
test_loss: 0.0043108515
train_loss: 0.00312158
test_loss: 0.004215969
train_loss: 0.0030039032
test_loss: 0.004311111
train_loss: 0.0029668573
test_loss: 0.00430489
train_loss: 0.0029919643
test_loss: 0.0041931593
train_loss: 0.0032226965
test_loss: 0.0044636317
train_loss: 0.003284985
test_loss: 0.004408878
train_loss: 0.003293979
test_loss: 0.0043603564
train_loss: 0.0032465956
test_loss: 0.004475528
train_loss: 0.0031027254
test_loss: 0.0044361865
train_loss: 0.003199329
test_loss: 0.00434448
train_loss: 0.002985155
test_loss: 0.0043114317
train_loss: 0.003013107
test_loss: 0.004247547
train_loss: 0.0033270782
test_loss: 0.0045116786
train_loss: 0.0033935665
test_loss: 0.0044587683
train_loss: 0.0032178713
test_loss: 0.0043309904
train_loss: 0.002773102
test_loss: 0.0043133656
train_loss: 0.0030989968
test_loss: 0.0042394316
train_loss: 0.0031192906
test_loss: 0.0042474084
train_loss: 0.0032310057
test_loss: 0.004331489
train_loss: 0.003114086
test_loss: 0.004316708
train_loss: 0.0031034695
test_loss: 0.004409448
train_loss: 0.0031988183
test_loss: 0.0044332957
train_loss: 0.0032314444
test_loss: 0.0042081424
train_loss: 0.0032333392
test_loss: 0.004281706
train_loss: 0.0033002605
test_loss: 0.0045472626
train_loss: 0.0032377145
test_loss: 0.0043928437
train_loss: 0.0033585131
test_loss: 0.00451002
train_loss: 0.0031728386
test_loss: 0.004616834
train_loss: 0.0033251531
test_loss: 0.0044395425
train_loss: 0.0033931672
test_loss: 0.0044199964
train_loss: 0.0031607407
test_loss: 0.004336541
train_loss: 0.0031534368
test_loss: 0.004300084
train_loss: 0.0027983708
test_loss: 0.004162248
train_loss: 0.0030502675
test_loss: 0.004351177
train_loss: 0.0032296972
test_loss: 0.0044210544
train_loss: 0.0032946747
test_loss: 0.004387146
train_loss: 0.003315806
test_loss: 0.004334142
train_loss: 0.0034196062
test_loss: 0.004412837
train_loss: 0.0033473955
test_loss: 0.004404942
train_loss: 0.0034963456
test_loss: 0.004406935
train_loss: 0.0031945943
test_loss: 0.004294523
train_loss: 0.0032924968
test_loss: 0.0045336615
train_loss: 0.0035573153
test_loss: 0.0045792013
train_loss: 0.0031961189
test_loss: 0.0047098445
train_loss: 0.0031044031
test_loss: 0.0042223893
train_loss: 0.0031002406
test_loss: 0.004308727
train_loss: 0.0032134827
test_loss: 0.004297421
train_loss: 0.003192247
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.004220984
train_loss: 0.0032130983
test_loss: 0.0044713975
train_loss: 0.0031258203
test_loss: 0.004354408
train_loss: 0.0033856444
test_loss: 0.004315749
train_loss: 0.003667175
test_loss: 0.004385635
train_loss: 0.00335218
test_loss: 0.0044895713
train_loss: 0.003109778
test_loss: 0.0043935333
train_loss: 0.0030193536
test_loss: 0.004434574
train_loss: 0.003221001
test_loss: 0.004516915
train_loss: 0.0029960098
test_loss: 0.0042094174
train_loss: 0.003278064
test_loss: 0.004499007
train_loss: 0.003360292
test_loss: 0.0046540345
train_loss: 0.0032098615
test_loss: 0.004363082
train_loss: 0.003188868
test_loss: 0.0044643898
train_loss: 0.0030329013
test_loss: 0.0044411444
train_loss: 0.003287295
test_loss: 0.0044974443
train_loss: 0.0034579444
test_loss: 0.004306635
train_loss: 0.0032404228
test_loss: 0.004347677
train_loss: 0.0029187899
test_loss: 0.004346709
train_loss: 0.0031734537
test_loss: 0.004233609
train_loss: 0.0031982618
test_loss: 0.00434386
train_loss: 0.0032521244
test_loss: 0.0044713337
train_loss: 0.0031515583
test_loss: 0.0044054654
train_loss: 0.0030851113
test_loss: 0.0044103954
train_loss: 0.003237112
test_loss: 0.004272363
train_loss: 0.0030191333
test_loss: 0.0044060033
train_loss: 0.0033662484
test_loss: 0.0043959892
train_loss: 0.0032823412
test_loss: 0.0042223046
train_loss: 0.0033197086
test_loss: 0.004428048
train_loss: 0.0030213157
test_loss: 0.0042828196
train_loss: 0.0028332118
test_loss: 0.004234043
train_loss: 0.0030190148
test_loss: 0.004360548
train_loss: 0.0032050372
test_loss: 0.0043635233
train_loss: 0.0030885083
test_loss: 0.004407821
train_loss: 0.00306018
test_loss: 0.00427948
train_loss: 0.0030899309
test_loss: 0.004287477
train_loss: 0.002999032
test_loss: 0.0041901134
train_loss: 0.002943871
test_loss: 0.004337223
train_loss: 0.0032563517
test_loss: 0.0043231044
train_loss: 0.0032949324
test_loss: 0.004430643
train_loss: 0.003218709
test_loss: 0.0044485154
train_loss: 0.003178105
test_loss: 0.004460796
train_loss: 0.003218939
test_loss: 0.0043646125
train_loss: 0.0030770663
test_loss: 0.004374814
train_loss: 0.0035135213
test_loss: 0.0043635243
train_loss: 0.0031337421
test_loss: 0.0043894965
train_loss: 0.0030213739
test_loss: 0.0042543467
train_loss: 0.0028966474
test_loss: 0.004090215
train_loss: 0.0030702222
test_loss: 0.004393413
train_loss: 0.003385645
test_loss: 0.0044753025
train_loss: 0.0032099597
test_loss: 0.004235666
train_loss: 0.0030644117
test_loss: 0.0042904015
train_loss: 0.0031997936
test_loss: 0.004487652
train_loss: 0.0033540637
test_loss: 0.0046996763
train_loss: 0.0034000315
test_loss: 0.0043099113
train_loss: 0.0031942888
test_loss: 0.0045633027
train_loss: 0.0032157637
test_loss: 0.0042949896
train_loss: 0.0030167229
test_loss: 0.004215023
train_loss: 0.003238359
test_loss: 0.0042347335
train_loss: 0.0033149645
test_loss: 0.0043318234
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a57e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a644158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a644268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a5da1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a5dae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a53ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a52f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a4dc840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a4dc400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a4dc510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a47a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a454c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a4642f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a464d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a3b67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a3fec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a3e9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a3b1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a3699d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a35df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a30a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a2c2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a2e1ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a29d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a28d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248a28d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f24808609d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2480876620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2480876158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248081c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f24807cbae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f24807f6268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f248079e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f24807b3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2480766598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2480766f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.11559127e-05
Iter: 2 loss: 1.69474697e-05
Iter: 3 loss: 1.691133e-05
Iter: 4 loss: 1.56433671e-05
Iter: 5 loss: 3.11243421e-05
Iter: 6 loss: 1.56291208e-05
Iter: 7 loss: 1.49807529e-05
Iter: 8 loss: 1.43349716e-05
Iter: 9 loss: 1.42010213e-05
Iter: 10 loss: 1.30425178e-05
Iter: 11 loss: 1.74206343e-05
Iter: 12 loss: 1.27623352e-05
Iter: 13 loss: 1.16931651e-05
Iter: 14 loss: 1.38203459e-05
Iter: 15 loss: 1.12562939e-05
Iter: 16 loss: 1.0315799e-05
Iter: 17 loss: 1.3597195e-05
Iter: 18 loss: 1.00699162e-05
Iter: 19 loss: 9.28332338e-06
Iter: 20 loss: 1.26214291e-05
Iter: 21 loss: 9.11604366e-06
Iter: 22 loss: 8.666555e-06
Iter: 23 loss: 8.35221272e-06
Iter: 24 loss: 8.192118e-06
Iter: 25 loss: 7.94567222e-06
Iter: 26 loss: 7.89119713e-06
Iter: 27 loss: 7.61131378e-06
Iter: 28 loss: 7.73434749e-06
Iter: 29 loss: 7.42058273e-06
Iter: 30 loss: 7.04366039e-06
Iter: 31 loss: 6.73169188e-06
Iter: 32 loss: 6.62310458e-06
Iter: 33 loss: 6.1686942e-06
Iter: 34 loss: 7.91051934e-06
Iter: 35 loss: 6.06141612e-06
Iter: 36 loss: 5.66041308e-06
Iter: 37 loss: 6.85885e-06
Iter: 38 loss: 5.53900418e-06
Iter: 39 loss: 5.43644364e-06
Iter: 40 loss: 5.3738986e-06
Iter: 41 loss: 5.20881076e-06
Iter: 42 loss: 5.46212232e-06
Iter: 43 loss: 5.13060104e-06
Iter: 44 loss: 5.00758233e-06
Iter: 45 loss: 4.95069071e-06
Iter: 46 loss: 4.89015792e-06
Iter: 47 loss: 4.7399e-06
Iter: 48 loss: 4.7398571e-06
Iter: 49 loss: 4.65166431e-06
Iter: 50 loss: 4.48943956e-06
Iter: 51 loss: 8.23426581e-06
Iter: 52 loss: 4.48912442e-06
Iter: 53 loss: 4.33726564e-06
Iter: 54 loss: 6.51709206e-06
Iter: 55 loss: 4.33696869e-06
Iter: 56 loss: 4.2239817e-06
Iter: 57 loss: 4.1603239e-06
Iter: 58 loss: 4.11116616e-06
Iter: 59 loss: 3.95849838e-06
Iter: 60 loss: 5.25052747e-06
Iter: 61 loss: 3.94950439e-06
Iter: 62 loss: 3.85799149e-06
Iter: 63 loss: 4.0302084e-06
Iter: 64 loss: 3.81932477e-06
Iter: 65 loss: 3.70257749e-06
Iter: 66 loss: 4.26825272e-06
Iter: 67 loss: 3.68195583e-06
Iter: 68 loss: 3.62575952e-06
Iter: 69 loss: 3.6352435e-06
Iter: 70 loss: 3.58353964e-06
Iter: 71 loss: 3.50114738e-06
Iter: 72 loss: 3.47552714e-06
Iter: 73 loss: 3.42698559e-06
Iter: 74 loss: 3.32642685e-06
Iter: 75 loss: 3.84932e-06
Iter: 76 loss: 3.31057663e-06
Iter: 77 loss: 3.2543951e-06
Iter: 78 loss: 3.25357837e-06
Iter: 79 loss: 3.19209016e-06
Iter: 80 loss: 3.29675117e-06
Iter: 81 loss: 3.16451064e-06
Iter: 82 loss: 3.12444035e-06
Iter: 83 loss: 3.05814319e-06
Iter: 84 loss: 3.05792628e-06
Iter: 85 loss: 3.00068723e-06
Iter: 86 loss: 2.99687463e-06
Iter: 87 loss: 2.96517032e-06
Iter: 88 loss: 2.90748449e-06
Iter: 89 loss: 4.26891256e-06
Iter: 90 loss: 2.90745334e-06
Iter: 91 loss: 2.8559748e-06
Iter: 92 loss: 3.49023367e-06
Iter: 93 loss: 2.85556871e-06
Iter: 94 loss: 2.81958273e-06
Iter: 95 loss: 2.91766264e-06
Iter: 96 loss: 2.80779159e-06
Iter: 97 loss: 2.77191907e-06
Iter: 98 loss: 2.78083235e-06
Iter: 99 loss: 2.74578497e-06
Iter: 100 loss: 2.7030178e-06
Iter: 101 loss: 3.06072457e-06
Iter: 102 loss: 2.70054488e-06
Iter: 103 loss: 2.67013888e-06
Iter: 104 loss: 2.78739526e-06
Iter: 105 loss: 2.6629823e-06
Iter: 106 loss: 2.63124207e-06
Iter: 107 loss: 2.62661069e-06
Iter: 108 loss: 2.60444062e-06
Iter: 109 loss: 2.56681869e-06
Iter: 110 loss: 2.5899426e-06
Iter: 111 loss: 2.54269025e-06
Iter: 112 loss: 2.50173116e-06
Iter: 113 loss: 2.64714072e-06
Iter: 114 loss: 2.49119034e-06
Iter: 115 loss: 2.47782555e-06
Iter: 116 loss: 2.46878926e-06
Iter: 117 loss: 2.45446313e-06
Iter: 118 loss: 2.42370174e-06
Iter: 119 loss: 2.90246771e-06
Iter: 120 loss: 2.42255874e-06
Iter: 121 loss: 2.39636347e-06
Iter: 122 loss: 2.48924312e-06
Iter: 123 loss: 2.38968573e-06
Iter: 124 loss: 2.36673759e-06
Iter: 125 loss: 2.71092927e-06
Iter: 126 loss: 2.36674168e-06
Iter: 127 loss: 2.35413881e-06
Iter: 128 loss: 2.32188859e-06
Iter: 129 loss: 2.60230627e-06
Iter: 130 loss: 2.31672493e-06
Iter: 131 loss: 2.28430576e-06
Iter: 132 loss: 2.63987386e-06
Iter: 133 loss: 2.28353474e-06
Iter: 134 loss: 2.25690974e-06
Iter: 135 loss: 2.40669488e-06
Iter: 136 loss: 2.25327403e-06
Iter: 137 loss: 2.23353118e-06
Iter: 138 loss: 2.25788654e-06
Iter: 139 loss: 2.22318135e-06
Iter: 140 loss: 2.20804873e-06
Iter: 141 loss: 2.35375e-06
Iter: 142 loss: 2.20752327e-06
Iter: 143 loss: 2.19102913e-06
Iter: 144 loss: 2.17194906e-06
Iter: 145 loss: 2.16975059e-06
Iter: 146 loss: 2.15348336e-06
Iter: 147 loss: 2.16104377e-06
Iter: 148 loss: 2.14247939e-06
Iter: 149 loss: 2.11858764e-06
Iter: 150 loss: 2.2643419e-06
Iter: 151 loss: 2.1157739e-06
Iter: 152 loss: 2.10544454e-06
Iter: 153 loss: 2.10350163e-06
Iter: 154 loss: 2.09586096e-06
Iter: 155 loss: 2.07589437e-06
Iter: 156 loss: 2.23205393e-06
Iter: 157 loss: 2.07205312e-06
Iter: 158 loss: 2.05272272e-06
Iter: 159 loss: 2.28657041e-06
Iter: 160 loss: 2.05252081e-06
Iter: 161 loss: 2.04208504e-06
Iter: 162 loss: 2.10408825e-06
Iter: 163 loss: 2.04073808e-06
Iter: 164 loss: 2.03002355e-06
Iter: 165 loss: 2.04432263e-06
Iter: 166 loss: 2.02454248e-06
Iter: 167 loss: 2.01545731e-06
Iter: 168 loss: 2.00014324e-06
Iter: 169 loss: 2.00017189e-06
Iter: 170 loss: 1.98069301e-06
Iter: 171 loss: 2.10068242e-06
Iter: 172 loss: 1.97831537e-06
Iter: 173 loss: 1.9588656e-06
Iter: 174 loss: 2.09116888e-06
Iter: 175 loss: 1.95702933e-06
Iter: 176 loss: 1.94688073e-06
Iter: 177 loss: 1.95068719e-06
Iter: 178 loss: 1.93973574e-06
Iter: 179 loss: 1.92479865e-06
Iter: 180 loss: 2.01936291e-06
Iter: 181 loss: 1.92309335e-06
Iter: 182 loss: 1.91264303e-06
Iter: 183 loss: 1.91543768e-06
Iter: 184 loss: 1.90515027e-06
Iter: 185 loss: 1.8946165e-06
Iter: 186 loss: 1.88297e-06
Iter: 187 loss: 1.88137199e-06
Iter: 188 loss: 1.88770559e-06
Iter: 189 loss: 1.87387536e-06
Iter: 190 loss: 1.86858256e-06
Iter: 191 loss: 1.86004991e-06
Iter: 192 loss: 1.85997612e-06
Iter: 193 loss: 1.8511837e-06
Iter: 194 loss: 1.84887676e-06
Iter: 195 loss: 1.84337296e-06
Iter: 196 loss: 1.83651696e-06
Iter: 197 loss: 1.83631403e-06
Iter: 198 loss: 1.82821543e-06
Iter: 199 loss: 1.81718372e-06
Iter: 200 loss: 1.81666485e-06
Iter: 201 loss: 1.80442748e-06
Iter: 202 loss: 1.812101e-06
Iter: 203 loss: 1.79664914e-06
Iter: 204 loss: 1.78264224e-06
Iter: 205 loss: 1.86848e-06
Iter: 206 loss: 1.78093512e-06
Iter: 207 loss: 1.7719974e-06
Iter: 208 loss: 1.90930291e-06
Iter: 209 loss: 1.77200843e-06
Iter: 210 loss: 1.76716924e-06
Iter: 211 loss: 1.76656533e-06
Iter: 212 loss: 1.76302672e-06
Iter: 213 loss: 1.75320599e-06
Iter: 214 loss: 1.75422383e-06
Iter: 215 loss: 1.74561865e-06
Iter: 216 loss: 1.73564649e-06
Iter: 217 loss: 1.86715988e-06
Iter: 218 loss: 1.73560829e-06
Iter: 219 loss: 1.72950831e-06
Iter: 220 loss: 1.72218802e-06
Iter: 221 loss: 1.72143768e-06
Iter: 222 loss: 1.71195245e-06
Iter: 223 loss: 1.77644449e-06
Iter: 224 loss: 1.71104932e-06
Iter: 225 loss: 1.70325143e-06
Iter: 226 loss: 1.8072426e-06
Iter: 227 loss: 1.70320516e-06
Iter: 228 loss: 1.69965278e-06
Iter: 229 loss: 1.69126463e-06
Iter: 230 loss: 1.78797814e-06
Iter: 231 loss: 1.69049213e-06
Iter: 232 loss: 1.68068266e-06
Iter: 233 loss: 1.69305474e-06
Iter: 234 loss: 1.67563985e-06
Iter: 235 loss: 1.67252017e-06
Iter: 236 loss: 1.66972075e-06
Iter: 237 loss: 1.66595282e-06
Iter: 238 loss: 1.65915071e-06
Iter: 239 loss: 1.65914912e-06
Iter: 240 loss: 1.65152699e-06
Iter: 241 loss: 1.65289578e-06
Iter: 242 loss: 1.64582048e-06
Iter: 243 loss: 1.63545042e-06
Iter: 244 loss: 1.69460441e-06
Iter: 245 loss: 1.63395805e-06
Iter: 246 loss: 1.62801484e-06
Iter: 247 loss: 1.62797369e-06
Iter: 248 loss: 1.62366598e-06
Iter: 249 loss: 1.61622268e-06
Iter: 250 loss: 1.61626906e-06
Iter: 251 loss: 1.61037769e-06
Iter: 252 loss: 1.61020375e-06
Iter: 253 loss: 1.60630611e-06
Iter: 254 loss: 1.60134732e-06
Iter: 255 loss: 1.60092895e-06
Iter: 256 loss: 1.59451724e-06
Iter: 257 loss: 1.63238587e-06
Iter: 258 loss: 1.59370416e-06
Iter: 259 loss: 1.58799264e-06
Iter: 260 loss: 1.65205927e-06
Iter: 261 loss: 1.58788885e-06
Iter: 262 loss: 1.58465605e-06
Iter: 263 loss: 1.57817715e-06
Iter: 264 loss: 1.69902864e-06
Iter: 265 loss: 1.57810882e-06
Iter: 266 loss: 1.57200066e-06
Iter: 267 loss: 1.61877892e-06
Iter: 268 loss: 1.57153852e-06
Iter: 269 loss: 1.56642466e-06
Iter: 270 loss: 1.56234569e-06
Iter: 271 loss: 1.56082069e-06
Iter: 272 loss: 1.55901546e-06
Iter: 273 loss: 1.55667249e-06
Iter: 274 loss: 1.55476073e-06
Iter: 275 loss: 1.54928625e-06
Iter: 276 loss: 1.57422301e-06
Iter: 277 loss: 1.54730162e-06
Iter: 278 loss: 1.53860333e-06
Iter: 279 loss: 1.57983777e-06
Iter: 280 loss: 1.53709084e-06
Iter: 281 loss: 1.53135022e-06
Iter: 282 loss: 1.52891789e-06
Iter: 283 loss: 1.52591042e-06
Iter: 284 loss: 1.51735355e-06
Iter: 285 loss: 1.57435227e-06
Iter: 286 loss: 1.51645713e-06
Iter: 287 loss: 1.51299218e-06
Iter: 288 loss: 1.51241318e-06
Iter: 289 loss: 1.50943322e-06
Iter: 290 loss: 1.50557184e-06
Iter: 291 loss: 1.50530764e-06
Iter: 292 loss: 1.50108849e-06
Iter: 293 loss: 1.51039433e-06
Iter: 294 loss: 1.49956986e-06
Iter: 295 loss: 1.49522521e-06
Iter: 296 loss: 1.54597387e-06
Iter: 297 loss: 1.49512789e-06
Iter: 298 loss: 1.49213031e-06
Iter: 299 loss: 1.50269693e-06
Iter: 300 loss: 1.491381e-06
Iter: 301 loss: 1.48783386e-06
Iter: 302 loss: 1.49002653e-06
Iter: 303 loss: 1.48554147e-06
Iter: 304 loss: 1.48192964e-06
Iter: 305 loss: 1.47660512e-06
Iter: 306 loss: 1.47650405e-06
Iter: 307 loss: 1.47096955e-06
Iter: 308 loss: 1.51187123e-06
Iter: 309 loss: 1.47051981e-06
Iter: 310 loss: 1.46654065e-06
Iter: 311 loss: 1.48124013e-06
Iter: 312 loss: 1.46548768e-06
Iter: 313 loss: 1.46118964e-06
Iter: 314 loss: 1.48985873e-06
Iter: 315 loss: 1.46077991e-06
Iter: 316 loss: 1.45739159e-06
Iter: 317 loss: 1.46671596e-06
Iter: 318 loss: 1.45627337e-06
Iter: 319 loss: 1.45340118e-06
Iter: 320 loss: 1.44865169e-06
Iter: 321 loss: 1.44864134e-06
Iter: 322 loss: 1.44308115e-06
Iter: 323 loss: 1.46542857e-06
Iter: 324 loss: 1.44186492e-06
Iter: 325 loss: 1.43624504e-06
Iter: 326 loss: 1.44528292e-06
Iter: 327 loss: 1.4336689e-06
Iter: 328 loss: 1.43069087e-06
Iter: 329 loss: 1.43057173e-06
Iter: 330 loss: 1.42702459e-06
Iter: 331 loss: 1.42575993e-06
Iter: 332 loss: 1.42379656e-06
Iter: 333 loss: 1.42044223e-06
Iter: 334 loss: 1.42882038e-06
Iter: 335 loss: 1.4192583e-06
Iter: 336 loss: 1.41584974e-06
Iter: 337 loss: 1.46499849e-06
Iter: 338 loss: 1.41587e-06
Iter: 339 loss: 1.4135087e-06
Iter: 340 loss: 1.40989846e-06
Iter: 341 loss: 1.40981e-06
Iter: 342 loss: 1.40702798e-06
Iter: 343 loss: 1.44662329e-06
Iter: 344 loss: 1.40700308e-06
Iter: 345 loss: 1.40466148e-06
Iter: 346 loss: 1.3993108e-06
Iter: 347 loss: 1.47030073e-06
Iter: 348 loss: 1.3989885e-06
Iter: 349 loss: 1.39478516e-06
Iter: 350 loss: 1.43459692e-06
Iter: 351 loss: 1.39460155e-06
Iter: 352 loss: 1.39081715e-06
Iter: 353 loss: 1.42964598e-06
Iter: 354 loss: 1.3906747e-06
Iter: 355 loss: 1.3890201e-06
Iter: 356 loss: 1.38484029e-06
Iter: 357 loss: 1.42986778e-06
Iter: 358 loss: 1.38439964e-06
Iter: 359 loss: 1.38001246e-06
Iter: 360 loss: 1.42577835e-06
Iter: 361 loss: 1.37995448e-06
Iter: 362 loss: 1.37664051e-06
Iter: 363 loss: 1.37973666e-06
Iter: 364 loss: 1.37478912e-06
Iter: 365 loss: 1.37037841e-06
Iter: 366 loss: 1.38612677e-06
Iter: 367 loss: 1.36921926e-06
Iter: 368 loss: 1.36649828e-06
Iter: 369 loss: 1.39521444e-06
Iter: 370 loss: 1.36639551e-06
Iter: 371 loss: 1.36368783e-06
Iter: 372 loss: 1.36630933e-06
Iter: 373 loss: 1.36217795e-06
Iter: 374 loss: 1.35973266e-06
Iter: 375 loss: 1.367918e-06
Iter: 376 loss: 1.35907158e-06
Iter: 377 loss: 1.35608468e-06
Iter: 378 loss: 1.37170377e-06
Iter: 379 loss: 1.35560413e-06
Iter: 380 loss: 1.3541063e-06
Iter: 381 loss: 1.35119194e-06
Iter: 382 loss: 1.41511828e-06
Iter: 383 loss: 1.35120933e-06
Iter: 384 loss: 1.34834636e-06
Iter: 385 loss: 1.37436928e-06
Iter: 386 loss: 1.34816025e-06
Iter: 387 loss: 1.3450765e-06
Iter: 388 loss: 1.34493575e-06
Iter: 389 loss: 1.34254515e-06
Iter: 390 loss: 1.33904689e-06
Iter: 391 loss: 1.34272318e-06
Iter: 392 loss: 1.33711e-06
Iter: 393 loss: 1.33366734e-06
Iter: 394 loss: 1.33745971e-06
Iter: 395 loss: 1.33179901e-06
Iter: 396 loss: 1.32794958e-06
Iter: 397 loss: 1.35040455e-06
Iter: 398 loss: 1.32746982e-06
Iter: 399 loss: 1.32527009e-06
Iter: 400 loss: 1.32520245e-06
Iter: 401 loss: 1.32294895e-06
Iter: 402 loss: 1.31864147e-06
Iter: 403 loss: 1.41042517e-06
Iter: 404 loss: 1.31863374e-06
Iter: 405 loss: 1.31550212e-06
Iter: 406 loss: 1.31943557e-06
Iter: 407 loss: 1.31390459e-06
Iter: 408 loss: 1.31037041e-06
Iter: 409 loss: 1.338733e-06
Iter: 410 loss: 1.31011711e-06
Iter: 411 loss: 1.30747549e-06
Iter: 412 loss: 1.32441141e-06
Iter: 413 loss: 1.30716546e-06
Iter: 414 loss: 1.30446097e-06
Iter: 415 loss: 1.31906654e-06
Iter: 416 loss: 1.30403339e-06
Iter: 417 loss: 1.30238197e-06
Iter: 418 loss: 1.30080889e-06
Iter: 419 loss: 1.3004169e-06
Iter: 420 loss: 1.29763475e-06
Iter: 421 loss: 1.29755836e-06
Iter: 422 loss: 1.29532759e-06
Iter: 423 loss: 1.29285991e-06
Iter: 424 loss: 1.29267653e-06
Iter: 425 loss: 1.29124908e-06
Iter: 426 loss: 1.28830243e-06
Iter: 427 loss: 1.33605454e-06
Iter: 428 loss: 1.28819715e-06
Iter: 429 loss: 1.28608394e-06
Iter: 430 loss: 1.28593e-06
Iter: 431 loss: 1.28432885e-06
Iter: 432 loss: 1.28166153e-06
Iter: 433 loss: 1.28163583e-06
Iter: 434 loss: 1.27825979e-06
Iter: 435 loss: 1.28730335e-06
Iter: 436 loss: 1.27715202e-06
Iter: 437 loss: 1.27474402e-06
Iter: 438 loss: 1.30075318e-06
Iter: 439 loss: 1.27466649e-06
Iter: 440 loss: 1.27188332e-06
Iter: 441 loss: 1.27298e-06
Iter: 442 loss: 1.26989994e-06
Iter: 443 loss: 1.26782015e-06
Iter: 444 loss: 1.26596478e-06
Iter: 445 loss: 1.26544478e-06
Iter: 446 loss: 1.26194811e-06
Iter: 447 loss: 1.28205068e-06
Iter: 448 loss: 1.26147404e-06
Iter: 449 loss: 1.26055465e-06
Iter: 450 loss: 1.26016903e-06
Iter: 451 loss: 1.25882252e-06
Iter: 452 loss: 1.25626354e-06
Iter: 453 loss: 1.31453362e-06
Iter: 454 loss: 1.25624092e-06
Iter: 455 loss: 1.25411088e-06
Iter: 456 loss: 1.25688962e-06
Iter: 457 loss: 1.2530021e-06
Iter: 458 loss: 1.25059455e-06
Iter: 459 loss: 1.26506029e-06
Iter: 460 loss: 1.25033819e-06
Iter: 461 loss: 1.24823487e-06
Iter: 462 loss: 1.25221959e-06
Iter: 463 loss: 1.24735584e-06
Iter: 464 loss: 1.24509074e-06
Iter: 465 loss: 1.2638601e-06
Iter: 466 loss: 1.24495341e-06
Iter: 467 loss: 1.24348139e-06
Iter: 468 loss: 1.2409555e-06
Iter: 469 loss: 1.24092958e-06
Iter: 470 loss: 1.2389653e-06
Iter: 471 loss: 1.23887071e-06
Iter: 472 loss: 1.23735811e-06
Iter: 473 loss: 1.23710424e-06
Iter: 474 loss: 1.23604673e-06
Iter: 475 loss: 1.23383961e-06
Iter: 476 loss: 1.23158406e-06
Iter: 477 loss: 1.23117286e-06
Iter: 478 loss: 1.22783695e-06
Iter: 479 loss: 1.24799112e-06
Iter: 480 loss: 1.22742517e-06
Iter: 481 loss: 1.22447e-06
Iter: 482 loss: 1.23578548e-06
Iter: 483 loss: 1.22379276e-06
Iter: 484 loss: 1.2220944e-06
Iter: 485 loss: 1.2219532e-06
Iter: 486 loss: 1.2209124e-06
Iter: 487 loss: 1.21856556e-06
Iter: 488 loss: 1.25392069e-06
Iter: 489 loss: 1.21849348e-06
Iter: 490 loss: 1.21843027e-06
Iter: 491 loss: 1.21727408e-06
Iter: 492 loss: 1.21659377e-06
Iter: 493 loss: 1.2145847e-06
Iter: 494 loss: 1.22022379e-06
Iter: 495 loss: 1.21343237e-06
Iter: 496 loss: 1.2104523e-06
Iter: 497 loss: 1.22768745e-06
Iter: 498 loss: 1.21010896e-06
Iter: 499 loss: 1.20808966e-06
Iter: 500 loss: 1.22037625e-06
Iter: 501 loss: 1.20781431e-06
Iter: 502 loss: 1.20554068e-06
Iter: 503 loss: 1.20952745e-06
Iter: 504 loss: 1.20449351e-06
Iter: 505 loss: 1.20221489e-06
Iter: 506 loss: 1.21252447e-06
Iter: 507 loss: 1.20171057e-06
Iter: 508 loss: 1.19980223e-06
Iter: 509 loss: 1.19837296e-06
Iter: 510 loss: 1.19772028e-06
Iter: 511 loss: 1.1958507e-06
Iter: 512 loss: 1.19583137e-06
Iter: 513 loss: 1.19413221e-06
Iter: 514 loss: 1.19238712e-06
Iter: 515 loss: 1.19207846e-06
Iter: 516 loss: 1.19009371e-06
Iter: 517 loss: 1.19653464e-06
Iter: 518 loss: 1.18950015e-06
Iter: 519 loss: 1.18764012e-06
Iter: 520 loss: 1.19540618e-06
Iter: 521 loss: 1.18726746e-06
Iter: 522 loss: 1.18488629e-06
Iter: 523 loss: 1.1954229e-06
Iter: 524 loss: 1.18435889e-06
Iter: 525 loss: 1.18309458e-06
Iter: 526 loss: 1.1844545e-06
Iter: 527 loss: 1.18240746e-06
Iter: 528 loss: 1.18030653e-06
Iter: 529 loss: 1.18913181e-06
Iter: 530 loss: 1.17987099e-06
Iter: 531 loss: 1.17860498e-06
Iter: 532 loss: 1.17651848e-06
Iter: 533 loss: 1.176502e-06
Iter: 534 loss: 1.17436139e-06
Iter: 535 loss: 1.17773379e-06
Iter: 536 loss: 1.17338845e-06
Iter: 537 loss: 1.17168361e-06
Iter: 538 loss: 1.17161824e-06
Iter: 539 loss: 1.17010404e-06
Iter: 540 loss: 1.16904789e-06
Iter: 541 loss: 1.16853175e-06
Iter: 542 loss: 1.16629337e-06
Iter: 543 loss: 1.17655225e-06
Iter: 544 loss: 1.16589945e-06
Iter: 545 loss: 1.16414583e-06
Iter: 546 loss: 1.16694605e-06
Iter: 547 loss: 1.16330921e-06
Iter: 548 loss: 1.16149067e-06
Iter: 549 loss: 1.17447337e-06
Iter: 550 loss: 1.16135516e-06
Iter: 551 loss: 1.15980697e-06
Iter: 552 loss: 1.1597831e-06
Iter: 553 loss: 1.15860132e-06
Iter: 554 loss: 1.1568743e-06
Iter: 555 loss: 1.15841988e-06
Iter: 556 loss: 1.1558858e-06
Iter: 557 loss: 1.15437842e-06
Iter: 558 loss: 1.15436046e-06
Iter: 559 loss: 1.15292278e-06
Iter: 560 loss: 1.15325201e-06
Iter: 561 loss: 1.15184537e-06
Iter: 562 loss: 1.15101761e-06
Iter: 563 loss: 1.15104126e-06
Iter: 564 loss: 1.15009334e-06
Iter: 565 loss: 1.14796023e-06
Iter: 566 loss: 1.1725607e-06
Iter: 567 loss: 1.1477207e-06
Iter: 568 loss: 1.14595991e-06
Iter: 569 loss: 1.14900035e-06
Iter: 570 loss: 1.14522675e-06
Iter: 571 loss: 1.14296085e-06
Iter: 572 loss: 1.1470629e-06
Iter: 573 loss: 1.14203942e-06
Iter: 574 loss: 1.14042246e-06
Iter: 575 loss: 1.14040461e-06
Iter: 576 loss: 1.13896158e-06
Iter: 577 loss: 1.14088323e-06
Iter: 578 loss: 1.13823296e-06
Iter: 579 loss: 1.13697752e-06
Iter: 580 loss: 1.13698161e-06
Iter: 581 loss: 1.13596832e-06
Iter: 582 loss: 1.13406804e-06
Iter: 583 loss: 1.1426323e-06
Iter: 584 loss: 1.13373687e-06
Iter: 585 loss: 1.13212445e-06
Iter: 586 loss: 1.13856845e-06
Iter: 587 loss: 1.13172098e-06
Iter: 588 loss: 1.13029034e-06
Iter: 589 loss: 1.13445606e-06
Iter: 590 loss: 1.12984299e-06
Iter: 591 loss: 1.1283521e-06
Iter: 592 loss: 1.12728435e-06
Iter: 593 loss: 1.12677344e-06
Iter: 594 loss: 1.1261784e-06
Iter: 595 loss: 1.1259026e-06
Iter: 596 loss: 1.12496355e-06
Iter: 597 loss: 1.12357031e-06
Iter: 598 loss: 1.12357225e-06
Iter: 599 loss: 1.12268992e-06
Iter: 600 loss: 1.12260022e-06
Iter: 601 loss: 1.12178543e-06
Iter: 602 loss: 1.12062196e-06
Iter: 603 loss: 1.1205608e-06
Iter: 604 loss: 1.11918825e-06
Iter: 605 loss: 1.11787972e-06
Iter: 606 loss: 1.11753354e-06
Iter: 607 loss: 1.11568011e-06
Iter: 608 loss: 1.12611428e-06
Iter: 609 loss: 1.11539282e-06
Iter: 610 loss: 1.11338954e-06
Iter: 611 loss: 1.13030148e-06
Iter: 612 loss: 1.11328018e-06
Iter: 613 loss: 1.11242434e-06
Iter: 614 loss: 1.11127633e-06
Iter: 615 loss: 1.11120346e-06
Iter: 616 loss: 1.10946758e-06
Iter: 617 loss: 1.11598752e-06
Iter: 618 loss: 1.1090774e-06
Iter: 619 loss: 1.10781605e-06
Iter: 620 loss: 1.12349949e-06
Iter: 621 loss: 1.10781696e-06
Iter: 622 loss: 1.10684027e-06
Iter: 623 loss: 1.1055165e-06
Iter: 624 loss: 1.10546807e-06
Iter: 625 loss: 1.10372366e-06
Iter: 626 loss: 1.11786028e-06
Iter: 627 loss: 1.10361907e-06
Iter: 628 loss: 1.10236545e-06
Iter: 629 loss: 1.10545989e-06
Iter: 630 loss: 1.10192605e-06
Iter: 631 loss: 1.1004314e-06
Iter: 632 loss: 1.11009626e-06
Iter: 633 loss: 1.10030885e-06
Iter: 634 loss: 1.09949747e-06
Iter: 635 loss: 1.10123324e-06
Iter: 636 loss: 1.09915709e-06
Iter: 637 loss: 1.09792563e-06
Iter: 638 loss: 1.09824782e-06
Iter: 639 loss: 1.09698385e-06
Iter: 640 loss: 1.09587972e-06
Iter: 641 loss: 1.09516623e-06
Iter: 642 loss: 1.09471489e-06
Iter: 643 loss: 1.0931625e-06
Iter: 644 loss: 1.0962176e-06
Iter: 645 loss: 1.09251152e-06
Iter: 646 loss: 1.09076529e-06
Iter: 647 loss: 1.09360303e-06
Iter: 648 loss: 1.08995869e-06
Iter: 649 loss: 1.08802931e-06
Iter: 650 loss: 1.0969984e-06
Iter: 651 loss: 1.0876596e-06
Iter: 652 loss: 1.08649579e-06
Iter: 653 loss: 1.08645486e-06
Iter: 654 loss: 1.08563734e-06
Iter: 655 loss: 1.08376298e-06
Iter: 656 loss: 1.10710789e-06
Iter: 657 loss: 1.08362701e-06
Iter: 658 loss: 1.08168649e-06
Iter: 659 loss: 1.09322332e-06
Iter: 660 loss: 1.08144195e-06
Iter: 661 loss: 1.08015956e-06
Iter: 662 loss: 1.09508414e-06
Iter: 663 loss: 1.08011727e-06
Iter: 664 loss: 1.07914354e-06
Iter: 665 loss: 1.08092638e-06
Iter: 666 loss: 1.07869892e-06
Iter: 667 loss: 1.07755886e-06
Iter: 668 loss: 1.08315919e-06
Iter: 669 loss: 1.07735514e-06
Iter: 670 loss: 1.07641108e-06
Iter: 671 loss: 1.08081917e-06
Iter: 672 loss: 1.07620622e-06
Iter: 673 loss: 1.07541177e-06
Iter: 674 loss: 1.07599499e-06
Iter: 675 loss: 1.07493599e-06
Iter: 676 loss: 1.07384221e-06
Iter: 677 loss: 1.07637197e-06
Iter: 678 loss: 1.07341066e-06
Iter: 679 loss: 1.0723702e-06
Iter: 680 loss: 1.07226026e-06
Iter: 681 loss: 1.07151072e-06
Iter: 682 loss: 1.07007281e-06
Iter: 683 loss: 1.06873335e-06
Iter: 684 loss: 1.06837172e-06
Iter: 685 loss: 1.06632785e-06
Iter: 686 loss: 1.07150504e-06
Iter: 687 loss: 1.06560913e-06
Iter: 688 loss: 1.06384118e-06
Iter: 689 loss: 1.09151e-06
Iter: 690 loss: 1.06383868e-06
Iter: 691 loss: 1.0626884e-06
Iter: 692 loss: 1.07514472e-06
Iter: 693 loss: 1.06264565e-06
Iter: 694 loss: 1.06178766e-06
Iter: 695 loss: 1.05982963e-06
Iter: 696 loss: 1.08545555e-06
Iter: 697 loss: 1.05971253e-06
Iter: 698 loss: 1.05788206e-06
Iter: 699 loss: 1.06568621e-06
Iter: 700 loss: 1.05753793e-06
Iter: 701 loss: 1.05689855e-06
Iter: 702 loss: 1.05669596e-06
Iter: 703 loss: 1.05595655e-06
Iter: 704 loss: 1.05617892e-06
Iter: 705 loss: 1.05537583e-06
Iter: 706 loss: 1.05428649e-06
Iter: 707 loss: 1.05827201e-06
Iter: 708 loss: 1.054005e-06
Iter: 709 loss: 1.05290928e-06
Iter: 710 loss: 1.05402e-06
Iter: 711 loss: 1.05230424e-06
Iter: 712 loss: 1.05131448e-06
Iter: 713 loss: 1.05630488e-06
Iter: 714 loss: 1.05113736e-06
Iter: 715 loss: 1.05009713e-06
Iter: 716 loss: 1.04860737e-06
Iter: 717 loss: 1.04854939e-06
Iter: 718 loss: 1.04703304e-06
Iter: 719 loss: 1.06129266e-06
Iter: 720 loss: 1.04701917e-06
Iter: 721 loss: 1.04580079e-06
Iter: 722 loss: 1.04580135e-06
Iter: 723 loss: 1.04486435e-06
Iter: 724 loss: 1.04325807e-06
Iter: 725 loss: 1.04569847e-06
Iter: 726 loss: 1.04248875e-06
Iter: 727 loss: 1.04089474e-06
Iter: 728 loss: 1.04958224e-06
Iter: 729 loss: 1.04067362e-06
Iter: 730 loss: 1.03953573e-06
Iter: 731 loss: 1.03954665e-06
Iter: 732 loss: 1.03897219e-06
Iter: 733 loss: 1.03740535e-06
Iter: 734 loss: 1.04566993e-06
Iter: 735 loss: 1.03690104e-06
Iter: 736 loss: 1.03638547e-06
Iter: 737 loss: 1.03603145e-06
Iter: 738 loss: 1.03506557e-06
Iter: 739 loss: 1.03600223e-06
Iter: 740 loss: 1.03446246e-06
Iter: 741 loss: 1.033422e-06
Iter: 742 loss: 1.037841e-06
Iter: 743 loss: 1.03317291e-06
Iter: 744 loss: 1.03218304e-06
Iter: 745 loss: 1.03281104e-06
Iter: 746 loss: 1.03150273e-06
Iter: 747 loss: 1.03048365e-06
Iter: 748 loss: 1.03456364e-06
Iter: 749 loss: 1.0302623e-06
Iter: 750 loss: 1.02920785e-06
Iter: 751 loss: 1.03021716e-06
Iter: 752 loss: 1.02857405e-06
Iter: 753 loss: 1.02746606e-06
Iter: 754 loss: 1.02809236e-06
Iter: 755 loss: 1.02682293e-06
Iter: 756 loss: 1.02556737e-06
Iter: 757 loss: 1.03162176e-06
Iter: 758 loss: 1.02529066e-06
Iter: 759 loss: 1.02399645e-06
Iter: 760 loss: 1.02384411e-06
Iter: 761 loss: 1.02290528e-06
Iter: 762 loss: 1.02139461e-06
Iter: 763 loss: 1.02869103e-06
Iter: 764 loss: 1.02110835e-06
Iter: 765 loss: 1.01991259e-06
Iter: 766 loss: 1.01990918e-06
Iter: 767 loss: 1.01904834e-06
Iter: 768 loss: 1.01776095e-06
Iter: 769 loss: 1.01774685e-06
Iter: 770 loss: 1.01627677e-06
Iter: 771 loss: 1.01847547e-06
Iter: 772 loss: 1.01549426e-06
Iter: 773 loss: 1.01448927e-06
Iter: 774 loss: 1.02749573e-06
Iter: 775 loss: 1.01448722e-06
Iter: 776 loss: 1.01326418e-06
Iter: 777 loss: 1.01559704e-06
Iter: 778 loss: 1.01279306e-06
Iter: 779 loss: 1.01200965e-06
Iter: 780 loss: 1.01533738e-06
Iter: 781 loss: 1.01183036e-06
Iter: 782 loss: 1.01099272e-06
Iter: 783 loss: 1.01149953e-06
Iter: 784 loss: 1.01042747e-06
Iter: 785 loss: 1.00955015e-06
Iter: 786 loss: 1.01108787e-06
Iter: 787 loss: 1.00913712e-06
Iter: 788 loss: 1.00795e-06
Iter: 789 loss: 1.01012802e-06
Iter: 790 loss: 1.00739942e-06
Iter: 791 loss: 1.00643979e-06
Iter: 792 loss: 1.00644752e-06
Iter: 793 loss: 1.00568491e-06
Iter: 794 loss: 1.00459533e-06
Iter: 795 loss: 1.01235582e-06
Iter: 796 loss: 1.00450529e-06
Iter: 797 loss: 1.00344539e-06
Iter: 798 loss: 1.00325235e-06
Iter: 799 loss: 1.00255045e-06
Iter: 800 loss: 1.00108991e-06
Iter: 801 loss: 1.00783654e-06
Iter: 802 loss: 1.00081343e-06
Iter: 803 loss: 9.99531721e-07
Iter: 804 loss: 1.00115199e-06
Iter: 805 loss: 9.9882925e-07
Iter: 806 loss: 9.97237066e-07
Iter: 807 loss: 1.01838668e-06
Iter: 808 loss: 9.97215238e-07
Iter: 809 loss: 9.96636231e-07
Iter: 810 loss: 9.95009486e-07
Iter: 811 loss: 1.00549983e-06
Iter: 812 loss: 9.94621246e-07
Iter: 813 loss: 9.9365252e-07
Iter: 814 loss: 9.93497906e-07
Iter: 815 loss: 9.92644686e-07
Iter: 816 loss: 1.00071907e-06
Iter: 817 loss: 9.92593641e-07
Iter: 818 loss: 9.91999741e-07
Iter: 819 loss: 9.9130034e-07
Iter: 820 loss: 9.91213938e-07
Iter: 821 loss: 9.90301714e-07
Iter: 822 loss: 1.0012177e-06
Iter: 823 loss: 9.90304e-07
Iter: 824 loss: 9.89788077e-07
Iter: 825 loss: 9.89278078e-07
Iter: 826 loss: 9.89208e-07
Iter: 827 loss: 9.88131433e-07
Iter: 828 loss: 9.92366267e-07
Iter: 829 loss: 9.87891667e-07
Iter: 830 loss: 9.87079602e-07
Iter: 831 loss: 9.85877e-07
Iter: 832 loss: 9.85857696e-07
Iter: 833 loss: 9.8409555e-07
Iter: 834 loss: 9.90604576e-07
Iter: 835 loss: 9.83695827e-07
Iter: 836 loss: 9.82361144e-07
Iter: 837 loss: 9.91610932e-07
Iter: 838 loss: 9.82276333e-07
Iter: 839 loss: 9.81193125e-07
Iter: 840 loss: 9.82875576e-07
Iter: 841 loss: 9.80701543e-07
Iter: 842 loss: 9.79490324e-07
Iter: 843 loss: 9.80321374e-07
Iter: 844 loss: 9.78716457e-07
Iter: 845 loss: 9.77871e-07
Iter: 846 loss: 9.7780719e-07
Iter: 847 loss: 9.76970455e-07
Iter: 848 loss: 9.77143372e-07
Iter: 849 loss: 9.76371552e-07
Iter: 850 loss: 9.75483772e-07
Iter: 851 loss: 9.78231e-07
Iter: 852 loss: 9.75251e-07
Iter: 853 loss: 9.74206614e-07
Iter: 854 loss: 9.80813411e-07
Iter: 855 loss: 9.74103386e-07
Iter: 856 loss: 9.73460942e-07
Iter: 857 loss: 9.72262796e-07
Iter: 858 loss: 9.95150572e-07
Iter: 859 loss: 9.72272e-07
Iter: 860 loss: 9.71491431e-07
Iter: 861 loss: 9.71458348e-07
Iter: 862 loss: 9.70624e-07
Iter: 863 loss: 9.69540338e-07
Iter: 864 loss: 9.69440293e-07
Iter: 865 loss: 9.68268751e-07
Iter: 866 loss: 9.68427457e-07
Iter: 867 loss: 9.67346e-07
Iter: 868 loss: 9.66144626e-07
Iter: 869 loss: 9.82878873e-07
Iter: 870 loss: 9.6613735e-07
Iter: 871 loss: 9.65085292e-07
Iter: 872 loss: 9.68756126e-07
Iter: 873 loss: 9.64836318e-07
Iter: 874 loss: 9.63965249e-07
Iter: 875 loss: 9.63118737e-07
Iter: 876 loss: 9.6296526e-07
Iter: 877 loss: 9.61859541e-07
Iter: 878 loss: 9.74429e-07
Iter: 879 loss: 9.6182157e-07
Iter: 880 loss: 9.60816465e-07
Iter: 881 loss: 9.60308398e-07
Iter: 882 loss: 9.59833756e-07
Iter: 883 loss: 9.59009867e-07
Iter: 884 loss: 9.58927558e-07
Iter: 885 loss: 9.58314104e-07
Iter: 886 loss: 9.58790906e-07
Iter: 887 loss: 9.57911539e-07
Iter: 888 loss: 9.57217253e-07
Iter: 889 loss: 9.62335e-07
Iter: 890 loss: 9.57166321e-07
Iter: 891 loss: 9.56461e-07
Iter: 892 loss: 9.55383825e-07
Iter: 893 loss: 9.55354608e-07
Iter: 894 loss: 9.54449092e-07
Iter: 895 loss: 9.54511506e-07
Iter: 896 loss: 9.53740937e-07
Iter: 897 loss: 9.53361337e-07
Iter: 898 loss: 9.53030622e-07
Iter: 899 loss: 9.52608616e-07
Iter: 900 loss: 9.51515631e-07
Iter: 901 loss: 9.63117145e-07
Iter: 902 loss: 9.51441393e-07
Iter: 903 loss: 9.50131607e-07
Iter: 904 loss: 9.56762733e-07
Iter: 905 loss: 9.49921287e-07
Iter: 906 loss: 9.49307548e-07
Iter: 907 loss: 9.49243315e-07
Iter: 908 loss: 9.48730644e-07
Iter: 909 loss: 9.4761225e-07
Iter: 910 loss: 9.65566414e-07
Iter: 911 loss: 9.4760162e-07
Iter: 912 loss: 9.46311445e-07
Iter: 913 loss: 9.49723244e-07
Iter: 914 loss: 9.45859654e-07
Iter: 915 loss: 9.44751207e-07
Iter: 916 loss: 9.58739065e-07
Iter: 917 loss: 9.44754902e-07
Iter: 918 loss: 9.43911346e-07
Iter: 919 loss: 9.46070486e-07
Iter: 920 loss: 9.43632472e-07
Iter: 921 loss: 9.42717634e-07
Iter: 922 loss: 9.46667626e-07
Iter: 923 loss: 9.42538179e-07
Iter: 924 loss: 9.41689507e-07
Iter: 925 loss: 9.45597435e-07
Iter: 926 loss: 9.41560302e-07
Iter: 927 loss: 9.40930306e-07
Iter: 928 loss: 9.40485904e-07
Iter: 929 loss: 9.40257735e-07
Iter: 930 loss: 9.39322092e-07
Iter: 931 loss: 9.39195616e-07
Iter: 932 loss: 9.38506957e-07
Iter: 933 loss: 9.37887876e-07
Iter: 934 loss: 9.37742129e-07
Iter: 935 loss: 9.372053e-07
Iter: 936 loss: 9.36095432e-07
Iter: 937 loss: 9.55427709e-07
Iter: 938 loss: 9.36044273e-07
Iter: 939 loss: 9.34849879e-07
Iter: 940 loss: 9.36111178e-07
Iter: 941 loss: 9.34173841e-07
Iter: 942 loss: 9.33087904e-07
Iter: 943 loss: 9.49562093e-07
Iter: 944 loss: 9.33089893e-07
Iter: 945 loss: 9.32180967e-07
Iter: 946 loss: 9.3385853e-07
Iter: 947 loss: 9.31776469e-07
Iter: 948 loss: 9.30831618e-07
Iter: 949 loss: 9.29940938e-07
Iter: 950 loss: 9.29701912e-07
Iter: 951 loss: 9.28560439e-07
Iter: 952 loss: 9.42535905e-07
Iter: 953 loss: 9.28551628e-07
Iter: 954 loss: 9.27728e-07
Iter: 955 loss: 9.36388574e-07
Iter: 956 loss: 9.27719668e-07
Iter: 957 loss: 9.27197561e-07
Iter: 958 loss: 9.28310953e-07
Iter: 959 loss: 9.26974337e-07
Iter: 960 loss: 9.26239863e-07
Iter: 961 loss: 9.27123153e-07
Iter: 962 loss: 9.25911422e-07
Iter: 963 loss: 9.25199686e-07
Iter: 964 loss: 9.24942128e-07
Iter: 965 loss: 9.24570031e-07
Iter: 966 loss: 9.23593859e-07
Iter: 967 loss: 9.28286113e-07
Iter: 968 loss: 9.23433163e-07
Iter: 969 loss: 9.2274297e-07
Iter: 970 loss: 9.27284304e-07
Iter: 971 loss: 9.22647814e-07
Iter: 972 loss: 9.21838591e-07
Iter: 973 loss: 9.21224569e-07
Iter: 974 loss: 9.20949844e-07
Iter: 975 loss: 9.20147841e-07
Iter: 976 loss: 9.19459922e-07
Iter: 977 loss: 9.19265574e-07
Iter: 978 loss: 9.1825018e-07
Iter: 979 loss: 9.34368245e-07
Iter: 980 loss: 9.18261662e-07
Iter: 981 loss: 9.17444481e-07
Iter: 982 loss: 9.20943876e-07
Iter: 983 loss: 9.17267e-07
Iter: 984 loss: 9.16558406e-07
Iter: 985 loss: 9.1641806e-07
Iter: 986 loss: 9.15935061e-07
Iter: 987 loss: 9.14881241e-07
Iter: 988 loss: 9.15378962e-07
Iter: 989 loss: 9.142218e-07
Iter: 990 loss: 9.13592658e-07
Iter: 991 loss: 9.13401777e-07
Iter: 992 loss: 9.12833343e-07
Iter: 993 loss: 9.1282692e-07
Iter: 994 loss: 9.12388316e-07
Iter: 995 loss: 9.11422262e-07
Iter: 996 loss: 9.14034388e-07
Iter: 997 loss: 9.1111724e-07
Iter: 998 loss: 9.10514814e-07
Iter: 999 loss: 9.10320182e-07
Iter: 1000 loss: 9.09953883e-07
Iter: 1001 loss: 9.0916825e-07
Iter: 1002 loss: 9.13386316e-07
Iter: 1003 loss: 9.09091113e-07
Iter: 1004 loss: 9.08348511e-07
Iter: 1005 loss: 9.1135189e-07
Iter: 1006 loss: 9.08204242e-07
Iter: 1007 loss: 9.07498929e-07
Iter: 1008 loss: 9.08087713e-07
Iter: 1009 loss: 9.07003084e-07
Iter: 1010 loss: 9.06315108e-07
Iter: 1011 loss: 9.05798743e-07
Iter: 1012 loss: 9.05560569e-07
Iter: 1013 loss: 9.04408694e-07
Iter: 1014 loss: 9.06148216e-07
Iter: 1015 loss: 9.03888917e-07
Iter: 1016 loss: 9.02895806e-07
Iter: 1017 loss: 9.02883812e-07
Iter: 1018 loss: 9.02200838e-07
Iter: 1019 loss: 9.01808448e-07
Iter: 1020 loss: 9.01532303e-07
Iter: 1021 loss: 9.00658733e-07
Iter: 1022 loss: 9.02583054e-07
Iter: 1023 loss: 9.00325517e-07
Iter: 1024 loss: 8.99738268e-07
Iter: 1025 loss: 8.99718202e-07
Iter: 1026 loss: 8.9924157e-07
Iter: 1027 loss: 8.98957751e-07
Iter: 1028 loss: 8.98747146e-07
Iter: 1029 loss: 8.98079577e-07
Iter: 1030 loss: 9.0344065e-07
Iter: 1031 loss: 8.9803126e-07
Iter: 1032 loss: 8.97642735e-07
Iter: 1033 loss: 8.96760355e-07
Iter: 1034 loss: 9.07443564e-07
Iter: 1035 loss: 8.96665483e-07
Iter: 1036 loss: 8.95639801e-07
Iter: 1037 loss: 9.07472e-07
Iter: 1038 loss: 8.95624225e-07
Iter: 1039 loss: 8.9490527e-07
Iter: 1040 loss: 9.00033683e-07
Iter: 1041 loss: 8.94878497e-07
Iter: 1042 loss: 8.9432524e-07
Iter: 1043 loss: 8.93749302e-07
Iter: 1044 loss: 8.93676088e-07
Iter: 1045 loss: 8.92622438e-07
Iter: 1046 loss: 8.95247751e-07
Iter: 1047 loss: 8.92226467e-07
Iter: 1048 loss: 8.91415709e-07
Iter: 1049 loss: 8.91598802e-07
Iter: 1050 loss: 8.90760418e-07
Iter: 1051 loss: 8.90080514e-07
Iter: 1052 loss: 8.90078468e-07
Iter: 1053 loss: 8.89341777e-07
Iter: 1054 loss: 8.89227294e-07
Iter: 1055 loss: 8.88727868e-07
Iter: 1056 loss: 8.87957412e-07
Iter: 1057 loss: 8.88956606e-07
Iter: 1058 loss: 8.87570138e-07
Iter: 1059 loss: 8.87000397e-07
Iter: 1060 loss: 8.86955945e-07
Iter: 1061 loss: 8.86445775e-07
Iter: 1062 loss: 8.8569459e-07
Iter: 1063 loss: 8.85704708e-07
Iter: 1064 loss: 8.84817894e-07
Iter: 1065 loss: 8.93982531e-07
Iter: 1066 loss: 8.84775091e-07
Iter: 1067 loss: 8.84323072e-07
Iter: 1068 loss: 8.83754e-07
Iter: 1069 loss: 8.83689097e-07
Iter: 1070 loss: 8.83023688e-07
Iter: 1071 loss: 8.87248e-07
Iter: 1072 loss: 8.82923871e-07
Iter: 1073 loss: 8.82118741e-07
Iter: 1074 loss: 8.84416067e-07
Iter: 1075 loss: 8.81878e-07
Iter: 1076 loss: 8.81193046e-07
Iter: 1077 loss: 8.81528422e-07
Iter: 1078 loss: 8.80750633e-07
Iter: 1079 loss: 8.79929189e-07
Iter: 1080 loss: 8.81561789e-07
Iter: 1081 loss: 8.79622689e-07
Iter: 1082 loss: 8.78634637e-07
Iter: 1083 loss: 8.79137e-07
Iter: 1084 loss: 8.77936429e-07
Iter: 1085 loss: 8.77146817e-07
Iter: 1086 loss: 8.8921297e-07
Iter: 1087 loss: 8.77141758e-07
Iter: 1088 loss: 8.76402623e-07
Iter: 1089 loss: 8.77017783e-07
Iter: 1090 loss: 8.75968681e-07
Iter: 1091 loss: 8.75282524e-07
Iter: 1092 loss: 8.75755291e-07
Iter: 1093 loss: 8.74833518e-07
Iter: 1094 loss: 8.73979729e-07
Iter: 1095 loss: 8.73986664e-07
Iter: 1096 loss: 8.73642875e-07
Iter: 1097 loss: 8.73479166e-07
Iter: 1098 loss: 8.7333e-07
Iter: 1099 loss: 8.72614294e-07
Iter: 1100 loss: 8.73333079e-07
Iter: 1101 loss: 8.72204396e-07
Iter: 1102 loss: 8.71456052e-07
Iter: 1103 loss: 8.7146168e-07
Iter: 1104 loss: 8.70834697e-07
Iter: 1105 loss: 8.70141832e-07
Iter: 1106 loss: 8.79385652e-07
Iter: 1107 loss: 8.70154e-07
Iter: 1108 loss: 8.69420774e-07
Iter: 1109 loss: 8.69096255e-07
Iter: 1110 loss: 8.68671236e-07
Iter: 1111 loss: 8.67799201e-07
Iter: 1112 loss: 8.69245582e-07
Iter: 1113 loss: 8.67387143e-07
Iter: 1114 loss: 8.66445e-07
Iter: 1115 loss: 8.70214421e-07
Iter: 1116 loss: 8.66182347e-07
Iter: 1117 loss: 8.65468962e-07
Iter: 1118 loss: 8.6752334e-07
Iter: 1119 loss: 8.65239258e-07
Iter: 1120 loss: 8.64488356e-07
Iter: 1121 loss: 8.67031304e-07
Iter: 1122 loss: 8.64338404e-07
Iter: 1123 loss: 8.63360469e-07
Iter: 1124 loss: 8.64009621e-07
Iter: 1125 loss: 8.62731326e-07
Iter: 1126 loss: 8.62082629e-07
Iter: 1127 loss: 8.69828398e-07
Iter: 1128 loss: 8.62090246e-07
Iter: 1129 loss: 8.61395392e-07
Iter: 1130 loss: 8.62771174e-07
Iter: 1131 loss: 8.61143e-07
Iter: 1132 loss: 8.60626074e-07
Iter: 1133 loss: 8.60710543e-07
Iter: 1134 loss: 8.60251305e-07
Iter: 1135 loss: 8.59375518e-07
Iter: 1136 loss: 8.61559158e-07
Iter: 1137 loss: 8.59050317e-07
Iter: 1138 loss: 8.58352166e-07
Iter: 1139 loss: 8.58177827e-07
Iter: 1140 loss: 8.57779e-07
Iter: 1141 loss: 8.57145949e-07
Iter: 1142 loss: 8.57147711e-07
Iter: 1143 loss: 8.56546421e-07
Iter: 1144 loss: 8.56230372e-07
Iter: 1145 loss: 8.55972928e-07
Iter: 1146 loss: 8.55327414e-07
Iter: 1147 loss: 8.55228336e-07
Iter: 1148 loss: 8.54731184e-07
Iter: 1149 loss: 8.53768142e-07
Iter: 1150 loss: 8.60552518e-07
Iter: 1151 loss: 8.53654e-07
Iter: 1152 loss: 8.52911967e-07
Iter: 1153 loss: 8.54031157e-07
Iter: 1154 loss: 8.52564938e-07
Iter: 1155 loss: 8.5172735e-07
Iter: 1156 loss: 8.57616556e-07
Iter: 1157 loss: 8.51678692e-07
Iter: 1158 loss: 8.50979859e-07
Iter: 1159 loss: 8.52182666e-07
Iter: 1160 loss: 8.50684e-07
Iter: 1161 loss: 8.50240212e-07
Iter: 1162 loss: 8.50231515e-07
Iter: 1163 loss: 8.49848107e-07
Iter: 1164 loss: 8.4934652e-07
Iter: 1165 loss: 8.492907e-07
Iter: 1166 loss: 8.4878036e-07
Iter: 1167 loss: 8.51279651e-07
Iter: 1168 loss: 8.48708851e-07
Iter: 1169 loss: 8.48110801e-07
Iter: 1170 loss: 8.47885588e-07
Iter: 1171 loss: 8.47531112e-07
Iter: 1172 loss: 8.46820626e-07
Iter: 1173 loss: 8.47146453e-07
Iter: 1174 loss: 8.46327e-07
Iter: 1175 loss: 8.45528575e-07
Iter: 1176 loss: 8.45554041e-07
Iter: 1177 loss: 8.45000727e-07
Iter: 1178 loss: 8.44587134e-07
Iter: 1179 loss: 8.44382498e-07
Iter: 1180 loss: 8.43675934e-07
Iter: 1181 loss: 8.43428438e-07
Iter: 1182 loss: 8.43042415e-07
Iter: 1183 loss: 8.42111263e-07
Iter: 1184 loss: 8.5322921e-07
Iter: 1185 loss: 8.42095e-07
Iter: 1186 loss: 8.41532142e-07
Iter: 1187 loss: 8.43093687e-07
Iter: 1188 loss: 8.41336771e-07
Iter: 1189 loss: 8.40761061e-07
Iter: 1190 loss: 8.4330145e-07
Iter: 1191 loss: 8.40614689e-07
Iter: 1192 loss: 8.40058419e-07
Iter: 1193 loss: 8.41506449e-07
Iter: 1194 loss: 8.3987635e-07
Iter: 1195 loss: 8.39352538e-07
Iter: 1196 loss: 8.44917963e-07
Iter: 1197 loss: 8.39358563e-07
Iter: 1198 loss: 8.38983055e-07
Iter: 1199 loss: 8.38230278e-07
Iter: 1200 loss: 8.51409709e-07
Iter: 1201 loss: 8.38189408e-07
Iter: 1202 loss: 8.37574873e-07
Iter: 1203 loss: 8.46765829e-07
Iter: 1204 loss: 8.37559071e-07
Iter: 1205 loss: 8.36931633e-07
Iter: 1206 loss: 8.36317952e-07
Iter: 1207 loss: 8.36174934e-07
Iter: 1208 loss: 8.35417836e-07
Iter: 1209 loss: 8.36751838e-07
Iter: 1210 loss: 8.35111507e-07
Iter: 1211 loss: 8.34678872e-07
Iter: 1212 loss: 8.3461606e-07
Iter: 1213 loss: 8.34281821e-07
Iter: 1214 loss: 8.33644833e-07
Iter: 1215 loss: 8.45849513e-07
Iter: 1216 loss: 8.33633862e-07
Iter: 1217 loss: 8.32805e-07
Iter: 1218 loss: 8.3404916e-07
Iter: 1219 loss: 8.32463797e-07
Iter: 1220 loss: 8.31773e-07
Iter: 1221 loss: 8.36332674e-07
Iter: 1222 loss: 8.3165105e-07
Iter: 1223 loss: 8.30894e-07
Iter: 1224 loss: 8.32687817e-07
Iter: 1225 loss: 8.30614624e-07
Iter: 1226 loss: 8.300118e-07
Iter: 1227 loss: 8.34438936e-07
Iter: 1228 loss: 8.29937562e-07
Iter: 1229 loss: 8.29408e-07
Iter: 1230 loss: 8.31314821e-07
Iter: 1231 loss: 8.29281817e-07
Iter: 1232 loss: 8.2866876e-07
Iter: 1233 loss: 8.31010766e-07
Iter: 1234 loss: 8.28561042e-07
Iter: 1235 loss: 8.28192526e-07
Iter: 1236 loss: 8.27738177e-07
Iter: 1237 loss: 8.27718111e-07
Iter: 1238 loss: 8.27141946e-07
Iter: 1239 loss: 8.34125728e-07
Iter: 1240 loss: 8.27158772e-07
Iter: 1241 loss: 8.26741e-07
Iter: 1242 loss: 8.26017413e-07
Iter: 1243 loss: 8.26033e-07
Iter: 1244 loss: 8.25148788e-07
Iter: 1245 loss: 8.25871325e-07
Iter: 1246 loss: 8.24653796e-07
Iter: 1247 loss: 8.24488154e-07
Iter: 1248 loss: 8.24194558e-07
Iter: 1249 loss: 8.23824109e-07
Iter: 1250 loss: 8.23027e-07
Iter: 1251 loss: 8.37184416e-07
Iter: 1252 loss: 8.23012613e-07
Iter: 1253 loss: 8.2227325e-07
Iter: 1254 loss: 8.24060635e-07
Iter: 1255 loss: 8.21958e-07
Iter: 1256 loss: 8.21241e-07
Iter: 1257 loss: 8.23245045e-07
Iter: 1258 loss: 8.20981086e-07
Iter: 1259 loss: 8.20372179e-07
Iter: 1260 loss: 8.27925589e-07
Iter: 1261 loss: 8.20342052e-07
Iter: 1262 loss: 8.19864852e-07
Iter: 1263 loss: 8.21054527e-07
Iter: 1264 loss: 8.1971541e-07
Iter: 1265 loss: 8.19186766e-07
Iter: 1266 loss: 8.22386937e-07
Iter: 1267 loss: 8.19128218e-07
Iter: 1268 loss: 8.18666479e-07
Iter: 1269 loss: 8.19002253e-07
Iter: 1270 loss: 8.18426713e-07
Iter: 1271 loss: 8.1789284e-07
Iter: 1272 loss: 8.17621697e-07
Iter: 1273 loss: 8.17387672e-07
Iter: 1274 loss: 8.16786269e-07
Iter: 1275 loss: 8.25253835e-07
Iter: 1276 loss: 8.1677581e-07
Iter: 1277 loss: 8.1630003e-07
Iter: 1278 loss: 8.16307761e-07
Iter: 1279 loss: 8.15964825e-07
Iter: 1280 loss: 8.15399289e-07
Iter: 1281 loss: 8.15008207e-07
Iter: 1282 loss: 8.1480448e-07
Iter: 1283 loss: 8.14150951e-07
Iter: 1284 loss: 8.23571895e-07
Iter: 1285 loss: 8.14134239e-07
Iter: 1286 loss: 8.13473378e-07
Iter: 1287 loss: 8.15034639e-07
Iter: 1288 loss: 8.13218e-07
Iter: 1289 loss: 8.12836106e-07
Iter: 1290 loss: 8.11856921e-07
Iter: 1291 loss: 8.23599748e-07
Iter: 1292 loss: 8.11781433e-07
Iter: 1293 loss: 8.10908773e-07
Iter: 1294 loss: 8.22378297e-07
Iter: 1295 loss: 8.10910365e-07
Iter: 1296 loss: 8.10244273e-07
Iter: 1297 loss: 8.13982695e-07
Iter: 1298 loss: 8.10194194e-07
Iter: 1299 loss: 8.09569599e-07
Iter: 1300 loss: 8.129457e-07
Iter: 1301 loss: 8.09476717e-07
Iter: 1302 loss: 8.08999403e-07
Iter: 1303 loss: 8.11894665e-07
Iter: 1304 loss: 8.0894165e-07
Iter: 1305 loss: 8.08546417e-07
Iter: 1306 loss: 8.08074731e-07
Iter: 1307 loss: 8.08027949e-07
Iter: 1308 loss: 8.07473668e-07
Iter: 1309 loss: 8.10034919e-07
Iter: 1310 loss: 8.07324852e-07
Iter: 1311 loss: 8.06802404e-07
Iter: 1312 loss: 8.08302843e-07
Iter: 1313 loss: 8.06611638e-07
Iter: 1314 loss: 8.06014782e-07
Iter: 1315 loss: 8.08222921e-07
Iter: 1316 loss: 8.05890238e-07
Iter: 1317 loss: 8.05502339e-07
Iter: 1318 loss: 8.05103696e-07
Iter: 1319 loss: 8.05015361e-07
Iter: 1320 loss: 8.04328863e-07
Iter: 1321 loss: 8.08425568e-07
Iter: 1322 loss: 8.04287936e-07
Iter: 1323 loss: 8.03566081e-07
Iter: 1324 loss: 8.07751917e-07
Iter: 1325 loss: 8.0348542e-07
Iter: 1326 loss: 8.03085413e-07
Iter: 1327 loss: 8.02488159e-07
Iter: 1328 loss: 8.02493e-07
Iter: 1329 loss: 8.01682518e-07
Iter: 1330 loss: 8.02744751e-07
Iter: 1331 loss: 8.01281431e-07
Iter: 1332 loss: 8.00413e-07
Iter: 1333 loss: 8.08024538e-07
Iter: 1334 loss: 8.00360624e-07
Iter: 1335 loss: 7.9996596e-07
Iter: 1336 loss: 7.99929353e-07
Iter: 1337 loss: 7.99630868e-07
Iter: 1338 loss: 7.99669124e-07
Iter: 1339 loss: 7.99430154e-07
Iter: 1340 loss: 7.98941e-07
Iter: 1341 loss: 7.98851033e-07
Iter: 1342 loss: 7.98528845e-07
Iter: 1343 loss: 7.98021745e-07
Iter: 1344 loss: 7.98888891e-07
Iter: 1345 loss: 7.97816597e-07
Iter: 1346 loss: 7.97238386e-07
Iter: 1347 loss: 8.01474471e-07
Iter: 1348 loss: 7.97178586e-07
Iter: 1349 loss: 7.96689619e-07
Iter: 1350 loss: 7.96887207e-07
Iter: 1351 loss: 7.96353675e-07
Iter: 1352 loss: 7.9568872e-07
Iter: 1353 loss: 7.96943141e-07
Iter: 1354 loss: 7.95409392e-07
Iter: 1355 loss: 7.94913035e-07
Iter: 1356 loss: 7.96955646e-07
Iter: 1357 loss: 7.94763366e-07
Iter: 1358 loss: 7.94270136e-07
Iter: 1359 loss: 7.98378494e-07
Iter: 1360 loss: 7.94211132e-07
Iter: 1361 loss: 7.93784693e-07
Iter: 1362 loss: 7.93035554e-07
Iter: 1363 loss: 7.93034303e-07
Iter: 1364 loss: 7.92323135e-07
Iter: 1365 loss: 7.92658454e-07
Iter: 1366 loss: 7.91888056e-07
Iter: 1367 loss: 7.91171374e-07
Iter: 1368 loss: 7.98819315e-07
Iter: 1369 loss: 7.91126581e-07
Iter: 1370 loss: 7.90508921e-07
Iter: 1371 loss: 7.98011797e-07
Iter: 1372 loss: 7.90496699e-07
Iter: 1373 loss: 7.90052411e-07
Iter: 1374 loss: 7.90002844e-07
Iter: 1375 loss: 7.897562e-07
Iter: 1376 loss: 7.89255751e-07
Iter: 1377 loss: 7.91214916e-07
Iter: 1378 loss: 7.89147066e-07
Iter: 1379 loss: 7.88706416e-07
Iter: 1380 loss: 7.88203e-07
Iter: 1381 loss: 7.88143382e-07
Iter: 1382 loss: 7.87737747e-07
Iter: 1383 loss: 7.87706881e-07
Iter: 1384 loss: 7.87326485e-07
Iter: 1385 loss: 7.86884357e-07
Iter: 1386 loss: 7.86836381e-07
Iter: 1387 loss: 7.86284659e-07
Iter: 1388 loss: 7.89109322e-07
Iter: 1389 loss: 7.86183705e-07
Iter: 1390 loss: 7.85648808e-07
Iter: 1391 loss: 7.86318253e-07
Iter: 1392 loss: 7.85339694e-07
Iter: 1393 loss: 7.8483373e-07
Iter: 1394 loss: 7.8484004e-07
Iter: 1395 loss: 7.84510689e-07
Iter: 1396 loss: 7.83853409e-07
Iter: 1397 loss: 7.93594722e-07
Iter: 1398 loss: 7.83784e-07
Iter: 1399 loss: 7.82967959e-07
Iter: 1400 loss: 7.85735097e-07
Iter: 1401 loss: 7.82706593e-07
Iter: 1402 loss: 7.82174652e-07
Iter: 1403 loss: 7.87379577e-07
Iter: 1404 loss: 7.82144298e-07
Iter: 1405 loss: 7.81599738e-07
Iter: 1406 loss: 7.84333793e-07
Iter: 1407 loss: 7.81487188e-07
Iter: 1408 loss: 7.81138e-07
Iter: 1409 loss: 7.8113294e-07
Iter: 1410 loss: 7.80880498e-07
Iter: 1411 loss: 7.80298308e-07
Iter: 1412 loss: 7.81817846e-07
Iter: 1413 loss: 7.80122832e-07
Iter: 1414 loss: 7.79649838e-07
Iter: 1415 loss: 7.80188884e-07
Iter: 1416 loss: 7.79450431e-07
Iter: 1417 loss: 7.79034337e-07
Iter: 1418 loss: 7.83385701e-07
Iter: 1419 loss: 7.79033428e-07
Iter: 1420 loss: 7.78657238e-07
Iter: 1421 loss: 7.78139679e-07
Iter: 1422 loss: 7.78106e-07
Iter: 1423 loss: 7.77532762e-07
Iter: 1424 loss: 7.79956736e-07
Iter: 1425 loss: 7.77350863e-07
Iter: 1426 loss: 7.76803063e-07
Iter: 1427 loss: 7.79637617e-07
Iter: 1428 loss: 7.76734851e-07
Iter: 1429 loss: 7.76212232e-07
Iter: 1430 loss: 7.78138144e-07
Iter: 1431 loss: 7.76071829e-07
Iter: 1432 loss: 7.75605315e-07
Iter: 1433 loss: 7.75510216e-07
Iter: 1434 loss: 7.75179899e-07
Iter: 1435 loss: 7.74608452e-07
Iter: 1436 loss: 7.75536591e-07
Iter: 1437 loss: 7.74348166e-07
Iter: 1438 loss: 7.73979878e-07
Iter: 1439 loss: 7.73982606e-07
Iter: 1440 loss: 7.73565603e-07
Iter: 1441 loss: 7.72928729e-07
Iter: 1442 loss: 7.72922363e-07
Iter: 1443 loss: 7.72329486e-07
Iter: 1444 loss: 7.76375146e-07
Iter: 1445 loss: 7.72279691e-07
Iter: 1446 loss: 7.71787484e-07
Iter: 1447 loss: 7.72577948e-07
Iter: 1448 loss: 7.7154607e-07
Iter: 1449 loss: 7.71094051e-07
Iter: 1450 loss: 7.7144324e-07
Iter: 1451 loss: 7.70825466e-07
Iter: 1452 loss: 7.70278916e-07
Iter: 1453 loss: 7.76384184e-07
Iter: 1454 loss: 7.70277722e-07
Iter: 1455 loss: 7.69897554e-07
Iter: 1456 loss: 7.69380335e-07
Iter: 1457 loss: 7.69368683e-07
Iter: 1458 loss: 7.68717655e-07
Iter: 1459 loss: 7.72524686e-07
Iter: 1460 loss: 7.68649159e-07
Iter: 1461 loss: 7.68080213e-07
Iter: 1462 loss: 7.71535667e-07
Iter: 1463 loss: 7.68013308e-07
Iter: 1464 loss: 7.67641382e-07
Iter: 1465 loss: 7.68890118e-07
Iter: 1466 loss: 7.67514393e-07
Iter: 1467 loss: 7.67159236e-07
Iter: 1468 loss: 7.66747462e-07
Iter: 1469 loss: 7.66686469e-07
Iter: 1470 loss: 7.66125e-07
Iter: 1471 loss: 7.70920167e-07
Iter: 1472 loss: 7.66094104e-07
Iter: 1473 loss: 7.65669142e-07
Iter: 1474 loss: 7.70822908e-07
Iter: 1475 loss: 7.65648167e-07
Iter: 1476 loss: 7.6539186e-07
Iter: 1477 loss: 7.64788183e-07
Iter: 1478 loss: 7.72635303e-07
Iter: 1479 loss: 7.64743561e-07
Iter: 1480 loss: 7.642642e-07
Iter: 1481 loss: 7.64259653e-07
Iter: 1482 loss: 7.63877438e-07
Iter: 1483 loss: 7.63451851e-07
Iter: 1484 loss: 7.63423714e-07
Iter: 1485 loss: 7.62877335e-07
Iter: 1486 loss: 7.68192706e-07
Iter: 1487 loss: 7.62882223e-07
Iter: 1488 loss: 7.62399054e-07
Iter: 1489 loss: 7.63078788e-07
Iter: 1490 loss: 7.62156276e-07
Iter: 1491 loss: 7.61695162e-07
Iter: 1492 loss: 7.62022864e-07
Iter: 1493 loss: 7.61449201e-07
Iter: 1494 loss: 7.61108595e-07
Iter: 1495 loss: 7.61107458e-07
Iter: 1496 loss: 7.60792148e-07
Iter: 1497 loss: 7.6068261e-07
Iter: 1498 loss: 7.6050344e-07
Iter: 1499 loss: 7.59981276e-07
Iter: 1500 loss: 7.61384968e-07
Iter: 1501 loss: 7.59850082e-07
Iter: 1502 loss: 7.59451041e-07
Iter: 1503 loss: 7.59616967e-07
Iter: 1504 loss: 7.59178e-07
Iter: 1505 loss: 7.58902331e-07
Iter: 1506 loss: 7.58863166e-07
Iter: 1507 loss: 7.58593501e-07
Iter: 1508 loss: 7.58193096e-07
Iter: 1509 loss: 7.58177805e-07
Iter: 1510 loss: 7.5768952e-07
Iter: 1511 loss: 7.58053602e-07
Iter: 1512 loss: 7.57384214e-07
Iter: 1513 loss: 7.56782583e-07
Iter: 1514 loss: 7.62233e-07
Iter: 1515 loss: 7.5676445e-07
Iter: 1516 loss: 7.56440386e-07
Iter: 1517 loss: 7.56117117e-07
Iter: 1518 loss: 7.56053169e-07
Iter: 1519 loss: 7.55575854e-07
Iter: 1520 loss: 7.62003765e-07
Iter: 1521 loss: 7.5557881e-07
Iter: 1522 loss: 7.55165956e-07
Iter: 1523 loss: 7.54975531e-07
Iter: 1524 loss: 7.5479e-07
Iter: 1525 loss: 7.54254756e-07
Iter: 1526 loss: 7.54866164e-07
Iter: 1527 loss: 7.53991287e-07
Iter: 1528 loss: 7.53470658e-07
Iter: 1529 loss: 7.53485722e-07
Iter: 1530 loss: 7.53149152e-07
Iter: 1531 loss: 7.53121526e-07
Iter: 1532 loss: 7.52864707e-07
Iter: 1533 loss: 7.5238205e-07
Iter: 1534 loss: 7.53071674e-07
Iter: 1535 loss: 7.5215678e-07
Iter: 1536 loss: 7.51683046e-07
Iter: 1537 loss: 7.53740267e-07
Iter: 1538 loss: 7.51585617e-07
Iter: 1539 loss: 7.5104424e-07
Iter: 1540 loss: 7.54719053e-07
Iter: 1541 loss: 7.51033e-07
Iter: 1542 loss: 7.50706931e-07
Iter: 1543 loss: 7.5027674e-07
Iter: 1544 loss: 7.50246159e-07
Iter: 1545 loss: 7.49828928e-07
Iter: 1546 loss: 7.53963945e-07
Iter: 1547 loss: 7.49823812e-07
Iter: 1548 loss: 7.49415619e-07
Iter: 1549 loss: 7.49165e-07
Iter: 1550 loss: 7.48979517e-07
Iter: 1551 loss: 7.48532102e-07
Iter: 1552 loss: 7.51875746e-07
Iter: 1553 loss: 7.48478271e-07
Iter: 1554 loss: 7.48073376e-07
Iter: 1555 loss: 7.49471837e-07
Iter: 1556 loss: 7.47941385e-07
Iter: 1557 loss: 7.47569061e-07
Iter: 1558 loss: 7.47227773e-07
Iter: 1559 loss: 7.47130741e-07
Iter: 1560 loss: 7.46597379e-07
Iter: 1561 loss: 7.49131971e-07
Iter: 1562 loss: 7.46492674e-07
Iter: 1563 loss: 7.4586103e-07
Iter: 1564 loss: 7.48496063e-07
Iter: 1565 loss: 7.45670491e-07
Iter: 1566 loss: 7.45241e-07
Iter: 1567 loss: 7.45493e-07
Iter: 1568 loss: 7.44964836e-07
Iter: 1569 loss: 7.44405781e-07
Iter: 1570 loss: 7.45449086e-07
Iter: 1571 loss: 7.44163174e-07
Iter: 1572 loss: 7.43827457e-07
Iter: 1573 loss: 7.43820863e-07
Iter: 1574 loss: 7.43492e-07
Iter: 1575 loss: 7.434802e-07
Iter: 1576 loss: 7.43213548e-07
Iter: 1577 loss: 7.42862312e-07
Iter: 1578 loss: 7.42682971e-07
Iter: 1579 loss: 7.42492375e-07
Iter: 1580 loss: 7.42005852e-07
Iter: 1581 loss: 7.46525757e-07
Iter: 1582 loss: 7.41981921e-07
Iter: 1583 loss: 7.41583449e-07
Iter: 1584 loss: 7.41416898e-07
Iter: 1585 loss: 7.41184863e-07
Iter: 1586 loss: 7.40732844e-07
Iter: 1587 loss: 7.44779072e-07
Iter: 1588 loss: 7.4070482e-07
Iter: 1589 loss: 7.40256894e-07
Iter: 1590 loss: 7.40611256e-07
Iter: 1591 loss: 7.39986035e-07
Iter: 1592 loss: 7.39562e-07
Iter: 1593 loss: 7.3955664e-07
Iter: 1594 loss: 7.39175107e-07
Iter: 1595 loss: 7.38705126e-07
Iter: 1596 loss: 7.45475802e-07
Iter: 1597 loss: 7.38719052e-07
Iter: 1598 loss: 7.38281528e-07
Iter: 1599 loss: 7.3886639e-07
Iter: 1600 loss: 7.38043184e-07
Iter: 1601 loss: 7.37650794e-07
Iter: 1602 loss: 7.37185246e-07
Iter: 1603 loss: 7.37118171e-07
Iter: 1604 loss: 7.36451625e-07
Iter: 1605 loss: 7.43252826e-07
Iter: 1606 loss: 7.36421839e-07
Iter: 1607 loss: 7.3616593e-07
Iter: 1608 loss: 7.36152856e-07
Iter: 1609 loss: 7.35938443e-07
Iter: 1610 loss: 7.35524679e-07
Iter: 1611 loss: 7.425636e-07
Iter: 1612 loss: 7.35493472e-07
Iter: 1613 loss: 7.35090794e-07
Iter: 1614 loss: 7.36188554e-07
Iter: 1615 loss: 7.34928335e-07
Iter: 1616 loss: 7.34445166e-07
Iter: 1617 loss: 7.37335313e-07
Iter: 1618 loss: 7.34384685e-07
Iter: 1619 loss: 7.33927209e-07
Iter: 1620 loss: 7.33945512e-07
Iter: 1621 loss: 7.33555339e-07
Iter: 1622 loss: 7.33093259e-07
Iter: 1623 loss: 7.37092535e-07
Iter: 1624 loss: 7.33060574e-07
Iter: 1625 loss: 7.32537103e-07
Iter: 1626 loss: 7.32064905e-07
Iter: 1627 loss: 7.31915179e-07
Iter: 1628 loss: 7.31379146e-07
Iter: 1629 loss: 7.34055902e-07
Iter: 1630 loss: 7.31291777e-07
Iter: 1631 loss: 7.30841123e-07
Iter: 1632 loss: 7.35181061e-07
Iter: 1633 loss: 7.30829868e-07
Iter: 1634 loss: 7.30447709e-07
Iter: 1635 loss: 7.30341696e-07
Iter: 1636 loss: 7.301187e-07
Iter: 1637 loss: 7.29683734e-07
Iter: 1638 loss: 7.29974e-07
Iter: 1639 loss: 7.29440615e-07
Iter: 1640 loss: 7.29004e-07
Iter: 1641 loss: 7.35626486e-07
Iter: 1642 loss: 7.28992177e-07
Iter: 1643 loss: 7.28663792e-07
Iter: 1644 loss: 7.30203e-07
Iter: 1645 loss: 7.28581426e-07
Iter: 1646 loss: 7.28264638e-07
Iter: 1647 loss: 7.275803e-07
Iter: 1648 loss: 7.37832465e-07
Iter: 1649 loss: 7.27525162e-07
Iter: 1650 loss: 7.27044494e-07
Iter: 1651 loss: 7.33987349e-07
Iter: 1652 loss: 7.27050065e-07
Iter: 1653 loss: 7.26621238e-07
Iter: 1654 loss: 7.27711154e-07
Iter: 1655 loss: 7.26514372e-07
Iter: 1656 loss: 7.26031772e-07
Iter: 1657 loss: 7.26317808e-07
Iter: 1658 loss: 7.25756649e-07
Iter: 1659 loss: 7.2538603e-07
Iter: 1660 loss: 7.25389555e-07
Iter: 1661 loss: 7.25147856e-07
Iter: 1662 loss: 7.24652523e-07
Iter: 1663 loss: 7.34737e-07
Iter: 1664 loss: 7.24662641e-07
Iter: 1665 loss: 7.24181518e-07
Iter: 1666 loss: 7.28562839e-07
Iter: 1667 loss: 7.24151107e-07
Iter: 1668 loss: 7.23653443e-07
Iter: 1669 loss: 7.25323162e-07
Iter: 1670 loss: 7.23520884e-07
Iter: 1671 loss: 7.2312497e-07
Iter: 1672 loss: 7.2276066e-07
Iter: 1673 loss: 7.22662776e-07
Iter: 1674 loss: 7.22015216e-07
Iter: 1675 loss: 7.24380129e-07
Iter: 1676 loss: 7.21870208e-07
Iter: 1677 loss: 7.21509593e-07
Iter: 1678 loss: 7.21512265e-07
Iter: 1679 loss: 7.21222534e-07
Iter: 1680 loss: 7.21267497e-07
Iter: 1681 loss: 7.21024435e-07
Iter: 1682 loss: 7.20720664e-07
Iter: 1683 loss: 7.2032924e-07
Iter: 1684 loss: 7.20319917e-07
Iter: 1685 loss: 7.19672869e-07
Iter: 1686 loss: 7.23011226e-07
Iter: 1687 loss: 7.19593118e-07
Iter: 1688 loss: 7.19080788e-07
Iter: 1689 loss: 7.22568927e-07
Iter: 1690 loss: 7.18995238e-07
Iter: 1691 loss: 7.18643491e-07
Iter: 1692 loss: 7.18759395e-07
Iter: 1693 loss: 7.18410888e-07
Iter: 1694 loss: 7.17890202e-07
Iter: 1695 loss: 7.21242372e-07
Iter: 1696 loss: 7.17828812e-07
Iter: 1697 loss: 7.17470812e-07
Iter: 1698 loss: 7.16886234e-07
Iter: 1699 loss: 7.16881e-07
Iter: 1700 loss: 7.16448881e-07
Iter: 1701 loss: 7.16426143e-07
Iter: 1702 loss: 7.16041e-07
Iter: 1703 loss: 7.1634247e-07
Iter: 1704 loss: 7.15821557e-07
Iter: 1705 loss: 7.15396368e-07
Iter: 1706 loss: 7.1484385e-07
Iter: 1707 loss: 7.14813041e-07
Iter: 1708 loss: 7.14621137e-07
Iter: 1709 loss: 7.14493524e-07
Iter: 1710 loss: 7.14254725e-07
Iter: 1711 loss: 7.15319857e-07
Iter: 1712 loss: 7.14169e-07
Iter: 1713 loss: 7.13915369e-07
Iter: 1714 loss: 7.13627742e-07
Iter: 1715 loss: 7.13630357e-07
Iter: 1716 loss: 7.13229838e-07
Iter: 1717 loss: 7.13796851e-07
Iter: 1718 loss: 7.13051236e-07
Iter: 1719 loss: 7.12566816e-07
Iter: 1720 loss: 7.15004035e-07
Iter: 1721 loss: 7.12476208e-07
Iter: 1722 loss: 7.11986445e-07
Iter: 1723 loss: 7.12966653e-07
Iter: 1724 loss: 7.11786868e-07
Iter: 1725 loss: 7.11337407e-07
Iter: 1726 loss: 7.12319093e-07
Iter: 1727 loss: 7.1114647e-07
Iter: 1728 loss: 7.1060839e-07
Iter: 1729 loss: 7.13115583e-07
Iter: 1730 loss: 7.10514087e-07
Iter: 1731 loss: 7.10156087e-07
Iter: 1732 loss: 7.0972942e-07
Iter: 1733 loss: 7.09717938e-07
Iter: 1734 loss: 7.09316282e-07
Iter: 1735 loss: 7.09272911e-07
Iter: 1736 loss: 7.0896931e-07
Iter: 1737 loss: 7.08648258e-07
Iter: 1738 loss: 7.08585333e-07
Iter: 1739 loss: 7.08110861e-07
Iter: 1740 loss: 7.0816742e-07
Iter: 1741 loss: 7.0777e-07
Iter: 1742 loss: 7.07597e-07
Iter: 1743 loss: 7.07481036e-07
Iter: 1744 loss: 7.07218703e-07
Iter: 1745 loss: 7.07065681e-07
Iter: 1746 loss: 7.06951937e-07
Iter: 1747 loss: 7.06602805e-07
Iter: 1748 loss: 7.06865592e-07
Iter: 1749 loss: 7.06396804e-07
Iter: 1750 loss: 7.0597639e-07
Iter: 1751 loss: 7.06195237e-07
Iter: 1752 loss: 7.05710306e-07
Iter: 1753 loss: 7.05289438e-07
Iter: 1754 loss: 7.10741915e-07
Iter: 1755 loss: 7.05292e-07
Iter: 1756 loss: 7.04960144e-07
Iter: 1757 loss: 7.05224465e-07
Iter: 1758 loss: 7.04745787e-07
Iter: 1759 loss: 7.04356921e-07
Iter: 1760 loss: 7.05626462e-07
Iter: 1761 loss: 7.04265517e-07
Iter: 1762 loss: 7.03812361e-07
Iter: 1763 loss: 7.04323952e-07
Iter: 1764 loss: 7.03586579e-07
Iter: 1765 loss: 7.03061289e-07
Iter: 1766 loss: 7.03090677e-07
Iter: 1767 loss: 7.02637067e-07
Iter: 1768 loss: 7.02380817e-07
Iter: 1769 loss: 7.02328066e-07
Iter: 1770 loss: 7.02013324e-07
Iter: 1771 loss: 7.0138492e-07
Iter: 1772 loss: 7.10990548e-07
Iter: 1773 loss: 7.01364456e-07
Iter: 1774 loss: 7.00865144e-07
Iter: 1775 loss: 7.04724e-07
Iter: 1776 loss: 7.00803298e-07
Iter: 1777 loss: 7.00468036e-07
Iter: 1778 loss: 7.00459282e-07
Iter: 1779 loss: 7.00198711e-07
Iter: 1780 loss: 6.99751126e-07
Iter: 1781 loss: 6.99741918e-07
Iter: 1782 loss: 6.99326165e-07
Iter: 1783 loss: 7.00614e-07
Iter: 1784 loss: 6.99203667e-07
Iter: 1785 loss: 6.98766826e-07
Iter: 1786 loss: 6.99080886e-07
Iter: 1787 loss: 6.9846476e-07
Iter: 1788 loss: 6.97997621e-07
Iter: 1789 loss: 7.03356193e-07
Iter: 1790 loss: 6.9799637e-07
Iter: 1791 loss: 6.97606367e-07
Iter: 1792 loss: 6.97542418e-07
Iter: 1793 loss: 6.97282417e-07
Iter: 1794 loss: 6.96875361e-07
Iter: 1795 loss: 7.01573924e-07
Iter: 1796 loss: 6.96874054e-07
Iter: 1797 loss: 6.96547659e-07
Iter: 1798 loss: 6.96307211e-07
Iter: 1799 loss: 6.96216205e-07
Iter: 1800 loss: 6.95667381e-07
Iter: 1801 loss: 6.96889742e-07
Iter: 1802 loss: 6.95494577e-07
Iter: 1803 loss: 6.95181143e-07
Iter: 1804 loss: 6.95170343e-07
Iter: 1805 loss: 6.94939e-07
Iter: 1806 loss: 6.94448488e-07
Iter: 1807 loss: 7.00690691e-07
Iter: 1808 loss: 6.94424955e-07
Iter: 1809 loss: 6.94048e-07
Iter: 1810 loss: 6.94059622e-07
Iter: 1811 loss: 6.93685593e-07
Iter: 1812 loss: 6.948286e-07
Iter: 1813 loss: 6.93587594e-07
Iter: 1814 loss: 6.93258642e-07
Iter: 1815 loss: 6.92904564e-07
Iter: 1816 loss: 6.92864546e-07
Iter: 1817 loss: 6.9237592e-07
Iter: 1818 loss: 6.93655124e-07
Iter: 1819 loss: 6.92219942e-07
Iter: 1820 loss: 6.91595574e-07
Iter: 1821 loss: 6.92943331e-07
Iter: 1822 loss: 6.91412e-07
Iter: 1823 loss: 6.90948809e-07
Iter: 1824 loss: 6.97244218e-07
Iter: 1825 loss: 6.90967113e-07
Iter: 1826 loss: 6.90667434e-07
Iter: 1827 loss: 6.90687841e-07
Iter: 1828 loss: 6.90428806e-07
Iter: 1829 loss: 6.9004227e-07
Iter: 1830 loss: 6.91990181e-07
Iter: 1831 loss: 6.89968772e-07
Iter: 1832 loss: 6.89645503e-07
Iter: 1833 loss: 6.89646527e-07
Iter: 1834 loss: 6.89388798e-07
Iter: 1835 loss: 6.8900107e-07
Iter: 1836 loss: 6.91264631e-07
Iter: 1837 loss: 6.88964406e-07
Iter: 1838 loss: 6.88570537e-07
Iter: 1839 loss: 6.89668468e-07
Iter: 1840 loss: 6.88432692e-07
Iter: 1841 loss: 6.88084242e-07
Iter: 1842 loss: 6.87677414e-07
Iter: 1843 loss: 6.8764831e-07
Iter: 1844 loss: 6.87449869e-07
Iter: 1845 loss: 6.87346073e-07
Iter: 1846 loss: 6.87075215e-07
Iter: 1847 loss: 6.86784801e-07
Iter: 1848 loss: 6.86725571e-07
Iter: 1849 loss: 6.86389058e-07
Iter: 1850 loss: 6.86433509e-07
Iter: 1851 loss: 6.86096e-07
Iter: 1852 loss: 6.85667374e-07
Iter: 1853 loss: 6.88877776e-07
Iter: 1854 loss: 6.85658449e-07
Iter: 1855 loss: 6.85305963e-07
Iter: 1856 loss: 6.86434248e-07
Iter: 1857 loss: 6.85228201e-07
Iter: 1858 loss: 6.848922e-07
Iter: 1859 loss: 6.85737689e-07
Iter: 1860 loss: 6.84741678e-07
Iter: 1861 loss: 6.84414886e-07
Iter: 1862 loss: 6.84757254e-07
Iter: 1863 loss: 6.84199108e-07
Iter: 1864 loss: 6.83722419e-07
Iter: 1865 loss: 6.8581835e-07
Iter: 1866 loss: 6.83635562e-07
Iter: 1867 loss: 6.83225892e-07
Iter: 1868 loss: 6.83317353e-07
Iter: 1869 loss: 6.82966686e-07
Iter: 1870 loss: 6.8260988e-07
Iter: 1871 loss: 6.86850171e-07
Iter: 1872 loss: 6.82598568e-07
Iter: 1873 loss: 6.82219763e-07
Iter: 1874 loss: 6.82229881e-07
Iter: 1875 loss: 6.81950723e-07
Iter: 1876 loss: 6.81523716e-07
Iter: 1877 loss: 6.81522181e-07
Iter: 1878 loss: 6.81185e-07
Iter: 1879 loss: 6.81146162e-07
Iter: 1880 loss: 6.80944879e-07
Iter: 1881 loss: 6.80769745e-07
Iter: 1882 loss: 6.80305675e-07
Iter: 1883 loss: 6.85114401e-07
Iter: 1884 loss: 6.80241214e-07
Iter: 1885 loss: 6.79689833e-07
Iter: 1886 loss: 6.80386165e-07
Iter: 1887 loss: 6.79379355e-07
Iter: 1888 loss: 6.78872539e-07
Iter: 1889 loss: 6.82225959e-07
Iter: 1890 loss: 6.78814445e-07
Iter: 1891 loss: 6.78291542e-07
Iter: 1892 loss: 6.80285439e-07
Iter: 1893 loss: 6.78193601e-07
Iter: 1894 loss: 6.77780577e-07
Iter: 1895 loss: 6.79692107e-07
Iter: 1896 loss: 6.77699745e-07
Iter: 1897 loss: 6.77320941e-07
Iter: 1898 loss: 6.77361527e-07
Iter: 1899 loss: 6.77015919e-07
Iter: 1900 loss: 6.76613922e-07
Iter: 1901 loss: 6.81440042e-07
Iter: 1902 loss: 6.76628702e-07
Iter: 1903 loss: 6.7639138e-07
Iter: 1904 loss: 6.76146726e-07
Iter: 1905 loss: 6.76068282e-07
Iter: 1906 loss: 6.75648721e-07
Iter: 1907 loss: 6.79470475e-07
Iter: 1908 loss: 6.75623369e-07
Iter: 1909 loss: 6.75237743e-07
Iter: 1910 loss: 6.75879392e-07
Iter: 1911 loss: 6.75078923e-07
Iter: 1912 loss: 6.74786463e-07
Iter: 1913 loss: 6.74409876e-07
Iter: 1914 loss: 6.74389071e-07
Iter: 1915 loss: 6.73946204e-07
Iter: 1916 loss: 6.73924319e-07
Iter: 1917 loss: 6.73714851e-07
Iter: 1918 loss: 6.73217414e-07
Iter: 1919 loss: 6.80251333e-07
Iter: 1920 loss: 6.73190925e-07
Iter: 1921 loss: 6.72654778e-07
Iter: 1922 loss: 6.73745e-07
Iter: 1923 loss: 6.72442638e-07
Iter: 1924 loss: 6.71951966e-07
Iter: 1925 loss: 6.75551519e-07
Iter: 1926 loss: 6.71906605e-07
Iter: 1927 loss: 6.71449584e-07
Iter: 1928 loss: 6.7289352e-07
Iter: 1929 loss: 6.71280873e-07
Iter: 1930 loss: 6.70952318e-07
Iter: 1931 loss: 6.71749262e-07
Iter: 1932 loss: 6.70811119e-07
Iter: 1933 loss: 6.70407132e-07
Iter: 1934 loss: 6.70402869e-07
Iter: 1935 loss: 6.7009546e-07
Iter: 1936 loss: 6.69501446e-07
Iter: 1937 loss: 6.74952332e-07
Iter: 1938 loss: 6.69485303e-07
Iter: 1939 loss: 6.69193696e-07
Iter: 1940 loss: 6.69207679e-07
Iter: 1941 loss: 6.68951429e-07
Iter: 1942 loss: 6.68557107e-07
Iter: 1943 loss: 6.72157853e-07
Iter: 1944 loss: 6.68544772e-07
Iter: 1945 loss: 6.68204848e-07
Iter: 1946 loss: 6.68029656e-07
Iter: 1947 loss: 6.67870381e-07
Iter: 1948 loss: 6.67502604e-07
Iter: 1949 loss: 6.69005658e-07
Iter: 1950 loss: 6.67437462e-07
Iter: 1951 loss: 6.6695452e-07
Iter: 1952 loss: 6.69996894e-07
Iter: 1953 loss: 6.66905464e-07
Iter: 1954 loss: 6.66689061e-07
Iter: 1955 loss: 6.66197934e-07
Iter: 1956 loss: 6.73685804e-07
Iter: 1957 loss: 6.66185883e-07
Iter: 1958 loss: 6.65568678e-07
Iter: 1959 loss: 6.67181325e-07
Iter: 1960 loss: 6.65328912e-07
Iter: 1961 loss: 6.64892923e-07
Iter: 1962 loss: 6.67654717e-07
Iter: 1963 loss: 6.64847903e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2
+ date
Wed Oct 21 12:31:30 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi1.6/300_300_300_1 --function f1 --psi -2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589fae59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f962488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589fa31268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589fa31f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f99e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f898598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f8456a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f911bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f9118c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f8dff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f8ef400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f829730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f829bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f829400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f75ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f829620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f7e11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899d3bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f8292f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899d58f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f799a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f77d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899cd56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899c8e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899c8e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899cc17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899c8e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899c35950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899c35510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899c84378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899ad7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899b8f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899b8f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899bbe1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899b76950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5899b1e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.050043546
test_loss: 0.0499618
train_loss: 0.0262179
test_loss: 0.025693156
train_loss: 0.019542407
test_loss: 0.017153224
train_loss: 0.013030596
test_loss: 0.013743249
train_loss: 0.010006244
test_loss: 0.012531781
train_loss: 0.008884949
test_loss: 0.011568857
train_loss: 0.014940969
test_loss: 0.011783932
train_loss: 0.010879952
test_loss: 0.011501472
train_loss: 0.010051995
test_loss: 0.01064819
train_loss: 0.0088078305
test_loss: 0.011446695
train_loss: 0.008236829
test_loss: 0.011172807
train_loss: 0.008384081
test_loss: 0.010574526
train_loss: 0.008493668
test_loss: 0.010042575
train_loss: 0.010806862
test_loss: 0.010276284
train_loss: 0.008018864
test_loss: 0.009466721
train_loss: 0.008145303
test_loss: 0.01003703
train_loss: 0.0073255715
test_loss: 0.009675553
train_loss: 0.008465951
test_loss: 0.009824849
train_loss: 0.0070811315
test_loss: 0.009714718
train_loss: 0.007447865
test_loss: 0.010090827
train_loss: 0.0068651084
test_loss: 0.010068132
train_loss: 0.007595261
test_loss: 0.010153638
train_loss: 0.007680636
test_loss: 0.009500552
train_loss: 0.0065270206
test_loss: 0.009314965
train_loss: 0.0068450714
test_loss: 0.00989657
train_loss: 0.0075542293
test_loss: 0.009743773
train_loss: 0.009740478
test_loss: 0.011873118
train_loss: 0.0074697705
test_loss: 0.010086211
train_loss: 0.0066032032
test_loss: 0.009435877
train_loss: 0.0074260016
test_loss: 0.008859606
train_loss: 0.006246
test_loss: 0.009364789
train_loss: 0.0061869277
test_loss: 0.00903333
train_loss: 0.0060102185
test_loss: 0.008623478
train_loss: 0.005996617
test_loss: 0.009073707
train_loss: 0.009678266
test_loss: 0.009247443
train_loss: 0.007051659
test_loss: 0.009914247
train_loss: 0.009676376
test_loss: 0.008895598
train_loss: 0.006383393
test_loss: 0.009101132
train_loss: 0.0064155855
test_loss: 0.009092918
train_loss: 0.0062415544
test_loss: 0.009149107
train_loss: 0.006745995
test_loss: 0.00980623
train_loss: 0.0061894087
test_loss: 0.009367041
train_loss: 0.0064885225
test_loss: 0.009584311
train_loss: 0.010945379
test_loss: 0.013904575
train_loss: 0.0081078475
test_loss: 0.011402613
train_loss: 0.007087452
test_loss: 0.009788226
train_loss: 0.0073019173
test_loss: 0.009926218
train_loss: 0.010269989
test_loss: 0.010601834
train_loss: 0.00707687
test_loss: 0.01021831
train_loss: 0.011226945
test_loss: 0.009698558
train_loss: 0.009791013
test_loss: 0.008894682
train_loss: 0.006578591
test_loss: 0.009655129
train_loss: 0.0086033875
test_loss: 0.0096505
train_loss: 0.0068919044
test_loss: 0.00911069
train_loss: 0.0059639136
test_loss: 0.008989156
train_loss: 0.007966776
test_loss: 0.008728111
train_loss: 0.0061530075
test_loss: 0.009319002
train_loss: 0.0059794993
test_loss: 0.009187589
train_loss: 0.0056473734
test_loss: 0.008718657
train_loss: 0.0067576664
test_loss: 0.009328187
train_loss: 0.008358303
test_loss: 0.009677463
train_loss: 0.0071546137
test_loss: 0.009183617
train_loss: 0.006415258
test_loss: 0.008608787
train_loss: 0.0050064716
test_loss: 0.008759444
train_loss: 0.006142479
test_loss: 0.0091204615
train_loss: 0.005491125
test_loss: 0.008790632
train_loss: 0.0063625155
test_loss: 0.008947091
train_loss: 0.006020316
test_loss: 0.008968093
train_loss: 0.006076336
test_loss: 0.008664951
train_loss: 0.0070198835
test_loss: 0.009536443
train_loss: 0.007002934
test_loss: 0.009131103
train_loss: 0.008021107
test_loss: 0.008908637
train_loss: 0.005521134
test_loss: 0.008943123
train_loss: 0.0062464215
test_loss: 0.00934381
train_loss: 0.0055682687
test_loss: 0.00864873
train_loss: 0.0065136272
test_loss: 0.009119821
train_loss: 0.006477314
test_loss: 0.009038653
train_loss: 0.008430082
test_loss: 0.009636292
train_loss: 0.005647947
test_loss: 0.008822879
train_loss: 0.0058868825
test_loss: 0.008879621
train_loss: 0.0059139146
test_loss: 0.009111292
train_loss: 0.0068320883
test_loss: 0.009731942
train_loss: 0.0057409173
test_loss: 0.009249394
train_loss: 0.0061185276
test_loss: 0.008443847
train_loss: 0.006318692
test_loss: 0.008991077
train_loss: 0.005161242
test_loss: 0.008311207
train_loss: 0.006516774
test_loss: 0.009228862
train_loss: 0.006617378
test_loss: 0.008675475
train_loss: 0.0050360826
test_loss: 0.008738025
train_loss: 0.005028025
test_loss: 0.008527523
train_loss: 0.005985259
test_loss: 0.008573605
train_loss: 0.0050664493
test_loss: 0.008535495
train_loss: 0.0047559366
test_loss: 0.008408051
train_loss: 0.0050586993
test_loss: 0.008437388
train_loss: 0.0063958317
test_loss: 0.0087866755
train_loss: 0.0073227407
test_loss: 0.008744193
train_loss: 0.0062225154
test_loss: 0.008924329
train_loss: 0.005376351
test_loss: 0.009053749
train_loss: 0.0051038694
test_loss: 0.008367749
train_loss: 0.005612429
test_loss: 0.008432706
train_loss: 0.0053617954
test_loss: 0.008493533
train_loss: 0.005971259
test_loss: 0.00873406
train_loss: 0.0061469595
test_loss: 0.008932582
train_loss: 0.005610143
test_loss: 0.008675072
train_loss: 0.005819489
test_loss: 0.009049453
train_loss: 0.00577259
test_loss: 0.008790839
train_loss: 0.0051276973
test_loss: 0.008578554
train_loss: 0.005239885
test_loss: 0.009026284
train_loss: 0.0053524883
test_loss: 0.008392081
train_loss: 0.007049241
test_loss: 0.008807589
train_loss: 0.0055155237
test_loss: 0.008537044
train_loss: 0.0052071856
test_loss: 0.008786465
train_loss: 0.0052252063
test_loss: 0.008655481
train_loss: 0.008631231
test_loss: 0.008469388
train_loss: 0.0061209733
test_loss: 0.009314625
train_loss: 0.0061542145
test_loss: 0.00841416
train_loss: 0.005566385
test_loss: 0.008748815
train_loss: 0.005381461
test_loss: 0.008681851
train_loss: 0.0064646094
test_loss: 0.008466603
train_loss: 0.005308219
test_loss: 0.008569089
train_loss: 0.005489696
test_loss: 0.008860148
train_loss: 0.005673724
test_loss: 0.008647938
train_loss: 0.005229859
test_loss: 0.008528169
train_loss: 0.005710928
test_loss: 0.008778304
train_loss: 0.005934763
test_loss: 0.008656575
train_loss: 0.005641202
test_loss: 0.008359705
train_loss: 0.005910109
test_loss: 0.008351236
train_loss: 0.0057753623
test_loss: 0.0086484235
train_loss: 0.0052925413
test_loss: 0.008954381
train_loss: 0.0058555994
test_loss: 0.008393559
train_loss: 0.0047876057
test_loss: 0.0083520925
train_loss: 0.0056042764
test_loss: 0.008766086
train_loss: 0.0064530517
test_loss: 0.0092883995
train_loss: 0.0055662026
test_loss: 0.008645588
train_loss: 0.0052012447
test_loss: 0.008690383
train_loss: 0.0051973457
test_loss: 0.008302862
train_loss: 0.005042499
test_loss: 0.008184846
train_loss: 0.005219187
test_loss: 0.008432414
train_loss: 0.004941081
test_loss: 0.008190497
train_loss: 0.007820972
test_loss: 0.009270821
train_loss: 0.0056028683
test_loss: 0.009307946
train_loss: 0.0048786993
test_loss: 0.0084418245
train_loss: 0.0052300906
test_loss: 0.008518425
train_loss: 0.0052258777
test_loss: 0.008619128
train_loss: 0.0052758018
test_loss: 0.008556733
train_loss: 0.0052032056
test_loss: 0.008884018
train_loss: 0.005000285
test_loss: 0.008697254
train_loss: 0.0058641927
test_loss: 0.008509557
train_loss: 0.005353508
test_loss: 0.008864815
train_loss: 0.006312463
test_loss: 0.0088063935
train_loss: 0.005905087
test_loss: 0.008567017
train_loss: 0.0064942483
test_loss: 0.008545364
train_loss: 0.0048789843
test_loss: 0.008773544
train_loss: 0.005244778
test_loss: 0.008646664
train_loss: 0.006209596
test_loss: 0.009097163
train_loss: 0.00543617
test_loss: 0.008808408
train_loss: 0.005087274
test_loss: 0.008467016
train_loss: 0.005465638
test_loss: 0.0085461335
train_loss: 0.005616282
test_loss: 0.0087552825
train_loss: 0.0056625577
test_loss: 0.008463511
train_loss: 0.0060399314
test_loss: 0.00884482
train_loss: 0.0049269143
test_loss: 0.0083654625
train_loss: 0.005112488
test_loss: 0.008450875
train_loss: 0.005703791
test_loss: 0.008581387
train_loss: 0.0059620948
test_loss: 0.008671029
train_loss: 0.004686119
test_loss: 0.0079852
train_loss: 0.0050704805
test_loss: 0.00868267
train_loss: 0.005674602
test_loss: 0.008183864
train_loss: 0.0057120016
test_loss: 0.008294096
train_loss: 0.0052107386
test_loss: 0.0085108
train_loss: 0.005150942
test_loss: 0.00860878
train_loss: 0.0061292723
test_loss: 0.008866642
train_loss: 0.0070467507
test_loss: 0.008551676
train_loss: 0.0049591325
test_loss: 0.008629773
train_loss: 0.0058657476
test_loss: 0.008898959
train_loss: 0.0060336
test_loss: 0.008763593
train_loss: 0.005633477
test_loss: 0.008989688
train_loss: 0.00601344
test_loss: 0.008562296
train_loss: 0.004575559
test_loss: 0.00834771
train_loss: 0.00522154
test_loss: 0.008469037
train_loss: 0.005196122
test_loss: 0.008822078
train_loss: 0.005379827
test_loss: 0.0087838285
train_loss: 0.005367908
test_loss: 0.008719564
train_loss: 0.0069455924
test_loss: 0.008790774
train_loss: 0.0057116663
test_loss: 0.008850418
train_loss: 0.0053633936
test_loss: 0.008758333
train_loss: 0.0050774487
test_loss: 0.008424579
train_loss: 0.005237001
test_loss: 0.008570116
train_loss: 0.0048326463
test_loss: 0.008329969
train_loss: 0.005623441
test_loss: 0.008810975
train_loss: 0.005827762
test_loss: 0.008456841
train_loss: 0.0060846442
test_loss: 0.008589669
train_loss: 0.0057127276
test_loss: 0.008971602
train_loss: 0.0050864695
test_loss: 0.008342478
train_loss: 0.0048724646
test_loss: 0.008451965
train_loss: 0.00642418
test_loss: 0.008547477
train_loss: 0.005859491
test_loss: 0.008750013
train_loss: 0.0055204816
test_loss: 0.008554579
train_loss: 0.0052378536
test_loss: 0.008643694
train_loss: 0.0053973687
test_loss: 0.008421706
train_loss: 0.0048373938
test_loss: 0.008494021
train_loss: 0.0050023007
test_loss: 0.008531231
train_loss: 0.005478929
test_loss: 0.008885725
train_loss: 0.005449741
test_loss: 0.008695953
train_loss: 0.006005232
test_loss: 0.008512065
train_loss: 0.0054306523
test_loss: 0.009115203
train_loss: 0.005067946
test_loss: 0.008420281
train_loss: 0.004903917
test_loss: 0.008491499
train_loss: 0.0043735215
test_loss: 0.008527052
train_loss: 0.005292335
test_loss: 0.0084172385
train_loss: 0.004516464
test_loss: 0.008355952
train_loss: 0.0053772302
test_loss: 0.008075659
train_loss: 0.0047073113
test_loss: 0.00847122
train_loss: 0.005256018
test_loss: 0.008936458
train_loss: 0.0051717195
test_loss: 0.0090055745
train_loss: 0.00465203
test_loss: 0.008590114
train_loss: 0.0046759136
test_loss: 0.00859286
train_loss: 0.0050529037
test_loss: 0.008461043
train_loss: 0.005718425
test_loss: 0.008136911
train_loss: 0.00494825
test_loss: 0.008874225
train_loss: 0.005444044
test_loss: 0.008988182
train_loss: 0.005223062
test_loss: 0.008571806
train_loss: 0.004983433
test_loss: 0.008622017
train_loss: 0.006029168
test_loss: 0.008662759
train_loss: 0.0046476778
test_loss: 0.008299244
train_loss: 0.005147485
test_loss: 0.008225387
train_loss: 0.005969726
test_loss: 0.008444352
train_loss: 0.0050728046
test_loss: 0.009067145
train_loss: 0.0059215203
test_loss: 0.008302887
train_loss: 0.0048519867
test_loss: 0.008311285
train_loss: 0.0059731426
test_loss: 0.009118395
train_loss: 0.0048697493
test_loss: 0.0083316825
train_loss: 0.005504085
test_loss: 0.008426429
train_loss: 0.0051028915
test_loss: 0.008589592
train_loss: 0.005140318
test_loss: 0.008524462
train_loss: 0.005157962
test_loss: 0.008287616
train_loss: 0.0050393986
test_loss: 0.008580097
train_loss: 0.006400609
test_loss: 0.00871479
train_loss: 0.0056531997
test_loss: 0.008296926
train_loss: 0.005769291
test_loss: 0.008681477
train_loss: 0.005214966
test_loss: 0.008646138
train_loss: 0.0048397817
test_loss: 0.00879184
train_loss: 0.005315818
test_loss: 0.008828803
train_loss: 0.0053990297
test_loss: 0.008809192
train_loss: 0.0053463415
test_loss: 0.00847544
train_loss: 0.0050571617
test_loss: 0.008515568
train_loss: 0.005119672
test_loss: 0.008451836
train_loss: 0.0050438736
test_loss: 0.008494225
train_loss: 0.0053456808
test_loss: 0.008709656
train_loss: 0.005242686
test_loss: 0.00869836
train_loss: 0.0048401677
test_loss: 0.008434704
train_loss: 0.0048749573
test_loss: 0.008381662
train_loss: 0.004978195
test_loss: 0.008810642
train_loss: 0.005694021
test_loss: 0.008829179
train_loss: 0.0052393735
test_loss: 0.008651618
train_loss: 0.004444681
test_loss: 0.008296508
train_loss: 0.0048920442
test_loss: 0.008249275
train_loss: 0.0073433667
test_loss: 0.008952831
train_loss: 0.006336865
test_loss: 0.00863881
train_loss: 0.004992828
test_loss: 0.008479453
train_loss: 0.005344631
test_loss: 0.0086108
train_loss: 0.004972728
test_loss: 0.0081522055
train_loss: 0.005136261
test_loss: 0.008456288
train_loss: 0.0047097
test_loss: 0.008582455
train_loss: 0.0050320365
test_loss: 0.008212205
train_loss: 0.0046191504
test_loss: 0.008506331
train_loss: 0.0073323417
test_loss: 0.008378182
train_loss: 0.004577483
test_loss: 0.008444258
train_loss: 0.006228633
test_loss: 0.008750029
train_loss: 0.006732859
test_loss: 0.008504396
train_loss: 0.0059176087
test_loss: 0.00921098
train_loss: 0.004392335
test_loss: 0.008450075
train_loss: 0.005660718
test_loss: 0.008998837
train_loss: 0.0047487444
test_loss: 0.008545544
train_loss: 0.0048388997
test_loss: 0.008370681
train_loss: 0.0073423125
test_loss: 0.008589662
train_loss: 0.0053474596
test_loss: 0.008623122
train_loss: 0.0050659017
test_loss: 0.008446926
train_loss: 0.0054130033
test_loss: 0.008113017
train_loss: 0.004796046
test_loss: 0.00847576
train_loss: 0.0049190195
test_loss: 0.008596229
train_loss: 0.0055139996
test_loss: 0.008344154
train_loss: 0.0059598023
test_loss: 0.009584672
train_loss: 0.006315364
test_loss: 0.009323989
train_loss: 0.0055425065
test_loss: 0.008864247
train_loss: 0.005664219
test_loss: 0.008509733
train_loss: 0.0052028727
test_loss: 0.008677802
train_loss: 0.006204726
test_loss: 0.008224393
train_loss: 0.004661921
test_loss: 0.008175371
train_loss: 0.0050330213
test_loss: 0.008295251
train_loss: 0.0047950656
test_loss: 0.008388743
train_loss: 0.005838955
test_loss: 0.008832601
train_loss: 0.00506225
test_loss: 0.00860442
train_loss: 0.0052340943
test_loss: 0.008836776
train_loss: 0.0055069765
test_loss: 0.008476995
train_loss: 0.0047523393
test_loss: 0.008444362
train_loss: 0.0046203383
test_loss: 0.008424825
train_loss: 0.00556737
test_loss: 0.008583657
train_loss: 0.0055671255
test_loss: 0.008486984
train_loss: 0.0051750867
test_loss: 0.0088990275
train_loss: 0.0049218927
test_loss: 0.008399447
train_loss: 0.005394607
test_loss: 0.00928294
train_loss: 0.004761643
test_loss: 0.008371356
train_loss: 0.00463956
test_loss: 0.008509347
train_loss: 0.0051847138
test_loss: 0.008558077
train_loss: 0.005575275
test_loss: 0.00894982
train_loss: 0.0047979355
test_loss: 0.008413944
train_loss: 0.0047604716
test_loss: 0.008603718
train_loss: 0.0053270725
test_loss: 0.008697009
train_loss: 0.0050423373
test_loss: 0.008490141
train_loss: 0.0049396716
test_loss: 0.008514585
train_loss: 0.0047342507
test_loss: 0.008708176
train_loss: 0.0050566574
test_loss: 0.008481774
train_loss: 0.005589677
test_loss: 0.009463056
train_loss: 0.00500022
test_loss: 0.008656995
train_loss: 0.005564563
test_loss: 0.008616173
train_loss: 0.005917595
test_loss: 0.008527248
train_loss: 0.0047518048
test_loss: 0.008647836
train_loss: 0.005166334
test_loss: 0.008403051
train_loss: 0.0044931257
test_loss: 0.008101444
train_loss: 0.0055245003
test_loss: 0.008326875
train_loss: 0.0049554985
test_loss: 0.008368939
train_loss: 0.004503972
test_loss: 0.0083287405
train_loss: 0.004663655
test_loss: 0.008426112
train_loss: 0.005194594
test_loss: 0.008569153
train_loss: 0.0050218473
test_loss: 0.008668359
train_loss: 0.0050846934
test_loss: 0.0085231485
train_loss: 0.0054569035
test_loss: 0.008536366
train_loss: 0.005590699
test_loss: 0.008746689
train_loss: 0.004763295
test_loss: 0.008854427
train_loss: 0.005140764
test_loss: 0.0083727855
train_loss: 0.0052523063
test_loss: 0.008651461
train_loss: 0.0050451793
test_loss: 0.008706667
train_loss: 0.0051910263
test_loss: 0.008550896
train_loss: 0.0046728095
test_loss: 0.00871108
train_loss: 0.0063577266
test_loss: 0.008512414
train_loss: 0.006235918
test_loss: 0.009164872
train_loss: 0.004893034
test_loss: 0.008427763
train_loss: 0.0059825527
test_loss: 0.0088713495
train_loss: 0.0046846676
test_loss: 0.0082320515
train_loss: 0.0046711485
test_loss: 0.008501854
train_loss: 0.0050066914
test_loss: 0.008247166
train_loss: 0.005091503
test_loss: 0.008590966
train_loss: 0.0048246323
test_loss: 0.008645905
train_loss: 0.0048612338
test_loss: 0.008283463
train_loss: 0.0046885535
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.008231613
train_loss: 0.0051031495
test_loss: 0.008286571
train_loss: 0.005627568
test_loss: 0.008391442
train_loss: 0.004571888
test_loss: 0.008834891
train_loss: 0.0052724564
test_loss: 0.008468631
train_loss: 0.0054309587
test_loss: 0.008766597
train_loss: 0.0047420175
test_loss: 0.008585917
train_loss: 0.0057245055
test_loss: 0.008823855
train_loss: 0.0065680044
test_loss: 0.009079291
train_loss: 0.004843034
test_loss: 0.008566299
train_loss: 0.0045029595
test_loss: 0.008502738
train_loss: 0.00531865
test_loss: 0.00853691
train_loss: 0.0045718644
test_loss: 0.008425758
train_loss: 0.005167241
test_loss: 0.0091555705
train_loss: 0.004830151
test_loss: 0.008669745
train_loss: 0.004871173
test_loss: 0.008617743
train_loss: 0.004051141
test_loss: 0.008319004
train_loss: 0.0042291456
test_loss: 0.008357557
train_loss: 0.0057490594
test_loss: 0.008547602
train_loss: 0.00537199
test_loss: 0.008635538
train_loss: 0.0047825794
test_loss: 0.0087101385
train_loss: 0.005137441
test_loss: 0.008648236
train_loss: 0.005401969
test_loss: 0.008311226
train_loss: 0.005908214
test_loss: 0.008359598
train_loss: 0.004542099
test_loss: 0.008634405
train_loss: 0.005256452
test_loss: 0.0087124035
train_loss: 0.007987684
test_loss: 0.008555093
train_loss: 0.0050379834
test_loss: 0.008800316
train_loss: 0.0049988097
test_loss: 0.008636819
train_loss: 0.0052172313
test_loss: 0.008951023
train_loss: 0.0057501746
test_loss: 0.008984037
train_loss: 0.0048638294
test_loss: 0.008824032
train_loss: 0.005060695
test_loss: 0.008757776
train_loss: 0.005275072
test_loss: 0.008515802
train_loss: 0.0044124112
test_loss: 0.0088006165
train_loss: 0.004840943
test_loss: 0.008722005
train_loss: 0.004571444
test_loss: 0.008450874
train_loss: 0.004756568
test_loss: 0.008446919
train_loss: 0.0043371543
test_loss: 0.008362511
train_loss: 0.0053226766
test_loss: 0.008647725
train_loss: 0.004447774
test_loss: 0.008456198
train_loss: 0.005122121
test_loss: 0.008691048
train_loss: 0.005546371
test_loss: 0.008253308
train_loss: 0.006122293
test_loss: 0.008733108
train_loss: 0.0049747024
test_loss: 0.008939519
train_loss: 0.004920072
test_loss: 0.008424333
train_loss: 0.004804892
test_loss: 0.0083987815
train_loss: 0.005202306
test_loss: 0.008509027
train_loss: 0.004957096
test_loss: 0.008745718
train_loss: 0.0043055667
test_loss: 0.0083079655
train_loss: 0.005744329
test_loss: 0.008512457
train_loss: 0.0064566727
test_loss: 0.008466275
train_loss: 0.0050116424
test_loss: 0.008938345
train_loss: 0.0048714275
test_loss: 0.008628668
train_loss: 0.0051370286
test_loss: 0.008860363
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85eae048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ffb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85eae598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ef5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85f0b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85f0b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85f0bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ddbe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ddb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85d9cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85db8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85d6d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85d6dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85d3a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85d3aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85cd6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85d0e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ccdc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ccdf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ccd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c85ccd950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c49744598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c49773598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c4971f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c497336a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c49733b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c4971f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c496b12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c496b1378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c247676a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c2470d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c24728400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c246b9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c246de620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c24684620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c24684e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000311927608
Iter: 2 loss: 0.000233899991
Iter: 3 loss: 0.000233852043
Iter: 4 loss: 0.000191860163
Iter: 5 loss: 0.000639762788
Iter: 6 loss: 0.000190432678
Iter: 7 loss: 0.000160995289
Iter: 8 loss: 0.000120822755
Iter: 9 loss: 0.000118343843
Iter: 10 loss: 7.81342387e-05
Iter: 11 loss: 0.000513641047
Iter: 12 loss: 7.68964092e-05
Iter: 13 loss: 5.77944738e-05
Iter: 14 loss: 0.000268592412
Iter: 15 loss: 5.7411904e-05
Iter: 16 loss: 4.51580454e-05
Iter: 17 loss: 9.12167598e-05
Iter: 18 loss: 4.20874858e-05
Iter: 19 loss: 3.58998186e-05
Iter: 20 loss: 0.000100645586
Iter: 21 loss: 3.57467979e-05
Iter: 22 loss: 3.29760078e-05
Iter: 23 loss: 3.28947863e-05
Iter: 24 loss: 3.07212e-05
Iter: 25 loss: 2.74758386e-05
Iter: 26 loss: 6.72186434e-05
Iter: 27 loss: 2.74432787e-05
Iter: 28 loss: 2.52505561e-05
Iter: 29 loss: 3.7690781e-05
Iter: 30 loss: 2.4943989e-05
Iter: 31 loss: 2.36060296e-05
Iter: 32 loss: 2.17106935e-05
Iter: 33 loss: 2.16435583e-05
Iter: 34 loss: 2.03483014e-05
Iter: 35 loss: 2.03044256e-05
Iter: 36 loss: 1.89849652e-05
Iter: 37 loss: 2.11972474e-05
Iter: 38 loss: 1.83904813e-05
Iter: 39 loss: 1.71810534e-05
Iter: 40 loss: 2.75035618e-05
Iter: 41 loss: 1.71136653e-05
Iter: 42 loss: 1.66521058e-05
Iter: 43 loss: 1.65893962e-05
Iter: 44 loss: 1.62305478e-05
Iter: 45 loss: 1.5283309e-05
Iter: 46 loss: 2.2205606e-05
Iter: 47 loss: 1.50823826e-05
Iter: 48 loss: 1.42062145e-05
Iter: 49 loss: 2.41417438e-05
Iter: 50 loss: 1.41905548e-05
Iter: 51 loss: 1.37691968e-05
Iter: 52 loss: 1.88885224e-05
Iter: 53 loss: 1.37643956e-05
Iter: 54 loss: 1.34068305e-05
Iter: 55 loss: 1.30940443e-05
Iter: 56 loss: 1.29996442e-05
Iter: 57 loss: 1.2524265e-05
Iter: 58 loss: 1.57400391e-05
Iter: 59 loss: 1.24772969e-05
Iter: 60 loss: 1.21139437e-05
Iter: 61 loss: 1.22659094e-05
Iter: 62 loss: 1.18645676e-05
Iter: 63 loss: 1.13566557e-05
Iter: 64 loss: 1.23503942e-05
Iter: 65 loss: 1.11468098e-05
Iter: 66 loss: 1.06714651e-05
Iter: 67 loss: 1.37740135e-05
Iter: 68 loss: 1.06202097e-05
Iter: 69 loss: 1.02607519e-05
Iter: 70 loss: 1.30279241e-05
Iter: 71 loss: 1.02341819e-05
Iter: 72 loss: 1.00525667e-05
Iter: 73 loss: 1.07452961e-05
Iter: 74 loss: 1.00094639e-05
Iter: 75 loss: 9.80264394e-06
Iter: 76 loss: 9.86953728e-06
Iter: 77 loss: 9.65602e-06
Iter: 78 loss: 9.46376895e-06
Iter: 79 loss: 9.46375712e-06
Iter: 80 loss: 9.34147829e-06
Iter: 81 loss: 9.07081085e-06
Iter: 82 loss: 1.29502941e-05
Iter: 83 loss: 9.05768e-06
Iter: 84 loss: 8.75480146e-06
Iter: 85 loss: 9.75009152e-06
Iter: 86 loss: 8.67086237e-06
Iter: 87 loss: 8.50827109e-06
Iter: 88 loss: 8.50250581e-06
Iter: 89 loss: 8.35468109e-06
Iter: 90 loss: 8.14722807e-06
Iter: 91 loss: 8.13915e-06
Iter: 92 loss: 7.94337666e-06
Iter: 93 loss: 9.57358679e-06
Iter: 94 loss: 7.93191066e-06
Iter: 95 loss: 7.76128491e-06
Iter: 96 loss: 8.02382783e-06
Iter: 97 loss: 7.68094833e-06
Iter: 98 loss: 7.51738935e-06
Iter: 99 loss: 7.62729951e-06
Iter: 100 loss: 7.41453096e-06
Iter: 101 loss: 7.23350968e-06
Iter: 102 loss: 8.94100776e-06
Iter: 103 loss: 7.22625646e-06
Iter: 104 loss: 7.10748463e-06
Iter: 105 loss: 7.84699296e-06
Iter: 106 loss: 7.09365941e-06
Iter: 107 loss: 6.98784243e-06
Iter: 108 loss: 7.15046e-06
Iter: 109 loss: 6.93764e-06
Iter: 110 loss: 6.82888185e-06
Iter: 111 loss: 7.54854591e-06
Iter: 112 loss: 6.81745132e-06
Iter: 113 loss: 6.73256181e-06
Iter: 114 loss: 6.87287275e-06
Iter: 115 loss: 6.69389465e-06
Iter: 116 loss: 6.59317766e-06
Iter: 117 loss: 6.54397445e-06
Iter: 118 loss: 6.49550066e-06
Iter: 119 loss: 6.39224e-06
Iter: 120 loss: 6.55191479e-06
Iter: 121 loss: 6.34319531e-06
Iter: 122 loss: 6.24241511e-06
Iter: 123 loss: 6.85860869e-06
Iter: 124 loss: 6.23022697e-06
Iter: 125 loss: 6.13742486e-06
Iter: 126 loss: 6.91181958e-06
Iter: 127 loss: 6.13191605e-06
Iter: 128 loss: 6.06889489e-06
Iter: 129 loss: 5.92985e-06
Iter: 130 loss: 7.94036714e-06
Iter: 131 loss: 5.92331162e-06
Iter: 132 loss: 5.7913644e-06
Iter: 133 loss: 7.25703376e-06
Iter: 134 loss: 5.78886556e-06
Iter: 135 loss: 5.6917479e-06
Iter: 136 loss: 6.17085334e-06
Iter: 137 loss: 5.67509778e-06
Iter: 138 loss: 5.59305045e-06
Iter: 139 loss: 5.83773181e-06
Iter: 140 loss: 5.56815075e-06
Iter: 141 loss: 5.52223082e-06
Iter: 142 loss: 6.15485533e-06
Iter: 143 loss: 5.52213169e-06
Iter: 144 loss: 5.47993932e-06
Iter: 145 loss: 5.44328122e-06
Iter: 146 loss: 5.43211536e-06
Iter: 147 loss: 5.35614072e-06
Iter: 148 loss: 5.91223215e-06
Iter: 149 loss: 5.34982883e-06
Iter: 150 loss: 5.3138283e-06
Iter: 151 loss: 5.38574113e-06
Iter: 152 loss: 5.29905674e-06
Iter: 153 loss: 5.25319228e-06
Iter: 154 loss: 5.19576452e-06
Iter: 155 loss: 5.19111563e-06
Iter: 156 loss: 5.12369797e-06
Iter: 157 loss: 5.4884913e-06
Iter: 158 loss: 5.11370808e-06
Iter: 159 loss: 5.06445531e-06
Iter: 160 loss: 5.11338931e-06
Iter: 161 loss: 5.03671e-06
Iter: 162 loss: 4.96828034e-06
Iter: 163 loss: 5.63724689e-06
Iter: 164 loss: 4.96587745e-06
Iter: 165 loss: 4.92554227e-06
Iter: 166 loss: 4.89020567e-06
Iter: 167 loss: 4.87952047e-06
Iter: 168 loss: 4.81858569e-06
Iter: 169 loss: 4.94607411e-06
Iter: 170 loss: 4.79448545e-06
Iter: 171 loss: 4.7435633e-06
Iter: 172 loss: 5.11035933e-06
Iter: 173 loss: 4.73922682e-06
Iter: 174 loss: 4.69635961e-06
Iter: 175 loss: 4.97265955e-06
Iter: 176 loss: 4.69150655e-06
Iter: 177 loss: 4.65461108e-06
Iter: 178 loss: 4.77937692e-06
Iter: 179 loss: 4.64470486e-06
Iter: 180 loss: 4.61063246e-06
Iter: 181 loss: 4.74592434e-06
Iter: 182 loss: 4.60283127e-06
Iter: 183 loss: 4.57420629e-06
Iter: 184 loss: 4.60211959e-06
Iter: 185 loss: 4.5579427e-06
Iter: 186 loss: 4.52265886e-06
Iter: 187 loss: 4.60942056e-06
Iter: 188 loss: 4.51023425e-06
Iter: 189 loss: 4.47886441e-06
Iter: 190 loss: 4.44837e-06
Iter: 191 loss: 4.44163743e-06
Iter: 192 loss: 4.40084e-06
Iter: 193 loss: 5.00401666e-06
Iter: 194 loss: 4.40076383e-06
Iter: 195 loss: 4.37109884e-06
Iter: 196 loss: 4.33753758e-06
Iter: 197 loss: 4.33312789e-06
Iter: 198 loss: 4.28295198e-06
Iter: 199 loss: 4.4930448e-06
Iter: 200 loss: 4.27226314e-06
Iter: 201 loss: 4.23931306e-06
Iter: 202 loss: 4.53957364e-06
Iter: 203 loss: 4.23779193e-06
Iter: 204 loss: 4.20243623e-06
Iter: 205 loss: 4.28264684e-06
Iter: 206 loss: 4.18909121e-06
Iter: 207 loss: 4.1638059e-06
Iter: 208 loss: 4.15344903e-06
Iter: 209 loss: 4.14023089e-06
Iter: 210 loss: 4.11769815e-06
Iter: 211 loss: 4.11730798e-06
Iter: 212 loss: 4.09422864e-06
Iter: 213 loss: 4.12163536e-06
Iter: 214 loss: 4.08214964e-06
Iter: 215 loss: 4.05607898e-06
Iter: 216 loss: 4.10327721e-06
Iter: 217 loss: 4.04478942e-06
Iter: 218 loss: 4.01858188e-06
Iter: 219 loss: 4.06747722e-06
Iter: 220 loss: 4.0075729e-06
Iter: 221 loss: 3.98119528e-06
Iter: 222 loss: 4.03633476e-06
Iter: 223 loss: 3.97071335e-06
Iter: 224 loss: 3.94336894e-06
Iter: 225 loss: 4.04569892e-06
Iter: 226 loss: 3.93669143e-06
Iter: 227 loss: 3.91410322e-06
Iter: 228 loss: 3.89108118e-06
Iter: 229 loss: 3.88670333e-06
Iter: 230 loss: 3.85313115e-06
Iter: 231 loss: 4.09673839e-06
Iter: 232 loss: 3.85033945e-06
Iter: 233 loss: 3.82530561e-06
Iter: 234 loss: 3.97049098e-06
Iter: 235 loss: 3.82210237e-06
Iter: 236 loss: 3.80104757e-06
Iter: 237 loss: 3.82634698e-06
Iter: 238 loss: 3.7900254e-06
Iter: 239 loss: 3.7711784e-06
Iter: 240 loss: 3.96183077e-06
Iter: 241 loss: 3.77072365e-06
Iter: 242 loss: 3.75092964e-06
Iter: 243 loss: 3.74406454e-06
Iter: 244 loss: 3.73295234e-06
Iter: 245 loss: 3.71952706e-06
Iter: 246 loss: 3.71921669e-06
Iter: 247 loss: 3.70570888e-06
Iter: 248 loss: 3.70705402e-06
Iter: 249 loss: 3.69543341e-06
Iter: 250 loss: 3.67973507e-06
Iter: 251 loss: 3.68807673e-06
Iter: 252 loss: 3.66931044e-06
Iter: 253 loss: 3.64760263e-06
Iter: 254 loss: 3.72307431e-06
Iter: 255 loss: 3.64204107e-06
Iter: 256 loss: 3.62725041e-06
Iter: 257 loss: 3.640881e-06
Iter: 258 loss: 3.61863795e-06
Iter: 259 loss: 3.59449496e-06
Iter: 260 loss: 3.64531206e-06
Iter: 261 loss: 3.58479565e-06
Iter: 262 loss: 3.56877854e-06
Iter: 263 loss: 3.57513682e-06
Iter: 264 loss: 3.55759016e-06
Iter: 265 loss: 3.53481255e-06
Iter: 266 loss: 3.61906314e-06
Iter: 267 loss: 3.52933148e-06
Iter: 268 loss: 3.51325616e-06
Iter: 269 loss: 3.61400112e-06
Iter: 270 loss: 3.51146718e-06
Iter: 271 loss: 3.49479683e-06
Iter: 272 loss: 3.51541803e-06
Iter: 273 loss: 3.48618414e-06
Iter: 274 loss: 3.46991578e-06
Iter: 275 loss: 3.64157427e-06
Iter: 276 loss: 3.46949673e-06
Iter: 277 loss: 3.45721105e-06
Iter: 278 loss: 3.46956313e-06
Iter: 279 loss: 3.45029775e-06
Iter: 280 loss: 3.43876127e-06
Iter: 281 loss: 3.59259275e-06
Iter: 282 loss: 3.43872375e-06
Iter: 283 loss: 3.42959379e-06
Iter: 284 loss: 3.41490363e-06
Iter: 285 loss: 3.41470059e-06
Iter: 286 loss: 3.39773442e-06
Iter: 287 loss: 3.45314174e-06
Iter: 288 loss: 3.39294775e-06
Iter: 289 loss: 3.37612846e-06
Iter: 290 loss: 3.43638453e-06
Iter: 291 loss: 3.37187203e-06
Iter: 292 loss: 3.35738423e-06
Iter: 293 loss: 3.36449921e-06
Iter: 294 loss: 3.34776405e-06
Iter: 295 loss: 3.3324759e-06
Iter: 296 loss: 3.47904597e-06
Iter: 297 loss: 3.33182084e-06
Iter: 298 loss: 3.32014361e-06
Iter: 299 loss: 3.31730894e-06
Iter: 300 loss: 3.30980129e-06
Iter: 301 loss: 3.29409272e-06
Iter: 302 loss: 3.30763169e-06
Iter: 303 loss: 3.28474e-06
Iter: 304 loss: 3.26784016e-06
Iter: 305 loss: 3.38070322e-06
Iter: 306 loss: 3.2660962e-06
Iter: 307 loss: 3.25149472e-06
Iter: 308 loss: 3.36901462e-06
Iter: 309 loss: 3.25061797e-06
Iter: 310 loss: 3.23968e-06
Iter: 311 loss: 3.2642065e-06
Iter: 312 loss: 3.23544691e-06
Iter: 313 loss: 3.22344522e-06
Iter: 314 loss: 3.28087481e-06
Iter: 315 loss: 3.22112965e-06
Iter: 316 loss: 3.21079187e-06
Iter: 317 loss: 3.25404e-06
Iter: 318 loss: 3.20860909e-06
Iter: 319 loss: 3.20075196e-06
Iter: 320 loss: 3.18698972e-06
Iter: 321 loss: 3.18702223e-06
Iter: 322 loss: 3.17324157e-06
Iter: 323 loss: 3.31363685e-06
Iter: 324 loss: 3.17287254e-06
Iter: 325 loss: 3.16233672e-06
Iter: 326 loss: 3.17523518e-06
Iter: 327 loss: 3.15686157e-06
Iter: 328 loss: 3.14594058e-06
Iter: 329 loss: 3.15744433e-06
Iter: 330 loss: 3.13985402e-06
Iter: 331 loss: 3.12730185e-06
Iter: 332 loss: 3.24229177e-06
Iter: 333 loss: 3.12674797e-06
Iter: 334 loss: 3.1188606e-06
Iter: 335 loss: 3.10732048e-06
Iter: 336 loss: 3.1069585e-06
Iter: 337 loss: 3.09221628e-06
Iter: 338 loss: 3.17110653e-06
Iter: 339 loss: 3.0900685e-06
Iter: 340 loss: 3.07781193e-06
Iter: 341 loss: 3.14446538e-06
Iter: 342 loss: 3.07592518e-06
Iter: 343 loss: 3.06461015e-06
Iter: 344 loss: 3.14314184e-06
Iter: 345 loss: 3.06351649e-06
Iter: 346 loss: 3.05608819e-06
Iter: 347 loss: 3.08372546e-06
Iter: 348 loss: 3.05429103e-06
Iter: 349 loss: 3.0456863e-06
Iter: 350 loss: 3.05841149e-06
Iter: 351 loss: 3.04151627e-06
Iter: 352 loss: 3.03208253e-06
Iter: 353 loss: 3.04270225e-06
Iter: 354 loss: 3.02697663e-06
Iter: 355 loss: 3.01925547e-06
Iter: 356 loss: 3.03527304e-06
Iter: 357 loss: 3.01620366e-06
Iter: 358 loss: 3.00717647e-06
Iter: 359 loss: 3.02160515e-06
Iter: 360 loss: 3.0030169e-06
Iter: 361 loss: 2.99254202e-06
Iter: 362 loss: 3.03345132e-06
Iter: 363 loss: 2.99027397e-06
Iter: 364 loss: 2.98235523e-06
Iter: 365 loss: 2.98321811e-06
Iter: 366 loss: 2.97625729e-06
Iter: 367 loss: 2.963631e-06
Iter: 368 loss: 3.0302042e-06
Iter: 369 loss: 2.96176768e-06
Iter: 370 loss: 2.95198606e-06
Iter: 371 loss: 2.96415601e-06
Iter: 372 loss: 2.94679035e-06
Iter: 373 loss: 2.93702533e-06
Iter: 374 loss: 2.93853918e-06
Iter: 375 loss: 2.92977143e-06
Iter: 376 loss: 2.92168602e-06
Iter: 377 loss: 2.92106324e-06
Iter: 378 loss: 2.91390734e-06
Iter: 379 loss: 2.92151435e-06
Iter: 380 loss: 2.90994217e-06
Iter: 381 loss: 2.90200182e-06
Iter: 382 loss: 2.96115786e-06
Iter: 383 loss: 2.90129719e-06
Iter: 384 loss: 2.89473155e-06
Iter: 385 loss: 2.88780666e-06
Iter: 386 loss: 2.88669116e-06
Iter: 387 loss: 2.87723583e-06
Iter: 388 loss: 2.93120183e-06
Iter: 389 loss: 2.87603962e-06
Iter: 390 loss: 2.86882869e-06
Iter: 391 loss: 2.86819522e-06
Iter: 392 loss: 2.86280056e-06
Iter: 393 loss: 2.85276428e-06
Iter: 394 loss: 2.92371169e-06
Iter: 395 loss: 2.85187207e-06
Iter: 396 loss: 2.84511634e-06
Iter: 397 loss: 2.85246733e-06
Iter: 398 loss: 2.84132739e-06
Iter: 399 loss: 2.83360259e-06
Iter: 400 loss: 2.84445264e-06
Iter: 401 loss: 2.82963606e-06
Iter: 402 loss: 2.81952748e-06
Iter: 403 loss: 2.87748344e-06
Iter: 404 loss: 2.81817438e-06
Iter: 405 loss: 2.81188159e-06
Iter: 406 loss: 2.80128552e-06
Iter: 407 loss: 2.80125846e-06
Iter: 408 loss: 2.7941137e-06
Iter: 409 loss: 2.79343044e-06
Iter: 410 loss: 2.78591347e-06
Iter: 411 loss: 2.80330505e-06
Iter: 412 loss: 2.78306857e-06
Iter: 413 loss: 2.77638514e-06
Iter: 414 loss: 2.82378551e-06
Iter: 415 loss: 2.77581785e-06
Iter: 416 loss: 2.77042159e-06
Iter: 417 loss: 2.7706576e-06
Iter: 418 loss: 2.76616e-06
Iter: 419 loss: 2.75990669e-06
Iter: 420 loss: 2.76551918e-06
Iter: 421 loss: 2.75616867e-06
Iter: 422 loss: 2.7479573e-06
Iter: 423 loss: 2.77052641e-06
Iter: 424 loss: 2.7452852e-06
Iter: 425 loss: 2.73915043e-06
Iter: 426 loss: 2.76965943e-06
Iter: 427 loss: 2.73807905e-06
Iter: 428 loss: 2.73167234e-06
Iter: 429 loss: 2.73167734e-06
Iter: 430 loss: 2.72661828e-06
Iter: 431 loss: 2.71792123e-06
Iter: 432 loss: 2.74707622e-06
Iter: 433 loss: 2.71554063e-06
Iter: 434 loss: 2.70842565e-06
Iter: 435 loss: 2.74496597e-06
Iter: 436 loss: 2.70721057e-06
Iter: 437 loss: 2.70028863e-06
Iter: 438 loss: 2.70604778e-06
Iter: 439 loss: 2.69611655e-06
Iter: 440 loss: 2.68869121e-06
Iter: 441 loss: 2.7088895e-06
Iter: 442 loss: 2.68626559e-06
Iter: 443 loss: 2.6808043e-06
Iter: 444 loss: 2.7661049e-06
Iter: 445 loss: 2.68080157e-06
Iter: 446 loss: 2.67551445e-06
Iter: 447 loss: 2.68086478e-06
Iter: 448 loss: 2.67257474e-06
Iter: 449 loss: 2.66678762e-06
Iter: 450 loss: 2.69997145e-06
Iter: 451 loss: 2.66605321e-06
Iter: 452 loss: 2.66231564e-06
Iter: 453 loss: 2.65630479e-06
Iter: 454 loss: 2.65621838e-06
Iter: 455 loss: 2.64883033e-06
Iter: 456 loss: 2.69421957e-06
Iter: 457 loss: 2.64792243e-06
Iter: 458 loss: 2.64099958e-06
Iter: 459 loss: 2.64551136e-06
Iter: 460 loss: 2.63656102e-06
Iter: 461 loss: 2.62963749e-06
Iter: 462 loss: 2.67678456e-06
Iter: 463 loss: 2.62900494e-06
Iter: 464 loss: 2.62251206e-06
Iter: 465 loss: 2.62286221e-06
Iter: 466 loss: 2.61742252e-06
Iter: 467 loss: 2.6103985e-06
Iter: 468 loss: 2.64999221e-06
Iter: 469 loss: 2.60935758e-06
Iter: 470 loss: 2.60295906e-06
Iter: 471 loss: 2.6210414e-06
Iter: 472 loss: 2.60082879e-06
Iter: 473 loss: 2.59445096e-06
Iter: 474 loss: 2.60127808e-06
Iter: 475 loss: 2.59089779e-06
Iter: 476 loss: 2.58447926e-06
Iter: 477 loss: 2.62648564e-06
Iter: 478 loss: 2.58376531e-06
Iter: 479 loss: 2.57785382e-06
Iter: 480 loss: 2.62608228e-06
Iter: 481 loss: 2.57744023e-06
Iter: 482 loss: 2.57407987e-06
Iter: 483 loss: 2.58417845e-06
Iter: 484 loss: 2.57298825e-06
Iter: 485 loss: 2.56908606e-06
Iter: 486 loss: 2.56180283e-06
Iter: 487 loss: 2.72394027e-06
Iter: 488 loss: 2.56179283e-06
Iter: 489 loss: 2.55475652e-06
Iter: 490 loss: 2.61111745e-06
Iter: 491 loss: 2.55422492e-06
Iter: 492 loss: 2.54912152e-06
Iter: 493 loss: 2.56242265e-06
Iter: 494 loss: 2.54735824e-06
Iter: 495 loss: 2.54133e-06
Iter: 496 loss: 2.54532733e-06
Iter: 497 loss: 2.53760822e-06
Iter: 498 loss: 2.5317845e-06
Iter: 499 loss: 2.57090596e-06
Iter: 500 loss: 2.5312404e-06
Iter: 501 loss: 2.5256727e-06
Iter: 502 loss: 2.52245923e-06
Iter: 503 loss: 2.51995289e-06
Iter: 504 loss: 2.51356346e-06
Iter: 505 loss: 2.57647571e-06
Iter: 506 loss: 2.51338088e-06
Iter: 507 loss: 2.50806102e-06
Iter: 508 loss: 2.5174686e-06
Iter: 509 loss: 2.50570884e-06
Iter: 510 loss: 2.50072571e-06
Iter: 511 loss: 2.51025267e-06
Iter: 512 loss: 2.49855793e-06
Iter: 513 loss: 2.4952833e-06
Iter: 514 loss: 2.49494769e-06
Iter: 515 loss: 2.49257573e-06
Iter: 516 loss: 2.4911933e-06
Iter: 517 loss: 2.49023651e-06
Iter: 518 loss: 2.48539823e-06
Iter: 519 loss: 2.48713718e-06
Iter: 520 loss: 2.48196147e-06
Iter: 521 loss: 2.47726257e-06
Iter: 522 loss: 2.47920411e-06
Iter: 523 loss: 2.47404773e-06
Iter: 524 loss: 2.4672272e-06
Iter: 525 loss: 2.49664276e-06
Iter: 526 loss: 2.46576928e-06
Iter: 527 loss: 2.45989349e-06
Iter: 528 loss: 2.47677644e-06
Iter: 529 loss: 2.45795695e-06
Iter: 530 loss: 2.452525e-06
Iter: 531 loss: 2.45877914e-06
Iter: 532 loss: 2.44958915e-06
Iter: 533 loss: 2.44256626e-06
Iter: 534 loss: 2.46944774e-06
Iter: 535 loss: 2.44093303e-06
Iter: 536 loss: 2.43596378e-06
Iter: 537 loss: 2.44994544e-06
Iter: 538 loss: 2.4342803e-06
Iter: 539 loss: 2.42924807e-06
Iter: 540 loss: 2.44581315e-06
Iter: 541 loss: 2.42785472e-06
Iter: 542 loss: 2.42282294e-06
Iter: 543 loss: 2.43363252e-06
Iter: 544 loss: 2.42081796e-06
Iter: 545 loss: 2.4180963e-06
Iter: 546 loss: 2.41796647e-06
Iter: 547 loss: 2.41525231e-06
Iter: 548 loss: 2.41538942e-06
Iter: 549 loss: 2.41313273e-06
Iter: 550 loss: 2.40952568e-06
Iter: 551 loss: 2.41735916e-06
Iter: 552 loss: 2.40802115e-06
Iter: 553 loss: 2.40394775e-06
Iter: 554 loss: 2.40183613e-06
Iter: 555 loss: 2.3999155e-06
Iter: 556 loss: 2.39500309e-06
Iter: 557 loss: 2.40641702e-06
Iter: 558 loss: 2.39319911e-06
Iter: 559 loss: 2.38709049e-06
Iter: 560 loss: 2.41928615e-06
Iter: 561 loss: 2.38614621e-06
Iter: 562 loss: 2.38229268e-06
Iter: 563 loss: 2.38627945e-06
Iter: 564 loss: 2.38008738e-06
Iter: 565 loss: 2.3748903e-06
Iter: 566 loss: 2.38569328e-06
Iter: 567 loss: 2.37294853e-06
Iter: 568 loss: 2.36736423e-06
Iter: 569 loss: 2.38662619e-06
Iter: 570 loss: 2.3657708e-06
Iter: 571 loss: 2.36140113e-06
Iter: 572 loss: 2.37104e-06
Iter: 573 loss: 2.35979405e-06
Iter: 574 loss: 2.35449897e-06
Iter: 575 loss: 2.37217864e-06
Iter: 576 loss: 2.35301923e-06
Iter: 577 loss: 2.34949675e-06
Iter: 578 loss: 2.37737777e-06
Iter: 579 loss: 2.34929712e-06
Iter: 580 loss: 2.34556569e-06
Iter: 581 loss: 2.35458447e-06
Iter: 582 loss: 2.34411527e-06
Iter: 583 loss: 2.34072741e-06
Iter: 584 loss: 2.34747063e-06
Iter: 585 loss: 2.33947299e-06
Iter: 586 loss: 2.33582796e-06
Iter: 587 loss: 2.33614401e-06
Iter: 588 loss: 2.33309697e-06
Iter: 589 loss: 2.3286475e-06
Iter: 590 loss: 2.33068795e-06
Iter: 591 loss: 2.32556522e-06
Iter: 592 loss: 2.32143839e-06
Iter: 593 loss: 2.37971926e-06
Iter: 594 loss: 2.32142747e-06
Iter: 595 loss: 2.31821514e-06
Iter: 596 loss: 2.31812101e-06
Iter: 597 loss: 2.31558192e-06
Iter: 598 loss: 2.31085596e-06
Iter: 599 loss: 2.31949957e-06
Iter: 600 loss: 2.30885644e-06
Iter: 601 loss: 2.3042046e-06
Iter: 602 loss: 2.33501123e-06
Iter: 603 loss: 2.3038458e-06
Iter: 604 loss: 2.29983425e-06
Iter: 605 loss: 2.29877469e-06
Iter: 606 loss: 2.29636817e-06
Iter: 607 loss: 2.29203715e-06
Iter: 608 loss: 2.34216509e-06
Iter: 609 loss: 2.29201441e-06
Iter: 610 loss: 2.28867748e-06
Iter: 611 loss: 2.29218904e-06
Iter: 612 loss: 2.28672661e-06
Iter: 613 loss: 2.28327531e-06
Iter: 614 loss: 2.28328349e-06
Iter: 615 loss: 2.28104795e-06
Iter: 616 loss: 2.27946475e-06
Iter: 617 loss: 2.27870305e-06
Iter: 618 loss: 2.27487362e-06
Iter: 619 loss: 2.28352405e-06
Iter: 620 loss: 2.27350392e-06
Iter: 621 loss: 2.2699187e-06
Iter: 622 loss: 2.27008491e-06
Iter: 623 loss: 2.26717316e-06
Iter: 624 loss: 2.26270822e-06
Iter: 625 loss: 2.27595206e-06
Iter: 626 loss: 2.26128691e-06
Iter: 627 loss: 2.25633767e-06
Iter: 628 loss: 2.28496765e-06
Iter: 629 loss: 2.25566282e-06
Iter: 630 loss: 2.25225131e-06
Iter: 631 loss: 2.25245367e-06
Iter: 632 loss: 2.24957967e-06
Iter: 633 loss: 2.24506675e-06
Iter: 634 loss: 2.27268811e-06
Iter: 635 loss: 2.24443397e-06
Iter: 636 loss: 2.24049199e-06
Iter: 637 loss: 2.24664745e-06
Iter: 638 loss: 2.23872075e-06
Iter: 639 loss: 2.23498137e-06
Iter: 640 loss: 2.24923974e-06
Iter: 641 loss: 2.23404186e-06
Iter: 642 loss: 2.23025199e-06
Iter: 643 loss: 2.24174391e-06
Iter: 644 loss: 2.22908466e-06
Iter: 645 loss: 2.22574317e-06
Iter: 646 loss: 2.26489851e-06
Iter: 647 loss: 2.22569406e-06
Iter: 648 loss: 2.22297695e-06
Iter: 649 loss: 2.22488529e-06
Iter: 650 loss: 2.22123936e-06
Iter: 651 loss: 2.21863434e-06
Iter: 652 loss: 2.22224753e-06
Iter: 653 loss: 2.21725941e-06
Iter: 654 loss: 2.2137574e-06
Iter: 655 loss: 2.21381674e-06
Iter: 656 loss: 2.21088476e-06
Iter: 657 loss: 2.20723746e-06
Iter: 658 loss: 2.21327446e-06
Iter: 659 loss: 2.2055774e-06
Iter: 660 loss: 2.20129186e-06
Iter: 661 loss: 2.23121606e-06
Iter: 662 loss: 2.20090192e-06
Iter: 663 loss: 2.19698359e-06
Iter: 664 loss: 2.19870276e-06
Iter: 665 loss: 2.1941928e-06
Iter: 666 loss: 2.19063372e-06
Iter: 667 loss: 2.20304355e-06
Iter: 668 loss: 2.18969353e-06
Iter: 669 loss: 2.18553737e-06
Iter: 670 loss: 2.19953517e-06
Iter: 671 loss: 2.184305e-06
Iter: 672 loss: 2.1812823e-06
Iter: 673 loss: 2.18740161e-06
Iter: 674 loss: 2.1800638e-06
Iter: 675 loss: 2.17629713e-06
Iter: 676 loss: 2.18866762e-06
Iter: 677 loss: 2.17520369e-06
Iter: 678 loss: 2.17261595e-06
Iter: 679 loss: 2.21432447e-06
Iter: 680 loss: 2.17259867e-06
Iter: 681 loss: 2.17053685e-06
Iter: 682 loss: 2.17189063e-06
Iter: 683 loss: 2.16912099e-06
Iter: 684 loss: 2.1665237e-06
Iter: 685 loss: 2.16707099e-06
Iter: 686 loss: 2.16466606e-06
Iter: 687 loss: 2.1613821e-06
Iter: 688 loss: 2.17437673e-06
Iter: 689 loss: 2.16061881e-06
Iter: 690 loss: 2.1582423e-06
Iter: 691 loss: 2.15548926e-06
Iter: 692 loss: 2.15512864e-06
Iter: 693 loss: 2.15154887e-06
Iter: 694 loss: 2.19932417e-06
Iter: 695 loss: 2.15157752e-06
Iter: 696 loss: 2.14869169e-06
Iter: 697 loss: 2.15266755e-06
Iter: 698 loss: 2.14726106e-06
Iter: 699 loss: 2.14407692e-06
Iter: 700 loss: 2.14430042e-06
Iter: 701 loss: 2.14157853e-06
Iter: 702 loss: 2.13816611e-06
Iter: 703 loss: 2.18030573e-06
Iter: 704 loss: 2.13813246e-06
Iter: 705 loss: 2.13574549e-06
Iter: 706 loss: 2.13581961e-06
Iter: 707 loss: 2.13383919e-06
Iter: 708 loss: 2.1302144e-06
Iter: 709 loss: 2.14311331e-06
Iter: 710 loss: 2.12932582e-06
Iter: 711 loss: 2.12633449e-06
Iter: 712 loss: 2.16398462e-06
Iter: 713 loss: 2.12635609e-06
Iter: 714 loss: 2.12398709e-06
Iter: 715 loss: 2.13085536e-06
Iter: 716 loss: 2.12331247e-06
Iter: 717 loss: 2.12123609e-06
Iter: 718 loss: 2.11866154e-06
Iter: 719 loss: 2.11841e-06
Iter: 720 loss: 2.11465454e-06
Iter: 721 loss: 2.13448425e-06
Iter: 722 loss: 2.11401925e-06
Iter: 723 loss: 2.11117367e-06
Iter: 724 loss: 2.11081442e-06
Iter: 725 loss: 2.10886697e-06
Iter: 726 loss: 2.10536291e-06
Iter: 727 loss: 2.11675388e-06
Iter: 728 loss: 2.10439839e-06
Iter: 729 loss: 2.10069311e-06
Iter: 730 loss: 2.12856389e-06
Iter: 731 loss: 2.10045914e-06
Iter: 732 loss: 2.09806922e-06
Iter: 733 loss: 2.09566724e-06
Iter: 734 loss: 2.0951843e-06
Iter: 735 loss: 2.09107611e-06
Iter: 736 loss: 2.11858355e-06
Iter: 737 loss: 2.09059317e-06
Iter: 738 loss: 2.08772462e-06
Iter: 739 loss: 2.10002418e-06
Iter: 740 loss: 2.08712868e-06
Iter: 741 loss: 2.08435404e-06
Iter: 742 loss: 2.08547817e-06
Iter: 743 loss: 2.08248525e-06
Iter: 744 loss: 2.08018355e-06
Iter: 745 loss: 2.08014785e-06
Iter: 746 loss: 2.07821085e-06
Iter: 747 loss: 2.08294318e-06
Iter: 748 loss: 2.077506e-06
Iter: 749 loss: 2.07532389e-06
Iter: 750 loss: 2.07354447e-06
Iter: 751 loss: 2.07287576e-06
Iter: 752 loss: 2.06992718e-06
Iter: 753 loss: 2.08610982e-06
Iter: 754 loss: 2.06952382e-06
Iter: 755 loss: 2.06711229e-06
Iter: 756 loss: 2.06745153e-06
Iter: 757 loss: 2.06520849e-06
Iter: 758 loss: 2.06219602e-06
Iter: 759 loss: 2.06857021e-06
Iter: 760 loss: 2.06098912e-06
Iter: 761 loss: 2.05856168e-06
Iter: 762 loss: 2.08489701e-06
Iter: 763 loss: 2.05850529e-06
Iter: 764 loss: 2.05618949e-06
Iter: 765 loss: 2.05323749e-06
Iter: 766 loss: 2.05303036e-06
Iter: 767 loss: 2.04933576e-06
Iter: 768 loss: 2.07027711e-06
Iter: 769 loss: 2.04886874e-06
Iter: 770 loss: 2.04588696e-06
Iter: 771 loss: 2.06134087e-06
Iter: 772 loss: 2.04547723e-06
Iter: 773 loss: 2.04282446e-06
Iter: 774 loss: 2.04442267e-06
Iter: 775 loss: 2.04097296e-06
Iter: 776 loss: 2.03865784e-06
Iter: 777 loss: 2.06998084e-06
Iter: 778 loss: 2.03864079e-06
Iter: 779 loss: 2.03656145e-06
Iter: 780 loss: 2.04415528e-06
Iter: 781 loss: 2.03607442e-06
Iter: 782 loss: 2.03394416e-06
Iter: 783 loss: 2.03429386e-06
Iter: 784 loss: 2.03239529e-06
Iter: 785 loss: 2.03001241e-06
Iter: 786 loss: 2.03386116e-06
Iter: 787 loss: 2.02894739e-06
Iter: 788 loss: 2.02600904e-06
Iter: 789 loss: 2.03119725e-06
Iter: 790 loss: 2.02470869e-06
Iter: 791 loss: 2.02215e-06
Iter: 792 loss: 2.02480192e-06
Iter: 793 loss: 2.0206935e-06
Iter: 794 loss: 2.01767534e-06
Iter: 795 loss: 2.03393529e-06
Iter: 796 loss: 2.0171974e-06
Iter: 797 loss: 2.01457397e-06
Iter: 798 loss: 2.02571914e-06
Iter: 799 loss: 2.01403827e-06
Iter: 800 loss: 2.01161106e-06
Iter: 801 loss: 2.00987074e-06
Iter: 802 loss: 2.00908335e-06
Iter: 803 loss: 2.00576051e-06
Iter: 804 loss: 2.02465526e-06
Iter: 805 loss: 2.00527256e-06
Iter: 806 loss: 2.00224099e-06
Iter: 807 loss: 2.01750754e-06
Iter: 808 loss: 2.00170916e-06
Iter: 809 loss: 1.99959936e-06
Iter: 810 loss: 2.00244108e-06
Iter: 811 loss: 1.99851956e-06
Iter: 812 loss: 1.99590067e-06
Iter: 813 loss: 2.02487104e-06
Iter: 814 loss: 1.99587248e-06
Iter: 815 loss: 1.9940444e-06
Iter: 816 loss: 1.99460328e-06
Iter: 817 loss: 1.9926897e-06
Iter: 818 loss: 1.99035048e-06
Iter: 819 loss: 1.99152191e-06
Iter: 820 loss: 1.9887425e-06
Iter: 821 loss: 1.98595308e-06
Iter: 822 loss: 1.99375222e-06
Iter: 823 loss: 1.98511179e-06
Iter: 824 loss: 1.98241105e-06
Iter: 825 loss: 1.98887892e-06
Iter: 826 loss: 1.98145722e-06
Iter: 827 loss: 1.97918689e-06
Iter: 828 loss: 1.9799611e-06
Iter: 829 loss: 1.97760892e-06
Iter: 830 loss: 1.97448162e-06
Iter: 831 loss: 2.00218301e-06
Iter: 832 loss: 1.97433155e-06
Iter: 833 loss: 1.97187364e-06
Iter: 834 loss: 1.97280906e-06
Iter: 835 loss: 1.97019472e-06
Iter: 836 loss: 1.96746123e-06
Iter: 837 loss: 1.97292229e-06
Iter: 838 loss: 1.96636393e-06
Iter: 839 loss: 1.9642298e-06
Iter: 840 loss: 1.99156693e-06
Iter: 841 loss: 1.96418659e-06
Iter: 842 loss: 1.96234714e-06
Iter: 843 loss: 1.96088968e-06
Iter: 844 loss: 1.96027509e-06
Iter: 845 loss: 1.95871462e-06
Iter: 846 loss: 1.95844518e-06
Iter: 847 loss: 1.95710891e-06
Iter: 848 loss: 1.95676103e-06
Iter: 849 loss: 1.95585699e-06
Iter: 850 loss: 1.95391021e-06
Iter: 851 loss: 1.95505163e-06
Iter: 852 loss: 1.95256689e-06
Iter: 853 loss: 1.95026678e-06
Iter: 854 loss: 1.95778512e-06
Iter: 855 loss: 1.94955464e-06
Iter: 856 loss: 1.94779136e-06
Iter: 857 loss: 1.95200596e-06
Iter: 858 loss: 1.94715403e-06
Iter: 859 loss: 1.94492031e-06
Iter: 860 loss: 1.94423706e-06
Iter: 861 loss: 1.94296285e-06
Iter: 862 loss: 1.94012273e-06
Iter: 863 loss: 1.95938674e-06
Iter: 864 loss: 1.93987444e-06
Iter: 865 loss: 1.93746564e-06
Iter: 866 loss: 1.94848167e-06
Iter: 867 loss: 1.93695109e-06
Iter: 868 loss: 1.93488e-06
Iter: 869 loss: 1.93372261e-06
Iter: 870 loss: 1.93275719e-06
Iter: 871 loss: 1.93030678e-06
Iter: 872 loss: 1.94707422e-06
Iter: 873 loss: 1.93007736e-06
Iter: 874 loss: 1.92776429e-06
Iter: 875 loss: 1.93631922e-06
Iter: 876 loss: 1.92717016e-06
Iter: 877 loss: 1.92510629e-06
Iter: 878 loss: 1.93606138e-06
Iter: 879 loss: 1.92473317e-06
Iter: 880 loss: 1.92246875e-06
Iter: 881 loss: 1.93269716e-06
Iter: 882 loss: 1.9220015e-06
Iter: 883 loss: 1.92066636e-06
Iter: 884 loss: 1.92019388e-06
Iter: 885 loss: 1.91947925e-06
Iter: 886 loss: 1.91712547e-06
Iter: 887 loss: 1.9199008e-06
Iter: 888 loss: 1.91581489e-06
Iter: 889 loss: 1.91387971e-06
Iter: 890 loss: 1.92148e-06
Iter: 891 loss: 1.91341064e-06
Iter: 892 loss: 1.91127128e-06
Iter: 893 loss: 1.91279059e-06
Iter: 894 loss: 1.91003232e-06
Iter: 895 loss: 1.90718038e-06
Iter: 896 loss: 1.90979017e-06
Iter: 897 loss: 1.90555011e-06
Iter: 898 loss: 1.90320816e-06
Iter: 899 loss: 1.90321e-06
Iter: 900 loss: 1.90126627e-06
Iter: 901 loss: 1.89967454e-06
Iter: 902 loss: 1.89910099e-06
Iter: 903 loss: 1.89695231e-06
Iter: 904 loss: 1.90645187e-06
Iter: 905 loss: 1.89657794e-06
Iter: 906 loss: 1.89444791e-06
Iter: 907 loss: 1.90745914e-06
Iter: 908 loss: 1.8942585e-06
Iter: 909 loss: 1.89267791e-06
Iter: 910 loss: 1.90122523e-06
Iter: 911 loss: 1.89240916e-06
Iter: 912 loss: 1.89096954e-06
Iter: 913 loss: 1.89943762e-06
Iter: 914 loss: 1.89081607e-06
Iter: 915 loss: 1.88960246e-06
Iter: 916 loss: 1.88744241e-06
Iter: 917 loss: 1.88745912e-06
Iter: 918 loss: 1.88501451e-06
Iter: 919 loss: 1.90328183e-06
Iter: 920 loss: 1.88484523e-06
Iter: 921 loss: 1.88333502e-06
Iter: 922 loss: 1.88349031e-06
Iter: 923 loss: 1.88216723e-06
Iter: 924 loss: 1.87984472e-06
Iter: 925 loss: 1.88436502e-06
Iter: 926 loss: 1.8788561e-06
Iter: 927 loss: 1.87649459e-06
Iter: 928 loss: 1.88580816e-06
Iter: 929 loss: 1.87590888e-06
Iter: 930 loss: 1.87385422e-06
Iter: 931 loss: 1.87779949e-06
Iter: 932 loss: 1.87294165e-06
Iter: 933 loss: 1.87052035e-06
Iter: 934 loss: 1.88476054e-06
Iter: 935 loss: 1.87027774e-06
Iter: 936 loss: 1.8685646e-06
Iter: 937 loss: 1.86920761e-06
Iter: 938 loss: 1.86743443e-06
Iter: 939 loss: 1.86531497e-06
Iter: 940 loss: 1.8668004e-06
Iter: 941 loss: 1.86403042e-06
Iter: 942 loss: 1.8621223e-06
Iter: 943 loss: 1.8620417e-06
Iter: 944 loss: 1.86085549e-06
Iter: 945 loss: 1.86787861e-06
Iter: 946 loss: 1.86074283e-06
Iter: 947 loss: 1.85949136e-06
Iter: 948 loss: 1.85784279e-06
Iter: 949 loss: 1.8577241e-06
Iter: 950 loss: 1.85563192e-06
Iter: 951 loss: 1.86001762e-06
Iter: 952 loss: 1.85481952e-06
Iter: 953 loss: 1.8526539e-06
Iter: 954 loss: 1.86227282e-06
Iter: 955 loss: 1.85216709e-06
Iter: 956 loss: 1.85046451e-06
Iter: 957 loss: 1.84874466e-06
Iter: 958 loss: 1.84840746e-06
Iter: 959 loss: 1.84605437e-06
Iter: 960 loss: 1.87754949e-06
Iter: 961 loss: 1.8460521e-06
Iter: 962 loss: 1.84434043e-06
Iter: 963 loss: 1.84466842e-06
Iter: 964 loss: 1.84309874e-06
Iter: 965 loss: 1.84079329e-06
Iter: 966 loss: 1.85223894e-06
Iter: 967 loss: 1.84045246e-06
Iter: 968 loss: 1.83831537e-06
Iter: 969 loss: 1.84250439e-06
Iter: 970 loss: 1.83740485e-06
Iter: 971 loss: 1.8354292e-06
Iter: 972 loss: 1.83496297e-06
Iter: 973 loss: 1.83377642e-06
Iter: 974 loss: 1.83238092e-06
Iter: 975 loss: 1.832267e-06
Iter: 976 loss: 1.83088559e-06
Iter: 977 loss: 1.83402358e-06
Iter: 978 loss: 1.8304072e-06
Iter: 979 loss: 1.82863414e-06
Iter: 980 loss: 1.83189889e-06
Iter: 981 loss: 1.82788131e-06
Iter: 982 loss: 1.82652343e-06
Iter: 983 loss: 1.82618726e-06
Iter: 984 loss: 1.82532665e-06
Iter: 985 loss: 1.82316842e-06
Iter: 986 loss: 1.83062946e-06
Iter: 987 loss: 1.82261147e-06
Iter: 988 loss: 1.82049985e-06
Iter: 989 loss: 1.82054328e-06
Iter: 990 loss: 1.81884161e-06
Iter: 991 loss: 1.81671783e-06
Iter: 992 loss: 1.82833242e-06
Iter: 993 loss: 1.8164028e-06
Iter: 994 loss: 1.81422524e-06
Iter: 995 loss: 1.81986093e-06
Iter: 996 loss: 1.81349697e-06
Iter: 997 loss: 1.81142411e-06
Iter: 998 loss: 1.81650421e-06
Iter: 999 loss: 1.81072141e-06
Iter: 1000 loss: 1.80864811e-06
Iter: 1001 loss: 1.81893643e-06
Iter: 1002 loss: 1.80831967e-06
Iter: 1003 loss: 1.80660902e-06
Iter: 1004 loss: 1.80501445e-06
Iter: 1005 loss: 1.8046469e-06
Iter: 1006 loss: 1.80262782e-06
Iter: 1007 loss: 1.82854524e-06
Iter: 1008 loss: 1.80265465e-06
Iter: 1009 loss: 1.80111488e-06
Iter: 1010 loss: 1.81515668e-06
Iter: 1011 loss: 1.80101654e-06
Iter: 1012 loss: 1.7998226e-06
Iter: 1013 loss: 1.802322e-06
Iter: 1014 loss: 1.79933954e-06
Iter: 1015 loss: 1.7980949e-06
Iter: 1016 loss: 1.79759775e-06
Iter: 1017 loss: 1.79695451e-06
Iter: 1018 loss: 1.79522897e-06
Iter: 1019 loss: 1.79959898e-06
Iter: 1020 loss: 1.79463427e-06
Iter: 1021 loss: 1.79272934e-06
Iter: 1022 loss: 1.79766971e-06
Iter: 1023 loss: 1.79207905e-06
Iter: 1024 loss: 1.79033123e-06
Iter: 1025 loss: 1.79038489e-06
Iter: 1026 loss: 1.78892788e-06
Iter: 1027 loss: 1.78685855e-06
Iter: 1028 loss: 1.80211975e-06
Iter: 1029 loss: 1.78672838e-06
Iter: 1030 loss: 1.78507025e-06
Iter: 1031 loss: 1.79088659e-06
Iter: 1032 loss: 1.78462858e-06
Iter: 1033 loss: 1.78291771e-06
Iter: 1034 loss: 1.78414507e-06
Iter: 1035 loss: 1.78192954e-06
Iter: 1036 loss: 1.77990273e-06
Iter: 1037 loss: 1.79224696e-06
Iter: 1038 loss: 1.7796732e-06
Iter: 1039 loss: 1.77827803e-06
Iter: 1040 loss: 1.77771119e-06
Iter: 1041 loss: 1.77696779e-06
Iter: 1042 loss: 1.7754237e-06
Iter: 1043 loss: 1.77542779e-06
Iter: 1044 loss: 1.77392337e-06
Iter: 1045 loss: 1.77901961e-06
Iter: 1046 loss: 1.77357481e-06
Iter: 1047 loss: 1.77255401e-06
Iter: 1048 loss: 1.7732948e-06
Iter: 1049 loss: 1.7718844e-06
Iter: 1050 loss: 1.77031734e-06
Iter: 1051 loss: 1.76889421e-06
Iter: 1052 loss: 1.76851336e-06
Iter: 1053 loss: 1.76687627e-06
Iter: 1054 loss: 1.78610526e-06
Iter: 1055 loss: 1.76686763e-06
Iter: 1056 loss: 1.76537947e-06
Iter: 1057 loss: 1.76388062e-06
Iter: 1058 loss: 1.76360345e-06
Iter: 1059 loss: 1.76137962e-06
Iter: 1060 loss: 1.76803246e-06
Iter: 1061 loss: 1.76078561e-06
Iter: 1062 loss: 1.75877324e-06
Iter: 1063 loss: 1.77388563e-06
Iter: 1064 loss: 1.75859907e-06
Iter: 1065 loss: 1.75693481e-06
Iter: 1066 loss: 1.75963692e-06
Iter: 1067 loss: 1.75614662e-06
Iter: 1068 loss: 1.7545799e-06
Iter: 1069 loss: 1.76003198e-06
Iter: 1070 loss: 1.75409195e-06
Iter: 1071 loss: 1.7523821e-06
Iter: 1072 loss: 1.75340199e-06
Iter: 1073 loss: 1.75126229e-06
Iter: 1074 loss: 1.74937259e-06
Iter: 1075 loss: 1.75800847e-06
Iter: 1076 loss: 1.74895717e-06
Iter: 1077 loss: 1.74749107e-06
Iter: 1078 loss: 1.74749539e-06
Iter: 1079 loss: 1.74665502e-06
Iter: 1080 loss: 1.74553e-06
Iter: 1081 loss: 1.74552747e-06
Iter: 1082 loss: 1.7436746e-06
Iter: 1083 loss: 1.74867284e-06
Iter: 1084 loss: 1.74307081e-06
Iter: 1085 loss: 1.74179195e-06
Iter: 1086 loss: 1.74230559e-06
Iter: 1087 loss: 1.74098773e-06
Iter: 1088 loss: 1.73885155e-06
Iter: 1089 loss: 1.74453373e-06
Iter: 1090 loss: 1.73813e-06
Iter: 1091 loss: 1.73658213e-06
Iter: 1092 loss: 1.73951616e-06
Iter: 1093 loss: 1.73590513e-06
Iter: 1094 loss: 1.73437752e-06
Iter: 1095 loss: 1.73868648e-06
Iter: 1096 loss: 1.73389026e-06
Iter: 1097 loss: 1.73222179e-06
Iter: 1098 loss: 1.74296235e-06
Iter: 1099 loss: 1.73202852e-06
Iter: 1100 loss: 1.7308862e-06
Iter: 1101 loss: 1.73167643e-06
Iter: 1102 loss: 1.73010324e-06
Iter: 1103 loss: 1.72833836e-06
Iter: 1104 loss: 1.72920852e-06
Iter: 1105 loss: 1.72712214e-06
Iter: 1106 loss: 1.72526325e-06
Iter: 1107 loss: 1.74206878e-06
Iter: 1108 loss: 1.72517775e-06
Iter: 1109 loss: 1.72433874e-06
Iter: 1110 loss: 1.7242761e-06
Iter: 1111 loss: 1.72340663e-06
Iter: 1112 loss: 1.72159469e-06
Iter: 1113 loss: 1.75366938e-06
Iter: 1114 loss: 1.72154807e-06
Iter: 1115 loss: 1.71977194e-06
Iter: 1116 loss: 1.73521789e-06
Iter: 1117 loss: 1.71966178e-06
Iter: 1118 loss: 1.71833949e-06
Iter: 1119 loss: 1.71858505e-06
Iter: 1120 loss: 1.71731904e-06
Iter: 1121 loss: 1.71563329e-06
Iter: 1122 loss: 1.71870261e-06
Iter: 1123 loss: 1.71482225e-06
Iter: 1124 loss: 1.713149e-06
Iter: 1125 loss: 1.72397745e-06
Iter: 1126 loss: 1.71288684e-06
Iter: 1127 loss: 1.71144791e-06
Iter: 1128 loss: 1.71053784e-06
Iter: 1129 loss: 1.70999397e-06
Iter: 1130 loss: 1.70808869e-06
Iter: 1131 loss: 1.71827332e-06
Iter: 1132 loss: 1.70784506e-06
Iter: 1133 loss: 1.70614339e-06
Iter: 1134 loss: 1.71917588e-06
Iter: 1135 loss: 1.7059773e-06
Iter: 1136 loss: 1.70477801e-06
Iter: 1137 loss: 1.70349176e-06
Iter: 1138 loss: 1.70328076e-06
Iter: 1139 loss: 1.70154124e-06
Iter: 1140 loss: 1.71539978e-06
Iter: 1141 loss: 1.70139606e-06
Iter: 1142 loss: 1.7000566e-06
Iter: 1143 loss: 1.70996884e-06
Iter: 1144 loss: 1.69988255e-06
Iter: 1145 loss: 1.6984651e-06
Iter: 1146 loss: 1.7034813e-06
Iter: 1147 loss: 1.69806435e-06
Iter: 1148 loss: 1.69714406e-06
Iter: 1149 loss: 1.69669011e-06
Iter: 1150 loss: 1.69619682e-06
Iter: 1151 loss: 1.69494297e-06
Iter: 1152 loss: 1.70246585e-06
Iter: 1153 loss: 1.69487544e-06
Iter: 1154 loss: 1.69365171e-06
Iter: 1155 loss: 1.6923791e-06
Iter: 1156 loss: 1.69211876e-06
Iter: 1157 loss: 1.69054783e-06
Iter: 1158 loss: 1.70144028e-06
Iter: 1159 loss: 1.69037401e-06
Iter: 1160 loss: 1.68878296e-06
Iter: 1161 loss: 1.68949714e-06
Iter: 1162 loss: 1.6876478e-06
Iter: 1163 loss: 1.68618158e-06
Iter: 1164 loss: 1.69063401e-06
Iter: 1165 loss: 1.68574024e-06
Iter: 1166 loss: 1.68413339e-06
Iter: 1167 loss: 1.68800716e-06
Iter: 1168 loss: 1.68357178e-06
Iter: 1169 loss: 1.68143822e-06
Iter: 1170 loss: 1.68646227e-06
Iter: 1171 loss: 1.68065355e-06
Iter: 1172 loss: 1.67910389e-06
Iter: 1173 loss: 1.68261886e-06
Iter: 1174 loss: 1.67850021e-06
Iter: 1175 loss: 1.67709572e-06
Iter: 1176 loss: 1.68538861e-06
Iter: 1177 loss: 1.67692e-06
Iter: 1178 loss: 1.67559745e-06
Iter: 1179 loss: 1.69065015e-06
Iter: 1180 loss: 1.67560052e-06
Iter: 1181 loss: 1.67492612e-06
Iter: 1182 loss: 1.67436497e-06
Iter: 1183 loss: 1.6741235e-06
Iter: 1184 loss: 1.67303e-06
Iter: 1185 loss: 1.67391568e-06
Iter: 1186 loss: 1.67229325e-06
Iter: 1187 loss: 1.67077076e-06
Iter: 1188 loss: 1.67783492e-06
Iter: 1189 loss: 1.67046323e-06
Iter: 1190 loss: 1.66916254e-06
Iter: 1191 loss: 1.67023677e-06
Iter: 1192 loss: 1.66830023e-06
Iter: 1193 loss: 1.66702227e-06
Iter: 1194 loss: 1.67004259e-06
Iter: 1195 loss: 1.66655298e-06
Iter: 1196 loss: 1.6650381e-06
Iter: 1197 loss: 1.67098176e-06
Iter: 1198 loss: 1.66469715e-06
Iter: 1199 loss: 1.66322036e-06
Iter: 1200 loss: 1.66291989e-06
Iter: 1201 loss: 1.66193638e-06
Iter: 1202 loss: 1.66037853e-06
Iter: 1203 loss: 1.67309827e-06
Iter: 1204 loss: 1.66027849e-06
Iter: 1205 loss: 1.65880488e-06
Iter: 1206 loss: 1.66532186e-06
Iter: 1207 loss: 1.65849315e-06
Iter: 1208 loss: 1.65735196e-06
Iter: 1209 loss: 1.65587426e-06
Iter: 1210 loss: 1.65582014e-06
Iter: 1211 loss: 1.65579502e-06
Iter: 1212 loss: 1.65504866e-06
Iter: 1213 loss: 1.65419442e-06
Iter: 1214 loss: 1.6529093e-06
Iter: 1215 loss: 1.65290021e-06
Iter: 1216 loss: 1.65159099e-06
Iter: 1217 loss: 1.65345546e-06
Iter: 1218 loss: 1.65095571e-06
Iter: 1219 loss: 1.64977246e-06
Iter: 1220 loss: 1.66137193e-06
Iter: 1221 loss: 1.64978144e-06
Iter: 1222 loss: 1.6487661e-06
Iter: 1223 loss: 1.64816879e-06
Iter: 1224 loss: 1.64772155e-06
Iter: 1225 loss: 1.64641528e-06
Iter: 1226 loss: 1.65443612e-06
Iter: 1227 loss: 1.64625658e-06
Iter: 1228 loss: 1.64510197e-06
Iter: 1229 loss: 1.6450897e-06
Iter: 1230 loss: 1.64418702e-06
Iter: 1231 loss: 1.64270182e-06
Iter: 1232 loss: 1.64958476e-06
Iter: 1233 loss: 1.64239145e-06
Iter: 1234 loss: 1.64122071e-06
Iter: 1235 loss: 1.64545486e-06
Iter: 1236 loss: 1.64089249e-06
Iter: 1237 loss: 1.63961033e-06
Iter: 1238 loss: 1.64297069e-06
Iter: 1239 loss: 1.63918628e-06
Iter: 1240 loss: 1.63812615e-06
Iter: 1241 loss: 1.64074879e-06
Iter: 1242 loss: 1.63769914e-06
Iter: 1243 loss: 1.63630307e-06
Iter: 1244 loss: 1.63933908e-06
Iter: 1245 loss: 1.63576169e-06
Iter: 1246 loss: 1.63456025e-06
Iter: 1247 loss: 1.63688424e-06
Iter: 1248 loss: 1.63405048e-06
Iter: 1249 loss: 1.63327331e-06
Iter: 1250 loss: 1.6331694e-06
Iter: 1251 loss: 1.63257801e-06
Iter: 1252 loss: 1.63104676e-06
Iter: 1253 loss: 1.64263065e-06
Iter: 1254 loss: 1.63075765e-06
Iter: 1255 loss: 1.62945128e-06
Iter: 1256 loss: 1.6327765e-06
Iter: 1257 loss: 1.62901642e-06
Iter: 1258 loss: 1.62751235e-06
Iter: 1259 loss: 1.63381412e-06
Iter: 1260 loss: 1.62713206e-06
Iter: 1261 loss: 1.62589583e-06
Iter: 1262 loss: 1.63734717e-06
Iter: 1263 loss: 1.62583819e-06
Iter: 1264 loss: 1.6250699e-06
Iter: 1265 loss: 1.62602578e-06
Iter: 1266 loss: 1.6246629e-06
Iter: 1267 loss: 1.62354968e-06
Iter: 1268 loss: 1.62473e-06
Iter: 1269 loss: 1.62297715e-06
Iter: 1270 loss: 1.62184108e-06
Iter: 1271 loss: 1.6254819e-06
Iter: 1272 loss: 1.62153265e-06
Iter: 1273 loss: 1.62055483e-06
Iter: 1274 loss: 1.62234426e-06
Iter: 1275 loss: 1.62019501e-06
Iter: 1276 loss: 1.61889579e-06
Iter: 1277 loss: 1.62178617e-06
Iter: 1278 loss: 1.61841069e-06
Iter: 1279 loss: 1.61733647e-06
Iter: 1280 loss: 1.62360197e-06
Iter: 1281 loss: 1.61716707e-06
Iter: 1282 loss: 1.61651712e-06
Iter: 1283 loss: 1.62280037e-06
Iter: 1284 loss: 1.61649552e-06
Iter: 1285 loss: 1.61568789e-06
Iter: 1286 loss: 1.61683045e-06
Iter: 1287 loss: 1.61538935e-06
Iter: 1288 loss: 1.61458081e-06
Iter: 1289 loss: 1.6158881e-06
Iter: 1290 loss: 1.61428011e-06
Iter: 1291 loss: 1.61362834e-06
Iter: 1292 loss: 1.61232754e-06
Iter: 1293 loss: 1.63375182e-06
Iter: 1294 loss: 1.61227888e-06
Iter: 1295 loss: 1.6108454e-06
Iter: 1296 loss: 1.62205572e-06
Iter: 1297 loss: 1.61079618e-06
Iter: 1298 loss: 1.6096875e-06
Iter: 1299 loss: 1.61673381e-06
Iter: 1300 loss: 1.6095828e-06
Iter: 1301 loss: 1.60849731e-06
Iter: 1302 loss: 1.61126684e-06
Iter: 1303 loss: 1.60811533e-06
Iter: 1304 loss: 1.60737784e-06
Iter: 1305 loss: 1.60714671e-06
Iter: 1306 loss: 1.60667685e-06
Iter: 1307 loss: 1.60550121e-06
Iter: 1308 loss: 1.61092225e-06
Iter: 1309 loss: 1.60526088e-06
Iter: 1310 loss: 1.6041771e-06
Iter: 1311 loss: 1.60796367e-06
Iter: 1312 loss: 1.6038656e-06
Iter: 1313 loss: 1.60288027e-06
Iter: 1314 loss: 1.60373986e-06
Iter: 1315 loss: 1.6023198e-06
Iter: 1316 loss: 1.60145123e-06
Iter: 1317 loss: 1.61209834e-06
Iter: 1318 loss: 1.6014535e-06
Iter: 1319 loss: 1.60065701e-06
Iter: 1320 loss: 1.60414879e-06
Iter: 1321 loss: 1.60053412e-06
Iter: 1322 loss: 1.59985461e-06
Iter: 1323 loss: 1.5999367e-06
Iter: 1324 loss: 1.59936167e-06
Iter: 1325 loss: 1.59843842e-06
Iter: 1326 loss: 1.5981575e-06
Iter: 1327 loss: 1.59761692e-06
Iter: 1328 loss: 1.59642e-06
Iter: 1329 loss: 1.59629644e-06
Iter: 1330 loss: 1.59541696e-06
Iter: 1331 loss: 1.59388787e-06
Iter: 1332 loss: 1.60608602e-06
Iter: 1333 loss: 1.593722e-06
Iter: 1334 loss: 1.59296053e-06
Iter: 1335 loss: 1.59294655e-06
Iter: 1336 loss: 1.59236242e-06
Iter: 1337 loss: 1.59146896e-06
Iter: 1338 loss: 1.59142e-06
Iter: 1339 loss: 1.59024762e-06
Iter: 1340 loss: 1.59657179e-06
Iter: 1341 loss: 1.59008903e-06
Iter: 1342 loss: 1.58909074e-06
Iter: 1343 loss: 1.59149863e-06
Iter: 1344 loss: 1.58873183e-06
Iter: 1345 loss: 1.58778448e-06
Iter: 1346 loss: 1.58976025e-06
Iter: 1347 loss: 1.58741068e-06
Iter: 1348 loss: 1.58640114e-06
Iter: 1349 loss: 1.59234276e-06
Iter: 1350 loss: 1.5862513e-06
Iter: 1351 loss: 1.58557646e-06
Iter: 1352 loss: 1.59430238e-06
Iter: 1353 loss: 1.58555736e-06
Iter: 1354 loss: 1.58494822e-06
Iter: 1355 loss: 1.58438922e-06
Iter: 1356 loss: 1.58419743e-06
Iter: 1357 loss: 1.58327816e-06
Iter: 1358 loss: 1.58438684e-06
Iter: 1359 loss: 1.5827768e-06
Iter: 1360 loss: 1.58180183e-06
Iter: 1361 loss: 1.58563182e-06
Iter: 1362 loss: 1.58156672e-06
Iter: 1363 loss: 1.58069315e-06
Iter: 1364 loss: 1.5816463e-06
Iter: 1365 loss: 1.58012676e-06
Iter: 1366 loss: 1.57910915e-06
Iter: 1367 loss: 1.5790074e-06
Iter: 1368 loss: 1.57826275e-06
Iter: 1369 loss: 1.57710599e-06
Iter: 1370 loss: 1.57709724e-06
Iter: 1371 loss: 1.57616114e-06
Iter: 1372 loss: 1.57652335e-06
Iter: 1373 loss: 1.57549334e-06
Iter: 1374 loss: 1.57440445e-06
Iter: 1375 loss: 1.57411057e-06
Iter: 1376 loss: 1.57336274e-06
Iter: 1377 loss: 1.57217369e-06
Iter: 1378 loss: 1.57918782e-06
Iter: 1379 loss: 1.57202885e-06
Iter: 1380 loss: 1.57066893e-06
Iter: 1381 loss: 1.57518502e-06
Iter: 1382 loss: 1.5702974e-06
Iter: 1383 loss: 1.5694219e-06
Iter: 1384 loss: 1.58180364e-06
Iter: 1385 loss: 1.56940791e-06
Iter: 1386 loss: 1.56866872e-06
Iter: 1387 loss: 1.56970179e-06
Iter: 1388 loss: 1.56823512e-06
Iter: 1389 loss: 1.56757096e-06
Iter: 1390 loss: 1.56698513e-06
Iter: 1391 loss: 1.56679221e-06
Iter: 1392 loss: 1.56553324e-06
Iter: 1393 loss: 1.57026238e-06
Iter: 1394 loss: 1.56523538e-06
Iter: 1395 loss: 1.56418923e-06
Iter: 1396 loss: 1.56586714e-06
Iter: 1397 loss: 1.56369856e-06
Iter: 1398 loss: 1.56254487e-06
Iter: 1399 loss: 1.56467104e-06
Iter: 1400 loss: 1.56201156e-06
Iter: 1401 loss: 1.56099622e-06
Iter: 1402 loss: 1.56904434e-06
Iter: 1403 loss: 1.56090164e-06
Iter: 1404 loss: 1.56010378e-06
Iter: 1405 loss: 1.56162764e-06
Iter: 1406 loss: 1.55976818e-06
Iter: 1407 loss: 1.55871419e-06
Iter: 1408 loss: 1.56093847e-06
Iter: 1409 loss: 1.5582882e-06
Iter: 1410 loss: 1.55728856e-06
Iter: 1411 loss: 1.55763951e-06
Iter: 1412 loss: 1.55655289e-06
Iter: 1413 loss: 1.55531075e-06
Iter: 1414 loss: 1.55699649e-06
Iter: 1415 loss: 1.55476982e-06
Iter: 1416 loss: 1.55378029e-06
Iter: 1417 loss: 1.55379632e-06
Iter: 1418 loss: 1.55279633e-06
Iter: 1419 loss: 1.55708926e-06
Iter: 1420 loss: 1.55261273e-06
Iter: 1421 loss: 1.55194584e-06
Iter: 1422 loss: 1.55163104e-06
Iter: 1423 loss: 1.55123712e-06
Iter: 1424 loss: 1.55011799e-06
Iter: 1425 loss: 1.55107625e-06
Iter: 1426 loss: 1.5493888e-06
Iter: 1427 loss: 1.54827626e-06
Iter: 1428 loss: 1.55049315e-06
Iter: 1429 loss: 1.54778104e-06
Iter: 1430 loss: 1.54654629e-06
Iter: 1431 loss: 1.55581688e-06
Iter: 1432 loss: 1.54640725e-06
Iter: 1433 loss: 1.54567101e-06
Iter: 1434 loss: 1.5456568e-06
Iter: 1435 loss: 1.54494671e-06
Iter: 1436 loss: 1.54358304e-06
Iter: 1437 loss: 1.54489248e-06
Iter: 1438 loss: 1.54274358e-06
Iter: 1439 loss: 1.54148643e-06
Iter: 1440 loss: 1.54145971e-06
Iter: 1441 loss: 1.54062889e-06
Iter: 1442 loss: 1.53927704e-06
Iter: 1443 loss: 1.53928954e-06
Iter: 1444 loss: 1.53769486e-06
Iter: 1445 loss: 1.54444274e-06
Iter: 1446 loss: 1.53740439e-06
Iter: 1447 loss: 1.53628e-06
Iter: 1448 loss: 1.54160728e-06
Iter: 1449 loss: 1.53604196e-06
Iter: 1450 loss: 1.53505368e-06
Iter: 1451 loss: 1.54994291e-06
Iter: 1452 loss: 1.53503447e-06
Iter: 1453 loss: 1.53427482e-06
Iter: 1454 loss: 1.53386191e-06
Iter: 1455 loss: 1.53352335e-06
Iter: 1456 loss: 1.53254814e-06
Iter: 1457 loss: 1.53278086e-06
Iter: 1458 loss: 1.53175756e-06
Iter: 1459 loss: 1.53054839e-06
Iter: 1460 loss: 1.53480164e-06
Iter: 1461 loss: 1.53027258e-06
Iter: 1462 loss: 1.52895041e-06
Iter: 1463 loss: 1.53239228e-06
Iter: 1464 loss: 1.52850487e-06
Iter: 1465 loss: 1.52752932e-06
Iter: 1466 loss: 1.52896314e-06
Iter: 1467 loss: 1.52709708e-06
Iter: 1468 loss: 1.52584039e-06
Iter: 1469 loss: 1.53146743e-06
Iter: 1470 loss: 1.525578e-06
Iter: 1471 loss: 1.52450582e-06
Iter: 1472 loss: 1.52681253e-06
Iter: 1473 loss: 1.52408165e-06
Iter: 1474 loss: 1.52289124e-06
Iter: 1475 loss: 1.52716166e-06
Iter: 1476 loss: 1.52264033e-06
Iter: 1477 loss: 1.52140865e-06
Iter: 1478 loss: 1.52322491e-06
Iter: 1479 loss: 1.52082794e-06
Iter: 1480 loss: 1.51968902e-06
Iter: 1481 loss: 1.52111807e-06
Iter: 1482 loss: 1.51911991e-06
Iter: 1483 loss: 1.51808786e-06
Iter: 1484 loss: 1.5268779e-06
Iter: 1485 loss: 1.51800702e-06
Iter: 1486 loss: 1.51676772e-06
Iter: 1487 loss: 1.52153621e-06
Iter: 1488 loss: 1.5164843e-06
Iter: 1489 loss: 1.51583617e-06
Iter: 1490 loss: 1.51438155e-06
Iter: 1491 loss: 1.53938959e-06
Iter: 1492 loss: 1.51427832e-06
Iter: 1493 loss: 1.51296035e-06
Iter: 1494 loss: 1.5284063e-06
Iter: 1495 loss: 1.51289623e-06
Iter: 1496 loss: 1.51181086e-06
Iter: 1497 loss: 1.51309132e-06
Iter: 1498 loss: 1.51127824e-06
Iter: 1499 loss: 1.51002644e-06
Iter: 1500 loss: 1.51101267e-06
Iter: 1501 loss: 1.50934852e-06
Iter: 1502 loss: 1.50808091e-06
Iter: 1503 loss: 1.51992617e-06
Iter: 1504 loss: 1.50800338e-06
Iter: 1505 loss: 1.50695064e-06
Iter: 1506 loss: 1.5060607e-06
Iter: 1507 loss: 1.50576761e-06
Iter: 1508 loss: 1.50413302e-06
Iter: 1509 loss: 1.52349e-06
Iter: 1510 loss: 1.50408971e-06
Iter: 1511 loss: 1.50304072e-06
Iter: 1512 loss: 1.50467849e-06
Iter: 1513 loss: 1.5025912e-06
Iter: 1514 loss: 1.5015496e-06
Iter: 1515 loss: 1.50373376e-06
Iter: 1516 loss: 1.5011376e-06
Iter: 1517 loss: 1.50010351e-06
Iter: 1518 loss: 1.50335825e-06
Iter: 1519 loss: 1.4997247e-06
Iter: 1520 loss: 1.49894345e-06
Iter: 1521 loss: 1.4989389e-06
Iter: 1522 loss: 1.49826906e-06
Iter: 1523 loss: 1.49711605e-06
Iter: 1524 loss: 1.49710911e-06
Iter: 1525 loss: 1.49596849e-06
Iter: 1526 loss: 1.49629898e-06
Iter: 1527 loss: 1.49516131e-06
Iter: 1528 loss: 1.49370067e-06
Iter: 1529 loss: 1.50062397e-06
Iter: 1530 loss: 1.49343327e-06
Iter: 1531 loss: 1.49221592e-06
Iter: 1532 loss: 1.50047481e-06
Iter: 1533 loss: 1.49212462e-06
Iter: 1534 loss: 1.49121479e-06
Iter: 1535 loss: 1.49032167e-06
Iter: 1536 loss: 1.49012931e-06
Iter: 1537 loss: 1.48862978e-06
Iter: 1538 loss: 1.49573179e-06
Iter: 1539 loss: 1.48831964e-06
Iter: 1540 loss: 1.48696245e-06
Iter: 1541 loss: 1.49837251e-06
Iter: 1542 loss: 1.48684217e-06
Iter: 1543 loss: 1.48594199e-06
Iter: 1544 loss: 1.48583899e-06
Iter: 1545 loss: 1.4851297e-06
Iter: 1546 loss: 1.48381469e-06
Iter: 1547 loss: 1.49351695e-06
Iter: 1548 loss: 1.48366917e-06
Iter: 1549 loss: 1.48280174e-06
Iter: 1550 loss: 1.48233494e-06
Iter: 1551 loss: 1.48194886e-06
Iter: 1552 loss: 1.4809641e-06
Iter: 1553 loss: 1.48098729e-06
Iter: 1554 loss: 1.47989613e-06
Iter: 1555 loss: 1.4797472e-06
Iter: 1556 loss: 1.47896253e-06
Iter: 1557 loss: 1.47792775e-06
Iter: 1558 loss: 1.47895207e-06
Iter: 1559 loss: 1.47736785e-06
Iter: 1560 loss: 1.47609558e-06
Iter: 1561 loss: 1.47608409e-06
Iter: 1562 loss: 1.4751032e-06
Iter: 1563 loss: 1.47378955e-06
Iter: 1564 loss: 1.48545701e-06
Iter: 1565 loss: 1.47374521e-06
Iter: 1566 loss: 1.47267542e-06
Iter: 1567 loss: 1.47499088e-06
Iter: 1568 loss: 1.47223523e-06
Iter: 1569 loss: 1.47089781e-06
Iter: 1570 loss: 1.47151457e-06
Iter: 1571 loss: 1.46993796e-06
Iter: 1572 loss: 1.46853813e-06
Iter: 1573 loss: 1.47529465e-06
Iter: 1574 loss: 1.46831439e-06
Iter: 1575 loss: 1.46703655e-06
Iter: 1576 loss: 1.47363403e-06
Iter: 1577 loss: 1.46684272e-06
Iter: 1578 loss: 1.46560762e-06
Iter: 1579 loss: 1.46615866e-06
Iter: 1580 loss: 1.46474383e-06
Iter: 1581 loss: 1.46345906e-06
Iter: 1582 loss: 1.46720572e-06
Iter: 1583 loss: 1.46301704e-06
Iter: 1584 loss: 1.46152217e-06
Iter: 1585 loss: 1.46907723e-06
Iter: 1586 loss: 1.46132152e-06
Iter: 1587 loss: 1.46052651e-06
Iter: 1588 loss: 1.46048933e-06
Iter: 1589 loss: 1.45994136e-06
Iter: 1590 loss: 1.45849117e-06
Iter: 1591 loss: 1.46956904e-06
Iter: 1592 loss: 1.45823685e-06
Iter: 1593 loss: 1.45678064e-06
Iter: 1594 loss: 1.46224761e-06
Iter: 1595 loss: 1.45643378e-06
Iter: 1596 loss: 1.45517515e-06
Iter: 1597 loss: 1.46282184e-06
Iter: 1598 loss: 1.45503316e-06
Iter: 1599 loss: 1.45382216e-06
Iter: 1600 loss: 1.4547245e-06
Iter: 1601 loss: 1.45310696e-06
Iter: 1602 loss: 1.45186345e-06
Iter: 1603 loss: 1.46000298e-06
Iter: 1604 loss: 1.4517193e-06
Iter: 1605 loss: 1.45057379e-06
Iter: 1606 loss: 1.45176682e-06
Iter: 1607 loss: 1.44996318e-06
Iter: 1608 loss: 1.44866044e-06
Iter: 1609 loss: 1.45136266e-06
Iter: 1610 loss: 1.44815101e-06
Iter: 1611 loss: 1.4469806e-06
Iter: 1612 loss: 1.46002935e-06
Iter: 1613 loss: 1.44695809e-06
Iter: 1614 loss: 1.44618298e-06
Iter: 1615 loss: 1.44480691e-06
Iter: 1616 loss: 1.44482885e-06
Iter: 1617 loss: 1.44350065e-06
Iter: 1618 loss: 1.4631371e-06
Iter: 1619 loss: 1.44349781e-06
Iter: 1620 loss: 1.44276328e-06
Iter: 1621 loss: 1.45088757e-06
Iter: 1622 loss: 1.44276851e-06
Iter: 1623 loss: 1.44208036e-06
Iter: 1624 loss: 1.4413821e-06
Iter: 1625 loss: 1.44123123e-06
Iter: 1626 loss: 1.43999205e-06
Iter: 1627 loss: 1.44029764e-06
Iter: 1628 loss: 1.43916475e-06
Iter: 1629 loss: 1.43807551e-06
Iter: 1630 loss: 1.4388288e-06
Iter: 1631 loss: 1.43740567e-06
Iter: 1632 loss: 1.43583418e-06
Iter: 1633 loss: 1.43857574e-06
Iter: 1634 loss: 1.43511727e-06
Iter: 1635 loss: 1.43373745e-06
Iter: 1636 loss: 1.45459308e-06
Iter: 1637 loss: 1.43375291e-06
Iter: 1638 loss: 1.43275156e-06
Iter: 1639 loss: 1.43203579e-06
Iter: 1640 loss: 1.43160491e-06
Iter: 1641 loss: 1.43026045e-06
Iter: 1642 loss: 1.44232035e-06
Iter: 1643 loss: 1.43016177e-06
Iter: 1644 loss: 1.42912791e-06
Iter: 1645 loss: 1.43117643e-06
Iter: 1646 loss: 1.42873239e-06
Iter: 1647 loss: 1.42760405e-06
Iter: 1648 loss: 1.43057798e-06
Iter: 1649 loss: 1.42725435e-06
Iter: 1650 loss: 1.4260329e-06
Iter: 1651 loss: 1.42781164e-06
Iter: 1652 loss: 1.42535669e-06
Iter: 1653 loss: 1.42440456e-06
Iter: 1654 loss: 1.43659713e-06
Iter: 1655 loss: 1.42440012e-06
Iter: 1656 loss: 1.42334807e-06
Iter: 1657 loss: 1.42540546e-06
Iter: 1658 loss: 1.42296631e-06
Iter: 1659 loss: 1.42227123e-06
Iter: 1660 loss: 1.42234421e-06
Iter: 1661 loss: 1.42175259e-06
Iter: 1662 loss: 1.42074953e-06
Iter: 1663 loss: 1.42138197e-06
Iter: 1664 loss: 1.42009867e-06
Iter: 1665 loss: 1.4189302e-06
Iter: 1666 loss: 1.41997953e-06
Iter: 1667 loss: 1.41826422e-06
Iter: 1668 loss: 1.41682062e-06
Iter: 1669 loss: 1.42124782e-06
Iter: 1670 loss: 1.41642818e-06
Iter: 1671 loss: 1.41507326e-06
Iter: 1672 loss: 1.42149815e-06
Iter: 1673 loss: 1.41485611e-06
Iter: 1674 loss: 1.41376518e-06
Iter: 1675 loss: 1.42262911e-06
Iter: 1676 loss: 1.41367582e-06
Iter: 1677 loss: 1.41289956e-06
Iter: 1678 loss: 1.41195756e-06
Iter: 1679 loss: 1.41188707e-06
Iter: 1680 loss: 1.41077908e-06
Iter: 1681 loss: 1.41075941e-06
Iter: 1682 loss: 1.4099312e-06
Iter: 1683 loss: 1.40948782e-06
Iter: 1684 loss: 1.40906081e-06
Iter: 1685 loss: 1.40794134e-06
Iter: 1686 loss: 1.41543569e-06
Iter: 1687 loss: 1.40785369e-06
Iter: 1688 loss: 1.40688189e-06
Iter: 1689 loss: 1.41464432e-06
Iter: 1690 loss: 1.40680936e-06
Iter: 1691 loss: 1.40612724e-06
Iter: 1692 loss: 1.40596148e-06
Iter: 1693 loss: 1.40557427e-06
Iter: 1694 loss: 1.40461964e-06
Iter: 1695 loss: 1.40414159e-06
Iter: 1696 loss: 1.4036666e-06
Iter: 1697 loss: 1.40251041e-06
Iter: 1698 loss: 1.40932138e-06
Iter: 1699 loss: 1.40237762e-06
Iter: 1700 loss: 1.40114457e-06
Iter: 1701 loss: 1.40174609e-06
Iter: 1702 loss: 1.40036445e-06
Iter: 1703 loss: 1.39901908e-06
Iter: 1704 loss: 1.40197835e-06
Iter: 1705 loss: 1.39842655e-06
Iter: 1706 loss: 1.39721158e-06
Iter: 1707 loss: 1.40644624e-06
Iter: 1708 loss: 1.39711824e-06
Iter: 1709 loss: 1.39614349e-06
Iter: 1710 loss: 1.4004595e-06
Iter: 1711 loss: 1.39587314e-06
Iter: 1712 loss: 1.39509211e-06
Iter: 1713 loss: 1.39477061e-06
Iter: 1714 loss: 1.39438725e-06
Iter: 1715 loss: 1.39303506e-06
Iter: 1716 loss: 1.40069164e-06
Iter: 1717 loss: 1.39278063e-06
Iter: 1718 loss: 1.39170368e-06
Iter: 1719 loss: 1.39555596e-06
Iter: 1720 loss: 1.3913816e-06
Iter: 1721 loss: 1.39081612e-06
Iter: 1722 loss: 1.39081419e-06
Iter: 1723 loss: 1.39023268e-06
Iter: 1724 loss: 1.38919813e-06
Iter: 1725 loss: 1.38918745e-06
Iter: 1726 loss: 1.38825487e-06
Iter: 1727 loss: 1.39113877e-06
Iter: 1728 loss: 1.38793882e-06
Iter: 1729 loss: 1.38699045e-06
Iter: 1730 loss: 1.38874236e-06
Iter: 1731 loss: 1.38657765e-06
Iter: 1732 loss: 1.38533551e-06
Iter: 1733 loss: 1.38448797e-06
Iter: 1734 loss: 1.38412372e-06
Iter: 1735 loss: 1.38269297e-06
Iter: 1736 loss: 1.40309487e-06
Iter: 1737 loss: 1.38268501e-06
Iter: 1738 loss: 1.38184305e-06
Iter: 1739 loss: 1.3831941e-06
Iter: 1740 loss: 1.38139376e-06
Iter: 1741 loss: 1.38031453e-06
Iter: 1742 loss: 1.38148891e-06
Iter: 1743 loss: 1.37972586e-06
Iter: 1744 loss: 1.37857e-06
Iter: 1745 loss: 1.39033057e-06
Iter: 1746 loss: 1.37854e-06
Iter: 1747 loss: 1.37763527e-06
Iter: 1748 loss: 1.37653933e-06
Iter: 1749 loss: 1.37641985e-06
Iter: 1750 loss: 1.37557868e-06
Iter: 1751 loss: 1.37553366e-06
Iter: 1752 loss: 1.3749011e-06
Iter: 1753 loss: 1.37542395e-06
Iter: 1754 loss: 1.37452412e-06
Iter: 1755 loss: 1.37351458e-06
Iter: 1756 loss: 1.37821382e-06
Iter: 1757 loss: 1.37337e-06
Iter: 1758 loss: 1.37263578e-06
Iter: 1759 loss: 1.37137749e-06
Iter: 1760 loss: 1.37138704e-06
Iter: 1761 loss: 1.37026962e-06
Iter: 1762 loss: 1.37591428e-06
Iter: 1763 loss: 1.37005793e-06
Iter: 1764 loss: 1.36899928e-06
Iter: 1765 loss: 1.37182292e-06
Iter: 1766 loss: 1.36863014e-06
Iter: 1767 loss: 1.36768017e-06
Iter: 1768 loss: 1.36885762e-06
Iter: 1769 loss: 1.3672352e-06
Iter: 1770 loss: 1.36609765e-06
Iter: 1771 loss: 1.36833751e-06
Iter: 1772 loss: 1.36557105e-06
Iter: 1773 loss: 1.36454844e-06
Iter: 1774 loss: 1.37180223e-06
Iter: 1775 loss: 1.36442168e-06
Iter: 1776 loss: 1.36347228e-06
Iter: 1777 loss: 1.36438859e-06
Iter: 1778 loss: 1.36286644e-06
Iter: 1779 loss: 1.36186759e-06
Iter: 1780 loss: 1.36945982e-06
Iter: 1781 loss: 1.36179756e-06
Iter: 1782 loss: 1.36095377e-06
Iter: 1783 loss: 1.36160543e-06
Iter: 1784 loss: 1.36045458e-06
Iter: 1785 loss: 1.35944538e-06
Iter: 1786 loss: 1.36528377e-06
Iter: 1787 loss: 1.35931725e-06
Iter: 1788 loss: 1.35837558e-06
Iter: 1789 loss: 1.36464541e-06
Iter: 1790 loss: 1.35829282e-06
Iter: 1791 loss: 1.35758035e-06
Iter: 1792 loss: 1.35868072e-06
Iter: 1793 loss: 1.35725031e-06
Iter: 1794 loss: 1.35672872e-06
Iter: 1795 loss: 1.35568e-06
Iter: 1796 loss: 1.37516531e-06
Iter: 1797 loss: 1.35562482e-06
Iter: 1798 loss: 1.35436562e-06
Iter: 1799 loss: 1.36155e-06
Iter: 1800 loss: 1.35421078e-06
Iter: 1801 loss: 1.35309801e-06
Iter: 1802 loss: 1.36027336e-06
Iter: 1803 loss: 1.352975e-06
Iter: 1804 loss: 1.35217329e-06
Iter: 1805 loss: 1.35121263e-06
Iter: 1806 loss: 1.35110929e-06
Iter: 1807 loss: 1.34976926e-06
Iter: 1808 loss: 1.36039193e-06
Iter: 1809 loss: 1.34967695e-06
Iter: 1810 loss: 1.34864683e-06
Iter: 1811 loss: 1.35383675e-06
Iter: 1812 loss: 1.34850268e-06
Iter: 1813 loss: 1.34750212e-06
Iter: 1814 loss: 1.34838479e-06
Iter: 1815 loss: 1.34693437e-06
Iter: 1816 loss: 1.34593324e-06
Iter: 1817 loss: 1.3538405e-06
Iter: 1818 loss: 1.34589118e-06
Iter: 1819 loss: 1.34505922e-06
Iter: 1820 loss: 1.34561969e-06
Iter: 1821 loss: 1.34455536e-06
Iter: 1822 loss: 1.34380412e-06
Iter: 1823 loss: 1.34380025e-06
Iter: 1824 loss: 1.34328604e-06
Iter: 1825 loss: 1.34362756e-06
Iter: 1826 loss: 1.34300467e-06
Iter: 1827 loss: 1.34233574e-06
Iter: 1828 loss: 1.34140032e-06
Iter: 1829 loss: 1.34132711e-06
Iter: 1830 loss: 1.34037202e-06
Iter: 1831 loss: 1.34330844e-06
Iter: 1832 loss: 1.34005109e-06
Iter: 1833 loss: 1.33906224e-06
Iter: 1834 loss: 1.34277684e-06
Iter: 1835 loss: 1.33881781e-06
Iter: 1836 loss: 1.33776939e-06
Iter: 1837 loss: 1.34133757e-06
Iter: 1838 loss: 1.33751075e-06
Iter: 1839 loss: 1.33658818e-06
Iter: 1840 loss: 1.33660865e-06
Iter: 1841 loss: 1.33580374e-06
Iter: 1842 loss: 1.33464914e-06
Iter: 1843 loss: 1.34118579e-06
Iter: 1844 loss: 1.33446906e-06
Iter: 1845 loss: 1.33361391e-06
Iter: 1846 loss: 1.33998674e-06
Iter: 1847 loss: 1.33358208e-06
Iter: 1848 loss: 1.33278115e-06
Iter: 1849 loss: 1.33232516e-06
Iter: 1850 loss: 1.33190099e-06
Iter: 1851 loss: 1.33086178e-06
Iter: 1852 loss: 1.34087963e-06
Iter: 1853 loss: 1.33082699e-06
Iter: 1854 loss: 1.33008598e-06
Iter: 1855 loss: 1.33436856e-06
Iter: 1856 loss: 1.33000117e-06
Iter: 1857 loss: 1.32918944e-06
Iter: 1858 loss: 1.3305322e-06
Iter: 1859 loss: 1.32876812e-06
Iter: 1860 loss: 1.32809009e-06
Iter: 1861 loss: 1.32803495e-06
Iter: 1862 loss: 1.32748414e-06
Iter: 1863 loss: 1.32657942e-06
Iter: 1864 loss: 1.32932541e-06
Iter: 1865 loss: 1.32631567e-06
Iter: 1866 loss: 1.3254961e-06
Iter: 1867 loss: 1.32514674e-06
Iter: 1868 loss: 1.32469381e-06
Iter: 1869 loss: 1.32347748e-06
Iter: 1870 loss: 1.32699006e-06
Iter: 1871 loss: 1.32313141e-06
Iter: 1872 loss: 1.32195919e-06
Iter: 1873 loss: 1.33542062e-06
Iter: 1874 loss: 1.32193827e-06
Iter: 1875 loss: 1.32119681e-06
Iter: 1876 loss: 1.32113757e-06
Iter: 1877 loss: 1.32064201e-06
Iter: 1878 loss: 1.31979527e-06
Iter: 1879 loss: 1.32251125e-06
Iter: 1880 loss: 1.3195538e-06
Iter: 1881 loss: 1.31844786e-06
Iter: 1882 loss: 1.32348168e-06
Iter: 1883 loss: 1.31830257e-06
Iter: 1884 loss: 1.31742195e-06
Iter: 1885 loss: 1.31923139e-06
Iter: 1886 loss: 1.31710988e-06
Iter: 1887 loss: 1.31632896e-06
Iter: 1888 loss: 1.32169055e-06
Iter: 1889 loss: 1.31624506e-06
Iter: 1890 loss: 1.31548938e-06
Iter: 1891 loss: 1.31852357e-06
Iter: 1892 loss: 1.31530453e-06
Iter: 1893 loss: 1.3147046e-06
Iter: 1894 loss: 1.31499769e-06
Iter: 1895 loss: 1.31422894e-06
Iter: 1896 loss: 1.3136314e-06
Iter: 1897 loss: 1.31375202e-06
Iter: 1898 loss: 1.313192e-06
Iter: 1899 loss: 1.3121437e-06
Iter: 1900 loss: 1.31321349e-06
Iter: 1901 loss: 1.31153547e-06
Iter: 1902 loss: 1.31053093e-06
Iter: 1903 loss: 1.3143e-06
Iter: 1904 loss: 1.31028355e-06
Iter: 1905 loss: 1.30927606e-06
Iter: 1906 loss: 1.30976355e-06
Iter: 1907 loss: 1.30854949e-06
Iter: 1908 loss: 1.30751118e-06
Iter: 1909 loss: 1.3208919e-06
Iter: 1910 loss: 1.3075213e-06
Iter: 1911 loss: 1.30661761e-06
Iter: 1912 loss: 1.30657338e-06
Iter: 1913 loss: 1.3059921e-06
Iter: 1914 loss: 1.30480839e-06
Iter: 1915 loss: 1.30777175e-06
Iter: 1916 loss: 1.30438775e-06
Iter: 1917 loss: 1.30356079e-06
Iter: 1918 loss: 1.31623256e-06
Iter: 1919 loss: 1.30355136e-06
Iter: 1920 loss: 1.30285514e-06
Iter: 1921 loss: 1.30202829e-06
Iter: 1922 loss: 1.30194155e-06
Iter: 1923 loss: 1.30129138e-06
Iter: 1924 loss: 1.30115814e-06
Iter: 1925 loss: 1.30060766e-06
Iter: 1926 loss: 1.29993509e-06
Iter: 1927 loss: 1.29987257e-06
Iter: 1928 loss: 1.29899308e-06
Iter: 1929 loss: 1.30200499e-06
Iter: 1930 loss: 1.2987291e-06
Iter: 1931 loss: 1.29799264e-06
Iter: 1932 loss: 1.29765169e-06
Iter: 1933 loss: 1.29724901e-06
Iter: 1934 loss: 1.29624e-06
Iter: 1935 loss: 1.30122339e-06
Iter: 1936 loss: 1.2960445e-06
Iter: 1937 loss: 1.29503644e-06
Iter: 1938 loss: 1.29818602e-06
Iter: 1939 loss: 1.29478212e-06
Iter: 1940 loss: 1.29388104e-06
Iter: 1941 loss: 1.29570674e-06
Iter: 1942 loss: 1.29350951e-06
Iter: 1943 loss: 1.29262889e-06
Iter: 1944 loss: 1.29592672e-06
Iter: 1945 loss: 1.29238492e-06
Iter: 1946 loss: 1.29151169e-06
Iter: 1947 loss: 1.29429179e-06
Iter: 1948 loss: 1.29123407e-06
Iter: 1949 loss: 1.29052091e-06
Iter: 1950 loss: 1.29175191e-06
Iter: 1951 loss: 1.29017781e-06
Iter: 1952 loss: 1.28922511e-06
Iter: 1953 loss: 1.2937694e-06
Iter: 1954 loss: 1.28902923e-06
Iter: 1955 loss: 1.28828037e-06
Iter: 1956 loss: 1.29245984e-06
Iter: 1957 loss: 1.28815873e-06
Iter: 1958 loss: 1.28737918e-06
Iter: 1959 loss: 1.29096304e-06
Iter: 1960 loss: 1.28728675e-06
Iter: 1961 loss: 1.28673264e-06
Iter: 1962 loss: 1.2856442e-06
Iter: 1963 loss: 1.30394233e-06
Iter: 1964 loss: 1.2855628e-06
Iter: 1965 loss: 1.28452712e-06
Iter: 1966 loss: 1.29868511e-06
Iter: 1967 loss: 1.28451927e-06
Iter: 1968 loss: 1.28386694e-06
Iter: 1969 loss: 1.28362785e-06
Iter: 1970 loss: 1.28319971e-06
Iter: 1971 loss: 1.28222632e-06
Iter: 1972 loss: 1.28342867e-06
Iter: 1973 loss: 1.28166914e-06
Iter: 1974 loss: 1.28064619e-06
Iter: 1975 loss: 1.28720797e-06
Iter: 1976 loss: 1.28051408e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.4
+ date
Wed Oct 21 14:07:24 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2/300_300_300_1 --function f1 --psi -2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a6d3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a52b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a6d32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a52b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a561730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a561bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95f3fae620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95f3fe1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f963a4d97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95f3fa88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95f3fa39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc7acd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc7d6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc7d6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95f3f597b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc7a99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95f3f20510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc71bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc6c9488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc6aef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc738598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc738048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc66c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc684f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc684d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc61fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc5987b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc59e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc59e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc5b8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc5b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc54e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc54e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc554488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc4bd1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f95cc4f7158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.10622512
test_loss: 0.1054124
train_loss: 0.060843967
test_loss: 0.070803285
train_loss: 0.08660334
test_loss: 0.11036842
train_loss: 0.055990458
test_loss: 0.08501481
train_loss: 0.046906684
test_loss: 0.06362656
train_loss: 0.0421164
test_loss: 0.051937632
train_loss: 0.034925275
test_loss: 0.041813143
train_loss: 0.023005312
test_loss: 0.037864752
train_loss: 0.02515577
test_loss: 0.032818504
train_loss: 0.02008786
test_loss: 0.029540552
train_loss: 0.02170552
test_loss: 0.028538324
train_loss: 0.015656255
test_loss: 0.028249161
train_loss: 0.014189102
test_loss: 0.024864724
train_loss: 0.01412234
test_loss: 0.025804887
train_loss: 0.01262017
test_loss: 0.023093646
train_loss: 0.017712975
test_loss: 0.022703426
train_loss: 0.012144958
test_loss: 0.021757761
train_loss: 0.012875065
test_loss: 0.020570345
train_loss: 0.011652782
test_loss: 0.020204034
train_loss: 0.012062667
test_loss: 0.020165559
train_loss: 0.011888095
test_loss: 0.019205019
train_loss: 0.012665361
test_loss: 0.018025424
train_loss: 0.010410462
test_loss: 0.019161675
train_loss: 0.010595595
test_loss: 0.017420897
train_loss: 0.009666943
test_loss: 0.01861016
train_loss: 0.009849001
test_loss: 0.016526086
train_loss: 0.011972663
test_loss: 0.019081589
train_loss: 0.010022854
test_loss: 0.016800918
train_loss: 0.009638453
test_loss: 0.018147899
train_loss: 0.012546624
test_loss: 0.017902592
train_loss: 0.0100402655
test_loss: 0.01766914
train_loss: 0.010213756
test_loss: 0.01618616
train_loss: 0.010342436
test_loss: 0.0171504
train_loss: 0.010120853
test_loss: 0.016392205
train_loss: 0.0090710055
test_loss: 0.017709712
train_loss: 0.011122284
test_loss: 0.017103016
train_loss: 0.0102732275
test_loss: 0.016820755
train_loss: 0.008897977
test_loss: 0.018087707
train_loss: 0.008953924
test_loss: 0.015853466
train_loss: 0.00828525
test_loss: 0.016549358
train_loss: 0.008188896
test_loss: 0.015208312
train_loss: 0.010277955
test_loss: 0.016629567
train_loss: 0.008697009
test_loss: 0.017050026
train_loss: 0.00854217
test_loss: 0.014994767
train_loss: 0.008129739
test_loss: 0.016071672
train_loss: 0.00948379
test_loss: 0.015998762
train_loss: 0.01034376
test_loss: 0.015705278
train_loss: 0.008351614
test_loss: 0.015050927
train_loss: 0.00889853
test_loss: 0.017073838
train_loss: 0.007504915
test_loss: 0.014493339
train_loss: 0.008126963
test_loss: 0.016812636
train_loss: 0.009014567
test_loss: 0.0157623
train_loss: 0.007796806
test_loss: 0.017045919
train_loss: 0.008257623
test_loss: 0.014520288
train_loss: 0.0081049185
test_loss: 0.015238249
train_loss: 0.007765674
test_loss: 0.0154455565
train_loss: 0.0102334
test_loss: 0.014778631
train_loss: 0.0077662994
test_loss: 0.015957784
train_loss: 0.008811289
test_loss: 0.014909925
train_loss: 0.011297748
test_loss: 0.015145184
train_loss: 0.0077361884
test_loss: 0.015840972
train_loss: 0.009561706
test_loss: 0.015622998
train_loss: 0.008036469
test_loss: 0.016009534
train_loss: 0.008428907
test_loss: 0.014936887
train_loss: 0.008427739
test_loss: 0.015389586
train_loss: 0.008159206
test_loss: 0.0151876
train_loss: 0.007614473
test_loss: 0.015140096
train_loss: 0.009573889
test_loss: 0.014038871
train_loss: 0.008148459
test_loss: 0.015633535
train_loss: 0.008530054
test_loss: 0.0152420215
train_loss: 0.00904616
test_loss: 0.015109802
train_loss: 0.009589218
test_loss: 0.014653728
train_loss: 0.007468838
test_loss: 0.015307199
train_loss: 0.0077726133
test_loss: 0.014585063
train_loss: 0.00815543
test_loss: 0.01427355
train_loss: 0.0079052355
test_loss: 0.014793486
train_loss: 0.007413677
test_loss: 0.015023416
train_loss: 0.0068543013
test_loss: 0.015050396
train_loss: 0.0070349956
test_loss: 0.013989323
train_loss: 0.006941982
test_loss: 0.015002756
train_loss: 0.007562205
test_loss: 0.014359631
train_loss: 0.0074769417
test_loss: 0.0150725795
train_loss: 0.009478056
test_loss: 0.015473176
train_loss: 0.008322143
test_loss: 0.013650967
train_loss: 0.0071454765
test_loss: 0.013840887
train_loss: 0.007953056
test_loss: 0.015750306
train_loss: 0.007688518
test_loss: 0.014514995
train_loss: 0.007950798
test_loss: 0.015465546
train_loss: 0.0075067035
test_loss: 0.013365185
train_loss: 0.0096036745
test_loss: 0.015730612
train_loss: 0.008181828
test_loss: 0.013954382
train_loss: 0.0073433304
test_loss: 0.014717017
train_loss: 0.009433495
test_loss: 0.014756683
train_loss: 0.007937161
test_loss: 0.013690628
train_loss: 0.006719731
test_loss: 0.015269392
train_loss: 0.0072930716
test_loss: 0.015029492
train_loss: 0.008990198
test_loss: 0.01426134
train_loss: 0.0070321998
test_loss: 0.013146011
train_loss: 0.007152573
test_loss: 0.0144700175
train_loss: 0.008056419
test_loss: 0.013944036
train_loss: 0.0068675177
test_loss: 0.014330516
train_loss: 0.008095032
test_loss: 0.013927697
train_loss: 0.007141103
test_loss: 0.013015523
train_loss: 0.0075854543
test_loss: 0.014529193
train_loss: 0.0069906805
test_loss: 0.014804585
train_loss: 0.0074411742
test_loss: 0.013370193
train_loss: 0.007957475
test_loss: 0.014508561
train_loss: 0.006762111
test_loss: 0.013656108
train_loss: 0.008537998
test_loss: 0.0140172355
train_loss: 0.007188554
test_loss: 0.01353435
train_loss: 0.0070410506
test_loss: 0.014189994
train_loss: 0.007792425
test_loss: 0.013724717
train_loss: 0.0068336898
test_loss: 0.013716068
train_loss: 0.007027399
test_loss: 0.013799424
train_loss: 0.0071208486
test_loss: 0.013023526
train_loss: 0.0070313658
test_loss: 0.014530161
train_loss: 0.006463458
test_loss: 0.013790369
train_loss: 0.0073744366
test_loss: 0.01509656
train_loss: 0.0063789585
test_loss: 0.014127315
train_loss: 0.007638534
test_loss: 0.014109442
train_loss: 0.0068330183
test_loss: 0.014576021
train_loss: 0.0076057604
test_loss: 0.01405233
train_loss: 0.0070470334
test_loss: 0.013494164
train_loss: 0.006894907
test_loss: 0.012712157
train_loss: 0.0069831903
test_loss: 0.014761709
train_loss: 0.007756929
test_loss: 0.013046447
train_loss: 0.007675496
test_loss: 0.014033285
train_loss: 0.00903921
test_loss: 0.013603747
train_loss: 0.00843498
test_loss: 0.014086216
train_loss: 0.008732662
test_loss: 0.013291599
train_loss: 0.007987659
test_loss: 0.0137511
train_loss: 0.0073059835
test_loss: 0.012665123
train_loss: 0.007290332
test_loss: 0.013189368
train_loss: 0.0068945354
test_loss: 0.012793215
train_loss: 0.0072628506
test_loss: 0.012959612
train_loss: 0.006866617
test_loss: 0.013159985
train_loss: 0.007082413
test_loss: 0.013383208
train_loss: 0.0065017925
test_loss: 0.012713986
train_loss: 0.006379719
test_loss: 0.013836457
train_loss: 0.0075203027
test_loss: 0.014266946
train_loss: 0.0074615986
test_loss: 0.013554228
train_loss: 0.0068692397
test_loss: 0.01326682
train_loss: 0.0071232393
test_loss: 0.014436556
train_loss: 0.0077867196
test_loss: 0.013784452
train_loss: 0.006841394
test_loss: 0.014946657
train_loss: 0.008956158
test_loss: 0.014255816
train_loss: 0.008326651
test_loss: 0.0145568205
train_loss: 0.008210202
test_loss: 0.014127521
train_loss: 0.007153739
test_loss: 0.014102247
train_loss: 0.008415091
test_loss: 0.014774076
train_loss: 0.0075012324
test_loss: 0.013172777
train_loss: 0.0067913397
test_loss: 0.013212955
train_loss: 0.007887347
test_loss: 0.013836282
train_loss: 0.0068066157
test_loss: 0.013383874
train_loss: 0.006929106
test_loss: 0.013191796
train_loss: 0.0070372396
test_loss: 0.013392268
train_loss: 0.008320022
test_loss: 0.014681781
train_loss: 0.0075138104
test_loss: 0.013245788
train_loss: 0.007279715
test_loss: 0.0135158645
train_loss: 0.0073900195
test_loss: 0.01393926
train_loss: 0.007541483
test_loss: 0.01282383
train_loss: 0.007486009
test_loss: 0.013809836
train_loss: 0.0064190663
test_loss: 0.013539639
train_loss: 0.0066652526
test_loss: 0.012918426
train_loss: 0.007447442
test_loss: 0.013919646
train_loss: 0.006825585
test_loss: 0.013084095
train_loss: 0.0075995987
test_loss: 0.013519081
train_loss: 0.007464643
test_loss: 0.013599135
train_loss: 0.006712083
test_loss: 0.013065188
train_loss: 0.007906849
test_loss: 0.013070265
train_loss: 0.008531282
test_loss: 0.015408697
train_loss: 0.006819579
test_loss: 0.014349969
train_loss: 0.008726707
test_loss: 0.012525769
train_loss: 0.007049748
test_loss: 0.012883831
train_loss: 0.00714423
test_loss: 0.013371349
train_loss: 0.0073200087
test_loss: 0.013204717
train_loss: 0.0071826233
test_loss: 0.013960093
train_loss: 0.007136826
test_loss: 0.015152343
train_loss: 0.007790726
test_loss: 0.013854345
train_loss: 0.006816272
test_loss: 0.01286142
train_loss: 0.0064816917
test_loss: 0.014529896
train_loss: 0.007156862
test_loss: 0.013595201
train_loss: 0.0065755043
test_loss: 0.01306566
train_loss: 0.0069959555
test_loss: 0.01308
train_loss: 0.006479547
test_loss: 0.012889461
train_loss: 0.006979091
test_loss: 0.013675934
train_loss: 0.0074888207
test_loss: 0.013755885
train_loss: 0.0070544286
test_loss: 0.015420725
train_loss: 0.006563493
test_loss: 0.01290808
train_loss: 0.007298574
test_loss: 0.013063772
train_loss: 0.008024707
test_loss: 0.013066832
train_loss: 0.008405253
test_loss: 0.014131837
train_loss: 0.0072225067
test_loss: 0.014169848
train_loss: 0.0073129656
test_loss: 0.012718847
train_loss: 0.008040775
test_loss: 0.013251326
train_loss: 0.0078934515
test_loss: 0.013406105
train_loss: 0.0071883113
test_loss: 0.013711931
train_loss: 0.007284062
test_loss: 0.012625588
train_loss: 0.0067990427
test_loss: 0.013954138
train_loss: 0.008059439
test_loss: 0.013635968
train_loss: 0.0070901616
test_loss: 0.014601447
train_loss: 0.007144396
test_loss: 0.013350934
train_loss: 0.007246404
test_loss: 0.013154106
train_loss: 0.0063379873
test_loss: 0.013460098
train_loss: 0.006818942
test_loss: 0.012710721
train_loss: 0.008286962
test_loss: 0.012959081
train_loss: 0.008018958
test_loss: 0.012890159
train_loss: 0.006739039
test_loss: 0.013519378
train_loss: 0.006950251
test_loss: 0.013636265
train_loss: 0.008376471
test_loss: 0.013220209
train_loss: 0.0062851883
test_loss: 0.013261292
train_loss: 0.0072872597
test_loss: 0.012327876
train_loss: 0.0071696597
test_loss: 0.013796275
train_loss: 0.0070632277
test_loss: 0.012957433
train_loss: 0.008362494
test_loss: 0.014077964
train_loss: 0.0068721105
test_loss: 0.015643207
train_loss: 0.0067053353
test_loss: 0.014572161
train_loss: 0.0064007407
test_loss: 0.013290183
train_loss: 0.008047486
test_loss: 0.013788073
train_loss: 0.006115148
test_loss: 0.013228743
train_loss: 0.006533832
test_loss: 0.012905218
train_loss: 0.0070843874
test_loss: 0.012823644
train_loss: 0.0062477477
test_loss: 0.013140696
train_loss: 0.008097921
test_loss: 0.012457385
train_loss: 0.007864963
test_loss: 0.013339053
train_loss: 0.006977409
test_loss: 0.013588096
train_loss: 0.0068157576
test_loss: 0.013014785
train_loss: 0.007084271
test_loss: 0.012858701
train_loss: 0.007032792
test_loss: 0.013472513
train_loss: 0.007544146
test_loss: 0.013108473
train_loss: 0.0071220556
test_loss: 0.013338141
train_loss: 0.007336319
test_loss: 0.013741424
train_loss: 0.006501521
test_loss: 0.014193973
train_loss: 0.0071521183
test_loss: 0.012901695
train_loss: 0.007537069
test_loss: 0.013084007
train_loss: 0.007031664
test_loss: 0.013160002
train_loss: 0.0070291073
test_loss: 0.01306664
train_loss: 0.0072409646
test_loss: 0.01335611
train_loss: 0.008725357
test_loss: 0.015180801
train_loss: 0.00685474
test_loss: 0.0137966825
train_loss: 0.0074885404
test_loss: 0.013641214
train_loss: 0.0068945666
test_loss: 0.014383987
train_loss: 0.0077488264
test_loss: 0.014310177
train_loss: 0.006793842
test_loss: 0.0135598425
train_loss: 0.008169783
test_loss: 0.014733291
train_loss: 0.0066158404
test_loss: 0.01490575
train_loss: 0.0061956714
test_loss: 0.013708752
train_loss: 0.0064798403
test_loss: 0.014017431
train_loss: 0.006996492
test_loss: 0.012833678
train_loss: 0.006984476
test_loss: 0.013749848
train_loss: 0.0071637565
test_loss: 0.013380148
train_loss: 0.0070000277
test_loss: 0.013421548
train_loss: 0.006540717
test_loss: 0.0135912765
train_loss: 0.008286697
test_loss: 0.014467702
train_loss: 0.008792814
test_loss: 0.013650185
train_loss: 0.0066393586
test_loss: 0.013151462
train_loss: 0.0071283136
test_loss: 0.013126173
train_loss: 0.007158837
test_loss: 0.013159644
train_loss: 0.007175704
test_loss: 0.013964227
train_loss: 0.0068922453
test_loss: 0.013392389
train_loss: 0.0066957255
test_loss: 0.013069458
train_loss: 0.0072667273
test_loss: 0.014416634
train_loss: 0.007438856
test_loss: 0.013259516
train_loss: 0.006856734
test_loss: 0.013119612
train_loss: 0.006495845
test_loss: 0.014396808
train_loss: 0.0066994373
test_loss: 0.0129245715
train_loss: 0.0063335863
test_loss: 0.013213676
train_loss: 0.00696302
test_loss: 0.013976373
train_loss: 0.00787376
test_loss: 0.012519846
train_loss: 0.0073152194
test_loss: 0.013244449
train_loss: 0.0075076837
test_loss: 0.01327942
train_loss: 0.007606797
test_loss: 0.013356891
train_loss: 0.0072404654
test_loss: 0.013008799
train_loss: 0.00687326
test_loss: 0.014224182
train_loss: 0.008507699
test_loss: 0.015078872
train_loss: 0.007387992
test_loss: 0.013166866
train_loss: 0.0069422233
test_loss: 0.01325474
train_loss: 0.006651589
test_loss: 0.012678852
train_loss: 0.0074417423
test_loss: 0.0132487025
train_loss: 0.008185318
test_loss: 0.013959607
train_loss: 0.006384896
test_loss: 0.013215164
train_loss: 0.0073627317
test_loss: 0.012732743
train_loss: 0.0067633362
test_loss: 0.013445081
train_loss: 0.0066507654
test_loss: 0.013771854
train_loss: 0.006145583
test_loss: 0.013306835
train_loss: 0.007516003
test_loss: 0.013460516
train_loss: 0.0075185103
test_loss: 0.01519051
train_loss: 0.0084697595
test_loss: 0.013599991
train_loss: 0.0078563085
test_loss: 0.013598842
train_loss: 0.006682516
test_loss: 0.013185641
train_loss: 0.006420429
test_loss: 0.014931078
train_loss: 0.007223492
test_loss: 0.012896059
train_loss: 0.0074445773
test_loss: 0.01368101
train_loss: 0.008291654
test_loss: 0.013078876
train_loss: 0.0077202944
test_loss: 0.013183188
train_loss: 0.006719646
test_loss: 0.0143584795
train_loss: 0.0065825046
test_loss: 0.013108858
train_loss: 0.0065959725
test_loss: 0.014055389
train_loss: 0.0077089258
test_loss: 0.013573189
train_loss: 0.007889413
test_loss: 0.013994717
train_loss: 0.0072039617
test_loss: 0.013929146
train_loss: 0.007406459
test_loss: 0.0138263805
train_loss: 0.0062740953
test_loss: 0.012797564
train_loss: 0.007386719
test_loss: 0.013199761
train_loss: 0.0071625253
test_loss: 0.014637018
train_loss: 0.0077305036
test_loss: 0.012658384
train_loss: 0.0075282576
test_loss: 0.013556966
train_loss: 0.006270229
test_loss: 0.012911273
train_loss: 0.0065185353
test_loss: 0.01334858
train_loss: 0.007296478
test_loss: 0.013777812
train_loss: 0.006248572
test_loss: 0.013328387
train_loss: 0.0065561174
test_loss: 0.012666497
train_loss: 0.006501175
test_loss: 0.0141147915
train_loss: 0.0072925477
test_loss: 0.0137859555
train_loss: 0.008540085
test_loss: 0.014544967
train_loss: 0.0065688198
test_loss: 0.013088176
train_loss: 0.0074970494
test_loss: 0.013537355
train_loss: 0.0073519032
test_loss: 0.013805743
train_loss: 0.0068864254
test_loss: 0.013964513
train_loss: 0.0064228624
test_loss: 0.0132244965
train_loss: 0.005971347
test_loss: 0.012730577
train_loss: 0.0060431985
test_loss: 0.013619527
train_loss: 0.006776454
test_loss: 0.012818918
train_loss: 0.006920689
test_loss: 0.01659189
train_loss: 0.0065322043
test_loss: 0.014036786
train_loss: 0.007957127
test_loss: 0.014110775
train_loss: 0.0062316954
test_loss: 0.0133472225
train_loss: 0.005517238
test_loss: 0.012496748
train_loss: 0.0062834877
test_loss: 0.012849605
train_loss: 0.0065237023
test_loss: 0.013900327
train_loss: 0.006820817
test_loss: 0.012704404
train_loss: 0.0066006775
test_loss: 0.01271593
train_loss: 0.0061726556
test_loss: 0.012888194
train_loss: 0.0063330512
test_loss: 0.013913719
train_loss: 0.006306406
test_loss: 0.012848559
train_loss: 0.005743339
test_loss: 0.012920488
train_loss: 0.006726429
test_loss: 0.01355095
train_loss: 0.006552032
test_loss: 0.013068956
train_loss: 0.0067229783
test_loss: 0.013854701
train_loss: 0.0064673712
test_loss: 0.01327272
train_loss: 0.006176032
test_loss: 0.01339506
train_loss: 0.0068688253
test_loss: 0.014382349
train_loss: 0.0064736367
test_loss: 0.014758248
train_loss: 0.007436396
test_loss: 0.014497358
train_loss: 0.008210858
test_loss: 0.014825672
train_loss: 0.0077063213
test_loss: 0.014396463
train_loss: 0.0066421404
test_loss: 0.012537386
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.00763472
test_loss: 0.012716201
train_loss: 0.0063552996
test_loss: 0.013176411
train_loss: 0.006876952
test_loss: 0.013755707
train_loss: 0.0075000664
test_loss: 0.013088352
train_loss: 0.0056988536
test_loss: 0.012792876
train_loss: 0.0062871794
test_loss: 0.014129272
train_loss: 0.006143172
test_loss: 0.012934693
train_loss: 0.005854078
test_loss: 0.012577549
train_loss: 0.006149863
test_loss: 0.013646774
train_loss: 0.007005107
test_loss: 0.0138794575
train_loss: 0.006953869
test_loss: 0.01363937
train_loss: 0.007081942
test_loss: 0.0137534505
train_loss: 0.006515681
test_loss: 0.014210917
train_loss: 0.007661179
test_loss: 0.01340737
train_loss: 0.007230758
test_loss: 0.013880862
train_loss: 0.007021893
test_loss: 0.014045844
train_loss: 0.006826963
test_loss: 0.012964799
train_loss: 0.0064088595
test_loss: 0.012615687
train_loss: 0.007260978
test_loss: 0.015909111
train_loss: 0.0063080136
test_loss: 0.013502585
train_loss: 0.009222415
test_loss: 0.013191428
train_loss: 0.0072674733
test_loss: 0.014512798
train_loss: 0.0066173123
test_loss: 0.013741083
train_loss: 0.007120877
test_loss: 0.013577976
train_loss: 0.0076444168
test_loss: 0.013490554
train_loss: 0.007139144
test_loss: 0.013377485
train_loss: 0.006401321
test_loss: 0.013134999
train_loss: 0.0069445637
test_loss: 0.014747001
train_loss: 0.007915658
test_loss: 0.014289133
train_loss: 0.007186775
test_loss: 0.013362779
train_loss: 0.007336653
test_loss: 0.01310999
train_loss: 0.0070268605
test_loss: 0.014061072
train_loss: 0.0072981827
test_loss: 0.013189816
train_loss: 0.0069720997
test_loss: 0.013494198
train_loss: 0.006501244
test_loss: 0.012971801
train_loss: 0.0065392125
test_loss: 0.012827451
train_loss: 0.006052318
test_loss: 0.013794068
train_loss: 0.0063557047
test_loss: 0.013668842
train_loss: 0.0064591644
test_loss: 0.013816509
train_loss: 0.0069793467
test_loss: 0.014459898
train_loss: 0.0076274844
test_loss: 0.01360802
train_loss: 0.005643484
test_loss: 0.01311124
train_loss: 0.006369385
test_loss: 0.013360078
train_loss: 0.0070928447
test_loss: 0.014341316
train_loss: 0.0066504045
test_loss: 0.013947941
train_loss: 0.0065226844
test_loss: 0.013672516
train_loss: 0.0065139085
test_loss: 0.014766279
train_loss: 0.0070461533
test_loss: 0.013836942
train_loss: 0.007035627
test_loss: 0.013054417
train_loss: 0.006985497
test_loss: 0.014089587
train_loss: 0.0066852523
test_loss: 0.013883483
train_loss: 0.007776304
test_loss: 0.01629413
train_loss: 0.008065809
test_loss: 0.015310072
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349ea6d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349eb39598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349ec0eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349ec0ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349eadd2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349eadd510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e9f7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e9ba6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e9d5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e9d56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e9d5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e953ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e953b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e90e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e8b1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e8b16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e86e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e8a2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e85c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f349e85df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474b8d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474bae400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474afa620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474afa840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474b230d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474afa7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474acf9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474aa8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474aa8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3474b23730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3450307a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f345032f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34502b8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f34502e4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3450293598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3450245400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000144741527
Iter: 2 loss: 0.00112465408
Iter: 3 loss: 0.0001041824
Iter: 4 loss: 9.50007598e-05
Iter: 5 loss: 8.96542188e-05
Iter: 6 loss: 8.57426494e-05
Iter: 7 loss: 7.47095255e-05
Iter: 8 loss: 0.000100823519
Iter: 9 loss: 7.06791761e-05
Iter: 10 loss: 6.702666e-05
Iter: 11 loss: 6.09734561e-05
Iter: 12 loss: 6.0950988e-05
Iter: 13 loss: 5.56910054e-05
Iter: 14 loss: 6.54631149e-05
Iter: 15 loss: 5.34475366e-05
Iter: 16 loss: 4.97501387e-05
Iter: 17 loss: 6.14177479e-05
Iter: 18 loss: 4.86776225e-05
Iter: 19 loss: 4.577047e-05
Iter: 20 loss: 4.33213427e-05
Iter: 21 loss: 4.25054022e-05
Iter: 22 loss: 3.84971172e-05
Iter: 23 loss: 4.76195492e-05
Iter: 24 loss: 3.69844565e-05
Iter: 25 loss: 3.3549095e-05
Iter: 26 loss: 3.98569027e-05
Iter: 27 loss: 3.20702493e-05
Iter: 28 loss: 2.94664751e-05
Iter: 29 loss: 5.11277904e-05
Iter: 30 loss: 2.93092726e-05
Iter: 31 loss: 2.78339248e-05
Iter: 32 loss: 2.87039184e-05
Iter: 33 loss: 2.6881411e-05
Iter: 34 loss: 2.46653362e-05
Iter: 35 loss: 2.95090558e-05
Iter: 36 loss: 2.38125394e-05
Iter: 37 loss: 2.24936484e-05
Iter: 38 loss: 2.90065036e-05
Iter: 39 loss: 2.22683702e-05
Iter: 40 loss: 2.12257219e-05
Iter: 41 loss: 3.12094307e-05
Iter: 42 loss: 2.11859842e-05
Iter: 43 loss: 2.01940784e-05
Iter: 44 loss: 3.07523769e-05
Iter: 45 loss: 2.01707626e-05
Iter: 46 loss: 1.98736707e-05
Iter: 47 loss: 2.00999275e-05
Iter: 48 loss: 1.96917463e-05
Iter: 49 loss: 1.93031246e-05
Iter: 50 loss: 1.97336121e-05
Iter: 51 loss: 1.90911978e-05
Iter: 52 loss: 1.85177742e-05
Iter: 53 loss: 1.90241153e-05
Iter: 54 loss: 1.81828473e-05
Iter: 55 loss: 1.76861304e-05
Iter: 56 loss: 1.80520474e-05
Iter: 57 loss: 1.73802e-05
Iter: 58 loss: 1.67928119e-05
Iter: 59 loss: 1.77924776e-05
Iter: 60 loss: 1.65306192e-05
Iter: 61 loss: 1.58965267e-05
Iter: 62 loss: 1.78735554e-05
Iter: 63 loss: 1.5711732e-05
Iter: 64 loss: 1.51737322e-05
Iter: 65 loss: 1.57091708e-05
Iter: 66 loss: 1.48697354e-05
Iter: 67 loss: 1.42780364e-05
Iter: 68 loss: 1.88073409e-05
Iter: 69 loss: 1.42336339e-05
Iter: 70 loss: 1.38875075e-05
Iter: 71 loss: 1.40922493e-05
Iter: 72 loss: 1.36637591e-05
Iter: 73 loss: 1.31662819e-05
Iter: 74 loss: 1.50733986e-05
Iter: 75 loss: 1.30479184e-05
Iter: 76 loss: 1.34965176e-05
Iter: 77 loss: 1.29179407e-05
Iter: 78 loss: 1.28524671e-05
Iter: 79 loss: 1.26684099e-05
Iter: 80 loss: 1.36259214e-05
Iter: 81 loss: 1.26083442e-05
Iter: 82 loss: 1.23289501e-05
Iter: 83 loss: 1.42114286e-05
Iter: 84 loss: 1.23011159e-05
Iter: 85 loss: 1.20544801e-05
Iter: 86 loss: 1.27851181e-05
Iter: 87 loss: 1.19793622e-05
Iter: 88 loss: 1.17630625e-05
Iter: 89 loss: 1.16813862e-05
Iter: 90 loss: 1.15627463e-05
Iter: 91 loss: 1.13093101e-05
Iter: 92 loss: 1.16058209e-05
Iter: 93 loss: 1.11739637e-05
Iter: 94 loss: 1.09416033e-05
Iter: 95 loss: 1.25594433e-05
Iter: 96 loss: 1.09197263e-05
Iter: 97 loss: 1.06973093e-05
Iter: 98 loss: 1.06978196e-05
Iter: 99 loss: 1.05190056e-05
Iter: 100 loss: 1.03023012e-05
Iter: 101 loss: 1.1776463e-05
Iter: 102 loss: 1.02813192e-05
Iter: 103 loss: 1.00940561e-05
Iter: 104 loss: 1.00991529e-05
Iter: 105 loss: 9.94563743e-06
Iter: 106 loss: 9.76166848e-06
Iter: 107 loss: 1.25149691e-05
Iter: 108 loss: 9.76162482e-06
Iter: 109 loss: 9.66170228e-06
Iter: 110 loss: 1.01285586e-05
Iter: 111 loss: 9.64286664e-06
Iter: 112 loss: 9.49148671e-06
Iter: 113 loss: 9.93468e-06
Iter: 114 loss: 9.44408566e-06
Iter: 115 loss: 9.39198253e-06
Iter: 116 loss: 9.45533066e-06
Iter: 117 loss: 9.36473316e-06
Iter: 118 loss: 9.29729686e-06
Iter: 119 loss: 9.52824848e-06
Iter: 120 loss: 9.2794362e-06
Iter: 121 loss: 9.2090022e-06
Iter: 122 loss: 9.16517638e-06
Iter: 123 loss: 9.13663462e-06
Iter: 124 loss: 9.02114698e-06
Iter: 125 loss: 9.08206766e-06
Iter: 126 loss: 8.94411733e-06
Iter: 127 loss: 8.82452514e-06
Iter: 128 loss: 8.96092206e-06
Iter: 129 loss: 8.75964633e-06
Iter: 130 loss: 8.62799243e-06
Iter: 131 loss: 9.77453237e-06
Iter: 132 loss: 8.62114939e-06
Iter: 133 loss: 8.53175879e-06
Iter: 134 loss: 8.56171391e-06
Iter: 135 loss: 8.46902185e-06
Iter: 136 loss: 8.34866751e-06
Iter: 137 loss: 8.67223844e-06
Iter: 138 loss: 8.30886893e-06
Iter: 139 loss: 8.21372851e-06
Iter: 140 loss: 8.46097464e-06
Iter: 141 loss: 8.18100398e-06
Iter: 142 loss: 8.056566e-06
Iter: 143 loss: 8.2899187e-06
Iter: 144 loss: 8.00348789e-06
Iter: 145 loss: 8.05668606e-06
Iter: 146 loss: 7.97077701e-06
Iter: 147 loss: 7.93546769e-06
Iter: 148 loss: 7.88161742e-06
Iter: 149 loss: 7.88056059e-06
Iter: 150 loss: 7.82820916e-06
Iter: 151 loss: 8.05052423e-06
Iter: 152 loss: 7.81724066e-06
Iter: 153 loss: 7.75808439e-06
Iter: 154 loss: 7.80755727e-06
Iter: 155 loss: 7.7226e-06
Iter: 156 loss: 7.67298843e-06
Iter: 157 loss: 7.82433563e-06
Iter: 158 loss: 7.65783534e-06
Iter: 159 loss: 7.60353396e-06
Iter: 160 loss: 7.52681171e-06
Iter: 161 loss: 7.52383039e-06
Iter: 162 loss: 7.4433e-06
Iter: 163 loss: 7.90116e-06
Iter: 164 loss: 7.43259307e-06
Iter: 165 loss: 7.35804224e-06
Iter: 166 loss: 7.59236354e-06
Iter: 167 loss: 7.33656179e-06
Iter: 168 loss: 7.26851613e-06
Iter: 169 loss: 7.49826586e-06
Iter: 170 loss: 7.24987149e-06
Iter: 171 loss: 7.19379659e-06
Iter: 172 loss: 7.13960253e-06
Iter: 173 loss: 7.12737801e-06
Iter: 174 loss: 7.03314618e-06
Iter: 175 loss: 7.97528719e-06
Iter: 176 loss: 7.03020169e-06
Iter: 177 loss: 6.98365147e-06
Iter: 178 loss: 7.07778145e-06
Iter: 179 loss: 6.96447296e-06
Iter: 180 loss: 6.92446429e-06
Iter: 181 loss: 6.92072035e-06
Iter: 182 loss: 6.89805256e-06
Iter: 183 loss: 6.85668601e-06
Iter: 184 loss: 7.82328152e-06
Iter: 185 loss: 6.85627856e-06
Iter: 186 loss: 6.81897336e-06
Iter: 187 loss: 7.27695669e-06
Iter: 188 loss: 6.81854135e-06
Iter: 189 loss: 6.79142522e-06
Iter: 190 loss: 6.75023148e-06
Iter: 191 loss: 6.74958437e-06
Iter: 192 loss: 6.70213649e-06
Iter: 193 loss: 7.07357503e-06
Iter: 194 loss: 6.69889687e-06
Iter: 195 loss: 6.66801589e-06
Iter: 196 loss: 6.64362869e-06
Iter: 197 loss: 6.63415676e-06
Iter: 198 loss: 6.57740111e-06
Iter: 199 loss: 6.65865355e-06
Iter: 200 loss: 6.54963469e-06
Iter: 201 loss: 6.50141e-06
Iter: 202 loss: 6.79671211e-06
Iter: 203 loss: 6.49578124e-06
Iter: 204 loss: 6.44137708e-06
Iter: 205 loss: 6.53372763e-06
Iter: 206 loss: 6.41708266e-06
Iter: 207 loss: 6.37814446e-06
Iter: 208 loss: 6.49210642e-06
Iter: 209 loss: 6.36644063e-06
Iter: 210 loss: 6.31752573e-06
Iter: 211 loss: 6.31730973e-06
Iter: 212 loss: 6.27834243e-06
Iter: 213 loss: 6.2353156e-06
Iter: 214 loss: 6.61058812e-06
Iter: 215 loss: 6.23312735e-06
Iter: 216 loss: 6.23987171e-06
Iter: 217 loss: 6.21694835e-06
Iter: 218 loss: 6.20741685e-06
Iter: 219 loss: 6.1799883e-06
Iter: 220 loss: 6.30784871e-06
Iter: 221 loss: 6.17047135e-06
Iter: 222 loss: 6.14330384e-06
Iter: 223 loss: 6.14274131e-06
Iter: 224 loss: 6.131193e-06
Iter: 225 loss: 6.11103405e-06
Iter: 226 loss: 6.11119731e-06
Iter: 227 loss: 6.0788725e-06
Iter: 228 loss: 6.16142052e-06
Iter: 229 loss: 6.06785079e-06
Iter: 230 loss: 6.03648277e-06
Iter: 231 loss: 6.08489e-06
Iter: 232 loss: 6.0218681e-06
Iter: 233 loss: 5.98963425e-06
Iter: 234 loss: 6.04918887e-06
Iter: 235 loss: 5.97600956e-06
Iter: 236 loss: 5.94787525e-06
Iter: 237 loss: 6.20223864e-06
Iter: 238 loss: 5.94645689e-06
Iter: 239 loss: 5.92295964e-06
Iter: 240 loss: 5.91969638e-06
Iter: 241 loss: 5.90294076e-06
Iter: 242 loss: 5.87200338e-06
Iter: 243 loss: 6.11758378e-06
Iter: 244 loss: 5.87005252e-06
Iter: 245 loss: 5.84553391e-06
Iter: 246 loss: 5.80926599e-06
Iter: 247 loss: 5.80841879e-06
Iter: 248 loss: 5.76987395e-06
Iter: 249 loss: 6.16284342e-06
Iter: 250 loss: 5.76876892e-06
Iter: 251 loss: 5.76033381e-06
Iter: 252 loss: 5.75638751e-06
Iter: 253 loss: 5.73933539e-06
Iter: 254 loss: 5.71391411e-06
Iter: 255 loss: 5.71313649e-06
Iter: 256 loss: 5.69693839e-06
Iter: 257 loss: 5.81547192e-06
Iter: 258 loss: 5.69547774e-06
Iter: 259 loss: 5.67994084e-06
Iter: 260 loss: 5.66477229e-06
Iter: 261 loss: 5.6612871e-06
Iter: 262 loss: 5.63778076e-06
Iter: 263 loss: 5.64697166e-06
Iter: 264 loss: 5.62123842e-06
Iter: 265 loss: 5.59831096e-06
Iter: 266 loss: 5.61912293e-06
Iter: 267 loss: 5.58505599e-06
Iter: 268 loss: 5.55857696e-06
Iter: 269 loss: 5.90554691e-06
Iter: 270 loss: 5.55849101e-06
Iter: 271 loss: 5.539624e-06
Iter: 272 loss: 5.58136571e-06
Iter: 273 loss: 5.53230575e-06
Iter: 274 loss: 5.51450739e-06
Iter: 275 loss: 5.50071672e-06
Iter: 276 loss: 5.49526567e-06
Iter: 277 loss: 5.46298861e-06
Iter: 278 loss: 5.65175696e-06
Iter: 279 loss: 5.45887724e-06
Iter: 280 loss: 5.44076556e-06
Iter: 281 loss: 5.48406206e-06
Iter: 282 loss: 5.43401893e-06
Iter: 283 loss: 5.40965e-06
Iter: 284 loss: 5.43656597e-06
Iter: 285 loss: 5.39623397e-06
Iter: 286 loss: 5.43242459e-06
Iter: 287 loss: 5.39040775e-06
Iter: 288 loss: 5.38612903e-06
Iter: 289 loss: 5.37366122e-06
Iter: 290 loss: 5.4312377e-06
Iter: 291 loss: 5.36909465e-06
Iter: 292 loss: 5.35282197e-06
Iter: 293 loss: 5.42922226e-06
Iter: 294 loss: 5.3498984e-06
Iter: 295 loss: 5.33089133e-06
Iter: 296 loss: 5.34647916e-06
Iter: 297 loss: 5.31987553e-06
Iter: 298 loss: 5.30782609e-06
Iter: 299 loss: 5.28934106e-06
Iter: 300 loss: 5.28924102e-06
Iter: 301 loss: 5.26259646e-06
Iter: 302 loss: 5.43660281e-06
Iter: 303 loss: 5.2597884e-06
Iter: 304 loss: 5.2443761e-06
Iter: 305 loss: 5.40992187e-06
Iter: 306 loss: 5.24401366e-06
Iter: 307 loss: 5.22859364e-06
Iter: 308 loss: 5.2142791e-06
Iter: 309 loss: 5.21076163e-06
Iter: 310 loss: 5.19366176e-06
Iter: 311 loss: 5.30624766e-06
Iter: 312 loss: 5.19171681e-06
Iter: 313 loss: 5.17593162e-06
Iter: 314 loss: 5.19150854e-06
Iter: 315 loss: 5.16704e-06
Iter: 316 loss: 5.14926705e-06
Iter: 317 loss: 5.22119444e-06
Iter: 318 loss: 5.14526391e-06
Iter: 319 loss: 5.13002942e-06
Iter: 320 loss: 5.26313579e-06
Iter: 321 loss: 5.12935912e-06
Iter: 322 loss: 5.11194276e-06
Iter: 323 loss: 5.20123e-06
Iter: 324 loss: 5.10919199e-06
Iter: 325 loss: 5.10392829e-06
Iter: 326 loss: 5.09464508e-06
Iter: 327 loss: 5.32263584e-06
Iter: 328 loss: 5.09463734e-06
Iter: 329 loss: 5.08266839e-06
Iter: 330 loss: 5.15225e-06
Iter: 331 loss: 5.08085031e-06
Iter: 332 loss: 5.06869856e-06
Iter: 333 loss: 5.04744457e-06
Iter: 334 loss: 5.04729e-06
Iter: 335 loss: 5.03010688e-06
Iter: 336 loss: 5.12410679e-06
Iter: 337 loss: 5.02729154e-06
Iter: 338 loss: 5.01264685e-06
Iter: 339 loss: 5.00475562e-06
Iter: 340 loss: 4.99821408e-06
Iter: 341 loss: 4.98496047e-06
Iter: 342 loss: 4.98335521e-06
Iter: 343 loss: 4.97293286e-06
Iter: 344 loss: 4.96901976e-06
Iter: 345 loss: 4.96344273e-06
Iter: 346 loss: 4.948678e-06
Iter: 347 loss: 4.97119709e-06
Iter: 348 loss: 4.94158212e-06
Iter: 349 loss: 4.92914933e-06
Iter: 350 loss: 4.96308076e-06
Iter: 351 loss: 4.92497384e-06
Iter: 352 loss: 4.9121868e-06
Iter: 353 loss: 5.04331638e-06
Iter: 354 loss: 4.9115697e-06
Iter: 355 loss: 4.90847697e-06
Iter: 356 loss: 4.90673892e-06
Iter: 357 loss: 4.90453158e-06
Iter: 358 loss: 4.89780086e-06
Iter: 359 loss: 4.91953642e-06
Iter: 360 loss: 4.89463e-06
Iter: 361 loss: 4.88442174e-06
Iter: 362 loss: 4.93978496e-06
Iter: 363 loss: 4.88289788e-06
Iter: 364 loss: 4.87318357e-06
Iter: 365 loss: 4.90519869e-06
Iter: 366 loss: 4.87045963e-06
Iter: 367 loss: 4.8641441e-06
Iter: 368 loss: 4.85056216e-06
Iter: 369 loss: 5.0515e-06
Iter: 370 loss: 4.84986776e-06
Iter: 371 loss: 4.83135955e-06
Iter: 372 loss: 4.90985894e-06
Iter: 373 loss: 4.82760424e-06
Iter: 374 loss: 4.81525512e-06
Iter: 375 loss: 4.8933357e-06
Iter: 376 loss: 4.81372672e-06
Iter: 377 loss: 4.80218932e-06
Iter: 378 loss: 4.86400404e-06
Iter: 379 loss: 4.8003194e-06
Iter: 380 loss: 4.79024357e-06
Iter: 381 loss: 4.79241407e-06
Iter: 382 loss: 4.78261791e-06
Iter: 383 loss: 4.77080312e-06
Iter: 384 loss: 4.77188769e-06
Iter: 385 loss: 4.76154946e-06
Iter: 386 loss: 4.74867056e-06
Iter: 387 loss: 4.8788e-06
Iter: 388 loss: 4.74830676e-06
Iter: 389 loss: 4.74522767e-06
Iter: 390 loss: 4.74272565e-06
Iter: 391 loss: 4.73769e-06
Iter: 392 loss: 4.72657212e-06
Iter: 393 loss: 4.8881484e-06
Iter: 394 loss: 4.72610282e-06
Iter: 395 loss: 4.71813382e-06
Iter: 396 loss: 4.74309581e-06
Iter: 397 loss: 4.71563908e-06
Iter: 398 loss: 4.70784562e-06
Iter: 399 loss: 4.75905154e-06
Iter: 400 loss: 4.70715077e-06
Iter: 401 loss: 4.69988572e-06
Iter: 402 loss: 4.68784765e-06
Iter: 403 loss: 4.68792769e-06
Iter: 404 loss: 4.67733526e-06
Iter: 405 loss: 4.73719774e-06
Iter: 406 loss: 4.6758023e-06
Iter: 407 loss: 4.66637948e-06
Iter: 408 loss: 4.66007123e-06
Iter: 409 loss: 4.6564237e-06
Iter: 410 loss: 4.6451637e-06
Iter: 411 loss: 4.64506638e-06
Iter: 412 loss: 4.63884135e-06
Iter: 413 loss: 4.67194877e-06
Iter: 414 loss: 4.63795823e-06
Iter: 415 loss: 4.63204196e-06
Iter: 416 loss: 4.61782611e-06
Iter: 417 loss: 4.79389564e-06
Iter: 418 loss: 4.61668333e-06
Iter: 419 loss: 4.60368665e-06
Iter: 420 loss: 4.70017494e-06
Iter: 421 loss: 4.60260071e-06
Iter: 422 loss: 4.59850889e-06
Iter: 423 loss: 4.59702915e-06
Iter: 424 loss: 4.58995873e-06
Iter: 425 loss: 4.59089279e-06
Iter: 426 loss: 4.58484283e-06
Iter: 427 loss: 4.5798588e-06
Iter: 428 loss: 4.57939677e-06
Iter: 429 loss: 4.57574242e-06
Iter: 430 loss: 4.56960788e-06
Iter: 431 loss: 4.59354078e-06
Iter: 432 loss: 4.568441e-06
Iter: 433 loss: 4.56017506e-06
Iter: 434 loss: 4.55641475e-06
Iter: 435 loss: 4.55229883e-06
Iter: 436 loss: 4.54376277e-06
Iter: 437 loss: 4.56299495e-06
Iter: 438 loss: 4.5403076e-06
Iter: 439 loss: 4.5306133e-06
Iter: 440 loss: 4.5271845e-06
Iter: 441 loss: 4.52171116e-06
Iter: 442 loss: 4.50995776e-06
Iter: 443 loss: 4.62136404e-06
Iter: 444 loss: 4.5093866e-06
Iter: 445 loss: 4.50039533e-06
Iter: 446 loss: 4.53669782e-06
Iter: 447 loss: 4.49833578e-06
Iter: 448 loss: 4.48909077e-06
Iter: 449 loss: 4.52435688e-06
Iter: 450 loss: 4.48702031e-06
Iter: 451 loss: 4.48118544e-06
Iter: 452 loss: 4.47858e-06
Iter: 453 loss: 4.47575e-06
Iter: 454 loss: 4.4665876e-06
Iter: 455 loss: 4.50736707e-06
Iter: 456 loss: 4.46482227e-06
Iter: 457 loss: 4.46665854e-06
Iter: 458 loss: 4.46107424e-06
Iter: 459 loss: 4.45959449e-06
Iter: 460 loss: 4.45456089e-06
Iter: 461 loss: 4.4549779e-06
Iter: 462 loss: 4.44918078e-06
Iter: 463 loss: 4.44240823e-06
Iter: 464 loss: 4.44242687e-06
Iter: 465 loss: 4.43850149e-06
Iter: 466 loss: 4.44888065e-06
Iter: 467 loss: 4.43717136e-06
Iter: 468 loss: 4.43189492e-06
Iter: 469 loss: 4.41975408e-06
Iter: 470 loss: 4.57191936e-06
Iter: 471 loss: 4.4189992e-06
Iter: 472 loss: 4.41040629e-06
Iter: 473 loss: 4.51709366e-06
Iter: 474 loss: 4.41020575e-06
Iter: 475 loss: 4.4026765e-06
Iter: 476 loss: 4.39755331e-06
Iter: 477 loss: 4.39462701e-06
Iter: 478 loss: 4.38698225e-06
Iter: 479 loss: 4.38706866e-06
Iter: 480 loss: 4.38054576e-06
Iter: 481 loss: 4.39080668e-06
Iter: 482 loss: 4.37752624e-06
Iter: 483 loss: 4.36868959e-06
Iter: 484 loss: 4.37415747e-06
Iter: 485 loss: 4.36305891e-06
Iter: 486 loss: 4.3554237e-06
Iter: 487 loss: 4.35800575e-06
Iter: 488 loss: 4.35013635e-06
Iter: 489 loss: 4.35310312e-06
Iter: 490 loss: 4.34666208e-06
Iter: 491 loss: 4.3423629e-06
Iter: 492 loss: 4.33414471e-06
Iter: 493 loss: 4.5165707e-06
Iter: 494 loss: 4.33411515e-06
Iter: 495 loss: 4.33029072e-06
Iter: 496 loss: 4.32710385e-06
Iter: 497 loss: 4.32585693e-06
Iter: 498 loss: 4.31864464e-06
Iter: 499 loss: 4.3549353e-06
Iter: 500 loss: 4.31744184e-06
Iter: 501 loss: 4.31076205e-06
Iter: 502 loss: 4.36706523e-06
Iter: 503 loss: 4.31031503e-06
Iter: 504 loss: 4.30657383e-06
Iter: 505 loss: 4.30374121e-06
Iter: 506 loss: 4.30259115e-06
Iter: 507 loss: 4.29618922e-06
Iter: 508 loss: 4.29769534e-06
Iter: 509 loss: 4.29153852e-06
Iter: 510 loss: 4.28472777e-06
Iter: 511 loss: 4.30095815e-06
Iter: 512 loss: 4.28248495e-06
Iter: 513 loss: 4.2735137e-06
Iter: 514 loss: 4.29850752e-06
Iter: 515 loss: 4.27054056e-06
Iter: 516 loss: 4.26576662e-06
Iter: 517 loss: 4.26562792e-06
Iter: 518 loss: 4.26133465e-06
Iter: 519 loss: 4.25454482e-06
Iter: 520 loss: 4.25439612e-06
Iter: 521 loss: 4.24699738e-06
Iter: 522 loss: 4.31195531e-06
Iter: 523 loss: 4.2466886e-06
Iter: 524 loss: 4.24254586e-06
Iter: 525 loss: 4.24744849e-06
Iter: 526 loss: 4.24033124e-06
Iter: 527 loss: 4.23650135e-06
Iter: 528 loss: 4.23609936e-06
Iter: 529 loss: 4.23454367e-06
Iter: 530 loss: 4.22994071e-06
Iter: 531 loss: 4.25233793e-06
Iter: 532 loss: 4.2283009e-06
Iter: 533 loss: 4.2230713e-06
Iter: 534 loss: 4.22822768e-06
Iter: 535 loss: 4.21996583e-06
Iter: 536 loss: 4.21205368e-06
Iter: 537 loss: 4.26279121e-06
Iter: 538 loss: 4.2112606e-06
Iter: 539 loss: 4.20619835e-06
Iter: 540 loss: 4.20822289e-06
Iter: 541 loss: 4.20259948e-06
Iter: 542 loss: 4.19733078e-06
Iter: 543 loss: 4.2002157e-06
Iter: 544 loss: 4.19400067e-06
Iter: 545 loss: 4.18723721e-06
Iter: 546 loss: 4.2082047e-06
Iter: 547 loss: 4.18537229e-06
Iter: 548 loss: 4.17920546e-06
Iter: 549 loss: 4.18193349e-06
Iter: 550 loss: 4.17510091e-06
Iter: 551 loss: 4.16881949e-06
Iter: 552 loss: 4.16879811e-06
Iter: 553 loss: 4.16427974e-06
Iter: 554 loss: 4.15806244e-06
Iter: 555 loss: 4.15771774e-06
Iter: 556 loss: 4.14985789e-06
Iter: 557 loss: 4.18525906e-06
Iter: 558 loss: 4.14815167e-06
Iter: 559 loss: 4.14849092e-06
Iter: 560 loss: 4.14613442e-06
Iter: 561 loss: 4.14365741e-06
Iter: 562 loss: 4.13840417e-06
Iter: 563 loss: 4.21830509e-06
Iter: 564 loss: 4.13815e-06
Iter: 565 loss: 4.1332928e-06
Iter: 566 loss: 4.1307494e-06
Iter: 567 loss: 4.12852432e-06
Iter: 568 loss: 4.12340114e-06
Iter: 569 loss: 4.16768489e-06
Iter: 570 loss: 4.12308236e-06
Iter: 571 loss: 4.11738256e-06
Iter: 572 loss: 4.13360613e-06
Iter: 573 loss: 4.11558358e-06
Iter: 574 loss: 4.11046267e-06
Iter: 575 loss: 4.10804478e-06
Iter: 576 loss: 4.10552457e-06
Iter: 577 loss: 4.10049324e-06
Iter: 578 loss: 4.10610392e-06
Iter: 579 loss: 4.09768336e-06
Iter: 580 loss: 4.09080621e-06
Iter: 581 loss: 4.13535417e-06
Iter: 582 loss: 4.09005088e-06
Iter: 583 loss: 4.08582218e-06
Iter: 584 loss: 4.08125197e-06
Iter: 585 loss: 4.08055212e-06
Iter: 586 loss: 4.07480184e-06
Iter: 587 loss: 4.0746545e-06
Iter: 588 loss: 4.07025527e-06
Iter: 589 loss: 4.07141442e-06
Iter: 590 loss: 4.06679465e-06
Iter: 591 loss: 4.06010577e-06
Iter: 592 loss: 4.05131641e-06
Iter: 593 loss: 4.05076753e-06
Iter: 594 loss: 4.08080632e-06
Iter: 595 loss: 4.04911179e-06
Iter: 596 loss: 4.04758839e-06
Iter: 597 loss: 4.04373486e-06
Iter: 598 loss: 4.07732432e-06
Iter: 599 loss: 4.04289767e-06
Iter: 600 loss: 4.03870399e-06
Iter: 601 loss: 4.03204558e-06
Iter: 602 loss: 4.03197282e-06
Iter: 603 loss: 4.02561454e-06
Iter: 604 loss: 4.05494575e-06
Iter: 605 loss: 4.0242935e-06
Iter: 606 loss: 4.01956913e-06
Iter: 607 loss: 4.01953503e-06
Iter: 608 loss: 4.01506895e-06
Iter: 609 loss: 4.0155669e-06
Iter: 610 loss: 4.01170109e-06
Iter: 611 loss: 4.00901263e-06
Iter: 612 loss: 4.00830413e-06
Iter: 613 loss: 4.00639328e-06
Iter: 614 loss: 4.00088629e-06
Iter: 615 loss: 4.00602767e-06
Iter: 616 loss: 3.99757573e-06
Iter: 617 loss: 3.993061e-06
Iter: 618 loss: 4.0313339e-06
Iter: 619 loss: 3.99286409e-06
Iter: 620 loss: 3.9892584e-06
Iter: 621 loss: 3.989795e-06
Iter: 622 loss: 3.98644625e-06
Iter: 623 loss: 3.982213e-06
Iter: 624 loss: 4.0463965e-06
Iter: 625 loss: 3.98219436e-06
Iter: 626 loss: 3.97941358e-06
Iter: 627 loss: 3.97501071e-06
Iter: 628 loss: 3.97493841e-06
Iter: 629 loss: 3.97583608e-06
Iter: 630 loss: 3.97238682e-06
Iter: 631 loss: 3.97001168e-06
Iter: 632 loss: 3.96518e-06
Iter: 633 loss: 4.04337379e-06
Iter: 634 loss: 3.96500627e-06
Iter: 635 loss: 3.9619e-06
Iter: 636 loss: 3.95828192e-06
Iter: 637 loss: 3.9577526e-06
Iter: 638 loss: 3.9515e-06
Iter: 639 loss: 3.96778023e-06
Iter: 640 loss: 3.94925519e-06
Iter: 641 loss: 3.94409381e-06
Iter: 642 loss: 3.95414281e-06
Iter: 643 loss: 3.94212657e-06
Iter: 644 loss: 3.93802657e-06
Iter: 645 loss: 3.93766913e-06
Iter: 646 loss: 3.93525534e-06
Iter: 647 loss: 3.93301707e-06
Iter: 648 loss: 3.93242499e-06
Iter: 649 loss: 3.92874244e-06
Iter: 650 loss: 3.92407583e-06
Iter: 651 loss: 3.92364109e-06
Iter: 652 loss: 3.91745971e-06
Iter: 653 loss: 3.96301493e-06
Iter: 654 loss: 3.91693447e-06
Iter: 655 loss: 3.91213962e-06
Iter: 656 loss: 3.93092159e-06
Iter: 657 loss: 3.91105186e-06
Iter: 658 loss: 3.90634932e-06
Iter: 659 loss: 3.9307929e-06
Iter: 660 loss: 3.90550804e-06
Iter: 661 loss: 3.90276728e-06
Iter: 662 loss: 3.92832862e-06
Iter: 663 loss: 3.90274226e-06
Iter: 664 loss: 3.90000605e-06
Iter: 665 loss: 3.91798176e-06
Iter: 666 loss: 3.89974821e-06
Iter: 667 loss: 3.89819161e-06
Iter: 668 loss: 3.8935068e-06
Iter: 669 loss: 3.91533831e-06
Iter: 670 loss: 3.8919e-06
Iter: 671 loss: 3.88678336e-06
Iter: 672 loss: 3.91763024e-06
Iter: 673 loss: 3.88599619e-06
Iter: 674 loss: 3.88173794e-06
Iter: 675 loss: 3.8850385e-06
Iter: 676 loss: 3.87925e-06
Iter: 677 loss: 3.87387354e-06
Iter: 678 loss: 3.8938565e-06
Iter: 679 loss: 3.87269529e-06
Iter: 680 loss: 3.86913689e-06
Iter: 681 loss: 3.90229661e-06
Iter: 682 loss: 3.86897591e-06
Iter: 683 loss: 3.86486499e-06
Iter: 684 loss: 3.86851207e-06
Iter: 685 loss: 3.8622893e-06
Iter: 686 loss: 3.85921794e-06
Iter: 687 loss: 3.85562453e-06
Iter: 688 loss: 3.85516705e-06
Iter: 689 loss: 3.8489934e-06
Iter: 690 loss: 3.86507463e-06
Iter: 691 loss: 3.8468288e-06
Iter: 692 loss: 3.84129271e-06
Iter: 693 loss: 3.85637532e-06
Iter: 694 loss: 3.83960378e-06
Iter: 695 loss: 3.83370934e-06
Iter: 696 loss: 3.88807803e-06
Iter: 697 loss: 3.83357929e-06
Iter: 698 loss: 3.83156248e-06
Iter: 699 loss: 3.83074666e-06
Iter: 700 loss: 3.82878534e-06
Iter: 701 loss: 3.82599092e-06
Iter: 702 loss: 3.82601502e-06
Iter: 703 loss: 3.82351664e-06
Iter: 704 loss: 3.81912514e-06
Iter: 705 loss: 3.81915379e-06
Iter: 706 loss: 3.81426798e-06
Iter: 707 loss: 3.85601606e-06
Iter: 708 loss: 3.81396012e-06
Iter: 709 loss: 3.81068958e-06
Iter: 710 loss: 3.80781512e-06
Iter: 711 loss: 3.80699134e-06
Iter: 712 loss: 3.80205688e-06
Iter: 713 loss: 3.86722877e-06
Iter: 714 loss: 3.80207189e-06
Iter: 715 loss: 3.79997277e-06
Iter: 716 loss: 3.79988592e-06
Iter: 717 loss: 3.79801304e-06
Iter: 718 loss: 3.793514e-06
Iter: 719 loss: 3.84488476e-06
Iter: 720 loss: 3.79310268e-06
Iter: 721 loss: 3.78900222e-06
Iter: 722 loss: 3.81387099e-06
Iter: 723 loss: 3.78857158e-06
Iter: 724 loss: 3.78542131e-06
Iter: 725 loss: 3.78068512e-06
Iter: 726 loss: 3.78046252e-06
Iter: 727 loss: 3.77511856e-06
Iter: 728 loss: 3.80357073e-06
Iter: 729 loss: 3.77433e-06
Iter: 730 loss: 3.77085098e-06
Iter: 731 loss: 3.77002652e-06
Iter: 732 loss: 3.76771436e-06
Iter: 733 loss: 3.76869798e-06
Iter: 734 loss: 3.76554635e-06
Iter: 735 loss: 3.76350931e-06
Iter: 736 loss: 3.76360458e-06
Iter: 737 loss: 3.76203684e-06
Iter: 738 loss: 3.76027833e-06
Iter: 739 loss: 3.75767e-06
Iter: 740 loss: 3.75771742e-06
Iter: 741 loss: 3.7536056e-06
Iter: 742 loss: 3.75320269e-06
Iter: 743 loss: 3.75020181e-06
Iter: 744 loss: 3.74646811e-06
Iter: 745 loss: 3.78083655e-06
Iter: 746 loss: 3.74636e-06
Iter: 747 loss: 3.74258343e-06
Iter: 748 loss: 3.73800844e-06
Iter: 749 loss: 3.73772809e-06
Iter: 750 loss: 3.73360558e-06
Iter: 751 loss: 3.7335426e-06
Iter: 752 loss: 3.73084981e-06
Iter: 753 loss: 3.75566196e-06
Iter: 754 loss: 3.73071066e-06
Iter: 755 loss: 3.72832733e-06
Iter: 756 loss: 3.72401792e-06
Iter: 757 loss: 3.72404793e-06
Iter: 758 loss: 3.71964757e-06
Iter: 759 loss: 3.73038733e-06
Iter: 760 loss: 3.71818669e-06
Iter: 761 loss: 3.71344277e-06
Iter: 762 loss: 3.71645365e-06
Iter: 763 loss: 3.71033411e-06
Iter: 764 loss: 3.7060513e-06
Iter: 765 loss: 3.75934724e-06
Iter: 766 loss: 3.70603107e-06
Iter: 767 loss: 3.70464295e-06
Iter: 768 loss: 3.70391263e-06
Iter: 769 loss: 3.70295925e-06
Iter: 770 loss: 3.69986856e-06
Iter: 771 loss: 3.70561293e-06
Iter: 772 loss: 3.69787631e-06
Iter: 773 loss: 3.69428972e-06
Iter: 774 loss: 3.74506885e-06
Iter: 775 loss: 3.69424106e-06
Iter: 776 loss: 3.69184136e-06
Iter: 777 loss: 3.6888066e-06
Iter: 778 loss: 3.68860174e-06
Iter: 779 loss: 3.68402107e-06
Iter: 780 loss: 3.69482314e-06
Iter: 781 loss: 3.6822687e-06
Iter: 782 loss: 3.67784219e-06
Iter: 783 loss: 3.68458859e-06
Iter: 784 loss: 3.67573034e-06
Iter: 785 loss: 3.67244502e-06
Iter: 786 loss: 3.67243274e-06
Iter: 787 loss: 3.66950644e-06
Iter: 788 loss: 3.68319525e-06
Iter: 789 loss: 3.66893141e-06
Iter: 790 loss: 3.6667077e-06
Iter: 791 loss: 3.66175982e-06
Iter: 792 loss: 3.73059493e-06
Iter: 793 loss: 3.66147151e-06
Iter: 794 loss: 3.65755068e-06
Iter: 795 loss: 3.70534144e-06
Iter: 796 loss: 3.65751953e-06
Iter: 797 loss: 3.65438063e-06
Iter: 798 loss: 3.65198684e-06
Iter: 799 loss: 3.6510221e-06
Iter: 800 loss: 3.65124015e-06
Iter: 801 loss: 3.64875018e-06
Iter: 802 loss: 3.64712946e-06
Iter: 803 loss: 3.64370317e-06
Iter: 804 loss: 3.70226098e-06
Iter: 805 loss: 3.64356447e-06
Iter: 806 loss: 3.64104426e-06
Iter: 807 loss: 3.640253e-06
Iter: 808 loss: 3.63883578e-06
Iter: 809 loss: 3.63419258e-06
Iter: 810 loss: 3.64952848e-06
Iter: 811 loss: 3.63283607e-06
Iter: 812 loss: 3.62936021e-06
Iter: 813 loss: 3.65006508e-06
Iter: 814 loss: 3.62883725e-06
Iter: 815 loss: 3.62620699e-06
Iter: 816 loss: 3.62263836e-06
Iter: 817 loss: 3.62260926e-06
Iter: 818 loss: 3.61839898e-06
Iter: 819 loss: 3.67348275e-06
Iter: 820 loss: 3.61845059e-06
Iter: 821 loss: 3.61636194e-06
Iter: 822 loss: 3.64210109e-06
Iter: 823 loss: 3.61643424e-06
Iter: 824 loss: 3.61406956e-06
Iter: 825 loss: 3.61013826e-06
Iter: 826 loss: 3.7112452e-06
Iter: 827 loss: 3.61009597e-06
Iter: 828 loss: 3.60695822e-06
Iter: 829 loss: 3.61333059e-06
Iter: 830 loss: 3.60570448e-06
Iter: 831 loss: 3.60165654e-06
Iter: 832 loss: 3.60928425e-06
Iter: 833 loss: 3.60021022e-06
Iter: 834 loss: 3.59896421e-06
Iter: 835 loss: 3.59823525e-06
Iter: 836 loss: 3.59612727e-06
Iter: 837 loss: 3.59773958e-06
Iter: 838 loss: 3.59475962e-06
Iter: 839 loss: 3.59323985e-06
Iter: 840 loss: 3.58946886e-06
Iter: 841 loss: 3.62271885e-06
Iter: 842 loss: 3.58891793e-06
Iter: 843 loss: 3.58515354e-06
Iter: 844 loss: 3.63294203e-06
Iter: 845 loss: 3.58516263e-06
Iter: 846 loss: 3.58305465e-06
Iter: 847 loss: 3.58647503e-06
Iter: 848 loss: 3.58200441e-06
Iter: 849 loss: 3.57899694e-06
Iter: 850 loss: 3.57737486e-06
Iter: 851 loss: 3.57604608e-06
Iter: 852 loss: 3.57264753e-06
Iter: 853 loss: 3.60644185e-06
Iter: 854 loss: 3.57248132e-06
Iter: 855 loss: 3.57023237e-06
Iter: 856 loss: 3.56649457e-06
Iter: 857 loss: 3.56647934e-06
Iter: 858 loss: 3.56731516e-06
Iter: 859 loss: 3.56479086e-06
Iter: 860 loss: 3.56307646e-06
Iter: 861 loss: 3.55993848e-06
Iter: 862 loss: 3.62819651e-06
Iter: 863 loss: 3.55997577e-06
Iter: 864 loss: 3.55630164e-06
Iter: 865 loss: 3.56032433e-06
Iter: 866 loss: 3.55430188e-06
Iter: 867 loss: 3.55074985e-06
Iter: 868 loss: 3.56258352e-06
Iter: 869 loss: 3.54998747e-06
Iter: 870 loss: 3.54970325e-06
Iter: 871 loss: 3.54858616e-06
Iter: 872 loss: 3.54682061e-06
Iter: 873 loss: 3.5438436e-06
Iter: 874 loss: 3.54378608e-06
Iter: 875 loss: 3.54226768e-06
Iter: 876 loss: 3.54049689e-06
Iter: 877 loss: 3.54038275e-06
Iter: 878 loss: 3.53740575e-06
Iter: 879 loss: 3.5478497e-06
Iter: 880 loss: 3.53676251e-06
Iter: 881 loss: 3.53474206e-06
Iter: 882 loss: 3.54754161e-06
Iter: 883 loss: 3.53459131e-06
Iter: 884 loss: 3.5328128e-06
Iter: 885 loss: 3.53229939e-06
Iter: 886 loss: 3.53126097e-06
Iter: 887 loss: 3.52860434e-06
Iter: 888 loss: 3.54906115e-06
Iter: 889 loss: 3.52842289e-06
Iter: 890 loss: 3.52661891e-06
Iter: 891 loss: 3.53024757e-06
Iter: 892 loss: 3.52579195e-06
Iter: 893 loss: 3.52385337e-06
Iter: 894 loss: 3.53017208e-06
Iter: 895 loss: 3.5234234e-06
Iter: 896 loss: 3.52078223e-06
Iter: 897 loss: 3.52932125e-06
Iter: 898 loss: 3.52015468e-06
Iter: 899 loss: 3.51857307e-06
Iter: 900 loss: 3.51646122e-06
Iter: 901 loss: 3.51633594e-06
Iter: 902 loss: 3.51409585e-06
Iter: 903 loss: 3.52956658e-06
Iter: 904 loss: 3.51391122e-06
Iter: 905 loss: 3.51444623e-06
Iter: 906 loss: 3.51301128e-06
Iter: 907 loss: 3.51259405e-06
Iter: 908 loss: 3.51110702e-06
Iter: 909 loss: 3.50900427e-06
Iter: 910 loss: 3.50865798e-06
Iter: 911 loss: 3.50670052e-06
Iter: 912 loss: 3.53824157e-06
Iter: 913 loss: 3.50665164e-06
Iter: 914 loss: 3.50502319e-06
Iter: 915 loss: 3.50400614e-06
Iter: 916 loss: 3.5033263e-06
Iter: 917 loss: 3.50042433e-06
Iter: 918 loss: 3.52088819e-06
Iter: 919 loss: 3.50012556e-06
Iter: 920 loss: 3.49869333e-06
Iter: 921 loss: 3.50456253e-06
Iter: 922 loss: 3.49830589e-06
Iter: 923 loss: 3.49670563e-06
Iter: 924 loss: 3.49695529e-06
Iter: 925 loss: 3.49534048e-06
Iter: 926 loss: 3.49318202e-06
Iter: 927 loss: 3.50682603e-06
Iter: 928 loss: 3.49283982e-06
Iter: 929 loss: 3.49140328e-06
Iter: 930 loss: 3.50343021e-06
Iter: 931 loss: 3.49139782e-06
Iter: 932 loss: 3.49008974e-06
Iter: 933 loss: 3.48870162e-06
Iter: 934 loss: 3.48840967e-06
Iter: 935 loss: 3.48624985e-06
Iter: 936 loss: 3.49161246e-06
Iter: 937 loss: 3.48550111e-06
Iter: 938 loss: 3.48467984e-06
Iter: 939 loss: 3.48456251e-06
Iter: 940 loss: 3.48341814e-06
Iter: 941 loss: 3.481e-06
Iter: 942 loss: 3.53189876e-06
Iter: 943 loss: 3.48100934e-06
Iter: 944 loss: 3.47920241e-06
Iter: 945 loss: 3.47708556e-06
Iter: 946 loss: 3.47691184e-06
Iter: 947 loss: 3.47434252e-06
Iter: 948 loss: 3.48037565e-06
Iter: 949 loss: 3.4733539e-06
Iter: 950 loss: 3.47054311e-06
Iter: 951 loss: 3.48860362e-06
Iter: 952 loss: 3.47036939e-06
Iter: 953 loss: 3.46847378e-06
Iter: 954 loss: 3.47607738e-06
Iter: 955 loss: 3.4679324e-06
Iter: 956 loss: 3.46597471e-06
Iter: 957 loss: 3.46573438e-06
Iter: 958 loss: 3.46427692e-06
Iter: 959 loss: 3.46146021e-06
Iter: 960 loss: 3.48363255e-06
Iter: 961 loss: 3.46137e-06
Iter: 962 loss: 3.45960461e-06
Iter: 963 loss: 3.46656861e-06
Iter: 964 loss: 3.45918897e-06
Iter: 965 loss: 3.45736839e-06
Iter: 966 loss: 3.46418096e-06
Iter: 967 loss: 3.45695162e-06
Iter: 968 loss: 3.45513945e-06
Iter: 969 loss: 3.45471472e-06
Iter: 970 loss: 3.45360309e-06
Iter: 971 loss: 3.4516795e-06
Iter: 972 loss: 3.45252147e-06
Iter: 973 loss: 3.45023886e-06
Iter: 974 loss: 3.45117405e-06
Iter: 975 loss: 3.44922023e-06
Iter: 976 loss: 3.44825071e-06
Iter: 977 loss: 3.44558089e-06
Iter: 978 loss: 3.46921638e-06
Iter: 979 loss: 3.44520572e-06
Iter: 980 loss: 3.44344267e-06
Iter: 981 loss: 3.4409411e-06
Iter: 982 loss: 3.44086811e-06
Iter: 983 loss: 3.43723855e-06
Iter: 984 loss: 3.4556806e-06
Iter: 985 loss: 3.43670445e-06
Iter: 986 loss: 3.43407737e-06
Iter: 987 loss: 3.44949626e-06
Iter: 988 loss: 3.43375086e-06
Iter: 989 loss: 3.43136298e-06
Iter: 990 loss: 3.43834745e-06
Iter: 991 loss: 3.43061924e-06
Iter: 992 loss: 3.42857652e-06
Iter: 993 loss: 3.42957082e-06
Iter: 994 loss: 3.42721955e-06
Iter: 995 loss: 3.42460521e-06
Iter: 996 loss: 3.43919896e-06
Iter: 997 loss: 3.42410704e-06
Iter: 998 loss: 3.42234307e-06
Iter: 999 loss: 3.44300338e-06
Iter: 1000 loss: 3.42232715e-06
Iter: 1001 loss: 3.42072303e-06
Iter: 1002 loss: 3.42396311e-06
Iter: 1003 loss: 3.42002932e-06
Iter: 1004 loss: 3.41829491e-06
Iter: 1005 loss: 3.41690497e-06
Iter: 1006 loss: 3.41643226e-06
Iter: 1007 loss: 3.41468717e-06
Iter: 1008 loss: 3.42852491e-06
Iter: 1009 loss: 3.41454142e-06
Iter: 1010 loss: 3.41386885e-06
Iter: 1011 loss: 3.41359782e-06
Iter: 1012 loss: 3.41309487e-06
Iter: 1013 loss: 3.41139548e-06
Iter: 1014 loss: 3.40991e-06
Iter: 1015 loss: 3.40926863e-06
Iter: 1016 loss: 3.4058512e-06
Iter: 1017 loss: 3.42091698e-06
Iter: 1018 loss: 3.40541e-06
Iter: 1019 loss: 3.40293377e-06
Iter: 1020 loss: 3.40532824e-06
Iter: 1021 loss: 3.40156726e-06
Iter: 1022 loss: 3.39905546e-06
Iter: 1023 loss: 3.42807834e-06
Iter: 1024 loss: 3.39903227e-06
Iter: 1025 loss: 3.39708595e-06
Iter: 1026 loss: 3.4006398e-06
Iter: 1027 loss: 3.39643566e-06
Iter: 1028 loss: 3.39400913e-06
Iter: 1029 loss: 3.3961478e-06
Iter: 1030 loss: 3.39247572e-06
Iter: 1031 loss: 3.3906565e-06
Iter: 1032 loss: 3.40954443e-06
Iter: 1033 loss: 3.39057169e-06
Iter: 1034 loss: 3.38874861e-06
Iter: 1035 loss: 3.39347571e-06
Iter: 1036 loss: 3.38809969e-06
Iter: 1037 loss: 3.38611358e-06
Iter: 1038 loss: 3.39219332e-06
Iter: 1039 loss: 3.38551126e-06
Iter: 1040 loss: 3.38424843e-06
Iter: 1041 loss: 3.38497512e-06
Iter: 1042 loss: 3.38339078e-06
Iter: 1043 loss: 3.38290101e-06
Iter: 1044 loss: 3.38263976e-06
Iter: 1045 loss: 3.38165364e-06
Iter: 1046 loss: 3.37978167e-06
Iter: 1047 loss: 3.41595774e-06
Iter: 1048 loss: 3.37972915e-06
Iter: 1049 loss: 3.37855e-06
Iter: 1050 loss: 3.37746633e-06
Iter: 1051 loss: 3.37720098e-06
Iter: 1052 loss: 3.37507231e-06
Iter: 1053 loss: 3.37504707e-06
Iter: 1054 loss: 3.37339907e-06
Iter: 1055 loss: 3.37034339e-06
Iter: 1056 loss: 3.38031214e-06
Iter: 1057 loss: 3.36950052e-06
Iter: 1058 loss: 3.36690368e-06
Iter: 1059 loss: 3.38340601e-06
Iter: 1060 loss: 3.36666267e-06
Iter: 1061 loss: 3.36461812e-06
Iter: 1062 loss: 3.37639131e-06
Iter: 1063 loss: 3.36433959e-06
Iter: 1064 loss: 3.36253174e-06
Iter: 1065 loss: 3.36247786e-06
Iter: 1066 loss: 3.36105268e-06
Iter: 1067 loss: 3.3587794e-06
Iter: 1068 loss: 3.37275515e-06
Iter: 1069 loss: 3.35847449e-06
Iter: 1070 loss: 3.35662753e-06
Iter: 1071 loss: 3.37465349e-06
Iter: 1072 loss: 3.35651066e-06
Iter: 1073 loss: 3.35508275e-06
Iter: 1074 loss: 3.35482082e-06
Iter: 1075 loss: 3.35380309e-06
Iter: 1076 loss: 3.35218328e-06
Iter: 1077 loss: 3.35928371e-06
Iter: 1078 loss: 3.35183722e-06
Iter: 1079 loss: 3.35176537e-06
Iter: 1080 loss: 3.35120058e-06
Iter: 1081 loss: 3.35072764e-06
Iter: 1082 loss: 3.34932656e-06
Iter: 1083 loss: 3.34946299e-06
Iter: 1084 loss: 3.34789502e-06
Iter: 1085 loss: 3.34533661e-06
Iter: 1086 loss: 3.35058326e-06
Iter: 1087 loss: 3.34416109e-06
Iter: 1088 loss: 3.34199694e-06
Iter: 1089 loss: 3.34372771e-06
Iter: 1090 loss: 3.34086053e-06
Iter: 1091 loss: 3.33747357e-06
Iter: 1092 loss: 3.34286392e-06
Iter: 1093 loss: 3.33582693e-06
Iter: 1094 loss: 3.33334856e-06
Iter: 1095 loss: 3.34248671e-06
Iter: 1096 loss: 3.33265371e-06
Iter: 1097 loss: 3.33037588e-06
Iter: 1098 loss: 3.34911033e-06
Iter: 1099 loss: 3.33020034e-06
Iter: 1100 loss: 3.32826789e-06
Iter: 1101 loss: 3.33611024e-06
Iter: 1102 loss: 3.32785294e-06
Iter: 1103 loss: 3.32621016e-06
Iter: 1104 loss: 3.32668105e-06
Iter: 1105 loss: 3.3250185e-06
Iter: 1106 loss: 3.32357808e-06
Iter: 1107 loss: 3.32358377e-06
Iter: 1108 loss: 3.32235641e-06
Iter: 1109 loss: 3.32271043e-06
Iter: 1110 loss: 3.32138234e-06
Iter: 1111 loss: 3.32003856e-06
Iter: 1112 loss: 3.32790387e-06
Iter: 1113 loss: 3.3199135e-06
Iter: 1114 loss: 3.3185338e-06
Iter: 1115 loss: 3.33268508e-06
Iter: 1116 loss: 3.31846263e-06
Iter: 1117 loss: 3.31794126e-06
Iter: 1118 loss: 3.31624869e-06
Iter: 1119 loss: 3.32430454e-06
Iter: 1120 loss: 3.31565752e-06
Iter: 1121 loss: 3.31364458e-06
Iter: 1122 loss: 3.3184956e-06
Iter: 1123 loss: 3.31295223e-06
Iter: 1124 loss: 3.31092906e-06
Iter: 1125 loss: 3.31630895e-06
Iter: 1126 loss: 3.31034539e-06
Iter: 1127 loss: 3.30787771e-06
Iter: 1128 loss: 3.30736816e-06
Iter: 1129 loss: 3.30584226e-06
Iter: 1130 loss: 3.30299054e-06
Iter: 1131 loss: 3.32643049e-06
Iter: 1132 loss: 3.30281591e-06
Iter: 1133 loss: 3.30053695e-06
Iter: 1134 loss: 3.30132866e-06
Iter: 1135 loss: 3.29890918e-06
Iter: 1136 loss: 3.29702061e-06
Iter: 1137 loss: 3.29685759e-06
Iter: 1138 loss: 3.29535192e-06
Iter: 1139 loss: 3.29341401e-06
Iter: 1140 loss: 3.29317027e-06
Iter: 1141 loss: 3.2911089e-06
Iter: 1142 loss: 3.29112504e-06
Iter: 1143 loss: 3.28942133e-06
Iter: 1144 loss: 3.29057821e-06
Iter: 1145 loss: 3.2883645e-06
Iter: 1146 loss: 3.28795522e-06
Iter: 1147 loss: 3.28754481e-06
Iter: 1148 loss: 3.28692977e-06
Iter: 1149 loss: 3.28501733e-06
Iter: 1150 loss: 3.29759837e-06
Iter: 1151 loss: 3.28465785e-06
Iter: 1152 loss: 3.28220585e-06
Iter: 1153 loss: 3.28469537e-06
Iter: 1154 loss: 3.28071496e-06
Iter: 1155 loss: 3.27887165e-06
Iter: 1156 loss: 3.28282636e-06
Iter: 1157 loss: 3.27803104e-06
Iter: 1158 loss: 3.27519274e-06
Iter: 1159 loss: 3.27603675e-06
Iter: 1160 loss: 3.27329644e-06
Iter: 1161 loss: 3.27004591e-06
Iter: 1162 loss: 3.28639226e-06
Iter: 1163 loss: 3.26939676e-06
Iter: 1164 loss: 3.26657619e-06
Iter: 1165 loss: 3.26815029e-06
Iter: 1166 loss: 3.26460668e-06
Iter: 1167 loss: 3.26132249e-06
Iter: 1168 loss: 3.27521752e-06
Iter: 1169 loss: 3.2604562e-06
Iter: 1170 loss: 3.25769315e-06
Iter: 1171 loss: 3.27286375e-06
Iter: 1172 loss: 3.25734186e-06
Iter: 1173 loss: 3.2551693e-06
Iter: 1174 loss: 3.25501696e-06
Iter: 1175 loss: 3.25323344e-06
Iter: 1176 loss: 3.25074097e-06
Iter: 1177 loss: 3.25064752e-06
Iter: 1178 loss: 3.24924304e-06
Iter: 1179 loss: 3.25839619e-06
Iter: 1180 loss: 3.24907978e-06
Iter: 1181 loss: 3.24762641e-06
Iter: 1182 loss: 3.25607266e-06
Iter: 1183 loss: 3.24738312e-06
Iter: 1184 loss: 3.24605435e-06
Iter: 1185 loss: 3.24574853e-06
Iter: 1186 loss: 3.24484949e-06
Iter: 1187 loss: 3.2431758e-06
Iter: 1188 loss: 3.24117332e-06
Iter: 1189 loss: 3.24089433e-06
Iter: 1190 loss: 3.23884433e-06
Iter: 1191 loss: 3.24169446e-06
Iter: 1192 loss: 3.23783456e-06
Iter: 1193 loss: 3.23497306e-06
Iter: 1194 loss: 3.24331e-06
Iter: 1195 loss: 3.23408358e-06
Iter: 1196 loss: 3.23148834e-06
Iter: 1197 loss: 3.23866698e-06
Iter: 1198 loss: 3.23045492e-06
Iter: 1199 loss: 3.22761593e-06
Iter: 1200 loss: 3.22505252e-06
Iter: 1201 loss: 3.22435881e-06
Iter: 1202 loss: 3.22024925e-06
Iter: 1203 loss: 3.26816416e-06
Iter: 1204 loss: 3.22028836e-06
Iter: 1205 loss: 3.21736525e-06
Iter: 1206 loss: 3.21821676e-06
Iter: 1207 loss: 3.21526522e-06
Iter: 1208 loss: 3.21165135e-06
Iter: 1209 loss: 3.22815094e-06
Iter: 1210 loss: 3.21098514e-06
Iter: 1211 loss: 3.20772642e-06
Iter: 1212 loss: 3.22097731e-06
Iter: 1213 loss: 3.20696154e-06
Iter: 1214 loss: 3.20511276e-06
Iter: 1215 loss: 3.20494973e-06
Iter: 1216 loss: 3.20334107e-06
Iter: 1217 loss: 3.20698859e-06
Iter: 1218 loss: 3.20265394e-06
Iter: 1219 loss: 3.20141316e-06
Iter: 1220 loss: 3.20004642e-06
Iter: 1221 loss: 3.19981427e-06
Iter: 1222 loss: 3.19722812e-06
Iter: 1223 loss: 3.19768219e-06
Iter: 1224 loss: 3.19516539e-06
Iter: 1225 loss: 3.19299124e-06
Iter: 1226 loss: 3.19455285e-06
Iter: 1227 loss: 3.19153787e-06
Iter: 1228 loss: 3.18836646e-06
Iter: 1229 loss: 3.19373476e-06
Iter: 1230 loss: 3.18694e-06
Iter: 1231 loss: 3.18419825e-06
Iter: 1232 loss: 3.21032212e-06
Iter: 1233 loss: 3.18415505e-06
Iter: 1234 loss: 3.18181787e-06
Iter: 1235 loss: 3.18200046e-06
Iter: 1236 loss: 3.17995273e-06
Iter: 1237 loss: 3.17699073e-06
Iter: 1238 loss: 3.18583761e-06
Iter: 1239 loss: 3.17612603e-06
Iter: 1240 loss: 3.1733955e-06
Iter: 1241 loss: 3.17309832e-06
Iter: 1242 loss: 3.1712309e-06
Iter: 1243 loss: 3.16758769e-06
Iter: 1244 loss: 3.19890478e-06
Iter: 1245 loss: 3.16741398e-06
Iter: 1246 loss: 3.16488604e-06
Iter: 1247 loss: 3.18025423e-06
Iter: 1248 loss: 3.1647287e-06
Iter: 1249 loss: 3.16155956e-06
Iter: 1250 loss: 3.18015373e-06
Iter: 1251 loss: 3.16127944e-06
Iter: 1252 loss: 3.15908164e-06
Iter: 1253 loss: 3.16154365e-06
Iter: 1254 loss: 3.15771558e-06
Iter: 1255 loss: 3.15618217e-06
Iter: 1256 loss: 3.15713896e-06
Iter: 1257 loss: 3.15513353e-06
Iter: 1258 loss: 3.1528798e-06
Iter: 1259 loss: 3.15157786e-06
Iter: 1260 loss: 3.15061084e-06
Iter: 1261 loss: 3.14758654e-06
Iter: 1262 loss: 3.15248212e-06
Iter: 1263 loss: 3.14622139e-06
Iter: 1264 loss: 3.14307226e-06
Iter: 1265 loss: 3.15162697e-06
Iter: 1266 loss: 3.14197223e-06
Iter: 1267 loss: 3.13865121e-06
Iter: 1268 loss: 3.14871204e-06
Iter: 1269 loss: 3.13766577e-06
Iter: 1270 loss: 3.13399664e-06
Iter: 1271 loss: 3.14021963e-06
Iter: 1272 loss: 3.13223973e-06
Iter: 1273 loss: 3.12867314e-06
Iter: 1274 loss: 3.15011357e-06
Iter: 1275 loss: 3.12819157e-06
Iter: 1276 loss: 3.12557449e-06
Iter: 1277 loss: 3.12588622e-06
Iter: 1278 loss: 3.12358907e-06
Iter: 1279 loss: 3.11942676e-06
Iter: 1280 loss: 3.13319606e-06
Iter: 1281 loss: 3.11826489e-06
Iter: 1282 loss: 3.11602707e-06
Iter: 1283 loss: 3.1400823e-06
Iter: 1284 loss: 3.11601934e-06
Iter: 1285 loss: 3.11348163e-06
Iter: 1286 loss: 3.12864677e-06
Iter: 1287 loss: 3.11305712e-06
Iter: 1288 loss: 3.11176836e-06
Iter: 1289 loss: 3.11021086e-06
Iter: 1290 loss: 3.10997666e-06
Iter: 1291 loss: 3.10745418e-06
Iter: 1292 loss: 3.11424697e-06
Iter: 1293 loss: 3.10660789e-06
Iter: 1294 loss: 3.10486575e-06
Iter: 1295 loss: 3.10181713e-06
Iter: 1296 loss: 3.1017737e-06
Iter: 1297 loss: 3.09802454e-06
Iter: 1298 loss: 3.13664736e-06
Iter: 1299 loss: 3.097844e-06
Iter: 1300 loss: 3.09551115e-06
Iter: 1301 loss: 3.09084089e-06
Iter: 1302 loss: 3.19600758e-06
Iter: 1303 loss: 3.09080838e-06
Iter: 1304 loss: 3.08558788e-06
Iter: 1305 loss: 3.12845509e-06
Iter: 1306 loss: 3.08526523e-06
Iter: 1307 loss: 3.08100289e-06
Iter: 1308 loss: 3.09858956e-06
Iter: 1309 loss: 3.08012022e-06
Iter: 1310 loss: 3.07619757e-06
Iter: 1311 loss: 3.08573544e-06
Iter: 1312 loss: 3.0746894e-06
Iter: 1313 loss: 3.07158598e-06
Iter: 1314 loss: 3.07907885e-06
Iter: 1315 loss: 3.07036726e-06
Iter: 1316 loss: 3.06697075e-06
Iter: 1317 loss: 3.0687745e-06
Iter: 1318 loss: 3.06459151e-06
Iter: 1319 loss: 3.06103175e-06
Iter: 1320 loss: 3.09124152e-06
Iter: 1321 loss: 3.06077982e-06
Iter: 1322 loss: 3.06100674e-06
Iter: 1323 loss: 3.05937942e-06
Iter: 1324 loss: 3.05844e-06
Iter: 1325 loss: 3.05560297e-06
Iter: 1326 loss: 3.06968286e-06
Iter: 1327 loss: 3.05468711e-06
Iter: 1328 loss: 3.0528081e-06
Iter: 1329 loss: 3.05279445e-06
Iter: 1330 loss: 3.05089316e-06
Iter: 1331 loss: 3.04734931e-06
Iter: 1332 loss: 3.1256036e-06
Iter: 1333 loss: 3.04731793e-06
Iter: 1334 loss: 3.04378682e-06
Iter: 1335 loss: 3.05114418e-06
Iter: 1336 loss: 3.04257e-06
Iter: 1337 loss: 3.03860497e-06
Iter: 1338 loss: 3.04283958e-06
Iter: 1339 loss: 3.03663796e-06
Iter: 1340 loss: 3.03373622e-06
Iter: 1341 loss: 3.07536357e-06
Iter: 1342 loss: 3.03373281e-06
Iter: 1343 loss: 3.0313181e-06
Iter: 1344 loss: 3.03427169e-06
Iter: 1345 loss: 3.0298786e-06
Iter: 1346 loss: 3.02670333e-06
Iter: 1347 loss: 3.03257593e-06
Iter: 1348 loss: 3.02525723e-06
Iter: 1349 loss: 3.02256217e-06
Iter: 1350 loss: 3.03182492e-06
Iter: 1351 loss: 3.02206126e-06
Iter: 1352 loss: 3.01897057e-06
Iter: 1353 loss: 3.02029639e-06
Iter: 1354 loss: 3.01684486e-06
Iter: 1355 loss: 3.01366026e-06
Iter: 1356 loss: 3.02157741e-06
Iter: 1357 loss: 3.0125675e-06
Iter: 1358 loss: 3.01529985e-06
Iter: 1359 loss: 3.01147361e-06
Iter: 1360 loss: 3.0105839e-06
Iter: 1361 loss: 3.00811371e-06
Iter: 1362 loss: 3.02052058e-06
Iter: 1363 loss: 3.00717738e-06
Iter: 1364 loss: 3.00517559e-06
Iter: 1365 loss: 3.0258027e-06
Iter: 1366 loss: 3.00511897e-06
Iter: 1367 loss: 3.00311126e-06
Iter: 1368 loss: 3.00290822e-06
Iter: 1369 loss: 3.00141892e-06
Iter: 1370 loss: 2.99934527e-06
Iter: 1371 loss: 3.00049805e-06
Iter: 1372 loss: 2.99799899e-06
Iter: 1373 loss: 2.99497788e-06
Iter: 1374 loss: 2.99575458e-06
Iter: 1375 loss: 2.9926664e-06
Iter: 1376 loss: 2.98963096e-06
Iter: 1377 loss: 3.00956663e-06
Iter: 1378 loss: 2.9893281e-06
Iter: 1379 loss: 2.98661e-06
Iter: 1380 loss: 2.98697432e-06
Iter: 1381 loss: 2.98454961e-06
Iter: 1382 loss: 2.9817179e-06
Iter: 1383 loss: 3.00033435e-06
Iter: 1384 loss: 2.98140048e-06
Iter: 1385 loss: 2.97872066e-06
Iter: 1386 loss: 2.99412022e-06
Iter: 1387 loss: 2.97844963e-06
Iter: 1388 loss: 2.97611314e-06
Iter: 1389 loss: 2.98358805e-06
Iter: 1390 loss: 2.97550923e-06
Iter: 1391 loss: 2.97382257e-06
Iter: 1392 loss: 2.97754013e-06
Iter: 1393 loss: 2.97321685e-06
Iter: 1394 loss: 2.9703956e-06
Iter: 1395 loss: 2.98506529e-06
Iter: 1396 loss: 2.96995836e-06
Iter: 1397 loss: 2.96910457e-06
Iter: 1398 loss: 2.96694907e-06
Iter: 1399 loss: 2.99619114e-06
Iter: 1400 loss: 2.9669136e-06
Iter: 1401 loss: 2.9652374e-06
Iter: 1402 loss: 2.96524468e-06
Iter: 1403 loss: 2.96387771e-06
Iter: 1404 loss: 2.96089684e-06
Iter: 1405 loss: 3.01221189e-06
Iter: 1406 loss: 2.96083249e-06
Iter: 1407 loss: 2.9579885e-06
Iter: 1408 loss: 2.96742814e-06
Iter: 1409 loss: 2.95716677e-06
Iter: 1410 loss: 2.95460586e-06
Iter: 1411 loss: 2.96529925e-06
Iter: 1412 loss: 2.95408654e-06
Iter: 1413 loss: 2.95177597e-06
Iter: 1414 loss: 2.95267e-06
Iter: 1415 loss: 2.95016343e-06
Iter: 1416 loss: 2.94740425e-06
Iter: 1417 loss: 2.95827203e-06
Iter: 1418 loss: 2.94682286e-06
Iter: 1419 loss: 2.94464871e-06
Iter: 1420 loss: 2.95329414e-06
Iter: 1421 loss: 2.94405731e-06
Iter: 1422 loss: 2.94164511e-06
Iter: 1423 loss: 2.94121196e-06
Iter: 1424 loss: 2.93965286e-06
Iter: 1425 loss: 2.93790072e-06
Iter: 1426 loss: 2.93794665e-06
Iter: 1427 loss: 2.93678613e-06
Iter: 1428 loss: 2.93677067e-06
Iter: 1429 loss: 2.93564381e-06
Iter: 1430 loss: 2.93588892e-06
Iter: 1431 loss: 2.93488529e-06
Iter: 1432 loss: 2.93387e-06
Iter: 1433 loss: 2.93196854e-06
Iter: 1434 loss: 2.9721873e-06
Iter: 1435 loss: 2.93194739e-06
Iter: 1436 loss: 2.93013454e-06
Iter: 1437 loss: 2.94430401e-06
Iter: 1438 loss: 2.93002677e-06
Iter: 1439 loss: 2.92801292e-06
Iter: 1440 loss: 2.93008975e-06
Iter: 1441 loss: 2.9269595e-06
Iter: 1442 loss: 2.925329e-06
Iter: 1443 loss: 2.92466666e-06
Iter: 1444 loss: 2.92396e-06
Iter: 1445 loss: 2.92153709e-06
Iter: 1446 loss: 2.92245886e-06
Iter: 1447 loss: 2.91989159e-06
Iter: 1448 loss: 2.91793867e-06
Iter: 1449 loss: 2.91797733e-06
Iter: 1450 loss: 2.91627293e-06
Iter: 1451 loss: 2.91618471e-06
Iter: 1452 loss: 2.91496463e-06
Iter: 1453 loss: 2.91268907e-06
Iter: 1454 loss: 2.92432628e-06
Iter: 1455 loss: 2.91215565e-06
Iter: 1456 loss: 2.91064134e-06
Iter: 1457 loss: 2.90945718e-06
Iter: 1458 loss: 2.9087339e-06
Iter: 1459 loss: 2.90635762e-06
Iter: 1460 loss: 2.9301043e-06
Iter: 1461 loss: 2.90624757e-06
Iter: 1462 loss: 2.90632761e-06
Iter: 1463 loss: 2.90544858e-06
Iter: 1464 loss: 2.90469279e-06
Iter: 1465 loss: 2.90282355e-06
Iter: 1466 loss: 2.92532923e-06
Iter: 1467 loss: 2.90271873e-06
Iter: 1468 loss: 2.90146727e-06
Iter: 1469 loss: 2.90483786e-06
Iter: 1470 loss: 2.90104754e-06
Iter: 1471 loss: 2.89944228e-06
Iter: 1472 loss: 2.89962645e-06
Iter: 1473 loss: 2.89821355e-06
Iter: 1474 loss: 2.89664467e-06
Iter: 1475 loss: 2.89664831e-06
Iter: 1476 loss: 2.89525315e-06
Iter: 1477 loss: 2.89425702e-06
Iter: 1478 loss: 2.89390937e-06
Iter: 1479 loss: 2.89218451e-06
Iter: 1480 loss: 2.89042509e-06
Iter: 1481 loss: 2.88996171e-06
Iter: 1482 loss: 2.88755609e-06
Iter: 1483 loss: 2.90368462e-06
Iter: 1484 loss: 2.88738329e-06
Iter: 1485 loss: 2.88509273e-06
Iter: 1486 loss: 2.88536467e-06
Iter: 1487 loss: 2.88334058e-06
Iter: 1488 loss: 2.88086881e-06
Iter: 1489 loss: 2.89433137e-06
Iter: 1490 loss: 2.88045612e-06
Iter: 1491 loss: 2.87826242e-06
Iter: 1492 loss: 2.88178262e-06
Iter: 1493 loss: 2.87720468e-06
Iter: 1494 loss: 2.8783852e-06
Iter: 1495 loss: 2.8765437e-06
Iter: 1496 loss: 2.87606122e-06
Iter: 1497 loss: 2.87533317e-06
Iter: 1498 loss: 2.87522926e-06
Iter: 1499 loss: 2.87422745e-06
Iter: 1500 loss: 2.87232933e-06
Iter: 1501 loss: 2.91592346e-06
Iter: 1502 loss: 2.87233661e-06
Iter: 1503 loss: 2.87044168e-06
Iter: 1504 loss: 2.89140985e-06
Iter: 1505 loss: 2.87028297e-06
Iter: 1506 loss: 2.86900013e-06
Iter: 1507 loss: 2.87480793e-06
Iter: 1508 loss: 2.86864679e-06
Iter: 1509 loss: 2.86707473e-06
Iter: 1510 loss: 2.86900695e-06
Iter: 1511 loss: 2.86620707e-06
Iter: 1512 loss: 2.86447676e-06
Iter: 1513 loss: 2.86676163e-06
Iter: 1514 loss: 2.86343356e-06
Iter: 1515 loss: 2.86160684e-06
Iter: 1516 loss: 2.86044929e-06
Iter: 1517 loss: 2.85970555e-06
Iter: 1518 loss: 2.85743658e-06
Iter: 1519 loss: 2.86542445e-06
Iter: 1520 loss: 2.85678402e-06
Iter: 1521 loss: 2.85444617e-06
Iter: 1522 loss: 2.86674231e-06
Iter: 1523 loss: 2.85409669e-06
Iter: 1524 loss: 2.85216902e-06
Iter: 1525 loss: 2.85664828e-06
Iter: 1526 loss: 2.85152873e-06
Iter: 1527 loss: 2.84959151e-06
Iter: 1528 loss: 2.85853912e-06
Iter: 1529 loss: 2.84934276e-06
Iter: 1530 loss: 2.84795283e-06
Iter: 1531 loss: 2.84792441e-06
Iter: 1532 loss: 2.84734188e-06
Iter: 1533 loss: 2.84569455e-06
Iter: 1534 loss: 2.84965427e-06
Iter: 1535 loss: 2.84469525e-06
Iter: 1536 loss: 2.84208022e-06
Iter: 1537 loss: 2.85347551e-06
Iter: 1538 loss: 2.84161479e-06
Iter: 1539 loss: 2.83981399e-06
Iter: 1540 loss: 2.8585946e-06
Iter: 1541 loss: 2.83977442e-06
Iter: 1542 loss: 2.8383547e-06
Iter: 1543 loss: 2.84171438e-06
Iter: 1544 loss: 2.8378829e-06
Iter: 1545 loss: 2.83631607e-06
Iter: 1546 loss: 2.83758686e-06
Iter: 1547 loss: 2.83536929e-06
Iter: 1548 loss: 2.8336874e-06
Iter: 1549 loss: 2.83755207e-06
Iter: 1550 loss: 2.83293e-06
Iter: 1551 loss: 2.83134978e-06
Iter: 1552 loss: 2.82956535e-06
Iter: 1553 loss: 2.82913038e-06
Iter: 1554 loss: 2.82662882e-06
Iter: 1555 loss: 2.84586417e-06
Iter: 1556 loss: 2.82646511e-06
Iter: 1557 loss: 2.82457404e-06
Iter: 1558 loss: 2.82445035e-06
Iter: 1559 loss: 2.82304973e-06
Iter: 1560 loss: 2.82073393e-06
Iter: 1561 loss: 2.84090265e-06
Iter: 1562 loss: 2.82053429e-06
Iter: 1563 loss: 2.82076917e-06
Iter: 1564 loss: 2.81976327e-06
Iter: 1565 loss: 2.81894154e-06
Iter: 1566 loss: 2.8172326e-06
Iter: 1567 loss: 2.85091846e-06
Iter: 1568 loss: 2.81715688e-06
Iter: 1569 loss: 2.81608163e-06
Iter: 1570 loss: 2.81466555e-06
Iter: 1571 loss: 2.81446501e-06
Iter: 1572 loss: 2.81191024e-06
Iter: 1573 loss: 2.81866596e-06
Iter: 1574 loss: 2.81108305e-06
Iter: 1575 loss: 2.80911445e-06
Iter: 1576 loss: 2.80923905e-06
Iter: 1577 loss: 2.80809627e-06
Iter: 1578 loss: 2.80923791e-06
Iter: 1579 loss: 2.80749464e-06
Iter: 1580 loss: 2.80595623e-06
Iter: 1581 loss: 2.80458835e-06
Iter: 1582 loss: 2.80414042e-06
Iter: 1583 loss: 2.80208246e-06
Iter: 1584 loss: 2.82032443e-06
Iter: 1585 loss: 2.80203858e-06
Iter: 1586 loss: 2.80053291e-06
Iter: 1587 loss: 2.7982469e-06
Iter: 1588 loss: 2.79822689e-06
Iter: 1589 loss: 2.79565938e-06
Iter: 1590 loss: 2.80818e-06
Iter: 1591 loss: 2.7953779e-06
Iter: 1592 loss: 2.79252845e-06
Iter: 1593 loss: 2.79501592e-06
Iter: 1594 loss: 2.79080837e-06
Iter: 1595 loss: 2.78842458e-06
Iter: 1596 loss: 2.79359529e-06
Iter: 1597 loss: 2.78747893e-06
Iter: 1598 loss: 2.79046299e-06
Iter: 1599 loss: 2.78660855e-06
Iter: 1600 loss: 2.78599282e-06
Iter: 1601 loss: 2.78433936e-06
Iter: 1602 loss: 2.79613664e-06
Iter: 1603 loss: 2.78396601e-06
Iter: 1604 loss: 2.78255357e-06
Iter: 1605 loss: 2.78452194e-06
Iter: 1606 loss: 2.78178345e-06
Iter: 1607 loss: 2.77994423e-06
Iter: 1608 loss: 2.78414291e-06
Iter: 1609 loss: 2.77926119e-06
Iter: 1610 loss: 2.77730396e-06
Iter: 1611 loss: 2.79955862e-06
Iter: 1612 loss: 2.77730078e-06
Iter: 1613 loss: 2.77610798e-06
Iter: 1614 loss: 2.77676645e-06
Iter: 1615 loss: 2.77542904e-06
Iter: 1616 loss: 2.77387085e-06
Iter: 1617 loss: 2.77398863e-06
Iter: 1618 loss: 2.77270465e-06
Iter: 1619 loss: 2.77048366e-06
Iter: 1620 loss: 2.77512845e-06
Iter: 1621 loss: 2.76949822e-06
Iter: 1622 loss: 2.76787068e-06
Iter: 1623 loss: 2.77029358e-06
Iter: 1624 loss: 2.76688434e-06
Iter: 1625 loss: 2.76485116e-06
Iter: 1626 loss: 2.77409981e-06
Iter: 1627 loss: 2.76434662e-06
Iter: 1628 loss: 2.76268815e-06
Iter: 1629 loss: 2.76043147e-06
Iter: 1630 loss: 2.76018818e-06
Iter: 1631 loss: 2.76274568e-06
Iter: 1632 loss: 2.7595845e-06
Iter: 1633 loss: 2.75874527e-06
Iter: 1634 loss: 2.75751063e-06
Iter: 1635 loss: 2.75747016e-06
Iter: 1636 loss: 2.75611615e-06
Iter: 1637 loss: 2.754246e-06
Iter: 1638 loss: 2.75426964e-06
Iter: 1639 loss: 2.75232446e-06
Iter: 1640 loss: 2.76156743e-06
Iter: 1641 loss: 2.75204093e-06
Iter: 1642 loss: 2.75008415e-06
Iter: 1643 loss: 2.75248385e-06
Iter: 1644 loss: 2.74913532e-06
Iter: 1645 loss: 2.74760987e-06
Iter: 1646 loss: 2.7476076e-06
Iter: 1647 loss: 2.74676904e-06
Iter: 1648 loss: 2.74579497e-06
Iter: 1649 loss: 2.74570402e-06
Iter: 1650 loss: 2.74415379e-06
Iter: 1651 loss: 2.74634522e-06
Iter: 1652 loss: 2.74330773e-06
Iter: 1653 loss: 2.74161357e-06
Iter: 1654 loss: 2.75327307e-06
Iter: 1655 loss: 2.74137824e-06
Iter: 1656 loss: 2.74017657e-06
Iter: 1657 loss: 2.73828846e-06
Iter: 1658 loss: 2.73824435e-06
Iter: 1659 loss: 2.7360943e-06
Iter: 1660 loss: 2.76416313e-06
Iter: 1661 loss: 2.73608384e-06
Iter: 1662 loss: 2.73484329e-06
Iter: 1663 loss: 2.73485057e-06
Iter: 1664 loss: 2.73373462e-06
Iter: 1665 loss: 2.73488649e-06
Iter: 1666 loss: 2.73304386e-06
Iter: 1667 loss: 2.7325e-06
Iter: 1668 loss: 2.73101477e-06
Iter: 1669 loss: 2.73886735e-06
Iter: 1670 loss: 2.73037858e-06
Iter: 1671 loss: 2.72892248e-06
Iter: 1672 loss: 2.73098294e-06
Iter: 1673 loss: 2.7281983e-06
Iter: 1674 loss: 2.72594457e-06
Iter: 1675 loss: 2.74012723e-06
Iter: 1676 loss: 2.72575835e-06
Iter: 1677 loss: 2.72432362e-06
Iter: 1678 loss: 2.74035574e-06
Iter: 1679 loss: 2.72436228e-06
Iter: 1680 loss: 2.7229994e-06
Iter: 1681 loss: 2.72350644e-06
Iter: 1682 loss: 2.72218199e-06
Iter: 1683 loss: 2.72074658e-06
Iter: 1684 loss: 2.72189959e-06
Iter: 1685 loss: 2.71981457e-06
Iter: 1686 loss: 2.7184542e-06
Iter: 1687 loss: 2.72547732e-06
Iter: 1688 loss: 2.71817044e-06
Iter: 1689 loss: 2.71682097e-06
Iter: 1690 loss: 2.71823e-06
Iter: 1691 loss: 2.71614567e-06
Iter: 1692 loss: 2.71453155e-06
Iter: 1693 loss: 2.71334829e-06
Iter: 1694 loss: 2.71279509e-06
Iter: 1695 loss: 2.71047702e-06
Iter: 1696 loss: 2.73481828e-06
Iter: 1697 loss: 2.71035242e-06
Iter: 1698 loss: 2.70905e-06
Iter: 1699 loss: 2.71826093e-06
Iter: 1700 loss: 2.70891019e-06
Iter: 1701 loss: 2.70780629e-06
Iter: 1702 loss: 2.72568036e-06
Iter: 1703 loss: 2.7078213e-06
Iter: 1704 loss: 2.70734699e-06
Iter: 1705 loss: 2.70577175e-06
Iter: 1706 loss: 2.70932469e-06
Iter: 1707 loss: 2.70489045e-06
Iter: 1708 loss: 2.70343276e-06
Iter: 1709 loss: 2.7068204e-06
Iter: 1710 loss: 2.70294981e-06
Iter: 1711 loss: 2.70122155e-06
Iter: 1712 loss: 2.70385453e-06
Iter: 1713 loss: 2.7002061e-06
Iter: 1714 loss: 2.69954e-06
Iter: 1715 loss: 2.69931434e-06
Iter: 1716 loss: 2.69843554e-06
Iter: 1717 loss: 2.69854513e-06
Iter: 1718 loss: 2.69769885e-06
Iter: 1719 loss: 2.69653606e-06
Iter: 1720 loss: 2.69942984e-06
Iter: 1721 loss: 2.69605198e-06
Iter: 1722 loss: 2.6948228e-06
Iter: 1723 loss: 2.69792486e-06
Iter: 1724 loss: 2.69451129e-06
Iter: 1725 loss: 2.69337283e-06
Iter: 1726 loss: 2.69616021e-06
Iter: 1727 loss: 2.69301563e-06
Iter: 1728 loss: 2.69203042e-06
Iter: 1729 loss: 2.69335442e-06
Iter: 1730 loss: 2.69149223e-06
Iter: 1731 loss: 2.6902942e-06
Iter: 1732 loss: 2.6907453e-06
Iter: 1733 loss: 2.68955455e-06
Iter: 1734 loss: 2.69044494e-06
Iter: 1735 loss: 2.68899043e-06
Iter: 1736 loss: 2.68854046e-06
Iter: 1737 loss: 2.68792837e-06
Iter: 1738 loss: 2.70302417e-06
Iter: 1739 loss: 2.68799067e-06
Iter: 1740 loss: 2.68738358e-06
Iter: 1741 loss: 2.68606618e-06
Iter: 1742 loss: 2.70104692e-06
Iter: 1743 loss: 2.68601138e-06
Iter: 1744 loss: 2.68479516e-06
Iter: 1745 loss: 2.6894013e-06
Iter: 1746 loss: 2.68451322e-06
Iter: 1747 loss: 2.68342228e-06
Iter: 1748 loss: 2.68180656e-06
Iter: 1749 loss: 2.68174381e-06
Iter: 1750 loss: 2.68007648e-06
Iter: 1751 loss: 2.69500288e-06
Iter: 1752 loss: 2.67992505e-06
Iter: 1753 loss: 2.67910286e-06
Iter: 1754 loss: 2.68876556e-06
Iter: 1755 loss: 2.67904284e-06
Iter: 1756 loss: 2.67814e-06
Iter: 1757 loss: 2.67679775e-06
Iter: 1758 loss: 2.67671544e-06
Iter: 1759 loss: 2.67531959e-06
Iter: 1760 loss: 2.68309645e-06
Iter: 1761 loss: 2.67515111e-06
Iter: 1762 loss: 2.67421547e-06
Iter: 1763 loss: 2.67391761e-06
Iter: 1764 loss: 2.67332507e-06
Iter: 1765 loss: 2.67161022e-06
Iter: 1766 loss: 2.67841347e-06
Iter: 1767 loss: 2.67128075e-06
Iter: 1768 loss: 2.67063706e-06
Iter: 1769 loss: 2.67057862e-06
Iter: 1770 loss: 2.66979146e-06
Iter: 1771 loss: 2.67344808e-06
Iter: 1772 loss: 2.66971983e-06
Iter: 1773 loss: 2.66916959e-06
Iter: 1774 loss: 2.6680591e-06
Iter: 1775 loss: 2.6789503e-06
Iter: 1776 loss: 2.66794223e-06
Iter: 1777 loss: 2.66685151e-06
Iter: 1778 loss: 2.66567213e-06
Iter: 1779 loss: 2.66549932e-06
Iter: 1780 loss: 2.66369807e-06
Iter: 1781 loss: 2.67962741e-06
Iter: 1782 loss: 2.66358916e-06
Iter: 1783 loss: 2.66228767e-06
Iter: 1784 loss: 2.66406096e-06
Iter: 1785 loss: 2.66158054e-06
Iter: 1786 loss: 2.66010238e-06
Iter: 1787 loss: 2.67111136e-06
Iter: 1788 loss: 2.66003872e-06
Iter: 1789 loss: 2.6589787e-06
Iter: 1790 loss: 2.66806728e-06
Iter: 1791 loss: 2.65889548e-06
Iter: 1792 loss: 2.65814879e-06
Iter: 1793 loss: 2.65695667e-06
Iter: 1794 loss: 2.65695326e-06
Iter: 1795 loss: 2.65535868e-06
Iter: 1796 loss: 2.65751623e-06
Iter: 1797 loss: 2.65453491e-06
Iter: 1798 loss: 2.65318363e-06
Iter: 1799 loss: 2.65582366e-06
Iter: 1800 loss: 2.65251538e-06
Iter: 1801 loss: 2.65102858e-06
Iter: 1802 loss: 2.66300162e-06
Iter: 1803 loss: 2.65087556e-06
Iter: 1804 loss: 2.65100198e-06
Iter: 1805 loss: 2.65044605e-06
Iter: 1806 loss: 2.65002109e-06
Iter: 1807 loss: 2.64910591e-06
Iter: 1808 loss: 2.66943789e-06
Iter: 1809 loss: 2.64909022e-06
Iter: 1810 loss: 2.64816e-06
Iter: 1811 loss: 2.64662867e-06
Iter: 1812 loss: 2.6466023e-06
Iter: 1813 loss: 2.64514347e-06
Iter: 1814 loss: 2.65054314e-06
Iter: 1815 loss: 2.64480786e-06
Iter: 1816 loss: 2.64328537e-06
Iter: 1817 loss: 2.64264895e-06
Iter: 1818 loss: 2.6418486e-06
Iter: 1819 loss: 2.64031473e-06
Iter: 1820 loss: 2.65446988e-06
Iter: 1821 loss: 2.64026767e-06
Iter: 1822 loss: 2.63881884e-06
Iter: 1823 loss: 2.64329969e-06
Iter: 1824 loss: 2.63846414e-06
Iter: 1825 loss: 2.63704783e-06
Iter: 1826 loss: 2.64779692e-06
Iter: 1827 loss: 2.6369371e-06
Iter: 1828 loss: 2.63594711e-06
Iter: 1829 loss: 2.63717038e-06
Iter: 1830 loss: 2.63551101e-06
Iter: 1831 loss: 2.63444463e-06
Iter: 1832 loss: 2.63266656e-06
Iter: 1833 loss: 2.67456517e-06
Iter: 1834 loss: 2.6325913e-06
Iter: 1835 loss: 2.63106949e-06
Iter: 1836 loss: 2.64731034e-06
Iter: 1837 loss: 2.63101242e-06
Iter: 1838 loss: 2.6295761e-06
Iter: 1839 loss: 2.63031416e-06
Iter: 1840 loss: 2.62865842e-06
Iter: 1841 loss: 2.62998333e-06
Iter: 1842 loss: 2.62801905e-06
Iter: 1843 loss: 2.62749927e-06
Iter: 1844 loss: 2.62651065e-06
Iter: 1845 loss: 2.64727123e-06
Iter: 1846 loss: 2.62647495e-06
Iter: 1847 loss: 2.62530739e-06
Iter: 1848 loss: 2.62563503e-06
Iter: 1849 loss: 2.62456183e-06
Iter: 1850 loss: 2.62309777e-06
Iter: 1851 loss: 2.62637559e-06
Iter: 1852 loss: 2.62259618e-06
Iter: 1853 loss: 2.62146159e-06
Iter: 1854 loss: 2.6213711e-06
Iter: 1855 loss: 2.62036019e-06
Iter: 1856 loss: 2.61878563e-06
Iter: 1857 loss: 2.62632511e-06
Iter: 1858 loss: 2.61843661e-06
Iter: 1859 loss: 2.6176715e-06
Iter: 1860 loss: 2.61754599e-06
Iter: 1861 loss: 2.61665218e-06
Iter: 1862 loss: 2.61607511e-06
Iter: 1863 loss: 2.61585683e-06
Iter: 1864 loss: 2.61477294e-06
Iter: 1865 loss: 2.61801642e-06
Iter: 1866 loss: 2.61445552e-06
Iter: 1867 loss: 2.61360265e-06
Iter: 1868 loss: 2.61498326e-06
Iter: 1869 loss: 2.61322884e-06
Iter: 1870 loss: 2.6120515e-06
Iter: 1871 loss: 2.61193236e-06
Iter: 1872 loss: 2.61098785e-06
Iter: 1873 loss: 2.60966408e-06
Iter: 1874 loss: 2.61328864e-06
Iter: 1875 loss: 2.60932848e-06
Iter: 1876 loss: 2.60843535e-06
Iter: 1877 loss: 2.60832712e-06
Iter: 1878 loss: 2.60795559e-06
Iter: 1879 loss: 2.60720344e-06
Iter: 1880 loss: 2.61202695e-06
Iter: 1881 loss: 2.60697584e-06
Iter: 1882 loss: 2.60614979e-06
Iter: 1883 loss: 2.60848356e-06
Iter: 1884 loss: 2.60576167e-06
Iter: 1885 loss: 2.60470347e-06
Iter: 1886 loss: 2.60693741e-06
Iter: 1887 loss: 2.60422166e-06
Iter: 1888 loss: 2.60329534e-06
Iter: 1889 loss: 2.60253296e-06
Iter: 1890 loss: 2.60239085e-06
Iter: 1891 loss: 2.6008106e-06
Iter: 1892 loss: 2.61347986e-06
Iter: 1893 loss: 2.60070715e-06
Iter: 1894 loss: 2.59947774e-06
Iter: 1895 loss: 2.61258629e-06
Iter: 1896 loss: 2.59948183e-06
Iter: 1897 loss: 2.59880608e-06
Iter: 1898 loss: 2.59826311e-06
Iter: 1899 loss: 2.59818898e-06
Iter: 1900 loss: 2.59687886e-06
Iter: 1901 loss: 2.59569561e-06
Iter: 1902 loss: 2.59544754e-06
Iter: 1903 loss: 2.59441572e-06
Iter: 1904 loss: 2.59434819e-06
Iter: 1905 loss: 2.59340231e-06
Iter: 1906 loss: 2.59248236e-06
Iter: 1907 loss: 2.59234844e-06
Iter: 1908 loss: 2.59125954e-06
Iter: 1909 loss: 2.60614479e-06
Iter: 1910 loss: 2.59125682e-06
Iter: 1911 loss: 2.59034209e-06
Iter: 1912 loss: 2.60344154e-06
Iter: 1913 loss: 2.59039916e-06
Iter: 1914 loss: 2.59006288e-06
Iter: 1915 loss: 2.58886166e-06
Iter: 1916 loss: 2.59060107e-06
Iter: 1917 loss: 2.58798013e-06
Iter: 1918 loss: 2.58683804e-06
Iter: 1919 loss: 2.60076376e-06
Iter: 1920 loss: 2.58683781e-06
Iter: 1921 loss: 2.58576029e-06
Iter: 1922 loss: 2.58615e-06
Iter: 1923 loss: 2.58497744e-06
Iter: 1924 loss: 2.58358614e-06
Iter: 1925 loss: 2.58694854e-06
Iter: 1926 loss: 2.58303362e-06
Iter: 1927 loss: 2.58189493e-06
Iter: 1928 loss: 2.58104e-06
Iter: 1929 loss: 2.58068144e-06
Iter: 1930 loss: 2.57978854e-06
Iter: 1931 loss: 2.57964484e-06
Iter: 1932 loss: 2.57846659e-06
Iter: 1933 loss: 2.5808838e-06
Iter: 1934 loss: 2.57803822e-06
Iter: 1935 loss: 2.57711054e-06
Iter: 1936 loss: 2.57587476e-06
Iter: 1937 loss: 2.57586453e-06
Iter: 1938 loss: 2.57430497e-06
Iter: 1939 loss: 2.57902548e-06
Iter: 1940 loss: 2.57394504e-06
Iter: 1941 loss: 2.57247029e-06
Iter: 1942 loss: 2.57708734e-06
Iter: 1943 loss: 2.57195e-06
Iter: 1944 loss: 2.57343982e-06
Iter: 1945 loss: 2.57152897e-06
Iter: 1946 loss: 2.57118563e-06
Iter: 1947 loss: 2.57016882e-06
Iter: 1948 loss: 2.57645297e-06
Iter: 1949 loss: 2.56994804e-06
Iter: 1950 loss: 2.56898397e-06
Iter: 1951 loss: 2.56817111e-06
Iter: 1952 loss: 2.56789963e-06
Iter: 1953 loss: 2.5667141e-06
Iter: 1954 loss: 2.58071691e-06
Iter: 1955 loss: 2.56669432e-06
Iter: 1956 loss: 2.5654781e-06
Iter: 1957 loss: 2.56497287e-06
Iter: 1958 loss: 2.56443491e-06
Iter: 1959 loss: 2.56299495e-06
Iter: 1960 loss: 2.57204147e-06
Iter: 1961 loss: 2.56288081e-06
Iter: 1962 loss: 2.56179101e-06
Iter: 1963 loss: 2.56237672e-06
Iter: 1964 loss: 2.56113481e-06
Iter: 1965 loss: 2.55932446e-06
Iter: 1966 loss: 2.56628778e-06
Iter: 1967 loss: 2.55899522e-06
Iter: 1968 loss: 2.55814302e-06
Iter: 1969 loss: 2.56419435e-06
Iter: 1970 loss: 2.55805344e-06
Iter: 1971 loss: 2.55722921e-06
Iter: 1972 loss: 2.56185695e-06
Iter: 1973 loss: 2.55706937e-06
Iter: 1974 loss: 2.55635746e-06
Iter: 1975 loss: 2.55492114e-06
Iter: 1976 loss: 2.58695945e-06
Iter: 1977 loss: 2.55490909e-06
Iter: 1978 loss: 2.55387886e-06
Iter: 1979 loss: 2.5573238e-06
Iter: 1980 loss: 2.55361829e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.8
+ date
Wed Oct 21 15:46:49 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.4/300_300_300_1 --function f1 --psi -2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cfe9d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8d065488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8d0548c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cfaf730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cfaf620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cf98c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ced1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cf457b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cf2d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cdaa510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ce68400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ce7e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ce7eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cde0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cde0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8cde0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b8e7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b8c3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b8eb158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b80eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b83b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b83b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ce18840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ce51598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d8ce51510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44714ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44684730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d446c1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d446c18c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d6b7ce7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d445f98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44681158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44646620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44652488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44610840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1d44558730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.101940334
test_loss: 0.09406038
train_loss: 0.06699291
test_loss: 0.06713697
train_loss: 0.039990783
test_loss: 0.044179928
train_loss: 0.029798746
test_loss: 0.031307947
train_loss: 0.026003132
test_loss: 0.027690325
train_loss: 0.02056898
test_loss: 0.023340937
train_loss: 0.019786207
test_loss: 0.023322588
train_loss: 0.018373638
test_loss: 0.020428736
train_loss: 0.016311305
test_loss: 0.018775418
train_loss: 0.014862297
test_loss: 0.017291034
train_loss: 0.015425305
test_loss: 0.017462777
train_loss: 0.015332253
test_loss: 0.016851079
train_loss: 0.015965234
test_loss: 0.017112073
train_loss: 0.013723678
test_loss: 0.015952686
train_loss: 0.0138883535
test_loss: 0.016257724
train_loss: 0.012701608
test_loss: 0.015339072
train_loss: 0.013845034
test_loss: 0.0150267985
train_loss: 0.012176029
test_loss: 0.014810327
train_loss: 0.012058962
test_loss: 0.014584965
train_loss: 0.012376024
test_loss: 0.01425511
train_loss: 0.013054087
test_loss: 0.014078218
train_loss: 0.012158741
test_loss: 0.013824467
train_loss: 0.011234304
test_loss: 0.013592578
train_loss: 0.011233607
test_loss: 0.013557482
train_loss: 0.011102913
test_loss: 0.013096092
train_loss: 0.011164635
test_loss: 0.0130320545
train_loss: 0.010633839
test_loss: 0.013369295
train_loss: 0.0106830085
test_loss: 0.01334831
train_loss: 0.009895794
test_loss: 0.01270021
train_loss: 0.010662867
test_loss: 0.013249732
train_loss: 0.009791948
test_loss: 0.0126400795
train_loss: 0.009646252
test_loss: 0.012177275
train_loss: 0.01065975
test_loss: 0.012624376
train_loss: 0.009871649
test_loss: 0.01301721
train_loss: 0.010127351
test_loss: 0.012300397
train_loss: 0.010592316
test_loss: 0.012245706
train_loss: 0.010358607
test_loss: 0.012337129
train_loss: 0.010108441
test_loss: 0.012144786
train_loss: 0.011254587
test_loss: 0.012436244
train_loss: 0.010424375
test_loss: 0.01231458
train_loss: 0.008953281
test_loss: 0.012401982
train_loss: 0.009552434
test_loss: 0.012348376
train_loss: 0.009804811
test_loss: 0.01265115
train_loss: 0.0093177315
test_loss: 0.011788763
train_loss: 0.009609973
test_loss: 0.01213573
train_loss: 0.009797441
test_loss: 0.012169466
train_loss: 0.009685341
test_loss: 0.012153394
train_loss: 0.009316371
test_loss: 0.012034784
train_loss: 0.008592932
test_loss: 0.012247793
train_loss: 0.009123341
test_loss: 0.011585658
train_loss: 0.009087247
test_loss: 0.011650029
train_loss: 0.008410418
test_loss: 0.012453419
train_loss: 0.009514812
test_loss: 0.011516352
train_loss: 0.009831285
test_loss: 0.011702307
train_loss: 0.008676462
test_loss: 0.01128046
train_loss: 0.008932266
test_loss: 0.011793871
train_loss: 0.009231817
test_loss: 0.011281791
train_loss: 0.009018764
test_loss: 0.012048696
train_loss: 0.00871111
test_loss: 0.011250249
train_loss: 0.009143414
test_loss: 0.01167229
train_loss: 0.008725926
test_loss: 0.011173091
train_loss: 0.00899685
test_loss: 0.011390184
train_loss: 0.009006907
test_loss: 0.012220733
train_loss: 0.00833286
test_loss: 0.011675815
train_loss: 0.009130987
test_loss: 0.012486127
train_loss: 0.008728342
test_loss: 0.011085228
train_loss: 0.009339256
test_loss: 0.0107401265
train_loss: 0.008943785
test_loss: 0.011094945
train_loss: 0.008455986
test_loss: 0.01056822
train_loss: 0.008288363
test_loss: 0.010934363
train_loss: 0.008882955
test_loss: 0.010889544
train_loss: 0.0077009695
test_loss: 0.010633734
train_loss: 0.008456395
test_loss: 0.010987772
train_loss: 0.008794616
test_loss: 0.010679627
train_loss: 0.008235987
test_loss: 0.01019491
train_loss: 0.008337935
test_loss: 0.010477391
train_loss: 0.0078118993
test_loss: 0.010093426
train_loss: 0.008313716
test_loss: 0.010185201
train_loss: 0.007616323
test_loss: 0.010466024
train_loss: 0.008647803
test_loss: 0.010412803
train_loss: 0.007862482
test_loss: 0.010170863
train_loss: 0.008116041
test_loss: 0.010699199
train_loss: 0.007569584
test_loss: 0.010444587
train_loss: 0.0075962865
test_loss: 0.010680532
train_loss: 0.007332377
test_loss: 0.010517921
train_loss: 0.008133754
test_loss: 0.010335879
train_loss: 0.007684555
test_loss: 0.009891362
train_loss: 0.008045642
test_loss: 0.010516614
train_loss: 0.007390458
test_loss: 0.010791573
train_loss: 0.008275092
test_loss: 0.010957189
train_loss: 0.008071478
test_loss: 0.010876506
train_loss: 0.007946684
test_loss: 0.010338484
train_loss: 0.0075164894
test_loss: 0.009600406
train_loss: 0.007489129
test_loss: 0.010352085
train_loss: 0.00872009
test_loss: 0.011342245
train_loss: 0.0078242235
test_loss: 0.0102718845
train_loss: 0.0067081293
test_loss: 0.010086193
train_loss: 0.0073970445
test_loss: 0.010342816
train_loss: 0.008003519
test_loss: 0.010690695
train_loss: 0.008030211
test_loss: 0.01055059
train_loss: 0.00872546
test_loss: 0.010771287
train_loss: 0.007258876
test_loss: 0.010043505
train_loss: 0.007198897
test_loss: 0.010067446
train_loss: 0.0077773416
test_loss: 0.01050147
train_loss: 0.0077863
test_loss: 0.010194529
train_loss: 0.0078100525
test_loss: 0.0104348175
train_loss: 0.0077229906
test_loss: 0.009954258
train_loss: 0.0069520744
test_loss: 0.009762345
train_loss: 0.0074198283
test_loss: 0.0096507985
train_loss: 0.0080428915
test_loss: 0.010544243
train_loss: 0.0076095704
test_loss: 0.0101400865
train_loss: 0.0075030383
test_loss: 0.010100421
train_loss: 0.007763434
test_loss: 0.010276282
train_loss: 0.0074439645
test_loss: 0.01026924
train_loss: 0.007136781
test_loss: 0.009915245
train_loss: 0.006917459
test_loss: 0.010180024
train_loss: 0.007365507
test_loss: 0.009771795
train_loss: 0.0072815507
test_loss: 0.010108751
train_loss: 0.0075753056
test_loss: 0.009840333
train_loss: 0.007768835
test_loss: 0.00979922
train_loss: 0.0072787446
test_loss: 0.009906357
train_loss: 0.0072201677
test_loss: 0.010032339
train_loss: 0.0076529216
test_loss: 0.010106063
train_loss: 0.007493941
test_loss: 0.010478448
train_loss: 0.007254348
test_loss: 0.010909882
train_loss: 0.0073475363
test_loss: 0.009715567
train_loss: 0.0069643697
test_loss: 0.009833313
train_loss: 0.00753288
test_loss: 0.010684609
train_loss: 0.007410055
test_loss: 0.010042909
train_loss: 0.0067669153
test_loss: 0.009586128
train_loss: 0.008272234
test_loss: 0.010258303
train_loss: 0.007028531
test_loss: 0.009903289
train_loss: 0.0075984932
test_loss: 0.009829041
train_loss: 0.0076056467
test_loss: 0.0100541245
train_loss: 0.007443222
test_loss: 0.00965517
train_loss: 0.0067486567
test_loss: 0.009957508
train_loss: 0.00768905
test_loss: 0.009672885
train_loss: 0.006964313
test_loss: 0.009585274
train_loss: 0.0068865083
test_loss: 0.0097935395
train_loss: 0.0069351126
test_loss: 0.009499936
train_loss: 0.0065349503
test_loss: 0.009588798
train_loss: 0.0073200488
test_loss: 0.010096139
train_loss: 0.006776858
test_loss: 0.009670371
train_loss: 0.007868008
test_loss: 0.0101951845
train_loss: 0.0073569175
test_loss: 0.009316683
train_loss: 0.0068277754
test_loss: 0.009216818
train_loss: 0.0066062203
test_loss: 0.009824175
train_loss: 0.006525171
test_loss: 0.00962179
train_loss: 0.0071072965
test_loss: 0.009332378
train_loss: 0.007939655
test_loss: 0.009912159
train_loss: 0.007376791
test_loss: 0.010722447
train_loss: 0.007601816
test_loss: 0.010083927
train_loss: 0.0073688775
test_loss: 0.009606328
train_loss: 0.0074292263
test_loss: 0.009869827
train_loss: 0.007723804
test_loss: 0.010082073
train_loss: 0.0066383113
test_loss: 0.009726578
train_loss: 0.006526801
test_loss: 0.00949954
train_loss: 0.0068208496
test_loss: 0.009427342
train_loss: 0.0071033956
test_loss: 0.009353057
train_loss: 0.006941296
test_loss: 0.009521664
train_loss: 0.006906598
test_loss: 0.009531118
train_loss: 0.0063717742
test_loss: 0.009311951
train_loss: 0.0075080674
test_loss: 0.010385305
train_loss: 0.0075852117
test_loss: 0.009451841
train_loss: 0.006989811
test_loss: 0.009354857
train_loss: 0.0073247543
test_loss: 0.009715119
train_loss: 0.006187454
test_loss: 0.009013365
train_loss: 0.006865436
test_loss: 0.009412945
train_loss: 0.006995837
test_loss: 0.009465992
train_loss: 0.0068046018
test_loss: 0.009452257
train_loss: 0.0062152212
test_loss: 0.009230156
train_loss: 0.0069399336
test_loss: 0.009332431
train_loss: 0.0065894537
test_loss: 0.0093528
train_loss: 0.0076062903
test_loss: 0.009718449
train_loss: 0.00693364
test_loss: 0.009717692
train_loss: 0.0071574245
test_loss: 0.009624041
train_loss: 0.0069578225
test_loss: 0.009137492
train_loss: 0.006667388
test_loss: 0.009550767
train_loss: 0.0071409116
test_loss: 0.009457457
train_loss: 0.0067136306
test_loss: 0.009773048
train_loss: 0.006790203
test_loss: 0.009586851
train_loss: 0.0068135206
test_loss: 0.009416529
train_loss: 0.0076360805
test_loss: 0.010104328
train_loss: 0.006346128
test_loss: 0.009008385
train_loss: 0.0073038572
test_loss: 0.009211737
train_loss: 0.006248554
test_loss: 0.009295114
train_loss: 0.0061353324
test_loss: 0.009151025
train_loss: 0.0061744917
test_loss: 0.008851836
train_loss: 0.006616486
test_loss: 0.009117237
train_loss: 0.0067437347
test_loss: 0.009270993
train_loss: 0.0066422457
test_loss: 0.009239948
train_loss: 0.0073534218
test_loss: 0.009347198
train_loss: 0.0066110063
test_loss: 0.009132133
train_loss: 0.006642648
test_loss: 0.00903149
train_loss: 0.0059692347
test_loss: 0.009200397
train_loss: 0.0071738465
test_loss: 0.009488222
train_loss: 0.006680232
test_loss: 0.00924143
train_loss: 0.0062253363
test_loss: 0.009358086
train_loss: 0.0061352476
test_loss: 0.009452624
train_loss: 0.006521652
test_loss: 0.009156775
train_loss: 0.0072045317
test_loss: 0.009321129
train_loss: 0.0060488973
test_loss: 0.008945506
train_loss: 0.0069785924
test_loss: 0.010009489
train_loss: 0.0065380246
test_loss: 0.009298287
train_loss: 0.006986236
test_loss: 0.009180611
train_loss: 0.0067532007
test_loss: 0.009157416
train_loss: 0.006132186
test_loss: 0.0090804715
train_loss: 0.007133308
test_loss: 0.0092614265
train_loss: 0.0061164224
test_loss: 0.008785844
train_loss: 0.00576713
test_loss: 0.008652684
train_loss: 0.0062067225
test_loss: 0.009095356
train_loss: 0.0060169734
test_loss: 0.009011681
train_loss: 0.0060570184
test_loss: 0.008979242
train_loss: 0.006132254
test_loss: 0.00870491
train_loss: 0.0061386046
test_loss: 0.008742355
train_loss: 0.0065747304
test_loss: 0.009045409
train_loss: 0.0060335966
test_loss: 0.00928543
train_loss: 0.0066404776
test_loss: 0.009368706
train_loss: 0.006090119
test_loss: 0.009476289
train_loss: 0.006804442
test_loss: 0.0093475
train_loss: 0.0068926727
test_loss: 0.0091284495
train_loss: 0.006092257
test_loss: 0.009089554
train_loss: 0.0063499734
test_loss: 0.009085131
train_loss: 0.0063411724
test_loss: 0.0090730265
train_loss: 0.0069418773
test_loss: 0.00949561
train_loss: 0.006574486
test_loss: 0.008927081
train_loss: 0.0059778313
test_loss: 0.009314754
train_loss: 0.006787768
test_loss: 0.009027408
train_loss: 0.0064293374
test_loss: 0.00915431
train_loss: 0.0062668016
test_loss: 0.008865486
train_loss: 0.006489163
test_loss: 0.008766531
train_loss: 0.006245673
test_loss: 0.008955217
train_loss: 0.006740639
test_loss: 0.008719095
train_loss: 0.0057463
test_loss: 0.00889993
train_loss: 0.0061963573
test_loss: 0.008936504
train_loss: 0.0056756604
test_loss: 0.00905138
train_loss: 0.0064443457
test_loss: 0.008706506
train_loss: 0.006155492
test_loss: 0.008702598
train_loss: 0.00652685
test_loss: 0.008971869
train_loss: 0.00598345
test_loss: 0.009152673
train_loss: 0.006286473
test_loss: 0.009070307
train_loss: 0.0070054377
test_loss: 0.00898759
train_loss: 0.007188137
test_loss: 0.008952663
train_loss: 0.0060333437
test_loss: 0.009184958
train_loss: 0.0068556718
test_loss: 0.0094704665
train_loss: 0.00625059
test_loss: 0.009226853
train_loss: 0.006432168
test_loss: 0.009510407
train_loss: 0.006530828
test_loss: 0.009547519
train_loss: 0.006183966
test_loss: 0.008779404
train_loss: 0.0061249533
test_loss: 0.0089642145
train_loss: 0.006563969
test_loss: 0.008803482
train_loss: 0.0074701873
test_loss: 0.009620454
train_loss: 0.0062605115
test_loss: 0.0095687695
train_loss: 0.0061356844
test_loss: 0.009188505
train_loss: 0.0062919995
test_loss: 0.008707098
train_loss: 0.006313711
test_loss: 0.008796233
train_loss: 0.006124203
test_loss: 0.008760026
train_loss: 0.0057743047
test_loss: 0.008613997
train_loss: 0.0060550496
test_loss: 0.008893723
train_loss: 0.005911154
test_loss: 0.008990231
train_loss: 0.0065676505
test_loss: 0.00878989
train_loss: 0.005506973
test_loss: 0.008546701
train_loss: 0.0064740274
test_loss: 0.008824478
train_loss: 0.0060414416
test_loss: 0.008805513
train_loss: 0.006349353
test_loss: 0.008872683
train_loss: 0.0057936837
test_loss: 0.00850477
train_loss: 0.006136001
test_loss: 0.008553796
train_loss: 0.0059994077
test_loss: 0.008813563
train_loss: 0.006450681
test_loss: 0.009058378
train_loss: 0.0067142365
test_loss: 0.009023553
train_loss: 0.006307355
test_loss: 0.008887343
train_loss: 0.006248484
test_loss: 0.008690629
train_loss: 0.006162678
test_loss: 0.008659625
train_loss: 0.0059408434
test_loss: 0.008778656
train_loss: 0.006369517
test_loss: 0.009160182
train_loss: 0.005687443
test_loss: 0.008631754
train_loss: 0.0059553464
test_loss: 0.0089195175
train_loss: 0.0059704534
test_loss: 0.008695089
train_loss: 0.005820697
test_loss: 0.008553374
train_loss: 0.006272242
test_loss: 0.008674969
train_loss: 0.006498786
test_loss: 0.008853063
train_loss: 0.0063316296
test_loss: 0.009219066
train_loss: 0.005744132
test_loss: 0.008626024
train_loss: 0.006024059
test_loss: 0.009006683
train_loss: 0.0062342873
test_loss: 0.009261283
train_loss: 0.006217234
test_loss: 0.008343696
train_loss: 0.0070811547
test_loss: 0.0095900465
train_loss: 0.006196564
test_loss: 0.008831095
train_loss: 0.006598993
test_loss: 0.008948585
train_loss: 0.006305485
test_loss: 0.008792909
train_loss: 0.005091831
test_loss: 0.008479866
train_loss: 0.0058731884
test_loss: 0.008968142
train_loss: 0.0063242824
test_loss: 0.008835023
train_loss: 0.0058654114
test_loss: 0.008681006
train_loss: 0.005617827
test_loss: 0.008782026
train_loss: 0.00554455
test_loss: 0.008869748
train_loss: 0.005819194
test_loss: 0.008781723
train_loss: 0.006348823
test_loss: 0.008579756
train_loss: 0.0058449423
test_loss: 0.008429204
train_loss: 0.0057693096
test_loss: 0.008760273
train_loss: 0.0054276157
test_loss: 0.008672379
train_loss: 0.0060678883
test_loss: 0.009056778
train_loss: 0.005907786
test_loss: 0.008729611
train_loss: 0.005396818
test_loss: 0.008568041
train_loss: 0.006002466
test_loss: 0.008807744
train_loss: 0.0059665353
test_loss: 0.008431489
train_loss: 0.006756477
test_loss: 0.008852644
train_loss: 0.0061772633
test_loss: 0.008433627
train_loss: 0.005878495
test_loss: 0.0090267
train_loss: 0.006118609
test_loss: 0.008668491
train_loss: 0.0060333484
test_loss: 0.008746898
train_loss: 0.0060617453
test_loss: 0.00888042
train_loss: 0.0065574613
test_loss: 0.0085716685
train_loss: 0.005998835
test_loss: 0.008508604
train_loss: 0.006864883
test_loss: 0.008469765
train_loss: 0.0064855907
test_loss: 0.0088581685
train_loss: 0.005228402
test_loss: 0.008431533
train_loss: 0.0065782648
test_loss: 0.009259668
train_loss: 0.0057519544
test_loss: 0.008474271
train_loss: 0.0059804204
test_loss: 0.0087042
train_loss: 0.005599035
test_loss: 0.008609863
train_loss: 0.0064710695
test_loss: 0.008447189
train_loss: 0.0052842824
test_loss: 0.008409438
train_loss: 0.005654199
test_loss: 0.00883739
train_loss: 0.0054535447
test_loss: 0.008255082
train_loss: 0.005480556
test_loss: 0.008444167
train_loss: 0.0055017713
test_loss: 0.008349704
train_loss: 0.0057279207
test_loss: 0.008748122
train_loss: 0.006224776
test_loss: 0.008918302
train_loss: 0.0057886983
test_loss: 0.008557964
train_loss: 0.0053062015
test_loss: 0.008523733
train_loss: 0.0058198385
test_loss: 0.00845663
train_loss: 0.006003574
test_loss: 0.008683331
train_loss: 0.006620031
test_loss: 0.008711707
train_loss: 0.0058307843
test_loss: 0.008229097
train_loss: 0.006025264
test_loss: 0.00832938
train_loss: 0.006343971
test_loss: 0.008704577
train_loss: 0.005669601
test_loss: 0.009250697
train_loss: 0.005784434
test_loss: 0.008640822
train_loss: 0.0059149396
test_loss: 0.008524572
train_loss: 0.006339283
test_loss: 0.008594095
train_loss: 0.0055780713
test_loss: 0.008230834
train_loss: 0.006046825
test_loss: 0.008173601
train_loss: 0.0054618116
test_loss: 0.0085590165
train_loss: 0.005408686
test_loss: 0.008552199
train_loss: 0.005281465
test_loss: 0.008173424
train_loss: 0.0057324125/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.008546352
train_loss: 0.0054514427
test_loss: 0.008320928
train_loss: 0.0052454425
test_loss: 0.008846264
train_loss: 0.0057261074
test_loss: 0.008467323
train_loss: 0.0062651895
test_loss: 0.008644069
train_loss: 0.0056824945
test_loss: 0.00852375
train_loss: 0.0052933805
test_loss: 0.008534162
train_loss: 0.0068738265
test_loss: 0.008890667
train_loss: 0.005905759
test_loss: 0.0090915
train_loss: 0.006177573
test_loss: 0.008603925
train_loss: 0.0063326433
test_loss: 0.008498497
train_loss: 0.006393709
test_loss: 0.0087312795
train_loss: 0.0062805987
test_loss: 0.008459111
train_loss: 0.0058162427
test_loss: 0.008570247
train_loss: 0.006070316
test_loss: 0.008830937
train_loss: 0.0065665976
test_loss: 0.009031929
train_loss: 0.005634154
test_loss: 0.008306411
train_loss: 0.005451601
test_loss: 0.008446029
train_loss: 0.006436924
test_loss: 0.008657725
train_loss: 0.006032289
test_loss: 0.008412699
train_loss: 0.006229967
test_loss: 0.008726615
train_loss: 0.0056123342
test_loss: 0.008730119
train_loss: 0.0056766314
test_loss: 0.008504941
train_loss: 0.0058987467
test_loss: 0.008570024
train_loss: 0.006112652
test_loss: 0.008817102
train_loss: 0.0052843583
test_loss: 0.008446054
train_loss: 0.0051360354
test_loss: 0.008264504
train_loss: 0.005631273
test_loss: 0.008360532
train_loss: 0.005612881
test_loss: 0.008709849
train_loss: 0.0057916823
test_loss: 0.008672105
train_loss: 0.006401022
test_loss: 0.008221868
train_loss: 0.005827829
test_loss: 0.008477629
train_loss: 0.0060884184
test_loss: 0.008184699
train_loss: 0.006091518
test_loss: 0.008616837
train_loss: 0.005597749
test_loss: 0.008395004
train_loss: 0.0056823995
test_loss: 0.008492075
train_loss: 0.005955821
test_loss: 0.008201677
train_loss: 0.0060473913
test_loss: 0.008842261
train_loss: 0.0056882245
test_loss: 0.008796307
train_loss: 0.0053017447
test_loss: 0.00842267
train_loss: 0.0063603255
test_loss: 0.008356867
train_loss: 0.0054061688
test_loss: 0.0083926385
train_loss: 0.005485276
test_loss: 0.008105549
train_loss: 0.0059360582
test_loss: 0.008316243
train_loss: 0.0052006207
test_loss: 0.00802707
train_loss: 0.0061047864
test_loss: 0.008364086
train_loss: 0.0053711473
test_loss: 0.008404155
train_loss: 0.006269027
test_loss: 0.008282357
train_loss: 0.0052144774
test_loss: 0.008417996
train_loss: 0.0055143777
test_loss: 0.008549158
train_loss: 0.0055110017
test_loss: 0.008361394
train_loss: 0.005536807
test_loss: 0.008553802
train_loss: 0.005580479
test_loss: 0.00817371
train_loss: 0.0055391975
test_loss: 0.008444349
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi -2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53e99620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53d5a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53d5aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53da3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53da3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53cf7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53cdf8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53cf7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53cf7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53c2c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53c15ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53c2c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53c0f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53c0f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53b749d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53b74840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53ba7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53b67840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53b14730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53b26f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53ac6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53adabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd53aa6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd428df730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd428df620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd428a8d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd428a8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd42868730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd42868378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd42815378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd427e5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd42801158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd427a37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd427b6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd4274f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd427179d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.1758692e-05
Iter: 2 loss: 4.96110733e-05
Iter: 3 loss: 4.95999448e-05
Iter: 4 loss: 4.54168585e-05
Iter: 5 loss: 4.50496227e-05
Iter: 6 loss: 4.35034672e-05
Iter: 7 loss: 3.90978348e-05
Iter: 8 loss: 5.94242811e-05
Iter: 9 loss: 3.75196323e-05
Iter: 10 loss: 3.20529e-05
Iter: 11 loss: 7.62416457e-05
Iter: 12 loss: 3.16894548e-05
Iter: 13 loss: 2.89589207e-05
Iter: 14 loss: 3.04274909e-05
Iter: 15 loss: 2.71635017e-05
Iter: 16 loss: 2.36487722e-05
Iter: 17 loss: 3.97761833e-05
Iter: 18 loss: 2.29941161e-05
Iter: 19 loss: 2.12500308e-05
Iter: 20 loss: 3.31771043e-05
Iter: 21 loss: 2.10748258e-05
Iter: 22 loss: 1.97359514e-05
Iter: 23 loss: 2.10581602e-05
Iter: 24 loss: 1.89785242e-05
Iter: 25 loss: 1.76203012e-05
Iter: 26 loss: 2.40441295e-05
Iter: 27 loss: 1.73738135e-05
Iter: 28 loss: 1.64284938e-05
Iter: 29 loss: 2.02885421e-05
Iter: 30 loss: 1.62217293e-05
Iter: 31 loss: 1.55023299e-05
Iter: 32 loss: 1.68040569e-05
Iter: 33 loss: 1.51901459e-05
Iter: 34 loss: 1.43376137e-05
Iter: 35 loss: 1.66374921e-05
Iter: 36 loss: 1.40550565e-05
Iter: 37 loss: 1.35614628e-05
Iter: 38 loss: 1.84331439e-05
Iter: 39 loss: 1.35446962e-05
Iter: 40 loss: 1.33397498e-05
Iter: 41 loss: 1.32963078e-05
Iter: 42 loss: 1.3083768e-05
Iter: 43 loss: 1.25343322e-05
Iter: 44 loss: 1.70362327e-05
Iter: 45 loss: 1.24368325e-05
Iter: 46 loss: 1.20433178e-05
Iter: 47 loss: 1.17120653e-05
Iter: 48 loss: 1.16014944e-05
Iter: 49 loss: 1.10816263e-05
Iter: 50 loss: 1.10812234e-05
Iter: 51 loss: 1.08482946e-05
Iter: 52 loss: 1.07912429e-05
Iter: 53 loss: 1.06437892e-05
Iter: 54 loss: 1.02844751e-05
Iter: 55 loss: 1.34428883e-05
Iter: 56 loss: 1.02663171e-05
Iter: 57 loss: 1.00688203e-05
Iter: 58 loss: 1.10508681e-05
Iter: 59 loss: 1.00355328e-05
Iter: 60 loss: 9.85285624e-06
Iter: 61 loss: 9.54383813e-06
Iter: 62 loss: 9.54342795e-06
Iter: 63 loss: 9.28193913e-06
Iter: 64 loss: 1.19568567e-05
Iter: 65 loss: 9.27429664e-06
Iter: 66 loss: 9.04470926e-06
Iter: 67 loss: 9.55235191e-06
Iter: 68 loss: 8.95678477e-06
Iter: 69 loss: 8.80361222e-06
Iter: 70 loss: 1.02109625e-05
Iter: 71 loss: 8.79689469e-06
Iter: 72 loss: 8.67557537e-06
Iter: 73 loss: 9.07504182e-06
Iter: 74 loss: 8.64186222e-06
Iter: 75 loss: 8.5616357e-06
Iter: 76 loss: 8.55317194e-06
Iter: 77 loss: 8.50909601e-06
Iter: 78 loss: 8.37624248e-06
Iter: 79 loss: 8.77863567e-06
Iter: 80 loss: 8.30990757e-06
Iter: 81 loss: 8.12254621e-06
Iter: 82 loss: 9.2830187e-06
Iter: 83 loss: 8.10030724e-06
Iter: 84 loss: 7.99746e-06
Iter: 85 loss: 8.00118687e-06
Iter: 86 loss: 7.91594175e-06
Iter: 87 loss: 7.79131915e-06
Iter: 88 loss: 8.89927469e-06
Iter: 89 loss: 7.78514277e-06
Iter: 90 loss: 7.69100188e-06
Iter: 91 loss: 7.67539495e-06
Iter: 92 loss: 7.61067076e-06
Iter: 93 loss: 7.49766332e-06
Iter: 94 loss: 8.67483232e-06
Iter: 95 loss: 7.49477113e-06
Iter: 96 loss: 7.41540953e-06
Iter: 97 loss: 7.48336561e-06
Iter: 98 loss: 7.36877337e-06
Iter: 99 loss: 7.24797928e-06
Iter: 100 loss: 7.67417805e-06
Iter: 101 loss: 7.21665538e-06
Iter: 102 loss: 7.14994894e-06
Iter: 103 loss: 7.55683413e-06
Iter: 104 loss: 7.14176758e-06
Iter: 105 loss: 7.07788286e-06
Iter: 106 loss: 6.97351607e-06
Iter: 107 loss: 6.97317955e-06
Iter: 108 loss: 6.90545357e-06
Iter: 109 loss: 6.90470324e-06
Iter: 110 loss: 6.87789543e-06
Iter: 111 loss: 6.87308238e-06
Iter: 112 loss: 6.83854068e-06
Iter: 113 loss: 6.76024956e-06
Iter: 114 loss: 7.81809831e-06
Iter: 115 loss: 6.75559522e-06
Iter: 116 loss: 6.71178623e-06
Iter: 117 loss: 6.8553968e-06
Iter: 118 loss: 6.69958217e-06
Iter: 119 loss: 6.64877e-06
Iter: 120 loss: 6.62566e-06
Iter: 121 loss: 6.60004343e-06
Iter: 122 loss: 6.53599591e-06
Iter: 123 loss: 7.00758028e-06
Iter: 124 loss: 6.53066627e-06
Iter: 125 loss: 6.48068954e-06
Iter: 126 loss: 6.50515585e-06
Iter: 127 loss: 6.44724969e-06
Iter: 128 loss: 6.38921119e-06
Iter: 129 loss: 6.62147158e-06
Iter: 130 loss: 6.37617086e-06
Iter: 131 loss: 6.31750845e-06
Iter: 132 loss: 6.53233747e-06
Iter: 133 loss: 6.30283512e-06
Iter: 134 loss: 6.26101382e-06
Iter: 135 loss: 6.23616779e-06
Iter: 136 loss: 6.21883919e-06
Iter: 137 loss: 6.15674116e-06
Iter: 138 loss: 6.88332602e-06
Iter: 139 loss: 6.15586487e-06
Iter: 140 loss: 6.11363339e-06
Iter: 141 loss: 6.09131439e-06
Iter: 142 loss: 6.07217089e-06
Iter: 143 loss: 6.02916134e-06
Iter: 144 loss: 6.02874024e-06
Iter: 145 loss: 6.02708133e-06
Iter: 146 loss: 6.01510965e-06
Iter: 147 loss: 6.00701469e-06
Iter: 148 loss: 5.98075439e-06
Iter: 149 loss: 6.01192141e-06
Iter: 150 loss: 5.96078507e-06
Iter: 151 loss: 5.91206481e-06
Iter: 152 loss: 6.17698288e-06
Iter: 153 loss: 5.90511627e-06
Iter: 154 loss: 5.88007515e-06
Iter: 155 loss: 5.95377287e-06
Iter: 156 loss: 5.87236809e-06
Iter: 157 loss: 5.84053896e-06
Iter: 158 loss: 5.81040331e-06
Iter: 159 loss: 5.80310643e-06
Iter: 160 loss: 5.77211449e-06
Iter: 161 loss: 6.2056929e-06
Iter: 162 loss: 5.77200944e-06
Iter: 163 loss: 5.74530259e-06
Iter: 164 loss: 5.73783427e-06
Iter: 165 loss: 5.72178033e-06
Iter: 166 loss: 5.6844674e-06
Iter: 167 loss: 6.0012685e-06
Iter: 168 loss: 5.68243604e-06
Iter: 169 loss: 5.65913933e-06
Iter: 170 loss: 5.63307185e-06
Iter: 171 loss: 5.62960622e-06
Iter: 172 loss: 5.58562078e-06
Iter: 173 loss: 5.87988643e-06
Iter: 174 loss: 5.58133888e-06
Iter: 175 loss: 5.55140468e-06
Iter: 176 loss: 5.60310309e-06
Iter: 177 loss: 5.53820473e-06
Iter: 178 loss: 5.51006724e-06
Iter: 179 loss: 5.68808809e-06
Iter: 180 loss: 5.50690675e-06
Iter: 181 loss: 5.4970269e-06
Iter: 182 loss: 5.49380184e-06
Iter: 183 loss: 5.47978925e-06
Iter: 184 loss: 5.45956527e-06
Iter: 185 loss: 5.45902139e-06
Iter: 186 loss: 5.44762679e-06
Iter: 187 loss: 5.43244596e-06
Iter: 188 loss: 5.43155966e-06
Iter: 189 loss: 5.40426026e-06
Iter: 190 loss: 5.46005322e-06
Iter: 191 loss: 5.39342545e-06
Iter: 192 loss: 5.37254164e-06
Iter: 193 loss: 5.45688818e-06
Iter: 194 loss: 5.36807647e-06
Iter: 195 loss: 5.34056926e-06
Iter: 196 loss: 5.37526739e-06
Iter: 197 loss: 5.32638478e-06
Iter: 198 loss: 5.31000114e-06
Iter: 199 loss: 5.54562757e-06
Iter: 200 loss: 5.3099543e-06
Iter: 201 loss: 5.2970563e-06
Iter: 202 loss: 5.27072098e-06
Iter: 203 loss: 5.75834747e-06
Iter: 204 loss: 5.27038219e-06
Iter: 205 loss: 5.24851657e-06
Iter: 206 loss: 5.58830652e-06
Iter: 207 loss: 5.24852612e-06
Iter: 208 loss: 5.22881828e-06
Iter: 209 loss: 5.22206392e-06
Iter: 210 loss: 5.21080756e-06
Iter: 211 loss: 5.18483193e-06
Iter: 212 loss: 5.40174324e-06
Iter: 213 loss: 5.18324623e-06
Iter: 214 loss: 5.16747332e-06
Iter: 215 loss: 5.1628781e-06
Iter: 216 loss: 5.1532279e-06
Iter: 217 loss: 5.14299518e-06
Iter: 218 loss: 5.14015392e-06
Iter: 219 loss: 5.12837096e-06
Iter: 220 loss: 5.19804871e-06
Iter: 221 loss: 5.12689621e-06
Iter: 222 loss: 5.12204861e-06
Iter: 223 loss: 5.10661403e-06
Iter: 224 loss: 5.126426e-06
Iter: 225 loss: 5.09522533e-06
Iter: 226 loss: 5.0733961e-06
Iter: 227 loss: 5.30326133e-06
Iter: 228 loss: 5.0728413e-06
Iter: 229 loss: 5.05655044e-06
Iter: 230 loss: 5.09024858e-06
Iter: 231 loss: 5.04989839e-06
Iter: 232 loss: 5.03334104e-06
Iter: 233 loss: 5.07617096e-06
Iter: 234 loss: 5.02752664e-06
Iter: 235 loss: 5.01161276e-06
Iter: 236 loss: 5.05066419e-06
Iter: 237 loss: 5.00588703e-06
Iter: 238 loss: 4.99162161e-06
Iter: 239 loss: 5.14085923e-06
Iter: 240 loss: 4.99117141e-06
Iter: 241 loss: 4.98044847e-06
Iter: 242 loss: 4.97051542e-06
Iter: 243 loss: 4.96783287e-06
Iter: 244 loss: 4.95066615e-06
Iter: 245 loss: 5.07423738e-06
Iter: 246 loss: 4.94914e-06
Iter: 247 loss: 4.93519474e-06
Iter: 248 loss: 4.92303343e-06
Iter: 249 loss: 4.91944957e-06
Iter: 250 loss: 4.90090679e-06
Iter: 251 loss: 5.18983688e-06
Iter: 252 loss: 4.90097318e-06
Iter: 253 loss: 4.89695049e-06
Iter: 254 loss: 4.8955817e-06
Iter: 255 loss: 4.88974092e-06
Iter: 256 loss: 4.88092701e-06
Iter: 257 loss: 4.88073874e-06
Iter: 258 loss: 4.87028228e-06
Iter: 259 loss: 4.86204863e-06
Iter: 260 loss: 4.85881e-06
Iter: 261 loss: 4.84665225e-06
Iter: 262 loss: 4.83074382e-06
Iter: 263 loss: 4.82967971e-06
Iter: 264 loss: 4.82116e-06
Iter: 265 loss: 4.81803363e-06
Iter: 266 loss: 4.80880408e-06
Iter: 267 loss: 4.79005848e-06
Iter: 268 loss: 5.11435155e-06
Iter: 269 loss: 4.78962966e-06
Iter: 270 loss: 4.77704771e-06
Iter: 271 loss: 4.77613321e-06
Iter: 272 loss: 4.76627793e-06
Iter: 273 loss: 4.75729257e-06
Iter: 274 loss: 4.75506522e-06
Iter: 275 loss: 4.73922046e-06
Iter: 276 loss: 4.92618801e-06
Iter: 277 loss: 4.73906357e-06
Iter: 278 loss: 4.73125692e-06
Iter: 279 loss: 4.74686658e-06
Iter: 280 loss: 4.7280605e-06
Iter: 281 loss: 4.71871954e-06
Iter: 282 loss: 4.71117073e-06
Iter: 283 loss: 4.7083272e-06
Iter: 284 loss: 4.69348561e-06
Iter: 285 loss: 4.76156629e-06
Iter: 286 loss: 4.69089082e-06
Iter: 287 loss: 4.684543e-06
Iter: 288 loss: 4.68350572e-06
Iter: 289 loss: 4.67608061e-06
Iter: 290 loss: 4.69798e-06
Iter: 291 loss: 4.67381415e-06
Iter: 292 loss: 4.6691448e-06
Iter: 293 loss: 4.65813537e-06
Iter: 294 loss: 4.81192319e-06
Iter: 295 loss: 4.65762514e-06
Iter: 296 loss: 4.64579489e-06
Iter: 297 loss: 4.69161387e-06
Iter: 298 loss: 4.64321965e-06
Iter: 299 loss: 4.63340575e-06
Iter: 300 loss: 4.63244123e-06
Iter: 301 loss: 4.62515754e-06
Iter: 302 loss: 4.6093578e-06
Iter: 303 loss: 4.69448287e-06
Iter: 304 loss: 4.60699721e-06
Iter: 305 loss: 4.59745206e-06
Iter: 306 loss: 4.60519e-06
Iter: 307 loss: 4.59182729e-06
Iter: 308 loss: 4.57777332e-06
Iter: 309 loss: 4.63214838e-06
Iter: 310 loss: 4.57462511e-06
Iter: 311 loss: 4.56499856e-06
Iter: 312 loss: 4.60467936e-06
Iter: 313 loss: 4.56304133e-06
Iter: 314 loss: 4.55135296e-06
Iter: 315 loss: 4.56536054e-06
Iter: 316 loss: 4.54518295e-06
Iter: 317 loss: 4.53522898e-06
Iter: 318 loss: 4.60990486e-06
Iter: 319 loss: 4.53438088e-06
Iter: 320 loss: 4.5258721e-06
Iter: 321 loss: 4.51768028e-06
Iter: 322 loss: 4.5157758e-06
Iter: 323 loss: 4.51741653e-06
Iter: 324 loss: 4.51040023e-06
Iter: 325 loss: 4.50530615e-06
Iter: 326 loss: 4.51049527e-06
Iter: 327 loss: 4.50252537e-06
Iter: 328 loss: 4.49853542e-06
Iter: 329 loss: 4.4884664e-06
Iter: 330 loss: 4.55947975e-06
Iter: 331 loss: 4.48624041e-06
Iter: 332 loss: 4.47832736e-06
Iter: 333 loss: 4.47814364e-06
Iter: 334 loss: 4.47227421e-06
Iter: 335 loss: 4.45978367e-06
Iter: 336 loss: 4.67871587e-06
Iter: 337 loss: 4.45968635e-06
Iter: 338 loss: 4.44987199e-06
Iter: 339 loss: 4.44987245e-06
Iter: 340 loss: 4.44206125e-06
Iter: 341 loss: 4.43386762e-06
Iter: 342 loss: 4.43241788e-06
Iter: 343 loss: 4.42020928e-06
Iter: 344 loss: 4.49924573e-06
Iter: 345 loss: 4.41897782e-06
Iter: 346 loss: 4.40948816e-06
Iter: 347 loss: 4.44396255e-06
Iter: 348 loss: 4.40704025e-06
Iter: 349 loss: 4.39771793e-06
Iter: 350 loss: 4.39853829e-06
Iter: 351 loss: 4.39045652e-06
Iter: 352 loss: 4.38190818e-06
Iter: 353 loss: 4.38155166e-06
Iter: 354 loss: 4.37646895e-06
Iter: 355 loss: 4.37977224e-06
Iter: 356 loss: 4.37316294e-06
Iter: 357 loss: 4.36763321e-06
Iter: 358 loss: 4.36772098e-06
Iter: 359 loss: 4.36129631e-06
Iter: 360 loss: 4.35751554e-06
Iter: 361 loss: 4.35489e-06
Iter: 362 loss: 4.34999856e-06
Iter: 363 loss: 4.34547246e-06
Iter: 364 loss: 4.34432695e-06
Iter: 365 loss: 4.33540026e-06
Iter: 366 loss: 4.34323601e-06
Iter: 367 loss: 4.33005152e-06
Iter: 368 loss: 4.32249908e-06
Iter: 369 loss: 4.41159045e-06
Iter: 370 loss: 4.32246043e-06
Iter: 371 loss: 4.31591661e-06
Iter: 372 loss: 4.30589898e-06
Iter: 373 loss: 4.30572572e-06
Iter: 374 loss: 4.29891497e-06
Iter: 375 loss: 4.298563e-06
Iter: 376 loss: 4.29274633e-06
Iter: 377 loss: 4.28200292e-06
Iter: 378 loss: 4.52097083e-06
Iter: 379 loss: 4.28199428e-06
Iter: 380 loss: 4.27444957e-06
Iter: 381 loss: 4.27406667e-06
Iter: 382 loss: 4.26755e-06
Iter: 383 loss: 4.25720827e-06
Iter: 384 loss: 4.25699091e-06
Iter: 385 loss: 4.24981772e-06
Iter: 386 loss: 4.2496913e-06
Iter: 387 loss: 4.24295968e-06
Iter: 388 loss: 4.24245218e-06
Iter: 389 loss: 4.23760048e-06
Iter: 390 loss: 4.23127403e-06
Iter: 391 loss: 4.23134406e-06
Iter: 392 loss: 4.22817448e-06
Iter: 393 loss: 4.22819085e-06
Iter: 394 loss: 4.2245365e-06
Iter: 395 loss: 4.22008725e-06
Iter: 396 loss: 4.21963432e-06
Iter: 397 loss: 4.21486038e-06
Iter: 398 loss: 4.21085088e-06
Iter: 399 loss: 4.20955621e-06
Iter: 400 loss: 4.20296692e-06
Iter: 401 loss: 4.20684319e-06
Iter: 402 loss: 4.19861635e-06
Iter: 403 loss: 4.19063872e-06
Iter: 404 loss: 4.22933317e-06
Iter: 405 loss: 4.18934633e-06
Iter: 406 loss: 4.18149966e-06
Iter: 407 loss: 4.21131335e-06
Iter: 408 loss: 4.17962565e-06
Iter: 409 loss: 4.17339243e-06
Iter: 410 loss: 4.17465753e-06
Iter: 411 loss: 4.16873718e-06
Iter: 412 loss: 4.15891191e-06
Iter: 413 loss: 4.20797505e-06
Iter: 414 loss: 4.15729028e-06
Iter: 415 loss: 4.15135673e-06
Iter: 416 loss: 4.17075489e-06
Iter: 417 loss: 4.14979831e-06
Iter: 418 loss: 4.14243095e-06
Iter: 419 loss: 4.14438728e-06
Iter: 420 loss: 4.13710814e-06
Iter: 421 loss: 4.13066391e-06
Iter: 422 loss: 4.21274217e-06
Iter: 423 loss: 4.13066118e-06
Iter: 424 loss: 4.12531108e-06
Iter: 425 loss: 4.12484451e-06
Iter: 426 loss: 4.12085683e-06
Iter: 427 loss: 4.12039481e-06
Iter: 428 loss: 4.11698602e-06
Iter: 429 loss: 4.11504e-06
Iter: 430 loss: 4.11082419e-06
Iter: 431 loss: 4.17640513e-06
Iter: 432 loss: 4.11062501e-06
Iter: 433 loss: 4.10609346e-06
Iter: 434 loss: 4.10125904e-06
Iter: 435 loss: 4.10038047e-06
Iter: 436 loss: 4.09130553e-06
Iter: 437 loss: 4.15073646e-06
Iter: 438 loss: 4.0904356e-06
Iter: 439 loss: 4.08614687e-06
Iter: 440 loss: 4.08957658e-06
Iter: 441 loss: 4.08364758e-06
Iter: 442 loss: 4.07663265e-06
Iter: 443 loss: 4.0799132e-06
Iter: 444 loss: 4.07195057e-06
Iter: 445 loss: 4.06600702e-06
Iter: 446 loss: 4.08861433e-06
Iter: 447 loss: 4.06465733e-06
Iter: 448 loss: 4.05908577e-06
Iter: 449 loss: 4.08508822e-06
Iter: 450 loss: 4.05804121e-06
Iter: 451 loss: 4.0524792e-06
Iter: 452 loss: 4.05268747e-06
Iter: 453 loss: 4.0483028e-06
Iter: 454 loss: 4.04157845e-06
Iter: 455 loss: 4.10077519e-06
Iter: 456 loss: 4.0412724e-06
Iter: 457 loss: 4.03548893e-06
Iter: 458 loss: 4.04304774e-06
Iter: 459 loss: 4.03267586e-06
Iter: 460 loss: 4.03087461e-06
Iter: 461 loss: 4.02984e-06
Iter: 462 loss: 4.02679325e-06
Iter: 463 loss: 4.02774867e-06
Iter: 464 loss: 4.02465594e-06
Iter: 465 loss: 4.02171554e-06
Iter: 466 loss: 4.01529951e-06
Iter: 467 loss: 4.11221117e-06
Iter: 468 loss: 4.01499801e-06
Iter: 469 loss: 4.00900444e-06
Iter: 470 loss: 4.07644529e-06
Iter: 471 loss: 4.00879389e-06
Iter: 472 loss: 4.00467297e-06
Iter: 473 loss: 4.00328645e-06
Iter: 474 loss: 4.00094177e-06
Iter: 475 loss: 3.99380224e-06
Iter: 476 loss: 4.0176692e-06
Iter: 477 loss: 3.99179225e-06
Iter: 478 loss: 3.98679549e-06
Iter: 479 loss: 3.99992314e-06
Iter: 480 loss: 3.98529801e-06
Iter: 481 loss: 3.9790084e-06
Iter: 482 loss: 3.98140401e-06
Iter: 483 loss: 3.97472286e-06
Iter: 484 loss: 3.96891119e-06
Iter: 485 loss: 4.00465433e-06
Iter: 486 loss: 3.96833912e-06
Iter: 487 loss: 3.9621068e-06
Iter: 488 loss: 3.96067117e-06
Iter: 489 loss: 3.95657707e-06
Iter: 490 loss: 3.95090319e-06
Iter: 491 loss: 3.98845168e-06
Iter: 492 loss: 3.95041025e-06
Iter: 493 loss: 3.9448546e-06
Iter: 494 loss: 3.97544818e-06
Iter: 495 loss: 3.94403196e-06
Iter: 496 loss: 3.94226254e-06
Iter: 497 loss: 3.94128e-06
Iter: 498 loss: 3.93972869e-06
Iter: 499 loss: 3.93590926e-06
Iter: 500 loss: 3.96649148e-06
Iter: 501 loss: 3.93524169e-06
Iter: 502 loss: 3.93026949e-06
Iter: 503 loss: 3.93171103e-06
Iter: 504 loss: 3.92660331e-06
Iter: 505 loss: 3.92143602e-06
Iter: 506 loss: 3.97149051e-06
Iter: 507 loss: 3.92133279e-06
Iter: 508 loss: 3.91776348e-06
Iter: 509 loss: 3.92151924e-06
Iter: 510 loss: 3.91581216e-06
Iter: 511 loss: 3.91162484e-06
Iter: 512 loss: 3.92672791e-06
Iter: 513 loss: 3.91049252e-06
Iter: 514 loss: 3.9061897e-06
Iter: 515 loss: 3.90402965e-06
Iter: 516 loss: 3.90194418e-06
Iter: 517 loss: 3.89716479e-06
Iter: 518 loss: 3.91545063e-06
Iter: 519 loss: 3.89596426e-06
Iter: 520 loss: 3.88923308e-06
Iter: 521 loss: 3.89591241e-06
Iter: 522 loss: 3.88564058e-06
Iter: 523 loss: 3.88042918e-06
Iter: 524 loss: 3.90099831e-06
Iter: 525 loss: 3.87934233e-06
Iter: 526 loss: 3.87339333e-06
Iter: 527 loss: 3.88466515e-06
Iter: 528 loss: 3.87093132e-06
Iter: 529 loss: 3.87197815e-06
Iter: 530 loss: 3.86951433e-06
Iter: 531 loss: 3.86763668e-06
Iter: 532 loss: 3.86229522e-06
Iter: 533 loss: 3.89301204e-06
Iter: 534 loss: 3.86076954e-06
Iter: 535 loss: 3.85493513e-06
Iter: 536 loss: 3.88262833e-06
Iter: 537 loss: 3.85386511e-06
Iter: 538 loss: 3.85051862e-06
Iter: 539 loss: 3.85497196e-06
Iter: 540 loss: 3.84878877e-06
Iter: 541 loss: 3.84381747e-06
Iter: 542 loss: 3.84552413e-06
Iter: 543 loss: 3.84017767e-06
Iter: 544 loss: 3.83520546e-06
Iter: 545 loss: 3.86750753e-06
Iter: 546 loss: 3.83456972e-06
Iter: 547 loss: 3.82937924e-06
Iter: 548 loss: 3.83593579e-06
Iter: 549 loss: 3.8266553e-06
Iter: 550 loss: 3.82217286e-06
Iter: 551 loss: 3.83801216e-06
Iter: 552 loss: 3.82098779e-06
Iter: 553 loss: 3.81605878e-06
Iter: 554 loss: 3.81705195e-06
Iter: 555 loss: 3.81231121e-06
Iter: 556 loss: 3.80640131e-06
Iter: 557 loss: 3.82407052e-06
Iter: 558 loss: 3.80452366e-06
Iter: 559 loss: 3.7998484e-06
Iter: 560 loss: 3.83171027e-06
Iter: 561 loss: 3.79919493e-06
Iter: 562 loss: 3.79456333e-06
Iter: 563 loss: 3.7962277e-06
Iter: 564 loss: 3.79137191e-06
Iter: 565 loss: 3.79036874e-06
Iter: 566 loss: 3.7881241e-06
Iter: 567 loss: 3.78573736e-06
Iter: 568 loss: 3.78148798e-06
Iter: 569 loss: 3.78155482e-06
Iter: 570 loss: 3.77845481e-06
Iter: 571 loss: 3.77161405e-06
Iter: 572 loss: 3.86614829e-06
Iter: 573 loss: 3.77115566e-06
Iter: 574 loss: 3.76592698e-06
Iter: 575 loss: 3.76566913e-06
Iter: 576 loss: 3.76189882e-06
Iter: 577 loss: 3.7602249e-06
Iter: 578 loss: 3.75829882e-06
Iter: 579 loss: 3.75223226e-06
Iter: 580 loss: 3.78834329e-06
Iter: 581 loss: 3.7515365e-06
Iter: 582 loss: 3.74774208e-06
Iter: 583 loss: 3.75355694e-06
Iter: 584 loss: 3.74605497e-06
Iter: 585 loss: 3.74057e-06
Iter: 586 loss: 3.74644833e-06
Iter: 587 loss: 3.73761804e-06
Iter: 588 loss: 3.73281227e-06
Iter: 589 loss: 3.77783e-06
Iter: 590 loss: 3.73250032e-06
Iter: 591 loss: 3.72926115e-06
Iter: 592 loss: 3.72205523e-06
Iter: 593 loss: 3.83551833e-06
Iter: 594 loss: 3.72158092e-06
Iter: 595 loss: 3.71723354e-06
Iter: 596 loss: 3.71670649e-06
Iter: 597 loss: 3.71369e-06
Iter: 598 loss: 3.71503143e-06
Iter: 599 loss: 3.7116813e-06
Iter: 600 loss: 3.70728185e-06
Iter: 601 loss: 3.7722557e-06
Iter: 602 loss: 3.70726093e-06
Iter: 603 loss: 3.70566136e-06
Iter: 604 loss: 3.70229554e-06
Iter: 605 loss: 3.76125195e-06
Iter: 606 loss: 3.70206862e-06
Iter: 607 loss: 3.69849568e-06
Iter: 608 loss: 3.69698432e-06
Iter: 609 loss: 3.69514646e-06
Iter: 610 loss: 3.69018812e-06
Iter: 611 loss: 3.72085333e-06
Iter: 612 loss: 3.68952897e-06
Iter: 613 loss: 3.68532619e-06
Iter: 614 loss: 3.70391717e-06
Iter: 615 loss: 3.68447627e-06
Iter: 616 loss: 3.68137e-06
Iter: 617 loss: 3.69021154e-06
Iter: 618 loss: 3.68049564e-06
Iter: 619 loss: 3.67631128e-06
Iter: 620 loss: 3.67683788e-06
Iter: 621 loss: 3.67313532e-06
Iter: 622 loss: 3.66923041e-06
Iter: 623 loss: 3.7040686e-06
Iter: 624 loss: 3.66915515e-06
Iter: 625 loss: 3.66570475e-06
Iter: 626 loss: 3.66145014e-06
Iter: 627 loss: 3.661145e-06
Iter: 628 loss: 3.65574715e-06
Iter: 629 loss: 3.72217892e-06
Iter: 630 loss: 3.65568349e-06
Iter: 631 loss: 3.65234155e-06
Iter: 632 loss: 3.64788684e-06
Iter: 633 loss: 3.6476049e-06
Iter: 634 loss: 3.64632115e-06
Iter: 635 loss: 3.64461857e-06
Iter: 636 loss: 3.64163429e-06
Iter: 637 loss: 3.65352412e-06
Iter: 638 loss: 3.64084144e-06
Iter: 639 loss: 3.63965682e-06
Iter: 640 loss: 3.63554818e-06
Iter: 641 loss: 3.64092057e-06
Iter: 642 loss: 3.63245886e-06
Iter: 643 loss: 3.6278534e-06
Iter: 644 loss: 3.67811253e-06
Iter: 645 loss: 3.62769833e-06
Iter: 646 loss: 3.62291212e-06
Iter: 647 loss: 3.63124923e-06
Iter: 648 loss: 3.62076412e-06
Iter: 649 loss: 3.61695675e-06
Iter: 650 loss: 3.6548613e-06
Iter: 651 loss: 3.61677394e-06
Iter: 652 loss: 3.61346088e-06
Iter: 653 loss: 3.61132925e-06
Iter: 654 loss: 3.61009825e-06
Iter: 655 loss: 3.60548825e-06
Iter: 656 loss: 3.63395952e-06
Iter: 657 loss: 3.60500076e-06
Iter: 658 loss: 3.60070317e-06
Iter: 659 loss: 3.6077945e-06
Iter: 660 loss: 3.59866613e-06
Iter: 661 loss: 3.59484397e-06
Iter: 662 loss: 3.61896e-06
Iter: 663 loss: 3.59440764e-06
Iter: 664 loss: 3.59081923e-06
Iter: 665 loss: 3.58977672e-06
Iter: 666 loss: 3.58760599e-06
Iter: 667 loss: 3.58360262e-06
Iter: 668 loss: 3.61291222e-06
Iter: 669 loss: 3.58316174e-06
Iter: 670 loss: 3.58053762e-06
Iter: 671 loss: 3.61566458e-06
Iter: 672 loss: 3.58047168e-06
Iter: 673 loss: 3.577391e-06
Iter: 674 loss: 3.57620343e-06
Iter: 675 loss: 3.57434078e-06
Iter: 676 loss: 3.57200724e-06
Iter: 677 loss: 3.5698979e-06
Iter: 678 loss: 3.56931037e-06
Iter: 679 loss: 3.56516671e-06
Iter: 680 loss: 3.56420878e-06
Iter: 681 loss: 3.56159035e-06
Iter: 682 loss: 3.55780548e-06
Iter: 683 loss: 3.59898559e-06
Iter: 684 loss: 3.55765769e-06
Iter: 685 loss: 3.55373049e-06
Iter: 686 loss: 3.55232146e-06
Iter: 687 loss: 3.55008297e-06
Iter: 688 loss: 3.54621238e-06
Iter: 689 loss: 3.57373256e-06
Iter: 690 loss: 3.54594818e-06
Iter: 691 loss: 3.54173517e-06
Iter: 692 loss: 3.54265353e-06
Iter: 693 loss: 3.53875203e-06
Iter: 694 loss: 3.5350231e-06
Iter: 695 loss: 3.53498399e-06
Iter: 696 loss: 3.53177779e-06
Iter: 697 loss: 3.52882216e-06
Iter: 698 loss: 3.52798861e-06
Iter: 699 loss: 3.52233928e-06
Iter: 700 loss: 3.55131806e-06
Iter: 701 loss: 3.52139068e-06
Iter: 702 loss: 3.51814674e-06
Iter: 703 loss: 3.51961398e-06
Iter: 704 loss: 3.51593599e-06
Iter: 705 loss: 3.51504559e-06
Iter: 706 loss: 3.51358e-06
Iter: 707 loss: 3.51165954e-06
Iter: 708 loss: 3.50987602e-06
Iter: 709 loss: 3.50932828e-06
Iter: 710 loss: 3.50741357e-06
Iter: 711 loss: 3.50567416e-06
Iter: 712 loss: 3.50533833e-06
Iter: 713 loss: 3.50169512e-06
Iter: 714 loss: 3.50474875e-06
Iter: 715 loss: 3.49945776e-06
Iter: 716 loss: 3.49506877e-06
Iter: 717 loss: 3.50175719e-06
Iter: 718 loss: 3.49308721e-06
Iter: 719 loss: 3.48915137e-06
Iter: 720 loss: 3.52131974e-06
Iter: 721 loss: 3.48882736e-06
Iter: 722 loss: 3.48555704e-06
Iter: 723 loss: 3.48337426e-06
Iter: 724 loss: 3.48223148e-06
Iter: 725 loss: 3.47825517e-06
Iter: 726 loss: 3.52508732e-06
Iter: 727 loss: 3.47817968e-06
Iter: 728 loss: 3.47475907e-06
Iter: 729 loss: 3.47576088e-06
Iter: 730 loss: 3.47216496e-06
Iter: 731 loss: 3.46865181e-06
Iter: 732 loss: 3.49547054e-06
Iter: 733 loss: 3.4683253e-06
Iter: 734 loss: 3.46462411e-06
Iter: 735 loss: 3.46349202e-06
Iter: 736 loss: 3.46126944e-06
Iter: 737 loss: 3.45739227e-06
Iter: 738 loss: 3.51340896e-06
Iter: 739 loss: 3.45739363e-06
Iter: 740 loss: 3.45704825e-06
Iter: 741 loss: 3.45630019e-06
Iter: 742 loss: 3.45535273e-06
Iter: 743 loss: 3.4526015e-06
Iter: 744 loss: 3.45198214e-06
Iter: 745 loss: 3.44937e-06
Iter: 746 loss: 3.44422756e-06
Iter: 747 loss: 3.4913644e-06
Iter: 748 loss: 3.44396562e-06
Iter: 749 loss: 3.44131195e-06
Iter: 750 loss: 3.44384625e-06
Iter: 751 loss: 3.43968986e-06
Iter: 752 loss: 3.4356276e-06
Iter: 753 loss: 3.44696264e-06
Iter: 754 loss: 3.43418901e-06
Iter: 755 loss: 3.43148258e-06
Iter: 756 loss: 3.43932788e-06
Iter: 757 loss: 3.43072429e-06
Iter: 758 loss: 3.42696012e-06
Iter: 759 loss: 3.42379553e-06
Iter: 760 loss: 3.42280532e-06
Iter: 761 loss: 3.4184493e-06
Iter: 762 loss: 3.44471505e-06
Iter: 763 loss: 3.4180182e-06
Iter: 764 loss: 3.41333521e-06
Iter: 765 loss: 3.42186581e-06
Iter: 766 loss: 3.41126815e-06
Iter: 767 loss: 3.40811516e-06
Iter: 768 loss: 3.41834311e-06
Iter: 769 loss: 3.40730048e-06
Iter: 770 loss: 3.40314546e-06
Iter: 771 loss: 3.41392206e-06
Iter: 772 loss: 3.40160614e-06
Iter: 773 loss: 3.39866756e-06
Iter: 774 loss: 3.42666226e-06
Iter: 775 loss: 3.39858821e-06
Iter: 776 loss: 3.39721441e-06
Iter: 777 loss: 3.3971196e-06
Iter: 778 loss: 3.39573762e-06
Iter: 779 loss: 3.39150415e-06
Iter: 780 loss: 3.41065515e-06
Iter: 781 loss: 3.38993459e-06
Iter: 782 loss: 3.38701898e-06
Iter: 783 loss: 3.39549888e-06
Iter: 784 loss: 3.38602922e-06
Iter: 785 loss: 3.38250175e-06
Iter: 786 loss: 3.38703421e-06
Iter: 787 loss: 3.38068071e-06
Iter: 788 loss: 3.37717825e-06
Iter: 789 loss: 3.407132e-06
Iter: 790 loss: 3.3769993e-06
Iter: 791 loss: 3.37448228e-06
Iter: 792 loss: 3.37566894e-06
Iter: 793 loss: 3.37277515e-06
Iter: 794 loss: 3.36945595e-06
Iter: 795 loss: 3.37967276e-06
Iter: 796 loss: 3.36845574e-06
Iter: 797 loss: 3.36500079e-06
Iter: 798 loss: 3.36945232e-06
Iter: 799 loss: 3.36306312e-06
Iter: 800 loss: 3.35981758e-06
Iter: 801 loss: 3.37262509e-06
Iter: 802 loss: 3.3590868e-06
Iter: 803 loss: 3.35560821e-06
Iter: 804 loss: 3.36232165e-06
Iter: 805 loss: 3.35419031e-06
Iter: 806 loss: 3.35108143e-06
Iter: 807 loss: 3.36726634e-06
Iter: 808 loss: 3.35066306e-06
Iter: 809 loss: 3.34704214e-06
Iter: 810 loss: 3.34374954e-06
Iter: 811 loss: 3.34291371e-06
Iter: 812 loss: 3.34049787e-06
Iter: 813 loss: 3.34025663e-06
Iter: 814 loss: 3.33860567e-06
Iter: 815 loss: 3.33860021e-06
Iter: 816 loss: 3.33693265e-06
Iter: 817 loss: 3.33409616e-06
Iter: 818 loss: 3.33405683e-06
Iter: 819 loss: 3.33209755e-06
Iter: 820 loss: 3.3372753e-06
Iter: 821 loss: 3.33139906e-06
Iter: 822 loss: 3.32931859e-06
Iter: 823 loss: 3.32517607e-06
Iter: 824 loss: 3.40874772e-06
Iter: 825 loss: 3.32514128e-06
Iter: 826 loss: 3.32170566e-06
Iter: 827 loss: 3.32169566e-06
Iter: 828 loss: 3.31863e-06
Iter: 829 loss: 3.31649176e-06
Iter: 830 loss: 3.31541014e-06
Iter: 831 loss: 3.31312458e-06
Iter: 832 loss: 3.31303318e-06
Iter: 833 loss: 3.31077445e-06
Iter: 834 loss: 3.3071758e-06
Iter: 835 loss: 3.3071035e-06
Iter: 836 loss: 3.30379362e-06
Iter: 837 loss: 3.33315347e-06
Iter: 838 loss: 3.30374178e-06
Iter: 839 loss: 3.30039802e-06
Iter: 840 loss: 3.29799605e-06
Iter: 841 loss: 3.29685554e-06
Iter: 842 loss: 3.2927187e-06
Iter: 843 loss: 3.3368342e-06
Iter: 844 loss: 3.29267391e-06
Iter: 845 loss: 3.2896105e-06
Iter: 846 loss: 3.29403247e-06
Iter: 847 loss: 3.28813098e-06
Iter: 848 loss: 3.28641227e-06
Iter: 849 loss: 3.28620854e-06
Iter: 850 loss: 3.28416854e-06
Iter: 851 loss: 3.28869055e-06
Iter: 852 loss: 3.2833359e-06
Iter: 853 loss: 3.28164606e-06
Iter: 854 loss: 3.2788339e-06
Iter: 855 loss: 3.27883117e-06
Iter: 856 loss: 3.27583143e-06
Iter: 857 loss: 3.28185024e-06
Iter: 858 loss: 3.2747746e-06
Iter: 859 loss: 3.27090856e-06
Iter: 860 loss: 3.28099463e-06
Iter: 861 loss: 3.26944337e-06
Iter: 862 loss: 3.26687859e-06
Iter: 863 loss: 3.26993313e-06
Iter: 864 loss: 3.26548366e-06
Iter: 865 loss: 3.26161171e-06
Iter: 866 loss: 3.2673197e-06
Iter: 867 loss: 3.25990777e-06
Iter: 868 loss: 3.25706719e-06
Iter: 869 loss: 3.27597354e-06
Iter: 870 loss: 3.25680594e-06
Iter: 871 loss: 3.25357496e-06
Iter: 872 loss: 3.25311157e-06
Iter: 873 loss: 3.25074916e-06
Iter: 874 loss: 3.24773237e-06
Iter: 875 loss: 3.28826241e-06
Iter: 876 loss: 3.24774101e-06
Iter: 877 loss: 3.24553503e-06
Iter: 878 loss: 3.24264784e-06
Iter: 879 loss: 3.24254597e-06
Iter: 880 loss: 3.23903259e-06
Iter: 881 loss: 3.26996428e-06
Iter: 882 loss: 3.23872746e-06
Iter: 883 loss: 3.23632867e-06
Iter: 884 loss: 3.24498524e-06
Iter: 885 loss: 3.23574977e-06
Iter: 886 loss: 3.23269978e-06
Iter: 887 loss: 3.26527061e-06
Iter: 888 loss: 3.23270342e-06
Iter: 889 loss: 3.2316791e-06
Iter: 890 loss: 3.22952405e-06
Iter: 891 loss: 3.2630719e-06
Iter: 892 loss: 3.22941014e-06
Iter: 893 loss: 3.22687583e-06
Iter: 894 loss: 3.22805818e-06
Iter: 895 loss: 3.22502069e-06
Iter: 896 loss: 3.22190726e-06
Iter: 897 loss: 3.23985705e-06
Iter: 898 loss: 3.22147253e-06
Iter: 899 loss: 3.21877928e-06
Iter: 900 loss: 3.22406049e-06
Iter: 901 loss: 3.21768675e-06
Iter: 902 loss: 3.21487505e-06
Iter: 903 loss: 3.22456799e-06
Iter: 904 loss: 3.21406787e-06
Iter: 905 loss: 3.21100606e-06
Iter: 906 loss: 3.20760819e-06
Iter: 907 loss: 3.20716913e-06
Iter: 908 loss: 3.20348317e-06
Iter: 909 loss: 3.25011547e-06
Iter: 910 loss: 3.20355275e-06
Iter: 911 loss: 3.20001845e-06
Iter: 912 loss: 3.20179834e-06
Iter: 913 loss: 3.19782021e-06
Iter: 914 loss: 3.19497713e-06
Iter: 915 loss: 3.22349797e-06
Iter: 916 loss: 3.1949312e-06
Iter: 917 loss: 3.19265473e-06
Iter: 918 loss: 3.18876732e-06
Iter: 919 loss: 3.18873435e-06
Iter: 920 loss: 3.19041101e-06
Iter: 921 loss: 3.1874265e-06
Iter: 922 loss: 3.18597267e-06
Iter: 923 loss: 3.18717412e-06
Iter: 924 loss: 3.18502862e-06
Iter: 925 loss: 3.18372872e-06
Iter: 926 loss: 3.1807117e-06
Iter: 927 loss: 3.21469201e-06
Iter: 928 loss: 3.18045227e-06
Iter: 929 loss: 3.17763761e-06
Iter: 930 loss: 3.1932077e-06
Iter: 931 loss: 3.17731201e-06
Iter: 932 loss: 3.1746647e-06
Iter: 933 loss: 3.17361355e-06
Iter: 934 loss: 3.17220588e-06
Iter: 935 loss: 3.16960723e-06
Iter: 936 loss: 3.20621325e-06
Iter: 937 loss: 3.16957221e-06
Iter: 938 loss: 3.16711589e-06
Iter: 939 loss: 3.1671359e-06
Iter: 940 loss: 3.16503611e-06
Iter: 941 loss: 3.16166e-06
Iter: 942 loss: 3.17901277e-06
Iter: 943 loss: 3.16118894e-06
Iter: 944 loss: 3.15837565e-06
Iter: 945 loss: 3.15957436e-06
Iter: 946 loss: 3.15661646e-06
Iter: 947 loss: 3.15345687e-06
Iter: 948 loss: 3.16807791e-06
Iter: 949 loss: 3.15277521e-06
Iter: 950 loss: 3.14981276e-06
Iter: 951 loss: 3.15207762e-06
Iter: 952 loss: 3.14803e-06
Iter: 953 loss: 3.14581712e-06
Iter: 954 loss: 3.14592239e-06
Iter: 955 loss: 3.14445037e-06
Iter: 956 loss: 3.16521664e-06
Iter: 957 loss: 3.14442423e-06
Iter: 958 loss: 3.14323802e-06
Iter: 959 loss: 3.1403797e-06
Iter: 960 loss: 3.16335263e-06
Iter: 961 loss: 3.13985265e-06
Iter: 962 loss: 3.13764485e-06
Iter: 963 loss: 3.14242129e-06
Iter: 964 loss: 3.13661621e-06
Iter: 965 loss: 3.13337296e-06
Iter: 966 loss: 3.13809483e-06
Iter: 967 loss: 3.13176611e-06
Iter: 968 loss: 3.12898692e-06
Iter: 969 loss: 3.14033082e-06
Iter: 970 loss: 3.12847419e-06
Iter: 971 loss: 3.12522388e-06
Iter: 972 loss: 3.12676593e-06
Iter: 973 loss: 3.12310954e-06
Iter: 974 loss: 3.12042198e-06
Iter: 975 loss: 3.14643421e-06
Iter: 976 loss: 3.12029965e-06
Iter: 977 loss: 3.11815757e-06
Iter: 978 loss: 3.12105567e-06
Iter: 979 loss: 3.11707936e-06
Iter: 980 loss: 3.11459144e-06
Iter: 981 loss: 3.12659608e-06
Iter: 982 loss: 3.11403051e-06
Iter: 983 loss: 3.11197482e-06
Iter: 984 loss: 3.11002236e-06
Iter: 985 loss: 3.10945461e-06
Iter: 986 loss: 3.1067807e-06
Iter: 987 loss: 3.1484e-06
Iter: 988 loss: 3.10676296e-06
Iter: 989 loss: 3.10634141e-06
Iter: 990 loss: 3.10581322e-06
Iter: 991 loss: 3.10509722e-06
Iter: 992 loss: 3.10323594e-06
Iter: 993 loss: 3.12154134e-06
Iter: 994 loss: 3.10303312e-06
Iter: 995 loss: 3.10104838e-06
Iter: 996 loss: 3.10247242e-06
Iter: 997 loss: 3.09980896e-06
Iter: 998 loss: 3.09711e-06
Iter: 999 loss: 3.10273981e-06
Iter: 1000 loss: 3.09602e-06
Iter: 1001 loss: 3.09361576e-06
Iter: 1002 loss: 3.10432256e-06
Iter: 1003 loss: 3.09306279e-06
Iter: 1004 loss: 3.09066763e-06
Iter: 1005 loss: 3.09034726e-06
Iter: 1006 loss: 3.08861922e-06
Iter: 1007 loss: 3.08619838e-06
Iter: 1008 loss: 3.1216864e-06
Iter: 1009 loss: 3.08625658e-06
Iter: 1010 loss: 3.08436688e-06
Iter: 1011 loss: 3.08250719e-06
Iter: 1012 loss: 3.08212748e-06
Iter: 1013 loss: 3.07960363e-06
Iter: 1014 loss: 3.11664894e-06
Iter: 1015 loss: 3.07957635e-06
Iter: 1016 loss: 3.07781647e-06
Iter: 1017 loss: 3.07838036e-06
Iter: 1018 loss: 3.07664095e-06
Iter: 1019 loss: 3.07386608e-06
Iter: 1020 loss: 3.07633377e-06
Iter: 1021 loss: 3.07217397e-06
Iter: 1022 loss: 3.07203368e-06
Iter: 1023 loss: 3.07129721e-06
Iter: 1024 loss: 3.06998982e-06
Iter: 1025 loss: 3.06852462e-06
Iter: 1026 loss: 3.06843708e-06
Iter: 1027 loss: 3.06681704e-06
Iter: 1028 loss: 3.06476932e-06
Iter: 1029 loss: 3.06451761e-06
Iter: 1030 loss: 3.06194033e-06
Iter: 1031 loss: 3.07169216e-06
Iter: 1032 loss: 3.06134461e-06
Iter: 1033 loss: 3.05844742e-06
Iter: 1034 loss: 3.06292759e-06
Iter: 1035 loss: 3.05708704e-06
Iter: 1036 loss: 3.05417871e-06
Iter: 1037 loss: 3.0614076e-06
Iter: 1038 loss: 3.05320509e-06
Iter: 1039 loss: 3.05058597e-06
Iter: 1040 loss: 3.07056575e-06
Iter: 1041 loss: 3.05038179e-06
Iter: 1042 loss: 3.04814e-06
Iter: 1043 loss: 3.04666401e-06
Iter: 1044 loss: 3.04581818e-06
Iter: 1045 loss: 3.04303376e-06
Iter: 1046 loss: 3.08046106e-06
Iter: 1047 loss: 3.04301875e-06
Iter: 1048 loss: 3.04138166e-06
Iter: 1049 loss: 3.04346668e-06
Iter: 1050 loss: 3.04050309e-06
Iter: 1051 loss: 3.03800834e-06
Iter: 1052 loss: 3.03881939e-06
Iter: 1053 loss: 3.03606475e-06
Iter: 1054 loss: 3.03404386e-06
Iter: 1055 loss: 3.06633774e-06
Iter: 1056 loss: 3.03406364e-06
Iter: 1057 loss: 3.03262414e-06
Iter: 1058 loss: 3.05315621e-06
Iter: 1059 loss: 3.03261118e-06
Iter: 1060 loss: 3.03157981e-06
Iter: 1061 loss: 3.02893022e-06
Iter: 1062 loss: 3.04020909e-06
Iter: 1063 loss: 3.02787839e-06
Iter: 1064 loss: 3.02544254e-06
Iter: 1065 loss: 3.03311663e-06
Iter: 1066 loss: 3.02478429e-06
Iter: 1067 loss: 3.02155672e-06
Iter: 1068 loss: 3.02629e-06
Iter: 1069 loss: 3.02005105e-06
Iter: 1070 loss: 3.01723458e-06
Iter: 1071 loss: 3.03717457e-06
Iter: 1072 loss: 3.01700175e-06
Iter: 1073 loss: 3.01484624e-06
Iter: 1074 loss: 3.01729779e-06
Iter: 1075 loss: 3.0136282e-06
Iter: 1076 loss: 3.01104251e-06
Iter: 1077 loss: 3.01800287e-06
Iter: 1078 loss: 3.01018918e-06
Iter: 1079 loss: 3.00747229e-06
Iter: 1080 loss: 3.01580508e-06
Iter: 1081 loss: 3.00677107e-06
Iter: 1082 loss: 3.00462671e-06
Iter: 1083 loss: 3.01358114e-06
Iter: 1084 loss: 3.00418924e-06
Iter: 1085 loss: 3.0018025e-06
Iter: 1086 loss: 3.00429656e-06
Iter: 1087 loss: 3.00049123e-06
Iter: 1088 loss: 2.99828253e-06
Iter: 1089 loss: 3.00958436e-06
Iter: 1090 loss: 2.99788121e-06
Iter: 1091 loss: 2.9968669e-06
Iter: 1092 loss: 2.99660405e-06
Iter: 1093 loss: 2.9953776e-06
Iter: 1094 loss: 2.99231465e-06
Iter: 1095 loss: 3.01772252e-06
Iter: 1096 loss: 2.99173325e-06
Iter: 1097 loss: 2.98955365e-06
Iter: 1098 loss: 2.9971884e-06
Iter: 1099 loss: 2.98915484e-06
Iter: 1100 loss: 2.98654959e-06
Iter: 1101 loss: 2.98421901e-06
Iter: 1102 loss: 2.98359055e-06
Iter: 1103 loss: 2.980933e-06
Iter: 1104 loss: 3.01457385e-06
Iter: 1105 loss: 2.98094778e-06
Iter: 1106 loss: 2.9782741e-06
Iter: 1107 loss: 2.97886845e-06
Iter: 1108 loss: 2.97627298e-06
Iter: 1109 loss: 2.97339557e-06
Iter: 1110 loss: 2.99159319e-06
Iter: 1111 loss: 2.97309339e-06
Iter: 1112 loss: 2.97059819e-06
Iter: 1113 loss: 2.9728094e-06
Iter: 1114 loss: 2.96918506e-06
Iter: 1115 loss: 2.96642042e-06
Iter: 1116 loss: 2.980923e-06
Iter: 1117 loss: 2.9658795e-06
Iter: 1118 loss: 2.96341477e-06
Iter: 1119 loss: 2.97030533e-06
Iter: 1120 loss: 2.96265807e-06
Iter: 1121 loss: 2.96060261e-06
Iter: 1122 loss: 2.96715643e-06
Iter: 1123 loss: 2.95991845e-06
Iter: 1124 loss: 2.95830159e-06
Iter: 1125 loss: 2.98245436e-06
Iter: 1126 loss: 2.95825339e-06
Iter: 1127 loss: 2.95619202e-06
Iter: 1128 loss: 2.95656491e-06
Iter: 1129 loss: 2.95461905e-06
Iter: 1130 loss: 2.95352652e-06
Iter: 1131 loss: 2.95219206e-06
Iter: 1132 loss: 2.95208042e-06
Iter: 1133 loss: 2.94967e-06
Iter: 1134 loss: 2.94912911e-06
Iter: 1135 loss: 2.94737947e-06
Iter: 1136 loss: 2.94508118e-06
Iter: 1137 loss: 2.9633843e-06
Iter: 1138 loss: 2.94485835e-06
Iter: 1139 loss: 2.94240749e-06
Iter: 1140 loss: 2.94307802e-06
Iter: 1141 loss: 2.94072515e-06
Iter: 1142 loss: 2.93819653e-06
Iter: 1143 loss: 2.96499366e-06
Iter: 1144 loss: 2.93807261e-06
Iter: 1145 loss: 2.93589528e-06
Iter: 1146 loss: 2.93423022e-06
Iter: 1147 loss: 2.93355833e-06
Iter: 1148 loss: 2.93094308e-06
Iter: 1149 loss: 2.9620262e-06
Iter: 1150 loss: 2.93092262e-06
Iter: 1151 loss: 2.92907907e-06
Iter: 1152 loss: 2.92993832e-06
Iter: 1153 loss: 2.92795266e-06
Iter: 1154 loss: 2.92478899e-06
Iter: 1155 loss: 2.92977893e-06
Iter: 1156 loss: 2.92342793e-06
Iter: 1157 loss: 2.92092636e-06
Iter: 1158 loss: 2.95394102e-06
Iter: 1159 loss: 2.92092454e-06
Iter: 1160 loss: 2.91899596e-06
Iter: 1161 loss: 2.93943754e-06
Iter: 1162 loss: 2.91889546e-06
Iter: 1163 loss: 2.91802417e-06
Iter: 1164 loss: 2.91619153e-06
Iter: 1165 loss: 2.94650681e-06
Iter: 1166 loss: 2.91610422e-06
Iter: 1167 loss: 2.91402557e-06
Iter: 1168 loss: 2.91256265e-06
Iter: 1169 loss: 2.91188462e-06
Iter: 1170 loss: 2.9089997e-06
Iter: 1171 loss: 2.93342578e-06
Iter: 1172 loss: 2.90871776e-06
Iter: 1173 loss: 2.90645585e-06
Iter: 1174 loss: 2.90641856e-06
Iter: 1175 loss: 2.90466051e-06
Iter: 1176 loss: 2.90182697e-06
Iter: 1177 loss: 2.92833101e-06
Iter: 1178 loss: 2.90165394e-06
Iter: 1179 loss: 2.89973309e-06
Iter: 1180 loss: 2.89967738e-06
Iter: 1181 loss: 2.89818172e-06
Iter: 1182 loss: 2.89501259e-06
Iter: 1183 loss: 2.91174592e-06
Iter: 1184 loss: 2.89452078e-06
Iter: 1185 loss: 2.89258969e-06
Iter: 1186 loss: 2.89588888e-06
Iter: 1187 loss: 2.89197442e-06
Iter: 1188 loss: 2.88920751e-06
Iter: 1189 loss: 2.89138688e-06
Iter: 1190 loss: 2.88766887e-06
Iter: 1191 loss: 2.88589e-06
Iter: 1192 loss: 2.88588421e-06
Iter: 1193 loss: 2.88480032e-06
Iter: 1194 loss: 2.89923855e-06
Iter: 1195 loss: 2.8848026e-06
Iter: 1196 loss: 2.88388242e-06
Iter: 1197 loss: 2.88138949e-06
Iter: 1198 loss: 2.90083108e-06
Iter: 1199 loss: 2.88087404e-06
Iter: 1200 loss: 2.87896091e-06
Iter: 1201 loss: 2.88755382e-06
Iter: 1202 loss: 2.87859029e-06
Iter: 1203 loss: 2.87670036e-06
Iter: 1204 loss: 2.87717921e-06
Iter: 1205 loss: 2.87531e-06
Iter: 1206 loss: 2.87273087e-06
Iter: 1207 loss: 2.87964599e-06
Iter: 1208 loss: 2.87205557e-06
Iter: 1209 loss: 2.86973227e-06
Iter: 1210 loss: 2.88509136e-06
Iter: 1211 loss: 2.86951013e-06
Iter: 1212 loss: 2.867896e-06
Iter: 1213 loss: 2.86711384e-06
Iter: 1214 loss: 2.86623322e-06
Iter: 1215 loss: 2.8635327e-06
Iter: 1216 loss: 2.88372212e-06
Iter: 1217 loss: 2.86332738e-06
Iter: 1218 loss: 2.86143973e-06
Iter: 1219 loss: 2.86411819e-06
Iter: 1220 loss: 2.86055365e-06
Iter: 1221 loss: 2.85788201e-06
Iter: 1222 loss: 2.86015575e-06
Iter: 1223 loss: 2.8563079e-06
Iter: 1224 loss: 2.85451779e-06
Iter: 1225 loss: 2.88045135e-06
Iter: 1226 loss: 2.85448095e-06
Iter: 1227 loss: 2.85315218e-06
Iter: 1228 loss: 2.86842351e-06
Iter: 1229 loss: 2.85316128e-06
Iter: 1230 loss: 2.8517411e-06
Iter: 1231 loss: 2.84973794e-06
Iter: 1232 loss: 2.84965699e-06
Iter: 1233 loss: 2.84825819e-06
Iter: 1234 loss: 2.84924658e-06
Iter: 1235 loss: 2.84740622e-06
Iter: 1236 loss: 2.84551425e-06
Iter: 1237 loss: 2.84386169e-06
Iter: 1238 loss: 2.8432396e-06
Iter: 1239 loss: 2.84052476e-06
Iter: 1240 loss: 2.86174031e-06
Iter: 1241 loss: 2.84039652e-06
Iter: 1242 loss: 2.83806094e-06
Iter: 1243 loss: 2.83951704e-06
Iter: 1244 loss: 2.8365157e-06
Iter: 1245 loss: 2.83362351e-06
Iter: 1246 loss: 2.84276507e-06
Iter: 1247 loss: 2.83273835e-06
Iter: 1248 loss: 2.8298698e-06
Iter: 1249 loss: 2.84562611e-06
Iter: 1250 loss: 2.8294935e-06
Iter: 1251 loss: 2.82750625e-06
Iter: 1252 loss: 2.830217e-06
Iter: 1253 loss: 2.82651217e-06
Iter: 1254 loss: 2.82372685e-06
Iter: 1255 loss: 2.83294321e-06
Iter: 1256 loss: 2.82306064e-06
Iter: 1257 loss: 2.82103633e-06
Iter: 1258 loss: 2.82724159e-06
Iter: 1259 loss: 2.8203126e-06
Iter: 1260 loss: 2.81870393e-06
Iter: 1261 loss: 2.8423965e-06
Iter: 1262 loss: 2.81872644e-06
Iter: 1263 loss: 2.81685925e-06
Iter: 1264 loss: 2.82064457e-06
Iter: 1265 loss: 2.81607436e-06
Iter: 1266 loss: 2.81506823e-06
Iter: 1267 loss: 2.81466e-06
Iter: 1268 loss: 2.81414896e-06
Iter: 1269 loss: 2.81264533e-06
Iter: 1270 loss: 2.80969107e-06
Iter: 1271 loss: 2.86882187e-06
Iter: 1272 loss: 2.80964696e-06
Iter: 1273 loss: 2.80745644e-06
Iter: 1274 loss: 2.8074669e-06
Iter: 1275 loss: 2.80564677e-06
Iter: 1276 loss: 2.80456698e-06
Iter: 1277 loss: 2.80386712e-06
Iter: 1278 loss: 2.80166932e-06
Iter: 1279 loss: 2.81745793e-06
Iter: 1280 loss: 2.80136987e-06
Iter: 1281 loss: 2.79933465e-06
Iter: 1282 loss: 2.80230324e-06
Iter: 1283 loss: 2.79826281e-06
Iter: 1284 loss: 2.79594565e-06
Iter: 1285 loss: 2.80424956e-06
Iter: 1286 loss: 2.79531105e-06
Iter: 1287 loss: 2.79313599e-06
Iter: 1288 loss: 2.80108293e-06
Iter: 1289 loss: 2.79245887e-06
Iter: 1290 loss: 2.79063443e-06
Iter: 1291 loss: 2.79676487e-06
Iter: 1292 loss: 2.79005599e-06
Iter: 1293 loss: 2.78831135e-06
Iter: 1294 loss: 2.79931305e-06
Iter: 1295 loss: 2.7881124e-06
Iter: 1296 loss: 2.78655261e-06
Iter: 1297 loss: 2.80958375e-06
Iter: 1298 loss: 2.78648554e-06
Iter: 1299 loss: 2.78588959e-06
Iter: 1300 loss: 2.78461494e-06
Iter: 1301 loss: 2.807084e-06
Iter: 1302 loss: 2.78459856e-06
Iter: 1303 loss: 2.78301059e-06
Iter: 1304 loss: 2.78157199e-06
Iter: 1305 loss: 2.78112043e-06
Iter: 1306 loss: 2.77854861e-06
Iter: 1307 loss: 2.79350274e-06
Iter: 1308 loss: 2.77820186e-06
Iter: 1309 loss: 2.77617437e-06
Iter: 1310 loss: 2.77515505e-06
Iter: 1311 loss: 2.77414028e-06
Iter: 1312 loss: 2.77154095e-06
Iter: 1313 loss: 2.80546283e-06
Iter: 1314 loss: 2.77151594e-06
Iter: 1315 loss: 2.76999708e-06
Iter: 1316 loss: 2.76963442e-06
Iter: 1317 loss: 2.76855417e-06
Iter: 1318 loss: 2.76597848e-06
Iter: 1319 loss: 2.7789315e-06
Iter: 1320 loss: 2.76557194e-06
Iter: 1321 loss: 2.76369587e-06
Iter: 1322 loss: 2.76777314e-06
Iter: 1323 loss: 2.76289029e-06
Iter: 1324 loss: 2.76079368e-06
Iter: 1325 loss: 2.7670435e-06
Iter: 1326 loss: 2.76005949e-06
Iter: 1327 loss: 2.75816592e-06
Iter: 1328 loss: 2.76980791e-06
Iter: 1329 loss: 2.75793968e-06
Iter: 1330 loss: 2.75677053e-06
Iter: 1331 loss: 2.75672573e-06
Iter: 1332 loss: 2.75588673e-06
Iter: 1333 loss: 2.75415346e-06
Iter: 1334 loss: 2.78297853e-06
Iter: 1335 loss: 2.75415914e-06
Iter: 1336 loss: 2.75251705e-06
Iter: 1337 loss: 2.75198818e-06
Iter: 1338 loss: 2.75101229e-06
Iter: 1339 loss: 2.74848617e-06
Iter: 1340 loss: 2.76238848e-06
Iter: 1341 loss: 2.7480412e-06
Iter: 1342 loss: 2.74623471e-06
Iter: 1343 loss: 2.74357e-06
Iter: 1344 loss: 2.74358422e-06
Iter: 1345 loss: 2.74104809e-06
Iter: 1346 loss: 2.74101058e-06
Iter: 1347 loss: 2.7394758e-06
Iter: 1348 loss: 2.73783758e-06
Iter: 1349 loss: 2.7375047e-06
Iter: 1350 loss: 2.73478145e-06
Iter: 1351 loss: 2.75775096e-06
Iter: 1352 loss: 2.73462024e-06
Iter: 1353 loss: 2.73270598e-06
Iter: 1354 loss: 2.73283467e-06
Iter: 1355 loss: 2.73121123e-06
Iter: 1356 loss: 2.72875695e-06
Iter: 1357 loss: 2.74474587e-06
Iter: 1358 loss: 2.72836e-06
Iter: 1359 loss: 2.72649345e-06
Iter: 1360 loss: 2.74038098e-06
Iter: 1361 loss: 2.72631542e-06
Iter: 1362 loss: 2.72556463e-06
Iter: 1363 loss: 2.7254232e-06
Iter: 1364 loss: 2.72473562e-06
Iter: 1365 loss: 2.7234837e-06
Iter: 1366 loss: 2.75166781e-06
Iter: 1367 loss: 2.72358557e-06
Iter: 1368 loss: 2.72209036e-06
Iter: 1369 loss: 2.72153602e-06
Iter: 1370 loss: 2.72092711e-06
Iter: 1371 loss: 2.71880754e-06
Iter: 1372 loss: 2.7305191e-06
Iter: 1373 loss: 2.7185115e-06
Iter: 1374 loss: 2.71695217e-06
Iter: 1375 loss: 2.71608178e-06
Iter: 1376 loss: 2.71530917e-06
Iter: 1377 loss: 2.71306089e-06
Iter: 1378 loss: 2.73092678e-06
Iter: 1379 loss: 2.71292765e-06
Iter: 1380 loss: 2.71139197e-06
Iter: 1381 loss: 2.71143131e-06
Iter: 1382 loss: 2.7102667e-06
Iter: 1383 loss: 2.70795317e-06
Iter: 1384 loss: 2.7196179e-06
Iter: 1385 loss: 2.70765872e-06
Iter: 1386 loss: 2.70602982e-06
Iter: 1387 loss: 2.71001409e-06
Iter: 1388 loss: 2.70549458e-06
Iter: 1389 loss: 2.7035228e-06
Iter: 1390 loss: 2.7072665e-06
Iter: 1391 loss: 2.70269e-06
Iter: 1392 loss: 2.70120199e-06
Iter: 1393 loss: 2.72006e-06
Iter: 1394 loss: 2.70122155e-06
Iter: 1395 loss: 2.70047144e-06
Iter: 1396 loss: 2.70045143e-06
Iter: 1397 loss: 2.69959082e-06
Iter: 1398 loss: 2.69824477e-06
Iter: 1399 loss: 2.69824454e-06
Iter: 1400 loss: 2.69691282e-06
Iter: 1401 loss: 2.6975913e-06
Iter: 1402 loss: 2.69595012e-06
Iter: 1403 loss: 2.69446173e-06
Iter: 1404 loss: 2.69866132e-06
Iter: 1405 loss: 2.69407633e-06
Iter: 1406 loss: 2.69237421e-06
Iter: 1407 loss: 2.69367047e-06
Iter: 1408 loss: 2.69144493e-06
Iter: 1409 loss: 2.68932536e-06
Iter: 1410 loss: 2.69440602e-06
Iter: 1411 loss: 2.68844315e-06
Iter: 1412 loss: 2.68651615e-06
Iter: 1413 loss: 2.69209e-06
Iter: 1414 loss: 2.68584677e-06
Iter: 1415 loss: 2.68376334e-06
Iter: 1416 loss: 2.68682766e-06
Iter: 1417 loss: 2.68273288e-06
Iter: 1418 loss: 2.6808284e-06
Iter: 1419 loss: 2.68750364e-06
Iter: 1420 loss: 2.68039093e-06
Iter: 1421 loss: 2.67816063e-06
Iter: 1422 loss: 2.68346048e-06
Iter: 1423 loss: 2.67732685e-06
Iter: 1424 loss: 2.6757e-06
Iter: 1425 loss: 2.68865483e-06
Iter: 1426 loss: 2.67561018e-06
Iter: 1427 loss: 2.67466817e-06
Iter: 1428 loss: 2.68838494e-06
Iter: 1429 loss: 2.67463179e-06
Iter: 1430 loss: 2.67336327e-06
Iter: 1431 loss: 2.67256519e-06
Iter: 1432 loss: 2.67207429e-06
Iter: 1433 loss: 2.67081032e-06
Iter: 1434 loss: 2.67188534e-06
Iter: 1435 loss: 2.67014025e-06
Iter: 1436 loss: 2.66886127e-06
Iter: 1437 loss: 2.66819507e-06
Iter: 1438 loss: 2.66752841e-06
Iter: 1439 loss: 2.66545203e-06
Iter: 1440 loss: 2.67818655e-06
Iter: 1441 loss: 2.6652674e-06
Iter: 1442 loss: 2.66362576e-06
Iter: 1443 loss: 2.66426969e-06
Iter: 1444 loss: 2.66245047e-06
Iter: 1445 loss: 2.66055e-06
Iter: 1446 loss: 2.66778852e-06
Iter: 1447 loss: 2.66004804e-06
Iter: 1448 loss: 2.65812764e-06
Iter: 1449 loss: 2.66254256e-06
Iter: 1450 loss: 2.65732888e-06
Iter: 1451 loss: 2.65567496e-06
Iter: 1452 loss: 2.65681183e-06
Iter: 1453 loss: 2.65478684e-06
Iter: 1454 loss: 2.65240601e-06
Iter: 1455 loss: 2.66725669e-06
Iter: 1456 loss: 2.65225117e-06
Iter: 1457 loss: 2.65075732e-06
Iter: 1458 loss: 2.65587687e-06
Iter: 1459 loss: 2.65039262e-06
Iter: 1460 loss: 2.64901701e-06
Iter: 1461 loss: 2.66064762e-06
Iter: 1462 loss: 2.64891e-06
Iter: 1463 loss: 2.6472926e-06
Iter: 1464 loss: 2.65281551e-06
Iter: 1465 loss: 2.64693881e-06
Iter: 1466 loss: 2.64623532e-06
Iter: 1467 loss: 2.64549567e-06
Iter: 1468 loss: 2.64535288e-06
Iter: 1469 loss: 2.6441171e-06
Iter: 1470 loss: 2.64281e-06
Iter: 1471 loss: 2.64249957e-06
Iter: 1472 loss: 2.64044479e-06
Iter: 1473 loss: 2.65630842e-06
Iter: 1474 loss: 2.64030064e-06
Iter: 1475 loss: 2.63867628e-06
Iter: 1476 loss: 2.63966513e-06
Iter: 1477 loss: 2.63766469e-06
Iter: 1478 loss: 2.63565744e-06
Iter: 1479 loss: 2.64196933e-06
Iter: 1480 loss: 2.63515858e-06
Iter: 1481 loss: 2.63341826e-06
Iter: 1482 loss: 2.63892616e-06
Iter: 1483 loss: 2.63293668e-06
Iter: 1484 loss: 2.6313121e-06
Iter: 1485 loss: 2.63215725e-06
Iter: 1486 loss: 2.6303228e-06
Iter: 1487 loss: 2.62811955e-06
Iter: 1488 loss: 2.63890865e-06
Iter: 1489 loss: 2.62769845e-06
Iter: 1490 loss: 2.62604385e-06
Iter: 1491 loss: 2.63436596e-06
Iter: 1492 loss: 2.62585081e-06
Iter: 1493 loss: 2.62443018e-06
Iter: 1494 loss: 2.63031097e-06
Iter: 1495 loss: 2.6241014e-06
Iter: 1496 loss: 2.62257322e-06
Iter: 1497 loss: 2.64213531e-06
Iter: 1498 loss: 2.62252047e-06
Iter: 1499 loss: 2.62190861e-06
Iter: 1500 loss: 2.62101662e-06
Iter: 1501 loss: 2.62099366e-06
Iter: 1502 loss: 2.61977129e-06
Iter: 1503 loss: 2.61835521e-06
Iter: 1504 loss: 2.618179e-06
Iter: 1505 loss: 2.61628725e-06
Iter: 1506 loss: 2.6394714e-06
Iter: 1507 loss: 2.61627474e-06
Iter: 1508 loss: 2.61503146e-06
Iter: 1509 loss: 2.6140865e-06
Iter: 1510 loss: 2.61372429e-06
Iter: 1511 loss: 2.61150399e-06
Iter: 1512 loss: 2.62212643e-06
Iter: 1513 loss: 2.61107198e-06
Iter: 1514 loss: 2.60948582e-06
Iter: 1515 loss: 2.61392825e-06
Iter: 1516 loss: 2.60890238e-06
Iter: 1517 loss: 2.60716206e-06
Iter: 1518 loss: 2.60971751e-06
Iter: 1519 loss: 2.6062869e-06
Iter: 1520 loss: 2.60415209e-06
Iter: 1521 loss: 2.60840466e-06
Iter: 1522 loss: 2.60327852e-06
Iter: 1523 loss: 2.6011212e-06
Iter: 1524 loss: 2.61526725e-06
Iter: 1525 loss: 2.60090587e-06
Iter: 1526 loss: 2.59919079e-06
Iter: 1527 loss: 2.60078923e-06
Iter: 1528 loss: 2.59822195e-06
Iter: 1529 loss: 2.59822264e-06
Iter: 1530 loss: 2.59722265e-06
Iter: 1531 loss: 2.59671833e-06
Iter: 1532 loss: 2.59580838e-06
Iter: 1533 loss: 2.61664081e-06
Iter: 1534 loss: 2.59578e-06
Iter: 1535 loss: 2.59456147e-06
Iter: 1536 loss: 2.59264516e-06
Iter: 1537 loss: 2.59264061e-06
Iter: 1538 loss: 2.59094622e-06
Iter: 1539 loss: 2.59100034e-06
Iter: 1540 loss: 2.58977411e-06
Iter: 1541 loss: 2.58915429e-06
Iter: 1542 loss: 2.58859063e-06
Iter: 1543 loss: 2.5865379e-06
Iter: 1544 loss: 2.59312128e-06
Iter: 1545 loss: 2.58594946e-06
Iter: 1546 loss: 2.58442333e-06
Iter: 1547 loss: 2.58804789e-06
Iter: 1548 loss: 2.58373916e-06
Iter: 1549 loss: 2.58166983e-06
Iter: 1550 loss: 2.58308251e-06
Iter: 1551 loss: 2.58029104e-06
Iter: 1552 loss: 2.57810234e-06
Iter: 1553 loss: 2.58556111e-06
Iter: 1554 loss: 2.57742795e-06
Iter: 1555 loss: 2.57531974e-06
Iter: 1556 loss: 2.58567752e-06
Iter: 1557 loss: 2.57507031e-06
Iter: 1558 loss: 2.57304964e-06
Iter: 1559 loss: 2.57726469e-06
Iter: 1560 loss: 2.57228703e-06
Iter: 1561 loss: 2.57283318e-06
Iter: 1562 loss: 2.57150668e-06
Iter: 1563 loss: 2.5710151e-06
Iter: 1564 loss: 2.56999829e-06
Iter: 1565 loss: 2.58925866e-06
Iter: 1566 loss: 2.56992462e-06
Iter: 1567 loss: 2.56877865e-06
Iter: 1568 loss: 2.56674343e-06
Iter: 1569 loss: 2.56672388e-06
Iter: 1570 loss: 2.56499925e-06
Iter: 1571 loss: 2.58786713e-06
Iter: 1572 loss: 2.56493513e-06
Iter: 1573 loss: 2.56365752e-06
Iter: 1574 loss: 2.56348721e-06
Iter: 1575 loss: 2.56251633e-06
Iter: 1576 loss: 2.56046633e-06
Iter: 1577 loss: 2.56898807e-06
Iter: 1578 loss: 2.56008275e-06
Iter: 1579 loss: 2.55850296e-06
Iter: 1580 loss: 2.56230851e-06
Iter: 1581 loss: 2.55796022e-06
Iter: 1582 loss: 2.55617761e-06
Iter: 1583 loss: 2.55987061e-06
Iter: 1584 loss: 2.55543955e-06
Iter: 1585 loss: 2.5538875e-06
Iter: 1586 loss: 2.55841633e-06
Iter: 1587 loss: 2.55348505e-06
Iter: 1588 loss: 2.55181294e-06
Iter: 1589 loss: 2.55406508e-06
Iter: 1590 loss: 2.55092823e-06
Iter: 1591 loss: 2.54894508e-06
Iter: 1592 loss: 2.55581199e-06
Iter: 1593 loss: 2.5485092e-06
Iter: 1594 loss: 2.54850829e-06
Iter: 1595 loss: 2.54770498e-06
Iter: 1596 loss: 2.54689257e-06
Iter: 1597 loss: 2.54581619e-06
Iter: 1598 loss: 2.54579891e-06
Iter: 1599 loss: 2.54450265e-06
Iter: 1600 loss: 2.54303245e-06
Iter: 1601 loss: 2.54271185e-06
Iter: 1602 loss: 2.54091287e-06
Iter: 1603 loss: 2.55403484e-06
Iter: 1604 loss: 2.54077236e-06
Iter: 1605 loss: 2.53932421e-06
Iter: 1606 loss: 2.53971029e-06
Iter: 1607 loss: 2.53833832e-06
Iter: 1608 loss: 2.53616281e-06
Iter: 1609 loss: 2.54284487e-06
Iter: 1610 loss: 2.53538269e-06
Iter: 1611 loss: 2.53376e-06
Iter: 1612 loss: 2.53652843e-06
Iter: 1613 loss: 2.53305757e-06
Iter: 1614 loss: 2.53100029e-06
Iter: 1615 loss: 2.53876988e-06
Iter: 1616 loss: 2.53058715e-06
Iter: 1617 loss: 2.52898894e-06
Iter: 1618 loss: 2.53458484e-06
Iter: 1619 loss: 2.52858172e-06
Iter: 1620 loss: 2.52698169e-06
Iter: 1621 loss: 2.5267268e-06
Iter: 1622 loss: 2.52559494e-06
Iter: 1623 loss: 2.52357154e-06
Iter: 1624 loss: 2.54118777e-06
Iter: 1625 loss: 2.52351151e-06
Iter: 1626 loss: 2.52263726e-06
Iter: 1627 loss: 2.52258315e-06
Iter: 1628 loss: 2.52156315e-06
Iter: 1629 loss: 2.52177415e-06
Iter: 1630 loss: 2.52070367e-06
Iter: 1631 loss: 2.51974893e-06
Iter: 1632 loss: 2.5185409e-06
Iter: 1633 loss: 2.51853794e-06
Iter: 1634 loss: 2.51688152e-06
Iter: 1635 loss: 2.52478185e-06
Iter: 1636 loss: 2.51663414e-06
Iter: 1637 loss: 2.51542201e-06
Iter: 1638 loss: 2.51485199e-06
Iter: 1639 loss: 2.51423376e-06
Iter: 1640 loss: 2.51214715e-06
Iter: 1641 loss: 2.52608947e-06
Iter: 1642 loss: 2.51191068e-06
Iter: 1643 loss: 2.51061556e-06
Iter: 1644 loss: 2.5108825e-06
Iter: 1645 loss: 2.5096972e-06
Iter: 1646 loss: 2.50746189e-06
Iter: 1647 loss: 2.51031679e-06
Iter: 1648 loss: 2.50632388e-06
Iter: 1649 loss: 2.50445441e-06
Iter: 1650 loss: 2.51508845e-06
Iter: 1651 loss: 2.50425592e-06
Iter: 1652 loss: 2.50245694e-06
Iter: 1653 loss: 2.50387779e-06
Iter: 1654 loss: 2.50130438e-06
Iter: 1655 loss: 2.49960885e-06
Iter: 1656 loss: 2.51069127e-06
Iter: 1657 loss: 2.49943355e-06
Iter: 1658 loss: 2.49831601e-06
Iter: 1659 loss: 2.51388292e-06
Iter: 1660 loss: 2.49829827e-06
Iter: 1661 loss: 2.49701202e-06
Iter: 1662 loss: 2.50025096e-06
Iter: 1663 loss: 2.49651634e-06
Iter: 1664 loss: 2.49564732e-06
Iter: 1665 loss: 2.49433788e-06
Iter: 1666 loss: 2.49431287e-06
Iter: 1667 loss: 2.49283494e-06
Iter: 1668 loss: 2.49894583e-06
Iter: 1669 loss: 2.49247728e-06
Iter: 1670 loss: 2.49111326e-06
Iter: 1671 loss: 2.49134473e-06
Iter: 1672 loss: 2.49010486e-06
Iter: 1673 loss: 2.48809329e-06
Iter: 1674 loss: 2.4988085e-06
Iter: 1675 loss: 2.48784863e-06
Iter: 1676 loss: 2.486413e-06
Iter: 1677 loss: 2.48776746e-06
Iter: 1678 loss: 2.48565675e-06
Iter: 1679 loss: 2.48364154e-06
Iter: 1680 loss: 2.48677952e-06
Iter: 1681 loss: 2.48262745e-06
Iter: 1682 loss: 2.48117544e-06
Iter: 1683 loss: 2.49193727e-06
Iter: 1684 loss: 2.48101014e-06
Iter: 1685 loss: 2.47954949e-06
Iter: 1686 loss: 2.47850971e-06
Iter: 1687 loss: 2.47800017e-06
Iter: 1688 loss: 2.4762985e-06
Iter: 1689 loss: 2.49446839e-06
Iter: 1690 loss: 2.47632488e-06
Iter: 1691 loss: 2.47498588e-06
Iter: 1692 loss: 2.48009724e-06
Iter: 1693 loss: 2.47459911e-06
Iter: 1694 loss: 2.47310163e-06
Iter: 1695 loss: 2.49197728e-06
Iter: 1696 loss: 2.47306957e-06
Iter: 1697 loss: 2.47252046e-06
Iter: 1698 loss: 2.47124399e-06
Iter: 1699 loss: 2.48551532e-06
Iter: 1700 loss: 2.47105731e-06
Iter: 1701 loss: 2.46962827e-06
Iter: 1702 loss: 2.47679054e-06
Iter: 1703 loss: 2.46934451e-06
Iter: 1704 loss: 2.467998e-06
Iter: 1705 loss: 2.46792501e-06
Iter: 1706 loss: 2.46692571e-06
Iter: 1707 loss: 2.4654878e-06
Iter: 1708 loss: 2.47935645e-06
Iter: 1709 loss: 2.46530817e-06
Iter: 1710 loss: 2.46418108e-06
Iter: 1711 loss: 2.46412037e-06
Iter: 1712 loss: 2.46323316e-06
Iter: 1713 loss: 2.46135096e-06
Iter: 1714 loss: 2.46596755e-06
Iter: 1715 loss: 2.46053787e-06
Iter: 1716 loss: 2.45906e-06
Iter: 1717 loss: 2.46437935e-06
Iter: 1718 loss: 2.458675e-06
Iter: 1719 loss: 2.45684487e-06
Iter: 1720 loss: 2.4556557e-06
Iter: 1721 loss: 2.45495266e-06
Iter: 1722 loss: 2.45313254e-06
Iter: 1723 loss: 2.47707021e-06
Iter: 1724 loss: 2.4531173e-06
Iter: 1725 loss: 2.45171645e-06
Iter: 1726 loss: 2.45446063e-06
Iter: 1727 loss: 2.45114e-06
Iter: 1728 loss: 2.45020419e-06
Iter: 1729 loss: 2.45000956e-06
Iter: 1730 loss: 2.4495157e-06
Iter: 1731 loss: 2.44832563e-06
Iter: 1732 loss: 2.46121886e-06
Iter: 1733 loss: 2.44813737e-06
Iter: 1734 loss: 2.4467688e-06
Iter: 1735 loss: 2.44795137e-06
Iter: 1736 loss: 2.4459157e-06
Iter: 1737 loss: 2.44434545e-06
Iter: 1738 loss: 2.45153774e-06
Iter: 1739 loss: 2.44395255e-06
Iter: 1740 loss: 2.44259036e-06
Iter: 1741 loss: 2.44444163e-06
Iter: 1742 loss: 2.44182047e-06
Iter: 1743 loss: 2.44031958e-06
Iter: 1744 loss: 2.44377384e-06
Iter: 1745 loss: 2.43965815e-06
Iter: 1746 loss: 2.43785325e-06
Iter: 1747 loss: 2.44255352e-06
Iter: 1748 loss: 2.43712748e-06
Iter: 1749 loss: 2.4356807e-06
Iter: 1750 loss: 2.43863497e-06
Iter: 1751 loss: 2.43507475e-06
Iter: 1752 loss: 2.43319914e-06
Iter: 1753 loss: 2.43601767e-06
Iter: 1754 loss: 2.43221348e-06
Iter: 1755 loss: 2.43082377e-06
Iter: 1756 loss: 2.44016019e-06
Iter: 1757 loss: 2.43065824e-06
Iter: 1758 loss: 2.42923352e-06
Iter: 1759 loss: 2.42892656e-06
Iter: 1760 loss: 2.42806027e-06
Iter: 1761 loss: 2.4291403e-06
Iter: 1762 loss: 2.42717738e-06
Iter: 1763 loss: 2.42674923e-06
Iter: 1764 loss: 2.42572901e-06
Iter: 1765 loss: 2.43985187e-06
Iter: 1766 loss: 2.42570059e-06
Iter: 1767 loss: 2.4246217e-06
Iter: 1768 loss: 2.42354849e-06
Iter: 1769 loss: 2.4232495e-06
Iter: 1770 loss: 2.42144915e-06
Iter: 1771 loss: 2.43774844e-06
Iter: 1772 loss: 2.42137139e-06
Iter: 1773 loss: 2.42011447e-06
Iter: 1774 loss: 2.41998259e-06
Iter: 1775 loss: 2.41907674e-06
Iter: 1776 loss: 2.41732914e-06
Iter: 1777 loss: 2.42609553e-06
Iter: 1778 loss: 2.41711609e-06
Iter: 1779 loss: 2.41569069e-06
Iter: 1780 loss: 2.42253418e-06
Iter: 1781 loss: 2.4154424e-06
Iter: 1782 loss: 2.41435919e-06
Iter: 1783 loss: 2.41511862e-06
Iter: 1784 loss: 2.41368889e-06
Iter: 1785 loss: 2.41221392e-06
Iter: 1786 loss: 2.41418911e-06
Iter: 1787 loss: 2.41134694e-06
Iter: 1788 loss: 2.40998042e-06
Iter: 1789 loss: 2.41656653e-06
Iter: 1790 loss: 2.40974509e-06
Iter: 1791 loss: 2.40819872e-06
Iter: 1792 loss: 2.40863847e-06
Iter: 1793 loss: 2.40719896e-06
Iter: 1794 loss: 2.4092642e-06
Iter: 1795 loss: 2.40676445e-06
Iter: 1796 loss: 2.40634836e-06
Iter: 1797 loss: 2.40563054e-06
Iter: 1798 loss: 2.42207807e-06
Iter: 1799 loss: 2.40562122e-06
Iter: 1800 loss: 2.40457416e-06
Iter: 1801 loss: 2.40267696e-06
Iter: 1802 loss: 2.44071e-06
Iter: 1803 loss: 2.4027006e-06
Iter: 1804 loss: 2.40117606e-06
Iter: 1805 loss: 2.40116287e-06
Iter: 1806 loss: 2.39999031e-06
Iter: 1807 loss: 2.40076e-06
Iter: 1808 loss: 2.39934525e-06
Iter: 1809 loss: 2.39780138e-06
Iter: 1810 loss: 2.40128929e-06
Iter: 1811 loss: 2.39722658e-06
Iter: 1812 loss: 2.39597398e-06
Iter: 1813 loss: 2.40104487e-06
Iter: 1814 loss: 2.39571432e-06
Iter: 1815 loss: 2.39432666e-06
Iter: 1816 loss: 2.39756628e-06
Iter: 1817 loss: 2.39392466e-06
Iter: 1818 loss: 2.39235806e-06
Iter: 1819 loss: 2.39316159e-06
Iter: 1820 loss: 2.39135875e-06
Iter: 1821 loss: 2.38982761e-06
Iter: 1822 loss: 2.39922474e-06
Iter: 1823 loss: 2.38964731e-06
Iter: 1824 loss: 2.3880234e-06
Iter: 1825 loss: 2.38761959e-06
Iter: 1826 loss: 2.38663461e-06
Iter: 1827 loss: 2.38827715e-06
Iter: 1828 loss: 2.38611938e-06
Iter: 1829 loss: 2.3856087e-06
Iter: 1830 loss: 2.38498887e-06
Iter: 1831 loss: 2.38492703e-06
Iter: 1832 loss: 2.38406301e-06
Iter: 1833 loss: 2.38252414e-06
Iter: 1834 loss: 2.38254415e-06
Iter: 1835 loss: 2.3812172e-06
Iter: 1836 loss: 2.39459132e-06
Iter: 1837 loss: 2.38115126e-06
Iter: 1838 loss: 2.38001894e-06
Iter: 1839 loss: 2.38076927e-06
Iter: 1840 loss: 2.37935546e-06
Iter: 1841 loss: 2.37772929e-06
Iter: 1842 loss: 2.38190955e-06
Iter: 1843 loss: 2.37723862e-06
Iter: 1844 loss: 2.37611243e-06
Iter: 1845 loss: 2.37747145e-06
Iter: 1846 loss: 2.37552968e-06
Iter: 1847 loss: 2.37388326e-06
Iter: 1848 loss: 2.37624454e-06
Iter: 1849 loss: 2.37311792e-06
Iter: 1850 loss: 2.37175209e-06
Iter: 1851 loss: 2.3831235e-06
Iter: 1852 loss: 2.37170707e-06
Iter: 1853 loss: 2.37058384e-06
Iter: 1854 loss: 2.37109771e-06
Iter: 1855 loss: 2.36980213e-06
Iter: 1856 loss: 2.36832329e-06
Iter: 1857 loss: 2.36860865e-06
Iter: 1858 loss: 2.36724236e-06
Iter: 1859 loss: 2.36605547e-06
Iter: 1860 loss: 2.36602637e-06
Iter: 1861 loss: 2.36510095e-06
Iter: 1862 loss: 2.37906465e-06
Iter: 1863 loss: 2.36505457e-06
Iter: 1864 loss: 2.36441974e-06
Iter: 1865 loss: 2.36302139e-06
Iter: 1866 loss: 2.3753264e-06
Iter: 1867 loss: 2.36269489e-06
Iter: 1868 loss: 2.36143205e-06
Iter: 1869 loss: 2.36553569e-06
Iter: 1870 loss: 2.36102983e-06
Iter: 1871 loss: 2.3595444e-06
Iter: 1872 loss: 2.36197366e-06
Iter: 1873 loss: 2.35885955e-06
Iter: 1874 loss: 2.35760717e-06
Iter: 1875 loss: 2.37197628e-06
Iter: 1876 loss: 2.35758239e-06
Iter: 1877 loss: 2.35676021e-06
Iter: 1878 loss: 2.35633865e-06
Iter: 1879 loss: 2.35607035e-06
Iter: 1880 loss: 2.35461675e-06
Iter: 1881 loss: 2.35947664e-06
Iter: 1882 loss: 2.35423113e-06
Iter: 1883 loss: 2.3532773e-06
Iter: 1884 loss: 2.35527978e-06
Iter: 1885 loss: 2.35286529e-06
Iter: 1886 loss: 2.35152902e-06
Iter: 1887 loss: 2.35379139e-06
Iter: 1888 loss: 2.35084622e-06
Iter: 1889 loss: 2.34979962e-06
Iter: 1890 loss: 2.35577272e-06
Iter: 1891 loss: 2.34960089e-06
Iter: 1892 loss: 2.348625e-06
Iter: 1893 loss: 2.34993468e-06
Iter: 1894 loss: 2.34806453e-06
Iter: 1895 loss: 2.34800336e-06
Iter: 1896 loss: 2.34750041e-06
Iter: 1897 loss: 2.34702475e-06
Iter: 1898 loss: 2.34564413e-06
Iter: 1899 loss: 2.3569678e-06
Iter: 1900 loss: 2.34538584e-06
Iter: 1901 loss: 2.34409663e-06
Iter: 1902 loss: 2.34769686e-06
Iter: 1903 loss: 2.34368235e-06
Iter: 1904 loss: 2.34250956e-06
Iter: 1905 loss: 2.3425946e-06
Iter: 1906 loss: 2.34163372e-06
Iter: 1907 loss: 2.34072786e-06
Iter: 1908 loss: 2.3407149e-06
Iter: 1909 loss: 2.33991886e-06
Iter: 1910 loss: 2.33906508e-06
Iter: 1911 loss: 2.3389432e-06
Iter: 1912 loss: 2.33746505e-06
Iter: 1913 loss: 2.34613572e-06
Iter: 1914 loss: 2.3372852e-06
Iter: 1915 loss: 2.33640094e-06
Iter: 1916 loss: 2.3366681e-06
Iter: 1917 loss: 2.33579976e-06
Iter: 1918 loss: 2.33445962e-06
Iter: 1919 loss: 2.34093886e-06
Iter: 1920 loss: 2.33423975e-06
Iter: 1921 loss: 2.33335231e-06
Iter: 1922 loss: 2.33613764e-06
Iter: 1923 loss: 2.3330731e-06
Iter: 1924 loss: 2.33198944e-06
Iter: 1925 loss: 2.33125365e-06
Iter: 1926 loss: 2.33082505e-06
Iter: 1927 loss: 2.33069773e-06
Iter: 1928 loss: 2.33022706e-06
Iter: 1929 loss: 2.32956472e-06
Iter: 1930 loss: 2.33086826e-06
Iter: 1931 loss: 2.32921e-06
Iter: 1932 loss: 2.32867069e-06
Iter: 1933 loss: 2.32743082e-06
Iter: 1934 loss: 2.34379968e-06
Iter: 1935 loss: 2.32737921e-06
Iter: 1936 loss: 2.32587809e-06
Iter: 1937 loss: 2.32786579e-06
Iter: 1938 loss: 2.32516231e-06
Iter: 1939 loss: 2.32391631e-06
Iter: 1940 loss: 2.33322135e-06
Iter: 1941 loss: 2.32373077e-06
Iter: 1942 loss: 2.32266484e-06
Iter: 1943 loss: 2.32583625e-06
Iter: 1944 loss: 2.32240382e-06
Iter: 1945 loss: 2.32119055e-06
Iter: 1946 loss: 2.32675484e-06
Iter: 1947 loss: 2.32092043e-06
Iter: 1948 loss: 2.32017101e-06
Iter: 1949 loss: 2.31935451e-06
Iter: 1950 loss: 2.31914964e-06
Iter: 1951 loss: 2.3176442e-06
Iter: 1952 loss: 2.32852358e-06
Iter: 1953 loss: 2.31745184e-06
Iter: 1954 loss: 2.31647778e-06
Iter: 1955 loss: 2.31783361e-06
Iter: 1956 loss: 2.31594413e-06
Iter: 1957 loss: 2.31456306e-06
Iter: 1958 loss: 2.31677836e-06
Iter: 1959 loss: 2.31396052e-06
Iter: 1960 loss: 2.31305421e-06
Iter: 1961 loss: 2.31302715e-06
Iter: 1962 loss: 2.31256627e-06
Iter: 1963 loss: 2.31253489e-06
Iter: 1964 loss: 2.31218291e-06
Iter: 1965 loss: 2.31109152e-06
Iter: 1966 loss: 2.31322429e-06
Iter: 1967 loss: 2.31025388e-06
Iter: 1968 loss: 2.30896899e-06
Iter: 1969 loss: 2.32285402e-06
Iter: 1970 loss: 2.30897899e-06
Iter: 1971 loss: 2.30793444e-06
Iter: 1972 loss: 2.3067139e-06
Iter: 1973 loss: 2.30664091e-06
Iter: 1974 loss: 2.30544492e-06
Iter: 1975 loss: 2.30537648e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi-2_phi3
+ date
Wed Oct 21 17:22:55 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi2.8/300_300_300_1 --function f1 --psi -2 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi-2_phi3/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c586e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c5816a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c5a82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c5a8d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c4d7488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c4d7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c3dbc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c476950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c476488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c45f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c3bea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c31de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c317488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c317c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c3868c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c2ea730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c2ea400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c28bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5149068950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5149093f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5149053620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f51490456a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f514c2668c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148fc4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148fca598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148fcabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f51490219d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5149009950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f51490096a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148f10620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148f10598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148e707b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148e70378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148f36510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148e52a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5148e1e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.023274278
test_loss: 0.022741927
train_loss: 0.011016102
test_loss: 0.011131989
train_loss: 0.007237138
test_loss: 0.00863787
train_loss: 0.00669277
test_loss: 0.00827559
train_loss: 0.0063235504
test_loss: 0.0075984863
train_loss: 0.0054894416
test_loss: 0.007980763
train_loss: 0.005630607
test_loss: 0.00738827
train_loss: 0.0055614808
test_loss: 0.0075218137
train_loss: 0.0049719648
test_loss: 0.007246999
train_loss: 0.005494129
test_loss: 0.0069656675
train_loss: 0.0054546795
test_loss: 0.007325562
train_loss: 0.0057539605
test_loss: 0.0073487526
train_loss: 0.0052466495
test_loss: 0.007100255
train_loss: 0.0054461746
test_loss: 0.0070541385
train_loss: 0.005377919
test_loss: 0.007025024
train_loss: 0.005231712
test_loss: 0.007183807
train_loss: 0.005137408
test_loss: 0.0070444113
train_loss: 0.0052017514
test_loss: 0.007292895
train_loss: 0.00515196
test_loss: 0.007022777
train_loss: 0.004486874
test_loss: 0.0068863295
train_loss: 0.005010705
test_loss: 0.006937164
train_loss: 0.00467307
test_loss: 0.0068160202
train_loss: 0.005416968
test_loss: 0.0068338783
train_loss: 0.0051068715
test_loss: 0.0073070433
train_loss: 0.005181445
test_loss: 0.0074662496
train_loss: 0.004660772
test_loss: 0.0069534257
train_loss: 0.0052830707
test_loss: 0.0067651607
train_loss: 0.0049985754
test_loss: 0.0068351687
train_loss: 0.005303207
test_loss: 0.0069389516
train_loss: 0.004712505
test_loss: 0.0067591933
train_loss: 0.0050300863
test_loss: 0.006794292
train_loss: 0.0047129192
test_loss: 0.0068841903
train_loss: 0.004781775
test_loss: 0.0068014115
train_loss: 0.004872055
test_loss: 0.0064585614
train_loss: 0.005119782
test_loss: 0.006602534
train_loss: 0.0045707575
test_loss: 0.006790493
train_loss: 0.004706994
test_loss: 0.00673415
train_loss: 0.004963182
test_loss: 0.007206591
train_loss: 0.004844828
test_loss: 0.0068671573
train_loss: 0.004461536
test_loss: 0.006575636
train_loss: 0.0046101385
test_loss: 0.0067959703
train_loss: 0.0047103344
test_loss: 0.0065554637
train_loss: 0.0043342984
test_loss: 0.0066061313
train_loss: 0.004813781
test_loss: 0.00659571
train_loss: 0.005066055
test_loss: 0.006623543
train_loss: 0.0044940803
test_loss: 0.006463256
train_loss: 0.0043308455
test_loss: 0.0067194174
train_loss: 0.004372559
test_loss: 0.0064746984
train_loss: 0.005247499
test_loss: 0.006981028
train_loss: 0.0046036066
test_loss: 0.006463317
train_loss: 0.0046578846
test_loss: 0.0065774303
train_loss: 0.004854331
test_loss: 0.0066086967
train_loss: 0.0045374893
test_loss: 0.0065676332
train_loss: 0.004620526
test_loss: 0.006793562
train_loss: 0.0047090263
test_loss: 0.0066919453
train_loss: 0.0045049977
test_loss: 0.006646032
train_loss: 0.004637533
test_loss: 0.006766746
train_loss: 0.0048882626
test_loss: 0.0066429763
train_loss: 0.0042773015
test_loss: 0.0065557845
train_loss: 0.004458382
test_loss: 0.006498665
train_loss: 0.004487479
test_loss: 0.0063764127
train_loss: 0.0044759153
test_loss: 0.0064521357
train_loss: 0.0042723706
test_loss: 0.0063320743
train_loss: 0.0046503083
test_loss: 0.0064803297
train_loss: 0.004481983
test_loss: 0.006364342
train_loss: 0.004561622
test_loss: 0.0065209754
train_loss: 0.004916268
test_loss: 0.0066744005
train_loss: 0.004710653
test_loss: 0.00646358
train_loss: 0.0046501067
test_loss: 0.0064538666
train_loss: 0.0048516276
test_loss: 0.0064590233
train_loss: 0.0049347593
test_loss: 0.0065804627
train_loss: 0.0046203705
test_loss: 0.006448767
train_loss: 0.004526497
test_loss: 0.0066846106
train_loss: 0.0043534073
test_loss: 0.006361062
train_loss: 0.004646114
test_loss: 0.006458958
train_loss: 0.0044172783
test_loss: 0.006430609
train_loss: 0.004500125
test_loss: 0.006376879
train_loss: 0.004466825
test_loss: 0.0066218185
train_loss: 0.0046188557
test_loss: 0.0064380853
train_loss: 0.0046704207
test_loss: 0.006505174
train_loss: 0.004402871
test_loss: 0.006397037
train_loss: 0.0044437884
test_loss: 0.0066409768
train_loss: 0.0050171595
test_loss: 0.006668797
train_loss: 0.0045522745
test_loss: 0.0066104885
train_loss: 0.0046144733
test_loss: 0.006511345
train_loss: 0.0044273306
test_loss: 0.006413293
train_loss: 0.004712229
test_loss: 0.0065944297
train_loss: 0.0042547355
test_loss: 0.006511032
train_loss: 0.0045766253
test_loss: 0.0063646105
train_loss: 0.004661373
test_loss: 0.0064931316
train_loss: 0.0045243865
test_loss: 0.0065061445
train_loss: 0.004556702
test_loss: 0.0062382105
train_loss: 0.0047745686
test_loss: 0.006302165
train_loss: 0.0045129787
test_loss: 0.006441755
train_loss: 0.004455127
test_loss: 0.00632644
train_loss: 0.0048283357
test_loss: 0.0064420793
train_loss: 0.0043621208
test_loss: 0.0064337626
train_loss: 0.004557842
test_loss: 0.0064302706
train_loss: 0.005098705
test_loss: 0.006634989
train_loss: 0.004756579
test_loss: 0.0065042395
train_loss: 0.0050234376
test_loss: 0.006672005
train_loss: 0.0048414716
test_loss: 0.006437649
train_loss: 0.004553241
test_loss: 0.0063773924
train_loss: 0.0042534145
test_loss: 0.0064903875
train_loss: 0.0044770716
test_loss: 0.0066803405
train_loss: 0.004615255
test_loss: 0.006523812
train_loss: 0.004474621
test_loss: 0.00640475
train_loss: 0.0044237636
test_loss: 0.006371678
train_loss: 0.0039243856
test_loss: 0.0061546103
train_loss: 0.0041540675
test_loss: 0.006368658
train_loss: 0.0041556368
test_loss: 0.006204418
train_loss: 0.0046618865
test_loss: 0.006446108
train_loss: 0.0047622225
test_loss: 0.006764238
train_loss: 0.0044064373
test_loss: 0.00665504
train_loss: 0.0044547617
test_loss: 0.0064585255
train_loss: 0.0043722484
test_loss: 0.0065215235
train_loss: 0.004416002
test_loss: 0.006228968
train_loss: 0.0042170417
test_loss: 0.006488044
train_loss: 0.0044837166
test_loss: 0.006409122
train_loss: 0.004241586
test_loss: 0.0063505895
train_loss: 0.0045800563
test_loss: 0.006217993
train_loss: 0.004341623
test_loss: 0.0063992455
train_loss: 0.0041961162
test_loss: 0.0063288347
train_loss: 0.004318118
test_loss: 0.006037308
train_loss: 0.004680037
test_loss: 0.006326096
train_loss: 0.0040618377
test_loss: 0.0061345547
train_loss: 0.0040185945
test_loss: 0.006136968
train_loss: 0.0042478074
test_loss: 0.0061988267
train_loss: 0.0044495636
test_loss: 0.006312143
train_loss: 0.0040871636
test_loss: 0.006139549
train_loss: 0.0047191717
test_loss: 0.0062272977
train_loss: 0.004378955
test_loss: 0.0062422957
train_loss: 0.0045093633
test_loss: 0.0069970023
train_loss: 0.0045594205
test_loss: 0.0064982534
train_loss: 0.004786635
test_loss: 0.0063172174
train_loss: 0.004951894
test_loss: 0.006377333
train_loss: 0.004421142
test_loss: 0.006406465
train_loss: 0.0041079554
test_loss: 0.006406048
train_loss: 0.0042857514
test_loss: 0.0063897897
train_loss: 0.0048244726
test_loss: 0.006499662
train_loss: 0.003888708
test_loss: 0.00608995
train_loss: 0.0046189027
test_loss: 0.0064933533
train_loss: 0.0045200055
test_loss: 0.006258481
train_loss: 0.004076706
test_loss: 0.0060041933
train_loss: 0.004146575
test_loss: 0.00597734
train_loss: 0.004411599
test_loss: 0.0062458892
train_loss: 0.004559106
test_loss: 0.006305991
train_loss: 0.0040500164
test_loss: 0.006108821
train_loss: 0.004393909
test_loss: 0.006094694
train_loss: 0.004432248
test_loss: 0.006318965
train_loss: 0.004290299
test_loss: 0.0061243847
train_loss: 0.0044494844
test_loss: 0.0067322515
train_loss: 0.0044277017
test_loss: 0.006309947
train_loss: 0.004097999
test_loss: 0.0063618743
train_loss: 0.004406574
test_loss: 0.0061933417
train_loss: 0.0045464444
test_loss: 0.0063677523
train_loss: 0.0043147057
test_loss: 0.006323362
train_loss: 0.004258401
test_loss: 0.0064820987
train_loss: 0.0042521944
test_loss: 0.0061981063
train_loss: 0.004007277
test_loss: 0.0060884343
train_loss: 0.0040784674
test_loss: 0.006343145
train_loss: 0.004330186
test_loss: 0.006291225
train_loss: 0.004426053
test_loss: 0.006380592
train_loss: 0.0046434435
test_loss: 0.006265145
train_loss: 0.0044252556
test_loss: 0.006589593
train_loss: 0.0043629175
test_loss: 0.0062448587
train_loss: 0.004640965
test_loss: 0.006360286
train_loss: 0.004363115
test_loss: 0.0062743695
train_loss: 0.0041580987
test_loss: 0.0061076507
train_loss: 0.0041136392
test_loss: 0.006499244
train_loss: 0.0043342314
test_loss: 0.0063045467